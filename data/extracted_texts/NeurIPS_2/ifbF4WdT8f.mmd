# EvoPrompting: Language Models for Code-Level Neural Architecture Search

Angelica Chen

New York University

angelica.chen@nyu.edu

Work done while a Student Researcher at Google DeepMind.

David M. Dohan

OpenAI

david@ddohan.com

Work done while at Google DeepMind.

Work done while a Student Researcher at Google DeepMind.

New York University

angelica.chen@nyu.edu

Work done while a Student Researcher at Google DeepMind.

David R. So

Jane Street

david.r.so.ai@gmail.com

Work done while a Student Researcher at Google DeepMind.

Work done while a Student Researcher at Google DeepMind.

Work done while a Student Researcher at Google DeepMind.

Work done while a Student Researcher at Google DeepMind.

Work done while a Student Researcher at Google DeepMind.

Work done while a Student Researcher at Google DeepMind.

###### Abstract

Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as general adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design _novel_ architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic reasoning tasks while maintaining similar model size. EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.

## 1 Introduction

Scaling of Transformers (Vaswani et al., 2017) has produced language models (LM) with impressive performance. Beyond achieving state-of-the-art results on conventional natural language processing tasks, these LMs demonstrate breakthrough technical capabilities, such as learning how to code (Chen et al., 2021), doing math (Noorbakhsh et al., 2021), and solving reasoning problems (Wei et al., 2022). Yet, despite these strides, several works have noted LMs' current limitations in solving complex problems and creating novel solutions (Qian et al., 2022; Dakhel et al., 2022). In this work, we improve upon a base LM's ability to propose novel and diverse solutions to complex reasoning problems by iteratively evolving in-context prompts and prompt-tuning the LM. We call this technique EvoPrompting and demonstrate its success on the difficult task of deep learning architecture design. Our key finding is that, while LMs perform poorly at designing novel and effective neural architectures via naive few-shot prompting, EvoPrompting enables LMs to create novel and effective deep neural architectures, particularly when combined with prompt-tuning methods.

[MISSING_PAGE_EMPTY:2]

shot prompting with EvoPrompting enables LMs to create architectures that outperform those designed by human experts.
3. Novel graph neural network architectures that were discovered using EvoPrompting. These architectures outperform the current state-of-the-art architecture, Triplet-GMPNN (Ibarz et al., 2022), on 21 out of 30 CLRS Algorithmic Reasoning Benchmark tasks (Appx. 3).

## 2 Related Work

LMs for code generationScaling Transformers (Vaswani et al., 2017) is currently a popular route for reliably creating state-of-the-art natural language systems (Brown et al., 2020; Du et al., 2021; BigScience Workshop et al., 2022; Zhang et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). Many works have observed that large LMs are capable of performing technical tasks such as writing code (Chen et al., 2021), doing math (Noorbakhsh et al., 2021), and solving complex reasoning problems (Wei et al., 2022). Our work is most closely related to efforts that have applied LMs to coding tasks (Chen et al., 2021; Odena et al., 2021; Xu et al., 2022; Wang et al., 2021; Ahmad et al., 2021; Feng et al., 2020), since our technique proposes architectures in code.

PromptingBrown et al. (2020) demonstrated that LMs can be prompted with in-context examples to steer LM decoding towards solving problems in-context without gradient updates. Numerous works have utilized this prompting to further boost LM abilities (Sanh et al., 2021; Wei et al., 2022; Kojima et al., 2022). Others have focused on optimizing these prompts (Min et al., 2022; Liu et al., 2021) as via approaches such as augmentation with retrieval systems (Rubin et al., 2021), permutations of few-shot examples (Lu et al., 2021; Zhao et al., 2021), generating prompts via LMs (Zhou et al., 2022), and instruction-tuning (Wei et al., 2021; Ouyang et al., 2022; Sanh et al., 2021). From the perspective of Dohan et al. (2022), prompts are parameters that can be tuned using probabilistic inference techniques. Brooks et al. (2022) proposes using few-shot prompts to implement both the rollout policy and world model of a policy iteration algorithm. Our EvoPrompting method extends these efforts by proposing evolutionary search as a means to both better design prompts for ICL and tune the base LM to use the prompt more effectively.

Evolutionary AlgorithmsOur method is closely related to evolutionary neural architecture search (NAS) (Real et al., 2017, 2018; Elsken et al., 2018; So et al., 2019; Liu et al., 2020), in which architectures are represented as discrete DNAs, and evolved and filtered based on fitness metrics that assess architecture performance. However, our method can search over arbitrary strings of code, whereas conventional evolutionary NAS algorithms rely on hand-crafted search spaces that can strongly bias and contrain the search (Li and Talwalkar, 2019; Sciuto et al., 2019; Bender et al., 2020; Real et al., 2020; So et al., 2021). A work close to ours is Lehman et al. (2022), in which an LM is fine-tuned to produce Python code diffs given one of three fixed messages that describe what should be changed, and then used as the mutation operator in an evolutionary algorithm. Their work is validated on the Sodarace domain. Our work differs in that we use an LM as a crossover operator, without specifying the class of changes to make, which may offer greater flexibility. Furthermore, we evaluate our approach on the real-world task of NAS, rely on mixed temperature sampling of the LM for diversity instead of using a QD algorithm, and also use prompt-tuning in our algorithm. We choose not to use a QD algorithm such as MAP-Elites since this approach requires the design and discretization of a descriptor space, which is complex and difficult to hand-design for the space of all possible neural networks.

Another concurrent work is Meyerson et al. (2023), which uses an LM as a crossover operator to produce variations of text-based genotypes in the domains of symbolic regression, text sentiment, images, and Sodaracer programs. Like Lehman et al. (2022), they use MAP-Elites to trade off quality with diversity in two of the domains and demonstrate that their overall algorithm reliably produces a diverse range of outputs. They additionally demonstrated performance comparable to state-of-the-art approaches on the toy task of symbolic regression. Their study varies from ours in a number of ways - we apply our algorithm to the real-world task of NAS, we optimize for a tradeoff between state-of-the-art task performance and model size, we condition on target performance in our prompts, we do not use MAP-Elites, and we use prompt-tuning to iteratively improve the LM's crossover abilities instead.

EvoPrompting Method

### Architecture search problem formulation

Let our target task be denoted by \(\) and \(\) be a dataset consisting of input-output pairs \((x,y)\) for task \(\). Define the probability distribution \(_{}:\{0,1\}\) over vocabulary \(\) as a language/code model parameterized by \(\), from which we can sample code segments \(c^{*}\) (for \(^{*}\) the Kleene closure of \(\), _i.e._ the set of all concatenations of symbols in \(\)). We also have an evaluation function \(_{}(c,):^{*} \) that trains the model architecture given by code \(c\) on \(\) and outputs some real-valued fitness score \(s\), which can be a function of model accuracy and other model characteristics. Our ultimate goal is to identify some set of code samples \(c^{*}\) that define neural network architectures that, when trained on \(\), maximize the reward \(_{}(c,)\).

### LMs for evolutionary crossover and mutation

The goal of our algorithm is to generate a set \(C\) consisting of \(k\) neural network architectures that maximize the reward \(_{}(c,)\) for arbitrary pairs of \((,)\):

\[_{C=[c]c_{}\\ |C|=k}_{c C}_{(x,y)}[ _{}(c,)] \]

Since this optimization problem is generally intractable, we turn to a black-box evolutionary approach for iteratively generating, scoring, and selecting the best neural network architectures. Indeed, evolution has been demonstrated to perform particularly well in this domain because of how sparse high quality solutions tend to be (Real et al., 2017, 2018). Although evolution has been used for architecture search many times before (Real et al., 2017, 2018; Elsken et al., 2018; So et al., 2019), we improve upon this approach by using an LM for crossover and mutation operations.

Using an LM in this manner has multiple appealing properties. While past evolutionary approaches for neural architecture search have required careful design and specification of a discrete search space (_e.g._ the space of high level modules (Real et al., 2018; So et al., 2019), TensorFlow statements (So et al., 2021), or basic mathematical operations (Real et al., 2020)), our algorithm's search space includes any neural network architecture that can be represented in Python. This allows for greater flexibility and diversity of the output architectures, and reduces the amount of manual design and human bias involved in the algorithm. Furthermore, modern pre-trained LMs are typically trained on massive datasets containing a significant number of source code files. This pre-training process encodes useful knowledge about code structure and functionality that is not otherwise available in evolutionary algorithms. Lastly, LMs can also be used as _self-adaptive crossover operators_, in which the crossover operator is incrementally trained round after round to generate higher reward crossovers.

### EvoPrompting meta-learning algorithm

Our complete algorithm is described in Algorithm 1. At the core of our algorithm is a scoring function, which describes the general "fitness" of a model on the task at hand. Since higher accuracy can often be achieved simply by increasing the number of parameters in a model, we use the negative product of the validation error and the model size as the fitness (see step 6 in Algorithm 3). More complicated objective functions have previously been used for dual objective neural architecture search (Bender et al., 2020), but we find this simple product works best in our case and requires minimal tuning. Generally the higher the fitness, the better (with some caveats, noted in our description of fitness-based selection below).

The end-to-end meta-learning algorithm has several stages, which we describe below:

InitializationWe start by setting our global historical population \(G\) to the empty list and initializing our current population \(P\) with a few seed architectures that are known to be well-designed (step 3 in Algorithm 1), which _warm-starts_ the search (So et al., 2019). These seed models are evaluated using the same \(_{}(c,)\) function that is used to evaluate new candidate models (see below).

```
1:Input: LM \(_{_{0}}\), dataset \(\), task \(\), \(T\) number of rounds, \(m\) number of few-shot prompts per round, \(n\) number of samples to generate per prompt, \(k\) number of in-context examples per prompt, \(p\) number of survivors to select per generation, \(\) the upper threshold for the test error
2:\(G[]\)
3:\(P(p)\)
4:\(t 0\)
5:while\(t<T\)do
6:\(C(_{_{t}},P,m,k,n)\)
7:\(C_{}(C,,,)\)
8:\(G G+C_{}\)
9:if\(t<T-1\)then
10:\(P(G,p)\)
11:\(_{t+1}(_{t},C_{} P)\)
12:endif
13:\(t t+1\)
14:endwhile
15: Return \((G,p)\)
```

**Algorithm 1** Complete meta-learning evolutionary algorithm using \(p_{}\) as a crossover and mutation operator.

```
1:Input: LM \(_{}\), population of code samples and fitnesses \(P=\{(c,s)\,|\,c^{*},_{}(c,)=s\}\), \(m\) number of few-shot prompts to create, \(k\) number of in-context examples in each prompt, and \(n\) number of samples to sample per prompt.
2:\(C[]\)
3:\(i 0\)
4:while\(i<m\)do
5:\(E\{x_{j}\}_{j=1}^{k}\), where \(x_{j}}{{}}(P)\)
6:\(p(E)\)
7:\(C_{i}\{c_{j}\}_{j=1}^{n}\), where \(c_{j}}{{}}_{}(\,\,|\,p)\)
8:\(C C+C_{i}\)
9:\(i i+1\)
10:endwhile
11:Output:\(C\)
```

**Algorithm 2** The crossover and mutation algorithm, \((_{_{t}},P,m,k,n)\), where \((P)\) denotes the uniform distribution over the set \(P\). The set of potential parents \(P\) consists of the top examples from the previous round.

```
1:Input: set of code samples \(C\), task \(\), dataset \(\), evaluation function \(_{}(c,)\), upper threshold for error \(\)
2:\(C_{}[]\)
3:for c in \(C\)do
4: c.error \(_{}(,)\)
5:if c.error \(<\)then
6:\(s-\)
7:\(C_{} C_{}+[(c,s)]\)
8:endif
9:endfor
10:Output:\(C_{}\)
```

**Algorithm 3** The algorithm for filtering and scoring child models, \((C,,,)\).

Crossing over and mutating the parent modelsTo mutate and apply crossover to the parents \(P\) selected in the last step, we use both the source code and the evaluation metrics of each model in \(P\) to create few-shot prompts.

In the last line of the prompt, we create a target set of metrics to condition \(_{}\)'s generations on that indicate the desired validation accuracy and model size of the proposed architecture. We set the target model size as \(90\%\) of the minimum model size of the parent models, rounded to the nearest 100 parameters, and the target validation accuracy as \(102\%\) of the maximum validation accuracy of the parent models, rounded to the nearest tenth of a percent. We create \(m\) such prompts per round, each with \(k\) in-context examples selected uniformly at random from \(P\). An example of a prompt is shown in Listing 1.

```
1"""Metrics:
2{'num_params': '4800', 'val_accuracy': '0.865'}
3"""
4classModel(nn.Module):
5@nn.compact
6def__call__(self,x):
7x=nn.Dense(features=10)(x)
8returnx
9
10"""Metrics:
11{'num_params': '4300', 'val_accuracy': '0.880'}
12"""
13classModel(nn.Module):
```

Listing 1: The format of our few-shot prompts. In practice we use 2-shot prompts but we omit the second in-context example here for brevity.

Finally, we use \(_{}\) to generate \(n\) samples per prompt, yielding a total of \(n m\) child samples per round of evolution. We denote this portion of the algorithm as \((_{_{t}},P,m,k,n)\) (Algorithm 2 and step 6 of Algorithm 1).

Filtering and scoring child samplesTo score and filter child samples \(c\) generated by \(_{}\), we use the evaluation function \(_{}(c,)\), which trains the model encoded by \(c\) on the dataset \(\) and returns the lowest validation error encountered during training. All child models are trained for the same number of steps, with the same optimizer hyperparameters. Since our fitness function can potentially be gamed by generating arbitrarily small models, we also add a validation error threshold \(\), which is the upper limit of the validation error that a model can incur without being removed from \(G\), the global population. We refer to this function as \((C,,,)\) (Algorithm 3 and step 7 of Algorithm 1). Lastly, we add the remaining trainable models and their associated fitness scores into \(G\) (step 8 of Algorithm 1).

Fitness-based selectionAfter evaluating all child models in the current round, we apply fitness-based selection to identify top candidate models for crossover (step 10 of Algorithm 1). We denote this as \((G,p)\), which refers simply to selecting the \(p\) models with the highest fitness scores from \(G\). Once these models have been selected, they are permanently removed from the population and cannot be used again as parents for crossover.

Training \(_{_{t}}\)Lastly, all child models generated in the current round that were not previously selected for crossover (_i.e._\(C_{} P\)) are used to prompt-tune \(_{}\) for the next round (step 11 of Algorithm 1).

## 4 Experiments and Results

We evaluate our meta-learning algorithm on two datasets - MNIST-1D (Greydanus, 2020) and the CLRS algorithmic reasoning benchmark (Velickovic et al., 2022). While the former benchmark is lightweight and permits us to do a more thorough analysis of our algorithm, the latter is a newer benchmark that covers 30 different algorithms with more headroom for discovering novel architectures with better performance.

In all of our experiments, our \(_{_{0}}\) (_i.e._ the crossover operator) is a 62B parameter PALM model (Chowdhery et al., 2022) pre-trained on 1.3T tokens of conversational, web, and code documents. It was additionally fine-tuned on a corpus of 64B tokens containing near-deduplicated, permissively-licensed Python source code files from Github. We always sample from \(_{_{0}}\) with mixed temperature sampling, in which the sampling temperature is selected uniformly from \([0.2,0.6,0.8,1.0]\). Between each round, the model is prompt-tuned (Lester et al., 2021) for 5 epochs with a soft prompt length of 16, batch size of 16, and learning rate of 0.1 (as described in Section 3.3 and Step 11 of Algorithm 1). Unless stated otherwise, we run 10 rounds of evolution with 10 prompts per round and 16 samples generated per prompt, yielding a total of 160 models generated per round and 1600 models generated during the entire search. Duplicate models and un-trainable models are not scored, but do count into the 1600. All other EvoPrompting hyperparameters are listed in Appendix A.1.

### Mnist-1d

DatasetWe apply our method first to MNIST-1D (Greydanus, 2020), a one-dimensional, scaled-down version of the MNIST-1D dataset containing examples that are 20 times smaller than the original MNIST dataset. Each example is only 40-dimensional, with 4000 examples in the training dataset and 1000 in test. Since there is no validation dataset, we randomly set aside 500 examples from the training dataset to use as the validation dataset. Despite being more lightweight, MNIST-1D distinguishes more between different architecture types (Greydanus, 2020) than its larger counterpart MNIST (LeCun et al., 1998).

Meta-learning set-upThroughout the model search we use the AdamW optimizer (Loshchilov and Hutter, 2019) to train each child model on a single NVIDIA Tesla P100 GPU for 8000 steps, with learning rate 0.01 and batch size 128. We score child models according to the best validation accuracy achieved during training. We also seed the search with 4 seed models - the 3 hand-designed neural baselines from the original MNIST-1D paper (Greydanus, 2020) (GRU, CNN, and MLP) and a fourth, larger CNN model of our own design. All four are implemented with Flax (Heek et al., 2020). We refer the reader to Appendix A.2 for the source code of these seed models.

BaselinesWe compare EvoPrompting with the following baselines:

* Naive few-shot prompting: This baseline simply generates code samples \(c_{_{0}}(|p)\), where \(p\) is a 2-shot prompt constructed using in-context examples randomly selected from the seed models (Listing 1). This is essentially an ablation of steps 7-12 in Algorithm 1 with \(T=1\). We increase the number of samples generated per prompt for the naive prompting baseline such that the total number of samples generated by \(_{}\) matches that of the other baselines.
* prompt-tuning): We run the entire algorithm as is, but without prompt-tuning between each round. This is an ablation of step 11 from Algorithm 1
* EvoPrompting (random parents): Instead of selecting the most fit models from the last round as parents for the next round, we select parents randomly. This is an ablation of Step 10 in Algorithm 1, which is the GetTop\((G,p)\) step.

EvoPrompting finds smaller and more accurate modelsFigure 1(a) shows a comparison of the test error and model size of the top 20 models discovered by EvoPrompting compared with those of our seed models and three baselines. The points approximate a Pareto frontier, below which each algorithm cannot improve on one dimension without hurting the other. EvoPrompting possesses the Pareto frontier closest to the origin, indicating that it finds more optimal models in terms of accuracy and size. In fact, many models in EvoPrompting's top 20 discovered models are orders of magnitude smaller than those of the other baselines, while still having lower test error.

We also note that - on this task in particular - EvoPrompting excels especially at optimizing convolutional architectures. Many of the top 20 models are narrower and deeper convolutional architectures, with smaller strides, less padding, and no dense layers. These models consistently perform better than the shallower, denser, and wider convolutional architectures seen in earlier rounds of the model search.

Another important aspect of a meta-learning algorithm is the relationship between the number of individuals evaluated and the maximum fitness observed so far, _i.e._ the sample efficiency. Neural architecture search can be an expensive process, with the most open-ended searches requiring the evaluation of trillions of individuals (Real et al., 2020). Thus, it is crucial to identify fit candidatesusing as few samples as possible. Figure 1(b) compares how the fitness of the best-performing child model improves as a function of the number of child samples generated thus far. The random parents baseline plateaus the quickest, reaching a maximum fitness by the time approximately 200 individuals have been generated. Furthermore, the maximum fitness it reaches is significantly worse than that of the other experiments. On the other hand, EvoPrompting without prompt-tuning and normal EvoPrompting do not plateau until much later on. EvoPrompting's plateau is the highest and therefore fitter on average than the individuals discovered by any of the other experiments.

It is also evident from both Figure 1(a) and 1(b) that performance suffers when any individual component is removed. Interestingly, Figure 1(a) indicates that prompting with randomly selected parents combined with prompt-tuning is no more effective than naive prompting alone. This highlights the importance of selecting helpful in-context examples, particularly in a task for which we assume that less training signal exists in the pre-training data. However, selecting more fit models as in-context examples without prompt-tuning also does not perform nearly as well as our full method.

Trajectory over meta-learning roundsWe also explored the trajectory of our meta-learning algorithm round over round, as shown in Appendix A.3. In general, we observe that EvoPrompting starts out further away from the origin (in round 0) and ends up closest to the origin in round 10, which signifies that it discovers - on average - the smallest and most accurate models in the last round. However, the search does not always yield improvements on both axes between consecutive rounds. In rounds 0-2 and 6-10, EvoPrompting improves test error while trading off model size. On the other hand, both dimensions are simultaneously improved upon in rounds 3-5.

Figure 3: Number of child models generated versus maximum fitness of top model seen so far (as estimated using 100 bootstrap samples of size 20 for each point along the x-axis) when searching over neural network models for three CLRS tasks. As mentioned in Section 4.2, these algorithms were selected because our preliminary analyses indicated that they had the most headroom for architectural improvements.

Figure 2: EvoPrompting discovers smaller and better performing architectures on MNIST-1D than alternative search methods.

### Clrs

Although the MNIST-1D task offers an efficient and practical setting for evaluating a meta-learning algorithm, CNN architectures already perform fairly well on this task and neural image classification architectures have been extensively studied as a whole. There also exists the possibility that our LM has seen many convolutional architectures in its pre-training data. Instead, we turn to a different learning task and class of neural network architectures in order to assess whether our meta-learning framework generalizes to other tasks, datasets, and neural architectures.

DatasetThe CLRS algorithmic reasoning benchmark (Velickovic et al., 2022) evaluates the ability of neural networks to learn algorithmic reasoning across a set of 30 classical algorithms covered in the _Introduction to Algorithms_ textbook by Cormen et al. (2009). This benchmark is useful not only as a difficult logical reasoning task for neural networks, but also as a measure of a neural network's _algorithmic alignment_(Xu et al., 2020). In brief, algorithmic alignment refers to a model's ability to reason like an algorithm (_i.e._ using the computation graph for a task), rather than relying upon memorization or other less sample efficient learning strategies. Although a model can approximate an algorithm by pattern-matching against similar inputs or relying on other shortcuts, it cannot generalize to arbitrarily long inputs or edge cases without learning the computation graph underlying the algorithm.

Accordingly, the CLRS benchmark represents the algorithms' inputs and outputs as graphs, and the steps of the algorithm as a _trajectory_ of operations over the input graph. This problem setup can be straightforwardly processed by graph neural networks, which is explored in Ibarz et al. (2022). They find that a Triplet-GMPNN model (a message-passing neural network (Gilmer et al., 2017) with gating and triplet edge processing) exhibits the best performance when trained and evaluated across all 30 algorithms at once.

Meta-learning set-upSimilar to our MNIST-1D set-up, we use the AdamW optimizer to train each child model on a single NVIDIA Tesla P100 GPU. However, since most of the explored child models were much larger than the MNIST-1D models, we only trained each child model for 2000 steps. Anecdotally, we observed that the performance of different models often diverged by 2000 steps, which provided sufficient signal for the model search process. We otherwise followed the hyperparameters for single-task training in Ibarz et al. (2022) and evaluated models using validation accuracy.

Unlike our MNIST-1D set-up, we only search over the triplet representations of a Triplet-GMPNN model (see Ibarz et al. (2022) for more details), rather than the entire graph processor. We also seed the search with nine different seed models - each a variant of a Triplet-GMPNN model with a different triplet representation. Each seed triplet representation incorporates a minor tweak of a single component of the original triplet representation designed by Ibarz et al. (2022). These include a fully-connected output layer, a sum aggregation, fully-connected node/edge/graph representations,

    &  &  &  \\  & & Ours & Baseline & Ours & Baseline \\    & QuaNodeMinMax & 497969 & 531913 & **93.5 \(\) 1.8\%** & 88.3\(\) 2.0\% \\  & MaxMean & 522931 & 523963 & **100.0 \(\) 0.0\%** & 99.7\(\) 0.0\% \\ Bubble Sort & ConcatRep & 568533 & 524477 & **88.9 \(\) 2.8\%** & 67.7\(\) 5.5\% \\ DFS & Div2Max & 660158 & 61190 & **68.1 \(\) 1.4\%** & 47.8\(\) 4.2\% \\ Floyd Warshall & ConcatRep & 669145 & 625089 & **61.4 \(\) 0.8\%** & 48.5\(\) 1.0\% \\ Heapsort & ConcatRep & 703710 & 659654 & **69.9 \(\) 4.2\%** & 31.0\(\) 5.8\% \\ Insertion Sort & Div2Mean & 523445 & 524477 & **89.5 \(\) 2.6\%** & 78.1\(\) 4.6\% \\ Quicksort & Div2Mean & 524727 & 525759 & **85.2 \(\) 4.3\%** & 64.6\(\) 5.1\% \\ Task Scheduling & TanhExpandTriplets & 262333 & 262333 & **88.2 \(\) 0.4\%** & 87.3\(\) 0.4\% \\   

Table 1: A comparison of OOD accuracy and model size (in number of parameters) of models newly discovered by EvoPrompting on select CLRS tasks where EvoPrompting has discovered more accurate architectures without large increases in model size, compared with the baseline model (the Triplet-GMPNN from Ibarz et al. (2022)). OOD accuracy numbers for the baseline model are from Ibarz et al. (2022). For the full table of results on all CLRS tasks, including accuracies of our own implementation of the Triplet-GMPNN, see Appendix 3.

a simple linear triplet representation, and a bilinear representation (Mnih and Hinton, 2007). All nine are implemented with Haiku (Hennigan et al., 2020), an object-oriented neural network library for Jax (see Appendix A.5 for the source code of the seed models.)

Generalizing beyond image classification modelsWe search using EvoPrompting on 3 individual algorithms in the CLRS benchmark - the articulation points, Graham scan, and Kruskal's minimum spanning tree algorithms. We select these algorithms because our preliminary analyses with hand-designed architectures showed that they had the most headroom for improvement, although we found that the discovered architectures transfer well to other CLRS benchmark tasks as well (Appx. 3). Our search results are shown in Figure 3. EvoPrompting continues to find models that are more "fit" than our other two baselines, though we observed that the results also show more variation than our results for MNIST-1D did.

Analyzing newly discovered modelsOur search across triplet representations yielded several new designs that we sought to evaluate across all algorithms in the CLRS benchmark. Although these new models were discovered in model searches over single algorithms, they oftentimes generalized to other algorithms that were unseen during the model search. Figure 5 shows the trajectory of validation accuracy during training and Table 1 provides OOD accuracies for these models on a few select algorithms. (We defer the reader to Appendix A.4 for the full source code of each newly discovered model and Table A.6 for the full list of OOD accuracies for every algorithm in the CLRS benchmark.)

We note that the model search suggested several simple but effective changes. For example, instead of taking the maximum of the triplet representation, the QuadNodeMinMax model uses quadruplet node representations instead of triplets, and it subtracts the minimum of the quad representation from the max instead. ConcatRep represents the node, edge, and graph representations as a concatenation of a projection feedforward layer, and MaxMean takes the maximum of the triplet representations prior to taking the mean and passing it through the output dense layer. Div2Mean scales each of the node representations by \(1/2\) and uses a mean aggregation of the triplet representations instead of the max aggregation. TanhExpandTriplets applies additional dimension expansion to the triplet representations and applies a hyperbolic tangent function after the max aggregation. See Appx. A.4 for the full code of each discovered model.

Of the 5 newly discovered models that we chose to analyze, ConcatRep is the only one that increases model size. However, as shown in Table 1, ConcatRep frequently yielded improvements in OOD accuracy that far exceeded the percent increase in model size. For instance, on the heapsort algorithm ConcatRep increased OOD accuracy by 125.19% while only increasing model size by 6.68% over the baseline. The other four newly discovered models shown in Table 1 simultaneously improved OOD accuracy while decreasing model size on the articulation points, BFS, DFS, insertion sort, quicksort, and task scheduling algorithms. On the rest of the CLRS algorithms (Table A.6), our newly discovered models typically achieved OOD accuracy comparable to or better than the baseline, while maintaining similar model size.

## 5 Conclusion

We have shown that embedding a pre-trained LM in an evolutionary algorithm significantly improves the LM's performance on the task of neural architecture design. Our approach has demonstrated success at not only optimizing convolutional architectures for the MNIST-1D task, but also at developing new kinds of GNNs for the CLRS algorithmic benchmark. This demonstrates: 1) using evolutionary techniques can vastly improve the in-context capabilities of pre-trained LMs, and 2) EvoPrompting can discover novel and state-of-the-art architectures that optimize for both accuracy and model size. Furthermore, EvoPrompting is general enough to be easily adapted to search for solutions to other kinds of reasoning tasks beyond NAS. We leave the adaptation of EvoPrompting for other tasks to future work.

However, our study is limited by the lack of an extensive comparison against other standard NAS techniques because EvoPrompting was designed for open-ended search, whereas other techniques were not, which would introduce a potential confounder. We include one such comparison on NATS-Bench in Appendix A.7, as well as a discussion of the confounders thereof.

Acknowledgements

We thank Maarten Bosma, Kefan Xiao, Yifeng Lu, Quoc Le, Ed Chi, Borja Ibarz, Petar Velickovic, Chen Liang, Charles Sutton, and the Google Brain AutoML team for providing valuable discussions and feedback that influenced the direction of this project. We also thank the Google Student Researcher program for providing the resources and opportunities necessary for this project to take place.