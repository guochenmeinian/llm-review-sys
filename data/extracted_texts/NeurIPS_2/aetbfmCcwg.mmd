# Debiasing Synthetic Data Generated by Deep Generative Models

Alexander Decruyenaere \({}^{*}\)

Ghent University Hospital - SYNDARA

&Heidelinde Dehaene \({}^{*}\)

Ghent University Hospital - SYNDARA

&Paloma Rabaey

Ghent University - imec

&Christiaan Polet

Ghent University Hospital - SYNDARA

&Johan Decruyenaere

Ghent University Hospital - SYNDARA&Thomas Demeester

Ghent University - imec

&Stijn Vansteelandt

Ghent University - Department of Applied Mathematics,

Computer Science and Statistics

\({}^{*}\)Joint first authors and corresponding authors

{firstname.lastname}@ugent.be

###### Abstract

While synthetic data hold great promise for privacy protection, their statistical analysis poses significant challenges that necessitate innovative solutions. The use of deep generative models (DGMs) for synthetic data generation is known to induce considerable bias and imprecision into synthetic data analyses, compromising their inferential utility as opposed to original data analyses. This bias and uncertainty can be substantial enough to impede statistical convergence rates, even in seemingly straightforward analyses like mean calculation. The standard errors of such estimators then exhibit slower shrinkage with sample size than the typical 1 over root-\(n\) rate. This complicates fundamental calculations like p-values and confidence intervals, with no straightforward remedy currently available. In response to these challenges, we propose a new strategy that targets synthetic data created by DGMs for specific data analyses. Drawing insights from debiased and targeted machine learning, our approach accounts for biases, enhances convergence rates, and facilitates the calculation of estimators with easily approximated large sample variances. We exemplify our proposal through a simulation study on toy data and two case studies on real-world data, highlighting the importance of tailoring DGMs for targeted data analysis. This debiasing strategy contributes to advancing the reliability and applicability of synthetic data in statistical inference.

## 1 Introduction

The concept of generating synthetic data as a means of privacy protection was initially introduced by Rubin (1993) within the framework of multiple imputation, a widely used technique for managing the statistical analysis of incomplete data sets. Since its inception, a substantial body of literature on synthetic data has emerged (Raghunathan et al., 2003; Raghunathan, 2021; Drechsler, 2011; Raab et al., 2016; Reiter, 2005), with a recent surge in interest propelled by advancements in deep generative modelling technology (Raghunathan, 2021; van Breugel et al., 2023; Wan et al., 2017; Yan et al., 2022; Nowok et al., 2016; Endres et al., 2022; Hernandez et al., 2022). In this work, we focus on tabular synthetic data and their inferential utility, which captures whether synthetic data can be used to obtain valid estimates and inference for a population parameter (Decruyenaere et al., 2024).

The substantial privacy protection potential offered by synthetic data is marred by significant challenges that undermine their inferential utility (Raab et al., 2016). Previous work by Decruyenaere et al. (2024) has shown that these challenges are much more severe when using deep generative models (DGMs), rather than parametric statistical models, which is why we focus on the former. First, standard confidence intervals and p-values obtained on synthetic data may drastically underestimate the uncertainty in synthetic data as these ignore the size of the original data. Indeed, synthetic data obtained via generators trained on small datasets will unsurprisingly deliver much worse quality than synthetic data obtained via generators trained on large datasets. This uncertainty in the generator must therefore be translated into analysis results, such as confidence intervals and p-values. Standard confidence intervals and p-values ignore this, as they do not distinguish whether the data are synthetic or real. Second, it is well known from the literature on plug-in estimation that data-adaptive methods (such as DGMs) cannot succeed to estimate _all_ features of the data-generating distribution well (Bickel et al., 1993; van der Laan and Rose, 2011; Chernozhukov et al., 2018; Hines et al., 2022). These methods are designed to optimally balance bias and variance w.r.t. a chosen criterion, such as prediction error. However, they cannot guarantee that such an optimal trade-off is simultaneously made w.r.t. all possible discrepancy measures that exist, such as mean squared error in specific functionals (mean, variance, least squares projections...) of the observed data distribution. Such data-adaptive methods therefore leave non-negligible bias in estimators of such functionals, leading to excess variability, slow convergence, and confidence intervals that do not cover the truth at nominal level (and may even never contain the truth, even in large samples) (Decruyenaere et al., 2024).

Related work.While several approaches have been proposed to account for the uncertainty arising from synthetic data generation, we are not aware of strategies for generating and analysing DGM-based synthetic data that guarantee valid inference. Raghunathan et al. (2003) developed a framework inspired by the work on multiple imputation for missing data, by combining the results of multiple synthetic datasets, but this is not readily applicable to DGM-based synthetic data. Rai≈°a et al. (2023) extended this work for differentially private (DP) synthetic data, acknowledging the additional DP noise, but continue to consider parametric (Bayesian) data generation strategies.

Our work instead focuses on obtaining valid inference from a single synthetic dataset, which is arguably more attractive for use by practitioners. Raab et al. (2016) derived alternative combining rules that reduce to the correction factor \(\) for the standard error (SE) of an estimator in the case of inference from a single (non-DP) synthetic dataset of size \(m\) generated from an original dataset of size \(n\). The method suggested by Awan and Cai (2020) to preserve efficient estimators in a single (DP or non-DP) synthetic dataset relies on generating data conditional on the estimate in the original data. Both procedures are only applicable to parametric generative models and therefore suffer from the same limitation as the aforementioned approaches. To enable Bayesian inference from a single DP synthetic dataset, Wilde et al. (2021) proposed a corrected analysis that relies on the availability of additional public data, while Ghalebikesabi et al. (2022) investigated importance weighting methods to remove the noise-related bias, but they do not study the impact on inference.

Contributions.Our work is the first to propose a generator-agnostic solution that mitigates the impact of the typical slower-than-\(\)-convergence of estimators in synthetic data created by DGMs. As far as we are aware, our approach is thus the only one that provides some formal guarantees for (more) honest inference in this setting. In this paper, we show how the statistical bias in estimators can be removed, by adapting results on debiased or targeted machine learning (van der Laan and Rose, 2011; Chernozhukov et al., 2018) to the current setting where machine learning is not necessarily used in the analysis, but rather in the generation of synthetic data. Although we build upon ideas from existing work, our extension is non-trivial, since (1) previous work did not consider synthetic data; and (2) we demonstrate, with significant generality, how to mitigate the estimation errors in the DGM that would otherwise propagate into the estimator calculated on synthetic data.

In Section 2 and 3, we propose a generator-agnostic debiasing strategy, directed towards the downstream statistical analysis of the synthetic data. As such, we obtain estimators that are less sensitive to the typical slow (statistical) convergence affecting the generators. We illustrate this with a simulation study in Section 4, showing that the coverage of both the mean and linear regression coefficient estimators indeed improves. In Section 5, we further cement the utility of our debiasing strategy in a practical setting through two case studies. While the proposed strategy is generator-agnostic, we focus our analyses on two DGMs for tabular data: CTGAN and TVAE (Xu et al., 2019). Finally, Section 6 concludes with a discussion on our method and its limitations.

Notation and Set-Up

The aim of this paper is to use synthetic data in order to learn a specific functional \((.)\) of the observed data distribution \(P\). We formalise the problem of learning \((P)\) based on synthetic data as follows. Suppose that, based on \(n\) independent (possibly high-dimensional) samples \(O_{1},...,O_{n}\) from \(P\), we estimate the observed data distribution as \(_{n}\). Here, \(_{n}\) may be obtained by fitting parametric models to the distribution of the observed data, or alternatively by training DGMs. Based on \(m\) independent (synthetic) random samples \(S_{1},...,S_{m}\) from \(_{n}\), we then estimate \((P)\). For this, the data analyst (to whom \(_{n}\) is unknown) uses the \(m\) synthetic samples \(S_{1},...,S_{m}\) to approximate \(_{n}\) as \(_{m}\), and obtains an estimator of \((P)\) given by \((_{m})\). We use \(_{n}\) to denote the empirical distribution of the observed data and \(}_{m}\) to denote the empirical distribution of the synthetic data. We index expectations by the distribution under which they are taken; e.g., \(E_{P}(Y)\) denotes the population expectation of \(Y\).

Throughout, we assume that the parameter of interest is sufficiently smooth in the data-generating distribution so that root-\(n\) consistent estimators (i.e., with SEs shrinking at 1 over root-\(n\) rate) can be obtained. Formally, we assume the \((P)\) is pathwise differentiable with efficient influence curve (EIC) \((.,P)\) under the nonparametric model, as is satisfied for (most) standard statistical analyses. The EIC is a functional derivative of \((P)\) w.r.t. the data-generating distribution \(P\) (in the sense of a Gateaux derivative), which has mean zero under \(P\)(Fisher and Kennedy, 2021; Hines et al., 2022).

In what follows, we illustrate our debiasing strategy via two examples. Here, we briefly outline some of their theoretical foundations we build upon. Appendix A.1 clarifies how these debiased estimators originate from their EIC. We further refer to them as debiased or EIC-based estimators.

Population mean.Adapting the traditional formulation of the population mean to the context of synthetic data, we choose \(_{m}=}_{m}\). As a result, we will study the large sample behaviour of the synthetic data sample average:

\[(_{m})=_{i=1}^{m}S_{i}.\]

The EIC of this sample average is \((S,_{n})=S-(_{n})\).

Linear regression coefficient.Second, suppose we are interested in a specific coefficient \((P)\) of some exposure \(A\) for the outcome \(Y\) in the linear model \(E_{P}(Y|A,X)= A+(X)\), where \(X\) is a possibly high-dimensional vector of covariates and \((.)\) is an unknown function. Our proposal allows \((X)\) to correspond to a standard linear model in all covariates, but is more generally valid. Building upon the nonparametric definition of \(\) as given in Appendix A.1, we adjust it to obtain an estimator in the setting of synthetic data. Let \(Y\), \(X\) and \(A\) be jointly or sequentially modelled by some generative model, from which a single synthetic dataset \(S_{i}=(_{i},_{i},_{i})\) is sampled (\(i=1,,m\)). We then consider the estimated regression coefficient of exposure \(A\) given by

\[(_{m})=_{i=1}^{m}\{_ {i}-E_{_{m}}(A|_{i})\}\{_{i }-E_{_{m}}(Y|_{i})\}}{_{i=1}^{m }\{_{i}-E_{_{m}}(A|_{i})\} ^{2}},\]

where the nuisance parameters \(E_{_{m}}(A|_{i})\) and \(E_{_{m}}(Y|_{i})\) are estimated based on synthetic data. This EIC-based estimator coincides with the Maximum Likelihood Estimator (MLE) when least squares predictions for these nuisance parameters are used. Its EIC is (Vansteelandt and Dukes, 2022)

\[(S,_{n})=-E_{_{n}}(A| )\}[Y-E_{_{n}}(Y|)-( _{n})\{-E_{_{n}}(A|) \}]}{E_{_{n}}[\{-E_{ _{n}}(A|)\}^{2}]}

In Appendix A.2 we derive that

\[(_{m})-(P) = _{i=1}^{m}(S_{i},_{n})- _{i=1}^{m}(S_{i},_{m})+R(_{m},_{n}) \] \[+\{(S,_{m})-(S,_{n}) \}d(}_{m}-_{n})\] \[+_{i=1}^{n}(O_{i},P)-_{i=1}^{n} (O_{i},_{n})+R(_{n},P)\] \[+\{(O,_{n})-(O,P)\}d(_{n}-P).\]

We now examine the eight terms of this equation and discuss why some may be negligible while other may introduce bias. First, \(R()\) are remainder terms, which can generally be shown to be small, but must be studied on a case-by-case basis; see Hines et al. (2022) for worked out examples. In order for these remainder terms to be \(o_{p}(m^{-1/2})\) and \(o_{p}(n^{-1/2})\), we generally need that faster than \(n^{-1/4}\) convergence rates are obtained for the unknown functionals of \(_{m}\) and \(_{n}\) that appear in the EICs. It is unknown whether these convergence rates are attainable for DGMs; whether they are, will partly depend on the number of parameters in the DGM itself, the dimension of the data and the complexity of the observed data distribution. The simulation study in Section 4 will give further insight into this.

Second, the two empirical process terms (1) and (2) in the von Mises expansion can be shown to be \(o_{p}(m^{-1/2})\) and \(o_{p}(n^{-1/2})\) by Markov's inequality, under weak conditions. For (1) to converge to zero, we will need the difference between \(_{m}\) and \(_{n}\) to converge to zero (in \(L_{2}(_{n})\) at any rate), and the estimator to be calculated on a different part of the data than the one on which \(_{m}\) was estimated (Chernozhukov et al., 2018). For (2) to converge to zero, we will need \(_{n}\) (e.g., the DGM) to consistently estimate \(P\) (in \(L_{2}(P)\) at any rate). In addition, it can be argued that the DGM needs to be trained on a different part of the data than that on which the debiasing step (see later) will be applied. In Appendix A.3, we elaborate on the necessity of sample splitting.

We thus foresee that the two remainder and two empirical process terms are negligible under certain conditions. Third, as elaborated in Appendix A.2, we show that the large sample behaviour of both

\[_{i=1}^{n}(O_{i},P) 56.905512pt(3)  56.905512pt_{i=1}^{m}(S_{i},_{n}) \]

is standard, and well understood; in particular, these terms vary around zero with variance that can be estimated well. By contrast, the terms

\[-_{i=1}^{m}(S_{i},_{m}) 56.905512pt(5)  56.905512pt-_{i=1}^{n}(O_{i},_{n}) \]

need some further discussion. Term (5) generally fails to have mean zero because the synthetic data \(S_{i}\) do not originate from the distribution \(_{m}\). This term therefore induces a bias in \((_{m})\) that results from using data-adaptive estimates \(_{m}\) on the synthetic data; a similar term would appear if we instead analysed the real data. Term (6) likewise fails to have mean zero because the observed data \(O_{i}\) do not originate from the distribution \(_{n}\). Also this term thus induces a bias, now resulting from the use of a generative model to obtain \(_{n}\). It may be large relative to (3) when DGMs are used, because of slow convergence of \(_{n}\). It is precisely this term that causes estimators based on synthetic data to converge slowly with increasing sample size, as observed in Decruyenaere et al. (2024).

After identifying the two problematic terms, we now propose in a second step a targeting or debiasing strategy to remove these bias terms (5) and (6). As in van der Laan and Rose (2011) and Chernozhukov et al. (2018), we will remove bias term (5) by analysing the data with debiased estimators based on the EIC that ensure that this bias term then becomes zero. Novel to our proposal is that we will additionally shift the generated data to ensure that also bias term (6) becomes zero. This bias term depends on the EIC, which itself depends on the target parameter of interest. In the next two paragraphs, we discuss how this can be done for the two considered estimators. Note that the proposed strategy does not require any actual finetuning or retraining of the DGM. For a given parameter of interest, a mere post-processing of the synthetic samples, based on access to the DGM as well as the original data it was trained on, allows eliminating the bias for a given parameter of interest. A graphical summary of the problem setting and our debiasing strategy can be found in Appendix A.4.

### Population mean

For the population mean, the debiasing step with respect to term (5) is implicit since the traditional estimator as given in Section 2 is a debiased estimator. Therefore, the proposal to debias a given DGM with respect to the population mean \((P)= odP(o)\) amounts to first training the DGM and then augmenting the output for the variable of interest to ensure that bias term (6) is zero, or hence

\[_{i=1}^{n}(O_{i},_{n})=_{i=1}^{n} O_{i}-(_{n})=-(_{n})=0.\]

In other words, the population mean of the synthetic data under the DGM should match the sample average of the real data. Here, \((_{n})\) can be approximated by generating a very large sample \(k\) (e.g., one million samples) based on the DGM and calculating their sample mean \(\). The generative model must then be corrected, to ensure that this sample mean equals \(\). To obtain a debiased synthetic sample, we shift all samples generated by the given DGM by adding \(-\) to the considered variable. Note that the sample average of such a set of \(m\) corrected synthetic samples will generally differ from \(\).

### Linear regression coefficient

For linear regression coefficients, this section shows how the samples generated by a given DGM need to be adapted to eliminate bias term (6), written as follows (see Appendix A.5):

\[b=^{n}\{A_{i}-E_{_{n}}(A|X_{i})\}\{ Y_{i}-E_{_{n}}(Y|X_{i})\}}{_{i=1}^{n}\{A_{i}-E_{ _{n}}(A|X_{i})\}^{2}}-(_{n}). \]

To compute this bias, we must generate, for each observed value \(X_{i}\), a very large sample of measurements of \(A\) and \(Y\) with the given level \(X_{i}\), based on the DGM. Then \(E_{_{n}}(Y|X_{i})\) and \(E_{_{n}}(A|X_{i})\) can be estimated as the sample average of those values for respectively \(Y\) and \(A\). Further based on a very large DGM-generated sample \(k\) (e.g. one million samples), we calculate \((_{n})\) as follows:

\[^{k}\{A_{j}-E_{_{n}}(A|X_{j})\}\{Y_ {j}-E_{_{n}}(Y|X_{j})\}}{_{j=1}^{k}\{A_{j}-E_{ _{n}}(A|X_{j})\}^{2}}. \]

Debiasing of the DGM can now be done by adding the product \(b_{i}-E_{_{n}}(A|_{i})}\) to the synthetic outcome observations \(_{i}\) generated by the DGM. An in-depth elaboration is provided in Appendix A.5. With the proposed shifting, we ensure that the debiasing with respect to term (6) is completed. We then proceed our analysis with these shifted synthetic observations and employ the EIC-based estimator of Section 2, which ensures that bias term (5) equals zero as well.

### Properties

To summarise, when the remainder terms \(R(.)\) and the empirical process terms (1) and (2) are all \(o_{p}(m^{-1/2})\) and \(o_{p}(n^{-1/2})\) and, furthermore, the suggested debiasing approach is used so as to remove terms (5) and (6), then we expect that

\[(_{m})-(P) = _{i=1}^{m}(S_{i},_{n})+_{i=1}^{n}(O_{i},P)+o_{p}(n^{-1/2})+o_{p}(m^{-1/2}).\]

In particular, the resulting estimator \((_{m})\) may even converge at root-\(n\) rates (under standard conditions of pathwise differentiability (Bickel et al., 1993; Hines et al., 2022)) and has an easy-to-calculate variance that acknowledges the uncertainty in the generation of synthetic data (provided that the statistical convergence of the generator is not too slow). In Appendix A.6, we show that the variance of \((_{m})\) may be approximated by

\[(+)E\{(O,P)^{2}\},\]thereby generalising results known for parametric synthetic data generators (Raab et al., 2016; Decruyenaere et al., 2024), where the correction factor \(\) was proposed. Thus, when \(m\), the debiased estimator \((_{m})\) based on debiased synthetic data has the same distribution as when the real data were analysed. Therefore, the proposed debiasing strategy delivers analysis results that are asymptotically equivalent to those obtained from the same analysis on the real data, provided that \(n/m=o(1)\). This means that results of the same quality and confidence intervals of the same expected length are then obtained, as will be illustrated in the case study in Section 5.2.

Since \(E\{(O,P)^{2}\}\) is unknown, it merely remains to estimate it as

\[_{i=1}^{m}(S_{i},_{m})^{2}.\]

This is a consistent estimator when \(_{m}\) converges to \(_{n}\) as \(m\) goes to infinity, and moreover, \(_{n}\) converges to \(P\) as \(n\) goes to infinity. We note that this sample variance will be subject to bias that results from 'poor' tuning of the DGM. Removing this bias is not required, because this variance will be scaled by \(1/m+1/n\) so that any bias becomes negligible in large samples. While the use of debiased estimators based on the EIC of \(E\{(O,P)^{2}\}\) may potentially improve performance, this goes beyond the scope of this work.

Practical implications.Sections 3.1 and 3.2 show how bias term (6) is eliminated by shifting the synthetic variable of interest. We describe how bias term (5) is also removed by using debiased estimators, which we referred to as EIC-based estimators (see Section 2). As mentioned earlier, the EIC-based estimator for the population mean always coincides with the sample average, while for the linear regression coefficient it only reduces to the ordinary least squares estimator when data-adaptive estimation of the nuisance parameters is not used. This implies that it may suffice for the applied researcher to 1) shift the synthetic data and apply the traditional estimators, and 2) to multiply the SE of the estimator with \(\) to obtain valid inference from a single synthetic dataset. However, the EIC-based estimators are recommended since they are robust against model misspecification by allowing for more flexibility in the estimation of the nuisance parameters.

## 4 Simulation study

Our proposed debiasing strategy is empirically validated by a simulation study that covers both estimators 3.1 (sample mean) and 3.2 (linear regression coefficient). Having full control over the data generating process allows us to calculate the bias, SE and convergence rate of both estimators in synthetic data, with and without our debiasing strategy. The data generating process consists of the following four variables: _age_ (normally distributed), atherosclerosis _stage_ (ordinal with four categories), _therapy_ (binary), and _blood pressure_ (normally distributed). The Directed Acyclic Graph (DAG) in Figure 1 represents the dependency structure and we refer to Appendix A.7.1 for more details. This setting allows us to simultaneously target the population mean of _age_ and the population effect of _therapy_ (\(A\)) on _blood pressure_ (\(Y\)) adjusted for _stage_ (\(X\)).

### Set-up

We conduct a Monte Carlo simulation study, where \(n\) independent records are sampled from the data generating process, forming the observed **original dataset**\(O_{1},...,O_{n}\). This process is repeated \(250\) times, with the sample size \(n\) varying log-uniformly between \(50\) and \(5000\) (i.e., \(n\{50,160,500,1600,5000\}\)). Per original dataset, following DGMs are trained: CTGAN and TVAE (Xu et al., 2019), of which a detailed explanation can be found in Appendix A.7.2. From these DGMs \(m\) synthetic data records are sampled that constitute the **default synthetic dataset**\(S_{1},...,S_{m}\). We set \(m=n\) to retain the dimensionality of the original data. Subsequently, each default synthetic dataset is debiased with respect to both estimands using the steps provided in Section 3, leading to a **debiased synthetic dataset**. Finally, two estimators are calculated in each of three datasets: the sample mean of _age_ and the linear regression coefficient of _therapy_ on _blood pressure_ adjusted for

Figure 1: DAG for the variables in the simulation study.

stage_. We always report the maximum likelihood estimation (MLE)-based estimators, as used in traditional statistical analysis, of which the standard errors (SEs) are inflated with the correction factor \(\) to acknowledge the sampling variability of synthetic data. These estimators will deliver similar estimates as the EIC-based estimators since no data-adaptive estimation is used (see Section 3.3 and Appendix A.7.5).

### Results

We now present the results of our simulation study. The DGMs were trained using the default hyperparameters as suggested by the package Synthcity(Qian et al., 2023). We also show results obtained for other hyperparameters (the default in the package SDV(Patki et al., 2016)) in Appendix A.7.4. In Section 4.2.1 we evaluate the empirical coverage of the \(95\)% confidence interval (CI) for the population parameters. Our debiasing strategy should enhance the coverage, preferably to the nominal level, allowing for (more) honest inference, which is the main contribution of our strategy. Then, we analyse step by step the various components that may influence the coverage by investigating the bias and SE of the estimators in Section 4.2.2, and their convergence rates in Section 4.2.3. Additional results are presented in Appendix A.7.4, including the convergence rates of the nuisance parameters estimated by the DGMs. Our code is available on Github: [https://github.com/syndara-lab/debiased-generation](https://github.com/syndara-lab/debiased-generation).

#### 4.2.1 Coverage

By definition, \(95\)% (_empirical_) of the \(95\)% CIs (_nominal_) should cover the population parameter. Figure 2 depicts the empirical coverage of the \(95\)% CIs obtained from both original and synthetic samples for the population mean of _age_ and the population effect of _therapy_ on _blood pressure_ adjusted for _stage_. The results indicate that our debiasing strategy delivers empirical coverage levels for the population mean that approximate the nominal level for all sample sizes and DGMs considered. By contrast, the coverage based on the default synthetic datasets decreases with increasing \(n\) due to slower shrinkage of the SE than the typical 1 over root-\(n\) rate, as calculated in Section 4.2.3 and previously elaborated in Decruyenaere et al. (2024). For the population regression coefficient, debiasing delivers coverage at the nominal level for TVAE across all sample sizes, but not for CTGAN, although it clearly provides more honest inferences than based on the default synthetic datasets. The residual loss of coverage likely results from not using (efficient) sample splitting (see Appendix A.3).

#### 4.2.2 Bias and Standard Error

Figures (a)a and (b)b depict the estimates and their SE, respectively, for the sample mean of _age_ obtained in the default and debiased synthetic datasets. Figures A5 and A6 in the appendix show these for the linear regression coefficient of _therapy_ on _blood pressure_ adjusted for _stage_. In Figure (a)a, each dot is an estimate per Monte Carlo run and the true population parameters are represented by the horizontal dashed line. This figure allows a qualitative assessment of two key properties of estimators: empirical bias (i.e., the average difference between the estimates and the population parameter, as represented by the solid line) and empirical SE (i.e., the standard deviation of the estimates, as indicated by the vertical spread of the estimates). Ideally, both converge to zero as the sample size grows larger. The convergence rate conveys the rate at which this happens. The funnel represents the default behaviour of an unbiased estimator based on original data of which the SE diminishes at a rate of 1 over root-\(n\).

As shown in Figure (a)a, the sample mean of _age_ is unbiased in the default synthetic datasets, but exhibits a large empirical SE that shrinks slowly with sample size due to the data-adaptive nature of

Figure 2: Empirical coverage of the \(95\)% confidence interval for the population mean of _age_ and the population effect of _therapy_ on _blood pressure_ adjusted for _stage_.

DGMs. It is exactly this variability that is, on average, underestimated by the MLE-based SE in Figure 2(b). Debiasing reduces the empirical variability and accelerates its shrinkage, such that the average MLE-based SE approximates the empirical SE. For the linear regression coefficient of _therapy_ on _blood pressure_ adjusted for _stage_, debiasing reduces finite-sample bias and also improves shrinkage of the empirical SE. Albeit less pronounced, the average MLE-based SE still underestimates the empirical SE with CTGAN despite debiasing (see Figures A5 and A6 in the appendix).

#### 4.2.3 Convergence Rate of Standard Error

Assuming a power law \(n^{-a}\) in convergence rate for the empirical SE, we estimate the exponent \(a\) from five logarithmically spaced sample sizes \(n\) between 50 and 5000, shown in Table 1 and Figure A7 in the appendix. Standard statistical analysis assumes that the bias converges faster than the SE with the latter diminishing at a rate of \(1/\). However, the SEs produced by DGMs converge much slower (i.e., \(a_{SE}<0.5\)), leading to a progressively increasing underestimation of the empirical SE by the MLE-based SE (which assumes \(\)-convergence) as the sample size grows larger. In turn, this results in too narrow CIs and poor coverage, as observed in Section 4.2.1. By contrast, our debiasing strategy renders estimators of which the SE converges at approximately root-\(n\) rates (i.e., \(a_{SE}=0.5\)), explaining the improvement in coverage of the CIs as a result of debiasing.

## 5 Case studies

To illustrate our findings and highlight their implications for the applied researcher, we conduct two case studies. First, Section 5.1 transfers the framework from our simulation study to the International Stroke Trial (IST) dataset (Sandercock et al., 2011). Second, Section 5.2 describes whether analysis results from the Adult Census Income dataset (Becker and Kohavi, 1996) are similar to those obtained from the real data, when the sample size \(m\) of the generated synthetic data is very large. In both case studies, estimated SEs in the default synthetic and debiased synthetic datasets are corrected by multiplying with factor \(\) to acknowledge the sampling variability of synthetic data.

### International Stroke Trial

We adapt the framework discussed in Section 4.1 to the IST dataset, one of the biggest randomised trials in acute stroke research (Sandercock et al., 2011). The dataset with \(19285\) complete cases now

  
**Estimator** & **CTGAN** & **TVAE** & **CTGAN** & **TVAE** \\   &  \\ Mean age & 0.01 [-0.16; 0.18] & 0.21 [0.04; 0.39] & 0.50 [0.46; 0.54] & 0.47 [0.44; 0.50] \\ Effect therapy & 0.31 [0.22; 0.40] & 0.09 [-0.13; 0.31] & 0.43 [0.31; 0.55] & 0.46 [0.38; 0.55] \\   

Table 1: Estimated exponent \(a\) [\(95\)% CI] for the convergence rate \(n^{-a}\) for empirical SE.

Figure 3: Each dot in Figure (a) is an estimate for the population mean of _age_ per Monte Carlo run. The funnel indicates the behaviour of an unbiased and \(\)-consistent estimator based on observed data. Figure (b) depicts the empirical and average MLE-based SE for the sample mean of _age_.

constitutes our _population_. We mimic different hypothetical settings where an institution only has access to a limited _sample_ of observations, with the sample size \(n\) varying between \(50\) and \(5000\). In order to easily share the data with other researchers, the institution generates a synthetic dataset with sample size \(m\), where \(m=n\). We repeated this process \(100\) times per sample size \(n\) to be able to calculate the empirical coverage levels. For illustration purposes, we focus on the effect of aspirin on the outcome at \(6\) months and report the proportion of deaths for the two treatment arms (aspirin and no aspirin), and its corresponding risk difference. For each value of \(n\), two default synthetic datasets were generated using both CTGAN and TVAE. Next, for each of these, we first split the default synthetic dataset by treatment, debias the data with relation to the population mean within each treatment arm, and then combine them back into one debiased synthetic dataset. We noticed that using the same hyperparameters as in the simulation study resulted in biased estimates, as can be seen in Figure A12 in the appendix. For this reason, we highlight the results obtained by training with the default hyperparameters suggested by the package SDV(Patki et al., 2016) instead.

One of the original research questions in Sandercock et al. (2011) was whether or not there is a difference in risk of death between the treatment arms. Suppose a researcher can repeatedly collect information on 500 subjects and uses original, default synthetic and debiased synthetic data to make an inferential statement about this risk difference. Figure 4 depicts the confidence intervals for the first 15 repetitions, with the vertical dashed lines representing the true risk difference of \(-0.009\) as calculated based on our population (the full dataset). Should the researcher use the default synthetic data, they would falsely conclude (in 7 out of these 15 repetitions) that the risk is significantly different from \(-0.009\), while using the debiased synthetic dataset basically eliminates this high number of false-positives (with all these 15 intervals containing the population parameter), as is the case in the original data as well. More results can be found in Appendix A.8.1.

### Adult Census Income Dataset

We also perform a case study on the Adult Census Income dataset, which comprises \(45222\) complete cases and \(14\) unique variables (Becker and Kohavi, 1996). We assume the researcher's interest lies in inferring the population mean of _hours_ worked per week (estimated via the sample mean) and the average _sex_-adjusted difference in _age_ between persons with an _income_ of \(>\$50\)K a year vs. not (estimated via a linear regression model \(age(Y) income(A)+sex(X)\)). Our goal in this case study is to confirm whether inferential results obtained using the debiased synthetic dataset are asymptotically equivalent (i.e. with \(m>>n\) in our debiasing strategy) to those obtained using the original data. To test this across different sample sizes, the original data constitute five different samples of the Adult Census Income dataset with sizes \(n\) varying log-uniformly between \(50\) and \(45222\). For each original dataset, a default synthetic dataset of size \(m=10^{6}\) was generated by TVAE. Subsequently, this dataset was debiased following the steps described in Section 3.1 (sample mean) and Section 3.2 (linear regression coefficient).

Figure 5 depicts the \(95\)% CIs for both estimators, the five original sample sizes and the three versions of datasets. This indeed confirms that analysis on the debiased synthetic dataset leads to results of similar quality and CIs of similar length compared to the original dataset. By contrast, the analysis on the default synthetic dataset may yield results of inferior quality and even incorrect conclusions.

Discussion

In this paper, we propose a new debiasing strategy that targets synthetic data created by DGMs towards the downstream task of statistical inference from the resulting synthetic data. We establish our theory for two estimators by applying insights from debiased or targeted machine learning literature (van der Laan and Rose, 2011; Chernozhukov et al., 2018) to the current setting where machine learning is not necessarily used in the analysis, but rather in the generation of synthetic data. We obtain estimators that are less sensitive to the typical slow (statistical) convergence affecting DGMs and thereby improve the inferential utility of the synthetic data.

We illustrated the impact of our proposal through a simulation study on toy data and two case studies on real-world data. Our debiasing strategy results in root-\(n\) consistent estimators based on the synthetic data and thereby better coverage of the confidence intervals, allowing for more honest inference. While coverage was clearly improved, it was not guaranteed to be at the nominal level. Indeed, it may remain anti-conservative for some estimators and DGMs, due to slow convergence inherent to these models and/or due to residual overfitting bias that could not be removed since sample splitting was not performed. Future work should focus on efficient sample splitting, where the resulting bias reduction outweighs the increase in finite-sample bias that arises from training on smaller sample sizes. Alternatively, findings from Ghalebikesabi et al. (2022) on importance weighting could be incorporated, with the weights being targeted to eliminate the impact of the data-adaptive estimation of the weights. This may potentially relax the fast baseline convergence assumption, and enable the same debiased synthetic data to be used for multiple downstream analyses.

A key advantage of our debiasing strategy is that it may deliver synthetic data created by DGMs of which the analysis is equivalent to the original data analysis, provided that the synthetic sample size is chosen to be much larger than the original sample size. Although the risk of disclosure may increase with the size of non-DP synthetic data (Reiter and Drechsler, 2010), this trade-off is beyond the scope of our paper. More interestingly, in the case of DP synthetic data, our debiasing strategy may exploit their post-processing immunity that allows for data transformations without compromising privacy guarantees (Dwork and Roth, 2014). However, our strategy needs to be extended to incorporate the DP-constraints when studying the difference between \((_{m})\) and \((P)\) in DP synthetic data.

Limitations of our proposal include the low-dimensional setting of our simulation and case studies, for which DGMs might be less well suited. The positive results found for two widely used estimators in this simple setting highlight the utility of a debiasing approach and are encouraging in terms of future larger-scale applications. However, before addressing these, it is important to first understand low-dimensional settings, where valid inference is already challenging to attain. While our debiasing strategy boils downs to a post-processing step, one could argue that the lack of change to the DGM's training strategy itself is actually a strength, since it renders our strategy generator-agnostic.

Another limitation concerns the fact that our debiasing strategy for a regression coefficient requires sampling of synthetic data conditional on a covariate, which is not available in all DGMs. However, this issue is partially mitigated in the case of conditioning on categorical variables, since one can always generate a synthetic dataset unconditionally and then only select samples that fit the condition - though this approach also has its limits, especially when conditioning on multiple covariates at once. Zhou et al. (2023) propose a deep generative approach to sample from a conditional distribution, even when working with high-dimensional data. Future work could explore this strategy.

Finally, our proposal requires that the person generating synthetic data is aware of the analyses that will be run on those data, and has access to the corresponding EICs needed for debiasing (which in particular rules out the possibility for debiasing w.r.t. non-pathwise differentiable parameters, such as conditional means or predictions). For each parameter of interest, the data generated by the DGM will then need to be debiased (simultaneously) w.r.t. that parameter's EIC, which is left for future research. In the case of original data, several debiased estimation strategies that do not require exact knowledge about the EIC already exist. These methods include a) approximating the EIC through finite-differencing (Carone et al., 2019; Jordan et al., 2022) or stochastic approximations via Monte Carlo (Agrawal et al., 2024), and b) automatically estimating the EIC from the data through auto-DML (Chernozhukov et al., 2022). Alternatively, kernel debiased plug-in estimation methods (Cho et al., 2023) enable simultaneous debiasing of all pathwise differentiable target parameters that meet certain regularity conditions, without requiring any knowledge about the EIC. Integrating their insights could further strengthen the foundations of our current work on the interplay between synthetic data, deep generative modelling, and debiased machine learning.