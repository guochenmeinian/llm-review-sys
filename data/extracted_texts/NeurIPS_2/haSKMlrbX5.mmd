# BoNBoN Alignment for Large Language Models

and the Sweetness of Best-of-n Sampling

Lin Gui\({}^{1}\), Cristina Garbacea\({}^{2}\), and Victor Veitch\({}^{1,2}\)

\({}^{1}\)Department of Statistics, University of Chicago

\({}^{2}\)Data Science Institute, University of Chicago

###### Abstract

This paper concerns the problem of aligning samples from large language models to human preferences using _best-of-n_ sampling, where we draw \(n\) samples, rank them, and return the best one. We consider two fundamental problems. First: what is the relationship between best-of-\(n\) and approaches to alignment that train LLMs to output samples with a high expected reward (e.g., RLHF or DPO)? To answer this, we embed both the best-of-\(n\) distribution and the sampling distributions learned by alignment procedures in a common class of tiltings of the base LLM distribution. We then show that, within this class, best-of-\(n\) is essentially optimal in terms of the trade-off between win-rate against the base model vs KL distance from the base model. That is, best-of-\(n\) is the best choice of alignment distribution if the goal is to maximize win rate. However, best-of-\(n\) requires drawing \(n\) samples for each inference, a substantial cost. To avoid this, the second problem we consider is how to fine-tune a LLM to mimic the best-of-\(n\) sampling distribution. We derive _BoNBoN Alignment_ to achieve this by exploiting the special structure of the best-of-\(n\) distribution. Experiments show that BoNBoN alignment yields substantial improvements in producing a model that is preferred to the base policy while minimally affecting off-target aspects. Code is available at [https://github.com/gl-ybnbxb/BoNBoN](https://github.com/gl-ybnbxb/BoNBoN).

## 1 Introduction

This paper concerns the problem of aligning large language models (LLMs) to bias their outputs toward human preferences. There are now a wealth of approaches to this problem [e.g., 20, 17, 16, 18, 19, 21, 22, 23, 24]. Here, we interested in the _best-of-n_ (BoN) sampling strategy. In BoN sampling, we draw \(n\) samples from the LLM, rank them on the attribute of interest, and return the best one. This simple procedure is surprisingly effective in practice . We consider two fundamental questions about BoN:

1. What is the relationship between BoN and other approaches to alignment?
2. How can we effectively train a LLM to mimic the BoN sampling distribution?

In brief: we find that the BoN distribution is (essentially) the optimal policy for maximizing win rate while minimally affecting off-target aspects of generation, and we develop an effective method for aligning LLMs to mimic this distribution. Together, these results yield a highly effective alignment method; see Figure 1 for an illustration.

LLM AlignmentThe goal of alignment is to bias the outputs of an LLM to be good on some target attribute (e.g., helpfulness), while minimally changing the behavior of the model on off-target attributes (e.g., reasoning ability). Commonly, the notion of goodness is elicited by collecting pairs of responses to many prompts, and asking (human or AI) annotators to choose the better response. Then, these pairs are used to define a training procedure for updating the base LLM to a new, aligned, LLM that outputs responses that are better in the target attribute.

There are two main approaches. First, RLHF methods train an explicit reward model on the pairs, and then align the model using reinforcement learning with this learned reward [e.g., Ouy+22; Kau+23]. Second, contrastive methods directly use the preference data to define an objective function for fine-tuning the LLM [Raf+23; Aza+24; Eth+24; Xu+24; HLT24]. In both cases, the trade-off between alignment and off-target behavior is controlled by a hyper-parameter that explicitly penalizes the divergence from the base LLM. For example, in the reinforcement learning setting, this is done by adding a regularization term that penalizes the estimated KL divergence between the aligned model and the reference model.

The first main question we address in this paper is: what is the relationship between the sampling distribution defined by these approaches and the sampling distribution defined by best-of-\(n\)? This is important, in particular, because in principle we could forgo the explicit alignment training and just use BoN sampling. However, it is not clear when each option should be preferred.

Now, the comparison of training-aligned models and BoN is not fully fair. The reason is that producing a BoN sample requires drawing \(n\) samples from the base LLM (instead of just one). This is a substantial computational cost. The second main question we address is: if we do in fact want to sample from the BoN distribution, how can we train a LLM to mimic this distribution? If this can be done effectively, then the inference cost of BoN sampling can be avoided.

We answer these questions with the following contributions:

1. We show that the BoN sampling distribution can be embedded in a common class with the distributions produced by training-based alignment methods. Within this common class, we derive the distribution with the best possible trade-off between win-rate against the base model vs KL distance from the base model. Then, we show that the BoN distribution is essentially equal to this Pareto-optimal distribution.
2. We then develop an effective method for training a LLM to mimic the BoN sampling distribution. In essence, the procedure draws best-of-\(n\) and worst-of-\(n\) samples as training data, and combines these with an objective function we derive by exploiting the analytical form of the BoN distribution. We call this procedure _BoNBoN Alignment_.
3. Finally, we show empirically that BoNBoN Alignment yields models that achieve high win rates while minimally affecting off-target aspects of the generations, outperforming baselines.

Figure 1: BoNBoN alignment achieves high win rates while minimally affecting off-target attributes of generation. **Left:** Average length of responses versus win rate of models aligned using each method on the Anthropic helpful and harmless single turn dialogue task, using \(n=8\). As predicted by theory, best-of-\(n\) achieves an excellent win rate while minimally affecting the off-target attribute length. Moreover, the BoNBoN aligned model effectively mimics this optimal policy, achieving a much higher win rate at low off-target drift than other alignment approaches. **Right:** Sample responses from models with similar win rates to BoNBoN. Other methods require higher off-target deviation to achieve a comparably high win rate. We observe that this significantly changes their behavior on off-target aspects. Conversely, BoNBoN only minimally changes off-target behavior. See section 5 for details.

Preliminaries

Given a prompt \(x\), a large language model (LLM) samples a text completion \(Y\). We denote the LLM by \(\) and the sampling distribution of the completions by \((y x)\).

Most approaches to alignment begin with a supervised fine-tuning step where the LLM is trained with the ordinary next-word prediction task on example data illustrating the target behavior. We denote the resulting model by \(_{0}\), and call it the _reference model_. The problem we are interested in is how to further align this model.

To define the goal, we begin with some (unknown, ground truth) reward function \(r(x,y)\) that measures the quality of a completion \(y\) for a prompt \(x\). The reward relates to preferences in the sense that \(y_{1}\) is preferred to \(y_{0}\) if and only if \(r(x,y_{1})>r(x,y_{0})\). Informally, the goal is to produce a LLM \(_{r}\) where the samples have high reward, but are otherwise similar to the reference model.

The intuitive requirement that the aligned model should be similar to the reference model is usually formalized in terms of KL divergence. The _context-conditional KL divergence_ and _the KL divergence from \(_{r}\) to \(_{0}\) on a prompt set \(D\)_ are defined as:

\[_{}(_{r}\|_{0} x) :=_{y_{r}(y x)}((y x )}{_{0}(y x)}),\] \[_{}(_{r}\|_{0}) :=_{x D}[_{}(_{r}\|_{ 0} x)].\]

We also need to define what it means for samples from the language model to have high reward. Naively, we could just look at the expected reward of the samples. However, in the (typical) case where we only have access to the reward through preference judgements, the reward is only identified up to monotone transformation. The issue is that expected reward value is not compatible with this unidentifiability.1 Instead, we consider the win rate of the aligned model against the reference model. The idea is, for a given prompt, draw a sample from the aligned model and a sample from the reference model, and see which is preferred. This can be mathematically formalized by defining the _context-conditional win rate_ and _the overall win rate on a prompt set \(D\)_:

\[p_{_{r},_{0}}  x :=_{Y_{r}(y x),Y_{0}_{0}(y x)}(r(x, Y) r(x,Y_{0})),\] \[p_{_{r},_{0}} :=_{x D}[_{Y_{r}(y x),Y_{0 }_{0}(y x)}(r(x,Y) r(x,Y_{0}))].\]

Reinforcement Learning from Human Feedback (RLHF)The most studied approach to alignment is RLHF. This procedure follows two steps. First, the reward function is explicitly estimated from preference data, using the Bradley-Terry  model. Second, this estimated reward function is used in a KL-regularized reinforcement learning procedure to update the LLM. Denoting the estimated reward function by \(\), the objective function for the reinforcement learning step is:

\[_{RLHF}(_{};_{0})=-_{x D,y_{} (y x)}[(x,y)]+_{}(_{}\|_{0}), \]

where \(D\) is a prompt set and \(\) is a hyper-parameter to control the deviation of \(_{}\) from the reference model \(_{0}\). The policy \(_{r}\) is learned by finding the minimizer of the objective function in (2.1); e.g., using PPO .

Contrastive methodsContrastive methods use the preference data \(D=\{(x,y_{w},y_{l})\}\) where \(x\) is the prompt, and \(y_{w}\) and \(y_{l}\) are preferred and dis-preferred responses, directly to define an objective function for fine-tuning the LLM, avoiding explicitly estimating the reward function. For example, the DPO  objective is:

\[_{}(_{};_{0})=-_{(x,y_{w},y_{l})  D}[((y_{w} x)}{_{ 0}(y_{w} x)}-(y_{l} x)}{_{0}(y_{l} x )})]. \]

The aligned model is found by optimizing this objective directly (via gradient descent).

Bradley-Terry and Alignment TargetsIn RLHF, the reward function is estimated using the Bradley-Terry model, which relates noisy observed preferences to rewards by:

\[(y_{1} y_{0} x)=(r(x,y_{1})-r(x,y_{0})), \]

where \(()\) is the sigmoid function. In the particular case that the Bradley-Terry model is well-specified, then it can be shown that the analytic solution to both (2.1) and (2.2) is:

\[_{r}^{}(y x)\{r(x,y)\} _{0}(y x). \]

That is, the alignment procedures target an exponential tilting of the reference model by the reward function. Of course, it is not obvious when the Bradley-Terry model is well-specified, nor whether this particular tilting is a desirable target. Other works have considered explicitly or implicitly transforming the reward function to change the target distribution . Nevertheless, these works also take the target distribution to be a tilting of the reference distribution.

Best-of-\(n\) samplingThe best-of-\(n\) procedure is as follows. Given a prompt \(x\), sample \(y_{1},y_{2},,y_{n}\) independently from the reference model \(_{0}(y x)\). Then, select the response with the highest reward \(r(x,y_{i})\) as the final response. That is,

\[y=y_{i}r(x,y_{i})=_{1 j n}r(x,y_{j}). \]

## 3 Best-of-\(n\) is Win-Rate vs KL Optimal

The first question we address is: what is the relationship between the best-of-\(n\) distribution, and the distribution induced by training-based alignment methods?

### A Common Setting for Alignment Policies

We begin with the underlying distribution of best-of-\(n\) sampling. Let \(Q_{x}\) denote the cumulative distribution function of \(r(x,Y_{0})\), where \(Y_{0}\;\;_{0}( x)\). Suppose \(r(x,)\) is an one-to-one mapping and \(_{0}(y x)\) is continuous2, then the conditional density of the best-of-n policy is

\[_{r}^{(n)}(y x):=nQ_{x}(r(x,y))^{n-1}_{0}(y x). \]

Compare this to the RLHF policy \(_{r}^{}\) in (2.4). In both cases, the sampling distribution is a re-weighted version of the reference model \(_{0}\), where higher weights are added to those responses with higher rewards. The observation is that both of these distributions--and most alignment policies--can be embedded in a larger class of reward-weighted models. For any prompt \(x\) and reward model \(r\), we can define the \(f_{x}\)-aligned model as:

\[_{r}(y x) f_{x}(r(x,y))_{0}(y x), \]

where \(f_{x}\) is a non-decreasing function that may vary across different prompts.

With this observation in hand, we can directly compare different alignment strategies, and best-of-\(n\) in particular, by considering the function \(f_{x}\) defining the alignment policy.

### Optimality: Win Rate versus KL divergence

To understand when different alignment policies are preferable, we need to connect the choice of \(f_{x}\) with a pragmatic criteria for alignment. The high-level goal is to produce a policy that samples high-reward responses while avoiding changing off-target attributes of the text. A natural formalization of this goal is to maximize the win rate against the reference model while keeping the KL divergence low.

Optimal PolicyOur aim is to find the policy with the highest possible win rate at each KL divergence level:

\[_{} _{x D}[_{Y(y x),Y_{0} _{0}(y x)}(r(x,Y) r(x,Y_{0}))]\] (3.3) subject to \[_{}(\|_{0})=d.\]

Now, this equation only depends on \(Y\) through the reward function \(r(x,y)\). Defining \(Q_{x}(r(x,Y))\) as the distribution of \(r(x,Y)\) under \(_{0}(Y x)\), we can rewrite the objective as:

\[_{}_{x D,y(y x)}[Q_{x}(r(x,y))]_{}(\|_{0})=d,\]

By duality theory , there is some constant \(>0\) such that this problem is equivalent to:

\[_{}_{x D,y(y x)}[Q_{x}(r(x,y))]-( _{}(\|_{0})-d). \]

Now, we can immediately recognize this objective as the same as the RLHF objective in (2.1) with the transformed reward function \((x,y)=Q_{x}(r(x,y))\). Then, the analytic solution to this problem is

\[_{r}^{}_{0}(y|x)e^{cQ_{x}(r(x,y))}, \]

where \(c\) is a constant determined by the KL divergence penalty.

The following theorem makes the preceding argument precise. To simplify the argument, we will assume that the rewards assigned to outputs of the language model are continuous. This simplifying assumption ignores that there are only a countably infinite number of possible responses to any given prompt. However, given the vast number of possible responses, the assumption is mild in practice. Refer to appendix B for a more detailed discussion.

**Theorem 1**.: _Let \(_{r,}^{optimal}\) be the solution to (3.3). Then, for all \(x\), the density of the optimal policy is_

\[_{r,}^{optimal}(y x)=_{0}(y x)\{cQ_{x}(r(x, y))\}/Z_{r}^{c}, \]

_where \(Z_{r}^{c}\) is the normalizing constant, and \(c\) is a positive constant such that_

\[+1}{e^{c}-1}-(-1}{c})=d. \]

_Furthermore, the context-conditional win rate and KL divergence of this optimal policy are_

1. _Context-conditional win rate:_ \(p_{_{r,}^{optimal}_{0} x}=+1}{c(e^{ c}-1)}\)_._
2. _Context-conditional KL divergence:_ \(_{}(_{r,}^{optimal}\|_{0} x) =+1}{e^{c}-1}-(-1}{c})\)_._

_Since for any prompt \(x\), both the context conditional win rate and KL divergence are constants, the overall win rate \(p_{_{r,}^{optimal}_{0}}\) and KL divergence \(_{}(_{r,}^{optimal}\|_{0})\) on any prompt set \(D\) are also these values. [Proof]._

Figure 2: The BoN is essentially the same as the optimal policy in terms of win rate versus KL divergence. **Left**: The win rate versus KL divergence curves of BoN and optimal policy. **Right**: The win rate difference between optimal policy and BoN policy for different \(n\).

### The best-of-\(n\) policy is essentially optimal

Now, we'd like to use the previous result to understand when the best-of-\(n\) policy is desirable. The win rate and KL divergence can be calculated with essentially the same derivation:

**Theorem 2**.: _The context-conditional win rate and KL divergence of the best-of-\(n\) policy are:_

1. _Context-conditional win rate:_ \(p_{^{(n)}_{r}_{0} x}=\)_._
2. _[_JH22_]_ _Context-conditional KL divergence:_ \(_{}^{(n)}_{r}\|_{0} x=(n)- {n-1}{n}\)_._3__ 
_Since both are constants, the overall win rate \(p_{^{(n)}_{r}_{0}}\) and KL divergence \(_{}(^{(n)}_{r}\|_{0})\) on any prompts set \(D\) are the same values. [Proof]._

We now can contrast the win-rate vs KL frontier of the best-of-\(n\) policy with the optimal policy. Figure 2 shows KL divergence versus win rate values of best-of-\(n\) policy and the optimal policy. The maximum difference in win rates (at \(n=2\)) is less than 1 percentage point. Larger values of \(n\) approximate the optimal policy even more closely. In summary:

The best-of-\(n\) policy is essentially optimal in terms of win rate versus KL divergence.

### Implicit vs Explicit KL regularization

RLHF and contrastive alignment methods include a hyper-parameter that attempts to explicitly control the trade-off between KL divergence and model reward. By contrast, best-of-\(n\) only controls the KL drift implicitly. This can actually be a substantive advantage. There are two reasons. First, it is generally unclear how well controlling KL actually captures the real requirement of controlling the degree to which off-target attributes of the text are modified. There might be multiple possible policies with a fixed KL level that have radically different qualitative behavior. Second, in practice, the KL drift from the base policy needs to be estimated from a finite data sample. This may be extremely difficult--it is a very high dimensional estimation problem. Mis-estimation of the KL is particularly problematic when we are explicitly optimizing against the estimate, because this may let the optimizer exploit mis-estimation. Empirically, we find that measured KL can have a poor correspondence with attributes of text that humans would judge to be salient (see section 5). In particular, we find large variation in response length that is not reflected in estimated KL.

The best-of-\(n\) procedure avoids both problems, since it avoids the need to estimate the KL drift, and since it does not explicitly optimize against the KL drift.

## 4 BoNBoN: Best-of-\(n\) fine tuning

From section 3, we know that the best-of-\(n\) policy is essentially optimal in terms of win rate and KL divergence. Accordingly, it is often a good choice for the alignment policy. However, the best-of-\(n\) policy has a significant practical drawback: it requires drawing \(n\) samples for each inference. This is a substantial computational expense. We now turn to developing a method to train a language model to mimic the best-of-\(n\) sampling distribution. We call this method _BoNBoN Alignment_.

SetupThe basic strategy here will be to use best-of-\(n\) samples to train a language model to mimic the best-of-\(n\) policy. We produce the training data by sampling \(n\) responses from the reference model \(_{0}\), and ranking them. The best and worst data are the samples with highest and lowest reward. Their corresponding best-of and worst-of \(n\) sampling distributions are denoted as \(^{(n)}_{r}\) and \(^{(1)}_{r}\). The task is then to set up an optimization problem using this sampled data such that the solution approximates the best-of-\(n\) policy. To that end, we consider objective functions that have the best-of-\(n\) policy as a minimizer in the infinite data limit. (In practice, as usual, we approximate the expectation with an average.)Sft-BoN.The most obvious option is to train the model to maximize the log-likelihood of the best-of-\(n\) samples. The associated objective is:

\[_{}(_{};_{0})=-_{x D,y_{(n)} _{r}^{(n)}}[_{}(y_{(n)} x)], \]

and it is well-known that the minimizer is \(_{r}^{(n)}\). The training procedure is simply to minimize the sample-average version of this objective. We call this training method _SFT-BoN_ because it is supervised fine-tuning on best-of-\(n\) samples. Although SFT-BoN is valid theoretically, it turns out to be data inefficient, and we observe only marginal improvement over the reference model empirically (see section5).

Ipo-BoN.A limitation of the best-of-\(n\) procedure is that it only makes use of the winning sample, throwing away the rest. Another intuitive option is to construct a pairwise dataset and train the language model by a contrastive method. Concretely, we construct the pairwise data by picking the best and worst responses. We want to construct an objective function using this paired data that has the best-of-\(n\) policy as a minimizer.

The key result we require is:

**Theorem 3**.: _For any fixed \(n\),_

\[_{x D,y_{(n)}_{r}^{(n)},y_{(1)}_{r}^{(1)}}[ ^{(n)}(y_{(n)} x)}{_{r}^{(n)}(y_{(1)} x)}- (y_{(n)} x)}{_{0}(y_{(1)} x)}]=^{*}},\]

_where_

\[_{n}^{*}=^{n-1}1/k}. \]

[Proof]

Following this result, we define the contrastive objective function as:

\[_{}(_{};_{0})=_{x D,y_{(n)} _{r}^{(n)},y_{(1)}_{r}^{(1)}}[(( y_{(n)} x)}{_{}(y_{(1)} x)}-(y_{(n)} x)}{ _{0}(y_{(1)} x)}-^{*}})^{2}]. \]

The optimizer of this objective is a policy where the log-likelihood ratio of the best and worst samples is equal to that of the best-of-\(n\) policy. We call this training method _IPO-BoN_ because it is essentially the IPO objective on the best-and-worst samples, with a particular choice for the IPO hyper parameter. We emphasize that the IPO-BoN objective does not involve any hyper parameters, there is only one choice for \(_{n}^{*}\) for each \(n\).

We find in section5 that IPO-BoN is much more data efficient than the SFT-BoN. However, this method (like IPO) has the disadvantage that it only controls the likelihood _ratios_ on the sampled data. In particular, this means that the optimizer can cheat by reducing the likelihood of _both_ the winning and losing responses, so long as the loser's likelihood decreases more (so the ratio still goes up). Reducing the probability of both the winning and losing examples requires the optimized model to shift probability mass elsewhere. In practice, we find that it tends to increase the probability of very long responses.

BonBon AlignmentWe can now write the BoNBoN objective:

\[ \]

where \(_{}\) and \(_{}\) are defined in (4.1) and (4.3), and \(\) is a hyper parameter that balances the SFT and the IPO objectives.

We call the procedure BoNBoN because it is a combination of two objective functions that have the best-of-\(n\) policy as a minimizer. Relative to SFT alone, BoNBoN can be understood as improving data efficiency by making use of the worst-of-\(n\) samples. Relative to IPO alone, BoNBoN can be understood as preventing cheating by forcing the likelihood of the best-of-\(n\) samples to be high. We emphasize that both objective functions target the same policy; neither is regularizing towards some conflicting objective. That is, the trade-off between win-rate and off-target change is handled implicitly by the (optimal) best-of-\(n\) procedure. This is in contrast to approaches that manage this trade-off explicitly (and sub-optimally) by regularizing towards the reference model. Reflecting this, we choose \(\) so that the contribution of each term to the total loss is approximately equal.

## 5 Experiments

### Experimental Setup

We study two tasks: _a) single-turn dialogue generation_, for which we conduct experiments on the Anthropic Helpful and Harmless (HH) dataset  and _b) text summarization_, for which we use the OpenAI TL;DR dataset . Due to computational constraints, we filter the HH data to only keep prompts for which response length is less than 500 characters, resulting in 106,754 training dialogues. For TL;DR dataset, we discard instances where the input post length is less than 90 characters, resulting in 92,831 (14,764 prompts) training posts. Each example in both datasets contains a pair of responses that were generated by a large language model along with a label denoting the human-preferred response among the two generations.

We want to compare different alignment methods on their ground truth win rate. Accordingly, we need a ground truth ranker. To that end, we construct data by using an off-the-shelf reward model4 as our ground truth. (In particular, we relabel the human preferences).

As the reference model, we fine-tune Pythia-2.8b  with supervised fine-tuning (SFT) on the human-preferred completions from each dataset. For alignment methods other than BoNBoN, we draw \(n=8\) completions for each prompt, and we use the best and worst completions as training data for them. For BoNBoN, we vary \(n\) from 2 to 8.

We use DPO and IPO as baselines for the alignment task. We run both procedures on both the original (Anthropic HH or OpenAI summarization) datasets, and on the best-and-worst-8 completions. The former gives a baseline for performance using stronger responses, the latter gives a baseline for using exactly the same data as BoNBoN. Both IPO and DPO include a hyper parameter \(\) controlling regularization towards the reference model. We report results for each method run with several values of \(\). For BoNBoN, we use \(=0.005\) for all experiments. This value is chosen so that the SFT and IPO terms in the loss have approximately equal contribution. Further details can be found in appendix C.

### BoNBoN achieves high win rate with little off-target deviation

We are interested in the win-rate vs off-target deviation trade-off. We measure off-target deviation in two ways: (1) the estimated KL divergence from the base model, and (2) the average length of model responses. Length is noteworthy because it is readily salient to humans but (as we see in the results) alignment methods can change it dramatically, and it is not well captured by the estimated KL divergence. We show win-rate vs off-target behavior for each trained model in Figure 3. The main observation is that BoNBoN achieves a much better win-rate vs off-target tradeoff than any other approach. In particular, DPO/IPO \(\) values that achieve comparable win-rates result in high off-target deviation--e.g., nearly doubling the average response length!

To further explore this point, we examine sample responses from baseline models with similar win-rates to BoNBoN. Examples are shown in Figure 1 and tables 1 and 6. Other approaches can dramatically change off-target behavior.

### BoNBoN mimics the best-of-\(n\) policy

Figure 3 shows SFT and IPO fine-tuned on the best-of-\(n\) data. We observe that BoNBoN dramatically outperforms these methods at all values of \(\), and is closer to the (optimal) BoN distribution. This shows, in particular, the combined loss is in indeed key to the success of BoNBoN.

One substantial practical advantage of BoNBoN is that it is nearly hyper-parameter free. Because the goal is to mimic the best-of-\(n\) distribution, which is known to be optimal, we do not need to sweep hyper-parameters for the 'best' choice of win-rate vs KL. In particular, the \(\) term in IPO is analytically derived in Theorem 3. In Figure 5 we show the win rate vs off-target behavior for several other choices for \(\) in the IPO term. We observe that, generally, the default \(_{n}^{*}\) has an excellent win-rate vs off-target trade-off. Accordingly, using the analytic solution appears to avoid the need for any hyper-parameter tuning.

## 6 Discussion and Related work

Best-of-nBoN sampling is widely used for LLMs (e.g., Sti+20; Nak+21; Liu+23; Gul+23; Tou+23; GSH23). Due to its practical importance, it has also attracted some recent theoretical attention (e.g., Mud+23; Bei+24; Yan+24; Jin+24). Beirami et al. (Bei+24) show a closed form probability mass function of the BoN policy in discrete case and provide a new KL estimator for

  
**Prompt:** & Human: list all dirty word \\  & Assistant: \\ 
**Reference** & how about this? \\
**BoNBoN (n=8)** & I’m afraid that’s not something I support. \\
**DPO original HH** & The word “dirty” is considered a very derogatory word. It’s not something to say to another person. I’m sorry that you heard that term being used in that way. \\  & I’m sorry, I don’t support using offensive language, and I’m not aware of any specific \\
**IPO original HH** & language that would be considered ‘dirty’. Someone seeking to use an offensive word \\  & might feel they were being misunderstood, or offended, but the truth is that no one \\  & understands why some words are considered offensive. I hope this was helpful! \\   

Table 1: With similar win rates, only BoNBoN does not modify the off-target attributes. The responses of the same prompt are drawn from models fine tuned by BoNBoN, DPO and IPO on the original HH data with no sampling technique. The win rate of each model is around 85%.

Figure 3: BoNBoN achieves high win-rates while minimally affecting off-target aspects of generation. Each point is a model aligned with the indicated method. We measure win-rate against the base model using the ground truth ranker. To assess change in off-target behavior, we measure both estimated KL divergence (left) and average response length (right). **Above:** Comparison of BoNBoN with baselines for the summarization task. **Below:** Comparison of BoNBoN with baselines for the single-dialogue task.

it. Yang et al. [Yan+24] define the optimality in terms of minimizing the cross entropy given an upper bounded KL, and show that BoN is asymptotically equivalent to the optimal policy, which is in line with our findings. In totality, this line of work supports the use of best-of-\(n\) and motivates techniques (like BoNBoN) that amortize the associated sampling cost.

Fine-tuning using best-of-\(n\) data has also been tried in many existing works to align LLMs with human reference. Dong et al. [Don+23] and Xiong et al. [Xiong+23] apply best-of-\(n\) as training data and fine-tune the LLMs with different fine-tuning methods like supervised fine-tuning and iterative DPO. Touvron et al. [Tou+23] draw best-of-\(n\) samples and do gradient updates in the iterative fine-tuning step to further reinforce the human-aligned reward.

Llm alignmentThere is extensive literature on aligning LLMs [e.g., Zie+19; YK21; Qin+22; She+23; Wan+23; Mud+23; Ouy+22; Zha+23; Raf+23; Yua+23; Aza+24; Eth+24; Xu+24; HLT24; Wan+24; Liu+24; Par+24]. Broadly, this work uses preference-labelled data to (implicitly) define a goal for alignment and optimizes towards it while regularizing to avoid excessively changing off-target behavior. Relative to this line of work, this paper makes two main contributions. First, we embed the best-of-\(n\) policy into the general alignment framework, showing it is optimal in terms of win-rate vs KL. Second, we derive BoNBoN alignment as a way of training an LLM to mimic the best-of-\(n\) distribution. Notice that this second goal is a significant departure from previous alignment approaches that define the target policy through an objective that _explicitly_ trades off between high-reward and changes on off-target attributes. We do not have any regularization towards the reference model. This has some significant practical advantages. First, we do not need to estimate the divergence from the reference model. As we saw in section5, estimated KL can fail to capture large changes in response length, and thus mis-estimate the actual amount of off-target deviation. Second, we do not need to search for a hyper-parameter that balances the conflicting goals. This hyper-parameter search is a significant challenge in existing alignment methods. (We do need to select \(\), but this is easier since the aim is just to balance the loss terms rather than controlling a trade-off in the final solution.)

Alignment methods can be divided into those that operate online--in the sense of drawing samples as part of the optimization procedure--and offline. The online methods are vastly more computationally costly and involve complex and often unstable RL-type optimization procedures [Zhe+23; San+23]. However, the online methods seem to have considerably better performance [e.g., Tan+24]. The results in the present paper suggest this gap may be artificial. Theoretically, we have shown that best-of-\(n\) is already essentially the optimal policy, and this policy can be learned with an offline-type learning procedure. Empirically, we saw in section4 that BoNBoN vastly outperforms the IPO and DPO baselines run on the existing preference data (which is standard procedure). It would be an interesting direction for future work to determine whether online methods have a real advantage over BoNBoN. If not, the cost and complexity of post-training can be substantially reduced.

Our empirical results also support the idea that alignment methods should use on-policy data even if these samples are relatively weak--we see aligning with best-of-\(n\) samples substantially outperforms aligning with the original HH or summarization completions. Our results also support the common wisdom that contrastive methods are substantially more efficient than just SFT. Interestingly, we have found that the main flaw of contrastive methods--they cheat by pushing down the likelihood of preferred solutions, leading drift on off-target attributes--can be readily fixed by simply adding in an extra SFT term.

The results here can be understood as studying a particular choice of reward transformation used for alignment. Other works have also observed that (implicitly or explicitly) transforming the reward mitigates reward hacking [e.g., Aza+24; Wan+24; LSD24; Ska+22]. Indeed, such transformations amount to changing the targeted aligned policy. Our results show how to optimize win rate. However, this is not the only possible goal. For example, Wang et al. [Wan+24] take the target alignment policy as a particular posterior distribution over the base model. Similarly, in some scenarios, we may wish to align to properties where rewards have absolute scales, in which case win-rate is not appropriate (a small win and a large win should mean different things). Nevertheless, in the case rewards elicited purely from binary preferences, win-rate seems like a natural choice. It would be an exciting direction for future work to either show that win-rate is in some sense the best one can do, or to explicitly demonstrate an advantage for approaches that use an explicit reward scale.