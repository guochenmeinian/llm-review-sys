# No-Regret Learning for Fair Multi-Agent

Social Welfare Optimization

 Mengxiao Zhang

University of Iowa

mengxiao-zhang@uiowa.edu

&Ramiro Deo-Campo Vuong

Cornell University

ramdcv@cs.cornell.edu

&Haipeng Luo

University of Southern California

haipeng1@usc.edu

###### Abstract

We consider the problem of online multi-agent Nash social welfare (NSW) maximization. While previous works of Hossain et al. (2021), Jones et al. (2023) study similar problems in stochastic multi-agent multi-armed bandits and show that \(\)-regret is possible after \(T\) rounds, their fairness measure is the product of all agents' rewards, instead of their NSW (that is, their geometric mean). Given the fundamental role of NSW in the fairness literature, it is more than natural to ask whether no-regret fair learning with NSW as the objective is possible. In this work, we provide a complete answer to this question in various settings. Specifically, in stochastic \(N\)-agent \(K\)-armed bandits, we develop an algorithm with \(}(K^{}T^{})\) regret and prove that the dependence on \(T\) is tight, making it a sharp contrast to the \(\)-regret bounds of Hossain et al. (2021); Jones et al. (2023). We then consider a more challenging version of the problem with adversarial rewards. Somewhat surprisingly, despite NSW being a concave function, we prove that no algorithm can achieve sublinear regret. To circumvent such negative results, we further consider a setting with full-information feedback and design two algorithms with \(\)-regret: the first one has no dependence on \(N\) at all and is applicable to not just NSW but a broad class of welfare functions, while the second one has better dependence on \(K\) and is preferable when \(N\) is small. Finally, we also show that logarithmic regret is possible whenever there exists one agent who is indifferent about different arms.

## 1 Introduction

In this paper, we study online multi-agent Nash social welfare (NSW) maximization, which is a generalization of the classic multi-armed bandit (MAB) problem (Thompson, 1933; Lai and Robbins, 1985). Different from MAB, in which the learner makes her decisions sequentially in order to maximize her own reward, in online multi-agent NSW maximization, the learner's decision affects multiple agents and the goal is to maximize the NSW over all the agents. Specifically, NSW is defined as the geometric mean of the expected utilities over all agents (Moulin, 2004), which can be viewed as a measure of fairness among the agents. This problem includes many important real-life applications such as resource allocation (Jones et al., 2023), where the learner needs to guarantee fair allocations among multiple agents. We refer the readers to (Hossain et al., 2021; Jones et al., 2023) for more applications of NSW maximization.

Recent work by Hossain et al. (2021); Jones et al. (2023) studies a similar problem but with NSW\({}_{}\) as the objective, a variant of NSW that is defined as the product of the utilities over agents instead oftheir geometric mean. While the optimal strategy is the same if the utility for each agent is stationary, this is not the case with a non-stationary environment. Moreover, \(_{}\) is homogeneous of degree \(N\) instead of degree \(1\), where \(N\) is the number of agents, meaning that \(_{}\) is more sensitive to the scale of the utility. Specifically, if the utilities of each agent are scaled by \(2\), then NSW is scaled by \(2\) as well, but \(_{}\) is scaled by \(2^{N}\). Therefore, it is arguably more reasonable to consider regret with respect to NSW, which has not been studied before (to our knowledge) and is the main objective of our work.

From a technical perspective, however, due to the lack of Lipschitzness, NSW poses much more challenges in regret minimization compared to \(_{}\). For example, one cannot directly apply the algorithm for Lipschitz bandits (Kleinberg et al., 2019) to our problem, while it is directly applicable to \(_{}\) as mentioned in (Hossain et al., 2021; Jones et al., 2023). Despite such challenges, we manage to provide complete answers to this problem in various setting. Specifically, our contributions are listed below (where \(T,N\), and \(K\) denote the number of rounds, agents, and arms/actions respectively):

* (Section 3) We first study the stochastic bandit setting, where the utility matrix at each round is i.i.d. drawn from an unknown distribution, and the learner can only observe the utilities (for different agents) of the action she picked. In this case, we develop an algorithm with \(}(K^{}T^{})\) regret.1 While our algorithm is also naturally based on the Upper Confidence Bound (UCB) algorithm as in Hossain et al. (2021); Jones et al. (2023), we show that a novel analysis with Bernstein-type confidence intervals is important for handling the lack of Lipschitzness of NSW. Moreover, we prove a lower bound of order \((} K^{}T^{})\), showing that the dependence on \(T\) is tight. This is in sharp contrast to the \(\)-regret bound of Hossain et al. (2021); Jones et al. (2023) and demonstrates the difficulty of learning with NSW compared to \(_{}\). * (Section 4.1) We then consider a more challenging setting where the utility matrix at each round can be adversarially chosen by the environment. Somewhat surprisingly, we show that no algorithm can achieve sublinear regret in this case, despite NSW being concave and the vast literature on bandit online maximization with concave utility functions (the subtlety lies in the slightly different feedback model). In fact, the same impossibility result also holds for \(_{}\) as we show.
* (Section 4.2) To bypass such impossibility, we further consider this adversarial setting under richer feedback, where the learner observes the full utility matrix after her decision (the so-called full-information feedback). Contrary to the bandit feedback setting, learning is not only possible now but can also be much faster despite having adversarial utilities. Specifically, we design two different algorithms with \(\)-regret. The first algorithm is based on Follow-the-Regularized-Leader (FTRL) with the log-barrier regularizer, which achieves \(()\) regret (Section 4.2.1). Notably, this algorithm does not have any dependence on the number of agents \(N\) and can also be generalized to a broader class of social welfare functions. The second algorithm is based on FTRL with a Tsallis entropy regularizer, which achieves \(}(K^{-})\) regret and is thus more favorable when \(K\) is much larger than \(N\) (Section 4.2.2). Finally, we also show that improved logarithmic regret is possible as long as at each round there exists at least one agent who is indifferent about the learner's choice of arm (Section 4.2.3).

### Related Work

Hossain et al. (2021); Jones et al. (2023) are most related to our work. Hossain et al. (2021) is the first to consider designing no-regret algorithms under \(_{}\) for the stochastic multi-agent multi-armed bandit problem. Specifically, they propose two algorithms. The first one is based on \(\)-greedy and achieves \((T^{})\) regret efficiently, and the second one is based on UCB and achieves \(}()\) regret inefficiently. Jones et al. (2023) improves these results by providing a better UCB-based algorithm that is efficient and achieves the same \(}()\) regret. To the best of our knowledge, there are no previous results for regret minimization over NSW under this particular setup.

However, several other models of fairness have been introduced in (single-agent or multi-agent) multi-armed bandits, some using NSW as well. These models differ in whether they aim to be fair among different objectives, different arms, different agents, different rounds, or others. Mostrelated to this paper is multi-objective bandits, in which the learner tries to increase different and possibly competing objectives in a fair manner. For example, Drugan and Nowe (2013) introduces the multi-objective stochastic bandit problem and offers a regret measure to explore Pareto Optimal solutions, and Busa-Fekete et al. (2017) investigates the same setting using the Generalized Gini Index in their regret measure to promote fairness over objectives. Their regret measure closely resembles the one we use, except they apply some social welfare function (SWF) to the cumulative expected utility of agents over all rounds as opposed to the expected utility of agents each round. On the other hand, some other works study fairness among different rounds which incentivizes the learner to perform well consistently over all rounds (Barman et al., 2023; Sawarri et al., 2024). Besides, there are other models that measure fairness in different ways, including how often each arm is pulled (Joseph et al., 2016; Liu et al., 2017; Gillen et al., 2018; Chen et al., 2020) and how the regret is allocated across different groups (Baek and Farias, 2021).

Kaneko and Nakamura (1979) axiomatically derives the NSW function. It is a fundamental and widely-adopted fairness measure and is especially popular for the task of fairly allocating goods. Caragiannis et al. (2019) justifies the fairness of NSW by showing that its maximum solution ensures some desirable envy-free property. This result prompted the design of approximation algorithms for the problem of allocating indivisible goods by maximizing NSW, which is known to be NP-hard even for simple valuation functions (Barman et al., 2018; Cole and Gkatzelis, 2015; Garg et al., 2023; Li and Vondrak, 2021).

There is a vast literature on the multi-armed bandit problem; see the book by Lattimore and Szepesvari (2020) for extensive discussions. The standard algorithm for the stochastic setting is UCB (Lai and Robbins, 1985; Auer et al., 2002a), while the standard algorithm for the adversarial setting is FTRL or the closely related Online Mirror Descent (OMD) algorithm (Auer et al., 2002b; Audibert and Bubeck, 2010; Abernethy et al., 2015). For FTRL/OMD, the log-barrier or Tsallis entropy regularizers have been extensively studied in recent years due to some of their surprising properties (e.g., (Foster et al., 2016; Wei and Luo, 2018; Zimmer and Seldin, 2019; Lee et al., 2020)). They are rarely used in the full-information setting as far as we know, but our analysis reveals that they are useful even in such settings, especially for dealing with the lack of Lipschitzness of NSW.

## 2 Preliminaries

General Notation.Throughout this paper, we denote the set \(\{1,2,,n\}\) by \([n]\) for any positive integer \(n\). For a matrix \(M^{m n}\), we denote the \(i\)-th row vector of \(M\) by \(M_{i,}^{n}\), the \(j\)-th column vector of \(M\) by \(M_{i, j}^{m}\), and the \((i,j)\)-th entry of \(M\) by \(M_{i,j}\). We say \(M 0\) if \(M\) is a positive semi-definite matrix. The \((K-1)\)-dimensional simplex is denoted as \(_{K}\), and its clipped version with a parameter \(>0\) is denoted as \(_{K,}=\{p_{K} p_{i}, i[K]\}\). We use \(\) and \(\) to denote the all-zero and all-one vector in an appropriate dimension. For two random variables \(X\) and \(Y\), we use \(X}{{=}}Y\) to say \(X\) is equivalent to \(Y\) in distribution.

For a twice differentiable function \(f\), we use \( f()\) and \(^{2}f()\) to denote its gradient and Hessian. For concave functions that are not differentiable, \( f()\) denotes a super-gradient. Throughout the paper, we study functions of the form \(f(u^{}p)\) for \(u^{m n}\) and \(p_{m}\). In such cases, the gradient, super-gradient, or hessian are all with respect to \(p\) unless denoted otherwise (for example, we write \(_{u}f(u^{}p)\), with an explicit subscript \(u\), to denote the gradient with respect to \(u\)).

Social Welfare FunctionsA social welfare function (SWF) \(f:^{N}\) measures the desirability of the agents' expected utilities. Specifically, for two different vectors of expected utilities \(,^{}^{N}\), \(f()>f(^{})\) means that \(\) is a fairer alternative than \(^{}\). In each setting we explore, each action by the learner yields some expected utility for each of the \(N\) agents, and the learner's goal is maximize some SWF applied to these \(N\) expected utilities.

Nash Social Welfare (NSW)For the majority of this paper, we focus on a specific type of SWF, namely the Nash Social Welfare (NSW) function (Nash, 1950; Kaneko and Nakamura, 1979). Specifically, for \(^{N}\), NSW is defined as the geometric mean of the \(N\) coordinates:

\[()=_{n[N]}_{n}^{1/N}. \]As mentioned, Hossain et al. (2021), Jones et al. (2023) considered a closely related variant that is simply the product of the coordinates: \(_{}()=_{n[N]}_{n}\). It is clear that NSW has a better scaling property since it is homogeneous: scaling each \(_{n}\) by a constant \(c\) also scales \(()\) by \(c\), but it scales \(_{}()\) by \(c^{N}\). This makes \(_{}\) an unnatural learning objective, which motivates us to use NSW as our choice of SWF. Learning with NSW, however, brings extra challenges since it is not Lipschitz in the small-utility regime (while \(_{}\) is Lipschitz over the entire \(^{N}\)). We shall see in subsequent sections how we address such challenges.

We remark that while our main focus is regret minimization with respect to NSW, some of our results also apply to \(_{}\) or more general classes of SWFs (as will become clear later).

Problem Setup.The \(N\)-agent \(K\)-armed social welfare optimization problem we consider is defined as follows (with \(N 2\) and \(K 2\) throughout). Ahead of time, with the knowledge of the learner's algorithm, the environment decides \(T\) utility matrices \(u_{1},,u_{T}^{K N}\), where \(u_{t,i,n}\) is the utility of agent \(n\) if arm/action \(i\) is selected at round \(t\). Then, the learner interacts with the environment for \(T\) rounds: at each round \(t\), the learner decides a distribution \(p_{t}_{K}\) and then samples an action \(i_{t} p_{t}\). In the full-information feedback setting, the learner observes the full utility matrix \(u_{t}\) after her decision, and in the bandit feedback setting, the learner only observes \(u_{t,i_{t},n}\) for each agent \(n[N]\), that is, the utilities of the selected action.

We consider two different types of environments, the _stochastic_ one and the _adversarial_ one, with a slight difference in their regret definition. In the stochastic environment, there exists a mean utility matrix \(u^{K N}\) such that at each round \(t\), \(u_{t}\) is an i.i.d. random variable with mean \(u\). Fix an SWF \(f\). The social welfare of a strategy \(p_{K}\) is defined as \(f(u^{}p)\), which is with respect to the agents' expected utilities over the randomness of both the learner's and the environment's. The regret is then defined as follows:

\[_{}=T_{p_{K}}f(u^{}p)- [_{t=1}^{T}f(u^{}p_{t})], \]

which is the difference between the total social welfare of the optimal strategy and that of the learner. When \(f\) is chosen to be \(_{}\), Eq. (2) reduces to the regret notion considered in Hossain et al. (2021), Jones et al. (2023).

On the other hand, in the adversarial environment, we do not make any distributional assumption on the utility matrices and allow them to be selected arbitrarily. The social welfare of a strategy \(p_{K}\) for time \(t\) is defined as \(f(u_{t}^{}p)\), and the overall regret of the learner is correspondingly defined as:

\[_{}=_{p_{K}}_{t=1}^{T}f(u_{t}^{}p)- [_{t=1}^{T}f(u_{t}^{}p_{t})]. \]

In both Eq. (2) and Eq. (3), the expectation is taken with respect to the randomness of the algorithm.

Social welfare of expected utilities versus expected social welfare of realized utilities.One might wonder why we measure fairness using the social welfare of expected utilities (e.g., \(f(u^{}p)\)), instead of the expected social welfare of realized utilities (e.g., \(_{i p}[f(u^{}e_{i})]\)). This is because the former is arguably more meaningful as a fairness measure. To see this, consider \(f=\) or \(f=_{}\) and a setting with 2 agents, 2 arms, and \(u\) being the identity matrix. Then, in terms of \(f(u^{}p)\), the uniform distribution is the best policy (which makes sense from a fairness viewpoint), while in terms of \(_{i p}[f(u^{}e_{i})]\), all distributions achieve the same value of 0, implying that all polices are as fair, which is clearly undesired.

Connection to Bandit Convex optimization.When taking \(f=\) (our main focus) and considering the bandit feedback setting, our problem is seemingly an instance of the heavily-studied Bandit Convex optimization (BCO) problem, since \(-\) is convex. However, there is a slight but critical difference in the feedback model: a BCO algorithm would require observing \(f(u_{t}^{}p_{t})\), or equivalently \(u_{t}^{}p_{t}\), at the end of each round \(t\), while in our problem the learner only observes \(u_{t,i_{t},:}\), a much more realistic scenario. Even though they have the same expectation, due to the non-linearity of NSW, this slight difference in the feedback turns out to cause a huge difference in terms of learning -- the minimax regret for BCO is known to be \(()\), while in our problem (with bandit feedback),

[MISSING_PAGE_FAIL:5]

To handle this issue, we require a more careful analysis. Specifically, using Freedman's inequality, we know that with a high probability,

\[_{t,i,n}[u_{i,n},u_{i,n}+8(NKT^{2})}{N_{ t,i}}}+}(1/N_{t,i})][u_{i,n},2u_ {i,n}+}(1/N_{t,i})]. \]

With the help of Eq. (6), we consider two different cases at each round \(t\). The first case is that there exists certain \(n[N]\) such that \( p_{t},u_{:,n}\) for some \(>0\) to be chosen later. In this case, we use Eq. (6) to show

\[|(_{t}^{}p_{t})-(u^{} p_{t})| ((u^{}p_{t}))+}_{i=1}^{K}}{N_{t,i}}^{ }\] \[^{}+} _{i=1}^{K}}{N_{t,i}}^{}, \]

where the first inequality uses Eq. (6) and the second inequality is because \((u^{}p_{t}) p_{t},u_{n}^{}\) for any \(n[N]\). For the second term in Eq. (7), a standard analysis shows that it is upper bounded by \(}K^{}T^{}\).

Now we consider the case where \( p_{t},u_{:,n}\) for all \(n[N]\). In this case, via a decomposition lemma (Lemma C.1), we show that

\[|(_{t}^{}p_{t})-(u^{} p_{t})|_{n=1}^{N}[ p_{t},_{t_{:,:,n}} ^{}- p_{t},u_{:,n}^{}]= (_{n=1}^{N},_{t_{:,:,n}}-u_{ :,n}}{N p_{t},u_{:,n}^{}}). \]

To bound Eq. (8), we use Eq. (6) again:

\[,_{t_{:,:,n}}-u_{:,n}}{ p_{t},u_{ :,n}^{}}(,u_{:,n }^{-}}_{i=1}^{K}}{N_{t, i}}})(^{-}_{i=1}^{K} }{N_{t,i}}}), \]

where the last inequality is due to the condition \( p_{t},u_{:,n}\) for all \(n[N]\). Finally, combining Eq. (7), Eq. (8), Eq. (9), followed by direct calculations, we show that

\[[_{}]_{t=1}^{T} |(_{t}^{}p_{t})-(u^{}p_{t})| }(T^{}+K^{}T^{ }+^{-}K).\]

Picking the optimal choice of \(\) finishes the proof.

We now highlight the importance of using a Bernstein-type confidence width in Eq. (4): if the standard Hoeffding-type confidence width is used instead, then one can only obtain \(_{t,i,n}-u_{i,n}(}})\), and consequently, Eq. (8) can only be bounded by \((^{-})\) after taking summation over \(t[T]\). This eventually leads to a worse regret bound of \(}(K^{}T^{})\).

### Lower Bound

Next, we prove an \((T^{})\) lower bound for this setting. This not only shows that the regret bound we achieve via Algorithm 1 is tight in \(T\), but also highlights the difference and difficulty of learning with NSW compared to learning with \(_{}\), since in the latter case, \(()\) regret is minimax optimal (Hossain et al., 2021; Jones et al., 2023).

**Theorem 3.2**.: _In the bandit feedback setting, for any algorithm, there exists a stochastic environment in which the expected regret (with respect to \(\)) of this algorithm is \((}{N^{3}} K^{}T^{})\) for \(N K\) and sufficiently large \(T\)._

We defer the full proof to Appendix A.2 and discuss the hard instance used in the proof below. First, the mean utility vector \(u_{:,n}\) for each agent \(n 2\) is a constant vector \(\). This makes the problem equivalent to a one-agent problem, but with \( p,u_{,1}^{1/N}\) as the reward, instead of \( p,u_{,1}\) as in standard stochastic \(K\)-armed bandits.

Then, for the first agent, different from the standard \(K\)-armed bandits, where the hardest instance is to hide one arm with a slightly better expected reward of \(+\) among other \(K-1\) arms with expected reward of exactly \(\),2 we hide one arm with expected reward \(K/T\) among other \(K-1\) arms with exactly \(0\) reward (so overall the rewards are shifted towards \(0\) but with a smaller gap between the best arm and the others). By standard information theory arguments, within \(T\) rounds the learner cannot distinguish the best arm from the others. Therefore, the best strategy she can apply is to pick a uniform distribution over actions, suffering \(((1-K^{-})(K/T)^{})=(K^{ }T^{-})\) regret per round and leading to \((K^{}T^{})\) regret overall.

## 4 Adversarial Environments

Now that we have a complete answer for the stochastic setting, we move on to consider the adversarial case where each \(u_{t}\) is chosen arbitrarily, a multi-agent generalization of the expert problem (full-information feedback)  and the adversarial multi-armed bandit problem (bandit feedback) . There are no prior studies on this problem, be it with \(f=\) or \(f=_{}\), as far as we know.

### Impossibility Results with Bandit Feedback

We start by considering the bandit feedback setting. As mentioned in Section 2, even though NSW is a concave function, our problem is not an instance of Bandit Convex Optimization, since we can only observe \(u_{t,i_{}:}\) instead of \((u_{t}^{}p_{t})\) at the end of round \(t\). Somewhat surprisingly, this slight difference in the feedback in fact makes a sharp separation in learnability -- while \(()\) regret is achievable in BCO, we prove that \(o(T)\) regret is impossible in our problem.

Before showing the theorem and its proof, we first give high level ideas on the construction of the hard environments. Specifically, we consider the environment with \(2\) agents, \(2\) arms, and binary utility matrix \(u_{t}\{0,1\}^{2 2}\). Similar to the hard instance in the stochastic environment, we set \(u_{t,,2}=\), reducing the problem to a single-agent one. For the first agent, we let \(u_{t,,1}\) at each round \(t\) be i.i.d. drawn from a stationary distribution over the \(4\) binary utility vectors \(\{(0,0),(0,1),(1,0),(1,1)\}\). Then, we construct two different distributions, \(\) and \(^{}\), over these \(4\) binary utility vectors satisfying that: 1) the distribution of the learner's observation is identical for \(\) and \(^{}\); 2) the optimal strategy for \(\) and \(^{}\) are significantly different. The first property guarantees that no algorithm can distinguish these two environments, while the second property ensures that there is no one single strategy that can perform well in both environments. Formally, we prove the following theorem.

**Theorem 4.1**.: _In the bandit feedback setting, for any algorithm, there exists an adversarial environment such that \([_{}]=(T)\) for \(f=\)._

Proof.: As sketched earlier, we consider two different environments with 2 agents, 2 arms, and binary utility matrices \(u_{t}\{0,1\}^{2 2}\), \(t[T]\). In both environments, we have \(u_{t,,2}=\). Next, we construct two different distributions from which \(u_{t,,1}\) is potentially drawn from, \(\) and \(^{}\), over \(\{(0,0),(0,1),(1,0),(1,1)\}\). Specifically, \(\) is characterized by \((q_{00},q_{01},q_{10},q_{11})=(,,, {10})\), where \(q_{xy}\) is the probability of the vector \((x,y)\) in \(\); \(^{}\) is characterized by \((q^{}_{00},q^{}_{01},q^{}_{10},q^{}_{11})=(,,,)\), where \(q^{}_{xy}\) is the probability of vector \((x,y)\) in \(^{}\). With a slight abuse of notation, we write \(u\) for a matrix \(u\{0,1\}^{2 2}\) if \(u_{,1}\) is drawn from \(\) and \(u_{,2}=\); the same for \(^{}\).

We argue that the learner's observations are equivalent in distribution in \(\) and \(^{}\), since the marginal distributions of the utility of each action are the same. Specifically,

* When action \(1\) is chosen, the distributions of the learner's observation in both \(\) and \(^{}\) are a Bernoulli random variable with mean \(q_{10}+q_{11}=q^{}_{10}+q^{}_{11}=\);* When action \(2\) is chosen, the distributions of the learner's observation in both \(\) and \(^{}\) are a Bernoulli random variable with mean \(q_{01}+q_{11}=q^{}_{01}+q^{}_{11}=\).

Direct calculation shows \(p_{}=*{argmax}_{p_{K}}_{u} [(u^{}p)]=(^{2}}{q_{01}^{2}+ q_{10}},^{2}}{q_{01}^{2}+q_{10}^{2}})=(0.2,0.8)\) and \(p^{}_{}=*{argmax}_{p_{K}}_{u ^{}}[(u^{}p)]=(^{2}}{q_{10}^{2}+q_{10}^{2}},^{2}}{q_{10}^{2}+q_{10}^{2}} )=(,)\), which are constant apart from each other. Pick a threshold value \(=(0.2,)\). Direct calculation shows that for a strategy \(p\) with \(p_{1}\), we have \(_{u}[(u^{}p_{})- (u^{}p)]\) where \(=\); similarly, for a strategy \(p\) with \(p_{1}<\), we have \(_{u^{}}[(u^{}p_{})- (u^{}p)]\) as well. Now, given an algorithm, let \(_{}\) be the probability that the number of rounds \(p_{t,1}\) is larger than \(\) under environment \(\), and \(_{^{}}\) be the probability of the complement of this event under environment \(^{}\). We have,

\[_{}[_{}]_ {}[_{t=1}^{T}(u_{t}^{}p_{})- _{t=1}^{T}(u_{t}^{}p_{t})] }T}{2},\] \[_{^{}}[_{}] _{^{}}[_{t=1}^{T}(u_{t}^ {}p^{}_{})-_{t=1}^{T}(u_{t}^{}p_{t})] _{^{}}T}{2}.\]

Finally, since the feedback for the algorithm is the same in distribution in these two environments, we know \(_{}+_{^{}}=1\), and thus

\[\{_{}[_{}],_{^{}}[_{}]\}_{}[ _{}]+_{^{}}[_{}]}{2}}+_{^{}})T }{4}=(T),\]

which finishes the proof. 

In fact, by a similar but more involved construction (that actually requires using two agents in a non-trivial way), the same impossibility result also holds for \(f=_{}\); see Appendix B.1. We remark that non-linearity of \(f\) in these results plays an important role in the hard instance construction, since otherwise, the optimal strategy for \(\) and \(^{}\) will be the same as they both induce the same marginal distributions.

### Full-Information Feedback

To sidestep the impossibility result due to the bandit feedback, we shift our focus to the full-information feedback model, where the learner observes the entirety of the utility matrix \(u_{t}\) at the end of round \(t\). As mentioned, this corresponds to a multi-agent generalization of the well-known expert problem (Freund and Schapire, 1997). We propose several algorithms for this setting, showing that the richer feedback not only makes learning possible but also leads to much lower regret.

#### 4.2.1 FTRL with Log-Barrier Regularizer

When \(f\) is concave, our problem is in fact also an instance of the well-known Online Convex Optimization (OCO) (Zinkevich, 2003). However, standard OCO algorithms such as Online Gradient Descent, an instance of the more general Follow-the-Regularized-Leader algorithm with a \(_{2}\) regularizer, require the utility function to also be Lipschitz and thus cannot be directly applied to learning NSW. Nevertheless, we will show that using a different regularizer that induces more stability than the \(_{2}\) regularizer can resolve this issue.

More specifically, the FTRL algorithm is shown in Algorithm 2, which predicts at time \(t\) the distribution \(p_{t}=*{argmin}_{p_{K}}(p,-_{s=1}^{t-1} f(u_{s }^{}p_{s}))+(p)\) for some learning rate \(\) and some strongly convex regularizer \(\). Standard analysis shows that the regret of FTRL contains two terms: the regularization penalty term that is of order \(1/\) and the stability term that is of order \(_{t}\| f(u_{t}^{}p_{t})\|_{^{2}-(p_{t})}^{2}\) where we use the notation \(\|a\|_{M}=Ma}\). To deal with the lack the Lipschitzness, that is, the potentially large \( f(u_{t}^{}p_{t})\), we need to find a regularizer \(\) so that the induced local norm \(\| f(u_{t}^{}p_{t})\|_{^{-2}(p_{t})}\) is always reasonably small despite \( f(u_{t}^{}p_{t})\) being large (in \(_{2}\) norm for example).

```
Inputs: a SWF \(f\), a learning rate \(>0\), and a strongly convex regularizer \(:_{K}\). for\(t=1,2,,T\)do  Play \(p_{t}=*{argmin}_{p_{K}} p,-_{s=1}^{t-1} f(u _{s}^{}p_{s})+(p)\).  Observe \(u_{t}\).  end for
```

**Algorithm 2** FTRL for \(N\)-agent \(K\)-armed SWF maximization with full-info feedback

It turns out that the log-barrier regularizer, \((p)=-_{i=1}^{K} p_{i}\), ensures such a property. In fact, it induces a small local norm not just for NSW, but also for a broad family of SWFs as long as they are concave and _Pareto optimal_ -- an SWF \(f:^{N}\) is Pareto optimal if for two utility vectors \(x\) and \(y\) such that \(x_{n} y_{n}\) for all \(i[N]\), we have \(f(x) f(y)\). NSW is clearly in this family, and there are many other standard fairness measures that fall into this class; see Appendix B.2.1. For any SWF in this family, we prove the following regret bound, which remarkably has no dependence on the number of agents \(N\) at all.

**Theorem 4.2**.: _For any \(f:^{N}\) that is concave and Pareto optimal, Algorithm 2 with the log-barrier regularizer \((p)=-_{i=1}^{K} p_{i}\) and \(=}\) guarantees \(*{Reg}_{}=()\)._

Proof Sketch.: Using the concrete form of \(\), it is clear that the local norm \(\| f(u_{t}^{}p_{t})\|_{^{-2}(p_{*})}^{2}\) simplifies to \(_{i=1}^{K}p_{t,i}^{2}[ f(u_{t}^{}p_{t})]_{i}^{2} p _{t}, f(u_{t}^{}p_{t})^{2}\), where the inequality is due to \([ f(u_{t}^{}p_{t})]_{i} 0\) implied by Pareto optimality. Furthermore, by concavity, we have \( p_{t}, f(u_{t}^{}p_{t}) f(u_{t}^{} p_{t})-f(0) 1\), and thus the local norm at most \(1\). The rest of the proof is by direct calculation. 

#### 4.2.2 FTRL with Tsallis Entropy Regularizer

In fact, when \(f=\) NSW, using the special structure of the welfare function, we find yet another regularizer that ensures a small \((N)\) local norm, with the benefit of having smaller dependence on \(K\) for the penalty term.

**Theorem 4.3**.: _For \(f=\) NSW, Algorithm 2 with the Tsallis entropy regularizer \((p)=^{K}p_{i}^{}}{1-}\), \(=\), and the optimal choice of \(\) guarantees \(*{Reg}_{}=}(K^{- })\)._

The proof is more involved and is deferred to Appendix B.2.3. While the regret in Theorem 4.3 suffers polynomial dependence on \(N\), it has better dependence on \(K\) compared to Theorem 4.2, and is thus more preferable when \(K\) is much larger than \(N\).

#### 4.2.3 Logarithmic Regret for a Special Case

Finally, we discuss a special case with \(f=\) NSW where logarithmic regret is possible. This is based on a simple observation that when there is one agent who is indifferent about the learner's choice (that is, the agent's utility is the same for all arms for this round), then \(-\)NSW is not only convex, but also exp-concave, a stronger curvature property. Therefore, by applying known results, specifically the EWOO algorithm [Hazan et al., 2007], we achieve the following result.

**Theorem 4.4**.: _Fix \(f=\) NSW. Suppose that for each time \(t\), there is a set of agents \(A_{t}[N]\) such that \(|A_{t}| M\) and \(u_{t,:,n}=c_{t,n}\) with \(c_{t,n} 0\) for each agent \(n A_{t}\). Then the EWOO algorithm guarantees \(*{Reg}_{}=( K T)\)._

The proof, which verifies the exp-concavity of \(-\)NSW in this special case, can be found in Appendix B.2.4. We note that the reason that we apply EWOO instead of Online Newton Step, another algorithm discussed in [Hazan et al., 2007] for exp-concave losses, is that the latter requires Lipschizness (which, again, is not satisfied by NSW).

Conclusion

In this work, motivated by recent research on social welfare maximization for the problem of multi-agent multi-armed bandits, we consider a variant with the arguably more natural version of Nash social welfare as the objective function, and develop multiple algorithms and regret upper/lower bounds in different settings (stochastic versus adversarial and full-information versus bandit feedback). Our results show a sharp separation between our problem and previous settings, including the heavily studied Bandit Convex Optimization problem.

There are many interesting future directions. First, in the stochastic bandit setting, we have only shown the tight dependence on \(T\), so what about \(K\) and \(N\)? Second, is there a more general strategy/analysis that works for different social welfare functions (similar to our result in Theorem 4.2)? Taking one step further, similar to the recent research on "omnipediction" [Gopalan et al., 2022], is there one single algorithm that works for a class of social welfare functions simultaneously?