# Not All Tokens Are What You Need for Pretraining

Zhenghao Lin\({}^{}^{}\)  Zhibin Gou\({}^{}\)  Yeyun Gong\({}^{}\)  Xiao Liu\({}^{}\)  Yelong Shen\({}^{}\)

Ruochen Xu\({}^{}\)  Chen Lin\({}^{}\)  Yujiu Yang\({}^{}\)  Jian Jiao\({}^{}\)  Nan Duan\({}^{}\)  Weizhu Chen\({}^{}\)

\({}^{}\)Xiamen University \({}^{}\)Tsinghua University \({}^{}\)Shanghai AI Laboratory

\({}^{}\)Microsoft

[https://aka.ms/rho](https://aka.ms/rho)

Equal contribution. See author contributions for details. Work done during their internships at Microsoft Research Asia. EUR2: zhenghaolin@stu.xmu.edu.cn; zebgou@gmail.com Correspondence authors.

###### Abstract

Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that _"Not all tokens in a corpus are equally important for language model training"_. Our initial analysis examines token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called Rho-1. Unlike traditional LMs that learn to predict every next token in a corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring tokens using a reference model, and then training the language model with a focused loss on tokens with higher scores. When continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks. After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively -- matching DeepSeeKMath with only 3% of the pretraining tokens. Furthermore, when continual pretraining on 80B general tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both data efficiency and performance of the language model pre-training.

Figure 1: We continual pretrain 1B and 7B LMs with 15B OpenWebMath tokens. Rho-1 is trained with our proposed Selective Language Modeling (SLM), while baselines are trained using causal language modeling. SLM improves average few-shot accuracy on GSM8k and MATH by over 16%, achieving the baseline performance 5-10x faster.

## 1 Introduction

Scaling up model parameters and dataset size has consistently elevated the next-token prediction accuracy in large language models, yielding significant advancements in artificial intelligence (Kaplan et al., 2020; Brown et al., 2020; OpenAI, 2023; Team et al., 2023). However, training on all available data is not always optimal or feasible. As a result, the practice of data filtering has become crucial, using various heuristics and classifiers (Brown et al., 2020; Wenzek et al., 2019) to select training documents. These techniques significantly improve data quality and boost model performance.

However, despite thorough document-level filtering, high-quality datasets still contain many noisy tokens that can negatively affect training, as illustrated in Figure 2 (Upper). Removing such tokens might alter the text's meaning, while overly strict filtering could exclude useful data (Welbl et al., 2021; Muennighoff et al., 2024) and lead to biases (Dodge et al., 2021; Longpre et al., 2023). Furthermore, research indicates that the distribution of web data does not inherently align with the ideal distribution for downstream applications (Tay et al., 2022; Wettig et al., 2023). For example, common corpus at the token level may include undesirable content like hallucinations or highly ambiguous tokens that are hard to predict. Applying the same loss to all tokens can lead to inefficient computation on non-essential tokens, potentially restricting LLMs from achieving more advanced levels of intelligence.

To explore how language models learn at the token level, we initially examined training dynamics, particularly how the token-level loss evolves during usual pretraining. In SS2.1, we evaluated the model's token perplexity at different checkpoints and categorized tokens into different types. Our findings reveal that significant loss reduction is limited to a select group of tokens. Many tokens are "easy tokens" that are already learned, and some are "hard tokens" that exhibit variable losses and resist convergence. These tokens can lead to numerous ineffective gradient updates.

Based on these analyses, we introduce Rho-1 models trained with a novel Selective Language Modeling (SLM) objective 3. As shown in Figure 2 (Right), this approach inputs the full sequence into the model and selectively removes the loss of undesired tokens. The detailed pipeline is depicted in Figure 4: First, SLM trains a reference language model on high-quality corpora. This model establishes utility metrics to score tokens according to the desired distribution, naturally filtering out unclean and irrelevant tokens. Second, SLM uses the reference model to score each token in a corpus using its loss (SS2.2). Finally, we train a language model only on those tokens that exhibit a high excess loss between the reference and the training model, selectively learning the tokens that best benefit downstream applications (SS2.2).

We show through comprehensive experiments that SLM significantly enhances token efficiency during training and improves performance on downstream tasks. Furthermore, our findings indicate that SLM effectively identifies tokens relevant to the target distribution, resulting in improved perplexity scores

Figure 2: **Upper:** Even an extensively filtered pretraining corpus contains token-level noise. **Left:** Previous Causal Language Modeling (CLM) trains on all tokens. **Right:** Our proposed Selective Language Modeling (SLM) selectively applies loss on those useful and clean tokens.

on benchmarks for models trained with the selected tokens. SS3.2 shows the effectiveness of SLM on math continual pretraining: both 1B and 7B Rho-1 outperform CLM-trained baselines by over 16% on the GSM8k and MATH datasets. SLM reaches baseline accuracy up to 10x faster, as shown in Figure 1. Remarkably, Rho-1-7B matches the state-of-the-art performance of DeepSeekMath-7B using only 15B tokens, compared to the 500B tokens required by DeepSeekMath. Upon fine-tuning, Rho-1-1B and 7B achieve 40.6% and 51.8% on MATH, respectively. Notably, Rho-1-1B is the first 1B LM to exceed 40% accuracy, nearing the early GPT-4's CoT performance of 42.5%. SS3.3 confirms the efficacy of SLM in general continual pretraining: Training Tinyllama-1B on 80B tokens with SLM improves 6.8% on average across 15 benchmarks, with gains over 10% in code and math tasks. In SS3.4, we demonstrate that in settings without high-quality reference data, we can use SLM for self-referencing, leading to an average improvement of up to 3.3% in downstream tasks.

## 2 Selective Language Modeling

### Not All Tokens Are Equal: Training Dynamics of Token Loss

Our investigation begins with a critical look at how individual tokens' losses evolve during standard pre-training. We continue pre-training Tinyllama-1B with 15B tokens from OpenWebMath, saving checkpoints after every 1B tokens. We then evaluate token-level loss at these intervals using the validation set of approximately 320,000 tokens. Figure 3(a) reveals a striking pattern: tokens fall into four categories based on their loss trajectory--persistent high loss (H\(\)H), increasing loss (L\(\)H), decreasing loss (H\(\)L), and consistent low loss (L\(\)L). For further details on these categories, see SSD.1. Our analysis uncovers that a mere 26% of tokens show a notable loss reduction (H\(\)L), while the majority (51%) remain in the L\(\)L category, indicating they have already been learned. Interestingly, 11% of the tokens are persistently challenging (H\(\)H), likely due to high aleatoric uncertainty . Additionally, 12% of tokens experience an unexpected loss increase (L\(\)H) during training.

Our second observation is that a significant number of token losses exhibit persistent fluctuations, and resist convergence. The loss of many L\(\)L and H\(\)H tokens, as depicted in Figure 3 (b) and (c), show high variance during training. In SSD.2, we visualize and analyze the content of these tokens and find that many of them are noisy, which is consistent with our hypothesis.

Consequently, we learn that the loss associated with each token during training does not decrease smoothly like the overall loss; instead, there is a complex training dynamic among different tokens. If we can select the appropriate tokens for the model to focus on during training, we may be able to stabilize the trajectory of the model's training and enhance its data efficiency.

### Selective Language Modeling

OverviewInspired by the practice of reference model in document-level filtering, we propose a simple pipeline of token-level data selection, termed "Selective Language Modeling (SLM)". Our method comprises three steps, as depicted in Figure 4. We begin by training a reference model on a curated, high-quality dataset. This model then assesses the loss of each token within the pretraining

Figure 3: **The loss of four categories of tokens during pretraining.** (a) shows the loss of H\(\)H, L\(\)H, H\(\)L, and L\(\)L tokens during pretraining. (b) and (c) show three cases of fluctuating tokens’ loss in L\(\)L and H\(\)H during pretraining, respectively.

corpus. In the final phase, we train the language model selectively, focusing on tokens with high excess loss between the training and reference model. The intuition is that tokens with high excess loss are more learnable and better aligned with the desired distribution, naturally excluding tokens that are either irrelevant or of low quality. Below, we provide a detailed description of each step.

Reference ModelingWe begin by curating a high-quality dataset that reflects the desired data distribution. We train a reference model (RM) using standard cross-entropy loss on the curated data. The resulting RM is then used to assess the token loss within a larger pretraining corpus. We compute the reference loss (\(_{}\)) of a token \(x_{i}\) based on the probability that the RM assigns to this token. The calculation is formalized as follows:

\[_{}(x_{i})=- P(x_{i}|x_{<i}) \]

By evaluating \(_{}\) for each token, we establish the reference loss for selective pretraining, allowing us to focus on the most influential tokens in language modeling.

Selective PretrainingNote that Causal Language Modeling (CLM) employs the cross-entropy loss:

\[_{}()=-_{i=1}^{N} P(x_{i}|x_{<i} ;) \]

Here, \(_{}()\) represents the loss function parameterized by model \(\). \(N\) is the length of the sequence, \(x_{i}\) is the \(i\)-th token in the sequence, and \(x_{<i}\) represents all tokens before the \(i\)-th token. In contrast, Selective Language Modeling (SLM) trains the language model with a focus on tokens that exhibit a high excess loss when compared to the reference model. The excess loss (\(_{}\)) for a token \(x_{i}\) is defined as the difference between the current training model loss (\(_{}\)) and the reference loss:

\[_{}(x_{i})=_{}(x_{i})-_{ }(x_{i}) \]

We introduce a token selection ratio \(k\%\), which determines the proportion of tokens to be included based on their excess loss. The cross-entropy loss for the selected tokens is computed as follows:

\[_{}()=-_{i=1}^{N}I_{k\%}(x_{i})  P(x_{i}|x_{<i};) \]

Here, \(N*k\%\) defines the number of tokens that fall within the top \(k\%\) of excess loss. The indicator function \(I_{k\%}(x_{i})\) is defined as:

\[I_{k\%}(x_{i})=1&$ ranks in the top $k\%$ by $S(x_{i})$}\\ 0& \]

Figure 4:

By default, we use \(_{}\) as the score function \(S\). This ensures that the loss is applied only to the tokens that are deemed most beneficial for the language model to learn from. In practice, token selection can be implemented by ranking the tokens in a batch according to their excess loss and using only the top \(k\%\) of tokens for training. This process eliminates the loss for undesired tokens without incurring additional costs during pretraining, making our approach both efficient and easily integrated.

## 3 Experiments

We continually pretrained models in both mathematical and general domain and designed ablation and analysis experiments to understand the effectiveness of SLM.

### Experimental Setup

Reference Model TrainingTo train our mathematical reference model, we gathered a dataset of 0.5B high-quality, math-related tokens. This dataset is a blend of synthetic data from GPT (Yu et al., 2024; Huang et al., 2024) and manually curated data (Yue et al., 2024; Ni et al., 2024). For the general reference model, we compiled a corpus of 1.9B tokens from open-source datasets, such as Tulu-v2 (Ivison et al., 2023) and OpenHermes-2.5 (Teknium, 2023). We trained the reference models for 3 epochs. The maximum learning rate was set at 5e-5 for 1B models and 1e-5 for 7B models, applying a cosine decay schedule. We set the maximum sequence lengths to 2048 for 1B models and 4096 for 7B models, packing multiple samples into these lengths for model input. In all main experiments, we initialized the continual pretraining model and the reference model with the _same_ base model.

    & |\)**Data**} &  **Uniq. Train** \\ **Tokes** \\  } &  **Train** \\ **Tokes** \\  } & ^{}\)**} &  &  &  **MMLU** \\ **STEM** \\  } & ^{}\)**} &  \\   & & & & & & & & & & & & & & \\   \\  Tinyllama & 1.1B & - & - & - & - & 2.9 & 3.2 & 11.0 & 18.1 & 20.4 & 12.5 & 14.6 & 16.1 & 21.9 & 13.4 \\ Phi-1.5 & 1.3B & - & - & - & 32.4 & 4.2 & 43.4 & 53.1 & 66.2 & 24.4 & 14.3 & 21.8 & 18.8 & 31.0 \\ Qwen1.5 & 1.8B & - & - & - & 36.1 & 6.8 & 48.5 & 63.6 & 79.0 & 29.2 & 25.1 & 31.3 & 40.6 & 40.0 \\ Gemma & 2.0B & - & - & - & 18.8 & 11.4 & 38.0 & 56.6 & 72.5 & 36.9 & 26.8 & 34.4 & 50.0 & 38.4 \\ DeepSeeKLM & 1.3B & OWM & 14B & 150B & 11.5 & 8.9 & - & - & - & - & - & 29.6 & 31.3 & - \\ DeepSeeKLM & 1.3B & - & 120B & 150B & 23.8 & 13.6 & - & - & - & - & - & 33.1 & 56.3 & - \\   \\  Tinyllama-CT & 1.1B OWM & 14B & 15B & 6.4 & 2.4 & 21.7 & 36.7 & 47.7 & 17.9 & 13.9 & 23.0 & 25.0 & 21.6 \\ Rho-1-Math & 1.1B OWM & 14B & 9B & 29.8 & 14.0 & 49.2 & 61.4 & 79.8 & 25.8 & 30.4 & 24.7 & 28.1 & 38.1 \\ \(\) & & -40\% & +23.4 & +11.6 & +27.5 & +24.7 & +32.1 & +7.9 & +16.5 & +1.7 & 43.1 & +**16.5** \\  Rho-1-Math & 1.1B OWM & 14B & 30B & 36.2 & 15.6 & 52.1 & 67.0 & 83.9 & 29.0 & 32.5 & 23.3 & 28.1 & 40.9 \\   \\  LLaMA-2 & 7B & - & - & 14.0 & 3.6 & 39.5 & 51.7 & 63.5 & 30.9 & 12.4 & 32.7 & 34.4 & 31.4 \\ Mistral & 7B & - & - & 41.2 & 11.6 & 64.7 & 68.5 & 87.5 & 52.9 & 33.0 & 49.5 & 59.4 & 52.0 \\ Minerva & 8B & - & 39B & 164B & 16.2 & 14.1 & - & - & - & - & - & 35.6 & - & - \\ Minerva & 62B & - & 39B & 109B & 52.4 & 27.6 & - & - & - & - & - & 53.9 & - & - \\ Minerva & 540B & - & 39B & 26B & 58.8 & 33.6 & - & - & - & - & 63.9 & - & - \\ LLemma & 7B & PPile & 55B & 200B & 38.8 & 17.2 & 56.1 & 69.1 & 82.4 & 48.7 & 41.0 & 45.4 & 59.4 & 50.9 \\ LLemma & 34B PPile & 55B & 50B & 54.2 & 23.0 & 67.9 & 75.7 & 90.1 & 57.0 & 49.8 & 54.7 & 68.8 & 60.1 \\ Interm-Math & 7B & - & 31B & 125B & 41.8 & 14.4 & 61.6 & 66.8 & 83.7 & 50.0 & 57.3 & 24.8 & 37.5 & 48.7 \\ Interm-Math & 20B & - & 31B & 125B & 65.4 & 30.0 & 75.7 & 79.3 & 94.0 & 50.9 & 38.5 & 53.1 & 71.9 & 62.1 \\ DeepSeeKLM & 7B & - & 120B & 500B & 64.1 & 34.2 & 74.0 & 83.9 & 92.4 & 63.4 & 62.4 & 56.4 & 84.4 & 68.4 \\   \\  Mistral-CT & 7B OWM & 14B & 15B & 42.9 & 22.2 & 68.6 & 71.0 & 86.1 & 45.1 & 47.7 & 52.6 & 65.6 & 55.8 \\ Rho-1-Math & 7B OWM & 14B & 10.5B & 66.9 & 31.0 & 77.8 & 79.0 & 93.9 & 49.9 & 58.7 & 54.6 & 84.4 & 66.2 \\ \(\) & & -30\% & +24.0 & +8.8 & +9.2 & +8.0 & +7.8 & +4.8 & +11.0 & +2.0 & +18.8 & +10.4 \\   

Table 1: **Few-shot CoT reasoning results of math pretraining.** All models are tested with few-shot prompting. Previous best results are highlighted in blue, while our best results are in purple. \({}^{*}\)Only unique math-related tokens are calculated. For Rho-1, we calculate only the selected tokens that are used for training. \({}^{}\)We use OpenAI’s MATH subset (Lightman et al., 2023) for evaluation, since some original test samples have been used in public training sets such as PRM800k. \({}^{}\)The SAT only has 32 four-choice problems, so we average our results over the last three checkpoints, if available.

Pretraining CorpusFor mathematical reasoning, we utilize the OpenWebMath (OWM) dataset , which comprises approximately 14B tokens sourced from math-related web pages in the Common Crawl. In the general domain, we combine the SlimPajama  and StarCoderData  (both part of the Tinyllama corpus) with OpenWebMath, training on a total of 80 billion tokens with a mix ratio of 6:3:1.

Pretraining SettingFor math pretraining, we continue pretraining on the Tinyllama-1.1B model  and the Mistral-7B model  with learning rates of 8e-5 and 2e-5, respectively. For the 1.1B model, we conducted our training on 32 x H100 80G GPUs. This configuration allowed us to train approximately 15 billion tokens in around 3.5 hours and 50 billion tokens in about 12 hours. In the case of the 7B model, training the same 15 billion tokens took approximately 18 hours under similar hardware conditions. For general domain, we set the learning rate for Tinyllama-1.1B model to 1e-4 and train 80B tokens under the same hardware conditions, which takes approximately 19 hours. The batch size is uniformly set to 1M tokens for both domains. Regarding the token selection ratio, we use 60% for the Tinyllama-1.1B model and 70% for the Mistral-7B model.

Baseline SettingWe use models that have been continually pretrained (Tinyllama-CT and Mistral-CT) through regular causal language modeling as baselines. Moreover, we compare Rho-1 with well-known and top-performing baselines, including Gemma , Owen1.5 , Phi-1.5 , DeepSeeKLLM , DeepSeeKMath , CodeLlama , Mistral , Minerva , Thyllama , LLemma , and InternLM2-Math . For fine-tuning results, we also compare with previous best models MAMmOTH and ToRA.

Evaluation SetupTo comprehensively evaluate pretrained models, we compare their few-shot capabilities and fine-tuning performance across a variety of tasks. We adopt the lm-eval-harness4 for general tasks, and develop math evaluation suite5 for math tasks. We use vlm (v0.3.2)  to speed up inference. Further details on our evaluation can be found in Appendix E.

### Math Pre-training Results

Few-shot CoT Reasoning ResultsWe evalute base models prompting with few-shot chain-of-thought (CoT)  examples following previous works . As results shown in Table 1, in comparison to continue pretraining directly, Rho-1-Math has achieved the average few-shot accuracy improvement of

  
**Model** & **Size** & **Tools** & **SFT Data** & **GSM8k** & **MATH** & **SVAMP** & **ASDiv** & **MAWPS** & **TAB** & **GSM-H** & **AVG** \\ 
**Used for SFT2** & & & & ✓ & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ &  & \\   \\  GPT4-0314 & - & ✗ & - & 92.0 & 42.5 & 93.1 & 91.3 & 97.6 & 67.1 & 64.7 & 78.3 \\ GPT4-0314 (PAL) & - & ✓ & - & 94.2 & 51.8 & 94.8 & 92.6 & 97.7 & 95.9 & 77.6 & 86.4 \\ MAMmOTH & 70B & ✓ & MI-260K & 76.9 & 41.8 & 82.4 & - & - & - & - & - & - \\ ToRA & 7B & ✓ & ToRA-69k & 68.8 & 40.1 & 68.2 & 73.9 & 88.8 & 42.4 & 54.6 & 62.4 \\ ToRA & 70B & ✓ & ToRA-69k & 84.3 & 49.7 & 82.7 & 86.8 & 93.8 & 74.0 & 67.2 & 76.9 \\ DeepSeeKMath & 7B & ✓ & ToRA-69k & 79.8 & 52.0 & 80.1 & 87.1 & 93.8 & 85.8 & 63.1 & 77.4 \\   \\  TinyLlama-CT & 1B & ✓ & ToRA-69k & 51.4 & 38.4 & 53.4 & 66.7 & 81.7 & 20.5 & 42.8 & 50.7 \\ Rho-1-Math & 1B & ✓ & ToRA-69k & 59.4 & 40.6 & 60.7 & 74.2 & 88.6 & 26.7 & 48.1 & 56.9 \\ \(\) & & & & +8.0 & +2.2 & +7.3 & +7.5 & +6.9 & +6.2 & +5.3 & +6.2 \\  Mistral-CT & 7B & ✓ & ToRA-69k & 77.5 & 48.4 & 76.9 & 83.8 & 93.4 & 67.5 & 60.4 & 72.6 \\ Rho-1-Math & 7B & ✓ & ToRA-69k & 81.3 & 51.8 & 80.8 & 85.5 & 94.5 & 70.1 & 63.1 & 75.3 \\ \(\) & & & & +3.8 & +3.4 & +3.9 & +1.7 & +1.1 & +2.6 & +2.7 & +2.7 \\   

Table 2: **Tool-integrated reasoning results of math pretraining.**16.5% on 1B models and 10.4% on 7B models. Furthermore, after training for multiple epochs on OpenWebMath, we find that Rho-1 could further increase the average few-shot accuracy to 40.9%. Compared to DeepSeeMath-7B, which pretrained on 500 billion math-related tokens, Rho-1-7B pretrained on only 15 billion tokens (selecting 10.5 billion tokens) achieved comparable results, demonstrating the efficiency of our approach.

Tool-Integrated Reasoning ResultsWe fine-tune Rho-1 and baseline models on 69k ToRA corpus , consisting of 16k GPT-4-generated trajectories in a tool-integrated reasoning format, and 53k answer-augmented samples using LLaMA. As presented in Table 2, Rho-1-1B and Rho-1-7B achieved a state-of-the-art 40.6% and 51.8% on MATH dataset, respectively. On some unseen tasks (_e.g.,_ TabMWP and GSM-Hard), Rho-1 also demonstrates a certain degree of generalizability, with an average few-shot accuracy improvement of 6.2% on the Rho-1-Math-1B and 2.7% on Rho-1-Math-7B.

### General Pre-training Results

We confirm the efficacy of the SLM in general pretraining by continual training Tinyllama-1.1B on 80 billion tokens. The results depicted in Figure 5 indicate that although Tinyllama has already undergone extensive training on the majority of these tokens, the application of SLM yields an average enhancement of 6.8% across 15 benchmarks compared to direct continual pretraining. The improvements were especially pronounced in code and math tasks, exceeding 10%.

### Self-Reference Results

In this section, we demonstrate that SLM can enhance the effectiveness of model pre-training using only pre-training corpora, without the need for additional high-quality data. Specifically, we initially trained the reference model on the OpenWebMath (OWM) corpus, a subset of Proof-Pile-2 (PPile). We evaluated OWM and PPile using the trained reference model and selected tokens for training. In this scenario, we assume the absence of downstream task-related data, a common situation in real-world applications. We hypothesize that the key factor is not scoring the desired distribution but filtering out noisy tokens. Therefore, we employed two different scoring functions based on the reference model loss, \(_{}\), and the information entropy of the next token, \(_{}\), which measures the uncertainty of the next token. Details are provided in Appendix H.

Figure 5: **General pretraining results.** We continual pretraining Tinyllama-1B on 80G general tokens. Tinyllama-CT is trained with CLM, while Rho-1 is trained with our proposed SLM.

The experimental results, as shown in Table 3, indicate that using only the OWM-trained reference model can effectively guide the model in pre-training on the same corpus, improving average downstream performance by +2.4%. Using only the information entropy as the score function brought about a similar improvement. Additionally, we considered training on the intersection of tokens selected by the two scoring functions and found better performance, with a 40% reduction in tokens and +3.3% performance. Furthermore, training the SLM on the PPile, despite only using the OWM subset to train the reference model, still achieved a 1.8% improvement with 30% fewer tokens used. For more details, please refer to Appendix H.

### Ablation Study and Analysis

Selected Token Loss Aligns Better with Downstream PerformanceWe utilized the reference model to filter tokens and assess their impact on validation and downstream losses after training. As depicted in Figure 6, we pretrained on 4B tokens and tracked loss variations across methods and validation sets. The Rho-1 showed greater loss reduction on selected tokens than regular pretraining. Cross-referencing figures (a), (b), and (c) reveals that selected-token pretraining substantially lowers downstream loss, while traditional pretraining's effect on downstream loss is less pronounced despite initial loss reductions. Therefore, we expect that selecting tokens for pretraining is more efficient.

In Figure 7, we demonstrate that the loss of selected tokens correlates with downstream task performance, following a power law similar to recent findings (Gadre et al., 2024). Our analysis shows that tokens selected by SLM positively impact performance, while those not selected have a negative impact. Thus, reducing loss across all tokens is not imperative for improved model performance. Refer to Appendix F for further details.

What Tokens are Selected with SLM?We aim to analyze the tokens selected by the SLM method in pretraining to further explore its working mechanism. To this end, we visualize the token selection process during the training of Rho-1 using the OpenWebMath. In SSG.1, we have highlighted in blue the tokens that were retained during actual pretraining. We observe that the majority of tokens chosen

    &  **Score** \\ **Function** \\  } &  **Data** \\ **Toks** \\  } &  **Uniq.** \\ **Toks** \\  } &  **Train** \\ **Toks** \\  } &  **Train** \\ **Toks** \\  } &  **Fain** \\ **Toks** \\  } &  **GSMSK** \\ **Match** \\  } &  **MATH** \\ **SVAMP** \\  } &  **ASDiv** \\ **AMWPS** \\  } &  **MAWPS** \\  } &  **MQA** \\ **AVG** \\  } &  **AVG** \\  } \\      & & & & & & & & & & & & & \\  TinyLama-CT (RM) & - & OWM & 14B & 15B & 6.3 & 2.6 & 21.7 & 36.7 & 47.7 & 13.9 & 21.5 \\ TinyLama-SLM & \(_{}\) & OWM & 14B & 10.5B & 6.7 & 4.6 & 23.3 & 40.0 & 54.5 & 14.3 & 23.9 \\ TinyLama-SLM & \(_{}\) & OWM & 14B & 10.5B & 7.0 & 4.8 & 23.0 & 39.3 & 50.5 & 13.5 & 23.0 \\ TinyLama-SLM & \(_{}_{}\) & OWM & 14B & 9B & 7.1 & 5.0 & 23.5 & 41.2 & 53.8 & 18.0 & 24.8 \\  TinyLama-CT & - & PPIie & 55B & 52B & 8.0 & 6.6 & 23.8 & 41.0 & 54.7 & 14.2 & 24.7 \\ TinyLama-SLM & \(_{}_{}\) & PPIie & 55B & 36B & 8.6 & 8.4 & 24.4 & 43.6 & 57.9 & 16.1 & 26.5 \\   

Table 3: **Self-Reference results. We use OpenWebMath (OWM) to train the reference model.**

Figure 6: **The dynamics of pretraining loss and downstream loss. (a) and (c) represent the loss of tokens selected/unselected by SLM during pretraining in both SLM and CLM methods, while (b) represents the loss of the SLM and CLM methods on MetaMath (Yu et al., 2024). We tested the above results through the process of pretraining with a total of 4 billion tokens.**

by the SLM method are closely related to mathematics, effectively training the model on the parts of the original corpus that are pertinent to mathematical content.

Furthermore, we investigated the differences in token filtering across various checkpoints during the training process and tested the perplexity of these tokens on different checkpoints. As illustrated in Figure 9, we found that the tokens selected by later checkpoints tend to have higher perplexity towards the later stages of training and lower perplexity in the earlier stages. This may suggest that the model first optimizes tokens with a larger learnable space, thereby increasing learning efficiency. Moreover, we noticed a sample-wise "double descent" [Nakkiran et al., 2021] on the loss of selected tokens, where the select token's perplexity initially increases before decreases. This might be an effect of selecting tokens based on excess loss, targeting those most in need at each checkpoint.

Effect of Token Select RatioWe investigate the impact of token selecting ratios of the SLM. Generally, the selecting ratio is defined by heuristic rules, similar to the approach previously employed in the training of Masked Language Models (MLMs) [Devlin et al., 2019, Liu et al., 2019]. As shown in Figure 9, the selected tokens is suitable for accounting for about 60% of the original tokens.

## 4 Conclusion

In this paper, we propose using Selective Language Modeling(SLM) to train Rho-1, which select more suitable tokens for current pretraining stage. We conducted the detailed analysis of the loss of tokens during the pretraining process and found that not all tokens are equal during pretraining. Our

Figure 8: **The PPL of tokens selected by different checkpoint. We test the PPL of the tokens selected at 2B, 5B, 8B, 11B, and 14B.** Figure 9: **Effect of token select ratio. We train 1B LM with SLM objective on 5B tokens.**

experiments and analysis in the fields of mathematics and general have demonstrated the effectiveness of the SLM method, emphasizing the importance of token level in the LLM pretraining process. In the future, how to improve pretraining of LLMs from the perspective of token level worthy of in-depth research.

#### Acknowledgments

Zhenghao Lin and Chen Lin were supported by National Key R&D Program of China (No. 2022ZD0160501), the Natural Science Foundation of China (No.62372390,62432011). Zhibin Gou and Yujiu Yang were supported by the Shenzhen Science and Technology Program (JCYJ20220818101001004) and the "Graph Neural Network Project" of Ping An Technology (Shenzhen) Co., Ltd.