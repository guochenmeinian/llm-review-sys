# R-divergence for Estimating Model-oriented Distribution Discrepancy

Zhilin Zhao  Longbing Cao

Data Science Lab, School of Computing & DataX Research Centre

Macquarie University, Sydney, NSW 2109, Australia

zhaozhl7@hotmail.com, longbing.cao@mq.edu.au

###### Abstract

Real-life data are often non-IID due to complex distributions and interactions, and the sensitivity to the distribution of samples can differ among learning models. Accordingly, a key question for any supervised or unsupervised model is whether the probability distributions of two given datasets can be considered identical. To address this question, we introduce R-divergence, designed to assess model-oriented distribution discrepancies. The core insight is that two distributions are likely identical if their optimal hypothesis yields the same expected risk for each distribution. To estimate the distribution discrepancy between two datasets, R-divergence learns a minimum hypothesis on the mixed data and then gauges the empirical risk difference between them. We evaluate the test power across various unsupervised and supervised tasks and find that R-divergence achieves state-of-the-art performance. To demonstrate the practicality of R-divergence, we employ R-divergence to train robust neural networks on samples with noisy labels.

## 1 Introduction

Most machine learning methods rely on the basic independent and identically distributed (IID) assumption , implying that variables are drawn independently from the same distribution. However, real-life data and machine learning models often deviate from this IID assumption due to various complexities, such as mixture distributions and interactions . For a given supervised or unsupervised task , a learning model derives an optimal hypothesis from a hypothesis space by optimizing the corresponding expected risk . However, this optimal hypothesis may not be valid if the distributions of training and test samples differ , resulting in distributional vulnerability . In fact, an increasing body of research addresses complex non-IID scenarios , including open-world problems , out-of-distribution detection , domain adaptation , and learning with noisy labels . These scenarios raise a crucial yet challenging question: _how to assess the discrepancy between two probability distributions for a specific learning model?_

Empirically, the discrepancy between two probability distributions can be assessed by the divergence between their respective datasets. However, the actual underlying distributions are often unknown, with only a limited number of samples available for observation. Whether the samples from two datasets come from the same distribution is contingent upon the specific learning model being used. This is because different learning models possess unique hypothesis spaces, loss functions, target functions, and optimization processes, which result in varying sensitivities to distribution discrepancies. For instance, in binary classification , samples from two datasets might be treated as positive and negative, respectively, indicating that they stem from different distributions. Conversely, a model designed to focus on invariant features is more likely to treat all samples as if they are drawn from the same distribution. For example, in one-class classification , all samples are considered positive, even though they may come from different components of a mixture distribution.

In cases where the samples from each dataset originate from multiple distributions, they can be viewed as drawn from a complex mixture distribution. We can then estimate the discrepancy between these two complex mixture distributions. This suggests that, in practice, it is both meaningful and necessary to evaluate the distribution discrepancy for a specific learning model, addressing the issue of _model-oriented distribution discrepancy evaluation_.

Estimating the discrepancy between two probability distributions has been a foundational and challenging issue. Various metrics for this task have been proposed, including F-divergence , integral probability metrics (IPM), and H-divergence. F-divergence metrics, like Kullback-Leibler (KL) divergence  and Jensen Shannon divergence , assume that two distributions are identical if they possess the same likelihood at every point. Importantly, F-divergence is not tied to a specific learning model, as it directly measures discrepancy by computing the statistical distance between the two distributions. IPM metrics, including the Wasserstein distance , maximum mean discrepancy (MMD), and L-divergence[5; 31], assess discrepancy based on function outputs. They postulate that any function should yield the same expectation under both distributions if they are indeed identical, and vice versa. In this vein, L-divergence explores the hypothesis space for a given model and regards the largest gap in expected risks between the two distributions as their discrepancy. This necessitates evaluating all hypotheses within the hypothesis space, a task which can be computationally demanding. Moreover, hypotheses unrelated to the two distributions could result in inaccurate or biased evaluations. On the other hand, H-divergence  contends that two distributions are distinct if the optimal decision loss is greater when computed on their mixed distribution compared to each individual one. It calculates this optimal decision loss in terms of training loss, meaning it trains a minimal hypothesis and evaluates its empirical risk on the identical dataset. A network with limited training data is susceptible to overfitting , which can lead to underestimated discrepancies when using H-divergence, even if the two distributions are significantly different.

To effectively gauge the model-oriented discrepancy between two probability distributions, we introduce R-divergence. This novel metric is built on the notion that _two distributions are identical if the optimal hypothesis for their mixture distribution yields the same expected risk on each individual distribution_. In line with this concept, R-divergence evaluates the discrepancy by employing an empirical estimator tailored to the given datasets and learning model. Initially, a minimum hypothesis is derived from the mixed data of the two datasets to approximate the optimal hypothesis. Subsequently, empirical risks are computed by applying this minimum hypothesis to each of the two individual datasets. Finally, the difference in these empirical risks serves as the empirical estimator for the discrepancy between the two probability distributions. The framework of R-divergence is illustrated in Figure 1.

## 2 Model-oriented Two-sample Test

We measure the discrepancy between two probability distributions \(p\) and \(q\) for a specific supervised or unsupervised learning method. Let \(\) and \(\) be the spaces of inputs and labels, respectively. Assume we observe two sets of \(N\) IID samples from the two distributions, i.e., \(=\{x_{i}\}_{i=1}^{N} p\), and \(=\{x_{i}^{}\}_{i=1}^{N} q\), respectively. We assume the mixture distribution \(u=(p+q)/2\) and its corresponding mixed data \(=\).

For a learning model \(\), we assume \(\) is its hypothesis space and \(l\) is its \(L\)-Lipschitz bounded loss function, i.e., \(|l(,)| c\). For an input \(x\), we suppose \(\|x\| B\) and \(a\) is its corresponding target function for the model. For example, \(a(x)=y\) for supervised classification and \(a(x)=x\) for unsupervised input reconstruction. In general, a loss function can be defined as \(l(h(x),a(x))\) for any hypothesis \(h\) and input \(x\). \(h(x)\) represents the hypothesis output for \(x\). For example, \(h(x)\) produces a predicted label by a classifier or the reconstructed input by unsupervised input reconstruction. For the mixture distribution \(u\) and its corresponding dataset \(\), we define the expected risk \(_{u}(h)\) and empirical risk \(_{}(h)\) for any \(h\):

\[_{u}(h)=_{x u}l(h(x),a(x)),_{ }(h)=_{x}l(h(x),a(x)). \]

Then, we define the optimal hypothesis \(h_{u}^{*}\) and A learning model of the task aims to learn the minimum hypothesis \(\) on observed data to approximate the optimal hypothesis \(h^{*}\). This aim can be converted to the _model-oriented two-sample test_: For datasets \(\) and \(\) and their learning task with hypothesis space \(\) and loss function \(l\), can we determine whether their probability distributions are identical (e.g., \(p=q\)) for the specific model? This research question is equivalent to: whether the samples of the two datasets can be treated as sampled from the same distribution for the model?

## 3 R-divergence for Distribution Discrepancy

To fulfill the model-oriented two-sample test, we develop R-divergence to estimate the discrepancy between two probability distributions \(p\) and \(q\) for a model \(\). R-divergence regards two distributions as identical if the optimal hypothesis on their mixture distribution has the same expected risk on each individual distribution. Accordingly, with the corresponding datasets \(\) and \(\), R-divergence estimates the optimal hypothesis \(h^{*}_{u}\) on the mixture distribution \(u\) by learning the minimum hypothesis \(_{}\) on the mixed dataset \(\). Then, R-divergence estimates the expected risks \(_{p}(h^{*}_{u})\) and \(_{q}(h^{*}_{u})\) on the two distributions by evaluating the empirical risks \(_{}(_{})\) and \(_{}(_{})\) on their data samples. R-divergence measures the empirical risk difference between \(_{}(_{})\) and \(_{}(_{})\) to estimate the distribution discrepancy.

Accordingly, for a supervised or unsupervised model with hypothesis space \(\) and loss function \(l\), R-divergence evaluates the discrepancy between distributions \(p\) and \(q\) by,

\[_{}(p\|q)=d(_{p}(h^{*}_{u}),_{q}(h^{*}_{u}) )=|_{p}(h^{*}_{u})-_{q}(h^{*}_{u})|. \]

\(d(,)\) represents the absolute difference. Intuitively, if \(p=q\), then \(u=p=q\) and \(h^{*}_{u}\) is the optimal hypothesis for both \(p\) and \(q\). Thus, the expected risks on the two distributions are the same, which leads to a zero discrepancy. In contrast, if \(p q\), \(h^{*}_{u}\) is the optimal hypothesis for \(u\), and it performs differently on \(p\) and \(q\). Then, the expected risk difference can evaluate the discrepancy between the two distributions for the specific model. Different from F-divergence without considering specific learning models, R-divergence involves the property of the specific model in evaluating the model-oriented distribution discrepancy conditional on the optimal hypothesis. Specifically, R-divergence infers the discrepancy by comparing the distance between the learning results on two distributions. The learning result is sensitive to the learning model, i.e., its hypothesis space and loss function.

Further, \(_{}(p\|q)\) can be estimated on datasets \(\) and \(\). R-divergence estimates the discrepancy by searching a minimum hypothesis \(_{u}\) on the mixed dataset \(=\), and calculates the empirical risks \(_{}(_{u})\) and \(_{}(_{u})\) on each individual dataset. Then, \(_{}(p\|q)\) can be estimated by the empirical risk difference \(}_{}(p\|q)\):

\[}_{}(\|)=d(_{}(_{}),_{}( _{}))=|_{}(_{ })-_{}(_{})|. \]

\(}_{}(\|)\) can be treated as an empirical estimator for the distribution discrepancy \(_{}(p\|q)\).

[MISSING_PAGE_FAIL:4]

[MISSING_PAGE_FAIL:5]

### Evaluation Metrics

The discrepancy between two probability distributions for a learning model is estimated on their given datasets IID drawn from the distributions, respectively. We decide whether two sets of samples are drawn from the same distribution. A model estimates the discrepancy on their corresponding datasets \(=\{x_{i}\}_{i=1}^{N} p\) and \(=\{x^{}_{i}\}_{i=1}^{N} q\). \(}_{}(\|)\) serves as an empirical estimator for the discrepancy \(_{}(p\|q)\) to decide whether \(p q\) if its output value exceeds a predefined threshold.

For an algorithm that decides if \(p q\), the _Type I error_ occurs when it incorrectly decides \(p q\), and the corresponding probability of Type I error is named the _significant level_. In contrast, the _Type II error_ occurs when it incorrectly decides \(p=q\), and the corresponding probability of not making this error is called the _test power_. In the experiments, the samples of two given datasets \(\) and \(\) are drawn from different distributions, i.e., the ground truth is \(p q\). Both the significant level and the test power are sensitive to the two probability distributions. Accordingly, to measure the performance of a learning algorithm, we calculate its test power when a certain significant level \(\) can be guaranteed. A higher test power indicates better performance when \(p q\).

The significant level can be guaranteed with a permutation test . For the given datasets \(\) and \(\), the permutation test uniformly randomly swaps samples between \(\) and \(\) to generate \(Z\) dataset pairs \(\{(^{*},^{*})\}_{z=1}^{Z}\). For \(\) and \(\), R-Div outputs the empirical estimator \(}_{}(\|)\). Note that if \(p=q\), swapping samples between \(\) and \(\) cannot change the distribution, which indicates that the permutation test can guarantee a certain significant level. If the samples of \(\) and \(\) are drawn from different distributions, the test power is \(1\) if the algorithm can output \(p q\) Otherwise, the test power is \(0\). Specifically, to guarantee the \(\) significant level, the algorithm output \(p q\) if \(}_{}(\|)\) is in the top \(\)-quantile among \(=\{}_{}(^{*}\|^{ *})\}_{z=1}^{Z}\). We perform the test for \(K\) times using different randomly-selected dataset pairs \((,)\) and average the output values as the test power of the algorithm. If not specified, we set \(=0.05\), \(Z=100\) and \(K=100\). The procedure of calculating the average test power of R-Div is summarized in Algorithm 2 (see Appendix B.2).

### Unsupervised Learning Tasks

**Benchmark Datasets:** Following the experimental setups as  and , we adopt four benchmark datasets, namely Blob, HDGM, HIGGS, and MNIST. We consider learning unsupervised models on these datasets, and the considered hypothesis spaces are for generative models. Specifically, MNIST adopts the variational autoencoders (VAE) , while the other low dimensional data adopt kernel density estimation (KDE) . For fair comparisons, we follow the same setup as  for all methods. Because all the compared methods have hyper-parameters, we evenly split each dataset into a training set and a validation set to tune hyper-parameters and compute the final test power, respectively. The average test powers for a certain significant level on HDGM , Blob , MNIST  and HIGGS  are presented in Figure 3, Figure 4, Table 8 (see Appendix D.1) and Table 2, respectively. Overall, our proposed R-Div achieves the best test power on all datasets. On HDGM, R-Div obtains the best performance for different dimensions and achieves the perfect test power with fewer samples. Specifically, the test power of R-Div decreases gracefully as the dimension increases on HDGM. On Blob with different significant levels, R-Div achieves the same test power as the second-best method with a lower variance. On MNIST, both H-Div and R-Div achieve the perfect test power on all the sample sizes. On HIGGS, R-Div achieves the perfect test power even on the dataset with the smallest scale.

The discrepancy as measured by R-Div is distinct for two different distributions, even with a small sample size. This advantage stems from mitigating overfitting of H-Div by learning the minimum hypothesis and evaluating its empirical risks across varying datasets. R-Div learns a minimum hypothesis based on the mixed data and yields consistent empirical risks for both datasets when samples are sourced from the same distribution. Conversely, if samples from the two datasets come from different distributions, the empirical risks differ. This is because the training dataset for the minimum hypothesis does not precisely match either of the individual datasets.

To better demonstrate the advantages of R-Div, we measure the amount of overfitting by calculating empirical risks on \(p\) and \(q\). The results presented in Table 1 indicate that R-Div achieves a notably clearer discrepancy, attributable to the significant differences in empirical risks on \(p\) and \(q\). This is because R-Div mitigates overfitting by optimizing a minimum hypothesis on a mixed datasetand assessing its empirical risks across diverse datasets. Consequently, R-Div can yield a more pronounced discrepancy for two markedly different distributions.

### Supervised Learning Tasks

**Multi-domain Datasets:** We adopt the PACS dataset  consisting of samples from four different domains, including photo (P), art painting (A), cartoon (C), and sketch (S). Following  and , we use AlexNet  as the backbone. Accordingly, samples from different domains can be treated as samples from different distributions. We consider the combinations of two domains and estimate their discrepancies. The results in Figure 5 (see Appendix D.2) represent that the test power of R-Div significantly improves as the number of samples increases. In addition, R-Div obtains the best performance on different data scales. The results in Table 3 show that R-Div achieves the superior test power across all the combinations of two domains. This is because the compared H-Div method suffers from the overfitting issue because it searches for a minimum hypothesis and evaluates its empirical risk on the same dataset. This causes empirical risks on different datasets to be minimal, even if they are significantly different. R-Div improves the performance of estimating the discrepancy by addressing this overfitting issue. Specifically, given two datasets, R-Div searches a minimum hypothesis on their mixed data and evaluates its empirical risks on each individual dataset. R-Div obtains stable empirical risks because the datasets used to search and evaluate the minimum hypothesis are not the same and partially overlap.

**Spurious Correlations Datasets:** We evaluate the probability distribution discrepancy between MNIST and Colored-MNIST (with spurious correlation) . The adopted model for the two datasets is fully-connected and has two hidden layers of 128 ReLU units, and the parameters are learned using Adam . The results in Table 4 reveal that the proposed R-Div achieves the best performance, which indicates that R-Div can explore the spurious correlation features by learning a minimum hypothesis on the mixed datasets.

**Corrupted Datasets:** We assess the probability distribution discrepancy between a given dataset and its corrupted variants. Adhering to the construction methods outlined by Sun et al. and the framework of H-Div, we conduct experiments using H-Div and R-Div. We examine partial corruption types across four corruption levels on both CIFAR10  and ImageNet , employing ResNet18  as the backbone. Assessing discrepancies in datasets with subtle corruptions is particularly challenging due to the minor variations in their corresponding probability distributions. Therefore, we employ large sample sizes, \(N=5000\) for CIFAR10 and \(N=1000\) for ImageNet,

   Methods & Hypothesis & \(_{p}(h_{p})\) & \(_{q}(h_{q})\) & \(|_{p}(h_{p})-_{q}(h_{q})|\) \\  L-Div & \(h_{p}=h_{q}\) & 713.0658 & 713.1922 & 0.1264 \\  H-Div & \(h_{p}*{arg\,min}_{h}_{p}(h)\) & 111.7207 & 118.9020 & 7.1813 \\  R-Div & \(h_{p}=h_{q}*{arg\,min}_{h}_{}(h)\) & 144.0869 & 99.6166 & **44.4703** \\   

Table 1: The empirical risks on \(p\) and \(q\) for L-Div, H-Div, and R-Div on MNIST when \(N=200\).

   N & 1000 & 2000 & 3000 & 5000 & 8000 & 10000 & Avg. \\  ME & 0.120\(\)0.007 & 0.165\(\)0.019 & 0.197\(\)0.012 & 0.410\(\)0.041 & 0.691\(\)0.691 & 0.786\(\)0.041 & 0.395 \\ SCF & 0.095\(\)0.022 & 0.130\(\)0.026 & 0.142\(\)0.025 & 0.261\(\)0.044 & 0.4and observe that methods yield a test power of \(0\) when the corruption level is mild. Consistently across different corruption types, R-Div demonstrates superior performance at higher corruption levels and outperforms H-Div across all types and levels of corruption. Moreover, our results on ImageNet substantiate that the proposed R-Div effectively estimates distribution discrepancies in large-resolution datasets.

**Advanced Network Architectures:** For supervised learning tasks, R-Div selects the network architecture for the specific model. To verify the effect of R-div, we perform experiments on a wide range of architectures. We compare R-div with H-div because both of them are model-oriented and the other comparison methods have specific network architectures. Specifically, we consider the classification task on CIFAR10 and Corrupted-CIFAR10 and adopt four advanced network architectures, including ResNet18 , SENet , ConvNeXt , ViT . The results are summarized in Table 6. For large-scale datasets (N = 10000), both H-Div and R-Div achieve perfect test power with different network architectures because more samples are applied to estimate the probability distribution discrepancy precisely. However, R-Div has the better test power for more challenging scenarios where the datasets are small (N = 1000, 5000). This may be because R-Div addresses the overfitting issue of H-Div by optimizing the minimum hypothesis and evaluating its empirical risk on different datasets. Furthermore, for different network architectures, CNN-based networks achieve better performance than the transformer-based models (ViT). According to the analysis of Park and Kim  and Li et al. , this may be because multi-head attentions (MHA) are low-pass filters with a shape bias, while convolutions are high-pass filters with a texture bias. As a result, ViT more likely focuses on the invariant shapes between CIFAR10 and the corrupted-CIFAR10 to make the two datasets less discriminative.

### Case Study: Learning with Noisy Labels

R-Div is a useful tool to quantify the discrepancy between two probability distributions. We demonstrate how it can learn robust networks on data with noisy labels by distinguishing samples with noisy labels from clean samples in a batch. Furthermore, we explain how the discrepancy between clean and noisy samples affects the classification generalization ability .

#### 5.4.1 Setup

We adopt the benchmark dataset CIFAR10 . As CIFAR10 contains clean samples, following the setup , we corrupt its training samples manually, i.e., flipping partial clean labels to noisy labels. Specifically, we consider symmetry flipping  and pair flipping . For corrupting a label, symmetry flipping transfers it to another random label, and pair flipping transfers it to a specific label. Following the setup , we adopt \(0.2\) noise rate, i.e., the labels of \(20\%\) samples will be flipped. The classification accuracy is evaluated on clean test samples.

We apply R-Div to train a robust DNN on a corrupted dataset. The training strategy separates clean and noisy samples by maximizing the discrepancy estimated by R-Div. The ground truth labels

   ME & SCF & C2STS-S & C2ST-L & MMD-O & MMD-D & H-Div & R-Div \\ 
0.383 & 0.204 & 0.189 & 0.217 & 0.311 & 0.631 & 0.951 & **1.000** \\   

Table 4: The average test power on MNIST and Colored-MNIST at the significant level \(=0.05\).

   Datasets & A+C & A+P & A+S & C+P & C+S & P+S & Ave. \\  H-Div & 0.846\(\)0.118 & 0.668\(\)0.064 & 0.964\(\)0.066 & 0.448\(\)0.135 & 0.482\(\)0.147 & 0.876\(\)0.090 & 0.714 \\ R-Div & **1.000\(\)0.000** & **1.000\(\)0.000** & **1.000\(\)0.000** & **0.527\(\)0.097** & **0.589\(\)0.160** & **0.980\(\)0.040** & **0.849** \\   

Table 3: The average test power \(\) standard error at the significant level \(=0.05\) on different domain combinations of PACS. P, A, C and S represent photo, art painting, cartoon, and sketch domains, respectively. The distribution discrepancies for six combinations of two domains are estimated. For example, A + C represents the combination of art painting and cartoon domains. Boldface values represent the relatively better discrepancy estimation.

for noisy samples are unreliable. Therefore, these labels can be treated as complementary labels representing the classes that the samples do not belong to. Accordingly, the training strategy optimizes the losses of ground truth and complementary labels for clean and noisy samples, respectively.

We assume that clean and noisy samples satisfy different distributions, i.e., clean and noisy distributions. Specifically, we apply R-Div to detect clean and noisy samples from a batch. Accordingly, we separate the batch samples \(\) into predicted clean samples \(_{c}\) and predicted noisy samples \(_{n}\) by maximizing R-Div. The batch and the whole dataset contain IID samples from an unknown mixture distribution about clean and noisy distributions. Both the minimum hypotheses on the batch and the whole dataset can be treated to approximate the optimal hypothesis of the mixture distribution. To prevent learning a minimum hypothesis for each given batch, we pretrain a network on the whole corrupted dataset rather than a batch and apply this pretrained network (minimum hypothesis) to separate clean and noisy samples in a batch.

Recall that R-Div calculates the empirical risk difference between two datasets under the same minimum hypothesis of their mixed dataset. Hence, to maximize the discrepancy of R-Div, we calculate the empirical risk of each sample in the batch by the pretrained network, sort the batch samples according to the empirical risks, and treat the samples with low and high empirical risks as predicted clean and noisy samples, respectively. Specifically, we separate noisy and clean samples according to the estimated noise rate \(\). For clean samples with trusted ground truth labels, we optimize a network by minimizing the traditional cross-entropy loss. In contrast, the labels for noisy samples are untrusted. However, their noisy labels indicate the classes they do not belong to. Accordingly, we can treat their untrusted ground truth labels as complementary labels and optimize a network by minimizing the negative traditional cross-entropy loss on noisy samples. Thus, we retrain a robust network by optimizing the objective function \(_{h}_{_{c}}(h)-_{_{n}}(h)\) for each batch, where the loss function \(l\) is the traditional cross-entropy loss. In this work, we adopt ResNet18  as the backbone. In the pretraining and retraining processes, the learning rate  starts at 0.1 and is divided by 10 after 100 and 150 epochs. The batch size is \(128\), and the number of epochs is \(200\). The hyper-parameter \(\) varies from \(0.1\) to \(0.7\) with a \(0.1\) step size.

    &  &  & ConvNeXt & ViT \\   &  \\ 
1000 & 0.116 / **0.252** & 0.201 / **0.296** & 0.186 / **0.322** & 0.119 / **0.211** \\
5000 & 0.622 / **0.748** & 0.478 / **0.790** & 0.778 / **0.844** & 0.492 / **0.651** \\
10000 & **1.000 / 1.000** & **1.000 / 1.000** & **

#### 5.4.2 Experimental Results

We evaluate the classification accuracy on clean test samples, the precision and recall rates of detecting clean and noisy samples, and the discrepancy between predicted clean and noisy samples. The results are reported in Figure 2 and Figure 6 (see Appendix D.3). Recall that the noise rate is \(0.2\). Figure 2(a) and Figure 6(a) show that the network retrained with R-Div improves the classification performance over the pretrained network when the estimated noise rate \([0.2,0.5]\). The retrained network achieves the best performance when the estimated noise rate is nearly twice as much as the real noise rate. To the best of our knowledge, how to best estimate the noise rate remains an open problem and severely limits the practical application of the existing algorithms. However, our method merely requires an approximate estimation which can be twice as much as the real noise rate.

Further, Figure 2(b) and Figure 6(b) show that there is a point of intersection between the precision and recall curves at \(=0.5\). The aforementioned results indicate that \( 0.5\) can lead to improved performance. Accordingly, we know that the detection precision is more essential than its recall rate. Specifically, a larger estimated noise rate (\( 0.5\)) indicates that more clean samples are incorrectly recognized as noisy samples. In contrast, an extremely lower estimated noise rate which is lower than the real one indicates that more noisy samples are incorrectly recognized as clean samples. Both situations can mislead the network in learning to classify clean samples.

In addition, Figure 2(c) and Figure 6(c) show that a larger \(\) leads to a smaller discrepancy. Because the majority of training samples are clean, most of the clean samples have small test losses. A larger \(\) causes more clean samples to be incorrectly recognized as noisy samples, which decreases the test loss of the set of predicted noisy samples. Thus, the empirical estimator will decrease. Based on the results reported in Figure 2(a) and Figure 6(a), the largest discrepancy cannot lead to improved classification performance because most of the noisy samples are incorrectly recognized as clean samples. In addition, we observe that the retrained network obtains improved classification performance when the estimated discrepancy approximates the mean value \(1.5 10^{-3}\), which can be used as a standard to select an estimated noise rate \(\) for retraining a robust network.

## 6 Conclusions

We tackle the crucial question of whether two probability distributions are identically distributed for a specific learning model using a novel and effective metric, R-divergence. Unlike existing divergence measures, R-divergence identifies a minimum hypothesis from a dataset that combines the two given datasets, and then uses this to compute the empirical risk difference between the individual datasets. Theoretical analysis confirms that R-divergence, serving as an empirical estimator, uniformly converges and is bounded by the discrepancies between the two distributions. Empirical evaluations reveal that R-divergence outperforms existing methods across both supervised and unsupervised learning paradigms. Its efficacy in handling learning with noisy labels further underscores its utility in training robust networks. If the samples from the two datasets are deemed identically distributed, a subsequent question to be investigated is whether these samples are also dependent.

Figure 2: Results on CIFAR10 with symmetry flipping. All values are averaged over five trials. **Left:** Classification accuracy of pretrained and retrained networks. **Middle:** Precision and recall rates of detecting clean and noisy samples. **Right:** Discrepancy between predicted clean and noisy samples.