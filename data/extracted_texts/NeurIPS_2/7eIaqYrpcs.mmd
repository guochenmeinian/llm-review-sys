# Vidu4D: Single Generated Video to High-Fidelity 4D Reconstruction with Dynamic Gaussian Surfels

Yikai Wang\({}^{*1}\), Xinzhou Wang\({}^{*1,2,3}\), Zilong Chen\({}^{1,2}\), Zhengyi Wang\({}^{1,2}\), Fuchun Sun\({}^{1}\), Jun Zhu\({}^{11,2}\)

\({}^{1}\)Department of Computer Science and Technology, BNRist Center, Tsinghua University

\({}^{2}\)ShengShu \({}^{3}\)College of Electronic and Information Engineering, Tongji University

yikaiw@outlook.com, wangxinzhou@tongji.edu.cn, dcszj@tsinghua.edu.cn

Equal contribution. \({}^{}\)The corresponding author.

###### Abstract

Video generative models are receiving particular attention given their ability to generate realistic and imaginative frames. Besides, these models are also observed to exhibit strong 3D consistency, significantly enhancing their potential to act as world simulators. In this work, we present Vidu4D, a novel reconstruction model that excels in accurately reconstructing 4D (_i.e._, sequential 3D) representations from single generated videos, addressing challenges associated with non-rigidity and frame distortion. This capability is pivotal for creating high-fidelity virtual contents that maintain both spatial and temporal coherence. At the core of Vidu4D is our proposed _Dynamic Gaussian Surfels_ (DGS) technique. DGS optimizes time-varying warping functions to transform Gaussian surfels (surface elements) from a static state to a dynamically warped state. This transformation enables a precise depiction of motion and deformation over time. To preserve the structural integrity of surface-aligned Gaussian surfels, we design the warped-state geometric regularization based on continuous warping fields for estimating normals. Additionally, we learn refinements on rotation and scaling parameters of Gaussian surfels, which greatly alleviates texture flickering during the warping process and enhances the capture of fine-grained appearance details. Vidu4D also contains a novel initialization state that provides a proper start for the warping fields in DGS. Equipping Vidu4D with an existing video generative model, the overall framework demonstrates high-fidelity text-to-4D generation in both appearance and geometry. Project page: [https://vidu4d-dgs.github.io](https://vidu4d-dgs.github.io).

## 1 Introduction

The field of multimodal generation exhibits significant advancements and holds great promise for various applications. Recently, video generative models have garnered attention for their remarkable capability to craft immersive and lifelike frames . These models produce visually stunning content while also exhibiting strong 3D consistency , largely increasing their potential to simulate realistic environments.

Parallel to these developments, high-quality 4D reconstruction has made great strides . This technique involves capturing and rendering detailed spatial and temporal information. When integrated with generative video technologies, 4D reconstruction potentially enables the creation of models that capture static scenes and dynamic sequences over time. This synthesis provides a more holistic representation of reality, which is crucial for applications such as virtual reality, scientific visualization, and embodied artificial intelligence.

## 1 Introduction

Figure 1: Text-(to-video)-to-4D samples generated by equipping Vidu4D with a pretrained video diffusion model . For each sample, we exhibit per-frame 3D rendering for novel-view color, normal, and surfel feature. We observe that Vidu4D can reconstruct precisely detailed and photo-realistic 4D representation. See our accompanying videos in our project page for better visual quality.

However, achieving high-fidelity 4D reconstruction from generated videos poses great challenges. Non-rigidity and frame distortion are prevalent issues that can undermine the temporal and spatial coherence of the reconstructed content, thus complicating the creation of a seamless and coherent depiction of dynamic subjects.

In this work, we introduce Vidu4D, a novel reconstruction pipeline designed to accurately reconstruct 4D representations from single generated videos, facilitating the creation of 4D content with high precision in spatial and temporal coherence. Vidu4D contains two novel stages, namely, the initialization of non-rigid warping fields and Dynamic Gaussian Surfels (DGS), together enabling the reconstruction of high-fidelity 4D content with detailed appearance and accurate geometry.

Specifically, the proposed DGS optimizes non-rigid warping functions that transform Gaussian surfels from static to dynamically warped states. This dynamic transformation accurately represents motion and deformation over time, crucial for capturing realistic 4D representations. Besides, DGS demonstrates superior 4D reconstruction performance due to two other key aspects. Firstly, in terms of geometry, DGS adheres to Gaussian surfels principles  to achieve precise geometric representation. Unlike existing methods, DGS incorporates warped-state normal consistency regularization to align surfels with actual surfaces with learnable continuous fields (_w.r.t._ spatial coordinate and time) to ensure smooth warping when estimating normals. Secondly, for appearance, DGS learns additional refinements on the rotation and scaling parameters of Gaussian surfels by a dual branch structure. This refinement reduces the flickering artifacts during warping and allows for the precise rendering of appearance details, resulting in high-quality reconstructed 4D representations.

By integrating Vidu4D with an existing powerful video generative model named Vidu , the overall framework demonstrates exceptional capabilities in text-to-4D generation. We provide 4D visualization results in Fig. 1. Extensive experiments based on the generated videos verify the effectiveness of our method compared to current state-of-the-art methods.

## 2 Related works

**3D representation.** Transforming 2D images into 3D representations has long been a central challenge in the field. Initially, triangle meshes were favored for their compactness and compatibility with rendering pipelines . However, the transition to more sophisticated volumetric methods was inevitable due to the limitations of surface-based approaches. Early volumetric representations included voxel grids  and multi-plane images , which, despite their straightforwardness, demanded intricate optimization strategies. The introduction of neural radiance fields (NeRF)  marked a great advancement, offering an implicit volumetric neural representation that could store and query the density and color of each point, leading to highly realistic reconstructions. The NeRF paradigm has since been improved upon in terms of reconstruction quality  and rendering . To address the limitations of NeRF, such as rendering speed and memory usage, recent work dubbed 3D Gaussian splatting (3DGS)  has proposed anisotropic Gaussian representations with GPU-optimized tile-based rasterization. This has opened up new avenues for surface extraction , generation , and large-scale scene reconstruction , with 3DGS emerging as a universal representation for 3D scenes and objects. Gaussian surfels methods  further exhibit advantages in modeling accurate geometry. While these methods have significantly advanced the field of static 3D representation, capturing the dynamic aspects of real-world scenes with non-rigid motion and deformation introduces a distinct set of challenges that demand innovative solutions.

**Dynamic reconstruction and generation.** The dynamic reconstruction of scenes from video captures presents a more complex challenge than static reconstruction, necessitating the capture of non-rigid motion and deformation over time . Traditional methods have explored dynamic reconstruction using synchronized multi-view videos  or have focused on specific dynamic elements like humans or animals. More recently, there has been a shift towards reconstructing non-rigid objects from monocular videos, which is a more practical yet challenging scenario. One approach involves incorporating time as an additional input to the neural radiance field , allowing for explicit querying of spatiotemporal information. Another line of research decomposes the spatiotemporal radiance field into a canonical space and a deformation field, representing spatial attributes and their temporal variations . With advancements in 3DGS, deformable-GS and 4DGS  have been developed, utilizing neural deformation fields with multi-layer perception (MLP) and triplane, respectively. SCGS  and dynamic 3D Gaussians  also advance the field by modeling time-varying scenes. Building on these advances, our work introduces dynamic Gaussian surfels, a novel extension of Gaussian representations that enhances the quality of both appearance and surface reconstruction under dynamic scenarios. A concurrent work DGM  builds time-consistent meshes from a monocular video with 3D Gaussian Splitting. In the realm of 3D or 4D generation, our approach diverges from recent progress in optimization-based [2; 13; 14; 40; 42; 62; 71; 88; 90], feed-forward [26; 91; 106], and multi-view reconstruction methods [15; 50; 51] by leveraging a video generative model to achieve generation capabilities. Our primary focus is on preserving high-quality appearance and geometrical integrity from generated videos. This results in a generation process that not only captures the nuances of motion and deformation but also maintains the high standards of realism and detail that are essential for creating immersive and lifelike virtual 3D representations.

## 3 Method

We start by introducing the problem definition for 4D reconstruction in Sec. 3.1. Following that, we introduce our Vidu4D which encompasses two novel stages. The first stage is designed to learn Dynamic Gaussian Surfels (DGS), ensuring precise representation of both visual appearance and geometric structure during the non-rigid reconstruction process, as detailed in Sec. 3.2. The second stage focuses on establishing the initial non-rigid warping fields of DGS, as detailed in Sec. 3.3.

### Problem Definition

When given a single sequence of RGB video with \(T\) frames, the goal of 4D reconstruction is to determine a sequential 3D representation that could be rendered to fit each video frame as much as possible. Specifically, suppose the 3D representation for the \(t\)-th frame (termed as time \(t\)) is parameterized by \(_{t}\), where \(t=1,,T\). Given a differentiable rendering mapping \(\), we could obtain the rendered color at the frame pixel \(}^{t}^{2}\). We choose volume rendering as commonly adopted in NeRF , Gaussian Splatting , and Gaussian Surfels [16; 28]. The optimization of 4D reconstruction can be implemented by minimizing the empirical loss as

\[_{}_{t=1}^{T}_{}^{t}} (}^{t})=_{t},\{^{ t}_{i}\}_{i=1,,N},}(}^{t}), \]

where \(^{t}_{i}^{3}\) is the \(i\)-th 3D point sampled or intersected with Gaussian primitives along the ray that emanates from the frame pixel \(}^{t}\); \(N\) is the number of sampled or intersected points per ray; \((}^{t})\) and \(}(}^{t})\) are the rendered color and the observed color at \(}^{t}\), respectively.

In the following, we detail the proposed **Vidu4D**, a reconstruction pipeline comprising two key stages as illustrated in Fig. 2, including a field initialization stage and a DGS stage.

### Dynamic Gaussian Surfels

By optimizing Eq. (1), essentially our goal is to build a sequential 3D representation that could deform to be consistent with each 2D frame. We first start by considering an ideal video exhibiting different views of the same static object without object deformation, movement, or video distortion. To model the 3D representation with high appearance fidelity and geometry accuracy, we follow the method of using differentiable 2D Gaussian primitives as proposed by recent Gaussian Surfels advances [16; 28].

Figure 2: Illustration of the pipeline of Vidu4D, including the initialization stage and the DGS stage.

Specifically, the \(k\)-th Gaussian surfel (of the total \(K\)) is characterized by a central point \(_{k}^{*}^{3}\) and a local coordinate system centered at \(_{k}^{*}\) with two principal tangential vectors \(_{u}^{*}^{3 1}\), \(_{v}^{*}^{3 1}\) and scaling factors \(s_{u}^{*}\), \(s_{v}^{*}\). Here, we use the notation "\(*\)" to represent parameters in the static state. A Gaussian surfel is computed as a 2D Gaussian defined in a local tangent plane in the world space. Following , for any point \(=(u,v)\) located on the \(uv\) coordinate system centered at \(_{k}^{*}\), its coordinate in the world space, denoted as \(P_{k}^{*}()^{3 1}\), is computed by

\[P_{k}^{*}()=_{k}^{*}+s_{u}^{*}_{u}^{*}u+s_{v}^{* }_{v}^{*}v=[_{k}^{*}_{k}^{*}_{k}^{*}](u,v,1,1)^{}, \]

where \(_{k}^{*}=[_{u}^{*},_{v}^{*},_{u}^ {*}_{v}^{*}](3)\) denotes the rotation matrix, and the diagonal matrix \(_{k}^{*}=(s_{u}^{*},s_{v}^{*},0)^{3 3}\) denotes the scaling matrix.

In this work, our focus is on 4D reconstruction from a single generated video, which may exhibit large non-rigidity, distortion, or illumination changes. We introduce **Dynamic Gaussian Surfels (DGS)**, a method designed to achieve precise 4D reconstruction while accommodating non-rigidity and other time-varying effects.

Motivated by recent advancements in non-rigid reconstruction methods [57; 88; 98], we aim to ensure that the target object maintains a consistent static state across different frames, thereby mitigating non-rigidity and distortion effects. To achieve this, we employ warping techniques on each Gaussian surfel represented by \(P_{k}^{*}()\), transforming them into a corresponding Gaussian surfel \(P_{k}^{t}()\) at time \(t\), which is centered at \(_{k}^{t}^{3}\) with a rotation matrix \(_{k}^{t}(3)\) and a scaling matrix \(_{k}^{t}^{3 3}\).

**Non-rigid warping for Gaussian surfels.** We now build the warping process from the static state to the warped state. We leverage a non-rigid warping function with \(B\) bones as key points to ease the training of deformation. In the static state, the \(b\)-th bone is represented by 3D Gaussian ellipsoids , with more details provided in the Appendix. We let \(_{b}^{t}(3)\) represent a rigid transformation that moves the \(b\)-th bone from its static state to the warped state at time \(t\). In effect, \(_{b}^{t}\) is achieved by non-linear mappings using a multi-layer perception (MLP) with \((3)\) guaranteed, as will be given later in Eq. (5). The non-rigid warping function can be written as the weighted combination of \(_{b}^{t}\), where we apply dual quaternion blend skinning (DQB)  to ensure valid \((3)\) after combination,

\[^{t}=_{b=1}^{B}w_{b}^{t}( _{b}^{t}), \]

where \(w_{b}^{t}\) is the \(b\)-th element of the skinning weight vector \(^{t}^{B 1}\), as detailed in the Appendix; \(\) and \(\) denote the quaternion process and the inverse quaternion process, respectively. In this case, there is \(^{t}(3)\).

We therefore rewrite the warping as \(^{t}=[}^{t},}^{t}]\) with the rotation \(}^{t}(3)\) and translation \(}^{t}^{3}\), and apply the corresponding transformation to Eq. (2) by

\[P_{k}^{t}()=^{t}P_{k}^{*}()=[}^{t}_{k}^{*}_{k}^{*}}^{t}_{k}^{*}+}^{t}](u,v,1,1)^{}. \]

Figure 3: Illustration of the overall framework and our DGS in detail. For DGS, Gaussian surfels in the static state are transformed to the warped state by learning non-rigid warping functions conditioned on time \(t\) and coordinate \(\). We incorporate warped-state normal regularization for accurate geometry, and refined rotation and scaling matrices of Gaussian surfels for detailed appearance. Both branches in the warped state, including with and without refinement, share the same centers of Gaussian surfels and the same warping functions. “Field init.” stands for field initialization as introduced in Sec. 3.3.

Note that Eq. (4) holds for any given point \(P_{k}^{*}()\) including the center point of the \(k\)-th Gaussian surfel (_i.e._, \(_{k}^{*}\)) when \(=(0,0)\). By deriving Eq. (4), we enable connection of the warping function _w.r.t._ to any point \(=(u,v)\) on the local coordinate system centered at \(_{k}^{*}\), which is needed later in Eq. (8) where \(\) is an intersection with Gaussian surfels and a ray that emanates from the frame pixel.

**Warped-state normal regularization.** To accurately capture the geometric representation, we follow similar methods in Gaussian Surfels  to add normal consistency regularization which encourages all Gaussian surfels to be locally aligned with the actual surfaces. Differently, unlike 3D reconstruction for static scenes, 4D reconstruction commonly faces non-rigidity and distortion. Thus simply performing regularization to promote surface-aligned Gaussian surfels like previous methods harms the structural integrity due to the non-rigid warping.

We therefore design a warped-state normal regularization. As mentioned, each point \(P_{k}^{t}()\) in the warped state at time \(t\) is transformed from its corresponding static point \(P_{k}^{*}()\) based on the warping function in Eq. (4), namely, \(P_{k}^{t}()=^{t}P_{k}^{*}()\) with \(^{t}\) composed by \(_{b}^{t}\). To maintain the structural integrity to a large extent when regularizing normal, we design \(_{b}^{t}\) as a continuous field that takes both the point \(P_{k}^{*}()\) (or equivalently, \(\) in the local coordinate system) and the time \(t\) as conditions. By this setting, \(_{b}^{t}\) is expected to change continuously with the change of \(\) or \(t\). We implement the continuous field by using a NeRF-style MLP which directly outputs a 6-dimensional dual quaternion, and rely on the inverse quaternion process \(\) to guarantee \((3)\), _i.e._,

\[_{b}^{t}=(_{b}^{t};,t) , \]

where \(_{b}^{t}\) is a learnable latent code for encoding the \(b\)-th bone at time \(t\); both \(\) and \(t\) are sent to the MLP as conditions to obtain \(_{b}^{t}\). Thus \(^{t}\) is also expected to be continuous _w.r.t._\(\) and \(t\).

Based on the above design, the normal consistency loss at time \(t\) is obtained similar to ,

\[_{n}=_{k}_{k}(1-_{k}^{}^{t}), ^{t}(x,y)=^{t}_{y} ^{t}}{|_{x}^{t}_{y}^{t}|}, \]

where \(k\) indexes over intersected surfels along the ray that emanates from the frame pixel \(}\); \(_{k}=_{k}\,_{k}((}))_{j= 1}^{k-1}(1-_{j}\,_{j}((})))\) denotes the blending weight of the intersection point; \(_{k}\) represents the normal of the surfel that is oriented towards the camera; \(^{t}\), computed with finite differences, is the surface normal estimated by the nearby depth point \(^{t}\) at warped state time \(t\).

In summary, by learning a continuous warping field and aligning the surfel normal with the estimated surface normal in the warped state, we ensure that all Gaussian surfels locally approximate the actual object surface without being noticeably impaired by the non-rigid warping.

**Dual branch structure with refinement.** To further achieve fine-grained appearance and reduce the texture flickering during warping, we propose to learn refinement terms for adjusting the rotation matrices \(_{k}^{*}\) and scaling matrices \(_{k}^{*}\) (defined in Eq. (2)) in the static state. We suppose the refinement terms are \(_{k}^{*}(3)\) and \(_{k}^{*}^{3 3}\), respectively. Note that the third-axis of \(_{k}^{*}\) is no longer necessarily \(0\). During refinement, we remain the center points \(_{k}^{*}\) and the warping \(^{t}\) (_i.e._, including both \(}^{t}\) and \(}^{t}\)) to be unchanged. The new warped process is formulated as,

\[P_{k}^{ t}()=}^{t}(_{k}^ {*}_{k}^{*})(_{k}^{*}+_{k}^{*})}^{t}_{k}^{*}+}^{t}\,(u,v,1,1)^{ }. \]

During the training of DGS, we maintain two branches including one with refinement and one without. In the warped state, both branches are jointly trained with shared warping functions and centers of Gaussian primitives2. Due to the involvement of \(_{k}^{*}\) and \(_{k}^{*}\), both branches have different rotation and scaling matrices of Gaussian primitives.

**Rasterization.** Given a frame pixel \(}\) and a camera ray that emanates from \(}\), following the static-state methods to calculate intersection coordinates with Gaussian primitives along the ray , we could obtain warped-state intersection coordinates based on Eq. (4) and Eq. (7). We then perform the volume rendering process  that integrates alpha-weighted appearance along the ray by

\[(})=_{k}_{k}\,_{k}\,_{k }(})_{j=1}^{k-1}1-_{ j}\,_{j}(}), \]where \(k\) indexes over intersected Gaussian primitives along the ray that emanates from the frame pixel \(}\); \(_{k}\) and \(_{k}\) denote the opacity and view-dependent appearance parameterized with spherical harmonics of the \(k\)-th Gaussian surfel, respectively; \(_{k}((}))=(-+v^{2}}{2})\) corresponds to the \(k\)-th intersection point \((})\) which could be directly calculated when given \(P_{k}^{t}()\) or \(P_{k}^{ t}()\) and the corresponding local coordinate system. During implementation, \(_{k}((})))\) is further applied a low-pass filter following [7; 28].

A detailed architecture of DGS is depicted in Fig. 3. Important symbols are summarized in our Appendix.

### Field Initialization

Given that the camera trajectory of generated videos is unknown, SfM methods like COLMAP struggle to converge due to rigidity violations. Additionally, since the background of generated videos appears to exhibit soft deformation or flickering colors, proper estimation of camera/body poses through background SfM is hindered. These challenges often result in very few successful registrations, as demonstrated in previous monocular 4D reconstruction tasks .

To address this, we design an implicit field before performing DGS to initialize the camera poses and establish the continuous warping field in Eq. (5). In this part, we propose the **field initialization** as another key component of our pipeline to initialize the continuous warping field of DGS for fast and stable convergence, as detailed below.

Initially, we train a neural Signed Distance Function (SDF) model , leveraging the same warping structure with bones as utilized in DGS. While DGS transforms Gaussian surfels from the static state to the warped state for rasterization, the neural SDF reverses this process, mapping points along camera rays from the warped state back to the static state. For the neural SDF component, we optimize the reverse warping process and deduce the forward warping as its inverse by minimizing a cycle loss, inspired by [10; 98]. Subsequently, we initialize \(()\) in Eq. (5) to obtain warping functions \(_{b}^{f}\) by the network weights learned by the neural SDF part.

During the rendering of the neural SDF, we perform backward warping on the warped-state sampling points to the static state,

\[^{f,-1}=_{b=1}^{B}w_{b}^{f}(_{b}^{f})^{-1},^{f}=^{f,-1}^{*}, \]

which is an inversion of Eq. (3). By querying the SDF with a sample point \(^{*}\) in the static state, we render RGB and compute the photometric loss to optimize the SDF and the warping field defined in Eq. (5).

Nevertheless, there are two discrepancies between the neural SDF warping and DGS warping. Firstly, sampling points of the neural SDF are distributed in the frustum of the camera, while sampling points of DGS are distributed on the object surface. Additionally, we train the inversion of during initialization, while we utilize the non-inverse ones in DGS. To resolve the distribution gap and ensure that faithfully models the forward warping, we add a cycle loss,

\[_{}=^{f}(^{f,-1}(^ {f}))-^{f}_{2}^{2}, \]

where \(^{f}\) could be either a mesh surface point or a sample point on the camera ray.

After initialization, we extract the canonical space mesh using marching cubes and initialize Gaussian surfels on it. We set the spherical harmonic in 0-th order to the RGB value of the nearest vertices. The warping field and learned camera poses are retained.

With the field initialization before DGS, our Vidu4D is capable of performing a text-(to-video)-to-4D generation task with the integration of existing video diffusion models.

## 4 Experiment

In this section, we provide an extensive evaluation of our method DGS (Sec. 3.2) with the initialization in Sec. 3.3, comparing both appearance and geometry against previous state-of-the-art methods. Additionally, we analyze the contributions of each proposed component in detail.

### Implementation

For all qualitative and quantitative experiments, we follow the standard pipeline for dynamic reconstruction , to construct our evaluation setup by selecting every fourth frame as a training frame and designating the middle frame between each pair of training frames as a validation frame.

Our model configuration involves several key parameters to balance reconstruction and regularization losses. For the field initialization stage, we use a similar architecture with \(8\) layers for volume rendering as in NeRF , and initialize MLP for predicting SDF as an approximate unit sphere . We obtain a neural SDF, a warping field, and camera poses after this stage. For the DGS stage, we initialize centers of the Gaussian surfels with the sampled surface points extracted from the neural SDF, and initialize the warping field by the forward field from the first stage. The dimension of the latent code embedding \(_{b}^{t}\) is set as \(128\). Following BANMo , we adopt 25 bones to optimize skinning weights. For each reconstruction, the overall training takes over 1 hour on an A800 GPU.

### Qualitative Evaluation

In the qualitative evaluation, we visually compare the novel-view reconstructions produced by our DGS against those generated by other state-of-the-art models, as illustrated in Fig. 4. Our evaluation focuses on several key aspects including detail preservation, texture quality, and geometric accuracy. Compared to methods based on implicit fields, the integration of Gaussian in our approach facilitates the rendering of highly detailed textures. Additionally, benefiting from a more geometry-aware representation, our method produces superior normal maps compared to those purely Gaussian-based methods. This also enhances the robustness of our method against artifacts of the generated videos

Figure 4: Novel-view qualitative evaluation compared with SOTA methods including NeRF-based methods (BANMo  and D-NeRF ) and Gaussian splatting-based methods (Deformable-GS  and SCGS ). We also provide our learned camera poses to baseline approaches for a fair comparison. These variants are denoted as “w. Poses”. Best view in color and zoom in.

like occlusions. For instance, in the third clip of the series, which features a dragon shrouded in fog, both SCGS and Deformable-GS methods tend to overfit and subsequently show a decline in performance. In contrast, our method consistently delivers superior results.

### Quantitative Evaluation

We provide the quantitative evaluation comparing our method with state-of-the-art works in Table 1. Metrics include Peak Signal-to-Noise Ratio (PSNR) to evaluate the fidelity of the reconstructed textures, Structural Similarity Index (SSIM) for the quality evaluation, and LPIPS  as a perceptual metric. Our method exhibits superiority over all baseline methods, even with our learned poses, _e.g._, \(\)2.5 PSNR increase over SCGS with poses for the averaged results.

### Ablations

To understand the contributions of each component in Vidu4D, especially DGS, we conduct ablation studies in this section. We remove or alter specific elements of our model and observe the resulting performance changes in both appearance and geometry reconstruction.

**Geometric regularization.** We evaluate the impact of warped-state normal regularization by disabling it during training. From Fig. 5(b)(d), we observe that when removing the regularization, there is an obvious degradation in the structural integrity of surface-aligned Gaussian surfels, leading to noticeable inconsistency in the reconstructed 4D models.

**Refinement strategy.** We examine the effect of omitting refinements by keeping one branch (the concept of branches could be better visualized in Fig. 3) during training, shown in Fig. 5(b)(c). The performance indicates that removing refinements increases the loss of fine-grained appearance details. Additionally, we also find that refinements are crucial for mitigating the texture flickering issue.

**Additional ablations.** Please refer to the Appendix for additional ablation studies that detail the effectiveness of our refinement strategy and field initialization.

    &  &  &  &  \\  &  &  &  &  &  &  &  &  &  \\  BLAN36  & 15.10 & 0.6514 & 0.2575 & 13.15 & 0.5910 & 0.3241 & 18.48 & 0.6423 & 0.3500 & 1.3500 & 1.28 & 2.9970 & 1.0153 & 0.0714 & 0.3738 & 0.0665 \\ D-NeRF  & 15.15 & 0.6537 & 0.2657 & 13.321 & 0.5930 & 0.3344 & 18.53 & 0.6849 & 0.3037 & 21.01 & 2.366 & 0.8519 & 0.0717 & 0.122 & 0.0754 \\  Deformable-GS  & 19.09 & 0.7815 & 0.2434 & 20.35 & 0.8039 & 1.982 & 24.19 & 0.9100 & 0.0092 & 13.22 & 4.32 & 0.5934 & 0.0853 & 0.3792 & 0.0763 \\ SCGS  & 19.46 & 0.7867 & 0.2405 & 20.87 & 0.8123 & 0.1919 & 24.03 & 0.9083 & 0.1009 & 21.17 & 2.69 & 0.8547 & 0.0691 & 0.1504 & 0.0737 \\ Deformable-GS + our field unit. & 21.94 & 0.8123 & 0.1386 & 2.241 & 0.8200 & 0.1687 & 26.05 & 0.9218 & 0.0944 & 2.625 & 2.14 & 0.8496 & 0.0483 & 0.1452 & 0.0354 \\ SCGS + our field unit. & 23.25 & 0.8529 & 0.1574 & 23.70 & 0.8338 & 0.1407 & 28.40 & 0.9375 & 0.0606 & 24.75 & 2.11 & 0.8660 & 0.0440 & 0.1201 & 0.0359 \\
**Ours** & **24.63** & **0.8432** & **0.1559** & **25.68** & **0.8843** & **0.1117** & **28.58** & **0.9392** & **0.04613** & **27.30** & **2.266** & **0.9152** & **0.0602** & **0.0877** & **0.0564** \\   

Table 1: Novel-view quantitative results on generated videos. Evaluation metrics are PSNR, SSIM, and LPIPS. We report results on three single videos and the averaged results over 30 single videos.

Figure 5: Ablation studies on the geometric regularization and refinement strategy. For our full model shown in (b), we provide our rendered color, rendered normal, and surface normal (estimated from the depth points for regularization). Additionally, for comparison, we visualize the rendered color for the case without refinements in (c) and the rendered normal for the case without warped-state normal regularization in (d), respectively. We showcase our model’s fidelity with close-ups.

Conclusion

We introduce Vidu4D as a novel reconstruction model to achieve high-fidelity 4D representations from single generated videos. Vidu4D is powerful with our proposed DGS which builds the non-rigid warping field to transform Gaussian surfels, ensuring precise capture of motion and deformation over time. DGS also introduces key innovations that greatly enhance the accuracy and fidelity of 4D reconstruction, including dual branch refinement and warped-state geometric regularization. Our experiments demonstrate that Vidu4D outperforms existing methods in both quantitative and qualitative evaluations, highlighting its superiority in generating realistic and immersive 4D content.

**Limitations and broader impact.** While Vidu4D with DGS presents a significant performance in 4D reconstruction, currently there are still limitations such as the reliance on video quality, scalability challenges for large scenes, and computational difficulties in real-time applications. Additionally, when equipping Vidu4D with generative models, as with any generative technology, there is a risk of producing deceptive content which needs more caution.

## 6 Acknowledgement

This work was partly supported by the NSFC Projects (Nos. 62350080, 62306163, 62276149, 92370124, 92248303, U2341228, 62061136001, 62076147), BNRist (BNR2022RC01006), Tsinghua Institute for Guo Qiang, and the High Performance Computing Center, Tsinghua University. J. Zhu was also supported by the XPIorer Prize. Y.K. Wang was also supported by the China National Postdoctoral Program (No. 2023M741951) and Major Project of the New Generation of Artificial Intelligence (No. 2018AAA0102900).