# AMDP: An Adaptive Detection Procedure for False Discovery Rate Control in High-Dimensional

Mediation Analysis

Jiarong Ding

School of Mathematics and Statistics

Xi'an Jiaotong University

djr9901@stu.xjtu.edu.cn &Xuehu Zhu

School of Mathematics and Statistics

Xi'an Jiaotong University

zhuxuehu@xjtu.edu.cn

Corresponding author

###### Abstract

High-dimensional mediation analysis is often associated with a multiple testing problem for detecting significant mediators. Assessing the uncertainty of this detecting process via false discovery rate (FDR) has garnered great interest. To control the FDR in multiple testing, two essential steps are involved: ranking and selection. Existing approaches either construct p-values without calibration or disregard the joint information across tests, leading to conservation in FDR control or non-optimal ranking rules for multiple hypotheses. In this paper, we develop an adaptive mediation detection procedure (referred to as "AMDP") to identify relevant mediators while asymptotically controlling the FDR in high-dimensional mediation analysis. AMDP produces the optimal rule for ranking hypotheses and proposes a data-driven strategy to determine the threshold for mediator selection. This novel method captures information from the proportions of composite null hypotheses and the distribution of p-values, which turns the high dimensionality into an advantage instead of a limitation. The numerical studies on synthetic and real data sets illustrate the performances of AMDP compared with existing approaches.

## 1 Introduction

Mediation analysis is regarded as a prevalent tool to dissect a mediation relationship between exposures and outcomes, and it has been widely applied in different fields, such as epidemiology , public health , policy evaluation , social sciences , neuroscience , and many others. Baron et al.  provided the basis for the advance of mediation analysis. They proposed a conventional regression-based approach, commonly referred to as the causal steps method, to examine the logical relationships among exposure, mediator, and outcome variables linking in a causal chain. While the causal steps method established necessary conditions for causal inference, it did not provide a joint test of the indirect effect of exposure on the outcome through a mediator, as recommended by MacKinnon et al. . Thus, MacKinnon et al.  investigated the normality-based Sobel's test  as a means to detect mediation effects under the framework of the product-coefficient method, the product of the exposure-mediator and mediator-outcome effects. However, as highlighted by MacKinnon et al. , the distribution of product coefficients is not normally distributed, leading the Sobel's test being overly conservative. Hence, MacKinnon et al.  proposed the joint significance test (also name as the MaxP test) to alleviate this conservatism. Nonetheless, the joint significance test still suffers from low statistical power, as it overlooks the impact of the composite null structure in mediation analysis [33; 42].

To address the aforementioned issues, Taylor et al.  recommended the utilization of the distribution of the product method or bootstrapping as alternative procedures. These methods had been shown to exhibit higher power while still maintaining reasonable control over the Type I error rate. In the study of Kamukama et al. , a z-value based Sobel's test was introduced to investigate the mediation effect of competitive advantage on the relationship between intellectual capital and financial performance. Nuijten et al.  proposed an approach based on Bayesian models for testing the presence of an indirect effect. More recently, Zhang  developed two data-adaptive tests that outperform both Sobel's test and joint significance tests.

Despite these advances, a further challenge is that the methods mentioned above typically assume a low-dimensional mediator, whereas the development of high-throughput technologies promotes a growing need for research dealing with high-dimensional data. The multiple testing problem arising from high-dimension mediation analysis is aimed at identifying the relevant mediators that explain the effect of exposure on an outcome. Differ from a single test, multiple testing approaches are aimed to handle simultaneous testing, and often use the false discovery rate (FDR) to measure the uncertainty of detecting process , that is,

\[=,j\}}{\# \{j\} 1},=[], \]

where \(j\) represents the index of mediator, \(_{0}\) is the index set of the null mediators, \(\) is the index set of selected mediators.

The control of FDR in multiple hypothesis testing consists of two primary steps: ranking and selection. In the initial step, a ranking statistic is calculated to evaluate the significance of each test, resulting in the ranking of hypotheses. The subsequent step is to maximize the selection set based on the established ranking order from the first step, while simultaneously maintaining the FDR at the target level. Although the p-value generated from the single hypothesis testing method can be considered as a ranking statistic and further combined with the BH procedure  for FDR control, there are still some limitations inherent to this approach. Firstly, the single p-value without calibration may be conservative and lead to excessive conservation in FDR control. Secondly, in the absence of joint information across multiple hypotheses, the ranking statistics based on the p-value may not be optimal.

To overcome these limitations, recent advancements in mediation analysis attempt to construct calibrated test statistics that account for the composite nature of the null hypothesis, and further achieve FDR control in multiple testing. For example, Dai et al.  developed a JS-mixture procedure, which utilizes the maxP statistics  as the ranking statistics and corrects the conservatism in the joint significance test by estimating the mixture distribution of p-values. Although JS-mixture sharply achieves FDR control, it is still underpowered for the reason that the maxP statistics does not account for the distribution information of two-dimension p-values under different hypotheses. Furthermore, Liu et al.  proposed a procedure called DACT focusing on constructing calibrated p-values for each single test by combining information across large-scale tests. Specially, DACT estimates the proportions of sub-null hypotheses and generates weighted p-values accordingly. However, DACT may suffer from underpowered performance in certain situations due to the following reasons. Intuitively, it ignores the alternative hypothesis information in its weighting scheme. On a deeper level, it does not fully consider the distributed information of two-dimensional p-values, limiting its ability to leverage valuable insights for improved power.

In this paper, we propose an adaptive mediation detection procedure (AMDP) for identifying relevant signals in high-dimensional mediation analysis. Our main contributions are summarized below:

* AMDP utilizes a two-dimensional p-value based local FDR as a test statistic, allowing for the comprehensive utilization of structural information of large-scale tests. Additionally, it determines the optimal rule for the order of selecting mediators.
* We establish theoretical results showing that AMDP enables asymptotic control of the FDR for selected mediators using the estimated local FDR.
* We reveal the critical importance of information retention in the ranking step for achieving optimal statistical power by discussing the limitations of the ranking statistics used in existing methods and conducting comparisons with AMDP.
* We empirically demonstrate the effectiveness of AMDP on synthetic and real data sets. Simulation results confirm the validity of our approach, and an application to a prostate cancer dataset illustrates its satisfactory performance in identifying CpG methylation sites that mediate between risk SNPs and gene expression.

The remainder of this paper is organized as follows: Section 2 formally introduces an optimal ranking rule based on AMDP along with an estimator of local FDR. We theoretically prove the ability of AMDP in controlling the FDR while mimicking the optimal power. Section 3 presents the simulation studies to evaluate the performance of AMDP. In Section 4, we demonstrate the practical utility of AMDP by applying it to the prostate cancer dataset in TCGA 2015. We conclude the paper in Section 5. The technical proofs and additional discussion are postponed to Appendix.

## 2 AMDP: an optimal multiple testing procedure for FDR control

Let \(X\) be the exposure (independent variable), \(\{M_{1},,M_{J}\}\) be candidate mediators, and \(Y\) be the outcome (dependent variable). In the context of mediation analysis in the genome-wide association studies (GWASs), \(X\) often refers to the single nucleotide polymorphisms (SNPs), \(M\) corresponds to DNA methylation, and \(Y\) pertains to gene expression or a risk of disease. As stated by Baron et al. , the mediation relationship can be expressed by the following models:

\[(M_{j} X)&= _{0j}+_{j}X,\\ (Y M_{j},X)&=_{0j}+ _{j}M_{j}+_{1j}X, \]

where \(_{j}\) denotes the effect of \(X\) on \(M_{j}\), \(_{j}\) represents the effect of \(M_{j}\) on \(Y\), totally the product of \(_{j}\) and \(_{j}\) represents indirect effect of \(X\) on \(Y\). \(_{1j}\) is the direct effect of \(X\) on \(Y\) with the \(M_{j}\) being fixed. We assume that there are no unmeasured confounding variables, also known as the sequential ignorability assumption . Any confounders can be adjusted by additional covariates , and such an adjustment is omitted in model (2) for simplification.

Testing whether \(\{M_{1},,M_{J}\}\) plays an intermediary role in the causal path from \(X\) to \(Y\) in (2) can be transformed into a multiple testing problem:

\[H_{0j}:_{j}_{j}=0H_{1j}:_{j}_{j} 0. \]

The above composite hypothesis can be decomposed into four disjoint cases as follows:

\[_{00,j}:&_{j}=0_{j}=0,\\ _{01,j}:&_{j}=0_{j} 0,\\ _{10,j}:&_{j} 0_{j}=0,\\ _{11,j}:&_{j} 0_{j} 0,\\ \]

where Case 1-3 represents the composite null hypothesis, and Case 4 is the alternative hypothesis. A rejection of \(H_{0j}\) indicates the presence of a mediation effect by \(M_{j}\). In this paper, the \(p\)-values for testing \(_{j}=0\) and \(_{j}=0\) are respectively denoted as \(p_{1j}=2\{1-(_{j}/_{_{j}})\}\) and \(p_{2j}=2\{1-(_{j}/_{_{j}})\}\), where \(_{j}\), \(_{j}\), \(_{_{j}}\) and \(_{_{j}

Then, the density function of \(p_{j}\) follows the following four-group model, which can be considered as a variant version of the random mixture model .

\[p_{j} f(p)=_{00}f_{00}(p)+_{01}f_{01}(p)+_{10}f_{10}(p)+_{11}f_{1 1}(p). \]

Under the four-group model (5), the local FDR [18; 19] is defined as

\[(p)=(H_{00,j} H_{01,j} H_{10,j}=1 p_{j} =p)=f_{00}(p)+_{01}f_{01}(p)+_{10}f_{10}(p)}{f(p)}. \]

It refers to the posterior probability that a hypothesis is null, given its corresponding p-value.

Before delving into the optimal ranking guidelines, we introduce several key definitions relevant to this objective. For any rejection region \(S^{2}\), we define the global FDR as

\[(S)=(H_{00} H_{01} H_{10}=1 p_{j} S), \]

where \(H_{00},H_{01}\), and \(H_{10}\) are composite null hypothesis. The power is defined as

\[(S)=\{p_{j} S H_{11}=1\}. \]

In the ranking step, the primary objective is to establish an optimal ranking rule that accurately reflects the significance order of the tests, while adhering to the optimality goal set in the selection step. Under the Neyman-Pearson framework , this optimality goal entails maximizing power while simultaneously controlling the global FDR at a targeted level of \(\). This process can be formulated as a constrained optimization problem, i.e.

\[_{S}(S)(S). \]

The optimal rule under the two-group model has been extensively studied in the literature. Researchers have proposed various methods for optimal decision-making based on different frameworks [5; 9]. Our optimality goal shares similarities with the work of Lei et al. . They had demonstrated that, under Bayes rule, the optimal rejection thresholds are the level surfaces of local FDR. We extend this insight to p-values in two dimensions, and define the form of the rejection region as \(S()=\{p:(p)\}\). A detailed and comprehensive explanation of this concept is provided in Theorem 1.

**Theorem 1**.: _Assume that_

1. \(f_{00}(p)\)_,_ \(f_{01}(p)\)_,_ \(f_{10}(p)\)_, and_ \(f_{11}(p)\) _are continuous;_
2. \((p:(p)=t)=0\) _for any_ \(t(0,1]\)_, where_ \(\) _is a Lebesgue-Stieltjes measure on the two-dimensional Borel space_ \((^{2},^{2})\)_._

_Then, for any given global FDR level \(\), there exists a unique value \(^{}\) such that \(S(^{})\) is the solution of the constrained optimization problem in (9). And the local FDR involved in \(S(^{})\) corresponds to the optimal ranking rule._

**Remark 1**.: _Genovese et al.  have shown that under weak conditions, \(=+O(})\), where \(J\) represents the number of mediators. Hence, controlling \(\) and \(\) are asymptotically equivalent as the number of mediators \(J\) tends to infinity. A similar result supporting this equivalence was also obtained by Storey ._

### The estimator of local FDR

From Theorem 1, we have established that the optimal ranking rule under the Neyman-Pearson framework is the local FDR. However, it is worth noting that the discussions in Section 2.1 are based on the assumption that the distribution of p-values and proportions of the composite hypothesis are known. In the following, we emphasize that our results still hold if the local FDR can be consistently estimated. Assuming that \(f_{00} 1\) (p-values follow uniform distribution in Case 1). The estimation of \((p)\) can be divided into three parts: (i) The proportions of the composite null hypothesis \(_{00},_{01},_{10}\); (ii) The mixture density \(f(p)\); (iii) The densities of the composite null hypothesis \(f_{01}(p),f_{10}(p)\).

Motivated from Storey et al. , \(_{01},_{10}\), and \(_{00}\) can be estimated as follows:

\[_{0}.()=>)}{J(1-)},_{0}()=>)}{J(1- )},_{00}()=>,p_{2 j}>)}{J(1-)^{2}}, \]where \(_{0}.()\) denotes the estimator of the proportion of null \(p_{1j}\), \(_{0}()\) denotes the estimator of the proportion of null \(p_{2j}\). \(I()\) is an indicator function, and \([0,1)\) is a tuning parameter. In practice, there is a bias versus variance tradeoff for choosing a suitable \(\). Further research on selecting an appropriate value of \(\) is detailed in the Appendix A. Following that

\[_{01}()=_{0}.()-_{00}(), _{10}()=_{.0}()-_{00}(). \]

Next, we turn to the estimation of \(f(p)\). For this purpose, we employed an adaptation of the beta kernel function proposed by Chen . This choice is made considering the fact that \(p=(p^{(1)},p^{(2)})\) falls within \(^{2}\). The beta kernel function allows for a flexible and smooth estimation of \(f(p)\), providing a suitable estimation approach for our analysis. Our beta kernel estimator is:

\[(p)=(p^{(1)},p^{(2)})=J^{-2}(_{j=1}^{J}K_{p^{(1)},b}^{ }(p_{1j}))(_{j=1}^{J}K_{p^{(2)},b}^{}(p_{2j})), \]

where \(K_{p,b}^{}\) is a boundary beta kernel defined as

\[K_{p,b}^{}(t)=K_{p/b,(1-p)/b}(t)&p(2b,1-2b),\\ K_{(p,b),(1-p)/b}(t)&p[0,2b],\\ K_{p/b,(1-p)}(t)&p[1-2b,1],\]

\(K_{u,v}\) be the density function of a \((u,v)\) random variable, \(b\) is a smoothing parameter, and \((p,b)=2p^{2}+2.5-+6p^{2}+2.25-p^{2}-p/b}\).

In the context of mediation analysis, the density of p-values follows a mixture distribution, as indicated in (5). This mixture distribution involves three distinct types of null hypotheses: \(H_{01}\), \(H_{10}\), and \(H_{00}\). Distinguishing between \(H_{01}\) and \(H_{10}\), as well as obtaining accurate estimators for \(f_{01}(p)\) (corresponding to \(H_{01}\)) and \(f_{10}(p)\) (corresponding to \(H_{10}\)) is indeed a challenging task. Motivated by the knockoff method , we consider leveraging the symmetry property of p-values under the composite null hypothesis to tackle this issue. Before diving into the details of utilization of the symmetry property for estimating \(f_{01}(p)\) and \(f_{10}(p)\), we introduce some essential notations and assumptions.

Denote \(_{00},_{01}\), and \(_{10}\) as the index set of the null mediators under the composite null hypothesis \(H_{00},H_{01}\), and \(H_{10}\), respectively. We define the region \(D\) as \([0,0.5)^{2}\), with its symmetric regions as follows: \(_{01}=[0.5,1][0,0.5)\), \(_{10}=[0,0.5)[0.5,1]\), and \(_{00}=[0.5,1]^{2}\). The assumptions are given as follows.

**Assumption 1**.: _For \(j_{00}\), the sampling distribution of \(p_{j}\) is symmetric about \(p^{(1)}=0.5\) and \(p^{(2)}=0.5\); For \(j_{01}\), the sampling distribution of \(p_{j}\) is symmetric about \(p^{(1)}=0.5\); For \(j_{10}\), the sampling distribution of \(p_{j}\) is symmetric about \(p^{(2)}=0.5\)._

**Assumption 2**.: _The symmetric regions of \(D\) satisfy: (i) For \(p_{00}\), \(_{n}f_{11}(p)=0\), \(_{n}f_{01}(p)=0\), (ii) For \(p_{01}\), \(_{n}f_{11}(p)=0\) and \(_{n}f_{10}(p)=0\); (iii) For \(p_{10}\), \(_{n}f_{11}(p)=0\) and \(_{n}f_{01}(p)=0\)._

Assumption 1 is only required for the null mediators. It indicates that at least one of \(p_{1j}\) and \(p_{2j}\) follows a uniform distribution under the composite null hypothesis. Assumption 2 holds for any reasonable p-value. Since a non-null p-value should fall within \([0,0.5)\), we can infer that as the sample size \(n\) tends to infinity, the probability of p-values under alternatives falling within \([0.5,1]\) approaches zero. Additional explanations on Assumptions 1-2 are detailed in the Appendix D.2.

Remarkably, under Assumption 1, we can decompose \(f_{10}(p)\) and \(f_{01}(p)\) as follows:

\[f_{10}(p)=f_{1}.(p^{(1)}) f_{ 0}(p^{(2)})=f_{1}.(p^{(1)}), f_{01}(p)=f _{0}.(p^{(1)}) f_{ 1}(p^{(2)})=f_{ 1}(p^{(2)}), \]

where \(f_{0}(p^{(1)})=f_{ 0}(p^{(2)})=1\). This decomposition allows us to transform the problem into estimating \(f_{1}(p^{(1)})\) and \(f_{ 1}(p^{(2)})\), representing the marginal probability density of \(p_{1j}\) and \(p_{2j}\) under alternatives, respectively. Assumption 2 provides the inspiration to utilize the symmetric regions about \(D\) to address this estimation task, that is,

\[k_{1}(p^{(1)})=+_{10}f_{1}(p^{(1)})}{_{00}+_{10}},  k_{2}(p^{(2)})=+_{01}f_{ 1}(p^{(2)})}{_{00}+_{01}}, \]where \(k_{1}(p^{(1)})\) denotes the density of \(p_{1j}\) under \(H_{10}\) and \(H_{00}\), and \(k_{2}(p^{(2)})\) denotes the density of \(p_{2j}\) under \(H_{01}\) and \(H_{00}\).

We apply the beta kernel function to \(p_{1j}\) in region \(_{10}\) and \(_{00}\), as well as to \(p_{2j}\) in region \(_{01}\) and \(_{00}\), to get the estimation of \(k_{1}(p^{(1)})\) and \(k_{2}(p^{(2)})\) as

\[_{1}(p^{(1)})=J_{1}^{-1}_{j=1}^{J_{1}}K_{p^{(1)},b}^{*}(p_{1j} ),_{2}(p^{(2)})=J_{2}^{-1}_{j=1}^{J_{2}}K_{p^{(2)},b}^{*} (p_{2j}), \]

where \(J_{1}\) is the number of p-values in region \(_{10}\) and \(_{00}\), \(J_{2}\) is the number of p-values in region \(_{01}\) and \(_{00}\). Combining the estimators in (10)-(11), (13)-(15), we obtain the estimation of \(f_{1}.(p^{(1)})\) and \(f_{ 1}(p^{(2)})\) as:

\[_{1}.(p^{(1)})=_{00}+_{10})_{1}(p^{(1)} )-_{00}}{_{10}},_{ 1}(p^{(2)})=_{00} +_{01})_{2}(p^{(2)})-_{00}}{_{01}}. \]

Therefore, the local FDR estimator is derived as:

\[}(p)=_{00}f_{00}(p)+_{01}_{0 1}(p)+_{10}_{10}(p)}{(p)}. \]

### Asymptotic FDR control

In this section, we present a selection strategy for the second step. The primary goal of our proposed selection strategy is to maximize power while simultaneously controlling the FDR based on the established ranking order from the first step, i.e., finding the optimal threshold \(^{*}\) of the optimization problem (9). However, determining such an optimal threshold \(^{*}\) is a challenging task, as it involves decision-making based on the estimation of global FDR. To address this challenge effectively, we propose a data-driven strategy. Based on the notations in Section 2.2, the form of the FDP and FDR are given by

\[()=_{01}_{ 10},j\}}{\#\{j:j\} 1}\ \ ()=(()), \]

where \(\) is the index set of selection in rejection region \(()=\{p:}(p)\}\), i.e., \(j\) when \(p_{j}()\), the denominator of \(()\) represents the total number of rejections and the numerator represents the number of false positives.

In mediation analysis, accurately estimating the number of false discoveries in (18) poses a challenge, since the rejection region \(()\) comprises a mixture of four distinct types of hypotheses. However, we can draw inspiration from Assumptions 1 and 2 to leverage the symmetry property of p-values under the composite null hypothesis to estimate the number of false positives. We define the symmetric regions of \(\) as \(_{01}=\{(1-p^{(1)},p^{(2)}):}(p)\}\), \(_{10}=\{(p^{(1)},1-p^{(2)}):}(p)\}\), \(_{00}=\{(1-p^{(1)},1-p^{(2)}):}(p)\}\). It's noteworthy that the rejection region \(\) is a subset of the region \(D\) defined in Section 2.2, and its symmetric regions, \(_{01}_{01}\), \(_{10}_{10}\), and \(_{00}_{00}\). Indeed, Assumptions 1 and 2 provide us with an approximation of the number of false positives in (18):

\[\#j_{01}_{00}\!:\!p_{j}()} \#\{j_{01}_{00}\!:\!p_{j}_{01}( )\}\#\{j\!:\!p_{j}_{01}()\},\]

\[\#j_{10}_{00}\!:\!p_{j}()} \#\{j_{10}_{00}\!:\!p_{j}_{10}( )\}\#\{j\!:\!p_{j}_{10}()\},\]

\[\#j_{00}\!:\!p_{j}()}\# \{j_{00}\!:\!p_{j}_{00}()\}\# \{j\!:\!p_{j}_{00}()\}.\]

The number of the selection \(\#\{j:p_{j}_{01}()\}+\#\{j:p_{j}_{ 10}()\}-\#\{j:p_{j}_{00}()\}+1\) can be considered as an overestimation of \(\#\{j:j_{00}_{01}_{10},j\}\), and \(}()\) is given by

\[}()=_{01}() \}+\#\{j:p_{j}_{10}()\}-\#\{j:p_{j} _{00}()\}+1}{\#\{j:p_{j}()\}  1}. \]Then the data-driven cutoff \(^{}\) can be determined as follows:

\[^{}=\{>0:}()\}, \]

and the final selection is \(_{^{}}=\{j:p_{j}(^{})\}\).

Finally, we summarize our proposed FDR control procedure in Algorithm 1.

```
1:Calculate a pair of p-values \(p_{j}=(p_{1j},p_{2j})\) following model (2), where \(j=1,,J\).
2:Estimate the proportions of the composite null hypothesis \(_{00},_{01},_{10}\).
3:Estimate the null densities \(_{01}(p),_{10}(p)\) and the mixture density \((p)\) using the adaptation of the beta kernel estimator.
4:Estimate the \(}(p)\) following (17).
5:For a nominal FDR level \((0,1)\), select the mediators \(\{j:p_{j}(^{})\}\) where \(()=\{p:}(p)\}\) and the cutoff \(^{}\) is \[^{}\!=\!\{>0:}()=_{01}()\}\!+\!\#\{j\!:\!p_{j}_{10}()\}\!-\!\#\{j\!:\!p_{j}_{00}()\} +1}{\#\{j:p_{j}()\} 1}\}.\]
```

**Algorithm 1** A data-driven algorithm for FDR control.

**Remark 2**.: _In a recent work of Deng et al. , a procedure called JM was introduced for detecting simultaneous signals across multiple independent experiments. The core idea behind JM is to partition the region of p-values into masked and unmasked areas, and then utilize p-values from each of these regions to estimate FDR and local FDR, respectively. By leveraging the partially revealed information from the unmasked area, JM updates the rejection region in the masked area iteratively until it reaches the desired FDR level. In contrast to the stepwise updates in the JM procedure, the AMDP does not require such iterative adjustment. By leveraging information from large-scale testing, AMDP can accurately estimate the local FDR. Additionally, motivated by the symmetric property of the composite null hypothesis, we proposed a data-driven algorithm to determine the optimal rejection region. As shown in (19), the number of the selection \(\#\{j:p_{j}_{01}()\}+\#\{j:p_{j}_ {10}()\}-\#\{j:p_{j}_{00}()\}+1\), provides a less conservative estimation of \(\#\{j:j_{00}_{01}_{10},j\}\) than JM, which relies on conditional mirror conservation to estimate FDP in masked region._

Theorem 2 below shows that for any nominal FDR level \((0,1)\), both \((^{})\) and \((^{})\) are under control using Algorithm 1, as the sample size \(n\) and the number of mediators \(J\) tend to infinity.

**Theorem 2**.: _Assume that_

1. \(_{j=1}^{J}|}(p_{j})-(p_{j })|}{{}}0n,J;\)__
2. _For_ \((0,1]\)_,_ \(((p_{j}) j_{00}_{01} _{10})\) _is continuous;_
3. _For any FDR level of_ \((0,1)\)_, there exists a constant_ \(_{}(0,1]\) _such that_ \(((_{})) 1\) _as_ \(J\)_._

_When Assumptions 1-2 holds, we have_

\[(^{})+o_{p}(1)_{n,J}(^{}).\]

Proofs of Theorems 1-2 are given in Appendix E.

**Remark 3**.: _To demonstrate the effectiveness of our proposed method, we provide examples under two scenarios that highlight how the loss of associated information across tests can result in decreased power. We compare our method, AMDP, with the JS-mixture test and the DACT , which provides further evidence for the utility of AMDP. Due to space limitations, this section is postponed to the Appendix B._

## 3 Simulation Study

In this section, we conduct a thorough set of simulations to assess the performance of our proposed method AMDP. For a comprehensive comparison, we evaluate two competing methods, the JS-mixture  and the DACT . The DACT method consists of two variants: DACT (Efron) and 

[MISSING_PAGE_FAIL:8]

To assess how the four methods are affected by effect size under sparse alternatives \((_{11}=0.05)\) and dense alternatives \((_{11}=0.2)\), we apply the four methods in Examples 1-2. The effect size, \(\), is varied from 0.6 to 1.2 in both examples. The results of estimated FDR and power are summarized in Figure A1. For sparse alternatives in Example 1, AMDP and JS-mixture maintain stable FDR control at the nominal level across various effect sizes. DACT (Efron) consistently controls the FDR but can be overly conservative, leading to potential under-identification of significant signals. Moreover, the FDR level of DACT (JC) exhibits inflation under weak effects. In terms of power analysis, AMDP emerges as the top performer, consistently outperforming the other three methods in Example 1. JS-mixture ranks second when the effect is strong, while DACT (Efron) lags behind due to its conservative behavior. For dense alternatives in Example 2, DACT (JC) fails to effectively control the FDR, leading to substantially higher FDR than the nominal level. While DACT (Efron) still exhibits overly conservative behavior. However, AMDP and JS-mixture maintain stable FDR levels across different effect sizes in Example 2, highlighting their robustness in controlling FDR. Regarding power analysis in Example 2, AMDP consistently outperforms the other methods, demonstrating its ability to handle scenarios with a substantial proportion of \(H_{01}\) and \(H_{10}\) while still achieving high power. JS-mixture ranks second in terms of power. It is noteworthy that in certain settings of Example 2, DACT (JC) may exhibit higher power than AMDP. Nevertheless, this higher power is often associated with severely inflated FDR levels. The conservation of DACT (Efron) results in lower power compared to the other three methods.

Next, we move on to investigate whether the four methods are sensitive to changes in the large mediator size \(J\) under both sparse and dense alternative scenarios. Panel (a) of Figure A2 displays the FDR and power performance of the four methods under Examples 3-4. In the sparse alternatives scenario of Example 3, AMDP and JS-mixture demonstrate remarkable stability in controlling FDR at the nominal level across different values of \(J\). DACT (Efron) exhibits conservative FDR control. While DACT (JC) is less conservative than DACT (Efron), it remains underpowered. In terms of power analysis, AMDP and JS-mixture are the leading methods. DACT (JC) demonstrates higher power when \(J\) is not very large, but its power decreases as \(J\) grows. DACT (Efron) consistently displays lower power in all settings due to its conservative behavior. In the dense alternative scenario of Example 4, DACT (JC) suffers from inflated FDR. In contrast, the FDRs of the other three methods are under control with varying mediator sizes, though DACT (Efron) continues to be overly conservative. Moreover, AMDP consistently delivers the highest power among all methods in the dense alternative scenario. We note that JS-mixture performs competitively in terms of power. On the other hand, the power of DACT (JC) decreases with the growth of \(J\), which raises concerns about its ability to detect true positives accurately in scenarios with larger mediator sizes. DACT (Efron) consistently displays lower power in all settings.

In Examples 5-6, we explore the influence of sample size on FDR and power performance under sparse and dense alternatives, respectively. The results of these analyses are presented in panel (b) of Figure A2. In Example 5, where a small proportion of alternative hypotheses is considered, AMDP and JS-mixture stand out as more accurate and stable in controlling the FDR among all methods. When the sample size is small, DACT (JC) exhibits a slightly higher FDR compared to AMDP and JS-mixture. DACT (Efron) remains overly conservative. The power of the four approaches initially decreases and then increases with the growth of \(n\). Specially, for all four methods, the lowest power is achieved at \(n=800\) among all the tested sample sizes. Moreover, AMDP consistently delivers reasonably higher power compared to the other three methods. In the dense alternatives of Example 6, DACT (JC) encounters challenges in maintaining FDR control, particularly when \(n\) is small. In contrast, AMDP, JS-mixture, and DACT (Efron) effectively control the FDR across different settings. Regarding power performance, AMDP, JS-mixture, and DACT (Efron) demonstrate a consistent increase in power as \(n\) grows. In some settings, DACT (JC) appears to perform better than AMDP. Nevertheless, this seemingly higher power of DACT (JC) is a result of the severely inflated FDR levels. We note that the consistent superiority of AMDP in both FDR control and power, as observed in Examples 5 and 6, aligns with the theoretical results presented in Theorem 1.

## 4 Data Analysis

Prostate cancer is a prevalent disease among men, with a multifactorial etiology involving genetic, environmental, and lifestyle factors. There is a growing recognition that DNA methylation plays an important role in regulating gene expression . Additionally, the number of GWASs-identified risk SNP that influence DNA methylation levels in prostate cancer has reached a total of 167 . Despite significant progress in understanding the role of DNA methylation in gene expression regulation and identifying prostate cancer risk SNPs, further research is strongly encouraged to uncover the specific CpG sites that contribute to the regulatory effects of risk SNPs on their target genes.

We apply our proposed AMDP, JS-mixture , and DACT , including DACT (Efron) and DACT (JC), to analyze the TCGA prostate cancer dataset. The dataset is freely available at [https://portal.gdc.cancer.gov](https://portal.gdc.cancer.gov). Our analysis focuses on 495 primary prostate tumor samples with information on 147 prostate cancer risk SNPs, DNA methylation, and gene expression. In total, we consider 69,602 CpG methylation probes (\(M\)) as potential mediators. The risk SNPs are the exposure variable (\(X\)), and gene expression is the outcome variable of interest (\(Y\)). The primary objective of our analysis is to explore the potential causal role of CpG methylation in the association between prostate cancer risk SNPs and gene expression. We estimate the null proportions as \(_{00}=0.52\), \(_{10}=0.03\), and \(_{01}=0.42\), respectively. Figure A2 of the Appendix C displays the number of significant triplets (\(X-M-Y\)) detected by AMDP, JS-mixture, DACT (Efron), and DACT (JC) at different nominal FDR levels \(\) ranging from \(0.01\) to \(0.1\). It can be seen that, in the majority of cases, AMDP outperforms the other three methods by identifying more triplets at the same FDR level. On average, the discoveries made by AMDP are approximately \(20.2\%\) higher than those of JS-mixture, \(86.6\%\) higher than those of DACT (Efron), and \(42.6\%\) higher than those of DACT (JC), across the range of \(\) values from \(0.01\) to \(0.1\). This substantial improvement in performance highlights the effectiveness of AMDP in identifying non-zero mediation effects in the prostate cancer dataset.

Additional results of Section 4 are postponed to the Appendix C.

## 5 Discussion

In this paper, we develop a novel adaptive mediation detection procedure (AMDP) to identify significant mediators in high-dimensional mediation analysis. The novel approach determines the optimal ranking for hypotheses, and then employs a data-driven strategy to select the threshold for mediator identification. We demonstrate the effectiveness of our proposed method through theoretical analysis and simulation results. There is a potential avenue for future research. We discuss the mediation effect based on the marginal model in this paper, where the p-values are independent. How to further study relevant mediators from two aspects of theory and application is an interesting topic.

**Limitation** Our approach effectively handles high-dimensional mediators but may not perform optimally when confronted with low-dimensional mediators. This distinction is attributed to the nature of our method, wherein the two-dimensional p-values linked to each exposure-mediator-outcome relationship effectively serve as "samples" for the estimation of local FDR and FDP. Consequently, the reduction in dimensionality can lead to less precise estimates of local FDR and FDP.