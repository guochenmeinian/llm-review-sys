# Demystifying amortized causal discovery with transformers

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Supervised learning approaches for causal discovery from observational data often achieve competitive performance despite seemingly avoiding explicit assumptions that traditional methods make for identifiability. In this work, we investigate CSIvA , a transformer-based model promising to train on synthetic data and transfer to real data. First, we bridge the gap with existing identifiability theory and show that constraints on the training data distribution implicitly define a prior on the test observations. Consistent with classical approaches, good performance is achieved when we have a good prior on the test data, and the underlying model is identifiable. At the same time, we find new trade-offs. Training on datasets generated from different classes of causal models, unambiguously identifiable in isolation, improves the test generalization. Performance is still guaranteed, as the ambiguous cases resulting from the mixture of identifiable causal models are unlikely to occur (which we formally prove). Overall, our study finds that amortized causal discovery still needs to obey identifiability theory, but it also differs from classical methods in how the assumptions are formulated, trading more reliance on assumptions on the noise type for fewer hypotheses on the mechanisms.

## 1 Introduction

Causal discovery aims to uncover the underlying causal relationships between variables of a system from pure observations, which is crucial for answering interventional and counterfactual queries when experimentation is impractical or unfeasible . Unfortunately, causal discovery is inherently ill-posed : unique identification of causal directions requires restrictive assumptions on the class of structural causal models (SCMs) that generated the data . These theoretical limitations often render existing methods inapplicable, as the underlying assumptions are usually untestable or difficult to verify in practice .

Recently, supervised learning algorithms trained on synthetic data have been proposed to overcome the need for specific hypotheses, which restrains the application of classical causal discovery methods to real-world problems . Seminal work from Lopez-Paz et al.  argues that this learning-based approach to causal discovery would allow dealing with complex data-generating processes and would greatly reduce the need for explicitly crafting identifiability conditions a-priori: despite this ambitious goal, the output of these methods is generally considered unreliable, as no theoretical guarantee is provided. A pair of non-identifiable structural causal models can be associated with different causal graphs \(}\), while entailing the same joint distribution \(p\) on the system's variables. It is thus unclear how a learning algorithm presented with observational data generated from \(p\) would be able to overcome these theoretical limits and correctly identify a unique causal structure. However, the available empirical evidence seems not to care about impossibility results, as these methods yield surprising generalization results on several synthetic benchmarks. Our work aims to bridge this gap by studying the performance of a transformer architecture for causal discovery throughthe lens of the theory of identifiability from observational data. Specifically, we analyze the CSIvA (Causal Structure Induction via Attention) model for causal discovery , focusing on bivariate graphs, as they offer a controlled yet non-trivial setting for the investigation. As our starting point, we provide closed-form examples that identify the limitations of CSIvA in recovering causal structures of linear non-Gaussian and nonlinear additive noise models, which are notably identifiable, and demonstrate the expected failures through empirical evidence. These findings suggest that the class of structural causal models that can be identified by CSIvA is inherently dependent on the specific class of SCMs observed during training. Thus, the need for restrictive hypotheses on the data-generating process is intrinsic to causal discovery, both in the traditional and modern learning-based approaches: assumptions on the test distribution either are posited when selecting the algorithm (traditional methods) or in the choice of the training data (learning-based methods). To address this limitation, we theoretically and empirically analyze _when_ training CSIvA on datasets generated by multiple identifiable SCMs with different structural assumptions improves its generalization at test time. In summary:

* We show that the class of structural causal models that CSIvA can identify is defined by the class of SCMs observed through samples during the training. We reinforce the notion that identifiability in causal discovery inherently requires assumptions, which must be encoded in the training data in the case of learning-based approaches.
* To overcome this limitation, we study the benefits of CSIvA training on mixtures of causal models. We analyze when algorithms learned on multiple models are expected to identify broad classes of SCMs (unlike many classical methods). Empirically, we show that training on samples generated by multiple identifiable causal models with different assumptions on mechanisms and noise distribution results in significantly improved generalization abilities.

Closely related works and their relation with CSIvA.In this paper, we study _amortized inference of causal graphs_, i.e. optimization of an inference model to directly predict a causal structure from newly provided data. This is the first work that attempts to understand the connection between identifiability theory and amortized inference, while several algorithms have been proposed. In the context of purely observational data, Lopez-Paz et al.  defines a distribution regression problem  mapping the kernel mean embedding of the data distribution to a causal graph, while Li et al.  relies on equivariant neural network architectures. More recently, Lippe et al.  and Lorch et al.  proposed learning on interventional data, in addition to observations (in the same spirit as CSIvA). Despite different algorithmic implementations, the target object of estimation of most of these methods is the distribution over the space of all possible graphs, conditional on the input dataset (similarly, the ENCO algorithm in Lippe et al.  models the conditional distribution of individual edges). This justifies our choice of restricting our study to the CSIvA architecture (despite this being a clear limitation), as in the infinite observational sample limit, these methods approximate the same distribution. Methods necessarily requiring interventional data [15; 16; 17], and learning-based algorithms unsuitable for amortized inference [18; 19; 20; 21; 22] are out of the scope of this work.

## 2 Background and motivation

We start introducing structural causal models (SCMs), an intuitive framework that formalizes causal relations. Let \(X\) be a set of random variables in \(\) defined according to the set of structural equations:

\[X_{i} f_{i}(X_{^{}_{i}},N_{i}),\ \  i=1, ,k. \]

\(N_{i}\) are _noise_ random variables. The function \(f_{i}\) is the _causal mechanism_ mapping the set of _direct causes_\(X_{^{}_{i}}\) of \(X_{i}\) and the noise term \(N_{i}\), to \(X_{i}\)'s value. The _causal graph_\(\) is a directed acyclic graph (DAG) with nodes \(X=\{X_{1},,X_{k}\}\), and edges \(\{X_{j} X_{i}:X_{j} X_{^{}_{i}}\}\), with \(^{}_{i}\) indices of the parent nodes of \(X_{i}\) in \(\). The causal model induces a density \(p_{X}\) over the vector \(X\).

### Causal discovery from observational data

Causal discovery from observational data is the inference of the causal graph \(\) from a dataset of i.i.d. observations of the random vector \(X\). In general, without restrictive assumptions on the mechanisms and the noise distributions, the direction of edges in the graph \(\) is not identifiable, i.e. it can not be found from the population density \(p_{X}\). In particular, it is possible to identify only a Markov equivalence class, which is the set of graphs encoding the same conditional independencies as the density \(p_{X}\). To clarify with an example, consider the causal graph \(X_{1} X_{2}\) associated with a structural causal model inducing a density \(p_{X_{1},X_{2}}\). If the model is not identifiable, there exists an SCM with causal graph \(X_{2} X_{1}\) that entails the same joint density \(p_{X_{1},X_{2}}\). The set \(\{X_{1} X_{2},X_{2} X_{1}\}\) is the Markov equivalence class of the graph \(X_{1} X_{2}\), i.e. the set of all graphs with \(X_{1},X_{2}\) mutually dependent. Clearly, in this setting, even the exact knowledge of \(p_{X_{1},X_{2}}\) cannot inform us about the correct causal direction.

**Definition 1** (Identifiable causal model).: Consider a structural causal model with underlying graph \(\) and \(p_{X}\) joint density of the causal variables. We say that the model is _identifiable_ from observational data if the density \(p_{X}\) can not be entailed by a structural causal model with graph \(}\).

We define the _post-additive noise model_ (post-ANM) as the causal model with the set of equations:

\[X_{i} f_{2,i}(f_{1,i}(X_{^{}_{i}})+N_{i}),\  i=1,,d, \]

with \(f_{2,i}\) invertible map and mutually independent noise terms. When \(f_{2,i}\) is a nonlinear function, the post-ANM amounts to the identifiable _post-nonlinear_ model (PNL) . When \(f_{2,i}\) is the identity function and \(f_{1,i}\) nonlinear, it simplifies to the nonlinear _additive noise model_ (ANM)[7; 23], which is known to be identifiable, and is described by the set of structural equations:

\[X_{i} f_{1,i}(X_{^{}_{i}})+N_{i}. \]

If, additionally, we restrict the mechanisms \(f_{1,i}\) to be linear and the noise terms \(N_{i}\) to a non-Gaussian distribution, we recover the identifiable _linear non-Gaussian additive model_ or LiNGAM :

\[X_{i}=_{j^{}_{i}}_{j}X_{j}+N_{i}, _{j}. \]

### Motivation and problem definition

Causal discovery from observational data relies on specific assumptions, which can be challenging to verify in practice . To address this, recent methods leverage supervised learning for the amortized inference of causal graphs [1; 10; 11; 12; 13; 16; 24], optimizing an inference model to directly predict a causal structure from a provided dataset. While these approaches aim to reduce reliance on explicit identifiability assumptions, they often lack a clear connection to the existing causal discovery theory, making their outputs generally unreliable. We illustrate this limitation through an example.

**Example 1**.: We consider the CSIvA transformer architecture proposed by Ke et al. , which can learn a map from observational data to a causal graph. The authors of the paper show that, in the infinite sample regime, the CSIvA architecture exactly approximates the conditional distribution \(p(|)\) over the space of possible graphs, given a dataset \(\). Identifiability theory in causal discovery tells us that if the class of structural causal models that generated the observations is sufficiently constrained, then there is only one graph that can fit the data within that class. For example, consider the case of a dataset that is known to be generated by a nonlinear additive noise model, and let \(p(|,)\) be the conditional distribution that incorporates this prior knowledge on the SCM: then \(p(|,)\) concentrates all the mass on a single point \(^{*}\), the true graph underlying the \(\) observations. Instead, in the absence of restrictions on the structural causal model, all the graphs in a Markov equivalence class are equally likely to be the correct solution given the data. Hence, \(p(|)\), the distribution learned by CSIvA, assigns equal probability to each graph in the Markov equivalence class of \(^{*}\).

Our arguments of Example 1 are valid for all learning methods that approximate the conditional distribution over the space of graphs given the input data [1; 10; 11; 12; 13], and suggest that these algorithms are at most informative about the equivalence class of the causal graph underlying the observations. However, the available empirical evidence does not seem to highlight these limitations, as in practice these methods can infer the true causal DAG on several synthetic benchmarks. Thus, further investigation is necessary if we want to rely on their output in any meaningful sense. In this work, we analyze these "black-box" approaches through the lens of established theory of causal discovery from observational data (causal inference often lacks experimental data, which we do not consider). We study in detail the CSIvA architecture  (see Appendix A), a variation of the transformer neural network  for the supervised learning of algorithms for amortized causal discovery. This model is optimized via maximum likelihood estimation, i.e. finding \(\) that minimizes \(-_{,}[(|; )]\)where \((|;)\) is the conditional distribution of a graph \(\) given a dataset \(\) parametrized by \(\). We limit the analysis to CSIvA as it is a simple yet competitive end-to-end approach to learning causal models. While this is clearly a limitation of the paper, our theoretical and empirical conclusions exemplify both the role of theoretical identifiability in modern approaches and the new opportunities they provide. Additionally, it fits well within a line of works arguing that specifically transformers can learn causal concepts [26; 27; 28] and identify different assumptions in context .

## 3 Experimental results through the lens of theory

In this section, we present a comprehensive analysis of causal discovery with transformers and its relation to the theoretical boundaries of causal discovery from observational data. We show that suitable assumptions must be encoded in the training distribution to ensure the identifiability of the test data, and we additionally study the effectiveness of training on mixtures of causal models to overcome these limitations, improving generalization abilities.

### Experimental design

We concentrate our research on causal models of two variables, causally related according to one of the two graphs \(X Y\), \(Y X\). Bivariate models are the simplest non-trivial setting with a well-known theory of causality inference [7; 8; 23], but also amenable to manipulation. This allows for comprehensive training and analysis of diverse SCMs and facilitates a clear interpretation of the results.

Datasets.Unless otherwise specified, in our experiments we train CSIvA on a sample of \(15000\) synthetically generated datasets, consisting of \(1500\) i.i.d. observations. Each dataset is generated according to a single class of SCMs, defined by the mechanism type and the noise terms distribution. The coefficients of the linear mechanisms are sampled in the range \([-3,-0.5][0.5,3]\), removing small coefficients to avoid _close-to-unfaithful_ effects . Nonlinear mechanisms are parametrized according to a neural network with random weights, a strategy commonly adopted in the literature of causal discovery [1; 9]. The post-nonlinearity of the PNL model consists of a simple map \(z z^{3}\). Noise terms are sampled from common distributions and a randomly generated density that we call _mlp_, previously adopted in Montagna et al. , defined by a standard Gaussian transformed by a multilayer perceptron (MLP) (Appendix B.2). We name these datasets _mechanism-noise_ to refer to their underlying causal model. For example, data sampled from a nonlinear ANM with Gaussian noise are named _nonlinear-gaussian_. More details on the synthetic data generation schema are found in Appendix B.2. All data are standardized by their empirical variance to remove opportunities to learn shortcuts [31; 32; 33].

Metric and random baseline.As our metric we use the structural Hamming distance (SHD), which is the number of edge removals, insertions or flips to transform one graph to another. In the context of bivariate causal graphs with a single edge, this is simply an error count, so correct inference corresponds to \(=0\), and an incorrect prediction gives \(=1\). Additionally, we define a reference random baseline, which assigns a causal direction according to a fair coin, achieving \(=0.5\) in expectation. Each architecture we analyze in the experiments is trained \(3\) times, with different parameter initialization and training samples: the SHD presented in the plots is the average of each of the \(3\) models on \(1500\) distinct test datasets of \(1500\) points each, and the error bars are \(95\%\) confidence intervals.

We detail the training hyperparameters in Appendix B.1. Next, we analyze our experimental results, starting by investigating how well CSIvA generalizes on distributions unseen during training.

### Warm up: is CSIvA capable of in and out-of-distribution generalization?

In-distribution generalization.First, we investigate the generalization of CSIvA on datasets sampled from the structural casual model that generates the train distribution, with mechanisms and noise distributions fixed between training and testing. We call this _in-distribution generalization_. As a benchmark, we present the performance of several state-of-the-art approaches from the literature on causal discovery: we consider the DirectLiNGAM, and NoGAM algorithms [34; 35], respectively designed for the inference on LiNGAM and nonlinear ANM generated data1. The results of Figure 1show that CSIvA can properly generalize to unseen samples from the training distribution: the majority of the trained models present SHD close to zero and comparable to the relative benchmark algorithm.

Out-of-distribution generalization.In practice, we generally do not know the SCM defining the test distribution, so we are interested in CSIvA's ability to generalize to data sampled from a class of causal models that is unobserved during training. We call this _out-of-distribution generalization_ (OOD). We study OOD generalization to different noise terms, analyzing the network performance on datasets generated from causal models where the mechanisms are fixed with respect to the training, while the noise distribution varies (e.g., given linear-mlp training samples, testing occurs on linear-uniform data). Orthogonally to these experiments, we empirically validate CSIvA's OOD generalization over different mechanism types (linear, nonlinear, post-nonlinear), while leaving the noise distribution (mlp) fixed across test and training. In Figure 1(a), we observe that CSIvA cannot generalize across the different mechanisms, as the SHD of a network tested on unseen causal mechanisms approximates that of the random baseline. Further, Figure 1(b) shows that out-of-distribution generalization across noise terms does not work reliably, and it is hard to predict when it might occur.

Implications.CSIVA generalizes well to test data generated by the same class of SCMs used for training, in line with the findings in Ke et al. , which validates our implementation and training procedure. However, it struggles when the test data are out-of-distribution, not generated by causal models with the _same mechanisms and noise terms_ it was trained on. While training on a wider class of SCMs might overcome this limitation, it requires caution. The identifiability of causal graphs indeed results from the interplay between the data-generating mechanisms and noise distribution. However, as we argue in our Example 1, the class of causal models that a supervised learning algorithm can identify is generally not clear. In what follows, we investigate this point and its implications for CSIvA, showing that the identifiability of the test samples can be ensured by imposing suitable assumptions on the class of SCMs generating the training distribution.

### How does CSIvA relate to identifiability theory for causal graphs?

The CSIvA algorithm does not make structural assumptions about the causal model underlying the input data. This implies that the output of this method is unclear: as CSIvA targets the conditional distribution \(p(|)\) over the space of graphs, in the absence of restrictions on the functional mechanisms

Figure 1: In-distribution generalization of CSIvA trained and tested on data generated according to the same structural causal models, fixing mechanisms, and noise distributions between training and testing). As baselines for comparison, we use DirectLiNGAM on linear SCMs and NoGAM on nonlinear ANM (we use their causal-learn and dodiscover implementations). CSIvA performance is clearly non-trivial and generalizing well.

and the distribution of the noise terms, the causal graph \(X Y\) is indistinguishable from \(Y X\), as they are both equally likely to underlie the joint density \(p_{X,Y}\) generating the data. As we discuss in Example 1, the graphical output of the trained architecture could at most identify the equivalence class of the true causal graph. Yet, our experiments of Section 3.2 show that CSIvA is capable of good in-distribution generalization, often inferring the correct DAG at test time. We explain this seeming contradiction with the following hypothesis, which motivates the analysis in the remainder of this section.

**Hypothesis 1**.: _The class of structural causal models that can be identified by CSIvA is defined by the class of structural causal models underlying the generation of the training data._

To support and clarify our statement, we present the following example, adapted from Hoyer et al. .

**Example 2**.: Consider the causal model \(Y=f(X)+N,\) where \(f(X)=-X\) and \(p_{X},p_{N}\) are Gumbel densities \(p_{X}(x)=(-x-(-x))\) and \(p_{N}(n)=(-n-(-n))\). This model satisfies the assumptions of the LiNGAM, so it is identifiable, in the sense that a backward linear model with the same distribution does not exist. However, in this special case, we can build a backward nonlinear additive noise model \(X=g(Y)+\) with independent noise terms: taking \(p_{Y}(y)=(-y-2(1+(-y)))\) to be the density of a logistic distribution, \(p_{}()=(-2-(-))\) and \(g(y)=(1+(-y))\); we see that \(p_{X,Y}\) can factorize according to two opposite causal directions, as \(p_{X,Y}(x,y)=p_{N}(y-f(x))p_{X}(x)=p_{}(x-g(y))p_{Y}(y)\). Given a dataset \(\) of observations from the forward linear model, causal discovery methods like DirectLiNGAM  can provably identify the correct causal direction \(X Y\), assuming that sufficient samples are provided. Instead, the behavior of CSIvA seems hard to predict: given that the network approximates the conditional distribution \(p(|)\) over the possible graphs, for \(\) with arbitrary many samples we have \(p(X Y|)=p(Y X|)=0.5\). On the other hand, given the prior knowledge that the data-generating SCM is a linear non-gaussian additive noise model, we have \(p(X Y|,)=1\), because the LiNGAM is identifiable. In this sense, the class of structural causal models that CSIvA correctly infers appears to be determined by the structural causal models underlying the generation of the training data. Under our Hypothesis 1, training CSIvA exclusively on LiNGAM-generated data is equivalent to learning the distribution \(p(|,)\), such that the network should be able to identify the forward linear model, whereas it could only infer the equivalence class of the causal graph if its training datasets include observations from a nonlinear additive noise model.

The empirical results of Figure 2(a) show that CSIvA behaves according to our hypothesis: when training exclusively occurs on datasets \(\{_{i},\}_{i}\) generated by the _forward linear-gumbel model_ of Example 2, the network can identify the causal direction of test data generated according to the same SCM. Similarly, the transformer trained on datasets \(\{_{i},\}_{i}\) from the _backward nonlinear model_ of the example can generalize to test data coming from the same distribution. According to our claim, instead, the network that is trained on the union of the training samples \(\{_{i},\}_{i}\{_{i},\}_{i}\) from the forward and backward models _(50:50_ ratio in Figure 2(a))_ displays the same test SHD (around \(0.5\)) as a random classifier assigning the causal direction with equal probability.

Figure 2: Out-of-distribution generalisation. We train three CSIvA models on data sampled from SCMs with linear, nonlinear additive, and post-nonlinear mechanisms; and noise fixed _mlp_ noise distribution. In Figure (a) we test across different noise distributions, with test mechanisms fixed from training. In Figure (b) we test each network on different mechanisms and fixed mlp noise. CSIvA struggles to generalize to unseen causal mechanisms and often displays degraded performance over new noise distributions.

Further, we investigate CSIvA's relation with known identifiability theory by training and testing the architecture on data from a linear Gaussian model, which is well-known to be unidentifiable. Not surprisingly, the results of Figure 2(b) show that none of the algorithms that we learn can infer the causal order of linear Gaussian models with test SHD any better than a random baseline.

Implications.Our experiments show that CSIvA learns algorithms that closely follow identifiability theory for causal discovery. In particular, while the method itself does not require explicit assumptions on the data-generating process, the chosen training data ultimately determines the class of causal models identifiable during inference. Notably, previous work has argued that supervised learning approaches in causal discovery would help with "dealing with complex data-generating processes and greatly reduce the need of explicitly crafting identifiability conditions a-priori", Lopez-Paz et al. . In the case of CSIvA, this expectation does not appear to be fulfilled, as the assumptions still need to be encoded explicitly in the training data. However, this observation opens two new important questions: (1) Can we train a single network to encompass multiple (or even all) identifiable causal structures? (2) How much ambiguity might exist between these identifiable models?

### A _low-dimensions_ argument in favor of learning from multiple causal models

Example 2 of the previous section shows that elements of distinct classes of identifiable structural causal models, such as LiNGAM and nonlinear ANM, may become non-identifiable when we consider their union. In this section, we show that in the class of post-additive noise models given by equation (2) (obtained as the union of the LiNGAM, the nonlinear ANM, and the post-nonlinear model), the set of distributions that is non-identifiable is negligible. Our proposition extends the results of Hoyer et al. , which are limited to the case of linear and nonlinear additive noise models, and Zhang and Hyvarinen , which provides the conditions of identifiability of the post-ANM without bounding the set of non-identifiable distributions.

Let \(X,Y\) be a pair of random variables generated according to the causal direction \(X Y\) and the post-additive noise model structural equation:

\[Y=f_{2}(f_{1}(X)+N_{Y}), \]

where \(N_{Y}\) and \(X\) are independent random variables, and \(f_{2}\) is invertible. If the SCM is non-identifiable, the data-generating process can be described by a _backward_ model with the structural equation:

\[X=g_{2}(g_{1}(Y)+N_{X}), \]

\(N_{X}\) independent from \(Y\), and \(g_{2}\) invertible. We introduce the random variables \(,\), such that the forward and backward equations can be rewritten as

\[Y=f_{2}(), f_{1}(X)+N_{Y},\] \[X=g_{2}(), g_{1}(Y)+N_{X}.\]

Figure 3: Experiments on identifiability theory. In Figure (a) we test the performance on linear-Gaussian data. Models are trained with different ratios of samples from linear and nonlinear SCMs with Gaussian noise terms. The validation results showcase that the networks were trained successfully. Figure (b) shows the SHD of models trained on different ratios of _linear_ and _nonlinear invertible_ data of Example 2. CSIvA behaves according to identifiability theory, failing to predict on linear Gaussian models and _invertible_ data (50:50 ratio).

We note that this implies that the following invertible additive noise models on \(,\) hold:

\[ =h_{Y}()+N_{Y}, h_{Y} f_{1} g_{2}, \] \[ =h_{X}()+N_{X}, h_{X} g_{1} f_{2}. \]

**Proposition 1** (Adapted from Hoyer et al. ).: _Let \(p_{N_{Y}},h_{X},h_{Y}\) be fixed, and define \(_{Y} p_{N_{Y}}\), \( p_{}\). Suppose that \(p_{N_{Y}}\) and \(p_{}\) are strictly positive densities, and that \(_{Y},,f_{1},f_{2},g_{1},\) and \(g_{2}\) are three times differentiable. Further, assume that for a fixed pair \(h_{Y},_{Y}\) exists \(\) s.t. \(_{Y}^{}(-h_{Y}())h_{Y}^{}() 0\) is satisfied for all but a countable set of points \(\). Then, the set of all densities \(p_{}\) of \(\) such that both equations (5) and (6) are satisfied is contained in a 2-dimensional space._

Implications.Our result is closely related to Theorem 1 of Hoyer et al. , which we simply generalize to the post-ANM. Intuitively, it says that the space of all continuous distributions such that the bivariate post-ANM is non-identifiable is contained in a 2-dimensional space. As the space of continuous distributions of random variables is infinite-dimensional, we conclude that the post-ANM is generally identifiable, which suggests that the setting of Example 2 is rather artificial. Our results provide a theoretical ground for training causal discovery algorithms on datasets generated from multiple identifiable SCMs. This is particularly appealing in the case of CSIvA, given the poor OOD generalization ability observed in our experiments of Section 3.2.

### Can we train CSIvA on multiple causal models for better generalization?

In this section, we investigate the benefits of training over multiple causal models, i.e. on samples generated by a combination of classes of identifiable SCMs characterized by different mechanisms and noise terms distribution. Our motivation is as follows: given that our empirical evidence shows that CSIvA is capable of in-distribution generalization, whereas dramatically degrades the performance when testing occurs out-of-distribution, it is thus desirable to increase the class of causal models represented in the training datasets. We separately study the effects of training over multiple mechanisms and multiple noise distributions and compare the testing performance against architectures trained on samples of a single SCM.

Mixture of causal mechanisms.We consider four networks optimized by training of CSIvA on datasets generated from pairs (or triples) of distinct SCMs, with fixed _mlp_ noise and which differ in terms of their mechanisms type: linear and nonlinear; nonlinear and post-nonlinear; linear and post-nonlinear; linear, nonlinear and post-nonlinear. The number of training datasets for each architecture is fixed (\(15000\)) and equally split between the causal models with different mechanism types. The results of Figure 4 show that the networks trained on mixtures of mechanisms all present significantly better test SHD compared to CSIvA models trained on a single mechanism type. We find that learning on multiple SCMs improves the SHD from \(\!0.5\) to \(\!0.2\) both on linear and nonlinear test data (Figures 3(a) and 3(b)), and even better accuracy is achieved on post-nonlinear samples, as shown in Figure 3(c).

Figure 4: Mixture of causal mechanisms. We train four models on samples from structural casual models with different mechanism types. We compare their test SHD (the lower, the better) against networks trained on datasets generated according to a single type of mechanism. The dashed line indicates the test SHD of a model trained on samples with the same mechanisms as test SCM. Training on multiple causal models with different mechanisms (_mixed_ bars) always improves performance compared to training on single SCMs.

Mixture of noise distributions.Next, we analyze the test performance of three CSIvA networks optimized on samples from structural causal models that have different distributions for their noise terms, while keeping the mechanism types fixed. Figure 5 shows that training over different noises (beta, gamma, gumbel, exponential, mlp, uniform) always results in a network that is agnostic with respect to the noise distributions of the SCM generating the test samples, always achieving \(<0.1\), with the exception of datasets with mlp error terms (\(0.2\) average SHD on nonlinear and pnl data).

Implications.We have shown that learning on mixtures of SCMs with different noise term distributions and mechanism types leads to models generalizing to a much broader class of structural causal models during testing. Hence, combining datasets generated from multiple models looks like a promising framework to overcome the limited out-of-distribution generalization abilities of CSIvA observed in Section 3.2. However, it is easier to incorporate prior assumptions on the class of causal mechanisms (linear, non-linear, post-non-linear) compared to the noise distributions (which are potentially infinite). This introduces a trade-off between amortized inference and classical methods for causal discovery: for example, RESIT, NoGAM, and CAM  algorithms require no assumptions on the noise type, but only work for a limited class of mechanisms (nonlinear).

## 4 Conclusion

In this work, we investigate the interplay between identifiability theory and supervised learning for amortized inference of causal graphs, using CSIvA as the ground of our study. Consistent with classical algorithms, we demonstrate that good performance can be achieved if (i) we have a good prior on the structural causal model generating the test data (ii) the setting is identifiable. In particular, prior knowledge of the test distribution is encoded in the training data in the form of constraints on the structural causal model underlying their generation. With these results, we highlight the need for identifiability theory in modern learning-based approaches to causality, while past works have mostly disregarded this connection. Further, our findings provide the theoretical ground for training on observations sampled from multiple classes of identifiable SCMs, a strategy that improves test generalization to a broad class of causal models. Finally, we highlight an interesting new trade-off regarding identifiability: traditional methods like LiNGAM, RESIT, and PNL require strong restrictions on the structural mechanisms underlying the data generation (linear, nonlinear or post-nonlinear) while generally being agnostic relative to the noise terms distribution. Training on mixtures of causal models instead offers an alternative that is less reliant on assumptions on the mechanisms, while incorporating knowledge about all possible noise distributions in the training data is practically impossible to achieve. We leave it to future work to reproduce our analysis on a wider class of architectures, as well as extending our study to interventional data with more than two nodes.

Figure 5: Mixture of noise distributions. We train three networks on samples from SCMs with different noise terms distributions and fixed mechanism types: linear, nonlinear, and post-nonlinear. We present their test SHD (the lower, the better) on data from SCMs with the mechanisms fixed with respect to training, and noise terms changing between each dataset. Training on multiple causal models with different noises (_all distributions_ bars) always improves performance compared to training on single SCMs with fixed mlp noise (_only mlp_ bars).