# Reward Machines for Deep RL

in Noisy and Uncertain Environments

 Andrew C. Li

University of Toronto

Vector Institute

&Zizhao Chen

Correspond University

&Toryn Q. Klassen

University of Toronto

Vector Institute

&Pashootan Vaezipoor

Georgian.io

Vector Institute

&Rodrigo Toro Icarte

Pontificia Universidad Catolica de Chile

Centro Nacional de Inteligencia Artificial

&Sheila A. McIlraith

University of Toronto

Vector Institute

###### Abstract

Reward Machines provide an automaton-inspired structure for specifying instructions, safety constraints, and other temporally extended reward-worthy behaviour. By exposing the underlying structure of a reward function, they enable the decomposition of an RL task, leading to impressive gains in sample efficiency. Although Reward Machines and similar formal specifications have a rich history of application towards sequential decision-making problems, prior frameworks have traditionally ignored ambiguity and uncertainty when interpreting the domain-specific vocabulary forming the building blocks of the reward function. Such uncertainty critically arises in many real-world settings due to factors like partial observability or noisy sensors. In this work, we explore the use of Reward Machines for Deep RL in noisy and uncertain environments. We characterize this problem as a POMDP and propose a suite of RL algorithms that exploit task structure under uncertain interpretation of the domain-specific vocabulary. Through theory and experiments, we expose pitfalls in naive approaches to this problem while simultaneously demonstrating how task structure can be successfully leveraged under noisy interpretations of the vocabulary.

**Code and videos are available at [https://github.com/andrewli77/reward-machines-noisy-environments](https://github.com/andrewli77/reward-machines-noisy-environments).**

## 1 Introduction

Formal languages, including programming languages such as Python and C, have long been used for objective specification in sequential decision making. Using a vocabulary of domain-specific properties, expressed as propositional variables, formal languages like Linear Temporal Logic (LTL)  capture complex temporal patterns--such as the objectives of an agent--by composing variables via temporal operators and logical connectives. These languages provide well-defined semantics while enabling semantics-preserving transformations to normal-form representations such as automata, which can expose the discrete structure underlying an objective to a decision-making agent. One such representation is the increasingly popular Reward Machine, which combines expression of rich temporally extended (non-Markovian) objectives via automata with algorithmic techniques such as automated reward shaping, task decomposition, and counterfactual learning updates to garnersignificant improvements in sample efficiency (e.g., [52; 54; 18]). Importantly, Reward Machines can be specified directly, constructed via translation from any regular language (including variants of LTL), synthesized from high-level planning specifications, or learned from data . For these reasons, formal languages, and in particular, Reward Machines, have been adopted across a diversity of domains, ranging from motion planning [30; 15; 50] and robotic manipulation [8; 26; 31], more recently, general deep Reinforcement Learning (RL) problems [e.g., 1; 34; 35; 37; 52; 53; 21; 64; 29; 61; 32; 22; 62; 63; 17; 46; 11; 28; 41; 49; 59; 12; 66; 8; 10; 38; 18; 48; 65; 58; 55].

Importantly, formal language frameworks for deep RL require an interpretation of the domain-specific vocabulary grounded in the RL environment. This is captured by a _labelling function_, a mapping from environment states to the truth or falsity of abstract propositions that constitute the building blocks of objective specifications. However, practical real-world environments are often partially observable and rely on high-dimensional sensor data such as images. As a result, labelling functions are, by necessity, noisy and uncertain, compromising the application of formal languages such as Reward Machines or LTL for objective specification. Consider an autonomous vehicle, whose desired behaviour at an intersection can be formally specified using temporal logic [5; 39]. In the real world, key determinations--whether a pedestrian is crossing, the colour of the light, the intent of other vehicles, and so on--must be made based on noisy or obstructed LiDAR and camera sensors, and may therefore be noisy or uncertain themselves. To address this problem, while benefiting from the advantages of Reward Machines, we make the following contributions.

**(1)** We propose a deep RL framework for Reward Machines in settings where the evaluation of domain-specific vocabulary is uncertain, characterizing the RL problem in terms of a POMDP. To our knowledge, this is the first deep RL framework for Reward Machines that broadly supports the imperfect detection of propositions, allowing us to extend Reward Machines to general partially observable environments.

**(2)** We propose and analyze a suite of RL algorithms that exploit Reward Machine structure under noisy and uncertain interpretations of the vocabulary. We show how preexisting _abstraction models_--noisy estimators of abstract, task-relevant features that may manifest as pretrained neural networks, sensors, heuristics, or otherwise--can be brought to bear to improve learning efficiency.

**(3)** We theoretically and experimentally evaluate our proposed RL algorithms. Theoretically, we discover a pitfall of naively leveraging standard abstraction models--namely, that errors from repeated queries of a model are correlated rather than i.i.d. We show that this can have serious ramifications, including unintended or dangerous outcomes, and demonstrate how this issue can be mitigated. Experimentally, we consider a variety of challenging domains involving partial observability and high-dimensional observations. Results show that our algorithms successfully leverage task structure to improve sample efficiency and total reward under uncertain interpretations of the vocabulary.

## 2 Background

**Notation.** Given a set of random variables, \(X\), \( X\) is the set of distributions over \(X\); \(\) denotes a particular distribution; and for a categorical distribution \( X\) and some \(x X\), we denote \([x]\) as the probability of \(x\) under \(\). We use \(x_{i:j}\) as a shorthand for the sequence \(x_{i},,x_{j}\).

**POMDPs.** A _Partially Observable Markov Decision Process_ (POMDP) \( S,O,A,P,R,,\) is defined by the state space \(S\), observation space \(O\), action space \(A\), reward function \(R\), transition distribution \(P:S A S\), observation distribution \(:S O\), and initial state distribution \( S\). An _episode_ begins with an initial state \(s_{1}()\) and at each timestep \(t 1\), the agent observes an observation \(o_{t}(s_{t})\), performs an action \(a_{t} A\), transitions to the next state \(s_{t+1} P(s_{t},a_{t})\), and receives reward \(r_{t}=R(s_{t},a_{t},s_{t+1})\). Denote the agent's observation-action history at time \(t\) by \(h_{t}=(o_{1},a_{1},,o_{t-1},a_{t-1},o_{t})\) and the set of all possible histories as \(H\). A fully observable _Markov Decision Process_ (MDP) serves as an important special case where the observation at each time \(t\) is \(o_{t}=s_{t}\).

**Reward Machines.** A _Reward Machine_ (RM)  is a formal automaton representation of a non-Markovian reward function that captures temporally extended behaviours. Formally, an RM \(= U,u_{1},F,,_{u},_{r}\), where \(U\) is a finite set of states, \(u_{1} U\) is the initial state, \(F\) is a finite set of terminal states (disjoint from \(U\)), \(\) is a finite set of atomic propositions representing the occurrence of salient events in the environment, \(_{u}:U 2^{}(U F)\) is the state-transitionfunction, and \(_{r}:U 2^{}\) is the state-reward function. Each transition in an RM is labelled with a scalar reward along with a propositional logic formula over \(\), while accepting states represent task termination.

**Labelling Function.** While an RM captures the high-level structure of a non-Markovian reward function, concrete rewards in an environment are determined with the help of a _labelling function_\(:S A S 2^{}\), a mapping that abstracts state transitions (\(s_{t-1},a_{t-1},s_{t}\)) in the environment to the subset of propositions that hold for that transition. To obtain rewards, the sequence of environment states and actions \(s_{1},a_{1},,s_{t},a_{t},s_{t+1}\) are labelled with propositional evaluations \(_{1:t}\), where \(_{i}=(s_{i},a_{i},s_{i+1}) 2^{}\). A sequence of transitions in the RM are then followed based on \(_{1:t}\) to produce the reward sequence \(r_{1:t}\). In an MDP environment, rewards specified by an RM are Markovian over an extended state space \(S U\); hence, there is an optimal policy of the form \((a_{t}|s_{t},u_{t})\) where \(u_{t} U\) is the RM state at time \(t\). During execution, \(u_{t}\) can be recursively updated given \(u_{t-1}\) by querying \(\) after each environment transition.

## 3 Problem Framework

### Formalization

We formalize the problem of solving an RM task under an uncertain interpretation of the vocabulary. Consider an agent acting in a POMDP environment (without the reward function) \(= S,O,A,P,,\). We define rewards \(r_{t}\) based on an RM \(= U,u_{1},F,,_{u},_{r}\) interpreted under a ground-truth labelling function \(:S A S 2^{}\) as described in Section 2.

An important aspect of our framework is that \(\) is not made accessible to the agent. Instead, we make the weaker assumption that the agent can query an _abstraction model_\(:H Z\). Here, \(\) captures the agent's preexisting knowledge over how a set of high-level features \(Z\) are grounded within the environment. Abstraction models are easier to obtain than ground-truth labelling functions for several reasons: they take as input observable histories \(H\) rather than states \(S\), they can map to any feature space \(Z\), and crucially, we allow their outcomes to be incorrect or uncertain. Note that the definition of abstraction models is quite general--in the real world, they might manifest as pretrained foundation models [45; 36; 4; 14], sensors , task-specific classifiers , or so on.

We refer to the tuple \(=,,,\) as a _Noisy Reward Machine Environment_ (depicted in Figure 1). Given \(\), our goal is to obtain a policy \((a_{t}|h_{t},z_{1:t})\) based on observations and outputs from the abstraction model up to time \(t\) that maximizes the expected discounted return \(_{}[_{t=0}^{}^{t}r_{t}]\) for some discount factor \((0,1]\). In this work, we assume \(\) is trained via RL, and we assume that the ground-truth rewards \(r_{i}\) are observable during training only. Notably, the trained policy \(\) can be deployed without access to the ground-truth labelling function \(\).

### Running Example

The _Gold Mining Problem_ (Figure 2) serves as a running example of a Noisy RM Environment. A mining robot operates in a grid with a non-Markovian goal: dig up at least one chunk of gold ( ) and deliver it to the depot ( ). The environment is an MDP where the robot observes its current grid position and its actions include moving in the cardinal directions and digging. A labelling function \(\) associates the propositions \(=\{\;}\;, \;}\;\}\) with grid states and actions as follows: \(\;}\;\) holds when the robot digs in the rightmost row, and \(\;}\;\) holds when the robot is at the bottom-left cell.

However, the robot does not have access to \(\) and cannot reliably distinguish gold from iron pyrite. Thus, it cannot ascertain whether it has obtained gold during the course of an episode. Luckily, the robot can make an educated guess as to whether a cell contains gold, which is captured by an

Figure 1: The _Noisy Reward Machine Environment_ framework. Blue elements highlight differences with respect to a standard RL framework. Dashed lines ( ) indicate that an element is required during training but not deployment.

abstraction model \(:H\) mapping the robot's current position (while ignoring the rest of the history) to its belief that the cell contains gold.

Note that if the agent _could_ observe \(\), then it could learn an optimal Markovian policy \((a_{t}|s_{t},u_{t})\) with a relatively simple form (\(u_{t}\) is easily computed with access to \(\)). Intuitively, such a policy should collect gold while in RM state \(u^{(0)}\) and head to the depot while in RM state \(u^{(1)}\). Unfortunately, when the agent does not have access to \(\), we cannot directly learn a policy with the simpler Markovian form above. In the following sections, we show how the agent's noisy belief captured by the abstraction model \(\) can be leveraged to simplify the learning problem.

## 4 Noisy RM Environments as POMDPs

We start with an analysis of the Noisy RM Environment framework, contrasting it with a standard RM framework. We ask: **(1)** What is the optimal behaviour in a Noisy RM Environment? **(2)** How does the abstraction model \(\) affect the problem? **(3)** How does not observing the ground-truth labelling function \(\) affect the problem? We provide proofs for all theorems in Appendix A.

Observe that uncertainty in the propositional values is only relevant insofar as it influences the agent's belief about the current RM state \(u_{t}\) since rewards from an RM \(\) are Markovian over extended states \((s_{t},u_{t}) S U\). Our first result is that a Noisy RM Environment \(,,,\) can be reformulated into an equivalent POMDP with state space \(S U\) and observation space \(O\) (Theorem 4.1). Here, we say two problems are equivalent if there is a bijection between policies for either problem such that the policies have equal expected discounted return and behave identically given the same history \(h_{t}\). Thus, optimal behaviour in a Noisy RM Environment can be reduced to solving a POMDP.

**Theorem 4.1**: A Noisy RM Environment \(,,,\) is equivalent to a POMDP over state space \(S U\) and observation space \(O\).

One may notice that the abstraction model \(\) doesn't appear in the POMDP reformulation at all. We later show that an appropriate choice of \(\) can improve policy learning in practice, but this choice ultimately does not change the optimal behaviour of the agent (Theorem 4.2).

**Theorem 4.2** (_Does the choice of \(\) affect optimal behaviour?_): Let \(\) be a Noisy RM Environment \(,,,\), and \(^{}\) be a Noisy RM Environment \(,,,^{}\). Then \(\) and \(^{}\) are equivalent.

We also contrast our proposed framework, where the agent does not have direct access to \(\), with prior RM frameworks where the agent does. We show that this difference does not affect the optimal behaviour in MDP environments, but can affect the optimal behaviour in POMDPs (Theorem 4.3).

**Theorem 4.3** (_Does observing \(\) affect optimal behaviour?_): Let \(\) be a Noisy RM Environment \(,,,\). Consider a problem \(^{}\) that is identical to \(\) except that the agent at time \(t\) additionally observes \((s_{t},a_{t},s_{t+1})\) after taking action \(a_{t}\) in state \(s_{t}\). If \(\) is an MDP, then \(\) and \(^{}\) are equivalent. If \(\) is a POMDP, \(\) and \(^{}\) may be non-equivalent.

Figure 2: The _Gold Mining Problem_ is a Noisy RM Environment where the agent’s interpretation of the vocabulary is uncertain. **Left:** The four rightmost cells yield gold ( ) while two cells in the second column yield iron pyrite, which has no value. The agent cannot reliably distinguish between the two metals—cells are labelled with the probability the agent _believes_ it yields gold. **Right:** The RM emits a (non-Markovian) reward of 1 for collecting gold and delivering it to the depot ( ).

Method

In this section, we consider how to train policies that do not require the ground-truth labelling function \(\) to act. Given Theorem 4.1, in principle we can apply any deep RL approach suitable for POMDPs, such as a recurrent policy that conditions on past actions and observations. However, such a learning algorithm may be inefficient as it doesn't exploit the known RM structure at all. Motivated by the observation that the RM state \(u_{t}\) is critical for decision making, we instead propose a class of policies that decouple inference of the RM state \(u_{t}\) (treated as an unknown random variable) and decision making into two separate modules (Algorithm 1).

The _inference_ module models a belief \(_{t} U\) of the current RM state \(u_{t}\) over the course of an episode with the help of an abstraction model \(\).

More precisely, the inference module is a function

\(f:Z^{+} U\) mapping the history of outputs from the abstraction model to a belief over RM states. The inference objective is to recover the (policy-independent) distribution \((u_{t}|h_{t})\), marginalized over all possible state trajectories \(_{t}=(s_{1},a_{1},,s_{t-1},a_{t-1},s_{t})\):

\[(u_{t}|h_{t})=(u_{t}|_{t})p(_{t}|h_{t})d_{t}\]

Here \((u_{t}|_{t})\) is deterministic--it is the RM state given the trajectory (under the ground-truth labelling function)--while the probability density function \(p(_{t}|h_{t})\) depends on the POMDP transition function and observation probabilities. The _decision-making_ module is a policy \((a_{t}|h_{t},_{t})\) that then leverages the inferred belief \(_{t}\). Below, we describe three inference modules that leverage different forms of abstraction models \(:H Z\) to predict \(_{t}\).

### Naive

Suppose that in lieu of the ground-truth labelling function \(\), we have a noisy estimator of \(\) that predicts propositional evaluations based on the observation history. This is captured via an abstraction model of the form \(:H 2^{}\) that makes discrete (and potentially incorrect) predictions about the propositions \((s_{t-1},a_{t-1},s_{t})\) that hold at time \(t\), given the history \(h_{t}\). Then, we can recursively model a discrete prediction of the RM state \(_{t} U\) (which can be seen as a belief over \(U\) with full probability mass on \(_{t}\)) using outputs of \(\) in place of \(\).

**Method 1** (_Naive_): Given \(:H 2^{}\), predict a discrete RM state \(_{t} U\) as follows. Set \(_{1}=u_{1}\). For \(t>1\), predict \(_{t}=_{u}(_{t-1},(h_{t}))\).

A weakness of this approach is that it does not represent the uncertainty in its prediction. Furthermore, since RM state predictions are updated recursively, an error when predicting \(_{t}\) will propagate to all future predictions \((_{t+1},_{t+2},)\).

**Example 5.1**: Returning to the running example, suppose the agent uses its belief of locations yielding gold to derive an abstraction model \(:H 2^{}\). For a history \(h_{t}\), \(\) takes the current grid position \(s_{t}\) and predicts \(\,\,\,\) is true if it believes there is at least a 50% chance it yields gold when it performs a digging action. We assume the agent can always predict \(\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,predicts probabilities over possible propositional evaluations \((s_{t-1},a_{t-1},s_{t})\), an enticing approach is to derive \(_{t}\) by probabilistically weighing all possible RM states at time \(t-1\) according to the previous belief \(_{t-1}\) along with all possible evaluations of \((s_{t-1},a_{t-1},s_{t})\) according to \(\).

**Method 2** (_Ibu_): Given \(:H(2^{})\), predict a distribution over RM states \(_{t} U\) as follows. Set \(_{1}[u_{1}]=1\) and \(_{1}[u]=0\) for \(u U\{u_{1}\}\). For \(t>1\), set

\[_{t}[u]=_{ 2^{},u^{} U} [_{u}(u^{},)=u]_{t-1}[u^{}] (h_{t})[]\]

On the surface, IBU may appear to solve the error propagation issue of the Naive approach by softening a discrete belief into a probabilistic one. Surprisingly, updating beliefs \(_{t}\) in this manner can still result in a belief that quickly diverges from reality with increasing \(t\). This is because IBU fails to consider that propositional evaluations are linked, rather than independent. Since the computation of \(_{t}\) aggregates \(t\) queries to \(\), noise in the outputs of \(\) can dramatically amplify if the _correlation_ between these noisy outputs is not considered. This is best illustrated by an example.

**Example 5.2**: The mining agent now considers a probability distribution over propositional assignments of \(\{[scale=0.4]{p.eps}\}\). We assume the agent always perfectly determines \([scale=0.4]{p.eps}\) and applies its belief of locations yielding gold; e.g., digging in the cell the agent believes has gold with 0.3 probability yields the distribution \((:0.7,\{[scale=0.4]{p.eps}\}:0.3,\{ [scale=0.4]{p.eps}\}:0,\{[scale=0.4]{p.eps}\}:0)\). Consider a trajectory where the agent digs at this cell multiple times. After mining once, IBU updates the RM state belief \(_{t}\) to reflect a 0.3 probability of having obtained gold. After mining twice, this increases to 0.51 and in general, the belief reflects a \(1-0.7^{k}\) probability after mining \(k\) times. In reality, mining more than once at this square should not increase the probability beyond \(0.3\) since all evaluations of \([scale=0.4]{p.eps}\) at that cell are linked--they are all true, or they are all false.

### Temporal Dependency Modelling (TDM)

Example 5.2 demonstrates a challenge when aggregating multiple predictions from a noisy classifier--the noise may be correlated between evaluations. One solution in the Gold Mining Problem is to update the RM state belief only the first time the agent digs at any particular square, accounting for the fact that all evaluations of \([scale=0.4]{p.eps}\) in that state yield the same result. Indeed, this type of solution is used by many approaches in tabular MDPs with noisy propositional evaluations , but this solution does not scale to infinite state spaces where propositional evaluations may be arbitrarily linked between "similar" pairs of states.

We instead consider an inference module that uses an abstraction model \(:H U\) designed to directly predict a distribution over RM states given the history. Such an abstraction model might manifest as a meta-classifier that aggregates outputs from another model but corrects for the correlation in these outputs. Another way to obtain \(\) is to train a recurrent neural network  end-to-end given a dataset of histories \(h_{t}\) and their associated (ground-truth) RM states \(u_{t}\). Given an abstraction model \(:H U\), TDM simply returns the output of \(\).

**Method 3** (_Tdm_): Given an abstraction model of the form \(:H U\) predict \((h_{t})\) directly.

### Comparison of Inference Modules

At first glance, it may seem challenging to compare different inference modules since they may operate under different abstraction models. Our goal is to elucidate the relatives advantages of each approach to better inform its use. We begin by considering a theoretical property of inference modules,

   Inference & Abstraction & Target & Consistent & Consistent \\ Module & Model & & (MDPs) & (POMDPs) \\  Naive & \(H 2^{}\) & \((s_{t-1},a_{t-1},s_{t})\) & ✓ & ✗ \\ IBU & \(H(2^{})\) & \((s_{t-1},a_{t-1},s_{t})\) & ✓ & ✗ \\ TDM & \(H U\) & \(u_{t}\) & ✓ & ✓ \\   

Table 1: Comparison of inference modules. For each, we highlight its prerequisite abstraction model, the target feature the abstraction model aims to predict, and its _consistency_ in MDPs and POMDPs.

with our conclusions summarized in Table 1. We consider how accurately an inference module models its target distribution \((u_{t}|h_{t})\). This largely depends on the veracity of the abstraction model \(\), but nonetheless, it is desirable that the inference module precisely recovers \((u_{t}|h_{t})\) under an ideal abstraction model \(^{*}\). If this is possible, then we say that an inference module is _consistent_.

**Definition 5.3** (_Consistency_): Consider an inference module \(f:Z^{+} U\). \(f\) is consistent if there exists some \(^{*}:H Z\) such that for every history \(h_{t} H\), running \(f\) on \(h_{t}\) using \(^{*}\) as the abstraction model results in the belief \((u_{t}|h_{t})\).

In the case that \(\) is an MDP, Naive, IBU, and TDM are all consistent since there exists an abstraction model that can recover the ground-truth labelling function \(\) (for Naive and IBU) and \(u_{t}\) (for TDM) with certainty. However, in the general case when \(\) is a POMDP, only TDM remains consistent. Proofs and counterexamples are provided in Appendix A.

## 6 Experiments

Our experiments assess the approaches in Section 5 in terms of RL sample efficiency and final total return, as well as accuracy in predicting a belief over RM states. We examine whether these methods are robust to uncertainty in the outputs of abstraction models, and whether they offer advantages over end-to-end RL algorithms that do not attempt to exploit task structure. Our experimental settings involve challenges that arise when scaling to complex, real-world environments: temporally extended reasoning, partial observability, high-dimensional observations, and sparse rewards.

### Environments

Our environments include the _Gold Mining_ Problem as a toy environment, along with two MiniGrid  environments with image observations and a MuJoCo robotics environment (Figure 3). Full details on the environments are provided in Appendix B.

_Traffic Light_ is a partially observable MiniGrid where the agent must drive along a road to pick up a package and then return home. A traffic light along the road cycles between green, yellow, and red at stochastic intervals and the agent receives a delayed penalty if it runs the red light. The agent only has a forward-facing camera and it can drive forwards, backwards, wait, or make a U-turn. We encode this task with an RM (Figure 7) with propositions for running a red light, collecting the package, and returning home. Crucially, the agent does not observe the light colour when entering the intersection in reverse, causing the agent to be uncertain about the evaluation of the red light proposition.

_Kitchen_ is a partially observable MiniGrid where a cleaning agent starts in the foyer of a home and is tasked with cleaning the kitchen before leaving. There are three chores: the agent must make sure the dishes are washed, the stove is wiped, and the trash is emptied, but not every chore necessarily requires action from the agent (e.g. there might be no dirty dishes to begin with). However, the agent doesn't know how clean the kitchen is until it enters it. For each chore, a proposition represents whether that chore is "done" (e.g. the dishes are clean) in the current state of the environment--thus, the propositional evaluations are unknown to the agent until it enters the kitchen. The RM for this task (omitted due to its size) uses the automaton state to encode the subset of chores that are currently done and gives a reward of 1 for leaving the house once the kitchen is clean.

Figure 3: Traffic Light (top left) and Kitchen (bottom left), are MiniGrids with image observations, where key propositions are partially observable. Colour Matching (right) is a MuJoCo robotics environment where the agent must identify colour names by their RGB values to solve a task.

In _Colour Matching_, the agent controls a robot car while observing LiDAR and RGB observations of nearby objects. The agent observes an instruction with the name of a colour (e.g. "blue") and it must touch only that pillar and then enter the portal. Propositions for each pillar evaluate whether the agent is touching it. The colours of the pillars (and in the instruction) are randomized from a set of 18 possible colours in each episode, so to reliably solve the task, the agent must learn the correct associations of colour names to their RGB values.

### Baselines

We consider the methods _Naive_, _IBU_, and _TDM_ from Section 5 that use the abstraction models described below. For RL experiments, we baseline against a _Memory-only_ method for general POMDPs that does not exploit the RM structure. As an upper bound on performance, we compare against an _Oracle_ agent that has access to the ground-truth labelling function. In the toy Gold Mining Problem, policies are trained using Q-learning  with linear function approximation . In all other domains, policies are neural networks trained with PPO , and the Memory-only policy is Recurrent PPO , a popular state-of-the-art deep RL method for general POMDPs.

### Abstraction Models

In the Gold Mining Problem, we consider toy abstraction models based on the probabilities in Figure 2 as described in the running examples. TDM is equivalent to IBU except it only updates the RM state belief when digging for the first time at the current cell.

In all other domains, abstractions models are neural networks trained via supervised learning. We collect training datasets comprising 2K episodes in each domain (equalling 103K interactions in Traffic Light, 397K interactions in Kitchen, and 3.7M interactions in Colour Matching), along with validation and test datasets of 100 episodes each. This data is generated by a random policy and labelled with propositional evaluations from \(\) and ground-truth RM states. To obtain abstraction models, we train classifiers on their respective target labels and we select optimal hyperparameters according to a grid search.1 We note that all abstraction models are trained on equivalent data, ensuring a fair comparison between different inference modules. To verify that abstraction models are indeed noisy, we use the test set to evaluate the precision and recall of a classifier trained to predict propositional evaluations (Figure 6). We find that key propositions towards decision making, such as running a red light in Traffic Light, or whether the chores are done in Kitchen, are uncertain.

### Results: Reinforcement Learning

We report RL learning curves for each method and environment in Figure 4. The key results are:

**(R1) TDM performs well in all domains.** Using only a noisy abstraction model, TDM achieves similar sample efficiency and final performance to the Oracle agent that has access to the ground truth.

Figure 4: RL curves averaged over 8 runs (shaded regions show standard error). TDM performs well in all domains, in the absence of the ground-truth labelling function, while Recurrent PPO fails.

**(R2): RL makes little progress on any domain when the task structure is not exploited.** Specifically, Memory only (recurrent PPO in the deep RL environments) fails since it does not leverage the temporal and logical structure afforded by the RM.

**(R3): The performance of Naive and IBU depends on the specific environment.** When the simplifying assumptions made by Naive or IBU are reasonable, these approaches can perform well. However, the use of these approaches under noisy abstraction models can also lead to dangerous or unintended outcomes (see Appendix B.5 for a discussion).

We now highlight the most notable qualitative behaviours that were observed. For a more in-depth discussion, refer to Appendix B.5. In Gold Mining, Naive often digs at the nearby cell believed to yield gold with 0.6 probability (but in actuality yielding iron pyrite) before immediately heading to the depot, and IBU repeatedly digs at the same cell to increase its belief of having obtained gold. On the other hand, TDM adopts a robust strategy of mining at multiple different cells to maximize its chance of having obtained gold. In Traffic Light, Naive often runs the red light by driving in reverse to get through the intersection faster. This shortcoming stems from its inability to represent uncertainty about running the red light. In Kitchen, IBU often stays in the foyer without ever entering the kitchen. Despite this, the RM state belief erroneously reflects that all chores have a high probability of being done. This is similar to Example 5.2--each chore is initially "clean" with some small probability, and this is compounded over time by the incorrect independence assumption. In reality, the state of the chore is linked between timesteps (and doesn't change unless the agent intervenes).

### Results: RM State Belief Inference

We compare the inference modules Naive, IBU, and TDM in terms of their predictive accuracy of the RM state (Figure 5). We evaluate each approach on a test set of 100 fixed trajectories generated by a random policy (following the same distribution as the data used to train the abstraction models). Since each inference module aims to capture a belief, we evaluate them according to the _log-likelihood_ of the true RM state under the belief, averaging over the predictions at all timesteps. To avoid \( 0\) when Naive makes an incorrect prediction, we lower bound all log-likelihoods at \( 0.01\).

**(R4): TDM is more accurate when predicting a belief over RM states compared to Naive or IBU on all domains.**

### Results: Vision-Language Models as Zero-Shot Abstraction Models

As an additional experiment, we assess whether GPT-4o can serve as an effective zero-shot abstraction model in the Traffic Light domain. We render RGB images of the environment and use GPT-4o to evaluate propositions described through text. With this abstraction model, we evaluate Naive and IBU on the test set from Section 6.5. We baseline these against the inference modules from Section 6.5 that use abstraction models trained via supervised learning, as well as abstraction models with randomly initialized weights. Results and further details are provided in Figure 8 of the Appendix.

**(R5): GPT-4o is an effective zero-shot abstraction model for Naive and IBU.** As an abstraction model, GPT-4o is nearly as effective as a custom model trained from ground-truth data, and is significantly more effective than a randomly initialized neural network.

Figure 5: Accuracy of inference modules measured by log-likelihood (higher is better) of the true RM state, averaged over 8 runs with lines showing standard error. TDM predicts the RM state belief more accurately than Naive and IBU.

Related Work

Many recent works leverage structured task specifications such as RMs or Linear Temporal Logic (LTL) in deep RL. However, the vast majority of these works do not consider the effects of error or uncertainty in the labelling function. We note a few works that explicitly avoid the assumption of an oracle labelling function. Kuo et al.  solve LTL tasks by encoding them with a recurrent policy. Andreas et al.  and Oh et al.  learn modular subpolicies and termination conditions, avoiding the need for a labelling function, but these approaches are restricted to simple sequential tasks.

Some recent works have considered applying formal languages to deep RL under a noisy evaluation of propositions. Nodari and Cerutti  empirically show that current RM algorithms are brittle to noise in the labelling function but do not offer a solution. Tuli et al.  teach an agent to follow LTL instructions in partially observable text-based games using the Naive method, but only consider propositions that are observable to the agent. Umili et al.  use RMs in visual environments with a method similar to IBU, while Hatanaka et al.  update a belief over LTL formulas using IBU, but only update the belief at certain time intervals. While these last three works offer solutions that tackle noise for a specific setting, they do not consider the efficacy of these methods more generally. Hence, we believe our framework and the insights from this paper will provide a useful contribution.

Despite the large corpus of prior work in fully observable settings, RMs and LTL have rarely been considered for general partially observable deep RL problems. We believe this is due to the difficulty of working with an uncertain labelling function when propositions are defined over a partially observed state. The closest work by Toro Icarte et al.  applies RMs to POMDPs but only considers propositions defined as a function of observations.

LTL has long been used for specification in motion planning , and some works consider an uncertain labelling function or partial observability. However, solutions nearly always depend on a small, tabular state space, while we focus on solutions that scale to infinite state spaces. Ding et al.  propose an approach assuming that propositions in a state occur probabilistically in an i.i.d. manner. Ghasemi et al. , Verginis et al. , Hashimoto et al. , and Cai et al.  consider a setting where propositions are initially unknown but can be inferred through interactions.

## 8 Conclusion

This work introduces a framework for Reinforcement Learning with Reward Machines where the interpretation or perception of the domain-specific vocabulary is uncertain. We propose a suite of algorithms that allow an RL agent to leverage the structure of the task, as exposed by the Reward Machine, while exploiting prior knowledge through the use of _abstraction models_--preexisting models that noisily ground high-level features into the environment. Through theory and experiments, we show the pitfalls of naively aggregating outputs from a noisy abstraction model, while simultaneously demonstrating how abstraction models that are aware of temporally correlated predictions can mitigate this issue. Ultimately, our techniques successfully leverage task structure to improve sample efficiency while scaling to environments with large state spaces and partial observability.

On the topic of societal impact, our work aims to ameliorate the impact of uncertain interpretations of symbols, which can lead to dangerous outcomes. We note that our proposed methods elucidate the internal decision-making process of RL agents within a formal framework, potentially providing enhanced interpretability. A limitation of TDM, our best-performing method, is that it relies on a task-specific abstraction model (to predict the state in the specific RM). For some real-world tasks, it might not be possible to get enough training data to learn accurate abstraction models (some RM transitions might very rarely be observed), so deploying TDM could lead to poor outcomes.

Our experiments show the promise of leveraging pretrained foundation models (e.g., GPT-4o) as general-purpose abstraction models in a Reward Machine framework. Two of our proposed methods, Naive and IBU, can employ such models directly in many settings where text or image descriptions of the environment are available. A further investigation on the integration of Reward Machines and foundation models is left to future work. Another promising direction is to relax the assumption of access to ground-truth rewards--rewards given by the RM under the ground-truth evaluation of propositions--during training.