# Zipper: Addressing Degeneracy in Algorithm-Agnostic Inference

Geng Chen

All authors contributed equally to this work and are listed in alphabetical order.

Yinxu Jia

Guanghui Wang

Guanghui Wang and Changliang Zou are corresponding authors.

NITFID, School of Statistics and Data Science, LPMC, KLMDASR, and LEBPS, Nankai University

gengchen.stat@gmail.com, yxjia@mail.nankai.edu.cn,

ghwang.nk@gmail.com, zoucl@nankai.edu.cn

All authors contributed equally to this work and are listed in alphabetical order.

###### Abstract

The widespread use of black box prediction methods has sparked an increasing interest in algorithm/model-agnostic approaches for quantifying goodness-of-fit, with direct ties to specification testing, model selection and variable importance assessment. A commonly used framework involves defining a predictiveness criterion, applying a cross-fitting procedure to estimate the predictiveness, and utilizing the difference in estimated predictiveness between two models as the test statistic. However, even after standardization, the test statistic typically fails to converge to a non-degenerate distribution under the null hypothesis of equal goodness, leading to what is known as the degeneracy issue. To addresses this degeneracy issue, we present a simple yet effective device, Zipper. It draws inspiration from the strategy of additional splitting of testing data, but encourages an overlap between two testing data splits in predictiveness evaluation. Zipper binds together the two overlapping splits using a slider parameter that controls the proportion of overlap. Our proposed test statistic follows an asymptotically normal distribution under the null hypothesis for any fixed slider value, guaranteeing valid size control while enhancing power by effective data reuse. Finite-sample experiments demonstrate that our procedure, with a simple choice of the slider, works well across a wide range of settings.

## 1 Introduction

Consider predicting response \(Y\) from covariates \(X^{p}\). Due to the popularity of black box prediction methods like random forests and deep neural networks, there has been a growing interest in the so-called "_algorithm (or model)-agnostic_" inference on the _goodness-of-fit_ (GoF) in regression . This framework aims to assess the appropriateness of a given model for prediction compared to a potentially more complex (often higher-dimensional) model. A common approach involves defining a predictiveness criterion, employing either the _sample-splitting_ or _cross-fitting_ strategy to estimate the predictiveness of the two models, and examining the difference in predictiveness. The main focus of this work is to address the issue of degeneracy encountered in predictiveness-comparison-based test statistics.

### Goodness-of-Fit Testing via Predictiveness Comparison

Let \(P\) represent the joint distribution of \((Y,X)\) and consider a class \(\) of prediction functions that effectively map the covariates to the response. Define a criterion \((,P)\), which quantifies the predictive capability of a prediction function \(\). Larger values of \((,P)\) indicate strongerpredictive capability. The optimal prediction function within the class \(\) is determined as \(f_{}(,P)\). Therefore, \((f,P)\) represents the highest achievable level of predictiveness within the class \(\). Examples of \(\) include the (negative) squared loss \((,P)=-E[\{Y-(X)\}^{2}]\) for continuous responses and the (negative) cross-entropy loss \((,P)=E[Y(X)+(1-Y)\{1-(X)\}]\) for binary responses, where \(E\) denotes the expectation under \(P\).

In GoF testing problems, there are typically two classes of prediction functions \(\) and \(_{}\), where \(_{}\) is a subset of \(\). Define a dissimilarity measure \(_{}=(f,P)-(f_{},P)\), which quantifies the deterioration in predictive capability resulting from constraining the model class to \(_{}\). Here, \(f_{}_{f_{}}(,P)\) denotes the optimal prediction function within the restricted class. Since \(_{}\), it follows that \(_{} 0\). The GoF testing is then formulated as

\[H_{0}:_{}=0H_{1}:_{}>0. \]

The formulation of assessing a scalar predictiveness quantity is inspired by the work of Williamson et al. , which focuses on evaluating variable importance. Other predictiveness or risk quantities have also been explored by [6; 7; 8; 9]. Here, we highlight a few examples where the aforementioned framework can be directly applied.

* (Specification testing) Model specification testing aims to evaluate the adequacy of a class of postulated models, such as parametric models, say, examining whether the conditional expectation \(E(Y X)=g_{}(X)\) holds almost surely , where \(g_{}\) is a known function up to an unknown parameter \(\). Under the framework (1), we can consider \(\) as a generally unrestricted class, while \(_{}\) represents a class of parametric models.
* (Model selection) GoF testing can also be employed to identify the superior predictive model from a set of candidates [11; 12]. This situation often arises when choosing between two prediction strategies, such as an unregularized model and a regularized one. Testing \(H_{0}\) is to assess whether the inclusion of a regularizer provides benefits for predictions.
* (Variable importance measure) A recently popular aspect of GoF testing involves evaluating the significance of a specific group of covariates \(U\) in predicting the response, where \(X=(U^{},V^{})^{}\). This assessment can be incorporated into (1) by defining a subset of prediction functions \(_{}\) that disregard the covariate group \(U\) when making predictions. When utilizing the squared loss, \(_{}\) simplifies to the LOCO (Leave Out COvariates) variable importance measure [1; 13; 14; 5; 15; 16], which quantifies the increase in prediction error resulting from the removal of \(U\). Specifically, taking \(_{}=E[\{Y-E(Y V)\}^{2}]-E[\{Y-E(Y X)\}^{2}]\) corresponds to testing for _conditional mean independence_[17; 18].

The measure \(_{}\) possesses the advantages of being both model-free and algorithm-agnostic. It is not tied to a specific model and remains independent of any particular model fitting algorithm. This flexibility makes \(_{}\) a versatile quantity for evaluating goodness-of-fit, enabling us to leverage diverse machine learning prediction algorithms in its estimation.

### The Degeneracy Issue

Let \(Z=(Y,X) P\), and suppose we have collected a set of independent realizations of \(Z\) as \(Z_{i}=(Y_{i},X_{i})\) for \(i=1,,n\). To effectively estimate \(_{}\) while ensuring algorithm-agnostic inference, the sample-splitting or cross-fitting has recently gained significant popularity. This approach relaxes the requirements imposed on estimation algorithms, allowing for the utilization of flexible machine learning techniques [13; 19; 2; 20; 21]. Taking sample-splitting as an example, the data is divided into a training set and a testing set. Based on the training data, estimators \(f_{n}^{}\) and \(f_{n,}^{}\) are obtained for the optimal prediction functions \(f\) and \(f_{}\), respectively. The dissimilarity measure \(_{}\) can then be evaluated using the testing data, i.e., \(_{n,}=(f_{n}^{},P_{n}^{})-(f_{n,}^{},P_{n}^{})\), where \(P_{n}^{}\) represents the empirical distribution function of the testing data. Notably, in the context of measuring variable importance, it has been established that when \(_{}>0\), the estimator \(_{n,}\) exhibits _asymptotic linearity_ under certain assumptions . Similar results are also applicable to the problem of comparing multiple algorithms or models [11; 12].

However, the situation becomes distinct when considering the null hypothesis \(H_{0}:_{}=0\). Recent studies have drawn significant attention to a phenomenon known as _degeneracy_[22; 23; 15; 18; 5].

Consider the simplest scenario where there are no covariates and the objective is to test whether \(:=E(Y)=0\); see also Example 1 in Lei . In this case, we set \(=\) and \(_{}=\{0\}\). Using the squared loss, we have \(_{}=E(Y^{2})=E\{(Y-)^{2}\}=^{2}\). The estimator for \(_{}\) based on sample-splitting is \(_{n,}=2_{n}^{}_{n}^{}-( _{n}^{})^{2}\), where \(_{n}^{}\) and \(_{n}^{}\) denote the sample means of the training and testing data, respectively. When \( 0\), \((_{n,}-)\) follows an asymptotic normal distribution. However, when \(=0\), \(_{n,}=O_{P}(n^{-1/2})\), indicating the presence of the degeneracy phenomenon. While inference at a \(n\)-rate remains feasible under degeneracy in this specific example, it is crucial to recognize that this would not hold true for intricate models and black box fitting algorithms. Williamson et al.  provide evidence for the occurrence of degeneracy in a general variable importance measure \(_{}\), where the influence function becomes exactly zero under the null hypothesis. It is therefore required to address this degeneracy issue in a generic manner to perform algorithm-agnostic statistical inference.

### Existing Solutions

By using the fact that the influence functions of the individual components \((f,P)\) and \((f_{},P)\) in \(_{}\) remain nondegenerate even under \(H_{0}\), Williamson et al.  proposed an additional data split of the testing data, where the two predictiveness functions are evaluated on two separate testing data splits. This approach ensures a nondegenerate influence function under \(H_{0}\), therefore restoring asymptotic normality. A similar approach has been independently explored by Dai et al. . However, performing additional data splits may reduce the actual sample size used in the testing, leading to a substantial loss of power. Alternatively, Rinaldo et al.  and Dai et al.  introduced data perturbation methods, where independent zero-mean noises are added to the empirical influence functions. These methods also restore asymptotic normality. However, determining the appropriate amount of perturbation to achieve desirable Type I error control remains a heuristic process. Furthermore, Verdinelli and Wasserman  proposed expanding the standard error of the estimator to mitigate the impact of degeneracy.

### Our Contributions

In this paper, we introduce the Zipper device for algorithm-agnostic inference under the null hypothesis \(H_{0}\) of equal goodness. Our approach is inspired by the method of additional splitting of testing data, as demonstrated in the works of Williamson et al.  and Dai et al.  for assessing variables with zero-importance. Instead of creating two distinct testing data splits to evaluate the discrepancy of predictiveness criteria between the expansive and restricted models, we encourage an overlap between them. The Zipper device serves to bind together the two overlapping splits, with a _slider_ parameter \([0,1)\) controlling the proportion of overlap. To ensure stable inference and accommodate versatile machine learning prediction algorithms, we incorporate a \(K\)-fold cross-fitting scheme. We will demonstrate that the proposed test statistic follows an asymptotically normal distribution \(H_{0}\) for any fixed value of \(\), ensuring valid size control while providing satisfactory power enhancement.

### Related Works

For variable importance assessments, in addition to LOCO methods within our framework, Shapley value-based measures are commonly used [24; 25; 26; 27]. These measures, which estimate the incremental predictive accuracy contributed by a specific variable across all possible covariate subsets, reveal complex inter-variable relationships but at a considerable computational expense. Furthermore, conditional randomization tests [28; 29] offer a robust alternative when covariate distributions are known or can be accurately estimated. These methods are especially beneficial in semi-supervised settings with extensive unlabeled data. Additionally, LIME  focuses on estimating variable importance locally for a specific instance, while LOCO methods are designed to assess global variable importance.

### Organization

The remainder of our paper is structured as follows. In Section 2, we introduce the Zipper device for addressing the degenerate issue, and present the asymptotic behaviors of the method. Finite-sample experiments are presented in Section 3. Section 4 concludes the paper. Proofs of theorems and additional numerical results are provided in Appendix.

## 2 Our Remedy

### The Zipper Device

To initiate the process, we randomly partition the data into \(K\) folds, denoted as \(_{1},,_{K}\), ensuring that each fold is approximately of equal size. For a given fold index \(k\{1,,K\}\), we construct estimators \(f_{k,n}\) and \(f_{k,n,S}\) for the oracle prediction functions \(f\) and \(f_{S}\) correspondingly, using the data excluding the fold \(_{k}\). To activate the Zipper device, we further randomly divide \(_{k}\) into two splits, labeled as \(_{k,A}\) and \(_{k,B}\), with approximately equal sizes, allowing for an overlap denoted as \(_{k,o}\). Specifically, \(_{k,A}_{k,B}=_{k}\) and \(_{k,A}_{k,B}=_{k,o}\). Let \(_{k,a}=_{k,A}_{k,o}\) and \(_{k,b}=_{k,B}_{k,o}\) represent the non-overlapping parts. For simplicity, we assume that \(|_{k}|=n/K\) and \(|_{k,A}|=|_{k,B}|\), where \(||\) represents the cardinality of set \(\). Let \(=|_{k,o}|/|_{k,A}|\) represent the proportion of the overlap. Visualize the two splits \(_{k,A}\) and \(_{k,B}\) as two pieces of fabric or other materials. The term "Zipper" is derived from the analogy of using a zipper mechanism to either separate or join them by moving the slider \(\); see Figure 1. To evaluate the discrepancy measure \(_{}=(f,P)-(f_{},P)\) based on the \(k\)th fold of testing data \(_{k}\), we denote \(P_{k,n,I}\) as the empirical distribution of the data split \(_{k,I}\), where \(I\{o,a,b\}\). We can estimate \(_{}\) by \(_{k,n}-_{k,n,}\), where

\[_{k,n} =(f_{k,n},P_{k,n,o})+(1-)(f_{k,n},P_{k,n,a})\] \[_{k,n,} =(f_{k,n,},P_{k,n,o})+(1-) (f_{k,n,},P_{k,n,b})\]

represent weighted aggregations of empirical predictiveness criteria corresponding to overlapping and non-overlapping parts, used for estimating \((f,P)\) and \((f_{},P)\), respectively. By employing the cross-fitting process, we obtain the final estimator of \(_{}\), denoted as

\[_{n,}=K^{-1}_{k=1}^{K}(_{k,n}-_{k,n, }), \]

which is the average over all testing folds.

Notably, if we fully open the Zipper, setting \(=0\) and leaving \(_{k,o}\) empty, our approach aligns with the vanilla data splitting strategy utilized in Williamson et al.  and Dai et al.  for assessing variables with zero-importance. Conversely, when we completely close Zipper with \(=1\), our method essentially involves evaluating the predictiveness discrepancy using the identical data split \(_{k,o}=_{k,A}=_{k,B}=_{k}\), which is known to result in the phenomenon of degeneracy . Therefore, to avoid such degeneracy, we restrict the slider parameter \([0,1)\); see also Remark 2.5 below.

### Asymptotic Linearity

We start by investigating the asymptotic linearity of the proposed test statistic \(_{n,}\) in (2), which serves as a foundation for establishing the asymptotic distribution under the null hypothesis, as well as for analyzing the test's power.

**Theorem 2.1** (Asymptotic linearity).: _If Conditions (C1)-(C4) in Section A hold for both tuples \((P,,f,f_{k,n})\) and \((P,_{},f_{},f_{k,n,})\), then_

\[_{n,}-_{}=_{k=1}^{K}_{i:Z_{i}_{k,a}}& (Z_{i})-_{i:Z_{i}_{k,b}}_{}(Z_{i})\\ &+_{i:Z_{i}_{k,o}}\{(Z_{i})-_{}(Z_{i})\}+o_{P}(n^{-1/2}), \]

Figure 1: Illustration of the mechanism of the Zipper device based on the \(k\)th fold of testing data.

[MISSING_PAGE_FAIL:5]

\(g(y,f_{k,n}(x))-n_{k}^{-1}_{i:Z_{i}_{k}}g(Y_{i},f_{k,n}(X_{i}))\). This formulation allows for the identification of the variance estimator \(_{k,n}^{2}\) as the sample variance of \(\{g(Y_{i},f_{k,n}(X_{i}))\}\), simplifying the computation. Moreover, Condition (C5) is immediately satisfied (see Section B.2.1). Examples of such function \(g\) include the squared loss and cross-entropy loss.

Based on Theorem 2.1 and Proposition 2.3, utilizing Slutsky's lemma, we can conclude that the normalized test statistic

\[T_{}:=\{n/(2-)\}^{1/2}_{n,}/_{n,,} }{{}}N(0,1)\]

under \(H_{0}\) for any \([0,1)\). For a prespecified significance level \((0,1)\), we reject the null hypothesis if \(T_{}>z_{1-}\), where \(z_{}\) denotes the \(\) quantile of \(N(0,1)\). A summary of the entire testing procedure can be found in Section C.

_Remark 2.5_.: In our asymptotic analysis of the null behavior, we explicitly exclude the case of \(=1\) due to the resulting degeneracy phenomenon. Specifically, under \(H_{0}\), when \(=1\), the linear leading term of (3) becomes exactly zero. Therefore, including \(=1\) can introduce a distortion in the Type I error.

### Power Analysis

Next, we turn our attention to the power analysis of the proposed test under the alternative hypothesis \(H_{1}:_{}>0\).

**Theorem 2.6** (Power approximation).: _Suppose the conditions stated in Theorem 2.1 and Proposition 2.3 hold. Then for any \([0,1)\), the power function \((T_{}>z_{1-} H_{1})=G_{,n,}()+o(1)\), where_

\[G_{,n,}()=(-,}^{(0)}}{ _{,}}z_{1-}+^{1/2}_{}}{_{,}}),\]

\(_{,}^{(0)}=\{(1-)(^{2}+_{}^{2})\} ^{1/2}\) _and \(\) denotes the distribution function of \(N(0,1)\). Furthermore, if \(Cov\{(Z),_{}(Z)\} 0\), then \(G_{,n,}()\) increases with \(\)._

_Remark 2.7_.: The form of the power function can be directly derived from Theorem 2.1 and the fact that the estimator of standard deviation \(_{n,,}}{{}}_{ ,}^{(0)}\) as \(n\) under \(H_{1}\). Recall that \(_{,}^{2}=(1-)(^{2}+_{}^{2})+ _{}^{2}\), which is provably a decreasing function of \(\) when \(Cov\{(Z),_{}(Z)\} 0\). Consequently, the approximate power function \(G_{,n,}()\) increase with \(\). For more details, please refer to Section B.3.

_Remark 2.8_.: The requirement \(Cov\{(Z),_{}(Z)\} 0\) is relatively benign. For instance, when considering \(_{}=E[\{Y-E(Y V)\}^{2}]-E[\{Y-E(Y X)\}^{2}]\) within the framework of evaluating variable importance, this condition is readily satisfied; refer to B.3.1.

Consider the "unzipped" version of the proposed test with \(=0\), which has been explored in the works of Williamson et al.  and Dai et al.  for assessing variable importance. According to Theorem 2.6, the approximate power function reduces to

\[G_{,n,}(0)=(-z_{1-}+_{ }}{(^{2}+_{}^{2})^{1/2}}),\]

aligning with the findings in Dai et al. . In contrast, for the Zipper with \([0,1)\), we have

\[G_{,n,}()}}{{}} (-z_{1-}+^{1/2}_{}}{(^{2}+ _{}^{2})^{1/2}})}}{{ }}G_{,n,}(0),\]

due to the facts that \(_{,}^{(0)}_{,}\), and \(_{,}(^{2}+_{}^{2})^{1/2}\) if \(Cov\{(Z),_{}(Z)\} 0\). Consequently, our method surpasses the vanilla sample-splitting or cross-fitting based inferential procedures that correspond to \(=0\). The improved power can be attributed to two sources: the introduction of the overlap mechanism \(\) (corresponding to Inequality (ii)), and the utilization of the variance estimator \(_{n,,}^{2}\) (Inequality (i)).

_Remark 2.9_.: As discussed, \(^{2}_{n,,}\) is inconsistent for the limiting variance \(^{2}_{,}\) under \(H_{1}\) when \(0<<1\). If the objective is to construct a valid confidence interval for the dissimilarity measure \(_{}\), it is crucial to use a consistent variance estimator regardless of whether \(H_{0}\) holds or not. This can be achieved by incorporating an additional plug-in estimator of \(^{2}_{}\), which is a component of the asymptotic variance \(^{2}_{,}\). For detailed construction of a valid confidence interval, please refer to Section D in Appendix.

### Efficiency-and-Degeneracy Tradeoff

Our asymptotic analysis demonstrates that the Zipper device ensures a valid testing size for any fixed slider parameter \([0,1)\). Moreover, as the slider \(\) moves away from \(0\), the power improves. In practical scenarios with finite sample sizes, selecting an appropriate value of \(\) involves a tradeoff between efficiency and degeneracy. Opting for a larger value of \(\) can indeed enhance the testing power. However, an excessively large \(\) approaching \(1\) would result in degeneracy and potential size inflation. This occurs because the normal approximation (4) breaks down under the null hypothesis. It is worth emphasizing that using a relatively small \(\) is generally safer and yields improved power compared to the vanilla splitting-based strategies with \(=0\).

To achieve better power while maintaining a reliable size, we propose a simple approach for selecting \(\). By (4), the asymptotic normality relies on comparing means from two independent samples \(_{k=1}^{K}_{k,a}\) and \(_{k=1}^{K}_{k,b}\), each with a size of \((1-)n/(2-)\). To ensure a favorable normal approximation, we can choose the sample size \((1-)n/(2-)\) such that it meets a predetermined "large" sample size, such as \(n_{0}=30\) or \(50\). Say, we can specify \(=_{0}:=(n-2n_{0})/(n-n_{0})\). In the case of very large \(n\), a truncation may be needed to safeguard against degeneracy. For example, we can set \(=\{_{0},0.9\}\). Our numerical experiments show that this selection of \(\) achieves satisfactory performances across a wide range of scenarios. For more details on the impact of \(\), please refer to Section E.1.

## 3 Finite-Sample Experiments

### Synthetic Experiments

#### 3.1.1 Variable Importance Assessment

For illustration, we conduct a simulation study to evaluate the performance of the proposed Zipper method in assessing variables with zero-importance, an area of active research. We compare the empirical size and power of Zipper against several benchmark procedures. Firstly, we consider Algorithm 3 proposed in Williamson et al.  (referred to as WGSC-3) and the two-split test in Dai et al.  (DSP-Split). Both procedures involve an additional splitting of the testing data and can be seen as approximate counterparts to the proposed Zipper test with \(=0\). Another benchmark procedure is Algorithm 2 from Williamson et al.  (WGSC-2), which can be viewed as a rough equivalent to Zipper with \(=1\). Additionally, we include the data perturbation method proposed by Dai et al.  (DSP-Pert). For each benchmark procedure, we follow the suggestions of the respective authors to select nuisance parameters. We specify the slider parameter \(=\{_{0},0.9\}\) with \(n_{0}=50\) as suggested in Section 2.5.

We consider two models: one with a normal response \(Y N(X^{},^{2}_{Y})\), and another with a binomial response \(Y(1,(X^{}))\), where \((t)=1/\{1+(-t)\}\). Both models assume that \(X N(0,)\), where \(=(0.2^{|i-j|})_{p p}\). For each model, we examine two scenarios. The first scenario is a low-dimensional setting with \(p\{5,10\}\) and \(=(,,5,0,5,0_{p-5})^{}\). The second scenario is a high-dimensional setting with \(p\{200,1000\}\) and \(=(,,5_{0.01p},0^{}_{0.99p-2})^{}\), where \(a_{q}\) represents a \(q\)-dimensional vector with all entries set to \(a\). In the normal model, we specify \(^{2}_{Y}\) such that the signal-to-noise ratio \(^{}/^{2}_{Y}=3\) by assigning \(=0\). The objective is to test whether the first two variables contribute significantly to predictions from a sequence of \(n\{200,500,1000\}\) independent realizations of \((Y,X)\).

Let \(\) represent a generally unrestricted model class, subjecting to a degree of sparsity under the high-dimensional settings. Consider \(_{}\) such that the prediction functions exclude the first two components of the covariates. To test the irrelevance of the first two variables in predictions, we examine \(H_{0}:_{}=0\) in (1). We adopt the squared loss for normal responses and the cross-entropyloss for binomial responses. The ordinary least-squares regression and the LASSO are utilized under the low-dimensional and high-dimensional scenarios, respectively. The significance level is chosen as \(=5\%\), and our experiments entail \(1,000\) replications. These experiments are executed on an Intel Xeon Gold 5118 CPU @ 2.30GHz.

Table 1 displays the empirical sizes for different testing procedures with \(n=500\) and \(p\{5,1000\}\). The results reveal that the Zipper, WGSC-3, and DSP-Split consistently maintain the correct size across all models, as anticipated. In contrast, the WGSC-2 exhibits conservative behavior in the low-dimensional setting and inflated sizes in the high-dimensional settings, primarily due to the degeneracy phenomenon. In addition, the data perturbation method, DSP-Pert, fails to control the size in some cases, particularly in the high-dimensional settings. This instability can be attributed to the selection of the amount of perturbation.

Figure 2 depicts the empirical power of various testing methods as a function of the magnitude \(\) representing variable relevance, when \(n=500\) and \(p\{5,1000\}\). As expected, the Zipper shows a substantial improvement in power compared to the vanilla cross-fitting based approaches, WGSC-3 and DSP-Split, with WGSC-3 and DSP-Split demonstrating similar performances. Under the high-dimensional settings, the WGSC-2 and DSP-Pert exhibit higher power than Zipper, but at the expense of losing valid size control. For a comprehensive analysis of the empirical sizes and power across various combinations of \(n\) and \(p\), please refer to Section E.2. These additional results consistently support the conclusion that the Zipper method demonstrates reliable empirical size performance and significant power enhancement compared to that methods that utilize non-overlapping splits.

#### 3.1.2 Model Specification Testing

We extend our investigation beyond the variable importance assessment problem to include an evaluation of the proposed Zipper device in addressing model specification issues. Our analysis focuses on the model defined as \(Y=X+\), where \(X N(0,)\) with \(=(0.2^{|i-j|})_{p p}\) and \( N(0,_{e}^{2})\). Here, we assume that \(\|\|_{0}=2\), indicating the presence of two nonzero components in \(\), while the positions of these components remain unknown. Our objective is to test the model specification hypothesis: \(H_{0}:=(*,*,0_{p-2})^{}\) versus \(H_{1}:\|\|_{0}=2\) but not \(H_{0}\), where \(*\) represents any nonzero value. To generate the data, we consider three scenarios: (i) \(=(0.4,0.4,0_{p-2})^{}\), (ii) \(=(0.4,0.0,4,0_{p-3})^{}\), and (iii) \(=(0,0,0.4,0.4,0_{p-4})^{}\). We determine the value of \(_{e}^{2}\) such that the signal-to-noise ratio \(^{}/_{e}^{2}=1\). Subsequently, we generate independent realizations \((Y_{i},X_{i})\) for \(i=1,,n\), with \(n=500\) and \(p\{5,1000\}\). To estimate \(\), we employ ordinary least squares under \(H_{0}\), and perform the best

   Model & \(p\) & Zipper & WGSC-3 & DSP-Split & WGSC-2 & DSP-Pert \\   & 5 & 3.9(0.19) & 5.1(0.22) & 4.6(0.21) & 0.1(0.03) & 10.2(0.30) \\  & 1000 & 4.3(0.20) & 6.2(0.24) & 5.9(0.24) & 16.7(0.37) & 35.0(0.48) \\  & 5 & 3.7(0.19) & 3.9(0.19) & 4.2(0.20) & 0.6(0.08) & 4.0(0.20) \\  & 1000 & 5.6(0.23) & 4.8(0.21) & 5.1(0.22) & 19.9(0.40) & 38.6(0.49) \\   

Table 1: Empirical sizes (in percentage) of various testing procedures, with standard deviations in brackets.

Figure 2: Empirical power of various testing methods as a function of the magnitude \(\) with \(n=500\) and \(p\{5,1000\}\). The dot-dashed horizontal line represents the intercept at \(=5\%\).

two subset selection under \(H_{1}\). For the case when \(p=5\), we conduct an exhaust search. For the case when \(p=1000\), we utilize the abess algorithm  to approximate the solutions. The results are summarized in Table 2, where we observe that under Scenario (i), corresponding to \(H_{0}\), Zipper maintains correct size control. Furthermore, under Scenarios (ii) and (iii), corresponding to \(H_{1}\), Zipper exhibits substantial improvements in power when compared to the vanilla cross-fitting based approaches, WGSC-3 and DSP-Split.

### Real-Data Examples

#### 3.2.1 MNIST Handwritten Dataset

We apply the Zipper method to the widely used MNIST handwritten digit dataset . The MNIST dataset consists of size-normalized and center-aligned handwritten digit images, each represented as a \(28 28\) pixel grid (resulting in \(p=28^{2}=784\)). For our analysis, we specifically extract subsets of the dataset representing the digits \(7\) and \(9\), following Dai et al. , resulting in a total of \(n=14251\) images.

In Figure 3, we calculate and graphically represent the average grayscale pixel values for images sharing the same numerals. We divide each image into nine distinct regions, as shown by the blank squares in Figure 3, with the objective of detecting regions that can effectively distinguish between these two digits. To achieve this, we perform a sequence of variable importance testing to assess the relevance of each region in making predictions while considering the remaining regions. Given the nature of the data, we employ a Convolutional Neural Network (CNN) as the underlying model, leveraging its proven effectiveness in image analysis. In the Zipper approach, we select the slider parameter \(\) such that \(n_{0}=50\), as recommended in the manuscript. As a benchmark, we adopt WGSC-3  (equivalent to DSP-Split ), which produces valid size, aligning with our approach. We set the predefined significance level for each test as \(=0.05/9\), applying the Bonferroni correction to account for multiple comparisons. The discovered regions, highlighted in red, are presented in Figure 3. Our findings indicate that the Zipper method outperforms WGSC-3 in identifying critical regions with greater efficacy.

#### 3.2.2 Bodyfat Dataset

We expand the application of our Zipper method to the bodyfat dataset , which provides an estimate of body fat percentages obtained through underwater weighing, along with various body circumference measurements from a sample of \(n=252\) men. Our objective is to conduct marginal variable importance tests for each body circumference while considering potential influences from essential attributes such as age, weight, and height. To accurately estimate the relevant regression functions within this dataset, we employ the random forest as our modeling technique. Table 3 presents the resulting p-values obtained from the Zipper and WGSC-3 methods. By applying the Bonferroni correction, the Zipper method identifies both Abdomen and Hip as significant factors at the significance level of \(=0.05/10\). In contrast, WGSC-3 suggests only Abdomen as important. It is worth noting that a recent study by Zhu et al.  proposed the formula (Waist+Hip)/Height as a straightforward evaluation index for body fat. Remarkably, our finds align with this fact, further supporting the validity and relevance of our Zipper method in identifying key factors.

   \(p\) &  &  \\   Scenario & Zipper & WGSC-3 & DSP-Split & WGSC-2 & Zipper & WGSC-3 & DSP-Split & WGSC-2 \\  (i) & 4.3(0.20) & 6.2(0.22) & 5.6(0.20) & 0.0(0.00) & 4.2(0.19) & 5.5(0.20) & 6.5(0.21) & 16.6(0.36) \\ (ii) & 96.9(0.17) & 31.2(0.46) & 34.9(0.46) & 100.0(0.00) & 94.2(0.22) & 29.8(0.46) & 31.4(0.46) & 97.3(0.16) \\ (iii) & 100.0(0.00) & 81.4(0.39) & 79.3(0.38) & 100.0(0.00) & 100.

## 4 Concluding Remarks

In this paper, we introduce Zipper, a simple yet effective device designed to address the issue of degeneracy in algorithm/model-agnostic inference. The mechanism of Zipper involves the recycling of data usage by constructing two overlapping data splits within the testing samples, which holds potential for independent exploration. A key component of Zipper is the slider parameter, which introduces an efficiency-and-degeneracy tradeoff. To ensure reliable inference, we propose a simple selection criterion by ensuring a large sample size to render asymptotic normality under the null hypothesis. Other data-adaptive strategies are possible and merit further investigation. Moreover, the predictiveness-comparison-based framework allows for the utilization of alternative forms of two-sample tests, such as rank-based methods. This capability proves beneficial when dealing with data exhibiting heavy-tailed distributions or outliers. Furthermore, incorporating the Zipper device into _large-scale comparisons_ to achieve error rate control warrants additional research. We can conduct a sequence of variable importance tests, each aimed at assessing the relevance of a specific variable \(X_{j}\) in the predictive model while controlling for a global error rate. This procedure necessitates the fitting of \(p+1\) models: one that includes all variables and \(p\) null models, each excluding a distinct variable. Such a process is computationally demanding. Moreover, accurately controlling error rates presents a considerable challenge due to complex dependency structures among the p-values.