# Tiny Graph Convolutional Networks with

Topologically Consistent Magnitude Pruning

Hichem Sahbi

Sorbonne University, UPMC, CNRS, LIP6, France.

hichem.sahbi@sorbonne-universite.fr

###### Abstract

Magnitude pruning is one of the mainstream methods in lightweight architecture design whose goal is to extract subnetworks with the largest weight connections. This method is known to be successful, but under very high pruning regimes, it suffers from topological inconsistency which renders the extracted subnetworks disconnected, and this hinders their generalization ability. In this paper, we devise a novel end-to-end Topologically Consistent Magnitude Pruning (TCMP) method that allows extracting subnetworks while guaranteeing their topological consistency. The latter ensures that only accessible and co-accessible -- impactful -- connections are kept in the resulting lightweight architectures. Our solution is based on a novel reparametrization and two supervisory bi-directional networks which implement accessibility/co-accessibility and guarantee that only connected subnetworks will be selected during training. This solution allows enhancing generalization significantly, under very high pruning regimes, as corroborated through extensive experiments, involving graph convolutional networks, on the challenging task of skeleton-based action recognition.

## 1 Introduction

Deep convolutional networks are nowadays becoming mainstream in solving many pattern classification tasks including visual recognition . Their principle consists in training convolutional filters together with pooling and attention mechanisms that maximize classification performances. Many existing convolutional networks were initially dedicated to grid-like data, including images . However, data sitting on top of irregular domains (such as skeleton graphs in action recognition) require extending convolutional networks to general graph structures, and these extensions are known as graph convolutional networks (GCNs) . Two families of GCNs exist in the literature: spectral and spatial. Spectral methods are based on graph Fourier transform  while spatial ones rely on message passing and attention . Whilst spatial GCNs have been relatively more effective compared to spectral ones, their precision is reliant on the attention matrices that capture context and node-to-node relationships . With multi-head attention, GCNs are more accurate but overparametrized and computationally overwhelming.

Many solutions are proposed in the literature to reduce time and memory footprint of convolutional networks including GCNs . Some of them pretrain oversized networks prior to reduce their computational complexity (using distillation , linear algebra , quantization  and pruning ), whilst others build efficient networks from scratch using neural architecture search . In particular, pruning methods, either unstructured or structured are currently mainstream, and their principle consists in removing connections whose impact on the classification performance is the least noticeable. Unstructured pruning  proceeds by dropping out connections individually using different proxy criteria, such as weight magnitude, and then retraining the resulting pruned networks. In contrast, structured pruning  removes groups of connections, entire filters or subnetworks using different mechanisms such as grouped sparsity. However, existing pruning methods eitherstructured or unstructured suffer from several drawbacks. On the one hand, structured pruning may reach high speedup on usual hardware, but its downside resides in the rigidity of the class of learnable architectures. On the other hand, unstructured pruning is more flexible, but its discrimination is limited at high pruning regimes due to _topological disconnections_, and handling the latter is highly intractable as adding or removing any connection _combinatorially_ affects the others.

As contemporary network sizes grow into billions of parameters, studying high compression regimes has been increasingly important on very large network architectures. Nevertheless, pruning mid-size (but still heavy) architectures, including GCNs, is even more challenging as this usually leads to highly disconnected and untrainable subnetworks, even at reasonably (not very) large pruning rates. Hence, we target our contribution towards mid-size network architectures including GCNs in order to fit not only the usual edge devices, such as smartphones, but also highly _miniaturized_ devices endowed with very limited computational resources (e.g., smart glasses). Considering the aforementioned issues, our contribution in this paper includes a new lightweight design which guarantees the topological consistency of the extracted subnetworks. Our proposed solution is variational and proceeds by training pruning masks and weight parameters that maximize classification performances while guaranteeing the _accessibility_ of the unpruned connections (i.e., their reachability from the network input) and their _co-accessibility_ (i.e., their actual contribution in the evaluation of the output). Hence, only topologically consistent (accessible and co-accessible) subnetwork connections are combinatorially selected. Extensive experiments, on the challenging task of skeleton-based action recognition, show the outperformance of our proposed TCMP method.

## 2 A Glimpse on GCNs

Let \(=\{_{i}=(_{i},_{i})\}_{i}\) denote a collection of graphs with \(_{i}\), \(_{i}\) being respectively the nodes and the edges of \(_{i}\). Each graph \(_{i}\) (denoted for short as \(=(,)\)) is endowed with a signal \(\{(u)^{s}:\ u\}\) and associated with an adjacency matrix \(\) with each entry \(_{uu^{}}>0\) iff \((u,u^{})\) and \(0\) otherwise. GCNs aim at learning a set of \(C\) filters \(\) that define convolution on \(n\) nodes of \(\) (with \(n=||\)) as \(()_{}=f\ ^{ }\ \), here \({}^{}\) stands for transpose, \(^{s n}\) is the graph signal, \(^{s C}\) is the matrix of convolutional parameters corresponding to the \(C\) filters and \(f(.)\) is a nonlinear activation applied entrywise. In \(()_{}\), the input signal \(\) is projected using \(\) and this provides for each node \(u\), the aggregate set of its neighbors. Entries of \(\) could be handcrafted or learned so \(()_{}\) implements a convolutional block with two layers; the first one aggregates signals in \(()\) (sets of node neighbors) by multiplying \(\) with \(\) while the second layer achieves convolution by multiplying the resulting aggregates with the \(C\) filters in \(\). Learning multiple adjacency (also referred to as attention) matrices (denoted as \(\{^{k}\}_{k=1}^{K}\)) allows us to capture different contexts and graph topologies when achieving aggregation and convolution. With multiple matrices \(\{^{k}\}_{k}\) (and associated convolutional filter parameters \(\{^{k}\}_{k}\)), \(()_{}\) is updated as \(f_{k=1}^{K}^{k}^{}^{k}\). Stacking aggregation and convolutional layers, with multiple matrices \(\{^{k}\}_{k}\), makes GCNs accurate but heavy. We propose subsequently a method that makes our network architectures lightweight and still effective.

## 3 Magnitude Pruning

In the rest of this paper, a given GCN is subsumed as a multi-layered neural network \(g_{}\) whose weights defined as \(=\{^{1},,^{L}\}\), with \(L\) being its depth, \(^{}^{d_{-1} d_{}}\) its \(^{}\) layer weight tensor, and \(d_{}\) the dimension of \(\). The output of a given layer \(\) is defined as \(^{}=f_{}(^{^{}}\ ^{-1})\), \(\{2,,L\}\), being \(f_{}\) an activation function. Without a loss of generality, we omit the bias in the definition of \(^{}\). Magnitude Pruning (MP) consists in zeroing the smallest weights in \(g_{}\) (up to a pruning rate), while retraining the remaining weights. A relaxed variant of MP is obtained by multiplying \(^{}\) with a differentiable mask \((^{})\) applied entrywise to \(^{}\). The entries of \((^{})\) are set depending on whether the underlying layer connections are kept or removed, so \(^{}=f_{}((^{}(^{}))^{}\ ^{-1})\), here \(\) stands for the element-wise matrix product. In this definition, \((^{})\) enforces the prior that smallest weights should be removed from the network. In order to achieve magnitude pruning, \(\) must be symmetric, bounded in \(\), and \(() 1\) when \(||\) is sufficiently large and \(() 0\) otherwise1.

Pruning is achieved using a global loss as a combination of a cross entropy term denoted as \(_{e}\), and a budget cost which measures the difference between the targeted cost (denoted as \(c\)) and the actual number of unpruned connections

\[_{\{^{}\}_{}}_{e}\{^{} (^{})\}_{}+_{=1}^{L-1 }_{d_{}}^{}(^{})_{d_{+1}}-c ^{2}, \]

here \(_{d}\) is a vector of \(d_{}\) ones. Eq. 1 focuses on minimizing the budget loss (with \(\) sufficiently large) while progressively making \(\{(^{})\}_{}\) crisp (almost binary) by linearly annealing the temperature of the sigmoid function that defines \(\). As training evolves, the right-hand side term reaches its minimum and stabilizes while the gradient of the global loss becomes dominated by the gradient of the left-hand side term, and this maximizes further the classification performances.

## 4 Proposed Method: TCMP

The aforementioned pruning formulation is relatively effective (as shown later in experiments), however, it suffers from several drawbacks. On the one hand, removing connections independently may result into _topologically inconsistent_ network architectures (see section 4.1), i.e., either completely disconnected or having isolated connections. On the other hand, high pruning rates may lead to an over-regularization effect and hence weakly discriminant lightweight networks, especially when the latter include isolated connections (see again later experiments). In what follows, we introduce a more principled pruning framework that guarantees the topological consistency of the pruned networks and allows improving generalization even at very high pruning rates.

### Accessibility and Co-accessibility

Our formal definition of topological consistency relies on two principles: _accessibility and co-accessibility_ of connections in \(g_{}\). Let's remind \((^{}_{ij})\) as a crisp (binary) function that indicates the presence or absence of a connection between the i-th and the j-th neurons of layer \(\). This connection is referred to as accessible if \( i_{1},,i_{-1}\), s.t. \((^{1}_{i_{1},i_{2}})==(^{-1}_{i_{L-1},i })=1\), and it is co-accessible if \( i_{+1},,i_{L}\), s.t. \((^{+1}_{j,i_{+1}})==(^{L}_{i_{L-1},i _{L}})=1\).

Considering \(^{}_{a}=(^{1})\)\((^{2})(^{-1})\) and \(^{}_{c}=(^{+1})\)\((^{+2})(^{L})\), and following the above definition, it is easy to see that a connection between \(i\) and \(j\) is accessible (resp. co-accessible) iff the i-th column (resp. j-th row) of \(^{}_{a}\) (resp. \(^{}_{c}\)) is different from the null vector. A network architecture is called topologically consistent iff all its connections are both accessible and co-accessible. Accessibility guarantees that incoming connections to the i-th neuron carry out effective activations resulting from the evaluation of \(g_{}\) up to layer \(\). Co-accessibility is equivalently important and guarantees that the outgoing activation from the j-th neuron actually contributes in the evaluation of the network output. A connection -- not satisfying accessibility or co-accessibility and even when its magnitude is large -- becomes useless and should be removed when \(g_{}\) is pruned.

For any given network architecture, parsing all its topologically consistent subnetworks and keeping only the one that minimizes Eq. 1 is highly combinatorial. Indeed, the accessibility of a given connection depends on whether its preceding and subsequent ones are kept or removed, and any masked connections may affect the accessibility of the others. A heuristic is proposed in  as a greedy approach to prune networks while guaranteeing their topological consistency; however, this approach is clearly suboptimal as (i) topologically consistent subnetwork selection is _decoupled_ from (ii) weight retraining. In what follows, we introduce our main contribution (TCMP) that _couples_ both steps (i) and (ii) during network pruning using two supervisory accessibility networks.

### Accessibility and Co-Accessibility Networks

Our solution relies on two supervisory networks that measure accessibility and co-accessibility of connections in \(g_{}\). These two networks, denoted as \(_{r}\) and \(_{l}\), have exactly the same architecture as \(g_{}\) with only a few differences: indeed, \(_{r}\) measures accessibility and inherits the same connections in \(g_{}\) with the only difference that their weights correspond to \(\{(^{})\}_{}\) instead of \(\{^{}(^{})\}_{}\). Similarly, \(_{l}\) inherits the same connections and weights as \(_{r}\), however these connections are reversed in order to measure accessibility in the opposite direction (i.e., co-accessibility). Note that weights \(\{^{}\}_{}\) are shared across all the networks \(g_{}\), \(_{r}\) and \(_{l}\).

Considering the definition of accessibility and co-accessibility, one may define layerwise outputs \(_{r}^{}:=h(_{r}^{-1}_{l}^{^{}}(_{-1}))^{}\;_{r}^{-1}\), and \(_{l}^{}:=h(_{r}^{}_{l}^{+1^{}}( _{}))_{l}^{+1}\) being \(_{r}^{1}=_{d_{1}}\), \(_{l}^{L}=_{d_{L}}\), \(_{d_{1}}\) the vector of \(d_{1}\) ones and \(h\) the Heaviside activation. With \(_{r}^{}\) and \(_{l}^{}\), the non-zero entries of the matrix \((_{r}^{}_{l}^{+1^{}})(^{})\) correspond to selected connections in \(g_{}\) which are also accessible and co-accessible. By plugging this matrix into Eq. 1, we redefine our topologically consistent pruning loss

\[_{e}\{^{}(^{}) _{r}^{}_{l}^{+1^{}}\}_{}+_{=1} ^{L-1}_{r}^{^{}}(^{})_{l}^{+1}-c ^{2}. \]

It is clear that accessibility networks in Eqs. 2 are interdependent and cannot be modeled using standard feedforward networks, so more complex (highly recursive and interdependent) networks should be considered which also lead to exploding gradient. In order to make accessibility and co-accessibility networks in Eqs. 2 simpler and still trainable with standard feedforward networks, we constrain entries of \((_{})\) to take non-zero values _iff_ the underlying connections are kept and accessible/co-accessible; in other words, \(_{r}^{^{}}(_{})_{l}^{+1}\) should approximate \(_{d_{L}}^{}(_{})_{d_{L+1}}\) in order to guarantee that (i)pruned connections are necessarily accessible/co-accessible and (ii) non accessible ones are necessarily pruned. Hence, instead of Eqs. 2, a surrogate loss is defined as

\[&_{e}\{^{} (^{})_{r}^{}_{l}^{+1^{}}\}_{} +_{=1}^{L-1}_{r}^{^{}}( ^{})_{l}^{+1}-c^{2}\\ &+_{=1}^{L-1}_{d_{L}}^{} (_{})_{d_{+1}}-_{r}^{^{}}( _{})_{+1}^{l}, \]

with now \(_{r}^{}:=h(_{-1})^{}\;_{r}^{-1} \) and \(_{l}^{}:=h(_{})\;_{l}^{+1}\).

### Optimization

Let \(\) denote the global loss in Eq. 3, the update of \(\{^{}\}_{}\) is achieved using the gradient of \(\) obtained by _simultaneously_ backpropagating the gradients through the networks \(g_{}\), \(_{r}\) and \(_{l}\). More precisely, considering Eq. 3 and \(_{r}^{}\), \(_{l}^{}\), the gradient of the global loss w.r.t. \(^{}\) is obtained as

\[}{^{}}+_{k=+1}^{L} {}{_{r}^{k}}^{k}}{_{r}^{k-1} }^{+1}}{^{}}+_{k=1} ^{}}{_{l}^{k}}^{k}}{ _{l}^{k+1}}^{}}{^{}}, \]

here the left-hand side term in Eq. 4 is obtained by backpropagating the gradient of \(\) from the output to the input of the network \(g_{}\) whereas the mid terms are obtained by backpropagating the gradients of \(\) from different layers to the input of \(_{r}\). In contrast, the right-hand side terms are obtained by backpropagating the gradients of \(\) through \(_{l}\) in the opposite direction. Note that the evaluation of the gradients in Eq. 4 relies on the straight through estimator (STE) ; the sigmoid is used as a differentiable surrogate of \(h\) during backpropagation while the initial Heaviside is kept when evaluating the responses of \(_{r}\), \(_{l}\) (i.e., forward steps). STE allows training differentiable accessibility networks while guaranteeing binary responses when evaluating these networks.

## 5 Experiments

We evaluate our different GCN architectures on the task of action recognition [71; 28; 20] using the challenging First-Person Hand Action (FPHA) dataset . This dataset, naturally suitable for GCNs, consists of 1175 skeletons whose ground-truth includes 45 action categories with a high variability in style, speed and scale as well as viewpoints. Each video, as a sequence of skeletons, is modeled with a graph \(=(,)\) whose given node \(v_{j}\) corresponds to the \(j\)-th hand-joint trajectory (denoted as \(\{_{j}^{}\}_{t}\)) and edge \((v_{j},v_{i})\) exists iff the \(j\)-th and the \(i\)-th trajectories are spatially neighbors. Each trajectory in \(\) is described using _temporal chunking_[65; 10]: this is obtained by first splitting the total duration of a video sequence into \(M\) equally-sized temporal chunks (\(M=32\) in practice), and assigning trajectory coordinates \(\{_{j}^{}\}_{t}\) to the \(M\) chunks (depending on their time stamps), and then concatenating the averages of these chunks in order to produce the raw description (signal) of \(v_{j}\).

**Implementation details and baseline GCN.** Our GCNs are trained end-to-end using Adam  for 2,700 epochs with a momentum of \(0.9\), batch size of \(600\) and a global learning rate (denoted as \((t)\)) set depending on the change of the loss in Eq. 3; when the latter increases (resp. decreases), \((t)\) decreases as \((t)(t-1) 0.99\) (resp. increases as \((t)(t-1)/0.99\)). The mixing parameter \(\) in Eq. 3 is set to \(1\) and \(\) is slightly overestimated to \(10\) in order to guarantee the implementation of the targeted pruning rates. All these experiments are run on a GeForce GTX 1070 GPU (with 8 GB memory) and classification performances -- as average accuracy through action classes -- are evaluated using the protocol in  with 600 action sequences for training and 575 for testing. The architecture of our baseline GCN (taken from ) consists of an attention layer of 16 heads applied to skeleton graphs whose nodes are encoded with 32-channels, followed by a convolutional layer of 128 filters, and a dense fully connected layer. This initial network architecture is relatively heavy (for a GCN); its includes 2 million parameters and it is accurate compared to the related work on the FPHA benchmark, as shown in Table 1-left. Considering this GCN baseline architecture, our goal is to make it lightweight while maintaining its high accuracy as much as possible.

**Lightweight GCNs (Comparison & Ablation).** We study the impact of TCMP on the performances of our lightweight GCNs for different pruning rates. Table. 1-right shows the positive impact of TCMP especially on highly pruned network architectures. This impact is less important (and sometimes negative) with low pruning regimes as the resulting architectures have enough (a large number of) Accessible and Co-accessible (AC) connections, so having a few of these connections neither accessible nor co-accessible, i.e. removed, produces a well known regularization effect  that enhances performances. In contrast, with high pruning rates and without Topological Consistency (TC), this leads to over-regularized and very disconnected lightweight architectures that suffer from under-fitting. With TC, both accessibility and co-accessibility are guaranteed even with very high pruning regimes, and this also attenuates under-fitting, and ultimately improves generalization as again shown in table 1-right.

## 6 Conclusion

We introduce in this paper a novel lightweight architecture design based on Topologically Consistent Magnitude Pruning (TCMP). The particularity of TCMP resides in its ability to select subnetworks with _only_ accessible and co-accessible connections. The latter make the learned lightweight subnetworks topologically consistent and more accurate particularly at very high pruning regimes. The proposed approach relies on two supervisory networks, that implement accessibility and co-accessibility, which are trained simultaneously with the lightweight networks using a novel loss function. Extensive experiments, involving graph convolutional networks, on the challenging task of skeleton-based recognition show the substantial gain of our method.

 
**Method** & **Color** & **Depth** & **Pose** & **Accuracy (\%)** \\  Two stream-color  & & & & 61.36 \\ Two stream-color  & & & & 69.91 \\ Two stream-color  & & & & 75.30 \\  HOG2-depth  & & & & 59.83 \\ HOG2-depthwise  & & & & 60.78 \\ HOSVD  & & & & 70.61 \\ Novel  & & & & 69.21 \\  Library LSTM  & & & & 72.73 \\  \(\)-layer LSTM  & & & & 80.14 \\  Moving Pose  & & & & 50.34 \\ Lie Group  & & & & 52.69 \\  \(\)-layer LSTM  & & & & 77.40 \\ Gram Matrix  & & & & 85.39 \\ T1-T  & & & & 80.69 \\  \(\)-layer LSTM  & & & & 60.78 \\  \(\)-layer LSTM  & & & & 60.71 \\  \(\)-layer LSTM  & & & & 70.61 \\  \(\)-layer LSTM  & & & & 72.73 \\  \(\)-layer LSTM  & & & & 80.14 \\  \(\)-layer LSTM  & & & & 80.