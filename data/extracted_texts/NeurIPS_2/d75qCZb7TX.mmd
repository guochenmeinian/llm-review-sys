# Contrast to data-only methods on unstructured data.

[MISSING_PAGE_FAIL:1]

performance across a large number of possible subgroups. Each subgroup test is equivalent to evaluating a separate hypothesis about the model's performance on that specific slice of data. This raises a subtle but important problem--from the perspective of hypothesis testing, such methods _implicitly test multiple hypotheses_ (Sec. 3.1). The more subgroups we test, the more hypotheses we implicitly evaluate. Consequently, this results in critical problems associated with multiple testing: (i) _high false positive_ and (ii) _high false negative rates_ (Sec. 3.2). A third complementary challenge is that each data slice tested is (iii) _not necessarily practically meaningful_. The practical implications of these drawbacks are quite concerning--they hinder our ability to accurately identify where the model performs poorly, thereby undermining the reliability of our model evaluations.

**Can we do better?** To address these three challenges, we propose to _loosen the restrictive assumption of reliance only on data_ and propose a new paradigm of testing called Context-Aware Testing (CAT) to offer an alternative to the dominant data-only view (Sec. 3.3). CAT provides a principled approach to incorporate external knowledge--or context-- to the ML testing process. With the view that each evaluated data slice corresponds to an implicit hypothesis test, we propose that ML evaluation on observational datasets can be achieved via a _context-guided sampling mechanism_ (Definition 1). This mechanism is a sampling procedure that uses context as an inductive bias to prioritize specific data slices to test for which have a higher chance of surfacing meaningful model failures. Therefore, CAT fundamentally helps to answer the question of "what should we test for?".

Let's consider an example of building an ML model to predict prostate cancer . Data-only methods employ a search procedure over the dataset to find divergence across a large number of possible feature combinations which may lead to (i) _high false positive rates_ by identifying spurious underperforming subgroups (e.g. based on eye color or patient ID); (ii) _high false negative rates_ by failing to identify true underperforming subgroups due to the large number of combinations tested and applied testing corrections; and (iii) _testing subgroups which are not practically meaningful_ (e.g. interaction between eye color and height). In contrast, a CAT-based approach would define and target task-relevant subgroups, limiting the number of tests conducted with better false positive control and greater statistical power. As we empirically show in Sec. 5, obtaining many false positives and false negatives is overwhelmingly common in current testing practices.

In bringing CAT to reality, we develop the framework called **SMART**2**Testing, which performs automated ML model evaluation by actively identifying potential failure cases (Sec. 4). SMART uses large language models (LLMs) to generate contextually relevant failure hypotheses to test. We further introduce a _self-falsification mechanism_, to automatically validate the generated failure hypotheses using the available data, allowing efficient pruning of spurious hypotheses. Finally, SMART generates comprehensive model reports that provide insights into the identified failure modes, their impact, and potential root causes, enabling stakeholders to make informed decisions.

**Contributions.**1 We identify critical gaps in predominant data-only ML testing, illustrating they miss important dimensions (Sec. 3). 2 We formalize the _Context-Aware Testing_ paradigm, providing a principled framework to incorporate context in addition to data into the testing process, which is then used to guide the generation of targeted tests (Sec. 3). 3 We build the first context-aware testing system, _SMART_ Testing, which employs LLMs to hypothesize likely and relevant model failures and empirically refutes them with data using a novel _self-falsification mechanism_ (Sec. 4). 4 We demonstrate the value of context for effective testing, challenging the de facto data-only paradigm by showing how SMART identifies impactful model failures while avoiding false positives across diverse settings, when compared to data-only testing. Additionally, SMART identifies failures on important societal groups and generates comprehensive model reports (Sec. 5).

## 2 Related work

To highlight the need for SMART Testing, we contrast it with other ML testing paradigms -- specifically Data-only testing methods which address the same testing problem as SMART. We provide an overview in Table 6 and an extended discussion in Appendix A.

_Data-only testing methods_: Address the question: "what should we test?". Data-only methods search the data to find "slices" where the model's predictions underperform compared to average performance, deeming those slices as model failures. Although automated, data-onlyapproaches operate _only_ on raw data without accounting for the problem context. Consequently, they must search across a large space of potential failures, usually covering all subsets of features and their distinct values. While an exhaustive search may seem beneficial, as we show in Sec. 3, the reality is that performing many tests on a finite dataset risks discovering slices where model failure is irrelevant or due to random variability, i.e. the multiple testing problem. SMART Testing addresses this challenge by prioritizing relevant and likely model failures through contextual awareness.

_Orthogonal testing dimensions:_ While SMART addresses what to test, several other dimensions of model testing exist that are orthogonal to our approach (detailed in Appendix A). (i) Behavioral Testing [16; 17; 5] evaluates model behaviors (i.e. responses to data) by operationalizing tests along pre-defined dimensions (often defined by humans) -- rather than discovering the test dimensions. SMART fundamentally differs by addressing the core issue of "what to test". (ii) Software functional testing [18; 19], aims to primarily test functional correctness (e.g. input-output functionality such as monotonicity), rather than testing for failures. In addition, test cases are either pre-specified or specified with an approximation of the underlying model to probe for functional correctness.

## 3 A context-aware testing framework for ML

The prevailing paradigm for testing ML models relies on _data-only_ methods which exclusively use data to surface model failures. In this section, we explore the limitations of data-only methods by viewing ML testing as a multiple hypothesis testing problem. We explore why data-only methods are uniquely prone to finding _false positive_ and _false negative_ model failures. To address this, we introduce a new paradigm of testing called **context-aware testing** which relies on external knowledge, or _context_, as an inductive bias to better identify where models fail.

### A multiple hypothesis testing view of ML evaluation

**Preliminaries.** Denote the feature space by \(\) and the label space by \(\), and \(\) the joint probability distribution over \(\). We wish to test a fixed, trained black-box model \(f:\), using a finite dataset \(\) usually split into \(_{train}=\{(x_{i},y_{i})\}_{i=1}^{N_{train}}\) and \(_{test}=\{(x_{i},y_{i})\}_{i=1}^{N_{test}}\). We assume the existence of a loss function \(:\) which measures the discrepancy between the model's prediction and the true labels point-wise.

The primary goal of ML testing is to identify _meaningful failure modes_--subgroups (data slices) of the data distribution where the model's performance is significantly worse than its average behavior. Formally, let \(\) denote a _data slice_ and let \(_{}=(|(x,y))\) be the conditional distribution induced by \(\). We aim to identify slices where the slice-specific expected loss \(_{}=_{(x,y)_{}}[(f(x),y)]\) significantly exceeds the population-level expected loss \(_{}=_{(x,y)}[(f(x),y)]\)3.

**Testing ML models is a multiple hypothesis testing problem**. We are interested in identifying failure modes that generalize beyond the training dataset. We can interpret the empirical dataset \(\) as a sample from a broader distribution \(\), and our goal is to make an _inferential_ claim on the performance on the data slices with respect to \(\). Suppose we have a candidate data slice \(}\). We can evaluate the empirical slice-specific loss as \(_{}=|}|^{-1}_{(x,y)} (f(x),y)\) and compare it to the empirical loss over the entire dataset \(_{}=||^{-1}_{(x,y)}(f(x),y)\). To make an inferential claim about the model's performance on the data slice \(\) w.r.t. \(\), we can follow the frequentist testing paradigm and formulate a hypothesis test where we evaluate whether the performance is significantly different. Therefore, \(H_{0}:_{}=_{}\) and alternative hypothesis \(H_{1}:_{}_{}\), where \(_{}=_{(x,y)_{}}[(f(x),y)]\) and \(_{}=_{(x,y)}[(f(x),y)]\) denote the _true_ slice-specific and population-level losses. In practice, this evaluation could be done by running an appropriate frequentist statistical test and evaluating whether \(p<\) for each slice, given some pre-defined \(\).

However, in realistic testing scenarios, we evaluate the model's performance not just on a single slice but on a large collection of candidates \(\{_{j}\}_{j=1}^{m}\). This amounts to conducting many simultaneous hypothesis tests. Accounting for multiple testing is important. A naive testing procedure that does not adjust for multiplicity could surface a large number of spurious failure modes simply by chance (Type I error). Conversely, controlling the false discovery rate  may involve adjusting the per-test significance threshold to \(^{}\), potentially sacrificing power to detect true failures (Type II error).

The multiple hypothesis testing viewpoint reveals a key challenge in ML model evaluation: To reliably surface meaningful failures, we require a principled procedure for generating a relatively small number of promising hypotheses (candidate slices) to test.

### The failures of data-only testing

Existing ML testing methodologies are _data-only_ in that they only use the available empirical data to test for model failures and vary in their optimization objective . However, data-only methods operate under the restrictive assumption that the available empirical data is the sole input for testing ML model performance. In practice, this is almost never the case. It is common to have an _a priori_ understanding of where models are likely to fail given the data distribution, model class, training algorithm, and deployment context. This restrictive assumption results in three challenges: **(i) High false positive rate**: data-only methods search over a large space of data slices and each evaluation amounts to an implicit hypothesis test (Sec. 3.1). Therefore, the probability of observing a false failure increases with every test performed. **(ii) High false negative rate**: The naive testing procedure can be made robust by correcting for the number of tests performed which reduces the statistical power to detect true failures. **(iii) Lack of meaningful failures**: Data-only methods are fundamentally limited by the fact that not all statistically significant slices are practically meaningful. We empirically validate these claims in Sec. 5.

The core limitation of data-only testing is the lack of a principled failure mode _discovery_ procedure that can incorporate prior knowledge to guide the search for meaningful errors.

### Formulating context-aware testing

To address the limitations of data-only testing, we introduce **context-aware testing** (CAT), a principled framework for identifying meaningful model failures using context. This could be the context implicitly encoded in the dataset (i.e. via meaningful feature names) or available external input (i.e. external contextual knowledge, such as a string of input information from a human). Let \(\) denote the space of all possible contextual information and \(c\) be specific external input. Our core insight is that we can use \(\) as an inductive bias to select which slices to test for.

**Definition 1** (Context-Aware Testing).: _Let \(\), \(\), \(\), \(f\), \(=\{(x_{i},y_{i})\}_{i=1}^{N}\), and \(\) be defined as in the standard supervised learning setup. Let \(\) be a space of contexts._

_Context-aware testing is defined by two procedures:_

1. _A context-guided slice sampling mechanism_ \(:() 2^{ }\) _such that_ \((c,,m)=\{_{1},,_{m}\}\)_, where_ \(c\) _is used as an inductive bias for function_ \(\) _to prioritize slices likely to contain meaningful failures, and_ \(m\) _are the number of slices to evaluate._
2. _A multiple hypothesis testing procedure:_ \(_{i}(c,,m)\)_, test_ \(H_{0}:_{_{i}}=_{}\) _vs._ \(H_{1}:_{_{i}}_{}\)_, where_ \(_{_{i}}=_{(x,y)_{_{i}}}[ (f(x),y)]\)_,_ \(_{}=_{(x,y)}[(f(x),y)]\)__

_A meaningful failure is characterized by statistical significance and practical relevance._

The targeted sampling mechanism \(\) uses the context \(\) as an inductive bias to prioritize testing of slices that are (i) _more relevant_ to the deployment context, and (ii) _more likely_ to exhibit significant performance gaps.

This principled slice selection offers several key advantages over data-only methods: _(i) Improved false positive control_: by limiting the number of tests conducted to \(m\), CAT controls the risk of spurious discoveries that arise when naively testing all possible slices. _(ii) Improved true positive rate_: The targeted selection of slices likely to contain failures maintains test power by avoiding the need to aggressively correct for multiple testing. _(iii) Meaningful failures_: context-guided sampling identifies failures that are both statistically significant and practically relevant.

Context-aware testing overcomes the limitations of data-only methods by employing a principled, context-guided slice sampling mechanism \(\) to prioritize the discovery of meaningful model failures.

The core technical challenge in realizing CAT is the development of an effective context-conditional sampling algorithm \(\). In the following section, we propose a concrete instantiation of \(\) using a large language model to generate plausible failure modes and guide testing of ML models.

## 4 SMART Testing

We now instantiate the CAT framework outlined in Sec. 3.3 with a method called **SMART** testing (**s**ystematic, **m**odular, **a**utomated, **r**o**uirements-responsive, and **t**n**fnerable). SMART generates relevant and likely hypotheses about potential model failures and empirically evaluates these hypotheses on available data. SMART follows a four-step procedure: (1) Hypothesis generation; (2) Operationalization; (3) Self-falsification; (4) Reporting. Table 1 provides an early illustration of what it practically means to generate hypotheses and test them with SMART. The procedure below details how the four steps are implemented.

**Step 1: Hypothesis Generation.** Recall from Sec. 3.3, we wish to define a sampling mechanism \(\) to sample slices \(\) which are both relevant and have a high relative likelihood of failure -- where \(\) should be both contextually-aware and able to integrate requirements to guide sampling.

We posit that LLMs have the potential to satisfy these properties due to the following capabilities: _Contextual understanding_: LLMs have been pretrained with a vast corpus of information and hence have extensive prior knowledge around different contexts and settings . _Integrate requirements_: LLMs are adept at integrating requirements or additional information about the problem via natural language . _Hypothesis proposers_: In proposing likely failure modes, LLMs have also been shown to be "phenomenonal hypothesis proposers" .

An LLM is defined as a probabilistic mapping \(l:^{*} P()\), where \(\) denotes the vocabulary space, and \(P()\) represents the probability distributions over \(\). The model processes input sequences \(s^{*}\), each a concatenation of tokens representing external (contextual) input \(_{e}\) (which can be null) and dataset contextualization \(_{D}\) is formalized as \(s=(_{e},_{D})\). We extract the contextualized description \(_{D}\) from the dataset \(\) using

  Hypothesis & Verification & p-value & \(| Acc|\) & Evidence \\  \(f\) underperforms an elderly pe-fent-safety & Elderly patients may have more complex health histories due to age-related comorbidities, which could make predictions less accurate. & age \(>\) = 72 & 0.000 & 0.194 & Supported \\ \(f\) underperforms on patients with multiple comorbidities could complicate the prediction multiple comorbidities. & The presence of multiple comorbidities could complicate the prediction model for an illustration because different labels conditions. & comorbidities \(>\) = 2 & 0.900 & 0.0270 & Not supported \\ \(f\) underperforms on patients under-going conservative management. & Conservative management might be choices for patients with more complex weights & treatment,conservative management = 1 & 0.000 & 0.200 & Supported \\ \(\) conservative management. & or less predictable cases, which could lead to worse predictive performance. & & & & \\  

Table 1: Example hypotheses on model failure, justifications, and operationalizations generated by the SMART framework on a healthcare dataset. The p-values show whether the modelâ€™s performance significantly differs from average performance with \(| Acc|\) measuring the effect size.

Figure 1: Overview of the SMART Testing Framework showing the four steps. All steps are automatically executed by an LLM.

Figure 2: SMART uses an LLM to integrate context \(\), and data context \(_{c}\). Relevant and likely failure hypotheses are then generated by LLM (i.e. sampling mechanism). The hypotheses are then operationalized in \(\) and evaluated. In contrast, data-only methods are not guided by context and requirements, searching more exhaustively in \(\) for divergent slices.

an extractor function \(:_{D}\), which captures essential dataset characteristics (e.g. feature relationships and high-level dataset information). Additionally, we highlight the the LLM will implicitly extract context based on the context encoded in the dataset (e.g. via meaningful feature names). Based on input \(s\), the LLM predicts a distribution over \(\) from which hypotheses of model failure and corresponding justifications are sampled.

As depicted in Fig. 2, given \(_{e}\) and \(_{D}\), we sample the \(N\) most likely hypotheses of failures, \(=\{H_{1},H_{2},,H_{N}\}\), and corresponding justifications \(=\{J_{1},J_{2},,J_{N}\}\), to provide explainability. This process is formalized using the LLM's mapping \(l\) as follows:

\[(H_{i},J_{i}) l(s),s=(_{e},_{D}),\;  i\{1,2,,N\}.\]

**Step 2: Operationalization.** The process of operationalizing each hypothesis \(H_{i}\) involves translating its natural language expression into a form that can directly operate on the training dataset \(_{train}\) (an example is provided in Table 1). To achieve this, we define an interpreter function \(I:\{0,1\}^{}\) that maps each natural language hypothesis \(H_{i}\) to a corresponding binary-valued function \(g_{i}:\{0,1\}\) on the feature space \(\), where \(g_{i}(x)=1\) if \(x\) satisfies the criteria and \(g_{i}(x)=0\) otherwise. Each function \(g_{i}\) induces a data slice \(_{i}_{}\) consisting of data points that satisfy the criteria of hypothesis \(H_{i}\), such that \(_{i}=\{(x,y)_{}:g_{i}(x)=1\}\). Therefore, each hypothesis, after operationalization, corresponds to a specific slice that is being tested on. Steps 1 and 2 serve to practically instantiate \(\) from Sec. 3.3.

**Step 3: Self-falsification** We introduce a novel self-falsification mechanism to empirically evaluate (or _refute_) the generated hypotheses4. Specifically, for each feasible hypothesis \(H_{i}\), we attempt to falsify the hypothesis with observed empirical data 5. This involves evaluating the model \(f\) over the data slice \(_{i}\) operationalized from \(H_{i}\). We then assess whether the slice performance on \(f\) has a significant deviation from the model's overall performance. For instance, in Table 1, this is done by computing \(||\) and the p-value. The significance of this deviation is determined through frequentist statistical testing, i.e. when \(p<\) for any \(\) which might also be adjusted for multiple hypothesis testing. This step effectively "reshuffles/reranks" the hypotheses based on their likelihood on the observed data. For example, when benchmarking we select the top \(n\) hypotheses based on statistical significance: \(^{*}=_{}}{}\)\(\{p_{i}<\}\).

_Remark:_ As shown in Fig. 1, we can exclude the self-falsification from SMART in cases of small-sample sizes. We denote this ablation of SMART as \(SMART_{NSF}\).

**Step 4. Model Performance Evaluation and Reporting.** Finally, SMART automatically generates a report of the overall performance of the model under varying conditions generated by the LLM, including a summary report, a complete summary of the tests carried out, intermediate and final results, and potential failure modes of the ML model.

\(\) SMART is a tabular CAT method which (i) directly _searches for model failures_, sampling targeted tests (Sec. 3.3) and (ii) _incorporates data, and context_ into ML testing.

### Practical use of SMART.

We highlight the practical use of SMART testing and emphasize SMART's ease of use and minimal input requirements needed. In particular, as shown in the example below, users do not need any prior knowledge to use SMART. Rather, we make use of the context inherently encoded in the dataset, feature names and task.

Figure 3: A self-falsification module within the SMART framework. A hypothesis generator \(l\) generates plausible hypotheses and justifications for when the model might fail. This is operationalized with \(_{i}\) and tested against the empirical data.

### Robustness to False Positives

**Goal.** We aim to underscore the role of contextual awareness in preventing false positives in model testing. In particular, we consider the scenario when dealing with tabular data that may contain many irrelevant or uninformative features , persisting even post-feature selection [31; 32]. We contrast SMART which explicitly accounts for context, to data-only approaches which are context-unaware and can only operate on the numerical data.

**Setup.** We fit a predictive model to the training dataset, varying the number of irrelevant, synthetically generated features contained in the dataset. The irrelevant features are drawn from different distributions. We then quantify the proportion of conditions in the identified slices that _falsely_ include the irrelevant synthetic features. We evaluate using five real-world tabular datasets spanning diverse domains, namely finance, healthcare, criminal justice and education: loan, breast cancer, diabetes, COMPAS recidivism  and OULAD education . These datasets have varying characteristics, from sample size to number of features and are representative of different contexts pertinent to tabular ML, to demonstrate the effectiveness of SMART across various real-world contexts

**Analysis.** Fig. 4 shows the proportions of irrelevant features included in slices for increasing numbers of irrelevant features. Data-only methods are unaware of context and are shown to spuriously include high proportions of irrelevant features in their slices; i.e. _false positives (FPs)_. Additionally, these false discoveries increase for data-only methods as the number of irrelevant features increases.

  
**Sec.** & **Experiment** & **Tategory** \\  Figure 4 & Access databases to irrelevant features & SMART consistently needs relevant features, outperforming data-only methods. \\ Table 3 & Access inhibitors to identify significant model failures & SMART demonstrates kits with larger performance discrepancies across models. \\ Table 4 & Measure PSRs in identifying underperforming subgroups & SMART attributes include the larger negative in all settings. \\ Table 5 & Access inhibitors to potential LLM biases & SMART effectively mitigate biases is identified in implementing subgroups. \\ Table 10 & Evaluate ability to safely rely on hyperparameters & SMART attribute and representation while maintaining statistical significance. \\ Figure 8 & Access inhibitors to suffer with irrelevant feature detection & SMART consistently avoid relevant feature profiles of sample size. \\ Table 11 & Evaluate performance in different deployment environments & SMART facilitates more significant false positives over environments. \\ Figure 9 & Access inhibitors of capsule and co-ordinates & SMART consistently outperform data-only methods. \\ Table 12 & Evaluate stability to the general features. & SMART avoids question features, unlike other methods. \\ Table 13 & Compare performance of GPT-3.5 and in SMART & SMART obtains full GPT-truth captions and irrelevant labels. \\ Table 14 & Evaluate SMART across different datasets. & SMART identifies positive feature descriptors. \\ Table 15 & Explain SMART across different datasets. & SMART identifies positive feature descriptors. \\ Table 16 & Explain the number of hypotheses or "yes-weight models" & SMART aims to predict the number of relevant features in the dataset. \\ Table 17 & Explain the number of hypotheses or "yes-weight models" & SMART identifies positive feature descriptors across various models. \\ Table 18 & Evaluate SMART. & SMART promote comprehensive multi-task learning, providing clear justifications for each hypothesis. \\   

Table 2: Summary of experiments and takeaways.

Consequently, the sensitivity to FPs of data-only methods risks that slices identified are neither empirically relevant nor meaningful. In contrast, SMART, by virtue of contextual-awareness when generating hypotheses is _not_ sensitive to extraneous and contextually irrelevant features. Importantly, SMART also maintains its robustness to FPs even with an increasing number of irrelevant features.

_Remark._ We also assess sensitivity to the number of data samples. Fig. 9, Appendix D.6 shows that SMART remains robust to variations irrespective of sample size. In contrast, data-only methods have variable performance with false discoveries sensitive to the sample size.

**Avoiding non-existent failure slices.** It is important that model testing does not flag failures when there are none. In Appendix D.5, we demonstrate that SMART's contextual awareness means that it is resistant to spurious failure slices that have no underlying relationships. In contrast, data-only approaches implicitly assume the existence of problematic slices, which we empirically show makes them prone to spuriously flagging non-existent failure slices.

**Takeaway 1.** SMART's contextual awareness ensures testing relevance, thereby reducing false positives across different scenarios, in contrast to data-only methods.

### Targeting model failures

**Goal.** We assess whether the identified failure slices persist when evaluated on new, unseen data across different tabular models. This evaluates the generalization of the identified model failures across multiple different ML models.

**Setup.** We use the prostate cancer dataset  and aim to discover slices indicative of underperformance. Thereafter, we assess generalizability of the identified failures across four different ML models (logistic regression, SVM, XGBoost and MLP). Specifically, we compare the top identified slice from each method and compute the absolute difference between the accuracies of that slice and the remainder of the dataset. We posit that a testing framework should be able to identify slices with high-performance discrepancies on unseen data across multiple ML models.

**Analysis.** Table 3 shows that SMART slices exhibit the greatest discrepancies in model accuracies on unseen test data across different ML models -- indicating the discovered failure modes are generalizable. We find that SMART surfaces slices with greater performance differences compared to \(SMART_{NSF}\) (ablation without self-falsification), highlighting the importance of the introduced self-falsification mechanism. In contrast, data-only methods fail to identify slices where the performance discrepancy is as large as SMART. This limitation can be attributed to the tendency of data-only methods to overfit the training data, leading to high false discovery rates.

**Takeaway 2.** SMART discovers failure slices where the model substantially underperforms, generalizing to unseen test data across different ML models. In contrast, data-only approaches fail to find slices where the difference in accuracies is as large, highlighting the lack of generalizability and reliability of their findings.

### Robustness to False Negatives

In our setup, false negatives (FNs) are directly tied to true positives (TPs). i.e. the more true positives we find, the fewer FNs we miss. Across multiple experiments (Fig. 4, Table 10, Table 3, Table 16), we consistently show that SMART identifies TPs at substantially higher rates than data-only. For example, in Table 16, SMART identifies an average of 9.6 out of 10 subgroups where the ML

Figure 4: Contextual-awareness in SMART reduces FPs, i.e. reducing the proportion of irrelevant features in slices. SMART is not sensitive to the number of irrelevant features, unlike data-only methods. \(\) is better

    & **LogisticRegression** & **SVM** & **XGBoostlier** & **MLPClassifier** \\   Australian & 0.24 \(\) 0.02 & 0.24 \(\) 0.02 & 0.09 \(\) 0.09 & 0.24 \(\) 0.02 \\ \(PSG\_{,B}\) & 0.23 \(\) 0.01 & 0.23 \(\) 0.01 & 0.11 \(\) 0.07 & 0.23 \(\) 0.01 \\ \(PSG\_{,A}\) & 0.23 \(\) 0.01 & 0.23 \(\) 0.01 & 0.11 \(\) 0.07 & 0.23 \(\) 0.01 \\ Divesfology & 0.05 \(\) 0.11 & 0.09 \(\) 0.13 & 0.14 \(\) 0.15 & 0.02 \(\) 0.05 \\ Stickender & 0.01 \(\) 0.00 & 0.01 \(\) 0.00 & 0.00 \(\) 0.00 & 0.01 \(\) 0.01 \\ Stickender & 0.26 \(\) 0.06 & 0.25 \(\) 0.06 & 0.18 \(\) 0.09 & 0.26 \(\) 0.06 \\  SMART\({}_{}\) & 0.17 \(\) 0.01 & 0.17 \(\) 0.01 & 0.09 \(\) 0.05 & 0.17 \(\) 0.01 \\
**SMART** & **0.37 \(\) 0.03** & **0.37 \(\) 0.03** & **0.26 \(\) 0.06** & **0.37 \(\) 0.03** \\   

Table 3: Identifying slices with the highest performance discrepancies. We show differences in accuracies (\(| Acc|\)) between the top identified divergent slice and average performance across four classifiers (over 5 runs). \(\) is better.

model significantly underperforms. The fact that data-driven methods discover fewer such subgroups implies that they are missing the ones SMART uncovers.

**Goal.** That said, we conduct an additional experiment to directly assess the false negative rate, wherein we can control issues (as FNs are naturally unspecified in real data).

**Setup.** We simulate a dataset to predict recidivism (\(Y\)) based on five covariates: gender, race, age, income, and education. \((|X_{i}=j)}{P(Y_{i} j)})=_{j}-(_{1}X_{}+_{2}C_{}+_{3}C_{})+,(_{0},_{0})\). We then train a predictor function \(\) on the data and synthetically introduce underperformance on certain corrupted subgroups. For an individual \(i\), if they belong to corrupted subgroup \(j\), the prediction \(_{i}\) is equal to \((X_{i})\) with probability \(1-p\), and a random prediction sampled from a Bernoulli distribution with probability \(p\). If the individual does not belong to subgroup \(j\), the prediction is simply \((X_{i})\). Finally, we measure how often each testing method identifies that the model \(\) underperforms on a subgroup.

**Analysis.** Table 4 demonstrates results where we synthetically manipulate/corrupt the performance of an ML model on a single subgroup (n=1), two subgroups (n=2), and three subgroups (n=3) out of a total of five. The results show the average of 20 runs, where the corrupted groups are randomly selected within each run. We find that SMART consistently is least susceptible to false negatives across all corrupted variables, when compared to data-only methods which struggle especially once more than one variable is corrupted. This serves to corroborate our earlier results.

**Takeaway 3.** SMART is less prone to FNs, compared to data-only methods across all settings.

### Assessing and mitigating potential LLM challenges and biases.

**Background.** In many settings, we want SMART to be part of a human-in-the-loop model evaluation, particularly to address challenges with LLMs, such as biases or missing dimensions. Let us first discuss how SMART addresses some issues by design and then perform an experimental assessment.

\(\)**Using data to mitigate LLM challenges**: We use data in two ways _(i) Data usage in the generation of hypotheses:_ Before generating explicit hypotheses of where the model is likely to fail, we provide the LLM with additional information about the data description and model failures of the training dataset. The hypotheses sampled are therefore reflective of the inductive bias of the LLM as well as being conditioned on the data itself; _(ii) Data usage in falsifying hypotheses:_ core to SMART is the self-falsification mechanism where we iteratively generate hypotheses and test them on a validation dataset. Data is therefore used to filter out hypotheses which are not supported by the data. Hence, even if "incorrect" hypotheses about group failures are proposed, this step ensures they are discarded.

\(\)**SMART provides clear and transparent testing**: This is done in two ways: _(i) SMART's model reports:_ that document specific failure cases with natural language justifications (see Appendix D.10 for examples). Automatically generated reports can be a useful tool for humans-in-the-loop experts to audit and validate the testing process, such as evaluating whether tests should be added or removed. For example, a domain expert (e.g. a clinician) could review the report to assess whether the identified failure modes are truly relevant and concerning in that specific context. _(ii) Tests include justifications:_ The justifications for the tests allow human users to inspect the model's testing procedures, understand the reasons, and audit for biases or missed dimensions.

**Goal.** Going beyond the mitigation strategies by design, we also assess SMART's robustness to prior biases. Specifically, we assess common ethnicity related biases of LLMs.

**Setup.** We use the same data generating process as Sec. 5.3.

We train a predictor function \(\) and simulate a scenario where we intentionally corrupt a model's predictions for a proportion \(\) of a minority subgroup ("white" or "black" ethnicity).

    & **FNR (n=1)** & **FNR (n=2)** & **FNR (n=3)** \\  Autostrat & \(0.75 0.44\) & \(0.88 0.22\) & \(0.92 0.15\) \\ pysubgroup\_beam & \(0.65 0.49\) & \(0.75 0.26\) & \(0.68 0.17\) \\ pysubgroup\_apriori & \(0.65 0.49\) & \(0.75 0.26\) & \(0.68 0.17\) \\ Diveyloper & \(0.05 0.22\) & \(0.40 0.35\) & \(0.72 0.31\) \\ Sliceline & \(0.25 0.44\) & \(0.50 0.36\) & \(0.70 0.28\) \\ 
**SMART** & **0.00 \(\) 0.00** & **0.05 \(\) 0.15** & **0.38 \(\) 0.27** \\   

Table 4: False Negative Rate (FNR) for different methods at various settings. \(\) is better.

    &  \\  \(\) & \(P_{}\) & \(P_{}\) & \(P_{}\) & \(P_{}\) \\  \(0.01\) & \(0.78 0.04\) & \(0.15 0.04\) & \(0.16 0.04\) & \(0.78 0.04\) \\ \(0.02\) & \(0.88 0.03\) & \(0.08 0.03\) &\(\) ranges from 0 (no corruption) to 1 (completely random predictions for the subgroup). SMART aims to identify the top underperforming subgroup without bias based on historical patterns. We measure the proportion of times the top subgroup contains the "white" (\(P_{}\)) or "black" (\(P_{}\)) minority, averaged over 20 runs and 5 seeds, separately corrupting "white" and "black" ethnicities.

**Analysis.** We show in Table 5 that SMART is able to identify where models underperform even in scenarios such as ethnicity bias, where LLMs exhibit prior biases from the training dataset. This links to the above discussion that SMART mitigates such biases by design both using the training dataset to guide hypothesis generation and using the self-falsification mechanism to empirically evaluate hypotheses (and discard those that aren't reflective of the data).

**Takeaway 4**. SMART mitigates potential biases in the LLM, both by using the real data to guide hypothesis generation, as well as using the self-falsification mechanism to filter spurious hypotheses.

## 6 Discussion and limitations

Responding to recent calls for better model testing , we formalize **Context-Aware Testing**; a new testing paradigm, actively seeking out relevant and likely model failures based on contextual awareness -- going beyond data alone. We develop **SMART Testing**, using LLMs to hypothesize likely and relevant model failures providing _improved_ and _automated_ testing of tabular ML models, compared to data-only methods in various scenarios.

**Model reports.** SMART produces comprehensive and automated model reports documenting failure cases and justifications, thereby providing data scientists, ML engineers and stakeholders increased visibility into model failures. We provide an example SMART report in Appendix D.10.

**Practical considerations**. Given the potential utility of SMART we highlight the following five practical considerations: _Hypothesis generation._ While CAT offers a principled framework for context-guided testing, LLMs present challenges in hypothesis generation. Although SMART has mechanisms to address these (see Sec. 5.4), it cannot guarantee the absence of biases. _Use with small datasets._ SMART may be limited in some cases by insufficient real data to operationalize and test hypotheses, suggesting future work could explore targeted data collection or synthetic data generation to enhance testing. _Extensions to other modalities._ SMART is formalized to test tabular ML models, due to the interpretable and structured features in tabular data to guide hypothesis generation. While extensions to other modalities such as image and text is beyond the scope of this work, this is a promising future research direction that would require addressing the lack of explicitly interpretable features possibly via external metadata and developing new ways to operationalize hypotheses on unstructured data. That said, tabular data is ubiquitous in real-world applications  with approximately 79% of data scientists working on tabular problems daily, vastly surpassing other modalities . This highlights the immediate impact and relevance of SMART. _Need for interpretable/meaningful feature names._ Feature names play an important role in finding model failures (see Appendix D.9). The need for interpretable/meaningful feature names (e.g. column labels such as sex, age, race etc) as a source of context is similar to human requirements of interpretable feature names to understand what the data refers to. While feature names are typically the de facto in research and industry datasets, in the rare occasions they are not, this will affect the performance of SMART. _Cost of SMART._ SMART is extremely accessible and cheap to use, approximately \(<\)0.10 USD for 5 hypotheses and \(<\)0.5 USD for 100 hypotheses for state-of-the-art models (see Appendix D.7).

**Broader impact**. Better testing practices can help ensure models are reliable, safe, and beneficial before being deployed in real-world applications. We hope that our work can help mitigate model testing risks in real-world applications as well as spur new testing regimes which are _context aware_.