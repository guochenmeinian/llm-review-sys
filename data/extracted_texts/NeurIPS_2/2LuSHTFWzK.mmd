# On the cohesion and separability of average-link for hierarchical agglomerative clustering

Eduardo S. Laber

Departmento de Informatica, PUC-RIO

laber@inf.puc-rio.br &Miguel Batista

Departmento de Informatica, PUC-RIO

miguel260503@gmail.com

###### Abstract

Average-link is widely recognized as one of the most popular and effective methods for building hierarchical agglomerative clustering. The available theoretical analyses show that this method has a much better approximation than other popular heuristics, as single-linkage and complete-linkage, regarding variants of Dasgupta's cost function [STOC 2016]. However, these analyses do not separate average-link from a random hierarchy and they are not appealing for metric spaces since every hierarchical clustering has a \(1/2\) approximation with regard to the variant of Dasgupta's function that is employed for dissimilarity measures [Moseley and Yang 2020]. In this paper, we present a comprehensive study of the performance of average-link in metric spaces, regarding several natural criteria that capture separability and cohesion, and are more interpretable than Dasgupta's cost function and its variants. We also present experimental results with real datasets that, together with our theoretical analyses, suggest that average-link is a better choice than other related methods when both cohesion and separability are important goals.

## 1 Introduction

Clustering is the task of partitioning a set of objects/points so that similar ones are grouped together while dissimilar ones are put in different groups. Clustering methods are widely used for exploratory analysis and for reducing the computational resources required to handle large datasets.

Hierarchical clustering is an important class of clustering methods. Given a set of \(\) of \(n\) points, a hierarchical clustering is a sequence of clusterings \((^{n},^{n-1},,^{1})\), where \(^{n}\) is a clustering with \(n\) unitary clusters, each of them corresponding to a point in \(\), and the clustering \(^{i}\), with \(i<n\), is obtained from \(^{i+1}\) by replacing two of its clusters with their union \(A^{i}\). A hierarchical clustering induces a strictly binary tree with \(n\) leaves, where each leaf corresponds to a point in \(\) and the \(i\)th internal node, with \(i<n\), is associated with the cluster \(A^{i}\); the points in \(A^{i}\) correspond to the leaves of the subtree rooted in \(A^{i}\). Hierarchical clustering methods are often taught in data science/ML courses, are implemented in many machine learning libraries, such as scipy, and have applications in different fields as evolution studies via phylogenetic trees [Eisen et al., 1998], finance [Tumminello et al., 2010] and detection of closely related entities [Kobren et al., 2017, Monath et al., 2021].

Average-link is widely considered one of the most effective hierarchical clustering algorithms. It belongs to the class of _agglomerative methods_, that is, methods that start with a set of \(n\) clusters, corresponding to the \(n\) input points, and iteratively use a linkage rule to merge two clusters. Due to its relevance, we can find some recent works dedicated to improving average-link' efficiency and scalability [Yu et al., 2021, Dhulipala et al., 2021, 2022, 2023] as well as recent theoretical work that try to understand its success in practice [Cohen-Addad et al., 2019, Charikar et al., 2019a, Moseley and Wang, 2023, Charikar et al., 2019b].

Most of the available theoretical works give approximation bounds for average-link regarding the cost function introduced by (Dasgupta, 2016) as well as for some variants of it. Let \(\) be the tree induced by a hierarchical clustering. Dasgupta's cost function and its variation for dissimilarities considered in (Cohen-Addad et al., 2019) are, respectively, given by

\[()=_{a,b}(a,b)|D(a,b )|\;\;\;\;()=_{a,b}(a,b)|D(a,b)|, \]

where \((a,b)\) (\((a,b)\)) is the similarity (dissimilarity) of points \(a\) and \(b\); \(D(a,b)\) is the subtree of \(\) rooted at the least common ancestor of the leaves corresponding to \(a\) and \(b\), and \(|D(a,b)|\) is the number of leaves in \(D(a,b)\). In general, the existing results show that average-link achieves constant approximation for variants of Dasgupta's function while other linkage methods do not.

However, there is significant room for further analysis due to the following reasons. First, Dasgupta's cost function, despite its nice properties, is less interpretable than traditional cost functions that measure compactness and separability. Second, although the analyses based on Dasg and its variants allow to separate average-link from other linkage methods as single-linkage and complete-linkage in terms of approximation, they do not separate average-link from a random hierarchy (Cohen-Addad et al., 2019; Moseley and Wang, 2023; Charikar et al., 2019). Moreover, for the case in which the points lie in a metric space every hierarchical clustering has \(1/2\) approximation for the maximization of \(\)(Wang and Moseley, 2020), so this cost function is less appealing in this relevant setting. Finally, to the best of our knowledge, \(\) does not reveal how good are the clusters generated for a specific range of \(k\). As an example, small \(k\) are important for exploratory analysis while large \(k\) is important for de-duplication tasks (Kobren et al., 2017).

### Our results

Motivated by this scenario, we present a comprehensive study of the performance of average-link in metric spaces, with regards to several natural criteria that capture separability and cohesion of clustering. In a nutshell, these results, as explained below, show that average link has much better global properties than other popular heuristics when these two important goals are taken into account.

Let \((,)\) be a metric space, where \(\) is a set of \(n\) points. The diameter \((S)\) of a set of points \(S\) is given by \((S)=\{(x,y)|x,y S\}\). For a cluster \(A\) and for two clusters \(A\) and \(B\), let

\[(A)=}_{x,y A}(x,y)\;\; \;\;(A,B)=_{x A}_{y B }(x,y)\]

Let \(=(C_{1},,C_{k})\) be a \(k\)-clustering for \((,)\). To study separability we consider the average (\(_{}\)) and the minimum (\(_{}\)) avg among clusters in \(\), that is,

\[_{}():=}_{i j} (C_{i},C_{j})\;\;\;\;_{}( ):=_{i j}\{(C_{i},C_{j})\}, \]

On the other hand, for studying cohesion, we consider the maximum diameter (\(\)-\(\)) and the maximum average pairwise distance (\(\)-\(\)) of the clusters in \(\). In formulae,

\[():=\{(C_{i})|1  i k\}\;\;\;\;(): =\{(C_{i})|1 i k\} \]

We also study natural optimization goals that capture both the separability and the cohesion of a clustering. We define the \(_{}\) and \(_{}\) of a clustering \(\) as

\[_{}():=()}{_{}()}\;\; \;\;_{}():= ()}{_{}()} \]

Let \(^{k}\) be a \(k\)-clustering produced by \(\)-link. We first prove through a simple inductive argument that \(_{}(^{k}) 1\). This result does not assume that the points in \(\) lie in a metric space and it is tight in the sense that there are instances in which \(_{}()=1\) for every \(k\)-clustering \(\). For the related cs-ratio\({}_{}\) criterion, we present a more involved analysis which shows that cs-ratio\({}_{}(^{k})\) as well as the approximation of average-link regarding OPT (the minimum possible cs-ratio\({}_{}\)) are \(O( n)\); these bounds are nearly tight since there exists an instance for which cs-ratio\({}_{}(^{k})\) and cs-ratio\({}_{}(^{k})/\) are \(()\). Both cs-ratio\({}_{}\) and cs-ratio\({}_{}\) allow an exponential separation between average-link and other linkage methods, as single-linkage and complete-linkage. Interestingly, in contrast to CKMM (Eq. 1), our criteria also allow a very clear separation between average-link and the clustering induced by a random hierarchy.

Next, we focus on separability criteria. Let \(_{}(k)\) be the maximum possible \(_{}\) of a \(k\)-clustering for \((,)\). We show that \(_{}(^{k})\) is at least \(_{}(k)}{k+2 n}\) and that this result is nearly tight. Furthermore, we argue that any hierarchical clustering algorithm that has bounded approximation regarding max-diam or max-avg does not have approximation better than \(1/k\) to \(_{}\). Regarding single-linkage and complete-linkage, we present instances that show that their approximation with respect to \(_{}\) are exponentially worse than that of average-link, for the relevant case that \(k\) is small.

We also investigate the cohesion of average-link. For a \(k\)-clustering \(\), let avg-diam be the average diameter of the \(k\) clusters in \(\). Let \(_{}(k)\) and \(_{}(k)\) be, respectively, the minimum possible max-diam and avg-diam of a \(k\)-clustering for \((,)\). We prove that for all \(k\), \((^{k})\{k,1+4 n\}k^{_{2}3}_{}(k)\). This result together with the instance given by Theorem 3.4 of  allow to separate average-link from single-linkage, in terms of approximation, when \(k\) is \((^{2.41}n)\). We also show that max-diam\((^{k})\) is \((k)_{}(k)\), which is, to the best of our knowledge, the first lower bound on the maximum diameter of average-link.

Finally, to **complement** our study, we present some experiments with 10 real datasets in which we evaluate, to some extent, if our theoretical results line up with what is observed in practice. These experiments conform with our theoretical results since they also suggest that average-link performs better than other related methods when both cohesion and separability are taken into account.

### Related work

There is a vast literature about hierarchical agglomerative clustering methods. Here, we focus on works that provide provable guarantees for average-link and some other well-known linkage methods.

**Average-link**. There are works that present bounds on the approximation of average-link regarding some criteria . All these works but  analyse the approximation of average-link regarding variants of Dasgupta's cost function.  assumes that the proximity between the points in \(\) is given by a similarity matrix. They show that average-link is a \(1/3\)-approximation with respect to the "dual" of Dasgupta's cost function. , as in our work, assumes that the proximity between points in \(\) is given by a dissimilarity measure and shows that average-link has \(2/3\) approximation for the problem of maximizing CKMM (Eq. 1).  show that these approximation ratio for average-link are tight. These papers also show that a random hierarchy obtained by a divisive heuristic that randomly splits the set of points in each cluster matches the \(1/3\) and \(2/3\) bounds.

 presents an interesting approach to derive upper bounds on cohesion criteria for a certain class of linkage methods that includes average-link. They show that \((A) k^{1.59}_{}(k)\) for every cluster \(A^{k}\). Our bound on the maximum diameter of a cluster in \(^{k}\) incurs an extra factor of \(\{k,1+4 n\}\) to this bound and its proof combines their approach with some new ideas/analyses.

**Other Linkage Methods**. There are also works that give bounds on the diameter of the clustering built by complete-linkage and single-linkage on metric spaces . Let \(\) and \(\) be the \(k\)-clustering built by these methods, respectively.  shows that max-diam\(()\) is \((k_{}(k))\) while .

and Laber, 2024] shows that \(diam}()\) is \(O(\{k^{1.30}_{}(k),k^{1.59}_{}(k)\})\). Regarding \(linkage}\), \(diam}()\) is \((k_{}(k))\)[Dasgupta and Long, 2005, Arutyunova et al., 2023] and \((k^{2}_{}(k))\)[Dasgupta and Laber, 2024]. [Ackermann et al., 2010, Grosswendt and Roglin, 2015] give bounds for the case in which \(\) is the Euclidean metric.

In terms of separability criteria, it is well known that \(linkage}\) maximizes the minimum spacing of a clustering [Kleinberg and Tardos, 2006][Chang 4.7]. Recently, [Laber and Murtinho, 2023] observed that it also maximizes the cost of the minimum spanning tree spacing, a stronger criterion. These criteria, in contrast to ours, just take into account the minimum distance between points in different clusters and then they can be significantly impacted by noise.

[Grosswendt et al., 2019] shows that Ward's method gives a 2-approximation for \(k\)-means when the optimal clusters are well-separated.

## 2 Preliminaries

Algorithm 2 shows a pseudo-code for \(link}\). The function \(_{AL}(A,B)\) at line 3 that measures the distance between clusters \(A\) and \(B\) is given by

\[_{AL}(A,B):=_{a A}_{b B}(a,b).\]

\(linkage}\) and \(linkage}\) are obtained by replacing \(_{AL}\), in Algorithm 2, with \(_{SL}(A,B):=\{(a,b)|(a,b) A B\}\) and \(_{CL}(A,B):=\{(a,b)|(a,b) A B\}\), respectively.

```
1:\(^{n}\) clustering with \(n\) unitary clusters, each one containing a point of \(\)
2:For\(i=n-1\) down to \(1\)
3:\((A,B)\) clusters in \(^{i+1}\) for which \(_{AL}(A,B)\) is minimum
4:\(^{i}^{i+1}-\{A\}-\{B\}\{A B\}\)
```

**Algorithm 2** Average Link

A version of the triangle inequality for averages will be employed a number of times in our analyses. Its proof can be found in Section A.

**Proposition 2.1** (Triangle Inequality for averages).: _Let \(A\), \(B\) and \(C\) be three clusters. Then,_

\[(A,C)(A,B)+(B,C).\]

For two disjoint clusters \(A\) and \(B\), the following identity holds

\[(A B)=(A)+|A||B| (A,B)+(B).\]

Dividing both sides by \(\), we conclude that \((A B)\) is a convex combination of \((A),(B)\) and \((A,B)\), a fact will be used a couple of times in our analyses.

The following notation will be used throughout the text. We use \(H_{p}=_{i=1}^{p}\) to denote the \(p\)th harmonic number and \(^{k}\) to refer to the \(k\)-clustering obtained by \(link}\) for the instance under consideration, which will always be clear from the context.

## 3 Cohesion and separability

In this section, we analyze the performance of \(link}\) with respect to both \(ratio}_{}\) and \(ratio}_{}\) (Eq. 4), criteria that simultaneously take into account the separability and the cohesion of a clustering. Moreover, we contrast its performance with that achieved by other linkage methods.

### The cs-ratio\({}_{ AV}\) criterion

We first show that \(_{ AV}(^{k}) 1\). The proof of this result can be found in Section B.1, it uses induction on the number of iterations of average-link together with a fairly simple case analysis.

**Theorem 3.1**.: _Let \(^{k}\) be a \(k\)-clustering built by_ average-link_. Then, for every \(k\), \(_{ AV}(^{k}) 1\)._

We note that the above result does not assume the triangle inequality and it is tight in the sense that for the instance \((,)\), in which the \(n\) points of \(\) have pairwise distance 1, every clustering has \(_{ AV}\) equal to 1.

In Section B.2, we present instances which show that \(_{ AV}\) can be \((n)\), \(()\) and unbounded in terms of \(n\) for single-linkage, complete-linkage and a random hierarchy, respectively. Interestingly, all the \(k\)-clustering, with \(2<k n/2\), induced by the hierarchical clustering obtained by these methods satisfy these bounds. Furthermore, since \(_{ ON}()_{ AV}( )\) for every clustering \(\), these bounds also hold for the \(_{ ON}\) criterion.

A natural question that arises is whether average-link has a "good" approximation with respect to \(_{ AV}\). Unfortunately, the answer is no. In fact, in Section B.3 we show an instance where the approximation is unbounded in terms of \(n\). However, as we show in the next section, average-link has a logarithmic approximation with respect to \(_{ DW}\).

### The \(_{ DW}\) criterion

We analyze the \(_{ DW}\) of average-link. The results of this section will have an important role in the analysis of both the separability and cohesion of average-link presented further.

First, we show that for every cluster \(X\) in \(^{k}\), the average distance of a point \(x X\) to the other points in \(X-x\) is at most a logarithmic factor of the average distance between any two clusters \(Y\) and \(Z\). The proof can be found in Section B.5. Let \(T_{i-1}\) be the cluster that contains \(x\) before the \(i\)th merge involving \(x\) and let \(S_{i}\) be the cluster that is merged with \(T_{i-1}\). We prove by induction that \((x,T_{i}-x) H_{|T_{i}|-1}(Y,Z)\), which implies on the desired result because \(T_{t}=X\) for some \(t\). To establish the induction, we use the triangle inequality to write \((x,T_{i}-x)\) as a function of both \((x,T_{i-1}-x)\) and \((T_{i-1},S_{i})\), and also argue that \((T_{i-1},S_{i})(X,Y)\).

**Lemma 3.2**.: _Let \(X\), \(Y\) and \(Z\), with \(|X| 2\) and \(Y Z\), be clusters of \(^{k}\). Then, for every \(x X\), we have that \((x,X)(x,X-x) H_{|X|-1}(Y,Z)\)._

The next result is a simple consequence of the previous one.

**Theorem 3.3**.: _Let \(k 2\) and let \(X\), \(Y\) and \(Z\), with \(Y Z\), be clusters of a \(k\)-clustering built by_ average-link_. Then, \((X) 2H_{|X|-1}(Y,Z)\)._

Proof.: If \(|X|=1\) the result holds because \((X)=0\). Thus, we assume that \(|X|>1\). Let \(x\) and \(x^{}\) be such that \((x,x^{})=(X)\). We have that

\[(x,x^{})(x,X)+(X,x^{})  2H_{|X|-1}(Y,Z)\]

where the first inequality follows from the triangle inequality and the second one due to Lemma 3.2. 

The next theorem shows that \(_{ DW}(^{k}) 2H_{n}\) and that average-link has a logarithmic approximation for the \(_{ DW}\) criterion. The first upper bound is a simple consequence of Theorem 3.3. Let OPT be the minimum possible \(_{ DW}\). To prove the bound on the approximation we consider two cases. If OPT \( 1/3\) the result holds because \(_{ DW}(^{k}) 2 n 6 n\). If OPT \(<1/3\), we argue that the clusters in the optimal clustering are "well separated" and, hence, average-link builds the optimal clustering.

**Theorem 3.4**.: _For all \(k\), the \(k\)-clustering \(^{k}\) built by_ average-link _satisfies \(_{ DW}(^{k}) 2H_{n}\). Furthermore, for all \(k\), \(_{ DW}(^{k})\) is \(O( n)\) where OPT is \(_{ DW}\) of the \(k\)-clustering with minimum possible \(_{ DW}\)._Proof.: The inequality \(_{}(^{k}) 2H_{n}\) is obtained by using Theorem 3.3, with \(X\) being the cluster with the largest diameter in \(^{k}\) and \(Y\) and \(Z\) being the clusters in \(^{k}\) that satisfy \((Y,Z)=_{}(^{k})\).

Now we prove that \(^{k}\) has logarithmic approximation. If \( 1/3\), then \(_{}(^{k}) 2H_{n} 6H_{n}\) and, hence, the desired result holds.

Thus, we assume \(<1/3\), Let \(^{*}(k)\) be a \(k\)-clustering that satisfies \(_{}(^{*}(k))=\). The following claim will be useful.

_Claim 1_.: Let \(C,C^{}\) be two clusters in \(^{*}(k)\) and let \(a,b\) be two closest points in \(C\) and \(C^{}\), that is, \((a,b)=\{(x,y)|(x,y) C C^{}\}\). Thus, \((a,b)>\{(C),(C^{})\}\).

Proof of the claim.: We assume w.l.o.g. that \((C)(C^{})\). For the sake of reaching a contradiction, assume that \((a,b)(C)\). Then, it follows from the triangle inequality that the maximum distance between a point in \(C\) and \(C^{}\) is at most \(3(C)\). Thus, \(_{}(^{*}(k))(C,C^{})  3(C)\) and so \(_{}(^{*}(k))(C)/3 (C)=1/3\), which contradicts our assumption. \(\).

Now, we argue that \(\) constructs the clustering \(^{*}(k)\) when \(_{}(^{*}(k))<1/3\), so its approximation is 1 in this case. For the sake of reaching a contradiction, let us assume \(^{k}^{*}(k)\). Hence, at some iteration \(\) merges two clusters, say \(A\) and \(B\), that satisfy the following properties: \(A C\) and \(B C^{}\), where \(C\) and \(C^{}\) are two different clusters in \(^{*}(k)\). Let \(t\) be the first iteration of \(\) when it occurs.

Case 1) \(A C\) or \(B C^{}\). Let us assume w.l.o.g. that \(A C\). In this case, there is a cluster \(A^{}\) at the beginning of iteration \(t\) such that \(A^{} A C\). We have that \((A,A^{})(C)\) and by the above claim the minimum distance between \(A\) and \(B\) is larger than \(\{(C),(C^{})\}\). Thus, \((A,B)>\{(C),(C^{})\} (A,A^{})\), which contradicts the choice of \(\).

Case 2) \(A=C\) and \(B=C^{}\). If \(k=2\) we are done. Otherwise, there exists a cluster \(C^{}^{*}(k)\) and two clusters \(X\) and \(Y\) at the beginning of iteration \(t\) such that \(X Y C^{}\). Thus, it follows from the condition \(<1/3\) that \((X,Y)(C^{})<_{}(^{*}(k))(C,C^{}) (C,C^{}),\) which again contradicts the choice of \(\). 

It is noteworthy that, in contrast to Theorem 3.1, the assumption that the points lie in a metric space is necessary to prove Theorem 3.4. In Section B.4 we present an instance that supports this observation.

Now, we present an instance, denoted by \(^{CS}\), that shows that the above results are nearly tight. This instance with small modifications will also be used to investigate the tightness of our results regarding the separability (Section 4) and the cohesion (Section 5) of \(\). We note that in most of the instances presented here, including \(^{CS}\), will have more than one possible execution for the methods we analyze. In these cases, we will always consider the execution that is more suitable for our purposes. These multiple executions can be avoided at the price of more complicated descriptions that involve the addition of small values \(\) to the distance or points to break ties.

Let \(t\) be an integer that satisfies \(t!=n\); note that \(t=()\). Moreover, let \(A_{0}\) be a set containing a single point located at position \(p_{0}\) in the real line and \(A_{i}\), for \(0<i t-1\), be a set of \((i+1)!-i!\) points that are located at position \(p_{i}\) of the real line. We define \(B_{0}=A_{0}\) and \(B_{i}=B_{i-1} A_{i}\), for \(i 1\). Set \(p_{0}=0,p_{1}=1\) and, for \(i>1\), \(p_{i}=p_{i-1}+(A_{i-1},B_{i-2})\). The set of points for our instance \(^{CS}\) is \(B_{t-1}\) and the distance between a point in \(A_{i}\) and a point in \(A_{j}\) is \(|p_{i}-p_{j}|\). The following lemma gives properties of \(^{CS}\) and, in particular, how \(\) behaves on it.

**Lemma 3.5**.: _For \(i 0\), we have that \(|B_{i}|=(i+1)!\) and for \(i 2\), we have \((B_{i-2})=i(i-1)/2\), \((B_{i-2},A_{i-1})=i+1\) and \(p_{i}=i(i+1)/2\). Furthermore, for \(k t\), \(\) obtains the \(k\)-clustering \(^{k}=(B_{t-k},\,A_{t-k+1},,A_{t-1})\) and, in particular, for \(k=2\) it obtains the clustering \(^{2}=(B_{t-2},A_{t-1})\)._

From Lemma 3.5, we have that \(_{}(^{2})=(B_{t-2},A_{t-1})=t+1\) and \((B_{t-2})=t(t-1)/2\), so \(_{}=\), which is \(()\).

Furthermore, for the clustering \(^{}=(A_{0},B_{t-1}-A_{0})\) we have that

\[_{}(^{})=(A_{0},B_{t-1}-A_{ 0})|}{|B_{t-1}|}(A_{0},A_{t-1})=()p_{t-1}=}{2} \]

and \((^{})(B_{t-1})=(t+1)(t+2)/2\). Thus, \(_{}(^{})=O(1)\) and the logarithmic approximation of average-link to \(_{}\) is also nearly tight.

## 4 Separability criteria

In this section, we investigate the separability of average-link. Recall that \(_{}(k)\) is the maximum possible \(_{}\) of a \(k\)-clustering for \((,)\). We show that for average-link sepav is at least \(_{}(k)}{k+2 n}\) and that this bound is nearly tight. We also show that there are instances in which the sepav of single-linkage and complete-linkage are exponentially smaller than that of average-link.

Theorem 4.2 gives an upper bound on sepav for average-link and its complete proof can be found in Section D.2. Here, we give an overview of the proof for the case \(k>2\), which is the most involved one. The proof uses the fact established by Proposition 4.1 that there exists a set of \(k\) points \(P\) that satisfies \((P)_{}(k)\). This holds because a set of \(k\) randomly selected points that intersect all clusters of a \(k\)-clustering with maximum sepav satisfies the the desired property (in expectation). Having this result in hands, it is enough to show that avg\((P)\) is \(O((k+H_{n-1})_{}(^{k}))\).

This bound on avg\((P)\) is obtained by relating the distance of each pair of points \(p,p^{} P\) with the average distance between clusters in \(^{k}\). Let \(p,p^{} P\) and let \(A\) and \(A^{}\) be clusters in \(^{k}\) such that \(p A\) and \(p^{} A^{}\). Moreover, let \(S\) be a cluster in \(^{k}\), with \(S\{A,A^{}\}\). From the triangle inequality we have that \((p,p^{})=(p,p^{})(p,A)+ (A,S)+(S,A^{})+(A^{},p^{ })\). Then, by bounding both avg\((p,A)\) and avg\((A^{},p^{})\) via Lemma 3.2, with \(Y\) and \(Z\) satisfying \((Y,Z)_{}(^{k})\), we conclude that \((p,p^{}) 2H_{n}_{}(^{k})+ (A,S)+(S,A^{})\). In general lines, the result is then established by averaging this inequality for all \(S\{A,A^{}\}\) and for all \(p,p^{} P\).

**Proposition 4.1**.: _There is a set of points \(P\) with the following properties: \(|P|=k\) and \((P)_{}(k)\)._

**Theorem 4.2**.: _For every \(k\), the \(k\)-clustering \(^{k}\) obtained by average-link satisfies \(_{}(^{k})_{ }(k)}{k+2H_{n}}\)._

We present two instances that, together, show that the previous theorem is nearly tight. The first is the instance \(^{CS}\) presented right after Theorem 3.4. For \(^{CS}\), the clustering \(^{2}=(A_{t-1},B_{t-2})\) built by average-link satisfies sepav\((^{2})=(A_{t-1},B_{t-2})=t+1\). On the other hand, Eq. (5) shows that sepav\((^{})=}{2}\), for the clustering \(^{}=(A_{0},B_{t-1}-A_{0})\). Thus, for \(^{CS}\), sepav\((^{2})\) is \(O(_{}(k) n}{ n})\).

Now, we present our second instance, denoted by \(^{sep}_{k}\). Let \(k\) be an odd number and let \(D\) and \(\) be positive numbers. The set of points of \(^{sep}_{k}\) is given by \(S_{1} S_{2} S_{3}\), where \(|S_{1}|=|S_{2}|=(k-1)/2\) and \(S_{3}=\{s_{i}|1 i k-2\}\). We have dist\((x,y)=\) for \(x,y S_{1}\), dist\((x,y)=\) for \(x,y S_{2}\), dist\((x,y)=1\) for \(x,y S_{3}\) and dist\((x,y)=D\) if \(x\) and \(y\) are (in terms of \(n\)) regarding max-diam or to max-avg (Equation 3) has to build the \(k\)-clustering \(^{k}\) for \(^{sep}_{k}\). Thus, by analysing \(^{sep}_{k}\) we can conclude that the approximation factor of \(M\) to \(_{}\) is \(O(1/k)\) and to \(_{}\) is \(O(1/D)\). The details can be found in Section D.4.

## 5 On the cohesion of average-link

In this section, we prove that \(diam}(^{k})\{k,1+4 n\}k^{1.59}_{}(k)\) and we also present an instance which shows that \(diam}(^{k}) k_{}(k)\).

Dasgupta and Laber (2024) presented an interesting approach to devise upper bounds on cohesion criteria for a class of linkage methods that includes \(link}\). Although this approach was used to show that the maximum pairwise average distance of a cluster in \(^{k}\) is at most \(k^{1.59}_{}(k)\), it cannot be employed, at least directly, to bound the maximum diameter of a cluster in \(^{k}\). Thus, to obtain our \((1+4 n)k^{1.59}_{}(k)\) bound we combine the results of (Dasgupta and Laber, 2024) with Theorem 3.4 while for the \(k^{1+1.59}_{}(k)\) bound we add some new ideas/analysis on top of those from (Dasgupta and Laber, 2024).

The analysis in Dasgupta and Laber (2024) keeps a dynamic partition of the clusters produced by the linkage method under consideration. Each group in the partition is a set of clusters denoted by _family_. A point \(p\) belongs to a family \(F\) if it belongs to some cluster in \(F\). Thus, \((F)\) is given by the maximum distance among the points that belong to \(F\). The approach bounds the diameter of each family \(F\) as (essentially) a function of the clusters that \(F\) touches in a target \(k\)-clustering \(=(T_{1},,T_{k})\). The bound on \((F)\) is then used to upper bound the diameter of the clusters in \(F\). For a \(k\)-clustering \(\), let \(diam}():=_{i=i}^{k} (C_{i})\). As in Dasgupta and Laber (2024), we use as the target clustering the one with minimum \(diam}\).

We explain how the families evolve along the execution of a linkage method, in particular \(link}\). Initially, we have \(k\) families, \(F_{1},,F_{k}\), where \(F_{i}\) is a family that contains \(|T_{i}|\) clusters, each one being a point from \(T_{i}\). Furthermore, the families are organized in a directed forest \(D\) that initially consists of \(k\) isolated nodes, where the \(i\)th node corresponds to family \(F_{i}\).

We specify how the families and the forest \(D\) are updated when the linkage method merges the clusters \(g\) and \(g^{}\) belonging to the families \(F\) and \(F^{}\), respectively. Assume w.l.o.g. \(|F||F^{}|\). We have the following cases:

* \(|F^{}|=1\) and \(|F|>1\). In this case two new families are created, \(F^{new}:=F-\{g\}\) and \(F^{new^{}}:=\{g g^{}\}\). Moreover, \(F^{new}\) and \(F^{new^{}}\) become, respectively, parents of \(F\) and \(F^{}\) in \(D\)
* \(|F^{}|>1\) or \(|F|=1\). In this case, only one family is created, \(F^{new}:=(F F^{}\{g g^{

[MISSING_PAGE_EMPTY:9]

## 6 Experiments

In this final section, we briefly present an experiment in which we evaluate whether average-link, in addition to having better theoretical bounds, it also has a better performance in practice for the studied criteria. We employed 10 datasets and used the Euclidean metric to measure distances. For each of them, we executed average-link, complete-linkage and single-linkage, for the following sets of values of \(k\): Small=\(\{k|2 k 10\}\), Medium=\(\{k|-4 k+4\}\) and Large=\(\{k|k=n/i2 i 10\}\). More details, as well as the results of our experiment with other distances, can be found in Section F.

Table 6 shows the average ratio between the result of a method and that of the best one, grouped by criterion and set of \(k\). Each entry is the average of 90 ratios (9 \(k\)'s and 10 datasets) and each of these ratios for a method \(M\) is a value between 0 and 1 that is obtained by dividing the minimum between the result of \(M\) and that of the best method by the maximum between them. The letters A, C and S are the initials of the evaluated methods.

Concerning separability criteria, single-linkage and average-link have the best results for sepav. The latter has some advantage when \(k\) is small, which is in line with its better worst-case bound for small \(k\) (results from Section 4). For sepmin, average-link has a huge advantage, which is not surprising since its linkage rule tries to increase sepmin at each step by merging the the clusters \(A\) and \(B\) for which avg\((A,B)=()\), where \(\) is the current clustering.

Regarding cohesion criteria, complete-linkage and average-link were the best methods. They had close results for max-avg while for max-diam the former had a strong dominance. These results align with ours and those from , in the sense that they show that these linkage methods present better worst-case upper bounds than single-linkage when the comparison is made against OPTav\((k)\). Moreover, the advantage of complete-linkage for max-diam is also expected since it is the "natural" greedy rule to minimize the maximum diameter (See Proposition 2.1 of Dasgupta and Laber ).

For cs-ratioDM, average-link and complete-linkage present the best results, with the former being slightly superior for the small \(k\) and the latter being slightly superior when \(k\) is not small. average-link has a huge dominance for the cs-ratioAV criterion, which lines up with the theoretical results from Section 3.1.

In summary, these experiments, together with our theoretical results, provide evidence that average-link is a better choice when both cohesion and separability are relevant.

**Limitations.** We have not identified a major limitation in our work. That said, the assumption that the points lie in a metric space used in our results (except Theorem 3.1) could be seen as a limitation. On the experimental side, having more than 10 datasets would give our conclusions more robustness.