# StreamFlow: Streamlined Multi-Frame Optical Flow Estimation for Video Sequences

Shangkun Sun

SECE, Peking University

Peng Cheng Laboratory

sunshk@stu.pku.edu.cn &Jiaming Liu

Tiamat AI

james.liu.nl@gmail.com &Huaxia Li

Xiaohongshu Inc.

lihx0610@gmail.com &Guoqing Liu

Minieye Inc.

liugq@ntu.edu.sg &Thomas H Li

SECE, Peking University

thomas@pku.edu.cn &Wei Gao

SECE, Peking University

Peng Cheng Laboratory

gaowei262@pku.edu.cn &Corresponding author.

###### Abstract

Prior multi-frame optical flow methods typically estimate flow repeatedly in a pairwise manner, leading to significant computational redundancy. To mitigate this, we implement a Streamlined In-batch Multi-frame (SIM) pipeline, specifically tailored to video inputs to minimize redundant calculations. It enables the simultaneous prediction of successive unidirectional flows in a single forward pass, boosting processing speed by \(44.43\%\) and reaching efficiencies on par with two-frame networks. Moreover, we investigate various spatiotemporal modeling methods for optical flow estimation within this pipeline. Notably, we propose a simple yet highly effective parameter-efficient Integrative spatiotemporal Coherence (ISC) modeling method, alongside a lightweight Global Temporal Regressor (GTR) to harness temporal cues. The proposed ISC and GTR bring powerful spatiotemporal modeling capabilities and significantly enhance accuracy, including in occluded areas, while adding modest computations to the SIM pipeline. Compared to the baseline, our approach, StreamFlow, achieves performance enhancements of \(15.45\%\) and \(11.37\%\) on the Sintel clean and final test sets respectively, with gains of \(15.53\%\) and \(10.77\%\) on occluded regions and only a \(1.11\%\) rise in latency. Furthermore, StreamFlow exhibits state-of-the-art cross-dataset testing results on Sintel and KITTI, demonstrating its robust cross-domain generalization capabilities. The code is available here.

## 1 Introduction

Optical flow estimation, which aims to model the per-pixel correspondence between two consecutive frames, is a fundamental task in computer vision. It has various downstream video applications, such as video generation , video editing , and video compression . In video streams, there is frequently a need for multi-frame input and continuous optical flow estimation.

Nevertheless, previous multi-frame methods usually perform spatiotemporal modeling in a pairwise way, which leads to redundant computation, as depicted in Fig. 1 and Alg. 1. Given input frames \(I_{t-1},I_{t},I_{t+1}\), previous pairwise methods such as VideoFlow  only output _one_ forward flow \(F_{t t+1}\). To derive the consecutive flow \(F_{t-1 t}\), one additional forward pass with the input of \(I_{t-2},I_{t-1},I_{t}\) is needed.

This gives rise to the design of what we refer to as in-batch estimation --a pipeline that simultaneously predicts all successive unidirectional flows within _one_ forward pass, which greatly reduces the redundant computation. For instance, given input frames \(I_{t-1},I_{t},I_{t+1}\), all consecutive flows \(F_{t-1 t}\) and \(F_{t t+1}\) could be derived within _one_ forward pass. It directly saves one forward pass time in the decoder. When \(T\) frames are input simultaneously in the batch, there is still only _one_ forward pass needed to obtain \(T-1\) flows, and more time is served on average as \(T\) increases.

Nevertheless, despite the acceleration brought by the in-batch estimation, its alteration of the pipeline renders it unable to directly apply past pairwise spatiotemporal modeling methods [16; 4; 38]. Notably, an effective spatiotemporal modeling approach is the key to resolving problems such as occlusions for multi-frame methods. Thus, this raises a new key question: _Under the constraint of in-batch estimation, how to perform effective spatiotemporal modeling while maintaining efficiency?_

In this work, we propose StreamFlow, a streamlined multi-frame optical flow estimation method tailored for video inputs. StreamFlow is made efficient through the Streamlined In-batch Multi-frame (SIM) pipeline, which avoids repetitive calculations when predicting unidirectional flows for videos. Furthermore, StreamFlow explores the challenge of effectively modeling spatiotemporal cues under the constraint of non-overlapping in-batch estimation. StreamFlow explores various spatiotemporal modeling methods and ultimately derives simple yet highly effective methods: a parameter-efficient Integrative spatiotemporal Coherence (ISC) module during encoding, and a lightweight Global Temporal Regressor (GTR) to decode all flows. Benefiting from these modules, StreamFlow achieves remarkable performance on Sintel and KITTI datasets _without self-supervised pre-training and the aim of bidirectional flows_. Moreover, StreamFlow attains state-of-the-art cross-dataset generalization results with comparable efficiency compared to two-frame methods, as illustrated in Figure 2.

In summary, our contributions are as follows: (1) We propose a Streamlined In-batch Multi-frame (SIM) pipeline for optical flow estimation, which eliminates the repetitive overlapping computation when computing forward flows for video inputs. (2) Under the constraint of a non-overlapping pipeline, we specifically designed the Integrative spatiotemporal Coherence (ISC) method to perform spatiotemporal modeling without additional parameters. (3) For the SIM pipeline, we devise a Global Temporal Regressor (GTR) during decoding to further exploit temporal cues with modest additional computation cost. (4) The proposed StreamFlow achieves superior performances on multiple benchmarks, particularly in occluded regions with comparable efficiency compared with two-frame methods, resulting in substantial improvements in optical flow estimation.

## 2 Related work

Two-frame optical flow.Optical flow estimation in the form of a supervised learning task has been performed by FlowNet  using Convolutional Neural Networks (CNN). The encoder-decoder architecture of FlowNet predicts flow from coarse to fine using the hierarchy of the flow pyramid. Thereafter, a number of refined coarse-to-fine approaches [12; 42; 43; 10; 11; 50; 54; 13] emerged.

Figure 1: Comparison between the pairwise and the proposed Streamlined In-batch Multi-frame (SIM) pipeline. Short dashed lines represent additional computations.

The flow pyramid is constructed for the coarse-to-fine approach, which predicts the flow based on the flow guidance at a higher pyramid level. However, the flow guidance is often too coarse to capture small motions delicately and creates errors in later estimation. RAFT  recently introduced an iterative all-pairs flow transform technique, which enables the prediction of high-resolution flow and recurrent refinement of the residual flow estimation. It addresses the challenges of small motions and has consequently received high interest and performance in the field, inspiring numerous follow-up works . To further address the occlusion issue, SKFlow  begins by expanding the spatial receptive field and designing effective large convolution kernel modules in the decoder of the flow network, with modest computational cost. Although StreamFlow is based on the SKFlow framework, it is designed to explore additional temporal cues and minimize redundant computations that are prevalent in earlier multi-frame methods. Their methodologies and foundational principles differ significantly.

Multi-frame optical flow.Exploiting temporal cues in optical flow estimation is an effective way to recover the occluded motion. Previous works  propose various approaches to fuse temporal cues, such as leveraging previously predicted motion feature, optical flow, or contextual information. For instance, ContinualFlow  uses previous flow priors to estimate the current occlusion map. STARFlow  passes extracted features in multiple scales, jointly with occlusion maps.  proposes a warm-start strategy to initialize the original flow with the past flow before prediction. MFCFlow  and MFRFlow  propose to leverage previously estimated motion features during decoding via feed-forward CNNs and self-similarity modeling, respectively. Nevertheless, these methods obtain a pairwise strategy when handling video sequences, which divide the input sequence into lots of overlapping groups and take huge repeated computations. SplatFlow  utilizes the differentiable splatting transformation to explore temporal cues. TransFlow  decodes all flows simultaneously and achieves impressive results. However, it needs self-supervised pre-training on the flow datasets to help the temporal modeling modules converge. Besides, its pure transformer architecture does not have advantages in time. VideoFlow  employs TROF and MOP modules to utilize multi-frame temporal cues and bidirectional optical flow to effectively mitigate occlusion issues. Nevertheless, it still follows the pairwise method to predict multiple unidirectional flows with the cost of predicting bidirectional flows. Differently, StreamFlow is proposed to avoid redundant, overlapping computation for consecutive unidirectional flow predictions while exploring efficient and effective temporal modules design under such a pipeline. It addresses the redundancy problems previous pairwise methods including VideoFlow encountered, and achieves excellent accuracy with latency similar to some two-frame methods.

## 3 Methodology

```
Input: frames \(I_{i}\), size \(N\), group size \(T\)  Initialize \(i=1\), stride \(T-1\). repeat \(F_{i},...,F_{i+stride-1}=Model(I_{i},...,I_{i+stride})\) \(i=i+stride\) until\(i+stride>N\) \(j=N-stride\) \(F_{j},F_{j+1},...,F_{N}=Model(I_{j},I_{j+1},...,I_{N})\).
```

**Algorithm 1** Pairwise Multi-frame Estimation

In this Section, we introduce StreamFlow, an efficient and effective in-batch framework for multi-frame optical flow estimation. Extensive experiments guided our specific design of each module in StreamFlow. The key components of StreamFlow consist of three parts: (1) The Streamlined In-batch Multi-frame (SIM) pipeline for efficient multi-frame estimation, which contributes to the speed improvement of StreamFlow. (2) Integrative spatiotemporal

Figure 2: Comparison between performance and efficiency. A larger bubble denotes more parameters. Models are trained via the (C+)T schedule and tested on the Sintel final pass.

poral Coherence (ISC) modeling, which is parameter-efficient and is specifically designed for spatiotemporal modeling in the encoder. (3) Global Temporal Regressor (GTR), which is quite lightweight and learns temporal relations during decoding. We will first give an overview of our methods in Section 3.1, and then introduce each module in Section 3.2, Section 3.3, and 3.4, respectively. In the end, we discuss the supervision in Section 3.5.

### Overview

The overall framework of StreamFlow is illustrated in Figure 3. For the basic encoder and decoder, similar to VideoFlow , StreamFlow adopts the Twins transformer  as the encoder and utilizes the motion encoder and updater in SKFlow  during decoding. The overall iterative-refinement design that adopts an iterative decoder is the paradigm proposed in IRR  and followed by a lot of subsequent works [44; 41; 15; 44; 9; 39]. Input frames are first passed to two feature encoders that share the same architecture to extract the correlation feature and contextual feature, respectively. Then, the multi-scale all-pairs correlation volume is calculated based on the correlation feature. Namely, given feature embeddings \(_{1}\) and \(_{2}\) from the target frame and the reference frame, respectively:

\[^{l}(i,j,m,n)=}_{u}^{2^{l}}_{v}^{2^{l}} _{1}(i,j),_{2}(2^{l}m+u,2^{l}n+v), \]

where the derived \(^{l}(i,j,m,n)\) is the average over the correlation in the local \(2^{l} 2^{l}\) window. \(l\) denotes the \(l^{th}\) correlation level. \(u\) and \(v\) are the horizontal and vertical pixel motions, respectively. \(,\) refers to the dot product function. In summary, \(^{l}(i,j,m,n)\) means the cost volume vector of \(_{1}\) and \(_{2}\) pooled with the \(2^{l} 2^{l}\) kernel.

Then, the iterative decoder refines the flows via several updates. As depicted in Figure 3, flows are initialized to zeros. The derived multi-scale correlation volume, extracted context feature, and the initialized flows are passed to the decoder, and then the refinement is conducted.

### Streamlined in-batch multi-frame pipeline

As illustrated in Figure 1, previous pairwise multi-frame networks typically perform redundant computations for video inputs, resulting in substantial computational overlap. We have briefly discussed this issue in the first two paragraphs of Section 1. Specifically, given \(N\) frames \(\{I_{1},I_{2},...,I_{N}\}\) and a group size of \(T\) (\(N T 3\)), pairwise methods need to form \(N-T+1\) groups, namely, \(\{I_{1},I_{2},...,I_{T}\},\{I_{2},I_{3},...,I_{T+1}\},...,\{I_{N-T+1},...,I_{ N}\}\). In each group, pairwise methods use spatiotemporal cues from other frames to compute the optical flow only for the current frame. In this case, \(T-1\) times of forward process is performed to derive \(T-1\) flows. In contrast, StreamFlow introduces a Streamlined In-batch Multi-frame (SIM) Pipeline designed to minimize redundancy. For the same input, the SIM pipeline only forms \(\) groups, namely,

Figure 3: Overview of StreamFlow. (a) illustrates the overall framework and \(<\),\(>\) denotes the dot-product operation. The computation of cost volume is limited to adjacent frames and is performed once in one forward pass. Flows are initialized to zeros. (b) depicts the details of the GTR decoder.

\(\{I_{1},I_{2},...I_{T}\},\{I_{T},I_{T+1},I_{2T-1}\},...,\{I_{N-T+1},...,I_{N}\}\). In this case, frames are divided into non-overlapping groups (except for the first frame of each group). Within each group, the SIM pipeline calculates all uni-directional flows by modeling the spatiotemporal cues interconnecting all frames. In other words, only _one_ forward pass is needed to derive \(T-1\) flows. From this process, we could learn that the upper limit of the saved time for the SIM pipeline is approximately \(\), where \(\) represents the computational time of the decoder. To approach the theoretical limit as closely as possible, a lightweight yet effective method of spatiotemporal modeling is crucial. This is precisely the key contribution that StreamFlow adds to the SIM pipeline, which will be discussed in detail in Section 3.3 and Section 3.4. Additionally, when directly comparing StreamFlow with other multi-frame methods in practice, we find that the speed improvements with StreamFlow far exceed the theoretical limits calculated above. This is because StreamFlow inherently includes a memory bank mechanism in its encoder, whereas previous multi-frame methods required special additional implementation to utilize Memory banks for caching features. Nevertheless, even when compared to methods that include a memory bank, StreamFlow still achieves significant speed advantages due to its efficiency in the decoder, as demonstrated in Figure 2 and Appendix A.1.

### Integrative spatiotemporal coherence

During the encoding process, we propose an Integrative spatiotemporal Coherence (ISC) modeling method, especially for the SIM pipeline. Our design principles for temporal modeling modules in the decoder encompass two facets: firstly, adherence to the design criteria of the SIM pipeline, with a focus on minimizing pair-wise overlap operations, such as the computation of cross-frame attention between every pair of consecutive frames. Secondly, the modules should be efficient enough and not impede the overall speed of the network.

Therefore, we design the ISC method, which introduces no additional parameters while learning spatiotemporal relations efficiently and effectively. The ISC method inherently takes the original modules in Twins. Specifically, after deriving patch embeddings from consecutive frames, patches from different frames are reorganized along the spatial dimension. Subsequently, it models the derived spatiotemporal graph using self-attention mechanisms and feed-forward layers in Twins, which can be formulated as,

\[_{d}^{j} =(_{1,d}^{j},_{2,d}^{j},..., _{T,d}^{j}), \] \[_{d}^{j} =f((_{d}^{j}),(_{d}^{j})) (_{d}^{j}),\] (3) \[_{d}^{j} =_{d}^{j}+_{}_{d}^{j}, \]

where \(f(,)\) is the attention function which conducts dot-product and softmax operation, \(_{t,d}^{j}\) is the \(j^{th}\) vector along spatial dimension at channel \(d\) of the \(t^{th}\) frame. \(\) denotes the _integration_ operation, which integrates temporally contiguous multiple input embeddings into a large feature embedding along the spatial dimension. \(,\) and \(\) is the derived query, key, and value vector. \(_{}\) is the projection matrix. By leveraging the derived spatiotemporal graph, the spatial and temporal relations are learned effectively, and no additional parameters are involved.

### Global temporal regressor

As for the decoder, we propose a Global Temporal Regressor (GTR) to predict and refine flows. Compared with the previous widely used decoder [45; 15; 44; 28; 53; 27], GTR introduces the temporal modeling module to exploit temporal cues from consecutive frames. Different from VideoFlow  that concatenates motion features along a temporal dimension and implicitly learns temporal relations or TransFlow  that applies a transformer symmetric to the encoder, the core of GTR is super convolution kernels  and a lightweight temporal transformer block. The input correlation volume, initialized flows, and contextual features are first passed into a motion encoder to derive motion features and then extracted for spatiotemporal features, which can be formulated as:

\[_{i}^{k} =(_{i-1}^{k},^{k,k+1}), \] \[_{i} =_{t-1}^{T}(_{i}^{t}),\] (6) \[_{i}^{k} =(_{i}^{k},^{k}),\] (7) \[_{i}^{k} =(_{i},_{i}^{k}),\] (8) \[_{i}^{k},_{i}^{k} =(_{i}^{k},_{i}^{k},_{i- 1}^{k}),\] (9) \[_{i}^{k} =_{i-1}^{k}+_{i}^{k} \]

where \(_{i}^{k}\) is the derived motion feature of frame \(k\) at the \(i^{th}\) update and \(_{i-1}^{k}\) denotes the flow of frame \(k\) after \(i-1^{th}\) refinement. \(^{k,k+1}\) denotes the correlation volume between frame \(k\) and \(k+1\). \(\) denotes the motion encoder which is the same as that in SKFlow . \(_{i}\) denotes the temporal feature embedding extracted from the motion features of all frames. Notably, the caching mechanism of the MemoryBank is employed, thus necessitating the calculation of \(_{i}\) only once for different frames. \(\) is a lightweight temporal-learning layer that consists of temporal attention and feed-forward layers. \(^{k}\) refers to the feature embedding of frame \(k\). Note that \(e^{k}\) and \(c^{k,k+1}\) are not updated during the refinement. \(\) denotes the spatial cross attention inspired by , but takes \(_{i}^{k}\) and \(^{k}\) as the input. \(\) represents the concatenation operation and \(\) refers to the motion updater. \(_{i}^{}\) denotes the extracted contextual information, which will be updated during each refinement. In practice, the decoder estimates the residual of flow \(_{i}^{k}\). And the final flow \(^{k}\) is updated via \(_{i}^{k}\) during each refinement.

### Supervision

StreamFlow adopts the overall loss in the same group as the total loss function. For each flow, StreamFlow adopts the same loss function as successful two-frame networks. Namely, the weighted sum for the predicted flows at different refinements. During both the training and the fine-tuning process, the supervision can be formulated as follows:

\[=_{k=1}^{T-1}_{i=1}^{N}^{N-i}\;||_{i}^{k}- _{gt}^{k}||_{1}, \]

where \(_{i}^{k}\) refers to the flow of frame \(k\) at the \(i^{th}\) refinement. \(T\) and \(N\) are the number of frames and refinements, respectively. \(\) denotes the weights on corresponding estimated flows. \(_{gt}\) is the ground truth flow and \(||||_{1}\) means the \(l_{1}\) distance between ground truth and our predicted flow. In practice, \(N\) is set to 12, \(\) is set to 0.8, the same as previous works  for a fair comparison.

## 4 Experiments

Experimental setup.In this study, we evaluate StreamFlow on the Sintel , KITTI , and Spring  datasets, following previous works . In previous works, models are initially pre-trained on the FlyingChairs  and FlyingThings  datasets using the "C+T" schedule and then are subsequently fine-tuned using the "C+T+S+K+H" schedule on Sintel and KITTI datasets. _After "C+T", the cross-dataset generalization tests are often performed, and the models from this stage are frequently used for various cross-domain video downstream tasks . In "+S+K+H", the evaluation typically focuses on intra-domain generalization capabilities._ In specific, for Sintel, models are trained on a combination of FlyingThings, Sintel, KITTI, and HD1K . Models are then trained on the KITTI dataset for KITTI evaluation and on the Spring dataset for Spring evaluation.

Implementation details.Our StreamFlow method is built with PyTorch  library, and our experiments are conducted on the NVIDIA A100 GPUs. During training, we adopt the AdamW  optimizer and the one-cycle learning rate policy , following previous works . The number of refinements in the decoder is set to 12, following previous works. Given the absence of multi-frame data information in the Chairs dataset, we follow VideoFlow  to directly train on the FlyingThings in the first stage. For the Spring dataset, we follow the settings of MemFlow  and fine-tune the model for 180k steps. The remaining training configurations are consistent with prior works . The temporal and non-temporal modeling modules are concurrently trained.

### Quantitative results

From Table 1 and Table 4.1, we can learn that StreamFlow achieves advanced 0-shot performance on Sintel and KITTI. Compared to previous methods, StreamFlow reduces the 0-shot end-point error by \(0.16\) and \(0.08\) on the challenging Sintel clean and final pass, respectively. On KITTL, StreamFlow outperforms the previous state-of-the-art 0-shot results with \(0.11\) and \(17.65\%\) lower EPE and Fl-all metric. Besides, without self-supervised pre-training or bi-directional flows, StreamFlow attains commendable accuracy and efficiency on the challenging Sintel and KITTI test benchmarks using (C)+T+S+K+H schedule. On the challenging Spring  dataset, StreamFlow also achieves enhanced performance both before and after fine-tuning. **A detailed analysis on its shortcomings is in Section 5**.

### Occlusion analysis

In this section, we validate if StreamFlow could help improve the performance on the occlusions. We compare StreamFlow with its base two-frame model Twins-SKFlow, which strengthens SKFlow  with the Twins  encoder. Evaluations are conducted on the matched and unmatched areas of the Sintel test dataset. The matched areas denote regions visible in adjacent frames and the unmatched areas refer to regions visible only in one of two adjacent frames. Our models are trained using the T+S+H+K schedule. We could learn that StreamFlow attains remarkable improvements on occluded areas, as shown in Table 3. We also visualize the performance on occluded regions, which are shown in the supplements. On the challenging Sintel final test set, StreamFlow attains the improvement of \(10.77\%\) and \(11.83\%\) on unmatched and matched regions, respectively. On the clean pass, StreamFlow improves the performance by \(15.53\%\), \(15.56\%\), and \(15.45\%\) on unmatched, matched, and overall

    &  &  &  &  &  \\   & & Clean & Final & Fl-epe & Fl-all & Clean & Final & Fl-all \\   & HD3  & 3.84 & 8.77 & 13.17 & 24.0 & - & - & - \\  & PWC-Net  & 2.55 & 3.93 & 10.35 & 33.7 & - & - & - \\  & RAFT  & 1.43 & 2.71 & 5.04 & 17.4 & - & - & - \\  & CRAFT  & 1.27 & 2.79 & 4.88 & 17.5 & - & - & - \\  & AGFlow  & 1.31 & 2.69 & 4.82 & 17.0 & - & - & - \\  & Separable Flow  & 1.30 & 2.59 & 4.60 & 15.9 & - & - & - \\  & GMA  & 1.30 & 2.74 & 4.69 & 17.1 & - & - & - \\  & SKFlow  & 1.22 & 2.46 & 4.27 & 15.5 & - & - & - \\  & FlowFormer  & 1.00 & 2.45 & 4.09 & 14.7 & - & - & - \\  & GAFlow  & 1.02 & 2.45 & 3.98 & 15.0 & - & - & - \\  & TransFlow  & 0.93 & 2.33 & 3.98 & 14.4 & - & - & - \\  & VideoFlow-BOF  & 1.03 & 2.19 & 3.96 & 15.3 & - & - & - \\  & SplatFlow  & 1.22 & 2.97 & **3.70** & 15.3 & - & - & - \\  & **Ours** & **0.87** & **2.11** & **3.85** & **12.6** & - & - & - \\   & IRR-PWC  & (1.92) & (2.51) & (1.63) & (5.3) & 3.84 & 4.58 & 7.65 \\  & MaskFlowNet  & - & - & - & - & 2.52 & 4.17 & 6.10 \\  & Separable Flow & (0.69) & (1.10) & (0.69) & (1.6) & 1.50 & 2.67 & 4.64 \\  & PWC-Fusion  & - & - & - & - & 3.43 & 4.57 & 7.17 \\  & StarFlow  & - & - & - & - & 2.72 & 3.71 & 7.65 \\  & RAFT\({}^{*}\) & (0.76) & (1.22) & (0.63) & (1.5) & 1.61 & 2.86 & 5.10 \\  & GMA\({}^{*}\) & (0.62) & (1.06) & (0.57) & (1.2) & 1.39 & 2.47 & 5.15 \\  & GMoRawNet  & (0.59) & (0.91) & (0.64) & (1.5) & 1.39 & 2.65 & 4.79 \\ (C+)T+S+K+H & AGFlow* & (0.65) & (1.07) & (0.58) & (1.2) & 1.43 & 2.47 & 4.89 \\  & SKFlow\({}^{*}\) & (0.52) & (0.78) & (0.51) & (0.9) & 1.28 & 2.27 & 4.84 \\  & FlowFormer  & (0.48) & (0.74) & (0.53) & (1.1) & 1.16 & 2.09 & 4.68 \\  & MFRFlow  & (0.64) & (1.04) & (0.54) & (1.1) & 1.55 & 2.80 & 5.03 \\  & MFCFlow  & (0.56) & (0.89) & (0.55) & (1.1) & 1.49 & 2.58 & 5.00 \\  & TransFlow  & (0.42) & (0.69) & (0.49) & (1.05) & 1.06 & 2.08 & 4.32 \\  & VideoFlow-BOF  & (0.37) & (0.54) & (0.52) & (0.85) & **1.00** & **1.71** & 4.44 \\  & SplatFlow  & (0.53) & (0.91) & (0.80) & (2.40) & 1.12 & 2.07 & 4.61 \\
**Ours** & **(0.28)** & **(0.38)** & **(0.47)** & **(0.77)** & 1.04 & 1.87 & **4.24** \\   

Table 1: Quantitative results on Sintel and KITTI. The average End-Point Error (EPE) is reported as the evaluation metric if not specified. \({}^{*}\) refers to the warm-start strategy  that use the previous flow for initialization. Bold and underlined metrics denote the method that ranks 1st and 2nd, respectively.

[MISSING_PAGE_FAIL:8]

SIM pipeline.We test the efficiency of the vanilla pairwise pipeline and our SIM pipeline. Pairwise methods utilize multi-frames to predict the flow of the current two frames and bring substantial redundant computation, while the SIM pipeline estimates multiple flows concurrently and minimizes the overlapping calculation. As shown in Table 4, the SIM pipeline brings great gain in efficiency. Notably, there might be information loss for the SIM pipeline due to different frame distances. For instance, given the frames \(I_{t-1},I_{t},I_{t+1}\), the estimated flow \(F_{t-1 t}\) is typically less accurate than that derived from the sequence \(I_{t-2},I_{t-1},I_{t}\). However, this issue tends to be alleviated with longer sequences. When the number of frames increases to 4, the impact on accuracy is significantly reduced. We will discuss this issue in more detail in the appendix.

Temporal modules.In this part, we explore the performance and efficiency of different temporal modeling methods in the flow encoder. Temporal attn refers to applying a temporal attention layer after each spatial self-attention modeling in Twins. Pseudo conv  denotes stacking 1D convolution layers in the temporal dimension to imitate 3D convolutions at minimal cost. We also apply 3D convolutions at the end of the flow encoder to learn temporal relations. As shown in Table 4, our ISC module achieves a good trade-off between efficiency and effectiveness. The improvements achieved by other methods are not as pronounced. We hypothesize that the limited volume of optical flow data impedes the efficient training of the spatiotemporal module from scratch to accomplish good optimization. For comparison, VideoFlow does not apply temporal modeling modules in the encoder, and TransFlow  applies self-supervised pre-training for better optimization.

Extra parameters.In this part, we aim to determine whether the performance gain is due to the additional parameters or the effective temporal modeling method. To this end, we introduce the additional parameters by widening the baseline network. Namely, we extract higher-dimension features along the spatial dimension and concatenate them with the original motion feature. All models in this section are equipped with the ISC module. "w/o" denotes the baseline Twins-SKFlow network. "w" means adding additional parameters. "Ours" denotes the method equipped with our temporal modeling modules. Results show the improvement achieved by simply adding more parameters is minor, and the performance gain is primarily attributed to the effectiveness of StreamFlow modules.

GTR module.We also examined whether the GTR module could enhance flow predictions. "w/o" means applying vanilla SKFlow decoder while "w" denotes using GTR. All models in this part utilize the ISC module in the encoder. Table 4 demonstrates the necessity of incorporating the GTR, which could achieve stable improvement on multiple benchmarks. We could learn that GTR especially helps

    &  &  &  & Param & Latency \\   & & Clean & Final & Occ & Noc & Fl-epe & Fl-all & (M) & (ms) \\   & w/o & 1.03 & 2.34 & 7.69 & 0.35 & 4.64 & 14.70 & 12.49 & 122.18 \\  & w/ & 1.03 & 2.34 & 7.69 & 0.35 & 4.64 & 14.70 & 12.49 & **84.59** \\   & w/o & 1.03 & 2.34 & 7.69 & 0.35 & 4.64 & 14.70 & **12.49** & **84.59** \\  & Tem. attn & **0.96** & 2.31 & 7.38 & 0.35 & 4.38 & 14.96 & 14.14 & 91.17 \\  & Pse. 3D conv & 1.05 & 2.36 & 7.60 & 0.38 & 4.46 & 15.20 & 13.48 & 87.41 \\  & 3D conv & 0.98 & 2.34 & 7.63 & 0.33 & 4.57 & 15.59 & 16.03 & 93.05 \\  & ISC & 0.97 & **2.29** & **7.11** & **0.32** & **4.14** & **14.16** & **12.49** & 88.35 \\   & w/o & 0.97 & 2.29 & 7.11 & 0.32 & 4.14 & 14.16 & **12.49** & **84.59** \\  & w/ & 0.98 & 2.24 & 7.33 & **0.31** & 4.15 & 13.94 & 13.77 & 89.29 \\  & Ours & **0.93** & **2.15** & **7.06** & **0.31** & **3.92** & **12.36** & 13.77 & 89.76 \\   & w/o & 0.97 & 2.29 & 7.11 & 0.32 & 4.14 & 14.16 & **12.49** & **88.35** \\  & w/ & 0.93 & **2.15** & **7.06** & **0.31** & **3.92** & **12.36** & 13.77 & 89.76 \\   & w/o & 1.01 & 2.19 & 7.23 & 0.33 & 4.06 & 13.95 & 13.77 & **86.02** \\  & w/ & **0.93** & **2.15** & **7.06** & **0.31** & **3.92** & **12.36** & 13.77 & 89.76 \\   & 3 & 0.93 & 2.15 & 7.06 & 0.31 & 3.92 & **12.36** & **13.77** & 89.76 \\  & 4 & **0.87** & **2.11** & **6.24** & 0.31 & **3.85** & 12.62 & 14.25 & **85.53** \\   

Table 4: Ablations on our proposed design. All models are trained using the "C+T" schedule. The number of refinements is 12 for all methods. The settings used in our final model are underlined.

the estimation on the challenging final pass, with the performance gain of \(0.14\). In _supplements_, we give a more detailed discussion of its initialization, which is key to its training process.

ISC module.We verify the effectiveness of the proposed ISC module. All models in this part adopt GTR as the flow decoder. From Table 4, we could learn that the ISC module is efficient and effective in temporal modeling and greatly contributes to the improvement of the pipeline. It introduces no additional parameters and a modest increase in runtime, while significantly boosting the performance.

Frames.We delve into the influence of different numbers of frames, as illustrated in Table 4. We set the number of frames to 4 due to limitations in GPU memory. From an efficiency standpoint, augmenting the number of frames results in a higher proportion of redundant computations eliminated by StreamFlow, consequently leading to a more substantial improvement in processing time. Although there is an increase in the parameter count for temporal modeling, the efficiency is further enhanced in the context of four input frames due to a reduced proportion of redundant computations, resulting in a shorter average prediction time per frame compared to the three-frame setting.

### Qualitative results

We demonstrate visualization results on both synthetic (Sintel ) and real-world scenes (KITTI ), as shown in Figure 4. In the _supplements_, we also show the visualizations on the real-world dataset DAVIS . Our models are pre-trained using the T+H+S+K schedule. We could learn that StreamFlow could still achieve remarkable qualitative results when generalized to real-world scenes.

### Efficiency analysis

We evaluate the efficiency of the StreamFlow in terms of runtime and parameter counts. Our experiments were conducted on NVIDIA A100 GPUs. Models are trained using the (C+)T schedule and evaluated on the Sintel dataset. The runtime is measured as the average inference time per frame of five runs on the Sintel training set. We could learn StreamFlow achieves comparable efficiency with state-of-the-art two-frame methods while achieving superior performance. The key to maintaining high efficiency is its SIM pipeline. StreamFlow does not perform pairwise redundant computation and predicts all flows simultaneously. Another reason for the high speed is its CNN-based decoder. We could learn that StreamFlow is much faster than the pure two-frame transformer architecture FlowFormer. Besides, the specially designed lightweight temporal modules also contribute to the performance, simultaneously aiding in better results compared to the 2-frame baseline Twins-SKFlow.

## 5 Limitations

StreamFlow faces two primary challenges: (1) GPU memory usage in training. Although it is not an issue during inference (e.g., with a \(432 1024\) input, it needs about only 2.4 G and 3.3 G when the # frame is 3 and 4, respectively). But it is significantly increased during training. The storage of gradients and the batch size cause the GPU memory on a single card to reach approximately 40 G in a 4-frame setting. (2) Inter-group cues utilization. Limited to the SIM pipeline, StreamFlow is confined to using only intra-group information, and it does not utilize inter-group information during modeling. Despite achieving commendable results and surpassing some methods that incorporate inter-group information, bidirectional flows, or self-supervised pre-training strategies in cross-dataset tests, how to address this while maintaining good efficiency is a worthwhile issue to consider in future work.

## 6 Conclusion

In this work, we proposed StreamFlow, a multi-frame optical flow estimation approach proficient in estimating optical flows across multiple video frames using efficient spatiotemporal relationship mining. StreamFlow aims to estimate consecutive unidirectional optical flows with less overlapping computation. It proposes to estimate multi-frame optical flows via the proposed SIM pipeline and introduces efficient and effective ISC and GTR methods for temporal modeling under such circumstances. Extensive experiments on multiple challenging benchmarks demonstrate the efficiency and effectiveness of the proposed StreamFlow method.

Acknowledgements.This work was supported by The Major Key Project of PCL (PCL2024A02), Natural Science Foundation of China (62271013, 62031013), Guangdong Province Pearl River Talent Program (2021QN020708), Guangdong Basic and Applied Basic Research Foundation (2024A1515010155), Shenzhen Science and Technology Program (JCYJ20230807120808017), Shenzhen Fundamental Research Program (GXWD20201231165807007-20200806163656003), and Sponsored by CAAI-MindSpore Open Fund, developed on Open Community (CAAIXSLJJ-2023-MindSpore07).