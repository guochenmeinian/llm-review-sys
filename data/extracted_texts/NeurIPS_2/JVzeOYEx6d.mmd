# ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation

Jiazheng Xu\({}^{}\)

JX & XL contributed equally. Corresponding authors: YD & JT (yuxiaod|jietang@tsinghua.edu.cn).

 Xiao Liu\({}^{}\)

JX & XX contributed equally. Corresponding authors: YD & JT (yuxiaod|jietang@tsinghua.edu.cn).

 Yuchen Wu\({}^{}\)

Yuxuan Tong\({}^{}\)

Qinkai Li\({}^{@sectionsign{}}\)

Work done when JX internet at Zhipu AI and QL visited Tsinghua University.

 Ming Ding\({}^{}\)

Jie Tang\({}^{}\)

Yuxiao Dong\({}^{}\)Tsinghua University \({}^{}\)Zhipu AI \({}^{@sectionsign}\)Beijing U. of Posts and Telecommunications

###### Abstract

We present a comprehensive solution to learn and improve text-to-image models from human preference feedback. To begin with, we build ImageReward--the first general-purpose text-to-image human preference reward model--to effectively encode human preferences. Its training is based on our systematic annotation pipeline including rating and ranking, which collects 137k expert comparisons to date. In human evaluation, ImageReward outperforms existing scoring models and metrics, making it a promising automatic metric for evaluating text-to-image synthesis. On top of it, we propose Reward Feedback Learning (ReFL), a direct tuning algorithm to optimize diffusion models against a scorer. Both automatic and human evaluation support ReFL's advantages over compared methods. All code and datasets are provided at [https://github.com/THUDM/ImageReward](https://github.com/THUDM/ImageReward).

## 1 Introduction

Text-to-image generative models, including auto-regressive [43; 11; 14; 16; 12; 63] and diffusion-based [37; 45; 42; 46] approaches, have experienced rapid advancements in recent years. Given appropriate text descriptions (i.e., prompts), these models can generate high-fidelity and semantically-related images on a wide range of topics, attracting significant public interest in their potential applications and impacts.

Despite the progress, existing self-supervised pre-trained  generators are far from perfect. A primary challenge lies in **aligning models with human preference**, as the pre-training distribution is noisy and differs from the actual user-prompt distributions. The inherent discrepancy leads to several well-documented issues in the generated images [15; 31], including but not limited to:

* **Text-image Alignment**: failing to accurately depict all the numbers, attributes, properties, and relationships of objects described in text prompts, as shown in Figure 1 (a)(b).
* **Body Problem**: presenting distorted, incomplete, duplicated, or abnormal body parts (e.g., limbs) of humans or animals, as illustrated in Figure 1 (e)(f).
* **Human Aesthetic**: deviating from the average or mainstream human preference for aesthetic styles, as demonstrated in Figure 1 (c)(d).
* **Toxicity and Biases**: featuring content that is harmful, violent, sexual, discriminative, illegal, or causing psychological discomfort, as depicted in Figure 1 (f).

These prevalent challenges, however, are difficult to address solely through improvements in model architectures and pre-training data.

In natural language processing (NLP), researchers have employed reinforcement learning from human feedback (RLHF) [55; 36; 39] to guide large language models [6; 7; 66; 48; 64] towards human preferences and values. The approach relies on learning a reward model (RM) to capture humanpreference from massive expert-annotated model output comparisons. Effective though it is, the annotation process can be costly and challenging , as it requires months of effort to establish labeling criteria, recruit and train experts, verify responses, and ultimately produce the RM.

**Contributions.** Recognizing the importance of addressing these challenges in generative models, we present and release the first general-purpose text-to-image human preference RM--ImageReward--which is trained and evaluated on 137k pairs of expert comparisons in total, based on real-world user prompts and corresponding model outputs. Based on the effort, we further investigate the direct optimization approach ReFL for improving diffusion generative models. Our main contributions are:

* We systematically identify the challenges for text-to-image human preference annotation, and consequently design a pipeline tailored for it, establishing criteria for quantitative assessment and annotator training, optimizing labeling experience, and ensuring quality validation. We build the text-to-image comparison dataset for training the ImageReward model based on the pipeline. The overall architecture is depicted in Figure 2.
* We demonstrate that ImageReward outperforms existing text-image scoring methods, such as CLIP  (by 38.6%), Aesthetic  (by 39.6%), and BLIP  (by 31.6%), in terms of understanding human preference in text-to-image synthesis through extensive analysis and experiments. ImageReward is also proven to significantly mitigate the aforementioned issues, providing valuable insights into how human preference can be integrated into generative models.
* We suggest that ImageReward could serve as a promising automatic text-to-image evaluation metric. Compared to FID  and CLIP scores on prompts from real users and MS-COCO 2014, ImageReward aligns consistently to human preference ranking and presents higher distinguishability across models and samples.

Figure 1: (Upper) Top-1 images out of 64 generations selected by different text-image scorers. (Lower) 1-shot generation after ReFL training using ImageReward as feedback. Images get better text coherence and human preference after ImageReward selection or ReFL training. In prompts (from real users, truncated), the **bold** roughly denotes content, and the _italic_ denotes style or function.

* We propose Reward Feedback Learning (ReFL) for tuning diffusion models regarding human preference scorers. Our unique insight on ImageReward's quality identifiability at latter denoising steps allows the direct feedback learning on diffusion models, which offer no likelihood for their generations. Extensive automatic and human evaluations demonstrate ReFL's advantages over existing approaches including data augmentation [61; 13] and loss reweighing .

## 2 ImageReward: Learning to Score and Evaluate Human Preferences

ImageReward is constructed using a systematic pipeline involving data collection and human annotation from experts. Based on the pipeline, we implement the RM training and derive the ImageReward.

### Annotation Pipeline Design

**Prompt Selection and Image Collection.** The dataset utilizes a diverse selection of real user prompts from DiffusionDB , an open-sourced dataset. To ensure diversity in selected prompts, we employ a graph-based algorithm that leverages language model-based prompt similarity [56; 44; 53]. This selection yields 10,000 candidate prompts, each accompanied by 4 to 9 sampled images from DiffusionDB, resulting in 177,304 candidate pairs for labeling (Cf. Appendix A.1 for details).

**Human Annotation Design.** Our annotation pipeline involves a prompt annotation stage, which includes categorizing prompts and identifying problematic ones, and a text-image rating stage, where images are rated based on _alignment_, _fidelity_, and _harmlessness_. Subsequently, annotators rank the images in order of preference. To manage potential contradictions in the ranking, we provide trade-offs in our annotation document (completely attached in Appendix B). Our annotation system is composed of three stages: Prompt Annotation, Text-Image Rating, and Image Ranking. Screenshots of our system are provided in Figure 8. Annotators were recruited in collaboration with a professional data annotation company, with a majority having at least college-level education. They are trained using documents that describe the labeling process and criteria. To ensure quality, we employ quality inspectors to double-check each annotation, with invalid annotations reassigned for relabeling. Due to the page limits, please refer to Appendix A.3, A.2, B for comprehensive details and discussion.

**Human Annotation Analysis.** After 2 months of annotation, we collected valid annotations for 8,878 prompts, resulting in 136,892 compared pairs. A comprehensive analysis of these prompts, annotations, and challenges discovered is discussed in detail in Appendix A.4.

Figure 2: An overview of the ImageReward and ReFL. (Upper) ImageReward’s annotation and training, consisting of data collection, annotation, and preference learning. (Lower) ReFL leverages ImageReward’s feedback to directly optimize diffusion models at a random latter denoising step.

### RM Training

Admittedly, human evaluation is after all the touchstone for human preference for synthesized images; but it is limited by labor costs and hard to scale up. We aim to model human preference based on annotations, which can lead to a virtual evaluator free from dependence on humans.

Similar to RM training for language model of previous works [55; 39], we formulate the preference annotations as rankings. We have \(k\) images ranked for the same prompt \(T\) (the best to the worst are denoted as \(x_{1},x_{2},...,x_{k}\)) and get at most \(C_{k}^{2}\) comparison pairs if no ties between two images. For each comparison, if \(x_{i}\) is better and \(x_{j}\) is worse, the loss function can be formulated as:

\[()=-_{(T,x_{i},x_{j})}[((f_ {}(T,x_{i})-f_{}(T,x_{j})))] \]

where \(f_{}(T,x)\) is a scalar value of preference model for prompt \(T\) and generated image \(x\).

**Training Techniques.** We use BLIP  as the backbone of ImageNetward, as it outperforms conventional CLIP (Cf. Table 2b) in our preliminary experiments. We extract image and text features, combine them with cross attention, and use an MLP to generate a scalar for preference comparison.

Training ImageReward is of no ease. We observe rapid convergence and consequent overfitting, which harms its performance. To address this, we freeze some backbone transformer layers' parameters, finding that a proper number of fixed layers improves ImageReward's performance (Cf. Section 4.1). ImageReward also exhibits sensitivity to training hyperparameters, such as learning rate and batch size. We perform a careful grid search based on the validation set to determine optimal values.

### As Metric: Re-Evaluating Human Preferences on Text-to-Image Models

Training text-to-image generative models is hard, but evaluating these models reasonably is even harder. In literature [11; 42; 12; 46], it has been a _de facto_ practice to evaluate text-to-image generative models on MS-COCO  image-caption dataset against the real images, using fine-tuned or zero-shot FID  scores following DALL-E  setting. Nevertheless, it remains quite dubious whether the FID really fits the current need , especially from the following aspects:

1. **Zero-shot Usage**: As generative models are now dominantly used by the public in a zero-shot manner without fine-tuning, fine-tuned FID may not honestly reflect models' actual performance in real use. In addition, despite the adoption of zero-shot FID in recent trends, the possible leak of MS-COCO in some models' pre-training data would make it a potentially unfair setting.
2. **Human Preference**: FID measures the average distance between generated images and reference real images, and thus fails to encompass human preference that is crucial to text-to-image synthesis in evaluation. Moreover, FID's relies on average over the whole dataset to provide an accurate assessment, whereas in many cases we need the metric to serve as a selector over single images.

Seeing these challenges, we propose ImageReward as a promising zero-shot automatic evaluation metric for text-to-image model comparison and individual sample selection.

**Better Human Alignment Across Models.** We conduct researcher annotation (i.e., by authors) across 6 popular high-resolution (around 512\(\)512) available text-to-image models: CogView 2 ,

    &  &  \\  Dataset \& Model & Human & Eval. & ImageReward &  &  & ^{*}\)} \\   & Rank & \#Win & Rank & Score & Rank & Score & Rank & Score & Rank & Score \\  Openjourney & 1 & 507 & 1 & 0.2614 & 2 & 0.2726 & 3 & -0.0455 & 5 & 20.7 \\ Stable Diffusion 2.1-base & 2 & 463 & 2 & 0.2458 & 4 & 0.2683 & 2 & 0.1553 & 4 & 18.8 \\ DALL-E 2 & 3 & 390 & 3 & 0.2114 & 3 & 0.2684 & 1 & 0.5387 & 1 & 10.9\({}^{*}\) \\ Stable Diffusion 1.4 & 4 & 362 & 4 & 0.1344 & 1 & 0.2763 & 4 & -0.0857 & 2 & 17.9 \\ Versatile Diffusion & 5 & 340 & 5 & -0.2470 & 5 & 0.2606 & 5 & -0.5485 & 3 & 18.4 \\ CogView 2 & 6 & 74 & 6 & -1.2376 & 6 & 0.2044 & 6 & -0.8510 & 6 & 26.2 \\  Spearman \(\) to Human Eval. & - & & 1.00 & & 0.60 & & 0.77 & & 0.09 \\   

Table 1: Text-to-image model ranking by humans and automatic metrics (ImageReward, CLIP, and FID). \({}^{*}\)Zero-shot FID (30k) scores of DALL-E 2 is taken from ; others are evaluated in 256\(\)256 resolution on MS-COCO 2014 validation set following prior practices.

Versatile Diffusion (VD) , Stable Diffusion (SD) 1.4 and 2.1-base , DALL-E 2 (via OpenAI API) , and Openjourney1, to identify the alignment of different metrics to human.

We sample 100 real-user test prompts for the alignment test, with each model generating 10 outputs as candidates. To compare these models, we first pick the best image out of 10 outputs by each model on each prompt. Then, the annotators rank the images from different models for each prompt, following the disciplines for ranking described in Section 2.1. We aggregate all annotators' annotations, and compute the final win count of each model to all others (Cf. Table 1).

For ImageReward and CLIP scores, we report their average for 1,000 text-image pairs per model. We also document all models' zero-shot FID and ImageReward score (30k) on MS-COCO 2014 valid set following prior practices [42; 12], where outputs are unified to 256\(\)256 resolution and optimal classifier-free guidance values are selected by grid search (i.e., [1.5, 2.0, 3.0, 4.0, 5.0]). As shown in Table 1, ImageReward aligns well with human ranking, whereas zero-shot FID and CLIP are not.

**Better Distinguishability Across Models and Samples.** Another highlight is that, compared to CLIP, we observe that ImageReward can better distinguish the quality between individual samples. Figure 3 presents a box plot of ImageReward and CLIP's score distributions on the 1,000 generations per model. The distributions are normalized to 0.0 to 1.0 using minimum and maximum values of ImageReward and CLIP scores per model, and outliers are discarded. As it demonstrates, ImageReward's scores in each model have a much larger interquartile range than that of CLIP, which means ImageReward can well distinguish the quality of images from each other. Besides, in terms of comparison across models, we discover that the medians of the ImageReward scores are also roughly in line with human ranking in Table 1. On the contrary, CLIP's medians fail to present the property.

## 3 ReFL: Reward Feedback Learning Improves Text-to-Image Diffusion

Though ImageReward can pick out highly human-preferred images from many generations of a prompt, the generate-and-then-filter paradigm could be expensive and inefficient in practical applications. Therefore, we seek to improve text-to-image generative models, particularly for the popular latent diffusion models, for allowing high-quality generation in single or very few trials.

**Challenge.** In NLP, researchers have reported using reinforcement learning algorithms (e.g., PPO ) to steer language models to align to human preference [55; 36; 39], which depends on the likelihood of a whole generation to update the model.

However, unlike language models, latent diffusion models (LDMs)'s multi-step denoising generation cannot yield likelihoods for their generations, and thus fail to adopt the same RLHF approaches. A potentially similar approach is classifier-guidance [54; 9] technique during LDM inference. Nonetheless, it is for inference only

Figure 4: ImageReward scores of a prompt with different generation seeds along denoising steps. Final image qualities become identifiable after 30 out of 40 steps.

Figure 3: Normalized distribution of ImageReward and CLIP scores of different generative models (outliers are discarded). ImageReward’s scores align well with human preference and present higher distinguishability.

and employs a classifier necessarily trained on noisy intermediate latents, which naturally contradicts RMs' annotation where images need to be completely denoised for humans to mark correct preference. Some concurrent works propose some alternative indirect solutions, such as using RMs to filter dataset for fine-tuning [61; 13], or to re-weight losses of training samples according to their qualities . Nevertheless, these data-oriented approaches are virtually indirect. They could rely heavily on proper fine-tuning data distributions and finally only improve the LDMs mildly.

**ReFL: Insight and Solution.** We endeavor to develop a direct optimization method for improving LDMs according to an RM (e.g., ImageReward). Looking into ImageReward scores along denoising steps (i.e., 40 in our case), we derive an intriguing insight (Cf. Figure 4) that when we directly predict \(x_{t} x_{0}^{}\) at a step \(t\) (different from the real latent \(x_{0}\) which experiences \(x_{t} x_{t+1}... x_{0}\)):

* When \(t 15\): ImageReward scores for all generations are unanimously low.
* When \(15 t 30\): High-quality generations begin to stand out, but overall we still cannot clearly judge all generations' final qualities based on the current ImageReward scores.
* When \(t 30\): Generations of different ImageReward scores are generally distinguishable.

In light of the observation, we conclude that ImageReward scores for generations \(x_{0}^{}\) after 30 steps of denoising, unnecessarily the final step, could serve as reliable feedback for improving LDMs.

We thus propose an algorithm to directly fine-tune LDMs by viewing the scores of an RM as human preference losses to back-propagate gradients (Cf. Algorithm 1) to a randomly-picked latter step \(t\) (in our case \(t\)) in the denoising process. The reason for the random selection of \(t\) instead of using the last step is that, if only the gradient of the last denoising step is retained, the training is proved very unstable and the results are bad. In practice, to avoid rapid overfitting and stabilize the fine-tuning, we re-weight ReFL loss and regularize with pre-training loss. The final loss form is written as

\[_{reward} =_{y_{i}}((r(y_{i},g_{} (y_{i})))) \] \[_{pre} =_{(y_{i},x_{i})}(_{(x_{i}),y_{i},(0,1),t}[\|-_{}(z _{t},t,_{}(y_{i}))\|_{2}^{2}]) \]

where \(\) denotes the parameters of the LDM, \(g_{}(y_{i})\) denotes the generated image of LDM with parameters \(\) corresponding to prompt \(y_{i}\). Meanings of other symbols are detailed in Algorithm 1, while the loss function of \(_{pre}\) is taken from .

```
1:Dataset: Prompt set \(=\{y_{1},y_{2},...,y_{n}\}\)
2:Pre-training Dataset: Text-image pairs dataset \(=\{(_{1},_{1}),...(_{n},_{n })\}\)
3:Input: LDM with pre-trained parameters \(w_{0}\), reward model \(r\), reward-to-loss map function \(\), LDM pre-training loss function \(\), reward re-weight scale \(\)
4:Initialization: The number of noise scheduler time steps \(T\), and time step range for fine-tuning \([T_{1},T_{2}]\)
5:for\(y_{i} T\) and \((_{i},_{i})\)do
6:\(_{pre}_{w_{i}}\) (\(_{i},_{i}\))
7:\(w_{i} w_{i}\) // Update LDM\({}_{w_{i}}\) using Pre-training Loss
8:\(t rand(T_{1},T_{2})\) // Pick a random time step \(t[T_{1},T_{2}]\)
9:\(x_{T}(0,1)\) // Sample noise as latent
10:for\(j=T,...,t+1\)do
11:no grad:\(x_{j-1}\) LDM\({}_{w_{i}}\{x_{j}\}\)
12:endfor
13:with grad:\(x_{t-1}\) LDM\({}_{w_{i}}\{x_{t}\}\)
14:\(x_{0} x_{t-1}\) // Predict the original latent by noise scheduler
15:\(z_{i} x_{0}\) // From latent to image
16:\(_{reward}(r(y_{i},z_{i}))\) // ReFL loss
17:\(w_{i+1} w_{i}\) // Update LDM\({}_{w_{i}}\) using ReFL loss
18:endfor
```

**Algorithm 1** Reward Feedback Learning (ReFL) for LDMs

## 4 Experiment

### ImageReward: On Human Preference Prediction

**Dataset & Training Setting.** Rankings of annotated images are collected to train ImageReward, which contains 8,878 prompts and 136,892 pairs of image comparisons. We divide the dataset according to prompts annotated by different annotators and select 466 prompts from annotators who have a higher agreement with researchers to consist for the model test. Except for prompts for testing, other more than 8k prompts of annotation are collected for training.

We load the pre-trained checkpoint of BLIP (ViT-L for image encoder, 12-layers transformer for text encoder) as the backbone of ImageReward, and initialize MLP head according to \((0,1/(d_{model}+1))\) decaying the learning rate with a cosine schedule. We sweep over several value settings of learning rate and batch size and fix different rates of backbone transformer layers. We find that fixing 70% of transformer layers with a learning rate of 1e-5 and batch size of 64 can reach up to the best preference accuracy. ImageReward is trained on 4 40GB NVIDIA A100 GPUs, with a per-GPU batch size of 16.

We use the CLIP score, Aesthetic score, and BLIP score as baselines to compare with the ImageReward. CLIP score and BLIP score are calculated directly as cosine similarity between text and image embedding, while the Aesthetic score is given by an aesthetic predictor introduced by LAION.

**Agreement Analysis.** Agreement assesses the likelihood of two individuals sharing consistent preferences for superior images. While most people generally agree on image quality, variations in model-generated images may lead to divergent judgments. Before assessing model performance, it's crucial to measure the likelihood of consensus in selecting superior images. We use other 40 prompts (778 pairs) to calculate preference agreement between researchers, annotators, annotator ensemble, and models. Table 2a shows the result.

**Main Results: Preference Accuracy.** Preference accuracy is the correctness of a scorer choosing the same one from two different images of one prompt with a human. As Table 3 shows, our model outperforms all the baselines. The preference accuracy of ImageReward reaches up to 65.14%, which is 15.14% more than 50% (random), about twice as much as 7.76% (that of BLIP score).

**Main Results: Human Evaluation.** To evaluate the ability of ImageReward to select the more preferred images among large amounts of generated images, we produce another dataset, collecting prompts with 9/25/64 generated images from DiffusionDB, and use different methods to select from those images to get top3 results. Then three annotators rank these selected top-3 images. Figure 5 shows the win rates. Qualitative results can be seen in Appendix G, showing that ImageReward can select images that are more aligned to text and with higher fidelity and avoid toxic contents.

**Ablation Study: Training dataset size.** To investigate the effect of training dataset sizes on the performance of the model, comparative experiments are conducted. Table 2b shows that adding up the scale of the dataset significantly improves the preference accuracy of ImageReward. It's promising that if we collect more annotation data in the future, ImageReward will get better performance.

    &  &  & annotator & CLIP &  & BLIP &  \\  & & & ensemble & Score & & Score & \\   & 71.2\% & 65.3\% & 73.4\% & 57.8\% & 55.6\% & 57.0\% & **64.5\%** \\  & \(\) 11.1\% & \(\) 8.5\% & \(\) 6.2\% & \(\) 3.6\% & \(\) 3.1\% & \(\) 3.0\% & \(\) **2.5\%** \\   & 65.3\% & 65.3\% & 53.9\% & 54.3\% & 55.9\% & 57.4\% & **65.3\%** \\  & \(\) 8.5\% & \(\) 5.6\% & \(\) 5.8\% & \(\) 3.2\% & \(\) 3.1\% & \(\) 2.7\% & \(\) **3.7\%** \\   & 73.4\% & 53.9\% &  & 54.4\% & 57.5\% & 62.0\% & **70.5\%** \\  & \(\) 6.2\% & \(\) 5.8\% & & \(\) 21.1\% & \(\) 15.9\% & \(\) 16.1\% & \(\) **18.6\%** \\   

Table 2: Data annotation agreement and ablation study on model backbones and dataset sizes.

(a) Agreement between different annotators, researchers, and models. Especially, “annotator ensemble” means, for each pair of images, we use the image considered better by most people as the better one.

    & Preference &  &  \\  & Acc. & @1 & @2 & @4 & @1 & @2 & @4 \\  CLIP Score & 54.82 & 27.22 & 48.52 & 78.17 & 29.65 & 51.75 & 76.82 \\ Aesthetic Score & 57.35 & 30.73 & 53.91 & 75.74 & 32.08 & 54.45 & 76.55 \\ BLIP Score & 57.76 & 30.73 & 50.67 & 77.63 & 33.42 & 56.33 & 80.59 \\  ImageReward (Ours) & **65.14** & **39.62** & **63.07** & **90.84** & **49.06** & **70.89** & **88.95** \\   

Table 3: Results of ImageReward and comparison methods on human preference prediction. Preference accuracy is from the test set of 466 prompts (6,399 comparisons); Recall and Filter’s scores are from another test set of 371 prompts with 8 images each. All scores are averaged per prompt.

**Ablation Study: RM backbone.** ImageReward adopts BLIP as the backbone, which may raise curiosity about how well BLIP compares to CLIP. We add MLP to CLIP, training in a similar way, and the result is also shown in Table 1(b). Even if CLIP uses a relatively larger training data set, its preference is still inferior to that of BLIP. The difference between these two as backbone may partly be because BLIP used bootstrapping of its training set. Moreover, we use BLIP's image-grounded text encoder as a feature encoder different from the separate encoder for text/image as CLIP.

### ReFL: On Improving Diffusion Models with Human Preference

**Training Settings.** We use Stable Diffusion v1.4 as the baseline generative model and fine-tune it for experiments. For the dataset, the pre-training dataset is from a 625k subset of LAION-5B selected by aesthetic score, while the prompt set for ReFL is sampled from DiffusionDB. The model is fine-tuned in half-precision on 8 40GB NVIDIA A100 GPUs, with a learning rate of 1e-5 and batch size of 128 in total (64 for pre-training and 64 for ReFL). For ReFL algorithm, we set \(=ReLU,=1e-3\) and \(T=40,[T_{1},T_{2}]=\).

**Evaluation Settings.** We collect 466 real user prompts from DiffusionDB and 90 designed challenging prompts from multi-task benchmark (MT Bench)  for evaluation. All fine-tuning methods use the same dataset as the pre-training dataset or generated dataset (both contain 20,000 samples), and train for one epoch with the same training settings (the same learning rate and batch size) for a fair comparison. The human evaluation is consistent with Section 2.3 and the form of dataset labeling, which involves humans sorting multiple images under a prompt. Table 4 and Figure 6 show the comparison results. All methods use the same pre-trained Stable Diffusion v1.4 and the same reward model ImageReward, using PNDM  noise scheduler and default classifier free guidance scale of 7.5 for inference.

We compare several important related methods for improving text-to-image generation [61; 23; 13], whose implementation details are provided in Appendix E. Compared to ReFL's direct tuning, these previous methods are all based on indirect data augmentation or loss reweighing.

    &  &  \\   & \#Win & WinRate & \#Win & WinRate \\  SD v1.4 (baseline)  & 1315 & - & 718 & - \\  Dataset Filtering  & 1394 & 55.17 & 735 & 51.72 \\ Reward Weighted  & 1075 & 39.52 & 585 & 43.33 \\ RAFT  (iter=1) & 1341 & 49.86 & 578 & 42.31 \\ RAFT (iter=2) & 753 & 30.85 & 452 & 33.02 \\ RAFT (iter=3) & 398 & 20.97 & 355 & 26.19 \\ 
**ReFL (Ours)** & **1508** & **58.79** & **808** & **58.49** \\   

Table 4: Human evaluation on different LDM optimization methods. ReFL performs the best with regard to total win count and WinRate against SD v1.4 baseline.

Figure 5: Win rates of ImageReward compared to other models. ImageReward wins most of the time. On average, 77.1% to random, 69.3% to CLIP, 69.8% to Aesthetic, and 65.8% to BLIP.

Figure 6: Win rates between all methods.

**Results and analysis.** When compared to the original version, ReFL fine-tuned model is mostly preferred with the most win rate and the highest win rate. When compared to each other, ReFL is always the preferred one.

Note that in our evaluation, neither RAFT  nor Reward Weighted  has become better compared to the baseline, although they have been verified in their own experiments. Note that both RAFT and Reward Weighted do not collect the prompts used by users in real scenarios at finetune, whereas Reward Weighted manually constructs a dataset to address the alignment issue by combining colors, numbers, backgrounds, and objects. The prompts used in our review are more widely distributed and complex, so the problems with their methods are more clearly exposed.

RAFT  suffers from over-fitting as the number of iterations increases.  propose using an expert generator as a regularizer to avoid overfitting the reward model. However, RAFT is constrained by the quality of the constructed dataset. It is important to note that even expert generators have limitations, and when fine-tuning is performed using prompts sampled from real user data, which can be challenging, there may be instances where the expert generator fails to generate high-quality images.

In the case of the Reward Weighted method , although real images are used for regularization, there is a problem with the coefficient used for the rewards, which is constrained within the  range. This implies that while preferred images are given larger weights and poor images are given smaller weights, the influence of the non-preferred images is not completely eliminated. Similarly, when utilizing real user prompts, it is likely that there will be non-preferred images (even those relatively the best) in the dataset, which can introduce interference. The failure to eliminate the impact of non-preferred images hinders the effectiveness of the Reward Weighted method.

Dataset Filtering , on the other hand, considers real images and handles non-preferred images by labeling them as "Weird image." However, this influence is indirect. In contrast, our proposed algorithm provides direct gradient feedback through rewards, allowing for guidance toward a "better" direction, which enables more effective problem-solving.

In summary, RAFT is constrained by the limited ability of the generator, and the Reward Weighted method suffers from the influence of non-preferred images due to the choice of reward coefficients.

Figure 7: Qualitative comparison between ReFL and other fine-tuning methods. ReFL fine-tuned model can produce images that are more preferred overall. For example, in the second row of prompts containing ”long pointy ears,” only the model fine-tuned with ReFL generates correct ears, while other images either lack ears or have inaccurate representations.

Dataset Filtering partially addresses the problem by considering real images and labeling abnormal images, but it is still indirect and limited. By directly incorporating rewards into the gradient feedback, our proposed algorithm ReFL offers a more effective solution to these challenges. Qualitative examples are in Figure 7.

## 5 Related Work

**Text-to-image Generation and Evaluation.** Text-to-image generation has come a long way since the popularization of GANs , with key developments including models like DALL-E  and CogView . Recently, diffusion models [52; 19; 10; 47] have achieved remarkable results, with Stable Diffusion  being particularly popular. Evaluation metrics such as Inception Score (IS)  and Frechet Inception Distance (FID)  are commonly used to assess model performance after fine-tuning, but they cannot evaluate either single image generations or text-image coherence.

For evaluating individual generated images based on a prompt, prior works [42; 46; 63] often use CLIP  to calculate text-image similarity. While these metrics are useful, they don't capture human preference comprehensively. Other predictors, like Aesthetic from LAION , partially contribute to this holistic evaluation by scoring image aesthetics using a CLIP-based architecture. RM in RLHF, on the other hand, considers a mixture of elements such as text-image alignment, fidelity, and aesthetics. Overall, RM such as ImageReward provides a more complete evaluation for individual text-to-image generations, making it better aligned with human preferences.

**Learning from Human Feedback.** There is often a gap between generative models' pre-training objectives and human intent. Thus human feedback has been utilized to align model performance with intent in various language applications [1; 22; 36; 67; 65] via training an RM [35; 8; 59; 20; 24] to learn human preference. Researchers have explored RL for language models to achieve more truthful, helpful, and harmless outcomes [39; 2; 68; 60; 55; 29; 49; 3]. Previous work [5; 55] used human feedback to train reward functions for summarization tasks, while InstructGPT  applied RLHF to GPT-3 for multi-task NLP, yielding significant improvements.

In text-to-image generation, however, there have been few studies on the topic. One concurrent work  has focused on text-image coherence in the closed domain using simple synthetic prompts based on templates, and propose to improve models using loss re-weighing. Other concurrent works [61; 21; 13] collects 1-of-n selection from noisy online user clicking, and thus do not enforce consistent standards and prompt diversity. Their optimization methods are based on indirect data filtering and augmentation. On the contrary, ImageReward serves as a general-purpose human preference scorer with quality ensured by rigorous annotation pipeline, and corresponding ReFL is the first direct tuning method for optimize diffusion models from scorer feedback.

## 6 Conclusion

In this work, we have presented ImageReward and ReFL, the first general-purpose text-to-image human preference reward model, and a direct fine-tuning approach for optimizing diffusion models by ImageReward feedback. Through our systematic pipeline for human preference annotation, we curate a dataset of 137k expert comparisons to train ImageReward, and build ReFL algorithm on top of it. They together address prevalent issues in generative models and help to better align text-to-image generation with human values and preferences.