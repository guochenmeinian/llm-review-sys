# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

planetary exploration , safety requirements must be satisfied at least with high probability for a finite time mission, where joint chance constraint is the desirable representation [33; 43].

Related work.The optimal policy for RL without constraints or with hard constraints is deterministic policy [10; 20; 39]. Introducing stochasticity into the policy can facilitate exploration [11; 38] and fundamentally alter the optimization process during training [16; 21; 37], affecting how policy gradients are computed and how the agent learns to make decisions. It has been shown that the optimal policy for a Markov decision process with expected cumulative safety constraints is always stochastic when the state and action spaces are countable . Policy-splitting method has been proposed to optimize the stochastic policy for safe RL with finite state and action spaces . In , an algorithm was proposed to compute a stochastic policy that outperforms a deterministic policy under chance constraints, given a known dynamical model. In more general settings for safe reinforcement learning, such as with uncountable state and action spaces, the theoretical foundation regarding whether and how a stochastic policy can outperform a deterministic policy under chance constraints remains an open problem. Developing practical algorithms to obtain optimal stochastic policies with chance constraints requires further investigation.

Contributions.We present a Bellman equation for CCMDPs and prove that a flipping-based policy archives the optimality for CCMDPs. Flipping-based policy selects the next action by tossing a potentially distorted coin between two action candidates where the flip probability and the two candidates depend on the state. While solving the problem with joint chance constraints is computationally challenging, the problem with the Expected Cumulative Safe Constraints (ECSCs) can be effectively solved by many existing safe RL algorithms, such as Constrained Policy Optimization (CPO, ). Thus, we establish a theory of conservatively approximating the joint chance constraints by ECSCs. We further show that a flipping-based policy achieves optimality for MDP with ECSCs. Leveraging the existing safe RL algorithms to obtain a conservative approximation of the optimal flipping-based policy with chance constraints is possible. Specifically, we present a framework for adapting CPO to train a flipping-based policy using existing safe RL algorithms. Finally, we show that our proposed flipping-based policy can improve the performance of the existing safe RL algorithms under the same limits of safety constraints on Safety Gym benchmarks. Figure 1 summarizes the main contributions.

## 2 Preliminaries: Markov Decision Process

A standard Markov decision process (MDP) is defined as a tuple, \(,,r,,_{0}\). Here, \(\) is the set of states, \(\) is the set of actions, \(r:\) is the reward function. This paper considers the general case with state and action sets in finite-dimension Euclidean space, which can be continuous or discrete. Let \(()\) be the Borel \(\)-algebra on a metric space and \(M()\) be the set of all probability measures defined on the corresponding Borel space. The state transition model \(: M()\) specifies a probability measure of a successor state \(^{+}\) defined on \(()\) conditioned on a pair of state and action, \((,)\), at the previous step. Specifically, we use \(p(|,)\) to define a conditional probability density associated with the state transition model \((,)\). Finally, \(_{0}\) is the distribution of the initial state \(_{0}\). A stationary policy \(: M()\) is a map from states to probability measures on \((,())\). We use \((|)\) to define a conditional probability density associated with \(()\), which specifies the _stationary policy_. Define a trajectory in the infinite horizon by \(_{}:=\{_{0},_{0},_{ 1},_{1},...,_{k},_{k},...\}.\) An initial state \(_{0}\) and a stationary policy \(\) defines a unique probability measure \(_{_{0},}^{}\) on the set \(()^{}\) of the trajectory \(_{}\)

Figure 1: Summary of the relations among main theorems and problems in this paper.

. The expectation associated with \(_{_{0},}^{}\) is defined as \(_{_{}_{_{0},}^{}}\). Given a policy \(\), the value function at an initial state \(_{0}=\) is defined by \(V^{}():=_{_{}_{_{0},}^{}}\{R(_{})_{0}= \}\) with \(R(_{}):=_{k=0}^{}^{k}r(_{k}, _{k}),\) where \((0,1)\) as the discount factor. Also, the action-value function is defined as \(Q^{}(,):=r(,)+ _{_{}_{_{0},}^{}}\{ V^{}(^{+})_{0}=,_{0}= \}.\)

## 3 Flipping-based Policy with Chance Constraints

A constrained Markov decision process (CMDP) is an MDP equipped with constraints restricting the set of policies. Let \(\) be the "safe" region of the state specified by a continuous function \(g:\) in the following way: \(:=\{:g() 0\}.\) Let \(T_{+}\) be the episode length. As suggested in [30; 31], the following joint chance constraints is imposed:

\[_{_{0},}^{}\{_{k+i} , i[T]_{k}\} 1-,\  k=0,1,2,... \]

where \([0,1)\) denotes a safety threshold regarding the probability of the agent going to an unsafe region and \([T]:=\{1,...,T\}\) is the index set. The left side of the chance constraint (1) is a conditional probability, specifying the probability of having states of future \(T\) steps in the safe region when \(_{k}\) is inside the safe region. When the system is involved with unbounded uncertainty \(\), it is impossible to ensure the safety with a given probability level in infinite-time scale . Instead, ensuring safety in a future finite time when the current state is within the safety region is reasonable and practical . This paper calls the MDP equipped with chance constraint (1) as Chance Constrained Markov decision processes (CCMDPs). It refers to the problem with almost surely safe constraint when \(=0\). The set of feasible stationary policies for a CCMDP is defined by \(_{}:=\{:_{k},\}.\) Chance constrained reinforcement learning (CCRL) for a CCMDP is to seek an optimal constrained stationary policy by solving

\[_{_{}}\ V^{}().\] (CCRL)

Define optimal solution set of Problem CCRL by \(_{}^{*}:=\{_{}:V^{}()= _{_{}}\ V^{}()\}.\) Let \(_{}^{*}_{}^{*}\) be an optimal solution of Problem CCRL. Associated with \(_{}^{*}\), we denote \(V_{}^{*}():=V^{_{}^{*}}()\) and \(Q_{}^{*}(,):=Q^{_{}^{*}}(, )\) for the optimal value and value-action functions.

Define a function \(^{*}(,):=_{, }^{_{}^{*}}\{_{k+i}, i [T]_{k}=,_{k}=\}.\) The continuity of \(^{*}(,)\) is guaranteed under mild conditions giving as Assumptions 1 and 2 (pp. 78-79 of ). Besides, the upper semicontinuity of \(Q_{}^{*}(,)\) is from Assumption 1.

**Assumption 1**.: _Suppose that \(\) is compact and \(r(,)\) is continuous1 on \(\). Besides, assume that the state transition model \(\) can be equivalently described by \(^{+}=f(,,),\) where \(^{s}\) is a random variable and \(f()\) is a continuous function on \(\). The probability density function is \(p_{}().\)_

Assumption 1 is natural since it only requires that the reward function is continuous and the state transition can be specified by a state space model with a continuous state equation, which is general in many applications. We do not require \(f()\) to be available.

**Assumption 2**.: _The constraint function \(g()\) is continuous. For every \(\) and \(\), we have_

\[_{,}^{_{*}^{*}}\{_{i[T]}\,g( _{k+i})=0_{k}=,_{k}= \}=0. \]

Assumptions 1 and 2 are essentially assuming the regularities of \(g\) and \((,),\) which is not a strong assumption. With \(^{*}(,)\), we define a probability measure optimization problem (PMO) by

\[_{ M()}\ \ _{}Q_{}^{*}( ,).. _{}^{*}(,) 1-.\] (PMO)

We have the following theorem for the optimal constrained stationary policy \(_{}^{*}\) of Problem CCRL:

**Theorem 1**.: _The optimal value of Problem_ PMO _equals \(V_{}^{*}()\) for any \(\). The probability measure \(_{}^{*}\) associated with \(_{}^{*}(|)\) is an optimal solution of Problem_ PMO _for any \(\)._

[MISSING_PAGE_FAIL:4]

where \(_{}(0,1)\) is the discount factor and \((z)\) defines an indicator function with \((z)=1\) if \(z>0\) and \((z)=0\) otherwise. The following theorem for Problem ECRL holds:

**Theorem 4**.: _A flipping-based policy exists in the optimal solution set of Problem ECRL._

See Appendix E for the proof. The proof follows the same pattern of Theorem 2. We first construct the Bellman recursion with the expected cumulative safety constraint and then prove the existence of a flipping-based policy as optimal policy. The optimality of flipping-based policy can also be extended to the safety constraint function with an additive structure in a finite horizon, written by \(_{i=1}^{T}_{,}^{}\{ _{i}_{0}=\}\). This safety constraint refers to affine chance constraints . We summarize the extension to problems with affine chance constraints in Appendix J.

**Remark 1**.: _Theorem 4 can be extended to a more general case where the cumulative safety constraint is not limited to an indicator function but can be any Lipschitz continuous function, thereby broadening the applicability of our theory to more practical scenarios._

### Conservative Approximation of Joint Chance Constraint

We resolve the curse of dimensionality by searching for the optimal policy within a set \(_{}\) of parametrized policies with parameters \(^{}\), for example, neural networks of a fixed architecture. Here, we use \(_{}\) to specify a policy parametrized by \(\). If the assumption of the existence of the universal approximator holds, we can approximate the optimal flipping-based policy by using a neural network with state \(\) as input and \(_{}^{}()\) as output. Another representation of the flipping-based policy is using Gaussian mixture distribution, written by \((|)=w()(}_{(1)}(),_{(1)}())+(1-w( ))(}_{(2)}(),_{(2)}( )).\) The output is \(}_{(1)}(),}_{(2)}(),\)\(w(),\)\(_{(1)}(),\) and \(_{(2)}().\)

If we have \(w()=w^{}(),\)\(}_{(1)}()=_{(1)}^{}(),\) and \(}_{(2)}()=_{(2)}^{}()\) for every \(\), the flipping-based policy using Gaussian mixture distribution can approximate the flipping-based policy with binary distribution when the covariances \(_{(1)}()\) and \(_{(2)}()\) vanish for every \(\). To simplify the implementation, we can use the neural network that outputs \(_{}^{}()\) and achieve the random search by adding a small Gaussian noise on \(_{(1)}^{}()\) and \(_{(2)}^{}()\) during implementation.

Rewrite the joint chance constraint (1) with \(_{0}=\) for parametrized policy by

\[_{,}^{}\{ _{k+i}, i[T]_{k} \} 1-,\; k=0,1,2,... \]

In local parametrized policy search (LPS) for CCMDPs, \(\) is updated by solving

\[_{}V^{}()\; \;\;\;\;\;\;()\;\;\;\;D(_{}\|_{_{k}}).\] (LPS)

Here, \(D()\) denotes a similarity metric between two policies, such as Kullback-Leibler (KL) divergence, and \(>0\) is a step size. The updated policy \(_{_{k+1}}\) is parametrized by the solution \(_{k+1}\) of Problem LPS. The updating process considers the joint chance constraint. Problem LPS is challenging to solve directly due to joint chance constraint. Since MDP with the expected discounted safety constraint can be solved by the existing safe RL algorithm (e.g. CPO). Thus, by introducing the conservative approximation of joint chance constraint, we enable the existing safe RL algorithm to obtain a conservatively approximate solution of Problem CCRL. First, we introduce the formal definition of the conservative approximation:

**Definition 1**.: _A function \(C:\) is called a conservative approximation of joint chance constraint (3) if we have_

\[C(,) 0,\; _{,}^{}\{ _{k+i}, i[T]_{k} \} 1-,\; k=0,1,2,... \]

Let \(H_{}(,):=_{_{ }_{_{0},}^{}} \{_{i=1}^{}_{}^{i}(g( _{i}))_{0}=\}\) be a value function for unsafety starting with state \(\). We have theorem of conservative approximation of joint chance constraint:

**Theorem 5**.: _Suppose that \(\) and \(\) are compact and Assumption 2 holds. Define a function \(C_{}(,)\) by \(C_{}(,):=H_{}(,)-.\) There exist large enough \(_{}\) and small enough \(T\) such that \(C_{}(,) 0\) is a conservative approximation of (3)._

The proof of Theorem 5 is summarized in Appendix F. We formulate a conservative approximation of Problem LPS (CLPS) as follows:

\[_{}V^{}()\; \;\;\;\;\;\;\;H_{}(, ),\;D(_{}\|_{_{k}}).\] (CLPS)By Theorem 5, the optimal solution of Problem \(\) is a feasible solution of Problem \(\) and thus the corresponding parametric policy is within \(_{}^{*}\). We have a remark on Theorem 5 as follows:

**Remark 2**.: _By the same procedure of proving Theorem 5, we can show that Problem \(\) is a conservative approximation of Problem \(\)._

### Practical Algorithms

Then, we present a practical way to train the optimal flipping-based policy using existing tools in the infrastructural frameworks for safe reinforcement learning research, such as OmniSafe . The provided tools can train the deterministic or Gaussian distribution-based stochastic policies. We take the parameterized deterministic policies as an example, which is specified by \(_{}^{}\). The parameter \(\) is within a compact set \(\). We write the reinforcement learning of parameterized deterministic policy (PDPRL) for CCMDPs by

\[_{}J():=_{_{} _{}^{}}\{R(_{})\}  F^{}() 1-.\] (PDPRL)

Here, the constraint function \(F^{}()\) is defined by

\[F^{}():=_{_{0}_{0}} \{_{,}^{_{}^{}} \{_{i}, i[T]|_{0}\} \}.\]

Let \(J_{}^{*}\) and \(_{}^{*}\) be the optimal value and optimal solution set of Problem PDPRL. Different from the previous discussions, we here consider the expectation of the initial state \(_{0}\) instead of considering the problem for each \(_{0}\). The reason is that the provided tools in OmniSafe, for example, CPO  and PCPO , address the problems in which the reward functions consider the expectation of the initial state. We extend the previous results of the flipping-based policy to this case.

Let \(()\) be the Borel \(\)-algebra (\(\)-field) on \(^{n_{}}\) with Euclidean distance. Let \( M()\) be a probability measure on \((,())\). With the above notation, associated with Problem PDPRL, a reinforcement learning of parameterized stochastic policy (PSPRL) is formulated as:

\[_{ M()}_{}J() _{}F^{}()  1-.\] (PSPRL)

Let \(M_{}():=\{ M():_{}F^{ }() 1-\}\) be the feasible set of Problem \(\). The optimal objective value and the optimal solution set of Problem \(\) are \(_{}^{*}:=_{ M_{}()}_{}J()\) and \(A_{}:=\{ M_{}():_{}J()=_{}^{*}\}\). A probability measure \(_{}^{*} A_{}\) is called an optimal probability measure for Problem \(\). Define \(_{}:=\{_{}^{2}:_{i=1}^{2} _{}(i)=1\}\). Let \(_{}:=(_{}(1),_{}(2),^{(1)},^{(2)})\) be a variable in the set \(_{}:=_{}^{2}\). Consider an optimization problem on \(_{}\), reinforcement learning of parameterized flipping-based policy (PFPRL), written as

\[_{_{}_{}}\;_{i=1}^{2}J(^{(i)})_{}(i)_{i=1}^{2}_{ }(i)F^{}(^{(i)}) 1-.\] (PFPRL)

Define \(_{,}:=\{_{}_{ }:_{i=1}^{2}F^{}(^{(i)})_{}(i ) 1-\}\) as the feasible set of Problem PFPRL. In addition, define the optimal objective value and optimal solution set of Problem PFPRL by \(_{}^{}:=_{i=1}^{2}J(^{( i)})_{}(i):_{}_{, }}\) and \(D_{}:=\{_{}_{,}: _{i=1}^{2}J(^{(i)})_{}(i)=_{}^{ }\}\), respectively. We have Theorem 6 for parameterized flipping-based policy.

**Theorem 6**.: _The optimal values of Problems \(\) and \(\) are equal, \(_{}^{*}=_{}^{}\). If \(J_{}^{*}\) is a strictly convex function of \(\) on an interval \((,),\) then, \((,),_{}^{*}> J_{}^{*}\) holds._

See Appendix G for the proof. Note that Theorem 6 clarifies the existence of a parameterized flipping-based policy achieving optimality and the condition under which it performs better than deterministic policies.

**Remark 3**.: _Theorems 6 and 2 have different results on the flipping probability. Theorem 2 claims a state-dependent flipping probability while the flipping probability in Theorem 6 is fixed. The distinction arises from a subtle difference between Problem \(\) and Problem \(\). In Problem \(\), the optimal policy for each state is derived based on a revised Bellman equation, ensuring that the joint chance constraint is satisfied pointwise for every state. On the other hand, Problem \(\) focuses on the expectation of the joint chance constraint, evaluated over the probability distribution of the initial state. This formulation eliminates the need for pointwise satisfaction across the state space, causing the state-dependent nature of the constraint to disappear._

There is no existing tool to solve Problem \(\) and we can only apply them to solve Problem \(\) for any given \(\). Let \(_{S}=\{_{i}\}_{i=1}^{S},\ _{i} ,\  i[S]\) be a set of probability levels. For each \(_{i}\), define \(}_{i}\) be the optimal solution of Problem \(\) with \(=_{i}\). Consider the linear program (LP):

\[_{_{}(1),,_{}(S)^{S}}\ _{i=1}^{S}J( }_{i})_{}(i)_{i=1}^{S} _{}(i)F^{}(}_{i}) 1-,\ _{i=1}^{S} _{}(i)=1.\] (LP)

Define the optimal objective value and optimal solution set \(_{}(_{S})\) of Problem LP by \(}_{}^{}(_{S})\) and \(_{}(_{S})\), respectively. The optimal flipping-based policy is characterized by \((_{}(j_{1}^{*}),_{}(j_{2}^{*}),}_{j_{1}^{*}},}_{j_{2}^{*}}),\) where \(j_{1}^{*}\) and \(j_{2}^{*}\) are the index for the non-zero elements of the optimal solution of linear program LP The following theorem holds for \(}_{}^{}(_{S})\) and \(_{}(_{S})\).

**Theorem 7**.: _There exists an optimal solution in \(_{}(_{S})\) such that the number of non-zero elements does not exceed two. Besides, if \(_{i}\) is extracted independently and identically (uniform distribution), as \(S\), we have \(}_{}^{}(_{S})_{}^{*}\) with probability 1._

See Appendix H for the proof. Theorem 6 shows that there exists an optimal solution to Problem \(\) that is a linear combination of two deterministic policies. Theorem 7 clarifies that we could obtain an approximate flipping-based policy to Problem \(\) by optimizing the linear combination of multiple trained optimal deterministic policies. One is the linear combination of two policies among all possible linear combinations. The above conclusions can be extended to the Gaussian distribution-based stochastic policies. Besides, **the above conclusions still hold after replacing the chance constraint with the expected cumulative safe constraint in CPO and PCPO.** We summarize a general algorithm for approximately training the flipping-based policy based on the existing safe RL algorithms in Algorithm 1. With \((_{}(j_{1}^{*}),_{}(j_{2}^{*}),}_{j_{1}^{*}},}_{j_{2}^{*}}),\) we implement the flipping-based policy by Algorithm 2. In the practical implementation, the weight is constant instead of a function of the initial state since Problems \(\) and \(\) consider the expectation of the initial state. Besides, \(_{i}\) in (1) is replaced by cost limit when using CPO or PCPO to obtain a conservative approximation of (3).

```
1: Observe the state \(_{k}\) at time step \(k=0,1,2,...\)
2: Randomly generate a number \(\) from \(\) obeying uniform distribution
3: If \(_{}(j_{1}^{*})\), generate \(_{k}\) by \(_{}_{j_{1}^{*}}}^{}\). Otherwise, generate \(_{k}\) implement \(_{}_{j_{2}^{*}}}^{}\)
```

**Algorithm 2** Flipping-based policy implementation

### Safety with Finite Samples

The update by solving Problem \(\) is difficult to implement practically since the evaluation of the constraint function \(H_{}(,)\) is necessary to clarify whether a policy \(\) is feasible,which is challenging in high-dimensional cases. Here, we apply the surrogate functions proposed in  to replace the objective and constraints of Problem \(\). With a probability \(_{}[0,)\), the CPO-based approximation of Problem \(\) (\(\)) is written by

\[&_{}_{_{ }_{_{k}},_{_{k}}}\{r(_{},)\}\\ &\ H_{}(_{k}, )+}}_{_{ }_{_{k}}}^{_{}}\{(g(^{+}))\}_{}, \ D(_{}\|_{_{k}}).\] ( \[\] )

Note that the optimal solution \(_{k+1}\) of Problem \(\) may differ from the one of Problem \(\). Proposition 2 of  gives the upper bound of CPO update constraint violation. The upper bound depends on the values of the step size \(\), probability level \(_{}\), discount factor \(_{}\), and the maximal expected risk defined by \(_{_{}}^{_{k+1}}:=_{_{} }\ _{_{_{k+1}}}\{ (g(^{+}))\}.\) The upper bound can be written by \(H_{}(_{k+1},)_{ }+_{}_{_{}}^{ _{k+1}}}{(1-_{})^{2}}.\) By choosing sufficiently small step size \(\), discount factor \(_{}\), and probability level \(_{}\), it is able to ensure that \(H_{}(_{k+1},)\).

In practical implementation, the exact value of \(_{_{_{k}},_{ }}\{(g(^{+}))\}\) is unavailable and samples of \(_{}_{_{k}}\) and \(_{}\) are used to approximate the CPO update. The data set is defined by \(_{N}:=\{(^{(i)},^{(i)},^{+,(i)})\}_{i=1}^{N},\) where \(N_{+}\) is the sample number and \(^{+,(i)}\) is a sample of successor with previous state \(^{(i)}\) and action \(^{(i)}\). Instead of directly solving Problem \(\), the following sample average approximate of Problem \(\) (\(\)-\(\)) is solved:

\[_{}_{i=1}^{N}r(_{}^{(i)},^{(i)})_{}^{}(_{k},,_{},_{N})_{},\ D(_{}\|_{_{k}}) .\] ( \[\] - \[\] )

Here, \(_{}^{}(_{k},, _{},_{N}):=H_{}( _{k},)+})N}_{i=1}^{N} (g(^{+,(i)}))\) and \(_{}[0,_{})\) is a probability level. The extraction of sample set \(_{N}\) is random, and thus the optimal solution \(}_{_{}}(,_{k}, _{N})\) of Problem \(\)-\(\) is a random variable due to the independence on the sample set \(_{N}\). We need to investigate the probability that \(}_{_{}}(,_{k}, _{N})\) admits a feasible policy for Problem \(\). We have Theorem 8 for the safety with finite sample number. See Appendix I for the proof.

**Theorem 8**.: _Suppose that the step size \(>0\) and \(_{}[0,)\) are adjusted to ensure that \(H_{}(_{k+1},)\). There exist \(\) and \(_{}\) such that, if \(_{}[0,_{}),\ _{}> _{},\) and \(T<\), such that \(}_{_{}}(,_{k}, _{N})\) admits a feasible policy for Problem \(\) with a probability larger than \(1-\{-2N(_{}-_{})^{2}(1-_ {})^{2}\}.\)_

Note that Theorems 5 and 8 only show the existence of the parameters for safety but do not show an explicit way to choose \(_{}\) for specified \(T\). If a conservative safety is desired for practical applications, we recommend using a \(_{}\) close to \(1\).

## 5 Experiments

### Numerical Example

We conduct a numerical example to illustrate how the flipping-based policy outperforms the deterministic policy in CCMDPs. The numerical example considers driving a point from the initial point \((0,0)\) to the goal \((15,15)\) with the probability of entering dangerous regions smaller than a required value. The uncertainties come from the disturbances to the implemented actions. The metric for evaluating the performance is the cumulative inverse distance to the goal, a reward function. Due to the page limit, we summarize the details of the model and heuristic method for obtaining the neural network-based policy in Appendix K. Figure 2 (a) shows trajectories by the deterministic policy in one thousand simulations with mean reward as \(0.8667\) and violation probability as \(17\%\). The red trajectories have intersections with the dangerous region. The deterministic policy led to a sideway in front of the dangerous regions since crossing the middle space violates the violation probability constraint. Figure 2 (b) shows trajectories by flipping-based policy. The mean reward was reduced to \(1.8259\) while the violation probability is \(17\%\), the same as the deterministic policy. The reason is that the flipping-based policy sometimes took the risk of crossing the middle space to improve the mean reward. To balance the violation probability, the sideway root taken by the flipping-based policy was more conservative than the deterministic policy. Figure 2 (c) gives the profile of the mean reward along with the violation probability. Until around \(22\%\), the flipping-based policy outperformed the deterministic policy since the violation probability of crossing the middle space is larger than that, and the deterministic policy can cross it. The profile in Figure 2 has a convex shape until \(22\%\). Theorem 6 points out that the strict convexity implies the better reward performance of the flipping-based policy.

### Safety Gym

We conduct experiments on Safety Gym , where an agent must maximize the expected cumulative reward under a safety constraint with additive structures. The reason for choosing Safety Gym is that this benchmark is complex and elaborate and has been used to evaluate various excellent algorithms. The infrastructural framework for performing safe RL algorithms is OmniSafe . The proposed method has been validated in two environments: PointGoal2 and CarGoal2. Four algorithms are used as baselines. The first is CPO , a well-known algorithm for solving CMDPs. The other three algorithms are PCPO , P3O , and CUP , recent algorithms that achieve superior performance compared to CPO. Due to space limitations, we only present the experimental results of the test processes for CPO and PCPO on PointGoal2. The details of the experimental setup, all four algorithms' training process results on PointGoal2 and their test process results on CarGoal2 are provided in Appendix L.

**Baselines and metrics.** We implement the practical algorithms presented in Section 4.3 to obtain the flipping-based policy. We modified Algorithm 1 to train the flipping-based policy based on CPO and PCPO. In Algorithm 1, the sample set \(_{S}\) consists of samples of violation probabilities. Since CPO and PCPO consider the expected cumulative safety constraints, the sample set \(_{S}\) includes the samples of cost limits of the expected cumulative safety constraints. Instead of using the training

Figure 3: Experimental results on Safety Gym (PointGoal2). Adopting the flipping-based policy increases the expected reward under the same expected cost for CPO and PCPO at intervals where the reward profile is convex. Error bars represent \(1\) confidence intervals across five different random seeds.

Figure 2: Results on the numerical example. Blue dashed lines are feasible trajectories that reach the goal set (grey shaded circle) and avoid dangerous regions (red shaded circles)). Red dashed lines mean that the constraint of avoiding dangerous regions is violated. (a) Trajectories by the deterministic policy with \(=17\%\). The mean reward is \(0.8667\); (b) Trajectories by the flipping-based policy with \(=17\%\). The mean reward is \(1.8259\); (c) Profile of the mean reward along with the violation probability. Error bars represent the minimal and maximal values across five different simulation sets.

process, we compared the performance of the testing process, where we implemented the trained policy with new randomly generated initial states and goal points and evaluated the expectations of the reward and cost for each trained policy. We investigate whether the expected reward of each baseline under the same expected cost limit can be improved by transforming the policy into a flipping-based policy without any other changes. We employ the expected cumulative reward and the expected cumulative safety as metrics to evaluate the flipping-based policy and the aforementioned baselines. We execute CPO and PCPO with five random seeds and compute the means and confidence intervals.

**Results.** The experimental results are summarized in Figure 3. As shown in the figure, for CPO and PCPO, the expected reward increases as the expected cost limit rises, and it exhibits convexity at some intervals. At intervals with convexity, the flipping-based policy significantly increases the expected reward. While at intervals without convexity, the flipping-based policy does not increase the expected reward. The above observation fits Theorem 6. From the experiments including more details in Appendix L, we also observe that our flipping-based policy can generally enhance existing safe RL algorithms, although the degree of improvement depends on the original algorithm's performance. Essentially, the flipping mechanism is a linear combination of a performance policy (risky but high-performing) and a safety policy (safe but lower-performing) designed to increase the reward while maintaining the required level of risk. One concern regarding the results in Figure 3 is that the flipping-based policy introduces broader confidence intervals. In theory, however, the flipping-based policy does not increase the size of the confidence intervals. This is demonstrated in the numerical example in Section 5.1, where the policy achieves solutions closer to the optimal ones as outlined in Theorem 2. The practical implementation described in Section 4.3, however, may experience broader confidence intervals due to the presence of two sources of Gaussian noise. It is possible to mitigate this issue by reducing the degree of stochasticity in the policy, for instance, by using smaller variances for the Gaussian noise. This adjustment would not negatively impact the performance in terms of mean reward and cost.

On the other hand, we summarize the results of the relation between expected cumulative safety and the violation probability in Figure 4. Expected cumulative safety and violation probability follows a linear causality, indicating that the flipping-based policy outperforms the deterministic policy under joint chance constraint. With the same expected cumulative safety, a larger \(T\) introduces a larger violation probability, which validates Theorem 5.

## 6 Conclusions

In this article, we first introduce the Bellman equation for CCMDP and prove that a flipping-based policy exists that achieves optimality. We then proposed practical implementation of approximately training the flipping-based policy for CCMDP. Conservative approximations of joint chance constraints were presented. Specifically, we introduced a framework for adapting Constrained Policy Optimization (CPO) to train a flipping-based policy. This framework can be easily adapted to other safe RL algorithms. Finally, we demonstrated that a flipping-based policy can improve the performance of safe RL algorithms under the same safety constraints limits on the Safety Gym benchmark.

Figure 4: Experimental results on Safety Gym (PointGoal2). The relationship between expected cumulative safety and violation probabilities.