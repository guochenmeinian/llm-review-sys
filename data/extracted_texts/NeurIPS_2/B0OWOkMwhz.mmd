# MVSplat360: Feed-Forward 360 Scene Synthesis

from Sparse Views

Yuedong Chen1 Chuanxia Zheng2 Haofei Xu34 Bohan Zhuang1

Andrea Vedaldi2 Tat-Jen Cham5 Jianfei Cai1

###### Abstract

We introduce MVSplat360, a feed-forward approach for 360\({}^{}\) novel view synthesis (NVS) of diverse real-world scenes, using only sparse observations. This setting is inherently ill-posed due to minimal overlap among input views and insufficient visual information provided, making it challenging for conventional methods to achieve high-quality results. Our MVSplat360 addresses this by effectively combining geometry-aware 3D reconstruction with temporally consistent video generation. Specifically, it refactors a feed-forward 3D Gaussian Splatting (3DGS) model to render features directly into the latent space of a pre-trained Stable Video Diffusion (SVD) model, where these features then act as pose and visual cues to guide the denoising process and produce photorealistic 3D-consistent views. Our model is end-to-end trainable and supports rendering arbitrary views with as few as 5 sparse input views. To evaluate MVSplat360's performance, we introduce a new benchmark using the challenging DL3DV-10K dataset, where MVSplat360 achieves superior visual quality compared to state-of-the-art methods on wide-sweeping or even 360\({}^{}\) NVS tasks. Experiments on the existing benchmark RealEstate10K also confirm the effectiveness of our model. Readers are highly recommended to view the video results at donydchen.github.io/mvsplat360.

## 1 Introduction

The rapid advancement in 3D reconstruction and NVS has been facilitated by the emergence of differentiable rendering . These methods, while fundamental and impressive, are primarily tailored for per-scene optimization, requiring hundreds or even thousands of images to comprehensively capture every aspect of the scene. Consequently, the optimization process for each scene can be time-consuming, and collecting thousands of images is impractical for casual users.

In contrast, we consider the problem of novel view synthesis in diverse real-world scenes using a limited number of source views through a feed-forward network. In particular, this work investigates _the feasibility of rendering wide-sweeping or even 360\({}^{}\) novel views using extremely sparse observations_, like fewer than \(5\) images. This task is inherently challenging due to the complexity of scenes, where the limited views do not contain sufficient information to recover the whole 3D scene. Consequently, there is a necessity to ensemble visible information under minimal overlap accurately and generate missing details reasonably.

This represents a new problem setting in sparse-view feed-forward NVS. Existing feed-forward methods typically focus on two distinct scenarios: 360\({}^{}\) NVS with extremely sparse observations, but only at _object-level_, or generating reasonable results for _scene-level_ synthesis, but only for nearby viewpoints . In contrast, we argue that the time is ripe to unify these previously distinct research directions.

Our goal should be to develop systems capable of synthesizing wide-sweeping or even 360\({}^{}\) novel views of large, real-world scenes with complex geometry and significant occlusion. Specifically, this work explores synthesising 360\({}^{}\) novel views from fewer than \(5\) input images. We show that in this challenging setting, existing feed-forward scene synthesis approaches  struggle to succeed. This failure arises from two main factors: i) the limited overlap among input views causes many contents to appear in only a few views or even a single one, posing significant challenges for 3D reconstruction; ii) the extremely sparse observations lack sufficient information to capture the comprehensive details of the whole scene, resulting in regions unobserved from novel viewpoints.

In this paper, we propose a simple yet effective framework to address these limitations and introduce the first benchmark for feed-forward 360\({}^{}\) scene synthesis from sparse input views. Our key idea is to leverage prior knowledge from a large-scale pre-trained latent diffusion model (LDM)  to "imagine" plausible unobserved and disoccluded regions in novel views, which are inherently highly ambiguous. Unlike existing 360\({}^{}\) object-level NVS approaches , large-scale real-world scenes comprise multiple 3D assets with _complex arrangements, heavy occlusions_, and _varying rendering trajectories_, which makes it particularly challenging to condition solely on camera poses, as also verified by concurrent work ViewCrafter .

To develop a performant framework for scene-level synthesis, we opt to treat the LDM as a refinement module, while relying on a 3D reconstruction model to process the complex geometric information. Broadly, we build upon the feed-forward 3DGS  model, MVSplat , to obtain coarse novel views by matching and fusing multi-view information with the cross-view transformer and cost volume. Although these results are imperfect, exhibiting visual artifacts and missing regions (see Fig. 1), they represent the reasonable geometric structure of the scene, as they are rendered from 3D representation. Furthermore, we choose Stable Video Diffusion (SVD)  over other image-based LDM as the refinement module, since its strong temporal consistency capabilities align better with the view-consistent requirement of the NVS task, as also observed by concurrent work 3DGS-Enhancer . Conditioning SVD with the 3DGS rendered outputs, our MVSplat360 produces visually appealing novel views that are multi-view consistent and geometrically accurate (see Fig. 1).

Importantly, the original MVSplat outputs only RGB images, which is not the optimal condition for the generator, and is difficult to optimize jointly with the SVD denoising module. To tackle this, we

Figure 1: **Examples of our MVSplat360**. Given sparse and wide-baseline observations of diverse in-the-wild scenes, MVSplat360 can directly render 360\({}^{}\) novel views (inward or outward facing) or other natural camera trajectory views in a _feed-forward_ manner, without any per-scene optimization.

propose a simple Gaussian feature rendering with multi-channels, supervised with an introduced latent space alignment loss. Despite a seemingly minor change, the additional feature condition for SVD leads to a significant impact: It bypasses the SVD's frozen image encoder, allowing the gradients from SVD to backpropagate to enhance the geometry backbone and lead to improved visual quality, especially on the new challenging DL3DV-10K dataset. While related work Reconfusion , CAT3D  and latentSplat  also combine the 3D representation with 2D generators, the former two focus more on per-scene optimisation, while the latter only shows 360\({}^{}\) NVS at the object level.

We conduct a series of experiments, mainly on two datasets. First, we establish a new benchmark on DL3DV-10K dataset , creating a new training and testing split for feed-forward wide-sweeping and 360\({}^{}\) NVS. In this challenging setting, our MVSplat360 achieves photorealistic 360\({}^{}\) NVS from sparse observations and demonstrates significantly better visual quality, where the previous scene-level feed-forward methods [9; 60; 60; 10] fail to achieve plausible results. Second, we deploy MVSplat360 on the existing RealEstate10K  benchmark. Following latentSplat , we estimate both interpolation and extrapolation NVS, and report state-of-the-art performance.

Our main contributions can be summarized as follows. 1) We introduce a crucial and pressing problem for novel view synthesis, _i.e._, how to do wide-sweeping or even 360\({}^{}\) NVS from sparse and widely-displaced observations of diverse in-the-wild scenes (_not objects_) in a feed-forward manner (_no any per-scene optimization_). 2) We propose an effective solution that nicely integrates the latest feed-forward 3DGS and the pre-trained Stable Video Diffusion (SVD) model with meticulous integration designs, where the former is for reconstructing coarse geometry and the latter is for refining the noisy and incomplete coarse reconstruction. 3) Extensive results on the challenging DL3DV-10K and RealEstate10K datasets demonstrate the superior performance of our MVSplat360.

## 2 Related Work

**Sparse view per-scene reconstruction and synthesis.** Differentiable rendering methods, such as NeRF  and 3DGS , are mainly designed for very dense views (_e.g._, 100) as inputs for per-scene optimization, which is impractical to collect for casual users in real applications. To bypass the requirement for dense views, various regularization terms have been proposed in per-scene optimization [33; 11; 69; 48]. Recently, ZeroNVS , Reconfusion  and concurrent submissions, including CAT3D , ReconX , ViewCrafter , LM-Gaussian , 3DGS-Enhancer , have leveraged large-scale diffusion models for generating pseudo dense views of a 3D scene, which are then input into a per-scene reconstruction pipeline. However, these methods are inherently slow for reconstructing unseen scenes due to the necessity of per-scene optimisation.

**Feed-forward scene reconstruction and synthesis.** To mitigate these limitations, early approaches like Light Field Networks  use ray querying to predict novel views. Subsequent methods [44; 43] employ epipolar attention for multi-view geometry estimation. Later, pixelNeRF  devised pixel-aligned features for NeRF reconstruction , leading to a range of subsequent methods that incorporate feature matching fusion [7; 9], Transformers [36; 13; 32] and 3D volume representation [7; 60]. Recently, 3D Gaussian Splatting  has been implemented into feed-forward networks, such as pixelSplat , MVSplat , Splatter Image , Flash3D  and latentSplat . While these methods were successful in novel view synthesis from sparse views, they fail to achieve this in a wide-sweeping or 360\({}^{}\) setting. Concurrent yet unpublished submissions, DepthSplat  and Long-LRM , also show promising results in the 360\({}^{}\) setting, but their frameworks have limited generation capabilities, necessitating the use of denser inputs, _e.g._, 12 or 32 views.

**Camera trajectory controllable synthesis.** Generative models have achieved remarkable results for image/video synthesis [14; 73; 8; 35; 5; 17; 39; 3], but they lack precise control over the viewpoint of generated images. To address this, several approaches fine-tune large-scale pre-trained diffusion models with explicit image and pose conditions [27; 26; 25; 4; 72; 64; 56; 50]. However, these methods mainly show 360\({}^{}\) NVS results on single objects, leaving the complex scene synthesis problem unsolved. Natural scenes comprise multiple objects with intricate occlusion relationships, presenting greater challenges that are not easily addressed by these single-object NVS models. Besides, camera trajectories can be highly irregular and varied when roving around such complex scenes. Although related works [55; 22; 38; 15] have explored training or fine-tuning diffusion models with camera control for scene synthesis, they often struggle with precise camera pose control [55; 22], and still rely on per-scene optimization for 3D reconstruction [38; 15].

## 3 Methodology

Given \(N\) sparse views \(=\{^{i}\}_{i=1}^{N}\) and the corresponding camera poses \(=\{^{i}\}_{i=1}^{N}\), with \(^{i}=(^{i},^{i},^{i})\) comprising intrinsic \(^{i}\), rotation \(^{i}\) and translation \(^{i}\), our goal is to learn a model \(\) that synthesizes wide-sweeping or even 360\({}^{}\) novel view synthesis (NVS).

We opt to go beyond per-scene optimisation [1; 2; 58; 15], and to deal with a more general feed-forward network capable of achieving 360\({}^{}\) NVS for unseen scenes, _yet without the need of additional per-scene training_. This requires effectively matching information between sparse views in 3D space, as well as generating sufficient content based on only partial observations. To achieve that, our MVSplat360 framework, illustrated in Fig. 2, comprises two main components: a multi-view geometry reconstruction module (Section 3.1) and a multi-frame consistent appearance refinement network (Section 3.2). The former is responsible for matching and fusing multi-view information from sparse observations to create a coarse geometry reconstruction, whereas the latter is designed to refine the appearance with a pre-trained latent video diffusion model. While similar two-step approaches have been explored in recent related works, _e.g._, [4; 58; 38; 15], we are the first (to the best of our knowledge) to explore it on wide-sweeping or even 360\({}^{}\) NVS for large-scale scenes from sparse views (as few as 5), _in a feed-forward manner_.

### Multi-View Coarse Geometry Reconstruction

The first module is built upon a feed-forward 3DGS reconstruction model, _i.e._, MVSplat  as in our implementation. Specifically, given sparse-view observations \(=\{^{i}\}_{i=1}^{N}\) and their corresponding camera poses \(=\{^{i}\}_{i=1}^{N}\), the model learns to predict 3D Gaussian parameters \(\{(_{i},_{i},_{i},_{i})\}_{i=1}^{H W  N}\), which can then be splatted to obtain a set of RGB images \(}^{}\) using the target camera poses \(^{}\). To ensure better integration with the following diffusion module, we predict an additional Gaussian feature \(}_{i}\), in parallel with other parameters, which can be rasterized to the corresponding latent features \(}^{}\). Furthermore, we also improve the view selection strategy to improve the model's robustness in handling widely displaced inputs.

**Coarse geometry reconstruction.** Our backbone comprises multi-view feature extraction, cost volume construction, depth estimation, and 3D Gaussian parameter predictions. First, a cross-view transformer encoder is applied to fuse multi-view information and obtain cross-view aware features \(=\{^{i}\}_{i=1}^{N}\). Then, \(N\) cost volumes \(=\{^{i}\}_{i=1}^{N}\) are constructed by matching feature correlations between cross-views. Specifically, it uniformly divides the depth into \(L\) layers in the near and far depth ranges, _i.e._\(=\{D_{m}\}_{m=1}^{L}\), and then warps the features from one view \(j\) to another view \(i\) via \(_{D_{m}}^{j i}=(^{j},^{i},^{j},D_{m})\). The cost volume \(^{i}=[_{D_{1}}^{i},_{D_{2}}^{i},,_{D_{L}}^{i}]\) is then collected by \(L\) correlations, where each correlation is expressed as \(_{D_{m}}^{i}=_{D_{m}}^{j i}^{i}}{}\), with \(C\) denoting channel dimension. Finally, the per-view estimated depth \(d\) is obtained by applying the softmax operation on the cost volumes in the depth dimension. After that, the Gaussian mean is computed by \(=^{-1}d+\), where \(\) is the camera intrinsic, \(=(u_{x},u_{y},1)\) denotes each pixel, and \(^{3}\) is the predicted offset, along with opacity \(a\), covariance \(^{3 3}\), and color \(^{3(S+1)^{2}}\)

Figure 2: **Overview of our MVSplat360**. (a) Given sparse posed images as input, we first match and fuse the multi-view information using a multi-view Transformer and cost volume-based encoder. (b) Next, a 3DGS representation is constructed to represent the coarse geometry of the entire scene. (c) Considering such coarse reconstruction is imperfect, we further adapt a pre-trained SVD, using features rendered from the 3DGS representation as conditions to achieve 360\({}^{}\) novel view synthesis.

where \(S\) is the order of the spherical harmonics . Once the model predicts a set of 3D Gaussian parameters \(\{(_{i},_{i},_{i},_{i})\}_{i=1}^{H W N}\), the target view \(}^{ tgt}\) can be rendered through rasterization.

**Gaussian feature rendering.** Given sparse-view observations, MVSplat  tends to render images with noticeable artifacts in wide-sweeping novel viewpoints (see Fig. 1), resulting in suboptimal conditioning for the subsequent SVD. Since the rendered images must first be encoded into the latent space using a frozen encoder (see Section 3.2), enhancing the backbone with gradients from SVD would be computationally expensive. To address this issue, we propose directly rasterising features \(}\) into _the latent space_ of SVD, by predicting an additional parameter \(}_{i}\) for each 3D Gaussian. This operation offers two advantages: (i) The latent feature includes multi-channel information, providing a more comprehensive representation of the scene; (ii) The entire framework is end-to-end connected by conditioning SVD on the rendered latent features instead of the image-encoded ones. It enables the SVD loss to optimize the Gaussian features, further enhancing the reconstruction backbone.

**Observed and novel viewpoints selection.** To enable 360\({}^{}\) scene synthesis, it is crucial to choose the correct camera viewpoints, so that they can cover most contents in diverse and complex scenes . It is impractical to assume a circular orbital camera trajectory like those object-level 360\({}^{}\) view synthesis , whereas it is suboptimal to randomly choose a video sequence like existing scene-level nearby viewpoint synthesis . To this end, we propose to choose views _evenly distributed_ within a set of targeted viewpoints as input. Specifically, for a given set of candidate views, we apply farthest point sampling over the camera locations to identify the input views and randomly choose from the rest as target views. The number of candidate views gradually increases throughout the training, stably improving the model's capability toward handling 360\({}^{}\) scene synthesis.

**View interaction within the local group.** Recalling that the 3D reconstruction backbone MVSplat is primarily designed for nearby viewpoints, with key components like multi-view transformers and cost volume assuming sufficient overlap among input views. However, in the more challenging 360\({}^{}\) settings, the widely displaced input views lead to minimal overlap between specific view pairs, hindering the effectiveness of the backbone. To mitigate this limitation, we refactor our backbone to use cross-view attention and construct the cost volume _only within a local group_ of input views based on camera locations, reducing memory consumption and ensuring stable model convergence.

### Multi-Frame Appearance Refinement

**Video diffusion model.** MVSplat360 utilizes an off-the-shelf multi-frame diffusion model, _i.e._ Stable Video Diffusion (SVD) , to refine the visual appearance of the aforementioned coarse reconstruction. SVD is pre-trained on large-scale video datasets and has strong prior knowledge of temporal consistency. It adheres to the original formulation used by Stable Diffusion (SD)  that conducts the denoising processing in the latent space. In particular, given a target sequence of \(x^{1:M}\) with \(M\) images, they are initially embedded into the latent space by a frozen encoder \(\), yielding \(z_{0}^{1:M}=(x^{1:M})\), and then perturbed by adding Gaussian noise \((0,)\) in a Markov process:

\[z_{t}^{1:M}=_{t}}z_{0}^{1:M}+_{t}} _{t}^{1:M}, \]

while \(_{0},,_{T}\) is a pre-defined noise schedule within \(T\) steps. Given noise input \(z_{t}^{1:M}\), the denoiser \(_{}\) is then trained by optimizing the following objective function:

\[_{_{}}_{(x^{1:M},y),t,e^{1:M}(0,1)} [\|e^{1:M}-_{}(z_{t}^{1:M},t,y)\|_{2}^{2}], \]

where \(x^{1:M}\) is the target images, and \(y\) is the conditional inputs. After \(_{}\) is trained, the model can generate a video by performing iterative denoising from pure Gaussians \(z_{T}^{1:M}\) conditioned on \(y\).

Note that SVD is trained with the latest \(\)-prediction formulation , instead of the original \(\)-prediction in SD. Hence, the final loss is calculated in latent space using the mean squared error (MSE) between the ground truth and its prediction \(||z_{0}^{1:M}-_{t}^{1:M}||_{2}^{2}\), where \(_{t}^{1:M}\) is obtained by translating the velocity \(=(z_{t}^{1:M},t,y)\) to latent space, _i.e._, \(_{t}^{1:M}=_{t}z_{t}^{1:M}-_{t}\).

**Multi-view hybrid conditions.** To ensure an accurate understanding of the scene, the model requires the integration of both low-level perception (_e.g._, depth and texture) and high-level understanding (_e.g._, semantics and geometry). Following , we adopt a hybrid conditioning mechanism to fine-tune the SVD model for wide-sweeping NVS with sparse observations.

In one stream, a CLIP  image embedding token of the original visible views \(\) is used as a global type and text prompt. At each UNet block, a cross-attention operation is applied to capture high-levelsemantics of the input images to the model. Since we have sparse views, we average these tokens to become one global token. In the other stream, the spatial conditions from the coarse geometry rendered features \(}^{}=\{}_{i}\}_{i=1}^{M}\) is channel-concatenated with the noised latent \(z_{t}^{1:M}\). These spatially conditional features assist the model to capture the view information, and learn low-level perception to maintain the texture of the scenes. Compared to the concurrent work CAT3D , this coarse feature conditioning not only provides accurate pose information from the 3DGS rendering, but also offers reasonable visual information.

**Color adjustment.** While our MVSplat360 can achieve photorealistic NVS, the synthesized videos sometimes exhibit oversaturated colors (detailed in Appendix C). This may be visually acceptable for video generation, but it can decrease performance when evaluated on NVS task. To mitigate this, we apply post-processing by matching the color histogram between the SVD refined views \(}^{}\) and our 3DGS rendered views \(}^{}\) before performing pixel-aligned measurements, _i.e._, PSNR and SSIM.

### Training Objectives

Our MVSplat360 predicts two sets of images, including the coarse one \(}^{}\) from the 3DGS module and the refined one \(}^{}\) from the SVD module, where the former is mainly rendered to help supervise the geometry backbone. The entire model is end-to-end trainable, using three groups of loss functions, namely reconstruction loss, diffusion loss and latent space alignment loss.

In particular, the reconstruction loss is a linear combination of \(_{2}\) and LPIPS , applied between the coarse outputs \(}^{}\) and the corresponding ground truth \(^{}\). The other two loss functions are applied to the following SVD module, whose gradients will backpropagate to the 3DGS module but will not update those structural parameters, _i.e._, \(,,\). This is achieved by stopping the gradients from the structural parameters when rendering the latent features \(}^{}\), since keeping those gradient flows will lead to unstable training, as also observed by latentSplat . We use the standard v-prediction formulation (detailed in Section 3.2) as the diffusion loss to fine-tune the denoising network of the SVD, keeping the first-stage encoder and decoder frozen. Since the SVD's released model is conditioned on image-encoded features while our diffusion module is on 3DGS-rendered ones, we find it beneficial to align these two spaces by regularizing with a latent space alignment loss, \(_{g_{6}}_{} g(})}\| (^{})-}^{}\|_{2} ^{2}\), where \(g\) refers to the geometry backbone with trainable parameters \(\) and \(}\) is the frozen SVD encoder.

## 4 Experiments

### Experimental Details

**Datasets.** To verify the effectiveness of MVSplat360 in synthesizing wide-sweeping and 360\({}^{}\) novel views, we have established a challenging benchmark derived from DL3DV-10K . It comprises 51.3 million frames from 10,510 real-world scenes, adhering to 65 point-of-interest (POI)  categories. For training, we use a subset in subfolders "3K" and "4K", resulting in ~2,000 scenes. We tested on the 140 benchmark scenes and filtered them out from the training set to ensure correctness. For each scene, we selected 5 input views using farthest point sampling based on camera locations and evaluated 56 views by equally sampling from the remaining, yielding a total of 7,840 test views. Additionally, since most DL3DV-10K scenes contain a two-round trajectory, we also report another setting by focusing only on half of the sequence, intending to cover the camera trajectory of one round. We denote the two settings as \(n=300\) and \(n=150\), where \(n\) refers to the frame distance span across all test views, as most scenes contain roughly 300 frames. We also assess our model on RealEstate10K , which contains real estate videos downloaded from YouTube. Consistent with existing works [6; 10], we train MVSplat360 on 67,477 scenes and test it on 7,289 scenes.

**Metrics.** To measure models from different perspectives, we follow  to report both the pixel-align metrics, _i.e._, PSNR and SSIM , and the perceptual metrics, _i.e._, LPIPS  and DISTS . Since MVSplat360 aims to generate plausible contents for unobserved and disoccluded regions, we also reported the distribution metric, _i.e._, Frechet Inception Distance (FID) 1, which measures the similarity between distributions of the generated images and the real ones.

**Implementation details.** MVSplat360 is implemented with PyTorch and a CUDA-implemented 3DGS renderer. For coarse geometry reconstruction (Section 3.1), we set hyperparameters following MVSplat , except that we apply cross-view attention and build each cost volume within the nearest 2 views rather than all other views. For multi-frame appearance refinement (Section 3.2), we fine-tune from the 14-frame SVD  pre-trained model, but using rendered Gaussian features as conditions. We also remove the original "motion value" and "fps" conditions, since they are unrelated to our NVS task. We rescale the rendered feature to have a similar shape as the original image-encoded latent feature in the pre-trained model, which is critical for getting better details as it affects the decoder (more discussions are in Appendix A). We train SVD using 14 frames sampled along natural camera trajectories, captured by the initial videos. At inference, we directly feed 56 views to SVD but change all related temporal attention blocks to local attention with a window size of 14 to better align with the training. More implementation details can be found in Appendix B, and the codes are publicly available at [https://github.com/donydchen/mvsplat360](https://github.com/donydchen/mvsplat360).

### Results on the New DL3DV-10K Benchmark

We first assess the ability of MVSplat360 and baselines to synthesize wide-sweeping and 360\({}^{}\) NVS in the newly constructed challenging benchmark with diverse scene categories.

**Baselines.** We perform a thorough comparison of MVSplat360 to the latest state-of-the-art (SoTA) 3DGS-based models, including pixelSplat , MVSplat  and latentSplat . All models are trained on the same training split and evaluated on the publicly available 140 scenes.

**Quantitative results.** All models are trained to 100K steps and reported at Table 1, except for the latentSplat, which suffers from unstable training due to its GAN-based architecture. Hence, we report its best performance at around 60K training steps before the subsequent collapse. Our MVSplat360 outperforms all existing SoTA models in all metrics on the two settings \(n=300\) and \(n=150\). All models generally perform better on the \(n=150\) setting than on the \(n=300\) one since the latter spans larger viewpoints. It can be seen that the two generative models (latentSpalt and MVSplat360) generally perform better than the other two regression models, suggesting the importance of additional refinement in addressing feed-forward scene reconstruction.

Although our improvement on pixel-aligned metrics appears minor, this is expected since refinement via either interpolation (for disoccluded regions) or extrapolation (for unobserved regions) does not guarantee matching the ground truth at the pixel level. It mainly aims to provide a reasonable solution to refine the images and ensure they align with real-world image distribution. This is verified by the fact that our improvements on perceptual metrics are larger, and it is even more apparent on FID, which measures the distribution deviation. The superiority of our MVSplat360 stands out more from the qualitative results presented below.

**Qualitative results.** The qualitative comparisons are visualized in Fig. 3. MVSplat360 achieves remarkable visual results even under challenging conditions. pixelSplat  and MVSplat  exhibit obvious artifacts due to the issue of floating Gaussians. latentSplat  improves the results with an additional decoder and adversarial training. However, its resulting object geometry and image quality are still far from satisfactory, suggesting that the GAN-based framework cannot provide enough prior knowledge for refining 360\({}^{}\) NVS in diverse real-world scenes. Readers are referred to our project page for video results with more comprehensive comparisons, where our MVSplat360 shows

    &  &  \\   & PSNR\(\) & SSIM\(\) & LPIPS\(\) & DISTS\(\) & FID\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & DISTS\(\) & FID\(\) \\  pixelSplat  & 14.83 & 0.401 & 0.576 & 0.383 & 142.83 & 16.05 & 0.453 & 0.521 & 0.348 & 134.70 \\ MVSplat  & 15.72 & 0.433 & 0.501 & 0.291 & 78.95 & 17.05 & 0.499 & 0.435 & 0.247 & 61.92 \\ latentSplat  & 16.68 & 0.469 & 0.439 & 0.234 & 37.68 & 17.79 & 0.527 & 0.391 & 0.206 & 34.55 \\ MVSplat360 & **16.81** & **0.514** & **0.418** & **0.175** & **17.01** & **17.81** & **0.562** & **0.352** & **0.151** & **18.89** \\   

Table 1: **Comparison with SoTA methods on DL3DV-10K. Below, \(n\) is the frame distance span across all the tested novel views within each scene, which is set to 300 by default as most DL3DV-10K scenes contain roughly 300 extracted frames. Since most DL3DV-10K scenes contain a two-round trajectory, we also report another setting of \(n=150\) aiming for coverage of one round.**multi-view consistent, high-quality visual results along complex camera trajectories, while others suffer from apparent artifacts.

### Results on the Existing RealEstate10K Benchmark

We also assess MVSplat360 on the existing benchmark, following the existing "Interpolation" setting [6; 10] and "Extrapolation" setting . We retrained latentSplat on Interpolation and MVSplat on Extrapolation for fair comparisons.

    &  &  \\   & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & DISTS\(\) & FID\(\) \\  PixelNeRF  & 20.43 & 0.589 & 0.550 & 20.05 & 0.575 & 0.567 & 0.371 & 160.77 \\ Du _et al._ & 24.78 & 0.820 & 0.213 & 21.83 & 0.790 & 0.242 & 0.144 & 11.34 \\ pixelSplat  & 25.89 & 0.858 & 0.142 & 21.84 & 0.777 & 0.216 & 0.130 & 5.78 \\ latentSplat\({}^{*}\) & 25.53 & 0.851 & 0.139 & 22.62 & 0.777 & 0.196 & 0.109 & 2.79 \\ MVSplat  & 26.39 & **0.869** & 0.128 & 23.04 & **0.812** & 0.185 & 0.110 & 3.83 \\  MVSplat360 & **26.41** & **0.869** & **0.126** & **23.16** & 0.810 & **0.176** & **0.104** & **1.79** \\   

Table 2: **Comparison with SoTA methods on RealEstate10K**. We report interpolation scores using the settings of [6; 10], where we retrain latentSplat  to maintain fair comparison (indicates with \({}^{*}\)). We report extrapolation scores by following .

Figure 3: **Qualitative comparisons on DL3DV-10K**. MVSplat360 shows significant improvement compared to existing SoTA models. Here, we showcase with a rich mix of diversity and complexity, including indoor (bounded) _vs._ outdoor (unbounded), high _vs._ low texture frequency, more _vs._ less reflection, and more _vs._ less transparency. More results are provided in Appendix E.

**Quantitative results.** Table 2 shows quantitative comparisons on RealEstate10K  of MVSplat360 and other approaches. Our MVSplat360 surpasses all previous state-of-the-art methods, mainly in terms of the perceptual metrics and the distribution metric. The former implies that our rendered views are more aligned with human perception, while the latter shows that our refined images correspond better to the dataset distribution. These observations can be further confirmed by visual assessment.

**Qualitative results.** The qualitative comparisons of the top four best models are in Fig. 4. PixelSplat  and MVSplat  fail to render any content for the unobserved regions due to the lack of generative capability. In contrast, latentSplat can perform extrapolation via its GAN-based decoder, improving the overall visual quality. However, we observed that the content generated by latentSplat is not visually reasonable. Our MVSplat360 generates more plausible content (see the "window" in 1st row and "chair" in 2nd row), thanks to the stronger generative capability of the diffusion model.

### Ablations and Analysis

We conduct ablations on DL3DV-10K  to analyze MVSplat360 further. Results are reported in Table 3, and discussed in detail next.

**Accessing model components.** The baseline refers to MVSplat  since our model is built on top of it. (i) A natural extension is to render novel views from MVSplat and use them directly as conditions

Figure 4: **Qualitative comparisons on RealEstate10K. MVSplat360 shows reasonable generations for disoccluded and unobserved regions, while latentSplat  fills in content with artifacts.**

  
**Models** & SSIM\(\) & LPIPS\(\) & DISTS\(\) & FID\(\) \\  Baseline & 0.433 & 0.501 & 0.291 & 78.95 \\  + SVD & 0.399 & 0.556 & 0.248 & 38.05 \\ + ctx-attn & 0.467 & 0.451 & 0.185 & 22.78 \\ + GS-feat. & **0.514** & **0.418** & **0.175** & **17.01** \\   

Table 3: **MVSplat360 ablations**. All models are trained and evaluated on the DL3DV-10K dataset.

in the SVD denoising process. However, this straightforward approach performs slightly worse than the original MVSplat, likely because SVD struggles to infer pose and visual cues from the noisy image-encoded features. (ii) To better utilize the input context views, we average CLIP-embedded tokens from all views instead of just the first. This provides SVD with richer scene information via the cross-attention blocks, leading to a noticeable improvement. (iii) Lastly, we render high-dimensional features via the 3DGS rasterizer and concatenate them directly into the diffusion latent space, enabling gradients from the denoising UNet to backpropagate through the geometry backbone. This end-to-end training improves performance significantly and is used in our default model.

**Accessing the number of input views.** As shown in Table 3(b), while our model is only trained with 5 input views, the performance can be gradually improved by adding more input views at testing. This is reasonable as more input views can provide more observable areas. On the contrary, reducing input views will inevitably result in worse performance. Surprisingly, even with 3 sparse views, our MVSplat360 still outperforms regression models (pixelSplat and MVSplat) that use 5 views.

**Assessing the geometry accuracy.** Our MVSplat360 builds on the video diffusion model SVD, which does ensure strong temporal/multi-frame consistency but does not inherently guarantee geometric accuracy. To confirm that our MVSplat360 produces geometrically accurate outputs, we run structure-from-motion (SfM) on both the input source views and the rendered novel views using VGGSfM . As shown in Fig. 5, VGGSfM recovers reasonable camera poses and 3D point clouds, confirming that our novel views are both multi-view consistent and geometrically correct. This highlights how the 3DGS backbone's latent features provide essential geometric cues, enhancing 3D consistency in the final SVD-based outputs.

## 5 Conclusion

We present MVSplat360, a feed-forward model that synthesises 360\({}^{}\) novel views of diverse real-world scenes from sparse input views. Our MVSplat360 leverages a feed-forward 3DGS model for recovering the coarse geometry and appearance from sparse observations of a 3D scene, which are then used to render latent features as the pose and visual cues to guide the following SVD in generating 3D consistent 360\({}^{}\) novel views. To demonstrate its effectiveness, we construct a challenging benchmark for 360\({}^{}\) NVS of real-world scenes. Experimental results show that MVSplat360 achieves superior visual quality compared to other SoTA feed-forward approaches.

AcknowledgementsThis research is supported by the Monash FIT Start-up Grant. Dr. Chuanxia Zheng is supported by EPSRC SYN3D EP/Z001811/1.

Figure 5: **SfM on input and rendered views**. Images with red borders are the input views, while others are rendered by our MVSplat360. The reasonably recovered camera poses and 3D point clouds via VGGSfM imply that our outputs are multi-view consistent and geometrically correct.