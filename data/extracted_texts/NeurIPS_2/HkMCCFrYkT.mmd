# HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting

Yuanhao Cai \({}^{1}\), Zihao Xiao \({}^{1}\), Yixun Liang \({}^{2}\), Minghan Qin \({}^{3}\),

**Yulun Zhang \({}^{4,}\)1, Xiaokang Yang \({}^{4}\), Yaoyao Liu \({}^{5}\), Alan Yuille \({}^{1}\) \({}^{1}\)** Johns Hopkins University, \({}^{2}\) HKUST, \({}^{3}\) Tsinghua University,

\({}^{4}\) Shanghai Jiao Tong University, \({}^{5}\) University of Illinois Urbana-Champaign

###### Abstract

High dynamic range (HDR) novel view synthesis (NVS) aims to create photorealistic images from novel viewpoints using HDR imaging techniques. The rendered HDR images capture a wider range of brightness levels containing more details of the scene than normal low dynamic range (LDR) images. Existing HDR NVS methods are mainly based on NeRF. They suffer from long training time and slow inference speed. In this paper, we propose a new framework, High Dynamic Range Gaussian Splatting (HDR-GS), which can efficiently render novel HDR views and reconstruct LDR images with a user input exposure time. Specifically, we design a Dual Dynamic Range (DDR) Gaussian point cloud model that uses spherical harmonics to fit HDR color and employs an MLP-based tone-mapper to render LDR color. The HDR and LDR colors are then fed into two Parallel Differentiable Rasterization (PDR) processes to reconstruct HDR and LDR views. To establish the data foundation for the research of 3D Gaussian splatting-based methods in HDR NVS, we recalibrate the camera parameters and compute the initial positions for Gaussian point clouds. Comprehensive experiments show that HDR-GS surpasses the state-of-the-art NeRF-based method by 3.84 and 1.91 dB on LDR and HDR NVS while enjoying 1000\(\) inference speed and only costing 6.3% training time. Code and data are released at [https://github.com/caiyuanhao1998/HDR-GS](https://github.com/caiyuanhao1998/HDR-GS)

## 1 Introduction

Compared to normal low dynamic range (LDR) images, high dynamic range (HDR) images capture a broader range of luminance levels to retain the details in dark and bright regions, allowing for more accurate representation of real-world scenes. Novel view synthesis (NVS) aims to produce photo-realistic images of a scene at unobserved viewpoints, given a set of posed images of the same scene. NVS has been widely applied in autonomous driving , image editing , digital human , _etc._ NVS is a challenging topic in computer vision because the limited capacity of camera sensor usually leads to a low dynamic range (from 0 to 255) of luminance in rendered images. This results in the loss of image details in very bright or dark areas, color distortions, and a limited capacity to display subtle gradations in light and shadow that the human eye can normally perceive. Hence, there is a growing demand to render HDR (from 0 to +\(\)) views for better image quality and visualization performance.

Figure 1: HDR-GS _vs._ HDR-NeRF. Our HDR-GS achieves better PSNR in dB, SSIM, and LPIPS performance with shorter training time in minutes and faster inference speed in fps.

Existing HDR NVS methods are mainly based on neural radiance fields (NeRF) . However, the ray tracing scheme in NeRF is very time-consuming because it needs to sample many 3D points and then compute their densities and colors for every single ray, severely slowing down the training and inference processes. For instance, the state-of-the-art NeRF-based method HDR-NeRF  takes 9 hours to train and 8.2 s to infer an image at the spatial size of 400\(\)400. This limitation impedes the application of NeRF-based algorithms in rendering real-time dynamic scenes.

Recently, 3D Gaussian Splitting (3DGS)  has achieved impressive inference speed than NeRF-based methods while yielding comparable results on LDR NVS, which inspires another technical route for HDR NVS. However, directly applying the original 3DGS to HDR imaging may encounter three issues. **Firstly**, the dynamic range of the rendered image is still limited to , which severely degrades the visual quality. **Secondly**, training 3DGS with images under different exposures may lead to a non-convergence problem because the spherical harmonics (SH) of 3D Gaussians can not adaptively model the change of exposures. This results in artifacts, blur, and color distortion in the rendered images, as shown in the upper part of Fig. 2. **Thirdly**, 3DGS cannot adapt the exposure level of the synthesized views, which limits its applications, especially in AR/VR, film, and gaming where specific moods and atmospheres are usually evoked by controlling the lighting condition.

To cope with these problems, we propose a novel 3DGS-based method, namely High Dynamic Range Gaussian Splatting (HDR-GS), for 3D HDR imaging. More advanced than the original 3DGS, our HDR-GS can not only render HDR views but also reconstruct LDR images with a controllable exposure time, as depicted in the lower part of Fig. 2. Specifically, we present a Dual Dynamic Range (DDR) Gaussian point cloud model that can jointly model the HDR and LDR colors. We achieve this by using the SH of 3D point clouds to model the HDR color. Then three independent MLPs are employed to model the classical nonparametric camera response function (CRF) calibration process  in RGB channels, respectively. By this means, the HDR color of the 3D point is tone-mapped to the corresponding LDR color with the user input exposure time. Subsequently, the HDR and LDR colors are fed into two Parallel Differentiable Rasterization (PDR) processes to render the HDR and LDR images. In addition, we also notice that existing datasets only provide the camera poses in the normalized device coordinate (NDC) system, which is not suitable for 3DGS-based methods. To establish the data foundation for the research of 3DGS-based methods in HDR imaging, we recalibrate the camera parameters and compute SfM  points to initialize 3D Gaussians. With the proposed techniques, HDR-GS outperforms state-of-the-art (SOTA) NeRF-based methods by 1.91 dB on the HDR novel view synthesis task while enjoying 1000\(\) inference speed and only requiring 6.3% training time, as shown in Fig. 1. In a nutshell, our contributions can be summarized as:

Figure 2: Comparisons of point clouds (left) and rendered views (right) between the original 3DGS  (top) and our HDR-GS (bottom). (i) 3DGS  renders blurry LDR views when training with images under different exposures. Its point clouds suffer from severe color distortion and can not accurately represent the scene. In addition, 3DGS cannot control the exposure of the rendered images. (ii) Our HDR-GS can not only reconstruct clear HDR images with 3D consistency but also render LDR views with controllable exposure time \( t\).

**(i)** We propose a novel framework, HDR-GS, for 3D HDR imaging. To the best of our knowledge, this is the first attempt to explore the potential of Gaussian splatting in 3D HDR reconstruction.

**(ii)** We present a Dual Dynamic Range Gaussian point cloud model with two Parallel Differentiable Rasterization processes that can render HDR images and LDR views with controllable exposure time.

**(iii)** We establish a data foundation by recalibrating camera parameters and computing initial points for 3DGS-based methods on the multi-view HDR datasets . Experiments show that our HDR-GS dramatically outperforms SOTA methods while enjoying much faster training and inference speed.

## 2 Related Work

**High Dynamic Range Imaging.** Conventional HDR imaging  techniques recover HDR images by directly fusing a series of LDR images under different exposure levels at a fixed pose  or calibrating the camera response function (CRF) from the LDR images [16; 20]. These traditional methods yield compelling results in static scenes but produce unpleasant ghost artifacts in dynamic scenes. To address this issue, later works [21; 22; 23; 24; 25] adopt an optical estimator to detect motion regions in the LDR images and then remove or align these regions in further fusion. With the development of deep learning, convolutional neural networks (CNNs) [26; 27; 28; 29] and Transformers [30; 31; 32; 33] have been used to learn an implicit mapping function from an LDR image to its HDR counterpart. Yet, these 2D HDR imaging methods lack 3D perception capabilities and are unable to render novel HDR views.

**Neural Radiance Field.** NeRF  learns an implicit mapping function from the position of a 3D point and view direction to the point color and volume density. NeRF achieves impressive performance on the NVS task, inspiring many follow-up works to improve its reconstruction quality [34; 35; 36; 37; 38] and inference speed [39; 40; 41; 42; 43; 44; 45] or expand its application area [14; 46; 47; 48; 49]. For example, Huang _et al._ present HDR-NeRF  that employs an MLP following the vanilla NeRF to learn an implicit mapping from physical radiance to digital color. Although good results are achieved, these NeRF-based methods suffer from slow training and inference speed due to their time-consuming ray-tracing scheme.

**Gaussian Splatting.** 3DGS  explicitly represents a scene by millions of Gaussian point clouds. Its parallelized rasterization in view rendering allows it to enjoy much faster inference speed than NeRF-based methods that suffer from the time-consuming ray-tracing scheme. Thus, 3DGS has been rapidly and widely applied in many areas such as dynamic scene rendering [50; 51; 52], SLAM [53; 54; 55; 56], inverse rendering [57; 58; 59], digital human [60; 61; 62], 3D generation [63; 64; 65], medical imaging , _etc._ However, the illuminance modeled by 3DGS is still limited to a low dynamic range. The potential of 3DGS in HDR imaging still remains under-explored. This work aims to fill this research gap.

## 3 Method

Figure 3 depicts the overall framework of our HDR-GS. To begin with, we use the structure-from-motion (SfM)  algorithm to recalibrate the camera parameters of the scene and initialize the Gaussian point clouds, as shown in Fig. 3 (a). Then we propose a Dual Dynamic Range (DDR) Gaussian point cloud model to jointly fit the HDR and LDR colors, as illustrated in Fig. 3 (b). The 3D Gaussians directly use the spherical harmonics (SH) to model the HDR color. Then three independent MLPs are employed to learn the tone-mapping operation in RGB channels. This tone-mapper renders the LDR color from the corresponding HDR color and a controllable exposure time \( t\). Subsequently, the LDR and HDR colors are fed into two Parallel Differentiable Rasterization (PDR) processes to render the HDR and LDR images, as depicted in Fig. 3 (c). In this section, we first introduce the DDR point cloud model, then PDR processes, and finally the initialization and optimization of HDR-GS.

### Dual Dynamic Range Gaussian Point Cloud Model

A scene can be represented by a set of our Dual Dynamic Range (DDR) Gaussian point clouds \(\) as

\[=\{G_{i}(_{i},_{i},_{i},_{i}^{l}, {c}_{i}^{h}, t,) i=1,2,,N_{p}\}, \]

where \(N_{p}\) is the number of 3D Gaussians and \(G_{i}\) represents the \(i\)-th Gaussian. Its center position, covariance, opacity, LDR color, and HDR color are denoted as \(_{i}^{3}\), \(_{i}^{3 3}\), \(_{i}\)\(_{i}^{l}^{3}\), and \(_{i}^{h}^{3}\). Besides these attributes, each \(G_{i}\) also contains an exposure time \( t\) that controls the light intensity of the LDR view and three global-shared MLPs with parameters \(\).

\(_{i}\) is represented by a rotation matrix \(_{i}^{3}\) and a scaling matrix \(_{i}^{3}\) as

\[_{i}=_{i}_{i}_{i}^{}_{i }^{}. \]

\(_{i}\), \(_{i}\), \(_{i}\), \(_{i}\), and \(\) are learnable parameters. The tone-mapping operation \(f_{TM}()\) models the camera response function (CRF) that non-linearly maps the HDR color \(_{i}^{h}\) into the LDR color \(_{i}^{l}\) as

\[_{i}^{l}=f_{TM}(_{i}^{h} t), \]

where the exposure time \( t\) can be read from the metadata of photos. We propose to employ MLPs to learn the tone-mapping process. There are two options. The first option is to directly model \(f_{TM}()\), which may result in the vanishing gradient problem because the multiplication operations may cause numerical overflow or underflow. Besides, the multiplication also leads to the nonlinearity and discontinuity of the input signal of MLPs, which also exacerbates the training instability. The second option is following the traditional non-parametric CRF calibration method of Debevec and Malik (2016) that transforms \(f_{TM}()\) from linear domain to logarithmic domain to enhance the stability of MLP training. We adopt the second option. Specifically, \(f_{TM}()\) in Eq. (3) is inversed and transformed as

\[f_{TM}^{-1}(_{i}^{l})=_{i}^{h}+ t, \]

where \(()\) refers to the natural logarithm function and its base is \(e=2.71828\). Subsequently, we take the inverse function of Eq. (4) on both sides and reformulate it as

\[_{i}^{l}=(f_{TM}^{-1})^{-1}(_{i}^{h}+ t). \]

Then we use three MLPs \(\) to model the function \((f_{TM}^{-1})^{-1}\) in RGB channels independently because the RGB colors are tone-mapped by different CRFs. For simplicity, we define the mapping function of our tone-mapper \(\) as \(g_{}(x)(f_{TM}^{-1})^{-1}(x)\). Then Eq. (5) is reformulated as

\[_{i}^{l}=g_{}(_{i}^{h}+ t), \]

here \(_{i}^{h}\) is modeled by the spherical harmonics (SH) with a set of coefficients \(=\{k_{l}^{m}|0 l L,-l m l\}^{(L+1)^{2 } 3}\). Each \(k_{l}^{m}^{3}\) is a set of three coefficients corresponding to the RGB components. \(L\) is the degree of SH. Then \(_{i}^{h}\) at the view direction \(=(,)\) is derived by

\[_{i}^{h}(,)=(_{l=0}^{L}_{m=-l}^{l }k_{l}^{m}\ Y_{l}^{m}(,)), \]

Figure 3: Pipeline of our method. (a) SfM (2017) algorithm is used to recalibrate camera parameters and initialize 3D Gaussians. (b) Dual Dynamic Range Gaussian point clouds use spherical harmonics to model the HDR color. Three MLPs are employed to tone-map the LDR color from the HDR color and user input exposure time. (c) The HDR and LDR colors are fed into two Parallel Differentiable Rasterization to render the HDR and LDR views.

where \(Y_{l}^{m}:^{2}\) is the SH function that maps 3D points on the sphere to real numbers and \(()\) represents the the exponential function. By substituting Eq. (7) into Eq. (6), we obtain \(_{i}^{l}\) as

\[_{i}^{l}(,, t)=g_{}(_{l=0}^{L}_{m =-l}^{l}k_{l}^{m}\:Y_{l}^{m}(,)+\: t+b), \]

here we add a constant bias \(b\) that helps adjust the SH function to better fit the data. The detailed architecture of our MLP-based tone-mapper \(\) is shown in Fig. 3 (b). \(g_{}()\) equals to the process that the RGB channels of log \(_{i}^{h}\) respectively undergo an independent MLP containing a fully connected (\(fc\)) layer, a ReLU activation, an \(fc\) layer, and a sigmoid activation to produce the LDR color \(_{i}^{l}\).

### Parallel Differentiable Rasterization

The computed HDR color \(_{i}^{h}\) in Eq. (7) and LDR color \(_{i}^{l}\) in Eq. (8) of Gaussian point clouds are fed into two parallel differentiable rasterization processes to jointly render the HDR and LDR views, as shown in Fig. 3 (c). The HDR rasterization \(F_{}\) and LDR rasterization \(F_{}\) are represented as

\[^{h}&=F_{}(_{int},_{ext},\{_{i},_{i},_{i},_{i}^ {h}\}_{i=1}^{N_{p}}),\\ ^{l}( t)&=F_{}(_{ int},_{ext},\{_{i},_{i},_{i},_{i}^{l}(  t)\}_{i=1}^{N_{p}}), \]

where \(^{h}\) and \(^{l}( t)^{H W 3}\) denote the rendered HDR image and LDR image with the exposure time \( t\), \(H\) and \(W\) refers to the height and width of the images, \(_{ext}^{4 4}\) represents the extrinsic matrix, and \(_{int}^{3 4}\) refers to the intrinsic matrix. Please note that we omit \(\) and \(\) in \(_{i}^{h}\) and \(_{i}^{l}\) for simplicity. Then we introduce the details of the parallel rasterization processes. First of all, we derive the possibility value of the \(i\)-th 3D Gaussian distribution at the point position \(^{3}\) as

\[P(|_{i},_{i})=-( -_{i})^{}_{i}^{-1}(-_{i}). \]

Subsequently, the splatting operation projects the 3D Gaussians to the 2D imaging plane. In this projection process, the center position \(_{i}\) is firstly transferred from the world coordinate system to the camera coordinate system and then projected to the image coordinate system as

\[}_{i}=[_{i},1]^{}=_{ext}\: }_{i}=_{ext}\:[_{i},1]^{}, }_{i}=[_{i},1]^{}=_{int}\: }_{i}=_{int}\:[_{i},1]^{}, \]

where \(_{i}^{2}\) and \(_{i}^{3}\) refer to the image coordinate and camera coordinate of \(_{i}\). \(}_{i}\), \(}_{i}\), and \(}_{i}\) are the homogeneous versions of \(_{i}\), \(_{i}\), and \(_{i}\), respectively. The 3D covariance \(_{i}\) is also transferred from the world coordinate system to \(_{i}^{{}^{}}^{3 3}\) in the camera coordinate system as

\[_{i}^{{}^{}}=_{i}_{i}_{i}_{i}^{}_{i}^{}, \]

where \(_{i}^{3 3}\) represents the Jacobian matrix of the affine approximation of the projective transformation \(_{int}_{ext}\). \(_{i}^{3 3}\) is the viewing transformation obtained by taking the first three rows and columns of \(_{ext}\). Similar to previous works [15; 66; 67; 68], the 2D covariance matrix \(_{i}^{{}^{}}^{2 2}\) is derived by directly skipping the third row and column of \(_{i}^{{}^{}}\). Subsequently, the 2D projection is divided into non-overlapping tiles. The 3D Gaussians (\(_{i}\),\(_{i}\)) are assigned to the tiles where their 2D projections (\(_{i}\),\(_{i}^{{}^{}}\)) cover. For each tile, the assigned 3D Gaussians are sorted according to the view space depth. Then the RGB value \(^{h}(p)\) and \(^{l}(p| t)^{3}\) at pixel \(p\) is obtained by blending \(\) ordered points overlapping pixel \(p\) in the corresponding tile as

\[^{h}(p)=_{j}_{j}^{h}\:_{j}_{k=1}^{ j-1}(1-_{k}),^{l}(p| t)=_{j}_{j}^ {l}( t)\:_{j}_{k=1}^{j-1}(1-_{k}), \]

where \(_{j}=_{j}P(_{j}|_{j},_{j})\) and \(_{j}\) is the \(j\)-th intersection 3D point between the ray, which starts from the optical center of the camera and lands at pixel \(p\), and the Gaussian point clouds in 3D space. \(_{j}^{h}\) and \(_{j}^{l}( t)\) are the HDR color and LDR color with exposure time \( t\) of \(_{j}\), respectively.

### Initialization and Optimization

An obstacle to the research of 3DGS-based methods in 3D HDR imaging is that the original multi-view HDR datasets  only provide the camera poses in the normalized device coordinate (NDC) system. This NDC system is not suitable for 3DGS-based methods for two main reasons. **Firstly**, NDC focuses on describing the positions on the 2D screen after perspective projection. However, 3D Gaussian is an explicit 3D representation. 3DGS requires transforming and projecting Gaussian point clouds in 3D space. **Secondly**, NDC rescales the coordinates into the range [-1, 1] or . The voxel resolution is limited, making it challenging to capture fine details in the scene. **Besides**, the original datasets  do not provide SfM  point clouds for the initialization of 3DGS.

To address these issues and establish a data foundation for the research of 3DGS-based algorithms in 3D HDR imaging, we use the SfM algorithm  to recalibrate the camera parameters and compute the initial positions for 3D Gaussians. The mapping function of SfM \(F_{}\) is summarized as

\[_{int},\ \{_{ext}^{j}\}_{j=1}^{N_{v}},\ N_{p},\ \{ _{i}\}_{i=1}^{N_{p}}\ =\ F_{}(\{}_{j}^{i}(t_{s})\}_{j=1}^{N_{v}}), \]

where \(N_{v}\) represents the number of viewpoints and \(}_{j}^{i}(t_{s})^{H W 3}\) refers to the LDR image at the \(j\)-th viewpoint with the exposure time \(t_{s}\) in the multi-view HDR datasets . The intrinsic matrix \(_{int}\) does not change with the viewpoint. Please note that we take the HDR views under the same exposure time as the inputs of SfM algorithms because SfM is based on multi-view feature detection and matching. Changes in exposure conditions may degrade the accuracy of SfM. Then we use the computed \(N_{p}\) and \(\{_{i}\}_{i=1}^{N_{p}}\) in Eq. (14) to initialize \(\) in Eq. (1). Other learnable parameters of \(\) are randomly initialized. The recalibrated camera pose-image data pairs \(\{_{ext}^{j},}_{j}^{i}( t)\}_{j=1}^{N_{v}}\) in Eq. (14) are used to train our HDR-GS with the weighted sum of \(_{1}\) loss and D-SSIM loss as

\[_{p}=_{j=1}^{B}_{1}(_{j}^{i}( t_{j}),}_{j}^{i}( t_{j}))+_{ }(_{j}^{l}( t_{j}),}_{j}^{l}( t_{j})), \]

where \(B\) is the training batch size and \(\) is a hyperparameter. Similar to HDR-NeRF  that uses the ground truth CRF correction coefficient \(C_{0}\) to restrict the HDR color on the synthetic scenes, we also enforce a constraint to the rendered HDR image in the \(\)-law  LDR domain as

\[_{c}=_{j=1}^{B}(1+(_{j}^{h}))}{(1+)}-(1+(}_{j}^{h}))}{(1+)}_{2}^{2}\,, \]

where \(\) is the amount of compression. \(()\) is the min-max normalization. \(_{j}^{h}\) and \(}_{j}^{h}^{H W 3}\) denote the rendered and ground-truth HDR image at the \(j\)-th viewpoint. The overall training loss is

\[=_{p}+_{c}, \]

where \(\) is a hyperparameter that controls the relative importance between \(_{p}\) and \(_{c}\). We do not use \(_{c}\) in the real scenes since the ground truth HDR images are not provided in the real datasets. Therefore, we set \(=0.6\) and 0 in the experiments on the synthetic and real datasets, respectively.

## 4 Experiments

### Experimental Settings

**Dataset.** We adopt the multi-view image datasets collected by , including 4 real scenes captured by a camera and 8 synthetic scenes created by the software Blender . Images with 5 different exposure time \(\{t_{1},t_{2},t_{3},t_{4},t_{5}\}\) are captured at 35 different viewpoints. Following HDR-NeRF , images at 18 views with exposure time randomly selected from \(\{t_{1},t_{3},t_{5}\}\) are used for training while other 17 views at exposure time \(\{t_{1},t_{3},t_{5}\}\) and \(\{t_{2},t_{4}\}\) and HDR images are used for testing.

**Implementation Details.** We implement HDR-GS by PyTorch . The models are trained with the Adam optimizer  (\(_{1}\) = 0.9, \(_{2}\) = 0.999, and \(\) = 1\(\)10\({}^{-15}\)) for 3\(\)10\({}^{4}\) iterations. The learning rate for point cloud position is initially set to 1.6\(\)10\({}^{-4}\) and exponentially decays to 1.6\(\)10\({}^{-6}\). The learning rates for point feature, opacity, scaling, and rotation are set to 2.5\(\)10\({}^{-3}\), 5\(\)10\({}^{-2}\), 5\(\)10\({}^{-3}\), and 1\(\)10\({}^{-3}\). The learning rate for the tone mapper network is initially set as 5\(\)10\({}^{-4}\) and exponentially decays to 5\(\)10\({}^{-5}\). All experiments are conducted on a single RTX A5000 GPU.

**Evaluation Metrics.** We adopt the peak signal-to-noise ratio, PSNR (higher is better), and structural similarity index measure, SSIM  (higher is better), to quantitatively evaluate the objective performance. Learned perceptual image patch similarity, LPIPS  (lower is better), is adopted as the perceptual metric. Similar to , we also quantitatively evaluate the rendered HDR images in the tone-mapped domain and qualitatively show HDR results tone-mapped by Photomatix pro . In addition, frames per second, fps (higher is faster), is used to measure the model inference speed.

### Quantitative Results

**Comparisons on the Synthetic Datasets.** The quantitative results of LDR and HDR NVS on the synthetic datasets are reported in Table 1. We compare our HDR-GS with three NeRF-based methods (NeRF , NeRF-W , and HDR-NeRF ) and the original 3DGS . Table 1 lists the training time, inference speed, PSNR, SSIM, and LPIPS results on LDR-OE, LDR-NE, and HDR. LDR-OE represents the LDR NVS results with exposure time \(t_{1},t_{3},\) and \(t_{5}\). LDR-NE denotes the LDR NVS results with exposure time \(t_{2}\) and \(t_{4}\). HDR refers to the HDR NVS results. Please note that only HDR-NeRF and HDR-GS can output both LDR and HDR views. Other methods can only render LDR images. Our method outperforms SOTA methods on all tracks except for the PSNR on LDR-NE. **(i)** When compared to the recent best method HDR-NeRF, our HDR-GS outperforms it by 2.03 and 1.91 dB on LDR-OE and HDR tracks while enjoying 1000\(\) faster inference speed and only costing 6.3% training time. **(ii)** When compared to the original 3DGS, our HDR-GS is 21.64 and 17.36 dB higher on LDR-OE and LDR-NE, respectively. Interestingly, HDR-GS is slightly faster

   & Training & Inference & ,t_{3},t_{3}\))} & ,t_{4}\))} &  \\  & Time (min) & Speed (fps) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  NeRF  & 405 & 0.190 & 13.97 & 0.555 & 0.376 & 14.51 & 0.522 & 0.428 & — & — & — \\
3DGS  & 38 & 121 & 19.46 & 0.690 & 0.276 & 18.97 & 0.778 & 0.309 & — & — & — \\ NeRF-W  & 437 & 0.178 & 29.83 & 0.936 & 0.047 & 29.22 & 0.927 & 0.050 & — & — & — \\ HDR-NeRF  & 542 & 0.122 & 39.07 & 0.973 & 0.026 & **37.53** & 0.966 & 0.024 & 36.40 & 0.936 & 0.018 \\  HDR-GS (Ours) & **34** & **126** & **41.10** & **0.982** & **0.011** & 36.33 & **0.977** & **0.016** & **38.31** & **0.972** & **0.013** \\  

Table 1: Quantitative results on the synthetic datasets. Metrics are averaged over all scenes. LDR-OE denotes the LDR results with exposure time \(t_{1}\), \(t_{3}\), and \(t_{5}\). LDR-NE denotes the LDR results with exposure time \(t_{2}\) and \(t_{4}\). HDR denotes the HDR results. HDR-GS yields the best results on all tracks.

Figure 4: LDR visual comparisons on the synthetic scenes. Previous methods introduce unpleasant black spots or render blurry images. Our method controls the exposure better while reconstructing more detailed structures.

than 3DGS. This is because 3DGS cannot adapt to different exposure levels. It is fragile and hard to converge when training with LDR images under different lighting intensities. The color change of the scene misleads the adaptive density control in 3DGS to split more Gaussian point clouds with distorted color to represent the complex variances in exposure levels, prolonging the training process.

To intuitively show the superiority of our method, Fig. 1 plots a radar chart that features concentric polygons representing the HDR NVS performance across 5 metrics of the SOTA method HDR-NeRF and our HDR-GS. It can be observed that our HDR-GS forms a much larger outermost polygon fully enclosing that of HDR-NeRF, indicating superior performance across all evaluated aspects. These results strongly demonstrate the advantages of our method in effectiveness and model efficiency.

**Comparisons on the Real Datasets.** Table 2 shows the quantitative comparisons on the real datasets. Please note that the real datasets do not provide HDR ground truth for quantitative evaluation. Hence, we only report the LDR NVS results in Table 2. When compared to the recent best method HDR-NeRF, our HDR-GS is 3.84 and 0.23 dB higher in PSNR on LDR-OE and LDR-NE. When compared to the original 3DGS, HDR-GS significantly surpasses it by 18.28 and 12.16 dB on LDR-OE and LDR-NE. These results suggest the outstanding generalization ability and effectiveness of our method.

### Qualitative Results

**LDR Novel View Rendering.** The comparisons of LDR novel view rendering with different exposure times on the synthetic (dog and sofa) and real (box and luckycat) scenes are shown in Fig. 4 and 5. It can be observed that NeRF, 3DGS, and NeRF-W fail to control the exposure. They either introduce black stripes and spots, or over-smooth the image, or over-enhance the image while distorting the color. HDR-NeRF can adapt the light intensity but it also produces blurry images. In contrast, our

   & ,t_{3},t_{5}\))} & ,t_{4}\))} \\  & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  NeRF  & 14.95 & 0.661 & 0.308 & 14.44 & 0.731 & 0.255 \\
3DGS  & 17.19 & 0.806 & 0.103 & 19.50 & 0.727 & 0.152 \\ NeRF-W  & 28.55 & 0.927 & 0.094 & 28.64 & 0.923 & 0.089 \\ HDR-NeRF  & 31.63 & 0.948 & 0.069 & 31.43 & 0.943 & 0.069 \\  HDR-GS (Ours) & **35.47** & **0.970** & **0.022** & **31.66** & **0.965** & **0.030** \\  

Table 2: Quantitative results on the real datasets. Metrics are averaged across all scenes. LDR-OE represents the LDR results with exposure time \(t_{1}\), \(t_{3}\), and \(t_{5}\). LDR-NE denotes the LDR results with exposure time \(t_{2}\) and \(t_{4}\). HDR refers to the HDR results. HDR-GS yields the best results on all tracks.

Figure 5: LDR visual comparisons on the real scenes. Previous methods introduce unpleasant black spots or render blurry images. Our method controls the exposure better while reconstructing more detailed structures.

HDR-GS can not only control the exposure level of LDR views according to the user input time but also reconstruct clearer images with high-frequency textures and structural contents.

**HDR Novel View Rendering.** The comparisons of HDR novel view synthesis on the synthetic (upper) and real (lower) datasets are depicted in Fig. 6. Please note that only HDR-NeRF and our HDR-GS can reconstruct HDR images. As can be seen, HDR-NeRF yields low-contrast and over-smooth images while sacrificing fine-grained details and introducing undesirable chromatic artifacts and black spots. On the contrary, our HDR-GS can render more perceptually pleasing HDR images with sharper textures and preserve the color and spatial smoothness of homogeneous regions.

### Ablation Study

In this section, we adopt the synthetic datasets to conduct ablation study. Table 3 lists the PSNR results averaged across all scenes on the LDR-OE, LDR-NE, and HDR tracks, respectively.

**Break-down Ablation.** We adopt 3DGS  trained with the original coordinates (NDC) as the baseline to conduct a break-down ablation on the synthetic datasets. Our goal is to study the effect of each component towards higher performance. The results are reported in Table 2(a). **(i)** The baseline model can only render LDR views. It achieves 12.35 and 11.83 dB on LDR-OE and LDR-NE. **(ii)** When using the recalibrated camera poses, the model yields an improvement of 2.27 and 2.58 dB on LDR-OE and LDR-NE because it is liberated from the constraint of the NDC system. **(iii)** When we apply the SfM points for the initialization of 3D Gaussians, the model gains by 4.84 dB and 4.56 dB

Table 3: Ablations on the synthetic datasets. The PSNR results on HDR, LDR-OE, and LDR-NE are reported.

Figure 6: HDR visual comparisons on the synthetic (upper) and real (lower) scenes. Our method can recover the details in both dark and bright regions while suppressing color distortion. Please zoom in for a better view.

because the SfM points provide a general shape of Gaussian point clouds to alleviate the overfitting issues of 3DGS. However, the model still cannot render HDR views nor change the exposure level of the LDR views until now, leading to limited LDR NVS performance. **(iv)** Then we apply our DDR point clouds, the model is enabled to render HDR views with 38.31 dB in PSNR performance. Besides, the model yields 21.64 and 17.36 dB improvements on LDR-OE and LDR-NE because our DDR point clouds allow the model to adapt the lighting intensity with controllable exposure time.

**CRF Domain.** We conduct experiments to compare the effects of modeling CRF in linear domain and logarithmic domain. As shown in Table 3, when the MLPs \(\) directly models \(f_{TM}()\) in Eq. (3), our method yields poor results of only 26.18, 29.53, and 27.44 dB on HDR, LDR-OE, and LDR-NE. In contrast, when the MLPs \(\) models \(g_{}()\) in Eq. (6), the performance is 12.13, 11.57, and 8.89 dB higher on HDR, LDR-OE, and LDR-NE. This is because the multiplication in \(f_{TM}()\) is transferred into the addition in \(g_{}()\), which enhances the training stability by suppressing the numerical nonlinearity and discontinuity problems. This evidence verifies our analysis in Sec. 3.1.

**Exposure Time Used for Training.** We conduct experiments in Table 3c to study the effect of the number of exposure times used in training. **(i)** According to the research of Debevec and Malik , modeling CRF requires at least two exposures. Thus, when we only use a single exposure \(\{t_{3}\}\), HDR-GS fails to reconstruct HDR views and LDR images with novel exposure time. **(ii)** When two exposures \(\{t_{1},t_{5}\}\) are used for training, HDR-GS gains by 9.20, 11.62, and 10.53 dB on HDR, LDR-OE, and LDR-NE. **(iii)** The performance of using three exposures \(\{t_{1},t_{3},t_{5}\}\) is close to that of using five exposures \(\{t_{1},t_{2},t_{3},t_{4},t_{5}\}\). Hence, it is a reasonable choice to use three exposure times.

**Recalibration of Camera Parameters.** In Eq. (14), we use the SfM algorithm to recalibrate the camera parameters and compute the initial positions of 3D Gaussians at the same exposure time \(t_{s}\). Here, we conduct experiments to study the effect of \(t_{s}\) in Table 3d. The performance achieves its maximum value when \(t_{s}=t_{4}=8\) seconds. Therefore, the optimal choice of \(t_{s}\) is \(t_{4}=8\) s.

## 5 Limitation and Broader Impact

The main limitation of this work is that the memory usage of 3DGS-based methods is non-trivial and maybe unaffordable to some low-RAM mobile devices. HDR imaging is a very important topic in computational photography. Nowadays, billions of LDR images are captured by mobile phones and cameras. Therefore, how to enhance the quality of these images, adapt the exposure level, and render HDR views is worth studying. Our HDR-GS is capable of reconstructing better HDR and LDR views with controllable exposure time at 1000\(\) speed than SOTA methods, showing great value in practical applications. Until now, 3D HDR imaging techniques have no negative social impact yet. Our proposed HDR-GS does not present any negative foreseeable societal consequences, either.

## 6 Conclusion

This paper focuses on studying the efficiency problem of 3D HDR imaging. We propose the first Gaussian Splatting-based framework, HDR-GS, for HDR novel view synthesis. Our HDR-GS is based on the Dual Dynamic Range Gaussian point clouds that can jointly model HDR color and LDR color with user input exposure time. Then, the HDR and LDR colors are fed into two Parallel Differentiable Rasterization processes to render the HDR and LDR views. To avoid the limitations of NDC system and establish a data foundation for the research of 3DGS-based methods, we recalibrate the camera parameters and compute the initial positions for Gaussian point clouds. Experiments show that our HDR-GS outperforms the SOTA NeRF-based method by 1.91 and 3.84 dB on HDR and LDR novel view rendering, while enjoying 1000\(\) inference speed and requiring only 6.3% training time.