# A2PO: Towards Effective Offline Reinforcement Learning from an Advantage-aware Perspective

Yunpeng Qing\({}^{1}\), Shunyu Liu\({}^{2}\), Jingyuan Cong\({}^{1}\), Kaixuan Chen\({}^{2,3}\), Yihe Zhou\({}^{1}\), Mingli Song\({}^{2,3}\)

\({}^{1}\) College of Computer Science and Technology, Zhejiang University

\({}^{2}\) State Key Laboratory of Blockchain and Data Security, Zhejiang University

\({}^{3}\) Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security

{qingyunpeng, liushunyu, kcj51, chenkx, zhouyihe, brooksong}@zju.edu.cn

Corresponding author

###### Abstract

Offline reinforcement learning endeavors to leverage offline datasets to craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the out-of-distribution problem. However, existing works often suffer from the constraint conflict issue when offline datasets are collected from multiple behavior policies, _i.e.,_ different behavior policies may exhibit inconsistent actions with distinct returns across the state space. To remedy this issue, recent advantage-weighted methods prioritize samples with high advantage values for agent training while inevitably ignoring the diversity of behavior policy. In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for offline learning under mixed-quality datasets. Specifically, A2PO employs a conditional variational auto-encoder to disentangle the action distributions of intertwined behavior policies by modeling the advantage values of all training data as conditional variables. Then the agent can follow such disentangled action distribution constraints to optimize the advantage-aware policy towards high advantage values. Extensive experiments conducted on both the single-quality and mixed-quality datasets of the D4RL benchmark demonstrate that A2PO yields results superior to the counterparts. Our code is available at [https://github.com/Plankson/A2PO](https://github.com/Plankson/A2PO).

## 1 Introduction

Offline Reinforcement Learning (RL)  aims to learn effective control policies from pre-collected datasets without online exploration, and has witnessed its unprecedented success in various real-world applications, including robot control , power grid control , _etc._ A formidable challenge of offline RL lies in the Out-Of-Distribution (OOD) problem , involving the distribution shift between data induced by the learned policy and data collected by the behavior policy. Consequently, the direct application of conventional online RL methods inevitably exhibits extrapolation error , where the unseen state-action pairs are erroneously estimated. To tackle this OOD problem, offline RL methods attempt to impose proper conservatism on the learning agent within the distribution of the dataset, such as restricting the learned policy with a regularization term  or penalizing the value overestimation of OOD actions .

Despite the promising results achieved, offline RL often encounters the constraint conflict issue when dealing with the mixed-quality dataset . Specifically, when training data are collected from multiple behavior policies with distinct returns, existing works still treat each sample constraintequally with no regard for the differences in data quality and diversity. This oversight results in improper constrain on conflict actions , ultimately leading to further suboptimal outcomes. To resolve this concern, the Advantage-Weighted (AW) methods  employ weighted sampling to prioritize training transitions with high advantage values from the offline dataset. However, we argue that these AW methods implicitly reduce the diverse behavior policies associated with the offline dataset into a narrow one from the viewpoint of the dataset redistribution. As a result, this redistribution operation of AW may exclude a significant number of critical transitions during training, imposing erroneous constraints on agent learning. To exemplify the above issue of AW, we conduct a didactic experiment on the recent advanced AW method, LAPO , as shown in Figure 1. The toy one-step jump task requires the agent to jump over obstacles and reach two designated goal positions with different rewards. The offline dataset mainly contains failed attempts, with only a few successful transitions, making it very challenging for the agent to learn an effective policy. The results in Figure 0(b) demonstrate that LAPO performs poorly in this task. Furthermore, Figure 0(c) reveals that the AW redistribution does not effectively prioritize either optimal or suboptimal actions in modeling the behavior policy. Instead, the AW redistribution can lead to an incorrect on bad actions, which results in unreliable policy optimization.

In this paper, we propose _Advantage-Aware Policy Optimization_, abbreviated as A2PO, to explicitly learn the advantage-aware policy with disentangled behavior policies from the mixed-quality offline dataset. Unlike previous AW methods devoted to dataset redistribution while reducing the data diversity, the proposed A2PO directly conditions the agent policy on the advantage values of all training data without any prior preference. Technically, A2PO comprises two alternating stages, _behavior policy disentangling_ and _agent policy optimization_. The former stage introduces a Conditional Variational Auto-Encoder (CVAE)  to disentangle different behavior policies into separate action distributions by modeling the advantage values of collected state-action pairs as conditioned variables. The latter stage further imposes an explicit advantage-aware policy constraint on the training agent within the support of disentangled action distributions. The advantage-conditioned CVAE can models the behavior policy distribution (Figure 0(c)), which is further utilized to construct advantage-aware constraint for agent optimization toward high advantage values, resulting in an effective decision-making policy (Figure 0(b)).

To sum up, our main contribution is the first dedicated attempt towards advantage-aware policy optimization to alleviate the constraint conflict issue under the mixed-quality offline dataset. The proposed A2PO can achieve advantage-aware policy constraint derived from different behavior policies, where a customized CVAE is employed to infer diverse action distributions associated with the behavior policies by modeling advantage values as conditional variables. Extensive experiments conducted on the D4RL benchmark , including both single-quality and mixed-quality datasets, demonstrate that the proposed A2PO method yields significantly superior performance to other advanced offline RL baselines, as well as the advantage-weighted competitors.

Figure 1: A didactic experiment. (a) The visualization of the toy one-step jump task and the composition of the mixed-quality dataset. The agent starts at position \(0\) and can make a one-step jump \(a[-10,10]\) to reach a new position and receive a reward \(r\). (b) Learning curves of A2PO and LAPO. (c) VAE-generated action distributions of A2PO and LAPO at the initial state. LAPO VAE conditions only on the state, while A2PO VAE conditions on both the state and the advantage \(\).

Related Works

**Offline RL** can be broadly classified into four categories: policy constraint [11; 44], value regularization [13; 14], model-based method [55; 52], and return-conditioned supervised learning [7; 23]. Policy constraint methods impose constraints on the learned policy to be close to the behavior policy . Previous studies directly introduce the explicit constraint on policy learning, such as behavior cloning , maximum mean discrepancy , or maximum likelihood estimation . In contrast, recent efforts [29; 46] mainly focus on realizing the policy constraints implicitly by approximating the formal optimal policy derived from KL-divergence constraint. On the other hand, value regularization methods make constraints on the value function to alleviate the overestimation of OOD action. Researchers try to approximate the lower bound of the value function with the Q-regularization term for conservative action selection [20; 27]. Model-based methods construct the environment dynamics to estimate state-action uncertainty for OOD penalty [17; 6]. Several works also converts offline RL into a return-conditioned supervised learning task. Decision Transformer (DT)  builds a transformer policy conditioned on both the current state and the additional sum return signal with supervised learning. Yamagata et al.  improve the stitching ability of DT policy on sub-optimal samples by relabeling the return signal with Q-learning results. However, in the context of offline RL with a mixed-quality dataset and no access to the trajectory return signals, all these methods treat each sample equally without considering data quality, thereby resulting in improper regularization and further suboptimal learning outcomes.

**Advantage-weighted Offline RL Method** employs weighted sampling to prioritize training transitions with high advantage values from the offline dataset. To enhance sample efficiency, Peng et al.  introduce an advantage-weighted maximum likelihood loss by directly calculating advantage values via trajectory return.  further use the critic network to estimate advantage values for advantage-weighted policy training. This technique has been incorporated as a subroutine in other works [18; 50] for agent policy extraction. Recently, AW methods have also been well studied in addressing the constraint conflict issue that arises from the mixed-quality dataset [5; 58; 40]. Several studies present advantage-weighted behavior cloning as a direct objective function  or an explicit policy constraint .  propose the Latent Advantage-Weighted Policy Optimization (LAPO) framework, which employs an advantage-weighted loss to train CVAE for generating high-advantage actions based on the state condition. Besides AW methods, Hong et al.  enhance the classical offline RL training objective with the weight of subsequent return. On the other hand, Hong et al.  directly learning the optimal policy density as the weight function to enable sampling from high-performing policies. However, this AW mechanism inevitably diminishes the data diversity in the dataset. In contrast, our A2PO directly conditions the agent policy on both the state and the estimated advantage value, enabling effective utilization of all samples with varying quality.

## 3 Preliminaries

We formalize the RL task as a Markov Decision Process (MDP)  defined by a tuple \(=,,P,r,,_{0}\), where \(\) represents the state space, \(\) represents the action space, \(P:\) denotes the environment dynamics, \(r:\) denotes the reward function, \((0,1]\) is the discount factor, and \(_{0}\) is the initial state distribution. At each time step \(t\), the agent observes the state \(s_{t}\) and selects an action \(a_{t}\) according to its policy \(\). This action leads to a transition to the next state \(s_{t+1}\) based on the dynamics distribution \(P\). Additionally, the agent receives a reward signal \(r_{t}\). The goal of RL is to learn an optimal policy \(^{*}\) that maximizes the expected return: \(^{*}=_{}_{}[_{k=0}^{}^{k}r_{ t+k}]\). In offline RL, the agent can only learn from an offline dataset without online interaction with the environment. In the single-quality settings, the offline dataset \(=\{(s_{t},a_{t},r_{t},s_{t+1}) t=1,,N\}\) with \(N\) transitions is collected by only one behavior policy \(_{}\). In the mixed-quality settings, the offline dataset \(=_{i}\{(s_{i,t},a_{i,t},r_{i,t},s_{i,t+1}) t=1,,N\}\) is collected by multiple behavior policies \(\{_{_{i}}\}_{i=1}^{M}\).

We evaluate the learned policy \(\) by the action value function \(Q^{}(s,a)=_{}[_{t=0}^{}^{t}r(s_{t},a_{t})  s_{0}=s,a_{0}=a]\). The state value function is defined as \(V^{}(s)=_{a}[Q^{}(s,a)]\), while the advantage function is defined as \(A^{}(s,a)=Q^{}(s,a)-V^{}(s)\). For continuous control, our A2PO implementation uses the TD3 algorithm  based on the actor-critic framework as a basic backbone for its robust performance. The actor network \(_{}\), known as the learned policy, is parameterized by \(\), while the critic networks consist of the Q-networkparameterized by \(\) and the V-network \(V_{}\) parameterized by \(\). The actor-critic framework involves two steps: policy evaluation and policy improvement. During policy evaluation phase, the Q-network \(Q_{}\) is optimized by the temporal-difference (TD) loss :

\[_{Q}()=_{(s,a,r,s^{}),a^{ }_{}(s^{})}Q_{}(s,a)-r(s,a)+ Q_{ }(s^{},a^{})^{2}, \]

where \(\) and \(\) are the parameters of the target networks that are regularly updated by online parameters \(\) and \(\) to maintain learning stability. The V-network \(V_{}\) can also be optimized by the similar TD loss. For policy improvement in continuous control, the actor network \(_{}\) can be optimized by the deterministic policy gradient loss [39; 38]:

\[_{}()=_{s}[-Q_{}(s, _{}(s))]. \]

Note that offline RL will impose conservative constraints on the optimization losses to tackle the OOD problem. Moreover, the performance of the final learned policy \(_{}\) highly depends on the quality of the offline dataset \(\) associated with the behavior policies \(\{_{_{i}}\}\).

## 4 Methodology

In this section, we provide details of our proposed A2PO approach, consisting of two key components: _behavior policy disentangling_ and _agent policy optimization_. In the _behavior policy disentangling_ phase, we disentangle behavior policies with a CVAE specifically modeling the action distribution conditioned on the advantage values of collected state-action pairs. By taking different advantage inputs, the newly formed CVAE allows the agent to infer distinct action distributions that are associated with various behavior policies. Then in the _agent policy optimization_ phase, the action distributions derived from the advantage condition serve as disentangled behavior policies, establishing an advantage-aware policy constraint to guide agent training. An overview of our A2PO is illustrated in Figure 2.

### Behavior Policy Disentangling

To realize behavior policy disentangling, we adopt a CVAE to **relate the behavior distribution of different specific behavior policies \(_{_{i}}\) to the advantage condition variables**, which is quite different from previous methods [11; 5; 56] utilizing CVAE only for **approximating the overall mixed-quality behavior policy set \(\{_{_{i}}\}_{i=1}^{M}\) conditioned only on specific state \(s\)**. Concretely, we have made adjustments to the architecture of the CVAE to be advantage-aware. The encoder \(q_{}(z|a,c)\) is fed with condition \(c\) and action \(a\) to project them into a latent representation \(z\). Given specific condition \(c\) and the encoder output \(z\), the decoder \(p_{}(a|z,c)\) captures the correlation between condition \(c\) and latent representation \(z\) to reconstruct the original action \(a\). Unlike previous methods [11; 5; 48] predicting action solely based on the state \(s\), we consider both state \(s\) and advantage value \(\) for CVAE condition. The state-advantage condition \(c\) is formulated as:

\[c=s\,||\,. \]

Figure 2: An illustrative diagram of the Advantage-Aware Policy Optimization (A2PO) method.

Therefore, given the current state \(s\) and the advantage value \(\) as a joint condition, the CVAE model is able to generate corresponding action \(a\) with varying quality positively correlated with the advantage value \(\). For a state-action pair \((s,a)\), the advantage value \(\) can be computed as follows:

\[=_{i=1,2}Q_{_{i}}(s,a)-V_{}(s), \]

where two Q-networks with the \(()\) operation are adopted to ensure conservatism in offline RL settings . Moreover, we employ the \(()\) function to normalize the advantage condition within the range of \((-1,1)\). This operation prevents excessive outliers from impacting the performance of CVAE, improving the controllability of generation. The optimization of Q-networks and V-network will be described in the following section.

The CVAE model is trained using the state-advantage condition \(c\) and the corresponding action \(a\). The training objective involves maximizing the Empirical Lower Bound (ELBO)  on the log-likelihood of the sampled minibatch:

\[_{}(,)=-_{} _{q_{}(z|a,c)}[(p_{}(a|z,c))]+ [q_{}(z|a,c) p(z)], \]

where \(\) is the coefficient for trading off the KL-divergence loss term, and \(p(z)\) denotes the prior distribution of \(z\) setting to be \((0,1)\). The first log-likelihood term encourages the generated action to match the real action as much as possible, while the second KL divergence term aligns the latent variable distribution with the prior distribution \(p(z)\).

At each round of CVAE training, a minibatch of state-action pairs \((s,a)\) is sampled from the offline dataset. These pairs are fed to the critic network \(Q_{}\) and \(V_{}\) to get corresponding advantage condition \(\) by Eq. (4). Then the advantage-aware CVAE is subsequently optimized by Eq. (5). By incorporating the advantage condition \(\), the CVAE captures the relation between \(\) and the action distribution of the behavior policies, as shown in the upper part of Figure 2. This further enables the CVAE to generate actions \(a\) based on the state-advantage condition \(c\) in a manner where the action quality is positively correlated with \(\). Furthermore, the advantage-aware CVAE is utilized to establish an advantage-aware policy constraint for agent policy optimization in the next stage.

### Agent Policy Optimization

The agent is constructed using the actor-critic framework . The critic comprises two Q-networks \(Q_{_{i=1,2}}\) and one V-network \(V_{}\) to approximate the value of the agent policy. The actor, advantage-aware policy \(_{}(|c)\), with input \(c=s\,||\,\), generates a latent representation \(\) based on the state \(s\) and the designated advantage condition \(\). This latent representation \(\), along with \(c\), is then fed into the decoder \(p_{}\) for an recognizable action \(a_{}\):

\[a_{} p_{}(,c),_{ }( c). \]

With this form, the advantage-aware policy \(_{}\) is expected to produce action with different qualities that are positively correlated to the designated advantage input \(\) which is normalized within \((-1,1)\) in Eq. 4). Therefore, the output optimal action \(a^{*}\) is obtained by \(c^{*}=s\,||\,^{*}\) input with \(^{*}=1\). It should be noted that the critic networks are to approximate the expected value of the optimal policy \(_{}(|c^{*})\). The agent optimization, following the actor-critic framework, encompasses policy evaluation and policy improvement steps. During the policy evaluation step, the critic is updated through the minimization of the temporal difference loss with the optimal policy \(_{}(|c^{*})\). Specifically, for the V-network \(v_{}\), we employ the one-step Bellman operator to approximate the state value under the current agent-aware policy, conditioned on the optimal advantage input \(^{*}=1\), as follows:

\[_{}()=_{(s,a,r,s^{ }),\\ ^{*}_{}(|^{*}),\\ a_{}^{*} p_{}(|^{*},c^{*})}r+ _{i}Q_{_{i}}(s^{},a_{}^{*})-V_{}(s)^{2}, \]

where \(Q_{}\) is the target network updated softly. As for the Q-networks, both of the two Q-network entities \(Q_{_{i}}\) are optimized with agent policy \(_{}(|c^{*})\) following Equation 1.

For the policy improvement, the actor loss is defined as:

\[_{}()=-_{s ,^{*}_{}(|c^{*}),\\ a_{}^{*} p_{}(|^{*},c^{*})}Q_ {_{1}}(s,a_{}^{*})+_{(s,a) ,^{*}_{}(|c),\\ a_{} p_{}(|,c)}(a-a_{})^{2} , \]where \(a_{}^{*}\) in the first term is the optimal action generated with the fixed maximum advantage condition \(^{*}=1\) input and \(a_{}\) in the second term is obtained with the advantage condition \(\) derived from the critic based on Eq. 4 applied to the sampled batch. Meanwhile, following TD3+BC , we add a normalization coefficient \(=/_{(s_{i},a_{i})}|Q(s_{i},a_{i})|\) to the first term to keep the scale balance between Q value objective and regularization, where \(\) is a hyperparameter to control the scale of the normalized Q value. The first term encourages the optimal policy condition on \(c^{*}\) to select actions that yield the highest expected returns represented by the Q-value. This aligns with the policy improvement step in conventional RL approaches . The second behavior cloning term explicitly imposes constraints on the advantage-aware policy, ensuring the policy selects in-sample actions that adhere to the advantage condition \(\) determined by the critic. Therefore, the suboptimal samples with low advantage condition \(\) will not disrupt the optimization of optimal policy \(_{}(|c^{*})\). And they enforce valid constraints on the corresponding policy \(_{}(|c)\), as shown in the lower part of Figure 2. It should be noted that the decoder \(p_{}\) is fixed during both policy evaluation and improvement.

Our A2PO implementation selects TD3+BC  as the base backbone for its robustness. The general framework derived above is thoroughly described in Appendix A.

## 5 Experiments

To illustrate the effectiveness of the proposed A2PO method, we conduct experiments on the D4RL benchmark . We aim to answer the following questions: (1) Can A2PO outperform the advanced offline RL methods in both the single-quality datasets and mixed-quality datasets? (Section 5.2) (2) How do different components of A2PO contribute to the overall performance? (Section 5.3 and Appendix D-G) (3) How does A2PO perform under mixed-quality datasets with varying single-quality samples? (Section 5.5 and Appendix H) (4) Can the A2PO agent effectively estimate the quality of different transitions? (Section 5.4 and Appendix I) (5) How does the time overhead of A2PO compare to other baselines? (Section 5.6)

### Experiment Settings

**Tasks and Datasets.** We consider four different domains of tasks in D4RL benchmark : Gym, Maze, Adroit, and Kitchen. Each domain contains several tasks and corresponding distinct datasets. We conduct experiments for each Gym task using single-quality and mixed-quality datasets. The single-quality datasets are generated with the _medium_ behavior policy. The mixed-quality datasets are the combinations of _random_, _medium_, and _expert_ single-quality datasets, including _medium-expert_, _medium-replay_, _random-medium_, _medium-expert_, and _random-medium-expert_. The D4RL benchmark only includes the first two mixed-quality datasets. Thus, following Hong et al. , we manually construct the last three mixed-quality datasets by combining the corresponding single-quality datasets in D4RL with equal proportions. For the other domains of tasks, the corresponding D4RL datasets exhibit a significant level of diversity to evaluate the effectiveness of our A2PO algorithm.

**Comparison Methods and Hyperparameters.** We compare the proposed A2PO to several advanced offline RL methods: BCQ , TD3+BC , CQL , EQL , especially the advantage-weighted offline RL methods: AWAC , IQL , CQL+AW , LAPO . Besides, we also select the vanilla BC method , the model-based offline RL method MOPO , and the emerging diffusion-based method Diffusion-QL , for comparison. We report the performance of baselines using the best results reported from their own paper. More comparison results can be found in Appendix C. The detailed hyperparameters of A2PO are given in Appendix B.2.

### Comparison on D4RL Benchmarks

**Results for Gym Tasks.** The experimental results of all compared methods in the D4RL Gym tasks are presented in Table 1. For the single-quality _medium_ dataset and mixed-quality _medium-expert_ and _medium-replay_ datasets from D4RL, our A2PO achieves state-of-the-art results with low variance. Meanwhile, both conventional offline RL approaches like EQL and advantage-weighted approaches like LAPO still learn acceptable policy, indicating that the conflict issue hardly occurs in these datasets with low diversity. However, the newly constructed mixed-quality datasets, namely _random-medium_, _random-expert_, and _random-medium-expert_, highlight the issue of substantial gaps between behavior policies. The results on these datasets reveal a significant drop in performance for all other baselines.

Instead, our A2PO continues to achieve the best performance on these datasets. When considering the total scores across all datasets, A2PO outperforms the next best-performing AW method, CQL+AW, by over 21%. The results reveal the exceptional ability of A2PO to capture and utilize high-quality interactions within the dataset in order to enforce a reasonable advantage-aware policy constraint and further obtain an optimal agent policy.

**Results for Maze, Kitchen, and Adroit Tasks.** Table 2 presents the experimental results of all the compared methods on the D4RL Maze, Kitchen, and Adroit tasks. The D4RL datasets for these tasks exhibit varying patterns in behavior policy samples. For instance, the Antmaze datasets are highly sub-optimal, while the Adroit datasets have a narrow state-action distribution. Among the offline RL baselines and AW methods, A2PO delivers remarkable performance in these challenging tasks and showcases the robust representation capabilities of the advantage-aware policy.

### Ablation Analysis

**Different Advantage condition during training.** The performance comparison of different advantage condition computing methods for agent training is given in Figure 3. Eq. 4 obtains continuous advantage condition \(\) in the range of \((-1,1)\). To evaluate the effectiveness of the continuous computing method, we design a discrete form of advantage condition: \(_{}=()_{||>}\), where \(()\) is the symbolic function, and \(_{||>}\) is the indicator function returning \(1\) if the absolute value of \(\) is greater than the hyperparameter of threshold \(\), otherwise \(0\). Thus, the advantage condition \(_{}\) is constrained to discrete value of \(\{-1,0,1\}\). Another special form of advantage condition is \(_{}=1\) for all state-action pairs, in which the advantage-aware ability is lost. Figure 3a shows that setting

  
**Source** & **Task** & **BC** & **BCQ** & **TD3+BC** & **CQL** & **MOOPO** & **EQL** & **Diffusion-QL** & **AWAC** & **IQL** & **CQL+AW** & **LAPO** & **A2PO (Ours)** \\   & halfCheetah & 42.6 & 47.0 & 48.3 & 44.0 & 42.3 & 47.2 & **51.1** & 43.5 & 47.4 & 49.0 & 46.0 & 47.1\(\)0.2 \\  & hopper & 52.9 & 56.7 & 59.3 & 58.5 & 28.0 & 74.6 & **90.5** & 57.0 & 66.3 & 71.0 & 51.6 & 80.3\(\)0.4 \\  & walker2d & 75.3 & 72.6 & 83.7 & 72.5 & 17.8 & 83.2 & **87.0** & 72.4 & 78.3 & 83.0 & 80.8 & 84.9\(\)0.2 \\   & halfCheetah & 36.6 & 40.4 & 44.6 & 45.5 & 53.1 & 44.5 & **47.8** & 40.5 & 44.2 & 47.0 & 41.9 & 44.8\(\)0.2 \\  & hopper & 18.1 & 53.3 & 60.9 & 95.0 & 67.5 & 98.1 & 101.3 & 37.2 & 94.7 & 99.0 & 50.1 & **101.6\(\)**1.3 \\  & walker2d & 26.0 & 52.1 & 81.8 & 81.6 & 39.0 & 76.6 & **95.5** & 27.0 & 73.9 & 87.0 & 60.6 & 82.8\(\)1.7 \\   & halfCheetah & 55.2 & 89.1 & 90.7 & 91.6 & 63.9 & 30.6 & **96.8** & 42.8 & 86.7 & 84.0 & 94.2 & 95.6\(\)0.8 \\  & hopper & 52.5 & 81.8 & 90.8 & 105.4 & 23.7 & 105.5 & 11.1 & 55.8 & 91.5 & 91.0 & 11.0 & 113.4\(\)0.5 \\  & walker2d & 107.5 & 109.0 & 11.0 & 108.4 & 44.6 & 110.2 & 110.1 & 74.5 & 109.6 & 109.0 & 110.9 & **112.1\(\)**0.2 \\   & halfCheetah & 2.3 & 12.7 & 47.7 & 31.9 & **52.7** & 42.3 & 48.4 & 46.5 & 42.2 & 46.5 & 18.5 & 48.5\(\)0.3 \\  & hopper & 23.2 & 9.2 & 7.4 & 3.3 & 19.9 & 1.7 & 6.9 & 19.5 & 6.2 & 22.6 & 4.2 & **62.1\(\)**2.8 \\  & walker2d & 19.2 & 0.2 & 10.7 & 0.2 & 40.2 & 31.4 & 3.3 & 0.0 & 54.6 & 82.0 & 23.6 & **82.3\(\)0.4** \\   & halfCheetah & 13.7 & 2.1 & 43.1 & 15.0 & 18.5 & 47.4 & 86.1 & 87.3 & 28.6 & 80.7 & 52.6 & **90.3\(\)**1.6 \\  & hopper & 10.1 & 8.5 & 78.8 & 7.8 & 17.2 & 68.6 & 102.0 & 84.7 & 58.5 & 109.6 & 82.3 & **112.5\(\)**1.3 \\  & walker2d & 14.7 & 0.6 & 7.0 & 0.3 & 4.6 & 9.1 & 56.3 & 11.7 & 90.9 & 108.6 & 0.4 & **109.1\(\)**1.4 \\   & halfCheetah & 2.3 & 15.9 & 62.3 & 13.5 & 26.7 & 42.8 & 81.2 & 2.3 & 61.6 & 76.8 & 71.1 & **90.6\(\)**1.6 \\  & hopper & 27.4 & 4.0 & 60.5 & 9.4 & 13.3 & 72.4 & 70.1 & 8.6 & 57.9 & 71.8 & 66.6 & **107.8\(\)**0.4 \\   & walker2d & 24.6 & 2.4 & 15.7 & 0.1 & 56.4 & 61.0 & 56.6 & -0.4 & 90.8 & 58.3 & 60.4 & **97.7\(\)**6.7 \\   & 604.2 & 65.76 & 1010.6 & 784.4 & 628.8 & 1107.2 & 1302.1 & 710.9 & 1183.9 & 1376.9 & 1026.8 & **1563.3** \\   & & & & & & & & & & & & & \\   

Table 1: Test returns of our proposed A2PO and baselines on the Gym tasks. \(\) corresponds to one standard deviation of the performance on 5 random seeds. The performance is measured by the normalized scores at the last training iteration. **Bold** indicates the best performance in each task.

  
**Task** & **BC** & **BCQ** & **TD3+BC** & **CQL** & **MOOPO** & **EQL** & **Diffusion-QL** & **AWAC** & **IQL** & **CQL+AW** & **LAPO** & **A2PO (Ours)** \\  maze2d-munze & 0.5 & 24.8 & 24.2 & 5.7 & -15.4 & 56.5 & 66.7 & 94.5 & 56.2 & 19.6 & 78.0 & **133.3\(\)**6 \\ maze2d-medium & 0.7 & 22.5 & 33.5 & 0.9 & 19.6 & 36.3 & 100.6 & 31.4 & 25.7 & 22.6 & 43.2 & **114.9\(\)**12 \\ maze2d-large & 1.1 & 43.0 & 128.5 & 12.5 & -0.5 & 57.0 & 116.3 & 43.9 & 45.7 & 10.3 & 69.7 & **156.4\(\)**5.8 \\ antmaze-unze-diverse & 45.6 & 55.0 & 71.4 & **84.0** & 0.0 & 50.8 & 66.2 & 49.3 & 62.2 & 54.0 & 0.0 & 72.6\(\)1.02 \\ antmaze-medium-diverse & 0.0 & 0.0 & 3.0 & 53.7 & 0.6 & 62.2 & 78.6 & 0.7 & 70.0 & 24.0 & 30.2 & **80.2\(\)**4.0 \\ antmaze-large-diverse & 0.0 & 2.2 & 0.0 & 14.9 & 0.0 & 38.0 & **56.6** & 1.0 & 47.5 & 40.0 & 22.3 & 52.1\(\)1.79 \\ 
**Maze Total** & 47.9 & 147.5 & 260.6 & 175.8 & 3.1 & 300.8 & 485.0 & 220.8 & 307.3 & 170.5 & 243.4 & **69.5** \\  kitchen-complete & 33.8 & 8.1 & 0.8 & 43.8 & 40.1 & 70.3 & **84.0** & 3.8 & 62.5 & 30.2 & 53.2 & 69.2\(\)4.9 \\ kitchen-partial & 33.8 & 18.9 & 0.0 & 49.8 & 6.7 & 74.5 & 60.5 & 0.3 & 46.3 & 36.0 & 53.7 & **75.8\(\)**1.2 \\ kitchen-mixed & 47.5 & 10.6 & 0.8 & 51.0 & 17.3 & 55.6 & 6.6 & 0.0 & 51.0 & 50.5 & 62.4 & **64.2\(\)**1.3 \\ 
**Kitchen Total** & 115.1 & 37.6 & 1.6 & 144.6 & 64.1 & 200.4 & 207.1 & 4.1 & 159.8 & 116.7 & 169\(_{}=1\) without explicitly advantage-aware mechanism leads to a significant performance decreasing, especially in the new mixed-quality dataset. Meanwhile, \(_{}\) with different values of threshold \(\) achieve slightly inferior results than the continuous \(\). This outcome strongly supports the efficiency of continuous \(\). Although the \(_{}\) signals are more stable, \(_{}\) hidden the concrete advantage value, causing a mismatch between the advantage value and the sampled transition.

**Different Advantage Condition for Test.** The performance comparison of different discrete advantage conditions input for the test is given in Figure 4. To ensure clear differentiation, we select \(\) from \(\{-1,0,1\}\). The different designated advantage conditions \(\) are fixed input for the actor, leading to different policies \(_{}(|s,)\). The final outcomes demonstrate the partition of returns corresponding to the policies with different \(\). Furthermore, the magnitude of the gap increases as the offline dataset includes samples from more diverse behavior policies. These observations provide strong evidence for the success of A2PO disentangling the behavior policies under the multi-quality dataset.

Figure 4: Learning curves of A2PO under different fixed advantage inputs during the test while using the original continuous advantage condition for training. Test returns are reported in Appendix E.

Figure 5: Visualization of A2PO latent representation after applying PCA with different advantage conditions and actual returns in the _walker2d-medium-replay_ and _hopper-medium-replay_ tasks. Each data point indicates a latent representation \(\) based on the initial state and different advantage conditions sampled uniformly from \([-1,1]\). The actual return is measured under the corresponding sampled advantage condition. The value magnitude is indicated with varying shades of color.

Figure 3: Test return difference of A2PO with different discrete advantage conditions during training compared with original A2PO with continuous advantage condition during training. Task abbreviations are listed in Appendix B.1. Test returns are reported in Appendix D.

### Visualization

Figure 5 presents the visualization of A2PO latent representation. The uniformly sampled advantage condition \(\) combined with the initial state \(s\), are fed into the actor-network to get the latent representation generated by the final layer of the actor. The result demonstrates that the representations converge according to the advantage and the actual return. Moreover, upon comparing Figure 5(a,b), as well as Figure 5(c,d) separately, we observe that the latent action representation follows the same alteration pattern based on the actual real return. These observations demonstrate that our advantage-aware policy effectively capture policies with different returns by the designated advantage input \(\). This provides compelling evidence for the efficacy of the A2PO policy construction. More experiments of advantage estimation conducted on different tasks and datasets are presented in Appendix I.

### Robustness

Figure 6 presents the experimental results of A2PO across mixed-quality datasets with varying proportions of single-quality samples. Following the methodology of [15; 16], we evaluate the effectiveness of A2PO on three mixed-quality datasets: _medium-expert_, _random-medium_, and _random-expert_. These datasets consist of a total number of \(1 10^{6}\) transitions. We vary the proportions \(\) of higher quality samples and \((1-)\) of lower quality samples. The results demonstrate that our A2PO effectively captures and infers high-quality potential behavior policies for proper policy regularization, even with a small proportion of high-quality samples. Additionally, as the \(\) becomes larger, the variance decreases. Thus, A2PO demonstrates its robustness in handling variations in the proportions of different single-quality samples, guaranteeing consistently high performance.

### Time Overhead

We measure the training times of A2PO as well as other baselines, which are presented in Table 7. The experiments are performed on a cluster of 4 A40 GPUs under _halfcheetah-medium-expert-v2_ scenarios for \(1 10^{6}\) steps. Although the A2PO runtime is longer than the lightweight algorithms like IQL due to CVAE training, our A2PO is more efficient compared to other AW methods such as LAPO and CQL+AW.

## 6 Conclusion

In this paper, we propose a novel approach, termed as A2PO, to tackle the constraint conflict issue on mixed-quality offline datasets with advantage-aware policy constraints. Specifically, A2PO utilizes a CVAE to effectively disentangle the action distributions associated with various behavior policies. This is achieved by modeling the advantage values of all training data as conditional variables. Consequently, advantage-aware agent policy optimization can be focused on maximizing high advantage values while conforming to the disentangled distribution constraint imposed by the mixed-quality dataset. Experimental results show that A2PO successfully decouples the underlying behavior policies and significantly outperforms advanced offline RL competitors. For our future work, we will extend A2PO to multi-task offline RL scenarios characterized by a greater diversity of behavior policies and a more prominent constraint conflict issue.

Figure 6: Compare the returns of A2PO under _random-expert_ dataset with different high-quality data proportions \(\) in the Gym tasks. Detail returns are reported in Appendix H.

Figure 7: Compare the time overhead of A2PO and other baselines.

Limitations.The limitation of A2PO is that it incorporates CVAE during training, which may lead to quite a large time overhead. However, the results presented in Section 5.6 show that the time overhead of A2PO remains reasonably acceptable when compared to other baseline methods.

## 7 Acknowledgement

This work was supported in part by the Hangzhou Joint Funds of the Zhejiang Provincial Natural Science Foundation of China under Grant No. LHZSD24F020001, in part by the Fundamental Research Funds for the Central Universities under Grant No. 226-2024-00058, and in part by the Zhejiang Province High-Level Talents Special Support Program "Leading Talent of Technological Innovation of Ten-Thousands Talents Program" under Grant No. 2022R52046.