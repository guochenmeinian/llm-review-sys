# Conditional Score Guidance for Text-Driven Image-to-Image Translation

Hyunsoo Lee\({}^{1}\)1 &Minsoo Kang\({}^{1}\)1 &Bohyung Han\({}^{1,2}\)

\({}^{1}\)ECE & \({}^{2}\)IPAI, Seoul National University

{philip21, kminsoo, bhhan}@snu.ac.kr

###### Abstract

We present a novel algorithm for text-driven image-to-image translation based on a pretrained text-to-image diffusion model. Our method aims to generate a target image by selectively editing regions of interest in a source image, defined by a modifying text, while preserving the remaining parts. In contrast to existing techniques that solely rely on a target prompt, we introduce a new score function that additionally considers both the source image and the source text prompt, tailored to address specific translation tasks. To this end, we derive the conditional score function in a principled way, decomposing it into the standard score and a guiding term for target image generation. For the gradient computation about the guiding term, we assume a Gaussian distribution for the posterior distribution and estimate its mean and variance to adjust the gradient without additional training. In addition, to improve the quality of the conditional score guidance, we incorporate a simple yet effective mixup technique, which combines two cross-attention maps derived from the source and target latents. This strategy is effective for promoting a desirable fusion of the invariant parts in the source image and the edited regions aligned with the target prompt, leading to high-fidelity target image generation. Through comprehensive experiments, we demonstrate that our approach achieves outstanding image-to-image translation performance on various tasks. Code is available at [https://github.com/Hleephilip/CSG](https://github.com/Hleephilip/CSG).

## 1 Introduction

Diffusion models  have recently shown remarkable performance in various tasks such as unconditional generation of text  or images  and conditional generation of images , 3D scenes , motion , videos , or audio  given text and/or images. Thanks to large-scale labeled datasets of text-image pairs , text-to-image diffusion models  have been successfully trained and have achieved outstanding performance. Despite the success of the text-to-image generation models, it is not straightforward to extend the models to text-driven image-to-image translation tasks due to the limited controllability on the generated images. For example, in the case of the "cat-to-dog" task, naive text-to-image diffusion models often fail to focus on the area of cats in a source image for updates and simply generate a dog image with a completely different appearance.

To sidestep such a critical problem, existing image-to-image translation methods  based on diffusion models aim to update the semantic content in the source image specified by a given condition while preserving the region irrelevant to the condition for target image generation. For example,  fine-tune the pretrained text-to-image diffusion model to reduce the distance between the background in the source image and the generated target image, while bringing the translated image closer to the target domain. On the other hand,  revise thegenerative processes based on the pretrained diffusion models for the text-driven image editing tasks without extra training.

The main idea of this work is to estimate a score function conditioned on both the source image and source text in addition to the standard condition with the target text. The new score function is composed of two terms: (a) the standard score function conditioned only on the target prompt and (b) the guiding term, _i.e._, the gradient of the log-posterior of the source latent given the target latent and the target prompt with respect to the target latent. Note that the posterior is modeled by a Gaussian distribution whose mean and covariance matrix are estimated without an additional training process. We also employ an effective mixup strategy in cross-attention layers to achieve high-fidelity image-to-image translation. The main contributions of our paper are summarized as follows:

* We mathematically derive a conditional score that provides guidance for controllable image generation in image-to-image translation tasks; the score can be conveniently incorporated into existing text-to-image diffusion models.
* We propose a novel cross-attention mixup technique based on text-to-image diffusion models for image-to-image translation tasks, which adaptively combines the two outputs of cross-attention layers estimated with the latent source and target images at each time step.
* We introduce an intuitive performance evaluation metric that measures the fidelity of pairwise relationships between images before and after translation. Experimental results verify that our method consistently achieves outstanding performance on various tasks with respect to the standard and the proposed metrics.

The rest of the paper is organized as follows. Section 2 reviews the related work and Section 3 discusses basic concepts and practices of image-to-image translation based on diffusion models. The details of our approach are described in Section 4, and the experimental results are presented in Section 5. Finally, we conclude this paper in Section 6.

## 2 Related Work

This section describes existing text-to-image diffusion models and their applications to image-to-image translation tasks.

### Text-to-Image Diffusion Models

Diffusion models [1; 2; 3; 4] have achieved exceptionally promising results in text-to-image generation. For example, Stable Diffusion and Latent Diffusion Models (LDM)  employ a two-stage framework [36; 37], where an input image is first projected onto a low-dimensional feature space using a pretrained encoder and a generative model is trained to synthesize the feature conditioned on a text embedding. At inference time, the image is sampled by generating the representation and then decoding it. In the two-stage framework, Denoising Diffusion Probabilistic Model (DDPM)  is employed as the generative model while a text prompt is represented using a text encoder given by Contrastive Language-Image Pretraining (CLIP) . On the other hand, DALL-E 2  consists of two main components: a prior model and a decoder. The prior model infers a CLIP image embedding from a text representation encoded by the CLIP text encoder while the decoder synthesizes an image conditioned on both its CLIP embedding and text information. In addition, Imagen  generates an image conditioned on a text embedding provided by a powerful language encoder , which is trained on text-only corpora to faithfully represent a text prompt.

### Text-Driven Image-to-Image Translation Methods using Diffusion Models

Existing image-to-image translation methods [30; 31; 32; 33; 34; 35; 31; 35] aim to preserve the background while editing the object part only. Specifically, Stochastic Differential Editing (SDEdit)  infers a latent variable at an intermediate time step based on a source image and then synthesizes a target image by solving the reverse time stochastic differential equation from the intermediate time step to the origin. DiffusionCLIP  fine-tunes a text-to-image generative network using the local directional loss  computed from CLIP  while this method also employs the \(L_{1}\) reconstruction loss for preserving the background and optionally the face identity loss  only for human face image manipulation tasks. Also, Imagic  optimizes a pretrained text-to-image diffusion model conditioned on the inferred text feature to faithfully reconstruct a given image. Using the fine-tuned model, at inference time, the target image is synthesized based on the linear combination of the two features of the predicted source and the target text. In contrast, Blended Diffusion  requires a user-provided background mask so that the latents of the source and target images are mixed according to the mask to preserve the background while selectively updating the region of interest.

## 3 Preliminary: Image-to-Image Translation based on Diffusion Models

### Estimation of Latent Variables for Source Images

Diffusion models [1; 2; 4] estimate a series of latent variables \(_{1},_{2},_{T}\) with the same dimensionality as a data sample \(_{0}^{H W C}\), where the forward sampling process of \(_{t}\) depends on \(_{t-1}\) and \(_{0}\) in DDIM  while DDPM  assumes the data generating process follows a Markov chain. For image-to-image translation, existing works [33; 11; 35] often employ a deterministic inference using DDIM instead of a stochastic method such as DDPM, which is given by

\[_{t+1}^{}=}f_{}(_{t}^{ },t,^{})+}_{} (_{t}^{},t,^{}), \]

where \(_{t}^{}\) and \(^{}\) denote the latent variable of a source image \(_{0}^{}\) and the text embedding of a source prompt using pretrained models [38; 39], and \(_{t}(0,1]\) is an element in a predefined decreasing sequence. In the above equation, \(_{}(,,)\) is a noise prediction network parametrized with a U-Net backbone  and \(f_{}(,,)\) is defined as

\[f_{}(_{t},t,):=_{t}-}_{}(_{t},t,)}{}}. \]

The last latent variable \(_{T}^{}\) is sampled recursively using Eq. (1), from which the reverse process starts for the generation of the target image or the reconstruction of the source image. Note that, for simplicity, we write equations for each channel of \(_{t}\) because the operations in all channels are identical.

### Generative Process of Target Images

A naive image-to-image translation technique based on diffusion models simply generates the target image \(_{0}^{}\) from \(_{T}^{}\), which is set to \(_{T}^{}\), recursively using the reverse DDIM process as follows:

\[_{t-1}^{}=}f_{}(_{t}^{ },t,^{})+}_{} (_{t}^{},t,^{}), \]

where \(^{}\) denotes the text embedding of the target prompt. Although the naive DDIM-based translation guarantees the cycle-consistency as discussed in , the simple translation algorithm often results in poor generation results, failing to maintain the content structure and the background information that should be invariant to translation, as presented in Figure 1.

To address this issue, previous approaches employ the information obtained from the reverse process of \(_{t}^{}\) when synthesizing the target image. Specifically, to preserve the background in the source image during the early steps in the reverse process of \(_{t}^{}\), Prompt-to-Prompt  replaces the cross-attention maps of the target latents with those obtained from the source latents while Plug-and-Play  injects the self-attention and intermediate feature maps into the matching layers in

Figure 1: Image translation results of \(_{0}^{}\) based on \(_{0}^{}\) using the simple naïve DDIM generative process based on Eq (3).

the noise prediction network. For the remaining steps, the reverse DDIM process defined as Eq. (3) is simply employed to generate the target image without any modification. On the other hand, Pix2Pix-Zero  first updates the latent \(_{t}^{}\) to minimize the distance between the cross-attention maps given by the reverse process of \(_{t}^{}\) and \(_{t}^{}\), and then performs the DDIM generation process using the updated latent.

## 4 Conditional Score Guidance with Cross-Attention Mixup

This section discusses how to derive our conditional score guidance using cross-attention mixup for high-fidelity text-driven image-to-image translation.

### Overview

The naive reverse process of DDIM for image-to-image translation can be rewritten by plugging Eq. (2) into Eq. (3) and adopting the approximate score function suggested in  as follows:

\[_{t-1}^{} =}}{}}_{t}^{ }-}_{t}_{}(_{t}^{ },t,^{}) \] \[}}{}}_{t} ^{}+(1-_{t})_{t}_{_{t}^{}} p (_{t}^{}|^{}), \]

where

\[_{_{t}^{}} p(_{t}^{}|^{})-}}_{}( _{t}^{},t,^{})\;\;\;\; _{t}:=}{_{t}}}-}{1- _{t}}}. \]

Our approach designs a new reverse process guided by the proposed conditional score, for which we replace the original score function, \(_{_{t}^{}} p(_{t}^{}|^{})\), in Eq. (5), with the one conditioned on the source information, \(_{_{t}^{}} p(_{t}^{}|_{0}^{},^{},^{})\). The rest of this section discusses how to derive the new conditional score function, where we incorporate a novel technique, cross-attention mixup, to estimate a mask with the foreground/background probability, and further revise the conditional score function using the mask. We refer our algorithm to Conditional Score Guidance (CSG). Figure 2 depicts the graphical model of the proposed text-driven image-to-image translation process, and Algorithm 1 shows the detailed procedure of CSG.

### Conditional Score Estimation

For the conditional reverse process of image-to-image translation, we propose the novel conditional score, \(_{_{t}^{}} p(_{t}^{}|_{0}^{},^{},^{})\), which is given by

\[_{_{t}^{}} p(_{t}^{}|_{0}^{},^{},^{}) =_{_{t}^{}} p(_{In Eq. (7), \(_{t}^{}\) is independent of \(^{}\) and \(_{t}^{}\) as illustrated in Figure 2, and \(_{t}^{}\) is orthogonal to \(^{}\). In Eq. (8), \(}_{t}^{}\) is a sample drawn from \(p(_{t}^{}|_{0}^{},^{})\) using the forward deterministic inference in Eq. (1). Then, we decompose the right-hand side of Eq. (8) as

\[_{_{t}^{}} p(_{t}^{}|}_{t}^{},^{})=_{_{t}^{ }} p(_{t}^{}|^{})+_ {_{t}^{}} p(}_{t}^{}|_{t}^{},^{}), \]

where the first term \(_{_{t}^{}} p(_{t}^{}|^{})\) is estimated by the noise prediction network as the standard reverse process. Finally, the conditional score in our approach is given by

\[_{_{t}^{}} p(_{t}^{}|_{0}^{},^{},^{}) _{_{t}^{}} p(_{t}^{}|^{})+_{_{t}^{}} p(}_{t}^{ }|_{t}^{},^{}). \]

### Cross-Attention Mixup

To improve the derived score function, we estimate a mask \(^{}(0,1)^{H W}\) indicating where to preserve and edit in the source image, and use the mask for the computation of the score function. To this end, motivated by the observed property  that the spatial layout of a generated image depends on the cross-attention map, we first compute the average cross-attention maps \(^{}^{L H W}\) of the time-dependent cross-attention maps \(\{^{}_{t}\}_{t=1:T}\) in the noise prediction network using \(_{t}^{}\)'s as its inputs. Then, we select a content mask \(^{}[k]^{H W}\), which contains the semantic information of the to-be-edited region in the source image, where \(k\) denotes the position of the source prompt token to be updated in the target prompt. However, the application of the content mask \(^{}[k]\) is limited to highlighting only small parts within the content of interest. For example, in the case of the "cat-to-dog" task, the head region in \(^{}[k]\) exhibits high activations while the body area yields relatively low responses as illustrated in Figure 2(b).

To alleviate such a drawback, our approach applies a regularization technique to the content mask \(^{}[k]\) for spatial smoothness as depicted in Figure 2(c). For the regularization, we compute the average self-attention map, \(^{}^{H W H W}\) over time-dependent self-attention maps in the noise prediction network during the reverse DDIM process. The motivation behind this strategy is from the observation in  that the averaged self-attention map tends to hold semantically well-aligned attention information.

Using the content mask and the average self-attention map, we compute the regularized content mask, \(}^{}[k]^{H W}\), as follows:

\[}^{}[k][h,w]:=(^{}[h,w](^{}[k])^{T}), \]

where \(()\) is the trace operator and \([h,w]\) denotes the pixel position. Then, the background mask \(^{}^{H W}\) for image-to-image translation is given by

\[^{}:=-}^{}[k], \]

where \(^{H W}\) denotes the matrix whose elements are 1. Each element in \(^{}\) indicates the probability that the corresponding pixel in the image belongs to the region to be preserved even after translation.

Given the background mask \(^{}\), we propose a new cross-attention guidance method called _cross-attention mixup_ to estimate the cross-attention map for an arbitrary \(^{}\) text token as

\[}_{t}^{}[]:=_{t}^{}[] ^{}+_{t}^{}[](-^{}), \]

Figure 3: Visualization of the source image, the content mask \(^{}[k]\), and the corresponding regularized mask \(}^{}[k]\), where the \(k^{}\) token in the source prompt contains the semantic information about the region to be edited in the source image.

where \(\) is the Hadamard product operator and \(_{t}^{}\) denotes the cross-attention map in the noise prediction network given an input \(_{t}^{}\). Note that the cross-attention mixup is helpful to preserve the background conditioned on the text prompts while allowing us to edit the regions of interest. By integrating the cross-attention mixup in Eq. (13), we obtain the modified prediction \(_{}(_{t}^{},t,^{})\) using \(}_{t}^{}\) instead of \(_{t}^{}\) at every time step \(t\).

### Conditional Score Guidance

We still need to formulate the guidance term, \(_{_{t}^{}} p(}_{t}^{}| _{t}^{},^{})\). To this end, we assume a Gaussian distribution for \(p(}_{t}^{}|_{t}^{},^{ })\) with a diagonal precision matrix \(_{t}^{HW HW}\), _i.e._, \(p(}_{t}^{}|_{t}^{},^{ })(_{t}^{},(1-_{t})_{t}^{-1})\). The guidance term is then given by

\[_{_{t}^{}} p(}_{t}^{}| _{t}^{},^{})=-_{ t}(_{t}^{}-}_{t}^{})}{1-_{t}}, \]

where each diagonal element in \(_{t}\) has a large precision value if it corresponds to the pixel that should be preserved and consequently has a low variance, and vice versa.

Figure 4: Visualization of the source image and estimated precision matrix \(_{t}\) ranging from time step \(t=T\) to \(t=1\).

Given the background mask \(^{}\) and a hyperparameter \(^{}\) to control the magnitude of each element in \(_{t}\), the diagonal entries of \(_{t}\) are given by

\[(_{t}):=^{}(_{t}^{}), \]

where \(()\) refers to the vectorized diagonal elements of the input matrix while \(()\) is an operator to reshape a matrix into a vector. In Eq. (15), \(_{t}^{H W}\) is a binary matrix dependent on \(t\) and \(^{}\), which is heuristically defined as

\[_{t}[h,w]:=[^{}[h,w] 1-()], \]

where \([]\) is the indicator function. Note that \(\) is a hyperparameter that controls the period of the cosine function and is simply set to \(1.5\) in our implementation, which makes the cosine function monotonically decreasing. The binary mask \(_{t}\) allows us to ignore the error between the true mean of the posterior \(p(}^{}_{t}|^{}_{t},^{ })\) and its estimate \(^{}_{t}\) in foreground regions. As visualized in Figure 4, \(_{t}\) encourages the editable foreground area to be larger as the time step \(t\) gets closer to \(0\).

### Update Equation

The new reverse process based on the proposed conditional score in Eq. (10) is given by

\[^{}_{t-1}}}{}}^{}_{t}+(1-_{t})_{t}(_{^{}_{t}} p(^{}_{t}|^{} )+_{^{}_{t}} p(}^{}_{t} |^{}_{t},^{})), \]

where the additional guidance term comes from the replacement of the standard score function in Eq. (5). Finally, the equation for our reverse process is derived by using Eq. (14) and converting to a similar form as Eq. (3) as

\[^{}_{t-1}=}_{}(^{ }_{t},t,^{})+}_{ }(^{}_{t},t,^{})-_{t} _{t}(^{}_{t}-}^{}_ {t}), \]

where the noise prediction \(_{}(^{}_{t},t,^{})\) is replaced by the enhanced noise estimation \(_{}(^{}_{t},t,^{})\) with the proposed cross-attention mixup. Therefore, compared to the original score function, the proposed conditional guidance adjusts \(^{}_{t}\) towards \(}^{}_{t}\) depending on the elements in the precision matrix \(_{t}\).

## 5 Experiments

This section compares the proposed method, referred to as CSG, with the state-of-the-art methods such as Prompt-to-Prompt  and Pix2Pix-Zero , along with the simple variant of DDIM described in Section 3.2 on top of the pretrained Stable Diffusion model . We also present ablation study results to analyze the performance of the proposed components.

### Implementation Details

Our method is implemented based on the publicly available official code of Pix2Pix-Zero  in PyTorch  and tested on a single NVIDIA A100 GPU. For faster generation, we adopt \(50\) forward steps using Eq. (1) and \(50\) reverse steps for target image generation. We obtain a source prompt by employing the pretrained vision-language model  while a target prompt is made by replacing words in the source prompt with the target-specific ones. For example, in the case of the "dog-to-cat" task, if the source prompt is "a cute little white dog", the target prompt is set to "a cute little white cat". For translating target images, all methods employ the classifier-free guidance . For fair comparisons, we run the official codes of Prompt-to-Prompt 1 and Pix2Pix-Zero 2, where we utilize the same final latent and target prompt when synthesizing target images.

### Evaluation Metrics

To compare the proposed method with previous state-of-the-art techniques, we employ two evaluation metrics widely used in existing studies: CLIP-similarity (CS)  and Structure Distance (SD) . CS assesses the alignment between the target image and the target prompt while SD estimates the overall structural disparity between the source and target images.

Furthermore, we introduce a novel metric referred to as Relational Distance (RD), which quantifies how faithfully the relational information between source images is preserved between translated target images. With an ideal image-to-image translation algorithm, the distance between two source images should have a strong correlation with the distance between their corresponding target images. Note that this metric has proven successful in the context of knowledge distillation . To compute the distance between two images, we adopt the perceptual loss . In Section A.1, we provide a detailed description of RD. For quantitative evaluation, we select \(250\) images with the highest CLIP similarity from the LAION 5B dataset .

### Quantitative Results

Table 1 presents the quantitative results in comparison with the state-of-the-art methods [33; 35] and the naive DDIM  inference using Eq. (3). As presented in the table, CSG is always the best in terms of SD and RD while it is outperformed by other methods in terms of CS. Although existing approaches have higher values of CS, our algorithm is more effective for preserving the structure. In addition, CSG is more efficient than Pix2Pix-Zero in terms of speed and memory usage since we do not need to perform the backpropagation through the noise prediction network.

### Qualitative Results

Figure 5 visualizes images generated by reconstruction, the naive DDIM method using Eq. (3), Prompt-to-Prompt , Pix2Pix-Zero , and CSG. As illustrated in the figure, CSG successfully edits the content effectively and preserves the background properly while all other methods often fail to preserve the structural information of the primary objects. In addition to the real examples, we present additional qualitative results using synthesized images by Stable Diffusion in Figure 6, which also demonstrates that CSG achieves outstanding performance.

    &  &  &  \\   & CS (\(\)) & SD (\(\)) & RD (\(\)) & CS (\(\)) & SD (\(\)) & RD (\(\)) & CS (\(\)) & SD (\(\)) & RD (\(\)) \\  cat \(\) dog & 0.2921 & 0.0725 & 0.4325 & 0.2992 & 0.0338 & 0.1756 & **0.3015** & **0.0226** & **0.1589** &

### Ablation Study

Table 2 presents the results from the ablation study that analyzes the effect of the proposed components on various tasks when implemented on top of the Stable Diffusion model. The results imply that the conditional score guidance without the cross-attention mixup in Eq. (13), denoted by 'CSG w/o Mixup', is still helpful, and the cross-attention mixup further enhances text-driven image editing performance when combined with the conditional score guidance.

## 6 Conclusion

We presented a training-free text-driven image translation method using a pretrained text-to-image diffusion model. Different from existing methods relying on a simple score conditioned only on

Figure 5: Qualitative comparisons between CSG and other methods tested with the real images sampled from LAION 5B dataset . CSG produces translated images with higher-fidelity.

a target textual input, we formulated a conditional score conditioned on both a source image and a source text in addition to the target prompt. To compute the additional guiding term in our novel conditional score function, we assumed a Gaussian distribution for the posterior distribution, where its mean is simply set to the target latent while the covariance matrix is estimated based on the background mask. For a more accurate estimation of the conditional score function, we incorporated a new cross-attention mixup. Experimental results show that the proposed method achieves outstanding performance in various text-driven image-to-image translation scenarios.

Figure 6: Qualitative results of the proposed method on the synthesized images by the pretrained Stable Diffusion .

[MISSING_PAGE_FAIL:11]

*  Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C.W., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al.: 'LAION-5B: An open large-scale dataset for training next generation image-text models. In NeurIPS Datasets and Benchmarks Track. (2022)
*  Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning. In ACL. (2018)
*  Changpinyo, S., Sharma, P., Ding, N., Soricut, R.: Conceptual 12m: Pushing Web-Scale Image-Text Pre-training To Recognize Long-Tail Visual Concepts. In CVPR. (2021)
*  Thomee, B., Shamma, D.A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth, D., Li, L.J.: YFCC100M: The New Data in Multimedia Research. Communications of the ACM (2016)
*  Byeon, M., Park, B., Kim, H., Lee, S., Baek, W., Kim, S.: Coyo-700m: Image-text pair dataset. [https://github.com/kakaobrain/coyo-dataset](https://github.com/kakaobrain/coyo-dataset) (2022)
*  Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: 'Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. In NeurIPS. (2022)
*  Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-Resolution Image Synthesis with Latent Diffusion Models. In CVPR. (2022)
*  Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., Chen, M.: Glide: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. In ICML. (2022)
*  Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical Text-Conditional Image Generation with CLIP Latents. arXiv preprint arXiv:2204.06125 (2022)
*  Gu, S., Chen, D., Bao, J., Wen, F., Zhang, B., Chen, D., Yuan, L., Guo, B.: Vector Quantized Diffusion Model for Text-to-Image Synthesis. In CVPR. (2022)
*  Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.Y., Ermon, S.: SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations. In ICLR. (2021)
*  Kim, G., Kwon, T., Ye, J.C.: DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation. In CVPR. (2022)
*  Avrahami, O., Lischinski, D., Fried, O.: Blended Diffusion for Text-Driven Editing of Natural Images. In CVPR. (2022)
*  Hertz, A., Mokady, R., Tenenbaum, J., Aherman, K., Pritch, Y., Cohen-or, D.: Prompt-to-Prompt Image Editing with Cross-Attention Control. In ICLR. (2023)
*  Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani, M.: Imagic: Text-Based Real Image Editing with Diffusion Models. In CVPR. (2023)
*  Parmar, G., Singh, K.K., Zhang, R., Li, Y., Lu, J., Zhu, J.Y.: Zero-Shot Image-to-Image Translation. arXiv preprint arXiv:2302.03027 (2023)
*  Van Den Oord, A., Vinyals, O., et al.: Neural Discrete Representation Learning. In NIPS. (2017)
*  Esser, P., Rombach, R., Ommer, B.: Taming Transformers for High-Resolution Image Synthesis. In CVPR. (2021)
*  Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning Transferable Visual Models from Natural Language Supervision. In ICML. (2021)
*  Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. JMLR (2020)*  Gal, R., Patashnik, O., Maron, H., Bermano, A.H., Chechik, G., Cohen-Or, D.: StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators. TOG (2022)
*  Deng, J., Guo, J., Xue, N., Zafeiriou, S.: Arcface: Additive Angular Margin Loss for Deep Face Recognition. In CVPR. (2019)
*  Ronneberger, O., Fischer, P., Brox, T.: U-Net: Convolutional Networks for Biomedical Image Segmentation. In MICCAI. (2015)
*  Su, X., Song, J., Meng, C., Ermon, S.: Dual Diffusion Implicit Bridges for Image-to-Image Translation. In ICLR. (2022)
*  Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: PyTorch: An Imperative Style, High-Performance Deep Learning Library. In NeurIPS. (2019)
*  Li, J., Li, D., Xiong, C., Hoi, S.: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. In ICML. (2022)
*  Ho, J., Salimans, T.: Classifier-Free Diffusion Guidance. In NeurIPS Workshop on Deep Generative Models and Downstream Applications. (2021)
*  Hessel, J., Holtzman, A., Forbes, M., Bras, R.L., Choi, Y.: CLIPScore: a reference-free evaluation metric for image captioning. In EMNLP. (2021)
*  Tumanyan, N., Bar-Tal, O., Bagon, S., Dekel, T.: Splicing ViT Features for Semantic Appearance Transfer. In CVPR. (2022)
*  Park, W., Kim, D., Lu, Y., Cho, M.: Relational Knowledge Distillation. In CVPR. (2019)
*  Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual Losses for Real-Time Style Transfer and Super-Resolution. In ECCV. (2016)
*  Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The Unreasonable Effectiveness of Deep Features as a Perceptual metric. In CVPR. (2018)Appendix

In this appendix, we first formulate our introduced metric regarding relational distance (RD). Then, we present additional quantitative results to show the effectiveness of the proposed method by measuring the background difference between source and target images using Learned Perceptual Image Patch Similarity metric  referred to as BG-LPIPS. Also, we demonstrate additional qualitative results of CSG compared with the state-of-the-art methods [4; 33; 35]. Finally, we discuss the limitations and potential negative societal impacts.

### Relational Distance

Relational distance (RD) is introduced to measure how faithfully the relational information between source images is preserved between synthesized target images, which is given by

\[=_{}\|G^{}- G^{}\|_{F} ^{2}, \]

where \(\|\|_{F}\) denotes the Frobenius norm. In the above equation, \(G^{}\) and \(G^{}\) are \(n n\) matrices, where the entry \(G^{}[i,j]\) in the \(i^{}\) row and \(j^{}\) column of \(G^{}\) denotes the perceptual distance  between the \(i^{}\) and \(j^{}\) target images while \(G^{}[i,j]\) represents the perceptual distance between the \(i^{}\) and \(j^{}\) source images.

### Additional Quantitative Results

In addition to Table 1 and 2, which utilize CLIP-similarity , structure distance , and relational distance for evaluation, we also report BG-LPIPS scores of the proposed method and existing frameworks [4; 33; 35], where BG-LPIPS aims to measure the background difference between source and target images based on the LPIPS metric . As presented in Table 3, CSG always achieves the lowest BG-LPIPS scores, implying that the proposed method more effectively preserves the background region. Moreover, Table 4 demonstrates the superiority of our conditional guidance and cross-attention mixup strategy.

### Additional Qualitative Results

We provide additional qualitative results using real images sampled from the LAION 5B dataset  in Figure 7, 8, 9, 10, and 11, which imply the superiority of CSG compared with the state-of-the-art

    & DDIM  & CSG w/o Mixup & CSG \\  & BG-LPIPS (\(\)) & BG-LPIPS (\(\)) & BG-LPIPS (\(\)) \\  cat \(\) dog & 0.3834 & 0.2502 & **0.2111** & **0.1867** \\ dog \(\) cat & 0.3602 & 0.2333 & **0.1983** & **0.1645** \\ wolf \(\) lion & 0.4042 & 0.2852 & **0.2402** & **0.2384** \\ zebra \(\) horse & 0.4127 & 0.3162 & **0.2312** & **0.2303** \\   

Table 4: Additional ablation study results from LAION 5B dataset  to analyze the effectiveness of the conditional score guidance and the cross-attention mixup. ‘CSG w/o Mixup’ synthesizes target images without using the cross-attention mixup.

    & DDIM  & CSG w/o Mixup & CSG \\  & BG-LPIPS (\(\)) & BG-LPIPS (\(\)) & BG-LPIPS (\(\)) \\  cat \(\) dog & 0.3834 & 0.2502 & **0.2111** & **0.1867** \\ dog \(\) cat & 0.3602 & 0.2333 & **0.1983** & **0.1645** \\ wolf \(\) lion & 0.4042 & 0.2852 & **0.2402** & **0.2384** \\ zebra \(\) horse & 0.4127 & 0.3162 & **0.2312** & **0.2303** \\   

Table 3: Additional quantitative results to compare the proposed method with text-driven image-to-image translation methods [4; 33; 35] using the pretrained Stable Diffusion , where real images are sampled from LAION 5B dataset . DDIM denotes the simple inference using Eq. 18. Black and red bold-faced numbers represent the best and second-best performance in each row.

methods . Additionally, Figure 12, 13, and 14 demonstrate that the proposed method achieves outstanding performance on synthesized images which are given by the pretrained Stable Diffusion . Furthermore, we visualize additional qualitative results in Figure 15 to compare CSG with Pix2Pix-Zero using the synthesized samples, which illustrates that CSG outperforms Pix2Pix-Zero. Unlike CSG, Pix2Pix-Zero struggles with dissimilar object-centric tasks such as house-to-Eiffel tower as presented in the figure since Pix2Pix-Zero enforces shape matching through cross-attention layers.

### Limitations and Potential Negative Societal Impacts

Our method may fail to edit images with complex prompts due to the incompetence of pretrained text-to-image diffusion models. Similar to other text-driven image-to-image translation methods, our approach also encounters a limitation that it cannot be applied to complex tasks, such as enlarging parts of an object or moving the object, where it would be an interesting future work to tackle these challenges. Regarding potential negative social impacts, our method can generate harmful or misleading contents due to the pretrained model.

Figure 7: Additional qualitative comparisons between CSG and other state-of-the-art methods tested with the real images sampled from LAION 5B dataset  on the cat-to-dog task.

Figure 8: Additional qualitative comparisons between CSG and other state-of-the-art methods tested with the real images sampled from LAION 5B dataset  on the dog-to-cat task.

Figure 9: Additional qualitative comparisons between CSG and other state-of-the-art methods tested with the real images sampled from LAION 5B dataset  on the zebra-to-horse task.

Figure 10: Additional qualitative comparisons between CSG and other state-of-the-art methods tested with the real images sampled from LAION 5B dataset  on the wolf-to-lion task.

Figure 11: Additional qualitative comparisons between CSG and other state-of-the-art methods tested with the real images sampled from LAION 5B dataset  on the dog-to-dog with glasses task.

Figure 12: Additional qualitative results of the proposed method on the synthesized images by the pretrained Stable Diffusion .

Figure 13: Additional qualitative results of the proposed method on the synthesized images by the pretrained Stable Diffusion .

Figure 14: Additional qualitative results of the proposed method on the synthesized images by the pretrained Stable Diffusion .

Figure 15: Qualitative comparisons between CSG and Pix2Pix-Zero on the synthesized images by the pretrained Stable Diffusion .