# RankUp: Boosting Semi-Supervised Regression with an Auxiliary Ranking Classifier

Pin-Yen Huang

Academia Sinica

Taipei, Taiwan

pyhuang97@gmail.com

&Szu-Wei Fu

NVIDIA

Taipei, Taiwan

szuweif@nvidia.com

&Yu Tsao

Academia Sinica

Taipei, Taiwan

yu.tsao@citi.sinica.edu.tw

###### Abstract

State-of-the-art (SOTA) semi-supervised learning techniques, such as FixMatch and it's variants, have demonstrated impressive performance in classification tasks. However, these methods are not directly applicable to regression tasks. In this paper, we present RankUp, a simple yet effective approach that adapts existing semi-supervised classification techniques to enhance the performance of regression tasks. RankUp achieves this by converting the original regression task into a ranking problem and training it concurrently with the original regression objective. This auxiliary ranking classifier outputs a classification result, thus enabling integration with existing semi-supervised classification methods. Moreover, we introduce regression distribution alignment (RDA), a complementary technique that further enhances RankUp's performance by refining pseudo-labels through distribution alignment. Despite its simplicity, RankUp, with or without RDA, achieves SOTA results in across a range of regression benchmarks, including computer vision, audio, and natural language processing tasks. Our code and log data are open-sourced at [https://github.com/pm25/semi-supervised-regression](https://github.com/pm25/semi-supervised-regression).

## 1 Introduction

The effectiveness of deep learning models heavily depends on the availability of labeled data. However, obtaining labeled data can be challenging in various scenarios. For instance, tasks like quality assessment often require multiple human annotators to label a single data point , resulting in a labor-intensive and time-consuming process. In domains where expert annotation is frequently required, such as medical data, the cost of acquiring labeled data can be extremely expensive . To address these challenges, semi-supervised learning provides a powerful approach to reduce reliance on labeled data for training deep learning models . By effectively leveraging the unlabeled data during model training, semi-supervised learning provides a means to enhance model performance while minimizing the need for extensive labeled data.

Recent state-of-the-art (SOTA) semi-supervised learning methods, such as FixMatch and its variants, use a confidence threshold technique to obtain high-quality pseudo-labels . This approach involves generating pseudo-labels from unlabeled data and then filtering out those with low confidence scores. The model is then trained to produce predictions consistent with these high-quality pseudo-labels. Despite its success across various classification tasks, directly applying this technique to regression tasks encounters several challenges. First, unlike classification models, regression models typically lack confidence measures for their predictions, making the confidence threshold technique unfeasible. Additionally, one of the motivations behind using pseudo-labels is to increase the model's confidence in its predictions for unlabeled data, based on the low-density assumption . However, as there are no confidence measures in the predictions of regression models, relying on the low-density assumption and increasing the confidence of unlabeled data becomes unfeasible.

In this paper, we introduce _RankUp_, a simple yet effective semi-supervised regression framework that leverages existing semi-supervised classification methods. RankUp achieves this by using an auxiliary ranking classifier, which concurrently solves a ranking task alongside the original regression task. The ranking task is derived from the original regression problem, where the objective is to compare the labels of pairs of samples to determine their relative rank (i.e., which one is larger or smaller). Since ranking problem is a type of classification problem, existing semi-supervised classification methods can be applied to assist in training the auxiliary ranking classifier (see Fig. 1).

Our empirical results demonstrate that enhancing the performance of the auxiliary ranking classifier also improves the performance of the original regression task, as measured by metrics, such as mean absolute error (MAE) , coefficient of determination (R\({}^{2}\)) , and Spearman rank correlation coefficient (SRCC) . Furthermore, we show that applying existing semi-supervised classification methods to the auxiliary ranking classifier can effectively utilize unlabeled data, leading to further improvements in the classifier's performance. This improvement, in turn, translates to enhanced performance in the original regression task, showcasing the potential of applying semi-supervised classification techniques to enhance regression models.

One of the key advantages of using the auxiliary ranking classifier is its ability to enhance the ranking relationship of pseudo-labels. Building upon this effect, we propose a novel _Regression Distribution Alignment_ (RDA) method, designed to further improve RankUp's performance by refining the distribution of regression pseudo-labels. RDA adjusts the distribution of these pseudo-labels to better align with the true underlying distribution of the unlabeled data. This approach assumes that the distributions of labeled and unlabeled data are similar, allowing us to estimate the distribution of the unlabeled data based on that of the labeled data distribution. This assumption holds true in many cases, especially when labeled data are randomly sampled from the same pool as the unlabeled data. By aligning these distributions, RDA improves the quality of the pseudo-labels, ultimately leading to better model performance when training with these refined pseudo-labels.

Our experimental results demonstrate that RankUp, even without RDA, achieves state-of-the-art (SOTA) results across a variety of regression datasets, including tasks in computer vision, audio, and natural language processing. Moreover, integrating RDA with RankUp provides an additional performance boost, leading to the highest performance observed in our experiments. For example, RankUp alone achieves at least a 13% improvement in MAE and a 28% improvement in R\({}^{2}\) compared to SOTA methods on the image age estimation dataset (UTKFace) with 50 labeled samples. The addition of RDA further boosts these results by an additional 6% and 7% in MAE and R\({}^{2}\), respectively. The empirical results of our experiments demonstrate that existing semi-supervised classification methods can be effectively leveraged to improve the performance of semi-supervised regression tasks. **These findings bridge the gap between future research in semi-supervised regression and classification, paving the way for further advancements in the field.**

Figure 1: Illustration of using FixMatch on the Auxiliary Ranking Classifier (ARC). This diagram uses the age estimation task as an example, where the goal is to predict the age of a person in an image. The auxiliary ranking classifier transforms this task into a ranking problem by comparing two images to determine which person is older. (Image sourced from the UTKFace dataset ).

Related Works

In this section, we review related research in semi-supervised learning. We categorize the literature into two groups: methods applicable to regression tasks, which will be discussed in Section 2.1, and methods applicable only to classification tasks, detailed in Section 2.2.

### Semi-Supervised Regression

In semi-supervised regression, methods commonly rely on the smoothness assumption [6; 25], which suggests that nearby data points in the feature space should share similar labels. Consistency regularization is a popular technique employed to achieve this assumption. It encourages models to generate consistent predictions for slightly perturbed data.

For example, the \(\)-model  applies data augmentation to unlabeled data and minimizes the squared difference between the predictions of the augmented data and their original counterparts. Techniques like Mean Teacher  involve model-weight ensembling to align the predictions of the model with its ensemble counterpart. Similarly, UCVME  employs a bayesian neural network to ensure consistency in uncertainty predictions across co-trained models. Additionally, CLASS  utilizes contrastive learning to encourage features of similar labels to be closer together.

### Semi-Supervised Classification

In semi-supervised classification, in addition to the smoothness assumption, another commonly relied-upon assumption is the low-density assumption [6; 25]. This assumption suggests that a classifier's decision boundary should ideally pass through low-density regions in the feature space. Pseudo-labeling  is a common approach used to achieve this assumption, where the highest probability class predictions on unlabeled data are utilized as pseudo-labels for training. By incorporating pseudo-labels, the model's confidence in predicting unlabeled data is increased, effectively pushing the decision boundary away from high-density regions towards low-density regions.

Recent SOTA semi-supervised learning methods combine pseudo-labeling with consistency regularization to achieve both the low-density and smoothness assumptions, leading to improved performance. For example, MixMatch  utilizes a mixup  technique and averages predictions from multiple augmented instances to ensure consistency, while also using a sharpening technique to boost prediction confidence on unlabeled data. Similarly, FixMatch  builds on this concept by generating high-quality pseudo-labels from weakly augmented unlabeled data using a confidence threshold and enforcing consistency between weakly and strongly augmented versions of the same input.

Despite the success of these methods on classification tasks. The low-density assumption doesn't directly translate to regression tasks, as regression models lack explicit confidence measures and decision boundaries like those in classification models. As a result, existing semi-supervised learning methods based on the low-density assumption cannot be directly applied in regression settings.

## 3 Method

The proposed framework, RankUp, introduces two additional components: ARC and RDA. The design of ARC is inspired by RankNet . To provide a clear understanding of the ARC's implementation, we first present background information on RankNet in Section 3.1. Subsequently, we will detail the implementation of ARC in Section 3.2 and introduce RDA in Section 3.3. Furthermore, we propose a warm-up scheme and techniques for reducing the computational time of RDA in Sections 3.4 and 3.5, respectively. Lastly, we outline the complete RankUp framework in Section 3.6.

### Background: RankNet

RankNet  is a deep learning model designed to predict the relevance scores of documents. The core idea behind RankNet is the use of a pairwise ranking loss. It compares two samples and predicts their relative ranking (i.e., which document is more relevant). This approach effectively transforms the relevance score prediction task into a pairwise classification problem. In the following, we will provide a detailed explanation of how the pairwise ranking prediction is performed and how the corresponding loss is calculated.

**Pairwise Ranking Prediction.** The output of RankNet is a single scalar value indicating the ranking score of the sample, where a higher score indicates greater relevance. To obtain the pairwise ranking prediction, two samples are fed separately into the model to get their respective ranking scores. The difference between these scores is then passed through a sigmoid function, which generates a prediction in the range . This prediction indicates the likelihood that the first sample is more relevant than the second. If the output is greater than 0.5, the model predicts that the first sample has higher relevance; if the output is less than 0.5, the second sample is considered more relevant. Mathematically, for two samples, \(x_{i}\) and \(x_{j}\), and the RankNet model \(g\), the formula to obtain the pairwise ranking prediction \(p_{ij}\) is as follows:

\[p_{ij}=(g(x_{i})-g(x_{j}))=)-g(x_{j}))}} \]

Here, \(g(x_{i})\) and \(g(x_{j})\) represent the ranking scores for samples \(x_{i}\) and \(x_{j}\), respectively. A higher value of \(p_{ij}\) indicates a higher likelihood that the ranking of \(x_{i}\) will be higher than that of \(x_{j}\).

**Pairwise Ranking Loss.** The pairwise ranking loss is calculated by comparing the model's predicted pairwise ranking \(p_{ij}\) with the ground truth label \(y_{ij}\). The label \(y_{ij}\) indicates the true relative ranking between samples \(x_{i}\) and \(x_{j}\). Specifically, \(y_{ij}=1\) indicates that sample \(x_{i}\) is ranked higher than sample \(x_{j}\), \(y_{ij}=0\) indicates that sample \(x_{i}\) is ranked lower than \(x_{j}\), and \(y_{ij}=0.5\) suggests the two samples are equally ranked. Since this is fundamentally a binary classification task, the pairwise ranking loss is calculated using the cross-entropy loss function. Mathematically, the pairwise ranking loss for a batch of data is defined as follows:

\[_{ranknet}=}_{i=1}^{N}_{j=1}^{N}(y_{ij},\;p_ {ij}) \]

Here, \(N\) denotes the batch size, CE is the cross-entropy loss function, \(p_{ij}\) is the predicted pairwise ranking between samples \(x_{i}\) and \(x_{j}\), and \(y_{ij}\) is the corresponding ground truth label. The loss iterates through all possible pairs of samples in the batch to calculate the average loss for the entire batch.

### Auxiliary Ranking Classifier (ARC)

The _Auxiliary Ranking Classifier_ (ARC) is designed to solve a ranking task alongside the primary regression task. It can be easily integrated into existing regression model architectures like ResNet , BERT , or Whisper . ARC is implemented as an additional output layer that shares the same hidden layers with the original regression model. This transforms the model into a multi-task architecture with two output tasks: the original output header continues to provide the regression output, while ARC generates a ranking score for the sample.

The core idea behind ARC is to transform the original regression task into a multi-class classification problem, allowing existing semi-supervised classification methods to assist in its training. To achieve this, ARC's design is inspired by RankNet, which can effectively convert the regression task into a binary classification task. However, since a multi-class classification output is required, we introduce two key modifications to RankNet to adapt it for this purpose:

1. The scalar output value of RankNet is changed to a two-class output, where each output class indicates which sample in a pair has a relatively greater ranking score.
2. The sigmoid function is replaced with softmax, which converts the model's output into a multi-class classification probability distribution.

Specifically, for the auxiliary ranking classifier \(r\), which outputs a two-class output, the formula to obtain the pairwise ranking prediction \(_{ij}\) of two data samples, \(x_{i}\) and \(x_{j}\), is as follows:

\[_{ij}=(r(x_{i})-r(x_{j})) \]

Here, \(_{ij}\) represents a two-class prediction that indicates which sample in the pair has a relatively higher regression label. The loss calculation for ARC remains the same as described in Equation 2.

This output format enables the integration of existing semi-supervised classification methods. We utilize FixMatch  as the semi-supervised classification technique for training ARC. An illustrative example of applying FixMatch to ARC can be found in Figure 1, with further details in Algorithm 1.

```
0: Labeled batch \(X=\{\ (x_{i},\ y_{i})\}_{i=1}^{N_{lb}}\), unlabeled batch \(U=\{\ u_{i}\}_{i=1}^{N_{ub}}\), confidence threshold \(\), unlabeled loss weight \(_{ulb}\), weak augmentation \(A_{w}\), strong augmentation \(A_{s}\)
1:\(_{lb}=)^{2}}_{i=1}^{N_{lb}}_{j=1}^{N_{lb}} r(A_{w}(x_{i}))-r(A_{w}(x_{j})),\ \{y_{i}>y_{j}\}\) { Compute cross-entropy labeled loss }
2:\(_{ulb}=0\) { Initialize unlabeled loss }
3:for\(i=1\)to\(N_{ulb}\)do
4:for\(j=1\)to\(N_{ulb}\)do
5:\(^{w}_{ij}=(r(A_{w}(u_{i}))-r(A_{w}(u_{j})))\) { Predict weak pairwise ranking }
6:\(^{s}_{ij}=(r(A_{s}(u_{i}))-r(A_{s}(u_{j})))\) { Predict strong pairwise ranking }
7:\(_{ulb}=_{ulb}+\{(^{w}_{ij})>\}\) CE\(((^{w}_{ij}),\ ^{s}_{ij})\) { Accumulate unlabeled loss }
8:endfor
9:endfor
10:\(_{ulb}=)^{2}}_{ulb}\) { Average unlabeled loss }
11:return\(_{}=_{lb}+_{ulb}_{ulb}\)
```

**Algorithm 1** Auxiliary Ranking Classifier (with FixMatch)

### Regression Distribution Alignment (RDA)

Distribution alignment is a commonly used technique in semi-supervised classification [2; 16; 28; 4], where pseudo-labels are refined by aligning their distribution with that of the labeled data. Training semi-supervised models with these refined pseudo-labels can lead to performance improvements. However, existing distribution alignment methods are designed for classification tasks involving discrete label distributions, making them unsuitable for regression settings. Moreover, applying distribution alignment to ARC is impractical, as the two output classes always have equal proportions. To address these challenges, we propose _Regression Distribution Alignment_ (RDA), enabling the direct application of distribution alignment to regression tasks.

The RDA process involves three key steps: (1) extracting the labeled data distribution, (2) generating the pseudo-label distribution, and (3) aligning the pseudo-label distribution with the labeled data distribution. These steps correspond to the orange, blue, and yellow parts of Figure 2, respectively.

**Step 1: Extracting the Labeled Data Distribution.** The labeled data is sorted according to its label values. To ensure a one-to-one correspondence with the pseudo-labels, the labeled data distribution must contain the same number of data points as the pseudo-label set. We use linear interpolation to extend the labeled distribution to match the size of the pseudo-labels.

**Step 2: Generating the Pseudo-Label Distribution.** The model generates pseudo-labels for all the unlabeled data. These pseudo-labels are then sorted by their values, either in ascending or descending order, as long as the sorting direction is consistent with that of the labeled data distribution.

**Step 3: Aligning the Distributions.** Once both distributions have been sorted and resized to the same size, the alignment is performed by replacing each pseudo-label value with its corresponding value from the labeled data distribution.

In each training iteration, RDA is applied to refine the pseudo-labels. The loss is then computed between the model's predictions and the RDA-aligned pseudo-labels to minimize their difference. For an unlabeled data point \(u_{i}\) with its corresponding regression prediction \(_{i}\) and RDA-aligned pseudo-label \(_{i}\), the RDA loss \(_{}\) for a batch of unlabeled data is defined as:

\[_{}=}_{i=1}^{N_{ulb}}L_{reg}(_{i },\ _{i}) \]

Here, \(N_{ulb}\) denotes the batch size of unlabeled data, \(L_{reg}\) represents a regression loss function (e.g., MAE, MSE).

The design of RDA is based on two key assumptions. First, it assumes that the distributions of labeled and unlabeled data are similar, which is often true since labeled data is typically randomly sampled from the unlabeled pool. Second, it assumes that the ranking of the pseudo-labels is reasonably accurate. Integrating RDA with ARC can reinforce this assumption, as ARC enhances the ranking relationships of pseudo-labels. Both assumptions are crucial for ensuring that RDA works properly.

### Warm-Up Scheme for RDA

In the early stages of training, the pseudo-label rankings may be poorly predicted, which can degrade the quality of the pseudo-labels refined through RDA. To address this, we introduce a linear warm-up scheme to stabilize the RDA process. The adjusted RDA loss, \(^{}_{}\), is defined as follows:

\[^{}_{}=(}},\;1.0 )_{} \]

Here, \(iter\) denotes the current training iteration, and \(_{}\) is a hyperparameter that controls the duration of the warm-up phase. The \(\) function ensures that the warm-up factor does not exceed 1.0, smoothly transitioning the model toward the full effect of \(_{}\).

### Reducing Computational Time of RDA

Applying RDA can be computationally expensive, as it requires inference all unlabeled data and sorting all pseudo-labels at every training iteration. This significantly increases the computational load compared to the original training process, especially when dealing with a large volume of unlabeled data, making the implementation of RDA impractical. To mitigate this challenge, we propose several techniques aimed at reducing the computational burden of RDA.

**Pseudo-label table.** RDA creates a table of the same size as the unlabeled dataset. This table stores the model's predicted pseudo-labels for each instance of unlabeled data. For each training iteration, the model generates new pseudo-labels, which are stored and updated within this table. This approach eliminates the need to rerun inference on all unlabeled data when applying RDA, as it only requires a simple lookup from the pseudo-label table.

**Applying RDA only every \(T\) steps.** To further reduce computational costs, RDA is applied only every \(T\) steps, where \(T\) is a hyperparameter. This is achieved by creating a second table of the same size as the unlabeled dataset, which stores the previously aligned results of the pseudo-labels generated by applying RDA. Between RDA updates, the model uses these stored aligned pseudo-labels, thereby avoiding the need to run RDA in every iteration. This strategy effectively reduces the computational cost associated with the RDA process to \(1/T\).

Figure 2: Illustration of RDA: This example includes three labeled data pairs \(\{(x_{i},\;y_{i})\}_{i=0}^{2}\) and five unlabeled data points with corresponding pseudo-labels \(\{(u_{i},\;_{i})\}_{i=0}^{4}\). Each data pair is represented by a single bar in the graph. The x-axis indicates the sample indices, while the y-axis represents their corresponding regression label values. The orange bars demonstrate the process of obtaining the labeled data distribution, the blue bars illustrate how the pseudo-label distribution is formed, and the yellow bars show the aligned pseudo-labels after applying RDA.

### Putting It All Together - RankUp

We introduce the term _RankUp_ to describe our proposed framework, which integrates two key components: ARC and RDA. The use of RDA is optional, depending on whether its underlying assumptions are satisfied. The final loss for RankUp is a combination of the regression loss and the ARC loss. The regression loss consists of the original labeled regression loss plus the unlabeled RDA loss. Specifically, the RankUp loss \(_{}\) is defined as follows:

\[_{}=(_{}+_{}_{}^{})+(_{}_{}) \]

In this equation, \(_{}\) represents the loss from the original labeled regression task, while \(_{}^{}\) denotes the RDA loss, as defined in Equation 5. The hyperparameter \(_{}\) controls the weight of the RDA loss. If RDA is not employed, the term \(_{}_{}^{}\) can be excluded from the equation. The term \(_{}\) corresponds to the loss from the ARC module, as detailed in Algorithm 1. Additionally, the hyperparameter \(_{}\) regulates the weight of the ARC loss.

## 4 Experiments

In this section, we evaluate RankUp's performance across various tasks. The experimental settings are described in Section 4.1. The main results for RankUp under different label configurations are presented in Section 4.2, while Section 4.3 provides additional results on audio and text datasets. Section 4.4 explores the use of alternative semi-supervised classification methods in place of FixMatch. Lastly, we discuss potential reasons why the smoothness and low-density assumptions are also effective for regression tasks in Section 4.5.

### Settings

**Evaluation Metrics.** We use three evaluation metrics: MAE, R\({}^{2}\), and SRCC, to assess the performance of semi-supervised regression methods. MAE measures the average absolute difference between the model's predictions and the actual values. The R\({}^{2}\) score indicates the proportion of variance in the data explained by the model. SRCC evaluates the correlation between the predicted rankings and the actual rankings.

**Evaluation Robustness.** To ensure the reliability of our evaluation results, each experiment is executed three times using fixed random seeds (0, 1, and 2). We report both the mean and standard deviation of each metric.

**Fair Comparison.** To ensure a fair comparison between our proposed methods and related works, we implement and evaluate all methods within the same codebase. Specifically, we adapt the popular semi-supervised classification framework USB , modifying it for regression tasks to implement both our proposed methods and related works. Weak augmentation is applied consistently to the labeled data across all semi-supervised and supervised methods. For specific details on the modifications made to USB, please refer to Appendix A.5. The code and full training logs of the experiments presented in this paper are open-sourced at [https://github.com/pm25/semi-supervised-regression](https://github.com/pm25/semi-supervised-regression).

**Hyperparameters.** We use the hyperparameters of USB as the base for fine-tuning. We first fine-tune the hyperparameters in the supervised baseline setting and find the hyperparameters that lead to lowest MAE score. These same hyperparameters are then applied to all semi-supervised regression methods to ensure a fair comparison. Only the additional hyperparameters specific to each semi-supervised method are further tuned. For more details on the hyperparameters, please refer to Appendix A.13.

**Base Model.** The base model used in our experiments varies depending on the data type. For image data, we use Wide ResNet-28-2 , which is not pre-trained. For audio data, we use the pre-trained Whisper-base , and for text data, we use the pre-trained Bert-Small .

**Dataset.** To simulate the semi-supervised setting, we randomly sample a portion of the dataset as labeled data, treating the remainder as unlabeled. To evaluate performance, we use three diverse datasets: UTKFace , an image age estimation dataset; BVCC , an audio quality assessment dataset; and Yelp Review , a text sentiment analysis (opinion mining) dataset. For more detailed information about these datasets, please refer to Appendix A.11.

### Main Results

To evaluate the performance of RankUp under different labeled data settings, we conducted experiments using the UTKFace dataset with 50 and 250 labeled samples. We tested two configurations of RankUp: one incorporating the RDA (RankUp + RDA) and the other without it (RankUp). Their performance was compared against other semi-supervised regression methods, with MixMatch specifically representing the consistency regularization component of the approach. Additionally, we included a supervised setting that used only the labeled data **without** incorporating any unlabeled data during training, as well as a fully-supervised setting that used **all** available data (both labeled and unlabeled), assuming the unlabeled data had known true labels. We also conducted experiments with a 2000 labeled samples setting; however, due to space limitations, the results for this configuration can be found in Appendix A.2.

The results are presented in Table 1. We observed that RankUp (without RDA) consistently outperforms existing semi-supervised regression methods, especially when the amount of labeled data is scarce. Specifically, in the 50-label setting, RankUp achieves at least a 12.9% improvement in MAE, a 28.2% improvement in R\({}^{2}\), and a 4.3% improvement in SRCC compared to other semi-supervised regression methods. In the 250-label setting, RankUp shows at least an 11.2% improvement in MAE, an 8.5% improvement in R\({}^{2}\), and a 0.4% improvement in SRCC.

Furthermore, the integration of RDA with RankUp further enhances the performance of RankUp. Specifically, in the 50-label setting, RankUp + RDA achieves an additional 6.3% improvement in MAE, a 7.4% improvement in R\({}^{2}\), and a 9.5% improvement in SRCC compared to RankUp alone. Similarly, in the 250-label setting, RankUp + RDA achieves an additional 6.9% improvement in MAE, a 4.1% improvement in R\({}^{2}\), and a 2.5% improvement in SRCC relative to RankUp.

These empirical results demonstrate the effectiveness of RankUp and RDA across different labeled settings. Another notable observation is that RankUp + RDA in the 50-label setting outperforms the supervised model that utilizes five times the labeled data (in the 250-label setting) across all three metrics. Specifically, RankUp + RDA achieves a 1.0% improvement in MAE, a 2.2% improvement in R\({}^{2}\), and an 8.1% improvement in SRCC while using only one-fifth of the labeled data, demonstrating its effectiveness in reducing labeling costs.

### Additional Results on Audio and Text Data

To further assess the performance of RankUp across different data types and tasks, we evaluated it on the BVCC and Yelp Review datasets using 250 labeled samples. The results are presented in Table 2. The table demonstrates that RankUp also consistently outperforms existing semi-supervised regression methods on both audio and text datasets. Specifically, on the BVCC dataset, RankUp (without RDA) achieves at least a 5.6% improvement in MAE, a 6.3% improvement in R\({}^{2}\), and a 0.3% improvement in SRCC compared to other semi-supervised regression methods. On the Yelp

    &  \\   &  &  \\   & MAE\(\) & R\({}^{2}\)\(\) & SRCC\(\) & MAE\(\) & R\({}^{2}\)\(\) & SRCC\(\) \\  Supervised & 14.13a0.56 & 0.090a0.092 & 0.371a0.071 & 9.42a0.16 & 0.540a0.014 & 0.712a0.010 \\  \(\)-Model & 13.82a1.02 & 0.100a0.086 & 0.387a0.092 & 9.45a0.30 & 0.534a0.030 & 0.706a0.015 \\ Mean Teacher & 13.92a0.20 & 0.127a0.037 & 0.423a0.023 & 8.85a0.25 & 0.586a0.020 & 0.745a0.013 \\ CLSS & 13.61a0.92 & 0.138a0.101 & 0.447a0.074 & 9.10a0.15 & 0.586a0.016 & 0.737a0.014 \\ UCVME & 13.49a0.95 & 0.157a0.110 & 0.412a0.127 & 8.63a0.17 & 0.626a0.006 & 0.767a0.007

[MISSING_PAGE_FAIL:9]

Furthermore, using different semi-supervised classification methods to utilize the unlabeled data further boost performance compared to using only labeled data. Specifically, we tested the \(\)-Model, Mean Teacher, and FixMatch, all of which demonstrated improvements over the "Supervised" setting across MAE, R\({}^{2}\), and SRCC metrics. Among these methods, FixMatch achieved the best results, showing a 21.8% improvement in MAE, a 27.7% improvement in \(R^{2}\), and an 11.9% improvement in SRCC compared to the "Supervised" setting. This highlights the effectiveness of leveraging unlabeled data and semi-supervised classification techniques to improve ARC and RankUp's overall performance.

### Understanding Smoothness and Low-Density Assumptions in Regression

The low-density assumption is crucial for understanding the effectiveness of semi-supervised learning methods. However, it does not directly apply to regression tasks due to the absence of decision boundaries and confidence measures. In this section, we explore why RankUp performs well in regression tasks by leveraging semi-supervised classification techniques that utilize the low-density assumption. This understanding can broaden our perspective on these assumption.

We adopt a broader interpretation of the smoothness and low-density assumptions. Rather than viewing them solely in the context of classification, we interpret the smoothness assumption as an effort to **group features with similar labels together**, while the low-density assumption aims to **separate features with dissimilar labels**. These perspectives align with RankUp's approach. By training ARC with pseudo-labels, the model is encouraged to have greater confidence in the pairwise ranking predictions of the unlabeled data, thus pushing the features with dissimilar pseudo-labels further apart. Additionally, ensuring consistent predictions between weakly and strongly augmented data assists in grouping features with similar labels. The t-SNE visualization demonstrated in Figure 3 supports this claim, showing that within RankUp, similar labels are closer together, while dissimilar labels are pushed further apart in the feature space.

## 5 Conclusion

Recent advancements in semi-supervised learning have achieved impressive results across various classification tasks; however, these methods are not directly applicable to regression tasks. In this work, we investigate the potential of leveraging existing semi-supervised classification techniques for regression tasks. We propose a novel framework, RankUp, which introduces two key components: the Auxiliary Ranking Classifier (ARC) and Regression Distribution Alignment (RDA). The empirical results of our experiments demonstrate the effectiveness of our methods across various labeled data settings (50, 250, and 2000 labeled samples) and different types of datasets (image, audio, and text). These findings show that semi-supervised classification techniques can be effectively adapted to regression tasks, bridging the gap between research in semi-supervised regression and classification, and paving the way for more advanced research in this area.

Figure 3: Comparison of t-SNE visualizations of feature representations for different semi-supervised regression methods on evaluation data. The supervised model is displayed on the left, MixMatch is in the center, and RankUp (without RDA) is shown on the right.

Acknowledgments

I sincerely appreciate everyone who made this work possible. I am especially thankful to my coauthors, Szu-Wei Fu and Prof. Yu Tsao for their mentorship and our weekly discussions; their insights and comments have been invaluable in shaping this research. I am also very grateful for my experience at CLLab and the guidance of Hsuan-Tien Lin, which sparked my interest in weakly-supervised learning and greatly influenced my research mindset and approach. My heartfelt thanks go to my family for their unwavering support throughout my academic journey. I also thank my friend Chi-Chang, whose discussions helped me develop a deeper understanding of how to approach research. Lastly, I want to thank Chia-Ling for her continuous support and encouragement during the development of this research. This work would not have been possible without all these individuals. Thank you!