# Open Visual Knowledge Extraction via

Relation-Oriented Multimodality Model Prompting

 Hejie Cui\({}^{1}\)1  Xinyu Fang\({}^{2}\)2  Zihan Zhang\({}^{2}\)  Ran Xu\({}^{1}\)  Xuan Kan\({}^{1}\)  Xin Liu\({}^{3}\)

Yue Yu\({}^{4}\)  Manling Li\({}^{5}\)  Yangqiu Song\({}^{3}\)  Carl Yang\({}^{1}\)

\({}^{1}\)Emory University \({}^{2}\)Tongji University \({}^{3}\) The Hong Kong University of Science and Technology

\({}^{4}\) Georgia Institute of Technology \({}^{5}\) Northwestern University

These authors contributed equally to this work.Correspondence to: j.carlyang@emory.edu

###### Abstract

Images contain rich relational knowledge that can help machines understand the world. Existing methods on visual knowledge extraction often rely on the pre-defined format (e.g., sub-verb-obj tuples) or vocabulary (e.g., relation types), restricting the expressiveness of the extracted knowledge. In this work, we take a first exploration to a new paradigm of open visual knowledge extraction. To achieve this, we present OpenVik which consists of an open relational region detector to detect regions potentially containing relational knowledge and a visual knowledge generator that generates format-free knowledge by prompting the large multimodality model with the detected region of interest. We also explore two data enhancement techniques for diversifying the generated format-free visual knowledge. Extensive knowledge quality evaluations highlight the correctness and uniqueness of the extracted open visual knowledge by OpenVik. Moreover, integrating our extracted knowledge across various visual reasoning applications shows consistent improvements, indicating the real-world applicability of OpenVik.

## 1 Introduction

Knowledge extraction has been widely studied on texts  for enhancing logical reasoning  and explainable AI , and recent studies have explored _open_ knowledge extraction through categorizing seed relations  and eliciting from language models . Visual knowledge extraction, on the other hand, captures intricate details like tools, sizes, and positional relationships, which are often difficult to express exhaustively in texts . Yet existing approaches of visual knowledge extraction are either restricted by a fixed knowledge format  or the predefined sets of objects/relations . While efficient at capturing interactions between objects, the produced visual knowledge is often limited in richness and confined to a single format, falling short in representing the diverse real-world information that can be complemented by visual data.

In this endeavor, we propose to further explore a new paradigm of open visual knowledge extraction (OpenVik). Specifically, we propose to generate relation-oriented, but format-free knowledge that includes a wider variety of elements, such as descriptions, insertions, and attributes, among others. Drawing inspiration from the wealth of knowledge encapsulated in large models , we propose to leverage pre-trained large multimodality models by eliciting open visual knowledge through relation-oriented visual prompting. This approach allows for a more nuanced understanding of visual data, mirroring how humans naturally emphasize certain aspects of visual scenes when perceiving and describing visual information, leading to more flexible visual knowledge extraction.

Our proposed OpenVik framework consists of two modules, an open relational region detector and a format-free visual knowledge generator. It is a unique challenge to detect the regions potentially containing relational knowledge, since traditional region detectors primarily focus on learning predefined object classes. To learn the regression of relational regions, we propose to use free-form knowledge descriptions as supervision and leverage knowledge generation as a training objective. With the detected regions, the remaining question is how to interpret these regions into free-form knowledge. We propose a visual knowledge generator by harnessing the power of language variety enhancement in large pre-trained multimodality models. Specifically, we prompt them to generate knowledge descriptions of any formats and condition the generation on the detected relational regions.

However, establishing a new paradigm of open visual knowledge extraction is challenging due to the absence of comprehensive and diverse training data. Existing datasets sources such as scene graphs [51; 24], dense captions , and dense relational subsets  often exhibit a long-tail distribution biased to more prevalent relations and entities . Brute-force merging of these datasets could exacerbate the distribution bias inherent in the data. To alleviate the bias, we propose two diversity-driven data enhancement strategies based on an adapted TF-IDF+ score, involving random dropping and data augmentation with external knowledge resources. These strategies optimize data distributions and richness, thus fostering diverse open visual knowledge extraction.

We implement extensive evaluations to assess the quality and utility of the open visual knowledge extracted by OpenVik, encompassing: 1) directly evaluating the performance of knowledge generation; 2) engaging human evaluators for a multi-faceted assessment of in-depth knowledge quality; and 3) comparing the open visual knowledge extracted with OpenVik with existing knowledge sources, such as non-parametric knowledge from the ConceptNet knowledge graph, and parametric knowledge from the GPT-3.5 large language model. Furthermore, the utility of the extracted open visual knowledge is validated through its integration with several common applications that require visual understanding, including text-to-image retrieval, grounded situation recognition, and visual commonsense reasoning. These applications demonstrate consistent improvements, affirming the practical utility of OpenVik.

## 2 Related Work

**Visual knowledge extraction.** Recent advancements in knowledge extraction have extended from being purely text-driven to incorporating images [11; 29]. VisKE  is designed to verify relations between pairs of entities, e.g., _eat(horse_, _hay_). Scene graphs, which locate objects in the image and identify visual predicates between subjects and objects in a triple format, e.g., (_man, on, chair_), are extensively studied for vision understanding [52; 63; 60]. A recent work OpenSGG  extends SGG to open-vocabulary objects, enabling the relation prediction for unseen objects. Other studies have explored caption-like formats, like dense captioning  with a set of object-centric descriptions across regions, and relational captioning  focusing on relational information between objects. Despite these advancements, existing methods either adhere to a pre-defined format and vocabulary or are constrained by the biased distribution of training sets. This highlights the pressing need for a format-free approach in visual knowledge extraction with knowledge diversity.

**Large model prompting.** Recently, large language and multimodality models have exhibited remarkable successes in capturing commonsense knowledge across various tasks, especially facilitating few-shot [15; 53; 25; 58] and zero-shot learning [23; 66; 59]. The potential of prompt-based learning for pre-trained vision-language models [2; 37; 42] has been explored for handling diverse data types across multiple modalities, such as images and texts, with improved performance in tasks including image classification [33; 67], segmentation  and visual question answering . Leveraging the substantial information encapsulated within these pre-trained multimodality models to extract explicit knowledge can enrich existing resources, potentially laying the groundwork for advances in interpretability research and mitigating the hallucination issue associated with large models [19; 10].

## 3 Method

In this section, we introduce our new paradigm and two key model design novelty featuring OpenVik, relation-oriented multimodality model prompting and diversity-driven data enhancement.

### Open Visual Knowledge Extraction

Given a dataset \(=\{(_{i},_{i},_{i})\}_{i=1}^{M}\) consisting of \(M\) samples, \(_{i}\) is the \(i\)-th image (such as the input image in Figure 1), \(_{i}=\{_{j}\}_{j=1}^{n_{i}}\) is a set of \(n_{i}\) region descriptions (such as "_the boat on water_" in Figure 1), \(_{i}=\{_{j}\}_{j=1}^{n_{i}}\) is the set of \(n_{i}\) relation-oriented visual regions, where each \(_{j}\) corresponds to a visual region \(_{j}_{i}\) in image \(_{i}\). The goal of our open visual knowledge discovery is to train a model \(\) capable of producing a set of format-free knowledge descriptions (such as "_large boat docked at pier_" in Figure 1) given any image \(_{k}\) during the inference stage.

### Relation-Oriented Multimodality Model Prompting

The overall architecture of OpenVik is shown in Figure 1. It comprises two modules: an open relational region detector \(_{v}\) and a format-free visual knowledge generator \(_{t}\). The two modules are learned separately during training with our diversity-enhanced data (Section 3.3) and combined to produce format-free visual knowledge at inference. Specifically, the relational region detector \(_{v}\) takes an image \(_{i}\) as the input and learns to select a flexible number of relational regions \(_{i}=\{(_{j})\}_{j=1}^{n_{i}}\) that captures object interactions, each corresponding to a description \(_{j}\) in \(_{i}\); the visual knowledge generator \(_{t}\) generates format-free knowledge descriptions by prompting and fine-tuning the multimodality model with the guidance of detected visual region \(_{j}\). All notations for the region detector and knowledge generator are detailed in Table 9 and Table 10, respectively.

**Open relational region detector.** Although existing object detection algorithms have been widely recognized for their efficiency in object detection, they are usually restricted to object-centric visual regions in a predefined set, and thus cannot directly capture open relational information with a single box. Detecting regions containing relational knowledge remains to be a challenge. We make two adaptions on the object detection FasterRCNN  to train the open relational region detector:

* _Region Regression_: we change the original object-centric region labels to our newly created relation-centric box labels, denoted as \(_{j}\). The foreground of each relation-centric region label \(_{j}\) is created by taking the union of the object-level bounding boxes of the entities, i.e., _boat, water_, contained in a ground truth region knowledge description \(_{j}\). This forms the region regression loss \(_{}\).

  
**Notation** & **Meaning** \\  \(_{i}\) & input image of the relational region detector \\ \(_{i}\) & distribution-centric box label \\ \(_{i}\) & set of relation-centric boxes of an image \\ \(_{i}\) & region-resolution of heat \\ \(_{i}\) & region-resolution of heat \\ \(_{}\) & region region-resolution of heat in images \\ \(_{}\) & region region-resolution loss supported by current regional boxes \\ \(_{}\) & knowledge generation loss supported by CF relational knowledge \\ \(_{}\) & the overall objective of the relational region detector \\   

Table 1: Notations for open region detector.

Figure 1: The overview of OpenVik. The left orange and purple panels illustrate key components of relation-oriented multimodality model prompting: open relational region detector and format-free visual knowledge generator. The right green one depicts diversity-driven data enhancement strategy. OpenVik is designed to extract relation-oriented format-free open visual knowledge with novel entities, diverse relations, and nuanced descriptive details.

* _Knowledge Supervision_: To assist with the refinement of the bounding box, we replaced the object-centric label classification in traditional object detectors with knowledge supervision. A pre-trained generator is finetuned to create the regional description grounded to the given region. This is supervised by the cross-entropy loss \(_{}\) with region description \(_{j}\).

The training objective \(_{l}\) of the relational region detector is formulated as below, where \(_{}\) is the regional regression loss and \(_{}\) is the knowledge supervision loss,

\[_{v}=_{}+_{}. \]

Format-free visual knowledge generator.OpenVik provides better knowledge grounding by conditioning the generator on the detected relational region, leading to a reasoning-driven generation. Specifically, the detected bounding box (such as the box containing "_boat_" and "_pier_" on the far left) is utilized as a visual prompt when fine-tuning the visual knowledge generator. The model architecture of the knowledge generator is built upon a combined large multimodality model, which composes a pre-trained vision transformer ViT-B  and the image-grounded text decoder of BLIP . The two modules are jointly trained on a generic image-text paired dataset comprising over 14 million entries and fine-tuned on the image captioning task, which delivered state-of-the-art performance.

In our visual knowledge generator, the decoder takes the ViT visual representation of the entire image as input and leverages the detected regional mask as a binary visual prompt. This prompt aids in filtering out the background and directing attention toward the relational foreground. The generation of format-free knowledge from the decoder is supervised by the language modeling loss \(_{}\), which further refines visual attention during the knowledge generation process. As a result, our approach facilitates the production of format-free outcomes that extend beyond the conventional sub-verb-obj form. Besides, to improve information variety, we introduce an amplifying penalty factor for highly similar knowledge generation. For any two generated sequences \(T_{a}\) and \(T_{b}\) describing image \(_{i}\),

\[_{}=}_{N_{i}}(- (1-(s(T_{a},T_{b})-))), \]

where \(N_{i}\) is the number of generated knowledge of image \(_{i}\), \(s(T_{a},T_{b})\) indicates the semantic cosine similarity, and \(\) is a hyper-parameter set as 0.01 controlling the penalty on sequences with only slight difference (e.g. "_dog chasing the man_" and "_dog licking the man_") to be relatively small.

The training objective \(_{l}\) of the format-free visual knowledge generator is formulated as

\[_{l}=_{}+(1-)_{}, \]

where \(\) is a weighting hyper-parameter we set as 0.7. The trained relational region detector and visual knowledge generator are combined during inference. Given any image \(\), the open relational region detector first detects a flexible number of open relations regions of interest, then each detected region \(\) is passed to the format-free visual knowledge generator, where a relation-oriented format-free knowledge phrase (such as "_flying jet leaving behind smoke_" in Figure 1) is generated to describe the given visual focus subarea \(\) of the image. To further encourage within-sequence language variety during inference, we leverage the contrastive decoding strategy from , which improves over nucleus sampling and beam search.

### Diversity-driven Data Enhancement

The training data for relational knowledge extraction usually exhibits a long-tail distribution, where more prevalent but simple relations such as _in_, _on_, and _wear_ dominate the training set . Consequently, the model trained with such a biased dataset may render limited, and repetitive knowledge. As a remedy, we propose two data enhancement techniques to optimize the data distribution. As the foundational measure for given relation \(r\)'s importance, we design a grid TF-IDF+ score \(_{r}\):

\[_{r}=((*_{1}}))^{_{2}}, \]

where \(N\) is the total number of knowledge phrases in the datasets, \(f_{r}\) is the number of occurrences of the relation \(r\), \(_{1}\) and \(_{2}\) are the grid scales whose values are selected based on \(f_{r}\).

Random dropping on low-quality data.We first remove repeated knowledge descriptions in the same image and then randomly drop descriptions that contain frequently occurring yet meaningless relations with a low \(_{r}\) (e.g., "_people on ground_") from the original dataset. Specifically, if the \(_{r}\) of the relation in a description is relatively low, i.e., 0.4, we remove it at a random dropping rate of 0.5. This process repeats for all descriptions in an image until the remaining set is 0.6 times the size of the original training set. Consequently, the training data bias is mitigated by removing low-quality data.

**Data augmentation with external knowledge resources.** For the relations with high TF-IDF+ scores, we leverage external knowledge resources from both non-parametric (i.e., ConceptNet ) and parametric (i.e., COMET ) knowledge resources to promote diverse knowledge generation . \(\)_Enhance Relation Recognition_: For each training description with a high TF-IDF+ score, we perform semantic parsing to get all the objects and complement additional relations (e.g., "_rest_" in Figure 1) between each pair of them by mapping the nodes and retrieving edges from the ConceptNet. Each retrieved knowledge triplet is converted to a knowledge phrase and added to the training set for generator training. With this introduced external knowledge, the knowledge generator ultimately yields a more robust and detailed representation of the underlying visual information of objects. This, in turn, bolsters the relation recognition of the visual knowledge generator. \(\)_Boost Entity Perception_: For the description with the highest-scored TF-IDF+ relation given each image, we also leverage ConceptNet to enrich similar objects (e.g., "_jet_") to the original object (e.g., "_plane_"). Additionally, we further introduce new entities (e.g., "_smoke_" in Figure 1) and attribute descriptions (e.g., "_blue_") by prompting the pre-trained attribute commonsense branch of the COMET model (Refer to Appendix A for more details). The entity-based enrichment potentially helps in boosting entity understanding and at the same time enhances the occurrence of important but rare relations in the training set.

### Implementation Details

Our training data are built based on Visual Genome  and its relation-enhanced version Dense Relational Captioning . Each sample includes an image identified by a unique ID and a set of relational descriptors describing interactions among objects in the image. Specifically, each relational descriptor includes the full description text, the subject and object names contained in the description text, the relation between them, as well as the bounding box coordinates of the subject and object. The dataset statistic information is summarized in Table 8 in the Appendix B.

Our model is implemented in PyTorch  and trained on two Quadro RTX 8000 GPUs. The open relational region detector is initialized from the ResNet50-FPN backbone, then finetuned for another 20 epochs with the relational bounding box. The model detects a maximum of 30 bounding boxes for each image with the highest confidence to avoid misleading noises. The format-free visual knowledge generator is initialized from BLIPbase with the basic ViT-B/16 and finetuned for 20 epochs. Full details on learning parameters can be referred to in Appendix C.

## 4 Evaluation

In this section, we directly evaluate the extracted open visual knowledge from OpenVik from two perspectives: (1) knowledge generation performance with traditional generative metrics and in-depth knowledge quality assessment; (2) comparison with existing knowledge sources. Besides, ablation studies are conducted to study the influence of diversity design on the generated knowledge and data.

### Evaluation on Generated Knowledge

**Generation performance**. To directly evaluate the visual knowledge generator, we compare the knowledge generated by OpenVik with a variety of baselines, including scene graph generation [52; 63; 44; 17] (of which Ov-SGG employs an open vocabulary), dense relational captioning , and region captioning [20; 65; 27; 26]. Evaluation metrics are traditional language generation measures such as BLEU, ROUGE-L, and METEOR. The results, displayed in the left side of Table 3, reveal that OpenVik outperforms captioning-based approaches and yields results on par with the best scene graph generation baseline. These findings underscore the effectiveness of the format-free visual knowledge generator through relation-oriented prompting of the large multimodality model.

**In-depth knowledge quality**. To more thoroughly evaluate the quality and richness of the format-free visual knowledge extraction, beyond simply evaluating it as a language generation model with the limitation of training data, we incorporate four additional metrics , which delve into an in-depth quality evaluation of the extracted visual knowledge from four distinct perspectives:

* _Validity_ (\(\)): whether the generated visual knowledge is valid to human.
* _Conformity_ (\(\)): whether the generated knowledge faithfully depicts the scenarios in the images.
* _Freshness_ (\(\)): the novelty of the knowledge, i.e., the proportion not present in the training set.
* _Diversity_ (\(\)): the language variance between a randomly sampled pair of knowledge pieces.

Among the four metrics, both the validity and conformity metrics involve human annotators. We randomly selected 100 images as the evaluative subset. Details regarding the scoring guidance and the interface provided to the annotators can be found in Appendix D. The remaining metrics, i.e., freshness and diversity, are calculated automatically. The in-depth knowledge quality evaluation results are displayed in the right part of Table 3, where the average pairwise Cohen's \(\) on human evaluation results is 0.76 (good agreement). The findings demonstrate that trained with the diversity-enhanced datasets, the format-free visual knowledge extracted by OpenVik significantly outperforms other types of baselines in terms of all four metrics. The improvement of diversity, in particular, reaches 14% relatively compared with the inference results from the second runner DenseCap, indicating the advantage of OpenVik in generating rich and comprehensive visual knowledge.

### Comparison with Existing Knowledge Sources

We compare the extracted visual knowledge with the non-parametric knowledge in the existing knowledge graph (KG) and the parametric knowledge from the large language model (LLM). The comparison insights from the three knowledge resources are shown in the Venn Diagram in Figure 2.

**Compare with non-parametric knowledge.** We take ConceptNet  as the representative in the comparison with non-parametric knowledge. To map the knowledge generated by OpenVik to ConceptNet, we parse the knowledge into triplets and associate the endpoints of these triplets with

    &  &  \\   & BLEU\(\) & ROUGE-L\(\) & METEOR\(\) & Validity\(\) & Conformity\(\) & Freshness\(\) & Diversity\(\) \\   \\  IMP  & 0.075 & 0.123 & 0.118 & 0.800 & 0.823 & 0.676 & 0.316 \\ Neural Motifs  & 0.229 & 0.283 & **0.273** & 0.822 & 0.767 & 0.667 & 0.349 \\ UhuisSGG  & 0.217 & 0.258 & 0.194 & 0.739 & 0.733 & 0.666 & 0.357 \\ Ov-SGG  & 0.167 & 0.210 & 0.183 & 0.712 & 0.633 & 0.693 & 0.413 \\   \\  MTTSNet+REM  & 0.240 & 0.226 & 0.228 & 0.897 & 0.852 & 0.754 & 0.375 \\   \\  DensCap  & 0.248 & 0.245 & 0.196 & 0.883 & 0.843 & 0.790 & 0.543 \\ Sub-GC  & 0.272 & 0.263 & 0.221 & 0.892 & 0.871 & 0.795 & 0.547 \\ BLIP  & 0.264 & 0.266 & 0.252 & 0.886 & 0.855 & 0.760 & 0.531 \\ BLIP2  & 0.275 & **0.285** & 0.257 & 0.892 & 0.871 & 0.766 & 0.535 \\   \\  OpenVik & **0.280** & 0.283 & 0.250 & **0.907** & **0.883** & **0.809** & **0.619** \\   

Table 3: Knowledge comparison of OpenVik and baselines on performance and in-depth quality (%).

Figure 2: The Venn diagram of knowledge comparison between the open visual knowledge from OpenVik with the non-parametric knowledge from existing knowledge graph (i.e., ConceptNet) and parametric knowledge from large language model (i.e., COMET).

nodes in ConceptNet. Then we calculate the similarity of embeddings3 between the parsed relation and all the edge relations among the mapped nodes in ConceptNet. If the similarity score exceeds a predetermined threshold, i.e., 0.75, we consider the mapping successful. As illustrated in Figure 2, we observe that compared with the non-parametric knowledge in KG, the extracted visual knowledge captures richer and more meaningful spatial details, e.g., "_three layer cake on table_", and motion dynamics, e.g., "_baby elephants walking around adventurous wood_".

**Compare with parametric knowledge.** We compare with parametric knowledge contained in LLM by prompting the gpt-3.5-turbo4 model with the object information in the image. The prompt template used is detailed in Appendix E. The mapping process follows the approach mentioned earlier. It is found that compared with the parametric knowledge in LLM, the extracted visual knowledge exhibits unique fine-grained visual details, e.g., "_red sticker on fence_", and provides precise scene information, e.g., "_the light shining from bright black background_".

### Ablation Study

**The influence on knowledge quality with information variety regularization and data strategies.** We conducted ablation studies to evaluate the effectiveness of the information variety regularizer, \(_{}\), and our diversity-driven data enhancement strategies. This involves an in-depth assessment of knowledge quality on the same evaluation subset. The results are presented in Figure 3. It is evident from the results that our proposed information variety design primarily impacts freshness and diversity, without compromising validity and conformity. For the freshness, the omission of data augmentation for entities and relations results in the most significant performance degradation. This implies the crucial role these strategies play in infusing novel knowledge into the generation process. As for diversity, the most notable changes in metrics are observed when the \(_{}\) and random dropping are removed. The strategy for augmenting entities and relations also plays a valuable role in enriching diversity.

**Ablation of the pre-training for the open relational region detector.** We conducted a comparison of the outcomes when loading a pre-trained detector backbone versus training the detector from scratch, as shown by the yellow bar in Figure 3. Results demonstrate a noticeable decrease in both knowledge diversity and freshness, which indicates the importance of loading the pre-trained model for region detection. This may be because omitting the pre-training step of the FasterRCNN model tends to result in the detection of more overlapping regions, which in turn causes the drop.

**The influence on dataset diversity with data strategies.** We conduct a direct analysis of the knowledge diversity of the existing datasets and our diversity-enhanced one, compared with the visual knowledge generated from OpenVik. The findings, presented in Table 4, show that the diversity-driven data enhancement strategies significantly boost knowledge diversity. Trained with this enhanced data, OpenVik can extract visual knowledge that exhibits greater diversity than that found in the _Visual Genome_ and _Relational Caps_, indicating the advantage of OpenVik to format-free visual knowledge generation and its ability to yield richer knowledge diversity.

### Case Study

We present two case studies in Figure 4 (See Appendix F for more) to showcase the format-free visual knowledge generated by OpenVik, in comparison to Visual Genome (Scene Graph and Region Description) and Relational Caps. Contrary to the rigidity of scene graphs, which strictly adhere to a

    &  & } \\  & _Visual Genome _ & _Relational Caps _ & _Diversity Enhanced (Ours)_ & OpenVik (_Ours_) \\ 
**Diversity** & 0.589 & 0.604 & 0.632 & 0.619 \\   

Table 4: Diversity of existing and enhanced datasets and generated knowledge from OpenVik.

Figure 3: The influence of information variety regularization and diversity-driven data enhancement strategies.

predefined format, OpenVik can generate knowledge with a flexible semantic structure, not strictly bound to the sub-verb-obj format (e.g., "_blue post attached to wall with white letter_"). Examples of this adaptability are highlighted in red. When compared to dense region descriptions, the relational knowledge extracted by OpenVik offers a deeper understanding of the multiple entity interactions within an image. In comparison to Relational Caps, which mainly focus on interactions between two objects, OpenVik significantly broadens the diversity of relation with vivid verbs (e.g., "_attached to_", "_adorning_"). Moreover, it introduces novel entities (e.g., "_post_", "_name_") and enriches the knowledge representation with nuanced details (e.g., "_full of_", "_striped_") that are missed by Relational Caps.

Note that we observe the unbalanced and noisy distributions within the training data can lead to errors in the knowledge produced. Viewing hallucinations as erroneous inferences based on input, the inaccuracies observed in OpenVik and similar baselines often stem from detection errors. These errors are typically caused by data biases that incorrectly associate features with a specific class or label. We further two illustrative failure cases in Figure 5. For example, a "_black speaker by flat lr_" is generated, although the speaker is not present in the image--possibly reflecting common co-occurrences within the dataset. Similarly, a ladder in the right figure has been misidentified as a towel, leading to the erroneous description of a "_blue towel hanging from dry shower_". The key to mitigating such incorrect inference is identifying the cofounder feature of class labeling.

## 5 Application

This section explores whether the extracted open visual knowledge from OpenVik can bolster reasoning and inference capabilities in multimodality downstream tasks by augmenting a baseline in the challenging zero-shot setting.

### Text-to-Image Retrieval

**Task Setting.** In the text-to-image retrieval task, a given caption is matched to a large set of candidate images, with the most relevant image returned as the result. Adopting the challenging zero-shot

Figure 4: Case study on the extracted open visual knowledge from OpenVik. Examples of format-free knowledge are highlighted in red. Compared with VG and Relational Caps, OpenVik performs better at capturing novel entities, broadening object interactions with diverse relations, and enriching the knowledge representation with nuanced descriptive details.

Figure 5: Examples of incorrectly knowledge resulting from distribution bias are highlighted.

setting, we generate the visual representation \(\) and textual representation \(\) of the given image \(\) and caption \(\) using a pre-trained clip-retrieval model . The baseline involves the image and text embedding similarly based on zero-shot CLIP Retrieval  and the fine-tuned model from BLIP .

To explore the potential of the extracted visual knowledge from OpenVik, we enrich the given caption \(\) with related contexts derived from the extracted visual knowledge. Specifically, for each query caption, we parse the caption to extract all subject-object pairs \((s,o)\) with the NLTK parser. Then \(s\) and \(o\) are mapped to the open visual knowledge, where knowledge phrases that contain relations occurring more than 30% of the time between \(s\) and \(o\) are enriched to the original caption \(\).

**Qualitative examples.** Figure 6 presents an example of OpenVik-based visual knowledge enrichment on captions. By incorporating related contexts from the generated open visual knowledge, the enriched captions convey more precise visual details, which enhances the alignment for text-image alignment.

**Quantitative results.** We curated a subset of 680 images from the testing set of the MS-COCO dataset containing parsed knowledge with at least eight nouns. This ensures an adequate degree of enrichment is achieved through the use of OpenVik. Standard image retrieval metrics, i.e., _Recall@1/5/10/_ and _Avg_, are employed to evaluate the performance. The results are presented in Table 5. It is evident that relational context enrichment leads to the average correction of more than 6.0% of the initial zero-shot, highlighting the practical benefits of extracted visual knowledge in visual reasoning tasks.

### Grounded Situation Recognition

**Task setting.** The event type prediction for the grounded situation recognition task is to predict the best match from predefined 504 event types  based on the image. We convert each candidate event verb into a description \(\): "_An image of \(verb\)_" for image description matching. Similarly to text-to-image retrieval, we include zero-shot CLIP and the fine-tuned model from BLIP as baselines.

To enrich with contextual knowledge from OpenVik, for each given verb \(v\), we find its nearest synonym in the extracted open visual knowledge and enrich the text description with the most common knowledge phrase containing it, regularized by the objects present in the image. Instead of directly concatenating the retrieved knowledge triplets to the original textual description, we employ an additive decomposition strategy: the similarity \(s(,v)\) of the candidate verb \(v\) with respect to the given image \(\) is calculated as \(s(,v)=_{d D(v)}(,v)\), where \(D(v)\) is the set of descriptors, including the original description and the enriched ones, and \(\) represents the single \(\) probability that descriptor \(d\) pertains to the image \(\).

**Qualitative examples.** Figure 7 presents a qualitative example of OpenVik-based context enrichment in the grounded situation recognition task. We observed that verbs like "shopping" and "talking" were appropriately enriched with their frequently occurring contexts from the open visual knowledge, leading to a reduced embedding distance between the description and its matching image.

**Quantitative results.** We assembled a test set of 900 samples from the testing set of GSR that included verbs such as "talking", "filming", and "picking", among others, from a list of 256 words

 
**Method** & **Accuracy** & **Precision** & **Recall** & **F\({}_{1}\)** \\ 
25-CLIP & 53.14 & 42.54 & 45.19 & 43.82 \\
25-CLIP & 53.16 & 61.84 & 62.15 & 63.38 \\
25-CLIP & 70.42 & 65.32 & 60.25 & 67.23 \\
25-CLIP & 80.25 & 72.55 & 70.61 & 71.37 \\  

Table 6: Grounded situation recognition results (%) of OpenVik enrichment compared with zero-shot baselines.

Figure 7: An example of OpenVik context enrichment on task GSR (See Appendix G.2 for more).

 
**Method** & **Recall@1** & **Recall@5** & **Recall@10** & **Avg** \\ 
25-CLIP & 36.16 & 63.47 & 78.66 & 60.10 \\
25-CLIP+25-CLIP & 40.35 & 73.29 & 84.53 & 86.12 \\
25-CLIP+25-CLIP & 63.11 & 86.30 & 91.10 & 80.17 \\
25-CLIP+25-CLIP & 65.23 & 87.71 & 79.90 & 81.61 \\  

Table 5: Text-to-image retrieval results (%) of OpenVik enrichment compared with zero-shot baselines.

that can be accurately mapped to extracted visual knowledge, as well as 138 verbs that have a fuzzy match through ConceptNet embedding comparison. The full lists of the exact and fuzzy-matched verbs are detailed in Appendix H. The evaluated metrics include Accuracy, Precision, Recall, and \(F_{1}\). The results are presented in Table 6. It can be observed that knowledge enrichment significantly outperforms the zero-shot and BLIP baselines. This suggests that the verb-related contexts introduced by OpenVik-generated knowledge are intuitive and greatly assist in understanding the semantics of event verbs, bolstered by related visual information.

### Visual Commonsense Reasoning

**Task setting.** The goal of visual commonsense reasoning is to predict an answer from four given option candidates for a given image and question. For the baseline approach, we compare the backbone model R2C from the VCR paper  and BLIP . In the visual knowledge-enhanced OpenVik Enriched approach, we perform two-level context augmentation, incorporating both entities and relations: (1) we parse the question and options to obtain all (S, 0) pairs and, for each entity pair, apply the same relation augmentation as in the image retrieval task; (2) for the V in each option, we enrich the visual context using the same method as illustrated in grounded situation recognition.

**Qualitative examples.** Figure 8 presents an example before and after applying the two-level visual knowledge-based enrichment for visual commonsense reasoning. The results indicate that visual knowledge enhances the correspondence between the correct answer and the image itself.

**Quantitative results.** We assembled a test set of 939 samples from the validation set of the VCR dataset . Each sample in this test set contains questions and answers with a minimum of five nouns and two relations, guaranteeing an adequate level of information complexity for meaningful engagement with open visual knowledge. The results can be found in Table 7. We observe that the enriched visual knowledge helps especially when solving reasoning questions on humans and their interactions with visually impressive entities, such as "_game_" in Figure 8. This enhancement results in a performance improvement above 3.0% over the zero-shot baseline.

## 6 Conclusion, Limitations, and Future Work

This work is the first exploration of a new paradigm of open visual knowledge extraction, which combines an open relational region detector to flexibly pinpoint relational regions and a format-free visual knowledge generator that generates visual knowledge by prompting a multimodality model conditioned on the region of interest. To further enhance the diversity of the generated knowledge, we explore two distinct data enhancement techniques. Extensive knowledge evaluations underscore the correctness and uniqueness of our extracted open visual knowledge, and the consistent improvements observed across various visual reasoning tasks highlight the real-world applicability of OpenVik.

While our approach has been shown effective in various scenarios, its performance at larger scales or on more diverse datasets remains to be studied. Future work could investigate its effectiveness across a broader range of tasks and contexts. Also, the current model requires fine-tuning for the visual knowledge extractor. Developing a model that can generalize well with prompt tuning or demonstration augmentation could be another interesting direction for future work.

## 7 Acknowledgments

Carl Yang was supported by the National Institute Of Diabetes And Digestive And Kidney Diseases of the National Institutes of Health under Award Number K25DK135913.

  
**Method** & **Accuracy** & **Precision** & **Recall** & \(_{i}\) \\  R2C & 56.66 & 56.73 & 56.72 & 56.72 \\ OpenVik \% \% \% & 96.60 & 60.01 & 60.03 & 60.02 \\ HLP & 62.50 & 62.50 & 62.45 & 62.47 \\ OpenVik \% \% \% \% & 67.40 & 67.54 & 67.43 & 67.48 \\   

Table 7: Visual commonsense reasoning results (%) of OpenVik context enrichment compared with zero-shot baselines.

Figure 8: An example of OpenVik context enrichment on the VCR task (See Appendix G.3 for more).