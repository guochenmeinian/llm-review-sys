# Efficient Model-Free Exploration in Low-Rank MDPs

Zakaria Mhammedi MIT

mhammedi@mit.edu &Adam Block MIT

ablock@mit.edu &Dylan J. Foster

Microsoft Research

dylanfoster@microsoft.com &Alexander Rakhlin

MIT

rakhlin@mit.edu

###### Abstract

A major challenge in reinforcement learning is to develop practical, sample-efficient algorithms for exploration in high-dimensional domains where generalization and function approximation is required. _Low-Rank Markov Decision Processes_--where transition probabilities admit a low-rank factorization based on an unknown feature embedding--offer a simple, yet expressive framework for RL with function approximation, yet existing algorithms either (1) are computationally intractable, or (2) require restrictive statistical assumptions such as latent variable structure or access to model-based function approximation. In this work, we propose the first provably sample-efficient algorithm for exploration in Low-Rank MDPs that is both computationally efficient and model-free, allowing for general function approximation while requiring no structural assumptions beyond a reachability condition that we show is substantially weaker than that assumed in prior work. Our algorithm, SpanRL, uses the notion of a _barycentric spanner_ for the feature embedding as an efficiently computable basis for exploration, performing efficient spanner computation by interleaving representation learning and policy optimization subroutines. Our analysis--which is appealingly simple and modular--carefully combines several techniques, including a new approach to error-tolerant barycentric spanner computation, and a new analysis of a certain minimax representation learning objective found in prior work.

## 1 Introduction

In reinforcement learning and control, many of the most promising application domains require the agent to navigate complex, high-dimensional state and action spaces, where generalization and function approximation is necessary. The last decade has witnessed impressive empirical success in domains where data are abundant , but when data are limited, ensuring efficient exploration in large domains is a major research question. For _statistical efficiency_, the foundations have recently begun to take shape, with a line of research providing structural conditions that facilitate sample-efficient exploration, as well as fundamental limits . _Computational efficiency_, however, remains a major challenge: outside of simple settings , existing algorithms with provable sample complexity guarantees are computationally inefficient, and typically require solving intractable non-convex optimization problems . The prospect of developing practical algorithms for exploration in high-dimensional state spaces that are both computationally and statistically efficient raises three fundamental questions:

1. What are the right computational primitives for exploration? That is, how can one efficiently represent and compute exploratory policies that allow the learner to explore the state space and gather useful data?
2. How should one leverage function approximation--for example, via representation learning--to discover such primitives in a computationally and statistically efficient fashion?
3. Given answers to the first two questions, how can one efficiently interleave function approximation and exploration to provide provably efficient algorithms?In this paper, we investigate these questions through the _Low-Rank MDP_ model . In a Low-Rank MDP, the state space is large and potentially continuous, but the transition probabilities admit an (unknown) low-rank factorization. Concretely, for a finite-horizon Low-Rank MDP with horizon \(H\), the transition densities for layer \(h[H]\) satisfy

\[T_{h}(x_{h+1} x_{h},a_{h})=^{*}_{h+1}(x_{h+1})^{}^{*}_{h}(x_{h}, a_{h}), \]

where \(^{*}_{h}(,)^{d}\) and \(^{*}_{h+1}()^{d}\) are state-action and next-state embeddings. The low-rank structure in (1) facilitates tractable exploration: if the embedding \(^{*}_{h}\) is known to the learner, one can efficiently learn a near-optimal policy with sample complexity polynomial in the feature dimension \(d\), and independent of the size of the state space ; in this regard, \(^{*}_{h}\) can be thought of as a low-dimensional _representation_ that enables sample-efficient RL. Following Agarwal et al. , we consider the challenging setting in which both \(^{*}_{h}\) and \(^{*}_{h+1}\) are _unknown_ to the learner. This formulation generalizes well-known frameworks such as the _Block MDP_ (BMDP) model , and necessitates the use of _representation learning_: the agent must learn an embedding that approximates \(^{*}_{h}\) as it explores the environment, and must use this learned embedding to drive subsequent exploration. This form of function approximation allows for great flexibility, as \(^{*}_{h}\) can be an arbitrary, nonlinear function of the state; in practice, it is common to model \(^{*}_{h}\) as a neural net .

The Low-Rank MDP is perhaps the simplest MDP structure that demands systematic exploration and nonlinear function approximation while allowing for a continuum of states, yet understanding of _efficient_ algorithm design for this model is surprisingly limited. Existing algorithms suffer from at least one of the following drawbacks:

1. Computational intractability .
2. Strong modeling assumptions (e.g., ability to model \(^{*}_{h+1}()\), which facilitates application of model-based RL techniques) ; in this work, we aim for _model-free_ methods that only require learning \(^{*}_{h}\).
3. Restrictive structural assumptions (e.g., non-negativity or latent variable structure for the embeddings in (1)) .

At the root of these limitations is the complex interplay between exploration and representation learning: the agent must learn a high-quality representation to guide in exploring the state space, but learning such a representation requires gathering diverse and informative data, which is difficult to acquire without having already explored the state space to begin with. Overcoming this challenge--particularly where computational efficiency is concerned--requires (1) representation learning procedures that lead to sufficiently expressive representations for downstream applications, (2) efficient exploration procedures that are robust to errors in learned representations, and 3) understanding the interaction between these procedures, which must be interleaved. In this work, we propose an algorithm that addresses each of these challenges, as detailed below.

Contributions.We provide the first provably computationally efficient and model-free algorithm for general Low-Rank MDPs. Our algorithm, SpanRL, uses the notion of a _barycentric spanner_ for the embedding \(^{*}_{h}\) as an efficiently computable basis for exploration, and combines this with a minimax representation learning approach . SpanRL interleaves exploration with representation learning in a layer-wise fashion, learning a new representation at each layer \(h\) using exploratory data gathered at previous layers, then uses this representation to facilitate computation of a collection of exploratory policies (a _policy cover_), which act as an approximate barycentric spanner for the features at layer \(h+1\), ensuring good coverage for subsequent iterations. SpanRL is simple and modular, and its analysis is surprisingly compact given the greater generality compared to prior work .

SpanRL can accommodate general-purpose function approximation to learn the representation \(^{*}\) (e.g., neural nets or other flexible classes) whenever a certain minimax representation learning objective  can be solved efficiently for the function class of interest. Compared to efficient algorithms from prior work, SpanRL: (1) is model-free (i.e., only requires access to a function class \(\) capable of modeling \(^{*}\), and does not need to model \(^{*}_{h+1}\)), and (2) applies to general Low-Rank MDPs, replacing strong additional assumptions such as non-negativity of the feature embeddings (so-called _latent variable_ structure) or block structure (see Table 1) with a reachability assumption that we show is substantially weaker than that assumed in prior work (see Appendix H). As a secondary benefit, the algorithm is reward-free. Our analysis carefully combines several new techniques, including (1) an error-tolerant variant of the classical barycentric spanner computation algorithm of Awerbuchand Kleinberg , and (2) a new analysis of a minimax representation learning objective introduced in Modi et al. , Zhang et al. , which shows for the first time that this objective can lead to meaningful guarantees in general Low-Rank MDPs without latent variable structure; this increased generality is meaningful, as we show in Appendix H that there is an exponential separation between our guarantees and those that require such a structure.

Organization.Section 2 formally introduces the Low-Rank MDP model and the online reinforcement learning framework we consider. In Section 3, we highlight challenges faced by previous approaches, introduce our main algorithm, SpanRL, and show how it overcomes these challenges, and then present its main sample complexity guarantee.

Comparison to ArXiv Version.After the initial submission of this work, we developed a substantially improved of the algorithm that removes the reachability assumption at the cost of a larger (but still polynomial) sample complexity guarantee. We have included this algorithm and its analysis in the ArXiv version of this paper .

## 2 Problem Setting

### Low-Rank MDP Model

We work in an episodic, finite-horizon reinforcement learning framework, where \(H\) denotes the horizon. A _Low-Rank MDP_ is a tuple \(=(,,(^{*}_{h})_{h[H]},(^{*}_{h})_{ h[H]},)\) consisting of a _state space_\(\), _action space_\(\) with \(||=A\), distribution over initial states \(()\), and mappings \(^{*}_{h+1}:^{d}\) and \(^{*}_{h}:^{d}\).3 Beginning with \(_{1}\), an episode proceeds in \(H\) steps, where for each step \(h[H]\), the state \(_{h}\) evolves as a function of the agent's action \(_{h}\) via

\[_{h+1} T_{h}(_{h},_{h}),\]

where \(T_{h}\) is a probability transition kernel, which is assumed to factorize based on \(^{*}_{b}\) and \(^{*}_{h}\). In detail, we assume that there exists a \(\)-finite measure \(\) on \(\) such that for all \(1 h H-1\), and for all \(x\) and \(a\), the function \(x^{}^{*}_{h+1}(x^{})^{}^{*}_{h}(x,a)\) is a probability density with respect to \(\) (i.e. the function is everywhere non-negative and integrates to \(1\) under \(\)). For any \(^{}\), the probability that \(_{h+1}^{}\) under \(_{h+1} T_{h}( x_{h},a_{h})\) is then assumed to follow the law

\[T_{h}(^{} x_{h},a_{h})=_{^{}}^{* }_{h+1}(x)^{}^{*}_{h}(x_{h},a_{h})(x). \]

For notational compactness, we assume (following, e.g., Jiang et al. ) that the MDP \(\) is _layered_ so that \(=_{1}_{H}\) for \(_{i}_{j}=\) for all \(i j\), where \(_{h}\) is the subset of states in \(\)

    & Comp. efficient & Model-free & General low rank & Sample comp. \\  OLIVE  & ✗ & ✓ & ✓ & \(^{5}_{h}[]}{^{2}}\) \\ (see also ) & ✗ & ✓1 & \(^{22}_{[]})}{^{10}}\) \\ FLAMBE  & ✓ & ✗ & ✓1 & \(^{2}^{5}_{[]})}{ ^{2}}\) \\ Rep-UCB  & ✓ & ✗ & ✓ & \(^{2}^{2}^{5}_{[]} )}{^{2}}\) \\ (see also ) & ✓ & ✗ & ✓ & \(^{2}^{3}^{2}^{10}[]} {^{3}^{7}^{11}}\) \\ MOFFLE 2 & ✓ & ✓ & ✗ & \(^{2}^{3}^{10}[]}{ ^{4}}\) \\ BRIEE  & ✓ & ✓ & ✗ & Block MDP & \(^{8}^{4}^{10}[]}{ ^{4}}\) \\ SpanRL (this paper) & ✓ & ✓ & ✓ & \(^{0}^{4}^{4}[]} {^{2}^{2}}\) \\   

Table 1: Comparison of sample complexity required learn an \(\)-optimal policy. \(\) denotes the feature class, and \(\) denotes an additional feature class capturing model-based function approximation. For approaches that require non-negative (latent variable) structure, \(d_{}\) [resp. \(\)] denotes the latent variable dimension [resp. the reachability parameter in the latent representation], and for BMDPs, \(\) denotes the size of the latent state space. For SpanRL, \(\) denotes the reachability parameter.

that are reachable at layer \(h[H]\). This can be seen to hold without loss of generality (modulo dependence on \(H\)), by augmenting the state space to include the layer index.

**Remark 2.1** (Comparison to previous formulations).: _Our formulation, in which the transition dynamics (2) are stated with respect to a base measure \(\), are a rigorous generalization of Low-Rank MDP formulations found in previous works [23; 1], which tend to implicitly assume the state space is countable and avoid rigorously defining integrals. We adopt this more general formulation to emphasize the applicability our results to continuous domains. However, in the special case where state space is countable, choosing \(\) as the counting measure yields \(T_{h}(^{} x_{h},a_{h})=_{x^{}}^ {*}_{h+1}(x)^{}^{*}_{h}(x_{h},a_{h})\), which is consistent with prior work._

**Policies and occupancy measures.** We define \(_{}=\{:()\}\) as the set of all randomized, Markovian policies. For a policy \(_{}\), we let \(^{}\) denote the law of \(_{1},_{1},,_{H},_{H}\) under \(_{h}(_{h})\), and let \(^{}\) denote the corresponding expectation. For any \(^{}_{h}\), we let \(^{}_{h}[^{}]=^{}_{h} ^{}\) denote the marginal law of \(_{h}\) under \(\). For \(x_{h}\), we define the _occupancy measure_\(d^{}(x):=^{}_{h}}{}(x)\) as the density of \(^{}_{h}\) with respect to \(\).

### Online Reinforcement Learning and Reward-Free Exploration

We consider a standard _online reinforcement learning_ framework where the Low-Rank MDP \(\) is unknown, and the learning agent interacts with it in _episodes_, where at each episode the agent executes a policy of the form \(:()\) and observes the resulting trajectory \((_{1},_{1}),,(_{H},_{H})\). While the ultimate goal of reinforcement learning is to optimize a policy with respect to a possibly unknown reward function, here we focus on the problem of _reward-free exploration_, which entails learning a collection of policies that almost optimally "covers" the state space, and can be used to efficiently optimize any downstream reward function [12; 33; 15; 31]. To wit, we aim to construct an _policy cover_, a collection of policies that can reach any state with near-optimal probability.

**Definition 2.1** (Policy cover).: _For \((0,1]\), a subset \(_{}\) is an \(\)-policy cover for layer \(h\) if_

\[ x_{h},_{}d^{}(x) _{^{}_{}}d^{^{}}(x). \]

We show (Appendix G) that given access to such a policy cover with constant \(\), it is possible to optimize any downstream reward function with polynomial sample complexity.

**Assumptions.** To facilitate learning a policy cover, we make the following _reachability_ assumption.

**Assumption 2.1** (\(\)-reachability).: _For any \(h[H]\) and \(x_{h}\), \(_{_{}}d^{}(x)\|^{*}_{h}(x)\|\)._

Reachability is necessary if one aims to build a policy cover that satisfies (3) uniformly for all states; without such a condition, one gives up on covering hard-to-reach states. Some notion of reachability is required in essentially all prior work on efficient model-free algorithms for Low-Rank MDPs [32; 35; 5], and was only very recently removed in the (more restrictive) BMDP setting [31; 49].

**Remark 2.2** (Comparison to other reachability-like assumptions).: _Assumption 2.1 generalizes and subsumes all previous reachability-like conditions of which we are aware [33; 46; 1; 35]. Notably, reachability is implied by the notion of feature coverage  (used in the context of transfer learning in Low-Rank MDPs), which asserts that \(_{_{}}_{}^{}[^{*}_{ h}(_{h},_{h})^{*}_{h}(_{h},_{h})^{} \), for some \(>0\). It is also implied by explorability , which is similar to feature coverage, but involves the first moments of \(^{*}_{h}\). Our reachability assumption is also weaker than that used in [1; 35] under the latent variable model, and generalizes that made for BMDPs . See Appendix H for details, as well as an exponential separation between our assumptions and analogous assumptions in [1; 35]._

Beyond reachability, we assume (following [1; 35]) for normalization that, for all \(h[H]\) and \((x,a)_{h}\), \(\|^{*}_{h}(x,a)\| 1\), and that for all \(g:_{h}\),

\[\|_{_{h}}^{*}_{h}(x)g(x)(x)\| {d}. \]

Function approximation and desiderata.We do not assume that the true features \((^{*}_{h})_{h[H]}\) or the mappings \((^{*}_{h})_{h[H]}\) are known to the learner. To provide sample-efficient learning guarantees we make use of function approximation as in prior work [3; 35], and assume access to a _feature class_\(\{:^{d}\}\) that contains \(^{*}_{h}\), for \(h[H-1]\).

**Assumption 2.2** (Realizability).: _The feature class \(\{:^{d}\}\) has \(^{*}_{h}\) for all \(h[H]\). Moreover, for all \(\), \(x\), and \(a\), it holds that \(|(x,a)| 1\)._

The class \(\) may consist of linear functions, neural networks, or other standard models depending on the application, and reflects the learner's prior knowledge of the underlying MDP. We assume that \(\) is finite to simplify presentation, but extension to infinite classes is straightforward, as our results only invoke finiteness through standard uniform convergence arguments. Note that unlike model-based approaches , we do not assume access to a class capable of realizing the features \(^{*}_{h}\), and our algorithm does not attempt to learn these features; this is why we distinguish our results as _model-free_.

For constant \(\), our goal is to learn an \(\)-policy cover using \((d,A,H,,^{-1})\) episodes of interaction. This guarantee scales with the dimension \(d\) of the feature map and the complexity \(\) of the feature class but, critically, does not depend on the size of the state space \(\); by , dependence on \(H\) and \(A=\) is necessary when \(^{*}\) is unknown. Given such a guarantee, we show in Appendix G how to optimize any downstream reward function to error \(\) with polynomial sample complexity.

Additional preliminaries.For any \(m,n\), we denote by \([mn]\) the integer interval \(\{m,,n\}\). We also let \([n][1n]\). For any sequence of objects \(o_{1},o_{2},\), we define \(o_{mn}(o_{i})_{i[mn]}\). A _partial policy_ is a policy defined over a contiguous subset of layers \([r][H]\). We denote by \(^{}_{}\{:_{h=}^{}_{h}()\}\) the set of all partial policies over layers \(\) to \(r\); note that \(_{}^{1:H}_{}\). For a policy \(^{}_{}\) and \(h[r]\), \((x_{h})\) denotes the action distribution for the policy at layer \(h\) when \(x_{h}_{h}\) is the current state. For \(1 t h H\) and any pair of partial policies \(^{1:t-1}_{},^{t}^{t:h}_{}\), we define \(_{}\,^{}^{1:h}_{}\) as the partial policy given by \((_{t}^{})(x_{})=(x_{})\) for all \(<t\) and \((_{t}^{})(x_{})=^{}(x_{})\) for all \([th]\).

We use the \(_{h}\) as shorthand to indicate that \(_{h}\) is drawn from the law \(^{}\), and likewise for \((_{h},_{h})\) and so on. For a set of partial policies \(\{^{(i)}:i[N]\}\), we define \(()\) as the random partial policy obtained by sampling \(([N])\) and playing \(^{()}\). We define \(_{}_{}\) as the random policy that selects actions in \(\) uniformly at random at each layer. We use \(\) to denote the Euclidean norm, \(_{}\) to denote the supremum norm on functions, and let \((r)^{d}\) denote the Euclidean ball of radius \(r\). We refer to a scalar \(c>0\) as an _absolute constant_ to indicate that it is independent of all problem parameters and use \(()\) to denote a bound up to factors polylogarithmic in parameters appearing in the expression.

## 3 SpanRL: Algorithm and Main Results

In this section, we present the SpanRL algorithm. We begin by describing challenges in deriving efficient, model-free algorithms using existing approaches (Section 3.1). We then formally describe SpanRL (Section 3.2) and build intuition as to how it is able to overcome these challenges, and finally state our main sample complexity guarantee (Section 3.3).

### Challenges and Related Work

Designing algorithms with provable guarantees in the Low-Rank MDP setting is challenging because of the complicated interplay between representation learning and exploration. Indeed, while there are many efficient algorithms for the so-called _linear MDP_ setting where the feature maps \((^{*}_{h})_{h[H]}\) are known (removing the need for representation learning) , these approaches do not readily generalize to accommodate unknown features. For Low-Rank MDPs, previous algorithms suffer from at least one of the following three drawbacks: (1) the algorithms are computationally inefficient; (2) the algorithms are model-based; or (3) the algorithms place strong assumptions on the MDP that are unlikely to hold in practice. To motivate the SpanRL algorithm, we briefly survey these results, highlighting several key challenges in avoiding these pitfalls.

Let us first discuss the issue of computational efficiency. While there are a number of algorithms--all based on the principle of _optimism in the face of uncertainty_--that provide tight sample complexity guarantees for Low-Rank MDPs in reward-based  and reward-free  settings, these algorithms involve intractable optimization problems, and cannot be implemented efficiently even when the learner has access to an optimization oracle for the representation class \(\). This intractability arises because these algorithms implement optimism via a "global" approach, in which the algorithm explores at each round by choosing the most optimistic value function in a certain _version space_ of candidate value functions; optimizing over this version space is challenging,as it involves satisfying non-convex constraints with a complicated dependence on the learned representation, and because the constraints are coupled globally across layers \(h[H]\).

To avoid the intractability of global optimism, several works have restricted attention to a simpler _model-based_ setting. Here, in addition to assuming that the feature maps \((^{*}_{h})_{h[H]}\) are realizable with respect to \(\), one assumes access to a second feature class \(\) capable of modeling the mappings \((^{*}_{h})_{h[H]}\); this facilitates direct estimation of the transition probability kernel \(T_{h}( x,a)\). For the model-based setting, it is possible to efficiently implement certain "local" forms of optimism , as well as certain non-optimistic exploration techniques based on policy covers . Unfortunately, model-based realizability is a restrictive assumption, and falls short of the model-free guarantees we aim for in this work; indeed, in general, one cannot hope to estimate the feature map \(^{*}_{h+1}\) without sample complexity scaling with the number of states.4

When one moves from model-based learning to model-free learning, representation learning becomes substantially more challenging--both for optimistic and non-optimistic approaches. Here, a key challenge is to develop representation learning procedures that are (1) efficient, yet (2) provide meaningful guarantees when the learned features are used downstream for exploration. To our knowledge, the only proposal for a representation learning procedure satisfying both desiderata comes from the work of Modi et al. , who introduced a promising "minimax" representation learning objective (described in detail in the sequel; cf. Algorithm 5), which Zhang et al.  subsequently showed to have encouraging empirical performance. However, to provide guarantees for this objective, both works place substantial additional restrictions on the low-rank factorization. In particular, Modi et al.  make the so-called _latent variable_ assumption , which asserts that \(^{*}_{h}\) and \(^{*}_{h}\) are non-negative coordinate-wise, and Zhang et al.  further restrict to the Block MDP model . Non-negativity is a substantial restriction, as the best non-negative factorization can have exponentially large dimension relative to the best unrestricted factorization , even when reachability is assumed (cf. Appendix H.1). The source of this restriction is the problem of how to quantify how close a learned representation \(\) is to the ground truth \(^{*}\), which depends strongly on the downstream exploration strategy. In what follows, we show that with the right exploration strategy, this challenge can be ameliorated, but prior to our work it was unclear whether the minimax objective could lead to meaningful guarantees in the absence of non-negativity.

### The SpanRL Algorithm

Our algorithm, SpanRL, is presented in Algorithm 1. The algorithm proceeds by building a policy cover layer-by-layer in an inductive fashion. For each layer \(h 2\), SpanRL uses a policy cover \(^{(h)}\) built at a previous iteration within a subroutine, RepLearn (Algorithm 5; deferred to Appendix B) to produce a feature map \(^{(h)}\) that approximates \(^{*}_{h}\). Using this feature map, the algorithm invokes a second subroutine, RobustSpanner (Algorithm 2 in Appendix B) to produce a collection of policies \(_{1},,_{d}\) that act as a _barycentric spanner_ for the feature map, ensuring maximal coverage in a certain sense; given these policies, a new policy cover for layer \(h+2\) is formed via \(^{(h+2)}=\{_{i}_{h+1}_{}:i[d]\}\). To invoke the RobustSpanner subroutine, SpanRL makes use of additional subroutines for policy optimization (PSDP; Algorithm 3 in Appendix B) and estimation of certain vector-valued functionals (EstVec; Algorithm 7 in Appendix B). We now describe each component of the algorithm in detail, highlighting how they allow us to overcome the challenges in the prequel.

Barycentric spanners.At the heart of SpanRL is the notion of a _barycentric spanner_ as an efficient basis for exploration. We begin by defining a barycentric spanner for an abstract set.

**Definition 3.1** (Awerbuch and Kleinberg ).: _Given a set \(^{d}\) such that \(()=^{d}\), we say that a set \(\{w_{1},,w_{d}\}\) is a \((C,)\)-approximate barycentric spanner for \(\) if for every \(w\), there exist \(_{1},,_{d}[-C,C]\) such that \(\|w-_{i=1}^{d}_{i}w_{i}\|\).5_

The utility of barycentric spanners for reward-free exploration is highlighted in the following lemma.

**Lemma 3.1**.: _Suppose that Assumption 2.1 holds. If \(_{}\) is a collection of policies such that \(\{^{}[^{*}_{h}(_{h},_{h})] \}^{d}\) is a \((C,)\)-approximate barycentric spanner for \(_{h}:=\{^{}[^{*}_{h}(_{h},_{h}) ]_{}\}\) with \(\), then \(\) is an \(\)-policy cover for layer \(h+1\) with \(=(2dC)^{-1}\)._

[MISSING_PAGE_FAIL:7]

1. For all \(^{d}\) with \(=1\), the output \(_{}:=()\) satisfies \(^{}w^{z_{}}_{z}^{}w^{ z}-\).
2. For all \(z\), the output \(_{z}:=(z)\) satisfies \(_{z}-w^{z}\).

The RobustSpanner algorithm (Algorithm 2) computes a \((C,)\)-approximate spanner for \(\) using \(O(d(d/))\) total calls to LinOpt and LinEst. RobustSpanner is an error-tolerant variant of the classical spanner computation algorithm of Awerbuch and Kleinberg , which was originally introduced and analyzed for spanner computation with an _exact_ linear optimization oracle. Tolerance to approximation errors in the linear optimization oracle is critical for our application to RL, where additive errors will arise in sampling trajectories, as well as estimating the feature maps \((_{h}^{*})_{h[H]}\). RobustSpanner achieves error tolerance by perturbing the vectors returned by LinOpt(\(\)) in the direction of \(\), which amounts to running the classical algorithm on an \(\)-fattening of \(\), and is necessary in order to ensure that the approximation error of LinOpt does not swamp the signal in directions \(\) in which \(\) is too "skinny." This technique may be of independent interest; see Appendix C for additional details and formal guarantees.

Representation learning.Ideally, we would like to use RobustSpanner to construct a barycentric spanner for the set \(\{^{}[_{h}^{*}(_{h},_{h})]_{}\}\) with \(=_{}\). Because we do not have access to \(_{h}^{*}\), we instead apply RobustSpanner with \(\{^{}[^{(h)}(_{h},_{h}) ]_{}\}\), where \(^{(h)}\) is a learned representation. We now describe how the feature map \(^{(h)}\) is learned, then show how to use these learned features to efficiently implement the oracles LinOpt(\(\)) and LinEst(\(\)).

To learn a representation for layer \(h\), we use the Replenarn algorithm (Algorithm 5), which was originally introduced in Modi et al. , Zhang et al. . The algorithm gathers a collection of triples \((_{h},_{h},_{h+1})\) by rolling in to \(_{h}\) with a policy sampled uniformly from the policy cover \(^{(h)}\) and selecting \(_{h}\) uniformly at random. Using this dataset, the algorithm solves a sequence of adversarial training sub-problems (Line 9 of Algorithm 5) which involve the feature class \(\) and an auxiliary discriminator class \(:\). As we discuss in detail in the sequel, these sub-problems, described in (7), are amenable to standard gradient-based training methods. The sub-problems are designed to approximate the following "idealized" max-min-max representation learning objective:

\[^{(h)}*{arg\,min}_{}_{f}_{w}^{(^{(h)})_{^{*} _{}}}[((_{h},_{h})^{}w- [f(_{h+1})_{h},_{h})]^{2}]. \]

The intuition for this objective comes from the fact that in a Low-Rank MDP, for any function \(f:\), the quantity \([f(_{h+1})_{h}=x,_{h}=a]\) is linear in \(_{h}^{*}(x,a)\). Thus, if \(\) is sufficiently expressive, we may hope that \(^{(h)}\) and \(^{*}\) are close. We adopt the simple discriminator class \(=\{\,x_{a}^{}(x,a) (1),\,\}\). We show that solving (6) with this choice for \(\), which is simpler than that in Modi et al. , Zhang et al. , yields an approximation guarantee for \(^{(h)}\) that is suitable for downstream use in spanner computation for general Low-Rank MDPs.

**Remark 3.1** (Improved analysis of Replenarn).: _To facilitate an analysis of SpanRL that does not require reachability assumptions, we use slightly different parameter values for_ Replenarn _than in Modi et al. , Zhang et al. , and provide a tighter sample complexity bound (Theorem E.1) which may be of independent interest._

_In more detail, prior work shows that the_ Replenarn _algorithm solves a variant of (6) with \(w(d^{1/2}(^{-1}))\), where \(>0\) is the desired bound on mean-squared error. Due to the polynomial dependence on \(^{-1}\), such a guarantee would lead to vacuous guarantees when invoked within our analysis of_ SpanRL_. Our improved analysis of_ Replenarn_, which is based on a determinantal potential argument, shows that \(w((d))\) suffices. A secondary benefit of our improved bound is a faster rate with respect to the number of trajectories._

Putting everything together.Having learned \(^{(h)}\) using RepLearn, in SpanRL we apply RobustSpanner with \(:=\{^{}[^{(h)}(_{h},_{h})] _{}\}\), \(=_{}\), and \(C=2\); that is, we plug-in the learned representation \(^{(h)}\) for the true representation \(_{h}^{*}\).7 With this choice, implementing LinOpt entails (approximately) solving \(*{arg\,max}_{_{}}^{}[^{ }^{(h)}(_{h},_{h})]\) for a given \((1)\), and implementing the LinEst oracle entails estimating \(^{}[^{(h)}(_{h},_{h})]\) for a given \(_{}\). We instantiate\(()\) as the Monte Carlo algorithm \(\) (Algorithm 7), which simply samples trajectories according to \(\) and returns the sample average of \(^{(h)}(_{h},_{h})\). To implement \(()\), we appeal to \(\) (Algorithm 3). \(\), given an arbitrary reward function \(r_{1:h}:\) and a function class \(\{g:\}\) capable of realizing all possible value functions induced by these rewards, can use the policy covers \(^{(1:h)}\) to efficiently compute a policy \(=(h,r_{1:h},,^{(1:h)},n)\) that approximately solves \(_{_{h}}^{}[_{t=1}^{h}r_{t}(_{t},_{t})]\), and does so using polynomially many episodes; see Appendix D for details and formal guarantees.8 Thus, implementing \(()\) is as simple as invoking \(\) with the rewards

\[r_{t}(x,a;)\{^{(h)}(x,a)^{ },&t=h,\\ 0,&.\]

With this, we have all the ingredients needed for spanner computation, and the algorithm is complete.

### Main Guarantee for SpanRL

The following result is the main sample complexity guarantee for \(\) (Algorithm 1).

**Theorem 3.2** (Main theorem for \(\)).: _Let \((0,1)\) be given, and suppose that realizability holds (Assumption 2.2) and that reachability (Assumption 2.1) is satisfied with parameter \(>0\). If \(=}\) and \(=(A,H,d,(||/))\) is sufficiently large, then the policies \(^{(1:H)}\) produced by \((,,,)\) are a \((,0)\)-policy cover with probability at least \(1-\). The total number of episodes used by \(\) is at most:_

\[}(A^{4}d^{9}H^{4}(d+(||/)) 1/ ^{2}).\]

Theorem 3.2 is the first provable, model-free sample complexity guarantee for general Low-Rank MDPs that is attained by an efficient algorithm. Prior to our work, all efficient model-free algorithms required non-negative features (latent variable structure) or stronger assumptions , even in the presence of similar reachability assumptions; see Table 1.

**Remark 3.2** (On the reachability assumption).: _While the reachability assumption is shared by the best prior efficient algorithms , which require non-negativity in addition to this assumption, it is natural to ask to what extent reachability restricts the generality of the Low-Rank MDP model. In Appendix H, we show that even when reachability holds, the embedding dimension in our model can be exponentially smaller than the best embedding dimension for the best non-negative (latent variable) embedding . Hence, our results are meaningfully more general than prior work._

While our guarantee is polynomial in all relevant problem parameters, improving the dependence further (e.g., to match that of the best known inefficient algorithms) is an interesting direction for future research, as is removing the reachability assumption.

Application to reward-based RL.By using the policy cover produced by \(\) within \(\) (Algorithm 3), we can optimize any downstream reward function to error \(\) using \((d,A,H,||) 1/^{2}\) episodes. See Appendix G for details.

Efficiency and practicality.We observe that \(\) is simple and practical. Defining \(_{}(,w,f)_{(x,a,x^{}) }((x,a)^{}w-f(x^{}))^{2}+\|w\|^{2}\), where \(\) is a dataset consisting of \((_{h},_{h},_{h},_{h+1})\) tuples, the algorithm is provably efficient whenever the adversarial objective

\[f^{(t)}*{arg\,max}_{f}_{}\{ _{w}_{}(^{(t)},w,f)-_{}_{}(,,f)\}, \]

in Line 9 of \(\) (Algorithm 5), can be implemented efficiently (note that by the definition of \(_{}\), the "inner" minima over \(w\) and \(\) in (7) can be solved in closed form). This objective was also assumed to be efficiently solvable in Modi et al. , Zhang et al.  and was empirically shown to be practical in ; note that the objective is amenable to standard gradient-based optimization techniques, and that \(\) can be over-parameterized. While a detailed experimental evaluation is outside of the scope of this paper, we are optimistic about the empirical performance of the algorithm in light of the encouraging results based on the same objective in Zhang et al. Outside of representation learning, the only overhead in SpanRL is the RobustSpanner subroutine, which has polynomial runtime. Indeed, RobustSpanner requires only polynomially many calls to the linear optimization oracle, instantiated as PSDP, which is efficient whenever standard least-squares regression problems based on the class \(\) can be solved efficiently, analogous to .

Analysis and proof techniques.The proof of Theorem 3.2, which is given in Appendix F, is appealing in its simplicity and modularity. The crux of the proof is to show that the representation learning guarantee in (6) is strong enough to ensure that the downstream spanner computation in RobustSpanner succeeds. It is straightforward to show that spanner computation would succeed if we had access to an estimated representation that \(^{(h)}\) that approximates \(^{*}_{h}\) point-wise (i.e., uniformly for all \((x,a)\) pairs), but the key challenge is that the guarantee in (6) only holds _on average_ under the roll-in distribution \((^{(h)})\). Prior works that make use of the same representation learning objective (BRIEE  and MOFFLE ) do not make use of spanners; instead, they appeal to exploration strategies based on elliptic bonuses, addressing the issue of approximation errors through additional assumptions (non-negativity of the factorization for MOFFLE, and Block MDP structure for BRIEE). Perhaps the most important observation in our proof is that barycentric spanners are robust to the average-case approximation error guarantee in (6) as-is, without additional structural assumptions. Intuitively, this benefit seems to arise from the fact that the spanner property only concerns the _first moment_ of the feature map \(^{*}\), while algorithms based on elliptic bonuses require approximation guarantees for the _second moment_; understanding this issue more deeply is an interesting question for future work. Another useful feature of our proof is to show that the notion of reachability in Assumption 2.1, which generalizes and extends all previous reachability conditions in the Low-Rank MDP and Block MDP literature , is sufficient to build a policy cover. We anticipate that this observation will find broader use.