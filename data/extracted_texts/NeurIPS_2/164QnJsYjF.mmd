# Dense Associative Memory Through the Lens of Random Features

Benjamin Hoover

IBM Research & Georgia Tech

benjamin.hoover@ibm.com

&Duen Horng Chau

Georgia Tech

polo@gatech.edu

&Hendrik Strobelt

IBM Research & MIT-IBM

hendrik.strobelt@ibm.com

&Parikshit Ram

IBM Research

parikshit.ram@ibm.com

&Dmitry Krotov

IBM Research

krotov@ibm.com

###### Abstract

Dense Associative Memories are high storage capacity variants of the Hopfield networks that are capable of storing a large number of memory patterns in the weights of the network of a given size. Their common formulations typically require storing each pattern in a separate set of synaptic weights, which leads to the increase of the number of synaptic weights when new patterns are introduced. In this work we propose an alternative formulation of this class of models using random features, commonly used in kernel methods. In this formulation the number of network's parameters remains fixed. At the same time, new memories can be added to the network by modifying existing weights. We show that this novel network closely approximates the energy function and dynamics of conventional Dense Associative Memories and shares their desirable computational properties.

## 1 Introduction

Hopfield network of associative memory is an elegant mathematical model that makes it possible to store a set of memory patterns in the synaptic weights of the neural network . For a given prompt \(_{i}(t=0)\), which serves as the initial state of that network, the neural update equations drive the dynamical flow towards one of the stored memories. For a system of \(K\) memory patterns in the \(D\)-dimensional binary space the network's dynamics can be described by the temporal trajectory \(_{i}(t)\), which descends the energy function

\[E=-_{=1}^{K}_{i=1}^{D}_{i}^{}_{i}^{2} \]

Here \(_{i}^{}\) (index \(=1...K\), and index \(i=1...D\)) represent memory vectors. The neural dynamical equations describe the energy descent on this landscape. In this formulation, which we call the **memory representation**, the geometry of the energy landscape is encoded in the weights of the network \(_{i}^{}\), which coincide with the memorised patterns. Thus, in situations when the set of the memories needs to be expanded by introducing new patterns one must introduce additional weights.

Alternatively, one could rewrite the above energy in a different form, which is more commonly used in the literature. Specifically, the sum over the memories can be computed upfront and the energy can be written as

\[E=-_{i,j=1}^{D}T_{ij}_{i}_{j}, T_{ ij}=_{=1}^{K}_{i}^{}_{j}^{} \]In this form one can think about weights of the network being the symmetric tensor \(T_{ij}\) instead of \(_{i}^{}\). One advantage of formulating the model this way is that the tensor \(T_{ij}\) does not require adding additional parameters when new memories are introduced. Additional memories are stored in the already existing set of weights by redistributing the information about new memories across the already existing network parameters. We refer to this formulation as **distributed representation**.

A known problem of the network (eqs. (1) and (2)) is that it has a small memory storage capacity, which scales at best linearly as the size of the network \(D\) is increased . This limitation has been resolved with the introduction of Dense Associative Memories (DenseAMs), also known as Modern Hopfield Networks . This is achieved by strengthening the non-linearities (interpreted as neural activation functions) in eq. (1), which can lead to the super-linear and even exponentially large memory storage capacity . Using continuous variables \(^{D}\), the energy is defined as1

\[E=-Q_{=1}^{K}FS^{},( ), \]

where the function \(:^{D}^{D}\) is a vector function (e.g., a sigmoid, a linear function, or a layernorm), the function \(F()\) is a rapidly growing separation function (e.g., power \(F()=()^{n}\) or exponent), \(S[,^{}]\) is a similarity function (e.g., a dot product or a Euclidean distance), and \(Q\) is a scalar monotone function (e.g., linear or logarithm). For instance, in order to describe the classical Hopfield network with binary variables (eq. (1)) one could take: linear \(Q\), quadratic \(F()=()^{2}\), dot product \(S\), and a sign function for \(g_{i}=sign(x_{i})=_{i}\). There are many possible combinations of various functions \(,F(),S(,)\) that lead to different models from the DenseAM family ; many of the resulting models have proven useful for various problems in AI and neuroscience . Diffusion models have been linked to even more sophisticated forms of the energy landscape .

From the perspective of the information storage capacity DenseAMs are significantly superior compared to the classical Hopfield networks. At the same time, most2 of the models from the DenseAM family are typically formulated using the memory representation, and for this reason

Figure 1: The **Distributed Rep**resentation for **D**ense **A**ssociative **M**emory (DrDAM) approximates both the energy and fixed-point dynamics of the traditional **M**emory **R**epresentation for **D**ense **A**ssociative **M**emory (**MrDAM**) while having a parameter space of constant size. A) Diagram of DrDAM using a **basis function** parameterized by random features (e.g., see eq. (8)). In the distributed representation, adding new memories does not change the size of the memory tensor. B) Comparing energy descent dynamics between DrDAM and MrDAM on 3x64x64 images from Tiny Imagenet . Both models are initialized on queries where the bottom two-thirds of pixels are occluded with zeros; dynamics are run while clamping the visible pixels and their collective energy traces shown. DrDAM achieves the same fixed points as MrDAM, and these final fixed points have the same energy. The energy decreases with time for both MrDAM and DrDAM, although the dependence of the energy relaxation towards the fixed point is sometimes different between the two representations. Experimental setup is described in appendix D.

require introduction of new weights when additional memory patterns are added to the network. The main question that we ask in our paper is: _how can we combine superior memory storage properties of DenseAMs with the distributed (across synaptic weights) formulation of these models in the spirit of classical Hopfield networks_ (eq. (2))? If such a formulation is found, it would allow us to add memories to the existing network by simply recomputing already existing synaptic weights, without adding new parameters.

A possible answer to this question is offered by the theory of random features and kernel machines. Given an input domain \(\), kernel machines leverage a positive definite _Mercer kernel function_\(k:_{+}\) that measures the similarity between pairs of inputs. The renowned "kernel trick" allows one to compute the inner-product

\[k(,^{})=(), (^{})=_{=1}^{Y}_{}() _{}(^{}) \]

between two inputs \(,^{}\) in a rich feature space defined by the feature map \(()\) without ever explicitly realizing the feature map \(()\). Various machine learning models (such as support vector machines , logistic regression, and various others ) can be learned with just access to pairwise inner-products, and thus, the kernel trick allows one to learn such models in an extremely expressive feature space. Kernel functions have been developed for various input domains beyond the Euclidean space such as images, documents, strings (such as protein sequences ), graphs (molecules , brain neuron activation paths) and time series (music, financial data) . Common kernels for Euclidean data are the radial basis function or RBF kernel \(k(,^{})=(-\|-^{} \|_{2}^{2})\) and the polynomial kernel \(k(,^{})=(,^{} +b)^{p}\). To appreciate the expressivity of these kernel machines, note that, for input domain \(^{D}\), the RBF kernel corresponds to an infinite dimensional feature space (\(Y=\)) and the polynomial kernel to a \(O(D^{p})\) dimensional feature space.

Interpreting the composition of the separation and similarity functions in eq. (3) as the left hand side of the kernel trick eq. (4) we can map the energy into the feature space, using appropriately chosen feature maps. Subsequently, the order of the summations over memories and features can be swapped, and the sum over memories can be computed explicitly. This makes it possible to encode all the memories in a tensor \(T_{}\), which we introduce in section SS 3, that contains all the necessary information about the memories. The energy function then becomes defined in terms of this tensor only, as opposed to individual memories. This functionality is summarized in fig. 1. Additionally, we show examples of retrieved Tiny ImageNet images that have been memorised using the original DenseAM model, which we call MrDAM, and the "fetaurized" version of the same model, which we call DrDAM (please see the explanations of these names in the caption to fig. 1). These examples visually illustrate that mapping the problem into the feature space preserves most of the desirable computational properties of DenseAMs, which normally are defined in the "kernel space".

**Contributions:**

* We propose a novel approximation of a DenseAM network utilizing random features commonly used in kernel machines. This novel architecture does not require the storage of the original memories, and can incorporate new memories without increasing the size of the network.

Figure 2: DrDAM achieves parameter compression over MrDAM, successfully storing \(20\) different 64x64x3 images from TinyImagenet  and retrieving them when occluding the lower 40% of each query. The memory matrix of MrDAM is of shape \((20,12288)\) while the memory tensor of DrDAM is of shape \(Y=2 10^{5}\), a \(\)20% reduction in the number of parameters compared to MrDAM; all other configurations for this experiment match those in appendix D. Further compression can be achieved with a higher tolerance for DrDAM’s retrieval error, smaller \(\), and fewer occluded pixels, see § 4. **Top:** Occluded query images. **Middle:** Fixed-point retrievals from DrDAM. **Bottom:** (ground truth) Fixed-point retrievals of MrDAM.

* We precisely characterize the approximation introduced in the energy descent dynamics by this architecture, highlighting the different critical factors that drive the difference between the exact energy descent and the proposed approximate one.
* We validate our theoretical guarantee with empirical evaluations.

In the past, kernel trick has been used for optimizing complexity of the attention mechanism in Transformers , and those results have been recently applied to associative memory , given the various connections between Transformers and DenseAMs [4; 20]. Existing studies [18; 19] focus on settings when attention operation or associative memory retrieval is done in a single step update. This is different from our goals here, which is to study the recurrent dynamics of the associative memory updates and convergence of that dynamics to the attractor fixed points. Iatropoulos et al.  propose kernel memory networks which are a recurrent form of a kernel support vector machine, and highlight that DenseAM networks are special cases of these kernel memory networks. Making a connection between nonparametric kernel regression and associative memory, Hu et al.  propose a family of provably efficient sparse Hopfield networks [23; 24], where the dynamics of any given input are explicitly driven by a subset of the memories due to various entropic regularizations on the energy. DenseAMs have been also used for sequences [25; 26; 24]. To reduce the complexity of computing all the pairs of \(F(S[,])\) for a given set of memories and queries, Hu et al.  leverage a low-rank approximation of this separation-similarity matrix using polynomial expansions. The kernel trick has also recently been used to increase separation between memories (with an additional learning stage to learn the kernel), thereby improving memory capacity . There are also very recent theoretical analysis of the random feature Hopfield networks [29; 30], where their focus in on the construction of memories using random features. Kernels are also related to density estimation , and recent works have leveraged a connection between mixtures of Gaussians and DenseAMs for clustering [32; 33]. Lastly, random features have been used for biological implementations of both Transformers and DenseAMs [34; 35].

To the best of our knowledge there is no rigorous theoretical and empirical comparison of DenseAMs and their distributed (featurized) variants in recurrent memory storage and retrieval settings, as well as results pertaining to the recovery of the fixed points of the energy descent dynamics. This is the main focus of our work.

## 2 Technical background

Given the energy function in eq.3, a variable \(\) is updated in the forward pass through the "layers" of this recurrent model such that its energy decreases with each update. If the energy is bounded from below, this ensures that the input will (approximately) converge to a local minimum. This can be achieved by performing a "gradient descent" in the energy landscape. Considering the continuous dynamics, updating the input \(\) over time with \(d/dt\), we need to ensure that \(dE/dt<0\). This can be achieved by setting \(d/dt-_{}E\).

Discretizing the above dynamics, the update of an input \(\) at the \(t\)-th recurrent layer is given by:

\[^{(t)}^{(t-1)}-^{(t-1)}_{}E^ {(t-1)}, \]

where \(^{(t)}\) is a (step dependent) step-size for the energy gradient descent, \(E^{(t)}\) is the energy of the input after the \(t\)-th layer, and the input to the first layer \(^{(0)}\). The final output of the associative memory network after \(L\) layers is \(^{(L)}\).

DenseAMs significantly improve the memory capacity of the associative memory network by utilizing rapidly growing nonlinearity-based separation-similarity compositions such as \(F(S[,^{}])=((,^{} ))\) or \(F(S[,^{}])=(-}{{2}}\|-^{}\|_{2})\) or \(F(S[,^{}])=(<,^{}> )^{p},p>2\), among other choices, with \(>0\) corresponding to the _inverse temperature_ that controls how rapidly the separation-similarity function grows. However, these separation-similarity compositions do not allow for the straightforward simplifications as in eq.2, except for the power composition. For a general similarity function, the update based on gradient descent over the energy in eq.3 is given by:

\[_{}E=-.|_{y=_{}F(S[^ {},()])}_{=1}^{K}(.|_{s=S[^{},()]}^{ },)}{d}|_{=()}. ()}{d}) \]For example, with \(Q()=(1/)()\), \(F()=()\), \(S[^{},]=^{},\) and \(()=/\|\|_{2}\), the energy function and the corresponding update3 are:

\[E()=-_{=1}^{K}(^{},()),\ _{}E()=- ^{K}(^{},())^{}}{_{=1}^{K}(^{ },())}()}{ d}. \]

This form does not directly admit itself to a distributed storage of memories as in eq. (2), and thus, in order to perform the gradient descent on the energy, it is necessary to keep all the memories in their original form. We will try to address this issue by taking inspiration from the area of _kernel machines_.

### Random Features for Kernel Machines

The expressivity of kernel learning usually comes with increased computational complexity both during training and inference, taking time quadratic and linear in the size of the training set respectively. The groundbreaking work of Rahimi and Recht  introduced random features to generate explicit feature maps \(:^{D}^{Y}\) for the RBF and other _shift-invariant_ kernels4 that approximate the true kernel function - that is \((),(^{}) k (,^{})\). Various such random maps have been developed for shift-invariant kernels [18; 19; 38] and polynomials kernels [39; 40; 41].

For the RBF kernel and the exponentiated dot-product or EDP kernel \(k(,^{})=(,^{})\), there are usually two classes of random features - trigonometric features and exponential features. For the RBF kernel \(k(,^{})=(-\|-^{}\|_{2} ^{2}/2)\), the trigonometric features  are given on the left and the exponential features  are on the right:

\[()=}[(< ^{1},>)\\ (<^{1},>)\\ ,\\ (<^{Y},>)\\ (<^{Y},>)],\ \ \ \ \ \ \ \ \ \ \ \ \ ()=\|_{2}^{2})}{} [(+<^{1},>)\\ (-<^{1},>)\\ ,\\ (+<^{Y},>)\\ (-<^{Y},>)], \]

where \(^{}(0,I_{D})\{1,,Y\}\) are the random projection vectors.5 A random feature map \(\) for the RBF kernel can be used for the EDP kernel by scaling \(()\) with \((\|\|_{2}^{2}/2)\). While the trigonometric features ensure that \(k(,)=(),()=1\), the exponential features ensure that \(()_{+}^{2Y}\), which is essential in certain applications as in transformers [18; 19]. Furthermore, while the random samples \(^{}(0,I_{D})\) are supposed to be independent, Choromanski et al.  show that the \(\{^{1},,^{Y}\}\) can be entangled to be exactly orthogonal to further reduce the variance of the approximation while maintaining unbiasedness. In general, the approximation of the random feature map is \(O()\), implying that a feature space with \(Y O(D/^{2})\) random features will ensure, with high probability, for any \(,^{}^{D}\), \(|k(,^{})-(),( ^{})|\). Scaling in the kernel functions such as \((-\|-^{}\|_{2}^{2}/2)\) or \((<,^{}>)\) can be handled with the aforementioned random feature maps \(\) by applying them to \(\) with \((),( ^{})(-\|-^{ }\|_{2}^{2}/2)\).

## 3 DrDAM with Random Features

Revisiting the general energy function in eq. (3), if we have available an explicit mapping \(:^{D}^{Y}\) such that \((^{}),() F(S[ ^{},])\), then we can simplify the general energy function in eq. (3) to

\[E()()=-Q(_{=1}^{K}(^{}),(()) )=-Q(<_{=1}^{K}(^{}),(())>). \]Denoting \(=_{}(^{})\), we can write a simplified general update step for any input \(\) as:

\[_{}=-.|_{s=(( ()),)}(.()}{d}|_{=()}^{} )()}{d} \]

where \(d()/d^{Y D}\) is the gradient of the feature map with respect to its input. In the presence of such an explicit map \(\), we can distribute the memory in a \(\) into the single \(Y\)-dimensional vector \(\), and be able to apply the update in eq. (10). We can then use the random feature based energy gradient \(_{}()\) instead of the true energy gradient \(_{}E()\) in the energy gradient descent step in eq. (5).6 We name this scheme "Distributed representation for Dense Associative Memory" or \(\), and we compare the computational costs of \(\) with the "Memory representation of \(\) Associative Memory" or \(\) in the following:

**Proposition 1**.: _With access to the \(K\) memories \(\{^{}^{D},[\![K]\!]\}\), \(\) takes \(O(LKD)\) time and \(O(KD)\) peak memory for \(L\) energy gradient descent steps (or layers) as defined in eq. (5) with the true energy gradient \(_{}E()\)._

Naively, the random feature based \(\) would require \(O(DY)\) memory to store the random vectors and the \(_{}()\) matrix. However, we can show that we can generate the random vectors on demand to reduce the overall peak memory to just \(O(Y)\). The various procedures in \(\) are detailed in Algorithm 1. The RF subroutine generates the random feature for any memory or input. The ProCMems subroutine consolidates all the memories into a single \(^{Y}\) vector. The GradComp subroutine compute the gradient \(_{}\). The following are the computational complexities of these procedures:

**Proposition 2**.: _The RF subroutine in Algorithm 1 takes \(O(DY)\) time and \(O(D+Y)\) peak memory._

**Proposition 3**.: _ProcMems in Algorithm 1 takes \(O(DYK)\) time and \(O(D+Y)\) peak memory._

**Proposition 4**.: _GradComp in Algorithm 1 takes \(O(D(Y+D))\) time and \(O(D+Y)\) peak memory._

Thus, the computational complexities of \(\) neural dynamics are (see appendix F.1 for proof and discussions):

**Theorem 1**.: _With a random feature map \(\) utilizing \(Y\) random projections \(\{_{},\{1,,Y\}\}\) and \(K\) memories \(\{^{}^{D},\{1,,K\}\}\), the random-feature based \(\) takes \(O(D(YK+L(Y+D)))\) time and \(O(Y+D)\) peak memory for \(L\) energy gradient descent steps (or layers) as defined in eq. (5) with the random feature based approximation gradient \(_{}()\) defined in eq. (10)._

However, note that the memory encoding only needs to be done once, while the same \(\) can be utilized for \(L\) steps of energy gradient steps for multiple input, and the cost of ProCMems is amortized over these multiple inputs. We also show that the computational costs of the inclusion of a new memories \(\):

**Proposition 5**.: _The inclusion of a new memory \(^{D}\) to a \(\) with \(K\) memories distributed in \(^{Y}\) takes \(O(DY)\) time and \(O(D+Y)\) peak memory._

The above result shows that inclusion of new memories correspond to constant time and memory irrespective of the number of memories in the current DenseAM. Next, we study the divergence between the output of a \(L\)-layered \(\) using the energy descent in eq. (5) with the true gradient in eq. (6) and that of \(\) using the random feature based gradient in eq. (10).

**Theorem 2**.: _Consider the following energy function with \(K\) memories \(\{^{}^{D},\{1,,K\}\}\) and inverse temperature \(>0\):_

\[E()=-(_{=1}^{K}(-} {{2}}\|^{}-\|_{2}^{2})). \]

_We further make the following assumptions: (A1) All memories \(^{}\) and inputs \(\) lie in \(=[0,}{{}}]^{D}\). (A2) Using a random feature map \(:^{D}^{Y}\) using \(Y\) random feature maps, for any \(,^{}^{d}\) there is a constant \(C_{1}>0\) such that \(|(\|-^{}\|_{2}^{2}/2)-<( ),(^{})>| C_{1}}{{Y}}}\). Given an input \(\), let \(^{(L)}\) be the output of the MrDAM defined by the energy function in eq. (11) using the true energy gradient in eq. (6) and \(}^{(L)}\) be the output of DrDAM with approximate gradient in eq. (10) using the random feature map \(\) using a constant step-size of \(>0\) in (5). Then_

\[\|^{(L)}-}^{(L)}\|_{2} 2 LC_{1}Ke^{  E()}( ))^{L}}{1- L(1+2K e^{/2})}) \]

Assumption (A1) just ensures that all the memories and inputs have bounded norm, and can be achieved via translating and scaling the memories and inputs. Assumption (A2) pertains to the approximation introduced in the kernel function evaluation with the random feature map, and is satisfied (with high probability) based on results such as Rahimi and Recht [37, Claim 1] and Choromanski et al. [18, Theorem 4]. The above result precisely characterizes the effect on the divergence \(\|^{(L)}-}^{(L)}\|\) of the (i) initial energy of the input \(E()\) - lower is better, (ii) the inverse temperature \(\) - lower is better, (iii) the number of memories \(K\) - lower is better, (iv) the ambient data dimensionality \(D\) - lower is better, (v) the number of random features \(Y\) - higher is better, and (vi) the number of layers \(L\) - lower is better. The proof and further discussion are provided in appendix F.2. Note that theorem 2 analyzes the discretized system, but as the step-size \( 0\), we approach the fully contracting continuous model. An appropriate choice for the energy descent step-size \(\) simplifies the above result, bounding the divergence to \(O()\):

**Corollary 1**.: _Under the conditions and definitions of theorem 2, if we set the step size \(=}{L(1+2K e^{/2})}\) with \(C_{2}<1\), the divergence is bounded as:_

\[\|^{(L)}-}^{(L)}\|_{2}C_{2} e^{(E()-1/2)}}{(1-C_{2})}. \]

These above results can be extended to the EDP based energy function \(E()=-}{{}}_{}(<^{ },>)+}{{2}}\|\|_{2}^{2}\) using the same proof technique.

## 4 Empirical evaluation

To be an accurate approximation of the traditional MrDAM, DrDAM must empirically satisfy the following desiderata for all possible queries and at all configurations for inverse temperature \(\) and pattern dimension \(D\):

1. for the same query, DrDAM must predict similar energies and energy gradients as MrDAM; and
2. for the same initial query, DrDAM must retrieve similar fixed points as MrDAM.

However, in our experiments we observed that the approximation quality of DrDAM is strongly affected by the choice of \(\) and that the approximation quality decreases the further the query patterns are from the stored memory patterns, as predicted by theorem 2. We characterize this behavior in the following experiments using the trigonometric "SinCos" basis function, which performed best in our ablation experiments (see appendix C), but note that the choice of the random features do play a significant role in the interpretations of these results.

### (D1) How accurate are the energies and gradients of DrDAM?

Figure 3 evaluates how well DrDAM, configured at different feature sizes \(Y\), approximates the energy and energy gradients of MrDAM configured with different inverse temperatures \(\) and storing random binary patterns of dimension \(D\). The experimental setup is described below.

We generated \(2K=1000\) unique, binary patterns (where each value is normalized to be \(\{0,}\}\)) and stored \(K=500\) of them into the memory matrix \(\) of \(\). We denote these stored patterns as \(^{}\{0,}\}^{D}\), \([\![K]\!]\), where \(D\) is a hyperparameter controlled by the experiment. For a given \(\), the memory matrix is converted into the featurized memory vector \(T_{}:=_{}_{}(^{})\) from eq. (9), where \([\![2Y]\!]\). The remaining patterns are treated as the "random queries" \(^{b}_{}\), \(b[\![K]\!]\) (i.e., queries that are far from the stored patterns). Finally, in addition to evaluating the energy at these random queries and at the stored patterns, we also want to evaluate the energy at queries \(^{b}_{}\) that are "near" the stored patterns; thus, we take each stored pattern \(^{}\) and perform bit-flips on \(0.1D\) of its entries.

For each set of queries \(^{b}\{^{b},^{b}_{},^{b}_{ }\}\), \(b[\![K]\!]\), and choice of \(\), \(Y\), and \(D\), we compute the **Mean** Approximation **E**rror (MAE) between \(\)'s energy \(E_{b}:=E(^{b};,)\) (whose gradient matrix is denoted \(_{}E_{b}\)) and \(\)'s energy \(_{b}:=(^{b};,)\) (whose gradient is denoted \(_{}_{b}\)).

\[_{}=_{b[\![K]\!]}|E_{b}-_ {b}|,_{}=_{b[\![K]\!]}\| _{}E_{b}-_{}_{b}\|_{2} \]

We found it useful to visualize the results using log-scale and to compare the errors against the expected error of a "random guess" of the energy/gradients (horizontal red dashed line in each plot of fig. 3). The "random guess error" was empirically computed by sampling a new set of random queries \(^{b}_{}\), \(b[\![K]\!]\) (independent of the reference queries) and computing the MAE between the standard energy on the reference queries vs. the approximate energies on the random queries. This error was averaged across \(Y\) for each \(\); the highest average error across all \(\)s is plotted.

**Observation 1: \(\) approximations are best for queries near stored patterns**\(\) approximations for both the energy and energy gradients are better the closer the query patterns are to the stored patterns. In this regime, approximation accuracy predictably improves when increasing

Figure 3: \(\) produces better approximations to the energies and gradients of \(\) when the queries are closer to the stored patterns. Approximation quality improves with larger feature dimension \(Y\), but decreases with higher \(\) and higher pattern dimension \(D\). Approximation error is computed on \(500\) stored binary patterns normalized between \(\{0,}\}\). The **Mean** Approximation **E**rrors (**MAE**, eq. (14)) is taken over \(500\) queries initialized: **at** stored patterns (i.e., queries equal the stored patterns), **near** stored patterns (i.e., queries equal the stored patterns where 10% of the bits have been flipped), and **randomly** (i.e., queries are random and **far** from stored patterns). Error bars represent the standard error of the mean but are visible only at poor approximations. Red horizontal lines represent the expected error of random energies and gradients. The theoretical error upper bounds of eq. (13) (dark curves on the gradient errors in the _right plot only_) show a tight fit to empirical results at low \(\) and \(D\) and are only shown if predictions are “better than random”. The shaded area shows the difference between the theoretical bound and the empirical results.

the value for \(Y\) within "reasonable" values (i.e., values corresponding into sizes of featurized queries and memories that can operate within 46GB of GPU memory).

Observation 2: DrDAM approximations worsen as inverse temperature \(\) increasesAcross nearly all experiments, DrDAM approximations worsen as \(\) increases. At queries near the stored patterns, \(=50\) has an energy error approximately \(10\) that of \(=30\) and \(100\) that of \(=10\) across all \(Y\). At high \(D\) and when queries are far from the patterns, the error of \(=50\) approaches \(1000\) the error of \(=10\). This observation similarly holds for the errors of corresponding gradients, corroborating the statement of theorem 2.

Observation 3: DrDAM approximations break at sufficiently high values of \(D\) and \(\)In general, DrDAM's approximation errors remain the same across choices for \(D\), especially when the queries are near the stored patterns. However, when both \(\) and \(D\) are sufficiently large (e.g., \( 40\) and \(D 100\) in fig. 3), increasing the value of \(Y\) does not improve the approximation quality: DrDAM continues to return almost random gradients and energies. We explore this phenomenon more in SS 4.2 in the context of the retrievability of stored patterns.

### (D2) How accurate are the memory retrievals using DrDAM?

_Memory retrieval_ is the process by which an initial query \(^{(0)}\) descends the energy function and is transformed into a fixed point of the energy dynamics. This process can be described by the discrete update rule in eq. (5), where \(E\) can represent either MrDAM's energy or the approximate energy of DrDAM. A memory is said to be "retrieved" when \(|E(^{(L)})-E(^{(L-1)})|<\) for some small \(>0\), at which point \(^{(L-1)}^{(L)}=:^{}\) is declared to be the retrieved _memory_ after \(L\) iterations because \(^{}\) lives at a local minimum of the energy function \(E\).

Quantifying retrieval errorGiven the same initial queries \(^{(0)}\{0,}\}^{D}\), we want to quantify the difference between the fixed points \(}^{}\) retrieved by descending DrDAM's approximate energy and the fixed points \(^{}\) retrieved by descending the energy of MrDAM. We follow the experimental setup of SS 4.1, only this time we run full memory retrieval dynamics until convergence.

Figure 4: **A) Retrieval errors predictably follow the approximation quality of fig. 3. Error is lowest at/near stored patterns but is completely random when energy and gradient approximations are poor, i.e., at high values of \(\) and \(D\). Note that error improves across \(Y\) but follows a different (and noisier) trace than the corresponding approximations for energy and gradient in fig. 3 due to error accumulating over multiple update steps. B) DrDAM’s approximation quality improves as \(Y\) increases (visible at low \(\)), but larger \(Y\)’s are needed for good approximations to the DAM’s fixed points at higher \(\)’s. (Left) The same corrupted query from CIFAR-10 where bottom 50% is masked is presented to DAM’s with different \(\)’s. (Middle) The fixed points of DrDAM for each \(\) at different sizes \(Y\) of the feature space. (Right) The “ground truth” fixed point of MrDAM. The top 50% of pixels are clamped throughout the dynamics.**

Note that since energy uses an L2-similarity kernel, memory retrieval is not guaranteed to return binary values. Thus, we binarize \(^{}\) by assigning each entry to its nearest binary value before computing the normalized Hamming approximation error \(_{H}\), i.e.,

\[ x:=},&x}\\ 0,&,_{H}:=}_{i[D]}^{}_{i}-}^{}_{i}. \]

The choice of normalized Hamming approximation error \(_{H}\) on our binary data is equivalent to the squared L2 error on the left side of our bound in eq.13 (up to a linear scaling of \(}\)).

Figure4A shows the results of this experiment. Many observations from SS4.1 translate to these experiments: we notice that retrieval is random at high \(\) and \(D\), and that retrievals are of generally higher accuracy nearer the stored patterns. However, we notice that high \(\) values can retrieve better approximations than lower values of \(\) when the queries are at or near stored patterns. Additionally, for sufficiently high \(\) (e.g., see \(D=1000\), \(=50\) near stored patterns), this accompanies an interesting "thresholding" behavior for \(Y\) where retrieval error starts to improve rapidly once \(Y\) reaches a minimal threshold. This behavior is corroborated in the high \(D\) regime in fig.4B.

Visualizing retrieval errorFigure4B shows what retrieval errors look like qualitatively. We stored \(K=10\) random images from CIFAR10  into the memory matrix of MrDAM, resulting in patterns of size \(D=3 32 32=3072\), and compared retrievals using \(\)s that produced meaningful image results with MrDAM. To keep \(\) values consistent with our previous experiments, each pixel was normalized to the continuous range between \(0\) and \(}\) s.t. \(^{}_{i}[0,}]\), with \([\![K]\!]\) and \(i[\![D]\!]\).

From SS4.1 and fig.4A, we know that approximate retrievals are inaccurate at high \(\) and high \(D\) if the query is far from the stored patterns. However, this is exactly the regime we test when retrieving images in fig.4B. The visible pixels (top half of the image) are clamped while running the dynamics until convergence. Retrieved memories at different configurations for DrDAM are plotted against their corresponding MrDAM retrievals in fig.4B.

As \(\) increases, insufficiently large values of \(Y\) fail to retrieve meaningful approximations to the dynamics of MrDAM. We observe that image completions generally become less noisy as \(Y\) increases, but with diminishing improvement in perceptible quality after some threshold where DrDAM goes from predicting noise to predicting meaningful image completions.

## 5 Conclusion

Our study is explicitly designed to characterize where DrDAM is a good approximation to the energies and dynamics of MrDAM. In pushing the limits of the distributed representation, we discovered that DrDAM is most accurate when: (1) query patterns are nearer to the stored patterns; (2) \(\) is lower; and (3) \(Y\) is large. Error bounds for these situations are explicitly derived in theorem2 and empirically tested in SS4.

We have explored the use of distributed representations via random feature maps in DenseAMs. We have demonstrated how this can be done efficiently, and we precisely characterized how it performs the neural dynamics relative to the memory representation DenseAMs. Our theoretical results highlight the factors playing a role in the approximation introduced by the distributed representations, and our experiments validate these theoretical insights. As future work, we intend to explore how such distributed representations can be leveraged in hierarchical associative memory networks , which can have useful inductive biases (e.g., convolutions, attention), and allow extensions with multiple hidden layers.