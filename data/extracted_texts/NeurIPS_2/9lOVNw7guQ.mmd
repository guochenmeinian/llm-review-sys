# Low-shot Object Learning with Mutual Exclusivity Bias

Anh Thai\({}^{1}\)   Ahmad Humayun\({}^{2}\)1   Stefan Stojanov\({}^{1}\)1   Zixuan Huang\({}^{3}\)

**Bikram Boote\({}^{1}\)   James M. Rehg\({}^{1,3}\)**

\({}^{1}\)Georgia Institute of Technology, \({}^{2}\)Google Deepmind,

\({}^{3}\)University of Illinois, Urbana-Champaign

Equal contribution.

###### Abstract

This paper introduces Low-shot Object Learning with Mutual Exclusivity Bias (LSME), the first computational framing of mutual exclusivity bias, a phenomenon commonly observed in infants during word learning. We provide a novel dataset, comprehensive baselines, and a state-of-the-art method to enable the ML community to tackle this challenging learning task. The goal of LSME is to analyze an RGB image of a scene containing multiple objects and correctly associate a previously-unknown object instance with a provided category label. This association is then used to perform low-shot learning to test category generalization. We provide a data generation pipeline for the LSME problem and conduct a thorough analysis of the factors that contribute to its difficulty. Additionally, we evaluate the performance of multiple baselines, including state-of-the-art foundation models. Finally, we present a baseline approach that outperforms state-of-the-art models in terms of low-shot accuracy. Code and data are available at [https://github.com/rehg-lab/LSME](https://github.com/rehg-lab/LSME).

Figure 1: We propose **Low-Shot Object Learning with Mutual Exclusivity Bias (LSME), a novel setting that is more realistic and significantly challenging than the standard low-shot object learning setting. Given a scene with multiple objects including seen and novel objects, the goal of LSME is to use mutual exclusivity bias to correctly identify the unfamiliar object and generalize this association to other instances of the novel category at testing time.**Introduction

Toddlers are incredible learners, acquiring an average of 10 to 20 new words per week during a period known as the vocabulary spurt . This is possible in part because of effective inductive biases that help them to overcome the inherent ambiguity that arises in inferring which real-world object a spoken object name (e.g., provided by a caregiver), is referring to. Inductive biases like shape bias have received more interest [44; 35; 36], due in part to widespread interest in the link between 3D object shape and categorization. In contrast, the key inductive bias of mutual exclusivity has not yet received significant attention [11; 27; 28]. Mutual exclusivity leverages the assumption that each object has only one label. Thus when presented with a scene containing multiple objects, some familiar and some unfamiliar, mutual exclusivity guides the child to bind new object names to the unfamiliar objects. For example, consider a toddler playing with three toys: her favorite Ducky, a Mickey Mouse figure, and a toy that she does not know the name of. When the caregiver says, "Look at the dinosaur," the toddler uses mutual exclusivity bias to rule out Ducky and Mickey, as she already knows their names. She then associates the word "dinosaur" with the unfamiliar object. After this mapping is made, the toddler can retain the category label "dinosaur," and use it later to label other dinosaur-like objects. The goal of this work is to create a novel dataset and associated baselines to spur the ML community in developing computational models for mutual exclusivity, as a further step towards modeling the inductive biases that underlie human object learning performance.

This paper introduces Low-shot Object Learning with Mutual Exclusivity Bias (LSME), a computational framing of mutual exclusivity as an inference problem, with low shot learning as the downstream measure of generalization performance (see Fig. 2 for an overview and Table 1 for related settings). The LSME task begins with a set of already-known object categories. The input is an RGB image containing multiple object instances from known categories and a single object instance of an unknown category, along with a novel category label. Solving the LSME problem requires three steps: 1) Localize the object instances within the scene via segmentation; 2) Associate the novel category label with the unknown category instance that it refers to; and 3) Solve a low-shot learning task by classifying other object instances from the novel category during inference. The ability to reliably solve LSME would be an important step in creating agents that can learn with minimal supervision. For example, household robotics is a long-standing goal of AI. Our work could ultimately yield a robot capable of rapidly learning the names of novel objects in the home, without the need for a burdensome supervised training stage.

LSME can be partitioned into three sub-tasks: 1) object localization, 2) open-world recognition, and 3) low-shot learning. The model must first localize each object in the scene. It must then distinguish between learned and unknown categories to correctly assign the novel label to the unknown instance. Finally, the model must generalize to new instances of the novel categories after only one or a few labeled samples. At the heart of LSME is the challenge of associating novel object instances with their category labels. In supervised pre-training, there is a danger of leakage if the datasets used in pre-training overlap with the low-shot category labels used in testing. In order to prevent this, we

Figure 2: Our proposed LSME (Low-shot Object Learning with Mutual Exclusivity Bias) setting. We leverage mutual exclusivity bias observed in humans to associate a novel word label with an unfamiliar object in a scene. Given a set of familiar categories (base classes), the task is to use this bias to correctly identify the unfamiliar object and generalize this association to other instances of the novel category.

constrain the LSME problem design so that pre-training prior to the low-shot phase cannot include any object-label association data (e.g. as in CLIP). This constraint further aligns with the investigation of a key question in developmental psychology: How can a child rapidly learn words in the absence of prior knowledge of object semantics? [51; 26]

It's natural to ask if LSME can be solved by simply combining existing SOTA models. We find that this is not the case. Errors in the first two sub-tasks propagate to the low-shot stage and degrade accuracy. For example, complex spatial interactions (e.g. occlusions) between objects make reliable instance segmentation challenging, leading to poor feature extraction for each object. Furthermore, any errors in identifying novel vs. known objects will result in label noise that degrades low-shot recognition. In addition, pre-trained feature representations from foundation models (e.g. DINOv1  and DINOv2 ) are not sufficient to solve our ask effectively (see Sec. 4).

One challenge we address in this paper is the creation of suitable LSME datasets. Since real-world data is not available, we introduce a generic data generation pipeline that can take any categorical 3D dataset as input and render training and testing data with realistic backgrounds, lighting variations, and realisic object poses. Our dataset is structured with increasing levels of complexity in order to aid in understanding the shortcomings of learning approaches.

Additionally, we benchmark the performances of various baselines on LSME, including SOTA foundation models with robust visual representations such as DINOv2 , ImageBind , and CLIP . Interestingly, we find that the performance of these baselines degrades significantly when there are occlusions/incomplete instance masks (see Sec. 4.3).

From these observations, we propose a baseline that involves pretraining the object representations on cluttered scenes with occlusion. Following the success of prior works that show performance improvement when training on multi-view inputs [43; 17], we propose self-supervised training on the large-scale multi-view multi-object ABC  dataset using contrastive learning. Our baseline composed of the robust 2D representations from foundation models and 3D features from multi-view inputs demonstrates improvement in accuracy compared to existing models when occlusions are present by approx. \(10\%\). To verify that this gain is significant, we evaluate our approach on the real-world dataset CO3D  and show the benefit of multi-view training with occlusion.

In summary, our contributions are 4-fold: 1) The first to provide a computational framing of mutual exclusivity via LSME; 2) A dataset generation pipeline that enables the creation of progressively more challenging LSME tasks using any set of 3D models; 3) Performance benchmarking of multiple baselines including foundation models on our novel task; and 4) A novel baseline method that defines the SOTA on LSME.

## 2 Related Work

Because we are the first to provide a computational framing of mutual exclusivity (ME), there is no direct prior work to compare to.  demonstrated that existing deep models fail at ME, but did not provide a comprehensive framing or SOTA methods. We now review three bodies of related work.

**Self-supervised and Weakly-Supervised Learning** Self-supervised learning aims to leverage the inherent structure in visual data to train representations suitable for downstream tasks, without relying on labels. Contrastive methods [7; 15; 8] work by treating image pairs under different data augmentations as positive examples, and other sample pairs as negatives. DINOv1 enforces "global-local" correspondences by using multiple image crops, while other methods [48; 55; 31] focus on pixel-level matching or combine both objectives . On the other hand, VISPE ,

   PropertiesSetting \\  &  Low-shot \\ Recognition \\  &  Open-set \\ Detection \\  &  Low-shot \\ Detection \\  &  Object \\ Discovery \\  &  GCC \\  \\  &  NCDL. \\  \\  &  Open-category \\ Detection \\  & 
 Image Conditioned \\ Detection \\  & LSME \\  Instance Localization & ✗ & ✓ & ✓ & ✓ & ✗ & ✓ & ✓ & ✓ & ✓ \\ Mutual Exclusivity Bias & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✓ \\ Discover Novel Classes & ✗ & ✓ & ✗ & ✓ & ✓ & ✓ & ✗ & ✓ \\ Label Novel Classes & ✓ & ✗ & ✓ & ✗ & ✓ & ✓ & ✗ & ✓ \\ Zero/Low-shot & ✓ & ✗ & ✓ & ✓ & ✗ & ✓ & ✓ & ✓ \\ No Pretrained LLM & ✓ & ✓ & ✓ & ✓ & ✓ & ✗ & ✓ & ✓ \\  

Table 1: Comparative analysis of LSME and other related settings. LSME requires a comprehensive understanding of scenes that involve the presence of multiple objects.

DOPE , and VSA  incorporate multi-view data by treating samples captured from different viewpoints as positive pairs. These approaches leverage 3D structure to gain improvements.

In contrast, recent weakly-supervised learning foundation models such as CLIP  and Image-Bind  take advantage of multi-modal signals, including language and audio in addition to visual information. Another body of work  attempts to learn the underlying visual structure of data by learning to reconstruct images from heavily masked inputs. In this work, we study the performance of these methods on our novel task. Moreover, we extend these representations by incorporating multi-view multi-object information. By integrating data from multiple viewpoints and considering information at the object level instead of the scene or pixel level, we aim to learn object representations that can reason about the spatial layout and category composition of scenes with multiple objects.

**Related Settings** Vaze et al. proposed Generalized Category Discovery (GCD) , a setting that allows input to come from both known and unknown distributions, and further aims to label novel data. We differ from this setting in that we do not assume object localization information. We further consider scene-centric data that provides a more holistic view of the entire scene instead of object-centric data as input. This enables our model to take into account the relationships between multiple objects within the scene.

Fomenko et al.  introduce the novel class discovery and localization (NCDL) setting, which closely relates to our proposed LSME task. Both tasks share the goal of localizing and classifying unknown objects in a scene. However, we operate in a low-shot setup, where only a limited number of samples from the novel categories are available. This contrasts with NCDL, which focuses on balancing a long-tail distribution of novel classes with varied number of samples per class. In addition, our setting requires mutual exclusivity bias--explicitly learning ambiguous word-object associations.

Open-category object detection and segmentation  have recently gained popularity for its generalization ability beyond the trained vocabulary. This task focuses on using large pre-trained vision-language models to detect and segment known and novel objects via prompting the LLMs. In contrast, our task focuses specifically on learning the association between objects and their corresponding labels. These labels may not necessarily align with the vocabulary typically used for LLM pretraining (e.g. using pseudo names for toy objects like "dax" as commonly seen in psychology experiments ). LSME can flexibly accommodate various vocabulary systems that potentially is a challenge for pretrained LLMs.

Image-conditioned object detection  is a related setting where language input is not required. The goal of this setting is to detect objects that share the same semantic characteristics as the template objects given as one or a few input images. Our setting solves a more general and challenging task where the model is required to identify the unknown object of interest before generalizing to other object instances from the novel category. LSME is further related to low-shot learning , open-world learning , and object instance segmentation  settings as it integrates these components into a holistic solution. (See Tab. 1)

**Computational Frameworks Motivated by Developmental Psychology** Recent works have investigated various learning scenarios to explore the strategies children employ when acquiring new concepts . This work complements these efforts by introducing a comprehensive benchmark that studies the mutual exclusivity bias commonly observed in infants during the initial stages of word learning.

Figure 3: Rendered scenes used for LSME. First column has scenes of ABC  objects that are used for pretaining. Second and third columns show data for low-shot evaluation on Toys4k  and ShapeNetCore.v2 

Synthetic 3D Datasets and Dataset GeneratorsSynthetic 3D datasets and environments have been explored to study a wide range of tasks  where real-world data might not be available at a large scale. Recent realistic data rendering engines  and image generation models  have significantly reduced the sim2real domain gap, enabling more effective generalization to real-world settings. In this work, we leverage the benefits of synthetic data to tackle a novel problem where real-world datasets are unavailable. This allows us to create diverse and customizable scenarios that closely resemble real-world settings to study the problem in a controlled and scalable manner.

## 3 Low-shot Categorization Using Mutual Exclusivity Bias (LSME)

### Task Formalization

Consider an RGB scene containing multiple objects, including known instances and a single novel instance that lacks any explicit localization information and a word for its label. Our task can be partitioned into solving three well-known subtasks:

**Object Localization.** The first step is to localize the objects in an RGB scene.

**Open-world Recognition.** The second step is to differentiate between known and novel categories. The objective is to identify the instance within each scene that belongs to a novel category. To enable this, we assume the availability of a collection of labelled images from known categories, referred to as the _base_ classes. Once the novel object identified it is associated with the novel word label, which we refer to as a _support_ object.

**Low-shot Learning.** The last step is low-shot generalization. Given one or a few _query_ images of objects from the novel categories, the goal is to correctly classify them based on the support objects.

**Data Generation Pipeline.** Given any 3D categorical dataset, following the convention of low-shot learning, we first partition these object categories into disjoint sets: base classes and low-shot classes. While base classes are commonly used to learn robust feature representations, low-shot classes are for evaluating the generalization ability of the models in a low data setting.

_Data Rendering._ To generate each scene, we first randomly select a subset of objects. The rotational poses for the objects are obtained using rigid body simulation. The objects are scaled and placed into the scene at random locations making sure collisions do not occur. Scenes are generated with varying background, illumination, and camera viewpoint. Please refer to the Supplement for more detailed descriptions of the generating process.

Figure 4: Different variants of LSME setting. **Left:** Single-object case where only a single object is present in the scene for both supports and queries and **Right:** Multi-object setting where multiple objects are present in the scene. Bounding boxes and texts indicate given localization information and labels respectively. The difficulty levels of these variants increase from left to right, with the hardest setting—our proposed LSME on the rightmost column requiring the models to achieve all 3 properties: localization, mutual exclusivity bias, and low-shot.

### Data Variants

To support a comprehensive analysis and gain insights into the limitations of different methods, we generate data with increasing difficulty levels of variability (please see Fig. 4).

The first group of variants considers scenes with a single object (-SObj) (left side of Fig. 4). The simplest setting concerns object instance recognition (Inst-SObj)--recognizing the same object based on a few images in the same pose. Our second group of variants requires models to generalize to different instances of the same category (Categ-SObj), while our third setting also takes into consideration random initial object pose sampling (Categ-SObj-PoseVar). Note that Categ-SObj is similar to the standard low-shot learning setting . These settings do not require mutual exclusivity bias.

The second group of has multiple objects (-MObj) (right side of Fig. 4). For the first, labels for the shots are given (Categ-MObj). This is similar to Categ-SObj-PoseVar, except the objects might be partially occluded. The second variant requires using mutual exclusivity bias to assign the label to the correct object before low-shot generalization (Categ-MObj-SuppAssign). Finally, LSME also requires object localization.

Note that while the first data variant (Inst-SObject) yields a straightforward task that does not require category generalization, all subsequent variants (Categ-SObject, Categ-SObject-PoseVar, and Categ-MObject) require instance-to-category generalization, as in the standard low-shot learning setting. Although not requiring mutual exclusivity bias, these variants are significantly more challenging in comparison to the standard low-shot learning formulation, because they introduce pose variability and include multiple objects, as described in Sec. 4.

### Our Approach

**Representation Learning.** Motivated by the understanding that multi-view observations provide rich visual information about the 3D environment that aids downstream tasks , we adopt a strategy of pretraining the feature extractor by leveraging contrastive learning on multi-view scenes. Unlike most self-supervised approaches that primarily operate on the scene level even when the scene consists of multiple objects, we focus on the object-level representation learning. Specifically, given scenes of multiple objects captured from various viewpoints, along with the corresponding instance masks for each object, we learn to match the representation of the same object across different views.

We use pre-trained backbones, (e.g. DINOv1 , DINOv2 ) contrastive training strategy with a momentum encoder . Given two views of the same scene, \(v_{1}\) and \(v_{2}\), we first use the instance mask associated with each object in the scene to eliminate the background and other objects. Subsequently, we extract the query object feature by performing a forward pass of the image encoder on \(v_{1}\). For each query feature, we minimize the InfoNCE  loss function. The positive sample is the feature of the same object in \(v_{2}\) while the negative set consists of object features from the memory queue as in MoCo-v2  and different objects from the same scene. This approach is inspired by the VISPE++

Figure 5: The process of Mutual Exclusivity Bias in our models. The input is the support scene with a novel word label. The model first needs to 1) localize the objects in the scene, 2) extract feature embeddings for these objects and compare with the features of the objects from the base classes using cosine distance, 3) take the maximum similarity score between each object embedding and the objects in the feature bank, 4) determine the unknown object by taking the object with the minimum similarity score and then assign the novel word label to this object.

baseline in . For each input view pair, we ensure to only train on objects that are visible in both views (e.g. with instance segmentation area greater than some threshold \(\) pixels)

**Unsupervised Instance Segmentation.** We use a fine-tuned FreeSOLO  model as our segmenter, benefiting from its strong performance in instance segmentation tasks. Fine tuning is done on 1K scenes from the ABC dataset, which have been annotated with instance segmentation masks.

**Low-shot Learning.** We apply the following support assignment and low-shot generalization technique in all methods.

_Support Assignment:_ We first collect a feature library consisting of object features extracted from the base classes as learned objects. During the low-shot training phase, for each input scene, our goal is to determine which object should be assigned the novel label. For every object in the scene, we calculate its similarity with each object in the feature library using cosine distance. To determine the final similarity score with the feature library, for each object in the scene, we select the maximum similarity measure across all objects in the library. Finally, the object in the scene with the lowest similarity score is assigned the novel label. Under this procedure, a learning agent would associate the novel label to the object that it is least familiar with (Fig. 5).

_Low-shot Generalization:_ Given a query scene, we compute the cosine distance between each object in the query scene and every support object identified in the support assignment phase. We then determine the nearest neighbor for each query object within the support set and assign the label of the query object to its closest counterpart in the support set.

## 4 Experiments

### Evaluation Setup

**Dataset.** Similar to prior works [44; 43] and the convention in low-shot learning, we partition the categories of ShapeNetCore.v2  and Toys4k . These datasets are divided into two disjoint subsets: base classes, for learning object-label associations, and test classes, for evaluating low-shot object recognition performance. We generate 1K scenes each for support and query sets, and 500 base scenes for multi-object experiments. Each scene in our dataset consists of 3 objects rendered in 20 views. To study mutual exclusivity bias, we ensure that each scene in the support set only contains 1 object from a novel category while the remaining objects come from base classes. We further incorporate a subset of CO3D  that shares 13 categories with the test classes in Toys4k to investigate the effectiveness of occlusion-aware feature representations in the real-world scenario. Since CO3D is an object-centric dataset, it is not possible to directly test LSME on CO3D. We only evaluate this dataset on the standard low-shot setting where mutual exclusivity bias is not considered.

We pre-train our models using the 3D object instance dataset ABC  without any category structure. We use a subset of ABC that consists of 100K objects. We render 10K scenes with 20 views per scene (8K for training and 2K for validation), each containing 2-3 objects with random object poses

  &  &  &  &  &  \\  & ViT S/8 & ViT S/14 & ViT B/14 & ViT B/16 & ViT H/16 & ViT H/16 \\  Variants & LSA & SA & LSA & SA & LSA & SA & LSA & SA \\  Categ-MObj & 56.99 & N/A & 56.95 & N/A & 57.92 & N/A & 56.76 & N/A & 60.49 & N/A \\  Categ-MObj & 40.21 & 51.68 & 41.26 & 52.28 & 43.21 & 54.96 & 41.22 & 51.64 & 45.91 & 58.58 \\  LSME & 36.44 & 46.92 & 37.08 & 48.16 & 39.24 & 50.88 & 38.25 & 48.96 & 38.85 & 50.24 \\ 

Table 3: Results on low-shot recognition on the Toys4k dataset in multi-object setting. All methods consistently experience a significant drop in low-shot accuracy when mutual exclusivity is required, and further decrease when instance segmentation is involved.

and scales. We further randomize the textures and surface materials of the objects in each scene (refer to Fig. 3).

**Evaluation Protocols.** We evaluate the performance of the baselines using the following metrics: 1) support assignment accuracy (SA) which quantifies the percentage of accurately identifying the novel instance within the scene, and 2) low-shot accuracy (LSA) for measuring low-shot performance, and 3) mIoU for instance segmentation. In this work, we follow the standard framing of low-shot inference, such that "1-shot 5-ways" means that each episode has 5 novel categories, each with only 1 object during the low-shot training phase. Unless stated otherwise, the experiments in this paper are evaluated on the 1-shot-5-way setup with 500 episodes. We report the confidence intervals and experiments on other setups in the Supplement. We make sure that novel objects are clearly visible during both support assignment and low-shot generalization phases (e.g. with instance segmentation area greater than threshold \(\) pixels).

**Representation Learning Baselines.** Our main design constraint is that pre-training prior to the low-shot phase cannot incorporate any object labels, in order to avoid any concerns of label leakage. We focus on analyzing the performance of models initialized from the SOTA self-supervised vision models DINOv1 and DINOv2 . This allows us to gain further insights into whether LSME can be tackled without prior language inputs. We further investigated the performance of large-scale vision-language foundation models, in order to characterize the difference between self-supervised and weakly-supervised pretraining approaches. We present findings using CLIP  and ImageBind  pre-trained models. Note that these models violate the constraint by having unrestricted (in terms of category composition) image captions in their pre-training.

### LSME and Related Tasks on Synthetic Data

We first analyze the performance of pre-trained representations on settings with varying difficulty, building up to LSME.

**Single Object Setting.** We present the results for the single object setting on Toys4k in Table 2 and Figure 6 on 1-shot 5-way and 1-shot 10-way set ups. We observe a decrease in performance when tested on the harder variants for all models. While the difference between Categ-SObj-PoseVar and Categ-SObj is not significant, we observe a more prominent gap between Categ-SObj and Inst-SObj. This indicates the challenge faced by the models when generalizing from instance to category level.

**Multiple Objects Setting.** We report these results in Table 3 and Figure 7. The performance of all models exhibits a significant decrease (\( 15\%\)) when mutual exclusivity bias is required (2nd row).

This decline can be attributed to the challenges associated with imperfect support assignment, which negatively affects the low-shot accuracy across all models.

_Effect of Segmentation Quality._ Table 6 provides additional evidence of the significance of the quality of the instance masks generated by the segmenter. It demonstrates that a decrease of \(0.1\) in mIoU (between the last 2 rows) can result in a substantial impact on performance, with approximately a \(6-7\%\) decrease in low-shot and support assignment accuracy. This emphasizes the critical role that accurate instance masks play in achieving high-performance results in both LSA and SA metrics.

_Effect of Occlusion._ Table 3 highlights the impact of occlusion caused by other objects in the scene on the low-shot performance of the models. While Categ-SObj-PoseVar and Categ-MObj address a similar problem, Categ-MObj considers potential occlusion in both support and query objects due to multiple objects being in the scene. We observe a decrease in performance for all models when occlusion is present. Specifically, DINOv1 experiences an average decrease of 11.85%, while DINOv2 models show greater decreases of 16.12% and 17.26% respectively.

These findings indicate the challenge posed by occlusion in low-shot learning scenarios. The presence of occlusion introduces additional complexity and ambiguity, making it more difficult for the models to accurately associate novel objects with their corresponding categories. This highlights the importance of developing methods purpose-built for the task of low-shot learning with mutual exclusivity.

**Contrastive Finetuning on ABC Improves Performance.** In Table 5 we show the advantage of contrastive finetuning on the ABC  dataset. While prior work  has demonstrated the benefits of pretraining feature representations on ABC, we are the first to show the advantages of ABC in a multi-object setting. Additionally, we train our model on an equal number of scenes from the base classes of Toys4k. Although this approach enhances the low-shot generalization performance on the test classes of Toys4k compared to DINOv2, its performance is inferior to the model trained on ABC. This highlights the benefits of leveraging ABC in the context of multi-object understanding and its potential for improving the capabilities of models in complex scenarios.

In addition, the performance comparison between DINOv2 with ViT B/14 backbone and our model fine-tuned on ABC is presented in Tab. 7 on the ShapeNetCore.v2 dataset. Notably, our model consistently outperforms DINOv2 across both the LSA and SA metrics, showcasing an improvement of approximately 10%.

### Low-shot Generalization on CO3D Dataset

  &  &  &  &  \\  & & & S/S-ABC & & S/14-ABC & & B/14-ABC \\   & Support & Query & LSA & SA & LSA & SA & LSA & SA \\  FreeSOLO  & 0.52 & 0.54 & 32.31 & 43.24 & 33.99 & 44.84 & 35.50 & 48.92 \\ CutLer  & 0.61 & 0.63 & 34.62 & 42.40 & 36.34 & 46.08 & 39.42 & 52.04 \\ SAM  & 0.72 & 0.73 & 35.72 & 48.96 & 38.58 & 52.04 & 42.38 & 56.92 \\ FreeSOLO-ABC & **0.81** & **0.83** & **41.19** & **55.40** & **43.92** & **57.88** & **47.70** & **61.32** \\ 

Table 6: Performance of our baseline finetuned with different backbones on Toys4k under LSME settings with four object segmenters. Best performance is highlighted in bold while underline represents second best performance. The quality of the instance masks plays a significant role in the low-shot and shot assignment performance for all methods.

 Method & LSA & SA \\  DINOv2 & 39.24 & 50.88 \\ Ours-DINOv2-Toys & 43.62 & 53.44 \\ Ours-DINOv2-ABC & **47.70** & **61.32** \\ 

Table 5: Performance of DINOv2 and our method fine-tuned on Toys4k and ABC on Toys4k under LSME setting. All methods use ViT B/14 as the backbone and our method is initialized with pretrained DINOv2 weights. Training on ABC improves the performance significantly, surpassing the model that was trained on the base classes of Toys4k with the same number of scenes.

We investigate the generalization capabilities of pretraining on the multi-object multi-view ABC dataset, extending beyond the synthetic data domain to real-world datasets. Considering the negative impact of object occlusions and incomplete object masks on the performance of models, we hypothesize that pretraining on ABC scenes with multiple objects where occlusions are present improves the models' ability to handle such scenarios. To test this hypothesis, we conduct an experiment where we randomly mask the instance segmentations and evaluate the models on these masked segmentations in the standard low-shot setting. Figure 8 showcases the performance of different approaches on CO3D dataset with varying mask ratios. The models finetuned on ABC exhibit a better performance compared to other models as the mask ratios increase. Even at a mask ratio of 0.5, our ABC-finetuned model with DINOv1 ViT S/8 pretrained weights performs on par with DINOv2 ViT G/14. Note that DINOv2 ViT G/14 has significantly more parameters and was trained on an order of magnitude more data than our model. These results demonstrate the benefit of occlusion-aware feature representations.

### Limitations & Future Work

One important limitation of our work is the absence of uncertainty reasoning. In real-world scenarios, agents often encounter multiple unknown objects, requiring the integration of novel labels acquired in diverse contexts to correctly associate objects with their corresponding labels. This more complex setting requires continuous integration of new information and reasoning in ambiguous situations. Another challenging real-life infant-learning scenario is where objects might have multiple names (e.g., "dog" and "husky" both referring to the breed "husky"). Due to the procedural nature of our data generation system, we have the ability to increase the task complexity as solutions to LSME improve beyond the current baselines. Future research can explore more challenging settings and developing approaches that incorporate uncertainty reasoning to improve performance in ambiguous scenarios.

**Negative Societal Impact.** Training and evaluating large-scale self-supervised learning models as well as generating data require extensive GPU usage, which negatively impacts the environment. Advancements in hardware design and techniques for optimizing deep models offer potential solutions to mitigate this impact.

## 5 Conclusion

We present a novel setting called Low-shot Object Learning with Mutual Exclusivity Bias (LSME), which requires comprehensive reasoning about scenes with complex object interactions. We conduct a thorough analysis of the challenges present in LSME and their impact on SOTA models by generating various problem variants. Based on these insights, we propose a pretraining strategy that outperforms SOTA baselines on both synthetic and real-world data. Additionally, we release our open-source data generation pipeline and the generated datasets for further research.

## Broader Impact

The challenging LSME task provides a framework for devising algorithms that can learn from limited data input. We hope that LSME is an example computational framework that can serve as a template to formulate learning tasks based on other insights from the developmental psychology community. Future development of this setting can provide a learning environment that closely resembles real-world scenarios, effectively for further studies of both computer vision and developmental psychology communities.

Figure 8: Performance of different methods on the segmentation masking experiment on CO3D. Low-shot performance of these methods decreases as the mask ratio increases. Models pretrained on ABC exhibit a slower decrease rate.