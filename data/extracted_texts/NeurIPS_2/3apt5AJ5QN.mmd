# Global Rewards in Restless Multi-Armed Bandits

Naveen Raman

Carnegie Mellon University

naveenr@cmu.edu

Zheyuan Ryan Shi

University of Pittsburgh

ryanshi@pitt.edu

Fei Fang

Carnegie Mellon University

feifang@cmu.edu

###### Abstract

Restless multi-armed bandits (rmab) extend multi-armed bandits so pulling an arm impacts future states. Despite the success of rmabs, a key limiting assumption is the separability of rewards into a sum across arms. We address this deficiency by proposing restless-multi-armed bandit with global rewards (rmab-g), a generalization of rmabs to global non-separable rewards. To solve rmab-g, we develop the Linear- and Shapley-Whittle indices, which extend Whittle indices from rmabs to rmab-gs. We prove approximation bounds but also point out how these indices could fail when reward functions are highly non-linear. To overcome this, we propose two sets of adaptive policies: the first computes indices iteratively, and the second combines indices with Monte-Carlo Tree Search (MCTS). Empirically, we demonstrate that our proposed policies outperform baselines and index-based policies with synthetic data and real-world data from food rescue.

## 1 Introduction

Restless multi-armed bandits (rmab) are models of resource allocation that combine multi-armed bandits with states for each bandit's arm. Such a model is "restless" because arms can change state even when not pulled. rmabs are an enticing framework because optimal decisions can be efficiently made using pre-computed Whittle indices . As a result, rmab have been used in scheduling , autonomous driving , multichannel access , and maternal health .

The existing rmab model assumes that rewards are separable into a sum of per-arm rewards. However, many real-world objectives are non-separable functions of the arms pulled. We find this situation in food rescue platforms. These platforms notify subsets of volunteers about upcoming food rescue trips and aim to maximize the trip completion rate . When modeling this with rmabs, arms correspond to volunteers, arm pulls correspond to notifications, and the reward corresponds to the trip completion rate. The trip completion rate is non-separable as we only need one volunteer to complete each trip (more details in Section 3). Beyond food rescue, rmabs can potentially model problems in peer review , blood donation , and emergency dispatch , but the rewards are again non-separable.

Motivated by these situations, we propose the restless multi-armed bandit with global rewards (rmab-g), a generalization of rmabs to non-separable rewards. Whittle indices from rmabs fail for rmab-gs due to the non-separability of the reward. Because of this, we propose a generalization of Whittle indices to global rewards called the Linear-Whittle and Shapley-Whittle indices. Our approximation bounds demonstrate that these policies perform well for near-linear rewards but struggle for highly non-linear rewards. We address this by developing adaptive policies that combineLinear- and Shapley-Whittle indices with search techniques. We empirically verify that adaptive policies outperform index-based and baseline approaches on synthetic and food rescue datasets. 1

Our contributions are: (1) We propose the rmab-g problem, which extends rmabs to situations with global rewards. We additionally characterize the difficulty of solving and approximating rmab-gs; (2) We develop the Linear-Whittle and Shapley-Whittle indices, which extend Whittle indices for global rewards, and detail approximation bounds; (3) We design a set of adaptive policies which combine Linear- and Shapley-Whittle indices with greedy and Monte Carlo Tree Search (MCTS) algorithms; and (4) We empirically demonstrate that adaptive policies improve upon baselines and pre-computed index-based policies for the rmab-g problem with synthetic and food rescue datasets.

## 2 Background and Notation

An instance of restless multi-armed bandits (rmabs) consists of \(N\) arms, each of which is defined through the following Markov Decision Process (mdp): \((,,R_{i},P_{i},)\). Here, \(\) is the state space, \(\) is the action space, \(R_{i}:\) is a reward function, \(P_{i}:\) is a transition function detailing the probability of an arm transitioning into a new state, and \(\) is a discount factor. For a time period \(t\), we define the state of all arm as \(^{(t)}=\{s^{(t)}_{1},,s^{(t)}_{N}\},s^{(t)}_{i}\), and for each round, a planner takes action \(^{(t)}=\{a^{(t)}_{1},,a^{(t)}_{N}\},a^{(t)}_{i}\); we drop the superscript \(t\) when clear from context. Following prior work [11; 12], we assume \(=\{0,1\}\) throughout this paper to represent not pulling and pulling an arm respectively. A planner chooses \(^{(t)}\) subject to a budget \(K\), so that at most \(K\) arms can be pulled in each time step: \(_{i=1}^{N}a^{(t)}_{i} K, t\). The objective is to find a policy, \(:^{N}^{N}\) that maximizes the discounted total reward, summed over all arms:

\[_{}_{(,)(P,)}[_{t=0}^{} _{i=1}^{N}^{t}R_{i}(s^{(t)}_{i},a^{(t)}_{i})] \]

To solve an rmab, one can pre-compute the "Whittle index" for each arm \(i\) and possible state \(s_{i}\): \(w_{i}(s_{i})=_{w}\{w|Q_{i,w}(s_{i},0)=Q_{i,w}(s_{i},1)\}\), where \(Q_{i,w}(s_{i},a_{i})=-wa_{i}+R_{i}(s_{i},a_{i})+_{s^{}}P_{i}(s _{i},a_{i},s^{})V_{i,w}(s^{})\), and \(V_{i,w}(s^{})=_{a}Q_{i,w}(s^{},a)\). Here, \(Q_{i,w}(s_{i},a_{i})\) represents the expected future reward for playing action \(a_{i}\), given a penalty \(w\) for pulling an arm. The Whittle index computes the minimum penalty needed to prevent arm pulls; larger penalties (\(w_{i}(s_{i})\)) indicate more value for pulling an arm. In each round, the planner pulls the arms with the \(K\) largest Whittle indices given the states of the arms. Such a Whittle index-based policy is asymptotically optimal [1; 2].

## 3 Defining Restless Multi-Armed Bandits with Global Rewards

Objectives in rmabs are written as the sum of per-arm rewards (Equation 1), while real-world objectives involve non-separable rewards. For example, food rescue organizations, which notify volunteers about food rescue trips, aim to maximize the trip completion rate, i.e., the fraction of completed trips. Suppose we model volunteers as arms and volunteer notifications as arm pulls. Then arm states correspond to volunteer engagement (e.g., active or inactive) and rewards are the probability that any notified volunteers complete the rescue. Here, we cannot split the reward into per-volunteer functions.

Inspired by situations such as food rescue, we propose the restless multi-armed bandit with global rewards (rmab-g) problem, which generalizes rmabs to problems with a global reward function:

**Definition 1**.: _We define an instance of rmab-g through the following MDP for each arm: \((,,R_{i},R_{},P_{i},)\). The aim is to find a policy, \(:^{N}^{N}\), that maximizes:_

\[_{}_{(,)(P,)}[_{t=0}^{} ^{t}(R_{}(^{(t)},^{(t)})+_{i=1}^{N }R_{i}(s^{(t)}_{i},a^{(t)}_{i}))] \]

Throughout this paper, we focus on scenarios where \(R_{}(,)\) is monotonic and submodular in \(\); \(R_{}(,)=F_{}(\{i|a_{i}=1\})\), where \(F\) is submodular and monotonic. Such rewards have diminishing returns as extra arms are pulled. We select this because a) monotonic and submodular functions can be efficiently optimized  and b) submodular functions are ubiquitous [14; 15; 8].

Our problem formulation can model various real-world scenarios including notification decisions in food rescue and reviewer assignments in peer review . In food rescue, let \(p_{i}(s_{i})\) be the probability that a volunteer picks up a given trip, given their state, \(s_{i}\). The global reward is the probability that any notified volunteer accepts a trip: \(R_{}(,)=1-_{i=1}^{N}(1-a_{i}p_{i}(s_{i}))\). In peer review, journals select reviewers for submissions where selection impacts future reviewer availability . The goal is to select a subset of available reviewers with comprehensive subject-matter expertise. If reviewer expertise is a set \(Y_{i}\), then we maximize the overlap between the required expertise, \(Z\), and the expertise across selected (\(a_{i}=1\)) and available (\(s_{i}=1\)) reviewers: \(R_{}(,)=_{i,s_{i}=1,a_{i}=1} Y_{i} Z|\). Our formulation inherits the per-arm reward \(R_{i}\) from rmabs as there may be per-arm rewards. For example, food rescue platforms might care about the number of active volunteers, so \(R_{i}(s_{i},a_{i})=s_{i}\).

Let \(R(,)=R_{}(,)+_{i=1}^ {N}R_{i}(s_{i},a_{i})\). We characterize the difficulty of rmab-g:

**Theorem 1**.: _The restless multi-armed bandit with global rewards is PSPACE-Hard._

**Theorem 2**.: _No polynomial time algorithm can achieve better than a \(1-1/e\) approximation to the restless multi-armed bandit with global rewards problem._

**Theorem 3**.: _Consider an rmab-g instance \((,,R_{i},R_{},P_{i},)\). Let \(Q_{}(,)=_{(^{},^{ })(,)}[_{t=0}^{}^{t}R(^{ },^{}))|^{(0)}=,^{(0)}= ]\) and let \(^{*}\) be the optimal policy. Let \(^{*}=_{}Q_{^{*}}(,)\). Suppose \(P_{i}(s,1,1) P_{i}(s,0,1) s,i\), \(P_{i}(1,a,1) P_{i}(0,a,1) i,a\), \(R(,)\) monotonic and submodular in \(\) and \(\), and \(g()=_{}R(,)\) submodular in \(\). Then, with oracle access to \(Q_{^{*}}\) we can compute \(}\) in \((N^{2})\) time so \(Q_{^{*}}(,})(1-)Q_{^{*}}(,^{*})\)._

Theorem 1 and 2 demonstrate the difficulty of finding solutions to rmab-gs while Theorem 3 describes a potential panacea via \(Q_{^{*}}\). Leveraging \(Q_{^{*}}\) is a polynomial-time algorithm, but the exponential state space makes it difficult to learn such a \(Q\), motivating the need for better algorithms (we empirically demonstrate this in Section 6.1). In essence, Lemma 3 demonstrates that it is possible to achieve a \(1-\) approximation for the Q-value; however, even computing such an approximation is difficult, and motivates the need for computationally feasible solutions. Our assumptions for Lemma 3 essentially capture the idea that rewards are submodular in both the state \(\) and action \(\); these assumptions are satisfied for all of our synthetic experiments in Section 6. We prove Theorem 1 through reduction from rmab, Theorem 2 through reduction from submodular optimization, and Theorem 3 through induction on value iteration. Full proofs are in Appendix K.

## 4 Linear-Whittle and Shapley-Whittle Indices

### Defining Linear-Whittle and Shapley-Whittle

Because the Whittle index policy is asymptotically optimal for rmabs, we investigate its extension to rmab-gs. We develop two extensions: the Linear-Whittle policy, which computes marginal rewards for each arm, and the Shapley-Whittle policy, which computes Shapley values for each arm. To define Linear-Whittle, we let the marginal reward be \(p_{i}(s_{i})=_{^{}|s^{}_{i}=s_{i}}R_{}(^{},_{i})\), where \(_{i}\) is a vector with 1 at index \(i\) and \(0\) elsewhere. The Linear-Whittle policy linearizes the global reward by optimistically estimating \(R_{}(,)_{i=1}^{N}p_{i}(s_{i})a_{i}\) and computing the Whittle index from this:

**Definition 2**.: _Linear-Whittle - Consider an instance of rmab-g \((,,R_{i},R_{},P_{i},)\). Define \(Q^{L}_{i,w}(s_{i},a_{i},p_{i})=-a_{i}w+R_{i}(s_{i},a_{i})+a_{i}p_{i}(s_{i})+ _{s^{}_{i}}P_{i}(s_{i},a_{i},s^{}_{i})V_{i,w}(s^{} _{i})\), where \(V_{i,w}(s^{}_{i})=_{a^{}_{i}}Q^{L}_{i,w}(s^{}_{i}, a^{}_{i},p_{i})\). Then the Linear-Whittle index is \(w^{L}_{i}(s_{i},p_{i})=_{w}\{w|Q^{L}_{i,w}(s_{i},0,p_{i})>Q^{L}_{i,w}( s_{i},1,p_{i})\}\). The Linear-Whittle policy pulls arms with the \(K\) highest Linear-Whittle indices._

To improve on the Linear-Whittle policy, we develop the Shapley-Whittle policy, which uses Shapley values to linearize the global reward. The Linear-Whittle policy linearizes global rewards via \(p_{i}(s_{i})\), but this approximation could be loose as \(_{i=1}^{N}p_{i}(s_{i})a_{i} R_{}(,)\). The Shapley value, \(u_{i}(s_{i})\), improves this by averaging marginal rewards across subsets of arms (see Shapley et al. ). Letbe the element-wise maximum, so a \(\)\(_{i}\) means arms in \(\) and arm \(i\) are pulled. Define:

\[u_{i}(s_{i})=_{^{}|s^{}_{i}=s_{i}}_{ \{0,1\}^{N},a_{i}=0,\|\|_{1} K-1}\|_{1}!(N-\| \|_{1}-1)!}{}(R_{}(^{},_{i})-R_{}(^{},)) \]

Here, \(R_{}(^{},_{i})-R_{}(^{},)\) captures the added value of pulling arm \(i\), when \(\) are already pulled. While the Linear-Whittle policy estimates the global reward as \(R_{}(,)_{i=1}^{N}p_{i}(s_{i})a_{i}\), the Shapley-Whittle policy estimates the global reward as \(R_{}(,)_{i=1}^{N}u_{i}(s_{i})a_{i}\). The Linear-Whittle policy overestimates marginal contributions, as \(_{i=1}^{N}p_{i}(s_{i})a_{i} R_{}(,)\) (proof in Appendix L), so by using Shapley values, we take a pessimistic approach (by taking the minimum over all \(^{}\)). This approach could lead to more accurate estimates of \(R_{}(,)\) because marginal contributions are averaged across many combinations of arms:

**Definition 3**.: _Shapley-Whittle - Consider an instance of rmab-g\((,,R_{i},R_{},P_{i},)\). Let \(Q^{S}_{i,w}(s_{i},a_{i},u_{i})=-a_{i}w+R_{i}(s_{i},a_{i})+a_{i}u_{i}(s_{i})+ _{s^{}_{i}}P_{i}(s_{i},a_{i},s^{}_{i})V_{i,w}(s^{} _{i})\), where \(V_{i,w}(s^{}_{i}

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

for MCTS Shapley-Whittle. Here, \(R(,)\) is the known current value, and \(_{i=1}^{N}(Q_{i,0}^{L}(s_{i},a_{i})-p_{i}(s_{i})a_{i}-R_{i}(s_{i},a_{i}))\) is the estimated future value when accounting for the present reward \(R(,)\).

## 6 Experiments

We evaluate our policies across both synthetic and real-world datasets. We compare our six policies (Section 4.1, Section 5.2, and Section 5.3) to the following baselines (more details in Appendix B):

1. **Random** - We uniform randomly select \(K\) arms at each timestep.
2. **Vanilla Whittle** - We run the vanilla Whittle index, which optimizes only for \(R_{i}(s_{i},a_{i})\).
3. **Greedy** - We select actions which have the highest value of \(p_{i}(s_{i})\).
4. **MCTS** - We run mcts up to some depth, \(cK\), where \(c\) is a constant.
5. **DQN** - We train a Deep Q Network (dqn) , and provide details in Appendix B.
6. **DQN Greedy** - Inspired by Theorem 3, we first learn a dqn then greedily optimize \(Q(,)\).
7. **Optimal** - We compute the optimal policy through value iteration when \(N 4\).

We run all experiments for 15 seeds and 5 trials per seed. We report the normalized reward, which is the accumulated discounted reward divided by that of the random policy. This normalization is due to differences in magnitudes within and across problem instances and we use discounted rewards due to prior work . We additionally compute the standard error across all runs.

### Results with Synthetic Data

To understand performance across synthetic scenarios, we develop synthetic problem instances that leverage each of the following global reward functions (details on \(m_{i}\) and \(Y_{i}\) choices in Appendix A):

1. **Linear** - Given \(m_{1},,m_{N}\), then \(R_{}(,)=_{i=1}^{N}m_{i}s_{i}a_{i}\).
2. **Probability** - Given \(m_{1},,m_{N}\), then \(R_{}(,)=1-_{i=1}^{N}(1-m_{i}a_{i}s_{i})\).
3. **Max** - Given \(m_{1},,m_{N}\), then \(R_{}(,)=_{i}s_{i}a_{i}m_{i}\).
4. **Subset** - Given sets, \(Y_{1},,Y_{N}\{1,,m\}\), \(m\), then \(R_{}(,)=|_{i s_{i}a_{i}=1}Y_{i}|\).

We select these rewards, as they range from linear or near-linear (_Linear_ and _Probability_) to highly non-linear (_Max_ and _Subset_). The reward functions can also be employed to model real-world situations; the _Probability_ reward models food rescue and the _Subset_ reward models peer review (see Section 3). For all rewards, we let \(R_{i}(,)=s_{i}/N\) and \(=\{0,1\}\), which matches prior work . We construct synthetic transitions parameterized by \(q\), so \(P_{i}(0,0,1) U(0,q)\), where \(U\) is the uniform distribution (see Appendix A). We construct transition probabilities so that having \(a_{i}=1\) or

Figure 1: We compare baselines to our index-based and adaptive policies across four reward functions. All of our policies outperform baselines. Across all rewards, our best policy is within 3% of optimal for \(N=4\). Among our policies, Iterative and MCTS Shapley-Whittle consistently perform best.

\(s_{i}=1\) only improves transition probabilities (see Wang et al. ). We vary \(q\) in Appendix G and find that our choice of \(q\) does not impact findings. Unless otherwise specified, we fix \(K=N/2\).

Figure 1 shows that our policies consistently outperform baselines. Among our policies, Iterative and MCTS Shapley-Whittle policies perform best. For \(N=10\), Iterative and MCTS Shapley-Whittle significantly outperform baselines (paired t-test \(p<0.04\)) and index-based policies except for the _Linear_ reward (\(p<0.001\)). dqn-based policies regress from on average \(4\%\) worse for \(N=4\) to on average \(9\%\) worse for \(N=10\) compared to our best policy, which occurs due to the exponential state space. In Appendix I, we consider more combinations of \(N\) and \(K\) and find similar results.

To understand policy performance across rewards, we analyze how reward linearity impacts performance. We quantify linearity via \(_{}\) (see Theorem 6) and compute \(_{}\) for _Subset_ instances varying in \(Y_{i}\) (see Appendix A). For non-linear reward functions (small \(_{}\)), Iterative and MCTS-Shapley-Whittle are best, outperforming Linear-Whittle by 56%, while for near-linear reward functions (large \(_{}\)), all policies are within 18% of each other (Figure 2, \(N=10\)). Appendix H shows that, for some rewards, adaptive policies outperform non-adaptive policies by 50%.

To understand the tradeoff between efficiency and performance we plot the time taken and normalized reward for various \(N\) (Figure 3). We evaluate variants of Iterative Shapley-Whittle and MCTS Shapley-Whittle because both perform well yet are computationally expensive. For Iterative Shapley-Whittle, 1000 samples are used for Shapley estimation by default, so we consider variants that use 100, 10, and 1 samples. For MCTS Shapley-Whittle, we run 400 iterations of MCTS by default, so we consider variants that run 40 and 4 iterations. Iterative Shapley-Whittle policies are the slowest

Figure 3: We compare the efficiency and performance of Iterative Shapley-Whittle methods which vary in Monte Carlo samples used for Shapley estimation and MCTS Shapley-Whittle methods which vary in MCTS iterations. While Iterative Shapley-Whittle is the slowest, decreasing the number of Monte Carlo samples can improve efficiency without impacting performance.

Figure 2: We plot policy performance for instances of the _Subset_ reward which vary in linearity. We see that the Iterative and MCTS Shapley-Whittle policies outperform alternatives for non-linear rewards (small \(_{}\)) while policy performances converge for linear rewards (large \(_{}\)).

but can be made faster without impacting performance by reducing the number of samples. Our results demonstrate how we can develop efficient adaptive policies which perform well.

### Results with Real-World Data in Food Rescue

To evaluate our policies in real-world contexts, we apply our policies to a food rescue dataset. We leverage data from a partnering multi-city food rescue organization and construct an rmab-g instance using their data (details in Appendix D). Here, arms correspond to volunteers, and states correspond to engagement, with \(s_{i}=1\) indicating an engaged volunteer. Notifications are budget-limited because sending out too many notifications can lead to burned-out volunteers.

We compute transition probabilities from volunteer participation data and compute match probabilities from volunteer response rates. Using this, we study whether our policies maintain a high trip completion rate and volunteer engagement. We assume that food rescue organizations are indifferent to the tradeoff between trip completion and engagement. Because such an assumption might not hold in reality, we explore alternative tradeoffs in Appendix I. If volunteer \(i\) is both notified (\(a_{i}=1\)) and engaged (\(s_{i}=1\)), then they match to a trip with probability \(m_{i}\). We do so because unengaged volunteers are less likely to accept food rescue trips. Under this formulation, our global reward is the probability that any volunteer accepts a trip, which is exactly the _Probability_ reward. For engagement, we let \(R_{i}(s_{i},a_{i})=s_{i}/N\), which captures the fraction of engaged volunteers. We model two food rescue scenarios, and detail scenarios with more volunteers in Appendix J:

**Notifications** - Food rescue organizations deliver notifications to volunteers as new trips arise . To model this scenario, we consider \(N=100\) volunteers and \(K=50\) notifications.

**Phone Calls** - When a trip remains unclaimed after notifications, food rescue organizations call a small set of experienced volunteers. We model this with \(N=20\), \(K=10\), and using only arms corresponding to experienced volunteers with more than 100 trips completed (details in Appendix D).

In Table 1, we compare our policies against baselines and find that our policies achieve higher normalized rewards. All of our policies outperform all baselines by at least 5%. Comparing between policies, we find that Iterative and MCTS Shapley-Whittle perform best, performing slightly better than the Shapley-Whittle policy. Currently, food rescue organizations perform notifications greedily, so an rmab-g framework could improve the engagement and trip completion rate.

## 7 Related Works and Problem Setup

**Multi-armed Bandits** In our work, we explore multi-armed bandits with two properties: combinatorial actions  and non-separable rewards. Combinatorial bandits can be solved through follow-the-perturbed-leader algorithms , which are necessary because the large action space causes traditional linear bandit algorithms to be inefficient . Prior work has tackled non-separable rewards in slate bandits , through an explore-then-commit framework, and adversarial combinatorial bandits , from a minimax optimality perspective. In this work, we leverage the structure of rmab-gs to tackle both combinatorial actions and non-separable rewards.

    & Vanilla Whittle & Greedy & MCTS & DQN & Linear Whittle \\  Notifications & 0.975 \(\) 0.012 & 1.829 \(\) 0.016 & 1.047 \(\) 0.012 & 1.547 \(\) 0.025 & **1.932 \(\) 0.017** \\ Phone Calls & 0.989 \(\) 0.009 & 1.228 \(\) 0.008 & 1.070 \(\) 0.016 & 1.209 \(\) 0.007 & 1.288 \(\) 0.008 \\    
    & Shapley Whittle & Iter. Linear & Iter. Shapley & MCTS Linear & MCTS Shapley \\  Notifications & 1.931 \(\) 0.016 & 1.921 \(\) 0.017 & & & & \\ Phone Calls & 1.290 \(\) 0.008 & 1.287 \(\) 0.009 & **1.294 \(\) 0.009** & 1.291 \(\) 0.008 & 1.293 \(\) 0.008 \\   

Table 1: We evaluate policies in real-world food rescue contexts by developing situations that mirror notifications (\(N=100,K=50\)) and phone calls (\(N=20,K=10\)). Our policies outperform baselines for both situations and achieve similar results due to the nearly linear reward function.

**Submodular Bandits** Additionally related to our work are prior investigations into submodular bandits. Submodular bandits combine submodular functions with bandits and have been investigated from an explore-then-commit perspective  and through upper confidence bound (UCB) algorithms . Similar to prior work, we focus on optimizing submodular functions under uncertainty. However, prior work focuses on learning the reward structure when the submodular function is unknown, while we focus on learning optimal decisions when state transitions are stochastic.

**Restless Multi-Armed Bandits**rmabs extend multi-armed bandits to situations where unpulled arms can transition between states . While finding optimal solutions to this problem is PSPACE-Hard , index-based solutions are efficient and asymptotically optimal . As a result, rmabs have been used across a variety of domains  and have been deployed in public health scenarios . While prior work has relaxed assumptions by considering rmabs that have contexts , combinatorial arms , graph-based interactions , and global state , we focus on combinatorial situations with a global reward.

## 8 Limitations and Conclusion

**Limitations** - While we present approximation bounds for index-based policies, our adaptive policies lack theoretical guarantees. Developing bounds for adaptive policies is difficult due to the difficulty in leveraging Whittle index optimality. However, these bounds may explain the empirical performance of adaptive policies. Our empirical analysis is limited to one real-world dataset (food rescue). We aimed to mitigate this by exploring different scenarios within food rescue, but empirical evaluation in domains such as peer review would be valuable. Finally, evaluating policies when global rewards are neither submodular nor monotonic would improve our understanding of rmab-g.

**Conclusion** - rmabs fail to account for situations with non-separable global rewards, so we propose the rmab-g problem. We tackle rmab-g by developing index-based, iterative, and MCTS policies. We prove performance bounds for index-based policies and empirically find that iterative and MCTS policies perform well on synthetic and real-world food rescue datasets. Our results show how rmabs can extend to scenarios where rewards are global and non-separable.