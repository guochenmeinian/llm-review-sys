# _AQuA_: A Benchmarking Tool for Label Quality Assessment

Mononito Goswami\({}^{*}\), Vedant Sanil, Arjun Choudhry\({}^{}\),

**Arvind Srinivasan\({}^{}\)**, **Chalisa Udompanyawit**, **Artur Dubrawski**

Auton Lab, School of Computer Science

Carnegie Mellon University

{mgoswami, vsanil, arjuncho, arvindsr, cudompan, awd}@cs.cmu.edu

{www.github.com/autonlab/aqua}

MG and VS contributed equally. MG is the corresponding author.AC and AS have equal contribution.

###### Abstract

Machine learning (ML) models are only as good as the data they are trained on. But recent studies have found datasets widely used to train and evaluate ML models, e.g. _ImageNet_, to have pervasive labeling errors. Erroneous labels on the train set hurt ML models' ability to generalize, and they impact evaluation and model selection using the test set. Consequently, learning in the presence of labeling errors is an active area of research, yet this field lacks a comprehensive benchmark to evaluate these methods. Most of these methods are evaluated on a few computer vision datasets with significant variance in the experimental protocols. With such a large pool of methods and inconsistent evaluation, it is also unclear how ML practitioners can choose the right models to assess label quality in their data. To this end, we propose a benchmarking environment AQuA to rigorously evaluate methods that enable machine learning in the presence of label noise. We also introduce a design space to delineate concrete design choices of label error detection models. We hope that our proposed design space and benchmark enable practitioners to choose the right tools to improve their label quality and that our benchmark enables objective and rigorous evaluation of machine learning tools facing mislabeled data.

## 1 Introduction

A lot of machine learning (ML) research is devoted to making efficient and effective use of available data to learn accurate, high-fidelity, and interpretable models, with little to no focus on the quality of the data they are trained and evaluated on. Nonetheless, it is widely recognized that ML models are only as good as the data they rely on, i.e., the quality of data imposes practical limits to what ML models can achieve. Not only are datasets used to train ML models; they also serve as benchmarks to measure the state-of-the-art and validate theoretical findings. Thus, high quality large labeled datasets are the cornerstone of progress in supervised machine learning. However, the data is rarely free of noise, which can both manifest in the features of the data (feature noise) and in labels that categorize them (label noise). Between feature and label noise, the former has been found to be much more harmful to machine learning models . To make matters worse, label noise is prevalent in popular ML benchmarks. A recent study estimated an average of at least 3.3% label errors across 10 datasets commonly used for benchmarking computer vision, natural language, and audio classification algorithms . Consequently, a growing body of research is devoted to understanding the harms of label noise and to developing techniques to identify and mitigate labeling errors.

In recent years, over **50** papers have been written on this topic, including **6** surveys, yet the literature lacks a comprehensive benchmark to evaluate the available methods. The evaluation of existing methods is lacking along the following dimensions:

**Arbitrary choice of datasets and limited data modalities.** To the best of our knowledge, relevant studies have used over **40** datasets (e.g., ImageNet ) and their variations (e.g., Imagenette , ImageNet-100 ) for evaluation, but mostly on computer vision related tasks, with less than **15** studies using text data, **7** using tabular data and only **1** paper using time-series data.

**Arbitrary choice of classification models.** The ultimate goal of identifying labeling errors is to learn a classification model using training data with clean labels. Much like the datasets, relevant studies have used over **47** different classification architectures (e.g., ResNet , MobileNet , ResNeXt , BERT , XLM-RoBERTa , etc) to measure the impact of label cleaning.

**Inconsistent evaluation protocols and metrics.** Different studies conduct different experiments to measure the efficacy of their proposed methods (e.g., the accuracy of the label cleaning method, or performance of the downstream model before and after label cleaning, etc.) and use various measures of success (e.g., high accuracy, \(F_{1}\)-score, or low error rate).

With such diversity and inconsistency in the way in which these methods are evaluated, it is hard to measure the state of the art. To bridge this gap, we propose the Annotation Quality Assessment, AQuA, the _first_ benchmark framework to evaluate machine learning methods in the presence of label noise (Fig. . We also elucidate the design space for such models, with the hope that it will not only foster future research on detecting labeling errors, but also enable ML practitioners to choose the appropriate label cleaning tools for their specific data and tasks. We run a large-scale experiment (> **1000** unique experiments) and make several interesting observations, demonstrating AQuA's efficacy in benchmarking machine learning models in the presence of label noise.

## 2 Background and Problem Formulation

**Sources of labeling errors.** Labeling errors can arise from automated labeling processes such as crowd-sourcing , programmatic weak supervision , and human error (e.g., due to lack of expertise or low confidence in expert assessment) . Errors may also stem from idiosyncrasies

Figure 1: _Overview of the AQuA benchmark framework._AQuA comprises of datasets from **4** modalities, **4** single-label and **3** multi-annotator label noise injection methods, **4** state-of-the-art label error detection models, classification models, and several evaluation metrics beyond metrics of predictive accuracy. We are in the process of integrating several fairness, generalization, and robustness metrics into AQuA. The red and blue arrows show two example experimental pipelines for image data and time-series data, respectively.

of the annotation procedure and the corresponding guidelines themselves . Finally, existing labels may also become inconsistent with prevailing knowledge due to constantly evolving problem definitions and domain knowledge leading to concept drift.

**Impact of labeling errors.** At training time, labeling errors can cripple an ML model's ability to generalize and introduce undesirable biases in its hypothesis space . Mislabeled training data is especially problematic for over-parameterized deep neural networks, which can achieve zero training error even on randomly-assigned labels . At test time, labeling errors can lead to noisy model evaluations and invalidate common model selection strategies. In safety-critical settings, models trained, evaluated, and selected using mislabeled data can be ineffective at best and can lead to disastrous outcomes at worst. Finally, recent studies in the context of fairness have shown that naively enforcing parity constraints based on noisy labels can harm groups that are unaffected by label noise .

**Problem formulation.** Due to the far-reaching consequences that labeling errors can have on model training and evaluation, the literature has attacked multiple different but related problems, for example: (1) _label error detection_, identify which data points have erroneous labels , (2) _label noise estimation_, estimate the proportion of data with noisy labels , (3) _label noise robust learning_, learn models robust to label noise , and (4) _noise transition matrix estimation_, estimate the parameters of the noisy label generation process .

In this work, we focus on the **label error detection problem**, because (a) it is the most _general_ of the above problem types, i.e., with knowledge of labeling errors, we can estimate the noise rate, parameters of the noise generation process and train ML models free from label noise, (b) it provides practitioners greater visibility of issues that plague their data, and (c) allows them to directly rectify these errors.

**Label error detection problem:** Assume a dataset \(^{*}=\{(_{i},y_{i}^{*})\}_{i=1}^{N}(, )\), where \(_{i}\) and \(y_{i}^{*}\) denote the features and labels, respectively. In practice, we do not have access to \(^{*}\), but instead observe a noisy dataset \(=\{(_{i},y_{i})\}_{i=1}^{N}(,^{ }\!\!_expert_ who can be queried to relabel suspicious data points . Some other methods assume access to data points called _anchor points_, which most certainly belong to a particular class . The number of anchor points required is generally proportional to the number of classes, and quickly becomes prohibitive for multi-class classification problems, and in more complicated noise settings . Finally, a vast majority of methods assume _access to classification models_, and primarily differ in their _number_ (model-free , one or multiple models ), _nature of access_ (prediction-only  versus access to logits , gradients , etc.), and _extent of pre-training_ (no pre-training  versus large-scale pre-training e.g. large language models ).

**What modeling assumptions can you make?** Different studies use different assumptions on _data_ (noise structure and clusterability), _heuristics_ (model self-confidence and perceptual uncertainty), and _modeling decisions_ (whether to explicitly model the transition matrix and multi-network training). Most studies in the literature explicitly assume some form of structure in the noise present in the data . Most early studies assumed class-dependent noise, i.e., the likelihood of error is only dependent on the latent true class, not on the data . There is growing interest in more realistic forms of noise where the probability of error also depends on the features of a data point (instance-dependent noise) . To this end, some recent studies have shown promising results by leveraging natural notions of similarity between data points and their labels. For example, Zhu et al.  assume that examples with similar features should have similar labels.

Many studies treat a trained model's low confidence that a data point belongs to its observed label as a heuristic likelihood to identify labeling errors . In a similar vein, a recent study used the loss of a pre-trained large language model on each data point to identify mislabeled examples . When multi-annotator labels are available, as discussed before, some studies have also used them to model the perceptual uncertainty in the annotators to identify labeling errors.

Finally, studies differ in their modeling decisions. While some explicitly estimate a data structure called the noise transition matrix, which encodes the joint probability of latent true and observed noisy labels , others do not . Finally, there is a body of work on label noise robust learning using multiple model instances either using knowledge distillation  or meta-learning . The key idea is to use a cooperative game between models to identify labeling errors and ensure that the eventually deployed model only learns from clean data.

**What outputs do you want, and what would you do with them?** All labeling error detection models identify data points that are likely to be labeling errors. With knowledge of the potentially mislabeled data points, most studies simply remove them from consideration . This strategy may be practical for large datasets, where only a small fraction of data is found to be mislabeled and domain experts are unavailable for supervision. We use this strategy by default in AQuA. A smaller number of methods predict the alternate class that the data point is most likely to belong to  and even provide explanations for their predictions . CINCER  is one of the few methods which not only finds labeling errors but also identifies counter-examples in the training data to serve as explanations for its suspicion. Some studies use the label predicted by these models and perform loss re-weighting or correction to learn robust classification models . When domain experts are available, some studies also leverage their insight to re-label mislabeled data point .

Figure 3: _Design space of labeling error detection models to delineate concrete design choices._

## 4 Benchmark Design

### Real-world, Popular Datasets, and Downstream Classification Models

**Datasets.**AQuA currently comprises of a collection of **17** popular real-world public datasets from **4** prevalent data modalities: _image_, _text_, _time-series_ and _tabular_. To evaluate label error detection models across various practical scenarios, we carefully choose datasets with diversity in the following characteristics: (1) _classification problems_ (_e.g._, sentiment classification vs. hate speech detection), (2) _number of classes_ (binary vs multi-class classification), (3) _relative prevalence of classes_ (e.g., skewed datasets like Credit Card Fraud  and balanced ones like IMDb ), (4) _sources of annotations_ (e.g., human vs rule-based annotation), and (5) _number of annotations per example_ (e.g., CIFAR-10N labeled by 3 annotators). Table  summarizes the key characteristics of datasets included as a part of AQuA. In particular, to make comparison with prior work easier while maintaining diversity across practical scenarios, we try to include datasets that have been used frequently by prior work (see usage in Table  and preprocess them in a manner consistent with those works. We do not use any data augmentation during training. App [A.3] provides detailed descriptions of the datasets.

**Classification models.** The ultimate goal of label cleaning is to train accurate downstream classifiers, but different studies use different classification models to measure the efficacy of their proposed label cleaning methods. To provide a level playing field for all cleaning methods, we include multiple classification model architectures for each data modality. Specifically, we include ResNet-18 , MobileNet  and FastViT-T8  for image datasets, all-distilroberta-v1  and all-MiniLM-L6-v2  for text datasets, ResNet-1D, PatchTST  and LSTM Fully Convolutional Network  for time-series datasets, and TabTransformer  and a Multi-Layer Perceptron for tabular datasets. While choosing classification models we prioritized _performant_ methods with (1) _different architectures_ and _inductive biases_, (2) ideally _pre-trained_ using different strategies, and (3) _previously-used_ either by label cleaning methods or task-relevant papers. App [A.4] and App [A.5] provide a detailed description of classification models and their hyperparameters, respectively.

### Advanced Label Error Detection Methods

AQuA provides easy-to-use Application Programmer Interfaces (Fig. 4) for **4** state-of-the-art label error detection methods, namely Area Under Margin ranking (AUM) , Confident Learning , Contrastive and Influent Counter Example Strategy (CINCER) , and Model-free Label Error Detection (SimiFeat) . Below, we provide a brief overview of these methods and their key ideas.

**Area Under the Margin Ranking (AUM) .** Given noisy data and access to the logits of a deep learning model, AUM exploits differences in training dynamics of clean and mislabeled samples to identify labeling errors. The key idea is to identify data points that do not contribute to the generalization of a model as labeling errors by leveraging the delicate tension between the label of a data point (via memorization) and its predicted label (via gradient updates), measured as the margin between the logits of a sample's assigned class and its highest unassigned class.

    & **Dataset** & **\# Train Test** & **\# Annotator/sample** & **LabelScore** & **Classification Task** & **Sample Size** & **Usage** \\   & CIFAR-1001 & 50K / 10K & 3 & Human annotation & Object & \(22 23 3\) & **50K** \\  & CIFAR-1001 & 0 / 10K & 47-63 & Human annotation & Object & \(32 32 3\) & **50K** \\  & CIFAR-1001 & 100K & 1 & N-labeled & Deep-labeled & ImageNet & \(20 256 10 1\) & **50K** \\  & NoisyCereb & 2867 / 3K & 1-3C & Human expert annotation & Pneumonia & \(102 102 1\) & **50K** \\   & BMC  & 25K / 25K & 1 & Human annotation & Sentiment & - & **CINCER** \\  & TextNet  & 100K & 1 & Human annotation & Hate speech & - & - \\   & Credit Card Fraud  & 248K & 1 & Human annotation & Credit card fraud & 28 & **65K** \\  & Adult  & 48K & 1 & Risk-based extraction & Salary & 14 & **60K** \\  & Day Benefit  & 18 & 1 & Vision system-based annotation & Brown variety & 17 & **100K** \\  & Car Evaluation  & 1K & 1 & Hierarchical decision model & CM condition & 6 & **60K** \\  & Monkown  & 8K & 1 & - & Mahonen ethnicity & 22 & **60K** \\  & COMPAN  & 6K & 1 & - & Recistituent & 28 & **60K** \\   & Credit Card & 7K / 10K & 1 & Hierarchical human tree &  &  &  \\  & Ensemble-Object & 49K / 7K & 1 & Human annotation & & & \\  & ShuffleNet  & 23K / 4K & 1 & Human expert annotation &  &  &  \\  & PathPoint & 7K / 3K & 1 & Human annotations & Handwritten digit & \(16 1\) & - \\   & WhichCab & 21K & 1 & 1 & & & \\   

Table 1: **Summary of datasets.**AQuA currently includes a variety of datasets for different classification problems, varying in the number of classes, sources of annotations, and data modalities. All datasets except those marked with \(\) are multi-class.

**Confident Learning (**C**on) . Given noisy data, confident learning estimates a data structure called _confident joint_, which is the joint probability distribution of observed noisy and latent true labels. The key idea is to leverage a model trained on held-out data drawn from the same (or similar) distribution to predict the probability that an example \(_{i}\) belongs to its observed label \(_{i}\). A low probability is then used as a heuristic-likelihood of \(_{i}\) being a label error. The confident joint can then be used to identify labeling errors and estimate the noise rate.

**Contrastive and Influent Counter Example Strategy (**C**ncer** or **C**in) . CINCER treats the problem of identifying labeling errors as a sequential decision making problem where a domain expert can be queried to relabel suspicious examples. CINCER uses the same heuristic as AUM to identify labeling errors, but also identifies counter-examples in the data to serve as explanations of the model's suspicion.

**Model-free Label Error Detection (**SimiFeat) .** Unlike other methods, SimiFeat does not need a (pre-)trained model to identify labeling errors. Instead, it utilizes labels of the \(k\) nearest neighbors to identify labeling errors based on the _clusterability_ assumption, _i.e._ data points with similar features should have the same true label with high probability.

There are many methods to detect labeling errors, but we choose these methods as a _starting point_ because they are recent, state-of-the-art, and have different inputs and core assumptions. While all these methods have existing public implementations, through AQuA, our goal is to create a one-stop shop for using and evaluating open-source label error detection models.

### Evaluation

There is significant variance in the ways that label cleaning methods are evaluated. To rigorously, fairly, and systematically assess these models, we unify the breadth of experimental settings through the following three dimensions of evaluation.

**Supervision.** Identifying labeling errors in practice is an _unsupervised_ problem since we do not know which data points are mislabeled. Hence, evaluating these methods is a challenging endeavor. Most studies in the literature gather noise labels either from human experts (_human-in-the-loop evaluation_) or by introducing synthetic label noise by design (_synthetic label noise_).

In human-in-the-loop evaluation, one or more human experts are asked to independently assess the true labels of data points identified as having erroneous labels . While this is a straightforward and precise evaluation method, it is in general unscalable, expensive, time-consuming, and limited to only measuring the _precision_ of models (and not _recall_), because the experts are typically only shown data points which a model considers erroneous.

A much more common and scalable way of evaluating these methods is to introduce various kinds of synthetic label noise and measure a model's ability to detect them. There are many ways of introducing label noise, but injected noise may not always be reflective of the true noise that occurs in natural datasets, and hence identifying realistic noise injection strategies is an active area of research . Moreover, model evaluation may still be noisy because there may be mislabeled examples for which our pseudo-noise labels are negative (or _correctly labeled_).

**Hypotheses.** In general, existing studies evaluate two hypotheses: (1) _cleaning labels on the train set improves the performance of the downstream classifier on the test set_, and (2) _cleaning methods can accurately identify mislabeled data on the train set_. Hypothesis 1 is practical since the primary goal of identifying labeling errors is to train accurate and unbiased classifiers. However, appropriately regularized deep learning models are known to be naturally robust to some label noise. Hence, hypothesis 2 allows researchers to directly measure the efficacy of label cleaning techniques.

Figure 4: AQuA makes identifying label issues, and evaluating new and existing label error detection models simple.

**Measures of goodness.** Different studies use different measures of predictive accuracy. While some measure error rate , others report the accuracy  or ROC-AUC  of their classification models. Similarly, for their cleaning methods, some studies report the \(F_{1}\) score while others report the precision or recall .

**More gaps in evaluation.** In addition to the lack of consistency, we believe that the experimental settings in many studies are occasionally (1) _unrealistic_, e.g., adding label noise to more than half (sometimes up to \(80\%\)) of the data points ; and (2) _uni-dimensional_, e.g., reporting only one metric of predictive performance.

AQuA's design.** To enable a realistic, multi-faceted and holistic evaluation of label error detection models, we implement **7** popular label noise injection techniques and multiple metrics of predictive performance. Specifically, for single-label datasets, we implement asymmetric , class-dependent , instance-dependent , and uniform  noise, and for datasets with labels from multiple annotators, we implement dissenting label, dissenting worker, and crowd majority . In terms of metrics of predictive accuracy, we implement \(F_{1}\), accuracy, (_weighted_) precision, recall, area under ROC curve (ROC-AUC), average precision (PR-AUC), and error rate. We are in the process of implementing some other metrics beyond predictive accuracy, such as generalization  and robustness  of models. Our hope is that AQuA's _config-driven_ design will allow non-technical users to integrate it into their labeling workflows and researchers to add new models, datasets, and evaluation pipelines seamlessly. Our choice of datasets and downstream classifiers ensures that the computational complexity of running experiments is not prohibitive. Finally, we make all code, pre-trained models, and experimental logs open-source to enable rigorous and fair evaluation of models.

## 5 Experiments, Results and Discussion

    &  &  &  &  \\   & AMR & CIN & CON & SILP & AMR & CIN & CON & SILP & AMR & CIN & CON & SILP & AMR & CIN & CON & SILP \\  CIFAR-10 & 73.3 & 74.1 & 45.6 & **76.7** & 74.3 & 70.8 & 47.7 & **75.5** & 93.5 & 80.5 & 42.6 & **93.6** & 68.0 & 69.9 & 44.8 & **70.9** \\ Clothing-100K & 70.0 & 70.0 & **76.6** & 76.5 & 74.2 & 68.4 & 73.6 & **75.7** & 76.3We conduct several experiments to support AQuA's design choices and demonstrate its utility in providing a comprehensive and holistic evaluation of machine learning models in the presence of label noise.

**Experimental Setup and Hyper-Parameter Tuning.** We run experiments for all combinations of cleaning methods (AUM (AUM), confident learning (CON), CINCER (CIN) and SimiFeat (SIM), including no label cleaning (NON), noise types (_asymmetric_, _class-dependent_, _instance-dependent_ and _uniform_); for four different noise rates (0%, 2%, 10% and 40%), for a total of **2400 unique experiments**. We conduct experiments using three distinct classification architectures for image and time-series data, and two different architectures for text and tabular data. To account for class imbalance in some datasets, we report the \(F_{1}\) weighted by the support of each class. Results for all other evaluation metrics can be found in App. A.8 We also adopt critical difference diagrams  to succinctly represent comparisons between multiple cleaning methods and other independent variables (e.g., data modality and noise type) on multiple datasets. These diagrams represent the average ranks of methods across datasets while grouping those with insignificant difference . We tuned hyper-parameters of all the classification and cleaning methods till they performed reasonably well on average on all the datasets using hyper-parameter grids used by prior work and reported in App. A.9 Finally, all our experiments were carried out on a computing cluster, with a typical machine having 128 AMD EPYC 7502 CPUs, 503 GB of RAM, and 8 NVIDIA RTX A6000 GPUs.

**Research Questions.** We aim to answer the following research questions through our experiments:

* _Which is the best cleaning method in terms of (i) its ability to identify synthetically injected label noise, and (ii) performance of the downstream classifier trained its cleaned labels?_
* _Do the rankings of cleaning methods differ across different (i) types of synthetic label noise, (ii) data modalities, and (iii) evaluation metrics (weighted \(F_{1}\) versus accuracy)?_

### Insights from Large-scale Experiments using AQuA

Tables 3, 2 and Fig. 5 report results from all our experiments aggregated by noise rate, and downstream classification models. Below we highlight some of our key findings. Due to lack of space, we defer finer grained results to App. A.8

**Best cleaning method.** Overall, we found SimiFeat (SIM)  to be the best cleaning method in terms of its ability to identify synthetically injected label noise, closely followed by CINCER (CIN)  (Fig. 5, _i_)). However, these differences shrink when evaluating cleaning methods using the performance of the downstream model trained using their cleaned labels (Fig. 5, _ii_)). Confident learning (CON)  consistently performed the worst among all the evaluated methods.

    &  &  &  &  &  &  &  &  \\   &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\  CIFAR-10 & **34** & 74.1 & 73.0 & 48.0 & 73.5 & 63.2 & 62.6 & **66.0** & **36.0** & 68.4 & 85.1 & **63.2** & 58.3 & **48.1** & 71.0 & **73.2** & 0.3 & 46.2 & 67.5 & 57.7 & 60.2 & **62.2** & **54.0** & 56.8 \\ Cooking-100k & **96.

**Deep learning models are inherently robust to label noise.** Perhaps unsurprisingly, we found that most downstream classifiers were reasonably robust to synthetic label noise, as can be seen from the insignificant difference between the setting where datasets were not explicitly cleaned (NON), compared to when they were cleaned using SIM, CIN and AUM. These results also illustrate the importance of measuring both hypotheses (performance of cleaning methods versus downstream models) when evaluating the performance of ML models in the presence of label noise.

**Adding label noise can sometimes improve model performance.** In the context of class-dependent or uniform noise, label noise serves as regularization to prevent models from overfitting. This phenomenon is not specific any one modality, but happens for multiple modalities, datasets, and noise types too, for example Electric Devices (time-series) under uniform noise, MIT-BIH (time-series), and Dry Bean (tabular) for class-dependent noise, in Table 16. Moreover, deep learning optimization is highly non-convex, so adding some noise might help the model reach the global minima by traversing an alternative path within the loss landscape.

**Impact of AQuA's design choices.** We found that cleaning methods perform differently for different data modalities. For instance, all cleaning methods barring CON perform on par on image datasets _(iii)_, but on tabular data _(iv)_, AUM performs significantly worse than CIN and SIM. This may be due to a variety of reasons beyond cleaning methods: size and nature of datasets, inductive biases of downstream classifier, and the quality of feature representations . We also observed that some types of label noise are easier to detect than others. For example, uniform noise and asymmetric noise were the easiest to detect, cleaning methods found it much hard to detect instance and class-dependent noise _(vi)_. Finally, we noticed differences in model rankings when measuring different evaluation metrics. As an example, the difference between CIN and AUM vanishes when we measure the accuracy _(v)_ of the cleaning methods instead of their weighted \(F_{1}\)_(i)_. These findings highlight the need to evaluate label error detection methods across multiple datasets from different modalities, noise types and evaluation metrics.

## 6 Conclusion and Future Work

We propose the first benchmark designed to rigorously evaluate machine learning models in the presence of label noise. We also elucidate the design space of these methods to not only enable ML practitioners to choose the right label cleaning tool for their data, but also foster academic research on the label noise problem. We demonstrate AQuA's utility by running large-scale experiments to clean several interesting findings. We believe that, as a benchmarking toolkit, AQuA would benefit from more cleaning methods, datasets, synthetic label noise injection strategies, and evaluation metrics.

Our short-term goals include experimenting with multi-annotator label noise, measuring the impact of feature noise on time-series and image data in comparison to label noise, incorporating several metrics for model generalization, robustness and fairness, and including audio datasets. While other types of noise are beyond the scope of this work, we believe that multi-annotator, multi-class multi-label, and noise in regression problems are exciting avenues of future work, and AQuA's modular design will enable researchers to experiment with both multi-annotator and multi-class multi-label classification problems easily. We restrict ourselves to multi-class but single-label classification (as opposed to multi-label classification).

We believe that future work on label error detection should address label issues in the multi-label classification and regression settings. We believe that our work on AQuA can both harness and facilitate the development of foundation models in the two ways: (1) foundation models can be used to identify labeling errors, without explicit supervision, and (2) methods within AQuA can be use to identify labeling errors which can affect foundation model pre-training and fine-tuning. We also believe that future work should

## 7 Limitations, Biases, and Social Impacts

We acknowledge the potential adverse impact of large-scale experimentation on the environment, but believe that our publicly accessible code and experimental findings can significantly reduce resource consumption for ML practitioners in this field. Label error detection models might perpetuate existing biases and impact the fairness of models. We included the Adult dataset, that is frequently used in the fairness literature, in AQuA, to evaluate the impact of label errors on the fairness of models. Wewould also like to acknowledge that our experiments were carried without extensive hyper-parameter tuning. Moreover, hyper-parameters for cleaning methods and downstream classifiers were chosen based on model performance on the observed training set and fixed throughout the training process. We futher discuss these design choices and their limitations in Appendix A.6