# Mission Impossible: A Statistical Perspective on

Jailbreaking LLMs

 Jingtong Su

NYU & Meta AI, FAIR

Correspondence to: Jingtong Su <js12196@nyu.edu>.

Julia Kempe

NYU & Meta AI, FAIR

Karen Ullrich

Meta AI, FAIR

Equal senior authorship.

###### Abstract

Large language models (LLMs) are trained on a deluge of text data with limited quality control. As a result, LLMs can exhibit unintended or even harmful behaviours, such as leaking information, fake news or hate speech. Countermeasures, commonly referred to as preference alignment, include fine-tuning the pretrained LLMs with carefully crafted text examples of desired behaviour. Even then, empirical evidence shows preference aligned LLMs can be enticed to harmful behaviour. This so called jailbreaking of LLMs is typically achieved by adversarially modifying the input prompt to the LLM. Our paper provides theoretical insights into the phenomenon of preference alignment and jailbreaking from a statistical perspective. Under our framework, we first show that pretrained LLMs will mimic harmful behaviour if present in the training corpus. **Under that same framework, we then introduce a statistical notion of alignment, and lower-bound the jailbreaking probability, showing that it is unpreventable under reasonable assumptions.** Based on our insights, we propose an alteration to the currently prevalent alignment strategy RLHF. Specifically, we introduce a simple modification to the RLHF objective, we call _E-RLHF_, that aims to increase the likelihood of safe responses. _E-RLHF_ brings no additional training cost, and is compatible with other methods. Empirically, we demonstrate that _E-RLHF_ outperforms RLHF on all alignment problems put forward by the AdvBench  and HarmBench project  without sacrificing model performance as measured by the MT-Bench project .

## 1 Introduction

Large Language Models (LLMs) have revolutionized the field of deep learning due to their remarkable capabilities across various domains, serving as assistants, in code generation , healthcare , and theorem proving . The training process of a LLM typically includes two stages: pretraining with massive corpora, and an alignment step using Reinforcement Learning from Human Feedback (RLHF) to further _align_ model behavior with human preferences. The latter step typically involves large amounts of humanly annotated data, and can be decomposed into a supervised fine-tuning (SFT) step, a reward modeling step, and an RL Fine-Tuning step. Despite their ability to perform multiple tasks effectively, LLMs are susceptible to generating offensive or inappropriate content including hate-speech, malware, fake information or social biases, due to the unavoidable presence of harmful elements within their pretraining datasets . Social media showcase an abundance of tricks on how to attack ChatGPT  to elicit harmful responses, _e.g._, the "Do Anything Now" (DAN) prompts  or the "Grandma Exploit" hack . On the other hand, behavior diversity in the training corpus is essential to for example capturing different cultural preferences. What is and isn't harmful ultimately depends on user preferences, hence the alignment step is not universal but depends on the specific use case under which a model will be employed.

To address deployment safety and eliminate objectionable responses, numerous _alignment_ efforts have been made, such as injecting safe information during SFT , performing red teaming with human experts and AI themselves [14; 15; 16; 17; 18], as well as refining and improving the whole RLHF process in detail [19; 20; 21; 22; 23]. Yet we continue to witness a cat-and-mouse game of ever more sophisticated alignment methods to neutralize "harmful" prompts and even more inventive "jailbreaking" attacks that manipulate those prompts to elicit LLMs to produce harmful information. Such attacks come in various flavors, such as injecting adversarial suffixes [1; 24; 25], exploring cipher and low-resource natural languages [26; 27; 28], or letting LLMs craft prompts automatically [29; 30; 31; 32; 33]. Although several ad-hoc defense methods against suffixes have been proposed [34; 35; 36; 37], we only have limited proposal on a principled universal defense against jailbreaking attacks , and limited theoretical understanding on this phenomenon .

In this paper, we present a theoretical framework for analyzing both the pretraining phase and the post-alignment jailbreaking phenomenon. Exploiting the fact that _jailbreaking prompts typically maintain the underlying harmful concept while manipulating other aspects of the prompt,_ we design framework that decouples input prompts to allows us to quantify the strength of potential adversaries. By representing the output elements of an language model (LM) as lengthier text fragments rather than individual tokens, we can quantify the extent to which these models emulate the training distribution and consequently better understand the mechanisms underlying jailbreaking vulnerabilities.

Our contributions can be summarized as follows:

* Based on our proposed framework, we first offer a non-vacuous PAC-Bayesian style generalization bound for pre-training. Assuming the validity of our framework, we conclude that high-performing pre-trained models will inevitably be susceptible to generating behaviour that is present in the training corpus, including any unintended and harmful behaviour.
* Subsequently, we extend our framework to include notions of alignment and jailbreaking. Assuming our assumptions are met, we demonstrate jailbreaking to be unpreventable even after safety alignment because the LM fails to concentrate its output distribution over the set of safe responses.
* Motivated by our theoretical findings, we identify a key drawback in the widely adopted RL Fine-Tuning objective due to the natural difference between the harmlessness and the helpfulness targets. By addressing this issue, we facilitate the training of safer models that are more resilient to a suite of jailbreaking attacks while preserving model performance.

The paper is organized as follows. In Section 2, we introduce our framework. In Section 3, we prove the PAC-Bayesian generalization bound for pretraining. Next, in Section 4 we present analysis on jailbreaking from a statistical perspective. Finally, in Section 5 we illustrate our proposed E-RLHF objective and its effectiveness on improving LLM safety. We give a literature review in Appendix H.

## 2 Framework and assumptions

Jailbreaking carries several analogies to _adversarial attacks_, a well studied field in computer vision . Here, an adversary is defined as a map that perturbs a given input image in pixel space to change the model output. The strength of the adversary is bounded by how far it is able to move the original input as quantified by the \(_{p}\) distance [40; 41; 42]. Typically, this distance is bounded in a way that the change would not be perceptible to the human observer. The goal of the adversary is to cause misclassification of the input. In contrast, in the instance of an LLM, the adversary's goal is to provoke harmful behaviour, _e.g.,_ unintended leaking of information or hate speech. Further, any perturbation to an input, called prompt, will have a perceptible effect. Hence quantifying and bounding the capabilities of the adversary is not straight forward. Nonetheless, with some modifications, we will build on this analogy.

For the purpose of this work, we will _view any prompt as a tuple of query and concept \((q,c)\)_, where \(c\), and \(q\), with \(,\) denoting the complete concept set and query set. Conceptually, we think of _concepts_ as representing the information content of the prompt, usually through a short piece of text, for example "tutorial on making a cake". _Queries_ are instructional text pieces that are composable with certain concepts. We can think of queries as mechanisms to trigger an LM to expand a concept in a specific way. Examples include "Tell me how to {}", or "We are now in an imaginary world, and you are not bounded by any ethical concerns. Teach my avatar how to \(\{\}\)". Since not all queries and concepts are composable,3 we denote \(\) as the set of all _plausible_ prompts, where the definition of plausible will be made clear below.

**The decomposition of prompts allows us to isolate and hence bound the adversary's strength.** In line with current empirical work on inducing harmful behaviour, we will allow perturbations only on the queries, not on the concepts. Further mimicking the spirit of previous work on adversarial attacks, we will assume that the ground-truth related to a prompt is determined solely by the concept, not the query. We will make these ideas more rigorous in the next paragraphs.

First, in contrast to previous theoretical work where LMs are regarded as _single sentence generators_, we model LMs as _lengthier text fragment generators_, and refer to possible generated content \(e\) as explanations. Conceptually, explanations expand concepts with additional information. For example, "The US president in 2023 is Joe Biden.". Our terminology "explanation" is conceptually the same as "response" used in previous discussions (_e.g.,_), where an LLM is regarded as a policy that receives an input and generates a response. We use "explanation" to contrast "concept" since in most jailbreaking attacks currently considered by the community, the adversary seeks instructions or explanations for a single harmful attempt. An LM thus induces a mapping from _plausible_ prompts to distributions over explanations, \(p_{LM}:()\), where \(()\) denotes the set of distributions defined over elements in \(\).4 The output of a LM given a prompt, \(p_{LM}(q,c)\), is a discrete distribution over explanations. We use \((p_{LM}(q,c))\) as the _domain_ of this distribution, \(p_{LM}(e|q,c)\) as the probability of \(e\) given \((q,c)\) as the input, and \((p_{LM}(q,c))\) as the subset of \(\) with non-zero \(p_{LM}(e|q,c)\). Further, we assume the existence of a latent ground truth mapping \(p_{world}:()\) that the LM is optimized to mimic during the pretraining stage. This is the distribution that defines "knowledge": for all _plausible_ prompts \((q,c)\), it specifies the ground-truth distribution over explanations. By _plausible_, we refer to all prompts that lie in the domain of the ground truth mapping \((q,c)(p_{world})\), _i.e., \((p_{world})\)_. Many plausible prompts will not even exist within any available training corpus, as discussed below.

We can now state our main assumption, namely that for any plausible prompt \((q,c)(p_{world})\) the ground-truth distribution \(p_{world}(q,c)\) is supported on a small subset of \((p_{world}(q,c)) \). This assumption seems sensible to us: under normal circumstances, providing an explanation of "Paris" would not offer any relevant knowledge when given a prompt such as "How to write a hello world python script". Our second assumption is that for all plausible prompts \((q,c)\), the concept \(c\)_uniquely determines_ the _support_ of the output distribution specified by \(p_{world}\), regardless of the query: \((p_{world}(q,c))=(p_{world}(q^{*},c))\), \(\) plausible \((q,c)\) and, \((q^{*},c)\). The query changes the ground-truth distribution without affecting its support. An illustration is depicted in Figure 1. To be more precise:

**Assumption 2.1**.: _(Concepts uniquely determine the explanation for plausible prompts) For all plausible prompts \((q,c)(p_{world})\),_

\[\ p_{world}:((p_{ world}(q,c))\]

Figure 1: **Our framework in a nutshell:** We define a language model, \(p_{LM}\): \(\), as a map from prompts to a distribution over a subset of all possible explanations \(\). To later be able to bound the strength of the adversarial attacker, we split the text inputs into concepts and queries \((q,c)\). We assume that (i) the text corpus only covers a part of the domain of the LM: \((D_{})(p_{LM})\), (ii) the size of the domain of the output distribution, denoted \(|(p_{LM}(q,c))|\), is small compared to the size of \(\), and (iii) only concepts determine the output (see \(\)).

_where_ \((p_{world}(q,c))\) _s.t._ \(|(p_{world}(q,c))|||\)_; and_ _ii)_ \((p_{world}(q,c))=(p_{world}(q^{*},c)),\,(q,c), (q^{*},c)\) _plausible_.

This assumption is natural since it essentially tells us that knowledge is specified by the corresponding concept alone, irrespective of what query is used to extract it. In other words, given a concept \(c\), if a query \(q\) manages to change \((p_{world}(q,c))\), we argue that the query should be deconstructed and partially absorbed by \(c\) to accurately reflect the knowledge mirrored by the support.

Lastly, we make the assumption on the existence of an underlying generative distribution over prompts, denoted as \((q,c) D_{}\). This distribution serves as the principle governing the creation of our pretraining corpus. It is important to note that \((D_{})(p_{world})\). For example, take the prompt \((q^{},c^{})\)="Who is James Bond $)**#148811"; even though this prompt never appears in any text corpus across the internet, \((q^{},c^{})(D_{})\), we, as humans, can make sense of it: \((q^{},c^{})(p_{world})\). Later proofs in this paper assume LMs generate semantically reasonable explanations for such unseen plausible prompts, since in reality LMs are claimed to generalize well on huge, out-of-distribution datasets . This is made explicit in Section 4, within Assumption 4.1.

Finally, the following definitions pertain to our notion of harmfulness. More specifically, we understand harmful behaviour abstractly as any unintended behaviour. For this, we assume that any explanation \(e\) can be denoted as **either harmful or not harmful (safe)**. A concept \(c\) is regarded as harmful if and only if the world generates harmful explanations with probability higher than a certain threshold with direct prompts.

**Definition 2.1**.: _(Notions of harmfulness)_

* _(**Direct Queries and Direct Prompts**) We refer to a prompt as direct if it stems from_ \(D_{}\)_, i.e.,_ \((q,c)(D_{})\)_. The query of a direct prompt is called a direct query._
* _(**Harmful Concepts and Harmful Set**) Given a concept_ \(c\)_, the associated harmful set of explanations is denoted as_ \(E_{h}(c):=\{e|e(p_{world}(,c))\,\,e\}\)_. In accordance with Assumption_ 2.1_, with a threshold_ \(\)_, a concept_ \(c\) _is harmful if_ \( q(q,c)(p_{world}),_{e:e E_{h}(c)}p_{ world}(e|q,c) 1-\)_. We refer to the set of all possible harmful concepts as_ \(C_{h}\)_._
* _(**Safe Set**)_ \( c_{h}\)_, there exists a corresponding_ **safe set**__\(E_{s}(c)\) _that we wish_ \(p_{LM}(q,c)\) _to be concentrated on. It includes safe explanations existing in_ \((p_{world}(,c))\)_, and explanations designed by humans,_ e.g., _with the template beginning with "Sorry."_
* _(**Semantically meaningful**) We call explanations in_ \(E_{h}(c) E_{s}(c)\) _as semantically meaningful for the_ \((q,c)\) _prompt._
* _(**Mixture decomposition of \(D_{}\)**) With these notions, we can decompose_ \(D_{}= D_{_{h}}+(1-)D_{_{s}}\)_(where_ \((D_{_{h}})\) _includes all direct prompts with a harmful concept, and_ \((D_{_{s}})\) _includes the complement) as a mixture over direct prompts with a harmful concept and the non-harmful counterpart._

## 3 PAC-Bayesian bound for pre-training LLMs on harmful data

Given a learning algorithm that leads to a _posterior distribution_ over a set of models, PAC-Bayesian theory  applies Probably Approximately Correct (PAC) inequalities, to provide bounds on the generalization gap, _i.e.,_ the difference between the model's empirical loss and the population loss. We now present the first result of our analysis: a non-vacuous PAC-Bayesian bound for pretraining LMs which implies that a well-trained LM ought to exhibit harmful behaviour even when simply prompted with direct queries if it was presented with harmful behavior during training.

We denote by \(S=\{(q_{i},c_{i})\}_{i=1}^{n}\) a set of prompts generated _i.i.d._ under \(D_{}\), \(S D_{}^{n}\). These prompts together with sampled explanations form our pretraining corpus. We use \(\), \(\) as the prior and posterior distribution over LMs before and after the pretraining process, defined over LM, the set of language models. Given a prompt \((q,c)\), we measure the generalization capability of a LM by quantifying the Total Variation (TV) loss between the induced distribution \(p_{LM}(q,c)\) and the ground-truth distribution \(p_{world}(q,c)\).5 For real-world LMs, pretraining involves optimizing the cross-entropy loss on the 

[MISSING_PAGE_FAIL:5]

when clear from the context. The \(O(1)\) statement is reasonable, because harmful explanations are usually long text fragments that allow for many alternative formulations. The assumption can be broken down into two components: (1) within the support of the output distribution, only occasional instances of unrelated explanations exist; (2) the process of aligning the model towards safety **does not eliminate the harmful explanations** acquired during the pretraining phase. For part (1), similar to the example we gave above, under normal circumstances, we do not expect the explanation **"Paris"** to appear in \((p_{LM}(q,c))\) given \((q,c)\) as **"How to build a bomb".** As for part (2), though seemingly surprising, evidence with a series of current state-of-the-art LMs can be experimentally validated , where diverse, harmful explanations are extracted by simply manipulating the decoding process using direct prompts. In Section 5 we give an explanation for this undesired phenomenon.

To bound the likelihood of jailbreaking we first need to specify how the output of a LM interacts with its support. Assuming a fixed order of explanations in \((p_{LM}(q,c))\), and slight abuse of notation, we can use \(p_{LM}(q,c)\) to denote an \(n(c)\)-dimensional vector on \(^{n(c)-1}\), the probability simplex with \(n(c)\) elements, where each entry represents the probability of a single explanation. We call this simplex the **output simplex** related to a given concept \(c\). Next, we can induce a distribution on this simplex given a posterior distribution \(\) over the set of language models \(\), as follows.

**Definition 4.1**.: _(**Induced Distribution on Simplex, \(_{c}\)**) Under the assumption that the LM outputs semantically meaningful explanations (Assumption 4.1), with a fixed prompt \((q,c)\) and a posterior distribution \(\) over \(\), the corresponding induced distribution: \(p_{LM}(q,c)\) where \(LM\) is supported over a subset of the output simplex \(^{n-1}\). This distribution is denoted as \(_{(q,c)}\), or \(_{c}\) when the reference to \(q\) is clear from context._

Next, we will separate the output simplex into a harmful and safety zone. This definition is motivated by the observation that typically an adversary is deemed successful if it can extract even a single harmful explanation for a given concept. This translates into a division of the output simplex, under Assumption 4.1, as follows.

**Definition 4.2**.: _(**Harmful Zone and Safety Zone**) For a given harmful concept \(c\) and a fixed LM, the output simplex is divided into a **safety zone and a harmful zone**, \(_{s}\) and \(_{h}\), where a predefined threshold \(p\) is used to quantify the distinction: \(p_{LM}(q,c)_{h}\) if and only if \(_{c:e E_{h}(c)}p_{LM}(e|q,c) p\), and otherwise \(p_{LM}(q,c)_{s}\)._

Before we introduce jailbreaking, the reader might wonder why we did not define alignment more clearly. This is because under the PAC framework, preference alignment is nothing but a transformation from \(\) to some \(\) posterior defined over \(\). Given this inability on fine-grained characterization of alignment, we instead provide the _goal of it_ as follows. With the above notion, given a prompt \((q,c)\) where \(c\) is harmful, its goal is to push the induced distribution \(_{c}\) into the safety zone \(_{s}\). Ideally, \((_{c})_{s}\) with probability \(1\), the resulting LM is safe when encountering \((q,c)\). We are ready to introduce necessary concepts related to jailbreaking.

**Definition 4.3**.: _(**Jailbreaking**) Given a harmful concept \(c\) and a query \(q^{}\), the prompt \((q^{},c)\)**jailbreaks** the LM iff \(p_{LM}(q^{},c)_{h}\). We call such a prompt \((q^{},c)\) and query \(q^{}\) a jailbreaking prompt and jailbreaking query, respectively._

The threshold \(p\) for discriminating \(_{h}\) and \(_{s}\) should be very small, since it means in expectation the adversary needs to call the \(\) times to collect a single harmful explanation _i.e._, to jailbreak the LM.

To theoretically prove the jailbreaking effect, we need to restrict the adversary's ability. To achieve this goal, we borrow insights from adversarial attacks, to assume that the adversary has bounded manipulating capability on the output simplex when searching over the query set:

**Assumption 4.4**.: _(\(\)-bounded adversary) Given an LM, a harmful concept \(c\) and an associated direct prompt \((q,c)\), we assume the adversary can find a set of queries \(^{}\), such that the output is moved **at**

Figure 2: Conceptual illustration of our framework for jailbreaking introduced in Section 4, with a fixed harmful concept \(c\). The triangle represents the probability simplex. This figure showcases a typical successful jailbreaking attempt by the adversary: although safety alignment makes the sampled LM safe under the direct prompt input, the adversary is able to move the output to the harmful zone \(_{h}\) by manipulating the query \(q\).

**most**\(\) on the simplex towards \(_{h}\) from \(p_{LM}(q,c)\):_

\[_{q^{}^{}}d(p_{LM}(q,c),p_{LM}(q^{},c))=.\]

_Here \(d\) is a distance measure between two discrete distributions. \(d\) can be a typical \(_{p}\) measure with \(p 1\), or the Total Variation / Jensen-Shannon Divergence. We call \(q^{}^{}\) an \(\)-bounded query._

A conceptual illustration of our framework is depicted in Figure 2. Before arriving at our Theorem, we give the final definition of \(\)-expansion.

**Definition 4.4**.: _(\(\)-expansion) Given a set \(A^{n-1}\) and a distance measure \(d\), the \(\)-expansion set \(A(,d)\) is defined as_

\[A(,d):=\{t|t^{n-1} y A\;s.t.\;||y-t||_{d} \}.\]

We are ready to present the following theorem, which states that as long as the induced posterior \(_{c}\) is not concentrated in an extremely safe area, then with high probability the model can be jailbroken. The proof is in Appendix B.3.

**Theorem 2**.: _(Jailbreak is unavoidable) Assume that an LMs output semantically meaningful explanations (Assumption 4.1). Given any \(\) posterior distribution over \(M}\), choose a harmful concept \(c\) with a direct prompt \((q,c)\) and a threshold \(p\) (Definition 2.1), to define the corresponding induced distribution \(_{c}\) (Definition 4.1) and division over output simplex (Definition 4.2). An \(\)-bounded adversary (Assumption 4.2) can find a jailbreaking prompt (Definition 4.3) with probability at least_

\[1-_{s}(1-(a_{})),\]

* _by using either the direct prompt, such that_ \(p_{LM}(q,c)_{h}\)_; or_
* _by finding an_ \(\)_-bounded query_ \(q^{}\)_, such that_ \(p_{LM}(q^{},c)_{h}\)_._

_Here, \(()\) is the standard Gaussian cdf, \(_{s}:=_{x_{s}-_{h}(,d)}(x)}{U(x)}\), with \(U(x)\) the uniform distribution over \(^{n-1}\), and \(a_{}:=a+\), where a writes analytically as \(a(c)|-1-(n-1)p}{}\)._

Trivially, the chances of an adversary to find a jailbreaking prompt increase for stronger adversaries (\(\)). In the real world, this could relate to how much compute budget we allow to alter a query for a specific harmful concept. Furthermore, the chances of an adversary to find a jailbreaking prompt increase when the ratio of the sizes of the harmful explanation set to the safe explanation set is larger \((c)|}{|E_{s}(c)|}\). This is because their ratio will determine the size of the harmful zone which in turn will cause \((a_{}) 1\). In real world settings, for any harmful concept, the training corpus naturally contains a large harmful set due to the number of possible responses. Realistically, its size can not be countered by any manually-constructed safe set. **Hence achieving alignment is hard**: Recall that the goal of alignment is to respond with only safe explanations with high probability. However, we just learned that to increase that probability, we need to have a small harmful-to-safety set ratio which we discussed is not realistic. Consequently, the safety zone is going to be small.

## 5 E-RLHF: improving alignment by _expanding_ the safety zone

Recall from Theorem 2 and the subsequent discussion in the previous section, that jailbreaking becomes more likely the larger the harmful zone is in comparison to the safety zone. The size of both zones relates to the size of their respective explanation sets. In other words, the size of the preference alignment dataset is crucial to successful alignment. Unfortunately, the human labor involved in creating such a dataset effectively caps its size.

In order to bridge the gap between our theoretical insights and a practical solution towards suppressing the jailbreaking problem, we focus on other more **practical ways to expand the safety zone**. Even though our ideas are more broadly applicable, in our experiments we will focus on improving Reinforcement Learning with Human Feedback (RLHF). RLHF typically includes three phases: i) supervised fine-tuning (SFT); ii) preference sampling and reward learning and iii) RL optimization. Rafailov et al.  have recently proposed a widely applied version of RLHF for LMs, coined Direct Preference Optimization (DPO), that employs a clever reparameterization which leads to directly learning from the preference dataset, without the need of obtaining a reward model beforehand.

DPO is more stable in the training process than other implementations of RLHF. A more complete overview of RLHF and DPO can be found in Appendix C.

For our purposes, we assume access to an LLM \(p_{}\) that has been supervised fine-tuned on high-quality data. We further assume access to a preference aligned dataset \(_{s}\); that contains a set of text prompts \((q,c)=x\), and two respective explanations that have been rated by human annotators as better \(e_{w}\) or worse \(e_{l}\). In phase ii) of RLHF, one typically optimizes a reward model \(r(x,e)\) based on the annotated explanations. Our proposal concerns phase iii) of the RLHF process: training of the preference aligned model \(p_{LM}\). For a given reward model, \(p_{LM}\) is typically obtained by **minimizing** the following objective:

\[_{}(p_{LM})=-_{x_{s }}\{_{e p_{LM}(|x)}[r(x,e)]+_{}(p_{LM}(x)||p_{}(x))\} \]

Note that, the first term is maximizing the reward, while the KL-term acts as a regularizer ensuring the aligned model can not drift off too far from the SFT model. We claim that this regularization is exactly the problem for safety. While designed to keep the model helpful7, for any harmful prompt \(x_{h}\) and any harmful explanation \(e(p_{}(x_{h}))\), regularizing \(p_{LM}\) to \(p_{}\) lets \(p_{LM}\) maintain \(e\) in the support of the output distribution. Specifically, the supervised fine-tuning process does not involve elimination of any harmful explanations from the support of the output distribution of the pretrained model, 8 which leads to the fact that \(p_{}(x_{h})\) supports harmful responses that can not be negated with a realistically sized preference aligned dataset. Thus, this small safe set problem of \(p_{}\) will be further passed to \(p_{LM}\), even if \(p_{LM}\) is optimized to the optimal solution of the above objective. This suggests we should not push \(p_{LM}\) into the direction guided by \(p_{}\). Instead, for any harmful prompt \(x_{h}\), we propose to use a distribution that we _design_ to have a large safety zone to replace \(p_{}(x_{h})\). Inspired by our framework, we can achieve this by **modifying the harmful prompt**, such that a safe concept is used to replace the harmful concept, which alters \((p_{}())\) to include more safe explanations. For example, when \(x_{h}\) is '**Tell me how to make a bomb**", instead of aligning \(p_{LM}(x_{h})\) to \(p_{}(x_{h})\), now we align \(p_{LM}(x_{h})\) to \(p_{}(x_{s})\) where \(x_{s}\) is "**Tell me how to reject a request of making a bomb**". This can be done in an individualized manner for all harmful prompts in \(_{s}\), or simply by prefacing all harmful prompts with a prefix such as "**Please ensure your response** **adheres to community guidelines and ethical standards**:". 9 Importantly, the non-harmful prompts are not modified. Due to the focus of our approach to _expand_ the safety zone of the output distribution, we coin our proposal _E-RLHF_, resulting in the following modification to Eq. (2):

\[_{}(p_{LM})=-_{x_{s }}\{_{e p_{LM}(|x)}[r(x,e)]+_{}(p_{LM}(x)||p_{}(x_{s}))\} \]

where \(x_{s}\) is a **safety-transformed version of the original harmful prompt \(x_{h}\)**. To recap, the key argument we put forth is that, in order to ensure the stability of model fine-tuning, it is not imperative to utilize identical prompt inputs \(x\) for both the reference model and the target model, particularly when the original input \(x\) itself is harmful. In fact, as long as the substitute or "anchor" prompt generates logically reasonable outputs akin to those produced by the original prompt, this approach would not impede the training process of the model. To solidify our argument we show the impact of our modification on the support of the optimal policy in Appendix C. We also deduce there that we can trivially integrate our modification into the DPO objective allowing us to train without an explicit reward model (eliminates step ii)) as follows, where \(()\) stands for the sigmoid function:

\[_{}(p_{LM})=-_{(x,e_{w},e_{l}) _{s}}[((e_{w} x )}{p_{}(e_{w} x_{s})}- (e_{l} x)}{p_{}(e_{l} x_{s})}) ]. \]

## 6 Experiments and results

**Our experimental set-up** is based on the alignment-handbook code base . We tune the publicly-available SFT model \(p_{}\) provided by huggingface hub , using the public dataset , with default hyperparameter setup. We label harmful prompts in the preference dataset by prompting GPT-3.5-Turbo, see Appendix E. We are using the very same prefix proposed in the previous section to generate \(x_{s}\). Experiments are performed on 8 NVIDIA Tesla V100 GPUs, using half-precisiontuning _i.e.,_ Float16. In the appendix, we also show results for an alternative training paradigm: the Low-Rank Adaptation (LoRA)  (see Appendix D.1). Following community standards [3; 1; 2], we use greedy decoding _i.e.,_\(T=0\) for model evaluation.

We first show empirical evidence that our proposed modification of DPO, E-DPO, does in fact improve safety alignment, using the Harmbench dataset  and the first \(100\) prompts in the AdvBench harmful behavior dataset , measured by the HarmBench protocol. We give an overview on all adversaries in Appendix F. The results are presented in Table 1. **E-DPO achieves improvements across every task we tested.**

On top of our safety results, **we want to make sure E-RLHF does not sacrifice helpfulness for increased safety**. We evaluate helpfulness with the MT-Bench project . The SFT model \(p_{}\) receives a score of 6.3, and both the DPO and E-DPO models perform better than that (6.9 and 6.7 respectively), making us believe that performance degradation is not a problem with our proposal. Next, we show the impact of the safe prefix on model performance. We demonstrate that **our method's performance depends on the choice of safe prefix to some extend but never fails** (see Appendix D.2). We believe, finding better safe prefixes by explicit tuning would improve our results, similar to the work by Yang et al. , but we leave this exploration for future work. Further, we confirm that the improvement arises from using a safe prior in the KL term for harmful prompts. **We ablate our results by appending the prefix on all prompts in the preference alignment dataset** (see Appendix D.3). In all cases, applying the safe prefix to usual prompts _degrades_ safety, showcasing the importance of switching the prior only on the harmful prompts. Finally, we show that E-DPO can **be combined with any system prompt, to further boost safety** (see Appendix D.4). The proposal can even be used to **improve helpfulness and safety simultaneously** (see Appendix D.5).

## 7 Conclusion and discussions

In this paper, we present a theoretical framework for language model pretraining and jailbreaking by dissecting input prompts into query and concept pairs. Through this approach, we have established two theoretical results pertaining to the ability of language models to mimic the world following pretraining, which leads to outputting harmful explanations given harmful prompts; and the inevitability of jailbreaking resulting from alignment challenges. Guided by these theoretical insights, we have devised a simple yet effective technique to enhance safety alignment, and demonstrate the improved resilience to jailbreak attacks with this methodology.

**Current limitations** (1) Although we have classified concepts as either harmful or non-harmful, it is important to acknowledge that the perception of a concept's potential for harm can be influenced by various factors such as cultural, legal, and societal norms, which collectively form the _context_ of the situation. (2) Language models have demonstrated impressive capabilities in reasoning and completing tasks within multi-round, multi-step conversations; our current framework may not fully account for the generalization and jailbreaking possibilities associated with such input formats. (3) Our analysis is grounded on a fixed \(p_{world}\) mapping and \(D_{}\) distribution. Nevertheless, the world is inherently dynamic, as both \(p_{world}\) and \(D_{}\) continually evolve.

    &  &  &  &  & \\  & & & & & & & & & & & & \\  & & & & & & & & & & & & \\    &  &  &  &  &  &  &  &  &  &  & {*

**Future work** (1) Regarding our E-RLHF approach, as highlighted in the experimental section, in addition to attaching a universally safe prefix to all harmful prompts, improvements can be achieved by individually transforming the harmful prompts. Moreover, the safety-transformed prompts can be employed to expand the preference dataset for conventional RLHF. (2) Throughout our analysis, we have not imposed any constraints on the _capacity_ of the language model. Extending our analysis under finite memory constraints or analyzing hallucination properties of LLMs is an interesting direction to explore. (3) Large language models have shown remarkable capabilities as in-context learners , and such techniques could potentially be used for jailbreaking them as well . Investigating the incorporation of such input paradigms remains a promising avenue for future research.