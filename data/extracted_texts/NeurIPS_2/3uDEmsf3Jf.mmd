# OASIS: Conditional Distribution Shaping for

Offline Safe Reinforcement Learning

Yihang Yao\({}^{*1}\), Zhepeng Cen\({}^{*1}\), Wenhao Ding\({}^{1}\), Haohong Lin\({}^{1}\), Shiqi Liu\({}^{1}\),

**Tingnan Zhang\({}^{2}\), Wenhao Yu\({}^{2}\), Ding Zhao\({}^{1}\) \({}^{1}\)** Carnegie Mellon University, \({}^{2}\) Google DeepMind

\({}^{*}\) Equal contribution, {yihangya, zcen}@andrew.cmu.edu

###### Abstract

Offline safe reinforcement learning (RL) aims to train a policy that satisfies constraints using a pre-collected dataset. Most current methods struggle with the mismatch between imperfect demonstrations and the desired safe and rewarding performance. In this paper, we mitigate this issue from a _data-centric_ perspective and introduce OASIS (**cO**ndition**Al** diStributIon **S**haping), a new paradigm in offline safe RL designed to overcome these critical limitations. OASIS utilizes a conditional diffusion model to synthesize offline datasets, thus shaping the data distribution toward a beneficial target domain. Our approach makes compliance with safety constraints through effective data utilization and regularization techniques to benefit offline safe RL training. Comprehensive evaluations on public benchmarks and varying datasets showcase OASIS's superiority in benefiting offline safe RL agents to achieve high-reward behavior while satisfying the safety constraints, outperforming established baselines. Furthermore, OASIS exhibits high data efficiency and robustness, making it suitable for real-world applications, particularly in tasks where safety is imperative and high-quality demonstrations are scarce. More details are available at the website [https://sites.google.com/view/saferl-oasis/home](https://sites.google.com/view/saferl-oasis/home).

## 1 Introduction

Offline Reinforcement Learning (RL), which aims to learn high-reward behaviors from a pre-collected dataset , has emerged as a powerful paradigm for handling sequential decision-making tasks such as autonomous driving , and robotics . Although standard offline RL has achieved remarkable success in some environments, many real-world tasks cannot be adequately addressed by simply maximizing a scalar reward function due to the existence of various safety constraints that limit feasible solutions. The requirement for _safety_, or constraint satisfaction, is particularly crucial in RL algorithms when deployed in real-world tasks .

To develop an optimal policy within a constrained manifold , _offline safe RL_ has been actively studied in recent years, offering novel ways to integrate safety requirements into offline RL . Existing research in this area incorporates techniques from both offline RL and safe RL, including the use of stationary distribution correction , regularization , and constrained optimization formulations . Researchers have also proposed the use of sequential modeling methods, such as the decision transformer  and the decision diffuser  to achieve advantageous policies and meet safety requirements.

Although these methods show promise, it is difficult to handle state-action pairs that are absent from the dataset, which is known notably as out-of-distribution (OOD) extrapolation issues . To solve this, many works utilize regularization methods to push the policy toward behavior policy to achieve pessimism . However, this approach worsens the situation when the dataset is imbalanced and biased: regularization by imperfect demonstrations such as datasetscomposed predominantly of low-reward data or containing few safe trajectories collected using unsafe behavior policies. This regularization also leads to another challenge: striking the optimal balance between learning objectives such as task utility efficiency and safety requirements, leading to reward degradation or aggressive behavior .

To address these challenges, we introduce a _data-centric_ learning paradigm in offline safe RL, OASIS (**c**O**ndition**Al** diStribution**I**n Shaping), which focuses on improving the training dataset quality by steering the offline data distribution to a beneficial target domain as shown in Fig. 1. OASIS distills knowledge from the imperfect dataset, and generates rewarding and safe data using a conditional diffusion model according to the safety preference to benefit offline safe RL training. This _data-centric_ approach is parallel to and compatible with general _model-centric_ offline safe RL algorithms which emphasize improvements to the learning algorithm and model architecture. The key contributions are summarized as follows.

**1. Identification of the safe dataset mismatch (SDM) problem in offline safe RL.** We identify the mismatch between the behavior policy and the target policy and investigate the underlying reasons for performance degradation with this condition.

**2. Introduction of the OASIS method to address the SDM problem.** To the best of our knowledge, this is the first successful application of a distribution shaping paradigm within offline safe RL. Our theoretical analysis further provides insights on performance improvement and safety guarantees.

**3. A comprehensive evaluation of our method across various offline safe RL tasks.** The experiment results demonstrate that OASIS outperforms baseline methods in terms of both safety and task efficiency for varying tasks and datasets.

## 2 Related Work

**Offline RL.** Offline RL addresses the limitations of traditional RL, which requires interaction with the environment. The key literature on offline RL includes BCQ , which mitigates the extrapolation error using a generative model, and CQL , which penalizes the overestimation of Q-values for unseen actions. BEAR  further addresses the extrapolation error by constraining the learned policy to stay close to the behavior policy. OptiDICE  directly estimates the stationary distribution corrections of the optimal policy, and COptiDICE  extends the method to the constrained RL setting. Recent advances have increasingly focused on the use of data generation to improve policy learning. S4RL  shows that surprisingly simple augmentations can dramatically improve policy performance.  explores leveraging unlabeled data to improve policy robustness, while  proposes survival instincts to enhance agent performance in challenging environments. Counterfactual data augmentation is another promising direction in offline RL , highlighting the potential of data generation to significantly improve efficiency and effectiveness.

**Safe RL.** Safe RL is formulated as a constrained optimization to maximize reward performance while satisfying the pre-defined safety constraints . Primal-dual framework is one common approach to solve safe RL problem . Another line of work for safe RL is to extend to offline settings, which learn from a fixed dataset to achieve both high reward and constraint satisfaction . Among them,  tailor online prime-dual-style algorithms to reduce the out-of-distribution issue in the offline setting.  use decision transformer  to avoid value estimation and exhibit consistent performances across various tasks. In addition to these _Model-centric_ approaches, _Data-centric_ approaches, which emphasize improving or optimizing the quality of the dataset used for model training , have gained more attention in recent studies. While some previous work proposed methods for learning from safe demonstration  or relabeling data to achieve conservativeness , how to systematically curate datasets for offline safe learning remains a largely unexplored area.

**Diffusion Models for RL.** Diffusion models have recently gained attention in RL for their capabilities in planning and data generation . Specifically, Diffuser  uses a diffusion process to plan the entire trajectory in complex environments.  extends this to the Decision Diffuser,

Figure 1: An example of distribution shaping in offline safe RL. We generate a low-cost and high-reward dataset from the original dataset for subsequent RL training.

which conditions the diffusion process on specific goals and rewards to improve decision-making. SafeDiffuser  and FISOR  enhance safety by ensuring the planned trajectories satisfying constraints. Combined with the data augmentation capability of diffusion models, AdaptDiffuser  achieves state-of-the-art results on offline RL benchmarks.  proposes Synthetic Experience Replay, leveraging diffusion models to create synthetic experiences for more efficient learning.  demonstrates that diffusion models are effective planners and data synthesizers for multi-task RL, showcasing their versatility and efficiency. In this work, we investigate the power of diffusion models for safe RL, where the balance between reward and cost presents further complexities.

## 3 Problem Formulation

### Safe RL with Constrained Markov Decision Process

We formulate Safe RL problems under the Constrained Markov Decision Process (CMDP) framework . A CMDP \(\) is defined by the tuple \((,,,r,c,,_{0})\), where \(^{m}\) is the state space, \(^{n}\) is the action space, \(:\) is the transition function, \(r:\) is the reward function, \(c:_{ 0}\) is the cost function, \(\) is the discount factor, and \(_{0}:\) is the initial state distribution. Note that this work can also be applied to multiple-constraint tasks, but we use a single-constraint setting for easy demonstration. A safe RL problem is specified by a CMDP and a constraint threshold \([0,+)\). Denote \(:\) as the policy and \(=\{(s_{1},a_{1},r_{1},c_{1}),(s_{2},a_{2},r_{2},c_{2}),\}\) as the trajectory. The stationary state-action distribution under the policy \(\) is defined as \(d^{}(s,a)=(1-)_{t}^{t}(s_{t}=s,a_{t}=a)\). The reward and cost returns are defined as \(R()=_{}r\), and \(C()=_{}c\). The value function is \(V^{}_{}(_{0})=_{,s0_{0}}[_{t=0 }^{}^{t}_{}],\{r,c\}\), which is the expectation of discounted return under the policy \(\) and the initial state distribution \(_{0}\). The goal of safe RL is to find the optimal policy \(^{*}\) that maximizes the expectation of reward return while constraining the expectation of cost return to the threshold \(\):

\[^{*}=_{}_{}R(), s.t. _{}C(). \]

### Regularized offline safe RL

For an offline safe RL problem, the agent can only access a pre-collected dataset \(=_{i}_{i}\), where \(_{i}_{i}^{B}\) is collected by the behavior policy \(_{i}^{B}^{B}\). To solve the problem in Eq. (1), we convert the constraint optimization problem into an unconstrained form:

\[(^{*},^{*})=_{}_{}(,), (,)=-_{}R()+(_{}R()-). \]

The primal-dual-based algorithm solves the optimal policy \(^{*}\) and the dual variable \(^{*}\) by updating \((,)\) iteratively [86; 23; 66]. In offline safe RL tasks, a regularization term is usually introduced to prevent the action OOD issue , that is, the objective is converted to:

\[(^{*},^{*})=_{}_{}_{}(,),_{}(,)=(,) +wL(,^{B}), \]

where \(w>0\) is a constant weight, \(L(,^{B})\) is a regularization term and \(^{B}\) is the empirical behavior policy and can be viewed as a mixture of \(\{_{i}^{B}\}\). Practically, regularization is formulated as the MSE regularization  or the evidence lower bound regularization [35; 41]. In offline safe RL, there are two main challenges: (1) **Distribution shift**. The agent has poor generalizability when facing OOD state-action pairs during online evaluation; and (2) **Efficiency-safety performance balancing**. The agent tends to be over-conservative or aggressive when overestimating or underestimating the safety requirements.

## 4 Method

In this section, we first identify the _safe dataset mismatch_ (SDM) problem, which leads to performance degradation when solving the regularized offline safe RL objective in Eq. (3). Then we present the proposed OASIS (**cOn**dition**Al** di**S**t**ibut**I**on **S**haping) method to solve this problem. In contrast to the _model-centric_ safe RL approaches that focus on optimizing policy update process and network architecture, OASIS is a _data-centric_ learning method that aims to improve the quality of the dataset, thus benefiting offline safe RL training. The OASIS method utilizes the diffusion model to realize conditional distribution shaping, solving the challenges mentioned above, thus benefiting offline safe RL training. Following the proposed algorithm, we provide a theoretical guarantee of the safety performance of the policy learned in this paradigm.

### Safe Dataset Mismatch Problem

The regularized offline safe RL objective in Eq. (3) pushes policy to behavior policy to prevent action OOD issues . When given an imperfect dataset, the state-action distribution deviates from the optimal distribution, and the SDM problem arises: if the behavior policy is too conservative with low costs and low rewards, it leads to task efficiency degradation; if the behavior policy is too aggressive with high costs and high rewards, it leads to safety violations. To further investigate the SDM problem and the effect of dataset distribution on offline safe RL, we define the **tempting dataset** and **conservative dataset**, which are based on tempting and conservative policies:

**Definition 1** (Tempting policy  and conservative policy).: The tempting policy class is defined as the set of policies that have a higher reward return expectation than the optimal policy, and the conservative policy class is defined as the set of policies that have lower reward and cost return expectations than the optimal policy:

\[^{T}\{:V_{r}^{}(_{0})>V_{r}^{^{*}}(_{0})\}, ^{C}\{:V_{r}^{}(_{0})<V_{r}^{^{*}}(_{0}),V_{c}^{}( _{0})<V_{c}^{^{*}}(_{0})\}. \]

Intuitively, a tempting policy is a more rewarding but less safe policy than the optimal one, and a conservative policy is with lower cost but less rewarding. According to these policies, we define two types of datasets:

**Definition 2** (Tempting and conservative dataset).: For an offline dataset \(D_{i}_{i}\), if \(_{i}^{B}^{T}\), then the dataset is tempting; if \(_{i}^{B}^{C}\), then the dataset is conservative.

Staying within the tempting dataset distribution results in tempting (unsafe) behavior, while staying within the conservative dataset distribution causes reward degradation. A theoretical analysis of performance degradation due to the SDM problem is presented in Sec. 4.4. Fig. 2 illustrates examples of both conservative and tempting datasets.

It is important to note that tempting and conservative datasets are prevalent in offline safe RL since optimal policies are rarely available for data collection. The SDM problem is a distinct feature of offline safe RL, indicating that training the policy on either tempting or conservative datasets will violate safety constraints or result in sub-optimality, both of which are undesirable. Therefore, addressing the SDM problem is essential for the development of regularized offline safe RL algorithms.

### Mitigating the Safe Dataset Mismatch

Inspired by recent research works on _data-centric_ learning , which emphasizes the quality of data used for training the agent, we focus on enhancing the data pipeline to ensure the agent learns effectively instead of modifying the RL algorithm or model architecture. We propose to use the distribution shaping of the dataset to mitigate the SDM problem, that is, generating a new dataset \(_{g}\) by reshaping the original data distribution. As shown in Fig. 1, the key idea is to adjust the dataset distribution towards the optimal distribution under \(^{*}\), reducing the distribution discrepancy and mitigating the SDM problem, thus both mitigating the action OOD issue and balancing efficiency and safety in offline safe RL.

Figure 3: (a) Reweighting in the dataset with comprehensive coverage. (b) Reweighting in the tempting dataset. (c) Performance evaluation with different weights and datasets.

Figure 2: \(_{1}\) is a conservative dataset, and \(_{2}\) is a tempting dataset. Each point represents \((C(),R())\) of a trajectory \(\) in the dataset.

Among the _data-centric_ algorithms, Dataset reweighting (weighted sampling) which assigns different sampling weights to data points, is a straightforward way to shape the data distribution [39; 89]. In the offline RL domain, researchers proposed methods to assign high weights to data that achieve high rewards and superior performance in many RL tasks [39; 89].

To validate this idea, we deploy a Boltzmann energy function considering both the reward and the cost for the reweighing strategy to solve the problem (see Appendix B for details). The experimental results, shown in Fig. 3, validate the effectiveness of this distribution shaping method when the coverage of the dataset is complete.

However, for a more general case where we can only access the low-quality dataset (e.g., tempting datasets in Fig. 3), simply performing data reweighting does not work well due to the absence of necessary data. Thus, we propose to use a conditional generative model for more flexible distribution shaping, which generates new data by stitching sub-optimal trajectories for offline training.

### Constraint-Conditioned Diffusion Model as Data Generator

To overcome the limitation of reweighing methods, we propose using diffusion models to generate the dataset that fits the target cost limit to achieve distribution shaping. In the following, we introduce the details of the generator training and dataset generation phases.

**Training.** In previous works [82; 79], the trajectory planning in offline RL can be viewed as the sequential data generation: \(=[s_{0},s_{1},,s_{L-1}],\) where \(\) is a subsequence of trajectory with length \(L\). Denote \(_{k}()\) and \(()\) as the \(k\)-step denoising output of the diffusion model and the denoising conditions such as reward and cost returns, respectively. Then the forward diffusion process is to add noise to \(_{k}()\) and gradually convert it into Gaussian noise:

\[q(_{k}()_{k-1}()):=( _{k}();}_{k-1}(),_{k}),\;k=1,...,K \]

where \(_{k}\) is a pre-defined beta schedule, \(K\) is the total denoising timestep. Then the trainable denoising step aims at gradually converting the Gaussian noise back to a valid trajectory:

\[p_{}(_{k-1}()_{k}(),() ):=(_{k-1}()_{}(_{k}( ),(),k),_{k}), \]

where \(\) is the trainable parameter. We use a simplified surrogate loss  for optimization:

\[_{}\,:=_{_{0}() q, (,)}[\|-_{}( _{k}(),(),k)\|^{2}]. \]

In this work, we use the classifier-free guidance  for conditional data generation. The condition \(()\) in Eq. (6) and Eq. (7) is set to \(()=[C(),R()]\). Thus, the denoising process depends on the target reward and cost returns of the planned subtrajectory. During training time, the diffusion model learns both an unconditional denoising core \(_{}(_{k}(),,k)\) and a conditional denoising core \(_{}(_{k}(),(),k)\). We adopt masking  for the training to zero out the condition of one training trajectory and categorize it as the \(\) class with probability \(0<p<1\). Within the given raw dataset, we also train an inverse dynamics model \(:\), and reward and cost models \((s,a,s^{}),(s,a,s^{}): \) for labeling.

Figure 4: OASIS: a _data-centric_ approach for offline safe RL. Conditioned on the human preference, OASIS first curates an offline dataset with a conditioned diffusion data generator and learned labeling models, then trains safe RL agents with this generated dataset.

**Generation.** After obtaining a trained model, the next step is to generate a new dataset following the conditions. For diffusion model inference, the denoising core \(_{}(_{k}(),,k)\) is calculated by:

\[_{}(_{k}(),(),k)=_{} (_{k}(),,k)+w_{}(_{} (_{k}(),_{c}(),k)-_{}(_{k}(),,k)), \]

where \(w_{}>0\) is a constant guidance scale and \(_{c}:=[,]\) is the generation condition, we set \(\) to align safety preference. For guided generation, we fix the initial state, which means that we replace the initial state of each \(k\)-step noised trajectory as \(_{k}=_{0}\)1.

After generating one subtrajectory \(_{g}=_{0}\), we can get the state and action sequence \(s_{g}=_{g}[-1],s^{}_{g}=_{g}[1:],a_{g}=(s_{g},s^{ }_{g})\), then label the data \(r_{g}=(s_{g},a_{g},s^{}_{g}),c_{g}=(s_{g},a_{g},s^{ }_{g})\). Finally, we get a generated dataset \(_{g}=\{s_{g},a_{g},s^{}_{g},r_{g},c_{g}\}\) with \(|_{g}|-1\) transition pairs. With this new dataset, we can further train offline safe RL agents.

In this work, we consider BCQ-Lag [35; 86] as the base offline safe RL algorithm. The process of generating one subtrajectory \(_{g}\) is summarized in Algorithm 1. More details of the implementation are available in Appendix C.

### Theoretical analysis

We first investigate how the distribution mismatch degrades the policy performance on constraint satisfaction. Suppose that the maximum one-step cost is \(C_{}=_{s,a}c(s,a)\). Based on Lemma 6 in  and Lemma 2 in , the performance gap between the policy \(\) learned with the dataset \(\) and the optimal policy is bounded by

\[|V_{c}^{}(_{0})-V_{c}^{*}(_{0})|}}{1-}D _{}(d^{}(s)\|d^{*}(s))+}}{1-} _{d^{*}(s)}[D_{}((a|s)\|^{*}(a|s))], \]

where \(d^{}(s),d^{*}(s)\) denote the stationary state distribution of the dataset and optimal policy. The proof is given in Appendix A.1. Therefore, a significant mismatch between the dataset and the optimal policy results in both a substantial state distribution TV distance and a large policy shift from the optimal one, which can cause notable performance degradation, especially when the offline RL algorithm enforces the learned policy to closely resemble the behavior policy of the offline data.

Then we provide a theoretical analysis of how our method mitigates this mismatch issue by shrinking the distribution gap, which provides a guarantee of the safety performance of the regularized offline safe RL policy. Let \(d_{g}(s|)\) denote the state marginal of the generated data with condition \(\). We first make the following assumptions.

**Assumption 1** (Score estimation error of the state marginal).: There exists a condition \(^{*}\) such that the score function error of the state marginal is bounded by

\[_{d^{*}(s)}\|_{s} d_{g}(s|^{*})-_{s} d^{*} (s)\|_{}^{2}, \]

where \(d^{*}(s)\) is the stationary state distribution induced by the optimal policy \(^{*}\).

This assumption is also adopted in previous work [94; 95]. For simplicity, we omit the condition \(^{*}\) in the following analysis and use \(d_{g}(s),d_{g}(s,a)\) to denote the generated state or the state-action distribution with condition \(^{*}\). As we use inverse dynamics \(f(a|s,s^{})\) to calculate actions based on the generated state sequence, the quality of the dataset is also determined by the inverse dynamics. Therefore, we further make the following assumption.

**Assumption 2** (Error of inverse policy).: The error of action distribution generated by the inverse dynamics is bounded by

\[_{d^{*}(s)}[D_{}(_{}(|s)\|^ {*}(|s))]_{}, \]where \(_{}(a|s)=_{s^{}}[(a|s,s^{})]\) denotes the empirical inverse policy, which is a marginal of inverse dynamics over \(s^{}\).

Then the distance of generated data distribution to the optimal one is bounded as:

**Theorem 1** (Distribution shaping error bound).: _Suppose that the optimal stationary state distribution satisfies that 1) its score function \(_{s} d^{*}(s)\) is \(L\)-Lipschitz and 2) its second momentum is bounded. Under Assumption 1 and 2, the gap of generated state-action distribution to the optimal stationary state-action distribution is bounded by_

\[D_{}(d_{g}(s,a)\|d^{*}(s,a))}(_{})+}/2}+C(d^{*}(s),L,K), \]

_where \(C(d^{*}(s),L,K)\) represents a constant determined by \(d^{*}(s),L\) and \(K\)._

The proof is given in Appendix A.2. Theorem 1 indicates that using the proposed OASIS method, we can shape the dataset distribution towards a bounded neighborhood of the optimal distribution.

Given the generated data, we will then train a regularized offline safe RL policy by Eq. (3). Notice that the regularization term in the objective function in Eq. (3) is equivalent to an explicit policy constraint, and the coefficient \(w\) is the corresponding dual variable. Therefore, we make the following assumption on the distance between the learned policy \(_{}\) and the behavior policy.

**Assumption 3**.: Denote the generated dataset as \(_{g}\) and the corresponding behavior policy as \(_{g}\), given a fixed coefficient \(w\), for the policy \(_{}\) optimized by Eq. (3), there exists a \(_{}\) such that

\[_{d_{g}(s)}[D_{}(_{}(|s )\|_{g}(|s))]_{}. \]

Based on the above assumptions, we can derive the bound of constraint violation of the policy learned on the offline data generated by OASIS. The proof is given in Appendix A.3.

**Theorem 2** (Constraint violation bound).: _For policy \(_{}\) optimized by regularized-based offline safe RL on generated dataset \(_{g}\), under Assumption 1, 2 and 3, the constraint violation of the trained policy is bounded as:_

\[V_{c}^{_{}}(_{0})-}}{1- }(}(_{} )+C(d^{*}(s),L,K)+}/2}+}/2}), \]

_where \(C(d^{*}(s),L,K)\) represents a constant determined by \(d^{*}(s),L\) and \(K\)._

The theoretical analysis sheds insights by answering two questions: (1) Why do we use the diffusion model for conditional distribution shaping, and (2) How does conditional distribution shaping benefit offline safe RL training? Theorem 1 shows that by using the conditional diffusion model as a data generator, the TV distance between optimal and generated state-action distribution is bounded; Theorem 2 shows that the safety performance of agent trained on the generated dataset is guaranteed with OASIS.

## 5 Experiment

In the experiments, we answer these questions: (1) How does the distribution of the dataset influence the performance of regularized offline safe RL? (2) How does our proposed distribution shaping method perform in offline safe RL tasks? (3) How well does the conditional data generator shape the dataset distribution? To address these questions, we set up the following experiment tasks.

**Environments.** We adopt the continuous robot locomotion control tasks in the public benchmark Bullet-Safety-Gym  for evaluation, which is commonly used in previous works [68; 28; 60]. We consider two tasks, Run and Circle, and three types of robots, Ball, Car, and Drone. We name the environment as Agent-Task. A detailed description is available in the Appendix B.

**Datasets.** Our experiment tasks are mainly built upon the offline safe RL dataset 0SRL . To better evaluate the tested algorithms with the challenging SDM problem, we create four different training dataset types, full, tempting, conservative, and hybrid. The tempting dataset contains sparse safe demonstrations, the conservative dataset lacks rewarding data points, and the hybrid dataset has scarcity in the medium-reward, medium-cost trajectories. We set different cost thresholds for different datasets. A detailed description and visualization of the datasets are available in Appendix B. For the main experiments presented in Table 1, we train on tempting dataset, with threshold \(=20\)

**Baselines and OASIS.** For baselines, we compared our method with both _model-centric_ and _data-centric_ baseline methods. _Model-centric_: (1) Q-learning-based algorithms: BCQ-Lag , BEAR-Lag , and CPQ ; (2) Imitation learning: Behavior Cloning (BC) ; (3) Distribution correction estimation: COptiDICE, and (4) Sequential modeling algorithms: CDT and FISOR; _Data-centric_: (5) Data augmentation: CVAE-BCQL: we train BCQ-Lag agents on the datasets generated by Conditional Variational Autoencoder (CVAE) . The CVAE training set and the dataset generation conditions are set as the same with our OASIS. For DASIS implementation, we set three cost conditions \(\) to align safety preference and guarantee data coverage. Code is available on our Github repository, checkpoints and curated datasets are available on our HuggingFace repository.

**Metrics.** We use the normalized cost return and the normalized reward return as the evaluation metric for comparison in Table 1 and 2. The normalized cost is defined as \(C_{}=C_{}/\), where \(C_{}\) is the cost return and \(\) is the cost threshold. The agent is safe if \(C_{} 1\). The normalized reward is computed by \(R_{}=R_{}/r_{}()\), where \(r_{}()\) is the maximum empirical reward return for task \(\) within the given dataset. We report the averaged results and standard deviations over \(3\) seeds for all the quantity evaluations.

    &  &  \\   & & BallRun & CarRun & DroneRun & BallCircle & CarCircle & DroneCircle \\   & reward\(\) & \(0.55 0.23\) & \(0.94 0.02\) & \(0.62 0.11\) & \(0.73 0.05\) & \(0.59 0.11\) & \(0.82 0.01\) \\  & & cost\(\) & \(2.04 1.32\) & \(1.50 1.11\) & \(3.48 0.68\) & \(2.53 0.15\

### How can conditional data generation benefit offline safe RL?

**Performance degradation with SDM problems.** The comparison results on the tempting dataset are presented in Table 1 with the cost threshold \(=20\) before normalization. Results of BC show the mismatch between the behavior policy and the safe policy, as the cost returns significantly violate the safety constraints. The results of BCQ-Lag and BEAR-Lag show this mismatch further influences the regularized-based algorithms, leading to constraint violations. This is because the regularization term pushes the policy towards the unsafe behavior policy. The conservative Q function estimation method CPQ, exhibits a significant reward degradation in all tasks, which arises from the drawback of the pessimistic estimation methods that learn over-conservative behaviors. COptiDICE also fails to learn safe and rewarding policies, showing that even using distribution correction estimation is not enough to solve the SDM problem. For sequential modeling algorithms, CDT shows poor safety performance and FISOR tends to be over-conservative with poor reward performance. This is because both methods require a large amount of high-quality data while the trajectories with low cost and high reward are sparse in this task. The unsatisfactory performance of these _model-centric_ algorithms further motivates the effective _data-centric_ learning algorithm for offline safe RL.

**Performance improvement using OASIS.** From Table 1, we find that only our method OASIS can learn safe and rewarding policies by mitigating the SDM problem. In addition to the results on the tempting dataset, we also provide evaluation results within different types of datasets and constraint thresholds in Fig. 4(a) and Fig. 4(b). We can observe that most baselines still fail to learn a safe policy within different task conditions due to the SDM issue. In contrast, our proposed OASIS method achieves the highest reward among all safe agents, which shows strength in more general cases.

**High data efficiency of OASIS.** In this experiment, we vary the amount of data for offline RL agent training to evaluate data efficiency. The evaluation results are shown in Fig. 6. The x-axis \(\) represents the size of the RL agent training dataset. For OASIS, it denotes the size of the generated data. A subsequent BCQ-Lag agent is trained on this generated dataset to obtain the safe RL policy. For the baseline methods, we randomly sample trajectories from the original dataset to construct the training dataset.

The comparison results indicate that we can still learn a good policy using a small amount of high-quality data (\(<2\%\)) generated by OASIS. In contrast, baseline methods show significant performance degradation when the data are sparse as the noisy data is of low quality.

This observation demonstrates that the agent can learn a good policy with high data efficiency given high-quality data with minimal safe dataset mismatch (SDM) issues. As OASIS offers a solution to shape the dataset distribution, it also reduces the required training dataset size while maintaining good performance, further reaffirming the effectiveness of _data-centric_ approaches in offline safe learning and highlighting that prioritizing quality is essential .

### How can OASIS shape the dataset distribution?

**Successful distribution shaping.** To show the distribution shaping capability of the proposed OASIS and baseline CVAE, we generate the dataset under different conditions and visualize them in Fig. 7. When using different conditions, the expectations of reward and cost of the generated dataset change accordingly. This shows the strong capability of our method in distribution shaping. We also visualize the density of the generated data. In the Car-circle task, the robot receives high rewards when moving along the circle boundary and receives costs when it exceeds the boundaries on both sides, as shown in Fig. 7(c). The original dataset contains trajectories with various safety performances.

    & 10 & 20 & 40 \\  Ball- & reward & \(0.71 0.02\) & \(0.70 0.01\) & \(0.71 0.01\) \\ Circle & cost & \(0.72 0.10\) & \(0.45 0.14\) & \(0.99 0.13\) \\ Ball- & reward & \(0.29 0.04\) & \(0.28 0.01\) & \(0.29 0.01\) \\ Run & cost & \(0.16 0.14\) & \(0.79 0.37\) & \(0.00 0.00\) \\   

Table 2: Ablation study on denoising step \(K\)When using a low-cost condition, the generated data are clustered within the safety boundary to satisfy the constraints. When using a high-reward condition, the generated data points are closer to the circle boundary and receive higher rewards. In contrast, the baseline CVAE cannot successfully incorporate the conditions in data generation, resulting in almost similar datasets with different conditions as shown in Fig. 7(c). More experiment results including the visualization of generated dataset comparison and corresponding analysis are available in Appendix B.2.

### Robust performance against denoising steps

We conduct an ablation study on the key hyperparameter of the proposed OASIS method. The experiment related to the denoising steps \(K\) is presented in Table 2. Performance does not change much with different values, which shows the robustness of the proposed OASIS method.

## 6 Conclusion

In this paper, we study the challenging problem in offline safe RL: the safe data mismatch between the imperfect demonstration and the target performance requirements. To address this issue, we proposed the OASIS method to employ a conditional diffusion model to shape the dataset distribution and benefit offline safe RL training. In addition to the theoretical guarantee of performance improvement, we also conduct extensive experiments to show the superior performance of OASIS in learning a safe and rewarding policy on many challenging offline safe RL tasks. More importantly, our method shows good data efficiency and robustness to hyperparameters, which makes it preferable for applications in many real-world tasks.

There are two limitations of OASIS: (1) Offline training takes longer: our method involves preprocessing the offline dataset to enhance quality, which requires more time and computing resources; (2) Achieving zero-constraint violations remains challenging with imperfect demonstrations. One potential negative social impact is that misuse of our method may cause harmful consequences and safety issues. Nevertheless, we believe that our proposed method can inspire further _data-centric_ research in the safe learning community and help to adapt offline RL algorithms to real-world tasks with safety requirements.