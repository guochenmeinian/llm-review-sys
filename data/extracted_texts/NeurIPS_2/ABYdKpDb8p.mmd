# Learning Transferable Features for

Implicit Neural Representations

Kushal Vyas

kushal.vyas@rice.edu

&Ahmed Imtiaz Humayun

imtiaz@rice.edu

&Aniket Dashpute

aniket.dashpute@rice.edu

Richard G. Baraniuk

richb@rice.edu

&Ashok Veeraraghavan

vashok@rice.edu

&Guha Balakrishnan

guha@rice.edu

###### Abstract

Implicit neural representations (INRs) have demonstrated success in a variety of applications, including inverse problems and neural rendering. An INR is typically trained to capture one signal of interest, resulting in learned neural features that are highly attuned to that signal. Assumed to be less _generalizable_, we explore the aspect of transferability of such learned neural features for fitting similar signals. We introduce a new INR training framework, strainer that learns transferrable features for fitting INRs to new signals from a given distribution, faster and with better reconstruction quality. Owing to the sequential layer-wise affine operations in an INR, we propose to learn transferable representations by sharing initial _encoder_ layers across multiple INRs with independent _decoder_ layers. At test time, the learned _encoder_ representations are transferred as initialization for an otherwise randomly initialized INR. We find strainer to yield extremely powerful initialization for fitting images from the same domain and allow for a \(+10dB\) gain in signal quality early on compared to an untrained INR itself. strainer also provides a simple way to encode data-driven priors in INRs. We evaluate strainer on multiple in-domain and out-of-domain signal fitting tasks and inverse problems and further provide detailed analysis and discussion on the transferability of strainer's features. Our demo can be accessed here.

## 1 Introduction

Implicit neural representations (INRs) are a powerful family of continuous learned function approximators for signal data that are implemented using multilayer perceptron (MLP) deep neural networks. An INR \(f_{}:^{m}^{n}\) maps _coordinates_ lying in a \(m\)-dimensional space to a value in a \(n\)-dimensional output space, where \(\) represents the MLP's tunable parameters. For example, a typical INR for a natural image would use an input space in \(^{2}\) (consisting of the \(x\) and \(y\) pixel coordinates), and an output space in \(^{3}\) (representing the RGB value of a pixel). INRs have demonstrated several useful properties including capturing details at all spatial frequencies , providing powerful priors for natural signals , and facilitating compression . For these reasons, in the past 5 years, INRs have found important uses in image and signal processing including shape representation , novel view synthesis , material rendering , computational imaging , medical imaging , linear inverse problems , virtual reality  and compression .

A key difference between training INRs compared to other neural architectures like CNNs and Transformers is that a single INR is trained on a single signal. The features learned by an INR, therefore, are finely tuned to the morphology of just the one signal it represents. SplineCAM  shows that INRs learn to finely partition the input coordinate space by essentially overfitting to the spatial gradients (edges) of the signal. While this allows an INR to represent its signal with high fidelity, its features can not "transfer" in any way to represent a second signal, even with similar content. If INRs could exhibit elements of transfer learning, as is the case with CNNs and Transformers, their potential would dramatically increase, such as by encoding data distribution priors for inverse imaging problems.

In this work, we take a closer look at INRs and transferable features, and demonstrate that the _first several layers_ of an INR can be readily transferred from one signal to another from a domain when trained in a shared setting. We propose strainer, a simple INR training framework to do so (see Figure 1). strainer separates an INR into two parts: an "encoder" that maps coordinates to features, and a "decoder" that maps those features to output values. We fit the encoder over a number of training signals (\(1\) to \(10\) in our experiments) from a domain, e.g., face images, with separate decoders for each signal. At test time, we initialize a new INR for the test signal consisting of the trained encoder and a randomly initialized decoder. This INR may then be further optimized according to the application of interest. strainer offers a simple and general means of encoding data-driven priors into an INR's parameter initialization.

We empirically evaluate strainer in several ways. First, we test strainer on image fitting across several datasets including faces (CelebA-HQ) and medical images (OASIS-MRI) and show (Figure 2) that strainer's learned features are indeed transferrable resulting in a \(\)+10dB gain in reconstruction quality compared to a vanilla siren model. We further assess the data-driven prior captured by strainer by evaluating it on inverse problems such as denoising and super resolution. Lastly, we provide a detailed exploration of how strainer learns transferable features by exploring INR training dynamics. We conclude by discussing consequences of our results for the new area of INR feature transfer.

## 2 Background

**Implicit neural representations.** We define \(f_{}(p)\) as an implicit neural representation (INR)  where \(f_{}\) is a multi-layer perceptron (MLP) with randomly initialized weights \(\) and \(p\) is the \(m\)-dimensional coordinates for the signal. Each layer in the MLP is an affine operation followed

Figure 1: **strainer - Learning Transferable Features for Implicit Neural Representations.** During training time (a), strainer divides an INR into _encoder_ and _decoder layers_. strainer fits similar signals while sharing the encoder layers, capturing a rich set of transferrable features. At test-time, strainer serves as powerful initialization for fitting a new signal (b). An INR initialized with strainer’s learned encoder features achieves (c) faster convergence and better quality reconstruction compared to baseline siren models.

by a nonlinearity such as ReLU , or sine . Given an \(n\)-dimensional signal \(I(p)\), the INR learns a mapping \(f:^{m}^{n}\). The INR is iteratively trained to fit the signal by minimizing a loss function such as \(L_{2}\) loss between the signal \(I(p)\) and its estimate \(f_{}(p)\):

\[^{*}=_{}_{i}\ ||\ f_{}(p_{i})-I(p_{i}) \ ||_{2}^{2}\, \]

where \(p_{i}\)'s \(^{m}\) span the given coordinates, \(^{*}\) are the optimal weights that represent the signal, and \(\) is a differentiable forward model operator such as identity for signal representation and a downsampling operation for inverse problems such as super-resolution.

**Representation capacity of INRs.** The representation capacity of an INR can be described as the quality of signal the INR can represent within some number of iterations. ReLU-based INRs suffer from spectral bias during training, preferentially learning low frequency details in a signal and thus leading to a blurry reconstruction of the represented signal. Fourier positional encodings  or sinusoidal activation functions (siren )  help better capture high frequency information.

Recent works increase the representation capacity of INRs with activations flexible in the frequency domain. WIRE  uses a continuous Gabor wavelet-based nonlinearity, and demonstrates impressive results for a range of forward and inverse problems. FINER  splits the sine nonlinearity to have a flexible frequency coverage, and DINER uses a hash map to non-uniformly map the input coordinates to a feature vector, effectively re-arranging the spatial arrangement of frequencies and leading to faster and better reconstruction quality.

**Weight initialization for INRs.** Previous work has shown that smart initialization of INR weights allows for faster convergence. As shown in the siren study , hyper-networks are proposed to capture a prior over the space of implicit functions, mapping a random code to the weights of a siren model. Further, TransINR also shows Transformer-hypernetworks as powerful meta-learners for INR weight initialization. MetaSDF  and Light Field Networks (LFN)  use meta-learning-based initialization schemes to fit signed distance functions (SDFs) and light fields. Neural Compression algorithms  use weights obtained from meta-learning optimization as a reference to store MLP weights, leading to better compression than naively storing MLP weights. Tancik et al.  propose meta-learning-based approaches such as Model-Agnostic Meta-Learning (MAML) and Reptile for coordinate based neural representations. While these meta-learning approaches yield powerful initialization, they often require long computation time (over 150K steps ) and ample numbers of training data, and are unstable to train . Further, meta-learning initial modulations for an INR which are later optimized to fit data within few gradient updates has been shown to be an effective and scalable strategy for smoothly representing data(sets) as functa(sets). Contrary to our approach, Implicit Pattern Composers (IPC) proposes to keep the second layer of an INR instance-specific, while sharing the remaining layers and use a transformer hypernetwork to learn the modulations for the INR.

**Prior informed INRs.** Recent work has also explored embedding a learned prior in INRs for tasks such as audio compression, noise removal, and better CT reconstructions. Siamese Siren  uses a similar approach where they propose a compact siamese INR whose initial layers are shared followed by 2 siamese decoders. Since 2 randomly initialized decoders will yield slightly different reconstructions, this difference is leveraged for better noise estimation in audio signal. NERP  learns an internal INR prior for medical imaging by first fitting high quality MRI or CT data. Weights

Figure 2: **strainer learns faster.** We show the reconstruction quality (PSNR) of different initialization schemes for in-domain image fitting on CelebA-HQ . We compare siren model initialized by (1) random weights (siren), (2) fitting on another face image (siren finetuned), (3) strainer -1 (trained using one face image), and (4) strainer-10 (trained using ten face images). We also evaluate against multiple baselines such as Meta-Learned 5K , TransINR, and IPC

of this learned INR are used as an initialization for reconstructing new MRI or CT undersampled data. While this paper shows a method to learn an implicit prior, their prior embedding is learned from a single MRI or CT scan of the same subject whereas our work explores learning a prior for INRs by constraining it to learn an underlying implicit representation across multiple different images. PINER  introduces a test-time INR adaptation framework for sparse-view CT reconstruction with unknown noise.

## 3 Methods

We introduce strainer. We first explain our motivation to share initial layers in an INR Section 3.1. In Section 3.2 we describe the training phase of strainer where we learn transferrable features for INRs by sharing the initial layers of \(N\) INRs being fit independently to \(N\) images. Section 3.3, details how our captured basis is used to fit an unseen image. In subsequent sections, we seek to understand what our shared basis captures and how to expand it to other problems such as super resolution. For simplicity, we build upon the siren model as our base model.

### Why share the initial INR layers?

A recent method called SplineCAM  provides a lens with which to visualize neural network partition geometries. SplineCAM interprets an INR as a function that progressively warps the input space and fits a given signal through layerwise affine transforms and non-linearities . For continuous piecewise affine activation functions, we use an approximation to visualize (see Figure 6) the deep network's partition geometry for different pre-activation level sets .

An INR fit to a signal highly adapts to the underlying structure of the data in a layer-wise fashion. Furthermore, by approximating the spatial position of the pre-activation zero level sets, we see that initial layers showcase a coarse, less-partitioned structure while deeper layers induce dense partitioning collocated with sharp changes in the image. Since natural signals tend to be similar in their lower frequencies, we hypothesize that initial layers of multiple INRs are better suited for _transferability_. We therefore design strainer to share the initial _encoder_ layers, effectively giving rise to an input space partition that can generalize well across different similar signals.

### Learning transferable features from \(N\) images

Consider a siren model \(h(p)\) with \(L\) layers. Let \(K\) out of \(L\) layers correspond to an _encoder_ sub-network represented as \(f_{}\) The remaining layers correspond to the _decoder_ sub-network represented as \(g_{}\) as seen in Figure 1(a). For given input coordinates \(p\), we express the siren model \(h_{,}(p)\) as a composition (\(\)) of our encoder-decoder sub-networks.

\[h_{,}(p)=g_{} f_{}(p)\, \]

In a typical case, given the task of fitting \(N\) signals, each of the \(N\) signals is independently fit to an INR, thus not leveraging any similarity across these images. Since we want to learn a shared representation transferrable across multiple similar images, our method shares the encoder\(f_{}\) across all \(N\) INRs while maintaining a set of individual signal-specific decoders \(g_{}^{1}\  g_{}^{N}\).Our overall architecture is shown in Figure 1. We call this strainer's training phase - Figure 1(a). We start with randomly initialized layers and optimize the weights to fit \(N\) signals in parallel. For each signal \(I_{i}(p)\), we use a \(L_{2}\) loss between \(I_{i}(p)\) and its corresponding learned estimate \(h_{,}^{i}(p)\) and sum the loss over all the \(N\) signals. Iteratively, we learn a set of weights \(\) that minimizes the following objective:

\[^{*}=_{}_{i=1}^{N} g_{}^{i} f_{ }(p)-I_{i}(p)_{2}^{2}\, \]

where \(=[,\ ^{1}\ ^{N}]\) represents the full set of weights of the shared encoder (\(\)) and the \(N\) different decoders (\(g_{}^{1} g_{}^{N}\)) and \(^{*}\) represents the resulting optimized weights.

### Fitting an unseen signal with strainer

After sufficient iterations during strainer's training phase, we get optimized encoder weights \(f_{}\), which corresponds to the rich shared representation learned over signals of the same category. To fit a novel signal \(I_{}(p)\) we initialize the strainer model with the learned shared encoder weights \(f_{=^{*}}\) and randomly initialize decoder \(g_{}^{}\) weights to solve for:

\[^{*},^{*}=_{,}\;||\;g_{}^{} f_{= ^{*}}(p)-I_{}(p)\;||_{2}^{2}\;. \]

\(f_{=^{*}}\) serves as a learned initial encoder features. Our formulation is equivalent to a initial set of learned encoder features followed by a set of random projections. While fitting an unseen signal, we iteratively update all the weights of the strainer model, similar to any INR.

### Learning an intermediate partition space in the shared encoder \(f_{^{*}}\)

During the training phase, explicitly sharing layers in strainer allows us to learn a set of INR features which exhibits a common partition space shared across multiple images. Since deep networks perform layer wise subdivision of the input space, sharing the encoder enforces the layer to find the partition that can be further subdivided into multiple coarse partitions corresponding to the tasks being trained. In Figure 6(a.ii), while pre-training an INR using the strainer framework on CelebA-HQ dataset, we see emergence of a face-like structure captured in our strainer encoder \(f_{^{*}}\). We expect our strainer encoder weights \(f_{^{*}}\) to be used as transferrable features and be used as initialization for fitting unseen in-domain samples.

In comparison, meta learning methods to learn initialization for INRs exhibit a partitioning of the input space that is closer to random. As seen in Figure 6(a.i) there is a faint image structure captured by the the learned initialization. This is an indication that the initial subdivision of the input space, found by the meta learned pre-training layers, captures less of the domain specific information therefore is a worse initialization compared to strainer. We further explain our findings in Section 5 and discuss strainer's learned features being more transferrable and lead to better quality reconstruction.

## 4 Experiments

In all experiments, we used the siren MLP with 6 layers and sinusoid nonlinearities. We considered two versions of strainer : (i) strainer (1 decoder), where the encoder layers are initialized using our shared encoder trained on a single image, and (ii) strainer-10 (10 Decoders), where the encoder layers are initialized using our shared encoder trained on 10 images. We considered the following baselines: (i) a vanilla siren model with random uniform initialization , (ii) a fine-tuned siren model initialized using the weights from another siren fit to an image from the same domain, (iii) a siren model initialized using Meta-learned 5K , (iv) transformer-based metalearning models such as TransINR and IPC. We ensured an equal number of learnable

Figure 3: **Visualization of learned features in strainer and baseline siren model**. We visualize (a) the first principal component of the learned encoder features for strainer and corresponding layer for siren. At iteration 0, strainer’s features already capture a low dimensional structure allowing it to quickly adapt to the cat image. High frequency detail emerges in strainer’s learned features by iteration 50, whereas siren is lacking at iteration 100. The inset showing the power spectrum of the reconstructed image further confirms that strainer learns high frequency faster. We also show the (b) reconstructed images and remark that strainer fits high frequencies faster.

parameters (neurons) for all models. We normalized all images between (0-1), and input coordinates between (-1,1). We used the Adam optimizer with a learning rate of \(10^{-4}\) for strainer's training and test-time evaluation, unless mentioned otherwise. Further implementation details are provided in Supplementary.

### Datasets

We mainly used the CelebA-HQ , Animal Faces-HQ (AFHQ) , and OASIS-MRI [18; 28] images for our experiments. We randomly divided CelebA-HQ into 10 train images and 550 test images. For AFHQ, we used only the cat data, and used ten images for training and 368 images for testing. For OASIS-MRI, we used 10 of the (template-aligned) 2D raw MRI slices for training, and 144 for testing. We also used Stanford Cars and Flowers to further validate out of domain generalization and Kodak  true images for demonstrating high-resolution image fitting.

### Training strainer's shared encoder

We first trained separate shared encoder layers of strainer on 10 train images from each dataset. We share five layers, and train a separate decoder for each training image. For each dataset, we trained the shared encoder for 5000 iterations until the model acheives PSNR \( 30dB\) for all training images. We use the resulting encoder parameters as initialization for test signals in the following experiments. For comparison, we also trained the Meta-learned 5K baseline using the implementation provided by Tancik et.al. with 5000 outer loop iterations. We also use the implementation provided by IPC as our baselines for TransINR and IPC and train them with 14,000 images from CelebA-HQ. We report a comparison of number of training images and parameters, gradient updates, and learning time in Table 5.

### Image fitting (in-domain)

We first evaluated strainer on the task of in-domain image fitting. We cropped and resized all images to \(178 178\) and ran test-time optimization on all models for \(2000\) steps.

At test-time, both strainer and strainer-10 use only 1 decoder, resulting in the same number of parameters as a siren INR. Table 1 shows average image metrics for in-domain image fitting reported with 1 std. deviation. Instead of naively fine tuning using another INR, strainer's design of sharing initial layers allows for learning highly effective features which transfer well across images in the same domain, resulting in high quality reconstruction across CelebA-HQ and AFHQ and comparable to Meta-learned 5K for OASIS-MRI images. Table 3(CelebA-HQ, ID) also shows that strainer initialization results in better quality reconstruction, when optimized at test-time, compared to more recent transformer-based INR approaches such as TransINR and IPC as well.

Figure 4: **strainer converges to low and high frequencies fast.** We plot the histogram of absolute gradients of layers 1,5 and last over 1000 iterations while fitting an unseen signal. At test time, strainer’s initialization quickly learns low frequency, receiving large gradients update at the start in its initial layers and reaching convergence. The Decoder layer in strainer also fits high frequency faster. Large gradients from corresponding siren layers show it learning significant features as late as 1000 iterations.

### Image fitting (out-of-domain)

To test out-of-domain transferability of learned strainer features, we used strainer-10's encoder trained on CelebA-HQ as initialization for fitting images from AFHQ (cats) and OASIS-MRI datasets (see Table 2). Since OASIS-MRI are single channel images, we trained Meta-learned 5K and strainer-10 (GRAY) on the green channel only of CelebA-HQ images. To our surprise, we see strainer-10 and strainer-10 (GRAY) clearly outperform not only Meta-learned 5K, but also strainer-10 (in-domain). To further validate out of domain performance of strainer, we train

    &  &  &  \\  Method & PSNR\(\) & SSIM\(\) & PSNR\(\) & SSIM\(\) & PSNR\(\) & SSIM\(\) \\  siren & 44.91 \(\) 2.13 & 0.991 \(\) 0.007 & 45.11 \(\) 3.13 & 0.991 \(\) 0.005 & 53.03 \(\) 1.72 & 0.999 \(\) 0.0002 \\ siren fine-tuned & 51.11 \(\) 3.16 & 0.997 \(\) 0.013 & 53.07 \(\) 3.47 & 0.997 \(\) 0.001 & 58.86 \(\) 4.12 & 0.999 \(\) 0.0012 \\ Meta-learned 5K & 53.08 \(\) 3.36 & 0.994 \(\) 0.053 & 53.27 \(\) 2.52 & 0.996 \(\) 0.044 & 67.02 \(\) 2.27 & 0.999 \(\) 0.0000 \\ strainer (1 decoder) & 50.34 \(\) 2.81 & 0.997 \(\) 0.001 & 51.27 \(\) 2.94 & 0.997 \(\) 0.001 & 57.76 \(\) 2.19 & 0.999 \(\) 0.0001 \\ strainer-10 & 57.80 \(\) 3.46 & 0.999 \(\) 0.001 & 58.06 \(\) 3.75 & 0.999 \(\) 0.001 & 62.80 \(\) 3.17 & 0.999 \(\) 0.0003 \\   

Table 1: **In-domain image fitting evaluation. strainer’s learned features yield powerful initialization at test-time resulting in high quality in-domain image fitting**

    &  &  \\  Method & PSNR\(\) & SSIM\(\) & PSNR\(\) & SSIM\(\) \\  Meta-learned 5K & 52.40 \(\) 4.21 & 0.991 \(\) 0.077 & 65.06 \(\) 1.04 & 0.999 \(\) 0.00001 \\ strainer-10 & 57.46 \(\) 3.39 & 0.999 \(\) 0.0003 & 72.21 \(\) 8.73 & 0.999 \(\) 0.0001 \\ strainer-10 (Gray) & – & – & 74.61 \(\) 9.96 & 0.999 \(\) 0.0003 \\   

Table 2: **Out of domain image fitting evaluation, when trained on CelebA-HQ and tested on AFHQ and OASIS-MRI. strainer’s learned features are a surprisingly good prior for fitting images out of its training domain.**

Figure 5: **Fitting MRI images from OASIS-MRI dataset. At just 100 iterations, strainer is able to represent medical images with high quality. strainer’s initialization allows for fast recovery for sparse and delicate structures, showing applicability in low-resource medical domains as well.**strainer-10's shared encoder on simply 10 images from Flowers and Stanford Cars datasets which have different spatial distribution of color and high frequencies than AFHQ and OASIS-MRI. For fair comparison, all models in Table 3(OOD) were fit with 3-channel RGB or concatenated gray images in case of OASIS-MRI. As shown in Table 3(OOD), strainer-10 provides superior out of domain performance for AFHQ trained on CelebA-HQ, followed by Flowers and Stanford Cars. For OASIS-MRI, we see strainer-10 having best performance when trained with StanfordCars. This result suggests that strainer is capable of capturing transferable features that generalize well across natural images.

strainer also fits high resolution Kodak images well and is comparable to siren networks with twice the network width.

### Inverse problems: super-resolution and denoising

strainer provides a simple way to encode data-driven priors, which can accelerate convergence on inverse problems such as super-resolution and denoising. We sampled \(100\) images from CelebA-HQ at \(178 178\) and added \(2dB\) of Poisson random noise. We report mean values of PSNR achieved by strainer and siren models along with the iterations required to achieve the values. For super-resolution, we demonstrate results on one image from DIV2K, downscaled to \(256 256\) for a low resolution input. We used the formulation shown in Equation (1), with \(\) set to a \(4\) downsampling operation. To embed a prior relevant for clean images, we trained the shared encoder of strainer with high quality images of resolution same as the latent recovered image. At test time, we fit the strainer model to the corrupted image, following Equation (1) and recovered the latent image during the iteration. We report strainer's ability to recover latent images fast as well as with high quality in Section 4.5

    &  &  &  \\  Method & PSNR & \# iterations & PSNR & \# iterations & PSNR & \# iterations \\  siren & 32.10 & 3329 & 32.10 & 3329 & 26.75 \(\) 1.67 & 203 \(\) 66 \\ strainer -10 & 31.56 & 1102 (\( 3 faster\)) & 32.43 & 3045 & 26.41 \(\) 1.39 & 76 \(\) 27 \\   

Table 6: strainer **accelerates recovery of latent images in inverse problems. strainer captures an implicit prior over the training data allowing it to recover a clean latent image of comparable quality \(3\) faster making it useful for inverse problems.**

    & &  &  &  \\  Method & Width & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  siren & 256 & 36.77 & 0.94 & 0.13 & 31.89 & 0.87 & 0.19 & 34.68 & 0.94 & 0.093 \\ strainer-10 & 256 & 39.55 & 0.96 & 0.087 & 35.03 & 0.92 & 0.09 & 37.84 & 0.96 & 0.037 \\ siren & 512 & 40.18 & 0.96 & 0.11 & 34.23 & 0.90 & 0.14 & 38.80 & 0.97 & 0.051 \\ strainer-10 & 512 & 44.38 & 0.97 & 0.021 & 38.96 & 0.96 & 0.023 & 43.92 & 0.98 & 0.008 \\   

Table 4: **Out-of-domain image fitting on Kodak Images . strainer (trained on CelebA-HQ ) allows better convergence comparable to high capacity siren models as indicated by PSNR metric.**

   Method & \# training images & \# learnable params & Gradient updates / iteration & Time (Nvidia A100) \\  siren & N/A & 264,707 & N/A & N/A \\ strainer (1 decoder) & 1 & 264,707 & 264,707 & 11.84s \\ strainer-10 (10 decoders) & 10 & 271,646 & 271,646 & 24.54s \\ Meta-learned SK & 10 & 264,707 & 794,121 (\(\) 3\(\)more) & 1432.3s = 23.8 min \\ TransINR & 14,000 & \( 40M\) & \( 40M\) & \( 1\) day \\ IPC w TTO & 14,000 & \( 40M\) & \( 40M\) & \( 1\) day \\   

Table 5: **Training time and compute complexity.** We train all the methods for \(5000\) steps. strainer instantly learns a powerful initialization with minimal data and significantly fewer gradient updates.

## 5 Discussion and Conclusion

Results in Table 1, 3 demonstrate that strainer can learn a set of transferable features across an image distribution to precisely fit unseen signals at test time. strainer-10 clearly achieves the best reconstruction quality in terms of PSNR and SSIM on CelebA-HQ and AFHQ datasets, and is comparable with Meta-learned 5K on OASIS-MRI images. strainer-10 also fits images fast and achieves highest reconstruction quality than all baselines as shown in Figure 2. Comparing strainer (1 decoder) with a fine-tuned siren, it seems that the representation learned on one image is not sufficiently powerful. However, as little as 10 images result in a rich and transferable set of INR features allowing strainer-10 to achieve \(\)7-10dB higher reconstruction quality than siren and siren fine-tuned.

As seen in Table 2, 3(OOD) strainer also performs well on out-of-domain tasks, which is quite surprising.

strainer's transferable representations are capable of recovering small and delicate structures as early as 100 iterations as shown in Figure 5 and do not let the scale of features from the training phase affect its reconstruction ability. Another interesting finding is that strainer-10 achieves far better generalization for OASIS-MRI (Table 2) when pretrained on CelebA-HQ. Further, strainer generalizes well to out-of-domain high-resolution images, as demonstrated by our experiments of training strainer on CelebA-HQ and testing on the Kodak data (see Table 4).

strainer is fast and cheap to run. Table 5 summarizes the time for learning the initialization for a \(6\) layered MLP INR for strainer, Meta-learned 5K and transformer-based methods such as TransINR and IPC. At \(5000\) iterations, strainer learns a transferable representation in just \(24.54\) seconds. Meta-learned 5K, in comparison, uses MAML which is far more computationally intensive and results in \(20\) slower runtime when exact number of gradient updates are matched. Further, strainer's training setup is an elegant deviation from recent methods such as TransINR and IPC, requiring large datasets and complex training routines.

Figure 6: **Visualizing density of partitions in input space of learned models.** We use the method introduced in  to approximate the input space partition of the INR. We present the input space partitions for layers 2,3,4 across (a) Meta-learned 5K and strainer initialization and (b) at test time optimization. strainer learns an input space partitioning which is more attuned to the prior of the dataset, compared to meta learned which is comparatively more random. We also observe that siren (iii) learns an input space partitioning highly specific to the image leading to inefficient transferability for fitting a new image (iv) with significantly different underlying partitioned input space This explains the better in-domain performance of strainer compared to Meta-learned 5K, as the shallower layers after pre-training provide a better input space subdivision to the deeper layers to further subdivide.

### Limitations

Due to the encoder layers of strainer being tuned on data and the later layers being randomly initialized, we have observed occasional instability when fitting to a test signal in the form of PSNR "drops." However, we observe that strainer usually quickly recovers, and the speedup provided by strainer outweighs this issue. While our work demonstrates that INR parameters may be transferred across signals, it is not fully clear what features are being transferred, how they change for different image distributions, and how they compare to the transfer learning of CNNs and Transformers. Further work is needed to characterize these.

### Further analysis of strainer

To further understand how strainer's initialized encoder enables fast learning of signals at test time, we explored the evolution of strainer's hidden features over iterations in Figure 3. In Figure 3(a), we visualize the first principal component of learned INR features of the strainer encoder and corresponding hidden layer for siren across iterations and observe that strainer captures high frequencies faster than siren. This is corroborated by the power spectrum inset plots of the reconstructed images. We also visualize a histogram of gradient updates in Figure 4, and observe that strainer receives large gradients in its encoder layers early on during training, suggesting that the encoder rapidly learns of low-frequency details.

Next, we visualize the input space partitions induced by strainer and the adaptability of strainer's initialization for fitting new signals. We use the local complexity(LC) measure proposed by Humayun et.al. to approximate different pre-activation level sets of the INR neurons. For ReLU networks, the zero level sets correspond to the spatial location of the non-linearities of the network. For periodic activations, there can be multiple non-linearities affecting the input domain. In Figure 6 we present the zero level sets of the network, and in Supplementary we provide the \(/2\) shifted level sets. Darker regions in the figure indicate high LC, i.e., higher local non-linearity. Figure 6 also presents partitions for the baseline models.

siren models tend to overfit, with partitions strongly adapting to image details. Since the sensitivity to higher frequencies is mapped to specific input partitions, when finetuning with siren, the network has to unlearn partitions of the pretrained image resulting in sub optimal reconstruction quality. When comparing Meta-learned 5K with strainer, we see that strainer learns an input space partitioning more attuned to the prior of the dataset, compared to Meta-learned 5K which is comparatively more random. While both partitions imply learning high-frequency details, strainer's partitions are better adapted to facial geometry, justifying its better in-domain performance.

## 6 Broader Impacts

strainer introduces how to learn transferable features for INRs resulting in faster convergence and higher reconstruction quality. We show with little data, we can learn powerful features as initialization for INRs to fit signals at test-time. Our method allows the use of INRs to become ubiquitous in data-hungry areas such as patient specific medical imaging, personalized speech and video recordings, as well as real-time domains such as video streaming and robotics. However, our method is for training INRs to represent signals in general, which can adopted regardless of underlying positive or negative intent.