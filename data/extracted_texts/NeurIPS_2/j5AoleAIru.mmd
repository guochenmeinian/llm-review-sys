# What You See is What You Read?

Improving Text-Image Alignment Evaluation

Michal Yarom\({}^{}\)1

Yonatan Bitton\({}^{}\)1

Soravit Changpinyo\({}^{}\)

Roee Aharoni\({}^{}\)

Jonathan Herzig\({}^{}\)

Oran Lang\({}^{}\)

Eran Ofek\({}^{}\)

Idan Szpektor\({}^{}\)

\({}^{}\)Google Research \({}^{}\)The Hebrew University of Jerusalem

michalyarom@google.com, yonatan.bitton@mail.huji.ac.il

Equal contribution. Yonatan participated in this work as part of an internship at Google Research.Data and code are attached to this submission.

###### Abstract

Automatically determining whether a text and a corresponding image are semantically aligned is a significant challenge for vision-language models, with applications in generative text-to-image and image-to-text tasks. In this work, we study methods for automatic text-image alignment evaluation. We first introduce SeeTRUE: a comprehensive evaluation set, spanning multiple datasets from both text-to-image and image-to-text generation tasks, with human judgements for whether a given text-image pair is semantically aligned. We then describe two automatic methods to determine alignment: the first involving a pipeline based on question generation and visual question answering models, and the second employing an end-to-end classification approach by finetuning multimodal pretrained models. Both methods surpass prior approaches in various text-image alignment tasks, with significant improvements in challenging cases that involve complex composition or unnatural images. Finally, we demonstrate how our approaches can localize specific misalignments between an image and a given text, and how they can be used to automatically re-rank candidates in text-to-image generation.1

Figure 1: Overview of our approach to text-image alignment evaluation using SeeTRUE. We curate diverse pairs of real and synthetic text and images and use automatic contradiction generation and human evaluation to create a benchmark dataset. We propose two methods for text-image alignment evaluation: VQ\({}^{2}\) and VNLI, demonstrated with example pairs.

Introduction

The recent success and proliferation of multi-modal large language models (LLMs) for text-to-image and image-to-text generation  make such technology increasingly useful for a wide range of creative applications. However, such models still struggle in generating semantically-aligned image-text pairs; in text-to-image generation, models do not cope with complex specifications  or fail to map words in the prompt to visual entities . In image captioning, object hallucination is a long-standing challenge  with generated captions still being inferior to human-written ones .

Given the above, the task of automatically determining whether a given text-image pair is semantically aligned is highly important, as it is useful both for _evaluating_ and for _improving_ text-to-image and image-to-text models. However, existing evaluation approaches are still far from ideal; common methods like CLIP  or BLIP  are based on encoding the image and text as fixed-size embeddings, making it hard to model complex semantics . In addition, while the task is relevant both to text-to-image and image-to-text generation, it is usually studied in silo while considering only one of the applications, thus impeding progress.

In this work, we promote a comprehensive approach to evaluating image-text alignment. We introduce SeeTRUE, a diverse evaluation suite which includes a wide range of image-text pairs with human judgments that determine if the image and text are semantically aligned. SeeTRUE encompasses both real and synthetic images and text, allowing the assessment of text-image alignment models' generalization capabilities across various tasks and 31,855 labeled examples from diverse sources. As part of constructing SeeTRUE, we also introduce a novel method for generating contradicting captions from existing ones by prompting a large language model with tailored instructions.

We present two approaches for automatic image-text alignment evaluation. The first, VQ2, utilizes question generation and visual question answering by generating questions related to the text  and ensuring that the correct answer is obtained when asking these questions with the provided image. The second method, Visual Entailment2 (VNLI), involves directly fine-tuning a large pretrained multimodal model to predict if a given image-text pair is semantically aligned. Both strategies are inspired by recent studies on evaluating factual consistency between two texts .

We conduct comprehensive experiments on SeeTRUE, demonstrating that both our VQ2 and VNLI methods outperform a wide range of strong baselines, including various versions of CLIP , COCA , BLIP , and OFA . While previous work showed that vision-and-language models tend to exhibit sub-optimal "bag-of-words" behavior , the VQ2 method particularly excels on datasets with compositional challenges, achieving state-of-the-art results on the Winoground dataset  e.g. by improving the _group score_ from 16% to 30.5%. Our methods also demonstrate improved performance when evaluating synthetic images (e.g. on DrawBench  and EditBench ). Finally, we showcase how VQ2 can identify specific sources of misalignment for a given text-image pair and how our methods can re-rank generated image candidates for a given prompt.

To summarize, our contributions are as follows: (1) We introduce the SeeTRUE benchmark for meta-evaluation of image-text alignment. (2) We introduce a novel method to generate contradicting image captions from given captions with LLMs. (3) We suggest two reference-free metrics for image-text alignment evaluation: VQ2, based on question generation and visual question answering, and VNLI, based on fine-tuning large multimodal language models. (4) We conduct extensive evaluation of the above approaches against strong baselines, demonstrating superior performance over multiple datasets. (5) We release our evaluation suite, models and code to foster future work.

## 2 SeeTRUE: A Comprehensive Text-Image Alignment Benchmark

We begin by introducing SeeTRUE, a diverse benchmark for meta-evaluation of image-text alignment methods, covering the 4-way combinations of real and synthetic text-and-image pairs. It addresses limitations in current benchmarks, which mainly focus on natural images and often lack challenging negative captions. SeeTRUE allows to better assess the generalization abilities of text-image alignment models across various tasks.

Defining how image-text alignment is assessed has a direct impact on the construction of evaluation datasets. As images can display more details than described in their caption or text prompt, we define 

[MISSING_PAGE_FAIL:3]

### Human Annotation and Evaluation

To standardize the labeling scheme across datasets, we follow TRUE  and use binary annotations for alignment/misalignment. In datasets with three-way annotations (e.g. Entailment, Contradiction, Neutral) we convert the labels to binary labels by collapsing all non-entailment/non-alignment labels to a single negative label.

Some datasets, such as COCO-Con and PickaPic-Con, start with automatically generated labels, while others lack annotations entirely (e.g. DrawBench). To make sure we have high quality labels we conduct human annotation for all test examples in such datasets. We ask three crowd-workers from Amazon Mechanical Turk (AMT) to evaluate whether a given image-text pair is aligned, by answering the question: "Does the image present all the details described in the text correctly?" with "Yes" or "No". If the answer is "No", the workers are also requested to describe the main misalignment to enhance the annotation quality. While the random chance of agreement is 25%, the annotators reached consensus in 80% of cases. Furthermore, we measured a Fleiss-Kappa  score of 0.722, showing a good level of agreement between the annotators. Full annotation details, AMT user interface example, and agreement numbers per dataset can be found in appendix A.3.

The datasets we annotated include DrawBench, COCO \(\)i, COCO-Con and PickaPic-Con, with statistics presented in Table 1. These datasets vary in their positive/negative distribution, with COCO t2i having the highest percentage of positives (63.6%) and DrawBench having the lowest (36.9%). The agreement with the auto-label is 94% for COCO-Con and 77% for PickaPic-Con. To prevent the inclusion of offensive images, particularly those that are synthetically generated, annotators are asked to mark any images that may be considered offensive and these were discarded.

### ConGen: Generating Contradicting Captions by Prompting LLMs

We propose an automatic method for generating unaligned captions from existing, aligned image-and-text pairs, with the goal of creating challenging examples for evaluation and training. Our method is inspired by the concept of contrast sets: given an original example with a corresponding label, we create a minimally perturbed example where the perturbation changes the corresponding

Figure 2: (a) The SeeTRUE generation process. (i) An image-text pair from a dataset is used to generate a contradicting caption using ConGen. (ii) An image (real or synthetic) is passed through a captioning model to generate a caption, which is then passed to ConGen to generate a contradicting caption. (iii) A text-to-image model is applied on captions from the dataset to create multiple image-text pairs. All the resulting examples are evaluated by human raters to create SeeTRUE. (b) The contradiction generation process (ConGen) takes a caption as input and instructs an LLM to generate variants that contradict it. An NLI model is used to select the variant with the lowest entailment score.

label [31; 32; 33; 34]. Contrast sets address the issue of supervised models exploiting data artifacts in i.i.d. train/test splits to achieve high test scores, while their performance degrades significantly on samples outside their training distribution.

To create contrast sets for image-text alignment, we go over the text captions from the image-text pairs in the COCO and PickaPic datasets, covering both natural and synthetic images. For each caption we instruct PaLM , a large language model, to generate several contradicting captions via few-shot inference with 7 positive and 8 negative examples. For instance, for the caption "_a knife sitting next to carrots on top of a cutting board_", the model replaces the word _knife_ with _spoon_ (see Fig. 2, left). We then use a Natural Language Inference (NLI) model  to score whether the generated caption is indeed contradicting the original, and select the generated caption with the highest contradiction score. Figure 2 illustrates this process. Human annotators verified that the resulting contradicting captions are of high quality, with 94% agreement with human labels in COCO and 77% agreement with human labels in PickaPic (more details in section 2.2).

## 3 Methods

Using our SeeTRUE benchmark, we would like to reassess the performance of multimodal alignment approaches. In this section we introduce two image-text alignment methods. In Section 4 we will compare their performance against established, previously published methods.

### \(Vq^{2}\): Zero-Shot Alignment via Question Generation and Visual Question Answering

Inspired by recent work on factual consistency evaluation in text-to-text tasks [36; 18; 37], we propose a zero-shot approach for automatically evaluating image-text alignment based on question generation and question answering. Figure 3 provides an overview of the method. The motivation is to extract question-answer pairs, which capture the important details of the text, and then to validate that they are presented correctly in the image. For a given image-text pair \(\{I,T\}\), we first extract a set of candidate answer spans \(\{a_{j}\}_{j=1}^{N}\) from the given text \(T\). Then, we use a question generation (QG) model to generate a question for each answer candidate \(q_{j}=QG(a_{j},T)\). Each generated question-answer pair \((q_{j},a_{j})\) is scored with a question answering (QA) model, and if \(QA(q_{j},a_{j},T)\) returns a low score, we filter out the corresponding pair. This results in a subset of \(M\) question-answer pairs \(\{(q_{j},a_{j})\}_{j=1}^{M}\).

Each generated question-answer pair \((q_{j},a_{j})\) is then independently validated based on the image \(I\) using a visual question answering (VQA) model, obtaining an answer alignment score \(s_{j}=VQA(q_{j},a_{j},I)\) (more details on how this score is computed are given in 3.1). The overall alignment score for a image-text pair, denoted as the \(VQ^{2}\) score, is the average over all \(s_{j}\) scores for all the generated \((q_{j},a_{j})\) pairs. We next describe each step in more detail.

Generating question-answer pairs.We follow the \(VQ^{2}A\) method  to generate question and answer pairs given an image caption in three steps. The purpose is to generate high quality question-answer pairs, which capture the most important details of the text. First, answer spans are extracted from text \(T\) using SpaCy , based on Part-of-Speech (POS) and dependency parse tree annotations. Then, for each answer span, a question \(q_{j}\) is generated given the answer span and the full caption as input using a T5-XXL model fine-tuned on SQuAD1.1 . Finally, each candidate question-answer pair \((q_{j},a_{j})\) is validated by answering \(q_{j}\) on \(T\) using a QA model, which is trained by fine tuning a

Figure 3: The \(Q^{2}\) pipeline: (a) given a text and an image, (b) generate question and answer pairs from the text, (c) re-write each pair as a yes-no question and obtain the ’yes’ answer probability from the image as an alignment score. (d) Finally, average all alignment pair scores as the final \(VQ^{2}\) score.

T5-XXL model on SQuAD2.0  and Natural Questions . Finally, we match the output answer \(a^{}_{j}\) to the expected answer \(a_{j}\) using token-level F1 comparison. As suggested in , if the answer comparison F1 score is lower than 0.54 the question-answer pair is filtered out.

Assessing question-answer pair alignment against the image.To determine if the information conveyed by the text \(T\) is presented correctly in the image \(I\), we use a VQA model based on PaLI-17B  as follows. We reformulate each question and answer candidate pair \((q_{j},a_{j})\) into a new yes-no predicate question \(q^{}_{j}\) using the format _"is \(\{a_{j}\}\) true for \(\{q_{j}\}\) in this image?"_. For example, for the text _"two girls are sitting on some grass"_, and the automatically induced question-answer pair {_"what are the girls sitting on?"_, _"some grass"_}, the reformulated question is "is _on some grass_ true for _what are the girls sitting on?_ in this image?". The VQA model is then invoked to answer the predicate question \((q^{}_{j})\) over image \(I\). We define the alignment score \(s_{j}\) as the probability of the model for answering "yes". We note that we also experimented with other answer alignment methods, e.g. ones that directly ask the generated question without formulating it as a yes/no question. However, the yes-no approach worked best. More details can be found in appendix A.4.

### End-to-end VNLI Models

Another approach is to train end-to-end Visual NLI models (VNLI) that receive an image and text as input, and directly predict an alignment score. We do so by fine-tuning multimodal pretrained models while formatting the examples as yes/no questions using the prompt: "Does this image entail the description: {text}?", followed by a binary "yes" or "no" answer. In inference time we measure the probabilities of predicting "yes" or "no", and use the relative ratio between the two as the alignment score. Specifically, we finetune BLIP2  and PaLI-17B  using a dataset comprising 110K text-image pairs labeled with alignment annotations. This includes 44K examples from COCO-Con, 3.5K from PickaPic-Con, 20K from COCO 12i and 40K from the training split of the SNLI-VE dataset. We generate COCO-Con and COCO 12i based on the COCO train split and PickaPic-Con with a distinct set of images, to ensure that there is no overlap with samples in the SeeTRUE benchmark. More technical details and training hyperparameters are described in appendix A.7.

## 4 Experiments

### Models and Metrics

We evaluate \(VQ^{2}\) and fine-tuned VNLI models based on PaLI and BLIP2 (Section 3) against several state-of-the-art multimodal models: (a) CLIP  and two larger versions - CLIP RN50x64 and CLIP ViT-L 14 , (b) CoCa , (c) BLIP Large , (d) BLIP2 FlanT5-XXL , and (e) OFA Large , and (f) TIFA . First five models were typically trained with either a contrastive objective or an image-text matching objective that samples positive or negative caption-label pairs. TIFA, like VQ\({}^{2}\), employs a VQA model with generated question-answer pairs. However, TIFA contrasts textual and visual answer candidates provided by the model, while our method checks if the textual answer is accurate given the image.

We assess each method's ability to detect misalignments in each dataset in SeeTRUE. We use a binary labeling scheme and report the Area Under the ROC Curve (ROC AUC) for each method. For Winoground, we use existing metrics: (1) _text score_: accuracy in selecting the right caption for an image; (2) _image score_: accuracy in choosing the correct image given a caption; (3) _group score_: accuracy requiring all four image-caption pairs to be correct for a successful example.

### Results

We present our main results in Table 2. Notably, our V\(Q^{2}\) approach excels as the top-performing zero-shot model across all datasets, surpassing other zero-shot baselines and even outperforming most of the fine-tuned models while achieving the highest score on the challenging Winoground dataset. This shows the robustness of the VQ\({}^{2}\) approach, which decomposes the alignment decision by generating multiple yes/no verification questions.

When looking at finetuned models, the PaLI variant finetuned on all the available datasets outperforms all the rest with an average score of 82.9, achieving the best results on 3 out of 7 datasets. The SNLI-VE-only variant is behind with an average score of 79.7, while achieving the highest scores for 2 out of 7 datasets. This shows that integrating synthetic training data leads to notable improvements on synthetic images on DrawBench (+4%), EditBench (+11.7%), COCO 2i (+5.5%), PickaPic-Con (+2.2%). Nevertheless, the inclusion of synthetic training data did not enhance performance on the COCO-Con dataset, comprised solely of natural images. This indicates that the variation in image types could be a contributing factor that calls for additional exploration. Notably, the last row shows a simple average between VQ\({}^{2}\) and our leading fine-tuned PaLI model, that produces higher performance, suggesting that they complement each other effectively.

    &  &  &  & Synthetic + Synthetic & **Avg.** \\  Model &  & Winoground & DrawBench & EditBench & COCO 2i & COCO-Con & PickaPic-Con & **Avg.** \\    } & CLIP RN50s64 & 66.6 & 53.6 & 59.2 & 67.1 & 58.8 & 71.1 & 66.8 & 63.3 \\  & CLIP ViT-L14 & 65.8 & 53.3 & 60.5 & 62.1 & 58.8 & 70.7 & 66.8 & 62.6 \\  & COCO-ViT-L14 & 68.5 & 53.1 & 67.4 & 66.3 & 62.1 & 74.2 & 68.1 & 65.7 \\  & COCO ViT-L14 & 65.7 & 53.1 & 66.2 & 68.3 & 66.2 & 76.5 & 67.2 & 66.8 \\  & (f.t.t. COCO) & 70 & 53.1 & 66.2 & 68.3 & 66.2 & 76.5 & 67.2 & 66.8 \\  & BLIP & 75.2 & 58.2 & 60.5 & 68 & 70.7 & 84.2 & 76.6 & 70.5 \\  & BLIP & 76.4 & 56.9 & 58.5 & 67.5 & 66.9 & 84.3 & 76.9 & 69.6 \\  & BLIP 2 (f.t. COCO) & 75.9 & 60 & 65.7 & 70 & 73.3 & 85.8 & 78 & 72.7 \\  & PAL & & 65.4 & 53.6 & 60.2 & 56.7 & 53.3 & 65.5 & 60.5 & 93.3 \\  & TIFA & & – & 58.0 & 73.4 & 67.8 & 72.0 & – & – & – \\  & VQ\({}^{2}\) (Ours) & 88.0 & **63.5** & 82.6 & 73.6 & **83.4** & 87.1 & 81.7 & **80.0** \\    } & OFA Large (470M) & 80.5 & 53.3 & 77.6 & 70.9 & 67.5 & 75.4 & 69.5 & 70.7 \\  & BLIP (12B) & 82.3 & 58.5 & 64.3 & 58.7 & 60.5 & 82.6 & 66.9 & 67.7 \\  & PAL (17B) & **95.1** & 61.7 & 82.8 & 65.5 & 77.7 & **91.2** & 83.7 & 79.7 \\  & PAL + Synthetic Data & 94.2 & 61.8 & **86.8** & **77.2** & 83.2 & 91 & **88.9** & **82.9** \\  ^{2}\), PaLI+Syn)} & 93.9 & **63.5** & **87.8** & **78.4** & **85.1** & **93** & **87.3** & **84.1** \\   

Table 2: Main Results on SeeTRUE, split into zero-shot and end-to-end fine-tuned methods across the real and synthetic image-text eval-sets. The numbers in the table are ROC AUC. Note that TIFA and \(VQ^{2}\) require higher computational cost. They generate question-answer pairs and use a VLM model, while the other models use the image-text directly.

Figure 4: Contradicting captions and the question/answer pairs with lower \(VQ^{2}\) alignment score, indicating the contradiction reason.

   Model & Test Score & Image Score & Group Score \\  \(VQ^{2}\) (Ours) & **47.00** & **42.20** & **30.50** \\ PaLI (f.t. SNLI-VE + Synthetic Data) & 46.5 & 38 & 28.75 \\ PaLI (f.t. SNLI-VE) & 45.00 & 41.50 & 28.70 \\ BLIP (f.t. COCO) & 44.00 & 26.00 & 23.50 \\ LaISlarge  & 42.50 & 19.75 & 16.00 \\ VinVL  & 37.75 & 17.75 & 14.50 \\ TIFA & 19.00 & 12.50 & 11.30 \\ CLIP RN50s64 & 26.50 & 13.75 & 10.25 \\ OFA Large (f.t. SNLI-VE) & 27.70 & 14.30 & 9.00 \\ COCO ViT-L14 (f.t. on COCO) & 28.25 & 11.50 & 8.25 \\  Random Chance  & 25.00 & 25.00 & 16.67 \\ Humans  & 89.50 & 88.50 & 85.50 \\   

Table 3: Results on the Winoground dataset, reporting text score, image score, and group score.

**Winoground Results.** Table 3 shows the performance of the different methods on the challenging Winoground dataset, which requires strong visual reasoning and compositional understanding skills. Our zero-shot approach, VQ\({}^{2}\), achieves state-of-the-art results on this dataset, surpassing other strong baselines, with a group score of 30.5%. This again indicates that VQ\({}^{2}\)'s approach that decomposes the alignment task into multiple question-answer pairs is a promising path for image-text alignment.

**Contradiction Generation.** We assessed the VQ\({}^{2}\) method's capacity to detect image-text contradictions, as shown in fig. 4. Contradiction generation relies on identifying the question-answer pair with the lowest VQA score, signaling the least likely alignment between the image and the text. Three paper authors evaluated whether a particular contradiction (consisting of a question and an answer) accurately represents the primary discrepancy between the image and the text. The majority vote among the authors determined the final outcome, yielding the following accuracy rates: 88% for Coco-Con, 74% for DrawBench, and 80% for Winoground. This indicates that our method is capable of identifying these contradictions by investigating the structure and content of the given caption and image. As a result, our method can achieve strong results, particularly on datasets that require compositional understanding.

**Comparing Generative Models.** VQ\({}^{2}\)'s ability to compare between generative models is demonstrated on the results of DrawBench and COCO-t2i, which include generated images from different models, together with human quality ratings. fig. 5 shows that the VQ\({}^{2}\) and our fine-tuned paLI ranking correlates very well with human ranking (\(R^{2}>0.92\)). In addition, since unlike human annotations, the VQ\({}^{2}\) score is consistent across datasets, it offers a way to evaluate dataset difficulty on an absolute scale.

**Reranking Using Alignment Assessment.** Alignment scores can also be used for reranking candidate generations, on top of evaluation. To demonstrate this, we re-rank the image candidate per prompt in the DrawBench and COCO-t2i datasets. We do so using VQ\({}^{2}\) and CLIP and measure the human-labeled quality of the top-ranked image for each method. The results, presented in table 4, show that ranking with VQ\({}^{2}\) consistently achieves higher quality scores when compared to ranking with CLIP. One such example is shown in fig. 6, where both VQ\({}^{2}\) and our top-performing fine-tuned paLI model demonstrate superior ranking by placing the brown-and-white cats above the white-only cats. This consistency between VQ\({}^{2}\) and paLI highlights their alignment evaluation models' potential for enhancing text-to-image systems, which contrasts with the divergent ranking exhibited by CLIP.

   Dataset & Model & Random & CLIP & PaLI & VQ\({}^{2}\) \\   & SD 1.4 & 68.6 & 74.6 & 88.2 & 86.4 \\  & SD 2.1 & 71.3 & 81.2 & 84.5 & 87.3 \\   & SD 1.4 & 66.7 & 77.4 & 77.4 & 87.1 \\  & SD 2.1 & 59.0 & 78.0 & 87.0 & 82.0 \\   

Table 4: Comparison of human-labeled quality scores for top-ranked images with model breakdown

Figure 5: Highly correlated VQ\({}^{2}\) and paLI scores vs. human rankings of text-to-image models

## 5 Related Work

Our work advances research in visual entailment (VE) , visual question answering (VQA) , text-to-image alignment evaluation, and cross-task consistency for multi-modal models, with a focus on enhancing the semantic understanding of image-caption relationships.

Textual Entailment (TE) [26; 47] evaluates the truthfulness of a textual hypothesis given a textual premise, providing a key benchmark for the semantic capabilities of neural network models [48; 49; 50; 35]. Recently, TE has been adapted to the multimodal domain as Visual Entailment (VE)  to assess the semantic alignment between images and text. Vision-and-language models like CLIP , CoCa , BLIP , BLIP2  and OFA  often act as bag-of-words models, lacking a deep comprehension of language compositionality . Our approach addresses this by generating multiple questions probing diverse semantic aspects, thereby improving performance on challenging compositional tasks like Winoground  and unnatural images as in Drawbench .

Unlike DrawBench  and DALL-Eval  which depend on human feedback and operate within a discrete set of alignments, our approach produces automated scores for a broader range of text-image alignments, facilitating efficient evaluation of vision-and-language models. Our approach also surpasses the recently proposed TIFA , which may be due to employing more question-answer pairs and tailored models for question generation and answering.

Several works have explored cross-task consistency in multi-modal models across various modalities. VQA studies have tackled inconsistencies and enhanced consistency using data augmentation and contrastive loss. NLP researchers have improved consistency across tasks or within a single task by employing counterfactual instances or contrast sets [26; 47]. Our research aligns with studies that evaluate natural text and images ; however, extending the focus to synthetic images and texts, and aligning with synthetic image understanding research [53; 54; 55; 56; 57; 58]. We introduce two unique approaches to address the complexities of image-text alignment.

Another related effort is PickScore , which predicts human preferences for image quality and aesthetics by ranking or choosing between two images. In contrast, our methods independently score a single image and focus specifically on image-text alignment.

## 6 Limitations

We recognize that in some cases, making a binary decision for whether a text and an image are aligned may be be difficult, also for humans. To tackle this limitation, we provided human annotators with comprehensive guidelines, which resulted in a high inter-annotator agreement (Fleiss-Kappa score of 0.722 with 80% of the cases where all annotators agreed on the entailment label).

Although many images in our datasets were obtained by others and not created by us, we made an effort to ensure that they do not contain harmful or potentially harmful content, such as NSFW or biased imagery. During the annotation process, three individuals examined each image and indicated if it could be considered offensive. Additionally, two of the authors manually reviewed the images

Figure 6: Four COCO-t2i text-to-image model outputs ranked by VQSQR scores, correlating with top PaLI model. Image order and CLIP (RN50x64) similarity scores given, but not aligned with VQ\({}^{2}\)/PaLI ranks.

for any harmful content. However, we understand that the perception of harmful or offensive content may vary among individuals and may be subject to personal interpretation.

## 7 Conclusion

We addressed the task of image-text alignment evaluation, which we find very close to the Visual Entailment (VNLI) task. We first introduced the SeeTRUE benchmark, which covers the mix of real and synthetic text and image pairs in text-to-image and image-to-text generation tasks, and includes challenging cases based on generated contradictions. We then proposed two methods, VQ\({}^{2}\) and end-to-end VNLI, which outperform strong baselines on SeeTRUE and can serve as a starting point for future research on the task.

In future work, we would like to employ our automatic evaluation models for guiding the training of text-to-image and image-to-text models towards more aligned outputs, following recent trends in text-to-text generation . For example, such models may be useful either for filtering training examples or as a reward when training models using reinforcement learning.