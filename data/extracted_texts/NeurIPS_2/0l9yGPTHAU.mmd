# Near-Optimal Distributionally Robust Reinforcement Learning with General \(L_{p}\) Norms

Pierre Clavier

Ecole Polytechnique, Inria

Laixi Shi

Caltech

&Erwan Le Pennec

Ecole polytechnique

&Eric Mazumdar

Ecole polytechnique

&Adam Wierman

Caltech

&Matthieu Geist

Cohere

CMAP, CNRS, Ecole Polytechnique, Institut Polytechnique de Paris, 91120 Palaiseau

Inria Paris, HeKA, 75015 Paris, France

###### Abstract

To address the challenges of sim-to-real gap and sample efficiency in reinforcement learning (RL), this work studies distributionally robust Markov decision processes (RMDPs) -- optimize the worst-case performance when the deployed environment is within an uncertainty set around some nominal MDP. Despite recent efforts, the sample complexity of RMDPs has remained largely undetermined. While the statistical implications of distributional robustness in RL have been explored in some specific cases, the generalizability of the existing findings remains unclear, especially in comparison to standard RL. Assuming access to a generative model that samples from the nominal MDP, we examine the sample complexity of RMDPs using a class of generalized \(L_{p}\) norms as the 'distance' function for the uncertainty set, under two commonly adopted \(sa\)-rectangular and \(s\)-rectangular conditions. Our results imply that RMDPs can be more sample-efficient to solve than standard MDPs using generalized \(L_{p}\) norms in both \(sa\)- and \(s\)-rectangular cases, potentially inspiring more empirical research. We provide a near-optimal upper bound and a matching minimax lower bound for the \(sa\)-rectangular scenarios. For \(s\)-rectangular cases, we improve the state-of-the-art upper bound and also derive a lower bound using \(L_{}\) norm that verifies the tightness.

## 1 Introduction

Reinforcement learning (RL) (Sutton, 1988) is a popular paradigm in machine learning, particularly noted for its success in practical applications. The RL framework, usually modeled within the context of a Markov decision process (MDP), focuses on learning effective decision-making strategies based on interactions with a fixed environment. However, the work of Mannor et al. (2004), among others, has highlighted a vulnerability in RL strategies, revealing the sensitivity to inherent shift or estimation errors in the reward and transition probabilities. A specific example of this is when, because of a sim-to-real gap, policies learned in idealized environments fail when deployed in environments with slight changes or adversarial perturbations (Klopp et al., 2017; Mahmood et al., 2018).

To address this issue, distributionally robust RL, usually formulated as robust MDPs (RMDPs), proposed by Iyengar (2005) and Nilim and El Ghaoui (2005), have attracted considerable attention. RMDPs are formulated as max-min problems, seeking policies that are resilient to model environmentperturbations within a specified uncertainty set. Despite the robustness benefits, solving RMDPs is NP-hard for general uncertainty sets (Nilim and El Ghaoui, 2005). To overcome this challenge, the rectangularity condition is often adopted so that the uncertainty set can be decomposed as products of independent subsets for each state or state-action pair, denoted as \(s\)-rectangular or \(sa\)-rectangular assumptions (see Definition 4 and 5). These assumptions facilitate computation traceability of methods such as robust value iteration and robust policy iteration, preserving many structural properties of MDPs (Ho et al., 2021). The \(s\)-rectangularity condition, though with less restrictive structure assumption, impose more challenges for algorithm design, while the \(sa\)-rectangularity condition allows for deterministic optimal policies akin to non-robust MDPs (Wiesemann et al., 2013). Note that dealing with uncertainty in transition kernels is much more difficult than that in rewards (Kumar et al., 2022; Derman et al., 2021).

The question of sample efficiency is central in RL problems ranging from practice to theory. Although minimax sample efficiency has been achieved for standard MDPs (Azar et al., 2013; Li et al., 2023), this goal in general remains open in RMDPs. Specifically, there exists prior work studying the sample complexity of distributionally robust RL for a few specific divergences such as total variation (\(TV\)) distance, \(^{2}\) divergence, Kullback-Leibler divergence (\(KL\)) divergence, and Wasserstein distance (see discussions in Appendix A) (Yang et al., 2022; Zhou et al., 2021; Panaganti and Kalathil, 2022). While such results remain unclear for more general class, such as the general smooth \(L_{p}\) norms (see Def. 1). To the best of our knowledge, minimax optimal sample complexity for the full range of uncertainty level has only been achieved for one case \(-TV\) distance (Shi et al., 2023). In this work, we focus on understanding the sample complexity of RMDPs with a general smooth \(L_{p}\) that will be defined in Def. 1. This generalized result is appealing for both practice and theory. In practice, numerous applications are based on optimizations or learning approaches that involve general norms beyond those specific cases that have been studied in prior works. Additionally, optimizing \(L_{p}\) norm weighted ambiguity sets for robust MDPs has been proposed in the context of RMDPs in Russel et al. (2019), which justifies our formulation. Theoretically, prior work has characterized the sample complexity of RMDPs for some specific norms have suggested intriguing insights about the statistical implications of distributional robustness in RL. It is interesting to further understand the statistical cost of robust RL in more general scenarios. One area of focus is the contrast between the sample efficiency of solving distributionally robust RL and solving standard RL. In particular, for the specific case of \(TV\) distance, Shi et al. (2023) shows that the sample complexity for solving robust RL is at least the same as and sometimes (when the uncertainty level is relatively large) could be smaller than that of standard RL. This motivates the following open question:

_Is distributionally robust RL more sample efficient than standard RL for some general class of norms (Def. (1))_?

A second question is about the comparisons between the sample complexity of solving \(s\)-rectangular RMDPs and that of solving \(sa\)-rectangular RMDPs. Note that \(s\)-rectangular RMDPs involve more complex optimization problems with additional variables (uncertainty levels for each action) to optimize. This leads to a richer class of optimal policy candidates--stochastic policies in \(s\)-rectangular cases, in contrast to the class of deterministic policies for \(sa\)-rectangular cases. In addition, existing sample complexity upper bounds for solving \(s\)-rectangular RMDPs are larger than that for solving \(sa\)-rectangularity (Yang et al., 2022) for the investigated cases. This motivates the curious question:

_Does solving \(s\)-rectangular RMDPs require more samples than solving \(sa\)-rectangular RMDPs with general smooth \(L_{p}\) norms defined in Def. 1?_

**Main contributions.** In this paper, we address each of the two questions discussed above. In particular, we provide the first sample complexity analysis for RMDPs with general \(L_{p}\) norms (cf. Def. 1) under both the \(s\)- and \(sa\)-rectangularity conditions. For convenience, we present detailed comparisons between the prior arts and our results in Table 1 for quick reference and discuss the contributions and their implications as below.

\(\) Considering the first question, we illustrate our results in both \(sa\)- and \(s\)-rectangular cases in Figure 1. In the case of \(sa\)-rectangularity, we derive a sample complexity upper bound for RMDPs using general smooth \(L_{p}\) norms (cf. Theorem 1) in the order of \((\{1-,C_{g} \}^{2}})\). Here, \(\) is the uncertainty level/radius of the uncertainty set, and \(C_{g}>0\) is a positive constant related to the geometry of the norm defined in Def. 1. For classical \(L_{p}\) norms, \(C_{g} 1\) so we can directly relax this constant to \(1\) to obtain the result in Table 1. In addition, we provide a matching minimax lower bound (cf. Theorem 2) that confirms the near-optimality of the upper bound for almost full range of the uncertainty level. Our results match the near-optimal sample complexity derived in Shi et al.  for the specific case using TV distance, while holding for broader cases using general \(L_{p}\) norms. The results rely on a new dual optimization form for \(sa\)-rectangular RMDPs and reveal the relationship between the sample complexity and this new dual form -- the infinite span seminorm (controlled in Lemma 5), which may be of independent interest.

In the case of \(s\)-rectangularity, we provide a sample complexity upper bound for solving RMDPs with general smooth \(L_{p}\) norms in the order of \((\{1-,C_{g}_{s}\|_{ s}\|,\}^{2}})\) with \(\|.\|_{*}\) the dual norm and \(\) the radius of the ball in the \(s\)-rectangular uncertainty set. This result improves the prior art \((e^{2}})\) in Clavier et al.  for classical \(L_{p}\) -- by at least a factor of \(O()\) when \( 1-\). Furthermore, we present a lower bound for a representative case with \(L_{}\) norm, which corroborates the tightness of the upper bound. To the best of our knowledge, this is the first lower bound for solving RMDPs with \(s\)-rectangularity.

\(\) We highlight the technical contributions as below. For the upper bounds, regarding optimization contribution, we derive new dual optimization problem forms for both \(sa-\) and \(s-\) rectangular cases (Lemma 3 and 4), which is the foundation of the covering number argument in finite-sample analysis. From a statistical point of view, a new concentration lemma (See Lemma 8 for dual forms) is introduced to obtain a lower sample complexity than standard RL, controlling the infinite span semi norm of the value function, both for \(sa-\) and \(s-\) rectangular case are derived (See Lemma 5 and 6). For the lower bound, the technical contributions are mainly in \(s\)-rectangular cases, which involves entire new challenges compared to \(sa\)-rectangularity case: the optimal policies can be stochastic and hard to be characterized as a closed form, compared to the deterministic one in \(sa\)-rectangular cases. Therefore, we construct new hard instances for \(s\)-rectangular cases that is distinct from those used in \(sa\)-rectangular cases or standard RL.

\(\) Considering the second question, as illustrated in Figure 1, our results highlight that robust RL is at least the same as and sometimes can be more sample-efficient to solve than standard RL for general smooth \(L_{p}\) norms (cf. Def. 1). This insight is of significant practical importance and serves to provide crucial motivation for the use and study of distributionally robustness in RL. Notably, robust RL does not only reduce the vulnerability of RL policy to estimation errors and sim-to-real gaps, but also leads to better data efficiency. In terms of comparing the statistical implications of \(sa\)- and \(s\)- rectangularity, our results show that solving \(s\)-rectangular RMDPs is not harder than solving \(sa\)-rectangular RMDPs in terms of sample requirement (See Theorem 3 and Figure 2, Right).

## 2 Problem Formulation: Robust Markov Decision Processes

In this section, we formulate distributionally robust Markov decision processes (RMDPs) in the discounted infinite-horizon setting, introduce the sampling mechanism, and describe our goal.

   &  &  &  &  \\   & & & \(0< 1-\) & \(1-<_{}\) & \(0< 1-\) & \(1-<_{}\) \\   & Yang et al. [2022a] & TV & \(A_{2}+s^{2}}{S^{(1-)^{2}}}\) & \(A_{2}+s^{2}}{S^{(1-)^{2}}}\) & \(A_{2}^{2}+s^{2}}{S^{(1-)^{2}}}\) & \(A_{2}^{2}+s^{2}}{S^{(1-)^{2}}}\) \\   & Panaganti and Kaluthi  & TV & \(A_{2}}{S^{(1-)^{2}}}\) & \(A_{2}}{(1-)^{2}}\) & \(\) & \(\) \\   & Shi et al.  & TV & \(A_{2}}{(1-)^{2}}\) & \(A_{2}}{(1-)^{2}}\) & \(\) & \(\) \\   & Clavier et al.  & \(L_{p}\) & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) \\   & **This paper** & \(L_{p}\) & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) \\   & **This paper** & General \(L_{p}\)  & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) \\   & Yang et al. [2022a] & TV & \(}{(1-)^{2}}\) & \((1-)^{2}}{S^{(1-)^{2}}}\) & \(\) & \(\) \\   & Shi et al.  & TV & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) & \(\) & \(\) \\    & **This paper** & \(L_{p}\) & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) & \(\) & \(\) \\    & **This paper** & \(L_{}\) & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) & \(}{(1-)^{2}}\) \\  

Table 1: Comparisons with prior results (up to log terms) regarding finding an \(\)-optimal policy for the distributionally RMDP, where \(\) is the radius of the uncertainty set and \(_{}\) defined in Theorem 1.

Standard Markov decision processes (MDPs).A discounted infinite-horizon MDP is represented by \(=(,,,P,r)\), where \(=\{1,,S\}\) and \(=\{1,,A\}\) are the finite state and action spaces, respectively, \([0,1)\) is the discounted factor, \(P:()\) denotes the probability transition kernel, and \(r:\) is the immediate reward function, which is assumed to be deterministic. Moreover, we assume that the reward function is bounded in \((0,1)\) without loss of generality of the results due to the variance reward invariance. Finally we denote \(1_{A}\) or \(1_{S}\) the unitary vector of respectively dimension \(A\) or \(S\). Moreover, \(e_{s}\) is the standard unitary vector supported on \(s\). The policy we are looking for is denoted by \(:()\), which specifies the probability of action selection over the action space in any state. Note that if the policy is deterministic in the \(sa\)-rectangular case, we overload the notation and refer to \((s)\) as the action selected by the policy \(\) in state \(s\). Finally, to characterize the cumulative reward, the value function \(V^{,P}\) for any policy \(\) under the transition kernel \(P\) is defined by \( s\)

\[V^{,P}(s)_{,P}[_{t=0}^{}^{t}r s_{t},a_{t}\,\,s_{0}=s]. \]

The expectation is taken over the randomness of the trajectory \(\{s_{t},a_{t}\}_{t=0}^{}\) generated by executing the policy \(\) under the transition kernel \(P\), such that \(a_{t}(\,|\,s_{t})\) and \(s_{t+1} P(\,|\,s_{t},a_{t})\) for all \(t 0\). In the same way, the Q function \(Q^{,P}\) associated with any policy \(\) under the transition kernel \(P\) is defined using expectation taken over the randomness of the trajectory under policy \(\) as

\[Q^{,P}(s,a)_{,P}[_{t=0}^{}^{t}r s_{t},a_{t}\,\,s_{0},a_{0}=s,a], \]

Distributionally robust MDPs.We consider distributionally robust MDPs (RMDPs) in the discounted infinite-horizon setting, denoted by \(_{}=\{,,,_{\| \|}^{}(P^{0}),r\}\), where \(,,,r\) are the same sets and parameters as in standard MDPs. The main difference compared to standard MDPs is that instead of assuming a fixed transition kernel \(P\), it allows the transition kernel to be arbitrarily chosen from a prescribed uncertainty set \(_{\|\|}^{}(P^{0})\) centered around a _nominal_ kernel \(P^{0}:()\), where the uncertainty set is specified using some called \(L_{p}\) smooth norm denoted \(\|\|\) defined in of radius \(>0\) defined in 1.

**Definition 1** (General smooth \(L_{p}\) norms and dual norms).: _A norm \(\|\|\) is said to be a general smooth \(L_{p}\), norm, \(p>1\) if_

* _for all_ \(x^{n}\)_,_ \(\|x\|\|x\|_{p,w}=(_{k=1}^{n}w_{k}(|x_{k}|) ^{p})^{1/p}\) _for some_ \(w^{n}_{+},\) _being an arbitrary positive vector._
* _it is twice continuously differentiable Rudin et al._ _[_1964_]_ _with the supremum of the Hessian Matrix over the simplex_ \(C_{S}=_{x_{S}}\|^{2}\|x\|\|_{2^{ }}\)_, where_ \(\|\|_{2}\) _here is the spectral norm.

Figure 1: **Left**: Sample complexity results for RMDPs with \(sa\)- and \(s\)-rectangularity with \(L_{p}\) with comparisons to prior arts  (for \(L_{1}\) norm, or called total variation distance) and  ; **Right:** The data and instance-dependent sample complexity upper bound of solving \(s\)-rectangular dependency RMDPs with \(L_{p}\) norms.

_Finally, we denote the dual norm of \(\) as \(_{*}\) s.t. \( y_{*}_{x}x^{T}y: x 1\). Moreover, for any metric \(\), we define \(C_{g}\)\(C_{g} 1/_{s} e_{s}\) where \(e_{s}^{S}\) is the standard basis vector with only \(1\) at the \(s\)-th entry, otherwise \(0\)._

Note that the quantity \(C_{S}\) exists, as the Hessian of a \(C^{2}\) functional is continuous and because the simplex is a compact set, so by Extreme Value Theorem Rudin et al. (1964), \(C_{S}\) is finite. For example, considering \(L_{p}\) norms with any \(p 2\), \(C_{S}\) is bounded by \((p-1)S^{1/q}\). (See (154) ) This definition is general and includes \(L_{p}\) norms (Rudin et al., 1964) for any \(p 2\) and all rescaled and weighted norms. Moreover, we could extend our results to a larger set than the one of the norms defined in Def. 1, where the further discussion can be found in Appendix B. However, it does not include divergences such as \(KL\) and \(^{2}\). Not that the case of \(TV\) which is not \(C^{2}\) smooth is treated independently with different arguments the proof but has the same sample complexity. In particular, given the nominal transition kernel \(P^{0}\) and some uncertainty level \(\), the uncertainty set--with arbitrary smooth \(L_{p}\) norm metric \(\ ^{S}^{+}\) in \(sa\) rectangular case or from \(^{S}\) in the \(s\)-rectangular case, is specified as \(^{}_{}(P^{0})_{s,a} ^{,}_{}(P^{0}_{s,a})\)

\[^{,}_{}(P^{0}_{ s,a})\{P_{s,a}(): P_{s,a}-P^{0}_{ s,a}\}, \] \[P_{s,a} P(\,s,a)^{1  S},P^{0}_{s,a} P^{0}(\,s,a)^{1 S}, \]

where we denote a vector of the transition kernel \(P\) or \(P^{0}\) at state-action pair \((s,a)\). In other words, the uncertainty is imposed in a decoupled manner for each state-action pair, obeying the so-called \(sa\)-rectangularity (Zhou et al., 2021; Wiesemann et al., 2013). More generally, we define \(s\)-rectangular MDPs as \(^{}_{}(P)=_{s}^{ ,}_{}(P_{s}),\) for the general smooth \(L_{p}\) norm \(\). The uncertainty is imposed in a decoupled manner for each state pair, and a fixed budget given a state for all action is defined. To get a similar meaning for the radius of the ball between \(sa\)-rectangular and \(s\)-rectangular assumptions, we need to rescale the radius depending on the norm like in Yang et al. (2022b). The \(s\)-uncertainty set is then defined using the rescaled radius \(\) as

\[^{,}_{} (P_{s})P^{}_{s}()^{}:  P^{}_{s}-P_{s}= 1_{ }}, \] \[P_{s} P(,\,s) ^{1 SA}, P^{0}_{s} P^{0}(, \,s)^{1 SA}. \]

where \(1_{A}^{A}\) denotes the unitary vector. For the specific case of respectively \(L_{1}\),\(L_{p}\) and \(L_{}\) norm, \(\) is equal to \(||,||^{1/p}\) and \(\). Note that this scaling allows for a fair comparison between \(sa\)- and \(s\)-rectangular MDPs. In RMDPs, we are interested in the worst-case performance of a policy \(\) over all the possible transition kernels in the uncertainty set. This is measured by the _robust value function_\(V^{,}\) and the _robust Q-function_\(Q^{,}\) in \(_{}\), defined respectively as \((s,a)\)

\[V^{,}(s)_{P^{,}_{ }(P^{0})}V^{,P}(s), Q^{,}(s,a)_{P ^{,}_{}(P^{0})}Q^{,P}( s,a). \]

Similarly for \(s\)-rectangularity, the value function is denoted \(V^{,}_{s}(s)_{P^{,}_{}(P^{0})}V^{,P}(s)\).

Optimal robust policy and robust Bellman operator.As a generalization of properties of standard MDPs in the \(sa\)-rectangular robust case, it is well-known that there exists at least one deterministic policy that maximizes the robust value function (resp. robust Q-function) simultaneously for all states (resp. state-action pairs) (Iyengar, 2005; Nilim and El Ghaoui, 2005) but not in the \(s\)-rectangular case. Therefore, we denote the _optimal robust value function_ (resp. _optimal robust Q-function_) as \(V^{*,}\) (resp. \(Q^{*,}\)), and the optimal robust policy as \(^{}\), which satisfy \((s,a)\)

\[V^{*,}(s) V^{^{},}(s)=_{}V^{ ,}(s), Q^{*,}(s,a) Q^{^{*},}(s,a)= _{}Q^{,}(s,a). \]

A key concept in RMDPs is a generalization of Bellman's optimality principle, encapsulated in the following _robust Bellman consistency equation_ (resp. _robust Bellman optimality equation_):

\[(s,a), Q^{, }(s,a)=r(s,a)+_{^{,}_{ }(P^{0}_{s,a})}V^{,}, \] \[(s,a), Q^{*,}(s,a )=r(s,a)+_{^{,}_{ }(P^{0}_{s,a})}V^{*,}. \]for the \(sa\)-rectangular case and same equation replacing \(P^{0}_{s,a}\) by \(P^{0}_{s}\) and \(\) by \(\). The robust Bellman operator (Iyengar, 2005; Nilim and El Ghaoui, 2005) is denoted by \(^{}():^{SA}^{SA}\)

\[^{}(Q^{})(s,a) r(s,a)+_{ ^{,}_{\|\|}(P^{0}_{s,a})}V,  V(s)_{}Q^{}(s,a)^{} \]

for \(sa\)-rectangular MDPs. Given that \(Q^{,}\) is the unique-fixed point of \(^{}\) one can recover the optimal robust value function and Q-function using a procedure termed _distributionally robust value iteration_ (\(DRVI\)). Generalizing the standard value iteration, \(DRVI\) starts from some given initialization and recursively applies the robust Bellman operator until convergence. As has been shown previously, this procedure converges rapidly due to the \(\)-contraction property of \(^{}\) with respect to the \(L_{}\) norm (Iyengar, 2005; Nilim and El Ghaoui, 2005).

## 3 Distributionally Robust Value Iteration

Generative model-based sampling.Following Zhou et al. (2021); Panaganti and Kalathil (2022), we assume access to a generative model or a simulator (Kearns and Singh, 1999), which allows us to collect \(N\) independent samples for each state-action pair generated based on the _nominal_ kernel \(P^{0}\): \((s,a)\), \(s_{i,s,a}}{{}}P^{0}(\,|\,s,a), i=1,2,,N.\) The total sample size is, therefore, \(NSA\). We consider a model-based approach tailored to RMDPs, which first constructs an empirical nominal transition kernel based on the collected samples and then applies distributionally robust value iteration (DRVI) to compute an optimal robust policy. As we decouple the statistical estimation error and the optimization error, we exhibit an algorithm that can achieve arbitrary small error \(_{opt}\) in the empirical MDP defined as an empirical nominal transition kernel \(^{0}^{SA S}\) that can be constructed on the basis of the empirical frequency of state transitions, i.e. \((s,a)\)

\[^{0}(s^{}\,|\,s,a)_{i=1}^ {N}s_{i,s,a}=s^{}}, \]

which leads to an empirical RMDP \(}_{}=\{,,,^{}_{\|\|}(^{0}),r\}\). Analogously, we can define the corresponding robust value function (resp. robust Q-function) of policy \(\) in \(}_{}\) as \(^{,}\) (resp. \(^{,}\)) (cf. (8)). In addition, we denote the corresponding _optimal robust policy_ as \(^{}\) and the _optimal robust value function_ (resp. _optimal robust Q-function_) as \(^{,}\) (resp. \(^{,}\)) (cf. (9)), which satisfies the robust Bellman optimality equation \((s,a)\):

\[^{,}(s,a)=r(s,a)+_{^ {,}_{\|\|}(^{0}_{,a})}^{ ,}. \]

Equipped with \(^{0}\), we can define the empirical robust Bellman operator \(}^{}\) as \((s,a)\)

\[}^{}(Q^{})(s,a) r(s,a)+ _{^{,}_{\|\|}(^{ 0}_{,a})}V, \]

with \(V(s)_{}Q^{}(s,a)\). The aim of this work is given the collected samples, to learn the robust optimal policy for the RMDP w.r.t. some prescribed uncertainty set \(^{}(P^{0})\) around the nominal kernel using as few samples as possible. Specifically, given some target accuracy level \(>0\), the goal is to seek an \(\)-optimal robust policy \(\) obeying

\[ s: V^{,}(s)-V^{,}(s), \] \[^{^{},}-^{ {},}_{}. \]

This formulation allows plugging any solver of RMDPs in this bound, for instance, the distributionally robust value iteration (DRVI) algorithm detailed in Appendix G.

## 4 Theoretical guarantees

In this section, we present our main results characterizing the sample complexity of solving RMDPs with \(sa\)-and \(s\)-rectangularity. Additionally, we discuss the implications of our results for the comparisons between standard and robust RL, and for comparisons between \(sa\)- versus \(s\)-rectangularity.

### \(sa\)-rectangular uncertainty set with general smooth norms

To begin, we consider the RMDPs with \(sa\)-rectangularity with general norms. We first provide the following sample complexity upper bound for certain oracle planning algorithms, whose proof is postponed to Appendix D.2. Technically, we derive two new dual forms for RMDPs problems using arbitrary norms in Lemmas 3 and 4 for respectively \(sa\)- and \(s\)-rectangular RMDPS. In these dual forms, a central quantity denoted \((.)_{*}\), representing the dispersion of the value function, appears and is the dual span semi-norm associated with the considered general \(L_{p}\) norm \(\|.\|\) defined in 1 in the initial primal problem. The main challenge in this analysis is to derive a tight upper bound on this quantity in Lemmas (5) and (6), leading to the following sample complexity.

**Theorem 1** (Upper bound for \(sa\)-rectangularity).: _Consider the uncertainty set \(U_{\|.\|}^{,}()\) associated with arbitrary \(L_{p}\) smooth norm \(\|\|\) defined in 1. We denote \(_{}_{p_{1},p_{2}()}\|p_{1}-p_{2}\|\) as the accessible maximal uncertainty level. Consider any \((0,1)\), discount factor \([,1)\), and uncertainty level \((0,_{}]\). Let \(\) be the output policy of some oracle planning algorithm with optimization error \(_{}\) introduced in (15). With introduced in 1, one has with probability at least \(1-\),_

\[ s: V^{,}(s)-V^{,}(s) +}}{1-} \]

_for any \((0,\}}]\), as long as the total number of samples obeys_

\[NSASA}{(1-)^{2}\{1-,C_{g}\} ^{2}}+SAC_{S}\|1_{S}\|_{*}}{(1-)^{2}} \]

_with \(c_{1},c_{2},c_{3}\) a universal positive constant. For a sufficiently small level of accuracy \((\{1-,C_{g}\})/(C_{S}\|1_{S}\|)\), the sample complexity is_

\[NSASA}{(1-)^{2}\{1-,C_{g}\} ^{2}}. \]

Note that this result is also true for \(TV\) without the geometric smooth term depending on \(C_{S}\). Considering \(L_{p}\) norms, \(C_{g} 1\) and \(C_{S} S^{1/q}(p-1)\). In Theorem 1, we introduce the following minimax-optimal lower bound to verify the tightness of the above upper bound; a proof is provided in Appendix E.

**Theorem 2** (Lower bound for \(sa\)-rectangularity).: _Consider the uncertainty set \(U_{\|.\|}^{,}()\) associated with arbitrary \(L_{P}\) norm \(\|\|\) defined in \(1\). We denote \(_{}_{p_{1},q_{1}()}\|p_{1}-p_{2}\|\) as the accessible maximal uncertainty level. Consider any tuple \((S,A,,,)\), where \([,1)\), \((0,_{}(1-c_{0})]\) with \(0<c_{0}\) being any small enough positive constant, and \((0,}{256(1-)}]\). We can construct two infinite-horizon RMDPs \(_{0},_{1}\) such that giving a dataset with \(N\) independent samples for each state-action pair over the nominal transition kernel (for either \(_{0}\) or \(_{1}\) respectively), one has_

\[_{}_{\{_{0},_{1} \}}\{_{}_{s}[V^ {,}(s)-V^{,}(s)]> \},\]

_where the infimum is taken over all estimators \(\), \(_{0}\) (resp. \(_{1}\)) are the probability when the RMDP is \(_{0}\) (resp. \(_{1}\)), as long as, for \(c_{7}\) is a universal positive constant,_

\[NSASA}{(1-)^{2}\{1-,C_{g}\}^{2 }}. \]

\(\) Near minimax-optimal sample complexity with general \(L_{p}\) norms.Recall that Theorem 1 shows that the sample complexity upper bound of oracle algorithms for RMDPs is in the order of \((\{1-,C_{g}\} ^{2}}).\) Combined with the lower bound in Theorem 2, we observe that the above sample complexity is near minimax-optimal, in almost the full range of uncertainty.

\(\) Solving RMDPs with general \(L_{p}\) norms can be easier than solving standard RL.Recall that the sample complexity of solving standard RL with a generative model [Agarwal et al., 2020, Liet al., 2024; Azar et al., 2013a) is: \((^{2}}).\) Comparing this with the sample complexity in (18), it highlights that solving robust MDPs (cf. (18)) using any norm as the divergence function for the uncertainty set is not harder than (and is sometimes easier than) solving standard RL (cf. (4.1)). Specifically, when the uncertainty level is small \( 1-\), the sample complexity of solving robust MDPs matches that of standard MDPs. While when the uncertainty level is relatively larger \(1-_{}\), the sample complexity of solving robust MDPs is smaller than that of standard MDPs by a factor or \(\), which goes to \(\) when \(=O(1)\).

\(\)Comparisons with prior arts.In Figure 1, we illustrate the comparisons with two state-of-the-arts (Clavier et al., 2023; Shi et al., 2023) which use some divergence functions belonging to the class of general norms considered in this work. In particular, Shi et al.  achieved the state-of-the-art minimax-optimal sample complexity \((\{1-,\} ^{2}})\) for specific \(L_{1}\) norm (or called total variation distance). In this work, we attain near minimax-optimal sample complexity for any general norm (including \(L_{1}\)) which matches the one in Shi et al.  when narrowing down to \(L_{1}\) norm. Note that in \(TV\) case, \(C_{g}=1\). This reveals that the finding of robust MDPs can be easier than standard MDPs (Shi et al., 2023) in terms of sample requirement does not only hold for \(L_{1}\) norm, but for any general norm. In addition, compared to Clavier et al.  which focuses on \(L_{p}\) norms for any \(1 p\): when \(1-_{}\), we improve the sample complexity \((^{2}})\) to \((^{2}})\) by at least a factor of \(\); otherwise, we match the results in Clavier et al. .

\(\)Burn-in Condition, \(C_{g}\) factor and \(TV\) case :In Th. 1 and 3 we need a sufficiently small level of accuracy \((\{1-,C_{g}\})/(C_{s}\|1_{S}\|)\), to obtain the sample complexity. This type of condition is usual in MDPS analysis Shi et al.  and is equivalent to burn in term. Moreover, the quantity \(C_{S}\) exists (see 1) and for example, considering \(L_{p}\) norms, \(C_{S}\) is bounded by \(S^{1/q}\). (See (154)) and the product \(C_{S}\|1_{S}\|\) is upper bounded by \(S\) for \(L_{2}\) norm. Moreover, note that our theorem for the smooth norm is also true for \(TV\) which is not \(C^{2}\) and has the same complexity as (Shi et al. ). In this case, the burn-in condition is not needed. (See Lemma D.3.3). Finally, the factor \(C_{g}=1/_{s}\|e_{s}\|\) is norm dependent and depends on how big the vector \(e_{s_{0}}\) is in the considered norm. Note for classical \(L_{p}\) this quantity is bigger than \(1\), which reduces the sample complexity.

### \(s\)-rectangular uncertainty set with general norms

To continue, we move on to the case when the uncertainty set is constructed under \(s\)-rectangularity smooth norm. The following theorem presents the sample complexity upper bound for learning an \(\)-optimal policy for RMDPs with \(s\)-rectangularity. A proof is shown in Appendix D.2.

**Theorem 3** (Upper bound for \(s\)-rectangularity).: _Consider the uncertainty set \(_{\|\|}^{s,}_{\|}()\) with \(s\)-rectangularity. Consider any discount factor \([,1)\), the rescaled uncertainty level \(=\|1_{A}\|\), and denote \(_{}\|1_{A}\|_{p_{1},p_{2} _{S}(})}\|p_{1}-p_{2}\|\) and \((0,1)\). Let \(\) be the output policy of an arbitrary optimization algorithm with error \(_{}\), with probability at least \(1-\), one has for any \((0,_{s}\|_{s}\|_{ }\}}]\), \( s: V^{,}(s)-V^{,}(s)+}} {1-}\) as long as the total number of samples obeys_

\[NSASA}{(1-)^{2}^{2}} {\{1-,C_{g}\}},_{s} \{\|_{s}^{*}\|_{}\|1_{A}\|,\| _{s}\|_{}\|1_{A}\|\}}}+SAC_{ S}\|1_{S}\|_{}}{(1-)^{2}} \]

_For a sufficiently small accuracy, \((\{1-,C_{g}\})/(C_{s}\|1_{S}\|)\) the sample complexity is_

\[NSASA}{(1-)^{2}^{2}} {\{1-,C_{g}\}},_{s} \{\|_{s}^{*}\|_{}\|1_{A}\|,\| _{s}\|_{}\|1_{A}\|\}}} \]

where \(_{s}_{A}\) denote the policy of the empirical RMPDs at state \(s\), \(_{s}^{*}_{A}\) the optimal policy given \(s\) of the true RMPDs, \(\|.\|_{}\) the dual norm and \(c_{4},c_{5},c_{6}\) are universal constant. Note that this result is also true for \(TV\) without the term depending on smoothness \(C_{S}\). In addition, we provide the lower bounds for a representative divergence function -- \(L_{}\) norm in the following. Note that for classical \(L_{p},C_{S}=S^{1/q}(p-1)\) and \(C_{g}\) can be lower bounded by \(1\). A proof is provided in Appendix F.

**Theorem 4** (Lower bound for \(s\)-rectangularity).: _Consider the uncertainty set \(_{L_{}}^{,}()\) associated with the \(L_{}\) norm. Consider any tuple \((S,A,,,)\) and \(0<c_{0}\) being any small enough positive constant, where \([,1)\), and \((0,}{256(1-)}]\). Correspondingly, we denote the accessible maximal uncertainty level for \(_{L_{}}^{,}()\) as \(_{}^{}_{p_{1},p_{1}()^{A}}\|p _{1}-p_{2}\|_{}=1\). Then we can construct a collection of infinite-horizon RMDPs \(_{L_{}}\) defined by the uncertainty set with \(_{L_{}}^{,}()\) so that for any \((0,_{}^{}(1-c_{0})]\), and any dataset with in total \(N_{}\) independent samples for all state-action pairs over the nominal transition kernel (for any RMDP inside \(_{L_{}}\)), one has_

\[_{}_{_{L_{}}}\{_{}_{s}[V^{,}(s)-V^{ ,}(s)]>\}, \]

_provided that for \(c_{8}\) is a universal positive constant,_

\[N_{}SA}{(1-)^{2}\{1-, \}^{2}}, \]

_with \(_{}\) the probability when the RMDP is \(\), and the infimum is taken over all estimators \(\)._

Now we can present some implications of Theorem 3 and Theorem 4.

\(\) Robust MDPs with \(s\)-rectangularity are at least as easy as \(sa\)-rectangularity.Theorem 3 shows that the sample complexity of solving RMDPs with \(s\)-rectangularity does not exceed the order of \((\{1-,C_{g}\} ^{2}}).\) This matches the sample complexity for \(sa\)-rectangularity (cf. (18)) and indicates that although \(s\)-rectangular RMDPs are of a more complicated formulation, solving \(s\)-rectangular RMDPs is at least as easy as solving \(sa\)-rectangular RMDPs in terms of the sample complexity. In addition to the worst-case sample complexity upper bound, Theorem 3 also provides a data and instance-dependent sample complexity upper bound for \(s\)-rectangular RMDPs (cf. in (20)).Taking the divergence function \(\|\|=L_{p}\) for instance, the data and instance-dependent sample complexity upper bound is

\[(^{2}} {1}{\{1-,\}})&_{s}(a\,|\,s)=_{s}^{ }(a\,|\,s)=,(s,a)\\ (^{2}}\}})&\|_{s}(\,|\,s)\|_{0}=\|_{s}^{ }(\,|\,s)\|_{0}=1, s.\]

where \(\|.\|_{0}\) corresponds to the total number of nonzero elements in a vector.The intuition beyond this theorem is that when the policy becomes proportional to uniform, the uncertainty budget of the \(s\)-rectangular MDPs is equally spread into all actions, and we retrieve the \(sa\)-rectangular case. When the policy becomes deterministic, all the uncertainty budget concentrates on one action. In this case, most of the actions are not robust except one, and the problem is simpler than classical MDP for this only specific action. An illustration of this result can be found in Fig. 2.

\(\) Comparisons with prior arts.In Figure 1, we illustrate the comparisons with Clavier et al. (2023) which use \(L_{p}\) norms functions belonging to the class of general norms considered in this work. We do not compare in this section to Yang et al. (2022) as it is not anymore state-of-the-art with regard to the work of Clavier et al. (2023). In particular, the latest achieves in the \(s\)-rectangular case at sample complexity of \((^{2}})\) in the regime where \( 1-\). In this regime, our result is the same but more general but in the regime where \( 1-\), they achieve sample complexity of \((^{2}})\) which is bigger than our result \((\{1-,\} ^{2}})\) by a factor at least \(\).

## 5 Conclusion

This work refined sample complexity bounds to learn robust Markov decision processes when the uncertainty set is characterized by an general \(L_{p}\) metric, assuming the presence of a generative model. Our findings not only strengthen the current knowledge by improving both the upper and lower bounds, but also highlight that learning \(s\)-rectangular MDPs is less challenging in terms of sample complexity compared to classical \(sa\)-rectangular MDPs. This work is the first to provide results with a minimax bound, as prior results concerning \(s\)-rectangular cases were not minimax optimal. Additionally, wehave established the minimax sample complexity for RMDPs using a general \(L_{p}\) norm, demonstrating that it is never larger than that required for learning standard MDPs. Our research identifies potential avenues for future work, such as exploring the characterization of tight sample complexity for RMDPs under a broader family of uncertainty sets, such as those defined by \(f\)-divergence. It would be highly desirable for a more unified theoretical foundation, as the distance between probability measures is more natural to define using divergence. Moreover, it would be interesting to focus on the finite-horizon Setting and linear setting, as our current analytical framework opens the door for potential extensions to address finite-horizon RMDPs. Such an extension would contribute to a more comprehensive understanding of tabular cases. Finally, the case of linear MDPs would be interesting to explore.

## 6 Acknowledgements

Fondation Mathematique Jacques Hadamard supported this work during Pierre Clavier's visiting the California Institute of Technology. Pierre Clavier has been supported by a grant from Region Ile-de-France; DIM Math Innov. The work of L. Shi is supported in part by the Resnick Institute and Computing, Data, and Society Postdoctoral Fellowship at California Institute of Technology. The work of E. Mazumdar is supported in part from NSF-2240110. The work of A. Wierman is supported in part from CNS-2146814, CPS-2136197, CNS-2106403, and NGSDI-2105648.