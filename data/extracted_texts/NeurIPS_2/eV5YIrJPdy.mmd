# The Expressive Capacity of State Space Models: A Formal Language Perspective

Yash Sarrof, Yana Veitsman, Michael Hahn

Saarland Informatics Campus

Saarland University, Germany

{ysarrof, yanav, mhahn}@lst.uni-saarland.de

###### Abstract

Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement length-generalizing solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically 1 on a recent SSM, Mamba.

## 1 Introduction

Transformers (Vaswani et al., 2017) power most large language models (LLMs) today, as they offer the advantage of parallelized training by avoiding recurrence, compared to the previously dominant recurrent achitectures (RNNs Elman, 1990; Hochreiter and Schmidhuber, 1997). However, building on a long history of continuous dynamical models (e.g. Kalman, 1960, 1963) and work on faster RNNs (Bradbury et al., 2016; Lei et al., 2018), a recent line of work has developed _state space models_ (SSMs) rivaling the performance of transformers (e.g. Gu et al., 2021; Gu and Dao, 2023; Sun et al., 2023; De et al., 2024; Yang et al., 2024; Qin et al., 2024a). These SSMs are recurrent models, formulated in terms of iterative state updates, while still allowing efficient parallelization.

The impressive empirical performance of such SSMs raises the question of whether they might have capabilities that the transformer architecture might lack in principle. Simultaneously, to understand whether SSMs may plausibly overtake the dominant role of transformers, it is an important question whether SSMs may lack abilities present in transformers. A better understanding of these questions may also point the way to future architectures that unite the strengths of both architectures.

One common approach to understanding the capabilities of computational architectures is through their expressive capacity in simulating automata and modeling language classes; indeed, a sizeable literature has studied transformers (e.g. Perez et al., 2019; Hahn, 2020; Bhattamisha et al., 2020; Yao et al., 2021; Liu et al., 2023; Ba et al., 2022; Strobl et al., 2024; Chiang et al., 2023; Sanford et al., 2024; Peng et al., 2024) and RNNs (e.g. Siegelman and Sontag, 1995; Horne and Hush, 1993; Indyk, 1995; Weiss et al., 2018; Hewitt et al., 2020) through this lens. As the difficultyof many computational problems is well-understood in terms of such language classes, results about expressive capacity directly yield results about the ability to model specific computational problems.

While a substantial number of results have been obtained for transformers and traditional RNNs, understanding remains largely open for SSMs. In an initial step, Merrill et al. (2024) showed that all problems computable by SSMs are contained in \(^{0}\), a circuit complexity class that is known to also cover transformers (Merrill and Sabharwal, 2023; Strobl, 2023). Under standard conjectures, this suggests that certain types of state tracking are hard for both models. Jelassi et al. (2024) and Bhattamisha et al. (2024) provided evidence of differences between these architectures, showing that transformers outperform SSMs on copying or retrieving from long strings-tasks well within \(^{0}\). Zubic et al. (2024) showed that multi-layer SSMs are constrained by their logarithmic space computational capacity, limiting their ability at algorithmic tasks such as multi-digit multiplication.

However, a more fine-grained understanding of the power of SSMs, and how they compare to RNNs and transformers, remains an open question. Our contribution in this paper is to provide rigorous understanding of SSMs' abilities in different classes of languages. We show that transformers and SSMs cover overlapping but distinct fragments of \(^{0}\). For instance, SSMs can model bounded hierarchical structure in ways similar to transformers and traditional RNNs, even without embedding a stack-like structure (Theorem 6). For regular languages involving modular counting, such as the PARITY function (Theorem 2), we identify a design choice that makes extant SSMs struggle in ways similar to transformers. In other cases, we show that SSMs resolve a failure case of transformers: they effortlessly model Flip Flop state tracking (Theorem 1). We discuss take-aways for SSM and LLM research in Section 5; among others, our results suggest future LM architectures might need to combine both attention and state spaces.

## 2 Background

### State Space Models

SSM LayersWe define a single layer of a state space model as a map, at input length \(T\),

\[^{T d}^{T d} (x_{t})_{t=1,,T}(z_{t})_{t=1,,T}\]

given by the recurrence

\[h_{t}= A(x_{t}) h_{t-1}+B(x_{t}) z_{t}= (h_{t},x_{t}) \]

where \(\) denotes elementwise product, and, for each \(x_{t}^{d}\),

\[h_{0} ^{d} B(x_{t})^{d}\] \[A(x_{t}) ^{d} :^{2d}^{d}\]

We allow \(A,B\) to be arbitrary smooth maps. The map \((h_{t},x_{t})\) includes a cascade of channel-mixing transformations and normalization, which we abstract as follows:

\[(h_{t},x_{t})=_{1}((_{2}(h_{t},x_{t})),x_{t}) \]

where \(_{j}()\) can contain linear or (Swi)GLU components (e.g. Qin et al., 2024; Gu and Dao, 2023). We will take Norm to implement RMSNorm Zhang and Sennrich (2019); LayerNorm Ba et al. (2016) can be covered by absorbing centering into \(_{2}\).

A Full SSMReal-world SSMs typically stack several layers of the form (1-2). Where needed, we use superscripts to indicate the layers in an SSM: \(h_{t}^{(1)},,h_{t}^{(L)}\), where \(L\) is the number of layers. We consider input words \(=w_{1|w|}\) over a discrete alphabet \(\), and assume an encoding in terms of token embeddings \(e()^{d}\), for \(\). We will also write \(e_{}\) for \(e()\). These feed into the lowest layer as \(x_{t}^{(l)}:=e(w_{t})\). The outputs of each layer feed into the next layer, as \(x_{t}^{(l+1)}=z_{t}^{(l)}\). The transformations in (1) are specific to each layer: \(A^{(1)},,A^{(L)}\) and similarly for \(B,\). To keep notation simple, we will only show the superscripts where necessary for disambiguation. The activations \(z_{t}^{(L)}\) at the highest layer are read out by some neural network \(\) into vectors \(q_{t}^{d_{pred}}\) describing classification or next-token predictions. We again take \(\) to be an arbitrary function; importantly, all our constructions will allow \(\) to operate correctly even at finite precision.

Implementation ChoicesIn Mamba, (1) directly maps onto Eqs. (2a) and (2b) in Gu and Dao (2023). The notation of Gu and Dao (2023) use a matrix multiplication \(h_{t-1}\) instead of elementwise multiplication \(A(x_{t}) h_{t-1}\) in (1), but importantly, Mamba's \(\) is diagonal, so we can take \(A(x_{t})_{i}=_{ii}\). Some SSMs assume nondiagonal \(A(x_{t})\), but typically this matrix is diagonalizable (e.g. Gu et al., 2021; Sun et al., 2023), so that the SSM is still equivalent to one of the form (1). We discuss how other SSMs instantiate (1) in Appendix A. Some models assume complex-valued activations (Appendix A); our results largely do not depend on this distinction, but take it into account where needed (Theorem 13). Some SSMs (e.g. Gu and Dao, 2023) use different numbers of channels in \(x_{t}\) and \(h_{t}\) using state expansion; as this does not affect expressive capacity, we will simply assume a constant dimensionality \(d\). Local convolutions (e.g. Fu et al., 2023) can be simulated with an SSM layer and do not increase expressive capacity (Remark 19).

We will find that two design choices have nontrivial impact on expressive capacity: The first one is time invariance: we call an SSM time-invariant if \(A(x_{t})\) does not depend on \(x_{t}\). Some SSMs, such as S4 (Gu et al., 2021) and Retnet (Sun et al., 2023) are time-invariant; Mamba (Gu and Dao, 2023), Griffin (De et al., 2024), GLA (Yang et al., 2024), HGRN (Qin et al., 2024b,a), QRNN/SRU Bradbury et al. (2016); Lei et al. (2018) are not (Appendix A). The second one is the sign of the entries of \(A(x_{t})\): Across all non-time-invariant SSMs surveyed, we find that the gate is always non-negative (Appendix A): \(A(x_{t}) 0\) (nonnegative) due to exponential or sigmoid parameterizations of the gate - this choice turns out to limit expressive capacity (Theorem 2).

Role of ParameterizationWhile the abstract form (1-2) is common across the SSM literature, differences in parameterization may have substantial effect on efficiency and training stability. In particular, the parameterization of \(A(x_{t})\) has been the subject of substantial research (e.g. Gu et al., 2020, 2021; Yu et al., 2023; Wang and Li, 2023). However, studying expressiveness allows us to abstract away from these differences to a remarkable degree: We will allow \(A,B,\) to be _arbitrary_ functions with the given input-output properties. Our negative results are based on abstract properties of the setup (1-2), which fundamentally bottlenecks SSMs through _elementwise linear_ state updates. For our positive results, will use empirical learnability experiments to verify that learnable solutions instantiating them (though not necessarily implementing the same constructions as used in the proofs) do exist in a recent SSM (Mamba, Gu and Dao, 2023).

We contrast SSMs with traditional RNNs such as simple RNNs or LSTMs: for these, the recurrence in Eq. (1) is replaced by \(h_{t}=(h_{t-1},x_{t})\) where \(\) could be linear, an MLP (Elman, 1990), or a more complex gated function (Hochreiter and Schmidhuber, 1997).

Finite Precision AssumptionWhile Eq.(1) assumes arbitrary real-valued activations, real-world implementations can only represent numbers with bounded precision. Formally, we adopt the _finite precision_ notion used by Weiss et al. (2018) in a study of the expressive power of traditional RNNs: We allow an unbounded number of integer bits, but only \(p\) fractional bits, independent of the length of the input. See Appendix E for discussion.

### Modeling Formal languages

We study three foundational types of data structures needed for modeling formal languages (Hopcroft et al., 2001): finite state automata (Theorem 1, 2, 4), counters (Theorem 5), and stacks (Theorem 6). These data structures can be understood in two equivalent forms: One is to track a state sequence over an input, where each input symbol engenders a specific transformation on the state. The other one, more commonly considered in research on expressive capacity, considers _formal languages_--sets of finite strings that are defined by the property that an automaton reaches one of a pre-specified set of "accepting" states after traversing the word. We focus on the latter, enabling easy comparison with existing results on transformers and RNNs.

A **finite-state-automaton** (see Definition 7) represents a general state tracking problem over a finite state space, without imposing further structure on the state space: The automaton keeps track of a single state from a finite state space; when reading a string from left to right, each symbol engenders a specific transformation of the state. At each position, the current state determines which symbols can come next; membership in a formal language is determined by the state reached after reading the full string. Finite-state-automata are equivalent in expressivity to regular expressions, and define the **regular languages**(Kleene, 1951).

Allowing an automaton to keep track of one or more **counters**(Fischer et al., 1968b)--integers that are incremented or decremented at each symbol read--turns the state space infinite, but in a highly structured manner. SSMs can model this datastructure (Theorem 5), as can RNNs and transformers (Weiss et al., 2018; Bhattamisha et al., 2020). **Stacks**, a first-in-first-out datastructure, enable automata to keep track of hierarchical structure, foundational to natural language (Chomsky, 1957). We show that SSMs can implement shortcut solutions to _bounded_ hierarchical structure even without implementing a stack (Theorem 6) - these are likely to be most useful to natural language given the boundedness of human memory (Miller, 1963; Karlsson, 2007).

### Formal Language Prediction and Recognition

We fix a finite alphabet \(\). Its elements are called _characters_ or _symbols_. The set of all finite strings \(\) over \(\) is denoted \(^{*}\); such strings are often referred to as _words_. The length of \(\) is denoted \(||\). A _formal language_\(L\) is a subset of \(^{*}\). Techically, we assume that the alphabet includes BOS and EOS symbols, which occurs at the beginning and end of each element of \(L\) and nowhere else.

We next need to define what it means for an SSM to model a formal language. The notion of _recognition_, where the task is to classify a full string as belonging to the language or not. Formally, for an SSM with \(d_{}=1\), we say that it **recognizes** a language \(L\) if the output \((z_{||}^{(L)})\) equals--when the SSM is run on \(^{*}\)--1 if \( L\) and \(0\) else.

However, such a classification task is arguably not always matched to dominant use cases in predictive sequence modeling, where the task is to predict the next token at each step. Thus, we also cast formal languages into a language modeling and sequence prediction framework. We adopt the task of Bhattamisha et al. (2020), where the model is asked to output at each step in a sequence the set of possible next symbols. Let \((L):=\{w:w^{*},w^{*} L\}\) the set of valid prefixes of \(L\). We then say that a model **predictively models** a language \(L\) if (Figure 1), given a valid prefix \(w(L)\), it outputs the finite set

\[\{:w^{*} L\} \]

We think of each such set as an atomic label; the set of possible labels is the power set of the finite alphabet \(\) (here, \(d_{}=2^{||}\)). Importantly, in both recognition and predictive modeling, we test the SSMs' ability across arbitrary input lengths, i.e. the choice of input length does not affect the inherent capability to recognize or predictively model the language. Predictive modeling can be easily converted into recognition by checking whether any symbol in the sequence is not in the predictive set at the preceding position; this can be done by adding 1 SSM layer. Conversely, if we can show that SSMs cannot recognize a language, this proves they also cannot perform predictive modeling for it, as they then cannot correctly predict where EOS can appear. To get the strongest results, we thus prove positive results for _predictive modeling_, and negative results for _recognition_.

Figure 1: Three key formal languages: prefixes with the sets of possible next characters: Flip Flop (Theorem 1), PARITY (Theorem 2), bounded-depth Dyck (Theorem 6). In Flip Flop, after a \(x\) (read) instruction, the bit must match what came after the last \(w\) (write) instruction (here, \(0\)). For PARITY, EOS can only follow when the number of ones in the prefix is even. For bounded-depth Dyck, a closing bracket can only appear if it matches the last unclosed opening bracket (here, “)” matches “(”)). Opening brackets can appear as long as the maximum depth (here, \(5\)) hasn’t been reached.

## 3 Theoretical Results

### Length-Generalizing Representations for Flip-Flop State Tracking

Flip Flop languages  are a simple instance of state tracking defined in terms of _write_, _read_, and _ignore_ instructions. Each _write_ instruction comes with a piece of information; whenever a _read_ instruction is encountered, the information written by the last _write_ instruction is recalled. Formally, \(_{FF}\) is the set of finite strings \(\) over \(=\{,,,0,1\}\), where \(x_{1},x_{3},\{,,\}\), \(x_{2},x_{4},\{0,1\}\), and where the bit following any \(\) matches the bit following the last preceding occurrence of \(\). Liu2023b show that the Flip Flop language, as an abstraction, is a fundamental ingredient of many long-range reasoning settings. It can be represented with a small finite-state-automaton, and LSTMs learn \(_{FF}\) well . Transformers can in principle represent it , though known constructions are not inherently length-generalizing, a fact confirmed empirically; intuitively, this may happen because attention heads aggregate information in a commutative manner, and reliably attending to the last _write_ instruction requires strong position dependence in the attention weights. SSMs, similar to traditional RNNs can easily represent Flip Flop at arbitrary input lengths and thus **avoid a failure mode of self attention**:

**Theorem 1**.: _There is a two-layer SSM that predictively models \(_{FF}\) at all lengths, at finite precision._

In the construction (Figure 3), the first layer records the last instruction token, achieved in (1) by setting \(A(e())=A(e())=A(e())=0\), and \(A(e(0)=A(e(1))=1\), and setting \(B(e(0))=B(e(1))=0\). Additional dimensions forward the current token to \(h_{t}^{(1)}\). In the output of the first layer \(z_{t}^{(1)}\), whenever the input is 0 or 1, the model now has access both to the current token \(w_{t}\) and the preceding token \(w_{t-1}\), which must have been an instruction. Based on this information, the model can set the gate to overwrite the state \(h_{t-1}^{(2)}\) with the current input token when the preceding token was \(\), and pass along the state \(h_{t-1}^{(2)}\) unaltered otherwise. This, together with \(z_{t}^{(1)}\), is sufficient for always identifying the legal next symbols in \(_{FF}\). The formal proof is in Appendix B.1.

### Difficulty of PARITY

PARITY, the language of bitstrings with an even number of ones, is recognized by a finite-state automaton with 2 states, and is straightforwardly encoded into a traditional RNN, even a linear one, with finite precision. It is in principle expressible for transformers , but is empirically hard for transformers to learn , as it can provably only be represented in sharp minima . A sufficiently general SSM could easily recognize it at \(d=1\) by setting \(h_{0}=1\), \(A(e_{1})=-1\), \(A(e_{0})=0\), \(B 0\), so that the sign of the single entry of \(h_{t}\) indicates the parity (Figure 2). Such an SSM would need to be non-time-invariant and require negative or complex gate values; i.e., satisfy neither time-invariant

Figure 2: (a) Visualizing the SSM equations 1, 2: The hidden state \(H\) is updated by a combination of its previous values, transformed by matrix \(A\), and the input \(X\), modulated by matrix \(B\). The updated hidden state and input are then processed through a \(Mix(.)\) layer, which can incorporate components like (Swi)GLU or Linear layers, with an optional RMSNorm for normalization. (b) An intuitive construction for recognizing PARITY with SSMs is achieved by setting \(B=0\) and \(A=-1\) when the input is 1, and \(A=1\) otherwise. However, this construction violates both nonnegative and time-invariant properties. We show that one of these properties is provably required to recognize PARITY at arbitrary lengths using an SSM (Theorem 2). (c) Modeling \(a^{n}b^{n}\): the matrix \(A\) adds the previous hidden state to the update, and depending on whether the input symbol requires counting up or down, matrix \(B\) is set to 1 or \(-1\), thus making the SSM simulate a counter (Theorem 5)nor nonnegative. Thus, these design choices necessitated by optimization, limit the power of an SSM in emulating finite-state-automata, establishing an **even stronger separation** between existing SSM variants and traditional RNNs than the circuit complexity arguments in Merrill et al. (2024)

**Theorem 2**.: _No SSM satisfying nonnegative can recognize PARITY at arbitrary input lengths with finite precision. In particular, this applies to Mamba._

The proof is in Appendix B.2; it examines inputs of the form \(1^{N}\) and shows that the activations \(z_{N}\) converge as \(N\), and thus cannot reliably encode the parity of \(N\). It should be noted that we require the layer-wise operations used in the SSM to be either linear or based on the GLU or SwiGLU activation functions, as seen for instance in Mamba (Remark 15). As we show in Theorem 13, the same result holds even for SSMs evading nonnegative when they are time-invariant, at least when the coefficients have rational angles in the complex planes. All extant SSMs we surveyed (Appendix, Section A) satisfy either Nonnegative or time-invariant. Hypothetical SSMs evading both nonnegative and time-invariant would be strictly stronger and can represent not only PARITY, but _all_ regular languages known to be in \(^{0}\) (Theorem 22).

### Exact characterization of Regular Languages modeled by SSMs

We combine Theorems 1 and 2 to derive an exact characterizations of the regular languages that modern non-time-invariant SSMs such as Mamba can recognize or predictively model - the two notions coincide here - in the finite-precision setting. The key insight is that \(_{FF}\) and PARITY are fundamental building blocks of two classes of regular languages: _star-free languages_ and their complement, _non-star-free languages_(Schutzenberger, 1965; McNaughton and Papert, 1971):

**Definition 3**.: _A regular language is star-free if it can be defined using regular expressions involving only the empty set, the empty string, individual symbols, concatenation, and Boolean combinations - avoiding the Kleene star operation._

\(_{FF}\) is star-free: there is a way to define it without Kleene star. PARITY is not star-free; any regular expression for it must involve the Kleene star. Some languages that are intuitively defined with Kleene stars may still be star-free.2 A language is star-free if and only if it can be defined logically using only first-order quantifiers and the order relation (Schutzenberger, 1965). Also, \(\) is non-star-free if and only if recognizing it involves counting modulo some finite integer \(K\)(McNaughton and Papert, 1971); Modern non-time-invariant SSMs such as Mamba cannot perform modulo counting, but they can model _all_ star-free languages:

**Theorem 4**.: _Let \(\) be a regular language. The following are equivalent:_

Figure 3: (a) Construction for Flip-Flop (Theorem 1): The first layer stores instruction bits to the hidden state, while data bits are forwarded to the output. Hence, the output always contains both the latest instruction and the associated data bit. In the second layer, if the instruction bit is \(\), the corresponding data bit is written to the hidden state, else the old value persists. This allows the model to consistently output the correct data bit. (b) Construction for Dyck(K, h) (Theorem 6): The first layer tracks the depth by counting up for each opening bracket, and down for each closing bracket. The second layer builds on the Flip-Flop construction to find the last opening bracket at the current depth; the next symbol can be either the matching closing bracket or – if the maximum depth has not been reached – an arbitrary opening bracket.

1. _There is an SSM satisfying_ nonnegative _that predictively models_ \(\) _at all input lengths, at finite precision_
2. \(\) _is star-free._

The proof in Appendix B.3 uses the Krohn-Rhodes theorem (Krohn and Rhodes, 1965) to reduce all star-free languages to flip flop-like state tracking. Importantly, there are well-known constructive criteria for deciding whether a given automaton defines a star-free language (Schutzenberger, 1965); hence, we have a _decidable criterion_ for the finite-state tracking problems that such SSMs satisfying Nonnegative can solve.

This is much simpler than the situation for transformers, where an exact characterization of their power within the regular languages is complicated: Angluin et al. (2023) show that a certain formal abstraction of transformers (masked unique hard attention) also recognizes exactly the star-free languages, but constructions of realistic transformers via Krohn-Rhodes in Liu et al. (2023) do not inherently length generalize. Both theoretical (Huang et al., 2024) and empirical research indicate difficulties in generalizing even for some simple star-free languages (Bhattamishra et al., 2020; Liu et al., 2023). Known length-generalizing constructions are limited to very simple subclasses such as the piecewise testable languages (Yang and Chiang, 2024). In contrast, for SSMs we have a single model per language, at finite precision and for arbitrarily long inputs. Thus, we expect that the SSM architecture confers an advantage in star-free state tracking problems when compared to transformers - a prediction we will find supported experimentally (Figure 5).

### SSMs can perform unbounded counting

Having characterized the regular languages modeled by SSMs, we now consider languages requiring unbounded counting (Fischer et al., 1968), specifically, languages recognized by keeping track of one or more counters, where each character causes a specific increment or decrement to each counter (Krebs et al., 2015; Hahn et al., 2015; Weiss et al., 2018; Kutrib et al., 2021). A prime example is the Dyck-1 language of well-formed strings over "(" and ")"; here a counter is incremented (decremented) whenever an opening (closing) bracket is encountered; a string is well-formed if and only if the counter is 0 at the end of the string. Some other relevant formal languages are Shuffle-Dyck-\(k\) (the shuffles of multiple Dyck-1 languages), \(a^{n}b^{n}\) - here, \(a\) increments the counter and \(b\) decrements it, and \(a^{n}b^{n}c^{n}\) - here, there are two counters, one keeping track of \(a^{n}b^{n}\) and one of \(b^{n}c^{n}\) (See Appendix C.2). Such counter languages are fundamental as basic context-free (Dyck-1, \(a^{n}b^{n}\)) or context-sensitive (e.g., \(a^{n}b^{n}c^{n}\)) languages (Hopcroft et al., 2001), and have been the subject of studies of both transformers (Bhattamishra et al., 2020) and RNNs (Weiss et al., 2018).

**Theorem 5**.: _The languages Dyck-1, Shuffle-Dyck-\(k\), \(n\)-ary Boolean Expressions, \(a^{n}b^{n}\), \(a^{n}b^{n}c^{n}\), and \(a^{n}b^{n}c^{n}d^{n}\), (defined in Appendix C.2) can each be predictively modeled by an SSM._

The proof is in Appendix B.4. Intuitively (Figure 2), an SSM can directly implement the required counters by setting \(A 1\) and by defining \((e_{})\) to be the increment or decrement cased by \(\). In modeling such languages, SSMs pattern with both transformers (Bhattamishra et al., 2020) and LSTMs (Weiss et al., 2018).

It may seem counterintuitive that nonnegative SSMs can perform unbounded counting but (by Theorem 2) not modular counting--the latter would seem to just require reading out the value of an unbounded counter. What is key is that, even though \(h_{t}\) can encode unbounded counts, reading out the modular value of an unbounded integer is a formidable problem for typical neural network nonlinearities, in particular when the information has been pushed through normalization (2).

We should note that there is a qualitative difference between this result and the preceding positive results about finite-state languages (Theorems 1 and 4), in that the construction in Theorem 5 uses unboundedly large entries in the state \(h_{t}\), whereas Theorems 1 and 4 use bounded values at finite precision. Indeed, we will find better length generalization in the finite-state case (Figure 5).

A consequence of Theorem 5 is that SSMs can recognize some languages transcending the context-free languages, as \(a^{n}b^{n}c^{n}\) is not context-free. A second application of the theorem, of great linguistic interest, is to bounded hierarchical structure, as we discuss next.

### Bounded Hierarchical Structure without Stacks

It is generally agreed that hierarchical structure is a key aspect of language, and comprehending language at a human-like level requires the computational ability to process such structures [Chomsky and Schutzenberger, 1963, Linzen et al., 2016, Everaert et al., 2015]. The fundamental data structure for the same is a stack, where information is stored and removed as one traverses to higher and lower levels of hierarchical embedding [Hopcroft et al., 2001]. We now show that SSMs' counting ability can offer shortcuts on languages modeling hierarchical structure, eschewing the need for a stack.

A useful abstraction of hierarchical structure as relevant to natural language is the family of Dyck languages. The bounded-depth Dyck language \(_{K,h}\) with \(K\) types of parentheses and depth \(h\) is the language of well-bracketed strings over \((_{1},)_{1},,(_{K},)_{K}\), such that the number of yet unclosed brackets never exceeds \(h\) in any prefix [Hewitt et al., 2020, Yao et al., 2021b]. The Chomsky-Schutzenberger theorem [Chomsky and Schutzenberger, 1963] asserts that any context-free language can be expressed as a homomorphic image of the intersection between a Dyck language and a regular language. Specifically, the Dyck language in question refers to the classical unbounded-depth Dyck language, where \(h\), underscoring its fundamental role as the structural backbone of context-free languages. Bounding the depth reflects the fact that deep embedding is rare in natural language [Karlsson, 2007, Blasi et al., 2019]. Prior work has found that two-layer transformers [Yao et al., 2021a] and traditional RNNs [Hewitt et al., 2020, Bhattamishra et al., 2020] both model all \(_{K,h}\) languages. The same turns out to hold for SSMs:

**Theorem 6**.: _There is a two-layer SSM with \(d=O(h K)\) that predictively models \(_{K,h}\) at all input lengths, at finite precision._

The proof is in Appendix B.5. Intuitively (Figure 3), the first layer records the depth of each parenthesis using the ideas from Theorem 5, and the second layer keeps track of the last open bracket at each depth using Theorem 1. We note that, since \(_{K,h}\) is star-free, Theorem 4 already guarantees the existence of representing SSMs, but the depth and width guaranteed by Theorem 6 is likely to be much better than what would be obtained by a black-box application of Theorem 4: As Hewitt et al.  show, \(h K\) units is optimal up to constants and is attained by traditional RNNs and LSTMs. The SSM construction is very different from that of Hewitt et al.  for traditional RNNs (both simple RNNs and LSTMs), which directly simulates a stack. Our construction is similar to the transformer construction in Theorem 4.2 in Yao et al. [2021a], which however has to rely on specific positional encodings, unlike the SSM construction. This highlights that stacks are not the only way of simulating bounded hierarchical structure in recurrent architectures, and non-stack-based strategies can even attain the same optimal scaling of hidden units. Probing whether such stack-free shortcuts are learned by SSM-based LLMs is an exciting problem for future research.

## 4 Experiments

We have derived a fine-grained theoretical characterization of expressiveness strengths and limitations of SSMs. We now show that our positive results can be instantiated and learned in a realistic SSM implementation, by evaluating a recent highly successful SSM, Mamba [Gu and Dao, 2023].

FlipFlopWe empirically instantiate Theorem 1 using the dataset of Liu et al. [2023a], reflecting the language \(_{FF}\) as defined in Section 3.1. Matching Figure 2 in Liu et al. [2023a], we evaluated both on in-distribution data, and on out-of-distribution data where the distance between read and write instructions tended to be larger. We evaluate for predicting the bits following \(r\) instructions3,

Figure 4: As predicted by Theorem 6, Mamba with 2 layers can model Dyck(K, h). Results for test set with strings of length \(700 n 1400\).

matching the "deterministic/clean" mode of Liu et al. (2023), and considered predictions to be correct only if all predictions within a sequence were correct. (Further details in Appendix D.2). A small one-layer4 Mamba model converged to 0 error in both validation sets after \(\) 1400 steps (Figure 6), compared to 500 steps for an LSTM reported by Liu et al. (2023). In contrast, Liu et al. (2023) found that transformers kept making occasional mistakes despite training for 10K steps.

Test Suite from Bhattamishra et al. (2020)To test our theoretical results on regular and counter languages (Theorems 2, 4, 5), we test Mamba on 27 formal languages, including 18 regular languages and 9 counter languages, based on a prior study comparing transformers and RNNs (Bhattamishra et al., 2020). The regular languages include a popular benchmark (Tomita, 1982) and various regular expressions; 11 are star-free. The counter languages include the languages covered by Theorem 5. (Definitions in Appendix C). We chose this test suite as it precisely covers Theorems 4 and 5, and we have proven (in)expressibility results for each language in the set.

Following Bhattamishra et al. (2020), we trained the model for predictive modeling, i.e., at each step, the model outputs a label indicating the set of possible next characters (3), including EOS when required. Following Bhattamishra et al. (2020), we count the model's response on an input string as correct if and only if predictive modelling was successful at _all_ positions in the input. Such a evaluation setup makes random baselines low, where a random predictor would have an accuracy exponentially small in \(N\) in each of the \(N\) positions. Training inputs have length in ; the model is evaluated on held-out bins with length  and . Further experimental details are in Appendix D.1.

We show our Mamba results, together with Transformer results reported by Bhattamishra et al. (2020), in Figure 5. LSTMs perform perfectly on all languages, and are thus not shown. In a striking confirmation of Theorem 4, Mamba learns all star-free languages with strong length generalization but performs poorly on non-star-free languages. Transformers show more mixed results, often failing to length-generalize even on star-free languages. Consistent with Theorem 5, Mamba, like Transformers, learns counter languages but struggles more with length generalization. The differences in Mamba's performance between star-free and counter languages may stem from the fact

Figure 5: Results on 27 formal languages, comparing our Mamba results (blue) with transformer results reported by Bhattamishra et al. (2020) (orange), on in-distribution lengths (solid) and out-of-distribution lengths (dotted). As predicted by Theorem 4, Mamba performs strongly on star-free languages, and even shows perfect length generalization. Again as predicted by Theorem 4, it performs poorly on non-star-free languages. Results for transformers from Bhattamishra et al. (2020) are mixed. Mamba also succeeds on learning the counter languages from Theorem 5, showing perfect accuracy at in-distribution lengths at in-distribution lengths, but length generalization lags behind transformers.

Figure 6: Test error on the validation set for \(_{FF}\), following Liu et al. (2023). Mamba shows near-zero test error in both In- (green) and Out-of-distribution (orange) settings, consistent with Theorem 1, and avoids the failure seen in transformers (Liu et al., 2023)

that the construction for the former class (Theorem 4) is able to use finite precision and bounded state values at arbitrary input lengths, while the latter (Theorem 5) uses unbounded state values.

Bounded Hierarchical StructureTo test Theorem 6, we recreate the experimental setup from Yao et al. (2021). Matching their Figure 4, we trained Mamba to predictively model \(Dyck_{K,h}\) at \(K=8\) and \(h=10\). The training and the validation set contained samples of length \( 700\), while the test set contained samples of length \(700 n 1400\). Yao et al. (2021) found both transformers and LSTMs achieved strong performance on this setup. We provide further details in Appendix D.3. Recall that Theorem 6 shows that two-layer SSMs can predictively model \(Dyck_{K,h}\). We trained Mamba with 1 or 2 layers and varying dimensionality, finding that two layers can achieve essentially perfect performance across model sizes, even on the test set (Figure 4 and 7).

## 5 Discussion

Related WorkOur work belongs to an incipient line of research into the expressiveness of SSMs (Jelassi et al., 2024; Merrill et al., 2024). It is closely related to a long string of work studying the expressive capacity of neural sequence models, which has so far focused on recurrent networks (e.g. Siegelman and Sontag, 1995; Bhattamishra et al., 2020; Hewitt et al., 2020) and, more recently, self attention (e.g. Chiang et al., 2023; Merrill and Sabharwal, 2023; Strobl et al., 2024). A second link is to the classical and long-standing study of linear dynamical systems and control theory (Kalman, 1960). For instance, Theorem 2 relies the asymptotic convergence of an SSM on certain inputs, establishing a link to the asymptotics of linear systems (e.g. Phillips and Solo, 1992).

Take-AwaysWhile theoretical in nature, our results have several actionable implications for SSM and LLM research, informing the rapidly growing research on SSM-based LLMs. _First_, encouragingly, SSMs can keep track of bounded hierarchical structure with optimal memory even without explicitly implementing a stack (Theorem 6), suggesting that simple diagonal linear state updates may be sufficiently powerful for modeling the hierarchical structure of language. _Second_, SSMs resolve a basic failure mode of self-attention in flip-flop state tracking while being parallelizable (Theorem 1). Overall, SSMs and attention have overlapping but distinct strengths. This lends support to the development of hybrid architectures interleaving SSM and attention layers, as instantiated very recently by Jamba (Lieber et al., 2024). _Third_, nonnegative gates as obtained by exponential or sigmoid parameterizations provably restrict expressive capacity, even in non-time-invariant SSMs (Theorem 2). While Gu and Dao (2023) found no evidence that complex-valued parameterizations improved over real-valued ones in the language modality, our results suggest revisiting this question, at least for tasks where periodic state-tracking abilities may be important. _Fourth_, while exactly characterizing the capacity of transformers has proven difficult even in the finite-state case, Theorem 4 provides a decidable characterization of the regular languages - equivalently, finite-state tracking problems - that SSMs such as Mamba can model. Such decidable characterizations may make it easier to theoretically predict abilities and anticipate failures of LLMs; exploring the implications of this characterization in more realistic setups is an exciting direction for future research.

LimitationsThe main limitation of our theoretical results is that they focus on in-principle expressiveness, and do not directly make statements about learning and generalization. Future work could address this, for example, by examining whether our constructions result in reasonably flat minima, or by studying gradient flow dynamics. While we empirically verified that our positive results can indeed be instantiated, in a learnable manner, in one realistic SSM implementation, implementational differences might still result in practical differences between implementations. Studying the role of such implementational differences is an interesting problem for future work; we have made a first step by theoretically elucidating the implications of nonnegative gate values.

## 6 Conclusion

We have studied the expressive capacity of modern state space models (SSMs), through the lens of automata and formal languages. We have shown theoretically that SSMs can express star-free languages, a range of counter languages, and bounded hierarchical structure. By providing rigorous results about the expressiveness of the SSM architecture, our results can provide guidance to work on SSM-based language models.