# Reduced Policy Optimization for Continuous Control with Hard Constraints

Shutong Ding\({}^{1}\)   Jingya Wang\({}^{1}\)   Yali Du\({}^{2}\)   Ye Shi\({}^{1}\)

\({}^{1}\)ShanghaiTech University  \({}^{2}\)King's College London

{dingsht, wangjingya, shive}@shanghaitech.edu.cn

yali.du@kcl.ac.uk

Corresponding author.

###### Abstract

Recent advances in constrained reinforcement learning (RL) have endowed reinforcement learning with certain safety guarantees. However, deploying existing constrained RL algorithms in continuous control tasks with general hard constraints remains challenging, particularly in those situations with non-convex hard constraints. Inspired by the generalized reduced gradient (GRG) algorithm, a classical constrained optimization technique, we propose a reduced policy optimization (RPO) algorithm that combines RL with GRG to address general hard constraints. RPO partitions actions into basic actions and nonbasic actions following the GRG method and output the basic actions via a policy network. Subsequently, RPO calculates the nonbasic actions by solving equations based on equality constraints using the obtained basic actions. The policy network is then updated by implicitly differentiating nonbasic actions with respect to basic actions. Additionally, we introduce an action projection procedure based on the reduced gradient and apply a modified Lagrangian relaxation technique to ensure inequality constraints are satisfied. To the best of our knowledge, RPO is the first attempt that introduces GRG to RL as a way of efficiently handling both equality and inequality hard constraints. It is worth noting that there is currently a lack of RL environments with complex hard constraints, which motivates us to develop three new benchmarks: two robotics manipulation tasks and a smart grid operation control task. With these benchmarks, RPO achieves better performance than previous constrained RL algorithms in terms of both cumulative reward and constraint violation. We believe RPO, along with the new benchmarks, will open up new opportunities for applying RL to real-world problems with complex constraints.

## 1 Introduction

The past few years have witnessed the significant success of reinforcement learning (RL)  in various fields such as mastering GO , robotic manipulations , autonomous driving , and smart grid controlling , etc. However, it is still challenging to deploy RL algorithms in real-world control tasks, such as operating robots on a specific surface, controlling power generation to fulfill the demand, etc. The principal reason here is that hard constraints must be taken into account in these control problems. Concretely, such constraints come in the form of both equality and inequality constraints and can be nonlinear or even nonconvex, which makes it difficult to handle them in RL. Moreover, unlike soft constraints, hard constraints take explicit form and require strict compliance, which poses additional challenges.

Existing work on constrained RL can be divided into two categories. The first category involves treating constraints as implicit or soft constraints and using Safe RL algorithms [4; 13; 14; 49; 25; 51; 26; 48]. These algorithms approximate the cumulative costs associated with the constraints and optimize the policy network to balance the trade-off between the cumulative rewards and costs. While Safe RL algorithms have provided certain guarantees on soft constraints, they cannot handle equality constraints since the constraints may not be satisfied due to approximation errors. Moreover, handling multiple constraints using Safe RL algorithms can be computationally expensive, and the existing benchmarks generally only involve simple soft constraints that do not reflect the complexity of real-world applications.

In the second category, the approaches treat the output of the policy network as a set of sub-optimal actions, correcting them to satisfy the constraints by adding an extra constrained optimization procedure. This technique has been explored in several works, such as [16; 29; 10; 33; 32; 24]. Compared to Safe RL algorithms, these algorithms can guarantee the satisfaction of hard constraints but are mostly designed for specific applications and hard constraints of a particular form. For instance, OptLayer  employs OptNet  into RL to ensure the satisfaction of linear constraints in robotic manipulation. As a result, these approaches to RL with hard constraints are limited in their ability to generalize and lack a generalized formulation.

To address the limitations of existing RL algorithms in handling hard constraints, we propose a constrained off-policy reinforcement learning algorithm called Reduced Policy Optimization (RPO). Our approach is inspired by Generalized Reduced Gradient (GRG), a classical optimization method. RPO partitions actions into basic actions and nonbasic actions following the GRG method and uses a policy network to output the basic actions. The nonbasic actions are then calculated by solving equations based on equality constraints using the obtained basic actions. Lastly, the policy network is updated by the reduced gradient with respect to basic actions, ensuring the satisfaction of the equality constraints. Moreover, we also incorporate a modified Lagrangian relaxation method with an exact penalty term into the loss function of the policy network to improve the initial actions. Our approach provides more confidence when deploying off-policy RL algorithms in real-world applications, as it ensures that the algorithms behave in a feasible and predictable manner. It is worth noting that there is currently a lack of RL environments with complex hard constraints. This motivates us to develop three new benchmarks to validate the performance of our proposed method. To summarize, our main contributions are as follows:

1) **Reduced Policy Optimization.** We present RPO, an innovative approach that introduces the GRG algorithm into off-policy RL algorithms. RPO treats the output of the policy network as a good initial solution and enforces the satisfaction of equality and inequality constraints via solving corresponding equations and applying reduced gradient projections respectively. To the best of our knowledge, this is the first attempt to fuse RL algorithms with the GRG method, providing a novel and effective solution to address the limitations of existing RL algorithms in handling hard constraints.

2) **RL Benchmarks with Hard Constraints.** We develop three benchmarks with hard constraints to validate the performance of our method, involving Safe CartPole, Spring Pendulum and Optimal Power Flow (OPF) with Battery Energy Storage. Comprehensive experiments on these benchmarks demonstrate the superiority of RPO in terms of both cumulative reward and constraint violation. We believe that these benchmarks will be valuable resources for the research community to evaluate and compare the performance of RL algorithm in environments with complex hard constraints.

## 2 Related Works

In this section, we first review the existing methods in the field of constrained RL and divide them into different types according to different constraints. We summarize the differences between our method and previous works in Table 1. Besides, we also introduce the literature on the GRG method as the motivation and background of our method.

**Soft-Constrained RL.** RL with soft constraints is well-studied and also known as safe reinforcement learning (Safe RL). One of the principal branches in Safe RL methods is based on the Lagrangian relaxation such as [13; 39; 22], where the primal-dual update is used to enforce the satisfaction of constraints. Besides, different penalty terms [25; 35; 42] are designed to maintain the tradeoff between the optimality in reward and safety guarantees. Moreover, Notable classical methods of safe reinforcement learning include CPO  based on the local policy search; Lyapunov-basedapproaches ; PCPO , FOCOPS , CUP  based on two-step optimization, EHCSB  based on bi-level optimization formulation, and SMPS  based on recovery and shielding. Besides, RL with soft instantaneous constraints was first studied separately in . This approach adds a safety layer to the policy with a pre-trained constraint-violation classifier but can only handle linear constraints. Other approaches include  based on the Recovery Policy and [44; 45] based on the Gaussian Process. Very recently, Unrolling Safety Layer  was proposed to handle the soft instantaneous constraints in RL. However, these approaches in soft-constrained RL tackle constraints implicitly and cannot ensure strict compliance with the constraints, especially the equality ones. By contrast, our method can handle both hard equality and inequality constraints effectively under the RL framework.

**Hard-Constrained RL.** Compared to RL with soft constraints, RL with hard constraints is rarely studied, and most schemes are designed for some specific application. Pham et al.  proposed a plug-in architecture called OptLayer based on OptNet , which belongs to a special neural network called optimization layer [5; 40], to avoid infeasible actions in robotics manipulation. In studying resource allocation problems, Bhatia et al.  developed further Optlayer techniques to deal with hierarchical linear constraints. Liu et al.  investigated robotics manipulation tasks based on RL and used manifold optimization to handle the hard constraints with the inverse dynamic model of robots. Other researchers such as [47; 33] incorporated special optimization techniques into RL to handle power operation tasks in smart grids. However, these methods are designed solely for specific applications or constraints of special types. For example, OptLayer can only handle linear constraints in robotics manipulation. By contrast, RPO is not designed for one specific application and can handle general hard constraints in decision-making problems.

**Generalized Reduced Gradient Method.** GRG  is a classical constrained optimization technique and has the powerful capability to handle optimization with nonlinear hard constraints . The basic idea of GRG is closely related to the simplex method in linear programming which divides variables into basic and nonbasic groups and then utilizes the reduced gradient to perform the update on the basic variables and nonbasic variables respectively. In the past decades, the GRG method has been applied to stock exchange , optimal control in very-large-scale robotic systems , optimal power flow models , and many other fields. Additionally, more recently, several works such as  also fuse the genetic algorithms with the GRG method. Besides, recent research like DC3  in deep learning is also based on the idea of the GRG method. DC3 was proposed for learning to solve constrained optimization problems and has demonstrated its good capability to obtain near-optimal decisions with the satisfaction of nonlinear constraints.

## 3 Preliminaries

**Markov Decision Process.** A classical Markov decision process (MDP)  can be represented as a tuple \((S,A,R,P,)\), where \(S\) is the state space, \(A\) is the action space, \(R:S A S\) is the reward function, \(P:S A S\) is the transition probability function (where \(P(s^{} s,a)\) is the transition probability from the previous state \(s\) to the state \(s^{}\) when the agent took action \(a\) in \(s\)), and \(:S\) is the distribution of the initial state. A stationary policy \(:S(A)\) is a map from states to probability distributions over actions, and \((a|s)\) denotes the probability of taking action \(a\) in state \(s\). The set of all stationary policies \(\) is denoted by \(\). The goal of RL is to find an optimal \(^{*}\) that maximizes the expectation of the discounted cumulative reward, which is \(J_{R}()=_{}[_{t=0}^{}^{t}R(s_{t}, a_{t},s_{t+1})]\). Here \(=(s_{0},a_{0},s_{1},a_{1})\) denotes a trajectory, and \(\) is the distribution of trajectories when the policy \(\) is employed. Then, The value function of

   Constraint & Method & Multiple & Inequality & Equality & Generative & Model Agnostic \\  Soft, Cumulative & CPO , RCO  & CPO  & ✗ & ✓ & ✗ & ✓ & ✓ \\ Soft, Cumulative & Lyapunov [14; 15] & ✗ & ✓ & ✗ & ✓ & ✓ \\ Soft, Cumulative & FOCOS , CUP  & ✗ & ✓ & ✗ & ✓ & ✓ \\ Soft, Cumulative & 100 , PGO  & ✓ & ✓ & ✗ & ✓ & ✓ \\  Soft, Cumulative/Insurances & Lagrangian [13; 11; 99], FAC  & ✗ & ✓ & ✗ & ✓ & ✓ \\ Soft, Instantaneous & Safety Layer  & ✓ & ✓ & ✗ & ✗ (Linear) & ✓ \\ Soft, Instantaneous & Recovery RL  & ✗ & ✓ & ✗ & ✓ & ✓ \\  Hard, Instantaneous & OptLayer , RCO-RL  & ✓ & ✓ & ✓ & ✗ (Specific Linear) & ✓ \\ Hard, Instantaneous &

[MISSING_PAGE_EMPTY:4]

[MISSING_PAGE_EMPTY:5]

**Theorem 1**.: _(GRG update in Tangent Space) If \( a^{N}=(a^{B})}{ a^{B}} a^{B}\), the GRG update for inequality constraints is in the tangent space of the manifold determined by linear equality constraints._

The proof is referred to in supplementary materials. Theorem 1 indicates that the projection stage will not violate the linear equality constraints. With regard to nonlinear equality constraints, we can approximate this nonlinear equality manifold with the tangent space at each GRG update. In that case, we need to set the projection step \(_{a}\) with a sufficiently small value, practically, smaller than \(10^{-3}\), to avoid destroying the satisfaction of equality constraints. In this way, we can ensure that the GRG update is conducted in the manifold defined by the nonlinear equality constraints. Additionally, a similar projection method was proposed in  with \(_{2}\) norm objective, which is likely to fail in the nonlinear situation as we analyze in the supplementary materials.

### Practical Implementation

Here we present details about the implementation of our RPO model and some training designs. They mainly include two aspects: 1) how to achieve better initial actions from the policy network and 2) how to incorporate such a decision process into the training of off-policy RL algorithms.

```
0: Policy network \(_{}(s)\), value network \(Q_{}(s,a)\), penalty factors \(\), replay buffer \(\).
1:for\(t\)\(\)\(1,2,,T\)do
2: Sample the basic actions \(a_{t}^{B}\) with the output of \(_{}(s_{t})\) and some random process.
3: Calculate the action \(a_{t}\) according to the construction stage 4.1 and projection stage 4.2.
4: Take the action \(a_{t}\) in the environment and store the returned transition in \(\).
5: Sample a mini-batch of transitions in \(\).
6: Update the parameters of the policy network using 8 and the penalty factors using 9.
7: Construct TD target as \(y_{t}=r_{t}+ Q_{}(s_{t+1},_{}(s_{t+1}))\).
8: Update the parameters of the value network using MSE loss.
9:endfor
```

**Algorithm 1** Training Procedure of RPO

**Policy Loss with Modified Lagrangian Relaxation.** While the above two-stage decision procedure can cope with the satisfaction of equality and inequality constraints, the policy network is also required to guarantee certain feasibility of inequality constraints for better initial actions \(\). Otherwise, we may need hundreds of GRG updates in the projection stage to satisfy all the inequality constraints. Here, we use an augment loss with Lagrangian relaxation for the policy network to obtain initial actions for the inequality projection stage. Common approaches such as  use fixed Lagrangian multipliers in their loss function. However, such fixed Lagrangian multipliers are not easy to obtain and may require extra information and computation to tune. In this paper, we perform a dual update on Lagrangian multipliers to adaptively tune them during the training period. As illustrated in Section 3, after the equality construction stage the constrained MDP problem is formulated as

\[_{}& J_{R}(_{}) \\ & g_{j}(_{}(s_{ t});s_{t}) 0,\  j,t. \]

This means we need to deal with instantaneous inequality constraints in all states. Hence, we cannot apply the primal-dual update method directly like PDO . Otherwise, we need to compute the dual variables on all states, which is obviously unrealistic. Fortunately, we can maintain only one Lagrangian multiplier \(^{j}\) for each inequality constraint in all states with the exact penalty term \(\{0,g_{j}((s_{t});s_{t})\}\). Accordingly, the new objective with the exact penalty term is

\[_{}}()-J_{R}(_{}) +_{s}[_{j}^{j}\{0,g_{j}((s_{ t});s_{t})\}] \]

The following theorem establishes the equivalence between the unconstrained problem (8) and the constrained problem (7).

**Theorem 2**.: _(Exact Penalty Theorem) Assume \(^{j}_{s}\) is the Lagrangian multiplier vector corresponding to \(j\)th constraints in state \(s\). If the penalty factor \(^{j}\|^{j}_{s}\|_{}\), the unconstrained problem (8) is equivalent to the constrained problem (7)._The proof is referred to in the supplementary materials.

According to the

\[_{k+1}^{j}=_{k}^{j}+_{}^{j}_{s}[\{0,g _{j}((s_{t});s_{t})\}] \]

where \(_{}^{j}\) is the learning rate of \(j\)-th penalty factors, \(_{k}^{j}\) is value of \(^{j}\) in the \(k\)-th step and \(_{0}^{j}=0\). Since the exact penalty term, \(_{s}[\{0,g_{j}((s_{t});s_{t})\}]\), is always non-negative, the penalty factors are monotonically increasing during the training procedure. Hence, we can always obtain sufficiently large \(^{j}\) that satisfies the condition in Theorem 2, i.e., \(^{j}\|_{s}^{j}\|_{}\). Besides, we also find that the adaptive penalty term does not prevent the exploration for higher rewards of the RL agent at the beginning of the training procedure in Section 5.

**Off-policy RL Training.** Since we augment the policy loss function with the exact penalty term, the actual objective of our algorithm is two-fold. One is to obtain the optimal policy with the satisfaction of hard constraints. Another is to reduce the number of GRG updates performed during the projection stage. This indicates there exists a gap between the behavioral policy and the target policy in RPO, which results from the changing times of GRG updates performed in the projection stage.

Hence, RPO should be trained like off-policy RL methods as Algorithm 1. Specifically, we regard the initial actions \(\) output by the construction stage as the optimization object rather than the actions \(a\) post-processed by the projection stage. Otherwise, the training process will be unstable and even collapse due to the changing times of GRG updates in the projection stage, which can also be viewed as a changing network architecture. Besides, we use the \(y_{t}=r_{t}+ Q_{}(s_{t+1},_{}(s_{t+1}))\) to construct the TD target since \(_{}\) is the actual policy we deploy in the environment.

## 5 Experiments

To validate our method and further facilitate research for MDP with hard constraints, we develop three benchmarks with visualization according to the dynamics in the real world, ranging from classical robotic control to smart grid operation. They involve Safe CartPole, Spring Pendulum, and Optimal Power Flow with Battery Energy Storage. Then, we incorporate RPO into two classical off-policy RL algorithms, DDPG  and SAC , which we call RPO-DDPG and RPO-SAC respectively.

RPO is compared with three representative Safe RL algorithms, including CPO , CUP , and Safety Layer . Notably, we transform the hard equality constraints into two inequality constraints since existing Safe RL methods cannot handle both general equality and inequality constraints. Furthermore, we also contrast the RPO-DDPG and RPO-SAC with DDPG-L and SAC-L, where DDPG-L and SAC-L represent DDPG and SAC only modified with the Lagrangian relaxation method we mentioned in Section 4.3 and without the two-stage decision process in RPO respectively. Besides, DDPG-L and SAC-L deal with the equality constraints as we mentioned in Safe RL algorithms. More details related to the concrete RPO-DDPG and RPO-SAC algorithms are illustrated in the supplementary materials. Our code is available at: [https://github.com/wadx2019/rpo](https://github.com/wadx2019/rpo).

### RL Benchmarks with Hard Constraints

Specifically, our benchmarks are designed based on , with extra interfaces to return the information of the hard constraints. To the best of our knowledge, it is the first evaluation platform in RL that considers both equality constraints and inequality constraints. Figure 2 shows the visualization of these three benchmarks, and the simple descriptions for them are presented in the contexts below. More details about them are provided in the supplementary materials.

**1) Safe CartPole.** Different from that standard CartPole environment in Gym , we control two forces from different directions in the Safe CartPole environment. The goal is to keep the pole upright as long as possible while the summation of the two forces should be zero in the vertical direction and be bounded by a box constraint in the horizontal direction. That is, the former is the hard equality constraint, while the latter is the hard inequality constraint.

**2) Spring Pendulum.** Motivated by the Pendulum environment , we construct a Spring Pendulum environment that replaces the pendulum with a light spring, which connects the fixed point and the ball. In order to keep the spring pendulum in the upright position, two torques are required to apply in both vertical and horizontal directions. Meanwhile, the spring should be maintained at a fixed length, which introduces a hard equality constraint. Unlike that in Safe CartPole, here equality constraint is state-dependent since the position of the spring is changed during the dynamical process. Besides, the summation of the two torques is also bounded by introducing a quadratic inequality constraint.

**3) Optimal Power Flow with Battery Energy Storage.** Optimal Power Flow (OPF) is a classical problem in smart grid operation to minimize the total cost of power generation with the satisfaction of grid demand. However, battery energy storage systems or electric vehicles have been integrated into smart grids to alleviate the fluctuations of renewable energy generation recently. As illustrated in , we need to jointly optimize the charging or discharging of batteries with OPF for the long-term effect of the smart grid operation. According to this real-world application, we design this environment, which is possessed of nonlinear power flow balance equality constraints and inequality box constraints on the actions for feasibility.

### Evaluation Metrics

We use three metrics to evaluate the algorithms as follows:

**Episodic Reward:** cumulative reward in a whole episode.

Figure 3: Learning Curves of different algorithms on our three benchmarks across 5 runs. The x-axis is the number of training epochs. The y-axis is the episodic reward (the first line), and max instantaneous constraint violation (the second line) respectively.

Figure 2: Visualization of RL benchmarks with hard constraints. (a) **Safe CartPole.** The indicator lamp in the cart will be green if the given actions are feasible, and will be red otherwise. (b) **Spring Pendulum.** The length of the spring will be changed if the equality constraint is violated. (c) **OPF with Battery Energy Storage.** The circle nodes represent the buses in the electricity grid, and the square nodes represent the batteries connected to the generator buses. We use light and shade to reflect the state of batteries and the power generation and demand of buses. In addition, the edge between the generator bus and the battery will be red if the battery is charging, and green if the battery is discharging.

**Max Instantaneous Constraint Violation:** maximum instantaneous violation of all constraints. It denotes the feasibility of actions in a state because the feasibility of actions relies on the constraint that is the most difficult to be satisfied.

**Max Episodic Constraint Violation:** maximum episodic violation of all constraints. Similar to Mean Constraint Violation, it denotes the feasibility of actions in a whole episode.

### Performance of RPO on Reward and Constraints

We plot the learning curves of CPO, CUP, Safety Layer, DDPG-L, SAC-L, RPO-DDPG, and RPO-SAC in Figure 3. Notably, to validate the importance of the two-stage decision procedure, here SAC-L and DDPG-L are actually the versions of RPO-SAC and RPO-DDPG without this procedure. Given the fairness of our experiments, we apply the same shared hyper-parameters for all the algorithms. This empirical result reflects that existing Safe RL algorithms cannot handle the MDP problems with hard constraints, and our approach outperforms other algorithms in terms of both episodic reward and the max constraint violation. Moreover, the learning curves confirm that RPO can also guarantee certain feasibility during the training period.

Besides the learning curves, Table 2 shows the performance of different algorithms after convergence. To present more details on the constraint violations, here the equality and inequality constraint violations are shown separately. Notably, since the \(tanh\) and state-dependent \(tanh\) activation function are added to limit the output of the neural network for the box constraints, there is no inequality constraint that needs to be satisfied for DDPG-L, SAC-L, and three Safe RL algorithms. That's why these two algorithms achieve zero violation in the inequality constraints in OPF with Battery Energy Storage. However, it is hard for them to satisfy both equality and inequality constraints exactly in OPF with Battery Energy Storage. This is because this environment is very difficult and involves many complex equality and inequality constraints. Practically, a tolerance 1e-3 is introduced here to evaluate the satisfaction of constraints.

## 6 Limitation and Future Work

We acknowledge that there still exist some limitations in RPO. One is that RPO is time-consuming compared to standard neural networks due to the projection stage, where the GRG updates may need to be performed several times. Another is that the equality equation solver required in the construction stage may need to be either chosen or specially designed with domain knowledge. Hence, accelerating RPO and developing more powerful RL algorithms that can handle hard constraints are under consideration in our future works. Besides, while we only validate RPO in RL benchmarks with hard constraints, our method can also be easily extended to cases with both hard constraints and soft constraints as long as a neural network is utilized to fit the mapping between the state-action pair and the cost. Moreover, RPO can be viewed as a supplement to Safe RL algorithms. In scenarios with both hard instantaneous constraints and soft cumulative constraints, we can always use RPO to handle the hard instantaneous constraints and apply some existing Safe RL methods to handle soft cumulative constraints.

   Task & Metrics & CPO & CUP & Safety Layer & DDPG-L & SAC-L & RPO-DDPG-L* & RPO-SAC(\({}^{}\)) \\    & Eng. reward & 20.1000(2019.8193) & 6.9300(166.629) & 53.8000(275.15600) & 1.5800(17.3464) & 4.5600(15.5772) & 20.0000(000.0000) & 20.0000(000.0000) \\  & Mn. its target & 0.0000(000.0000) & 0.0000(000.0000) & 0.0000(000.0000) & 0.0000(000.0Conclusion

In this paper, we outlined a novel algorithm called RPO to handle general hard constraints under the off-policy reinforcement learning framework. RPO consists of two stages, the construction stage for equality constraints and the projection stage for inequality constraints. Specifically, the construction stage first predicts the basic actions, then calculates the nonbasic action through an equation-solving procedure, and finally concatenates them as the output of the construction stage. The projection stage applies GRG updates to the concatenated actions until all the inequality constraints are satisfied. Furthermore, we also design a special augmented loss function with the exact penalty term and illustrate how to fuse RPO with the off-policy RL training process. Finally, to validate our method and facilitate the research in RL with hard constraints, we have also designed three benchmarks according to the physical nature of the real-world applications, including Safe CartPole, Spring Pendulum, and Optimal Power Flow with Battery Energy Storage. Experimental results in these benchmarks demonstrate the superiority of RPO in terms of both episodic reward and constraint violation.