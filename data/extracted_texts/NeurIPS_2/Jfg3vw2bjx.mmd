# APIGen: Automated PIpeline for Generating Verifiable and Diverse Function-Calling Datasets

Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh Murthy, Liangwei Yang, Silvio Savarese, Juan Carlos Niebles, Huan Wang, Shelby Heinecke, Caiming Xiong Salesforce AI Research, USA

{zuxin.liu, thai.hoang, jianguozhang}@salesforce.com

###### Abstract

The advancement of function-calling agent models requires diverse, reliable, and high-quality datasets. This paper presents APIGen, an automated data generation pipeline designed to synthesize high-quality datasets for function-calling applications. We leverage APIGen and collect 3,673 executable APIs across 21 different categories to generate diverse function-calling datasets in a scalable and structured manner. Each data in our dataset is verified through three hierarchical stages: format checking, actual function executions, and semantic verification, improving its reliability and correctness. We demonstrate that models trained with our curated datasets, even with only 7B parameters, can achieve state-of-the-art performance on the Berkeley Function-Calling Benchmark, outperforming multiple GPT-4 models. Moreover, our 1B model achieves exceptional performance, surpassing GPT-3.5-Turbo and Claude-3 Haiku. We release a dataset containing 60,000 high-quality entries, aiming to advance the field of function-calling agent domains. The dataset is available on Huggingface 1 and the project homepage 2.

## 1 Introduction

Function-calling agents represent a significant advancement in artificial intelligence, specifically within the realm of Large Language Models (LLMs). These models, such as GPT4 , Gemini , and Mistral , have evolved to not only understand and generate human-like text but also to execute functional API calls based on natural language instructions. For instance, consider a user requesting the weather in Palo Alto, as illustrated in Fig. 1. The function-calling agent interprets this query, accesses the relevant API--such as get_weather("Palo Alto", "today")--and retrieves the weather information, all in real-time. This capability extends the utility of LLMs beyond simple conversation tasks to include dynamic interactions with a variety of digital services and applications, ranging from social media platforms to financial services .

Despite their growing popularity and potential, the deployment of function-calling agents is often hampered by the quality of the datasets used for training. Current datasets are largely static and lack comprehensive verification, leading to potential inaccuracies and inefficiencies of model fine-tuning in real-world applications . This limitation is particularly evident when models trained on these datasets encounter new, unseen APIs. For example, a model trained primarily on restaurant booking APIs may struggle when suddenly tasked with retrieving stock market data, as it lacks the specific training data or the adaptability to handle new domains.

To address these challenges, we introduce APIGen, an **A**utomated **PI**peline for **G**enerating verifiable and diverse function-calling datasets. Our framework is designed to facilitate the fine-tuning of function-calling LLMs by providing high-quality, diverse datasets that better reflect the variability and complexity of real-world API use. Crucially, each generated data point undergoes rigorous multi-stage verification processes--format, execution, and semantic--to improve accuracy and applicability. We fine-tune function-calling models using the dataset generated by APIGen. The results show the strong performance of our models, surpassing many existing powerful LLMs with much fewer parameters, highlighting the effectiveness of APIGen and the high quality of the dataset it produces.

With APIGen, we release a comprehensive dataset containing 60,000 entries with 3,673 APIs across 21 categories. They include various query styles, such as parallel function calling data (asking the agent to produce multiple concurrent function calls in a single response) , which is rarely found in public datasets, to the best of our knowledge. This large-scale synthetic dataset is intended to catalyze further research and development in the field of function-calling agents, offering researchers and developers a foundation for training and testing their models. The data is available on Huggingface and our project homepage.

The contributions of this work are summarized as follows:

* We introduce APIGen, a function-calling data generation pipeline that features quality, scalability, and diversity of the data. APIGen is compatible with a range of models and APIs to construct high-quality synthetic function-calling datasets.
* We train two function-calling models of different sizes, 1.3B and 6.7B, using APIGen-constructed training data. Extensive experiments demonstrate that the 6.7B model achieves a rank of 3rd on the Berkeley Function-Calling Leaderboard , surpassing GPT-4o and Gemini-1.5-Pro, while the 1.3B model outperforms GPT-3.5-Turbo.
* We also release a synthetic function-calling dataset containing 60,000 high-quality data generated by APIGen using several strong open-source LLMs, which can potentially benefit the research community in developing advanced function-calling models.

## 2 Related Work

**Tool-use Agent.** Recent works have developed frameworks and models that enable LLMs to interact with APIs and tools [15; 16; 17; 18; 19; 20; 21; 22]. RestGPT  connects LLMs to RESTful APIs using a Planner, API selector, and API executor to handle complex instructions. Toolformer  is an early work that enables agents to use tools like Question Answering, Calculator, and Wikipedia Search through a supervised-finetuned model. [25; 26] propose the xLAM model series, showing strong tool usage capability across several benchmarks. Octopus-v4  presents a methodology to incorporate multiple specialized language models to solve corresponding tasks. While NexusRaven  and Gorilla OpenFunctions-v2  are strong open-sourced models that focus on function calling, neither provides access to their training datasets.

**Agent Datasets.** Several datasets have been created to support the development of agent models. AgentInstruct  consists of 6 datasets for different agent tasks, including AlfWorld , WebShop , Mind2Web , Knowledge Graph, Operating System, and Database . APIBank  is a benchmark designed for tool-augmented LLMs, providing a training set containing tool-use dialogues from various APIs. Toolalpaca  constructs a varied and well-structured tool-use dataset by randomly selecting APIs and generating documentation using ChatGPT. ToolBench  creates

Figure 1: Workflow of an LLM-based function-calling agent.

an instruction-tuning dataset for tool use by collecting diverse REST APIs and generating their descriptions using ChatGPT. AgentOhana  and Lumos  design a unified data and training pipeline for efficient agent learning, covering multiple different datasets and environments. However, most of these datasets were not rigorously verified, and usually contain noisy data.

**Benchmarks.** Recent studies have established several benchmarks to assess agent abilities on various tasks such as web interactions, reasoning, decision making, function calling, code generation, and tool usage . Specifically, AgentBoard  includes 9 tasks, with ToolOperation and ToolQuery designed to evaluate agent ability on multi-turn interaction with external tools. ToolEval  assesses functional calling capabilities via RapidAPI, containing around 1,000 test cases and asking GPT-3.5 to assess the Win Rate. Furthermore, the Berkeley Function-Calling Leaderboard (BFCL)  provides a robust and comprehensive framework to evaluate models' abilities to call functions, with 1,700 test cases covering a wide range of scenarios. We use BFCL as our testing ground as it provides the most thorough comparison among popular LLMs.

## 3 APIGen Framework

This section introduces the detailed design of APIGen, an **A**utomated **PI**peline for **G**enerating verifiable and diverse function-calling datasets. Our framework is designed with three key factors in mind: data quality, data diversity, and collection scalability. We achieve these through the key modules shown in Fig. 2: the multi-stage data verification process ensures data quality, the seed QA (query-answer) data sampler, API sampler, and various prompt templates ensure diversity, and our structured modular design using a unified format enables the system to scale to diverse API sources, including but not limited to Python functions and representational state transfer (REST) APIs.

### Data Generation Overview

Figure 2 outlines the data generation process using the APIGen framework, which begins by sampling one or more APIs and example query-answer (QA) pairs (seed data) from the library, then formatting them into a standardized JSON format (see Fig. 3 for examples). A prompt template is selected based on the desired data generation objectives, which steers the LLM in generating relevant query-answer pairs. Each answer in the generated pairs is a function call formatted in JSON.

Figure 2: Illustration of the post-process filters.

The adoption of a standardized JSON format for APIs, function calls, and generator outputs (as shown in Figure 3) provides several advantages. Firstly, it establishes a structural way to verify whether the generator's output contains all necessary fields. Outputs that fail to comply with these format requirements are discarded. Secondly, the JSON structure enables efficient checking of function calls for correct parsing and validity of arguments. Calls that include arguments not present in the API library or hallucinate non-existent functions are excluded, enhancing the overall quality of the dataset. Another key benefit is the scalability it enables. With this uniform format, APIGen can easily incorporate data from diverse sources (Python functions, REST APIs, etc) by developing format converters that adapt them into these basic JSON elements, without modifying other core components, such as the prompting library, making the framework highly adaptable and extensible.

The generated function calls are subjected to a multi-stage verification process to improve their correctness and relevance. First, a format checker verifies correct JSON formatting and parseability. Next, the API execution engine processes the calls and sends the results and queries to a semantic checker, another LLM, which assesses alignment between the function calls, execution results, and query objectives. Data points passing all stages are added back to the seed dataset as high-quality examples to enhance future generation diversity. We detail each checker in the next section.

### Multi-Stage Data Verification

Prioritizing quality is crucial, as previous research has shown that small amounts of high-quality fine-tuning data can substantially enhance model performance on domain-specific tasks . This motivates our multi-stage dataset verification process to align large language models effectively.

The key insight driving our framework design is that, unlike synthetic chat data which can be difficult to evaluate, function-calling answers can be directly executed via their corresponding APIs. This enables checking if the output API and parameters' formats are correct, if the generated API calls are executable, and if execution results match the query's intent, etc. Based on this observation, we propose a three-stage verification process:

**Stage 1: Format Checker**: This stage performs sanity checks to filter out poorly formatted or incomplete data. The LLM output must strictly follow a JSON format with the "query" and "answer" fields, as shown in Fig. 3. We usually also include an additional "thought" field, which is known as the chain-of-thought (CoT) prompting technique , to increase the pass rate of the generated data. The data is discarded if these fields cannot be properly extracted for function calls. Additionally, the function calls are checked for correct JSON parsing and valid arguments. Generated calls whose arguments or functions are not present in the given APIs are eliminated to reduce hallucination and improve data quality.

**Stage 2: Execution Checker**: Well-formatted function calls from Stage 1 are executed against the appropriate backend (e.g. Python functions are directly imported and executed in a separate subprocess, while REST APIs are called to obtain results and status codes). Unsuccessful executions are filtered out, and fine-grained error messages are provided for failures, including argument type errors, invalid parameters, runtime errors, timeout, syntax errors, missing arguments, etc.

**Stage 3: Semantic Checker**: Successful Stage 2 execution results, available functions, and the generated query are formatted and passed to another LLM to assess if the results semantically align

Figure 3: JSON data format examples.

with the query's objective. Query-answer pairs that execute successfully but produce meaningless results due to infeasible queries or incorrect arguments are filtered out. The main decision factors for this stage are: 1) whether the function call aligns with the query's objective and has proper arguments; 2) whether the function call and arguments are appropriately chosen from the available functions; 3) whether the number of function calls matches the user's intent; 4) whether the execution results contain errors or indicate unsuccessful function execution; 5) whether the execution results are relevant and match the query's purpose. APIGen's design offers the flexibility to select one or multiple LLMs as checkers, and the filtering rules can be readily adjusted--either tightened or relaxed--depending on specific use cases. Though the final stage can not guarantee correctness, the execution feedback information from stage 2 allows the checker to better assess the quality of the data, thus improving the decision accuracy.

Data points that pass all three verification stages are regarded as high-quality and added back to improve future diverse data generation. This multi-stage verification process is the key to ensuring the APIGen framework produces a dataset that is not only diverse but also of a high degree of confidence in data quality, enabling more effective fine-tuning of LLMs to domain-specific API-related tasks.

### Methods to Improve Dataset Diversity

Encouraging diversity in training datasets is crucial for developing robust function-calling agents that can handle a wide range of real-world scenarios. In APIGen, we promote data diversity through multiple perspectives, including query style diversity, sampling diversity, and API diversity.

**Query Style Diversity.** APIGen's dataset is structured into four main categories: simple, multiple, parallel, and parallel multiple, each designed to challenge and enhance the model's capabilities in different usage scenarios. These categories are inspired by the Berkeley function-calling benchmark  and are controlled by corresponding prompts and seed data. We show examples of them in the supplementary material. The categories are as follows:

* **Simple:** This query style includes straightforward scenarios where a single function call is made based on the user's input with a single provided JSON format API description.
* **Multiple:** In this style, user queries could be answered by one of several function calls. The challenge lies in selecting the most appropriate function from multiple provided APIs. It represents one of the most common real-world use cases.
* **Parallel:** This query style requires executing multiple function calls simultaneously in response to a single user query, which may consist of one or more sentences but with only one API provided. For instance, if the user wants to know the weather in both Palo Alto and Paris, the model should call the get_weather function twice with corresponding city names in a single response.
* **Parallel Multiple:** This query style combines the parallel and multiple categories, where multiple function and API documents are provided, and each function call might be invoked multiple times based on the query's requirements.

While there exist publicly available training data for _simple_ and _multiple_ categories , however, to the best of our knowledge, we offer the first large-scale and high-quality datasets that include the _parallel_-related function-calling scenario.

**Sampling Diversity.** APIGen utilizes a sampling system designed to maximize the diversity and relevance of the generated datasets, which include three main components, as shown in Fig. 2:

* **API Sampler:** This module extracts one or more function descriptions from executable API libraries, standardizing them into a uniform JSON format. The diverse sources of APIs ensure a wide range of function calls are available for inclusion in the training dataset.
* **Example Sampler:** It samples a specified number of seed examples corresponding to the different categories. These examples are transformed into structured queries, function descriptions, and answers, serving as an important few-shot reference for data generation.
* **Prompt Sampler:** This sampler draws from a diverse prompt library to generate a variety of query-answer pairs. The prompts for each query style contain different contexts, ranging from simple, concise query-answer pairs to more realistic scenarios, such as ambiguous or misspelled user requests, enhancing the model's ability to handle real-world interactions.

We provide some prompt templates and seed data in the supplementary material. In APIGen, the number of examples and APIs sampled for each dataset iteration is randomly chosen from a predefined range. This randomization enhances dataset variability by preventing repetitive patterns and ensuring a broad coverage of scenarios. We next introduce our API diversity.

## 4 Dataset Preparation and Collection

We begin by discussing our dataset preparation process, which includes selecting and cleaning API libraries. Then we present our dataset collection setup and an overview of the resulting dataset.

### Dataset API Sources

To ensure a high-quality and diverse dataset, we focused on collecting real-world APIs that could be readily executed and came with thorough documentation. We primarily sourced APIs from ToolBench , a comprehensive tool-use dataset that includes 16,464 REST APIs across 49 coarse-grained categories from RapidAPI Hub. This hub is a leading marketplace featuring a vast array of developer-contributed APIs. To further enhance the usability and quality of the APIs, we perform the following filtering and cleaning procedures on the ToolBench dataset:

* **Data Quality Filtering:** We remove APIs with incorrectly parsed documentation and those lacking required or optional parameters. APIs requiring no parameters were excluded to maintain the challenge level appropriate for our dataset needs.
* **API Accessibility Testing:** We tested API accessibility by making requests to each endpoint using example parameters provided in the dataset and through the Stable Toolbench server . APIs that could not be executed or returned errors, such as timeouts or invalid endpoints, were discarded.
* **Docstring Regeneration:** To improve the quality of API documentation, we regenerated docstrings for the APIs that have noisy and unusable descriptions.

After cleaning, we obtain 3,539 executable REST APIs with good documentation. Additionally, we incorporated Python functions as another API type, inspired by the executable evaluation categories of the Berkeley functionalities benchmark . We collected 134 well-documented Python functions covering diverse fields such as mathematics, finance, and data management. Sample API examples are provided in the supplementary material.

The original ToolBench dataset contained semantically overlapping categories such as Finance and Financial. We consolidated these into 21 distinct categories to ensure clarity and balance across the dataset. Figure 4 illustrates the distribution of the 3,673 executable APIs across these redefined categories, spanning sectors like technology, social sciences, education, and sports. This diverse collection of APIs provides a strong foundation for synthetic data generation and is a valuable asset for ensuring data quality and reliability.

### Collection Setup and Dataset Details

To validate the effectiveness of the APIGen framework, we generated datasets targeting various query styles as outlined in Section 3.3. We utilized several base LLMs for data generation, including DeepSeek-V2-Chat (236B) , DeepSeek-Coder-33B-Inst , Mixtral-8x22B-Inst, and Mixtral-8x7B-Inst . For each model, our target was to generate 40,000 data points by sampling different combinations of APIs, seed data, and prompt templates. To foster diversity in the generated responses,

Figure 4: The category distribution of the 3,673 executable APIs.

we set the generation temperature to 0.7 across all models. Examples of the prompt templates and APIs used are provided in the supplementary materials for reference.

Table 1 presents statistics for the data generation process with different models, including the total verified data point count and the number of filtered data points at each verification stage. The filtering process successfully removes many low-quality data points due to formatting issues, execution errors, or failure to pass the semantic check. The first two stages, format checker and execution checker, typically filter out the majority of low-quality data. These data points often have infeasible argument ranges, incorrect types, missing required parameters, or more severe issues such as hallucination of function calls or parameters. Our systematic verification process provides a rigorous way to reduce the occurrence of these situations.

The semantic checker also plays a crucial role in filtering generated data that does not align with the query's objectives. For instance, if a user's query contains multiple requests, but the returned results only address one, or if the generated function-call data and execution results do not match the user's query, the data point will be filtered out. Including these data points in the training set for model training could potentially harm the performance, as demonstrated in the experiments.

We observe that stronger models like DeepSeek-V2-Chat and Mixtral-8x22B-Inst have better format-following capabilities and higher pass rates, while the two relatively smaller models have a much higher likelihood of producing data that cannot be executed. This suggests that when using weaker models to generate data, a strict verification process is recommended to filter out low-quality data.

We are releasing approximately 60,000 high-quality function-calling datasets generated from the two strongest models: Mixtral-8x22B-Inst and DeepSeek-V2-Chat (236B). These datasets include all the query styles mentioned in Sec. 3.3 and cover a wide range of practical situations, with 3,673 diverse APIs across 21 categories. Each data point has been verified with high confidence of correctness using real-world API executions and the semantic checker. We also conducted human inspection on 600 sampled data. The results show that over 95% of the data are correct (details in Appendix A.3), showing the effectiveness of the framework. By making this dataset publicly available, we aim to benefit the research community and facilitate future work in this area.

## 5 Experiments

### Experiment Setup

To evaluate the utility and effectiveness of the collected dataset, we conducted experiments by training function-calling models with the generated data. Our aim is to answer two key questions: 1) To what extent can the generated data boost the model's function-calling capability, and how does it compare to existing models? 2) How effective is the APIGen framework in filtering out low-quality data?

To address these questions, we train two versions of base models: DeepSeek-Coder-1.3B-instruct and DeepSeek-Coder-7B-instruct-v1.5  using the xLAM (large action model) training pipeline proposed in . We refer to these models as xLAM-1B (FC) and xLAM-7B (FC), where FC stands for the Function-Calling mode, similar to this mode in other existing models that output JSON-format function calls . We compare the performance of these small-sized models against state-of-the-art models, including different versions of GPT-4 series , Claude-3 series , Gemini series , Llama3 , Mixtral , OpenFunctions-v2 , Command R+ , etc.

**Benchmark.** We evaluate the trained models' performance on the Berkeley Function-Calling Benchmark (BFCL) , which provides a comprehensive evaluation framework for assessing the function-calling capabilities of LLMs across various programming languages and application domains. Designed to reflect real-world use cases, the BFCL includes 1,700 testing cases, covering

   Model & Verified Data & Fail Format & Fail Execution & Fail Semantic & Pass Rate \\  DeepSeek-Coder-33B-Inst & 13,769 & 4,311 & 15,496 & 6,424 & 34.42\% \\ Mixtral-8x7B-Inst & 15,385 & 3,311 & 12,341 & 7,963 & 38.46\% \\ Mixtral-8x22B-Inst & 26,384 & 1,680 & 5,073 & 6,863 & 65.96\% \\ DeepSeek-V2-Chat (236B) & 33,659 & 817 & 3,359 & 2,165 & 84.15\% \\   

Table 1: Filtering statistics for the generated datasets using different base LLMs.

complex scenarios such as parallel and multiple-function calls. The benchmark contains diverse API sources like Java, JavaScript, and Python, offering a detailed analysis of each model's ability to correctly interpret and execute commands under different conditions. BFCL serves as a highly detailed and scalable benchmark for evaluating LLMs' function-calling capabilities and provides a leaderboard to track the most recent and powerful LLMs, both commercialized and open-source.

**Evaluation Metrics.** The Berkeley Function-Calling Leaderboard (BFCL) evaluates LLMs using two main categories: Abstract Syntax Tree (AST) Evaluation and Executable Function Evaluation. The AST evaluation focuses on the syntactic accuracy of the generated function calls, ensuring that the model's output matches a predefined function documentation in structure and parameters. This includes checks for correct function names, required parameters, and appropriate data types. The Executable Function Evaluation goes a step further by running the generated function calls to verify their operational correctness. This executable test ensures that the functions not only compile but also execute correctly, providing the expected results, which is crucial for practical applications where real-time performance is essential.

### Experiment Results Analysis

**Can the generated data improve the model's function-calling capability and how does it compare to other most powerful models?** The performance of our models, xLAM-7B and xLAM-1B, as presented in Table 2, highlights the effectiveness of our APIGen framework and the quality of the datasets produced. Notably, our xLAM-7B model ranks 3rd among the most powerful LLMs listed on the BFCL leaderboard, surpassing several versions of GPT-4 (GPT-4o, GPT4-Turbo-FC), Llama3-70B, multiple Claude-3 models, and a series of strong models which are known for their exceptional capabilities in various tasks, including function-calling. This achievement demonstrates the significant impact of our high-quality dataset on the model's function-calling performance.

Our smaller xLAM-1B model also shows remarkable results, securing the 25th position and outperforming many larger models, such as Claude-3 Haiku , Command-R-Plus , DBRX-Instruct , Mistral-large , and GPT-3.5-Turbo-0125. The results highlight the effectiveness of the APIGen pipeline in enhancing a model's function-calling capabilities, even with a much smaller size. Both xLAM-7B and xLAM-1B demonstrate substantial improvements in handling complex query types, particularly in the _parallel_ and _multiple_ function-calling scenarios, which are typically

   &  &  &  &  &  Evaluation \\  } &  Evaluation \\  } &  Evaluation \\  } &  Evaluation \\  } &  Evaluation \\  } &  Evaluation \\  } &  Evaluation \\  } &  Detection \\  } \\    & & & Simple & Multiple & Parallel & & & & & & & & & & \\ 
1 & 90.12 & 17.5-2.5-Sweet (Weinberg) & 86.73 & 90.5 & 92.5 & 92.2 & 100 & 96 & 82 & 89 & 85.24 \\
2 & 88.29 & GPT-40.125-Prester (Prosp) & 88.36 & 95 & 92 & 92 & 94.1 & 94 & 84 & 75 & 70.42 \\ 
0 & 88.84 & 
 \(\)DBRX-Instruct \\  & 86.84 & 94.1 & 88.7 & 96.04 & 88 & 84 & 80 & 84.88 \\ 
4 & 87.71 & Claude-3.098-204203 (Prosp) & 86.73 & 94 & 86.5 & 89 & 97.65 & 92 & 80 & 75 & 80.42 \\
5 & 86.53 & Nemoto-1.340-Instruct (Prosp) & 83.45 & 92.5 & 90.5 & 85.5 & 92.84 & 96 & 82 & 77.5 & 78.33 \\
6 & 86.35 & Gemini-1.5-Two-Front-0514 (FC) & 80.18 & 92 & 91.5 & 88 & 91.76 & 88 & 76 & 77.5 & 89.58 \\
7 & 85.88 & Gemini-1.5-Two-Front-059 (FC) & 80 & 92.5 & 90.5 & 87.5 & 90 & 90 & 74 & 77.5 & 88.75 \\
8 & 85.88 & GPT-41.106-Front-05 & 84 & 91.5 & 92.5 & 86.5 & 89.41 & 92 & 78 & 67.5 & 80.42 \\
9 & 85.88 & GPT-4.102-Front-042 (FC) & 86.55 & 95 & 91 & 90 & 97.65 & 94 & 80 & 72.5 & 62.5 \\
10 & 84.65 & Gemini-0.05-Front-05 (FC) & 88 & 95 & 87.5 & 86.5 & 94.71 & 94 & 70 & 67.5 & 61.25 \\
11 & 84.59 & GPT-4.015-Front-05 (FC) & 80.18 & 93 & 90.5 & 84.5 & 83.1 & 92 & 86 & 77.5 & 82.90 \\
12 & 84 & Meta-Llama-3.700-Instruct (Prosp) & 81.45 & 90 & 91.5 & 86 & 91.76 & 88 & 84 & 77.5 & 69.17 \\
13 & 83 & GPT-4.020-2403 (FC) & 78.91 & 90 & 88 & 84.5 & 86.77 & 78 & 82 & 75 & 81.25 \\
14 & 82.94 & GPT-4.020-2404 (AP) & 74.73 & 90 & 90 & 88 & 82.94 & 88 & 76 & 67.5 & 88.75 \\      22 \\ 23 \\ 24 \\ 25 \\ 26 \\ 27 \\ 27 \\ 28 \\ 27 \\ 28 \\ 26 \\ 27 \\ 28 \\ 26 \\ 27 \\ 28 \\ 29 \\ 28 \\ 26 \\ 27 \\ 28 \\ 28 \\ 26 \\ 28 \\ 27 \\ 28 \\ 28 \\ 28 \\ 28 \\ 29 \\ 29 \\ 29 \\ 24 \\ 30 \\ 31 \\ 32 \\ 33 \\ 34 \\ 35 \\ 36 \\ 36 \\ 66 \ in existing publicly available dataset. This validates the value of our pipeline and datasets in addressing practical scenarios involving complex API interactions and multiple concurrent API calls, especially considering that the base model, DeepSeek-Coder-v1.5, only ranks 45th on the leaderboard and performs poorly in these categories.

Next, we answer the question: **how effective is the APIGen framework in filtering out low-quality data?** We conducted an ablation study by adding the datasets that were filtered out by stage 3 (semantic checker) and stage 2 (execution checker) back to the training set, simulating situations where generated data is used without the rigorous verification process. The performance comparison on the BFCL benchmark, shown in Fig. 5, reveals that using these filtered datasets for training harms the final performance, with a more significant impact on the smaller model. This indicates that directly using generated data might not yield the best results and demonstrates the effectiveness of our APIGen framework in filtering out low-quality data.

These results provide compelling evidence for the effectiveness of the APIGen framework in generating high-quality, diverse datasets for function-calling tasks. The impressive performance achieved by our small-sized models highlights the efficiency of our approach, demonstrating that by focusing on data quality and diversity, we can effectively boost the performance of smaller models, making them competitive with much larger ones in this function-calling agent domain.

## 6 Conclusion

In this paper, we introduced APIGen, a novel framework that generates reliable and diverse function-calling datasets by leveraging a multi-stage verification process. Our experiments demonstrate the effectiveness of APIGen in producing high-quality datasets from a wide range of executable APIs. This has significant implications for the development of more efficient and accessible language models, as it shows that high-quality data can be as important as model size in achieving strong performance. By enabling smaller models to achieve competitive results and significantly enhancing their function-calling capabilities, our approach and released dataset open up new possibilities for the development of efficient and powerful language models in the agent tool-use domains.

However, the current version of APIGen and the generated dataset have some limitations. Presently, the framework and dataset only consider REST APIs and Python functions. Additionally, although APIGen is a general framework, it currently only implements the generation procedure for single-turn function calling. Future work will focus on extending APIGen to support more scenarios, programming languages, and APIs. We also plan to extend the framework to handle multi-turn and more complex interactions between agents, human users, and tools. Despite these limitations, we believe that APIGen and the generated dataset represent a significant step forward in the development of efficient and effective function-calling agents.