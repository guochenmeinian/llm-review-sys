# Many-body Approximation for Non-negative Tensors

Kazu Ghalamkari

RIKEN AIP

kazu.ghalamkari@riken.jp

Mahito Sugiyama

National Institute of Informatics

SOKENDAI

mahito@nii.ac.jp

&Yoshinobu Kawahara

Osaka University

RIKEN AIP

kawahara@ist.osaka-u.ac.jp

###### Abstract

We present an alternative approach to decompose non-negative tensors, called _many-body approximation_. Traditional decomposition methods assume low-rankness in the representation, resulting in difficulties in global optimization and target rank selection. We avoid these problems by energy-based modeling of tensors, where a tensor and its mode correspond to a probability distribution and a random variable, respectively. Our model can be globally optimized in terms of the KL divergence minimization by taking the _interaction between variables (that is, modes)_, into account that can be tuned more intuitively than ranks. Furthermore, we visualize interactions between modes as _tensor networks_ and reveal a nontrivial relationship between many-body approximation and low-rank approximation.We demonstrate the effectiveness of our approach in tensor completion and approximation.

## 1 Introduction

Tensors are generalizations of vectors and matrices. Data in fields such as neuroscience , bioinformatics , signal processing , and computer vision  are often stored in the form of tensors, and features are extracted from them. _Tensor decomposition_ and its non-negative version  are popular methods to extract features by approximating tensors by the sum of products of smaller tensors. These methods usually seek to minimize the reconstruction error, the difference between an original tensor and the tensor reconstructed from obtained smaller tensors.

In tensor decomposition approaches, a _low-rank structure_ is typically assumed, where a given tensor is essentially represented by a linear combination of a small number of bases. Such decomposition requires two kinds of information: structure and the number of bases used in the decomposition. The structure specifies the type of decomposition such as CP decomposition  and Tucker decomposition . _Tensor networks_, which were initially introduced in physics, have become popular in the machine learning community recently because they help intuitive and flexible model design, including tensor train decomposition , tensor ring decomposition , and tensor tree decomposition . Traditional tensor structures in physics, such as MERA  and PEPS , are also used in machine learning. The number of bases is often referred to as the rank. Larger ranks increase the capability of the model while increasing the computational cost, resulting in the tradeoff problem of rank tuning that requires a careful treatment by the user. Since these low-rank structure-based decompositions via reconstruction error minimization are non-convex, which causes initial value dependence [17, Chapter 3], the problem of finding an appropriate setting of the low-rank structure is highly nontrivialin practice as it is hard to locate the cause if the decomposition does not perform well. As a result, in order to find proper structure and rank, the user must often perform decomposition multiple times with various settings, which is time- and memory-consuming. Although there is an attempt to avoid rank tuning by approximating it by trace norms , it also requires another hyperparameter: the weights of the unfolding tensor. Hence, such an approach does not fundamentally solve the problem.

Instead of the low-rank structure that has been the focus of attention in the past, in the present paper we propose a novel formulation of non-negative tensor decomposition, called _many-body approximation_, that focuses on the relationship among modes of tensors. The structure of decomposition can be naturally determined based on the existence of the interactions between modes. The proposed method requires only the decomposition structure and does not require the rank value, which traditional decomposition methods also require as a hyperparameter and often suffer to determine.

To describe interactions between modes, we follow the standard strategy in statistical mechanics that uses an energy function \(()\) to treat interactions and considers the corresponding distribution \((-())\). This model is known to be an energy-based model in machine learning  and is exploited in tensor decomposition as Legendre decomposition . Technically, it parameterizes a tensor as a discrete probability distribution and reduces the number of parameters by enforcing some of them to be zero in optimization. We explore this energy-based approach further and discover the family of parameter sets that represent interactions between modes in the energy function \(()\). How to choose non-zero parameters in Legendre decomposition has been an open problem; we start by addressing this problem and propose many-body approximation as a special case of Legendre decomposition. Moreover, although Legendre decomposition is not factorization of tensors in general, our proposal always offers factorization, which can reveal patterns in tensors. Since the advantage of Legendre decomposition is inherited to our proposal, many-body approximation can be achieved by convex optimization that globally minimizes the Kullback-Leibler (KL) divergence .

Furthermore, we introduce a way of representing mode interactions, that visualizes the presence or absence of interactions between modes as a diagram. We discuss the relation to the tensor network and point out that an operation called coarse-grained transformation  - in which multiple tensors are viewed as a new tensor - reveals unexpected relationship between the proposed method and existing methods such as tensor ring and tensor tree decomposition.

We summarize our contribution as follows:

* By focusing on the interaction between modes of tensors, we introduce rank-free tensor decomposition, called many-body approximation. This decomposition is achieved by convex optimization.
* We present a way of describing tensor many-body approximation called interaction representation, which is a diagram that shows interactions within a tensor and can be transformed into tensor networks.
* Many-body approximation can perform tensor completion via the \(em\)-algorithm, which empirically shows better performance than low-rank based existing methods.

## 2 Many-body Approximation for Tensors

Our proposal, many-body approximation, is based on the formulation of Legendre decomposition for tensors, which we first review in Section 2.1. Then we introduce interactions between modes and its visual representation in Section 2.2, many-body approximation in Section 2.3, and transformation of interaction representation into tensor networks in Section 2.4. In the following discussion, we consider \(D\)-order non-negative tensors with the size \((I_{1},,I_{D})\), and we denote by \([K]=\{1,2,,K\}\) for a positive integer \(K\). We assume the sum of all elements in a given tensor to be \(1\) for simplicity,

Figure 1: Interaction representations corresponding to (**a**) second- and third-order energy (**b**) two- and (**c**) three-body approximation.

while this assumption can be eliminated using the general property of the Kullback-Leibler (KL) divergence, \( D_{KL}(,)=D_{KL}(,)\), for any \(_{>0}\).

### Reminder to Legendre Decomposition and its optimization

Legendre decomposition is a method for decomposing a non-negative tensor by regarding the tensor as a discrete distribution and representing it with a limited number of parameters. We describe a non-negative tensor \(\) using parameters \(=(_{2,,1},,_{I_{1},,I_{D}})\) and its energy function \(\) as

\[_{i_{1},,i_{D}}=_{i_{1},,i_{D}})}, _{i_{1},,i_{D}}=-_{i^{}_{1}=1}^{i_{1}} _{i^{}_{D}=1}^{i_{D}}_{i^{}_{1},,i^{}_{D}}, \]

where \(_{1,,1}\) has a role of normalization. It is clear that the index set of a tensor corresponds to sample space of a distribution and the value of each entry of the tensor is regarded as the probability of realizing the corresponding index .

As we see in Equation (1), we can uniquely identify tensors from the parameters \(\). Inversely, we can compute \(\) from a given tensor as

\[_{i_{1},,i_{D}}=_{i^{}_{1}=1}^{I_{1}}_{i^{ }_{D}=1}^{I_{D}}_{i_{1},,i_{D}}^{i^{}_{1},,i^{ }_{D}}_{i^{}_{1},,i^{}_{D}} \]

using the Mobius function \(:S S\{-1,0,+1\}\), where \(S\) is the set of indices, defined inductively as follows:

\[_{i_{1},,i^{}_{D}}^{i^{}_{1},,i^{}_{D}}= 1&i_{d}=i^{}_{d}, d[D],\\ -_{d=1}^{D}_{j_{d}=i_{d}}^{i^{}_{d}-1}_{i_{1},,i_{D}}^{ j_{1},,j_{D}}&i_{d} i^{}_{d}, d[D],\\ 0&.\]

Since distribution described by Equation (1) belongs to the exponential family, \(\) corresponds to natural parameters of the exponential family, and we can also identify each tensor by expectation parameters \(=(_{2,,1},,_{I_{1},,I_{D}})\) using Mobius inversion as

\[_{i_{1},,i_{D}}=_{i^{}_{1}=i_{1}}^{I_{1}}_{i^{ }_{D}=i_{D}}^{I_{D}}_{i^{}_{1},,i^{}_{D}}, _{i_{1},,i_{D}}=_{i^{}_{1}=1}^{I_{1}} _{i^{}_{D}=1}^{I_{D}}_{i^{}_{1},,i^{}_{D}}^{i^ {}_{1},,i^{}_{D}}, \]

where \(_{1,,1}=1\) because of normalization. See Section A in Supplement for examples of the above calculation. Since distribution is determined by specifying either \(\)-parameters or \(\)-parameters, they form two coordinate systems, called the \(\)-coordinate system and the \(\)-coordinate system, respectively.

#### 2.1.1 Optimization

Legendre decomposition approximates a tensor by setting some \(\) values to be zero, which corresponds to dropping some parameters for regularization. It achieves convex optimization using the dual flatness of \(\)- and \(\)-coordinate systems. Let \(B\) be the set of indices of \(\) parameters that are not imposed to be \(0\). Then Legendre decomposition coincides with a projection of a given nonnegative tensor \(\) onto the subspace \(}=\{_{i_{1},,i_{D}}=0\;\; \;(i_{1},,i_{D}) B\}\).

Let us consider projection of a given tensor \(\) onto \(}\). The space of probability distributions, which is not a Euclidean space, is studied in information geometry. By taking geometry of probability distributions into account, we can introduce the concept of flatness for a set of tensors. The subspace \(}\) is \(e\)-flat if the logarithmic combination, or called \(e\)-geodesic, \(\{(1-t)_{1}+t_{2}-(t) 0<t<1\}\) of any two points \(_{1},_{2}}\) is included in the subspace \(}\), where \((t)\) is a normalizer. There is always a unique point \(}\) on the \(e\)-flat subspace that minimizes the KL divergence from any point \(\).

\[}=*{arg\,min}_{; }}D_{KL}(,). \]

[MISSING_PAGE_FAIL:4]

two-body interaction between modes \((k,m)\) and the three-body interaction among modes \((k,m,p)\) in Figure 1**(a)**. Combining these interactions, the diagram for the energy function including all two-body interactions is shown in Figure 1**(b)**, and all two-body and three-body interactions are included in Figure 1**(c)** for \(D=4\). This visualization allows us to intuitively understand the relationship between modes of tensors. For simplicity, we abbreviate one-body interactions in the diagrams, while we always assume them. Once interaction representation is given, we can determine the corresponding decomposition of tensors.

In Boltzmann machines, we usually consider binary (two-level) variables and their second-order interactions. In our proposal, we consider multi-level \(D\) variables, each of which can take a natural number from \(1\) to \(I_{d}\) for \(d[D]\). Moreover, higher-order interactions among them are allowed. Therefore, our proposal is a form of a multi-level extension of Boltzmann machines with higher-order interaction, where each node of Boltzmann machines corresponds to the tensor mode. In the following section, we reduce some of \(n\)-body interactions, - that is, \(H_{i_{1},,i_{n}}^{(l_{1},,l_{n})}=0\) - by fixing each parameter \(_{i_{1},,i_{n}}^{(l_{1},,l_{n})}=0\) for all indices \((i_{1},,i_{l_{n}})\{2,,I_{l_{1}}\}\{2,, I_{l_{n}}\}\).

### Many-body approximation

We approximate a given tensor by assuming the existence of dominant interactions between the modes of the tensor and ignoring other interactions. Since this operation can be understood as enforcing some natural parameters of the distribution to be zero, it can be achieved by convex optimization through the theory of Legendre decomposition. We discuss how we can choose dominant interactions in Section 3.1.

As an example, we consider two types of approximations of a nonnegative tensor \(\) by tensors represented in Figure 1**(b, c)**. If all energies greater than second-order or those greater than third-order in Equation (7) are ignored - that is, \(H_{i_{1},,i_{n}}^{(l_{1},,l_{n})}\) = 0 for \(n>2\) or \(n>3\) - the tensor \(_{i_{1},i_{2},i_{3},i_{4}}\) is approximated as follows:

\[_{i_{1},i_{2},i_{3},i_{4}} _{i_{1},i_{2},i_{3},i_{4}}^{ 2}= _{i_{1},i_{2}}^{(1,2)}_{i_{1},i_{3}}^{(1,3)}_{i_ {1},i_{4}}^{(1,4)}_{i_{2},i_{3}}^{(2,3)}_{i_{2},i_{4}}^{(2, 4)}_{i_{3},i_{4}}^{(3,4)},\] \[_{i_{1},i_{2},i_{3},i_{4}} _{i_{1},i_{2},i_{3},i_{4}}^{ 3}= _{i_{1},i_{2},i_{3}}^{(1,2,3)}_{i_{1},i_{2},i_{4}}^{(1,2,4)}_{i_{1 },i_{3},i_{4}}^{(1,3,4)}_{i_{2},i_{3},i_{4}}^{(2,3,4)},\]

where each of matrices and tensors on the right-hand side is represented as

\[_{i_{k},i_{m}}^{(k,m)} =}H_{i_{k}}^{(k)}+H_{i _{k},i_{m}}^{(k,m)}+H_{i_{m}}^{(m)},\] \[_{i_{k},i_{m},i_{p}}^{(k,m,p)} =}}^{(k)}}{3}+ {H_{i_{m}}^{(m)}}{3}+}^{(p)}}{3}+H_{i_{k},i_{m}}^{(k, m)}+H_{i_{m},i_{p}}^{(m,p)}+H_{i_{k},i_{p}}^{(k,p)}+H_{i _{k},i_{m},i_{p}}^{(k,m,p)}.\]

The partition function, or the normalization factor, is given as \(Z=)}\), which does not depend on indices \((i_{1},i_{2},i_{3},i_{4})\). Each \(^{(k,m)}\) (resp. \(^{(k,m,p)}\)) is a factorized representation for the relationship between \(k\)-th and \(m\)-th (resp. \(k\)-th, \(m\)-th and \(p\)-th) modes. Although our model can be

Figure 2: **(a)**(left) Interaction representation of an example of cyclic two-body approximation and **(a)**(right) its transformed tensor network for \(D=4\). Each tensor is enclosed by a square and each mode is enclosed by a circle. A black circle \(\) is a hyper diagonal tensor. Edges through \(\) between modes mean interaction existence. **(b)** Tensor network of tensor ring decomposition. **(c)** The relationship among solution spaces of tensor-ring decomposition \(}_{}\), two-body approximation \(}_{2}\), and cyclic two-body approximation \(}_{}\).

transformed into a linear model by taking the logarithm, our convex formulation enables us to find the optimal solution more stable than traditional linear low-rank-based nonconvex approaches. Since we do not impose any low-rankness, factorized representations, such as \(^{(k,m)}\) and \(^{(k,m,p)}\), can be full-rank matrices or tensors.

For a given tensor \(\), we define its _\(m\)-body approximation_\(^{ m}\) as the optimal solution of Equation (4) - that is, \(^{ m}=_{}_{m}}D_{KL}( ,)\) - where the solution space \(}_{m}\) is given as

\[}_{m}=}{_{i_{1},,i_{D}}=0}_{i_{1},,i_{D}}n(>m)}.\]

Note that \(^{ D}=\) always holds. For any natural number \(m<D\), it holds that \(}_{m}}_{m+1}\). Interestingly, the two-body approximation for a non-negative tensor with \(I_{1}==I_{D}=2\) is equivalent to approximating the empirical distribution with the fully connected Boltzmann machine.

In the above discussion, we consider many-body approximation with all the \(n\)-body parameters, while we can relax this constraint and allow the use of only a part of \(n\)-body parameters in approximation. Let us consider the situation where only one-body interaction and two-body interaction between modes \((d,d+1)\) exist for all \(d[D]\) (\(D+1\) implies \(1\) for simplicity). Figure 2(**a**) shows the interaction representation of the approximated tensor. As we can confirm by substituting \(0\) for \(H^{(k,l)}_{i_{k},i_{l}}\) if \(l k+1\), we can describe the approximated tensor as

\[_{i_{1},,i_{D}}^{}_{i_{1},,i _{D}}=^{(1)}_{i_{1},i_{2}}^{(2)}_{i_{2},i_{3}} ^{(D)}_{i_{D},i_{1}}, \]

where

\[^{(k)}_{i_{k},i_{k+1}}=}(H^{ (k)}_{i_{k}}+H^{(k,k+1)}_{i_{k},i_{k+1}}+H^{(k+1)}_{i_{k+1}})\]

with the normalization factor \(Z=(-_{1,,1})\). When the tensor \(\) is approximated by \(^{}\), the set \(B\) contains only all one-body parameters and two-body parameters \(^{(d,d+1)}_{i_{d},i_{d+1}}\) for \(d[D]\). We call this approximation _cyclic two-body approximation_ since the order of indices in Equation (9) is cyclic. It holds that \(}_{}}_{2}\) for the solution space of cyclic two-body approximation \(}_{}\) (Figure 2(**c**)).

### Connection to tensor networks

Our tensor interaction representation is an undirected graph that focuses on the relationship between modes. By contrast, tensor networks, which are well known as diagrams that focus on smaller tensors after decomposition, represent a tensor as an undirected graph, whose nodes correspond to matrices or tensors and edges to summation over a mode in tensor products .

We provide examples in our representation that can be converted to tensor networks, which implies a tight connection between our representation and tensor networks. For the conversion, we use a hyper-diagonal tensor \(\) defined as \(_{ijk}=_{ij}_{jk}_{ki}\), where \(_{ij}=1\) if \(i=j\) and \(0\) otherwise. The tensor \(\) is often represented by \(\) in tensor networks. In the community of tensor networks, the tensor \(\) appears in the CNOT gate and a special case of the Z spider . The tensor network in Figure 2(**a**) represents Equation (9) for \(D=4\).

A remarkable finding is that the converted tensor network representation of cyclic two-body approximation and the tensor network of tensor ring decomposition, whose tensor network is shown in Figure 2(**b**), have a similar structure, despite their different modeling. If we consider the region enclosed by the dotted line in Figure 2(**a**) as a new tensor, the tensor network of the cyclic two-body approximation coincides with the tensor network of the tensor ring decomposition shown in Figure 2(**b**). This operation, in which multiple tensors are regarded as a new tensor in a tensor network, is called coarse-graining transformation .

Formally, cyclic two-body approximation coincides with tensor ring decomposition with a specific constraint as described below. Non-negative tensor ring decomposition approximates a given tensor \(^{I_{1} I_{D}}_{ 0}\) with \(D\) core tensors \(^{(1)},,^{(D)}^{R_{d-1} I_{d} R_{d}}_{  0}\) as

\[_{i_{1},,i_{D}}_{r_{1}=1}^{R_{1}}_{r_{D}= 1}^{R_{D}}^{(1)}_{r_{D},i_{1},r_{1}}^{(D)}_{r_{D-1},i_{D},r_{D} }, \]where \((R_{1},,R_{D})\) is called the tensor ring rank. The cyclic two-body approximation also approximates the tensor \(\) in the form of Equation (10), imposing an additional constraint that each core tensor \(^{(d)}\) is decomposed as

\[^{(d)}_{r_{d-1},i_{d},r_{d}}=_{m_{d}=1}^{I_{d}}^{(d)}_{r_{d-1 },m_{d}}_{m_{d},i_{d},r_{d}} \]

for each \(d[D]\). We assume \(r_{0}=r_{D}\) for simplicity. We obtain Equation (9) by substituting Equation (11) into Equation (10).

This constraint enables us to perform convex optimization. This means that we find a subclass \(}_{}\) that can be solved by convex optimization in tensor ring decomposition, which has suffered from the difficulty of non-convex optimization. In addition, this is simultaneously a subclass of two-body approximation, as shown in Figure 2(**c**).

From Kronecker's delta \(\), \(r_{d}=i_{d}\) holds in Equation (11); thus, \(^{(d)}\) is a tensor with the size \((I_{d-1},I_{d},I_{d})\). Tensor ring rank after the cyclic two-body approximation is \((I_{1},,I_{D})\) since the size of core tensors coincides with tensor ring rank. This result firstly reveals the relationship between Legendre decomposition and low-rank approximation via tensor networks.

Here we compare the number of parameters of cyclic two-body approximation and that of tensor ring decomposition. The number of elements of an input tensor is \(I_{1}I_{2} I_{D}\). After cyclic two-body approximation, the number \(|B|\) of parameters is given as

\[|B|=1+_{d=1}^{D}(I_{d}-1)+_{d=1}^{D}(I_{d}-1)(I_{d+1}-1), \]

where we assume \(I_{D+1}=I_{1}\). The first term is for the normalizer, the second for the number of one-body parameters, and the final term for the number of two-body parameters. In contrast, in the tensor ring decomposition with the target rank \((R_{1},,R_{D})\), the number of parameters is given as \(_{d=1}^{D}R_{d}I_{d}R_{d+1}\). The ratio of the number of parameters of these two methods is proportional to \(I/R^{2}\) if we assume \(R_{d}=R\) and \(I_{d}=I\) for all \(d[D]\) for simplicity. Therefore, when the target rank is small and the size of the input tensor is large, the proposed method has more parameters than the tensor ring decomposition. The correspondence of many-body approximation to existing low-rank approximation is not limited to tensor ring decomposition. We also provide another example of the correspondence for tensor tree decomposition in Section C.2 in Supplement.

### Many-body approximation as generalization of mean-field approximation

Any tensor \(\) can be represented by vectors \(^{(1)},,^{(D)}^{I_{d}}\) as \(_{i_{1},,i_{D}}=x^{(1)}_{i_{1}}x^{(2)}_{i_{2}} x^{(D)}_ {i_{D}}\) if and only if all \(n( 2)\)-body \(\)-parameters are \(0\)[10; 12]. The right-hand side is equal to the Kronecker product of \(D\) vectors \(^{(1)},,^{(D)}\); therefore, this approximation is equivalent to the rank-\(1\) approximation since the rank of the tensor that can be represented by the Kronecker product is always 1, which is also known to correspond to mean-field approximation. In this study, we propose many-body approximation by relaxing the condition for the mean-field approximation that ignores \(n( 2)\)-body interactions. Therefore many-body approximation is generalization of rank-\(1\) approximation and mean-field approximation.

The discussion above argues that the one-body approximation - that is, the rank-\(1\) approximation that minimizes the KL divergence from a given tensor - is a convex problem. Although finding the rank-\(1\) tensor that minimizes the Frobenius norm from a given tensor is known to be an NP-hard problem , it has been reported that finding the rank-\(1\) tensor that minimizes the KL divergence from a given tensor is a convex problem . Therefore, our claim is not inconsistent with those existing studies. It should be also noted that, except for the rank-\(1\) case, there is no guarantee that traditional low-rank decomposition that optimizes the KL divergence is a convex problem [5; 13].

### Low-body tensor completion

Since our proposal is an alternative to the existing low-rank approximation, we can replace it with our many-body approximation in practical applications. As a representative application, we demonstrate missing value estimation by the \(em\)-algorithm , where we replace the \(m\)-step of low-rank approximation with many-body approximation. We call this approach low-body tensor completion (LBTC). The \(em\)-algorithm with tensor low-rank approximation does not guarantee the uniqueness of the \(m\)-step because the model space is not flat, while LBTC ensures that the model space is flat, and thus the \(m\)-step of LBTC is unique. See more discussion of information geometry for LBTC in Section C.4. We provide its pseudo-code in Algorithm 1. Instead of hyper-parameters related to ranks that traditional tensor completion methods require, LBTC requires only information about interactions to be used, but it can be intuitively tuned, as also seen in Section 3.1. We examine the performance of LBTC in Section 3.2.

## 3 Experiments

We conducted three experiments to see the usefulness and effectiveness of many-body approximation. Datasets and implementation details are available in Supplement.

### An example of tuning interaction

We extracted 10 color images from Columbia Object Image Library (COIL-100)  and constructed a tensor \(_{ 0}^{40 40 3 10}\), where each mode represents the width, height, color, or image index, respectively. Then we decomposed \(\) with varying chosen interactions and showed the reconstructed images. Figure 3(**a**) shows \(^{ 4}\), which coincides with the input tensor \(\) itself since its order is four. Reconstruction quality gradually dropped when we reduced interactions from Figure 3(**a**) to Figure 3(**d**). We can explicitly control the trend of reconstruction by manually choosing interaction, which is one of characteristics of the proposal. In Figure 3(**c**), each reconstructed image has a monotone taste because the color is not directly interacted with the pixel position (width and height). In Figure 3(**d**), although the whole tensor is monotone because the color is independent of any other modes, the reconstruction keeps the objects' shapes since the interaction among width, height, and image index are retained. See Figure 7 in Supplement for the role of the interaction. Interactions between modes are interpretable, in contrast to the rank of tensors, which is often difficult to interpret. Therefore, tuning of interactions is more intuitive than that of ranks.

We also visualize the interaction between color and image as a heatmap of a matrix \(^{(c,i)}^{3 10}\) in Figure 3(**e**), which is obtained in the case of Figure 3(**c**). We normalize each column of \(^{(c,i)}\) with the

    & **Scenario 1** & **Scenario 2** & **Scenario 3** \\  LBTC & **0.948\(\)**0.00152 & **0.917\(\)**0.000577 & **0.874\(\)**0.00153 \\ HaLRTC & 0.864 & 0.842 & 0.825 \\ SiLRTC & 0.863 & 0.841 & 0.820 \\ SiLRTCTT & 0.948 & 0.915 & 0.868 \\ PTRCRW & 0.945\(\)0.0000 & 0.901\(\)0.0000 & 0.844\(\)0.0000 \\   

Table 1: Recovery fit score for tensor completion.

L2 norm for better visualization. As expected, indices corresponding to red objects (that is, 1, 4, and 7) have large values in the third red-color row. We also visualize the interaction among width, height, and image index, which corresponds to a tensor \(^{(w,h,i)}^{40 40 10}\) without any information of colors, as a series of heatmaps in Figure 3**(**f**). They provide the gray-scale reconstruction of images. The above discussion implies that many-body approximation captures patterns or features of inputs.

### Tensor completion by many-body approximation

To examine the performance of LBTC, we conducted experiments on a real-world dataset: the traffic speed dataset in District 7, Los Angeles County, collected from PeMS . This data was obtained from fixed-point observations of vehicle speeds every five minutes for 28 days. We made \(28 24 12 4\) tensor \(\), whose modes correspond to date, hour, minute, and lane. As practical situations, we assumed that a disorder of a lane's speedometer causes random and continuous deficiencies. We prepared three patterns of missing cases: a case with a disorder in the speedometer in Lane 1 (Scenario 1), a case with disorders in the speedometers in Lanes 1, 2, and 3 (Scenario 2), and a case with disorders in all speedometers (Scenario 3). The missing rates for scenarios 1, 2, and 3 were 9 percent, 27 percent, and 34 percent, respectively. We evaluate the recovery fit score \(1-\|_{}-}_{}\|_{F}/\|_{ }\|_{F}\) for the reconstruction \(}\) and missing indices \(\).

In the proposal, we need to choose interactions to use. Intuitively, the interaction between date and minute and that between minute and lane seem irrelevant. Therefore, we used all the two-body interactions except for the above two interactions. In addition, the interaction among date, hour, and minute is also included because it is intuitively relevant.

As baseline methods, we use SiLRTC, HaLRTC , SiLRTC-TT , and PTRCRW . SiLRTC-TT and PTRCRW are based on tensor train and ring decompositions, respectively, and PTRCRW is known as the state of the art. The reconstructions are available in Figure 8 in Supplement with ground truth. We tuned hyperparameters of baseline methods and compared their best results with LBTC. The resulting scores (see Table 1) show that our method is superior to other methods. Moreover, we can intuitively tune interactions in the proposal, while other baselines need typical hyperparameter tuning as seen in Figure 14 in Supplement. In our method, interactions among non-associated modes could worsen the performance as seen in Table 2 in Supplement. Since the proposal and PTRCRW are non-convex optimizations, we ran each of them 10 times with random initialization and reported mean values and the standard error. Many-body approximation itself is always convex, while our tensor completion algorithm LBTC is non-convex due to the use of the \(em\)-algorithm.

Figure 3: **(a,b) Four-body and three-body approximation for the COIL dataset. (c,d) Appropriate selection of interactions controls the richness of color. Visualization of interaction between (e) color and image index, and (f) width, height, and image index.**

### Comparison with ring decomposition

As seen in Section 2.4, many-body approximation has a close connection to low-rank approximation. For example, in tensor ring decomposition, if we impose the constraint that decomposed smaller tensors can be represented as products with hyper-diagonal tensors \(\), this decomposition is equivalent to a cyclic two-body approximation. Therefore, to examine our conjecture that cyclic two-body approximation is as capable of approximating as tensor ring decomposition, we empirically examined the effectiveness of cyclic two-body approximation compared with tensor ring decomposition. As baselines, we used five existing methods of non-negative tensor ring decomposition: NTR-APG, NTR-HALS, NTR-MU, NTR-MM, and NTR-lraMM [39; 40]. We also describe the efficiency in Supplement.

We evaluated the approximation performance by the relative error \(\|-}\|_{F}/\|\|_{F}\) for an input tensor \(\) and a reconstructed tensor \(}\). Since all the existing methods are based on nonconvex optimization, we plotted the best score (minimum relative error) among five restarts with random initialization. In contrast, the score of our method is obtained by a single run as it is convex optimization and such restarts are fundamentally unnecessary.

**Synthetic data** We performed experiments on two synthetic datasets with low tensor ring rank. We create sixth- and seventhth-order tensors with ring ranks \((15,...,15)\) and \((10,...,10)\), respectively. In Figure 4(**a**), relative errors are plotted with gradually increasing the target rank of the tensor ring decomposition, which is compared to the score of our method, plotted as the cross point of horizontal and vertical red dotted lines. Please note that our method does not have the concept of the rank, so the score of our method is invariant to changes of the target rank unlike other methods. If the cross point of red dotted lines is lower than other lines, the proposed method is better than other methods. In both experiments, the proposed method is superior to comparison partners in terms of the reconstruction error.

**Real data** Next, we evaluated our method on real data. TT_ChartRes and TT_Origami are seventh-order tensors that are produced from TokyoTech Hyperspectral Image Dataset [23; 24]. Each tensor has been reshaped to reduce the computational complexity. As seen in Figure 4(**b**), the proposed method keeps the competitive relative errors. In baselines, a slight change of the target rank can induce a significant increase of the reconstruction error due to the nonconvexity. These results mean that we eliminate the instability of non-negative tensor ring decomposition by our convex formulation.

## 4 Conclusion

We propose _many-body approximation_ for tensors, which decomposes tensors by focusing on the relationship between modes represented by an energy-based model. This method approximates and regularizes tensors by ignoring the energy corresponding to certain interactions, which can be viewed as a generalized formulation of mean-field approximation that considers only one-body interactions. Our novel formulation enables us to achieve convex optimization of the model, while the existing approaches based on the low-rank structure are non-convex and empirically unstable with respect to the rank selection. Furthermore, we introduce a way of visualizing activated interactions between modes, called interaction representation. We have demonstrated transformation between our representation and tensor networks, which reveals the nontrivial connection between many-body approximation and the classical tensor low-rank tensor decomposition. We have empirically showed the intuitive model's designability and the better usefulness in tensor completion and approximation tasks compared to baseline methods.

**Limitation** Proposal only works on non-negative tensors. We have examined the effectiveness of LBTC on only traffic datasets.

Figure 4: Experimental results for (**a**) synthetic data and (**b**) real datasets. The vertical red dotted line is \(|B|\) (see Equation (12)).