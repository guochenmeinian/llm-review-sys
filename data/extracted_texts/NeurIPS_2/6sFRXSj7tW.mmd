# Multimodal Lego: Model Merging and Fine-Tuning Across Topologies and Modalities

Konstantin Hemker

Department of Computer Science & Technology

University of Cambridge

Cambridge, United Kingdom

konstantin.hemker@cl.cam.ac.uk

Nikola Simidjievski

Department of Oncology

University of Cambridge

Cambridge, United Kingdom

ns779@cam.ac.uk

Mateja Jamnik

Department of Computer Science & Technology

University of Cambridge

Cambridge, United Kingdom

mateja.jamnik@cl.cam.ac.uk

###### Abstract

Learning holistic computational representations in physical, chemical or biological systems requires the ability to process information from different distributions and modalities within the same model. While there are many available multimodal fusion and alignment approaches, most of them require end-to-end training, scale quadratically with the number of modalities, cannot handle cases of high modality imbalance in the training set, or are highly topology-specific, making them too restrictive for many biomedical learning tasks. This paper presents _Multimodal Lego_ (MM-Lego), a general-purpose fusion framework to turn any set of encoders into a competitive multimodal model with no or minimal fine-tuning. We achieve this by introducing a wrapper for any unimodal encoders that enforces shape consistency between modality representations and harmonises these representations by learning features in the frequency domain to enable model merging with little signal interference. We show that MM-Lego 1) can be used as a _model merging_ method which achieves competitive performance with end-to-end fusion models _without any fine-tuning_, 2) can operate on any unimodal encoder, and 3) is a _model fusion_ method that, with minimal fine-tuning, achieves state-of-the-art results on six benchmarked multimodal biomedical tasks.

## 1 Introduction

The utility and demand for multimodal machine learning approaches has sharply risen due to their potential to derive holistic representations in various systems, including physics , chemistry , neuroscience , or biology . Multimodal models in the vision & language domains leverage the same data distributions, which are represented across different modalities [5; 6; 7], such as vision-text pairs of the same concepts. However, in many biomedical domains, modalities represent data at different scales (e.g., cellular, genomic, transcriptomic, etc.), cardinalities that are not paired (e.g., many single-cell reads for a single tissue slide per patient), and follow separate distributions. While large foundation models have excelled in tasks confined to individual modalities [8; 9; 10], training these models across modalities is an expensive end-to-end process, that requires paired modalities. One recently emergent solution to these challenges is presented through _model merging_ (also referred to as _knowledge fusion_), an approach commonly used in the context of multi-task settings and languagemodelling, which capitalises on combining well-performing unimodal models trained in isolation. Model merging methods attempt to combine two architecturally identical models trained on different distributions through interpolation, arithmetic manipulation and aggregation of their weights , or stacking their layers , often without additional training/fine-tuning. While model merging has been extended to some multimodal vision and language tasks , its crucial challenges in a multimodal setting are that: a) the merged components are still trained in isolation, and b) we cannot assume topological equivalence between two models for separate modalities due to their separate input shapes.

In this paper, we present Multimodal Lego (_MM-Lego_) - a flexible framework for combining various unimodal models into a multimodal model with no or minimal fine-tuning (Figure 1). We introduce two approaches within our framework - _LegoFuse_ and _LegoMerge_, enabling performant multimodal models given a set of unimodal encoders with either little (_LegoFuse_) or no (_LegoMerge_) fine-tuning. We show that MM-Lego satisfies multiple desirable properties in a range of real-world multimodal applications combining imaging, tabular, and time series modalities. We demonstrate the utility of MM-Lego on seven medical datasets across three separate downstream tasks, showing that it is: 1) performant without end-to-end training, 2) topology agnostic, 3) is scalable, and 4) handles modality imbalance and non-overlapping sets.

**Fusion methods.** Given multiple data inputs or latent representations, fusion methods construct single learning representations that can be used for a downstream task, often whilst reducing dimensionality. Many fusion methods  first learn a set of modality-specific encoders \(=\{g_{m}:m^{(m)}\}\) assuming a single task label \(\). This results in a set of latent representations \(=\{g_{m}(m),m\}\), which are combined with a fusion operator to obtain the final fused representation \(=()\) and its final prediction \(}=f()\). Following this problem setup, fusion methods can be differentiated by: 1) the choice of the _fusion operator_\(()\); 2) the _fusion stage_ in the pipeline of when \(()\) is applied; and 3) the _fusion order_ in which the fusion operations are applied (i.e., sequential vs. parallel). The _fusion operator_ can be either static (e.g., concatenation , Kronecker product ) or learnable (e.g., low-rank tensor fusion , cross-attention mechanisms , mixture of experts ). The _fusion stage_ is typically characterised as early, intermediate or late fusion. Early fusion methods apply a static fusion operator \(()\) to the raw data while only applying this after passing each modality through \(\). Intermediate fusion methods often don't apply a static aggregation but rather learn a fusion function (i.e., a small sub-network or neural layer) in the latent space as part of its end-to-end training .

**Model merging.** The core idea behind model merging, typically deployed in multi-task settings, is that earlier layers in a network may learn similar features that may be used across tasks . Using linear interpolation  or arithmetic manipulation  of the task-specific weights, model merging approaches have shown that they can effectively generalise to new tasks without any fine-tuning.

Figure 1: The Multimodal Lego workflow to turn a set of encoders into a performant multimodal model. _LegoBlock_ (1) makes unimodal encoders compatible with model merging techniques by learning a latent representation in the frequency-domain to prevent signal interference effects upon aggregation. Any set of _LegoBlocks_ can be merged into a multimodal model without any fine-tuning (_LegoMerge_ (2a)) or with minimal fine-tuning to achieve state-of-the-art performance (_LegoFuse_ (2b)).

Formally, given multiple tasks \(^{()}\) for the same modality \(A\), they first learn the set of task-specific functions \(=\{f_{t}(_{t}):A}^{(t)} t \}\) where \(\) denotes the corresponding model parameters. Assuming the same architecture for each model in \(\), parameters from a pre-trained base model \(_{base}\) can be used to derive task vectors as \(=\{_{t}_{t}-_{base} t\}\). Given these task vectors, a multi-task model can be constructed by updating the weights of the base model \(^{}=_{base}+_{t}^{}_{t}\). This idea has been extended by the TIES  and DARE  that merge models through sparsifying and resolving sign conflicts in the task vectors. Another popular approach is spherical linear interpolation (SLERP), a method used to smoothly interpolate between two vectors while respecting the geometric properties of the vector space . More specifically, given model parameters \(_{T_{1}}\) and \(_{T_{2}}^{d}\), derived from two models with identical architectures, the merged multi-task model parameters can be calculated as \(^{}=_{T_{1}}+ _{T_{2}}\) where \(\) is the radian between the two vectors \(_{T_{1}}\) and \(_{T_{2}}\). The underlying assumption of the above model merging approaches is that the models should have an equivalent network topology, ensuring that the dimensions \(^{d}\) match up between tasks. However, while this is an acceptable constraint for multi-task learning, it is infeasible for multimodal models where modality shapes and the corresponding network topologies vary greatly.

## 2 Multimodal Lego

**Preliminaries.** Let \(^{()}=_{m}x_{m}\) be a multimodal dataset where \(=\{A,B,\}\) represents the set of modalities \(m\) such as images (\(A\)), tabular data (\(B\)), time series (\(C\)), etc. Let \(^{(A)}_{i,j,k}\) correspond to the element in the dataset for modality \(A\) at sample \(i\), column \(j\), and channel \(k\), assuming \(A^{I J K}\) where \(1 i N\), \(1 j J\), \(1 k K\). Each sample in \(\) has a set of discriminative task labels \(^{()}=_{t}^{(t)}\), where \(=\{T_{1},T_{2},T_{c}\}\) is the set of possible tasks such that \(^{(T_{1})}=\{y_{1}^{T_{1}},y_{2}^{T_{1}},...,y_{N}^{T_{1}}\}\) are the scalar target values for task \(T_{1}\) for \(N\) samples.

**Architecture.** Rather than learning a single fusion operator \(()\) that applies to all latent representations at once, we learn a set of latent update functions for each modality, in the form of

\[=\{_{m}:(g_{m}(X^{(m)}),L_{s}^{(m)}) L_{s+1}^{(m)}  s S,m\}, \]

where \(L_{t}^{(m)}\) is our target latent representation for each modality that we will later use in the merge and fusion, and \(S^{+}\) is the number of update steps.

Using the iterative update architecture with latent state passing in Equation 1 has a number of advantages. First, iterative attention architectures have been shown to be highly generalisable across modalities , and effective at dealing with missing individual modalities [32; 33]. Second, since all modalities are encoded into a self-defined latent representation, we can impose a dimensionality constraint such that each latent has the same dimensions (e.g., \(L^{(A)},L^{(B)},L^{(C)}^{c d}\) for latent channels and dimensions \(c\) and \(d\)). Third, we can do latent state passing between elements in \(\), which allows us to "stack" the update functions on top of each other (hence the name) to sequentially encode each modality's signal into the same latent representation.

**LegoBlocks.** Each element in \(\) represents a _LegoBlock_, which learns the latent update function \(_{m}\) for any given encoder \(g_{m}\). Acknowledging that different data modalities and structures require different inductive biases to effectively encode each modality's information (\(g_{m}\)), _LegoBlock_ acts as a wrapper to accurately encode its latent \(h_{m}\) into \(L^{(m)}\). The benefit of training each modality update function separately instead of end-to-end is that we can train on entirely separate samples for the same tasks. For example, in many medical domains, we may have single-cell data for one subset of patients and bulk sequencing data for a different subset, while having the same task labels for the entire set. To address this, we use a latent representations \(L\) that effectively encode signal across modalities, and are robust or invariant to transformations (shifts, rotations, etc.), noise and signal interference.

This motivated us to design MM-Lego for learning latent representations in the frequency domain, taking advantage of a number of desirable properties for multimodal merging and fusion. In particular, frequency-domain representations are: 1) _signal-preserving_ as frequency features are less prone to signal interference upon aggregation (see Appendix F); 2) _distance-preserving_, as the Euclidean distance between two signals remains unchanged after the Fourier Transform (following from Parseval's Theorem ), making them suitable for distance-based loss functions; 3) _invertible_ as the spatial/temporal domain can be reconstructed, allowing for the iterative updates outlined in Equation 1; and 4) _efficient_, as the Fast Fourier Transform (FFT) has a time complexity of \(O(n\ log(n))\), making it scalable to very large datasets .

Starting with the latent representation in the spatial domain, we first apply a discrete FFT \(()\) along each dimension of the 2D Tensor to yield a frequency domain representation. \(L_{t}^{}(u,v)=_{i=0}^{c-1}_{j=0}^{d-1}L_{t}(i,j)e^{-2 i( +)}\), where \(i,j\) denote the spatial-domain indices, and \(u,v\) denote the frequency-domain indices. This results in a complex frequency-domain representation from which we separate the real (symmetrical) and imaginary (asymmetrical) components of the FFT (\((L_{t}^{})^{r}\) and \((L_{t}^{})^{i}\)) . We update the real component using a standard cross-attention layer , where we aim to learn the weight matrices \(W_{m}^{q}\) for the update query \((L_{t}^{})^{r}\), and \(W_{m}^{k}\), \(W_{m}^{v}\) for the keys and values (\(h^{(A)}\)) resulting in the latent update:

\[(L_{t+1}^{})^{r}=(^{})^{r }W_{m}^{q}(h^{(A)}W_{m}^{k})^{}}{}})(h^{(A)}W_{ m}^{v}). \]

In contrast to other Fourier-based architectures , which only use the real component of the transform, we keep track of the imaginary component \((L_{t}^{})^{i}\) as well. This allows us to reconstruct the complex representation, and subsequently apply the inverse transform. We found this to be critical for our iterative architecture, as otherwise the signal gets distorted and we lose phase information (encoded in the imaginary component) at each update pass. Once we reconstruct the complex representation, we apply the inverse transform to recover the spatial representation in preparation of the next pass \(L_{t+1}=^{-1}((L_{t+1}^{})^{r}+i(L_{t}^{})^{i})\). Finally, the last task-specific heads of each block are a fully-connected layer after applying layer normalisation. We omit the inverse transform after the last update such that each head is trained in the frequency domain. This ensures that we can apply aggregations with low signal interference on \(\) during _LegoMerge_.

**LegoMerge.**_LegoMerge_ constructs a performant multimodal model without any additional training. With the architectural assumptions imposed on each modality encoder in \(\) through _LegoBlocks_\(\), we can apply model merging techniques in a multimodal setting. With \(^{c d}\) and each element in \(\) being in the frequency domain, we can use aggregation functions \(()\), which are less prone to cancelling out signal. For example, let \(L^{(A)}\) and \(L^{(B)}\) be the final frequency domain latent representations for modalities \(A\) and \(B\), then we can calculate a merged multimodal representation as:

\[(L^{(A)},L^{(B)})=(||L^{(B)}|}{|L^{(B)}|+|L^{(A)}|})  e^{i(L^{(A)}+^{(B)}}{2}}, \]

where the real component is the harmonic mean of the magnitudes (\(||\)), and the imaginary component is the arithmetic mean of the phases (\(\)) of \(L^{(A)}\) and \(L^{(B)}\). We take the harmonic mean since it is less prone to outliers , that is, the merged representation is less likely to be strongly skewed towards either modality by very large frequency components. With the cross-modal combined representation \(L^{()}\), we need to combine the task heads of each block, where we apply spherical linear interpolation (SLERP)  for the set of task heads \(\) from each element in \(\).

**LegoFuse.**_LegoFuse_ overcomes the limitations of LegoMerge of training each element in \(\) in isolation, thus allowing for modalities to mutually contextualise each other. As such it requires a minimal amount of fine-tuning. To avoid fine-tuning a potentially noised signal emerging from the merged latent \(L^{()}\), _LegoFuse_ operates at the layer level (by sequentially passing through all layers in \(\)), rather than directly fine-tuning the merged model (at the parameter-level). Specifically, the shape consistency introduced by \(^{c d}\) allows the stacked model to pass the Fourier-transformed latent states either between blocks (stacking) or different layers between blocks (weaving), as illustrated in Figure 1. We then fine-tune the stacked/weaved model for a few epochs with all (paired) modalities, such that the state updates are conditioned on all modalities' updates. This, in turn, becomes the query for the cross-attention layer. Note that, both the stacked and weaved variants of _LegoFuse_ allow for fine-tuning all model parameters, including the ones of the initial modality-specific encoders.

## 3 Results & Discussion

**Experiments.** We evaluate MM-Lego (_LegoMerge_ and _LegoFuse_) and its components (_LegoBlock_) on seven multimodal medical datasets covering three separate modalities (images, tabular, time series)

[MISSING_PAGE_FAIL:5]