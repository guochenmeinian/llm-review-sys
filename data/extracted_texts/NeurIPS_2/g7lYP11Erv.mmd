# Point-PRC: A Prompt Learning Based Regulation Framework for Generalizable Point Cloud Analysis

Hongyu Sun\({}^{1,2}\)   Qiuhong Ke\({}^{2}\)   Yongcai Wang\({}^{1}\)

Wang Chen\({}^{1}\)   Kang Yang\({}^{1}\)   Deying Li\({}^{1}\)   Jianfei Cai\({}^{2}\)

\({}^{1}\)Department of Computer Science, Renmin University of China, China

\({}^{2}\)Department of Data Science & AI, Monash University, Australia

Corresponding author. {sunhongyu,ycw}@ruc.edu.cn, {qiuhong.ke,jianfei.cai}@monash.edu

###### Abstract

This paper investigates the 3D domain generalization (3DDG) ability of large 3D models based on prevalent prompt learning. Recent works demonstrate the performances of 3D point cloud recognition can be boosted remarkably by parameter-efficient prompt tuning. However, we observe that the improvement on downstream tasks comes at the expense of a severe drop in 3D domain generalization. To resolve this challenge, we present a comprehensive regulation framework that allows the learnable prompts to actively interact with the well-learned general knowledge in large 3D models to maintain good generalization. Specifically, the proposed framework imposes multiple explicit constraints on the prompt learning trajectory by maximizing the mutual agreement between task-specific predictions and task-agnostic knowledge. We design the regulation framework as a plug-and-play module to embed into existing representative large 3D models. Surprisingly, our method not only realizes consistently increasing generalization ability but also enhances task-specific 3D recognition performances across various 3DDG benchmarks by a clear margin. Considering the lack of study and evaluation on 3DDG, we also create three new benchmarks, namely base-to-new, cross-dataset and few-shot generalization benchmarks, to enrich the field and inspire future research. Code and benchmarks are available at [https://github.com/auniquesum/Point-PRC](https://github.com/auniquesum/Point-PRC).

## 1 Introduction

3D point cloud data is widely adopted in many industrial and civil areas, such as autonomous driving , robotics , geospatial mapping  and entertainment games . Recognizing 3D objects from point cloud data is a basic need of these applications. Relevant research topics have been explored for a long time and their development can be summarized in three stages. In the early phase, PointNet series  sparked a wave of directly operating raw point cloud data using deep learning techniques. Later methods improved upon PointNet and PointNet++ in terms of local information aggregation , optimization techniques , geometry prior , model architecture , _etc_. Although remarkable progress has been made, these works tend to design specific architectures targeting downstream benchmarks while paying little attention to the model generalization, resulting in disappointed performances when deploying in the wild, especially in the case of unseen domains and corrupted data. On the other hand, training point cloud recognition models on each benchmark is not always feasible due to the narrow set of 3D visual concepts and expensive labeled data.

The above factors call for the investigation of the domain generalization routes for the deep point cloud models so that they can learn robust and transferable representations. Related studies have been extensively conducted in image recognition [28; 29; 31; 30; 32; 87; 13; 84] while to our best knowledge, there are only a few methods to discuss the domain adaptation and domain generalization in 3D. Several years ago, PointDAN  first investigated domain adaptation for point cloud classification models by aligning multi-scale features of 3D objects across the source and target domains. MetaSets  proposed to meta-learn on a group of transformed point sets to obtain generalizable representations to handle the sim-to-real geometry shifts. PDG  decomposed 3D objects into shared part space to reduce domain gap and developed a part-level domain generalization model for 3D point cloud classification.

However, the above methods are all built on small models (e.g., PointNet with 1.2M parameters) and small datasets (e.g., ModelNet with 9,843 training samples) and the overall transferability is still suppressed compared to prevalent large 3D foundation models [76; 90; 71; 78; 79; 72], which have been pre-trained on numerous volume of 3D data [11; 10] and demonstrated promising zero-shot capability. Recent works stand on the shoulder of large 3D foundation models and push the boundary of downstream 3d tasks by parameter-efficient adaptation, such as prompt learning [74; 58], adapter [59; 88], and their combination. They insert learnable prompts in the inputs or adapter inside the Transformer  blocks to adapt the foundation models to specific 3D tasks. However, optimizing the newly introduced small modules targeting downstream benchmarks is prone to overfitting, thus disturbing the internal representations and compromising the inherent generalization of the foundation models [85; 36; 89; 26; 24; 25]. As Fig. 1 demonstrates, lightweight prompt tuning can notably lift the recognition accuracy of representative large 3D models on seen classes while hindering the generalization on unseen new classes, where the performances consistently lag behind corresponding zero-shot predictions of these models.

In this paper, we develop our approach based on large 3D foundation models through lightweight prompt learning and propose a comprehensive framework that consists of three regulation constraints to allow the learning trajectory to interact with the well-learned knowledge in large 3D models actively, achieving better task-specific performances and task-agnostic generalization at the same time. Specifically, we propose the mutual agreement constraint to regulate the learnable prompts to produce consistent feature distributions and predictions with the pre-trained foundation models. Then, we exploit the flexible and diverse text descriptions derived from LLMs or manual templates to reflect the attributes of different classes of point clouds and enhance the generalization. Finally, we develop a weighted model ensemble strategy to update the learnable prompts smoothly and predictably, avoiding giant and unexpected leaps toward overfitting the downstream datasets. Some recent works also explore parameter-efficient tuning for point cloud analysis [74; 59; 58; 88], they focus on the performances of downstream tasks while failing to take the model generalization into account. As far as we are aware, our work initiates the first attempt to impose explicit regulation constraints and improve the 3D domain generalization based on large 3D models.

In addition, we argue existing 3D domain generalization evaluation benchmarks, such as PointDA  and Sim-to-Real , may not be comprehensive to evaluate common generalization capabilities. Only \(\)10 point cloud object classes are included in these benchmarks. They emphasize the generalization among the shared categories between the source and target domain, without considering transferring to unseen new classes, corrupted data, _etc_, which are frequent in real-world scenarios.

Figure 1: **Motivation of our research: to promote the performances on downstream 3D tasks while maintaining good generalization of large 3D models.** The experiments are conducted on ShapeNetCoreV2. ULIP-2 can reach 71.22% zero-shot recognition accuracy on this dataset. Recent works built on ULIP-2 introduce lightweight prompt tuning (PT) to further boost target tasks (75.80% accuracy). However, we observe the improvements come at the expenses of a severe drop in 3D domain generalization (e.g., 57.07% accuracy on new classes, much behind 71.22%), and develop a systematic regulation constraint (**RC**) framework to address this challenge.

In this paper, three new benchmarks are created to enrich 3D domain generalization evaluation, including _base-to-new class_ generalization, _cross-dataset_ generalization and _few-shot_ generalization. We will dissect the details of benchmark curation and usage in Section A.1. We supply comprehensive experiments and analysis to examine the proposed regulation constraint framework, ablate the effectiveness of distinct components, and draw some new insights from our newly introduced 3DDG evaluation benchmarks. The results verify the proposed method not only enhances the task-specific 3D point cloud recognition but also extends the task-agnostic generalization ability by a clear margin.

In short, the contributions of this work are threefold. _Firstly_, to our knowledge, we firstly bring the 3DDG problem in front of large multi-modal 3D models and present an effective regulation framework based on lightweight prompt tuning, which not only strengthens downstream 3D task performances but also lifts the domain generalization capability remarkably. _Secondly_, we implement our regulation framework as a plug-and-play module to seamlessly integrate into the existing large multi-modal 3D models. Consistent improvements are obtained over representative large 3D models, indicating the proposed regulation framework is general and model-agnostic. _Thirdly_, we carefully craft three new benchmarks to enrich the evaluation of 3D domain generalization. Our benchmarks introduce new evaluation dimensions for 3DDG which are vital in the real world but absent in existing ones, including base-to-new, cross-dataset, and few-shot generalization. These new and more challenging benchmarks will drive the future research of 3D domain generalization.

## 2 Related Work

**3D domain generalization**. Although domain generalization has been widely studied in image recognition [17; 28; 34; 29; 31; 30; 32; 83; 21; 87; 5; 38; 13; 84; 80], it is still not the case for 3D. A large body of works in point cloud recognition focuses on improving the performances on specific benchmarks by supervised [48; 49; 33; 67; 60; 63; 18; 69; 82; 37; 68; 50; 14; 44] or self-supervised [73; 43; 75; 57] learning. However, they lack systematic strategies to address the generalization challenge and related evaluation is absent. Only a few methods investigate the 3D domain adaptation  and domain generalization [20; 64] problem. They either create a common feature space between the source and target domain (e.g., PointDAN , PDG ), or utilize the meta-learning framework (e.g., MetaSets ) to obtain robust representations to handle the domain shifts. Nevertheless, those methods are all built on small-size point cloud encoders targeting small-scale datasets and the overall generalization is unsatisfactory. In contrast, we explore the 3D domain generalization based on representative large multi-modal 3D models, like PointCLIP series [76; 90] and ULIP series [71; 72]. Meanwhile, we do not touch the backbone and only conduct lightweight prompt tuning on those large 3D models.

**Prompt learning for large 3D models**. Prompt learning for 3D point cloud understanding has been studied in recent works [74; 58; 59; 88]. IDPT , Point-PEFT  and DAPT  explore this problem in pure point cloud modality and do not establish connections with flexible language descriptions, thus these methods cannot conduct open-vocabulary 3D recognition. PPT  firstly constructs a prompt learning pipeline based on the multi-modal framework ULIP . It achieves open-vocabulary recognition with promising performances and relatively small costs. Our work is closely related to PPT but distinguished in the following aspects: First, PPT focuses on optimizing specific 3D tasks with learnable prompts and fails to consider generalization on unseen data. Instead, our work develops systematic strategies to regulate prompt learning to boost generalization as well as target tasks. Second, PPT only introduces learnable prompts in the text branch while our method conducts multi-modal prompt tuning on both text and 3D branches.

## 3 Method

We firstly revisit lightweight prompt learning for existing large 3D models in section 3.1. Then, a comprehensive regulation framework is proposed to promote the generalization capability of large 3D models based on the plug-and-play prompt tuning strategy in section 3.2. Finally, we introduce the implementation details of the devised method in section 3.3. The overall pipeline of our method is visualized in Fig. 2. The creation and analysis of our new 3DDG benchmarks are elaborated in Appendix A.1 due to space limitation.

### Preliminary

Existing large multi-modal 3D models [76; 90; 71; 72] have different branches that encode the inputs from point cloud and text. In the 3D branch, a point cloud \(P^{N 3}\) is divided and projected into \(u\) point patches. Then, a class token \(p_{cls}^{d}\) is inserted before the patches to form the input \(=\{p_{cls},p_{1},p_{2},,p_{u}\}^{(1+u) d}\) of the 3D encoder \(f_{P}(,_{p})\), where \(_{p}\) represents the encoder parameters. In the text branch, the descriptions of each 3D category are converted into the sequence \(=\{t_{sos},t_{1},t_{2},,t_{v},t_{c},t_{os}\}^{(3+v)  d}\) for the text encoder \(f_{T}(,_{t})\). Here \(t_{c}\) is the embedding of \(\{\}\), \(t_{sos}\) and \(t_{cos}\) stands for the the start and end flag token of a sentence. So we can obtain the 3D features \(_{P}=f_{P}(,_{p})\) and text features \(_{T}=f_{T}(,_{t})\). When executing zero-shot recognition for a downstream 3D dataset of \(C\) categories, \(_{p}\) and \(_{t}\) are frozen, the model outputs the class probability distribution \(\) for point cloud \(P\) by computing \(_{P},_{T})/)}{_{i=1}^{C}(sim( _{P},_{T}^{2})/)}\), where \(sim(,)\) measures the cosine similarity of the inputs and \(\) is a temperature coefficient.

Although zero-shot inference is flexible, the performances on target tasks may not be satisfactory. Multi-modal prompt learning introduces learnable prompts in the inputs of different branches. Specifically, we insert \(r\) learnable prompts \(^{P}=\{e_{1}^{P},e_{2}^{P},,e_{P}^{P}\}^{r d}\) into \(\) and \(s\) learnable prompts \(^{T}=\{e_{1}^{T},e_{2}^{T},,e_{s}^{T}\}^{s d}\) into \(\), respectively. Thereupon, the modified inputs for point cloud and text encoder become \(}=\{p_{cls},p_{1},,p_{u},e_{1}^{P},,e_{r}^{P}\}\) and \(}=\{t_{sos},t_{1},t_{2},,t_{v},t_{c},e_{1}^{T},,e_{s} ^{T},t_{os}\}\). After transforming by the encoders, we obtain the new point cloud and text representations denoted with \(}_{P}=f_{P}(},_{p})\) and \(}_{T}=f_{T}(},_{t})\), where \(_{p}=\{_{p},^{P}\}\) and \(_{t}=\{_{t},^{T}\}\). Similarly, the predicted class distribution \(}\) and optimization objective can be formulated by Eq. 1.

\[}=}_{P},}_{ T})/)}{_{j=1}^{C}(sim(}_{P},}_{ T}^{2})/)},\{^{P*},\;^{T*}\}=*{arg\,min}_{\{^{P},\; ^{T}\}}_{(P,y)_{gt}}_{CE}(},y) \]

where \(_{gt}\) is the ground truth distribution of point cloud data and \(y\) is the category of point cloud \(P\). Note that \(_{p}\) and \(_{t}\) are still frozen and only \(E^{P}\) and \(E^{T}\) are updatable with the cross entropy (\(CE\)) loss. Also, the learnable prompts can be inserted at each layer of the 3D and text encoder, not only at the very first layer. We call this scheme deep multi-modal prompt learning that will be regarded as an important baseline in our experiment settings.

### Our Regulation Framework

Prompt learning aims to elicit well-learned knowledge of pre-trained large models by introducing a small number of learnable parameters in the input space. But optimizing the learnable prompts targeting specific datasets easily compromises the general knowledge. To handle the above problems, we propose a comprehensive regulation framework consisting of three components: mutual agreement constraint, text diversity constraint, and model ensemble constraint, as elaborated below.

Figure 2: **The overall architecture of our point cloud analysis prompt regulation constraint framework**, namely **Point-PRC**, consisting of three core components as in the figure.

#### 3.2.1 Mutual Agreement Constraint (MAC)

Large foundation models unfold overall better robustness and transferability on a broad spectrum of evaluations than conventional models learned on specific datasets, supported by representative works in vision , language , and multi-modal understanding . The first component of the proposed framework is to interact with large 3D models actively by maximizing the mutual agreement between learnable prompts and pre-trained knowledge.

Specifically, we engage with large 3D models by aligning extracted features and predicted distributions simultaneously. Let us denote the frozen point cloud feature extracted by the 3D foundation model as \(_{P}\), the point cloud feature containing learnable prompts as \(}_{P}\). Now we compute the difference between \(_{P}\) and \(}_{P}\) and mark it as \(_{p}\). Similarly, in the text modality, we have \(_{t}\) which measures the difference between \(_{T}\) and \(}_{T}\). On the other side, \(\) and \(}\) are two class distributions given by the frozen and promptable large 3D models, respectively. The difference between \(\) and \(}\) is denoted as \(_{D}\). Our mutual agreement constraint aims to minimize the feature and prediction distribution discrepancy to ensure the learning trajectory not to forget the task-agnostic knowledge in large pre-trained models.

\[_{p}=_{i}|\ _{P}^{i}-}_{P}^{i} \ |,_{t}=_{i}|\ _{T}^{i}-}_{T}^{i} \ |,_{D}=_{i}_{KL}(_{i}\ ||\ }_{i}) \]

As formulated in Eq. 2, \(_{1}\) distance is employed to compute \(_{p}\) and \(_{t}\), and Kullback-Leibler (KL) divergence is used to characterize the distribution discrepancy. We will examine the design choices in the ablation study.

#### 3.2.2 Text Diversity Constraint (TDC)

Inspired by the flexibility and versatility of language expressions, we propose to leverage diverse text descriptions to guide the lightweight prompt tuning to produce transferrable features. Specifically, we obtain multiple text descriptions for each point cloud object category by prompting LLMs (\(e.g.\), GPT-3.5 , GPT-4 , PointLLM ) or utilizing manual templates. Then, we aggregate the text feature of all descriptions for each single category by pooling operation, \(_{T}=(_{j}_{T}^{j})\), which will integrate rich semantic information extracted by powerful large models, prevent a point cloud category biasing towards some specific descriptions and finally enhance the model transferability. In the case of describing point clouds with LLMs, we design three kinds of prompts, including question answering, caption generation, and making sentences using keywords, as demonstrated in Fig. 3. For each instruction to the LLM, we acquire \(N_{t}=10\) responses.

#### 3.2.3 Model Ensemble Constraint (MEC)

The model ensemble constraint aims to synthesize the opinions from different models by weighted voting to avoid some extreme and failure cases of a single model. The idea has been widely discussed in statistical machine learning  and deep learning . Robust tuning of multi-modal large models by ensemble learning also has been studied in recent literature . The ensemble strategy mainly involves interpolating weights between zero-shot and fully fine-tuned large models. But it has not been investigated in the context of prompt tuning for large 3D models and its effectiveness is unknown. In this paper, we propose to ensemble models by aggregating the model parameters in different training epochs with a Gaussian weighted strategy. The basic idea is that in the initial learning stage, the prompts are randomly initialized and not well optimized so we distribute them very

Figure 3: **Illustration of diverse questions to LLMs, including GPT-3.5, GPT-4 and PointLLM. The responses given by LLMs are regarded as the text descriptions to the point cloud and fed into the text encoder.**small weights. As the training iterates, the model gradually gets a sense of downstream tasks; thus, increasing weights are assigned to the model parameters in these epochs. As the training ends, the learnable prompts are adjusted well to adapt downstream datasets while having the risk of overfitting, so we decrease the weights to the model parameters. The varying weights of the above process can be approximated by a gaussian curve. Finally, the weighted models in different epochs are ensembled to generate the model parameters \(_{t}\) and \(_{p}\), shown in Eq. 3.

\[_{p}=_{i=1}^{e}w_{i}_{p}^{i},_{t}=_{i=1}^{e}w_{i}_{t}^{i} \]

where \(e\) is the number of epochs and \(w_{i}=}(-}{2^{2}})\). \(\) and \(^{2}\) represent the mean and variance of a gaussian distribution. \(_{p}^{i}=\{_{t}^{i},E^{P_{i}}\}\) and \(_{t}^{i}=\{_{t}^{i},E^{T_{i}}\}\) indicate the model parameters after the \(i\)th epoch of training in the text and point cloud branch, respectively. Note that a simple accumulated addition can implement Eq. 3 and we do not need to store all \(e\) copies of the parameters, referring to Appendix for details.

**Optimization.** The overall optimization objective consists of two parts, the task-specific cross entropy loss \(_{CE}\) and the task-agnostic regulation constraint loss \(_{RC}\), displayed in Eq. 4, where \(,,\) are hyperparameters. Unlike trivial prompt tuning a multi-modal large model on downstream tasks, this design allows the learnable prompts to actively interact and align with the general knowledge in a pre-trained large model while learning on specific 3D tasks.

\[=_{CE}+_{RC},_{RC}= _{p}+_{t}+_{D} \]

### Implementation Details

We choose PointCLIP , PointCLIP V2 , ULIP , and ULIP-2  as the 3D foundation models for experiments. All experiments are running with three random seeds and we report the mean and standard deviation. The learnable prompts are inserted into the inputs of first 9 Transformer layers in these models and the prompt length is set to 2. Unless specified, the prompts are optimized using 16-shot learning. Note that previous 3DDG methods [51; 20; 64] use the full training set. We set \(=10\), \(=25\) and \(=1\). The optimizer is SGD, the initial lr is 0.0025 and we use cosine scheduler to update it. More details about the model configuration can be found in Appendix. We will justify the design choices in the ablation study.

## 4 Experiments

In this section, we first explain the evaluation settings of our newly curated and existing benchmarks. Then, comprehensive comparison and analysis across various generalization settings are presented to show the advantages of the proposed method. Finally, we justify the effectiveness of different components in our regulation framework through systematic controlled experiments.

### 3DDG Evaluation Settings

**Base-to-New**. This benchmark includes 5 point cloud datasets which are ModelNet40 , three variants of ScanObjectNN  (S-PB_T50_RS, S-OBJ_BG, S-OBJ_ONLY) and ShapeNetCoreV2 . Each dataset is equally split into base and new classes, where the former is used for prompt tuning while the latter only serves the test purpose. **Cross-Dataset**. This benchmark has four types of evaluation, including _OOD generalization_, _data corruption_, _PointDA_ and _Sim-to-Real_. We established the first two assessments and the latter two already existed. For _OOD generalization_, models are trained on the source domain and evaluated on the target domains. For _data corruption_, models are trained on clean ModelNet  and tested on the corrupted data in ModelNet-C. **Few-Shot**. This setting inspects the model generalization in an extremely low-data regime, where 1, 2, 4, 8, and 16 shots are randomly sampled for prompt learning, and the recognition accuracy is calculated on the whole test set, respectively. The explanations are brief and we encourage the readers to check the details in Appendix.

### Base-to-new Class Generalization

In this benchmark, models are learned on the base classes and evaluated on the test sets of base and novel classes. In addition to ULIP and ULIP-2, we also implement the same prompt tuning for PointCLIP  (P-CLIP) and PointCLIP V2  (P-CLIP2) for comparison, shown in Tab. 1.

**Loss of Generalization in P-CLIP and ULIP Series**. We observe notable gaps occur between base and new class recognition accuracy of P-CLIP, P-CLIP2, ULIP, ULIP-2 when prompt tuning without the proposed regulation constraints. For instance, P-CLIP2 achieves 93.98% accuracy on the base classes of ModelNet40 while dropping by 48.77% absolute points on the whole test set of the new classes, which even lags behind the zero-shot accuracy of the frozen P-CLIP2 (64.22%). The results are consistent across five datasets, suggesting the loss of generalization of original models.

**Lifting the Generalization by Our Framework**. As shown in Tab. 1, the proposed framework composite of three regulation constraints boosts the unseen class recognition accuracy across different models and datasets by a clear margin, thanks to the active communication and alignment with the general knowledge in large 3D models. For example, the improvement of the harmonic mean on ULIP reaches 10.65% absolute points averaged over 5 datasets.

**Lifting the Specific 3D Tasks by Our Framework**. Surprisingly, the task-specific performances are not be hindered by the regulation constraints while enhancing the task-agnostic generalization, referring to the base class accuracy of ULIP+**RC** and ULIP-2+**RC** averaged over 5 datasets, increasing by 4.87% and 5.27%, respectively.

### Cross-Dataset Generalization

This setting differs from the base-to-new counterpart where the base and new classes belong to the same dataset. We present the analysis for _OOD generalization_ and _data corruption_ as below, and put the comparison on _Sim-to-Real_ and _PointDA_ in Appendix.

_OOD Generalization_ demonstrates the models' transferability to other unseen domains by learning from an existing domain. To evaluate on this benchmark, we implement the lightweight prompt learning for ULIP and ULIP-2 then impose the proposed regulation constraints on them. Prompt learning for P-CLIP  and P-CLIP2  with same settings are also implemented for comparison. The results are reported in Tab. 2. By wrapping ULIP and ULIP-2 with the devised framework, we achieve consistent positive gains on each of the five target domains. The average gains over them are enlarged with increasing ability of ULIP, \(e.g.\), +6.20% for ULIP-2 vs. +1.79% for ULIP. Meanwhile, we notice that the performances on Omni3D  are rather limited and the methods here seem not to

Table 1: **Base-to-new class generalization comparison for representative large 3D models based on prompt learning**. Each number here is the mean of three runnings. Base: base class accuracy (in %, same below). New: new class accuracy. HM: harmonic mean of base and new class accuracy. **+RC** demonstrates the models with our regulation constraint framework.

work, especially for P-CLIP series and ULIP (less than 10% accuracy). This dataset contains a large vocabulary of real 3D objects (216 categories) and exhibits the long-tail attribute. When transferring the models that learn from a narrow set of 3D object concepts (55 classes in ShapeNetV2) to Omni3D, they suffer from new 3D concepts thus perform poorly.

_Data Corruption_ are common in point clouds due to complex geometry, sensor inaccuracy and processing imprecision. We investigate the generalization of the proposed framework on ModelNet-C , which includes common corruptions, such as dropping some parts or adding global outliers. The compared methods are same as those in OOD generalization and the results are exhibited in Tab. 3. Our method not only boosts the recognition accuracy on clean data (+1.44% for ULIP and +1.40% for ULIP-2), but also strengthen the robustness of representative large 3D models against collapsed data. By averaging on 7 types of corruption, we receive +1.51% and +4.78% gains for ULIP and ULIP-2, respectively.

### Few-shot Generalization

In this setting, ULIP and ULIP-2 with (w.) and without (w.o.) our regulation constraints (RC) are compared. As visualized in Fig. 4, the solid lines of ULIP and ULIP-2 exceed the corresponding dashed lines by clear margins average over 5 datasets, indicating the devised framework strengthens the 3DDG capability considerably. The advantages are enlarged especially for the extreme 1-shot learning, \(e.g.\), +8.05% acc. for ULIP and +5.39% acc. for ULIP-2. Note that in some cases, \(e.g.\), on ModelNet40, ULIP-2 w.o. RC (1-shot, 66.63%) even lags behind zero-shot ULIP-2 (71.23%), implying that simple prompt tuning disturbs the well-learned representations of ULIP-2. In contrast, the developed framework brings 2.4% absolute improvements over the zero-shot ULIP-2, obtaining 73.63% acc. under the 1-shot setting.

### Ablation Study

In this section, we examine the effectiveness of several critical components in the proposed framework via a series of controlled experiments. ULIP-2 is adopted as the baseline and we compare the variants on the base-to-new benchmark and report the harmonic mean (HM) averaged over 5 datasets.

    &  &  &  \\    & ShapeNetV2 & & ModelNet40 & & & & & \\  P-CLIP  & 67.41(0.09) & 33.20(1.86) & 15.51(0.58) & 18.59(1.40) & 22.89(2.32) & 0.48(0.17) & 22.55(1.54) \\  P-CLIP2  & 68.93(1.43) & 54.73(1.48) & 39.53(4.22) & **34.30**(1.28) & **25.63**(1.16) & 8.63(2.52) & 32.56(2.13) \\ **+RC**(Ours) & **69.80**(2.86) & **55.37**(1.78) & **39.77**(0.45) & 34.20(0.54) & 24.50(1.26) & **10.20**(0.40) & **32.81**(0.89) \\  ULIP  & 87.33(0.95) & 56.17(1.15) & 26.83(2.15) & 39.43(2.17) & 43.53(1.32) & 6.37(0.90) & 34.47(1.54) \\ **+RC**(Ours) & **90.40**(3.86) & **58.00**(0.57) & **28.43**(0.68) & **40.33**(0.71) & **46.33**(1.54) & **8.20**(0.50) & **36.26**(0.80) \\  ULIP-2  & 76.70(1.37) & 65.27(0.66) & 40.07(0.34) & 53.80(1.78) & 48.53(1.72) & 17.27(0.54) & 44.99(1.01) \\ **+RC**(Ours) & **76.70**(1.59) & **72.10**(0.93) & **46.77**(2.43) & **59.03**(3.02) & **56.27**(0.97) & **21.80**(0.49) & **51.19**(1.57) \\   

Table 2: **Comparison of OOD generalization in cross-dataset benchmark**. ShapeNetV2 serves as the source domain and the other five datasets are deployed as the target domain. ShapeNetV2: 55 classes, ModelNet40: 40 classes, SONN: 15 classes, Omni3D: 216 classes. Some common categories are shared between the source and target domain. Note that Omni3D has much more new 3D object concepts than others. The last column indicates the average over five target datasets.

    &  &  &  \\    & ModelNet & & & & & & & & \\  P-CLIP  & 80.97(0.02) & 80.97(1.02) & 80.97(1.02) & 64.95(1.08) & 68.31(1.93) & 65.75(1.19) & 72.04(1.33) & 52.09(1.28) & 69.30(1.26) \\ P-CLIP2  & 83.49(0.51) & 83.49(0.51) & 83.49(0.51) & 68.85(2.32) & 66.67(1.96) & 70.13(1.33) & 75.68(0.15) & 61.21(2.16) & 72.79(1.41) \\  ULIP  & 82.43(1.25) & 82.50(0.90) & 82.27(1.17) & 80.77(1.03) & 65.43(1.02) & 72.27(1.56) & 74.67(1.58) & **45.60**(0.65) & 71.93(1.14) \\ **+RC**(Ours) & **83.87**(0.34) & **83.83**(0.40) & **83.93**(0.19) & **81.83**(0.52) & **67.37**(1.72) & **79.10**(0.36) & **76.37**(0.09) & 41.87(4.79) & **73.44**(1.15) \\  ULIP-2  & 85.07(0.21) & 81.97(0.79) & 82.03(0.96) & 79.93(0.92) & 60.03(1.21) & 80.30(0.93) & 75.77(0.74) & 44.27(2.13) & 72.04(1.10) \\ **+RC**(Ours) & **86.47**(0.56) & **86.57**(0.48) & **86.30**(0.51) & **84.87**(0.48) & **67.80**(1.20) & **81.64**(0.22) & **81.71**(0.45) & **46.43**(2.45) & **76.82**(0.91) \\   

Table 3: **Comparison of corruption generalization on ModelNet-C when trained on clean data**. The results are reported for the corruption severity=2 in ModelNet-C.

[MISSING_PAGE_FAIL:9]

experiments investigate the effect of the point cloud descriptions generated from different sources, including large language models like GPT-3.5 , GPT-4 , PointLLM  and manual templates (see Appendix for details). As shown in Tab. (b)b, point cloud descriptions from general-purpose LLMs, such as GPT-3.5 and GPT-4, bring decent performances on base classes. However, they lag behind PointLLM regarding new class recognition accuracy by a clear margin (-2.39% for GPT-3.5 and -2.28% for GPT-4). We infer it is due to the fact that PointLLM has seen massive point cloud data and related text descriptions thus generates more accurate and domain-related responses. Surprisingly, by combining 64 simple sentences written by human beings , ULIP-2 achieves decent base class accuracy and the best performance on new classes, resulting in even better HM than that of ULIP-2 with LLMs' descriptions.

**The depth and length of learnable prompts**. Two variables that should be determined for the learnable prompts \(\{^{P},^{T}\}\) are the depth of prompt layers \(D\) and the length of prompt tokens \(L\). For simplicity, the prompt depth is kept the same in the point cloud and text encoders, and similarly for the prompt length. We ablate the two variables and visualize the results in Fig. 5. In general, increasing the prompt layers promotes the harmonic mean. But it is not always beneficial to deepen the learnable prompts, \(e.g.\) ULIP-2 with \(D=12\) achieves 78.26% HM, slightly lower than 78.67% HM of ULIP-2 with \(D=9\). We also find that it is not necessary to construct very long prompt tokens to achieve better generalization, \(e.g.\), ULIP-2 with \(L=2\) surpasses other variants average on 5 datasets clearly. Thus we let \(D=9\) and \(L=2\) by default.

## 5 Conclusion

This paper initializes the efforts of addressing the corrupted generalization of large 3D models when adapting to downstream 3D tasks by a comprehensive regulation framework. The framework enables the learnable prompts to actively engage with large 3D models by maximizing the mutual agreement between task-specific prediction and general knowledge. Consistent generalization gains are obtained over different large 3D models, suggesting the model-agnostic attribute of the proposed framework. We also contribute to the study of 3DDG by developing new and more challenging evaluation benchmarks that will drive further investigation. Nevertheless, this work focuses on the point cloud recognition, and we plan to discuss the segmentation and detection tasks in future work.

**Limitations and Broader Impacts.** The proposed framework has demonstrated effectiveness and scalability on the object-level recognition task but not been validated on the scene-level tasks, such as 3D semantic segmentation and object detection. Different solutions may be required to handle scene-level point cloud data. On the other hand, when exploiting the power of LLMs to reflect critical characteristics of 3D objects, we simply ensemble multiple descriptions through the pooling operation, more sophisticated prompting and fusion strategy can be developed. For broader impacts, we are the first to investigate the generalization ability of large multi-modal 3D models, which mirrors the progress of the vision-language field (CLIP-based image recognition) and probably inspires a series of follow-up works. We do not perceive the potential negative impacts of this work.

Figure 5: **Ablation study for the prompt depth and length**. We compare the harmonic mean on five datasets of the base-to-new benchmark and the average results are displayed in dashed lines.