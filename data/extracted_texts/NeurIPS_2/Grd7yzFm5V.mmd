# Bayesian Domain Adaptation with Gaussian Mixture Domain-Indexing

Yanfang Ling

Sun Yat-sen University

lingyf3@mail2.sysu.edu.cn &Jiyong Li

Sun Yat-sen University

lijy373@mail2.sysu.edu.cn &Lingbo Li

InfMind Technology Ltd

lingbo@infmind.ai &Shangsong Liang

Sun Yat-sen University

liangshangsong@gmail.com

Corresponding author.

###### Abstract

Recent methods are proposed to improve performance of domain adaptation by inferring domain index under an adversarial variational bayesian framework, where domain index is unavailable. However, existing methods typically assume that the global domain indices are sampled from a vanilla gaussian prior, overlooking the inherent structures among different domains. To address this challenge, we propose a Bayesian Domain Adaptation with **G**aussian **M**ixture **D**omain-**I**ndexing(GMDI) algorithm. GMDI employs a Gaussian Mixture Model for domain indices, with the number of component distributions in the "_domain-themes_" space adaptively determined by a Chinese Restaurant Process. By dynamically adjusting the mixtures at the domain indices level, GMDI significantly improves domain adaptation performance. Our theoretical analysis demonstrates that GMDI achieves a more stringent evidence lower bound, closer to the log-likelihood. For classification, GMDI outperforms all approaches, and surpasses the state-of-the-art method, VDI, by up to 3.4%, reaching 99.3%. For regression, GMDI reduces MSE by up to 21% (from 3.160 to 2.493), achieving the lowest errors among all methods. Source code is publicly available from [https://github.com/lingyf3/GMDI](https://github.com/lingyf3/GMDI).

## 1 Introduction

Machine learning models often suffer from performance degradation when applied to new domains that differ from their training domains, a phenomenon known as domain shift . Domain Adaptation (DA)  seeks to mitigate this issue by producing domain-invariant features, thereby enhancing generalization from source to target domains .

Recent research has explored the use of domain identity and domain index to improve domain-invariant data encoding and enhance domain adaptation performance . _Domain identity_, a one-hot discrete variable vector, differentiates between domains, whereas _domain index_, a real-valued continuous variable vector, captures domain semantics. Due to the limited information in the discrete domain identity vector, research has increasingly focused on the domain index. Current approaches to incorporating domain index in domain adaptation include: (1) Directly using existing additional information in the dataset as the domain index , which is impractical for datasets lacking such indices , and (2) Treating the domain index as a latent variable to be inferred . However, these methods typicallymodel the domain indices with a simple Gaussian distribution, limiting the domain indices space and thus hindering adaptation to diverse target domains, resulting in suboptimal performance.

To address the aforementioned issues, we propose a Bayesian Domain Adaptation with Gaussian Mixture Domain-Indexing (GMDI) algorithm. The proposed adversarial Bayesian algorithm assumes that domain indices follow a mixture of Gaussian distributions, with the number of mixture components dynamically determined by a Chinese Restaurant Process. As shown in Figure 1, a single Gaussian distribution struggles to adequately fit the domain indices, neglecting the inherent structures among different domains. This observation motivates us to model domain indices from different domains collectively as a Gaussian mixture distribution. To the best of our knowledge, we are the first to model domain indices as a mixture of Gaussian distributions to address the aforementioned challenges. Inspired by , the latent space of the mixture is defined as the "domain-themes" space. The mixtures of distributions provide a higher level of flexibility in a larger latent space, thereby increasing the capability to adapt to various target domains with domain shift. Our theoretical analysis demonstrates that GMDI achieves a more rigorous evidence lower bound, and that maximizing this bound along with adversarial loss effectively infers optimal domain indices. Extensive experimental results validate the significant effectiveness of GMDI.

Our key contributions are summarized as: (1) Our proposed GMDI is the first one to consider the entire distributions of domain indices in the "domain-themes" space following a mixture of Gaussian distributions, and dynamically determining the number of components in the mixture with the Chinese Restaurant Process. (2) Our detailed theoretical analysis demonstrates that training with GMDI's superior evidence lower bound together with adversarial loss can yield optimal and more interpretable domain indices. (3) Extensive experiments on classification and regression tasks showcase the strong domain index modeling capability of GMDI, significantly outperforming the state-of-the-art.

## 2 Related Work

**Adversarial domain adaptation.** There exists a substantial body of work on domain adaptation . They focus on generating domain-invariant data encoding by aligning the distributions of source and target domains to adapt to target domains. This alignment is achieved by directly matching the statistics of distributions  or by employing adversarial loss , which encourages domain confusion through adversarial objective with a discriminator. Adversarial domain adaptation is widely used due to its integration with deep learning, strong theoretical foundation , and superior performance. Various different types of adversarial losses have been explored:  uses an inverted label GAN loss,  utilizes a minimax loss, and  employs a cross-entropy loss against the uniform distribution. Typically, the discriminators in these models rely on _domain identity_, which contains limited information, to align data encoding distributions.  and  also pay attention to domain identity. Our work, however, focuses on _domain index_, providing a more detailed representation of domains.

**Domain adaptation related to domain indices.** Recently, there has been growing interest in using continuous _domain index_, which contain richer and more interpretable information, to enhance domain adaptation performance.  use the rotation angle of images as the domain index for the Rotating MNIST dataset and patients' ages as the domain index for Healthcare Datasets. Their theoretical analysis demonstrates the value of utilizing domain indices to generate domain-invariant features.  employ graph node embeddings as domain indices to achieve domain adaptation in graph-relational domains. These methods assume that domain indices are available. However, in practice, domain indices are not always accessible .  generates features representing the similarity between different domains but do not formally define the domain index.  formally define the domain index and treat it as a latent variable to be inferred. Although  takes steps towards Bayesian approximation to parameter distributions, it only assumes a single domain index

Figure 1: Illustration of domain indices modeled by different distributions.

distribution, limiting its capability to adapt to diverse target domains effectively. In contrast, we address this issue by representing the domain index with a dynamically updated mixture model.

## 3 Background

### Problem setup

We aim at unsupervised domain adaptation: given \(N\) domains with different domain shifts, each domain has a domain identity \(w=[N]\{1,...,N\}\), and each domain contains \(D_{w}\) data points. Similar to the conventional unsupervised domain adaptation setting, the \(N\) domains are divided into source domains with labeled data \(^{S}=\{(_{i}^{s},y_{i}^{s},w_{i}^{s})\}_{i=1}^{n_{s}}\) and target domains with unlabeled data \(^{T}=\{(_{i}^{t},w_{i}^{t})\}_{i=1}^{n_{t}}\). A foundational element that builds up our research problem is the diverse domain shifts  between different target domains and source domains. For source domains, the complexity of each target domain varies, which motivates us to dynamically infer domain indices in the "domain-themes" space and model them with dynamic Gaussian Mixture Model. We aim to (1) predict the label \(\{y_{i}^{t}\}_{i=1}^{n_{t}}\) of target domain data, and (2) infer local domain index \(_{w}^{B_{u}}\) and global domain index \(_{w}^{B_{}}\) in the dynamic "domain-themes" space. The summary of the notations is presented in Appendix J.

### Preliminary

**Domain index.** The domain index, distinct from domain identity \(w\), represents domain semantics, thus empowering it to significantly enhance domain adaptation performance. As per its definition [36; 41], the domain index satisfies the following : (1) To acquire domain-invariant data encoding \(\), the global domain index \(\) must remain independent of data encoding \(\), i.e., \(\!\!\! z\) or equivalently \(p()=p()\). (2) Effectively representing data point \(\) while averting the occurrence of collapsing. (3) Ensuring optimal performance of downstream tasks utilizing the data encoding \(\) learned by the encoder under the aforementioned constraints, and necessitating the maintenance of sensitivity to labels.

**Variational domain index.** In circumstances where the domain index may not be readily accessible, the Variational Domain Index (VDI)  is a Bayesian approach to infer the domain index \(\) and \(\) as latent variables. VDI factorizes the generative model \(p(,y,,,)\) as:

\[p(,y,,,)=p( )p()p()p(,,)p(y)\,, \]

where \(\) denotes the parameters for the prior probability distribution of the domain index \(

[MISSING_PAGE_FAIL:4]

[MISSING_PAGE_EMPTY:5]

where \(_{k}\) and \(_{k}^{2}\) are mean vector and semi-positive covariance matrix of the \(k\)-th component in dynamic Gaussian mixture of domain indices, respectively. Figure 3 illustrates the generative process of VDI with a single distribution and GMDI with a mixture of distributions for domain indices. Since CRP is computationally intensive. To improve computational efficiency, we consider the stick-breaking construction to transform the infinite Gaussian mixture of domain indices into a finite one. It can be achieved by directly specifying an upper bound \(K\) for the number of components in Gaussian mixture of domain indices. Selecting an appropriate \(K\) allows to effectively reduce computational overhead. The finite version of the generative process of GMDI is available in Appendix A.

Accordingly, the generative model can be factorized as follows:

\[p(,y,,,,v,)=p( )p(v)p(^{v})p(^{v})p()p(,,^{v})p(y)\,. \]

The predictor \(p(y)\) is a categorical distribution for classification tasks and a Gaussian distribution for regression tasks.

### Evidence Lower Bound

The exact posterior of all latent variables, i.e., \(p(,,,v,)\) is intractable, variational inference is used to approximate the posterior. Compared to the Monte Carlo sampling, variational inference allows both uncertainty quantification and computational efficiency. We employ structured variational inference to approximate the exact posterior, factorizing the approximate posterior \(q(,,,v,)\):

\[q(,,,v,)=q(;)q (v;)q(;_{u})q(^{v}; {}_{})q(,,^{v};_{z})\,, \]

where \(,,_{u},_{}\) and \(_{z}\) respectively represent the parameters of the variational distributions \(q()\), \(q(v)\), \(q()\), \(q(^{v})\) and \(q(,,^{v})\).

We train GMDI by maximizing the evidence lower bound(ELBO) to obtain the optimal variational distributions which best approximate exact posterior distributions. Section 5 demonstrates that our proposed GMDI has a more stringent evidence lower bound. Based on generative and inference process of GMDI, we calculate the ELBO as follows:

\[_{} =_{q(,^{v},|;)q(v; )}[ p(y|)]+_{q(|;_{u})}[  p(|)]\] \[+_{q(v;)q(;)q(|;_{u})q(^{v}|;_{})}[ p(| ^{v})]-[q(;)||p()]\] \[-_{q(;)}[[q(;)||p(;_{v})]]-_{q(|;_{ })q(v;)}[[q(^{v}|;_{ })||p(^{v})]]\] \[-_{q(|;_{u})}[ q(| ;_{u})]\,, \]

where \(\) and \(\) represent the parameters of the variational distributions \(q(,^{v},|)\) and \(q(,^{v}|)\), respectively, and \([||]\) is the Kullback-Leibler divergence.

### Adversarial loss with a discriminator

To ensure the independence between global domain index \(\) and data encoding \(\) as defined, we follow VDI  by training an additional discriminator D with an adversarial loss. As we prove in Section 5 that the independence between global domain index \(\) and data encoding \(\) relies on the independence between domain identity \(w\) and data encoding \(\), the adversarial loss is simplified to discriminate the domain identity \(w\):

\[_{}=_{p(w,)}_{q(|; _{z})}[(w|)]\,. \]

### Objective function

Combining Equation 14 and Equation 15, the final objective of GMDI is:

\[_{}=_{D}_{}-* _{}\,, \]

where \(\) denotes the hyper-parameter that balances two terms. Since the exact posterior of all latent variables is intractable, we propose to use a structured variational inference method to approximate the exact posterior. More details can be viewed in the appendix B.

**Variational distribution of \(\).** To derive the optimal variational distribution of \(\), we only consider the terms related to \(\) in \(_{}\), then we can get the posterior \(q(_{k};_{k})=(_{k};_{k,1}, _{k,2})\) with parameters \(_{k,1}=1+_{k}\) and \(_{k,2}=+_{i=k+1}^{K}_{i}\).

**Variational distribution of \(v\).** Similarly, the variational posterior of \(v\) can be calculated as a Categorical distribution \(q(v;)=_{K}(v;)\), where the pareameters can be updated as:

\[_{k} _{q(;)}[]+_{q( |;_{u})q(^{v}|;_{})}[  p(|^{v})]-_{q(|;_{u})}[ [q(^{v}|;_{})||p(^{v})]]\] \[-_{q(,^{v}|;)}[[q(|,,;_{z})||p(|,,)]]\,, \]

where \(_{k=1}^{K}_{k}=1\) and \(q(;)=_{k=1}^{K-1}q(_{k};_{k})\).

**Variational distribution of \(\),\(\) and \(\).** With assuming that the latent parameters are sampled from Gaussian, we have the following forms:

\[q(^{v}|;_{}) =(_{},_{}^{2})\,, \] \[q(|;_{u}) =(_{u},_{u}^{2})\,,\] (19) \[q(|,,^{v};_{z}) =(_{z},_{z}^{2})\,, \]

where \(\) and \(^{2}\) are mean vector and semi-positive covariance matrix of Gaussian distribution. The parameters are updated by gradient descend. Specifically, we follow VDI  by using Earth Mover's ELBO is better than the VDI, which means that a mixture of Gaussian prior can get better results. See Appendix C for detailed proof.

**Lemma 1**: _The ELBO of \(p(,y)\) is bounded by the following formula with the Mutual Information, the Entropy and the \(\)-divergence:_

\[_{p(,y)}[_{}(p(,y))] I(y;)+I(;,,,v)-(H()+H(y))\] \[-_{q(,,,,v)}[[q (|,,v,)||p(|,,v,)]]\] \[-[q(,,v,|)||p(, ,,)]\,.\]

The main difference between and Lemma 1 in GMDI and Lemma 4.1 in VDI  is the last two \(\) terms and the inclusion of \(v\).

**Lemma 2**: _(Information Decomposition of the Adversarial Loss )We can decompose the global maximum of adversarial loss as follows:_

\[_{D}_{p(w,)}_{q(|)}[( w|)]=I(;)+I(;w|)-H(w)\,.\]

_The global minimum of the function is achieved if and only if \(I(;)=0\) and \(I(;w|)=0\)._

**Theorem 1**: _The upper bound of the objective function can be decomposed as follows:_

\[_{} I(y;)+I(;,,,v)-I(;)-I(;w|)-(H()+H(y)-H(w))\,.\]

The main difference between Theorem 1 in GMDI and Theorem 4.1 in VDI  is the inclusion of \(v\).

**Theorem 2**: _The global optimum is achieved if and only if: (1)\(I(;)=I(;w|)=0\), (2)\(I(y;)\) and \(I(;,,,v)\) are maximized, (3)\([q(,,v,|)||p(,,v,)]=0\) and \([q(|,,v,)||p(|,,v,)]=0\)._

The main difference between Theorem 2 in GMDI and Theorem 4.2 in VDI  is that \(I(;,,,v)\), which includes \(v\), needs to be maximized, and the two \(\) divergences should equal zero.

**Theorem 3**: _Assuming the ELBO and objective of VDI are \(_{ VDI-ELBO}\) and \(_{ VDI}\) respectively, where domain indices are sampled from a simple Gaussian prior, we can prove that our objective achieves a more stringent evidence lower bound which is closer to the log-likelihood, and also a tighter upper bound of the objective: \(_{ VDI-ELBO}_{ ELBO} p(,y)\) and \(_{ VDI}_{ GMDI}\)._

## 6 Experimental Study

We verify the effectiveness of GMDI via experimental comparison and analysis. In particular, we answer three research questions: (**RQ1**) Can the performance of GMDI for domain adaptation outperform baselines? (**RQ2**) How effective is the global domain indices inferred by GMDI? (**RQ3**) How does the number of mixture components K affect results? Additional experimental results are available in Appendix K.

### Experimental setup

**Datasets.** We compare GMDI with existing DA methods on the following datasets (see Appendix H and Appendix I for more details): _Circle_ is used for binary classification task. _DG-15_ and _DG-60_ are synthetic datasets used for binary classification task. _TPT-48_ dataset is a real-world dataset used for regression task. W (6) \(\) E (42): Adapting models from the 6 states in the west to the 42 states in the east. N (24) \(\) S (24): Adapting models from the 24 states in the north to the 24 states in the south. _level-1 target domains_: one hop away from the closest source domain. _level-2 target domains_: two hops away from the closest source domain. _level-3 target domains_: more than two hops away from the closest source domain. _CompCars_ dataset is a real-world dataset for 4-way classification task.

**Baselines.** To evaluate our proposed GMDI, we compare it against eight state-of-the-art domain adaptation methods: Domain Adversarial Neural Networks (**DANN**) , Adversarial Discriminative Domain Adaptation (**ADDA**) , Conditional Domain Adaptation Neural Networks (**CDANN**) , Margin Disparity Discrepancy (**MDD**) , **SENTRY**, Domain to Vector (**D2V**) , and Variational Domain Index (**VDI**) . Additionally, we include the results for models trained and tested only on the source domain (**Source-only**). Note that D2V is not applicable to regression tasks, so its results are not reported on the _TPT-48_ dataset. Moreover, since our proposed GMDI focuses on inferring domain indices when they are unavailable, whereas  and  assume domain indices

    &  \\  Dataset & Source-only & DANN & ADDA & CDANN & MDD & SENTRY & D2V & VDI & GMDI (Ours) \\  _Circle_ & 55.5 & 53.4 & 56.2 & 54.9 & 53.4 & 59.5 & 60.1 & 94.3 & **96.9** \\ _DG-15_ & 39.7 & 43.3 & 33.5 & 38.8 & 37.2 & 42.6 & 79.9 & 94.7 & **96.5** \\ _DG-60_ & 55.0 & 66.3 & 60.8 & 65.3 & 54.6 & 51.3 & 82.1 & 95.9 & **99.3** \\ _CompCars_ & 39.1 & 38.9 & 42.8 & 41.8 & 41.4 & 41.8 & 40.7 & 42.51 _ & **44.4** \\   

Table 1: Accuracy on binary classification tasks (_Circle_, _DG-15_, and _DG-60_) and 4-way classification task (_CompCars_).

Figure 4: MSE of domain indices on _TPT-48_ dataset. **Left**:N (24) \(\) S (24), ground-truth domain indices are latitude. **Right**: W (6) \(\) E (42), ground-truth domain indices are longitude.

are available, they are not applicable to our setting. Detailed explanations of these algorithms can be found in the respective references.

### Results and discussion

Rq1: Performance on classification and regression tasks.(1) _Circle, DG-15_ and _DG-60_. The results in Table 1 show that, on all three datasets, the performance of baselines other than D2V and VDI is only marginally better or worse than random guess (accuracy of 50\(\%\) ). This is likely due to the complex relationships between domains within the datasets, making it difficult to adapt to target domains. Additionally, Source-only performs poorly due to overfitting. Compared to VDI, our GMDI improves accuracy by up to 3.4\(\%\), achieving very high accuracy (over 96.5\(\%\)). This improvement is attributed to proposal of modeling the global domain index as the mixture distributions (e.g., Figure 8). (2) _TPT-48_. In Table 2, we report the mean square error (MSE) of the evaluated methods on _TPT-48_. In both E (6) \(\) W (42) and N (24) \(\) S (24) regression tasks, all methods except DANN, SENTRY, and VDI performed worse than Source-only, indicating the occurrence of negative transfer. In contrast, GMDI significantly reduced the MSE compared to VDI, with average MSE decreases of 16\(\%\) and 21\(\%\), respectively. (3) _CompCars_. The results in Table 1 show that our method achieves the best classification accuracy. All domain adaptation methods improved to varying degrees compared to Source-only, but our method achieved the highest increase in accuracy, with an improvement of up to 5\(\%\). In Figure 7(left), the data encoding generated by VDI are clustered together, indicating a mixture of points from different class labels. In contrast, in Figure 7(right), the data encoding of GMDI are separated by class label, demonstrating that GMDI can better distinguish points by class label. **Across all datasets, GMDI significantly outperforms baselines, with minimum accuracy of 96.5 \(\%\) on synthetic datasets, while MSE is reduced by at least 16 \(\%\) on _TPT-48_ dataset.**

RQ2: Effectiveness of inferred domain indices.Note that our proposed GMDI have no access to ground-truth domain indices \(\) during traning. To evaluate the effectiveness of GMDI in inferring domain indices, we compare the inferred domain indices with the ground-truth domain indices and calculate MSE between them. As shown in Figure 5, for nearly all 30 domains on _Circle_ dataset, the MSE of the domain indices inferred by GMDI is significantly lower than that inferred by VDI. On _TPT-48_ dataset, the domain indices for E (6) \(\) W (42) and N (24) \(\) S (24) regression tasks correspond to longitude and latitude of 48 states. Therefore, we use longitude and latitude as the ground-truth domain indices and calculate the corresponding MSE. In Figure 4, it is evident that the MSE of the domain indices inferred by GMDI is still substantially lower than that of VDI. Although the _CompCars_ lacks ground-truth domain indices, the data encoding visualization of VDI and GMDI(Figure 7) show that data encoding generated by GMDI form more distinct clusters compared to VDI. It indirectly indicates the effectiveness of the domain indices inferred by GMDI, demonstrating the considerable impact of modeling the global domain indices as Gaussian Mixture Model. **On all datasets, the domain indices inferred by GMDI outperform those by VDI, owing to the dynamic mixture of domain indices distributions.**

RQ3: Number of mixture components.We utilize the stick-breaking representation of the CRP to improve computational efficiency, setting an upper bound \(K\) on the number of components in GMM. To study the impact of the number of components on domain adaptation performance, we report classification accuracy on _CompCars_, more complex dataset for different values of K. For other 4 datasets, the best results are achieved with \(K=2\), while for _CompCars_, the best performance

   Task & Domain & Source-only & DANN & ADDA & CDANN & MDD & SENTRY & VDI & GMDI(Ours) \\   & Average of 4 level-1 domains & **1.184** & 1.984 & 5.448 & 6.168 & 5.544 & 2.515 & 2.160 & 1.346 \\  & Average of 6 level-2 domains & 3.128 & 5.112 & 7.624 & 7.016 & 7.912 & 5.136 & 3.000 & **2.393** \\  & Average of 32 level-3 domains & 5.272 & 5.880 & 7.256 & 6.986 & 8.008 & 5.872 & 2.448 & **2.122** \\   & Average of all 42 domains & 4.576 & 5.400 & 7.136 & 6.896 & 7.76 & 5.456 & 2.496 & **2.087** \\    & Average of 10 level-1 domains & 1.648 & 1.832 & 5.872 & 1.832 & 2.736 & 3.976 & 1.536 & **1.479** \\  & Average of 6 level-2 domains & 3.128 & 3.296 & 6.888 & 2.856 & 6.144 & 3.760 & 2.584 & **2.119** \\   & Average of 6 level-3 domains & 9.280 & 6.744 & 7.088 & 7.688 & 10.608 & **3.672** & 5.624 & 3.942 \\    & Average of all 24 domains & 4.560 & 3.840 & 6.528 & 4.040 & 6.216 & 3.816 & 3.160 & **2.493** \\   

Table 2: MSE for various DA methods in both tasks W (6) \(\) E (42) and N (24) \(\) S (24) on _TPT-48_. We report the average MSE of all domains as well as more detailed average MSE of level-1, level-2, level-3 target domains, respectively. Note that there is only one single DA model per column. We mark the best result with **bold face**.

is obtained with \(K=3\). The results in Figure 6 show that accuracy is the lowest when \(K=1\), suggesting that maintaining a single domain index distribution is insufficient for diverse target domains. The choice of \(K\) is related to the concentration parameter of CRP and the dataset; the larger the concentration parameter and the more complex the dataset, the larger the value of \(K\) should be.

## 7 Conclusion and Limitations

In this work, we propose GMDI, a novel Gaussian Mixture Domain-Indexing algorithm, to address the challenge of inferring domain indices when they are unavailable. Unlike existing methods that assume global domain indices are sampled from a single static Gaussian, GMDI is the first one to utilize a mixture of dynamic Gaussians. The number of mixture components is determined adaptively by the Chinese Restaurant Process, enhancing the flexibility and effectiveness of domain adaptation. Our theoretical analysis confirms that GMDI achieves a more stringent evidence lower bound, closer to the log-likelihood. Extensive experiments validate the effectiveness of GMDI in inferring domain indices and highlight its potential practical applications. Specifically, for classification tasks, GMDI outperforms all approaches, and surpasses the state-of-the-art method, VDI, by up to 3.4%, reaching 99.3%. For regression tasks, GMDI reduces MSE by at least 16% (from 2.496 to 2.087) and by 21% (from 3.160 to 2.493), achieving the lowest errors among all methods. Despite these advantages, GMDI still relies on the availability of domain identities and cannot infer them as latent variables. Future work will focus on developing algorithms capable of inferring domain indices together with domain identities to further enhance the robustness and applicability of our approach.