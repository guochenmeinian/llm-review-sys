# Language Grounded Multi-agent Reinforcement Learning with Human-interpretable Communication

Huao Li

University of Pittsburgh

hul52@pitt.edu

&Hossein Nourkhiz Mahjoub

Honda Research Institute USA, Inc.

hossein_nourkhizmahjoub@honda-ri.com

&Behdad Chalaki

Honda Research Institute USA, Inc.

behdad_chalaki@honda-ri.com &Vaishnav Tadiparthi

Honda Research Institute USA, Inc.

vaishnav_tadiparthi@honda-ri.com

&Kwonjoon Lee

Honda Research Institute USA, Inc.

kwonjoon_lee@honda-ri.com &Ehsan Moradi-Pari

Honda Research Institute USA, Inc.

emoradipari@honda-ri.com &Michael Lewis

University of Pittsburgh

ml@sis.pitt.edu &Katia Sycara

Carnegie Mellon University

sycara@andrew.cmu.edu

Work done while interning at Honda Research Institute USA. Code available at [https://romanlee6.github.io/langground_web/](https://romanlee6.github.io/langground_web/).

###### Abstract

Multi-Agent Reinforcement Learning (MARL) methods have shown promise in enabling agents to learn a shared communication protocol from scratch and accomplish challenging team tasks. However, the learned language is usually not interpretable to humans or other agents not co-trained together, limiting its applicability in ad-hoc teamwork scenarios. In this work, we propose a novel computational pipeline that aligns the communication space between MARL agents with an embedding space of human natural language by grounding agent communications on synthetic data generated by embodied Large Language Models (LLMs) in interactive teamwork scenarios. Our results demonstrate that introducing language grounding not only maintains task performance but also accelerates the emergence of communication. Furthermore, the learned communication protocols exhibit zero-shot generalization capabilities in ad-hoc teamwork scenarios with unseen teammates and novel task states. This work presents a significant step toward enabling effective communication and collaboration between artificial agents and humans in real-world teamwork settings.

## 1 Introduction

Effective communication is crucial for multiple agents to collaborate and solve team tasks. Especially in ad-hoc teamwork scenarios where agents do not coordinate a priori, the ability to share information via communication is the keystone for successful team coordination and good team performance . Multi-Agent Reinforcement Learning (MARL) methods have shown promise in allowing agents to learn a shared communication protocol by maximizing the task reward . However, merelymaximizing the utility of communication in specific task goals might compromise task-agnostic objectives such as optimizing language complexity and informativeness [40; 16], making the learned communication protocols 1) hard to interpret for humans or other agents that are not co-trained together [18; 20; 6] and 2) highly data-inefficient . In addition, most previous works learn communication protocols with atomic symbols or a combination of them leaving the relation between symbols unexplored . Only a few researches attempt to learn a semantic space for zero-shot communication of unseen states [42; 17]. One of the popular directions for interpretable communication is to regulate the learning process with external knowledge from human languages [41; 22; 28; 1]. However, this process is challenging due to the divergent characteristics between human and machine languages by nature . For example, most agent training methods (e.g., deep reinforcement learning) require a huge amount of data that is impractical for human-in-the-loop training or even collecting from humans .

The rise of Large Language Models (LLMs) provides new opportunities in grounding agent communication with human languages. Recent generative models fine-tuned with human instructions (e.g., GPT-4, Llama 3) show reasonable capabilities in completing team tasks and communicating in a human-like fashion via embodied interaction [36; 25]. Essentially, LLMs encapsulate a highly trained model of human language patterns in teamwork, allowing them to generate descriptions and responses that are well-grounded in natural language. For the purpose of guiding multi-agent communication, they represent the most generally available reference based on a vast corpus of human language data that would be infeasible to collect through other means. However, LLMs are known to suffer from a lack of grounding with the task environments (a.k.a. hallucinations), which prevents embodied agents from generating actionable plans . While attempts have been made to ground LLMs with reinforcement learning or interactive data collected from environments [47; 4; 39], none of them involve teamwork nor communication among multiple embodied agents.

In this paper, we propose LangGround, a novel computational pipeline for artificial agents to learn human-interpretable communication for ad-hoc human-agent teamwork. Specifically, we use synthetic data generated by LLM agents in interactive teamwork scenarios to align the communication space between MARL agents with human natural language. Learning signals from both language grounding and environment reinforcement regulate the emergence of a communication protocol to optimize both team performance and alignment with human language. We have also evaluated the learned communication protocol in ad-hoc teamwork scenarios with unseen teammates and novel task states. The aligned communication space enables translation between high-dimensional embeddings and natural language sentences, which facilitates ad-hoc teamwork. The proposed computational pipeline does not depend on specific MARL architecture or LLMs and should be generally compatible. We have sought to minimize the influence of prompt engineering to ensure the seamless applicability of our approach in diverse environments. To the best of our knowledge, this work is among the very first attempts at training MARL agents with human-interpretable natural language communication and evaluating them in ad-hoc teamwork experiments.

## 2 Related Work

### Multi-Agent Communication

Reinforcement learning has been used to coordinate the teamwork and communication among multiple agents in partially observable environments. In earlier works such as DIAL , CommNet , and IC3Net , agents learn differentiable communication in an end-to-end fashion under the pressure of maximizing task reward. Other works use shared parameters  or a centralized controller  to stabilize the non-stationary learning process of multi-agent communication. More recently, representation learning methods such as autoencoder  and contrastive learning  are used to ground an agent's communication on individual observations. However, comm-MARL methods usually suffer from overfitting to specific interlocutors trained together . The learned communication protocols can not be understood by unseen teammates in ad-hoc teams, let alone another human.

Another relevant trend of research is Emergent Communication (EC), where researchers focus more on simulating the development of natural (i.e., symbolic) language with artificial agents [19; 15]. The most common task scenarios used in the EC community are reference games or Lewis signaling games , in which a speaker must describe an object to a listener, who must then recognize it among a set of distractions . However, previous research has shown that learning EC in more complicated, scaled-up, and multi-round interactive task scenarios can be challenging or even infeasible [9; 7]. Even in situations where agents can learn to communicate, the learned protocols are usually either not human-interpretable  or semantically drifting from human language .

### Human-Interpretable Communication

To address the above-mentioned challenges, several recent works propose human-interpretable communication in RL settings. Lazaridou et al. leverage pre-trained task-specific language models to provide high-level guidance for natural language communication. A few other works align low-level communication tokens with human language[21; 41], or learn discrete prototype communication in a semantically meaningful space [42; 17]. But as pointed out in several studies [8; 49], the low mutual intelligibility between human language and neural agent communication makes the alignment process non-trivial. Our work is closest to , in which researchers alternate imitating human data via supervised learning and self-play to maximize reward in a reference game. The differences are that in  authors try to train neural agents for reference games in an end-to-end fashion with backpropagation, while we train MARL agents in interactive team tasks. The exploration of this research direction is still very limited, as no previous work has ever evaluated natural language communication agents within interactive task environments and for ad-hoc human-agent teams.

### Language-Grounded Reinforcement Learning

Reinforcement learning is known to struggle with long-horizon problems with sparse reward signals . Natural language guidance has been used to provide auxiliary rewards to improve the data efficiency and learning robustness . Goyal et al. use step-by-step natural language instructions provided by human annotators to construct auxiliary reward-learning modules, encouraging agents to learn from expert trajectories. Narasimhan et al. research the impact of language grounding on representation learning and transfer learning of RL agents in a 2D game environment. Additional work has explored grounding RL with other formats of materials such as game manuals [14; 46] and human commands . However, none of those works has ever used the communication messages as the language ground nor guided the information-sharing process in multi-agent teamwork.

The most relevant research to our proposed method is CICERO, which empirically evaluates the proposed AI agent in the complex natural language strategy game Diplomacy with human players . CICERO has an RL module for strategic reasoning and a language model for generating messages. The two modules are trained separately on different datasets, namely self-play trajectories and conversation data collected from human players. The two modules in CICERO function independently, with the only connection being that the language model takes intention estimation from the planning module as input. While in our work, both action and communication are generated by individual RL agents that are trained end-to-end with a combination of RL loss and supervised learning loss.

## 3 Preliminaries

We formulate the problem as a decentralized partially observable Markov Decision Process , which can be formally defined by the tuple \((,,,,,,,,)\), where \(\) is the finite set of \(n\) agents, \(s\) is the global state space, \(=_{i}_{i}\) is the set of actions, and \(=_{i}_{i}\) is the set of communication messages for each of \(n\) agents. \(:\) is the transition function that maps the current state \(s_{t}\) into next state \(s_{t+1}\) given the joint agent action. In our partially observable environments, each agent receives a local observation \(o^{i}\) according to observation function \(:\). Finally, \(:\) is the reward function, while \([0,1)\) is the discount factor. At each time stamp \(t\), each agent \(i\) takes an action \(a^{i}_{t}\) and sends out a communication message \(c^{i}_{t}\) after receiving the partial observation of task state \(s_{t}\) along with all messages sent by other agents from last time stamp \(c_{t-1}\). Each agent then receives the individual reward \(r^{i}_{t}(s_{t},a_{t})\). We consider fully cooperative settings in which the reinforcement learning objective is to maximize the total expected return of all agents:

\[_{^{i}:}[ _{t}_{i}^{t}(s^{i}_{t}, a^{i}_{t})|a^{i}_{t}^{i},o^{i}_{t}] \]We borrow the definition of language learning from . We define a target language \(L*\) that we want the agents to learn, assuming \(\) is the set of natural language and \(L*\) is the optimal communication language for achieving a good team performance in the specific task. Specifically, we consider a language \(L\) to be a set of communication messages \(\) which are mapped from agent observations to communication messages defined as \(L:\). In typical RL settings, this can be thought of as the mapping between input observation vectors and English descriptions of the observation. We consider a dataset \(\) consisting of \(||\) (observation, action) pairs, which comes from expert trajectories generated by LLM embodied agents using the target language \(L*\). The language learning objective is to train agents to speak language \(L*\) in order to collaborate with experts in ad-hoc teamwork. It is worth noting that we want the learned language to generalize to unseen examples that are not contained in \(\).

To train agents that perform well on the team task and speak human-interpretable language, we need to solve a multi-objective learning problem by combining the learning signals from both environment reward and supervised dataset \(\). The process of training an optimal communication-action policy can be defined as solving an optimization problem with two constraints 1) agents must learn to communicate effectively to maximize team performance and 2) agents must learn to use similar language as in the supervised dataset \(\).

## 4 Language Grounded Multi-agent Communication

In this section, we propose a computational pipeline for learning language-grounded multi-agent communication (LangGround). The general framework of LangGround is illustrated in Figure 1 consisting of two parts: collecting grounded communication from LLM agents and aligning MARL communication with language grounds.

### Grounded communication from LLM agents

We use embodied LLM agents to collect samples of target language \(L*\). To allow LLM agents to interact with the task environments, a text interface \(I\) is implemented following  to translate between abstract representations and English descriptions. Essentially, each of the \(n\) LLM agents is provided with general prompts about the team task and instructed to collaborate with others to achieve the common goal. At each time stamp \(t\), the LLM agent \(i\) receives English descriptions of its own observation of the environment \(I(o^{i}_{t})\) which also includes communication messages from teammates in the last time stamp \(C_{t-1}\).The LLM agent is prompted to output its next action and communication message which are then encoded into abstract \(^{i}_{t}\) and \(^{i}_{t}\) and used to update the task environment.

Theoretically, we construct a text environment in parallel with the actual task environment to ensure that embodied LLM agents are exposed to the equivalent information as RL agents, albeit in a different

Figure 1: Illustrations of our proposed computational pipeline, LangGround. The framework consists of three modules: 1) collecting grounded communication from LLM agents, 2) aligning MARL communication with language grounds, 2) translating aligned communication vectors into natural language messages via cosine similarity matching.

format. The action and communication policy of LLM agents emerge from the backbone LLM, since the provided prompts do not include any explicit instructions on team coordination or communication strategy. In the results section we will confirm findings from previous literature that modern LLMs (e.g., GPT-4) is able to perform reasonably well and communicate effectively in collaborative tasks. The expert trajectories generated by LLMs are used to construct the supervised dataset \(\) which maps individual agent's (observation, action) pairs to natural language communication messages. \(\) is used during the training of MARL to provide supervised learning signals to align the learned communication protocols with human language. More implementation details of LLM agents and data collection can be found in the Appendix.

### Multi-agent Reinforcement Learning with aligned communication

The MARL with communication pipeline is similar to IC3Net  in which each agent has an independent controller model to learn how and when to communicate. During each time-step, input observation \(o^{i}_{t}\) is encoded and passed into each agent's individual LSTM. The hidden state of LSTM \(h^{i}_{t}\) is then passed to the probabilistic gating function to decide whether to pass a message to other agents. A single-layer communication network transforms \(h^{i}_{t}\) into communication vector \(c^{i}_{t}\). The mean communication vector of all agents is finally used by each agent's LSTM to produce the action \(a^{i}_{t}\).

To shape the learned communication protocol toward human language, we introduce an additional supervised learning loss during the training of MARL. Specifically, at each time step, we sample a reference communication from the dataset based on each agent's observation and action, \(D(o^{i}_{t},a^{i}_{t})\), representing how a human (LLM) would communicate in the same situation. We then calculate the cosine similarity between the agent communication vector \(c^{i}_{t}\) and the word embedding of the reference communication \(c_{h}\). To align the communication space learned by MARL with the high-dimensional embedding space of natural language, we construct the supervised loss function as follows:

\[L_{sup}=_{t T}_{i}[1-cos(c^{i}_{t},D(o^{i}_{t}, a^{i}_{t}))] \]

\[D(o^{i}_{t},a^{i}_{t})=c_{h}&(o^{i}_{t},a^{i}_{t}) \\ & \]

The construction of the communication message is shaped by two learning signals: 1) the reinforcement learning objective which determines useful information to share with other agents based on the policy loss gradient, and 2) the supervised learning objective which imitates the communication messages used by LLM agents in dataset \(\). The total loss function is formulated as follows:

\[L=L_{RL}+ L_{sup} \]

The hyperparameter \(\) is used to scale the supervised loss. Each agent's policy is optimized with backpropagation to minimize the joint loss.

## 5 Experiments

### Environments

In this section, we evaluate our proposed method in two multi-agent collaborative tasks with varied setups and different characteristics. The first environment, Predator Prey , is widely used in comm-MARL research as a benchmark. We include this environment to represent team tasks that require all agents to share their partial observations for the team to form a complete picture of the task state. The second environment, Urban Search & Rescue (USAR) [25; 34], presents a more demanding challenge due to the inclusion of heterogeneous team members and the temporal dependence between their behaviors. Here, agents must communicate not only their observations but also their intentions and requests to effectively coordinate. Illustrations of the two environments are shown in Figure 1, and more details are provided in the Appendix.

### Experiment setups

We compare our proposed pipeline LangGround against previous methods, including IC3Net , autoencoded communication (aeComm) , Vector-Quantized Variational Information Bottleneck (VQ-VIB) , prototype communication (protoComm) , and a control baseline of independent agents without communication (noComm). aeComm represents the state-of-the-art multi-agent communication methods that grounds communication by reconstructing encoded observations. It has been shown to outperform end-to-end RL methods and inductive biased methods in independent, decentralized settings. VQ-VIB and prototype-based method are representative solutions for human-interpretable communication, which learn a semantic space for discrete communication tokens and perform reasonably well in human-agent teams. Finally, IC3Net has a similar architecture to our proposed pipeline representing an ablating baseline without language grounding.

All methods are implemented with the same centralized training decentralized execution (CTDE) architecture for a fair comparison. Each agent has an observation encoder, an LSTM for action policy, a single-layer fully-connected neural network for transferring hidden states into communication messages, and a gate function for selectively sharing messages. The parameters of the action policy and obs encoder are shared during training for a more stable learning process. We use REINFORCE  to train both the gating function and policy network. The communication messages are continuous vectors of dimension \(D=256\).

## 6 Results

As for the evaluation matrices, we first consider if LangGround allows MARL agents to complete collaborative tasks successfully (i.e., task utility) and converge to a shared communication protocol quickly (i.e., data-efficiency), in comparison with other state-of-the-art methods as the baselines. Then we analyze the properties of aligned communication space such as human interpretability, topographic similarity, and zero-shot generalizability, to show how close the learned language is to the target human natural language. Finally, we evaluate the ad-hoc teamwork performance in which MARL agents must communicate and collaborate with unseen LLM teammates via natural language.

### Task performance

In Figure 2, we compare the task performance of multi-agent teams using different communication methods by plotting out the average episode length during training over 3 random seeds with standard errors.

In Predator Prey vision = 1 (i.e., \(pp_{v1}\)), our method LangGround achieves a similar final performance with IC3Net and aeComm, outperforming other baselines. However, the improvement is not outstanding due to the simplicity of the task environment. In Predator Prey vision = 0 (i.e., \(pp_{v0}\)), the predator's vision range is limited to their own location making effective information sharing more important in solving this search task. As shown in the middle figure, LangGround has a

Figure 2: Learning curves of LangGround in comparison with baseline methods. The y-axis is task performance measured by the episode length until task completion, which is lower the better. The x-axis is the number of training timestamps. Shaded areas are standard errors over three random seeds.

comparable final performance with aeComm and outperforms other baselines. Finally, in the most challenging USAR environment, LangGround outperforms all baselines in solving the task in fewer steps with the same amount of training time-steps. In addition, language grounding also stabilizes the communication learning process such that the variance of LangGround is much smaller than other methods.

To test the performance of proposed method in scaled environments, we ran additional experiments in Predator Prey with a larger map size (i.e., \(pp_{v1}\) (10 by 10)). The learning curves of LangGround and baselines are presented in Fig 4. As shown in the figure, our method outperforms ablating baselines without language grounding (i.e., IC3Net) or without communication (i.e., noComm). This result demonstrates the benefit of introducing LangGround in stabilizing the learning process of emergent communication of MARL agents in scaled environments.

In summary, LangGround enables multi-agent teams to achieve on-par performance in comparison with SOTA multi-agent communication methods. Introducing language grounds as an auxiliary learning objective does not compromise the task utility of learned communication protocols while providing interpretability.

### Aligned communication space

In addition to task utility, we are also interested in other properties of the learned communication space, such as human interpretability, topographic similarity, and zero-shot generalizability.

#### 6.2.1 Semantically meaningful space

To evaluate whether the learned communication space is semantically meaningful, we visualize the learned communication embedding space by clustering message vectors sent by agents over 100 evaluation episodes in \(pp_{v0}\), following . The high dimensional vectors are reduced to a two dimension space via t-SNE , and clustered with DBSCAN . As shown in Figure 3, agent communication messages can be clustered into several classifications with explicit meanings associated with agent observation from the environment. For example, the pink cluster on the right side corresponds to the situation where the agent locates in coordinates (0, 3) without vision of prey. We can look up from dataset \(\) for the reference message that has the most similar word embedding with agent communication vectors in the pink cluster. The reference message (i.e., "moving down from (0, 3)") accurately refers to the agent observation, indicating the learned communication space is semantically meaningful and highly aligned with natural language embedding space.

Figure 3: Learned communication embedding space. Communication vectors between agents in \(pp_{v0}\) are visualized with t-SNE and clustered with DBSCAN. Two semantically meaningful clusters are identified as examples, each corresponding to a specific agent observation. We also present the most similar reference message from dataset \(\) to illustrate the alignment between the agent communication space and the human language embedding space.

#### 6.2.2 Human interpretability

Given the goal of aligning agent communication with human language, it is intuitive to evaluate the human interpretability of language-grounded agent communication. We use the offline dataset \(\) as the reference to calculate the similarity between communication messages shared by LangGround agents and LLM agents in same situations. Given 100 evaluation episodes in \(pp_{v0}\), we calculate 1) the cosine similarity between word embedding and agent communication vectors, and 2) BLEU score between natural language messages and reference messages in \(\) with the most similar word embedding as agent communication vectors. The results are shown in Table 1, demonstrating that LangGround achieves significant gains in both metrics, cosine similarity and BLEU score, compared to baselines without alignment. These measures for other baseline methods would be equivalent to random chance because none of them are grounded with language during training, and hence we do not compute their performance on these metrics.

### Ad-hoc Teamwork

The ultimate goal of our proposed pipeline is to facilitate ad-hoc teamwork between unseen agents without pre-coordination. Here, we propose two experiments to evaluate the zero-shot generalizability and ad-hoc collaboration capability of our trained agents.

#### 6.3.1 Zero-shot generalizability

One prerequisite of ad-hoc teamwork is the ability to communicate about unseen states to their teammates. We evaluate this capability by removing a subset of prey spawn locations from environment initialization of \(pp_{v0}\) and training LangGround agents from scratch. In this condition, agents would neither be exposed to nor receive any language grounding for those situations during training. During the evaluation, we record the communication messages used by agents in those unseen situations and compare them with ground truth communication from dataset \(\). Results show that agents are still able to complete tasks when the prey spawns in those 4 unseen locations. As shown in Table 3, the communication messages agents used to refer to those unseen locations are similar to natural language sentences generated by LLMs.

   Env & \(\) Cos sim & \(\) Bleu score \\  \(pp_{v1}\) & 0.82\(\)0.02 & 0.52\(\)0.03 \\ \(pp_{v0}\) & 0.81\(\)0.03 & 0.45\(\)0.12 \\ \(USAR\) & 0.79\(\)0.12 & 0.42\(\)0.04 \\   

Table 1: Similarity gain w/ LangGround

Figure 4: Team performance of LangGround and baselines on Predator Prey with 10 by 10 map and vision range of 1. Figure 5: Team performance of LangGround agent with different levels of language grounding on \(pp_{v1}\) (10 by 10).

   Grounding & Cos sim & Bleu score \\ 
100\% & 0.775 & 0.633 \\
75\% & 0.667 & 0.498 \\
50\% & 0.304 & 0.308 \\
25\% & 0.188 & 0.224 \\   

Table 2: Zero-shot generalization evaluation results on \(pp_{v1}\) (10 by 10).

Additionally, we train LangGround agents on Predator Prey (10 by 10) but only provide language grounding in a subset of states (i.e., 25%, 50%, 75%, 100%). The learning curves of LangGround agents with different levels of language grounding are presented in Fig. 5 and Fig. 6. As shown in the figures, the more grounded states, the better the team performance, as well as the better the communication alignment. Table 2 shows similar results of communication alignment in un-grounded states during evaluation. The first column refers to percentage of grounding data used during training. The other two columns are alignment measurements when agents encounter un-grounded states during evaluation.

To summarize, these findings confirm the alignment between the agent communication space and the human language word embedding space in zero-shot conditions. More importantly, we show that LangGround is not merely a memorization of one-to-one mapping between observations and communications but also shapes the continuous communication space in a semantically meaningful way. We could ground the agent communication space with the word embedding space of human language on a limited number of instances, and expect LangGround to output interpretable messages in un-grounded states via topographic similarity (discussed more in the Appendix) of the aligned space. In practice, this assumption depends on many factors such as the coverage of offline dataset, number of grounded states, dimension of the communication space, scale of the problem, etc. The above experiments illustrate the impact of language grounding percentage, and we leave further investigation to future work.

#### 6.3.2 Ad-hoc teamwork between MARL and LLM agents

Finally, we evaluate the performance of our agents to work with unseen teammates in ad-hoc teamwork settings. Ad-hoc teamwork refers to situations where agents collaborate with unseen teammates without pre-coordination. In this work, we use embodied LLM agents to emulate human behaviors in human-agent ad-hoc teams. Teams with 2 MARL agents and 1 LLM agent were evaluated on 8 episodes over 3 random seeds, resulting in a total of 24 evaluation episodes per condition. Ad-hoc teamwork performance is measured by the number of steps taken to complete the task; therefore, lower is better. Means and standard deviations of each condition are reported in Table 4.

We find that 1) homogeneous teams (i.e., LangGround and LLM) achieve better performance than ad-hoc teams because of their common understanding of both action and communication. As those agents are either co-trained together or duplicates of the same network, they form a stable strategy for team coordination and information sharing. Since ad-hoc teams (e.g., LangGround + LLM) were not trained together nor speak the same language, their decreased performance is expected. 2) The ad-hoc team performance of LangGround agents is better than noComm and aeComm agents in at least two out of three evaluation scenarios. Because aeComm is not aligned with human language, it serves as a baseline with a coordinated action policy and a random communication policy. The advantage of our method over aeComm and noComm merely comes from effective information sharing via the

   Team composition & \(pp_{v1}\) & \(pp_{v0}\) & \(USAR\) \\  LangGround & 4.3 \(\) 1.20 & 10.9 \(\) 4.53 & 22.0 \(\) 4.24 \\ LLM & 6.8 \(\) 5.20 & 11.6 \(\) 5.30 & 15.9 \(\) 3.37 \\  LangGround + LLM & **8.5 \(\) 5.76** & **15.5 \(\) 4.80** & 23.2 \(\) 10.61 \\ aeComm + LLM & 10.3 \(\) 6.46 & 17.5 \(\) 4.60 & 20.3 \(\) 9.07 \\ noComm + LLM & 10.6 \(\) 5.73 & 20.0 \(\) 0.00 & 32.4 \(\) 13.47 \\   

Table 4: Ad-hoc teamwork performance (lower is better)

   Prey Loc & Cos sim & Bleu score & Example message \\  (1,1) & 0.81 & 0.41 & Moving up to converge on prey location at (1,0) for capture \\ (1,3) & 0.81 & 0.27 & Converging on prey location at (1,3) \\ (3,1) & 0.82 & 0.51 & Moving up toward prey location at (3,1) \\ (3,3) & 0.78 & 0.72 & Converging on prey location at (3,3) \\   

Table 3: Zero-shot generalizability in \(pp_{v0}\)aligned language with unseen teammates. The empirical evidence presented in this section confirms the application of our method in ad-hoc teamwork.

## 7 Discussion

In this work, we developed a novel computational pipeline to enhance the capabilities of MARL agents to interact with unseen teammates in ad-hoc teamwork scenarios. Our approach aligns the communication space of MARL agents with an embedding space of human natural language by grounding agent communications on synthetic data generated by embodied LLMs in interactive teamwork scenarios.

Through extensive evaluations of the learned communication protocols, we observed a trade-off between utility and informativeness. According to the Information Bottleneck principle , informativeness corresponds to how well a language can be understood in task-agnostic situations, while utility corresponds to the degree to which a language is optimized for solving a specific task. By introducing the additional supervised learning signal, our method pushes the trade-off toward informativeness compared to traditional comm-MARL methods that merely optimize for utility. This partially explains the "inconsistent" patterns we observe in Figure 2 and Table 4 across different task scenarios. In relatively easy tasks such as predator-prey, the learned communication is more optimized for informativeness, aligning better with human language and generalizing better in ad-hoc teamwork. In more challenging tasks such as USAR, the learned communication is more shaped toward task utility, resulting in faster convergence but less interpretability to unseen teammates.

Additionally, we found that introducing language grounding does not compromise task performance but even accelerates the emergence of communication, unlike the results reported in previous literature, where jointly optimizing communication reconstruction loss and RL loss leads to a drop in task performance [26; 27]. The main reason for this contradiction is that our dataset \(\) consists of expert trajectories from LLM embodied agents with a well-established grounding on the task. Therefore, the language grounding loss not only shapes the communication but also guides the action policy of MARL agents by rewarding behavior cloning and providing semantic representations of input observations.

It worth noting that LangGround is an extremely flexible pipeline with most of the modules being interchangeable, such as the word embedding model, base MARL-comm model, message dataset source, and the translation module. This allows us to allocate appropriate tasks to LLMs and RL respectively, considering LLMs are known to have good linguistic capabilities (e.g., describing) while struggle with formal reasoning (e.g., planning). While we use embodied LLM agents to collect grounding communication datasets for LangGround as explained in Section 4, These datasets can also come from rule-based agents or human participants, as long as they show effective communication in solving collaborative tasks. Because only communication messages are used, LLM agents' hallucinations and does not impact MARL's task performance directly. In more complicated scenarios in which communication is beyond describing observation and action, we could still expect LLMs to generate reasonable outputs. Compared to alternative methods in embodied agents where LLMs must make correct action planning at every timestamp, it is more feasible to collect semantically meaningful messages from either LLMs or any other sources.

We believe our work would benefit the broader society for the following reasons. This research provides empirical evidence of linguistic principles during language evolution among neural agents, which might provide insights for broader research communities, including computational linguistics, cognitive science, and social psychology. The usage of embodied LLM agents as interactive simulacra of human team behaviors has a broad impact since it has potential applications in social science and may deepen our understanding of modern LLMs. Most importantly, our proposed pipeline takes initial steps in enabling artificial agents to communicate and collaborate with humans via natural language, shedding light on the broad research direction of Human-centered AI.

As for future directions, we plan to evaluate our proposed pipeline in more complicated task environments at scale and experiment with different selections of MARL algorithms, backbone LLMs, and word embeddings. Particularly, we plan to replace the use of a static dataset \(\) by querying LLMs online during the training of MARL. This may allow us to capture complex information exchanged among team members in addition to individual observations, such as beliefs, intents, and requests.