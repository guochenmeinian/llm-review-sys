# Towards a Comprehensive Benchmark for High-Level Synthesis Targeted to FPGAs

Yunsheng Bai, Atefeh Sohrabizadeh, Zongyue Qin, Ziniu Hu, Yizhou Sun, Jason Cong

Department of Computer Science

University of California, Los Angeles

{yba,atefehsz,qinzongyue,bull,yzsun,cong}@cs.ucla.edu

###### Abstract

High-level synthesis (HLS) aims to raise the abstraction layer in hardware design, enabling the design of domain-specific accelerators (DSAs) targeted for field-programmable gate arrays (FPGAs) using C/C++ instead of hardware description languages (HDLs). Compiler directives in the form of pragmas play a crucial role in modifying the microarchitecture within the HLS framework. However, the number of possible microarchitectures grows exponentially with the number of pragmas. Moreover, the evaluation of each candidate design using the HLS tool consumes significant time, ranging from minutes to hours, leading to a slow optimization process. To accelerate this process, machine learning models have been used to predict design quality in milliseconds. However, existing open-source datasets for training such models are limited in terms of design complexity and available optimizations. In this paper, we present HLSyn, a new benchmark that addresses these limitations. It contains more complex programs with a wider range of optimization pragmas, making it a comprehensive dataset for training and evaluating design quality prediction models. The HLSyn benchmark consists of 42 unique programs/kernels, each of which has many different pragma configurations, resulting in over 42,000 labeled designs. We conduct an extensive comparison of state-of-the-art baselines to assess their effectiveness in predicting design quality. As an ongoing project, we anticipate expanding the HLSyn benchmark in terms of both quantity and variety of programs to further support the development of this field.

## 1 Introduction

In recent decades, the demand for specialized computing systems tailored to specific applications has significantly increased. This has led to the emergence of domain-specific accelerators (DSAs) being implemented in either application-specific integrated circuits (ASICs) or field-programmable gate arrays (FPGAs). By leveraging the unique characteristics of specific workloads, the designer can design DSAs to enhance performance and energy efficiency. This becomes particularly valuable when general-purpose processors like CPUs and GPUs cannot meet the performance or efficiency requirements of certain applications due to the end of Dennard scaling . For instance, Google has developed its custom-designed DSA in the form of an ASIC named the Tensor Processing Unit (TPU) , which is highly optimized for machine learning workloads, offering remarkably faster performance and improved energy efficiency compared to CPUs and GPUs. In addition, FPGAs offer a cost-effective alternative with reconfigurability, making them increasingly appealing for accelerating applications across various domains, including search engines and numerous datacenter applications , machine learning inference acceleration , and autonomous vehicles , among others.

Nevertheless, the design of DSAs poses distinct difficulties in contrast to general-purpose hardware like CPUs and GPUs . DSAs are commonly developed using hardware description languages (HDLs) at the register-transfer level (RTL), specifically Verilog and VHDL, which are primarily known to circuit designers. To address this challenge, high-level synthesis (HLS)  was introduced and is now supported by most EDA (Electronic Design Automation) and FPGA companies . HLS raises the level of design abstraction to C/C++/OpenCL/SystemC, enabling designers to describe the high-level behavioral representation of their designs rather than the low-level data transition in RTL. This abstraction eliminates the need for explicit clock scheduling specifications in the HLS code. Instead, HLS tools analyze the behavior description to schedule operations across different clock cycles, assign operations to available resources, and establish the required control structure. Finally, the HLS tool automatically generates RTL code based on these analyses. It can take several minutes to hours for the HLS tool to generate this RTL code . The RTL can then be passed through logic synthesis and physical design steps, which can consume several hours, to be implemented on the target FPGA. This HLS tool enhances design productivity, shortens design cycles, and allows designers to rapidly explore various design options without the need for manual RTL code writing.

Despite the increased level of design abstraction offered by high-level synthesis (HLS) tools, they still require a considerable amount of hardware design expertise to utilize synthesis directives in the form of pragmas. These pragmas play a crucial role in specifying various aspects of the design, such as memory organization, caching strategies, memory buffer partitioning, parallelization and pipelining of computations, etc. As demonstrated by Chi et al. , although the performance of a DSA with no performance-optimizing pragmas can be \(108\) slower than a CPU, through proper optimization, it can achieve a remarkable performance improvement, surpassing a CPU by \(89\). However, the optimization process for architecture-specific enhancements is typically limited to hardware programmers and falls beyond the capabilities of the average software programmers. Consequently, there has been a growing focus on automating this optimization process. While some approaches treat the HLS tool as a black box and develop custom-designed heuristics to search through design candidates, a more recent research paradigm leverages machine learning and deep learning techniques. These approaches either learn the behavior of the HLS tool and construct predictive models or employ data-driven exploration methods to search through the solution space. The goal of automating the optimization process is to democratize customized computing and make it more accessible for the average software programmers, allowing them to utilize the tailored hardware acceleration.

To address the need for automating pragma insertion and parameter tuning in high-level synthesis (HLS), machine learning techniques can be used. This approach aims to achieve optimal quality in terms of latency and resource utilization. However, the lack of open-source datasets in this domain and the limitations of existing datasets, which are constrained in terms of design complexity and available optimizations, restrict their practicality. To bridge this gap, this paper introduces HLSyn, the first

Figure 1: High-level synthesis (HLS) transforms the source code of the kernel 2mm written in C into a lower-level programming language and eventually implements the design on the target FPGA as shown with the chip die photo.

comprehensive benchmark for HLS designs targeted to FPGAs for performance optimization. This benchmark provides more complex programs and a wider range of optimization pragmas, facilitating advanced research and facilitating in-depth exploration of machine learning techniques in the context of HLS. In our study, we define a design as a piece of C/C++ source code (referred to as a kernel) with associated pragmas. Our primary focus is on predicting the quality of a design using supervised learning by running a model trained on a collection of labeled designs with corresponding quality metrics. However, our dataset can be utilized in various training scenarios, including training agents to efficiently explore the solution space.

## 2 Background

The task of HLSyn is to predict the quality of the HLS design specified by a program (kernel) with a specific optimization pragma design. Our target implementation platform is FPGA, although similar techniques can be applied to ASIC accelerator designs as well. The quality of a design is defined as a function of its performance, which is measured by its latency in cycle counts, and its area/resource utilization, such as the usage of digital signal processing blocks (DSP), blocks' RAMs (BRAM), flip-flops (FF), and lookup-tables (LUT), which are the fundamental building blocks for implementing digital logic circuits in FPGA designs.

In this work, we specifically consider the optimization pragmas of the Merlin Compiler, an open-source source-to-source optimization tool used for efficient AMD/Xilinx HLS designs. The Merlin Compiler provides three types of optimization pragmas, namely PIPELINE, PARALLEL, and TIIE to define the desired microarchitecture .

As illustrated in Figure 1, these pragmas can be applied at the loop level and offer control over the type of pipelining, the parallelization factor, and the amount of data caching. If setting the pragmas properly to non-default parameters for proper parallelizing and pipelining the computation, the resulting accelerator can be \(10\) or even \(100\) faster than a single-core CPU. However, without any pragma insertion, the resulting hardware can be \(10\) slower than a CPU. Figure 2 demonstrates an example where the prediction targets are sensitive to the pragma settings. It is noteworthy that in some other examples, a change in the pragma settings does not lead to any change in the output targets. The machine learning model must learn from existing labeled designs and understand the source code as well as the pragmas in order to accurately forecast the outcome of each design when eventually being executed on an FPGA.

Table 1 summarizes the parameter space of these pragmas. For a given program/kernel, any change in the option of any of the pragmas results in a different design with a unique microarchitecture. The "fg" option in pipelining refers to the case where all the inner loops are unrolled (parallelized with separate logic) and each parallel unit is pipelined. The "cg" option, on the other hand, results in coarse-grained processing elements (PEs) that are pipelined together. For example, it can create pipelined load-compute-store units. The PARALLEL and TIIE pragma take numeric values that determine the degree of parallelization and loop tiling, respectively.

Figure 2: Two example designs selected from the 2mm kernel. In Figure 1, the kernel contains 14 pragmas, and each row has the format “\(\)<pragma name\(>\)’: <pragma setting>”. A small change in one of the pragma parameters leads to changes in the FPGA HLS results, i.e., the five prediction targets for the regression task.

## 3 Related Work

In previous research, optimizing HLS designs has been approached in different ways. One category of methods treats the HLS tool as a black box and utilizes problem-independent heuristics or develops dedicated heuristics to explore the solution space and evaluate the quality of results (QoR) directly using the tool [34; 43; 35]. However, this approach is time-consuming as each evaluation takes several minutes to hours. To mitigate this issue, another category of methods aims to create surrogate models for the HLS tool. Some of these methods construct dependency graphs of the program and employ traditional graph analysis techniques to schedule operations and estimate the QoR accordingly [46; 38], while others develop analytical performance and area models to estimate the QoR [45; 47]. Nevertheless, due to the different heuristics employed by HLS tools in the design process, these models may not accurately predict the QoR . Some methods address this limitation by focusing on designs that can exploit pre-defined microarchitecture templates or follow specific computation patterns [33; 37; 12], but this can limit their generality. Alternatively, a data-driven approach utilizing machine learning and deep learning models has been proposed to enhance prediction accuracy [23; 22]. Graph neural networks (GNNs) have gained attention in this context and demonstrated promising results in enhancing prediction accuracy [30; 5; 40; 36].

A fundamental aspect of these approaches is the availability of a large-scale database to effectively train the models. Recent works have focused on gathering such databases [16; 41; 30]. Unfortunately, existing datasets have limitations. The dataset in  predominantly consists of synthetic programs that do not utilize any pragmas. DB4HLS  targets programs from the MachSuite benchmark but overlooks the inclusion of a key optimization pragma, pipelining. Additionally, DB4HLS views each function in the program as an individual kernel. GNN-DSE  targets programs from the Polyhedral benchmark , which features more complex kernels for FPGA mapping, in addition to the MachSuite benchmark. This dataset considers a program with all its sub-functions as a kernel, further increasing design complexity. Despite covering a wide range of optimization pragmas for each kernel, the generated dataset is small, with only 9 target kernels and a total of 4,752 data points. To address these limitations, we propose HLSyn, which includes kernels from both the Polyhedral and MachSuite benchmarks. It encompasses a diverse range of optimization pragmas that can pipeline and/or parallelize computation, as well as adjust data caching. Our benchmark is comprised of 42 unique kernels from various domains summarized in Table 2, totaling over 42,000 design points, providing a comprehensive resource for advancing research and facilitating an in-depth exploration of machine learning techniques for HLS.

## 4 The HLSyn Benchmark

In this section, we introduce the datasets in HLSyn1. The input data source comes from 42 selected kernels in the MachSuite benchmark  and the PolyBench benchmark suite . Our selected 42 kernels cover a wide range of applications whose descriptions are shown in Table 2.

The benchmark consists of 2 datasets accumulated in the past three years, corresponding to two versions of AMD/Xilinx HLS tools: (1) SDx (v1)  and Vitis (v2) , with the AMD/Xilinx Alveo U200 as the target FPGA and a working frequency of 250MHz. For each dataset, we select 6 kernels as the held-out testing kernels. They will test the ability of a model to generalize to kernels that it did not see during the training. For the rest of the kernels, we perform a random split with the training, validation, and testing ratio being 70:15:15. Summary statistics of datasets are given in Table 3.

For each dataset kernel in each dataset, we run the two versions of the tools described above to obtain a set of labeled designs. Since the design space size is exponential to the number of pragmas, we rely

 Pragma Name & Parameter Name & Parameter Space & Examples of Pragma Settings \\  PARALLEL & factor & integer & 4, 8 \\ PIPELINE & mode & “cg”, “fg”, off & ‘flatten’ resulting in the “fg” mode \\ TILE & factor & integer & 2, 4 \\ 

Table 1: Target pragmas with their options.

on heuristics provided by AutoDSE  to generate the labels for a subset of all possible designs. The labels come in the form of 5 target values: PERF, DSP, BRAM, LUT, and FF. In addition, according to whether the PERF is greater than a threshold value, we classify a design into two categories: valid and invalid. For the valid designs, we perform the regression task of predicting each one of the 5 target values. Table 4 shows the statistics of the prediction targets, i.e., output of a machine learning model.

**Dataset** & **\#D for r** & PERF & DSP & BRAM & LUT & FF & **\#D for c** & **T.F** \\   SDx (v1) & 37 & 8.0 & 629.9 & 366.7 & 589.6 & 4 & 7 &  & 82 & 8 & 56 & 3 & 4 \\ Vrits (v2) & 29 & 7.6 & 629.9 & 334.0 & 534.7 & 4 & 7 &  & 74 & 8 & 56 & 3 & 4 \\ 

Table 4: Dataset statistics for the output. The meanings of columns are **#D for r**: # designs for the regression task, PERF: range of the PERF target, DSP: range of the DSP target, BRAM: range of the BRAM target, LUT: range of the LUT target, FF: range of the FF target, **#D for c**: # designs for the classification task, **T.F**: True class (Valid) vs False class (Invalid) design ratio.

**Kernel** & **Source** & **Description** & **\# pragmas** & **\# in v1** & **\# in v2** \\ 
23mm & PolyBench & 2 Matrix Multiplications & 14 & 812 & 861 \\
3mm & PolyBench & 3 Matrix Multiplications & 21 & 848 & - \\ adj & PolyBench & Alternating Direction Implicit solver & 13 & 551 & - \\ ases & MachSuite & Advanced Encryption Standard & 3 & 45 & 43 \\ atax & Multi transpose and Vector Multiplication & 5 & 884 & 902 \\ atax-medium & PolyBench & Matrix Transpose and Vector Multiplication & 5 & 362 & 544 \\ nbc & PolyBench & BiCG Sub Kernel of BiCGStab Linear Solver & 5 & 512 & 498 \\ bicc-large & PolyBench & BiCG Sub Kernel of BiCGStab Linear Solver & 4 & - & 456 \\ bicc-medium & PolyBench & BiCG Sub Kernel of BiCGStab Linear Solver & 5 & 316 & - \\ correlation & PolyBench & Correlation Computation & 17 & 1522 & 699 \\ covariance & PolyBench & Covariance Computation & 13 & - & 356 \\ bottom & PolyBench & Multiresolution Analysis & 6 & 179 & 172 \\ bottom-r & PolyBench & Multiresolution Analysis & 7 & 595 & 230 \\ fptd-2D & PolyBench & 2-D Finite Different Time Domain Kernel & 16 & 660 & - \\ fptd-2D-t & PolyBench & 2-D Finite Different Time Domain Kernel & 16 & - & 240 \\ gemm-b & MachSuite & Blocked Version of Matrix Multiplication & 9 & 775 & 440 \\ gemm-m & MachSuite & Matrix Transpose and Vector Multiplication & 7 & 749 & 540 \\ gemm-r & PolyBench & Matrix Multiplication & 8 & 1160 & 714 \\ gemm-p-l & PolyBench & Matrix Transpose and Vector Multiplication & 8 & - & 199 \\ gemmver & PolyBench & Vector Multiplication and Matrix Addition & 13 & 924 & 712 \\ gemmvmv & PolyBench & Vector Multiplication and Matrix Addition & 13 & 3365 & - \\ gesummv & PolyBench & Scalar, Vector and Matrix Multiplication & 4 & 442 & 371 \\ gesummv-m & PolyBench & Scalar, Vector and Matrix Multiplication & 4 & 304 & - \\ hear-3D & PolyBench & Heat Equation over 3D Data Domain & 11 & 1664 & - \\ jacobi-1D & PolyBench & 1-D Jacobi Stencil Computation & 5 & 595 & - \\ jacobi-2D & PolyBench & 2-D Jacobi Stencil Computation & 11 & 1862 & - \\ mb & MacSuite & -body Molecular Dynamics & 3 & 12 & - \\ mvt & PolyBench & Matrix-Vector Product and Transpose & 8 & 1175 & 1452 \\ mvt-m & MacSuite & Matrix-Vector Product and Transpose & 8 & 416 & - \\ nn & MacSuite & Dynamic Programming for Sequence Alignment & 6 & 1347 & 615 \\ seidel-2d & PolyBench & 2-D Seidel Stencil Computation & 7 & 1314 & - \\ spmv-cts & MacSuite & Sparse Mat-Vector Mult. w1 Variable-Len. Neighbor & 3 & 114 & 114 \\ spmv-ELpack & MacSuite & Sparse Mat-Vector Mult. w1 Fixed-size Neighbor & 3 & 114 & 102 \\ stencil & MachSuite & A Two-Dimensional Stencil Computation & 7 & 1404 & 1016 \\ stencil-3d & MacSuite & A Three-Dimensional Stencil Computation & 5 & 239 & 239 \\ svmm & PolyBench & Symmetric Matrix Multiplication & 7 & 153 & 158 \\ svmm-opt & PolyBench & Symmetric Matrix Multiplication & 8 & - & 281 \\ svm-opt-m & PolyBench & Symmetric Matrix Multiplication & 8 & 315 & - \\ svmk & PolyBench & Symmetric Rank-2k Operations & 8 & 433 & 793 \\ syrk & PolyBench & Symmetric Rank-k Operations & 8 & 660 & 234 \\ trmm & PolyBench & Triangular Matrix Multiplication & 7 & 231 & 968 \\ trmm-opt & PolyBench & Triangular Matrix Multiplication & 7 & 964 & 281 \\ 

Table 2: There are 42 kernels in total across SDx (v1) and Vitis (v2) spanning multiple domains such as linear algebra on vectors and matrices, data mining, stencil operations, encryption, dynamic programming, etc. **#p** denotes the number of pragmas in the kernel. **# in v1** and **# in v2** denote the number of labeled designs in SDx (v1) and Vitis (v2) respectively.

**Dataset** & **\#K** & **\#K** & **\#AFT** & **\#N** & **\#E** & **full** & **\#pt** & **\#U** & **\#U** & **\#U** & **\#U** \\  SDx (v1) & 37 & 8.0 & 629.9 & 366.7 & 589.6 & 4 & 7 &  & 82 & 8 & 56 & 3 & 4 \\ Vrits (v2) & 29 & 7.6 & 629.9 & 334.0 & 534.7 & 4 & 7 &  & 74 & 8 & 56 & 3 & 4 \\ 

Table 3: Dataset statistics for the input. The meanings of columns are: **#K**: # kernels, **A#K**: average # pragmas per kernel’, **A#T**: average # source code tokens per kernel, **A#N**: average # nodes per kernel’s graph, **A#E**: average # edges per kernel’s graph, **#nt**: # node types, **#pt**: # pragma node types, **nr**: numeric attribute range, **#it**: # instruction type, **#ft**: # flow types, **#bt**: # block types, **#ept**: # edge position types, **#left**: # edge position type.

Since our task aims to predict the validity (classification) and quality (5-target regression) of the designs, we provide both the source code and the graph representation derived from ProGraML. Specifically, we follow  to compile a kernel's source code into assembly code, and then transform the assembly codes into a control data flow graph (CDFG) with call relation, and eventually add pragma nodes to the ProGraML graph following . The resulting pragma-augmented ProGraML graphs for four selected kernels are depicted in Figure 3.

It is noteworthy that there are multiple ways to represent the input source programs. As an illustration, we include in this dataset the graph representation used in . Other representations are possible, as such abstract syntax trees. Since we release the source code, it is possible to derive and generate such other representations.

## 5 Experiments on HLSyn

This section provides several baseline experiments with their results to investigate the performance of various methods on the task of design quality prediction.

### Baseline Methods

All the baseline methods share the same encoder-decoder architecture and differ only in the encoder. Specifically, each method encodes a design into a \(D\)-dimensional vector where \(D=512\) by default and uses a MLP-based decoder to transform the design embedding into the targets.

code2vec code2vec is a path-based attention model. It first decomposes the code to a collection of paths in its abstract syntax tree and represent them as a bag of distributed vector representations. Then it uses an attention mechanism to compute a learned weighted average of the path vectors as the overall representation of the code.

code5-rand, coded5-frozen, and coded5These three methods are based on the code2vec method which performs pre-training on a large amount of source code. We utilize the small version released by the authors to encode the design. coded5 fine-tunes both the encoder and the decoder, coded5-frozen freezes the encoder and only fine-tunes the decoder, while coded5-rand fine-tunes both but replaces the encoder parameters/weights with a random initialization to study the effect of pre-training on our tasks.

In order to handle the long source code as a sequence of code tokens, we set the maximum sub-sequence length (i.e., the maximum number of tokens allowed in a sub-sequence) to be 64, and apply a sliding window of size 64 over the source code to obtain multiple sub-sequences as input to the transformer-based encoder. At the beginning of each sub-sequence, a special starting token is inserted and its embedding is taken as the sub-sequence level embedding, and all the sub-sequence embeddings are aggregated into the final \(D\)-dimensional embedding for the design.

g-codebert and g-codebert-LSimilar to coded5, g-codebert is another pre-trained source code encoder, yet with a larger embedding dimension (768 instead of 512), and is thus

Figure 3: Visualization of the pragma-augmented ProGraML graphs of four selected kernels. Node colors indicate the block attribute derived from the assembly code. Edge colors indicate the edge flow.

presumably more expressive. g-codebert-L uses a larger maximum sub-sequence length, 128 instead of 64, which would capture a longer dependency between source code tokens, and may yield better results.

gnn-dse and gnn-dse-2lIn contrast to the previous methods which only receive the source code as input, these methods receive the assembly-level graph (with examples in Figure 3) as input and use TransformerConv with a jumping knowledge network  to produce the design embeddings. gnn-dse employs 8 layers whereas gnn-dse-2L only utilizes 2.

[codet5,gnn-gse] and [g-codebert,gnn-gse] These two methods concatenate the design embeddings produced by a source code transformer and a graph neural network-based encoder, i.e., the input embedding into the decoder is \(2 D\) instead of \(D\).

### Evaluation Protocol

MetricsThere are two tasks for our dataset: regression and classification. The goal of the regression task is to predict the five targets: PERF, DSP, BRAM, LUT, and FF. We use rooted mean square error (RMSE) to evaluate each method. And the goal of the classification task is to predict whether a design is valid or not, i.e. whether the downstream RTL and physical synthesis are likely to complete or not. We use the classification accuracy as the evaluation metric.

In addition, recall that we have two versions of the dataset (SDx (v1) and Vitis (v2)). Each (version,task) combination receives a separate evaluation. For each evaluation, there are two evaluating settings: transductive and inductive testing. Specifically, for each (version, task) combination: (1) We select six kernels as the held-out testing kernels. These kernels are never seen during training and are used for the inductive testing; (2) For the rest of the kernels, we merge all the labeled designs, and randomly split them into training, validation, and transductive testing designs with the 70:15:15 ratio; (3) Using the training designs for 1000 epochs for the regression task, and 200 epochs for the classification task, we train each baseline method. We employ the validation set to determine the best epoch to use for testing; (4) We test the trained model on the transductive testing set. It is called transductive (**"Trans"**) since this testing set contains designs from kernels that are seen during training; (5) We test the trained model on the held-out six kernels. Specifically, we (5.1) select the 30% designs from each held-out kernel as the testing designs, (5.2) then repeat the following procedure 5 times. For each kernel, from the rest of the 70% remaining designs, 10 designs are sampled and are utilized to adapt the trained model for 10 epochs. Then, the adapted model is tested on the 30% designs for that held-out kernel. We call such a setting inductive because the model is tested on six kernels that are not visible in the training stage in a zero-shot (**"Ind"**) or few-shot learning setting (**"Ind Adapt"**).

### Results and Analysis

The overall regression and classification results are exhibited in Tables 5 and 6. Tables 7, 8, 9, and 10 reveal the breakdown results over individual held-out kernels.

  &  &  \\
**Method** & **Trans** & **Ind** & **Ind Adapt** & **Trans** & **Ind** & **Ind Adapt** \\  code2vec & 0.7576 & 0.5662 & 0.6617 & 0.7060 & **0.5444** & 0.6337 \\ coded-gram & 0.8524 & 0.5257 & 0.7015 & 0.7924 & 0.4851 & 0.6291 \\ coded-progzen & 0.7515 & 0.6098 & 0.7486 & 0.7334 & 0.4161 & 0.6061 \\ coded5 & 0.9501 & 0.6394 & 0.7447 & 0.9045 & 0.4781 & 0.6734 \\ g-c-dcepter & **0.9536** & 0.6478 & 0.7610 & **0.9233** & 0.5415 & 0.7024 \\ g-c-dcepter & 0.9204 & 0.5730 & 0.7701 & 0.8970 & 0.5342 & 0.7180 \\ gnn-dse & 0.9422 & **0.6529** & **0.7623** & 0.9045 & 0.4781 & 0.6734 \\ gnn-dse-2L & 0.8912 & 0.6085 & 0.7421 & 0.9126 & 0.5303 & **0.7632** \\  code2vec & 0.9434 & 0.6141 & 0.7385 & 0.9195 & 0.5053 & 0.7002 \\ (g-c-dcepter & 0.9212 & 0.6174 & 0.7446 & 0.9126 & 0.5001 & 0.7160 \\ 

Table 6: Classification accuracy on SDx (v1) and Vitis (v2).

**Method** & **dominance** & **fptd-2D-l** & **GEMM-N** & **gdm-p-l** & **SYMM** & **trmm-opt** \\  code2vec & 2.49747.009 & 3.125940.18 & 4.52626.109 & 3.738940.03 & 1.420940.11 & 2.637948.01 \\ coded5-rand & 1.63880.13 & 2.458740.22 & 2.84740.15 & 3.07224.04 & 0.496940.03 & 1.22958.009 \\ coded5-frozen & 1.21370.02 & 2.449440.22 & 2.657640.10 & 3.14200.02 & 0.385440.02 & 1.219940.02 \\ coded5 & 1.35940.15 & 2.295840.07 & 2.843640.37 & 2.780340.07 & 0.409240.09 & **0.842742.014** \\ g-coder & 1.065220.06 & 1.924540.09 & 2.377240.31 & **2.391340.14** & 0.49109.08 & 0.945240.09 \\ g-coderbert-L & **0.910740.08** & **1.774040.11** & 2.663540.33 & 2.502940.09 & 0.338440.03 & 1.146940.11 \\ g-c-dcepter & 1.14966.04 & 0.641440.0 & 2.315740.23 & 2.782840.18 & 0.411040.02 & 0.779096.006 \\ g-coder & 1.145740.02 & 1.952140.09 & 2.024840.26 & 2.96910.10 & 0.406740.04 & 0.914640.03 \\  \{coder5,gnn-dse} & 1.098840.07 & 1.853240.10 & 2.243040.23 & 2.548940.04 & **0.37660.06** & 1.044040.09 \\ g-coderbert,gnn-dsej & 1.118240.07 & 1.908840.10 & 2.535240.25 & 2.765540.08 & 0.384240.05 & 0.898640.13 \\ 

Table 8: Regression result breakdown on Vitis (v2) on individual test kernels.

**Method** & **dominance** & **fptd-2D-l** & **GEMM-N** & **gdm-p-l** & **SYMM** & **trmm-opt** \\  code2vec & 2.49747.009 & 3.125940.18 & 4.526260.19 & 3.738940.03 & 1.420940.11 & 2.637948.01 \\ coded5-rand & 1.63880.13 & 2.458740.22 & 2.84740.15 & 3.072240.04 & 0.496940.03 & 1.22958.009 \\ coded5-frozen & 1.21370.02 & 2.449440.22 & 2.657640.10 & 3.142040.02 & 0.385440.02 & 1.219940.02 \\ coded5 & 1.35940.15 & 2.295840.07 & 2.843640.37 & 2.780340.07 & 0.409240.09 & **0.842742.014** \\ g-coder & 1.065220.06 & 1.924540.09 & 2.377240.31 & **2.391340.14 & 0.49109.08 & 0.945240.09 \\ g-coderbert-L & **0.910740.08** & **1.774040.11** & 2.663540.33 & 2.502940.09 & 0.338440.03 & 1.146940.111 \\ gnn-dse & 1.149640.04 & 1.889740.04 & 2.315740.23 & 2.782840.18 & 0.411040.02 & 0.779096.006 \\ g-coderbert5 & 0.723640.04 & 0.645540.02 & 0.508404.04 & 0.86810.01 & 0.704240.05 & 0.946740.01 \\ g-coderbert,gnn-dsej & 1.060740.06 & 0.645540.03 & **0.671440.03** & 0.859040.02 & 0.80000.04 & 0.937740.02 \\ g-coderbert-L & 0.723640.08 & 0.655650.03 & 0.574140.07 & 0.879240.01 & **0.839440.05** & 0.94880.01 \\ g-coderbert5-frozen & 0.698940.00 & 0.688940.02 & 0.604202.03 & 0.865240.03 & 0.783040.03 & 0.948840.00 \\ g-coderbert5 & 0.667440.05 & 0.677840.03 & 0.654540.02 & 0.685340.03 & 0.664840.04 & 0.934940.03 \\ g-coderbert,gnn-dsej & 0.702240.07 & 0.636440.02 & 0.643840.03 & 0.841940.03 & 0.662020.04 & 0.944640.01 \\ g-coderbert,gnn-dsej & 0.71240.06 & **0.709110.04** & 0.6393940.04 & 0.861640.04 & 0.602820.09 & 0.94260.02 \\ 

Table 9: Classification result breakdown on SDx (v1) on individual test kernels.

design embeddings does not consistently yield better performance. Particularly, [codet5,gnn-gse] and [g-codebert,gnn-gse] achieve the lowest regression error under the transductive setting but fall short when adapted to new kernels. Such results imply that future efforts can be made on studying the generalization abilities of machine learning models on our HLSyn benchmark.

Observation 2: In general, pre-training helps with the performance of source code transformer models.This can be seen by comparing codet5-rand and codet5, where the former starts training from scratch while the latter loads a pre-trained model released by . This should not come as a surprise, because pre-training has been shown to demonstrate success in natural language processing. Our experimental results verify the effectiveness of pre-training on the HLSyn benchmark. One implication is that one can design better pre-training methods on source code related to electronic design automation, or even design pre-training for GNNs operating on assembly-level graphs.

Observation 3: More GNN message passing layers does not necessarily improve the performance of GNN models.In many cases, the 2-layer version, gnn-dse-2L, performs even better than the 8-layer version, gnn-dse. This may be attributed to the fact that an attention-based global readout function is used to aggregate node embeddings into a graph-level embedding representing the entire design, and thus each node does not necessarily need to reach far-away nodes in the local message passing stage.

Observation 4: Generalization to new kernels is difficult, and the performance after adaptation differs across kernels.For example, g-codebert-L achieves the overall lowest error ("Ind Adapt") on SDx (v1) on the regression task, but Table 7 demonstrates that g-codebert-L does not always yield the lowest error on each of the six held-out kernels. For example, [g-codebert,gnn-gse] performs the best on stencil-3d and trmm-opt, but poorly on dotigen-r, and thus the average regression error over the six held-out kernels is higher than g-codebert-L. This calls for a more in-depth study of the discrepancy between kernels and a model that is capable of generalizing to a diverse set of kernels. In general, adaptation is necessary, since across all the experiments, without any adaptation ("**Ind**"), directly applying the trained model to new unseen kernels leads to poor regression error and classification accuracy.

## 6 Conclusion and Future Work

This work introduces the task of design quality prediction in the forms of regression and classification tasks and presents the HLSyn benchmark to evaluate state-of-the-art program representation learning methods. Although there is no method that consistently outperforms all the other methods, we notice several trends and identify promising directions toward a more accurate prediction model design. As program representation learning is a continuously growing research domain, we plan to maintain the benchmark to test new methods. For example, our recent work  uses hierarchical graphs for program representations to predict design performance. Our HLSyn benchmark is a growing project. We expect to include more kernels and labeled designs running newer versions of HLS tools and establish a leaderboard 2 to encourage participation. In addition, the current benchmark does not consider the design space exploration (DSE) stage, which will be added as the project develops. In fact, the regression task aims to be eventually integrated into the DSE process, which traverses the design space in order to find the optimal pragma setting.

## 7 Acknowledgement

This work was partially supported by NSF 2211557, NSF 1937599, NSF 2119643, NSF 2303037, NASA, SRC JUMP 2.0 Center, Okawa Foundation, Amazon Research, Cisco, Picsart, Snapchat, and CDSC industrial partners ([https://cdsc.ucla.edu/partners/](https://cdsc.ucla.edu/partners/)). We would also like to thank AMD/Xilinx for HACC equipment donation and Marci Baum for editing the paper.