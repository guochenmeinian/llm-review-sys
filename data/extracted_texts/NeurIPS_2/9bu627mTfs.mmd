# Context and Geometry Aware Voxel Transformer for Semantic Scene Completion

Zhu Yu\({}^{1}\)   Runmin Zhang\({}^{1}\)   Jiacheng Ying\({}^{1}\)   Junchen Yu\({}^{1}\)

**Xiaohai Hu\({}^{3}\)**  **Lun Luo\({}^{4}\)**  **Si-Yuan Cao\({}^{2,1}\)1   Hui-Liang Shen\({}^{1}\)1

\({}^{1}\)Zhejiang University  \({}^{2}\)Ningbo Innovation Center, Zhejiang University

\({}^{3}\)University of Washington  \({}^{4}\)HAOMO.AI Technology Co., Ltd.

[https://github.com/pkqbajng/CGFormer](https://github.com/pkqbajng/CGFormer)

###### Abstract

Vision-based Semantic Scene Completion (SSC) has gained much attention due to its widespread applications in various 3D perception tasks. Existing sparse-to-dense approaches typically employ shared context-independent queries across various input images, which fails to capture distinctions among them as the focal regions of different inputs vary and may result in undirected feature aggregation of cross-attention. Additionally, the absence of depth information may lead to points projected onto the image plane sharing the same 2D position or similar sampling points in the feature map, resulting in depth ambiguity. In this paper, we present a novel context and geometry aware voxel transformer. It utilizes a context aware query generator to initialize context-dependent queries tailored to individual input images, effectively capturing their unique characteristics and aggregating information within the region of interest. Furthermore, it extend deformable cross-attention from 2D to 3D pixel space, enabling the differentiation of points with similar image coordinates based on their depth coordinates. Building upon this module, we introduce a neural network named CGFormer to achieve semantic scene completion. Simultaneously, CGFormer leverages multiple 3D representations (i.e., voxel and TPV) to boost the semantic and geometric representation abilities of the transformed 3D volume from both local and global perspectives. Experimental results demonstrate that CGFormer achieves state-of-the-art performance on the SemanticKITTI and SSCBench-KITTI-360 benchmarks, attaining a mIoU of 16.87 and 20.05, as well as an IoU of 45.99 and 48.07, respectively. Remarkably, CGFormer even outperforms approaches employing temporal images as inputs or much larger image backbone networks.

## 1 Introduction

Semantic Scene Completion (SSC) aims to jointly infer the complete scene geometry and semantics. It serves as a crucial step in a wide range of 3D perception tasks such as autonomous driving , robotic navigation , mapping and planning . SSCNet  initially formulates the semantic scene completion task. Subsequently, many LiDAR-based approaches  have been proposed, but these approaches usually suffer from high-cost sensors.

Recently, there has been a shift towards vision-based SSC solutions. MonoScene  lifts the input 2D images to 3D volumes by densely assigning the same 2D features to both visible and occluded regions, leading to many ambiguities. With advancements in bird's-eye-view (BEV) perception , transformer-based approaches  achieve feature lifting by projecting 3D queries from 3Dspace to image plane and aggregating 3D features through deformable attention mechanisms . Among these, VoxFormer  introduces a sparse-to-dense architecture, which first aggregates 3D information for the visible voxels using the depth-based queries and then completes the 3D information for non-visible regions by leveraging the reconstructed visible areas as starting points. Building upon VoxFormer , the following approaches further improve performance through self-distillation training strategy , extracting instance features from images , or incorporating an image-conditioned cross-attention module .

Despite significant progress, existing sparse-to-dense approaches typically employ shared voxel queries (referred to as context-independent queries) across different input images. These queries are defined as a set of learnable parameters that are independent of the input context. Once the training process is completed, these parameters remain constant for all input images during inference, failing to capture distinctions among various input images, as the focal regions of different images vary. Additionally, these queries also encounter the issue of undirected feature aggregation in cross-attention, where the sampling points may fall in irrelevant regions. Besides, when projecting the 3D points onto the image plane, many points may end with the same 2D position with similar sampling points in the 2D feature map, causing a crucial depth ambiguity problem. An illustrative diagram is presented in Fig. 1(a).

In this paper, we propose a context and geometry aware voxel transformer (CGVT) to lift the 2D features. We observe that context-dependent query tend to aggregate information from the points within the region of interest. Fig. 3 presents an example of the sampling points of the context-dependent queries. Thus, before the aggregation of cross-attention, we first utilizes a context aware query generator to predict context-aware queries from the input individual images. Additionally, we extend deformable cross-attention from 2D to 3D pixel space, which allows points ending with similar image coordinates to be distinguished by their depth coordinates, as illustrated in Fig. 1(b). Furthermore, we propose a simple yet efficient depth refinement block to enhance the accuracy of estimated depth probability. This involves incorporating a more precise estimated depth map from a pretrained stereo depth estimation network , avoiding the heavy computational burden as observed in StereoScene .

Based on the aforementioned module, we devise a neural network, named as CGFormer. To enhance the obtained 3D volume from the view transformation we integrate multiple representation, _i.e._, voxel and tri-perspective view (TPV). The TPV representation offers a global perspective that encompasses more high-level semantic information, while the voxel representation focuses more on the fine-grained structures. Drawing from the above analyses, we propose a 3D local and global encoder (LGE) that dynamically fuses the results of the voxel-based and TPV-based branches, to further enhance the 3D features from both local and global perspectives.

Our contributions can be summarized as follows:

* We propose a context and geometry aware voxel transformer (CGVT) to improve the performance of semantic scene completion. This module initializes the queries based on the

Figure 1: Comparison of feature aggregation. (a) VoxFormer  employs a set of shared context-independent queries for different input images, which fails to capture distinctions among them and may lead to undirected feature aggregation. Besides, due to the ignorance of depth information, multiple 3D points may be projected to the same 2D point, causing depth ambiguity. (b) Our CGFormer initializes the voxel queries based on individual input images, effectively capturing their unique features and aggregating information within the region of interest. Furthermore, the deformable cross-attention is extended from 2D to 3D pixel space, enabling the points with similar image coordinates to be distinguished based on their depth coordinates.

context of individual input images and extends the deformable cross-attention from 2D to 3D pixel space, thereby improving the performance of feature lifting.
* We introduce a simple yet effective depth refinement block to enhance the accuracy of estimated depth probability with only introducing minimal computational burden.
* We devise a 3D local and global encoder (LGE) to strengthen the semantic and geometric discriminability of the 3D volume. This encoder employs various 3D representations (voxel and TPV) to encode the 3D features, capturing information from both local and global perspectives.
* Benefiting from the aforementioned modules, our CGFormer attains state-of-the-art results with a mIoU of \(16.63\) and an IoU of \(44.41\) on SemanticKITTI, as well as a mIoU of \(20.05\) and an IoU of \(48.07\) on SSCBench-KITTI-360. Notably, our method even surpasses methods employing temporal images as inputs or using much larger image backbone networks.

## 2 Related Work

### Vision-based 3D Perception

Vision-based 3D perception [24; 25; 31; 20; 12; 45; 62; 17; 44; 19; 58; 48; 52; 53; 56] has received extensive attention due to its ease of deployment, cost-effectiveness, and the preservation of intricate visual attributes, emerging as a crucial component in the autonomous driving. Current research efforts focus on constructing unified 3D representations (_e.g.,_ BEV, TPV, voxel) from input images. LiftSplat  lifts image features by performing outer product between the 2D image features and their estimated depth probability to generate a frustum-shaped pseudo point cloud of contextual features. The pseudo point cloud features are then splatted to predefined 3D anchors through a voxel-pooling operation. Building upon this, BEVDet  extends LiftSplat to 3D object detection, while BEVDepth  further enhances performance by introducing ground truth supervision for the estimated depth probability. With the advancements in attention mechanisms [4; 61; 57; 33; 45], BEVFormer [24; 54] transforms image features into BEV features using point deformable cross-attention . Additionally, many other methods, such as OFT , Petr [28; 29], Inverse Perspective Mapping (IPM) , have also been presented to transform 2D image features into a 3D representation.

### Semantic Scene Completion

SSCNet  initially introduces the concept of semantic scene completion, aiming to infer the semantic voxels. Following methods [38; 18; 51; 5] commonly utilize explicit depth maps or LIDAR point clouds as inputs. MonoScene  is the pioneering method for directly predicting the semantic occupancy from the input RGB image, which presents a FLoSP module for 2D-3D feature projection. StereoScene introduces explicit epipolar constraints to mitigate depth ambiguity, albeit with heavy computational burden for correlation. TPVFormer  introduces a tri-perspective view (TPV) representation to describe the 3D scene, as an alternative to the BEV representation. The elements fall into the field of view aggregates information from the image features using deformable cross-attention [24; 61]. Beginning from depth-based  sparse proposal queries, VoxFormer  construct the 3D representation in a coarse-to-fine manner. The 3D information from the visible queries are diffused to the overall 3D volume using deformable self-attention, akin to the masked autoencoder (MAE). HASSC  introduces a self-distillation training strategy to improve the performance of VoxFormer , while MonoOcc  further enhance the 3D volume with an image-conditioned cross-attention module. Symphonize  extracts high level instance features from the image feature map, serving as the key and value of the cross-attention.

## 3 CGFormer

### Overview

As shown in Fig. 2, the overall framework of CGFormer is composed of four parts: feature extraction to extract 2D image features, view transformation (Section 3.2) to lift the 2D features to 3D volumes,3D encoder (Section 3.3) to further enhance the semantic and geometric discriminability of the 3D features, and a decoding head to infer the final result.

**Image Encoder.** The image encoder consists of a backbone network for extracting multi-scale features and a feature pyramid network (FPN) to fuse them. We adopt \(^{}^{H W C}\) to represent the extracted 2D image feature, where \(C\) is the channel number, and \((H,W)\) refers to the resolution.

**Depth Estimator.** In alignment with VoxFormer , we use an off-the-shelf stereo depth estimation model  to predict the depth \((u,v)\) for each image pixel \((u,v)\). The resulting estimated depth map is then employed to define the visible voxels located on the surface, serving as query proposals. Additionally, it is also used to refine the depth probability for lifting the 2D features.

### View Transformation

A detailed diagram of our context and geometry aware voxel transformer is presented in Fig. 2(b). The process begins with a context aware query generator, which takes the context feature map to generate the context-dependent queries. Subsequently, the visible query proposals are located by the depth map from the pretrained depth estimation network. These proposals then attend to the image features to aggregate semantic and geometry information based on the 3D deformable cross-attention. Finally, the aggregated 3D information is further diffused from the updated proposals to the overall 3D volume via deformable self-attention.

**Context-dependent Query Generation.** Previous coarse-to-fine approaches  typically employ shared context-independent queries for all the inputs. However, these approaches may overlook the differences among different images and aggregate information from irrelevant areas. In contrast, CGFormer first generates context-dependent queries from the image features using a context aware query generator. To elaborate, the extracted \(^{}\) is fed to the context net and the depth net  to generate the context feature \(^{H W C}\) and depth probability \(^{H W D}\), respectively. Then the query generator \(f\) takes \(\) and \(\) as inputs to generate voxel queries \(}^{X Y Z C^{}}\)

\[}=f(,), \]

where \((X,Y,Z)\) denotes the spatial resolution of the 3D volume. Benefiting from the initialization, the sampling points of the context-dependent queries usually locate within the region of interest. An example of the deformable sampling points is displayed in Fig. 3.

**Depth-based Query Proposal.** Following VoxFormer , we determine the visible voxels through the conversion of camera coordinates to world coordinates using the pre-estimated depth map, except that we do not employ an additional occupancy network . A pixel \((u,v)\) will be transformed to a 3D point \((x,y,z)\), utilizing the camera intrinsic and extrinsic matrices (\(K^{4 4},E^{4 4}\)).

Figure 2: Schematics and detailed architectures of CGFormer. (a) The framework of the proposed CGFormer for camera-based semantic scene completion. The pipeline consists of the image encoder for extracting 2D features, the context and geometry aware voxel (CGVT) transformer for lifting the 2D features to 3D volumes, the 3D local and global encoder (LGE) for enhancing the 3D volumes and a decoding head to predict the semantic occupancy. (b) Detailed structure of the context and geometry aware voxel transformer. (c) Details of the Depth Net.

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

To demonstrate the versatility of our model, we also conduct experiments on the SSCBench-KITTI-360  dataset and list the results in Table 2. It is observed that CGFormer surpasses all the published camera-based methods by a margin of \(3.95\) IoU and \(1.67\) mIoU. Notably, CGFormer even outperforms the two LiDAR-based methods in terms of mIoU. The above analyses further verifies the effectiveness and excellent performance of CGFormer.

### Ablation Study

We conduct ablation analyses on the components of CGFormer on the SemanticKITTI validation set.

**Ablation on the context and geometry aware voxel transformer (CGVT).** Table 3 presents a detailed analysis of various architectural components within CGFormer. The baseline can be considered as a simplified version of VoxFormer , without utilizing the extra occupancy network  to generate coarse occupancy masks. It begins with an image encoder to extract image features, a depth-based query proposal layer to define the visible queries, the deformable cross-attention layer to aggregate features for visible areas, the deformable self-attention layer to diffuse features from the visible regions to the invisible ones. Extending the cross-attention to 3D deformable cross-attention (a) brings a notable improvement of 1.63 mIoU and 2.15 IoU. The performance is further enhanced by giving the voxel queries a good initialization by introducing the context ware query generator (b).

**Ablation on the local and global encoder (LGE).** After obtaining the 3D features, CGFormer incorporates multiple representation to refine the 3D volume form both local and global perspectives. Through experimentation, we validate that the voxel-based and TPV-based branches collectively contribute to performance improvement with a suitable fusion strategy. Specifically, we compute the results of solely utilizing the local voxel-based branch (c), simply adding the results of dual branches (d), and dynamically fusing the dual-branch outputs (h), as detailed in Table 3. The accuracy is notably elevated to an IoU of \(45.99\) and mIoU of \(16.87\) by dynamically fusing the dual-branch outputs. Additionally, we conduct an ablation study on the three TPV planes utilized in the TPV-based branch (e,f,g). The results demonstrate that any individual plane improves performance compared to the model with only the local branch. Combining the information from all three planes into the TPV representation achieves superior performance, underscoring the complementary nature of the three TPV planes in effectively representing complex 3D scenes.

**Ablation on the context aware query generator.** We present the ablation analysis of the context-aware query generator in Table 4. We remove the CAQG and increase the number of attention layers, where the results of the previous layers can be viewed as a initialization of the queries for the later layers. This configuration (6 cross-attention layers and 4 self-attention layers) significantly improves IoU but only marginally lifts mIoU, and it requires much more training memory. Employing

   Method & IoU & mIoU & Params (M) & Memory (M) \\   w/o: CAQG \\  } & 40.14 & 14.34 & 86.17 & 15150 \\  More attention layers \\ FL:SSP  \\ Voxel Pooling \\  } & 41.83 & 14.43 & 86.97 & 21556 \\  FL:SSP  \\ Voxel Pooling \\  } & 41.54 & 14.66 & 86.19 & 15907 \\  Voxel Pooling \\  } & **42.86** & **15.60** & 86.19 & 15488 \\   

Table 4: Ablation on the choices of context aware query generator.

    &  &  &  &  &  &  \\    & 3D-DCA & CAQG & & & & & & & & 37.99 & 12.71 & 76.57 & 13222 \\  & & & & & & & 40.14 & 14.34 & 86.17 & 15150 \\  & & & & & & & 42.86 & 15.60 & 86.19 & 15488 \\   & & & & & & & & & 44.84 & 16.41 & 93.78 & 17843 \\   (d) & & & & & & & & 44.63 & 16.54 & 122.42 & 19188 \\   (e) & & & & & & & & & 45.46 & 16.38 & 122.12 & 19024 \\   (f) & & & & & & & & & 45.53 & 16.74 & 122.12 & 18912 \\   (g) & & & & & & & & & 45.71 & 16.49 & 122.12 & 18912 \\   & & & & & & & & & **45.99** & **16.87** & 122.42 & 19330 \\     with the occupancy-aware depth module  can improve performance without much additional computational burden. By replacing it with the voxel pooling , the model achieves the best performance. Thus, we employ voxel pooling as our context aware query generator.

**Ablation on the depth refinement block.** Table 5 presents analyses of the impact of each module within the depth net. By removing the stereo feature \(_{S}\) and employing the same structure as BEVDepth , we observe a performance drop of \(1.42\) IoU and \(1.79\) mIoU. When incorporating the stereo feature \(_{S}\) but fusing it with a simple concatenate operation without using the neighborhood attention, the model achieves a mIoU of \(45.72\) and an IoU of \(16.26\). These results emphasize that deactivating any component of the depth net leads to a decrease of the accuracy of the full network. Additionally, we replace the depth refinement module with the dense correlation module from StereoScene . Compared to this, our depth refinement module achieves comparable results while using significantly fewer parameters and less training memory.

### Qualitative Results

Fig. 4 presents visualizations of predicted results on the SemanticKITTI validation set obtained from MonoScene, VoxFormer , OccFormer , and our proposed CGFormer. Notably, CGFormer outperforms other methods by effectively capturing the semantic scene and inferring previously invisible regions. The predictions generated by CGFormer exhibit distinctly clearer geometric structures and improved semantic discrimination, particularly for classes like cars and the overall scene layout. This enhancement is attributed to the precision achieved through the context and geometry aware voxel transformer applied in our proposed approach. In contrast, the instances generated by the comparison methods appear to be influenced by depth ambiguity, resulting in vague shapes.

## 5 Conclusions

In this paper, we present CGFormer, a novel neural network for Semantic Scene Completion. CGFormer dynamically generates distinct voxel queries, which serve as a good starting point for the attention layers, capturing the unique characteristics of various input images. To improve the accuracy of the estimated depth probability, we propose a simple yet efficient depth refinement module, with minimal computational burden. To boost the semantic and geometric representation abilities, CGFormer incorporates multiple representations (voxel and TPV) to encode the 3D volumes from both local and global perspectives. We experimentally show that our CGFormer achieves state-of-the-art performance on the SemanticKITTI and SSCBench-KITTI-360 benchmarks.

Figure 4: Qualitative visualization results on the SemanticKITTI  validation set.