# Policy Finetuning in Reinforcement Learning

via Design of Experiments using Offline Data

Ruiqi Zhang

Department of Statistics

University of California, Berkeley

rqzhang@berkeley.edu

Andrea Zanette

Department of EECS

University of California, Berkeley

zanette@berkeley.edu

###### Abstract

In some applications of reinforcement learning, a dataset of pre-collected experience is already available but it is also possible to acquire some additional online data to help improve the quality of the policy. However, it may be preferable to gather additional data with a single, non-reactive exploration policy and avoid the engineering costs associated with switching policies.

In this paper we propose an algorithm with provable guarantees that can leverage an offline dataset to design a single non-reactive policy for exploration. We theoretically analyze the algorithm and measure the quality of the final policy as a function of the local coverage of the original dataset and the amount of additional data collected.

## 1 Introduction

Reinforcement learning (RL) is a general framework for data-driven, sequential decision making (Puterman, 1994; Sutton and Barto, 2018). In RL, a common goal is to identify a near-optimal policy, and there exist two main paradigms: _online_ and _offline_ RL.

Online RL is effective when the practical cost of a bad decision is low, such as in simulated environments (e.g., (Mnih et al., 2015; Silver et al., 2016)). In online RL, a well designed learning algorithm starts from tabula rasa and implements a sequence of policies with a value that should approach that of an optimal policy. When the cost of making a mistake is high, such as in healthcare (Gottesman et al., 2018) and in self-driving (Kiran et al., 2021), an offline approach is preferred. In offline RL, the agent uses a dataset of pre-collected experience to extract a policy that is as good as possible. In this latter case, the quality of the policy that can be extracted from the dataset is limited by the quality of the dataset.

Many applications, however, fall between these two opposite settings: for example, a company that sells products online has most likely recorded the feedback that it has received from its customers, but can also collect a small amount of additional strategic data in order to improve its recommendation engine. While in principle an online exploration algorithm can be used to collect fresh data, in practice there are a number of practical engineering considerations that require the policy to bedeployed to be **non-reactive**. We say that a policy is non-reactive, (or passive, memoryless) if it chooses actions only according to the current state of the system. Most online algorithms are, by design, reactive to the data being acquired.

An example of a situation where non-reactive policies may be preferred are those where a human in the loop is required to validate each exploratory policy before they are deployed, to ensure they are of high quality (Dann et al., 2019) and safe (Yang et al., 2021), as well as free of discriminatory content (Koenecke et al., 2020). Other situations that may warrant non-reactive exploration are those where the interaction with the user occurs through a distributed system with delayed feedback. In recommendation systems, data collection may only take minutes, but policy deployment and updates can span weeks (Afsar et al., 2022). Similar considerations apply across various RL application domains, including healthcare (Yu et al., 2021), computer networks (Xu et al., 2018), and new material design (Raccuglia et al., 2016). In all such cases, the engineering effort required to implement a system that handles real-time policy switches may be prohibitive: deploying a single, non-reactive policy is much preferred.

**Non-reactive exploration from offline data** Most exploration algorithms that we are aware of incorporate policy switches when they interact with the environment (Dann and Brunskill, 2015; Damn et al., 2017; Azar et al., 2017; Jin et al., 2018; Damn et al., 2019; Zanette and Brunskill, 2019; Zhang et al., 2020b). Implementing a sequence of non-reactive policies is necessary in order to achieve near-optimal regret: the number of policy switches must be at least \((H|||| K)\) where \(,,H,K\) are the state space, action space, horizon and the total number of episodes, respectively (Qiao et al., 2022). With no switches, i.e., when a fully non-reactive data collection strategy is implemented, it is information theoretically impossible (Xiao et al., 2022) to identify a good policy using a number of samples polynomial in the size of the state and action space.

However, these fundamental limits apply to the case where the agent learns from tabula rasa. In the more common case where offline data is available, we demonstrate that it is possible to leverage the dataset to design an effective non-reactive exploratory policy. More precisely, an available offline dataset contains information (e.g., transitions) about a certain area of the state-action space, a concept known as _partial coverage_. A dataset with partial coverage naturally identifies a'sub-region' of the original MDP--more precisely, a sub-graph--that is relatively well explored. We demonstrate that it is possible to use the dataset to design a non-reactive policy that further explores such sub-region.

The additional data collected can be used to learn a near-optimal policy in such sub-region.

In other words, exploration with no policy switches can collect additional information and compete with the best policy that is restricted to an area where the original dataset has sufficient information. The value of such policy can be much higher than the one that can be computed using only the offline dataset, and does not directly depend on a concentrability coefficient (Munos and Szepesvari, 2008; Chen and Jiang, 2019).

Perhaps surprisingly, addressing the problem of reactive exploration in reinforcement learning requires an approach that _combines both optimism and pessimism_ in the face of uncertainty to explore efficiently. While optimism drives exploration, pessimism ensures that the agent explores conservatively, in a way that restricts its exploration effort to a region that it knows how to navigate, and so our paper makes a technical contribution which can be of independent interest.

**Contributions** To the best of our knowledge, this is the first paper with theoretical rigor that considers the problem of designing an experiment in reinforcement learning for online, passive exploration, using a dataset of pre-collected experience. More precisely, our contributions are as follows:

* We introduce an algorithm that takes as input a dataset, uses it to design and deploy a non-reactive exploratory policy, and then outputs a locally near-optimal policy.
* We introduce the concept of sparsified MDP, which is actively used by our algorithm to design the exploratory policy, as well as to theoretically analyze the quality of the final policy that it finds.
* We rigorously establish a nearly minimax-optimal upper bound for the sample complexity needed to learn a local \(\)-optimal policy using our algorithm. 1Related Work

In this section we discuss some related literature. Our work is related to low-switching algorithms, but unlike those, we focus on the limit case where _no-switches_ are allowed. For more related work about low-switching algorithms, offline RL, task-agnostic RL, and reward-free RL we refer to Appendix F.

**Low-switching RL** In reinforcement learning,  first proposed Q-learning with UCB2 exploration, proving an \(O(H^{3}|||| K)\) switching cost. This was later improved by a factor of \(H\) by the UCBadvantage algorithm in . Recently,  generalized the policy elimination algorithm from  and introduced APEVE, which attains an optimal \(O(H|||| K)\) switching cost. The reward-free version of their algorithm (which is not regret minimizing) has an \(O(H||||)\) switching cost.

Similar ideas were soon applied in RL with linear function approximation  and general function approximation . Additionally, numerous research efforts have focused on low-adaptivity in other learning domains, such as batched dueling bandits , batched convex optimization , linear contextual bandits , and deployment-efficient RL .

Our work was inspired by the problem of non-reactive policy design in linear contextual bandits. Given access to an offline dataset,  proposed an algorithm to output a single exploratory policy, which generates a dataset from which a near-optimal policy can be extracted. However, there are a number of additional challenges which arise in reinforcement learning, including the fact that the state space is only partially explored in the offline dataset. In fact, in reinforcement learning,  established an exponential lower bound for any non-adaptive policy learning algorithm starting from tabula rasa.

## 3 Setup

Throughout this paper, we let \([n]=\{1,2,...,n\}\). We adopt the big-O notation, where \(()\) suppresses poly-log factors of the input parameters. We indicate the cardinality of a set \(\) with \(||\).

**Markov decision process** We consider time-homogeneous episodic Markov decision processes (MDPs). They are defined by a finite state space \(\), a finite action space \(\), a trasition kernel \(\), a reward function \(r\) and the episodic length \(H\). The transition probability \((s^{} s,a)\), which does not depend on the current time-step \(h[H]\), denotes the probability of transitioning to state \(s^{}\) when taking action \(a\) in the current state \(s\). Typically we denote with \(s_{1}\) the initial state. For simplicity, we consider deterministic reward functions \(r:\). A deterministic non-reactive (or memoryless, or passive) policy \(=\{_{h}\}_{h[H]}\) maps a given state to an action.

The value function is defined as the expected cumulated reward. It depends on the state \(s\) under consideration, the transition \(\) and reward \(r\) that define the MDP as well as on the policy \(\) being implemented. It is defined as \(V_{h}(s;,r,)=_{,}[_{i=h}^{H }r(s_{i},a_{i}) s_{h}=s]\), where \(_{,}\) denotes the expectation generated by \(\) and policy \(\). A closely related quantity is the state-action value function, or \(Q\)-function, defined as \(Q_{h}(s,a;,r,)=_{,}[_{i=h} ^{H}r(s_{i},a_{i}) s_{h}=s,a_{h}=a]\). When it is clear from the context, we sometimes omit \((,r)\) and simply write them as \(V_{h}^{}(s)\) and \(Q_{h}^{}(s,a)\). We denote an MDP defined by \(,\) and the transition matrix \(\) as \(=(,,)\).

### Interaction protocol

```
1:Offline dataset \(\)
2:Offline phase: use \(\) to compute the exploratory policy \(_{ex}\)
3:Online phase: deploy \(_{ex}\) to collect the online dataset \(^{}\)
4:Planning phase: receive the reward function \(r\) and use \(^{}\) to extract \(_{final}\)
5:Return \(_{final}\)
```

**Algorithm 1** Design of experiments in reinforcement learning In this paper we assume access to an _offline dataset_\(=\{(s,a,s^{})\}\) where every state-action \((s,a)\) is sampled in an i.i.d. fashion from some distribution \(\) and \(s^{}( s,a)\), which is common in the offline RL literature (Xie et al., 2021; Zhan et al., 2022; Rashidinejad et al., 2021; Uehara and Sun, 2021). We denote \(N(s,a)\) and \(N(s,a,s^{})\) as the number of \((s,a)\) and \((s,a,s^{})\) samples in the offline dataset \(\), respectively. The interaction protocol considered in this paper consists of three distinct phases, which are displayed in algorithm 1. They are:

* the **offline phase**, where the learner uses an _offline dataset_\(\) of pre-collected experience to design the non-reactive exploratory policy \(_{ex}\);
* the **online phase** where \(_{ex}\) is deployed to generate the _online dataset_\(^{}\);
* the **planning phase** where the learner receives a reward function and uses all the data collected to extract a good policy \(_{final}\) with respect to that reward function.

The objective is to minimize the number of online episodic interactions needed to find a policy \(_{final}\) whose value is as high as possible. Moreover, we focus on the reward-free RL setting (Jin et al., 2020; Kaufmann et al., 2021; Li et al., 2023), which is more general than reward-aware RL. In the offline and online phase, the data are generated without specific reward signals, and the entire reward information is then given in the planning phase. One of the primary advantages of using reward-free offline data is that it allows for the collection of data without the need for explicit reward signals. This can be particularly beneficial in environments where obtaining reward signals is costly, risky, ethically challenging, or where the reward functions are human-designed.

## 4 Algorithm: balancing optimism and pessimism for experimental design

In this section we outline our algorithm _Reward-Free Non-reactive Policy Design_ (RF-NPD), which follows the high-level protocol described in algorithm 1. The technical novelty lies almost entirely in the design of the exploratory policy \(_{ex}\). In order to prepare the reader for the discussion of the algorithm, we first give some intuition in section 4.1 followed by the definition of sparsified MDP in section 4.2, a central concept of this paper, and then describe the implementation of line 1 in the protocol in algorithm 1 in section 4.3. We conclude by presenting the implementation of lines 2 and 3 in the protocol in algorithm 1.

### Intuition

In order to present the main intuition for this paper, in this section we assume that enough transitions are available in the dataset for every edge \((s,a) s^{}\), namely that the _critical condition_

\[N(s,a,s^{})=(H^{2}) \]

holds for all tuples \((s,a,s^{})\) (the precise value for \(\) will be given later in eq. (5.1)). Such condition is hardly satisfied everywhere in the state-action-state space, but assuming it in this section simplifies the presentation of one of the key ideas of this paper.

The key observation is that when eq. (4.1) holds for all \((s,a,s^{})\), we can use the empirical transition kernel to design an exploration policy \(_{ex}\) to eventually extract a near-optimal policy \(_{final}\) for any desired level of sub-optimality \(\), despite eq. (4.1) being independent of \(\). More precisely, let \(}\) be the empirical transition kernel defined in the usual way \(}(s^{} s,a)=N(s,a,s^{})/N(s,a)\) for any tuple \((s,a,s^{})\). The intuition--which will be verified rigorously in the analysis of the algorithm--is the following:

_If eq. (4.1) holds for every \((s,a,s^{})\) then \(}\) can be used to design a non-reactive exploration policy \(_{ex}\) which can be deployed on \(\) to find an \(\)-optimal policy \(_{final}\) using \(}\) samples._

We remark that even if the condition 4.1 holds for all tuples \((s,a,s^{})\), the empirical kernel \(}\) is not accurate enough to extract an \(\)-optimal policy from the dataset \(\) without collecting further data. Indeed, the threshold \(=(H^{2})\) on the number of samples is independent of the desired sub-optimality \(>0\), while it is well known that at least \(}\) offline samples are needed to find an \(\)-optimal policy. Therefore, directly implementing an offline RL algorithm to use the available offline dataset \(\) does not yield an \(\)-optimal policy. However, the threshold \(=(H^{2})\) is sufficient to _design_ a non-reactive exploratory policy \(_{ex}\) that can discover an \(\)-optimal policy \(_{final}\) after collecting \(}\) online data.

### Sparsified MDP

The intuition in the prior section must be modified to work with heterogeneous datasets and dynamics where \(N(s,a,s^{})\) may fail to hold everywhere. For example, if \((s^{} s,a)\) is very small for a certain tuple \((s,a,s^{})\), it is unlikely that the dataset contains \(N(s,a,s^{})\) samples for that particular tuple. In a more extreme setting, if the dataset is empty, the critical condition in eq. (4.1) is violated for all tuples \((s,a,s^{})\), and in fact the lower bound of Xiao et al. (2022) states that finding \(\)-optimal policies by exploring with a non-reactive policy is not feasible with \(}\) sample complexity. This suggests that in general it is not possible to output an \(\)-optimal policy using the protocol in algorithm 1.

However, a real-world dataset generally covers at least a portion of the state-action space, and so we expect the condition \(N(s,a,s^{})\) to hold somewhere; the sub-region of the MDP where it holds represents the connectivity graph of the _sparsified MDP_. This is the region that the agent knows how to navigate using the offline dataset \(\), and so it is the one that the agent can explore further using \(_{ex}\). More precisely, the sparsified MDP is defined to have identical dynamics as the original MDP on the edges \((s,a) s^{}\) that satisfy the critical condition 4.1. When instead the edge \((s,a) s^{}\) fails to satisfy the critical condition 4.1, it is replaced with a transition \((s,a) s^{}\) to an absorbing state \(s^{}\).

**Definition 4.1** (Sparsified MDP).: _Let \(s^{}\) be an absorbing state, i.e., such that \((s^{} s^{},a)=1\) and \(r(s^{},a)=0\) for all \(a\). The state space in the sparsified MDP \(^{}\) is defined as that of the original MDP with the addition of \(s^{}\). The dynamics \(^{}\) of the sparsified MDP are defined as_

\[^{}(s^{} s,a)=\{(s^{} s,a)&N(s,a,s^{})\\ 0&N(s,a,s^{})<,.^{ }(s^{} s,a)=_{s^{} s^{ }\\ N(s,a,s^{})<}(s^{} s,a). \]

_For any deterministic reward function \(r:,\) the reward function on the sparsified MDP is defined as \(r^{}(s,a)=r(s,a)\); for simplicity we only consider deterministic reward functions._

The _empirical sparsified MDP_\(^{}}=(\{s^{}\},,^{}})\) is defined in the same way but by using the empirical transition kernel in eq. (4.2). The empirical sparsified MDP is used by our algorithm to design the exploratory policy, while the (population) sparsified MDP is used for its theoretical analysis. They are two fundamental concepts in this paper. By formulating the sparsified MDP, we restrict the transitions and rewards within the area where we know how to navigate, embodying the principle of pessimism. Various forms of pessimistic regularization have been introduced to address the challenges of partially covered offline data. Examples include a pessimistic MDP (Kidambi et al., 2020) and limiting policies to those covered by offline data (Liu et al., 2020).

### Offline design of experiments

In this section we describe the main sub-component of the algorithm, namely the sub-routine that uses the offline dataset \(\) to compute the exploratory policy \(_{ex}\). The exploratory policy \(_{ex}\) is a mixture of the policies \(^{1},^{2},\) produced by a variant of the reward-free exploration algorithm of (Kaufmann et al., 2021; Menard et al., 2021). Unlike prior literature, the reward-free algorithm is not interfaced with the real MDP \(\), but rather _simulated_ on _the empirical sparsified MDP_\(^{}}\). This avoids interacting with \(\) with a reactive policy, but it introduces some bias that must be controlled. The overall procedure is detailed in algorithm 2. To be clear, no real-world samples are collected by algorithm 2; instead we use the word 'virtual samples' to refer to those generated from \(^{}}\).

At a high level, algorithm 2 implements value iteration using the empirical transition kernel \(^{}}\), with the exploration bonus defined in eq. (4.3) that replaces the reward function. The exploration bonus can be seen as implementing the principle of optimism in the face of uncertainty; however, the possibility of transitioning to an absorbing state with zero reward (due to the use of the absorbing state in the definition of \(^{}}\)) implements the principle of pessimism.

[MISSING_PAGE_FAIL:6]

line 3 a reward function \(r\) is received, and the value iteration algorithm (See Appendix E) is invoked with \(r\) as reward function and \(}^{}\) as dynamics, and the near-optimal policy \(_{final}\) is produced. The use of the (updated) empirical sparsified dynamics \(}^{}\) can be seen as incorporating the principle of pessimism under uncertainty due to the presence of the absorbing state.

Our complete algorithm is reported in algorithm 3, and it can be seen as implementing the interaction protocol described in algorithm 1.

```
0: Offline dataset \(\), target suboptimality \(>0\), failure tolerance \((0,1]\).
1: Construct the empirical sparsified MDP \(}^{}\).
2: Offline phase: run RF-UCB\((}^{},K_{ucb},,)\) to obtain the exploratory policy \(_{ex}\).
3: Online phase: deploy \(_{ex}\) on the MDP \(\) for \(K_{de}\) episodes to get the online dataset \(^{}\).
4: Planning phase: receive the reward function \(r\), construct \(}^{}\) from the online dataset \(^{}\), compute \(_{final}\) (which is the optimal policy on \(}^{}\)) using value iteration (Appendix E).
```

**Algorithm 3** Reward-Free Non-reactive Policy Design (RF-NPD)

## 5 Main Result

In this section, we present a performance bound on our algorithm, namely a bound on the suboptimality of the value of the final policy \(_{final}\) when measured on the sparsified MDP \(^{}\). The sparsified MDP arises because it is generally not possible to directly compete with the optimal policy using a non-reactive data collection strategy and a polynomial number of samples due to the lower bound of Xiao et al. (2022); more details are given in Appendix C.

In order to state the main result, we let \(K=K_{ucb}=K_{de}\), where \(K_{ucb}\) and \(K_{de}\) are the number of episodes for the offline simulation and online interaction, respectively. Let \(C\) be some universal constant, and choose the threshold in the definition of sparsified MDP as

\[=6H^{2}(12H||^{2}||/). \]

**Theorem 5.1**.: _For any \(>0\) and \(0<<1,\) if we let the number of online episodes be_

\[K=||^{2}||}{ ^{2}}(||,||,H,,),\]

_then with probability at least \(1-\), for any reward function \(r\), the final policy \(_{final}\) returned by Algorithm 3 satisfies the bound_

\[_{}V_{1}(s_{1};^{},r^{},)-V _{1}(s_{1};^{},r^{},_{final}). \]

The theorem gives a performance guarantee on the value of the policy \(_{final}\), which depends both on the initial coverage of the offline dataset \(\) as well as on the number of samples collected in the online phase. The dependence on the coverage of the offline dataset is implicit through the definition of the (population) sparsified \(^{}\), which is determined by the counts \(N(,)\).

In order to gain some intuition, we examine some special cases as a function of the coverage of the offline dataset.

**Empty dataset** Suppose that the offline dataset \(\) is empty. Then the sparsified MDP identifies a _multi-armed bandit_ at the initial state \(s_{1}\), where any action \(a\) taken from such state gives back the reward \(r(s_{1},a)\) and leads to the absorbing state \(s^{}\). In this case, our algorithm essentially designs an allocation strategy \(_{ex}\) that is uniform across all actions at the starting state \(s_{1}\). Given enough online samples, \(_{final}\) converges to the _action_ with the highest instantaneous reward on the multi-armed bandit induced by the start state. With no coverage from the offline dataset, the lower bound of Xiao et al. (2022) for non-reactive policies precludes finding an \(\)-optimal policy on the original MDP \(\) unless exponentially many samples are collected.

**Known connectivity graph** On the other extreme, assume that the offline dataset contains enough information everywhere in the state-action space such that the critical condition 4.1 is satisfied for all \((s,a,s^{})\) tuples. Then the sparsified MDP and the real MDP coincide, i.e., \(=^{}\), and so the final policy \(_{final}\) directly competes with the optimal policy \(^{*}\) for any given reward function in eq.5.2. More precisely, the policy \(_{final}\) is \(\)-suboptimal on \(\) if \((H^{2}||^{2}||/ ^{2})\) trajectories are collected in the online phase, a result that matches the lower bound for reward-free exploration of Jin et al. (2020) up to log factors. However, we achieve such result with a data collection strategy that is completely passive, one that is computed with the help of an initial offline dataset whose size \(||||^{2}| |=(H^{2}||^{2}| |)\) need _not depend on final accuracy \(\)_.

**Partial coverage** In more typical cases, the offline dataset has only _partial_ coverage over the state-action space and the critical condition 4.1 may be violated in certain state-action-successor states. In this case, the connectivity graph of the sparsified MDP \(^{}\) is a sub-graph of the original MDP \(\) augmented with edges towards the absorbing state. The lack of coverage of the original dataset arises through the sparsified MDP in the guarantees that we present in theorem5.1. In this section, we 'translate' such guarantees into guarantees on \(\), in which case the 'lack of coverage' is naturally represented by the concentrability coefficient

\[C^{*}=_{s,a}d_{}(s,a)/(s,a),\]

see for examples the papers (Munos and Szepesvari, 2008; Chen and Jiang, 2019) for background material on the concentrability factor. More precisely, we compute the sample complexity--in terms of online as well as offline samples--required for \(_{final}\) to be \(\)-suboptimal with respect to any comparator policy \(\), and so in particular with respect to the optimal policy \(_{*}\) on the "real" MDP \(\). The next corollary is proved in appendixB.3.

**Corollary 5.2**.: _Suppose that the offline dataset contains_

\[||^{2}| |C^{*}}{},\]

_samples and that additional_

\[||^{2}| |}{^{2}}\]

_online samples are collected during the online phase. Then with probability at least \(1-\), for any reward function \(r\), the policy \(_{final}\) is \(\)-suboptimal with respect to any comparator policy \(\)_

\[V_{1}(s_{1};,r,)-V_{1}(s_{1};,r,_{ final}). \]

The online sample size is equivalent to the one that arises in the statement of theorem5.1 (expressed as number of online trajectories), and does not depend on the concentrability coefficient. The dependence on the offline dataset in theorem5.1 is implicit in the definition of sparsified MDP; here we have made it explicit using the notion of concentrability.

Corollary5.2 can be used to compare the achievable guarantees of our procedure with that of an offline algorithm, such as the minimax-optimal procedure detailed in (Xie et al., 2021). The proceede described in (Xie et al., 2021) achieves (5.3) with probability at least \(1-\) by using

\[(||C^{*}}{ ^{2}}+||C^{*}}{}) \]

offline samples3. In terms of offline data, our procedure has a similar dependence on various factors, but it depends on the desired accuracy \(\) through \((1/)\) as opposed to \((1/^{2})\) which is typical for an offline algorithm. This implies that in the small-\(\) regime, if sufficient online samples are collected, one can improve upon a fully offline procedure by collecting a number of additional online samples in a non-reactive way.

Finally, notice that one may improve upon an offline dataset by collecting more data from the distribution \(\), i.e., without performing experimental design. Compared to this latter case, notice that our _online sample complexity does not depend on the concentrability coefficient_. Further discussion can be found in appendixB.

Proof

In this section we prove theorem 5.1, and defer the proofs of the supporting statements to the Appendix A.

Let us define the comparator policy \(_{*}^{}\) used for the comparison in eq. (5.2) to be the (deterministic) policy with the highest value function on the sparsified MDP:

\[_{*}^{}:=*{arg\,max}_{}V_{1}(s_{1};^{ },r^{},).\]

We can bound the suboptimality using the triangle inequality as

\[V_{1}(s_{1};^{},r^{},_{*}^{ })-V_{1}(s_{1};^{},r^{},_{final} )|V_{1}(s_{1};^{},r^{},_{*}^{ })-V_{1}(s_{1};}^{},r^{}, _{*}^{})|\] \[ 2\,_{,r}|V_{1}(s_{1}; ^{},r^{},)-V_{1}(s_{1};}^{},r^{},)|.\]

The middle term after the first inequality is negative due to the optimality of \(_{final}\) on \(}^{}\) and \(r^{}\). It suffices to prove that for any arbitrary policy \(\) and reward function \(r\) the following statement holds with probability at least \(1-\)

\[(s_{1};^{},r^{}, )-V_{1}(s_{1};}^{},r^{}, )|}_{}. \]

Bounding the estimation error using the population uncertainty functionIn order to prove eq. (6.1), we first define the population _uncertainty function_\(X\), which is a scalar function over the state-action space. It represents the maximum estimation error on the value of any policy when it is evaluated on \(}^{}\) instead of \(\). For any \((s,a)\), the uncertainty function is defined as \(X_{H+1}(s,a):=0\) and for \(h[H]\),

\[X_{h}(s,a):=H-h+1;\ 9H(m(s,a))+(1+) _{s^{}}\). The empirical uncertainty is defined as \(U_{H+1}^{k}(s,a)=0\) for any \(k,s,a\) and

\[U_{h}^{k}(s,a)=H\{1,(n^{k}(s,a))\}+}^{}(s,a) ^{}(_{a^{}}U_{h+1}^{k}(,a^{})), \]

where \(n^{k}(s,a)\) is the counter of the times we encounter \((s,a)\) until the beginning of the \(k\)-the virtual episode in the simulation phase. Note that, \(U_{h}^{k}(s,a)\) takes a similar form as \(X_{h}(s,a),\) except that \(U_{h}^{k}(s,a)\) depends on the empirical transition probability \(}^{}\) while \(X_{h}(s,a)\) depends on the true transition probability on the sparsified MDP. For the exploration scheme to be effective, \(X\) and \(U\) should be close in value, a concept which is at the core of this work and which we formally state below and prove in appendix A.2.2.

**Lemma 6.2** (Bounding uncertainty function with empirical uncertainty functions).: _With probability at least \(1-\), we have for any \((h,s,a)[H],\)_

\[X_{h}(s,a)_{k=1}^{K}U_{h}^{k}(s,a).\]

Notice that \(X_{h}\) is the population uncertainty after the online samples have been collected, while \(U_{h}^{k}\) is the corresponding empirical uncertainty which varies during the planning phase.

Rate of decrease of the estimation errorCombining lemmas 6.1 and 6.2 shows that (a function of) the agent's uncertainty estimate \(U\) upper bounds the estimation error in eq. (6.1). In order to conclude, we need to show that \(U\) decreases on average at the rate \(1/K\), a statement that we present below and prove in appendix A.2.3.

**Lemma 6.3**.: _With probability at least \(1-\), we have_

\[_{k=1}^{K}U_{1}^{k}(s,a)||^{2}| |}{K}(K,||\,,||\,,H, ,). \]

_Then, for any \(>0\), if we take_

\[K:=||||}{^ {2}}+||(||,||,H,, ),\]

_then with probability at least \(1-\), it holds that_

\[_{k=1}^{K}U_{1}^{k}(s_{1},a)^{2}.\]

After combining lemmas 6.1 to 6.3, we see that the estimation error can be bounded as

\[|V_{1}(s_{1};^{},r^{},)-V_{1} (s_{1};}^{},r^{},)| _{a}X_{1}(s_{1},a)+CX_{1} (s_{1},a)}\] \[ C_{a}[_{k=1}^{K}U_{1}^{k}(s_{ 1},a)}+_{k=1}^{K}U_{1}^{k}(s_{1},a)]\] \[ C(+^{2})\] \[ C\] (for

\[0<<const\]

)

Here, the constant \(C\) may vary between lines. Rescaling the universal constant \(C\) and the failure probability \(\), we complete the upper bound in equation (6.1) and hence the proof for the main result.