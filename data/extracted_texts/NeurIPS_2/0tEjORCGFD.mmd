# Collaborative Score Distillation

for Consistent Visual Editing

 Subin Kim\({}^{*,}\)\({}^{1}\)  Kyungmin Lee\({}^{*,}\)\({}^{1}\)  June Suk Choi\({}^{1}\)  Jongheon Jeong\({}^{1}\)

**Kihyuk Sohn\({}^{2}\)  Jinwoo Shin\({}^{1}\) \({}^{1}\)**KAIST \({}^{2}\)**Google Research

*{subin-kim, kyungm1ele}@kaist.ac.kr

Equal contribution.

###### Abstract

Generative priors of large-scale text-to-image diffusion models enable a wide range of new generation and editing applications on diverse visual modalities. However, when adapting these priors to complex visual modalities, often represented as multiple images (e.g., video or 3D scene), achieving consistency across a set of images is challenging. In this paper, we address this challenge with a novel method, Collaborative Score Distillation (CSD). CSD is based on the Stein Variational Gradient Descent (SVGD). Specifically, we propose to consider multiple samples as "particles" in the SVGD update and combine their score functions to distill generative priors over a set of images synchronously. Thus, CSD facilitates the seamless integration of information across 2D images, leading to a consistent visual synthesis across multiple samples. We show the effectiveness of CSD in a variety of editing tasks, encompassing the visual editing of panorama images, videos, and 3D scenes. Our results underline the competency of CSD as a versatile method for enhancing inter-sample consistency, thereby broadening the applicability of text-to-image diffusion models.1

## 1 Introduction

Text-to-image diffusion models  have been scaled up by using billions of image-text pairs  and efficient architectures , showing impressive capability in synthesizing high-quality, realistic, and diverse images with the text given as an input. Furthermore, they have branched into various applications, such as image-to-image translation , controllable generation , or personalization . One of the latest applications in this regard is to translate the capability into other complex modalities, viz., beyond 2D images  without modifying diffusion models using modality-specific training data. This paper focuses on the problem of adapting the knowledge of pre-trained text-to-image diffusion models to more complex high-dimensional visual manipulation tasks beyond 2D images without modifying diffusion models using modality-specific training data.

We start from an intuition that many complex visual data, e.g., videos and 3D scenes, are represented as a _set of images_ constrained by modality-specific consistency. For example, a video is a set of frames requiring temporal consistency, and a 3D scene is a set of multi-view frames with view consistency. Unfortunately, image diffusion models do not have a built-in capability to ensure consistency between a set of images for synthesis or editing because their generative sampling process does not take into account the consistency when using the image diffusion model as is. As such, when applying image diffusion models to manipulate these complex data without consistency in consideration, itresults in a highly incoherent output, as in Figure 2 (Patch-wise Crop), where one can easily identify where images are being stitched. Such behaviors are also reported in video editing, thus, recent works [22; 23; 24; 25] propose to handle video-specific temporal consistency when using the image diffusion model.

Here, we take attention to an alternative approach, Score Distillation Sampling (SDS) , which enables the optimization of arbitrary differentiable operators by leveraging the rich generative prior of text-to-image diffusion models. SDS poses generative sampling as an optimization problem by distilling the learned diffusion density scores. While Poole et al.  has shown the effectiveness of SDS in generating 3D objects from the text by resorting on Neural Radience Fields  priors which inherently suppose coherent geometry in 3D space through density modeling, it has not been studied for consistent visual manipulation of other modalities, where modality-specific consistency constraints should be considered when manipulating.

In this paper, we propose _Collaborative Score Distillation_ (CSD), a simple yet effective method that extends the singular of the text-to-image diffusion model for consistent visual manipulation. The crux of our method is two-fold: first, we establish a generalization of SDS by using Stein variational gradient descent (SVGD), where multiple samples share their knowledge distilled from diffusion models to accomplish inter-sample consistency. Second, we present CSD-Edit, an effective method for consistent visual editing by leveraging CSD with Instruct-Fix2Pix , a recently proposed instruction-guided image diffusion model (See Figure 1).

We demonstrate the versatility of our method in various editing applications such as panorama image editing, video editing, and reconstructed 3D scene editing. In editing a panorama image, we show that CSD-Edit obtains spatially consistent image editing by optimizing multiple patches of an image. Also, compared to other methods, our approach achieves a better trade-off between source-target image consistency and instruction fidelity. In video editing experiments, CSD-Edit obtains temporal consistency by taking multiple frames into optimization, resulting in temporal frame-consistent video editing. Furthermore, we apply CSD-Edit to 3D scene editing and generation, by encouraging consistent manipulation and synthesis among multiple views.

## 2 Preliminaries

### Diffusion models

Generative modeling with diffusion models consists of a forward process \(q\) that gradually adds Gaussian noise to the input \(_{0} p_{}()\), and a reverse process \(p\) which gradually denoises from

Figure 1: **Method overview**. CSD-Edit enables various visual-to-visual translations with two novel components. First, a new score distillation scheme using Stein variational gradient descent, which considers inter-sample relationships (Section 3.1) to synthesize a set of images while preserving modality-specific consistency constraints. Second, our method edits images with minimal information given from text instruction by subtracting image-conditional noise estimate instead of random noise during score distillation (Section 3.2). By doing so, CSD-Edit is used for text-guided manipulation of various visual domains, e.g., panorama images, videos, and 3D scenes (Section 3.3).

the Gaussian noise \(_{T}(,)\). Formally, the forward process \(q(_{t}|_{0})\) at timestep \(t\) is given by \(q(_{t}|_{0})=(_{t};_{t} _{0},_{t}^{2})\), where \(_{t}\) and \(_{t}^{2}=1-_{t}^{2}\) are pre-defined constants designed for effective modeling [8; 28; 29]. Given enough timesteps, reverse process \(p\) also becomes a Gaussian and the transitions are given by posterior \(q\) with optimal MSE denoiser , i.e., \(p_{}(_{t-1}|_{t})=(_{t-1};_{t}-}_{}(_{t};t),_{t}^{2})\), where \(}_{}(_{t};t)\) is a learned optimal MSE denoiser. Ho et al.  proposed to train an U-Net  autoencoder \(_{}(_{t};t)\) by minimizing following objective:

\[_{}(;)=_{t(0,1 ),(,)}w(t)\|_{}(_{t};t)-\|_{2}^{2},_{t} =_{t}_{0}+_{t} \]

where \(w(t)\) is a weighting function for each timestep \(t\). Text-to-image diffusion models [1; 2; 4; 3] are trained by Eq. (1) with \(_{}(_{t};y,t)\) that estimates the noise conditioned on the text prompt \(y\). To effectively guide the text-conditional generation, Ho et al.  proposed classifier-free guidance (CFG), where they jointly train the unconditional and conditional model and interpolate the unconditional and conditional model during the inference, _i.e._, the noise estimate is given by

\[_{}^{}(_{t};y,t)=_{}(_{t};t)+_{y}_{}(_{t};y,t)-_{}(_{t};t), \]

where \(_{y} 0\) is a guidance scale that controls the sample fidelity. Specifically, increasing \(_{y}\) enhances sample fidelity at the expense of sample diversity. Throughout the paper, we refer \(p_{}^{_{y}}(_{t};y,t)\) a conditional distribution of a text \(y\).

Instruction-based image editing by Instruct-Pix2Pix.Recently, many works have demonstrated the capability of diffusion models in editing or stylizing images [10; 13; 11; 12; 14]. Among them, Brooks et al.  proposed Instruct-Pix2Pix, where they finetuned Stable Diffusion  model with the source image, text instruction, (edited) target image (edited by Prompt-to-Prompt ) triplets to enable instruction-based editing of an image. Then during the inference, Instruct-Pix2Pix starts from a source image and conducts diffusion sampling by the diffusion model that takes instruction \(y\). In specific, given the source image \(}\) and instruction \(y\), the noise estimate at time \(t\) is given by

\[_{}^{_{x},_{y}}(_{t}; {},y,t)=_{}(_{t};t) +_{s}_{}(_{t};},t)-_{}(_{t};t) \] \[+_{y}_{}(_{t};},y,t)-_{}(_{t};},t) ,\]

where \(_{y} 0\) is the CFG parameter for text instruction as in Eq. (2) and \(_{s} 0\) is an additional CFG parameter that controls the fidelity to the source image \(}\).

### Score distillation sampling

Poole et al.  proposed Score Distillation Sampling (SDS), an alternative sample generation method by distilling the rich knowledge of text-to-image diffusion models. SDS allows optimization of any differentiable image generator, e.g., Neural Radiance Fields  or the image space itself.

Figure 2: **Panorama image editing**. (Top right) Instruct-Pix2Pix  on cropped patches results in inconsistent image edits. (Second row) Instruct-Pix2Pix on overlapping patches edits to a consistent image, but less fidelity to the instruction, even with high guidance scale \(_{y}\). (Third row) CSD-Edit provides consistent image editing with better instruction-fidelity by setting a proper guidance scale.

Formally, let \(=g()\) be an image rendered by a differentiable generator \(g\) with parameter \(\), then SDS minimizes density distillation loss  which is a variational inference via minimizing KL divergence between the posterior of \(=g()\) and the text-conditional density \(p_{}^{}\):

\[_{}_{};=g() =_{t,}_{t}/_{t}\,D_{ }q_{t}|=g()\,\|\,p_{}^{ }(_{t};y,t), \]

where \(_{t}=_{t}+_{t}\) with \(=g()\) and \((,)\). They derive SDS by differentiating Eq. 4 with respect to generator parameter \(\), but omitting the U-Net Jacobian term due to its poor performance and computationally inefficient. The SDS gradient update is given as follows:

\[_{}_{};=g() =_{t,}[w(t)_{}^{ }(_{t};y,t)-}{ }]. \]

In its implementation, we randomly sample timestep from uniformly distributed interval \([t_{},t_{}]\). The range of timesteps \(t_{}\) and \(t_{}\) are chosen to sample from not too small or large noise levels, and the guidance scales are chosen to be larger than those used for image generation.

### Stein variational gradient descent

The original motivation of Stein variational gradient descent (SVGD)  is to solve a variational inference problem, where the goal is to approximate a target distribution from a simpler distribution by minimizing KL divergence. Formally, suppose \(p\) is a target distribution with a known score function \(_{} p()\) that we aim to approximate, and \(q()\) is a known source distribution. Liu and Wang  showed that the steepest descent of KL divergence between \(q\) and \(p\) is given as follows:

\[_{q()}()^{}_{} p()+(_{}()) , \]

where \(:^{D}^{D}\) is any smooth vector function that satisfies \(_{\|\|}p()()=0\). Remark that Eq. (6) becomes zero if we replace \(q()\) with \(p()\) in the expectation term, which is known as

Figure 3: **Video editing**. Qualitative results on the lucia video in DAVIS 2017 . CSD shows frame-wise consistent editing providing coherent content across video frames e.g., consistent color and background without changes in person. Compared to Gen-1 , a video editing method trained on a large video dataset, CSD-Edit shows high-quality video editing results reflecting given prompts.

Stein's identity . Here, the choice of the critic \(\) is crucial in its convergence and computational tractability. To that end, Liu and Wang  proposed to constrain \(\) in the Reproducing Kernel Hilbert Space (RKHS) which yields a closed-form solution. Specifically, given a positive definite kernel \(k:^{D}^{D}^{+}\), Stein variational gradient descent provides the greedy directions as follows:

\[-,= _{q(^{})}k(,^{}) _{^{}} p(^{})+_{^{ }}k(,^{}), \]

with small step size \(>0\). The SVGD update in Eq. (7) consists of two terms that play different roles: the first term moves the particles towards the high-density region of target density \(p()\), where the direction is smoothed by kernels of other particles. The second term acts as a repulsive force that prevents the mode collapse of particles. One can choose different kernel functions, while we resort to standard Radial Basis Function (RBF) kernel \(k(,^{})=(-\|- ^{}\|_{2}^{2})\) with bandwidth \(h>0\).

## 3 Method

In this section, we introduce _Collaborative Score Distillation_ (CSD) for consistent synthesis and editing of multiple samples. We first derive a collaborative score distillation method using Stein variational gradient descent (Section 3.1) and propose an effective image editing method using CSD, i.e., CSD-Edit, that leads to coherent editing of multiple images with instruction (Section 3.2). Lastly, we present various applications of CSD-Edit in editing panorama images, videos, and 3D scenes (Section 3.3).

### Collaborative score distillation

Suppose a set of parameters \(\{_{i}\}_{i=1}^{N}\) that generates images \(^{(i)}=g(_{i})\). Similar to SDS, our goal is to update each \(_{i}\) by distilling the smoothed densities from the diffusion model by minimizing KL divergence in Eq. (4). On the contrary, CSD solves Eq. (4) using SVGD demonstrated in Section 2.3 so that each \(_{i}\) can be updated in sync with updates of other parameters in the set \(\{_{i}\}_{i=1}^{N}\). At each update, CSD samples \(t(t_{},t_{})\) and \((,)\), and update each \(_{i}\) as follows:

\[_{_{i}}_{}_{i}=_{j=1}^{N}k(^{(j)}_{t},^{(i)}_{t})( ^{}_{}(^{(j)}_{t};y,t)-)+ _{^{(j)}_{t}}k(^{(j)}_{t},^{(i)}_{t}) \,^{(i)}}{_{i}}, \]

for each \(i=1,2,,N\). We refer to Appendix A for full derivation. Note CSD is equivalent to SDS in Eq. (5) when \(N=1\), showing that CSD is a generalization of SDS to multiple samples. As the pairwise kernel values are multiplied by the noise prediction term, each parameter update on \(_{i}\) is affected by other parameters, i.e., the scores are mixed with importance weights according to the affinity among samples. The more similar samples tend to exchange more score updates, while different samples tend to interchange the score information less. The gradient of the kernels acts as a repulsive force that prevents the mode collapse of samples. Moreover, we note that Eq. (8) does not

Figure 4: **3D NeRF scene editing. Visualizing novel-views of edited Fangzhou NeRF scene . CSD-Edit leads to high-quality editing of 3D scenes and better preserves semantics of source scenes, e.g., obtains sharp facial details (left) and makes him smile without giving beard (right).**

make any assumption on the relation between \(_{i}\)'s or their order besides them being a set of images to be synthesized coherently with each other. As such, CSD is also applicable to arbitrary image generators, as well as text-to-3D synthesis in DreamFusion , which we compare in Section 4.4.

### Instruction-guided editing by collaborative score distillation

In this section, we introduce an instruction-guided visual editing method using Collaborative Score Distillation (CSD-Edit). Given source images \(}^{(i)}\!=\!g(_{i})\) with parameters \(_{i}\), we optimize new target parameters \(\{_{i}\}_{i=1}^{N}\) with \(^{(i)}\!=\!g(_{i})\) such that 1) each \(^{(i)}\) follows the instruction prompt, 2) preserves the semantics of source images as much as possible, and 3) the obtained images are consistent with each other. To this end, we update each parameter \(_{i}\), initialized with \(_{i}\), using CSD with noise estimate \(_{}^{_{},_{}}\) of Instruct-Pix2Pix . However, this approach often results in blurred outputs, leading to the loss of details of the source image (see Figure 6). This is because the score distillation term subtracts random noise \(\), which perturbs the undesirable details of source images.

We handle this issue by adjusting the noise prediction term that enhances the consistency between source and target images. Subtracting a random noise \(\) in Eq. (5) when computing the gradient is a crucial factor, which helps optimization by reducing the variance of a gradient. Therefore, we amend the optimization by changing the random noise into a better baseline function. Since our goal is to edit an image with only minimal information given text instructions, we set the baseline by the image-conditional noise estimate \(_{}^{_{}}\) of the Instruct-Pix2Pix model without giving text instructions on the source image. To be specific, our CSD-Edit is given as follows:

\[_{_{i}}_{}_ {i} =_{j=1}^{N}(k(_{t}^{(j)},_{t}^{(i)})\,}_{t}^{(i)}+_{_{t}^{(j)}}k( _{t}^{(j)},_{t}^{(i)}))^{(i) }}{_{i}}, \] \[}_{t}^{(i)} =_{}^{_{},_{}}(_{t}^{(i)};},y,t)-_{}^{_{}}( }_{t}^{(i)};},t).\]

In Section 4.4, we validate our findings on the effect of baseline noise on image editing performance. We notice that CSD-Edit presents an alternative way to utilize Instruct-Pix2Pix in image-editing without any finetuning of diffusion models, by posing an optimization problem.

### CSD-Edit for various complex visual domains

Panorama image editing.Diffusion models are usually trained on a fixed resolution (e.g., \(512 512\) for Stable Diffusion ), thus when editing a panorama image (i.e., an image with a large aspect ratio), the editing quality significantly degrades. Otherwise, one can crop an image into smaller patches and apply image editing on each patch. However this results in spatially inconsistent images (see Figure 2, Patch-wise Crop, Appendix D). To that end, we propose to apply CSD-Edit on patches to obtain spatially consistent editing of an image, while preserving the semantics of the source image. Following , we sample patches of size \(512 512\) that overlap using small stride and apply CSD-Edit on the latent space of Stable Diffusion . Since we allow overlapping, some pixels might be updated more frequently. Thus, we normalize the gradient of each pixel by counting the appearance. Remark that one can give different instructions on the different regions of an image while maintaining consistency (See Appendix C for details).

Video editing.Editing a video with an instruction should satisfy the following: 1) temporal consistency between frames such that the degree of changes compared to the source video should be consistent across frames, 2) ensuring that desired edits in each edited frame are in line with the given prompts while preserving the original structure of source video, and 3) maintaining the sample quality in each frame after editing. To meet these requirements, we randomly sample a batch of frames and update them with CSD-Edit to achieve temporal consistency between frames.

3D scene editing.We consider editing a 3D scene reconstructed by a Neural Radiance Field (NeRF) , which represents volumetric 3D scenes using 2D images. To edit reconstructed 3D NeRF scenes, it is straightforward to update the training views with edited views and finetune the NeRF with edited views. Here, the multi-view consistency between edited views should be considered since inconsistencies between edits across multiple viewpoints lead to blurry and undesirable artifacts, hindering the optimization of NeRF. To mitigate this, Haque et al.  proposedInstruct-NeRF2NeRF, which performs editing on a subset of training views and updates them sequentially at training iteration with intervals. However, image-wise editing results in inconsistencies between views, thus they rely on the ability of NeRF in achieving multi-view consistency. Contrary to Instruct-NeRF2NeRF, we update the dataset with multiple consistent views through CSD-Edit, which serves as better training resources for NeRF, leading to less artifacts and better preservation of source 3D scene.

## 4 Experiments

### Text-guided panorama image editing

For the panorama image-to-image translation task, we compare CSD-Edit with different versions of Instruct-Pix2Pix: one is which using naive downsizing to \(512 512\) and performing Instruct-Pix2Pix, and another is updating Instruct-Pix2Pix on the patches cropped with overlapping as in MultiDiffusion  (Instruct-Pix2Pix + Overlapping). For comparison, we collect a set of panorama images (i.e., which aspect ratio is higher than 3), and edit each image to various artistic styles and different guidance scales \(_{y}\). For evaluation, we use pre-trained CLIP  to measure two different metrics: 1) consistency between source and target images by computing similarity between two image embeddings, and 2) CLIP directional similarity  which measures how the change in text agrees with the change in the images. The experimental details are in Appendix B.1.

In Figure 5, we plot the CLIP scores of different image editing methods with different guidance scales. We notice that CSD-Edit provides the best trade-off between the consistency between source and target images and fidelity to the instructions. Figure 2 provides a qualitative comparison between panorama image editing methods. Note that applying Instruct-Pix2Pix to patches cropped with overlapping (Instruct-Pix2Pix + Overlapping) is able to edit images while preserving spatial consistency, as evidenced by high CLIP image similarity, however, the edited images show inferior fidelity to the text instruction even when using a large guidance scale, resulting in much lower CLIP directional similarity. We conjecture that this happens because the scores are diluted by other scores, i.e., one patch may respond much more or much less to the instruction compared to others. Additional qualitative results are in Appendix D.

### Text-guided video editing

For the video editing experiments, we primarily compare CSD-Edit with existing zero-shot video editing schemes that employ text-to-image diffusion models such as FateZero  and Pix2Video . To emphasize the effectiveness of CSD-Edit over learning-based schemes, we also compare it with Gen-1 , a state-of-the-art video editing method trained on a large video dataset. For quantitative evaluation, we report CLIP image-text directional similarity as in Section 4.1 to measure the alignment between changes in text and images. We also measure CLIP image consistency and LPIPS  between consecutive frames to evaluate temporal consistency. In addition to the objective metrics

   & CLIP Directional & CLIP Image & LPIPS \\  & Similarity \(\) & Consistency \(\) & \(\) \\  FateZero  & 0.312\({}_{0.008}\) & 0.948\({}_{0.001}\) & 0.264\({}_{0.002}\) \\ Pix2Video  & 0.229\({}_{1.000}\) & 0.948\({}_{1.001}\) & 0.282\({}_{1.000}\) \\
**(CSD-Edit (Ours)** & **0.319\({}_{1.000}\)** & **0.057\({}_{1.000}\)** & **0.235\({}_{1.000}\)** \\  

Table 1: **Video editing.** Quantitative comparison of CSD-Edit with baselines on video editing. Bold indicates the best results.

   & CLIP Directional & CLIP Image & LPIPS \\  & Similarity \(\) & Consistency \(\) & \(\) \\  IN2N  & 0.177\({}_{0.002}\) & 0.993\({}_{0.002}\) & 0.053\({}_{0.003}\) \\
**(CSD-Edit (Ours)** & **0.215\({}_{1.000}\)** & **0.994\({}_{1.000}\)** & **0.045\({}_{1.000}\)** \\  

Table 2: **3D scene editing.** Quantitative comparison of CSD-Edit with baselines on 3D scene editing. Bold indicates the best results.

Figure 5: **Panorama image editing.** Comparison of CSD-Edit with baselines at different guidance scales \(_{y}\{3.0,5.0,7.5,10.0\}\).

for evaluating consistency and instruction fidelity, we also conduct a user study, given the subjective nature of editing tasks. We measure the user rankings for temporal consistency between edited frames, frame-wise instruction fidelity, and the editing quality, respectively. We use video sequences from the popular DAVIS  dataset at a resolution of \(1920 1080\). Please refer to Appendix B.2 and Appendix C.3 for a detailed description of the baseline methods and experimental setup.

Table 1 and Table 6 in Appendix C.3 summarize a quantitative comparison between CSD-Edit and the baselines. We note that CSD-Edit consistently outperforms the existing zero-shot video editing approaches in terms of both temporal consistency and fidelity to given text prompts. Furthermore, Figure 3 qualitatively demonstrates the superiority of CSD-Edit over the baselines. Specifically, CSD-Edit maintains a consistent style across all frames for both the woman and the background elements (e.g., bench, trees), ensuring a consistent degree of editing throughout the video. On the other hand, FateZero and Pix2Video result in noticeably inconsistent edits from one frame to the next. Impressively, CSD-Edit not only demonstrates temporally consistent edits compared to Gen-1, but it also excels at preserving the original semantics of the source video, even without training on a large video dataset and without requiring any architectural modifications to the diffusion model. Additional qualitative results, including video stylization and object-aware editing tasks, are in Appendix D.

### Text-guided 3D scene editing

For the text-guided 3D scene editing experiments, we mainly compare our approach with Instuct-NeRF2NeRF (IN2N) . For a fair comparison, we exactly follow the experimental setup which they used, and faithfully find the hyperparameters to reproduce their results. For evaluation, we render images at the novel views (i.e., views not seen during training), and report CLIP image similarity and LPIPS between consecutive frames in rendered videos to measure multi-view consistency, as well as CLIP image-text similarity to measure fidelity to the instruction. In addition, we conduct user studies to evaluate the multi-view consistency, instruction-fidelity, and the editing quality, respectively. Detailed explanations for each dataset sequence and training details can be found in Appendix B.3.

Figure 4, Table 2, and Table 7 in Appendix C.3 summarize the comparison between CSD-Edit and IN2N. We notice that CSD-Edit enables a wide-range control of 3D NeRF scenes, such as delicate attribute manipulation (e.g., facial expression alterations) and scene-stylization (e.g., conversion to the animation style). Especially, we notice two advantages of CSD-Edit compared to IN2N. First, CSD-Edit presents high-quality details to the edited 3D scene by providing multi-view consistent training views during NeRF optimization. In Figure 4, one can observe that CSD-Edit captures sharp details of the anime character, while IN2N results in a blurry face. Second, CSD-Edit is better at preserving the semantics of source 3D scenes, e.g., backgrounds or colors. For instance in Figure 4, we notice that CSD-Edit allows subtle changes in facial expressions without changing the color of the background or adding a beard to the face.

Figure 6: **Ablation study**. Given a source video (top left), CSD-Edit without SVGD results in inconsistent frames (bottom left), and replacing the subtraction of image-conditioned noise in CSD-Edit to the subtraction of random noise results in loss of details and original structures (top right). CSD-Edit obtains and preserves consistency between edits without loss of semantics (bottom right).

[MISSING_PAGE_FAIL:9]

works mainly focus on applying SDS to enhance the synthesis quality of 3D scenes by introducing 3D-specific frameworks [47; 48; 49; 50; 51]. Although there exists some work to apply SDS for visual domains other than 3D assets, they have limited their scope to image editing , or image generation . Here, we argue that the current main challenge preventing the wider application of SDS, especially in higher-dimensional visual manipulations beyond single 2D images at fixed resolutions, is the lack of control over inter-sample consistency. To the best of our knowledge, our work is the first to identify this challenge and to lay the novel foundations for principled adaptation of text-to-image diffusion models to more diverse and high-dimensional visual manipulations.

## 6 Conclusion

In this paper, we propose Collaborative Score Distillation (CSD) for consistent visual synthesis and manipulation. CSD is built upon Stein variational gradient descent, where multiple samples share their knowledge distilled from text-to-image diffusion models during the update. Furthermore, we propose CSD-Edit that gives us consistent editing of images by distilling minimal, yet sufficient information from instruction-guided diffusion models. We demonstrate the effectiveness of our method in text-guided translation of diverse visual contents, such as in high-resolution images, videos, and real 3D scenes, outperforming previous methods both quantitatively and qualitatively.

**Limitations and future works.** Since we use pre-trained text-to-image diffusion models, obtained results are often imperfect due to the inherent inability of diffusion models to understand language. Furthermore, our method relies on generative priors derived from large text-to-image diffusion models, which may inadvertently contain biases due to the auto-filtering process applied to the vast training dataset. However, we believe that employing Consistent Score Distillation (CSD) can assist us in identifying and understanding such undesirable biases. By leveraging the inter-sample relationships and aiming for consistent generation and manipulation of visual content, our method provides a valuable avenue for comprehending the interaction between samples and prompts. Further exploration of this aspect represents an intriguing future direction.

In addition, although our primary interest is in the editing (not the generation) of panoramic images, videos, or 3D scenes, we believe that CSD has the potential to be used in their generation. As presented in Section C.2 in Appendix C, we show how CSD can improve generation performance over SDS  on text-to-3D generation experiments. In particular, we verify the effect of CSD in improving the geometry and quality of text-to-3D generation. In this sense, exploring this aspect of synthesis with CSD could be an interesting research topic and we leave it for future work.

**Societal impact.** Our research introduces a comprehensive image editing framework that encompasses various modalities, including high-resolution images, videos, and 3D scenes. While it is important to acknowledge that our framework might be potentially misused to create fake content, this concern is inherent to image editing techniques as a whole. We expect future research on the detection of generated visual content.

Figure 8: **Ablation study for the effect of SVGD. Our kernel mixing scores in CSD-Edit act as a regularizer that prevents abrupt changes in images, ensuring better consistency: the tiger is generated in the unwanted region of a source image when SVGD is not applied in the update.**