# Fully Explicit Dynamic Gaussian Splatting

Junoh Lee\({}^{1}\), Changyeon Won\({}^{2}\), Hyunjun Jung\({}^{2}\), Inhwan Bae\({}^{2}\), Hae-Gon Jeon\({}^{1,2}\)

\({}^{1}\)School of Electrical Engineering and Computer Science \({}^{2}\)AI Graduate School

Gwangju Institute of Science and Technology

{juno,cywon1997,hyunjun.jung,inhwanbae}@gm.gist.ac.kr, haegonj@gist.ac.kr

###### Abstract

3D Gaussian Splatting has shown fast and high-quality rendering results in static scenes by leveraging dense 3D prior and explicit representations. Unfortunately, the benefits of the prior and representation do not involve novel view synthesis for dynamic motions. Ironically, this is because the main barrier is the reliance on them, which requires increasing training and rendering times to account for dynamic motions. In this paper, we design Explicit 4D Gaussian Splatting (Ex4DGS). Our key idea is to firstly separate static and dynamic Gaussians during training, and to explicitly sample positions and rotations of the dynamic Gaussians at sparse timestamps. The sampled positions and rotations are then interpolated to represent both spatially and temporally continuous motions of objects in dynamic scenes as well as reducing computational cost. Additionally, we introduce a progressive training scheme and a point-backtracking technique that improves Ex4DGS's convergence. We initially train Ex4DGS using short timestamps and progressively extend timestamps, which makes it work well with a few point clouds. The point-backtracking is used to quantify the cumulative error of each Gaussian over time, enabling the detection and removal of erroneous Gaussians in dynamic scenes. Comprehensive experiments on various scenes demonstrate the state-of-the-art rendering quality from our method, achieving fast rendering of 62 fps on a single 2080Ti GPU.

## 1 Introduction

The recent flood of video content has encouraged view synthesis techniques for highly engaging, visually rich content creation to maintain viewer interest. However, even short form videos require a huge time for both data pre-processing, computing frame-wise 3D point clouds, and training time in novel view synthesis for dynamic motions. Furthermore, these techniques must be considered for running on mobile devices which have a limited computing power and storage space. In this aspect, it is crucial to not only achieve photorealistic rendering results, but also reduce the computational cost related to storage, memory and rendering pipeline. To achieve this, the spatio-temporal representation should be explicit and efficient to handle the complexity of dynamic motions in videos.

Recent methods for dynamic view synthesis are typically based on Neural Radiance Fields (NeRF) , which uses implicit multi-layer perceptron (MLP) with a combination of 5-DoF spatial coordinates and the additional time axis [2; 3; 4; 5; 6; 7; 8]. The MLP-based methods have shown high-fidelity rendering quality. However, the inevitable cost of decoding the implicit representation makes rendering critically slow. Various methods try to reduce the cost by adapting explicit representation, such as voxel [9; 10] and matrix decomposition [11; 12]. Nevertheless, since NeRF-based approaches require per-pixel ray sampling and dense sampling for each ray, it is hard to achieve real-time and high-resolution renderings.

Meanwhile, a promising alternative, 3D Gaussian Splatting (3DGS) , has emerged, which achieves photo-realistic rendering results with significantly faster training and rendering speeds.

Unlike NeRF-based approaches, 3DGS exploits fully explicit point-based primitives of 3D and employs a rasterization-based rendering pipeline. Recent advances [14; 15] attempt to extend 3DGS to 4D domain, handling the motion over time by storing the additional transformation information of 3D coordinates or 4D bases. However, these methods can only be trained under a restricted condition with dense point clouds. Moreover, these approaches borrow implicit representations for implementation, which lose the inherent advantage of 3DGS and make real-world applications more difficult. To become a more scalable model for real-world situations, it is important to be trained under more in-the-wild conditions (i.e., sparse point cloud) with more concise representation.

In this paper, we present Explicit 4D Gaussian Splatting (Ex4DGS), a keyframe interpolation-based 4D Gaussian splatting method that works well in a fully explicit on-time domain. Our key idea is to apply interpolation techniques under temporal explicit representation to realize the scalable 3DGS model. Temporal interpolation is a widely used technique in computer graphics  that only stores keyframe information in video and determines smooth transitions for the rest of the frames. We select keyframes with sparse time intervals and save the additional temporal information at each keyframe which includes each Gaussian's position, rotation and opacity. This information is fully explicit; it is stored without any encoding process, and continuous motion is calculated to have smooth temporal transitions between adjacent keyframes. Here, we use a polynomial basis interpolator for position, a spherical interpolator for rotation, and a simplified Gaussian mixture model for opacity. Specifically, the polynomial basis of cubic Hermite spline (CHip)  is used to effectively avoid overfitting or over-smoothing problems by spanning low-degree polynomials. For rotation, we introduce Spherical Linear Interpolation (Slerp)  to do a linear transition over angles. Lastly, we introduce a simplified Gaussian mixture model, which allows temporal opacity to handle the appearance and disappearance of objects.

For further optimization, we reduce the computational cost by isolating dynamic points from static points, and only storing additional temporal information of dynamic points. Here, we aim for this separation to be possible without any additional inputs such as object masks . To tackle this, we introduce motion-based triggers to distinguish static and dynamic points in a scene. We first initialize all Gaussian points in a scene to be static which are assumed to move linearly. During training, static points with large movements are automatically classified as dynamic points. Next, we adopt a progressive training scheme to train our model even under sparse point cloud conditions. The progressive training, starting with a short duration and scaling up, can prevent falling into local minima by reducing the sudden appearance of objects. Lastly, an additional point backtracking technique is introduced to enhance rendering quality. Detecting redundant points in a dynamic scene is challenging because we need to consider all visible timestamps. To measure accumulated errors over time, we apply the point-backtracking technique that can track and prune high-error Gaussians in dynamic scenes.

The effectiveness of our method is validated through experiments on two major real-world video datasets: Neural 3D Video dataset  and Technicolor dataset . Experimental results demonstrate that our approach significantly improves rendering results even with sparse 3D point clouds. Furthermore, benefiting from the proposed optimization scheme, our model only requires low storage and memory size without any auxiliary modules or tricks to encode lengthy temporal information. Finally, our model achieves 62 fps on a single 2080Ti GPU on 300-frame scenes with a \(1352 1014\) resolution.

## 2 Related Works

### Novel View Synthesis

Photorealistic novel view synthesis can be achieved using a set of densely sampled images through image-based rendering [21; 22]. While the dense sampling of views is limited by memory constraints and results in aliased rendering, novel view synthesis has advanced with the development of neural networks. The ability of neural networks to process implicit information in images enables novel view synthesis with a sparse set of observations. Prior works on constructing multi-plane images (MPI)[23; 24; 25] use aggregated pixel information from correspondences in sparsely sampled views. MPI representation may fail with wide angles between the camera and depth planes. This can be mitigated using geometric proxies like depth information [26; 27], plane-sweep volumes [28; 29], and meshes [30; 31]. These methods risk unrealistic view synthesis with inaccurate proxy geometry.

Joint optimization of proxy geometry [32; 33; 34; 35] can help, but direct mesh optimization often gets stuck in local minima due to poor loss landscape conditioning.

In recent years, continuous 3D representation through neural networks has received widespread attention. NeRF  is the foundational work that started this trend. It implicitly learns the shape of an object as a density field, which makes optimization via gradient descent more tractable. The robustness of such geometric neural representations enables to reconstruct accurate geometry from a few given images [36; 37; 38; 39], large-scale geometry [40; 41; 42; 43; 44; 45], disentangled textures [46; 47; 48], and material properties [49; 50; 51; 52; 53]. However, one major issue on these neural representations is also slow rendering speeds coming from volume renderers. To resolve this issue, works in [54; 55] develop light field-inspired representations for single-pass pixel rendering. Both  and  introduce efficient voxel-based scene representations to improve the rendering speed. However, continuous representation inevitably combines with neural networks to implement its complex nature, and this limits rendering speed. As an alternative, 3DGS  is designed to construct an explicit radiance field through rasterization, not requiring MLP-based inference. This method leverages anisotropic 3D Gaussians as the scene representation and proposes a differentiable rasterizer to render them by splatting onto the image plane for static scenes.

### Dynamic Novel View Synthesis

The time domain in dynamic scenes is typically parameterized with Plenoptic function . Classical image-based rendering [21; 22] and novel view synthesis methods using explicit geometry [30; 31] are restricted to a limited memory space  when they extend to the time dimension. This is because they require additional optimization procedures for frame-by-frame dynamic view synthesis and storage for the parameterized space. Fueled by implicit representations, works in [2; 3; 8; 59; 4; 7; 5] handle challenging tasks using neural volume rendering. They learn dynamic scenes by optimizing deformation and canonical fields for object motions [2; 3; 6], using human body priors [60; 59; 61; 62; 63; 64; 65; 66], and decoupling dynamic parts [4; 5; 67; 68; 69; 70; 71; 8; 72]. These methods introduce additional neural fields or learnable variables to represent the time domain, taking more memory usage and rendering time as well.

Temporally extended 3DGS has been considered as a feasible solution to dynamic novel view synthesis. A work in  assigns parameters to 3DGS at each timestamp and imposes rigidity through a regularization. Another work in  leverages Gaussian probability to model density changes over time to explicitly represent dynamic scenes. However, they require many primitives to capture complex temporal changes. Concurrently, works in [14; 75; 76; 77; 78; 79] utilize MLPs to represent the temporal changes. These methods inherit the drawbacks of dynamic neural representations, resulting in slower rendering speeds. The others in [15; 80] explicitly parameterize the motion of dynamic 3D Gaussians to preserve the rendering speed of 3DGS by predicting their trajectory function. However, they only handle the motion as a continuous trajectory, and require multiple Gaussians for motions that disappear and reappear due to self-occlusion, increasing memory burden.

In contrast, our key idea for dynamic 3D Gaussian uses keyframes to minimize primitives and to devise a progressive optimization to cope with scenarios where face disappearing/reappearing objects. Thanks to our schemes, we can improve rendering speed, memory efficiency, and achieve impressive performance for dynamic novel view synthesis.

## 3 Preliminary: 3D Gaussian Splatting

Our model starts from the point-based differentiable rasterization of 3DGS . 3DGS uses three-dimensional Gaussian as geometry primitive, which is composed of position (mean) \(\), covariation \(\), density \(\) and color \(\). The 3D Gaussian is referred to as follows:

\[()=e^{-(-)^{}^{-1}( -)}. \]

We need to project the 3D Gaussian onto a 2D plane to render an image. In this process, the approximated graphics pipeline is used to render 2D Gaussians. The covariance matrix \(^{}\) in camera coordinate is given as follows:

\[^{}=\,\,\,\,^{}^{}, \]

where \(\) is the Jacobian of the affine approximation of the perspective projection and \(\) is a viewing transformation. By skipping the third row and column of \(^{}\), it is approximated to two-dimensional anisotropic Gaussian on the image plane.

The covariance is a positive semi-definite which can be decomposed into a scale \(\) and a rotation \(\) as:

\[=^{}^{}. \]

Spherical harmonics coefficients are used to represent view-dependent color changes as proposed in . A rendered color from the Gaussian uses point-based \(\) blending similar to NeRF's volume rendering. For the interval between points along ray \(\) which can be obtained from \(()\), the color of ray is

\[C=_{i=1}^{N}T_{i}(1-e^{-_{i}_{i}})_{i},\ T_{i}=e^{-_ {j=1}^{i-1}_{j}_{j}}, \]

where \(N\) is the number of visible Gaussians along the ray, \(i\) and \(j\) denote the order of Gaussians by depth.

## 4 Methodology

To achieve both memory efficiency and rendering capacity, our scheme is two-fold: (1) Keyframe-based interpolation to span position and rotation of Gaussians over time; (2) Classification of static and dynamic Gaussian. These are described in Sections 4.1 and 4.2. After that, we introduce our progressive training scheme to handle a variety of running times in Section 4.3, and deal with details of the optimization process of our method in Section 4.4. The overview of our method is depicted in Figure 1.

### Static Gaussians

Static Gaussian \(_{s}\) is modeled as the same as the 3DGS model except for its position. \(_{s}\) changes the position linearly over time, which can be formulated with the position \(\) at time \(t\) as below:

\[(t)=+t^{},t^{}= \]

where **x** is a pivot position of \(_{s}\) and **d** is a vector representing the translation, and \(l\) is the duration of a scene. We normalize \(t\) with \(l\) to prevent **d** from becoming too large.

### Dynamic Gaussians

The dynamic Gaussian model is based on interpolations of keyframes, as visualized in Figure 2. Specifically, the state of the dynamic Gaussian \(_{d}\) at an intermediate timestamp is synthesized from adjacent keyframes. In this work, we assume that the keyframe interval is uniform for simplicity. The keyframe is defined as \(=\{t\ |\ t=nI,n,t\}\) where \(I\) is its interval and \(\) is a set of timestamps. \(_{d}\) acquires position \(\) and rotation in quaternion \(r\) from keyframe information. We use different interpolators with different properties for smooth and continuous motion: CHip using polynomial bases is applied for positions, and a Slerp is used for rotations. We further adapt the Gaussian mixture model for temporal opacity to handle changes in the visibility of objects over time.

#### 4.2.1 Cubic Hermite Interpolator for Temporal Position

CHip uses a third-degree polynomial. It is commonly used to model dynamic motions or shapes . The interpolator function can be defined with third-degree polynomials and four variables: position and tangent vector of the start and end points. On the unit interval \(\), given a start point \(p_{0}\) at \(t=0\) and an end point \(p_{1}\) at \(t=1\) with start tangent \(m_{0}\) at \(t=0\) and an end tangent \(m_{1}\) at \(t=1\), CHip can be defined as:

Figure 1: Overview of our method. We first initialize 3D Gaussians as static, modeling their motion linearly. During optimization, dynamic and static objects are separated based on the amount of predicted motion, and the 3D Gaussians between the selected keyframes are interpolated and rendered.

\[(_{0},_{0},_{1},_{1};t )\ =&\ (2t^{3}-3t^{2}+1)_{0}+(t^{3}-2t^{2}+t)_{0}\\ &+(-2t^{3}+3t^{2})_{1}+(t^{3}-t^{2})_{1},\ \ \ t. \]

Based on CHip, we compute the position \(\) of \(_{d}\) at time \(t\) as follows:

\[(t)=(_{n},_{n},_{n+1},_{n+1};t^{}),\\ \ \ n=,\ \ t^{}=,\ \ _{n}=_{n+1}-_{n-1}}{2I},\ \ _{n+1}=_{n+2}-_{n}}{2I}, \]

where \(_{n}\) is a position of Gaussian at \(n-\) keyframe. We use tangent values that is calculated using the position of two neighbor keyframes. This design can reduce additional requirements for storing tangent values at each keyframe, while still keeping the representational power for complex motion.

Other interpolators such as linear interpolation or piecewise cubic Hermite interpolating polynomial can be alternative choices. In this work, the cubic Hermit interpolator is selected because we can approximate the complex movements of points without any additional computational cost or storage.

#### 4.2.2 Spherical Linear Interpolation for Temporal Rotation

Slerp is typically used for interpolating rotations  because linear interpolation causes a bias problem when it interpolates angular value. On the unit interval \(\), given the unit vector \(_{0}\) and \(_{1}\) which represent rotations at \(t=0\) and \(t=1\) each, Slerp is defined as follows:

\[(_{0},_{1};t)= _{0}+_{1},\ \ \ \ t\ \ \ =_{0}_{1}. \]

Slerp can be directly applied to quaternion rotations since it is independent of quaternions and dimensions. We thus have a rotation of intermediate frames in the quaternion of \(_{d}\) at time \(t\) without any modification as follows:

\[q(t)=(_{n},_{n+1};t^{})\ \ \ \ n=,\ \ t^{}=, \]

where \(_{n}\) is the rotation of Gaussian at \(n-\) keyframe.

#### 4.2.3 Temporal Opacities

Modeling the temporal opacity is important because it is directly related to appearing/disappearing objects. A straightforward model for temporal opacity is to directly use a single Gaussian. However, there is a limitation to model diverse cases only using the single Gaussian, such as sudden emerging/slowly vanishing objects and objects disappearing in videos, as illustrated in Figure 3. We introduce the Gaussian mixture model to handle these situations. Since using too many Gaussians is impractical, we approximate the model with two Gaussians. We divide the temporal opacity into three cases: when an object appears, the object remains and the object disappears. One Gaussian handles the appearance of the object and the other manages disappearance. The interval between two Gaussians indicates the duration of the object when it is fully visible.

Let a Gaussian with a smaller mean value be \(g_{s}^{o}\) and the other is \(g_{f}^{o}\) where \(a_{s}^{o}<a_{f}^{o}\). And, \(a_{s}^{o}\), \(b_{s}^{o}\), \(a_{f}^{o}\) and \(b_{f}^{o}\) are the mean and variance of \(g_{s}^{o}\) and the mean and variance of \(g_{f}^{o}\) each. The temporal opacity

Figure 3: Comparison between the single Gaussian, Gaussian mixture, and our model for temporal opacity modeling.

Figure 2: Effectiveness of our keyframe interpolation.

\(_{t}\) at time \(t\) is defined as follows:

\[_{t}(t)=e^{-(^{o}}{b_{f}^{o}})^{2}},&  t<a_{s}^{o}\\ 1,& a_{s}^{o} t a_{f}^{o}\\ e^{-(^{o}}{b_{f}^{o}})^{2}},& t>a_{f}^{o }\,. \]

Using a single Gaussian may require multiple points to represent an object over a long duration. In contrast, our model can handle both the short and long temporal opacity of an object using two Gaussians.

### Training Scheme

#### 4.3.1 Progressive training scheme

Our goal is to minimize both memory and computational costs of the entire pipeline, including preprocessing, not just reducing the representation of 3DGS model in a dynamic scene. To this end, we adopt a progressive training scheme that allows to learn over a long duration using only a small amount of point clouds obtained from the first frame, which is illustrated in Figure 4. To effectively handle objects moving or disappearing quickly, our model starts to learn a small part of an input video and gradually increases the video duration. The duration is incremented every specific step and made longer by the interval size.

Expanding time durationAs the time duration increases, the number of keyframes in the dynamic Gaussian obviously increases. We estimate the position and rotation of the Gaussian by linear regression using the last \(\) frames when the number of keyframes increases so that the motion information of the previous frame can be shared with the next keyframe.

Extracting dynamic points from static pointsWe want to separate dynamic points from static points without auxiliary information or supervision, such as masks or scribbles . The separation is done based on the motion of the static points which is modeled to be movable, so we select the dynamic points based on the distance they moved. In particular, to avoid biased selection of distant points, we measure the motion in image space, normalizing the translation by the distance between points and the camera at the last timestamp. Therefore, if the distance to a point from the camera at the last timestamp is \(\), then the expression is \(}\|}{\|\|^{2}}\). We sort points by the measured movement and convert the top-\(\) percent of points (in this work, \(=2\) is empirically set) into dynamic points. The position of the converted dynamic points is estimated at each keyframe using \(}\) and \(}\). The rotation is made to have the same value in all keyframes, and the opacity is initialized to be visible in all keyframes. We perform the extraction when we increase the duration or at specific iterations.

### Optimization

Point backtracking for pruningSince it is difficult to filter out unnecessary dynamic points in a temporal context, we introduce a way to track errors in the image as points. Unlike contemporary works  that track points, we let our model track the value on a single backward pass. We use two measures, L1 distance and SSIM, whose formula is as follows:

\[=(_{i}_{j=1}^{i-1}(1-_{j }) q_{k})}{_{k}(_{i}_{j=1}^{i-1}(1- _{j}))}, \]

where \(q_{k}\) is the measured error in image space, \(k\) is a pixel index, and \(i\) and \(j\) are the order of Gaussian by depth which is visible at \(k-\) pixel. The accumulated error \(_{total}\) is as follows:

\[_{total}=}_{v}}{_{v }}, \]

where \(\) is a set of training views. We prune the points over \(_{total}\) at every pre-defined step.

Regularizations and lossesWe use regularization for large motions on both static and dynamic points. The regularization minimizes \(\|}\|\) for static points and \(\|_{n+1}-_{n}\|\) for dynamic points. The optimization process follows 3DGS, which uses differentiable rasterization based on gradient backpropagation. Both L1 loss and SSIM loss, which measure the error between a rendered image and its ground truth, are used.

Figure 4: Progressive learning of dynamic Gaussians.

### Implementation Details

Our codebase is built upon 3DGS  and Mip-Splatting  and uses almost its hyperparameters. For initialization, our experiments use only COLMAP  point clouds from the first frame. The time interval and initial duration are both set to \(10\). We increment the duration by its interval every 400 iterations. Both static and dynamic regularization parameters are set to \(0.0001\). We employ the RAdam optimizer  for training.

## 5 Experiments

In this section, we conduct comprehensive experiments on two real-world datasets, Neural 3D Video  and Technicolor dataset  in Sections 5.1 and 5.2 each. We follow a conventional evaluation protocol in , which uses subsequences divided from whole videos. We report PSNR, SSIM and LPIPS values. For SSIM, we use _scikit image_ library. Here, SSIM\({}_{1}\) and SSIM\({}_{2}\) are computed using _data_range_ value of \(1\) and \(2\), respectively. We also measure frame-per-second (FPS) for rendering speed, and training time including preprocessing time. To compare the robustness of our method according to initial point clouds, we additionally test contemporary works on sparse point cloud initialization, which uses only the point cloud of the first frame in Sections 5.1 and 5.2. We also visualize the separation of static and dynamic points to show that our model can successfully distinguish them in Section 5.3. The ablation study shows the effectiveness of each component in our model in Section 5.4.

### Neural 3D Video Dataset

Neural 3D Video dataset  provides six sets of multi-view indoor videos, captured with a range of 18 to 21 cameras with a 2704\(\)2028 resolution and 300 frames. Following the conventional evaluation protocol, both training and evaluation procedures are performed at half the original resolution, and the center camera is held out as a novel view for evaluation. For a fair comparison, we train all models for all 300 frames including concurrent works, except for STG , NeRFlayer  and HyperReel . For NeRFlayer and HyperReel, we directly borrow the results from . For STG, it is not possible to train for all 300 frames due to a GPU memory issue, so we report the results for only 150 frames, which is the maximum duration running on a single NVIDIA H100 80GB GPU.

4K-AD  & N/A & N/A & N/A & N/A & N/A & 32.86 & N/A & 110 & N/A \\   shown in Table 1, our model outperforms most of the contemporary models while maintaining the low computational cost. The example is displayed in Figure 5, which shows that our model produces high-quality rendering results over the comparison methods.

Comparison on sparse conditionsWe also carry out an experiment to check if concurrent methods work well with sparse point cloud initialization, which uses only it for the first frame. We report the result in Table 1. Interestingly, all the concurrent methods yield unsatisfactory results because motions in videos are learned by relying on the point clouds, not temporal changes of objects in the training phase. This implies that they require well-reconstructed 3D point clouds for proper initialization, while our method is free from the initial condition.

### Technicolor Dataset

Technicolor light field dataset encompasses video recordings captured using a 4\(\)4 camera array, wherein each camera is synchronized temporally, and the spatial resolution is 2048\(\)1088. Adhering to the methodology introduced in HyperReel , we reserve the camera positioned at the intersection of the second row and second column for evaluation purposes. Evaluation is conducted on five distinct scenes (Birthday, Fabien, Painter, Theater, Trains) using their original full resolution. We retrain STG  using the COLMAP point cloud from the first frame, instead of the point cloud from every frame, for strictly fair comparison.

As shown in Table 2, Ex4DGS is comparable with the second-best model in the sparse COLMAP scenario. Although the Technicolor dataset contains various colorful objects, our model successfully synthesizes the novel view without dense prior or additional parameters. The reason why STG shows the impressive performance is that Technicolor dataset does not have rapid movements.

   Model & PSNR & SSIM\({}_{1}\) & SSIM\({}_{2}\) & LPIPS \\  DyNeRF  & 31.80 & N/A & 0.958 & 0.142 \\ HyperReel  & 32.73 & 0.906 & N/A & 0.109 \\ STG\({}^{}\) & 33.23 & 0.912 & 0.960 & 0.085 \\
4DGS  & 29.54 & 0.873 & 0.937 & 0.149 \\
4DGaussians  & 30.79 & 0.843 & 0.921 & 0.178 \\
**Ours** & 33.62 & 0.916 & 0.962 & 0.088 \\   

Table 2: Comparison results on the Technicolor dataset . \(\): Trained with sparse point cloud input.

Figure 5: Comparison of our Ex4DGS with other the state-of-the-art dynamic Gaussian splatting methods on Neural 3D Video  dataset.

Figure 6: Visualization of our static and dynamic point separation on Coffee Martini, Flame Steak and Fabien scene in Neural 3D Video  and Technicolor  datasets.

### Separation of Dynamic and Static Points

Ex4DGS has a capability to separate static and dynamic points during the learning process. To check how well they are separated, we render them individually. Figure 6 shows the separation result. The static and dynamic points are rendered on both the Neural 3D Video and Technicolor datasets. The results demonstrate that the dynamic points are successfully separated from the static points, even if they are trained in an unsupervised manner. As a result, view-dependent color-changing or reflective objects are also identified as dynamic parts. Furthermore, in Coffee Martini scene, Ex4DGS demonstrates the ability to detect dynamic fluid in the transparent glasses. It is also worth highlighting that the same object can have static and dynamic components, as shown in the dog's legs and head being classified as distinct points in the Flame Steak scene.

### Ablation Studies

We conduct an extensive ablation study to check the effectiveness of the proposed technical components in Table 3.

We first examine the effectiveness of our interpolation method by changing them into linear models. The results show that linear modeling of the position and rotation reduces the quality of rendering. Interestingly, using different types of interpolations further exacerbates the performance. If an equal level of polynomial bases are not assigned to both attributes, one of them falls short of the representational capacity, resulting in overfitting or over-smoothing.

We also show how our dynamic point extraction affects the rendering quality. As expected, complex motions can only be handled with dynamic point modeling. We then evaluate the efficacy of our temporal opacity modeling, and observe the performance degradation when no temporal opacity changes. Points can only disappear by minimizing the size and hiding back to other Gaussian points, making them act as flutters without being removed properly.

We then check the effectiveness of our progressive growing strategy. Without this strategy, the optimization gets stuck in local minima. This is due to our approach using only the point cloud from the first frame, which results in a misalignment with their corresponding objects of future frames.

We evaluate our regularization terms for the temporal dynamics of both static and dynamic points within a scene. As expected, incorporating the additional regularization term into the learning process makes the dynamic scene representations better. This benefit comes from the reduction of accumulated motion errors, preventing the points from moving excessively and locating them at correct positions.

Lastly, we examine the effectiveness of our point backtracking approach for pruning step. As expected, the correct removal of the misplaced points mitigates the errors and leads to the best result.

## 6 Conclusion

We have proposed a novel parameterization of dynamic 3DGS by explicitly modeling 3D Gaussians' motions. To achieve this, we initially set keyframes and predict their position and rotation changes. Primitive parameters of the 3D Gaussians between keyframes are then interpolated. Our strategy for learning dynamic motions enables us to decouple static and dynamic parts of scenes, opening up more intuitive and interpretable representation in 4D novel view synthesis.

LimitationsAlthough we achieve a memory-efficient explicit representation of dynamic scenes, two challenges remain. First, our reconstruction can get stuck in local minima for newly appeared objects that are not initialized with 3D points and have no relevant 3D Gaussians in neighboring frames. This issue could be mitigated by initializing new 3D points with an additional geometric prior such as depth information. Second, as 3DGS suffers from scale ambiguity, training on monocular videos is challenging. This is because every 3D Gaussians are treated as dynamic due to the lack of accurate geometric clues for objects at each timestamp. This challenge can be addressed by incorporating an additional semantic cue information like object mask and optical flow, which account for objects' motions more explicitly.

   Method & PSNR & SSIM\({}_{1}\) & LPIPS & Size(MB) \\  w/ Linear position & 31.12 & 0.9385 & 0.0524 & 204 \\ w/o Temporal opacity & 31.42 & 0.9394 & 0.0521 & 186 \\ w/ Linear rotation & 31.26 & 0.9392 & 0.0525 & 148 \\ w/o Progressive growing & 31.02 & 0.9389 & 0.0550 & 168 \\ w/ Linear position\&rotation & 31.32 & 0.9394 & 0.0521 & 172 \\ w/o Regularization & 31.37 & 0.9395 & 0.0522 & 174 \\ w/o Dynamic point extraction & 28.58 & 0.9280 & 0.0756 & 58 \\ w/o Point backtracking & 31.40 & 0.9394 & 0.0529 & 169 \\ 
**Ours** & 32.11 & 0.9422 & 0.0478 & 115 \\   

Table 3: Ablation studies of the proposed methods.