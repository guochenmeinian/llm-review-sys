# Asynchronous Perception Machine

for Efficient Test Time Training

 Rajat Modi

Correspondence to rajatmodi62@gmail.com.

Yogesh Singh Rawat

Centre for Research in Computer Vision

University of Central Florida

Orlando, FL 32765

rajat.modi@ucf.edu,yogesh@crcv.ucf.edu

[https://rajatmodi62.github.io/apm_project_page](https://rajatmodi62.github.io/apm_project_page)

###### Abstract

In this work, we propose Asynchronous Perception Machine (APM), a computationally-efficient architecture for test-time-training (TTT). APM can process patches of an image one at a time in any order _asymmetrically,_ and _still encode_ semantic-awareness in the net. We demonstrate APM's ability to recognize out-of-distribution images _without_ dataset-specific pre-training, augmentation or any-pretext task. APM offers competitive performance over existing TTT approaches. To perform TTT, APM just distills test sample's representation _once_. APM possesses a unique property: it can learn using just this single representation and starts predicting semantically-aware features.

APM demonstrates potential applications beyond test-time-training: APM can scale up to a dataset of 2D images and yield semantic-clusterings in a single forward pass. APM also provides first empirical evidence towards validating GLOM's insight, i.e. if input percept is a field. Therefore, APM helps us converge towards an implementation which can do _both_ interpolation and perception on a _shared_-connectionist hardware. Our code is publicly available at this link.

## 1 Introduction

In these past centuries, computing-machines have become a lot faster . This made it possible to train higher-parameterized neural nets and led to interesting emergent abilities . As was predicted by Turing himself, and as were his suspicions of Lady Lovelace's arguments _against_ learning machines: neural-nets can now finally learn without human-feedback , paint pictures  and even compose a sonnet . Even with such impressive-progress, a key question still remains: how can these nets recognize images whose distribution is far different from the ones which were used during training? For e.g., consider a self-driving car trying to stop when it encounters a pedestrian crossing a road. Such practical scenarios require 'instantaneous-decisions'2 for ensuring human-safety in autonomous-systems .

Test-time-training (TTT)  is one of the promising techniques for handling such distribution shifts: a neural net adapts to a test sample 'on the fly'. Since the label of the sample is not known, the net performs some auxiliary pre-text task like data augmentation , rotation  or prompt tuning  on it. After several such iterations, the net recognizes the test sample. The key idea is that the net is allowed to _dynamically_ adjust its decision boundary even _after_ it has been trained, thereby bringing it much closer to how humans keep learning 'continuously' throughout their lifespans .

Despite the success of existing TTT approaches, several limitations need to be addressed [84; 15; 86]: 1) **The Information Bottleneck Problem**: Multiple TTT iterations requires feed-forward through many hidden layers _multiple times_, making it computationally expensive. 2) **Reliance on a surrogate pre-text task:** the optimal data-augmentation pipeline or the best pretext task is not known beforehand, worsening the issue even further in an online setting. 3) Furthermore, TTT leverages architectures like transformers which rely on **parallel perception**: this requires projecting all input patches into a shared representational space, thereby consuming significant memory.

**Inspired by GLOM's philosophy**, we hereby propose Asynchronous Perception Machine (APM), a new architecture for efficient test-time-training. 1) It handles the information-bottleneck problem by directly learning a shortcut from input image to final representation from last layer of a model . 2) During TTT, we compute test-sample's representation _only once_. Subsequent iterations involve over-fitting on this representation only and _doesn't_ require any data-augmentation/pretext task. 3) APM can operate on a _single_ patch at any location _asynchronously_ and still encode semantic-awareness in the net, thereby offering a _fresh perspective_ towards machine-perception.

We make the following contributions in this work:

* We propose APM, a GLOM-inspired architecture that can perform test-time-training _without_ requiring data augmentation/auxiliary-tasks. APM is a step towards validating GLOM's philosophical insight: a percept is _really_ a field .
* APM is computationally efficient, i.e it almost _halves_ the number of FLOPs over existing methods [15; 86]. APM matches/surpasses TTT performance by \(0.4\%-8\%\) across \(16\) datasets.
* APM is architecturally simple: a convolutional layer and a single MLP of 5 layers. APM can still learn using one sample and semantically clusters on a collection of images .

## 2 Background

We draw from insights previously philosophically mentioned in GLOM . Consider how machine perception has been done classically: an input \(x^{c h w}\) is transformed by a non-linear function \(f\) to a perceptual feature grid \(y^{c^{} h w}\), \(c\) denotes image channels, \(h,w\) are input spatial dimensions.

**Islands of agreement ** Rather than viewing this matrix \(y\) as a cuboidal grid, one can now see this as column vectors \(v_{c^{}}\) at each location. There are \(h w\) such columns in \(y\). Therefore, there exists a one-to-one mapping, between each input patch i.e. \((x,y)\) with the vector \(v_{c^{}}\) at that location. A neural net \(f\) can then thus learn to fit \(f(I,x,y) v_{c^{}}\). These vectors \(v_{c^{}}\) have been termed islands of agreement in GLOM . They were recently demonstrated on realistic datasets by , showing some promise that trained models can serve as free supervision-sources for vectors \(v_{c^{}}\).

Figure 1: **(i) Asynchronous Perception Machine (APM)**: An image \(I\) passes through a column module and routes to a trigger column \(T_{i}\). \(T_{i}\) then unfolds and generates \(h w\) location-specific queries. These queries are i.i.d and can be parallelized across cores of a gpu . Each query \(T_{i}\) is passed through a _shared_ MLP and yields the vector \(f^{}_{xy}\) and \(f^{}_{ryb}\). MLP is queried iteratively until whole grid \(f^{}\) comes into existence. Classification then involves comparing the averaged representation \(f\) with class-specific textual representations in the contrastive space. **(ii) Folded State**: The parameters which the net learns are parameters of \(T\) and MLP. **(iii) Unfolded State**: \(T\) expands to yield \(h w\) queries ‘on-the-fly’. Learning involves oscillating this net between folded and unfolded states. This net can be trained via backpropogation [78; 28].

Next, we imagine feed-forwarding one patch \((x,y)\) into the neural net \(f\) and estimating the response \(v_{c^{}}\). It thus becomes possible to process these patches one-by-one rather than keeping them all together in memory . We then further imagine auto-regressively querying \(f\) until the whole grid \(y\) has been brought into existence.

**Implicit-representations/Neural-Fields:** The neural net \(f\) inputs a coordinate based query, \((I,x,y)\) where \((x,y)\) is the 2D location in an image \(I\). It then gives the answer \(v_{c^{}}\). Such implicit representations have been studied extensively in 3D novel-view synthesis . However, in the above problem \(I\) is a simple 2D image. In a recent work , it was hypothesized that neural-implicit representations can work on 2D images as well without having to retrain it every-time.

**Layer Skipping **: We now combine the previous two insights together. Imagine that \(v_{c^{}}\) has been estimated from a model after travelling through several layers. We can distill these \(v_{c^{}}\) into an implicit representation \(f\) and learn a direct mapping between input \(x\) and last layer of a model . During inference, we can skip feed-forwarding through _all_ the model layers. The only feed-forward cost we then pay is what it costs to travel through \(f\). Provided that the number of parameters in \(f\) are lesser than parameters in the teacher model, _we can expect computational speedups_.

We now introduce a connectionist-net \(f\) motivated by these insights. It can perform efficient test-time-training on a given 2D input.

## 3 Asynchronous Perception Machine (APM)

APM processes an input image \(I\) via two novel mechanisms: 1) A column module \(T\) which is said to contain an input image \(I\), 2) a column folding-unfolding mechanism that operates during each forward-pass. We first provide a technical analogy to better understand APM.

### A Technical Analogy

A neural field does 3D novel view synthesis by querying an MLP with a point \((x,y,z)\) and yielding corresponding rgb. In a similar way, APM does _2D perception_ by querying an MLP with an image \(I\), and a location \((x,y)\) to yield location-specific feature \(f_{xy}\). APM features a new mechanism to query the MLP, i.e _a column module_\(T\). Next, we define this column representation \(T\).

### Global Column Module: Defining compressed representation T

We define a column \(T\) as a vector of dimensions \(1 1 d\). Our aim is to map image \(I\) in this \(T\), so that \(T\) can summarize its entire identity. Given an image of dimensions \(c h w\), we run a 2D convolution on it. Number of filters are set as accordingly. The resultant \(1 h^{} w^{}\) feature map is then flattened into a single column vector T of dimensions \(d=h^{} w^{}\). We shall refer to this column T as "triggering hyper-column"(seed). _The only learnable parameters in this column are parameters of a convolution filter_.3.

### Abstract view: The Column Unfolding-Folding Mechanism

The trigger column T now starts undergoing cycles of folding-unfolding (Fig 1). During unfolding, the column T copies itself to yield \(h w\)_location-aware_ columns. During folding, these \(h w\) columns collapse back into a single column \(T\). The neural-net then _oscillates_ between these folded-unfolded phases during learning iterations.

### Computational Principle: Location-aware columns and their collapse

**The unfolding-process** shall now be concerned with generating location-aware column \(T_{ij}\) from \(T\). We generate \(h*w\) 2-D _non-parametric_ positional encoding similar to the ones being used in transformers  and neural fields . The trigger column \(T_{ij}\) is then given by \(T_{ij}=(T|p_{ij})\), where \(|\) denotes the stacking operator and \((1,1)(i,j)(H,W)\). T can be said to encode _identity_ of an image.

**The folding-process** involves collapsing all columns \(T_{ij}\) back into \(T\) from which they had begun. This is achievable since the \(p_{ij}\) used in \(T_{ij}\) was deterministic. An abstract-mathematical intuition on folding is that _all_\(p_{ij}\) in \(T_{ij}\) get deleted/annihilated at the end of every forward-pass, and only \(T\) is left. Positional-encodings contain periodic-sinusoids which offers a strong positional-prior in practice . Gradients collected from all \(T_{ij}\) then update the parameters of the convolutional filter in the column \(T\). This sharing of T across different locations \((i,j)\) now induces a new fundamental behavior  already hypothesized in GLOM (more details in section 5.3).

This representation \(T_{ij}=(T|p_{ij})\) exhibits a strong-symmetry breaking behavior 4. For e.g., consider different locations across _same_ image \(I\). Although the identity T will be the same, the positional encoding \(p_{ij}\) shall _differentiate_ among different locations. The converse holds true: given _the same_ position \(p_{ij}\) in two _different_ images, the column \(T\) shall _differentiate_ among identities of different images.

All the experiments in this paper are performed _without-injecting_ the local patch \(I_{xy}\) in the trigger column \(T_{ij}\). This showcases the strong-symmetry breaking behaviour of positional-encodings: they can disentangle location-information from _global \(I\) without-requiring_ an additional patch-prior.

### Firing location-aware columns into a shared MLP

Each column \(T_{ij}\) is passed through an MLP to yield location-specific features \(f_{ij}\) and RGB values \(RGB_{ij}\). Number of neurons in the first layer of the MLP is same as dimensionality of column \(T_{ij}\).

**Column Independence:** The MLP is shared across all the columns. One column is also independent of another as illustrated in Fig1. Therefore, the MLP can be queried _sequentially_. Firing a column \(T_{ij}\) into the MLP yields a column vector \(v_{c}\). Once \(h w\) columns are finished firing, we get a feature grid \(f\) of dimensions \(h w c\). Note that the number of columns can be as low as 1.

### Training and Losses

We detail how the \(t^{th}\) iteration of test-time-training could be performed. First, the obtained feature grid \(f^{h w c}\) from APM is averaged to yield \(f_{avg}^{c}\). For the first TTT iteration, i.e. \(t=1\), the image \(I\) is feed-forwarded through a multi-modal teacher like CLIP to get a CLS token \(f_{cls}\) and corresponding text representation \(t_{cls}\). APM then learns to estimate this _same_ target feature \(f_{cls}\). We enforce this by a simple \(L_{2}\) constraint as:

\[L_{i}=L_{2}(f_{avg,t},f_{cls}) \]

where \(f_{avg,t}\) is the averaged output feature from APM at a particular TTT iteration t. Note that **the target \(f_{cls}\) is only estimated in \(t=1\) and remains the same for \(t 2\)**, i.e. subsequent feed-forward through teacher is _not_ needed.

**Memory-efficient estimation of \(f_{avg,t}\)**: During a TTT iteration \(t\), \(f_{avg}\) is computed as a simple average of \(f^{h w c}\). This would require \(h w\) columns to exist in the memory _simultaneously_. APM's design assumes column independence which allows estimating \(f_{avg}\) as a statistical running average, i.e.

\[f_{avg}=+f_{i,j}}{n+1} \]

assuming, \(n\) columns have already been fired into the APM and one additional column corresponding to position \((i,j)\) of image \(I\) is in the process of firing. This procedure is repeated until all positions \((i,j)\) are exhausted. We represent the sequential column-firing by a 'Gather-Grid' operator in Fig1.

**Predicting image class-label:** After certain TTT iterations, (say t = 20), the output feature \(f_{avg,t}\) and the textual features \(t_{cls}\) are obtained. Image-classification then follows the standard practice of comparing the distance of \(f_{avg,t}\) with each plausible class feature \(t_{cls}\) in the contrastive space and choosing the closest one as the prediction .

## 4 Experimenting with APM

We quantitatively evaluate APM on _zero-shot_ test-time-training on popular benchmarks containing distribution-shifts [15; 86; 84]. Next, we quantitatively explore its computational efficiency.

**Datasets:** Cifar-10C  contains \(5\) level of corruptions on the test-set with \(15\) types of noises. _Larger_ datasets with significant distribution shifts consists of ImageNet val/other curated splits. For e.g., **ImageNet-V2** contains natural images consisting of \(10k\) images across \(1000\) classes. **ImageNet-A** contains \(7500\) adversarial-images consisting of \(200\) categories. **ImageNet-R** consists of \(30000\)artistic-images across \(200\) ImageNet categories. **ImageNet-Sketch** consists of black/white sketches of \(50000\) images for \(1000\) classes. **ImageNet-C** consisting of \(15\) types of corruptions with \(5\) levels of severity. There are additional \(9\) cross-dataset generalization datasets .

**Baselines:** We compare against standard TTT-online , TTT-MAE , TPT , CLIP VIT-B/16, Coop, CocoOP. We also benchmark CLIP VIT-L/14 and the strongest OpenCLIP VIT-H/14-quickgelu variant pre-trained on _dfn5b_.

**Results and Analysis:** We study APM's performance on test-time-training on several datasets. APM processes each test sample individually: i.e. the weights are drawn from a normal distribution after processing every sample to prevent information leakage. For zero-shot classification of a test-sample, APM leverages the \(80\) textual-prompt templates similar to the ones used in CLIP. In Tab 1, APM scales up to _zero-shot_ classification task to datasets with \(1000\) classes. Using CLIP-ViT B/16 as a teacher, we surpass TPT  with an avg score of \(62.6\) and avg good-score of \(61.2\). Next, we benchmark OpenCLIP-VITH/14 against all these splits. Using the same model as our teacher, we get an absolute improvement of \(3\%[84.6\%]\) on ImageNet val set, \(5.1\%[84.2\%]\) on ImageNet-A, \(3.2\%[83.9\%]\) on ImageNet-V2, \(2\%[94.9\%]\) on ImageNet-R, \(4.3\%[77.1\%]\) on ImageNet-Sketch respectively. A similar trend is observed with a VIT-L/14 backbone. This _might_ lead to the conclusion that a stronger teacher seems to benefit APM. In Table 2, we show results on Imagenet-C. Using a CLIP VIT-L/14 teacher, APM gets highest accuracy on \(11/15\) noises with the highest average score of \(50.3\). Note that TTT-MAE also uses a VIT-L encoder, and is \(pretrained\). In contrast, \(APM\)_doesn't_ need any dataset specific-pretraining. Finally, in Tab 3, APM improves upon \(4/9\) datasets, comes close on remaining \(5\) and gets the highest average accuracy score of \(65.5\).

**APM is computationally efficient:** All experiments are run on a _same_ desktop-workstation containing 1x rtx a6000/96GB ram/Ubuntu 22.04/2TB ssd. Flops were counted with meta's fvcore library . In Tab4, we perform 20 TTT iterations. TTT on CLIP-VIT B/16 baseline used in  consumes 462Gflops. Next, we do TTT leveraging APM. At \(t=1\), feed-forwards involves clip-teacher and consumes \(20.5\) flops. Remaining \(19\) TTT iterations only involve overfitting on distilled image token at \(t=1\), and consumes \(10\) flops/TTT-iteration. The entire profile-dump yields \(241.7\) flops, which is an almost \(50\%\) reduction over \(462\) flops. The total memory occupied by APM i.e. \(2.7GB\) is more than CLIP-VIT/B \(2.3GB\) since teacher is kept in memory during TTT. However, APM only occupies \(600MB\) and reduces actual consumed-flops by \(50\%\).

   Method & p & ImageNet & ImageNet-A & ImageNet-V2 & ImageNet-R & ImageNet-Sketch & Average & OOD Average \\  & Top1 acc. \(\) & Top1 acc. \(\) & Top1 acc. \(\) & Top1 acc. \(\) & Top1 acc. \(\) & Top1 acc. \(\) & Average & OOD Average \\  CLIP-ViT-B/16 & ✗ & 66.7 & 47.8 & 60.8 & 73.9 & 46.0 & 59.1 & 57.2 \\ Ensemble & ✗ & 68.3 & 49.8 & 61.8 & **77.6** & 48.2 & 61.2 & 59.4 \\ TPT & ✗ & **68.9** & **54.7** & 63.4 & 77.0 & 47.9 & 62.4 & 60.8 \\ APM (Ours) & ✗ & 68.1 & 52.1 & **67.2** & 76.5 & **49.3** & **62.6** & **61.2** \\  Coop & ✓ & 71.5 & 49.7 & 64.2 & 75.2 & 47.9 & 61.7 & 59.2 \\ CooOp & ✓ & 71.0 & 50.6 & 64.0 & 76.1 & 48.7 & 62.1 & 59.9 \\ TPT + CoOp & ✓ & 73.6 & 57.9 & 66.8 & 77.2 & 49.2 & 64.9 & 62.8 \\ TPT + CoCoOp & ✓ & 71.0 & 58.4 & 64.8 & 78.6 & 48.4 & 64.3 & 62.6 \\  CLIP VIT-L/14 & ✗ & 76.2 & 69.6 & 72.1 & 85.9 & 58.8 & 72.5 & 71.6 \\ APM (Ours) & ✗ & **77.3** & **71.8** & **72.8** & **87.1** & **62.2** & **74.2** & **73.4** \\  OpenCLIP-ViT-H/14 & ✗ & 81.6 & 79.1 & 80.7 & 92.9 & 72.8 & 81.4 & 81.3 \\ APM (Ours) & ✗ & **84.6** & **84.2** & **83.9** & **94.9** & **77.1** & **84.9** & **85.0** \\   

Table 1: **APM’s Robustness to Natural Distribution Shifts**. CoOp and CoCoOp are tuned on ImageNet using 16-shot training data per category. Baseline CLIP, prompt ensemble, TPT and our APM do not require training data. A ✓ in P means that method leveraged **pre-trained weights** on clean variant of train set aka, Image-net and downstream-ttt on corrupted version.

    & P & high & cont & detec & elas & tog & frost & gauss & glass & impul & jeps & memo & pixel & shot & snow & zoom & Average \\  Joint Train & ✓ & 62.3 & 4.5 & 26.7 & 39.9 & 25.7 & 30.0 & 5.8 & 16.3 & 5.8 & 45.3 & 30.9 & 45.9 & 7.1 & 25.1 & 31.8 & 24.8 \\ Fine-Tune & ✓ & 67.5 & 7.8 & 33.9 & 32.4 & 36.4 & 38.2 & 22.0 & 15.7 & 23.9 & 51.2 & 37.4 & 51.9 & 23.7 & 37.6 & 37.1 & 33.7 \\ VIT Pube & ✓ & 68.3 & 6.4 & 24.2 & 31.6 & 36.4 & 38.4 & 17.4 & 18.4 & 18.2 & 51.2 & 32.2 & 49.7 & 18.2 & 35.9 & 32.2 & 29.2 \\ TTT-MAE & ✓ & 69.1 & 9.8 & 34.0 & 37.4 & 30.7 & 30.5 & 36.9 & 32.4 & 63.0 & 41.9 & 63.0 & 32.0 & 42.8 & 45.9 & 44.4 \\  OpenCLIP VIT-L/14 & ✗ & 71.9 & 47.0 & 50.3 & 32.7 & 58.3 & 46.9 & 26.0 & 26.5 & 28.1 & 62.7 & 37.7 & 58.3 & 28.2 & 50.4 & 37.9 & 42.1 \\ APM (Ours) & ✗ & **77.4** & **51.9** & **56.6** & **37.9** & **64.8** & **53.2** & **28.7** & **31.4** & **33.0** & **68.4** & **44.1** & **64.5** & **33.1** & **56.9** & **43.9** & **50.3** \\   

Table 2: **APM’s performance on ImageNet-C, level 5**. The first three rows are fixed models without test-time training. The third row, ViT probing, is the baseline used in . A ✓ in P means that method leveraged **pre-trained weights** on clean variant of train set aka, Image-net and downstream-ttt on corrupted version. CLIP VIT-L/14 is generally more robust. APM does better on \(11/15\) noises with an average accuracy score of \(50.3\).

**APM does patch-based processing:** In Fig 2, we estimate GFlops/time to process a \(224 224\) image using CLIP VIT-B/16 & APM. CLIP VIT-B/16 consumes 20.5 Gflops. On the other hand, APM takes only 10 Gflops in total, in part due to lesser parameters. APM's effectiveness comes from processing as low as 7 patches at the same time: it occupies lesser memory but takes more time (i.e. \(1.5\) seconds). \(1.5sec\) is still lower than VIT-B/16's \(2.2sec\). The extreme lies when all patches are processed: Inference time in APM goes down to \(0.002\) seconds compared to VIT-B/16's \(2.2\) seconds, thereby indicating its effectiveness. Note that VIT-B/16 can't do this patch-based processing.

APM's computational effectiveness stems from this _unique_ ability to overfit on a _single_ test sample's embedding which was distilled only at \(t=1\). This merits a deeper investigation.

    &  &  &  &  &  &  &  &  &  &  &  \\  CoOp & \(\) & 68.7 & 41.9 & 89.1 & 66.5 & 93.7 & 85.3 & 64.2 & 18.5 & 46.4 & 63.9 \\ CoCoOp & \(\) & 70.9 & 45.5 & 90.5 & 68.4 & 93.8 & 84.0 & 66.9 & 22.3 & 39.2 & **64.6** \\  CLIP-VIT-B/16 & ✗ & 67.4 & 44.3 & **88.3** & 65.1 & 93.4 & 83.7 & 62.6 & 23.7 & 42.0 & 63.6 \\ Ensemble & ✗ & 67.0 & 45.0 & 86.9 & 65.2 & 93.6 & 82.9 & 65.6 & 23.2 & 50.4 & 64.6 \\ TPT & ✗ & **69.0** & 47.8 & 87.8 & 68.0 & **94.2** & **84.7** & 65.5 & 24.8 & 42.4 & 65.1 \\ APM (Ours) & ✗ & 62.0 & **48.9** & 81.6 & **72.6** & 89.6 & 84.2 & **65.7** & **29.7** & **55.7** & **65.5** \\   

Table 3: **Cross-dataset generalization** from ImageNet to fine-grained classification datasets. CoOp and CoCoOp are tuned on ImageNet using 16-shot training data per category. Baseline CLIP, prompt ensemble, TPT and APM do not require training data or annotations. We report top-1 accuracy.

Figure 3: **Overfitting on a _single_ distilled token representation leads to islands of agreement:** APM is overfit on a test-sample’s representation distilled from a teacher. We plot t-sne clustering of output features over 250ttt iterations. \(L_{2}\) loss between predicted features and distilled sample falls from 1e-3 to 1e-12. Moving left to right shows that wholes break into smaller parts.

**APM can learn on a single sample:** In Fig 3, we feed-forward a single image through our APM and perform \(250\) TTT iterations5. We only _overfit_ on one distilled test-sample embedding and plot the 2D

    & \(t\) & Params(M)\(\) & \(M_{}(GB)\) & \(M_{i}(GB)\) & \(GFlops_{}\) & \(GFlops_{i}\) \\  Swin & 1-20 & 87 & 1.5 & 1.4 & 353 & 308 \\  TPT & 1-20 & 151.3 & 3.1 & 2.7 & 529 & 476 \\  CLIP VIT-B/16 & 1-20 & 149.2 & 2.3 & 1.8 & 462 & 410 \\  CLIP VIT-B/16(u) & 1 & 149.2 & 1.8 & 1.8 & 20.5 & 20.5 \\ APMs & 1 & 174.2(s+u) & 2.7 (s+u) & 1.6(u) + 0.6(s) & 20.5(u) & 20.5(u) \\ APM(s) & 2 & 174.2(s+u) & 2.7 (s+u) & 1.8(u) + 0.6(s) & 10(s) & 10(s) \\ Peak Use & 1-20 & 174.2(s+u) & 2.7 (s+u) & 1.8(u) + 0.6(s) & **241.7 (s+u)** & **210.5 (s+u)** \\   

Table 4: **APM’s computational analysis**: TTT for 20 iterations on APM. Baseline is CLIP VIT-B/16 which is used as a teacher in . \(M_{}(GB)\), \(GFlops_{}(GB)\) are tmeasured stats. \(M_{i}(GB)\), \(GFlops_{i}(GB)\) are idealistic estimates. (t): \(t^{th}\) ttt iteration, (s): student, (u): teacher, (s+u): portion of memory/flops consumed by student/teacher respectively. Note that APM is a \(25M\) net.

Figure 2: **APM’s analysis with variable number of patches:** (left) Gflops of CLIP VIT-B/16 and APM as a function of number of processed patches. (right) Feed-forward time vs number of patches.

t-sne clustering of predicted APM. It can be seen that the elements of the scene have gradually started to cluster. This suggests that APM solves an _inverse_ problem: given a single test-sample embedding, what features in the image led to its formation? Over several TTT iterations, APM's features become representative enough to explain different parts of a scene. Therefore, the same-net _could be made to move_ up-down the part-whole hierarchy, although it requires TTT-iterations _for now_.

Next, we explore if data augmentation _improves_ APM's performance. We perform TTT iterations on CIFAR-10's test set and find that data augmentation drops APM's performance from \(98.6\) to \(76.7\). This quantitatively demonstrates that APM works best when it does _one-sample_ overfitting. It now aligns with the earlier qualitative experiment: over TTT-iterations, the network is learning to cluster the elements in the scene3. Data augmentation _distorts_ the sample and makes it difficult for APM's predicted features to _agree_ on a stable, _relaxed_-representation that explains the scene

Till now, APM's operation has involved random-initialization of weights for _every_ test-sample and performing test-time-training. There is another mode that it can be made to operate in.

## 5 APM Training (Qualitative Analysis)

APM can also scale up and do learning on a _batch_ of samples (for e.g., COCO images) distilled from a teacher. This requires introducing several new mechanisms. Note that this section is meant to _qualitatively_ demonstrate how scaling up APM can improve the net's interpretability, and help seed future research. Quantitative experiments beyond test-time-training remain _out of the scope_ of this paper. APM's training follows a standard setup in self-supervised-learning. We have provided the full algorithm for SSL-training/test-time-training in the AppendixC. During inference, APM takes any 2D image \(x_{k}\) and predicts its RGB reconstruction \(RGBk\)/higher dimensional features \(f_{k}\). The net then begins to demonstrate several interesting properties, which we will discuss next.

### APM can do RGB reconstruction for any 2D input.

Given a sample \(x_{k}\), APM can reconstruct its RGB. In Fig4, we achieve this by estimating \(f_{rgb,ij}=(T_{ij}|f_{ij})\). This skip-connection from trigger column \(T_{ij}\) to output feature has a _subtle_ reason: consider a _white_ dog and _brown_ dog. The predicted object-level feature for both will be almost identical. However, \(T_{ij}\) is different for both since it contains lower patch-level features. Therefore, this helps us break symmetry. Without this skip-connection, the net fails to predict RGB. The network is trained to reconstruct RGB for a batch of images, \(L_{rgb}=_{i=1}^{N}_{j=1}^{h w}L_{2}(p^{}_{j},p_{j})\), where \(p_{j}\)/\(p^{}_{j}\) are ground truth/predicted RGB-logits respectively.

### APM is asynchronous yet encodes semantic-awareness in the net.

Given a sample \(x_{k}\), APM can directly _learn_ to mimic the entire _last layer_ feature-grid which a teacher model would have generated. We enforce this by a \(L_{grid}=_{i=1}^{N}_{j=1}^{h w}L_{2}(f^{}_{j},f_{grid})\). In Fig 5(ii) we estimate the error map between the features predicted by our APM and Dinov2. The error map is mostly black which shows that it closely approximates Dinov2's grid6. Note that APM does patch-based _asynchronous_ processing whereas DINOv2 relies on parallel perception. Finally, Fig 5(iii) shows a simple feed-forward of a CIFAR-10 sample through APM. We can see semantically-aware features. Note that this is a _single_ feed-forward through APM. Predicting output feature grid allows the net to encode dark knowledge, i.e. the knowledge of both correct

Figure 4: **RGB Decoding in APM: Input trigger column \(T_{ij}\) is concatenated with predicted feature \(f_{ij}\) and fed to downstream RGB head. This decodes RGB logit at location (i,j) for _any_ 2D input \(x_{k}\). (ii) Input \(x_{k}\) sampled from Coco-val set. \(RGB_{out}\): reconstructed RGB, \(f_{out}\): Predicted feature grid.**

### APM is a step towards validating GLOM's insight: input percept is a field.

In Fig 6, we show that APM can interpolate between any two images in the wild. We choose two images \(I_{1}\) and \(I_{2}\). These images are then funneled through the trigger column \(T\) and yield two vectors \(v_{1}\) and, \(v_{2}\) respectively. Next, we generate a intermediate latents separated by an equal linear distance by \(v_{j}=v_{1}+-v_{1}}{n}\). Each latent then brings into existence its own set of location-aware columns and decodes an image from the MLP. Such an interpolation has been previously observed in other models . APM now functions as a _new form of addressing mechanism_: the trigger column T acts a key. _Copying_ T across locations yields image-specific queries. Values are synapses triggered in the MLP. RGB decoding happens in the output head. Hence, such _continuous_ keys and queries exist _outside_ the net.

Classically, auto-regression has unrolled a shared-decoder over time. In contrast, APM holds the whole sequence \(I\) in \(T\), and directly hops onto a space/time-step by querying the MLP with a location-column \(T_{ij}\). Note that \(T_{ij}\) is generated by unfolding \(T\). Recurrence/feedback-loops are compensated for by a form of feature-expression. This is a step towards validating GLOM's insight, i.e. input-percept is a field and one can now interpolate in it (gestalt psychology). Furthermore, the trigger column \(T\) resides in a continuous embedding space, and not discrete hardware locations(classical AI). Therefore, APM tries to integrate insights from both fields.

## 6 Ablations on APM

The experiments on TTT had relied on a curious ability of APM: it could simply overfit on a test-sample's distilled representation at \(t=1\). This merits further investigation:

**Effect of one-sample test-time training:** In Table 4(a), we investigate whether existing networks are capable of one-sample test-time-training. We employ a randomly-initialized network to overfit on a distilled test sample's token obtained in the first TTT iteration . Standard MLP achieves low accuracies of \(9.0\) and \(3.8\) on CIFAR-10 and CIFAR-100, respectively. Notably, an 11.4M parameter ResNet18 outperforms the larger ResNet34 with 21.5M parameters. A reason might be that too many parameters in ResNet34 gives it too many degrees of freedom: it finds it hard to overfit on one

Figure 5: **APM feature Analysis:** (i) TTT iterations on an input image leads to semantically aware clustering. top: 2D t-sNE. bottom: 3D t-sNE. [64; 29]. (ii) APM is trained via self-supervision using DINOv2-Teacher. (from left) Input, Dinov2 grid, APM grid. APM’s grid **closely approximates** Dinov2 grid evident from black regions in error map. Note that APM does asynchronous patch-based processing whereas Dinov2 does parallel perception. (iii) Cifar-10 samples feed-forwarded through SSL-trained APM yields features of significant semantic quality.

Figure 6: **APM is a step towards validating GLOM’s insight **: input percept is a field. An interpolation between _any_ two images in the wild. This field arises in APM’s MLP consisting of _5 layers_. Trigger column \(T\) acts as a key which retrieves an image from the APM’s memory. T resides in a continuous embedding space, not discrete addressing space.

sample. Our APM demonstrates strong performance on both CIFAR-10 and CIFAR-100, surpassing the CLIP VIT-L/14 baseline.

**Effect of various losses on APM:** We analyze each row in Table 4(b). Initially, only the CLS token from the teacher was distilled into our network, resulting in a yield of \(94.2\%\). When we added RGB reconstruction loss for input, accuracy dropped to \(91.0\), attributed to the difficulty of breaking RGB symmetry . Subsequently, mimicking the entire feature grid and CLS token from the teacher increased accuracy to \(96.1\). Adding both \(L_{rgb}\) and \(L_{grid}\) further improved performance to \(96.5\). \(L_{grid}\) here refers to the last-layer of the teacher. Notably, while simple RGB reduction decreased performance (\(91.0\)), a combination of \(L_{grid}\) and \(L_{rgb}\) enhanced our network, as lower RGB and higher object features complement each other . Injecting local patch information \(I_{xy}\) into the trigger column improved performance to \(96.8\). Finally, routing output tokens from a single VIT layer into the trigger column \(T_{vit}\) strengthened the column and improved performance.

**Effect of increasing number of convolution in T:** Increasing number of convolutional kernels from \(1\) to \(3\) improves from \(96.08\) to \(97.67\).

These ablations reveal: 1) APM can do one-sample overfitting for a test sample, 2) It helps to have both local patch and a strong image representation in the trigger column T, and 3) Increasing the levels of part-whole supervision strengthens the net.

## 7 Related Work

**Prompting Approaches:** Prompting is a mechanism to adapt a foundational-model to a downstream task in a zero-shot manner. However, prompting typically requires well-designed hand-crafted prompts. Prompt-tuning methods consist of learnable prompts which enable a parameter-efficient approach to fine-tune a foundational-model. CoOp applied prompt-tuning to CLIP. However, CoOp is sensitive to OOD-data, which CoCoOp in turn compensates for by conditioning the prompts on model inputs. Similarly, TPT optimizes a prompt to encourage consistent predictions across multiple augmented views of the same test sample, and uses confidence-selection to filter out noisy-predictions. Note that TPT performs prompt-tuning over ViT-B/16 but requires feed-forward through _all the model-layers_ for _every_ iteration.

**Test-Time-Optimization:** Introduces the notion where a model adjusts its decision boundary dynamically during testing, for eg, improving robustness to distribution shifts. Test-time training (TTT) generally adds a self-supervised multi-task branch, and performs a SSL-task like rotation, or masked-reconstruction to adapt the network to the test-sample. These approaches typically initialize the net with pre-trained weights, for eg, Imagenet before undergoing ttt-iterations on a downstream corrupted-dataset. An alternate line of work, for eg, TENT proposes to minimize entropy of batch-wise test-samples. However, TENT requires more than one test-sample to converge towards an optimal solution, whereas APM can also operate on one test sample. Another line of work adjusts internal batch-norm-stats of a network. However, this makes the network-architecture inflexible and requires more than one test sample for optimization. Several other papers following the original pioneering-TTT paper have worked on different problem formulations, for eg, assuming access to an entire dataset (e.g. ) or a batch (e.g. TENT ) of test inputs.

In contrast, APM evaluates on a each test-sample independently. Inductively, APM does not require any dataset specific pre-training, a pretext task or prompt tuning. Some approaches also use higher-parameterized transformer/diffusion models making the optimization compute-intensive. However in APM, for TTT iteration \(t>1\), feed-forward is done through only 25M parameter APM and not the 149.2M Clip VIT-B/16, resulting in computational efficient test-time-training(Fig 2).

  &  &  &  \\  Zeroshot &  &  &  &  \\ CLIP VIT-L/14 & 428M & 95.37 & 73.28 \\  MLP & 21M & 9.0 & 3.8 \\ ResNet18 & 11.4M & 85.69 & 21.77 \\ ResNet34 & 21.5M & 78.24 & 12.89 \\  APM & 25M & **97.04** & **77.98** \\  
  & \)} & \)} & \)} & \)} & Acc \\  \(T_{c}\) & ✗ & ✓ & ✗ & ✗ & 94.2 \\ \(T_{c}\) & ✗ & ✓ & ✓ & ✗ & 91.0 \\ \(T_{c}\) & ✓ & ✓ & ✗ & ✗ & 96.1 \\ \(T_{c}\) & ✓ & ✓ & ✓ & ✗ & 96.5 \\ \(T_{c}\) & ✓ & ✓ & ✓ & ✓ & **96.8** \\  \(T_{vit}\) & ✓ & ✓ &APM also inherits zero-shot behaviour from CLIP, which allows it to bypass training a separate dataset-specific linear-probe for downstream TTT. Finally, some works in areas like source-free domain-adaptation do perform TTT on smaller datasets like Cifar-10 etc. APM additionally shows results on Imagenet splits (Tab1,2) and various cross-generalization datasets(Tab3).

**Part-whole hierarchies:** The idea of encoding part-whole dynamic-parse-trees as distributed representations in neural nets can be traced back to , with recent attempts leading to capsule networks. However, the EM/attention-based routing forces _each_ capsule to represent only one part. This fundamental flaw prevents capsules from scaling-up and generalizing to multiple OOD objects. Recently, GLOM proposed a _theoretical_ system of representing each input pixel as containing a column-vector. These vectors then undergo a routing procedure such that pixels corresponding to same part come to 'agree' with each other. demonstrated GLOM on Cifar splits. APM does not need any routing because it processes location-aware columns independently. APM now reveals an _additional_ perceptual-interpolation property not shown earlier(Fig6).

**Knowledge Distillation:** There has been a long history of training data-specific mixture/product of experts and having their ensemble vote towards a prediction, with the key idea to make the experts as different from as each other as possible, and only one expert deciding on the predictions of a particular input sample. This is better than having all experts give equal opinion on a sample. Other methods attempted to 'gather' their collective knowledge to a single model for edge-device deployment. Recently, knowledge has been transferred from a larger teacher model to smaller ones, for eg, a foundational model. The student often retains/becomes-better than its teacher. In practice, the weights of teachers are fluctuated slowly (EMA) as compared to the student. This mechanism then helps realize Kahnman's theory of slow-fast thinking.

Typically, distillation requires boltzmann-matching predicted distribution between students and teachers, with the distribution's sharpness being governed by a temperature parameter. In contrast, APM directly mimics the entire last layer feature grid of a teacher via \(L_{2}\) norm. Furthermore, APM possesses a novel-inductive bias that _can recover_ semantic-features from a single CLS-token distilled from a teacher[Fig 5]. This validates the intuition that CLS tokens encode useful geometric-information of a scene _after_ cross-attention of CLS token with patch tokens of an input image.

**Routing Mechanisms**: Routing mechanisms involve routing correct object-specific information to correct neurons.  proposed slots which perform binding by iterative rounds of self-attention. Lowe et Al proposed 2D complex autoencoder and rotating features where presence of an object is encoded in phase of a neuron and follows the minimum description length principle. In APM, binding is done via the location itself. GroupVIT  routes information to multiple group tokens and shows that semantic segmentation emerges with just image-text contrastive supervision.

## 8 Conclusion

We propose APM, inspired from the insights presented in GLOM. APM _promises_ to be an efficient architecture for test-time-training and asynchronous patch-processing. APM shows robustness to extreme distribution shifts. APM demonstrates that MLP's can be made to semantically cluster a given image. We hope that APM will help _inspire_ further research on _simpler_ weight-sharing, lower-memory, higher-bandwidth efficient-nets .

**Limitations:** In this work, we have mainly-focused on image-classification. Furthermore, APM requires multiple TTT iterations for now, although they might be reduced by exploring pre-training on a source-dataset. APM can work on just 1 sample with randomly-initialized weights. However, it still requires a single CLS token which has been distilled from a teacher pre-trained on a large-scale dataset. Our preliminary-experiments have revealed that APM can still do RGB-reconstruction _without_ a teacher, showing potential that APM can be self-sufficient and independent.

## 9 Acknowledgements

This research is supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DOI/IBC) contract number 140D0423C0074. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DOI/IBC, or the U.S. Government. We also thank many other people and MLCollective whose support made this work possible.