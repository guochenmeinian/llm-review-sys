# Geometric-Averaged Preference Optimization

for Soft Preference Labels

 Hiroki Furuta\({}^{1,2}\)1  Kuang-Huei Lee\({}^{1}\)  Shixiang Shane Gu\({}^{1}\)  Yutaka Matsuo\({}^{2}\)

**Aleksandra Faust\({}^{1}\)  Heiga Zen\({}^{1}\)  Izzeddin Gur\({}^{1}\) \({}^{1}\)**

\({}^{1}\)Google DeepMind \({}^{2}\)The University of Tokyo

furuta@weblab.t.u-tokyo.ac.jp

###### Abstract

Many algorithms for aligning LLMs with human preferences assume that human preferences are binary and deterministic. However, human preferences can vary across individuals, and therefore should be represented distributionally. In this work, we introduce the distributional _soft preference labels_ and improve Direct Preference Optimization (DPO) with a weighted geometric average of the LLM output likelihood in the loss function. This approach adjusts the scale of learning loss based on the soft labels such that the loss would approach zero when the responses are closer to equally preferred. This simple modification can be easily applied to any DPO-based methods and mitigate over-optimization and objective mismatch, which prior works suffer from. Our experiments simulate the soft preference labels with AI feedback from LLMs and demonstrate that geometric averaging consistently improves performance on standard benchmarks for alignment research. In particular, we observe more preferable responses than binary labels and significant improvements where modestly-confident labels are in the majority.

## 1 Introduction

Large Language Models (LLMs)  capture a wide range of behaviors and values from training data. However, we would usually prefer these models to focus on useful and safe expressions and abide by social norms. To solve these problems, preference optimization approaches have been popular, either way through reinforcement learning from feedback (RLHF)  or direct preference optimization (DPO) methods . These methods usually finetune supervised models on preference data and labels generated by human raters with a wide variety of priorities, backgrounds, knowledge, and skill sets. Nevertheless, existing RLHF and direct preference optimization methods usually assume binary preferences, which ignore the subtle relationship and amplify the bias in the preference labels.

To address this issue, we introduce the concept of distributional _soft preference labels_ and improve DPO and its algorithmic families by incorporating a weighted geometric average of LLM output likelihood into the loss function. This approach adjusts the scale of learning loss based on the soft labels and effectively minimizes the loss when presented with equally preferred responses.

In the experiments, we simulate the soft preference labels with AI feedback from LLMs  and show that soft preference labels and weighted geometric averaging achieve consistent improvement to the baselines on popular benchmarks for the alignment research literature, such as Reddit TL;DR , and Anthropic Helpful and Harmless , as well as original natural language planning dataset based on Plasma . In particular, our results highlight that the proposed methods significantly improve the performance with the data dominated by modestly-confident labels, while conservative DPO(cDPO) , a method leveraging soft labels via linear interpolation of objectives, is stuck to suboptimal performances there. When the models are trained with rich modestly-confident labels, the responses are preferable to those from the models trained with binary labels biased to high-confidence regions. The performance on preference label classification also reveals that cDPO struggles with objective mismatch between the text generation and preference modeling and the weighted geometric averaging could successfully balance both.

Our primary contributions are:

* We introduce _soft preference labels_, which can reflect the distributional preference and the fine-grained relationship between the response pairs (Section 2.1). Soft preference labels contribute to mitigating over-optimization issues (Section 5.3) and aligning the models to more preferable responses than binary labels (Section 5.1).
* We propose the weighted geometric averaging of the output likelihood in the loss function. This can be applied to a family of any algorithms derived from DPO (Section 3).
* We point out the objective mismatch between text generation and preference modeling. The better preference accuracy from DPO-style objectives does not ensure better alignment, which conservative DPO suffers from and our geometric averaging can resolve (Section 5.2).

## 2 Preliminaries

We denote \(x\) as a text prompt from the set of prompts \(\), \(y\) as an answer corresponding to the prompts from the set of possible candidates \(\), and \((y x)\) as a LLM (i.e. policy). We use \(y_{1} y_{2}\) to indicate that \(y_{1}\) is more preferable than \(y_{2}\), and denote a dataset of the paired preference as \(=\{(x^{(n)},y_{1}^{(n)},y_{2}^{(n)})\}_{n=1}^{N}\). We assume that \(y_{1} y_{2}\) always holds (\(y_{1}\) is always preferred or equal) in this paper unless specified otherwise.

In the RLHF pipeline, we typically go through three phases, such as supervised finetuning (SFT), reward model training, and RL-finetuning [34; 69]. The SFT phase is maximum likelihood training of pre-trained LLMs on downstream tasks, which results in an initial model or reference model \(_{}\) for the later RL-finetuning. For the reward modeling, the Bradley-Terry model  is often assumed as underlying modeling for the oracle human preference such as

\[p^{*}(y_{1} y_{2} x)=(x,y_{1}))}{(r^{*}(x,y_{1}))+ (r^{*}(x,y_{2}))}=(r^{*}(x,y_{1})-r^{*}(x,y_{2})), \]

where \(r^{*}(x,y)\) is a true reward function and \(()\) is a sigmoid function. Following this assumption, the parameterized reward function \(r_{}\) is initialized with a supervisedly-finetuned LLM \(_{}\) and trained with negative log-likelihood loss: \(_{}-[(r_{}(x,y_{1})-r_{}(x,y_{2}))]\). RL-finetuning phase leverages the learned reward to update the LLM \(_{}\) by optimizing the following objective [34; 69],

\[_{}\ _{x,y_{}(y x)}[r_ {}(x,y)]- D_{}(_{}(y x)_{ }(y x)), \]

where \(>0\) is a coefficient to control the KL-divergence regularization. Online RL approaches, such as PPO , are often used to maximize Equation 2, but they are usually computational inefficiency and require a complex pipeline in practice. In contrast, offline preference optimization approaches, such as DPO , are relatively simpler and lightweight in terms of implementation.

### Soft Preference Labels

While a reward model is often trained with binary preferences, we can usually assume distributional _soft_ feedback via majority voting among the human raters or AI feedback with scoring  (e.g. \(y_{1}\) is better than \(y_{2}\) at a 70% chance). With soft preference labels, we can still easily recover the binary preference with a threshold.

We assume that the binary preference labels, \(l(y_{1} y_{2}|x)=1\), are sampled from the Bradley-Terry model preference distribution with the parameter \(p^{*}(y_{1} y_{2}|x)\). We define soft preference labels as estimates of the true preference probability:

\[_{x,y_{1},y_{2}}:=(y_{1} y_{2}|x) p^{*}(y_{1} y _{2}|x). \]

We denote \(_{x,y_{1},y_{2}}\) as \([0.5,1.0]\) for simplicity in the later sections. For instance, we can estimate this via Monte Carlo sampling such as \(=_{i=1}^{M}l_{i}\) where \(l_{i}\{0,1\}\) is a sampled binary label,which is done via majority voting among \(M\) people in practice. Because soft preference labels reflect fine-grained relationships between the responses, they may contribute to aligning the models to more preferable responses than binary labels. Alternatively, we can also estimate the soft preference directly via Bradley-Terry models with some reward function. This direct estimation is often adopted in AI feedback with scoring (see Section 4.1 for further details) or the cases with multiple reward models.

The sampled binary preference may sometimes flip with probability \(\) (i.e. label noise [11; 27; 30]). If the degree of label noise is known, we may consider the expectation over the noise such as: \(=(1-_{i=1}^{M}_{i})_{i=1}^{M}l_{i }+_{i=1}^{M}_{i}_{i=1}^{M}(1-l_{i})\), or we may ignore the noise when \(_{i}\) is small and \(M\) is sufficiently large.

### Direct Preference Optimization and Related Methods

Let's start with a brief review of DPO and the variants derived from it, such as conservative DPO, IPO, and ROPO. DPO maximizes the estimate of preference probability under the Bradley-Terry model, \(p_{}(y_{1} y_{2} x)=(r_{}(x,y_{1})-r_{}(x,y_ {2}))\), by parameterizing reward models with the policy model \(_{}\) itself, which comes from the following relationship in the constraint Lagrangian of RLHF objective (Equation 2),

\[r_{}(x,y)=(y x)}{_{}(y x )}+ Z(x), \]

where \(Z(x)=_{y}_{}(y x)(r(x,y))\) is the partition function. Substituting Equation 4 into \( p_{}(y_{1} y_{2} x)\), the following objective is derived:

\[_{}(_{},_{}) =-_{(x,y_{1},y_{2})}[ (h_{}(x,y_{1},y_{2}))] \] \[=-_{(x,y_{1},y_{2})}[ ((y_{1} x)_{}(y_{2} x)}{ _{}(y_{1} x)_{}(y_{2} x)})].\]

Note that we define the reward difference function as \(h_{}(x,y_{1},y_{2}):=r_{}(x,y_{1})-r_{}(x,y_{2})\).

Conservative Direct Preference OptimizationConservative DPO (cDPO)  is the most representative work that incorporates soft labels. cDPO smooths the objective functions with soft preference labels via linear interpolation, such as

\[_{}(_{},_{}) =-_{(x,y_{1},y_{2},)}[(h_{}(x,y_{1},y_{2}))+(1-)( h_{}(x,y_{2},y_{1}))] \] \[=-_{}[(h_{ }(x,y_{1},y_{2}))+(1-)(1-(h_{}(x,y_{1}, y_{2})))],\]

where the later term is the DPO loss under flipped labels (i.e. \(y_{2} y_{1}\)). Moreover, prior works incorporating an extra reward model \(r_{}\) to DPO objective have also adopted this formulation [9; 21], by replacing \(\) into \((r_{}(x,y_{1})-r_{}(x,y_{2}))\).

Identity Preference OptimizationAssuming the Bradley-Terry model as an underlying preference modeling causes over-optimization issues in DPO [2; 52]. To mitigate this problem, IPO  has been introduced by replacing reward maximization in Equation 2 with preference distribution maximization. The objective of IPO can be written as,

\[_{}(_{},_{})=_{(x,y_{1}, y_{2})}[(h_{}(x,y_{1},y_{2})- )^{2}], \]

where \(>0\) is a regularization hyper-parameter. Similar to cDPO, we can also introduce conservative IPO (cIPO) [2; 27], which results in

\[_{}(_{},_{})=_{(x,y_{1}, y_{2},)}[(h_{}(x,y_{1},y_{2})--1}{2})^{2}]. \]

Robust Preference OptimizationROPO  designs the objective to resolve the instability under noisy label problems, which is inspired by the unhinged loss  and reverse cross-entropy loss  in the noise-tolerant supervised learning literature. The objective is a combination of the regularization term and original DPO loss such as,

\[_{}(_{},_{}) =_{(x,y_{1},y_{2},)}[ (h_{}(x,y_{2},y_{1}))]-_{(x,y_{2}, y_{1})}[(h_{}(x,y_{1},y_{2}))] \] \[=(1-_{(x,y_{1},y_{2},) }[(h_{}(x,y_{1},y_{2}))])+ _{}(_{},_{}),\]

where \(>0\) and \(>0\) are extra hyper-parameters to balance the contribution of each term.

## 3 Methods

As DPO and the related methods assume binary preference, they cannot reflect the fine-grained relationship between the pair of responses during training. The conservative formulation of DPO can use the soft preference labels, but we found that it could not achieve good performance if modestly-confident labels shape the distribution as a majority (see Section 5). In this section, we propose a simple yet effective modification, weighted geometric averaging of LLM output likelihood in the learning loss, which can be applied to a family of algorithms derived from DPO.

### Weighted Geometric Averaging and Practical Algorithms

We assume that the pairs of winner and loser outputs (\(y_{w}\), \(y_{l}\)) are sampled from the weighted geometric average of LLM policies \(( x)\) such as,

\[(y_{w} x) := (x)}(y_{1} x)^{}(y_{2} x)^{ 1-} \] \[(y_{l} x) := (x)}(y_{1} x)^{1-}(y_{2} x) ^{},\]

where \(Z_{,w}(x):=_{y_{j},y_{k},}(y_{j} x)^{}(y_{k} x )^{1-}\) and \(Z_{,l}(x):=_{y_{j},y_{k},}(y_{j} x)^{1-}(y_{k}  x)^{}\) (\(y_{j} y_{k}\)). Because it is difficult to obtain precise estimation of these values with sampling, we set those normalization terms to constant and ignore them in practice, which is a common assumption in deep RL literature [18; 39; 46; 61]. If we have true binary labels (i.e. \(=1\)), Equation 10 reduces to the original formulation under the assumption of \(y_{1} y_{2}\).

Weighted geometric averaging can be considered as a regularization, which pushes the large likelihood down to small when the soft preference is far from 1. In the following, we present three modified DPO-based methods: Geometric DPO (GDPO), Geometric IPO (GIPO), and Geometric ROPO (GROPO), by replacing the winner output likelihood \((y_{1} x)(y_{1} x)^{}(y_{2} x)^{1-}\) and the loser output likelihood \((y_{2} x)(y_{1} x)^{1-}(y_{2} x)^{}\) for both \(_{}\) and \(_{}\):

**Geometric Direct Preference Optimization (GDPO)**

\[_{}(_{},_{}) =-_{}[((y_{1} x)^{}_{}(y_{2} x)^{1-} _{}(y_{1} x)^{1-}_{}(y_{2} x)^{}}{_{}(y_{1} x)^{}_{}(y_{2} x)^{1- }_{}(y_{1} x)^{1-}_{}(y_{2} x)^{}})] \] \[=-_{(x,y_{1},y_{2},)}[ ((2-1)(y_{1} x)_{}(y_{2} x)}{_{}(y_{1} x)_{}(y_{2} x )})],\]

**Geometric Identity Preference Optimization (GIPO)**

\[_{}(_{},_{})=_{(x,y_{1},y_{2},)}[(2-1)^{2}(h_{}(x,y_{1},y_{2})-)^{2}], \]

**Geometric Robust Preference Optimization (GROPO)**

\[_{}(_{},_{})=(1- _{}[((2-1)(y_{1} x)_{}(y_{2} x)}{_{}(y_{1} x )_{}(y_{2} x)})])+_{}(_{},_{}). \]

Figure 1: **(Left) Scaling factors \(w_{}\) in the gradient of each objective (DPO, cDPO, and GDPO), which is a function of \(h(x,y_{1},y_{2})\). Geometric averaging (GDPO) can adjust the scale of gradient based on the soft preference labels; if soft preference labels are close to 1 (\(=0.95\)), the scaling factor of GDPO is almost the same, and small soft labels (\(=0.55\)) make the scaling factor small while the norm reaches zero. (Right) A 1-D bandit problem with 100 actions, illustrating the histogram of train data and true reward function, preference distribution, and action distribution from the learned policies. Although cDPO accurately fits the data distribution and has the mode in a low-reward region, DPO and GDPO can assign a probability mass in a high-reward region.**These objectives are consistent with original ones (Equation 5, 7, 9) when we have binary preferences.

### Geometric Averaging Can Adjust the Scale of Gradients

To analyze the role of weighted geometric averaging, we consider the gradient of loss function with respect to model parameters \(\) in a general form, which can be written as:

\[_{}=-_{(x,y_{1},y_{2},) }[(x,y_{1},y_{2},)}_{}_{}(y_{1} x)-_{ }_{}(y_{2} x)]}_{}], \]

where \(w_{}(x,y_{1},y_{2},)\) is a scaling factor of positive and negative gradients. While defining an estimated preference probability by their own policy LLMs under the Brady-Terry model as:

\[_{}:=((y_{1} x)_{ {ref}}(y_{2} x)}{_{}(y_{1} x)_{}(y_{2} x)} ),\;^{}_{}:=((2-1)(y_{1} x)_{}(y_{2} x)}{_{}(y_{1} x )_{}(y_{2} x)}), \]

we summarize the scaling factor of each method in Table 1. Comparing \(_{}_{}\) and \(_{}_{}\), DPO optimizes the model until the estimate preference \(_{}\) reaches 1 (\(w_{}=1-_{}\)), and cDPO does until \(_{}\) matches the soft preference \(\) by assigning a high weight when the estimation is wrong (\(w_{}=-_{}\)). DPO pushes the distribution to the oracle preferable outputs, and cDPO may work well as a regularization if the label has high confidence (e.g. \(=0.95\)). However, the gradient of cDPO may also cause unnecessary model updates around \(=0.5\). Intuitively, \(=0.5\) means either candidate answers \((y_{1},y_{2})\) are equally good, but \(_{}_{}\) forces their likelihoods to be balanced.

In contrast, GDPO adjusts the gradient scale based on soft preference by multiplying \((2-1)\), which can also ignore the gradient from even candidate pairs. Figure 1 (left) visualizes that weighted geometric averaging can adjust the scale of gradient based on the soft preference labels. If soft preference labels are close to 1 (e.g. \(=0.95\)), the norm of the scaling factor is almost the same, and small soft preference makes the scaling factor small while the norm reaches zero (e.g. \(=0.55\)). This maintains the effect from clear relationship pairs, reduces the effect from equally good outputs, and reflects the detailed preference signals among the responses. In practice, we set a larger value for \(\) in GDPO than in DPO to maintain and amplify the scale of the gradient for acceptable preference pairs, which works as an implicit filtering of soft preference labels. We will explain this in Section 5.1.

### Analysis in 1-D Synthetic Bandit Problem

To highlight the advantage of geometric averaging and the failure case of linear interpolation as done in cDPO (Equation 6), we consider a 1-D bandit problem with 100 discrete actions and a linear reward function. Figure 1 (right) illustrates the histogram of train data and true reward function, paired preference distribution, and action distributions from the learned policies. The 500,000 training instances are sampled from a bimodal mixture of Gaussian distribution (with the mode in the 20-th and 70-th indices), and we prepare the paired data from those while labeling preferences with the Bradley-Terry model. We train the parameterized reward \(r_{}\) by minimizing \(_{}\), \(_{}\), and \(_{}\), and then recover the learned policies analytically as \(_{r_{}}(y)_{}(y)(r_{}(y))\), where \(_{}(y)\) is an underlying train data distribution. The results demonstrate that cDPO accurately fits the data distribution, which is because the linear interpolation of the loss function in Equation 6 can be interpreted as a minimization of KL divergence \([D_{}(_{})]\). However, this could result in a sub-optimal solution when the train data has a peak in a low-reward region. Because greedy decoding considers the mode of learned distributions, this accurate modeling in cDPO is not aligned with the text generation objectives. On the other hand, DPO and GDPO can assign a probability mass in a high-reward region. GDPO has an advantage against cDPO by resolving such an objective mismatch. Similar trends can be observed in the LLM experiments (Section 5).

   Method & Scaling Factor \(w_{}\) \\ 
**DPO** & \(1-_{}\) \\
**cDPO** & \(-_{}\) \\
**GDPO** (ours) & \((2-1)(1-^{}_{})\) \\ 
**IPO** & \(-}{1-_{}}}{ _{}(y_{1} x)_{}(y_{2} x)}\) \\
**cDPO** & \(-1}{^{2}}-}{1-_{ }}\) \\
**GIPO** (ours) & \((2-1)^{2}(-}{2}_{}}{1-_{}}}{_{}(y_{1} x)_{}( y_{2} x)})\) \\ 
**ROPO** & \((-_{})(1-_{})\) \\
**GROPO** (ours) & \((2-1)(-_{})(1-^{}_{})\) \\   

Table 1: Scaling factor \(w_{}(x,y_{1},y_{2},)\) in the gradient of loss function (Equation 14). The estimated preference probabilities \(_{}\) and \(^{}_{}\) are defined in Equation 15. Compared to others, geometric averaging has a product of \((2-1)\) in a scaling factor, which forces the norm of gradients from the equally preferable responses close to zero.

## 4 Experiments

In the experiments, we use PaLM 2-XS  for the base LLM, as done in prior works [16; 20; 24; 43] (Appendix L uses Gemma-2B/7B as base LLMs). We use the popular RLHF datasets, such as Reddit TL;DR [49; 55] (summarization), and Anthropic Helpful and Harmless  (conversation) for the benchmark. To simulate the soft preference labels, we relabel the preference to the datasets by leveraging AI feedback [4; 24] from instruction-tuned PaLM 2-L (Section 4.1). However, because we found that the soft label distributions in popular RLHF datasets only have similar shapes concentrating on high-confidence regions such as \([0.95,1.0]\) (Figure 10 in Appendix I), we prepared (1) new competitive paired responses from a winner in the original dataset and from LLMs and (2) the novel preference dataset based on Plasma Plan , a dataset of daily-life natural language planning, which simulate more diverse preference label distributions we may face in a practical scenario. For instance, Plasma Plan has a pair of instruction \(x\) (e.g. _see a movie_) and the human-written gold plan \(y\) (e.g. _Step 1: Choose a movie, Step 2: Buy a ticket, Step 3: Go to the theater_). To construct a pair of plans, we generated the plans to all the instructions using PaLM 2-L with few-shot prompting, and then obtained the triplet \((x,y_{},y_{})\). We gathered about 60K response pairs for train split and 861 examples for test split. Following this procedure, we prepared about 93K (Reddit TL;DR), 44K (Anthropic Helpful), and 42K (Harmless) response pairs as train split. To reduce the inference cost, we sample 1000 test prompt-response tuples in Reddit TL;DR while removing the duplicated ones. For other datasets, we have 1639 (Helpful) and 1614 (Harmless) examples in the test split.

To prepare the SFT models, we finetune PaLM 2-XS using 50% of winner responses in train split for Reddit TL;DR, Anthropic Helpful, and Harmless, and using the responses from PaLM 2-L for Plasma Plan. We use those SFT models as an initial checkpoint of preference methods and the reference models \(_{}\). See Appendix B for further details on training.

### Simulating Soft Preference with AI Feedback

Following prior works [4; 10; 15; 24], as reliable alternatives to human raters, we simulate the soft preference labeling with AI feedback from LLMs. AI rating is well aligned with humans and is often used as a proxy of human evaluation in the RLHF literature . Throughout the work, we use PaLM 2-L instruction-tuned on Flan dataset  as an AI rater. To obtain the soft preferences, we put the context \(x\), first output \(y_{1}\), second output \(y_{2}\), and the statement such as _"The more preferable output is: "_, and then get the log probability (score) of token "(1)" and "(2)" from LLMs. Assuming the Bradley-Terry model, we compute the AI preference as follows:

\[_{}(y_{1} y_{2} x)=((1)))}{ (((1)))+(((2)))}. \]

Lastly, to reduce the position bias [40; 58] in LLM rating, we take the average of \(_{}\) by flipping the ordering of \((y_{1},y_{2})\) in the prompt. See Appendix F for the prompts of AI rating. For a fair comparison, we prepare the binary labels based on \(_{}\) rather than the original labels in the dataset.

Figure 2 shows the histogram of soft preference labels from the AI feedback in the preference datasets. We construct competitive paired samples with winner responses and the ones from PaLM 2-L to simulate diverse preference distributions that have uniformity or a peak around the modest confidence (e.g. \([0.7,0.9)\)). We also prepare two other datasets based on Plasma Plan, with different distributions; Plasma Plan Skewed is the more skewed preference dataset by cutting off the high soft preference labels such as \( 0.8\), and Plasma Plan Stairs has lower confident samples more while the number of high confident samples monotonically decreases (\([0.65,0.9)\)). Those distributions could happen in practice when we make pairs of the responses from the capable LLMs

Figure 2: Histogram of soft preference labels \(\) in preference dataset simulated with AI feedback from PaLM 2-L, instruction-tuned on Flan dataset. We prepare Reddit TL;DR [69; 49], Anthropic Helpful and Harmless , and Plasma Plan . We construct competitive paired samples with winner responses and PaLM 2-L to simulate diverse preference distributions that have a peak around the modest confidence (e.g. \([0.7,0.9)\)).

(see Appendix E) and also help more clearly demonstrate the behavior of each algorithm when modestly-confident labels are in the majority. They could lead to better performance than preference labels concentrating on high confidence. The train split of Plasma Plan Skewed and Stairs consists of about 30K/27K response pairs, and the test splits are shared among the dataset from Plasma Plan.

### Binary and Percentage Judge for Evaluation

For the evaluation, we conduct a pairwise comparison between the response from the trained models (\(y_{}\)) and the reference response from PaLM 2-L, and GPT-4  (\(y_{}\)). The reference responses from PaLM 2-L and GPT-4 are generated with few-shot prompting (see Appendix G). In addition, we directly compare our methods and corresponding baselines (e.g. GIPO v.s. IPO or cIPO).

As evaluation metrics, we use the winning rate from binary and percentage judge. We first calculate the AI preference between the response from the trained models and evaluation data as explained in Section 4.1. We calculate the average binary and percent winning rate as follows:

\[=|}_{(x,y_{},y_{})} [_{}(y_{} y_{} x ).5],\ \ =|}_{(x,y_{},y_{})} _{}(y_{} y_{} x). \]

Note that \(_{}\) is also averaged among the flipped order to alleviate the position bias.

## 5 Results

We first compare the alignment performance among the algorithms with binary feedback (DPO, IPO, ROPO), their conservative variants (cDPO, cIPO), and weighted geometric averaging with soft feedback (GDPO, GIPO, GROPO) on six preference datasets (Section 5.1), and evaluate the preference label classification by the learned models (Section 5.2). We also analyze the log-likelihood ratio and reward gap during training (Section 5.4), and then demonstrate the online alignment performances (Section 5.3).

    &  &  &  \\   & vs. Path 2-L & vs. crf-4 & vs. plat-L & vs. crf-4 & vs. eval. \#2-L & vs. crf-4 & vs. pat-L & vs. grf-4 \\ Methods & Binary & \% & Binary & \% & Binary & \% & Binary & \% & Binary & \% & Binary & \% \\ 
**SFT** & 16.2\% & 41.0\% & 3.80\% & 33.38\% & 62.60\% & 56.69\% & 5.74\% & 20.67\% & 62.76\% & 57.83\% & 31.54\% & 36.42\% \\ 
**DPO** & 16.90\% & 40.91\% & 4.00\% & 33.51\% & 86.21\% & 75.40\% & 16.23\% & 33.98\% & 75.40\% & 65.95\% & 41.02\% & 42.79\% \\
**cDPO** & 17.20\% & 41.61\% & 3.80\% & 33.38\% & 83.28\% & 74.04\% & 16.11\% & 33.28\%

### Weighted Geometric Averaging Improves the Alignment Performance

Table 2 presents the winning rate on Reddit TL;DR, Anthropic Helpful and Harmless, Plasma Plan, Plasma Plan Skewed, and Stairs. We compare the performance between the baseline algorithms derived from DPO (SFT, DPO, cDPO, IPO, cIPO, ROPO) and the ones applying geometric averaging (GDPO, GIPO, GROPO). Through the experiments, we set the temperature to 0.0 for the inference.

The results demonstrate that the methods applying geometric averaging (GDPO, GIPO, GROPO) achieve consistently better or comparable performances against binary preference methods (DPO, IPO, ROPO) or their conservative variants (cDPO, cIPO). The trend is clearer on Plasma Plan, Plasma Plan Skewed, and Stairs, which have richer modestly-confident labels. The improvement of GDPO, GIPO, and GROPO compared to baselines have statistical significance with \(p<0.01\) on the Wilcoxon signed-rank test, compared to SFT, binary preference methods, and soft preference methods with linear interpolation. Appendix I also provides the results with the original paired response from Reddit TL;DR, Anthropic Helpful, and Harmless, where many soft labels concentrate on \([0.95,1.0]\). Table 2 highlights that rich soft labels help align LLMs better than those binary ones. Focusing on DPO variants, cDPO does not work well while GDPO performs the best. We hypothesize that this comes from the objective mismatch between the text generation and preference modeling, which we verify in Section 5.2. Moreover, Figure 3 shows the binary winning rates in the direct comparison between corresponding methods, such as GDPO v.s DPO, and GIPO v.s. cIPO, etc, which also reveals that geometric averaging consistently outputs more preferable responses.

Large \(\) as Implicit Preference FilteringAs discussed in Section 3.2, weighted geometric averaging makes the norm of the gradient smaller based on soft preference label \(\). However, an unnecessarily small gradient could stick to sub-optimal solutions. It would be necessary to maintain and even amplify the scale of the gradient from reliable preference pairs. For the rescaling of the gradient, we set larger \(\) because, in geometric averaging, we can regard as using smaller \(^{}:=[2-1]<\). Such a larger \(\) works as an implicit filtering of soft preference labels. Figure 4 (left) presents the binary winning rate of DPO and GDPO with different \([0.1,0.5]\) on Plasma Plan dataset. GDPO has a peak at \(=0.3\), which is larger than that of DPO (\(=0.1\)), and GDPO can achieve better performance. See Appendix B for further details of hyper-parameters.

### Preference Label Classification

Since DPO objective (Equation 5) is derived from the assumption under the Bradley-Terry model, we can regard it as training reward models and implicitly estimating preference probability. We here compare DPO, cDPO, and GDPO, estimate the preference probability \(_{}\) from Equation 15, make a binary label classification (as done in Equation 17), and then compute the average accuracy between predicted labels and true labels given via AI rating. We use Plasma Plan and prepare three different pairs of outputs between PaLM 2-L and (1) humans, (2) GPT-4, and (3) GPT-3.5.

Figure 4 (left) shows that all the methods can classify preference labels well when the test split is composed of the responses from PaLM 2-L and humans, which is the same data distribution as the train split. However, DPO sharply decreases the performance for classifying out-of-distribution pairs, such as from GPT-4 and GPT-3.5 (94.0% \(\) 61.6%/66.3%). cDPO achieves the best classification accuracy on average, and GDPO mitigates the performance drop in DPO. Despite the best accuracy through the proper preference modeling, cDPO does not work well in text generation (Section 5.1). As pointed out in Section 3.3, this can be attributed to an objective mismatch between text generation and preference modeling. While preference modeling aims to fit the models into the given data

Figure 3: Binary winning rates in the direct comparison between weighted geometric averaging (e.g. GDPO) and the corresponding baselines (e.g. DPO, cDPO). The results against SFT are averaged among GDPO, GIPO, and GROPO (Figure 9). Geometric averaging consistently outputs more preferable responses than competitive baselines with about 70% winning rate on average. See Appendix K for the example responses.

distribution, the model in the text generation outputs the mode of distribution with greedy decoding, which might cause a significant mismatch when the mode of distribution is in the low-reward region. These empirical results highlight that GDPO successfully incorporates the strong performance in DPO and the nuanced relationship from the soft labels while avoiding a mismatch.

### Weighted Geometric-Averaging Suppresses Over-Optimization

The analysis of the log-likelihood ratio and the estimated reward gap can characterize the behavior of offline alignment algorithms . In Figure 5, we measure the log-likelihood ratio of winner/loser responses and estimated reward gap on Plasma Plan and Anthropic Harmless.

DPO aggressively pushes down both log ratios and increases the reward gap, since DPO objective forces the model to achieve \(r_{}(x,y_{w})-r_{}(x,y_{l})\), which causes an over-optimization issue. cDPO is more conservative in pushing down the log ratio while leading to worse alignment quality due to objective mismatch. GDPO mitigates the issues of such objective mismatch and over-optimization by maintaining the reward gap increase modestly. Note that, because our paper has focused on open-ended generation tasks, the decrease in the log-likelihood measured with preferable responses does not always matter in contrast to mathematical reasoning or code generation . Our target tasks require pushing down the likelihood of both winner and loser responses to further improve the response quality through the exploration into out-of-distribution regions.

### Weighted Geometric-Averaging Can Help Online Alignment

Offline alignment methods can be extended to online updates  by introducing online feedback processes such as extra reward models or self-rewarding . Due to the cost constraints, online feedback is often asked to be fast and lightweight. However, the quality of preference labels significantly affects the alignment performances. In this section, we demonstrate that weighted geometric averaging can improve online alignment performance by mitigating the quality issues in online feedback. We employ the following two feedback processes: incorporating an extra reward model \(r_{}(x,y)\) and leveraging estimated self-preference \(_{}=((x,y_{w})_{}(x,y)}{ _{}(x,y_{w})_{}(x,y)})\). Note that we apply stop gradient operation for the self-preference. For the extra reward model, we use PaLM 2-XS, the same as a policy LLM.

Figure 6 shows that GDPO performs the best in both settings. This is because GDPO can cancel the gradient from less-confident soft preferences as discussed in Section 3.2, which comes from the case when the on-policy responses are equally good or the estimated preferences in online feedback are

Figure 4: **(Left) Binary winning rate of DPO and GDPO with different \([0.1,0.5]\). GDPO peaks at \(=0.3\), which is larger than that of DPO (\(=0.1\)). (Right) Accuracy of preference label classification on Plasma Plan dataset. All the methods can classify the labels well when the test split is composed of the response pairs from PaLM 2-L and humans; the same data distribution as the train split. However, DPO significantly drops the classification performance when facing out-of-distribution pairs, such as from GPT-4 and GPT-3.5. cDPO achieves the best classification performance on average, and GDPO mitigates the performance drop in DPO.**

Figure 5: Log-likelihood ratio and estimated reward gap on Plasma Plan and Anthropic Harmless. GDPO mitigates the issues of objective mismatch in cDPO and over-optimization in DPO by suppressing the reward gap increase modestly. While Plasma Plan and Anthropic Harmless have different soft preference distributions from each other, the trends of the log-likelihood ratio and reward gap among the algorithms are the same.

not calibrated enough. GDPO demonstrates a significant gain in self-preference. In contrast, DPO degrades the performance worse because the binarization increases the gap from the true preference.

## 6 Discussion and Limitation

We show that geometric averaging consistently improves the performance of DPO, IPO, and ROPO with soft preference labels. We also observe that uniformly distributed soft preference labels achieve better alignments than the original dataset (Appendix I). In fact, the modestly-confident labels do not always mean that the paired responses are noisy or low-quality but even also are more informative, because that could often happen if both responses are good enough. While, as seen in Figure 10 (Appendix I), most datasets for RLHF research only consist of highly-confident pairs, rethinking the effect of preference data distribution on the performances is an important future direction for the practitioners.

The automatic AI rating has a good correlation to the human rating, and is a popular and scalable alternative recently [10; 15; 24; 68]. Our experiments have been conducted on datasets labeled by LLMs as a proximal simulation of soft preference due to the cost constraints. Leveraging actual human preferences labeled via majority voting is another possible future work.

## 7 Related Works

From the helpfulness and safety perspective, it is important to align the outputs from LLMs to the social agreements and our common sense. RLHF [12; 49; 63] is the most popular choice, where we train the reward models to score the predictions and maximize the learned reward with deep RL algorithms [45; 62]. However, this requires additional computational costs from the two independent LLMs and complex pipelines due to on-policy samples. As appealing alternatives, offline algorithms with a single model have been proposed; one of the most representative is DPO , which has been actively extended with different constraints [52; 56], loss function [2; 17; 67; 64], iterative online training [9; 20; 66], nash equilibrium [32; 44; 50], and combination to rejection sampling .

In addition to algorithmic improvements, the alignment problem has been studied from the data perspective [14; 22; 23; 60], which argues that the high-quality, fine-grained preference data without label noise is critical for the performance [19; 31; 35; 37]. Since the preference labels from the human raters must have disagreements and be diverse, Bayesian  or distributional reward modeling [26; 47] and noise-tolerant objectives [11; 27] have been investigated, to maintain the high-quality learning signals even from practical diverse preferences.

Our work newly introduces the notion of soft preference labels - a more general and practical formalization of noisy labels - and then a simple yet effective technique to incorporate the distributional preference into algorithms that have only accepted the binary preference before.

## 8 Conclusion

While the preference is inherently diverse among humans, most prior works only focus on binary labels. To reflect a more detailed preference relationship, we introduce soft preference labels and a simple yet effective modification via weighted geometric averaging that can be applicable to any DPO algorithmic variants. The results demonstrate that soft labels and geometric averaging consistently improve the alignment performance compared to binary labels and conservative methods with linear interpolation of objectives. Using soft labels improves model responses over binary labels by mitigating over-optimization. We also identify that conservative methods, that can fit the preference distribution much better, suffer from the objective mismatch between the text generation and preference modeling. In contrast, geometric averaging can balance both and empirically works better. We hope our work encourages more uses of soft preference labels for alignment in future.

Figure 6: Online alignment with extra reward model (top) and self-preference (bottom) on Plasma Plan. GDPO performs the best with both types of feedback.