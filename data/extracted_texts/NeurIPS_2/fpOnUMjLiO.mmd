# Theoretical characterisation of the Gauss-Newton conditioning in Neural Networks

Jim Zhao

University of Basel, Switzerland

jim.zhao@unibas.ch

First two authors have equal contribution

Sidak Pal Singh

ETH Zurich, Switzerland

sidak.singh@inf.ethz.ch

Aurelien Lucchi

University of Basel, Switzerland

aurelien.lucchi@unibas.com

###### Abstract

The Gauss-Newton (GN) matrix plays an important role in machine learning, most evident in its use as a preconditioning matrix for a wide family of popular adaptive methods to speed up optimization. Besides, it can also provide key insights into the optimization landscape of neural networks. In the context of deep neural networks, understanding the GN matrix involves studying the interaction between different weight matrices as well as the dependencies introduced by the data, thus rendering its analysis challenging. In this work, we take a first step towards theoretically characterizing the conditioning of the GN matrix in neural networks. We establish tight bounds on the condition number of the GN in deep linear networks of arbitrary depth and width, which we also extend to two-layer ReLU networks. We expand the analysis to further architectural components, such as residual connections and convolutional layers. Finally, we empirically validate the bounds and uncover valuable insights into the influence of the analyzed architectural components.

## 1 Introduction

The curvature is a key geometric property of the loss landscape, which is characterized by the Hessian matrix or approximations such as the Gauss-Newton (GN) matrix, and strongly influences the convergence of gradient-based optimization methods. In the realm of deep learning, where models often have millions of parameters, understanding the geometry of the optimization landscape is essential to understanding the effectiveness of training algorithms. The Hessian matrix helps identify the directions in which the loss function changes most rapidly, aiding in the selection of appropriate learning rates and guiding optimization algorithms to navigate the complex, high-dimensional space of parameters. However, in practice, the Hessian is not easily accessible due to the high computational cost and memory requirements. Instead, the GN matrix (or its diagonal form) is commonly employed in adaptive optimization methods such as Adam (Kingma and Ba, 2014) with the goal to improve the conditioning of the landscape. Although the Gauss-Newton matrix \(_{O}\) is only an approximation to the full Hessian matrix, it does seem to capture the curvature of the loss very well given the success of many second-order optimization methods based on approximations of the Gauss-Newton matrix, such as K-FAC (Martens and Grosse, 2020), Shampoo (Gupta et al., 2018) or Sophia (Liu et al., 2023). Particularly interesting is the last method, in which the authors observe that their optimizer based on the Gauss-Newton matrix performs even better than their optimizer based on the full Hessian matrix, implying that the Gauss-Newton matrix is a good preconditioner and captures the curvature of the loss landscape well.

The prevalence of adaptive optimizers and their success in training neural networks illustrates the ability of the GN matrix to capture curvature information similar to that of the Hessian matrix.

Although the non-convexity of neural loss landscapes can be considered as a given, some landscapes can be tougher to traverse than others. This can, for instance, arise due to, or be amplified by, aspects such as: (i) disparate scales of the input data and intermediate features, (ii) initializing the optimization process at a degenerate location in the landscape (imagine an entire set of neurons being dead), (iii) architectural aspects such as width, depth, normalization layers, etc.

Given the close relation of the GN matrix to the network Jacobian (with respect to the parameters) and hence signal propagation (Lou et al., 2022), its significance also reaches more broadly to problems such as model pruning and compression. For instance, it has been extensively observed empirically that training extremely sparse neural networks from scratch poses a significantly greater challenge compared to pruning neural networks post-training (Evci et al., 2019). In an illustrative example shown in Fig. 1, we observe that training sparse networks from scratch using stochastic gradient descent results in a slowdown of training, which is also reflected in an increase in the condition number of the GN matrix (similar experiments on Vision Transformers and other architectures can be found in Appendix I.1). This underscores the relevance of the conditioning of the GN matrix for understanding the behaviour of gradient-based optimizers and emphasizes the significance of maintaining a well-behaved loss landscape of the neural network throughout training.

In fact, many fundamental components of deep learning frameworks, such as skip-connections as well as various normalization techniques (Ioffe and Szegedy, 2015; Salimans and Kingma, 2016; Ba et al., 2016; Wu and He, 2018; Miyato et al., 2018), have been to some degree designed to mitigate the challenges posed by ill-conditioning in neural network optimization. This line of work continues to expand to this date, seeking out better normalization schemes, optimization maneuvers, and regularization strategies (Kang et al., 2016; Wan et al., 2013) that allow for easier and faster optimization while avoiding directions of pathological curvature. But despite the numerous approaches to redress the poor conditioning of the landscape, understanding the precise factors of ill-conditioning within the network structure, and their relative contributions, has remained largely underdeveloped.

Hence, our aim in this work is to carry out a detailed theoretical analysis of how the conditioning of the Gauss-Newton matrix is shaped by constituting structural elements of the network -- i.e., the hidden-layer width, the depth, and the presence of skip connections, to name a few. We will shortly formally introduce the Gauss-Newton matrix and how it is connected to the Hessian of the loss function. Concretely, we would like to provide tight bounds for the condition number of these two terms as a function of the spectra of the various weight matrices and reveal the interplay of the underlying architectural parameters on conditioning. Furthermore, we aim to investigate the impact of both the dataset's structure and the initialization procedure on the conditioning of the loss landscape.

Contributions.Taking inspiration from prior theoretical analyses of deep linear networks, we make a first foray into this problem by rigorously investigating and characterizing the condition number of the GN matrix for linear neural networks (the extension to the second term of the Gauss-Newton decomposition is touched upon in the Appendix). Our analysis holds for arbitrary-sized networks and unveils the intriguing interaction of the GN matrix with the conditioning of various sets of individual layers and the data-covariance matrix. We also complement our analysis in the linear case by studying the effect of non-linearities, via the Leaky-ReLU activation, albeit for two-layer networks. Importantly, as a consequence of our analysis, we show the precise manner in which residual networks

Figure 1: Training loss (left) and condition number \(\) of GN (right) for a ResNet20 trained on a subset of Cifar10 (\(n=1000\)) with different proportions of pruned weights. Weights were pruned layerwise by magnitude at initialization.

with their skip connections or batch normalization can help enable better conditioning of the loss landscape. While our work builds on Singh et al. (2021), our main contribution is the introduction of tight upper bounds for the condition number of the Gauss-Newton (GN) matrix for linear and residual networks of arbitrary depth and width. To the best of our knowledge, this has not been addressed in the literature before. Lastly, given that our bounds are agnostic to the specific values of the parameters in the landscape, we show the phenomenology of conditioning during the training procedure and the corresponding validity of our bounds.

## 2 Setup and background

Setting.Suppose we are given an i.i.d. dataset \(S=\{(_{1},_{1}),,(_{n},_{n})\}\), of size \(|S|=n\), drawn from an unknown distribution \(p_{,}\), consisting of inputs \(^{d}\) and targets \(^{k}\). Based on this dataset \(S\), consider we use a neural network to learn the mapping from the inputs to the targets, \(_{}:\), parameterized by \(^{p}\). To this end, we follow the framework of Empirical Risk Minimization (Vapnik, 1991), and optimize a suitable loss function \(:\). In other words, we solve the following optimization problem,

\[^{*}=*{argmin}_{}\ ()=_{i=1}^{n}(;(_{i},_{i}))\,,\]

say with a first-order method such as (stochastic) gradient descent and the choices for \(\) could be the mean-squared error (MSE), cross-entropy (CE), etc. For simplicity, we will stick to the MSE loss.

Gauss-Newton Matrix.We analyze the properties of the outer gradient product of the loss function, \(_{}\), which we call the Gauss-Newton matrix, defined as

\[_{}=_{i=1}^{n}_{}_{}(_{i})\,_{}_{}(_{i})^{}, \]

where, \(_{}_{}(_{i})^{p  k}\) is the Jacobian of the function with respect to the parameters \(\), \(p\) is the number of parameters, \(k\) is the number of outputs or targets. This outer product of the gradient is closely related to the Hessian of the loss function via the Gauss-Newton decomposition (Scharatolph, 2002; Sagun et al., 2017; Martens, 2020; Botev, 2020), hence the chosen name, which decomposes the Hessian via the chain rule as a sum of the following two matrices:

\[_{}=_{}+_{}=_{i=1}^{n}_{}_{}(_{i})_{}^{2}\,_{i}\, _{}_{}(_{i})^{}+_{i=1}^{n}_{c=1}^{K}[_{_{}}_{i}]_{c}\,_{}^{2}\,_{}^{c}(_{i}),\]

where \(_{_{}}^{2}\,_{i}^{k k}\) is the Hessian of the loss with respect to the network function, at the \(i\)-th sample. Note that if \(_{i}\) is the MSE loss, then \(_{}=_{}\).

**Remark R1** (Difference between \(_{}\) and \(_{}\)).: _When considering MSE loss, the difference between the Gauss Newton matrix \(_{}\) and the Hessian of the loss function \(_{L}\) depends on both the residual and the curvature of the network \(F_{}()\). Thus, close to convergence when the residual becomes small, the contribution of \(_{F}\) will also be negligible and \(_{}\) is essentially equal to \(_{L}\). Furthermore, Lee et al. (2019) show that sufficiently wide neural networks of arbitrary depth behave like linear models during training with gradient descent. This implies that the Gauss-Newton matrix is a close approximation of the full Hessian in this regime throughout training._

Condition number and its role in classical optimization.Consider we are given a quadratic problem, \(*{argmin}_{^{p}}^{ }\), where \( 0\) is a symmetric and positive definite matrix. The optimal solution occurs for \(^{*}=0\). When running gradient descent with constant step size \(>0\), the obtained iterates would be \(_{k}=(-)_{k-1}\). This yields a convergence rate of \(_{k}-^{*}\|}{\|_{k-1}-^{*}\|} (|1-_{}()|,|1-_{}()|)\). The best convergence is obtained for \(=2(_{}()+_{}())^{-1}\), resulting in \(\|_{k}\|\|_{k-1}\|\), where \(()=()}{_{}()}\) is called the condition number of the matrix. This ratio which, intuitively, measures how disparate are the largest and smallest curvature directions, is an indicator of the speed with which gradient descent would converge to the solution. When \(\), the progress can be painfully slow, and \( 1\) indicates all the curvature directions are balanced, and thus the error along some direction does not trail behind the others, hence ensuring fast progress.

Effect of condition number at initialization on the convergence rateAs the condition number is a very local property, it is in general hard to connect the conditioning at network initialization to a global convergence rate. However, we would like to argue below that an ill-conditioned network initialization will still affect the rate of convergence for gradient descent (GD) in the initial phase of training. Let us denote the Lipschitz constant by \(L\) and the smoothness constant by \(\). Furthermore, let the step size be such that \(_{k}\). We present a modified analysis of GD for strongly convex functions, where we use local constants \((k)\) and \(L(k)\) instead of the global smoothness and Lipschitz constant, respectively. Then by the definition of a single step of gradient descent and using the strong convexity and smoothness assumption2 we have:

\[||_{k+1}-^{*}||^{2}(1-_{k})||_{k}- ^{*}||^{2} \]

So by recursively applying (2) and replacing \(\) by the local smoothness constants \((k)\):

\[||_{k}-^{*}||^{2}_{i=0}^{k-1}(1-_{i}(i)) ||_{0}-^{*}||^{2} \]

One can clearly see the effect of \((0)\) in the bound, which is even more dominant when \((k)\) changes slowly. Of course, the effect of \((0)\) attenuates over time, and that's why we are talking about a local effect. However, one should keep in mind that overparametrization leads the parameter to stay closer to initialization (at least in the NTK regime (Lee et al., 2019)).

## 3 Related work

Since the Gauss-Newton matrix is intimately related to the Hessian matrix, and the fact that towards the end of training, the Hessian approaches the Gauss-Newton matrix (Singh et al., 2021), we carry out a broader discussion of the related work, by including the significance of the Hessian at large.

**The relevance of the Hessian matrix for neural networks.** (i) _Generalization-focused work:_ There is a rich and growing body of work that points towards the significance of various Hessian-based measures in governing different aspects of optimization and generalization. One popular hypothesis (Hochreiter and Schmidhuber, 1997; Keskar et al., 2016; Dziugaite and Roy, 2017; Chaudhari et al., 2019) is that flatter minima generalize better, where the Hessian trace or the spectral norm is used to measure flatness. This hypothesis is not undisputed (Dinh et al., 2017), and the extent to which this is explanatory of generalization has been put to question recently (Granziol, 2021; Andriushchenko et al., 2023). Nevertheless, yet another line of work has tried to develop regularization techniques that further encourage reaching a flatter minimum, as shown most prominently in proposing sharpness-aware minimization (Foret et al., 2021).

_(ii) Understanding Architectural and Training aspects of Neural Networks:_ Some other work has studied the challenges in large-batch training via the Hessian spectrum in Yao et al. (2018). Also, the use of large learning rates has been suggested to result in flatter minima via the initial catapult phase (Lewkowycz et al., 2020). The effect of residual connections and Batch normalization on Hessian eigenspectrum were empirically studied in Ghorbani et al. (2019); Yao et al. (2020), which introduced PyHessian, a framework to compute Hessian information in a scalable manner. More recently, the so-called edge of stability (Cohen et al., 2021) phenomenon connects the optimization dynamics of gradient descent with the maximum eigenvalue of the loss Hessian. Very recently, the phenomenon of Deep neural collapse was studied in Beaglehole et al. (2024) via the average gradient outer-product.

_(iii) Applications:_ There has also been a dominant line of work utilizing the Hessian for second-order optimization, albeit via varying efficient approximations, most notably via K-FAC (Martens and Grosse, 2020) but also others such as Yao et al. (2021); Liu et al. (2023), Lin et al. (2023). Given its versatile nature, the Hessian has also been used for model compression through pruning (LeCun et al., 1989; Hassibi et al., 1993; Singh and Alistarh, 2020) as well as quantization (Dong et al., 2019; Frantar et al., 2023), but also in understanding the sensitivity of predictions and the function learned by neural networks via influence functions (Koh and Liang, 2020; Grosse et al., 2023), and countless more.

**Theoretical and empirical studies of the Hessian.** There have been prior theoretical studies that aim to deliver an understanding of the Hessian spectrum in the asymptotic setting (Pennington and Bahri, 2017; Jacot et al., 2019), but it remains unclear how to extract results for finite-widthnetworks as used in practice. Besides, past work has analyzed the rank, empirically (Sagun et al., 2016, 2017) as well as theoretically (Singh et al., 2021, 2023). Further, the layer-wise Hessian of a network can be roughly approximated by the Kronecker product of two smaller matrices, whose top eigenspace has shown to contain certain similarities with that of the Hessian (Wu et al., 2020). In a different line of work by Liao and Mahoney (2021), the limiting eigenvalue distribution of the Hessian of generalized generalized linear models (G-GLMs) and the behaviour of potentially isolated eigenvalue-eigenvector pairs is analyzed.

**Hessian and landscape conditioning.** The stock of empirical repertoire in deep learning has been enriched by successful adaptive optimization methods plus their variants (Kingma and Ba, 2014) as well as various tricks of the trade, such as Batch Normalization (Ioffe and Szegedy, 2015), Layer Normalization (Ba et al., 2016), orthogonal initialization (Saxe et al., 2013; Hu et al., 2020), and the kind, all of which can arguably be said to aid the otherwise ill-conditioning of the landscape. There have also been theoretical works establishing a link between the conditioning of the Hessian, at the optimum and the double-descent like generalization behavior of deep networks (Belkin et al., 2019; Singh et al., 2022).

**Gauss-Newton matrix and NTK.** In the context of over-parametrized networks, \(_{}\) is for instance connected to the (empirical) Neural Tangent Kernel, which has been the focus of a major line of research in the past few years (Jacot et al., 2018; Wang et al., 2022; Yang, 2020) as the NTK presents an interesting limit of infinite-width networks. As a result the asymptotic spectrum and the minimum eigenvalue of the NTK has been studied in (Nguyen et al., 2022; Liu and Hui, 2023), but the implications for finite-width networks remain unclear.

Despite this and the other extensive work discussed above, a detailed theoretical study on the Gauss Newton conditioning of neural networks has been absent. In particular, there has been little work trying to understand the precise sources of ill-conditioning present within deep networks. Therefore, here we try to dissect the nature of conditioning itself, via a first principle approach. We hope this will spark further work that aims to precisely get to the source of ill-conditioning in neural networks and, in a longer horizon, helps towards designing theoretically-guided initialization strategies or normalization techniques that seek to also ensure better conditioning of the GN matrix.

## 4 Theoretical characterisation

The main part of this paper will focus on analyzing the conditioning of \(_{}\) in Eq. (1) as prior work (Ren and Goldfarb, 2019; Schraudolph, 2002) has demonstrated its heightened significance in influencing the optimization process. We will further discuss an extension to \(_{}\) in Appendix F. Tying both bounds together yields a bound on the condition number of the overall loss Hessian in some simple setups.

**Pseudo-condition number.** Since the GN matrix of deep networks is not necessarily full rank (Sagun et al., 2017; Singh et al., 2021), we will analyze the pseudo-condition number defined as the ratio of the maximum eigenvalue over the minimum _non-zero_ eigenvalue. This choice is rationalized by the fact that gradient-based methods will effectively not steer into the GN null space, and we are interested in the conditioning of the space in which optimization actually proceeds. For brevity, we will skip making this distinction between condition number and pseudo-condition number hereafter.

### Spectrum of Gauss-Newton matrix

We will start with the case of linear activations. In this case a network with \(L\) layers can be expressed by \(_{}()=^{L}^{L-1} \,^{1}\), with \(^{}^{a_{} a_{-1}}\) for \(=1,,L\) and \(a_{L}=k,a_{0}=d\). To facilitate the presentation of the empirical work, we will assume that the widths of all hidden layers are the same, i.e. \(_{}=m\) for all \(=1,,L-1\). Also, let us denote by \(=_{i=1}^{n}_{i}_{i}^{T} ^{d d}\) the empirical input covariance matrix. Furthermore, we introduce the shorthand-notation \(^{k:}=^{k}^{}\) for \(k>\) and \(k<\), \(^{k:}=^{k^{}}^{^{}}\). Then, Singh et al. (2021) show that the GN matrix can be decomposed as \(_{}=(_{k}) ^{}\), where \(_{k}\) is the identity matrix of dimension \(k\) and \(^{p kd}\) is given by:

\[=(^{2:L}_{d}\,\,^{ +1:L}^{-1:1}\,\,_{k}^ {L-1:1})^{}.\]

By rewriting \((_{K})^{}=( _{K}^{1/2})(_{K}^{1/2})^{}\), where \(^{1/2}\) is the unique positive semi-definite square root of \(\) and noting that \(\) and \(\) have the same non-zero eigenvalues, we have that the non-zero eigenvalues of \(_{}\) are the same as those of

\[}_{O}=(_{K}^{1/2})^{ }(_{K}^{1/2})\,, \]

where \(^{}^{Kd Kd}\) is equal to

\[^{}=_{=1}^{L}^{L:+1}^{ +1:L}^{1:-1}^{-1:1}. \]

Warm-up: the one-hidden layer case.In the case of one-hidden layer network, \(_{}()=\), we have that \(}_{O}=^{}+ _{k}^{1/2}^{} ^{1/2}\,.\) To derive an upper bound of the condition number, we will lower bound the smallest eigenvalue \(_{}(}_{O})\) and upper bound the largest eigenvalue \(_{}(}_{O})\) separately. Using standard perturbation bounds for matrix eigenvalues discussed in Appendix A, we obtain the following upper bound on the condition number:

**Lemma 1**.: _Let \(_{w}=_{}^{2}()/(_{}^{2}()+ _{}^{2}())\). Then the condition number of GN for the one-hidden layer network with linear activations with \(m>\{d,k\}\) is upper bounded by_

\[(}_{O})\,()\,\,^{2}()+_{}^{2}()}{_{}^{2} ()+_{}^{2}()}=\,()\,\, (_{w}\,()^{2}+(1-_{w})\,()^{2}). \]

Proof sketch.: Choosing \(=^{}\) and \(=_{k}^{1/2}^{}^{1/2}\) and \(i=j=1\) for the Weyl's inequality, \(i=j=n\) for the dual Weyl's inequality and using the fact that \(^{}\) and \(_{k}^{1/2}^{} ^{1/2}\) are positive semidefinite yield the result. 

It is important to point out that the convex combination in Eq. (6) is crucial and a more naive bound where we take the maximum of both terms is instead too loose, see details in Appendix E. Later on, we will observe that the general case with \(L\) layers also exhibits a comparable structure, wherein the significance of the convex combination becomes even more pronounced in deriving practical bounds.

**Remark R2** (Role of the data covariance).: _Besides the above dependence in terms of the convex combination of the bounds, we also see how the conditioning of the input data affects the conditioning of the GN spectra. This is observed in Figure 19, where the condition number of the GN matrix is calculated on whitened and not whitened data. This observation might also shed light on why data normalization remains a standard choice in deep learning, often complemented by the normalization of intermediate layer activations._

### The general \(L\)-layer case

Following our examination of the single-hidden layer case, we now broaden our analysis to include \(L\)-layer linear networks. As before, we will first derive an expression of the GN matrix and subsequently bound the largest and smallest eigenvalue separately to derive a bound on the condition number. Obtaining the GN matrix involves combining (4) and (5), which yields

\[}_{O}=(_{K}^{1/2}) ^{}(_{K}^{1/2})=_ {l=1}^{L}(^{L:+1}^{+1:L})( ^{1/2}^{1:-1}^{-1:1}^{1/2}).\]

By repeatedly applying Weyl's inequalities, we obtain an upper bound on the condition number.

**Lemma 2**.: _Assume that \(m>\{d,k\}\) and that \(_{}:=_{}^{2}(^{L:+1})_{}^{2} (^{1:-1})>0\)\(=1,,L\). Let \(_{}:=}{_{i=1}^{L}_{i}}\). Then the condition number of the GN matrix of a \(L\)-layer linear network can be upper-bounded in the following way:_

\[(}_{O})()_{=1}^{L} _{}(^{L:+1})^{2}(^{1:-1})^{2 }()_{1 L}\{(^{L :+1})^{2}(^{1:-1})^{2}\}.\]

As mentioned earlier, we observe the same convex combination structure which, as we will soon see experimentally, is crucial to obtain a bound that works in practice.

Empirical validation.The empirical results in Figure 1(a) show that the derived bound seems to be tight and predictive of the trend of the condition number of GN at initialization. If the width of the hidden layer is held constant, the condition number grows with a quadratic trend. However, the condition number can be controlled if the width is scaled proportionally with the depth. This gives another explanation of why in practice the width of the network layers is scaled proportionally with the depth to enable faster network training.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_EMPTY:8]

To improve readability, we will write \(:=_{i=1}^{m}^{i}^{}_{i }^{}_{i}^{i}\) hereafter.

**Leaky ReLU activation.** Let's now consider the case where the non-linear activation \(\) is LeakyReLU\((x)=\{ x,x\}\) for some constant \([0,1)\). Typically \(\) is chosen to be close to zero, e.g. \(=0.01\). Again, the condition number \((}_{O})\) can be upper bounded by bounding the extreme eigenvalues independently. By observing that all terms in Eq. (10) are symmetric, we can again apply the Weyl's and dual Weyl's inequality. However to get a lower bound larger than zero for \(_{}(}_{O})\) a different approach is needed as \(_{,i}_{,i}^{}\) is rank-deficient and the same steps to bound the smallest eigenvalue would lead to a vacuous value of zero. Instead, we will use the following observation that we can bound the extreme eigenvalues of the sum of a Kronecker product of PSD matrices \(_{i},_{i}\) for \(i=1,,m\) by \(_{}(_{i=1}^{m}(_{i}_{i})) _{1 j m}_{}(_{j})\,_{} (_{i=1}^{m}_{i})\).

By first using Weyl's inequalities and subsequently applying the above bound with \(_{i}=^{i}^{} ^{i}\) and \(_{i}=_{,i}_{,i}^{}\) we achieve the following bound for the condition number for \((}_{O})\).

**Lemma 4**.: _Consider an one-hidden layer network with a Leaky-ReLU activation with negative slope \(\). Then the condition number of the GN matrix defined in Equation (10) can be bounded as:_

\[(}_{O})^{2}()\,_{ }^{2}()+_{}()}{^{2} \,_{}^{2}()\,_{}^{2}()+_{} ()} \]

The proof can be found in Appendix H. We further empirically validate the tightness of the upper bounds on a subset of the MNIST dataset (\(n=500\)), where we chose \(=0.01\), which is the default value in Pytorch (Paszke et al., 2019). As can be seen in Figure 6, contrary to the linear setting the condition number increases with width. The upper bound also becomes tighter and more predictive with increasing width.

**Comparison to linear network.** By running the same setup for varying values of \(\) we can interpolate between ReLU (\(=0\)) and linear activation (\(=1\)) to see how the introduction of the non-linearity affects the conditioning. In Figure 23, which had to be deferred to the appendix due to space constraints, we can see how reducing the value of \(\) seems to consistently improve the conditioning of the GN matrix for the one-hidden layer case.

Figure 5: Comparison of condition number of GN \((}_{O})\) between Linear Network and Residual Network with \(=1\) for whitened MNIST (left) and whitened Cifar-10 (right) at initialization using Kaiming normal initialization over three seeds. The upper bounds refer to the first upper bound in Lemma 2 and Eq. (7), respectively.

Figure 6: Condition number of a one-hidden layer Leaky ReLU network with \(=0.01\) and upper bounds for whitened MNIST over \(n=500\) data points. The upper bound refers to Eq. (11). Note that the y-axis is log scaled.

Conditioning under batch normalization

Based on the observation that conditioning of the input data affects the conditioning of the GN spectrum, we also tested whether Batch normalization (BN) , which is a very commonly used normalization scheme, has a similar effect on the condition number. For this, the condition number of the GN matrix was calculated for a one-hidden layer linear network with and without a Batch normalization layer. The experiment was run on a downsampled and subsampled version of Cifar-10, which was converted to grayscale with \(d=64\) and \(n=1000\). The data was not whitened to see the effect of BN more strongly. As can be seen in Figure 7, we indeed observe a clear improvement of the condition number when Batch normalization layers are added. Also, the trend of improved conditioning with increasing width remains after adding BN.

## 7 Discussion and conclusion

**Summary.** In this work, we derived new analytical bounds for the condition number of neural networks, showcasing the following key findings: **a)** The conditioning of the input data has a nearly proportional impact on the conditioning of the GN spectra, underscoring the significance of data normalization. Empirical evidence further demonstrates that Batch Normalization similarly enhances the condition number. **b)** For linear networks, we showed that the condition number grows quadratically with depth for fixed hidden width. Also, widening hidden layers improves conditioning, and scaling the hidden width proportionally with depth can compensate for the growth. **c)** We showed how adding residual connections improves the condition number, which also explains how they enable the training of very deep networks. **d)** Preliminary experiments suggest that the ReLU activation seems to improve the conditioning compared to linear networks in the one-hidden layer case.

**Interesting use cases of our results.** Through our analysis, we highlighted that the condition number as a tool from classical optimization is also an attractive option to better understand challenges in neural network training with gradient-based methods. Especially, knowing how different architectural choices will affect the optimization landscape provides a more principled way to design the network architecture for practitioners, for instance how to scale the width in relation to the depth of the network. The paper also gives a justification of why pruned networks are more difficult to train as they have worse conditioning. Although this is not our focus, it is possible that our analysis could inspire better techniques for pruning neural networks.

**Limitations and future work.** We made a first step toward understanding the impact of different architectural choices on the conditioning of the optimization landscape. However, there are still many design choices, that have not been covered yet, such as an extension to non-linear networks for arbitrary layers, other architectures, such as transformers, and analytic bounds for normalization schemes, such as batch or layer normalization, which we will leave to future work. Another limitation of our current work is that the derived upper bounds are agnostic to the training dynamics. Therefore, they cannot distinguish the difference between random initializations and solutions of deep learning models after convergence. Incorporating training dynamics into the upper bounds would allow us to characterize solutions found by different architectures, which is left for future work. Another future direction is to extend the analysis to the Generalized Gauss-Newton matrix, which is particularly relevant for training with cross-entropy loss.