# TFGDA: Exploring Topology and Feature Alignment

in Semi-supervised Graph Domain Adaptation

through Robust Clustering

Jun Dan\({}^{1*}\), Weiming Liu\({}^{1*}\), Chunfeng Xie\({}^{2}\), Hua Yu\({}^{3}\), Shunjie Dong\({}^{4}\), Yanchao Tan\({}^{5,6,7}\)

\({}^{1}\)Zhejiang University \({}^{2}\)Queen Mary University of London

\({}^{3}\)Dalian University of Technology \({}^{4}\)Shanghai Jiao Tong University \({}^{5}\)Fuzhou University

\({}^{6}\)Engineering Research Center of Big Data Intelligence, Ministry of Education

\({}^{7}\)Fujian Key Laboratory of Network Computing and Intelligent Information Processing

danjun@zju.edu.cn, 21831010@zju.edu.cn, c.xie@qmul.ac.uk

yhiccd@mail.dlut.edu.cn, sjdong@sjtu.edu.cn, yctan@fzu.edu.cn

Equal Contribution, \({}^{}\) Corresponding Author.

###### Abstract

Semi-supervised graph domain adaptation, as a branch of graph transfer learning, aims to annotate unlabeled target graph nodes by utilizing transferable knowledge learned from a label-scarce source graph. However, most existing studies primarily concentrate on aligning feature distributions directly to extract domain-invariant features, while ignoring the utilization of the intrinsic structure information in graphs. Inspired by the significance of data structure information in enhancing models' generalization performance, this paper aims to investigate how to leverage the structure information to assist graph transfer learning. To this end, we propose an innovative framework called TFGDA. Specially, TFGDA employs a structure alignment strategy named STSA to encode graphs' topological structure information into the latent space, greatly facilitating the learning of transferable features. To achieve a stable alignment of feature distributions, we also introduce a SDA strategy to mitigate domain discrepancy on the sphere. Moreover, to address the overfitting issue caused by label scarcity, a simple but effective RNC strategy is devised to guide the discriminative clustering of unlabeled nodes. Experiments on various benchmarks demonstrate the superiority of TFGDA over SOTA methods.

## 1 Introduction

With the rise of deep learning, node classification techniques have made significant progress in diverse fields. However, due to the distribution shift issue, well-trained models often suffer severe performance degradation when applied directly to a new domain. Graph transfer learning (GTL)  has been proposed to tackle this issue by transferring domain-invariant features from a labeled source graph to an unlabeled target graph, effectively boosting the model's performance on the target graph.

Although current studies on GTL have made significant strides, they often rely on the assumption that all nodes in the source graph are labeled. However, this ideal assumption does not hold true in many scenarios, as annotating the entire source graph is time-consuming, especially for large-scale networks. Therefore, in this paper, we focus on a more realistic application scenario known as semi-supervised graph domain adaptation (SGDA) , where the source graph only has a few labeled nodes. Utilizing the transferable knowledge acquired from the label-scarce source graph to enhance the model's adaptation performance on the target graph is the most crucial challenge for SGDA.

However, most existing studies [3; 4; 1] tend to focus on directly aligning feature distributions across domains to extract domain-invariant node features, while overlooking the utilization of the intrinsic structure information in graphs. Notably, the recent advancements in unsupervised learning [5; 6; 7; 8; 9] have showcased the significance of data structure information in enhancing models' generalization. Considering the complex topological structure information presented in the graph, this paper seek to leverage these critical structure information to assist the transfer of shared node knowledge. To this end, we propose an innovative SGDA framework named **TFGDA** that employs a Subgraph Topological Structure Alignment (**STSA**) strategy to encode structure information into latent space. Specially, STSA utilizes persistent homology (PH)  to extract the topological structure information of the input and latent spaces, and then align the topological structures of two space, significantly improving the model's transfer performance. Furthermore, current works [2; 11; 4] primarily utilize adversarial training to reduce domain discrepancy. However, adversarial training is an unstable process that may destroy the discriminative details hidden in features , thereby affecting the transfer of shared knowledge. To remedy this issue, we propose a Sphere-guided Domain Alignment (**SDA**) strategy, aiming to achieve more stable domain alignment. Concretely, SDA initially maps node features to the spherical space. Then, geodesic projection  is utilized to project spherical features onto multiple great circles, where the spherical sliced-Wasserstein (SSW) distance  is employed to quantify the feature distributions discrepancy across domains.

More importantly, in the SGDA scenarios, due to the label scarcity of source graph, well-trained model on only a few labeled source nodes is likely to encounter overfitting. Consequently, it may make ambiguous or even incorrect predictions for certain target graph nodes located near the decision boundaries or far from their corresponding class centers. To address this overfitting issue, we devise a Robustness-guided Node Clustering (**RNC**) strategy to effectively enhance the model's robustness. RNC aims to guide the discriminative clustering of unlabeled nodes by maximizing the mutual information between the soft cluster assignment of the original node and its perturbed version, significantly improving the model's generalization performance on the target graph.

In summary, the main contributions are listed as follows:

**(1)** To the best of our knowledge, this is first attempt to utilize the intrinsic topological structure information hidden in graphs to assist GTL. A novel STSA strategy is proposed to preserve the topological structure information in latent space.

**(2)** A strategy named SDA is introduced to stably align the node feature distributions across domains.

**(3)** To address the overfitting issue, a simple but effective RNC strategy is devised to guide the discriminative clustering of unlabeled nodes.

**(4)** Experimental results show that our TFGDA outperforms SOTA methods on various benchmarks.

## 2 Related Works

**Graph Transfer Learning.** GTL [15; 16] has gained widespread attention for relieving the burden of collecting labeled data for new tasks. Early studies usually use source nodes to pre-train expressive models for related tasks in target domain [17; 18; 19; 20; 21]. To enhance model's generalization, recent studies have shifted their emphasis to domain adaptation [11; 22; 23]. There are two main ways to extract domain-invariant node features: (1) Using adversarial training to enforce domain confusion [16; 4; 2; 24]; (2) Minimizing the statistical distance between two domains [25; 26; 27; 28; 29]. However, existing works tend to focus on domain alignment while overlooking the utilization of structure information in graphs.

**Semi-supervised Learning on Graphs.** Semi-supervised learning on graphs addresses node classification with a small fraction of labeled nodes. Previous works [30; 31; 32] commonly adopt the message passing paradigm to extract discriminative features. Recent studies have explored various techniques, including adversarial training [33; 34], data augmentation [35; 36], continuous graph , contrastive learning  and meta learning [38; 39] to further enhance model's generalization.

**Persistent Homology (PH).** PH is an essential method in topology for extracting structure information from point clouds [40; 41]. Recently, PH has shown significant advantages in various areas, including signal processing, shape matching, and design of network . Some studies have investigated its differentiability [45; 46]. Recent works have also explored its potential in image segmentation [47; 48], action/image recognition [49; 50; 51; 52; 53], and evaluation of GANs .

Preliminaries: Persistent Homology (PH)

PH is a method used to capture the topological structure of complex point clouds as a scale parameter \(\) is varied. In this section, we briefly introduce some key concepts. Further details on PH can be found in [10; 40].

**Notation.**\(:=\{x_{i}\}_{i=1}^{m}\) denotes a point cloud and \(\) is a distance metric over \(\).

**Vietoris-Rips (VR) Complex.** The VR complex  is a unique simplicial complex constructed from a set of points, providing an approximation of the underlying space's topology. The VR complex of \(\) at scale \(\), denoted as \(_{}()\), contains all simplices of \(\). Each component of \(\) satisfies the distance constraint: \((x_{i},x_{j})\) for any \(i,j\). Additionally, the VR complex exhibits a nesting property: \(_{_{i}}_{_{j}}\) for any \(_{i}_{j}\), which enables us to track the evolution progress of simplical complex as \(\) increases.

**Persistence Diagram (PD).** The PD \(dgm\) is a multi-set of points \((g_{1},g_{2})\) in the Cartesian plane \(^{2}\), encoding lifespan information of topological features. Concretely, it summarizes the birth time \(g_{1}\) and death time \(g_{2}\) information of each topological feature with a homology group. The birth time \(g_{1}\) indicates the scale of feature creation and death time \(g_{2}\) refers to the scale of feature destruction.

## 4 Methodology

### Problem Definition

**Source Domain Graph**: The source graph is defined as \(^{s}=(^{s,l},^{s,u},Y^{s,l},A^{s},X^{s})\), where \(^{s,l}\) is the labeled node set, and \(^{s,u}\) is the remaining unlabeled node set in \(_{s}\). \(Y^{s,l}^{|^{s,l}| K}\) denotes the label matrix of \(^{s,l}\), where \(K\) is the number of node classes. If a node \(n^{s}_{i}^{s,l}\) belongs to the \(k\)-th class, \(y^{s}_{i,k}=1\); otherwise, \(y^{s}_{i,k}=0\). \(A^{s}^{N^{s} N^{s}}\) is an adjacency matrix, where \(N^{s}=|^{s,l}|+|^{s,u}|\) is the number of nodes in \(^{s}\). If there is an edge between nodes \(n_{i}\) and \(n_{j}\), the value of \(A^{s}_{ij}\) is set to 1; otherwise, it is set to 0. \(X^{s}^{N^{s} e}\) indicates an attribute matrix, where \(e\) is the dimension of node attributes. Notably, \(|^{s,l}|\) is much smaller than \(|^{s,u}|\) in the SGDA setting.

**Target Domain Graph**: Similarly, the target graph is represented as \(^{t}=(^{t},A^{t},X^{t})\), which is a completely unlabeled graph with an unlabeled node set \(^{t}\). \(A^{t}^{N^{t} N^{t}}\) is an adjacency matrix, and \(X^{t}^{N^{t} e}\) is a node attribute matrix, where \(N^{t}=|^{t}|\) denotes the number of nodes in \(_{t}\).

**Semi-Supervised Graph Domain Adaptation (SGDA)**: Given a partially labeled source graph \(^{s}\) and an unlabeled target graph \(^{t}\), the goal of SGDA is to precisely annotate target graph nodes by utilizing transferable knowledge learned from the limited labeled source nodes [2; 16].

### Network Architecture

The architecture of our TFQDA model is depicted in Figure 1. It consists of two components: a graph convolutional network (GCN)-based feature extractor \(\) and a node classifier \(\). Mathematically, given an input graph \(=(,A,X)\), the node features extracted by \(\) is denoted as \(H=()^{|| d}\), and it is further normalized to map onto a spherical space \(_{r}^{d-1}\) to obtain spherical features \(Z^{|| d}\), where \(d\) is the feature dimension, \(r\) is the radius and \(||\) denotes the number of nodes in \(\). The classification probability predicted by \(\) is denoted as \(=(Z)^{|| K}\).

To capture more precise adjacency relationships of graph \(\), we compute the positive point-wise mutual information (PPIM) between nodes following . Specially, for a given graph \(=(,A,X)\), we utilize random walk to sample a collection of paths on \(A\) and generate a frequency matrix \(R\). Based on \(R\), we can compute the PPIM matrix \(\) as follows:

\[_{ij}=}{_{i,j}R_{ij}},\; _{i,*}=R_{ij}}{_{i,j}R_{ij}},\;_{*,j}= R_{ij}}{_{i,j}R_{ij}},\\ P_{ij}=\{(_{ij}}{_{i,*} _{*,j}}),0\}, \]where \(P_{ij}\) denotes the positive mutual information between nodes \(n_{i}\) and \(n_{j}\), which quantifies the topological proximity between nodes. A higher value of \(P_{ij}\) indicates a strong connection between \(n_{i}\) and \(n_{j}\). Then, the output of the \(l\)-th GCN layer \(Conv^{(l)}()\) is defined as:

\[H^{(l)}=Conv^{(l)}(P,H^{(l-1)})=(D^{-}D^{-}H^{(l-1)}W^{(l)}), \]

where \(D\) is the diagonal degree matrix of \(P\), and \(=P+I\) (\(I\) is a identity matrix). \(W^{(l)}\) refer to the trainable parameters of the \(l\)-th layer, \(()\) is an activation function, and \(H^{(0)}=X\). Thus, the feature extractor \(\) can be constructed by sequentially stacking \(L\) layers of GCN \(Conv^{(l)}(l=1,2,,L)\). Given the source labeled node set \(^{s,l}\), the classification loss on the source graph \(^{s}\) is defined as:

\[_{cls}=^{s,l}|}_{n_{i}^{s}^ {s,l}}_{ce}(((n_{i}^{s})),y_{i}^{s}). \]

where \(_{ce}\) represents the standard cross-entropy loss.

### Subgraph Topological Structure Alignment

As mentioned in Section 1, the graph consists of numerous nodes (e.g., _ACMv9_ has over 9000 nodes), thereby containing rich structure information. Inspired by the significance of data structure information in enhancing models' generalization [56; 51; 6], we seek to leverage such critical structure information to facilitate the learning of domain-invariant features.

To achieve this goal, we treat the graph as a point cloud and attempt to directly capture its underlying topological structure using PH. However, we inevitably encounter huge computational burden due to the complex attributes and adjacency relationships of graph. Fortunately, Refs. [57; 58] have indicated that the properties of graph can be well preserved in its multiple local subgraphs. To this end, we propose a Subgraph Topological Structure Alignment (**STSA**) strategy to encode structure information of input space into the latent space. For a given graph \(=(,A,X)\), STSA first sample \(a\) subgraphs \(\{}_{1},}_{2},,}_{ a}\}\) using random walk, and then employ PH to capture the intrinsic topological structure information of each subgraph \(}_{i}\).

Specially, for each subgraph \(}_{i}\), we represent its spherical features extracted by \(\) as \(_{i}^{|}_{i}| d}\), which are obtained by indexing the local nodes features from the complete nodes features \(Z^{|| d}\). Here \(|}_{i}|\) denotes the number of nodes in each subgraph. Then we construct the VR complexes

Figure 1: Global overview of the TFGDA model. STSA strategy encodes critical structure information of graphs into spherical space (\(_{stsa}=_{str}(^{s})+_{str}( ^{t})\)), greatly improving the modelâ€™s generalization. SDA strategy aims to extract domain-invariant node features by minimizing domain discrepancy on sphere (\(_{sda}\)). Moreover, to effectively solve the overfitting issue, RNC strategy is introduced to guide the discriminative clustering of unlabeled nodes (\(_{rnc}=_{mi}(^{s})+_{mi}(^{t})\)).

\(_{}(}_{i})\) and \(_{}(_{i})\) for point clouds \(}_{i}\) and \(_{i}\), employ PH to extract their topological structures, and obtain their corresponding PDs \(dgm(}_{i})\) and \(dgm(_{i})\) respectively. To align the topological structures of the input and latent spherical spaces, we adopt the 1-Wasserstein distance \(_{1}\) to measure the discrepancy between two PDs (i.e., \(dgm(}_{i})\) for input space and \(dgm(_{i})\) for spherical space), which aims to seek the optimal transport plan \(^{*}\) between two PDs:

\[^{*}&=*{arg\, min}_{}_{1}(dgm(}_{i}),dgm(_{i}))\\ &=*{arg\,min}_{}_{(,) }\|-\|_{} \]

where \( dgm(}_{i})\), \( dgm(_{i})\), and \(\|\|_{}\) is the \(l_{}\) distance. After obtaining \(^{*}\), the local structure discrepancy \(_{str}^{sub}(}_{i})\) between two spaces of \(}_{i}\) can be calculated as:

\[_{str}^{sub}(}_{i})=_{(,)^{ *}}\|-\|_{2}^{2}. \]

Hence, we can estimate the global structure discrepancy between the input and spherical spaces of graph \(\) by aggregating the local structure discrepancy of all subgraphs:

\[_{str}()=_{i}_{str}^{sub}(}_{i}) \]

In the SGDA scenario, the STSA strategy will be applied to both the \(^{s}\) and \(^{t}\), and its loss function is defined as:

\[_{stsa}=_{str}(^{s})+_{str}( ^{t}). \]

Notably, preserving topological structure information has the potential to guide unlabeled nodes towards achieving discriminative clustering, thereby promoting the learning of transferable node features, as verified in Section 5.3.

Although many GCNs-based methods have already been proposed to exploit graph structure information to promote the learning of features, these methods are not effective in addressing the SGDA task. Specifically, these GCNs-based methods [59; 60; 61; 62] typically mine the graph structure information in the deep feature space by designing well-crafted GCN architectures or introducing some complex modules. However, recent studies [27; 63; 64; 16] have pointed out that GCNs are insufficient in capturing the sophisticated structure information in graph, which means that the graph structure information may be lost or destroyed after passing through the GCNs-based feature extractors. Thus, directly mining graph structure information from the deep feature space is a suboptimal way, which affects the learning of transferable node features in our SGDA setting.

The proposed STSA strategy aims to extract the graph structure information directly from the input space and encode these powerful information into the latent spherical space by aligning the topological structures of the two spaces. This method does not lose or destroy the graph structure information during training. Furthermore, our STSA strategy does not introduce any changes to the network architecture, effectively avoiding an increase in model's complexity and ensuring its adaptability to integration with other methods.

### Sphere-guided Domain Alignment

As mentioned in Section 1, adversarial training has been widely adopted by existing GTL models to reduce domain discrepancy. However, it is an unstable process that may destroy the discriminative information hidden in node features, thereby impacting the learning of shared features.

To tackle this issue, we propose a Sphere-guided Domain Alignment (SDA) strategy that achieves stable alignment of cross-domain node features distributions in the spherical space. Our SDA strategy mainly comprises three steps: **(1)** Map node features onto the sphere space \(_{r}^{d-1}\). **(2)** Use geodesic projection  to project node features from \(_{r}^{d-1}\) to multiple great circles. **(3)** Compute the feature distributions discrepancy on great circles and minimize it during training.

**Step (1):** Motivated by the effectiveness of spherical features in improving model's transfer performance , we first normalize node features \(H^{|| d}\) extracted by \(\) with \(z_{i}=r}{\|h_{i}\|}\) to obtain the spherical features \(Z^{|| d}\) in the sphere space \(_{r}^{d-1}=\{z_{i}^{d}:\|z_{i}\|_{2}=r\}\), where \(h_{i}\) and \(z_{i}\) is the \(i\)-th row of \(H\) and \(Z\), respectively. Notably, Ref.  has proved that a proper radius \(r\) is lower bounded by parameters \(\) and \(K\):

\[r \]

where \(\) denotes the expected minimal classification probability of class center and \(K\) is the number of classes. In this work, \(\) is set to 0.999, and radius \(r\) is set to the lower bound.

**Step (2):** Previous studies [67; 68; 69; 70] have shown the superiority of optimal transport in aligning feature distributions. Let \(\) be a probability space and \(\), \(\) be two probability measures in \(()\). For any \(q 1\), the \(q\)-Wasserstein distance between \(\) and \(\) is defined as:

\[_{q}^{q}(,)=_{(,)}_{ }^{q}(z,g)d(z,g) \]

where \((,)=\{()|_{1\#}=, _{2\#}=\}\) is the set of couplings, \(_{1}\) and \(_{2}\) denote the two marginal projections of \(\) to \(\), \(\#\) denotes the push-forward operator, and \(:^{+}\) is a geodesic metric. However, we find that directly using the classical Wasserstein distance to compute features distributions discrepancy on the sphere \(_{r}^{d-1}\) is computationally expensive, due to numerous nodes in the graph.

For more efficient calculations, we utilize geodesic projection \(I^{U}\) to project node features lying on \(_{r}^{d-1}\) to \(b\) great circles \(\{_{1},_{2},,_{b}\}\). On the hypersphere, great circles  are circles whose diameter is equal to that of the sphere, and they correspond to the geodesics. Specially, the geodesic projection \(I^{U}\) is determined by \(U\):

\[I^{U}(z)=U^{}*{arg\,min}_{g( {UU}^{})_{r}^{d-1}}_{_{r}^{d-1}}(z,g)= *{arg\,min}_{g_{r}^{d}}_{_{r}^ {d-1}}(z,Ug), U_{d,2}, z_{r}^{d-1}. \]

where \(_{_{r}^{d-1}}(z,g)=( z,g)\), and \(_{d,2}=\{U^{d d},U^{}U=I_{2}\}\) is the Stiefel manifold .

**Step (3):** Next, we utilize the spherical sliced-Wasserstein (SSW) distance  to measure the feature distributions discrepancy of two domain on multiple great circles, which can be formulated as:

\[SSW_{q}^{q}(u,v)=_{_{d,2}}_{q}^{q}(I_{\#}^{U},I_{ \#}^{U})(), \]

where \(\) is the uniform distribution over \(_{d,2}\). In SSW, the geodesic metric \(^{q}(z,g)\) in Wasserstein distance \(_{q}^{q}(,)\) is defined as the geodesic distance \(_{_{r}^{1}}(z,g)=(|z-g|,r-|z-g|)\).

In practice, it's common to approximate the source and target feature distributions using samples \((z_{i}^{s})_{i=1}^{N^{}}\) and \((z_{j}^{t})_{j=1}^{N^{t}}\) (i.e., through the empirical approximations \(=}_{i=1}^{N^{}}_{z_{i}^{s}}\) and \(=}_{j=1}^{N^{}}_{z_{j}^{t}}\)), where \(\) is the Dirac function. As a result, the node features distributions discrepancy between two domains can be measured as:

\[_{sda}=SSW_{p}^{p}(,)_ {m=1}^{b}_{q}^{q}(,) \]

where \(b\) is the number of projections. As training progresses, SDA strategy gradually reduces domain discrepancy, making the learning of domain-invariant features easier.

### Robustness-guided Node Clustering

Due to the label scarcity in \(^{*}\), the model is prone to overfitting when solely relying on \(_{cls}\) for optimization, severely degrading the model's generalization performance on \(^{t}\). To alleviate this overfitting issue, we devise a novel Robustness-guided Node Clustering (**RNC**) strategy to enhance model's robustness by guiding the discriminative clustering of unlabeled nodes. Specially, RNC first introduces trainable shift parameters \(_{s}=\{_{s}^{(1)},_{s}^{(2)},,_{s}^{(L)}\}\) and \(_{t}=\{_{t}^{(1)},_{t}^{(2)},,_{t}^{(L)}\}\) to perturbs the source and target node features respectively, at each layer of \(\):

\[H^{s/t,_{s/t},t}(l)=Conv^{(l)}(P^{s/t},X^{s/t})+_{s/t}^{(l)}, &l=1\\ Conv^{(l)}(P^{s/t},H^{s/t,(l-1)})+_{s/t}^{(l)},&1<l L \]where \(H^{s,_{s},(l)}\) and \(H^{t,_{t},(l)}\) denotes the perturbed source and target node features encoded by \(Conv^{(l)}\) respectively, and each \(_{s/t}^{(i)}\) is specific to the output of \(Conv^{(l)}\). The shift parameters \(_{s}\) and \(_{t}\) are defined as randomly initialized multi-layer parameter matrices. After perturbation, based on \(H^{s,_{s},(L)}\) and \(H^{t,_{t},(L)}\), we can obtain the perturbed source node spherical features \(Z^{s,_{s}}\) and the perturbed target node spherical features \(Z^{t,_{t}}\) respectively.

For brevity, we will omit domain-specific notations in the following text. Ideally, regardless of how the node feature is perturbed, the model's prediction for it should remain unchanged because its class label has not changed. To achieve this goal, RNC aims to maximize the mutual information between the soft cluster assignment (i.e., classification prediction of classifier \(\)) of the spherical features \(Z\) and its perturbed version \(Z^{}\), capturing their intrinsic invariant information between \(Z\) and \(Z^{}\).

Concretely, given a node spherical feature \(z_{i}\), the classification probability predicted by \(\) is denoted as \((z_{i})^{K}\), that can be viewed as the distribution of a discrete random variable \(\) over \(K\) classes: \((=k|z)=_{k}(z_{i})\). Let \(\) and \(^{}\) denote the cluster assignment variables of \(z_{i}\) and \(z_{i}^{}\), respectively. Then, their conditional joint distribution is defined as: \((=k,^{}=k^{}|z_{i},z_{i}^{})=_{k}(z_{i})_{ k}(z_{i}^{})\). After marginalization, the joint probability distribution can be formulated as a matrix \(^{K K}\):

\[=|}_{i=1}^{||}(z_{i}) (z_{i}^{})^{}, \]

where \(_{kk^{t}}=(=k,^{}=k^{})\), \(_{k}=(=k)\), and \(_{k^{t}}=(^{}=k^{})\). To preserve the equivalence between pairs \((z_{i},z_{i}^{})\) and \((z_{i}^{},z_{i})\), matrix \(\) is typically symmetrized using \((+^{})/2\). In this way, the mutual information [73; 74] between the soft cluster assignment of \(Z\) and \(Z^{}\) can be computed as:

\[_{mi}()=_{k=1}^{K}_{k^{t}=1}^{K}_{kk^{ }}_{kk^{}}}{_{k}_{k^{ }}},s.t.,||^{(l)}||_{F},^{(l)}. \]

where \(\) is a coefficient that controls the scale of feature perturbation.

In the SGDA scenario, all target domain nodes \(_{t}\) and source domain nodes \(^{s,l}^{s,u}\) are used to calculate matrices \(^{t}\) and \(^{s}\), respectively. Therefore, the objective function \(_{rnc}\) of RNC can be expressed as:

\[_{rnc}=_{mi}(^{s})+_{mi}(^{t}), \]

\[s.t.,||^{s,(l)}||_{F},^{s,(l)}^{s},||^{t,(l)} ||_{F},^{t,(l)}^{t}.\]

Notably, we leverage source labeled nodes \(^{s,l}\) in RNC as they can significantly guide the discriminative clustering of unlabeled nodes in the right direction. With the help of RNC, more and more intrinsic invariant features of nodes are extracted, which greatly promotes the learning of transferable features. Moreover, unlike previous studies that employ pseudo-labels strategy  or conditional entropy term  to guide the learning of unlabeled nodes, our RNC strategy does not involve any pseudo-labels and naturally avoids degenerate clustering solutions (see Figure 3 for further analysis).

### Model Optimization

In summary, the total objective of TFGDA can be expressed as follows:

\[_{,,^{s},^{t}}_{cls}+ _{sda}+_{stsa}-_{rnc} \] \[s.t.,||^{s,(l)}||_{F},^{s,(l)}^{s}, ||^{t,(l)}||_{F},^{t,(l)}^{t}.\]

where hyper-parameters \(\), \(\) and \(\) are used to balance the contributions of the corresponding term.

### Theoretical Analysis

The theoretical analysis of our method is based on the theory of domain adaptation (DA) [76; 77].

Formally, let \(\) be the hypothesis space. Given two domains \(\) and \(\), the probabilistic bound of error of hypothesis \(h\) on the target domain is defined as: \(_{}(h)_{}(h)+d_{ }(,)+^{*}\), where the expected error on the target domain \(_{}(h)\) are bounded by three terms: **(1)** the expectederror on source domain \(_{}(h)\); **(2)** the \(\)-divergence between the source and target domains \(d_{}(,)\); **(3)** the combined error of ideal joint hypothesis \(^{*}=_{h^{}}_{}(h^{})+_{ }(h^{})\).

The goal of DA is to lower the upper bound of the expected target domain error \(_{}(h)\). Note that in unsupervised domain adaptation (UDA), minimizing \(_{}(h)\) can be easily achieved with source label information, as source domain samples are completely annotated. However, in our SGDA setting, due to the label scarcity of source domain, the model is prone to overfitting when solely relying on the source domain classification loss \(_{cls}\) for optimization. Therefore, we introduce the **RNC** strategy (\(_{rnc}\)) to address the overfitting issue, with the aim of guiding \(_{}(h)\) towards further minimization.

Most DA methods mainly focus on reducing the domain discrepancy \(d_{}(,)\), such as utilizing techniques like adversarial learning, MMD, optimal transport [78; 79; 80; 81], and CORAL. In comparison to these methods, our **SDA** strategy (\(_{sda}\)) effectively eliminates the feature norm discrepancy in spherical space \(_{d}^{d-1}\) and guide a more stable alignment of feature distributions. Furthermore, considering that graph data contains rich structure information that encodes complex relationships among nodes and edges, and existing GTL methods usually adopt GCNs-based feature extractors to learn domain-invariant node features. However, recent studies [27; 63; 64; 16] have pointed out that GCNs are insufficient in capturing the sophisticated structure information in graph, which seriously affects the transfer of domain-invariant knowledge and consequently limits the model's generalization ability. To solve this problem, we thus propose the **STSA** strategy (\(_{ssta}\)) to align the topological structures of the input space and the spherical space, in order to facilitate the GCNs-based feature extractors to capture more domain-invariant node features. Consequently, the combination of **SDA** (\(_{sda}\)) and **STSA** (\(_{ssta}\)) strategies further promotes the minimization of the domain discrepancy \(d_{}(,)\).

Notably, \(^{*}\) is expected to be extremely small, and therefore it is often neglected by previous methods. However, it is possible that \(^{*}\) tends to be large when the cross-domain category distributions are not well aligned [82; 83]. In this paper, we leverage the **RNC** strategy (\(_{rnc}\)) to guide both labeled nodes and unlabeled nodes toward achieving robust clustering, effectively promoting the fine-grained alignment of category distributions and ensuring that \(^{*}\) remains at a relatively small value.

In summary, our proposed method not only minimizes the source expected error \(_{}(h)\) and domain discrepancy \(d_{}(,)\), but also keeps \(^{*}\) at a small value, thereby ensuring a low upper bound.

## 5 Experiments

### Setup

**Datasets.** Our experiments involve three real-world graphs: _ACMv9_ (**A**), _Citationv1_ (**C**), and _DBLpv7_ (**D**), obtained from ArnetMiner . Since these graphs have varying sets of node attributes, we union their attribute sets and adjust the attribute dimension to 6775 following . Each node is assigned a five-class label, determined by its relevant research areas. Six typical transfer tasks are considered in our experiments: **A\(\)C**, **A\(\)D**, **C\(\)A**, **C\(\)D**, **D\(\)A** and **D\(\)C**. Due to the page size limitation, more settings and implementation details are placed on **Appendix**.

**Compared Methods.** We compare TFGDA with several SOTA **(1)** graph semi-supervised learning methods and **(2)** graph domain adaptation methods as Ref.: **(1) GCN**, **GSAGE**, **GAT**, **GIN**, **(2) DANN**, **CDAN**, **UDA-GCN**, **AdaGCN**, **CoCo**, **StruRW** and **SGDA**. DANN\({}_{GCN}\) and CDAN\({}_{GCN}\) are two variants that adopt GCN-based feature extractor.

### Results and Discussion

Following , to showcase the superiority of our TFGDA, we report its performance on the challenging scenario, where only **5%** of the nodes in the source graph are labeled. **Micro-F1** and **Macro-F1** are employed as evaluation metrics, and the classification results on the target graph are gathered in Table 1. For all transfer tasks, we run each experiment 5 times and record the average accuracy with standard deviation. And we sample different label sets each time to mitigate the randomness. As can be seen, our model obtains the overall best results on all transfer tasks. Specially, TFGDA greatly surpasses the SOTA method SGDA  by \(+7.3\%\) and \(+10.0\%\) on "Micro-F1" and "Macro-F1" respectively for the **C\(\)A** task, implying the superiority in extracting domain-invariant features. Notably, TFGDA enhances performance substantially on two hard transfer tasks, **C\(\)A** and **D\(\)A**,

[MISSING_PAGE_FAIL:9]

variant TFGDA-TR exhibits better intra-class compactness and inter-class separability in feature space, implying that the incorporation of topological structure information helps guide the discriminative clustering of unlabeled nodes, thereby promoting the learning of domain-invariant nodes features.

**4) Effect of RNC:** To show the effectiveness of our RNC strategy, we compare it with existing node clustering strategies, including conditional entropy minimization strategy (**CEM**)  and the recently proposed re-weighted pseudo-labeling strategy (**RPL**) . We investigate the trend of Micro-F1 score during training on tasks **A\(\)C** and **A\(\)D**. The curves in Figure 3 reflect the following observations: **(1)** TFGDA-R converges more smoothly and quickly, achieving higher transfer performance, which suggests that RNC strategy can effectively accelerate the learning of domain-invariant features. **(2)** Compared to TFGDA-S (baseline), TFGDA-S + CEM suffers from severe performance degradation, as CEM may enforce over-confident probability on some misclassified unlabeled nodes. **(3)** RPL strategy fails to achieve satisfactory performance as it's sensitive to pseudo-label noise.

**5) Effect of Label Rate:** To verify the model's robustness under different label scarcity settings, we evaluate the performance of different methods on tasks **A\(\)C** and **A\(\)D**, using the following label rates for the source graph: 1%, 5%, 7%, 9%, and 10% respectively, as shown in Figure 4. It can be observed that our TFGDA significantly outperforms other competitors, even in the most challenging environment of 1% label rate, indicating the superiority of TFGDA in capturing transferable features.

## 6 Conclusion

In this paper, we develop a novel model named TFGDA for SGDA. Specially, we propose a STSA strategy to encode critical structure information into latent space, significantly improving model's transfer performance. Moreover, to stably reduce domain discrepancy, the SDA strategy is introduced to align features distributions on sphere. We also devise the RNC strategy to guide the clustering of unlabeled nodes to address the overfitting issue, greatly enhancing the model's robustness. Comprehensive experiments and analysis verify the superiority of our TFGDA.

Figure 3: The trend of Micro-F1 during training. Figure 4: Performance with different label rates.

Figure 2: The t-SNE visualization of representations learned by SGDA, TFGDA and its two variants on **A\(\)C** task with 5% label rate. In all subfigures, the marks \(\) and \(\) represent the source domain and target domain, respectively. **Fig 2(a-d)** depict category alignment (Different colors denotes different classes). **Fig 2(e-h)** depict domain alignment (Red: Source domain; Blue: Target domain).

Acknowledgements

This work was supported in part by the National Natural Science Foundation of China under Grants (No.62302098, No.62401355), and the Start-up Program for New Young Teacher of Shanghai Jiao Tong University (KJ3-0221-22-6349).