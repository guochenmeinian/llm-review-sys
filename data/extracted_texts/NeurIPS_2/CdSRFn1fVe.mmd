# Smooth, exact rotational symmetrization

for deep learning on point clouds

Sergey N. Pozdnyakov and Michele Ceriotti

Laboratory of Computational Science and Modelling,

Institute of Materials, Ecole Polytechnique Federale de Lausanne,

Lausanne 1015, Switzerland

sergey.pozdnyakov@epfl.ch, michele.ceriotti@epfl.ch

###### Abstract

Point clouds are versatile representations of 3D objects and have found widespread application in science and engineering. Many successful deep-learning models have been proposed that use them as input. The domain of chemical and materials modeling is especially challenging because exact compliance with physical constraints is highly desirable for a model to be usable in practice. These constraints include smoothness and invariance with respect to translations, rotations, and permutations of identical atoms. If these requirements are not rigorously fulfilled, atomistic simulations might lead to absurd outcomes even if the model has excellent accuracy. Consequently, dedicated architectures, which achieve invariance by restricting their design space, have been developed. General-purpose point-cloud models are more varied but often disregard rotational symmetry. We propose a general symmetrization method that adds rotational equivariance to any given model while preserving all the other requirements. Our approach simplifies the development of better atomic-scale machine-learning schemes by relaxing the constraints on the design space and making it possible to incorporate ideas that proved effective in other domains. We demonstrate this idea by introducing the Point Edge Transformer (PET) architecture, which is not intrinsically equivariant but achieves state-of-the-art performance on several benchmark datasets of molecules and solids. A-posteriori application of our general protocol makes PET exactly equivariant, with minimal changes to its accuracy.

## 1 Introduction

Contrary to 2D images that are well-described by regular and dense pixel grids, 3D objects usually have non-uniform resolution and are represented more naturally by an irregular arrangement of points in 3D. The resulting _point clouds_ are widely used in many domains, including autonomous driving, augmented reality, and robotics, as well as in chemistry and materials modeling, and are the input of a variety of dedicated deep-learning techniques. Whenever they are used for applications to the physical sciences, it is desirable to ensure that the model is consistent with fundamental physical constraints: invariance to translations, rotations, and permutation of identical particles, as well as smoothness with respect to geometric deformations. In the case of atomistic modeling, the application domain we focus on here, _exact_ compliance with these requirements is highly sought after. Lack of smoothness or symmetry breaking are common problems of conventional atomistic modeling techniques. These have been shown in the past to lead to artifacts ranging from numerical instabilities (which are particularly critical in the context of high-throughput calculations) to qualitatively incorrect and even absurd simulation outcomes.

These concerns have led to the development of dedicated models for atomistic simulations that rigorously incorporate all these constraints [12; 13; 14; 15] by using only symmetry preserving operations. These restrictions limit the design space of these models, for example, leading to the lack of universal-approximation property in many popular methods, as we discuss in Section 2. Furthermore, symmetry requirements prevented the application of models developed in other domains to atomistic simulations. As an illustrative example, one can mention PointNet++, an iconic architecture for generic point clouds. This model is not rotationally invariant. Therefore, even though it might have excellent accuracy, it has never been applied to atomistic simulations to the best of our knowledge. It is essential to distinguish between exact, rigorous equivariance and an approximate one, which any model can learn by rotational augmentations. Due to the subtle nature of possible artifacts, an approximate equivariance is considered insufficient.

Out of the mentioned symmetry constraints, the only challenging one is rotational equivariance. As we discuss in Section 4, all the other requirements are either already fulfilled in most functional forms used by existing models for generic point clouds or can be enforced with trivial modifications.

In this paper, we introduce a general symmetrization protocol that enforces exact rotational invariance _a posteriori_, for an arbitrary backbone architecture, without affecting its behavior with respect to all the other requirements. It eliminates the wall between communities, making most of the developments for generic point clouds applicable to atomistic simulations. Furthermore, it simplifies the development of new, more efficient architectures by removing the burden of incorporating the exact rotational invariance into their functional form. As an illustration, we design a model named Point Edge Transformer (PET) that achieves state-of-the-art performance on several benchmarks ranging from high-energy CH\({}_{4}\) configurations to a diverse collection of small organic molecules to the challenging case of periodic crystals with dozens of different atomic species. Being not intrinsically rotationally equivariant, PET has benefited from the enlarged design space. Our symmetrization method a posteriori makes it rigorously equivariant and, thus, applicable to atomistic simulations.

## 2 Equivariant models and atomic-scale applications

Models used in chemical and materials modeling achieve rotational invariance by two main mechanisms. The first involves using only invariant internal coordinates such as interatomic distances, angles, or even dihedral angles. The second involves using equivariant hidden representations that transform in a predictable manner under symmetry operations. For example, some intermediate activations in a neural network can be expressed as vectors that rotate together with the input point cloud. This approach restricts the design space to only such functional forms that preserves the equivariance.

Local decomposition.Models in atomistic machine learning often rely on the prediction of local properties associated with atom-centered environments \(A_{i}\), either because they are physical observables (e.g., NMR chemical shieldings[19; 20]) or because they provide a decomposition of a global extensive property, such as the energy, into a sum of atomic contributions, \(y=_{i}y(A_{i})\). Here, \(y(A_{i})\) indicates a learnable function of the environment of the \(i-\)th atom, defined as all the neighbors within the cutoff radius \(R_{}\). This local decomposition is rooted in physical considerations and is usually beneficial for the transferability of the model.

Local invariant descriptors.The classical approach to construct approximants for \(y(A_{i})\) is to first compute smooth, invariant descriptors for each environment[22; 23; 24; 25; 26; 27; 14] and then feed them to a conventional machine learning model. Such models can be 1) linear regression, 2) kernel regression, or 3) feed-forward neural network with a smooth activation function..

Distance-based message-passing.Another popular method involves constructing a molecular graph and feeding it to a Graph Neural Network (GNN). Early, and very popular, models rely on invariant two-body messages based only on interatomic distances[30; 31]. In this case, a molecular graph is constructed by representing all the atoms by nodes, drawing edges between all the atoms within a certain cutoff radius, and decorating edges with the Euclidean distance between the corresponding atoms.

Recently, it was discovered that such models are not universal approximators. Specifically, they cannot distinguish between certain atomic configurations. This issue can be directly attributed to the restricted design space of rotationally equivariant models. In an enlarged design space, one could decorate the edges of a molecular graph with x, y, and z components of the corresponding displacement vectors. This would immediately ensure the universal approximality of the model. However, the necessity to enforce rotational equivariance of the predictions dictates decorating the edges of a molecular graph only with rotationally invariant Euclidean distances. Such a restriction severely limits the expressiveness of the corresponding models.

**Higher-order message-passing.** Subsequently, more expressive models were developed. For example, some of them use angles, or even dihedrals, in addition to distances between the atoms. Others, such as Tensor Field Network, Nequip, MACE, SE(3) transformers, employ SO(3) algebra to maintain equivariance of hidden representations. These models solve the incompleteness issue of simple atom-centered geometric descriptors and distance-based GNNs. The quality of a model, however, doesn't reduce to the simple presence of a universal approximation behavior. An extended design space can still be beneficial to obtain more accurate or efficient models.

## 3 Models for generic point clouds

Point clouds have found many applications beyond atomistic modeling. For example, detectors such as LiDARs represent the scans of the surrounding world as collections of points. Multiple methods have been developed for such domains. Contrary to atomistic machine learning, there are no such strict symmetry requirements for practical applications. Consequently, most models do not exactly incorporate rotational equivariance and rely instead on rotational augmentations.

Many successful models based on 2D projections of the cloud have been proposed in computer vision [38; 39; 40; 41]. Here, we focus on explictly 3D models, that are most relevant for chemical applications.

**Voxel-based methods.** The complete 3D geometric information for a structure can be encoded in a permutation-invariant manner by projecting the point cloud onto a regular 3D voxel grid, and further manipulated by applying three-dimensional convolutions. The computational cost of a naive approach, however, scales cubically with the resolution, leading to the development of several schemes to exploit the sparsity of the voxel population[42; 43; 44], which is especially pronounced for high resolutions.

**Point-based methods.** By extending the convolution operator to an irregular grid, one can avoid the definition of a fixed grid of voxels. For instance[45; 46], one can evaluate an expression such as

\[(*g)()_{m}=_{i}_{n}^{N_{in}}g_{mn}(_{i} -)f_{n}^{i}, \]

where, \(f_{n}^{i}\) is the \(n\)-th feature of the point \(i\), and \(g\) is a collection of \(N_{in}N_{out}\) learnable three-dimensional functions, where \(N_{in}\) and \(N_{out}\) are the numbers of input and output features respectively. The output features \((*g)()_{m}\) can be evaluated at an arbitrary point \(\), allowing for complete freedom in the construction of the grid. PointNet is a paradigmatic example of an architecture that eliminates the distinction between point features and positions. The core idea of these approaches is that an expression such as \(f(_{1},_{2},...,_{n})=(_{i}(\{h( _{i})\}))\), where \(\) and \(h\) are learnable functions, can approximate to arbitrary precision any continuous function of the point set \(\{_{i}\}\). PointNet++ and many other models[46; 48; 49] apply similar functional form in a hierarchical manner to extract features of local neighborhoods. Many of these methods have graph-convolution or message-passing forms, similar to those discussed in Section 2, even though they do not enforce invariance or equivariance of the representation. Many transformer models for point clouds have also been proposed[50; 51; 52; 53], that incorporate attention mechanisms at different points of their architecture.

## 4 Everything but rotational equivariance

The generic point-cloud models discussed in the previous section do not incorporate all of the requirements (permutation, translation and rotation symmetry as well as smoothness) that are required by applications to atomistic simulations. Most models, however, do incorporate everything but rotational invariance, or can be made to with relatively small modifications. For instance, translational invariance can be enforced by defining the grids or the position of the points in a coordinate system that is relative to a reference point that is rigidly attached to the object.

Another common problem of many of these methods is the lack of smoothness. However, most architectures can be made differentiable with relative ease, with the exception of very few operationssuch as downsampling via farthest point sampling. For example, derivative discontinuities associated with non-smooth activation functions, such as ReLU, can be eliminated simply by using a smooth activation functions instead[55; 56; 57; 58]. A slightly less trivial problem arises for models using the convolutional operator of Eq. (1), that introduces discontinuities related to (dis)appearance of new points at the cutoff sphere, even for smooth functions \(g_{mn}\). These discontinuities can be removed by modifying the convolutional operator as:

\[(*g)()_{m}=_{i}_{n}^{N_{in}}g_{mn}(_{i} -)f_{n}^{i}f_{}(\|_{i}-\|\|R_{},_{R_{}}), \]

where the cutoff function \(f_{}\), illustrated in Fig. 5c, can be chosen as an infinitely differentiable switching function that smoothly zeroes out contributions from the points approaching the cutoff sphere, and the parameter \(_{R_{}}>0\) controls how fast it converges to \(1\) for \(r<R_{}\). A similar strategy can be applied to PointNet-like methods. 3D CNNs on regular grids that use smooth activation and pooling layers such as sum or average operations are also smooth functions of the voxel features. Including also a smooth projection of the point cloud on the voxel grid suffices to make the overall methods smooth. We further discuss in the Appendix G examples of such smooth projections, along with other modifications that can be applied to make the general point-cloud architectures discussed in Sec. 3 differentiable. Finally, most - but not all - of the generic models for point clouds are invariant to permutations.

In summary, there is a wealth of point-cloud architectures that have proven to be very successful in geometric regression tasks, and are, or can be easily made, smooth, permutation and translation invariant. The main obstacle that hinders their application to tasks that require a fully-symmetric behavior, such as those that are common in atomistic machine learning, is invariance or covariance under rigid rotations. The ECSE protocol presented in the next section eliminates this barrier.

## 5 Equivariant Coordinate System Ensemble

Our construction accepts a smooth, permutationally, translationally, but not necessarily rotationally invariant backbone architecture along with a point that is rigidly attached to the point cloud, and produces an architecture satisfying all four requirements. While in principle, there are many choices of the reference point, we will focus for simplicity on the case in which it is one of the nodes in the point cloud. More specifically, we formulate our method in the context of a local decomposition of the (possibly tensorial) target \((A)=_{i}(A_{i})\) where the reference point is given by the position of the central atom \(_{i}\), and the model estimates an atomic contribution \((A_{i})\) given the local neighborhood. We name our approach Equivariant Coordinate System Ensemble (ECSE, pron. e[e]).

Local coordinate systems.A simple idea to obtain a rotationally equivariant model would be to define a coordinate system rigidly attached to an atomic environment. Next, one can express Cartesian coordinates of all the displacement vectors from the central atom to all the neighbors and feed them to any, possibly not rotationally invariant, backbone architecture. Since the reference axes rotate together with the atomic environment, the corresponding projections of the displacement vectors are invariant with respect to rotations, and so is the final prediction of the model.

This approach can be applied easily to rigid molecules[60; 61], but in the general case it is very difficult to define a coordinate system in a way that preserves smoothness to atomic deformations. For example, the earliest version of the DeepMD framework used a coordinate system defined by the central atom and the two closest neighbors. Smooth distortions of the environment can change the selected neighbors, leading to a discontinuous change of the coordinate systems and therefore to discontinuities in the predictions of the model. For this reason, later versions of DeepMD switched to descriptors that can be seen as a close relative of Behler-Parrinello symmetry functions, which guarantee smoothness and invariance.

Ensemble of coordinate systems.The basic formulation of our symmetrization protocol, illustrated in Fig. 5a, is simple: using _all_ the possible coordinate systems defined by all pairs of neighbors instead of singling one out, and averaging the predictions of a non-equivariant model over this ensemble of reference frames:

\[_{}(A_{i})=_{jj^{} A_{i}}w_{jj^{}}_{jj^{}}[_{0}(_{jj^{}}^{-1}[A_{i}])]_ {jj^{} A_{i}}w_{jj^{}}, \]where \(_{}\) indicates the symmetrized model, \(_{0}\) the initial (non-equivariant) backbone architecture, and \(_{jj^{}}[]\) indicates the rotation operator for the coordinate system defined by the neighbors \(j\) and \(j^{}\) within the \(i\)-centered environment \(A_{i}\). In other words, \(_{jj^{}}^{-1}[A_{i}]\) is an atomic environment expressed in the coordinate system defined by neighbors \(j\) and \(j^{}\). The summation is performed over all the ordered pairs of neighbors within a cutoff sphere with some cutoff radius \(R_{c}\).

Given a pair of neighbors with displacement vectors from central atom \(_{ij}\) and \(_{ij^{}}\) the corresponding coordinate system consists of the vectors \(_{ij}\), \(_{ij}_{ij^{}}\), and \(_{ij}[_{ij}_{ij^{}}]\). If the model predicts a vectorial (or tensorial) output, it is rotated back into the original coordinate system by an outer application of operator \(_{jj^{}}\). Thus, our symmetrization scheme allows for getting not only invariant predictions, but also covariant ones, such as vectorial dipole moments. It is worth to note that the idea of using all the possible coordinate systems has also been used to define a polynomially-computable invariant metric to measure the similarity between crystal structures, and to construct provably complete invariant density-correlation descriptors .

Fig. 5b depicts the necessity to use the weighted average with weights \(w_{jj^{}}\) instead of a plain one. The use of plain average in Eq. 3 would lead to the lack of smoothness. Indeed, if a new atom enters the cutoff sphere, it immediately yields new terms into the summation in eq. 3, which would lead to a discontinuous gap in predictions. Furthermore, if two neighbors and a central atom appear to be collinear, the associated coordinate system is ill-defined.

Both issues can be solved by introducing a mechanism in which each coordinate system is assigned a different weight, depending on the positions \((_{ij},_{ij^{}})\) of the neighbors that define the reference frame:

\[w_{jj^{}}=w(_{ij},_{ij^{}})=f_{}(r_{ ij}|R_{},_{R_{}})f_{}(r_{ij^{}}|R_{}, _{R_{}})q_{}(|_{ij}_{ij^{}}|^ {2}|,_{}), \]

where the cutoff functons \(f_{}\) and \(q_{}\) are illustrated in Fig. 5c. Thanks to the presence of \(f_{}\), the terms in Eq. (3) that are associated with atoms entering or exiting the cutoff sphere have zero weights. Similarly, \(q_{}\) ensures that pairs of neighbors that are nearly collinear do not contribute to the symmetrized prediction. One last potential issue is the behavior when _all_ pairs of neighbors are

Figure 1: (a) Equivariant coordinate-system ensemble: Each ordered pair of neighbors defines a local coordinate system. Next, an atomic environment is projected on all of them (which is equivalent to rotation) and used as input for a backbone architecture. If outputs are covariant, such as vectors, they are rotated back to the initial coordinate system. Finally, predictions are averaged over. (b) Discontinuities related to plain average. The weighted average with weights \(w_{jj^{}}\) resolves these problems. (c) Cutoff functions \(f_{}\) and \(q_{}\) used to define weights \(w_{jj^{}}\) (d) To reduce the computational cost, an adaptive cutoff \(R_{}\) is used, which adjusts to a given geometry instead of being a global user-specified constant.

(nearly) collinear, which would make Eq. (3) ill-defined, falling to \(\) ambiguity. We discuss in the Appendix F.3 two possible solutions for this corner case.

Adaptive cutoff.To make the ECSE protocol practically feasible, it is necessary to limit the number of evaluations of the non-equivariant model, given that the naive number of evaluations grows quadratically with the number of neighbors. An obvious consideration is that there is no reason why the cutoff radius used by ECSE should be the same as the one used by the backbone architecture. Thus, one can achieve significant computational savings by simply defining a smaller cutoff for symmetrization. However, particularly for inhomogeneous point cloud distributions, a large cutoff may still be needed to ensure that all environments have at least one well-defined coordinate system. For this reason, we introduce an adaptive inner cutoff \(R_{}(A_{i})\), which is determined separately for each atomic environment \(A_{i}\) instead of being a global, user-specified constant. Eq. (3) requires that at least one pair of neighbors is inside the cutoff sphere. Simultaneously, it is desirable to make it as small as possible for computational efficiency. Thus, it makes sense to define \(R_{}(A_{i})\) along the lines of the distance from the central atom to the second closest neighbor. It should be larger, but not much larger, than the second-nearest-neighbor distance.

Our definition of \(R_{}(A_{i})\), given in the Appendix F.4, is inspired by this simple consideration, and has the following properties: (1) Contrary to the naive second-neighbor distance, \(R_{}(A_{i})\) depends smoothly on the positions of all the atoms. (2) It encompasses at least one non-collinear pair of neighbors, which yields at least one well-defined coordinate system: the functional form is chosen so that if \(k\) nearest neighbors are collinear, \(R_{}(A_{i})\) is expanded to enclose the \(k+1\)-th for any \(k\).

With such an adaptive cutoff, only a few pairs of neighbors are used to construct a coordinate system. Thus, this construction greatly reduce the cost of evaluating the ECSE-symmetrized model.

Training and symmetrization.The most straightforward way to train a model using the ECSE protocol is to apply it from the very beginning and train a model that is overall rotationally equivariant. This approach, however, increases the computational cost of training, since applying ECSE entails multiple evaluations of the backbone architecture. An alternative approach, which we follow in this work, is to train the backbone architecture with rotational augmentations, and apply ECSE only for inference. Given that ECSE evaluates the weighted average of the predictions of the backbone architecture, this increases the cost of inference, but may also increase the accuracy of the backbone architecture, playing the role of test augmentation.

Message-passing.Message-passing schemes can be regarded as local models with a cutoff radius given by the receptive field of the GNN, and therefore ECSE can be applied transparently to them. In a naive implementation, however, the same message would have to be computed for the coordinate systems of all atoms within the receptive field. This implies a very large overhead, even though the asymptotic cost remains linear with system size. An alternative approach involves symmetrizing the outgoing messages with the ECSE protocol. This requires a deeper integration within the model, that also changes the nature of the message-passing step, given that one has to specify a transformation rule to be applied to messages computed in different coordinate systems. We discuss in the Appendix F.10 the implications for the training strategy, and some possible workarounds.

Related work.There are several approaches, such as the earliest version of the DeepMD framework, discussed above, that use local coordinate systems to enforce rotational equivariance for any backbone architecture. The frame averaging (FA) framework  proposes to use the eigenvectors of the centered covariance matrix to define local coordinate systems, which leads to discontinuities every time the eigenvalues coincide with each other. To the best of our knowledge, ECSE is the first general symmetrization method allowing to enforce rotational equivariance while preserving smoothness, which is a highly desirable property for atomistic simulations.

Finally, a completely different approach is introduced by the vector neurons framework that converts arbitrary backbone architecture into an equivariant one by enforcing all hidden representations in the model to transform as vectors with respect to rotations.

## 6 Point Edge Transformer

An obvious application of the ECSE protocol would be to enforce equivariance on some of the point-cloud architectures discussed in Section 3, making them directly-applicable to atomistic modeling and to any other domain with strict symmetry requirements. Here we want instead to focus on a less obvious, but perhaps more significant, application: demonstrating that lifting the design constraint of rotational equivariance makes it possible to construct better models. To this end, we introduce a new architecture that we name Point Edge Transformer (PET), which is built around a transformer that processes edge features and achieves state-of-the-art accuracy on several datasets that cover multiple subdomains of atomistic ML. Due to the focus on edge features, PET shares some superficial similarities with the Edge Transformer developed for natural language processing, Allegro (a strictly local invariant interatomic potential that loosely resembles a single message-passing block of PET) as well as with early attempts by Behler et al. applying MLPs to invariant edge features.

The PET architecture is illustrated in Fig. 2. More details are given in Appendix A. The core component of each message-passing block is a permutationally-equivariant transformer that takes a set of tokens of size \(d_{}\) associated with the central atom and all its neighbors, and generates new tokens that are used both to make predictions and as output messages. The transformer we use in PET is a straightforward implementation of the classical one, with a modification to the attention mechanism that ensures smoothness with respect to (dis)appearance of the neighbors at the

Figure 2: Architecture of the Point-Edge Transformer (PET). White and colored boxes represent layers; gray boxes and lines represent data. (a) PET is a message-passing architecture. At each of the \(n_{}\) message-passing (MP) interactions, messages are communicated between all the pairs of atoms closer than a certain cutoff distance \(R_{c}\). At each stage, the corresponding MP block computes output messages and predictions of the target property given the input messages and geometry of the point cloud. (b) For each atom in the system, we define atom-centered environment \(A_{i}\) as a collection of all the neighbors within the cutoff distance \(R_{c}\). The MP block is applied to each such atomic environment. Given 1) the geometry of the atomic environment, 2) the chemical species of the atoms, and 3) input messages from all the neighbors to the central atom it produces output messages from the central atom to all the neighbors and contribution to the prediction of the target property. The first step is to encode all the information associated with each neighbor to an abstract token of dimensionality \(d_{}\). Next, the collection of such tokens (with the one associated with the central atom) is fed into the transformer with \(n_{}\) self-attention layers. The transformer does permutationally covariant transformation. Thus, the association between the tokens and neighbors is preserved. Therefore, we can simply treat output tokens as output messages to the corresponding neighbors. (c) The Encoder layer first maps all the sources of information into dimensionality \(d_{}\). Next, all 3 tokens are concatenated and compressed into a single one of the desired size.

cutoff radius. The attention coefficients \(_{ij}\) determining the contribution from token \(j\) to token \(i\) are modified as \(_{ij}_{ij}f_{c}(r_{j}|R_{c},_{R_{c}})\), and then renormalized.

One of the key features of PET is that it operates with features associated with each _edge_, at variance with other deep learning architectures for point clouds that mostly operate with vertex features. Whereas the calculation of new vertex features by typical GNNs involves aggregation over the neighbors, the use of edge features allows for the construction of an _aggregation-free_ message-passing scheme, avoiding the risk of introducing an information bottleneck.

The transformer itself can be treated as a GNN, thus our architecture can be seen as performing _localized message passing_, increasing the complexity of local interactions it can describe, while avoiding an over-increase of the receptive field. The latter is undesirable because it makes parallel computation less efficient, and thus hinders the application of such models to large-scale molecular dynamics simulations.

Since transformers are known to be universal approximators, so is _each_ of the message-passing blocks used in PET.

**Limitations.** As discussed in Section 5, the most efficient application of the ECSE protocol on top of a GNN requires re-designing the message-passing mechanism. For the current moment, our proof-of-principle implementation follows a simpler approach (see more details in Appendix F.11) and favors ease of implementation instead of computational efficiency (e.g., all weights and most of the intermediate values used in the ECSE scheme are stored in separate zero-dimensional PyTorch tensors). This leads to a significant (about 3 orders of magnitude) overhead over the inference of the backbone architecture. The base PET model, however, gains efficiency by operating directly on the Cartesian coordinates of neighbors. Even with this inefficient implementation of ECSE, exactly equivariant PET inference is much cheaper than the reference first-principles calculations.

## 7 Benchmarks

We benchmark PET and the ECSE scheme over six different datasets, which have been previously used in the literature and which allow us to showcase the performance of our framework, and the ease with which it can be adapted to different use cases. The main results, are compared with the state of the art in Figure 3 and Table 1, while in-depth analyses can be found in the Appendix C.

As a first benchmark, we conduct several experiments with the liquid-water configurations from Ref. . This dataset is representative of those used in the construction of interatomic potentials, and presents interesting challenges in that it contains distorted structures from path integral molecular dynamics and involves long-range contributions from dipolar electrostatic interactions. One of the key features of PET is the possibility to increase the expressive power by either adding MP blocks or by making transformers in each block deeper. Panel (a) in Fig. 3 shows that increasing the number of GNN blocks improves the accuracy of PET, and that for a given number of blocks, a shallow transformer with a single layer performs considerably worse than one with two or more layers. Given that stacking multiple GNN blocks increases both the receptive field and the flexibility in describing local interactions, it is interesting to look at the trend for a fixed total number of transformer layers (Fig. 3b), that shows that a \(6 2\) model outperforms a large \(12 1\) stack of shallow transformers, even though the latter has additional flexibility because of the larger number of heads and pre-processing units. With the exception of the shallower models, PET reduces the error over the state-of-the-art equivariant model NEQUIP by \( 30\%\). In problems that are less dependent on long-range physics, it may be beneficial to increase the depth of transformers and reduce the number of MP blocks. The accuracy of the model can also be further improved by extending \(R_{}\) beyond 3.7A (a value we chose to ensure approximately 20 neighbors on average), reaching a force MAE of 14.4 meV/A when using a cutoff of 4.25A.

We then move to the realm of small molecules with the COLL dataset, that contains distorted configurations of molecules undergoing a collision. COLL has been used extensively as a benchmark for chemical ML models, in particular for GNNs that employ information on angles and dihedrals to concile rotaional invariance and universal approximation. PET improves by \(\)13% the error on forces (the main optimization target in previous studies) while reducing by a factor of 4 the error on atomization energies relative to the respective state of the art. In order to assess the accuracy of PET for extreme distortions, and a very wide energy range, we consider the database of CHconfigurations first introduced in Ref. . This dataset contain random arrangements of one C and 4 H atoms, that are only filtered to eliminate close contacts. The presence of structures close to those defying C-centered angle-distances descriptors adds to the challenge of this dataset. The learning curves for PET in Figure 3c and d demonstrate the steep, monotonic improvement of performance for increasing train set size, surpassing after a few thousand training points the best existing models (a linear many-body potential for the energy-only training and REANN, an angle-based GNN for combined energy/gradient training).

The PET performs well even for condensed-phase datasets that contain dozens of different atomic types. The HEA dataset contains distorted crystalline structures with up to 25 transition metals _simultaneously_. Ref.  performs an explicit compression of chemical space, leading to a model that is both interpretable and very stable. The tokenization of the atomic species in the encoder layers of PET can also be seen as a form of compression. However, the more flexible form of the model compared to HEA25-4-NN allows for a a 3(5)-fold reduction of force(energy) hold-out errors. The model, however, loses somewhat in transferability: in the extrapolative test on high-temperature MD trajectories suggested in Ref. , PET performs less well than HEA25-4-NN (152 vs 48 meV/at.

   dataset &  &  &  &  \\ metric & MAE \(f\) & MAE \(E\) & RMSE \(f\) & RMSE \(E\)/at. & MAE \(|f|\) & MAE \(E\)/at. & MAE \(f\) & MAE \(E\)/at. \\  SOTA & 26.4 & 47 & 125 & 1.11 & 138 & 15.7 & 190 & 10 & 10 \\ model & GemNet & DimeNet++ &  &  &  \\  PET (\(y_{0}\)) & 23.1 & 12.0 & 22.7 & 0.312 & 140.5 \(\) 2.0 & 17.8 \(\) 0.1 & 60.2 & 1.87 \\ PET (\(y_{8}\)) & 23.1 & 11.9 & 22.7 & 0.304 & 141.6 \(\) 1.9 & 17.8 \(\) 0.1 & 60.1 & 1.87 \\ PET (ens.) & & & & & 128.5 & 16.8 & & \\   

Table 1: Comparison of the accuracy of PET and current state-of-the-art models for the COLL, MnO, HM21 and HEA data sets. A more comprehensive comparison with leading models is provided in the Appendix C. Energy errors are given in meV/atom, force errors in meV/Ã…. In all cases the PET model is nearly equivariant, and the difference between the accuracy of \(y_{0}\) and \(y_{8}\) is minuscule. For HME21, we report error bars over 5 random seeds, and results for an ensemble of symmetrized models.

Figure 3: (a-c) Accuracy of PET potentials (\(y_{0}\)) of liquid water, compared with NEQUIP. (a) Accuracy for different numbers of message-passing blocks \(n_{}\) and transformer layers \(n_{}\); (b) Accuracy as a function of \(n_{}\), for constant \(n_{} n_{}=12\).; (c) Accuracy as a function of cutoff. (d-f) Learning curves for different molecular data sets, comparing symmetrized PET models (\(y_{8}\)) with several previous works, including the current state of the art. (d) Random CH\({}_{4}\) dataset, training only on energies; (e) Random CH\({}_{4}\) dataset, training on energies and forces; (f) Vectorial dipole moments in the QM9 dataset.

MAE at 5000 K) even though room-temperature MD trajectories are better described by PET. The case of the HME21 dataset, which contains high-temperature molecular-dynamics configurations for structures with a diverse composition, is also very interesting. PET outperforms most of the existing equivariant models, except for MACE, which incorporates a carefully designed set of physical priors, inheriting much of the robustness of shallow models. PET, however, comes very close: the simple regularizing effect of a 5-models PET ensemble is sufficient to tip the balance, bringing the force error to 128.5 meV/at. Another case in which PET performs well, but not as well as the state of the art, is for the prediction of the atomization energy of molecules in the QM9 dataset. PET achieves a MAE of 6.7 meV/molecule, in line with the accuracy of DimeNet++, but not as good as the 4.3 meV/molecule MAE achieved by Wigner kernels.

Finally, we consider two cases that allow us to showcase the extension of PET beyond the prediction of the cohesive energy of molecules and solids. The MnO dataset of Eckhoff and Behler includes information on the colinear spin of individual atomic sites, and demonstrates the inclusion of information beyond the chemical nature of the elements in the description of the atomic environments. The improvement with respect to the original model is quite dramatic (Tab. 1). Finally, the QM9 dipole dataset allows us to demonstrate how to extend the ECSE to targets that are covariant, rather than invariant. This is as simple as predicting the Cartesian components of the dipole in each local coordinate system, and then applying to the prediction the inverse of the transformation that is applied to align the local coordinate system (cf. Eq. (3)). The accuracy of the model matches that of a recently-developed many-body kernel regression scheme for small dataset size, and outperforms it by up to 30% at the largest train set size.

## 8 Discussion

In this work, we introduce ECSE, a general method that enforces rotational equivariance for any backbone architecture while preserving smoothness and invariance with respect to translations and permutations. To demonstrate its usage, we also develop the PET model, a deep-learning architecture that is not intrinsically rotationally invariant but achieves state-of-the-art results on several datasets across multiple domains of atomistic machine learning. The application of ECSE makes the model comply with all the physical constraints required for atomistic modeling.

We believe our findings to be important for several reasons. On the one hand, they facilitate the application of existing models from geometric deep learning to domains where exact equivariance is a necessary condition for practical applications. On the other, they challenge the notion that equivariance is a necessary ingredient for an effective ML model of atomistic properties. The state-of-the-art performance of PET on several benchmarks reinforces early indications that non-equivariant models trained with rotational augmentation can outperform rigorously equivariant ones. Similar observations were also made independently by other groups. Spherical Channel Network and ForceNet achieve excellent performance on the Open Catalyst dataset while relaxing the exact equivariance of the model.

It is worthwhile to mention that in the other domains involving point clouds, the application of not invariant models fitted with rotational augmentations is a predominant approach. As an example, one can examine the models proposed for, e.g., the ModelNet40 dataset, a popular benchmark for point cloud classification. Even though the target is rotationally invariant, most of the developed architectures are not rigorously equivariant, in the exact sense, as discussed in Section. 2. This provides considerable empirical evidence that the use of not rigorously equivariant architectures with rotational augmentations might be more efficient.

The ECSE method we propose in our work allows to symmetrize _exactly_, and a-posteriori, any point-cloud model, making them suitable for all applications, such as atomistic modeling, which have traditionally relied on symmetry to avoid qualitative artifacts in simulations. This shall facilitate the translation of methodological advances from other domains to atomistic modeling and the implementation into generic point-cloud models of physically-inspired inductive biases (such as smoothness, range, and body order of interactions) that have this far been conflated only with a specific class of equivariant architectures.

Acknowledgements

We thank Marco Eckhoff and Jorg Behler for sharing the MnO dataset with the collinear spins and Artem Malakhov for useful discussions.

This work was supported by the Swiss Platform for Advanced Scientific Computing (PASC) and the NCCR MARVEL, funded by the Swiss National Science Foundation (SNSF, grant number 182892).