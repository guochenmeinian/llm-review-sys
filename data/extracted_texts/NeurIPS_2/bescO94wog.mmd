# Active Learning with LLMs for Partially Observed

and Cost-Aware Scenarios

 Nicolas Astorga, Tennison Liu, Nabeel Seedat & Mihaela van der Schaar

DAMTP, University of Cambridge

Cambridge, UK

nja46@cam.ac.uk

###### Abstract

Conducting experiments and collecting data for machine learning models is a complex and expensive endeavor, particularly when confronted with limited information. Typically, extensive _experiments_ to obtain features and labels come with a significant acquisition cost, making it impractical to carry out all of them. Therefore, it becomes crucial to strategically determine what to acquire to maximize the predictive performance while minimizing costs. To perform this task, existing data acquisition methods assume the availability of an initial dataset that is both fully-observed and labeled, crucially overlooking the _partial observability_ of features characteristic of many real-world scenarios. In response to this challenge, we present Partially Observable Cost-Aware Active-Learning (POCA), a new learning approach aimed at improving model generalization in data-scarce and data-costly scenarios through label and/or feature acquisition. Introducing \(\)POCA as an instantiation, we maximize the uncertainty reduction in the predictive model when obtaining labels and features, considering associated costs. \(\)POCA enhance traditional Active Learning metrics based solely on the observed features by generating the unobserved features through Generative Surrogate Models, particularly Large Language Models (LLMs). We empirically validate \(\)POCA across diverse tabular datasets, varying data availability, acquisition costs, and LLMs.

## 1 Introduction

In real-world machine learning (ML) applications, _fully-observed_, pristine training data is an exception rather than the norm. This challenge is especially evident during the initial stages of model development when training data is limited and varies in its informativeness across samples . At this stage, obtaining additional data is crucial for improving model generalization but is fraught with challenges . In particular, acquiring new data can be costly, often resulting in only essential features and labels being collected, leading to _partially observed_ features in training data. Therefore, it's vital that acquisition is efficient, yet it remains unclear which features and labels from each instance will ultimately prove essential. Furthermore, data sources themselves can also be _partially observed_, with different features available across samples, further complicating the acquisition process. These challenges emphasize the importance of a new problem we call _Partially Observable Cost-Aware Active-Learning (POCA)_ illustrated in Figure 1. Before its formalization in Section 2, we provide an intuitive overview:

_"In situations with limited labeled data and partial feature observations, our objective is to enhance the generalization capabilities of a predictive model by strategically collecting features and/or labels. This goal should account the cost associated with data collection, as well as the varying levels of informativeness of labels and features across different instances"_Addressing the POCA problem is vital when building systems with partial observation or relevant features are yet to be defined, particularly in fields like customer churn, monitoring, healthcare, and finance (see Appendix A). For example, developing a churn customer prediction system might start with some basic client information, such as demographics and income. However, to build such a system, additional features may be needed, which could be gathered through further customer interactions or surveys. At the outset, it's uncertain which specific features will prove essential, and acquiring additional information and relevant labels (e.g., churn events) necessary to refine the ML system involves costs related to money, time, or risks limiting the data acquisition in practice. From a practical perspective, we envision POCA to be useful in applications or fields where missing features exist, and also data acquisition techniques like Active Learning (AL) are necessary. Applications from different fields dealing with missing features and/or applying AL can be found in Table 2.

**Related work.** The most related data acquisition technique is AL . **AL** centers around enhancing model generalization through the acquisition of _only_ additional labels. It operates under the assumption of having access to an initial small, fully observed training set (referred to as the historical labeled set) and seeks to acquire additional labels for samples from an unlabeled dataset (referred to as the pool set). This set is assumed to be _fully observed_ in features, missing only labels. The distinctions between POCA and AL are illustrated in Figure 1. Tangentially, Active Feature Acquisition (AFA) methods  have been proposed to enhance the prediction of individual samples at test time--where the sample is partially observed. Like AL, it assumes a fully-observed historical labeled set, on which a model has already been trained. Given the trained model, the task then becomes identifying the most relevant unobserved features to acquire for partially observed instances at test time. We emphasize that AFA's primary focus is on optimizing feature acquisition for individual test samples, differing from our broader goal of data collection to enhance model training.

**Towards an Instantiation of POCA.** Given this problem definition, it is natural to wonder whether traditional AL metrics can be employed straightforwardly in the POCA setting. These metrics are usually derived from a predictive model that typically operates with fixed-size inputs. Consequently, predictions on partially observed instances can adversely affect the accuracy of AL metric estimations, leading to acquiring poor quality samples . To overcome this challenge, we incorporate Generative Surrogate Models (GSM) to impute missing features in partially observed inputs, facilitating a more precise estimation of AL metrics. The effectiveness of GSM hinges on its ability to discern feature interrelations from available but unlabeled data. This task is particularly challenging due to the varying degrees of missingness in the instances and the constraints of limited sample sizes. To address these complexities, we employ Large Language Models (LLMs) to instantiate GSMs, utilizing their generation ability based on arbitrary conditioning and strong sample efficiency, allowing robust imputations to support the estimation of AL metrics under partial observability 

_Uncertainty POCA._ We term this instantiation _uncertainty_ POCA (\(\)POCA), due to its connection with Bayesian Experimental Design , and its application in Bayesian Active Learning (BAL) and Bayesian Optimization (BO). From a Bayesian perspective, \(\)POCA maximizes the expected information gain or also known as expected uncertainty reduction, in the model's hypothesis resulting from an experiment. More specifically, \(\)POCA extends the concepts of expected information gain in the model's parameters (EIG) and expected predictive information gain (EPIG) to partially observed scenarios, introducing PO-EIG and PO-EPIG, respectively . Here, these methodologies maximize the expected uncertainty reduction when acquiring labels and a subset of features. Since the impact of unacquired features cannot be directly assessed, GSMs facilitate the computation of these metrics.

In summary, we make the following contributions:

Figure 1: **Overview of data acquisition methods.** POCA acquires features and/or labels from a partially-observed pool incorporating them into a partially-observed training set. In contrast, AL targets label acquisition assuming a fully-observed pool set and training set.

1 We address the unexplored challenge of costly data acquisition to enhance model generalization in partially observed scenarios. This leads us to introduce and formalize POCA, a novel ML paradigm for the acquisition of features and/or labels in the partially observed setting.

2 We propose \(\)POCA, a cost-aware Bayesian instantiation of POCA that maximizes the uncertainty reduction when acquiring data. \(\)POCA extends traditional AL metrics by imputing partially observed instances using GSMs. We theoretically show that the uncertainty reduction is larger than using vanilla AL metrics.

3 We propose the use of LLMs as a specific instance of GSMs, designed to address challenges in partially observed scenarios, including data efficiency, arbitrary information conditioning, and handling both categorical and numerical feature values.

4 We empirically demonstrate \(\)POCA outperforms standard active learning on a variety of partially observability scenarios spanning datasets, sample availability, and acquisition metrics--highlighting the usefulness and applicability of \(\)POCA.

## 2 POCA: Partially Observable Cost-Aware Active-Learning

**Preliminaries.** Partially Observable Cost-Aware Active-Learning is a data acquisition problem that focuses on improving the predictive performance of \(p_{o}(y|)\) in the supervised setting, with \(\) the models we can employ. We denote \(}\) and \(y\) as instances of observed features and target, alongside the respective random variables (RV) \(\) and \(Y\). Bold variables, expressed as \(=\{x_{j}\}_{j=1}^{J}\), represent a set of variables, in this case, features indexed by \(j[J]=\{1,,J\}\), where the **bold form** of \(j\) indicates a set of sub-indices \(\). The sample index \(i[I]=\{1,,I\}\), representing possible indexes in the pool set, is omitted when unnecessary, i.e., \(x_{i,j} x_{j}\). We denote \(}\) as the observed features with \([J]\).1 In the general case, we assume that each _feature_\(x_{i,j}\) considered for acquisition and the output of interest \(y_{i}\) have associated acquisition costs \(c_{i,j}\) and \(c_{i,J+1}\). Here, \(c_{i,}\) represents the total cost of acquiring the variables indexed by \(\) for instance \(i\).

**POCA**

In the context of _partially observed_ data, our focus is on efficiently gathering features and/or labels to optimize a utility function, \(U_{t}()\) subject to an acquisition constraint \(r_{t}()\) at iteration \(t\). \(U_{t}()\) quantifies the trade-off between the costs of data acquisition and the increased generalization capabilities of the model \(\), estimated from the available information \(}\) and the hypothetical acquisition of a specific set of features and/or labels. We formulate the optimization of this utility as follows:

\[(i,)^{*}=*{arg\,max}_{i[I],[J+1]}U_{t}(i, ),r_{t}(i,). \]

\(U_{t}()\) is broadly defined, potentially estimated as result of using Bayesian techniques [18; 23; 27], frequentist techniques [29; 30; 31], RL techniques [32; 33], or can even be subjectively defined through human desires. Note, optimizing \(U_{t}()\) involves an iterative process of 1 selecting the instance and variables \((i,)^{*}\) to acquire (features and/or labels); 2 adding these variables into the training set; 3 updating the model \(\) using the updated training set. In a more general case, this could also encompass batch acquisition [34; 35] by using \(\) instead of \(i\). Note that Eq. (1) represents the most general form of POCA, supporting model generalization when only features are acquired, as in semi-supervised or self-supervised learning. Our specific \(\)POCA instantiation (Section 2.1) focuses on the supervised case, where selected features **and** labels are acquired.

**Common modalities for POCA.** We anticipate that most applications of POCA will center on the tabular domain (see Table 2). However, it could also find valuable uses in fields like medical and satellite imaging, where noise-induced occlusion is common. In these cases, determining when a sample requires additional information (features) is essential for enhancing prediction accuracy and model training. Likewise, interactive robots that learn through vision may benefit from this approach, as they need to discern which scenarios (samples) merit interaction to effectively learn the relationship between features (objects) and labels (task to solve).

### \(\)Poca: A Bayesian implementation of POCA

Although several techniques can be used to implement POCA, we opt for a Bayesian approach due to its widespread success in data acquisition literature. Building on the foundational principles of _Bayesian Experimental Design_, which provides a comprehensive framework for integrating various sources of information , we introduce an instantiation of POCA within a Bayesian framework. This new approach, termed _uncertainty_ POCA or \(\)POCA, leverages information theory  to recast Eq. (1) as a cost-aware uncertainty reduction problem . The core of \(\)POCA is centered on reducing uncertainty through a class of models that are exclusively trained using supervised learning, focusing on feature **and** label acquisition in partially observed scenarios. Here, we denote \(_{i,}\) as the uncertainty reduction for acquiring the label and features \(\) for sample \(i\), which varies based on the approximation or method used.

\[\)**POCA**\(\)

We reformulate the optimization problem (1) by substituting \(U_{t}\) with a utility function \(\). This function \(\) is designed to capture the trade-off between uncertainty reduction, \(_{i,}\), and the acquisition costs associated with features and labels, represented by \(_{i,}\). The new objective can be expressed as:

\[(i,)^{*}=*{arg\,max}_{i[I],[J]}( _{i,},_{i,}),r(i,), \]

here \(_{i,}=c_{i,}+c_{i,J+1}\). In our research, we explore one specific instantiation, among potentially infinite options, denoted by \(_{i,}=_{i,}\) and \(r(i,)=_{i,}<c\), respectively, with \(c\) indicating the iteration's budget.2

**How to obtain this uncertainty?** We aim to minimize _epistemic uncertainty_ by acquiring data, decreasing the predictive uncertainty produced by the possible hypothesis explaining the data. We work within the supervised model framework, hence we represent hypotheses as distributions over parameters. Our approach assumes the predictive model \(p_{}(y|})\) can be expressed as:

\[p_{}(y|})=_{p_{}()}[p_{}(y|},)], \]

where \(\) is an instance of the parameter space and \(\) its associated RV. Here, \(\) specifies the model choice, defining the functional form of \(p_{}()=p(|)\), the posterior given the observed training set \(\), and the posterior predictive distribution \(p_{}(y|})\), marginalized over \(\). Here, \(}\) represents a partially observed input, so estimating \(p_{}(y|})\) must be adaptable to varying lengths of \(}\). To achieve this flexibility, models capable of handling variable-length inputs (such as Transformers) or, more broadly, marginalization techniques introduced in Section 3.2 can be employed.

This formulation is general, encompassing Bayesian models, neural networks with certain stochastic parameters , and ensemble models . It also applies to Gaussian processes  when the posterior \(p(|)\) is interpreted as a distribution over functions.

## 3 Method: Optimizing \(\)Poca

The challenge in optimizing \(\)POCA is in developing an uncertainty reduction metric, \(_{i,}\), that accurately represents the decrease in uncertainty when acquiring a subset of features \(\) for instance \(i\), which has not been thoroughly investigated in the Bayesian literature. To address this, let's first provide some key background information. For data acquisition in ML, the primary focus has been on maximizing the expected uncertainty reduction, also known as expected information gain, when acquiring data . This concept can be mathematically defined as:

\[(A,B):=(A)-(A|B), \]

where \((A)\) quantifies the uncertainty (entropy) about \(A\), and \((A|B)\) represents the uncertainty of \(A\) after observing \(B\) (in expectation). Existing AL approaches that utilize the expected reduction of uncertainty are summarized in Table 1. These methods maximize the uncertainty reduction of \((,Y|)\) when \(Y\) is observed.

Here, we use \(\) to represent any random variable aligned with the generalization capabilities of the model and \(\) any arbitrary conditioning. In the Appendix, for completeness, we derive the estimation for these acquisition metrics.

### Metrics for uncertainty reduction in _partially observed_ scenarios

**Challenges in designing \(_{i,}\).** In real-world scenarios, the challenge is estimating uncertainty reduction based solely on accessible data \(_{o}\). Traditional AL acquisition metrics, denoted as \(_{}(_{o})\), estimate uncertainty scores assuming \(_{o}\). However, in partially observed scenarios where only a subset of inputs, \(_{o}\), is available, the observed features may lack sufficient informativeness for precise \(y\) estimates and reliable uncertainty scores \(_{}(_{o})\).

**Generative Surrogate Model (GSM) to estimate metrics.** A more accurate estimate of current metrics can be achieved using the aforementioned AL metrics by imputing the potential missing features in expectation:

\[^{}_{,}(_{o}):=_{_{j}}}[_{ }(_{o}_{j}})]. \]

Here, the samples \(_{j}}\) are obtained with a GSM denoted as \(p_{}(_{j}|_{o})\), which sample possible unobserved features \(_{j}\) based on the observed \(_{o}\). It's worth noting that training \(p_{}(_{j}|_{o})\) could be done leveraging unlabeled data. In Figure 2, we illustrate the acquisition process of \(\)POCA using GSMs.

**Why generative imputation can help Active Learning?** In Bayesian active learning, acquisition is closely linked to the concept of uncertainty reduction. To identify which features need to be acquired, it is essential to estimate the possible unobserved values. If these values lie in areas of high uncertainty within the hypothesis space, acquiring these features is beneficial, as it will help reduce this uncertainty. Conversely, if the possible values for certain unobserved features show little or no impact on uncertainty, then acquiring these features may not be necessary. Notably, deterministic imputation cannot achieve this, as the lack of variability prevents assessment of its effect on uncertainty within the hypothesis space. This concept is illustrated in Figure 11 from Appendix H.

**Are we doing better?** We demonstrate the theoretical value of this approach for a family of acquisition metrics presented in Table 1, delving into their impact on the optimization process. These propositions convey the intuitive idea that acquiring more information, in this case, features, leads to a higher reduction in uncertainty for the predictive model (proofs can be found in Appendix B).

**Proposition 1**.: _Let \((_{o})\) be an acquisition metric that can be written as \((,Y|)\), with \(\) and \(\) representing the same variables observed in traditional AL (Table 1), and with \(Y\), \(}\) as previously defined. If \(}|\), the following equality holds:_

\[(,(Y,})|)=_{}} (,Y|},) \]

**Corollary 1**.: _Under the assumptions of Proposition 1, the subsequent inequality is established:_

\[_{}}(,Y|},) (,Y|) \]

_Equality is attained when \(}|Y,\)._

**Q** Proposition 1 states that the _uncertainty reduction_ of \(\) (e.g., the random variable of the parameters, \(\)) by knowing \(Y\) and \(}\) is equivalent to the expected _uncertainty reduction_ achieved by knowing \(Y\) while conditioning on unobserved variables \(}\). This is convenient as the conditioning on \(}\) can be computed using Monte-Carlo approximation .

  
**Method** & \(\) & \(\) & objective \\   BALD  & \(\) & \(_{o},\) & min. parameter uncertainty \\ EPIG  & \(Y_{}\) & \(_{o},,}}\) & min. predictive uncertainty \\ JEPIG  & \(Y_{}\) & \(_{o},,}}\) & min. predictive uncertainty \\   

Table 1: AL metrics with form of \((,Y|)\).

Figure 2: \(\)POCA leverages GSMs trained on unlabeled data for imputing missing features. The imputed observations are used as an input for the predictive model, whose outputs are used to compute the acquisition metric.

**Q**: Corollary 1 implies that acquiring both _labels_ and _features_ results in greater uncertainty reduction compared to acquiring only _labels_, the objective maximized in traditional AL (Table 1). The uncertainty reduction is equivalent when, given \(\) and \(Y\), the unobserved features \(_{j}\) don't have any impact in generalization \(\).

Note that the independence assumption of Proposition 1 is valid in the supervised models we consider. In essence, this is because acquiring features without labels do not aid parameter updates and in consequence generalization improvements. The foundation of this assumption lies in the predictive mapping process from \( Y\), rather than in the data itself. Appendix B provides a more detailed explanation of this independence assumption's validity. Additionally, empirical evidence supporting the validity of Corollary 1 and, by extension, Proposition 1, is shown in Appendix K.

Equations (6) and (7) always apply to the true random variable of unobserved features or any of its approximations. However, the terms in Eq. (6) reflect the uncertainty reduction of obtaining the actual features when the approximated distribution of the GSM accurately reflects the distribution of the true random variable. We empirically investigate this approximation and its practical utility.

**PO Active learning metrics.** Building on Proposition 1 and Corollary 1, we extend BALD and EPIG as \(\)_Partially Observable Expected Information Gain (PO-EIG)_: \(_{_{}}(,Y|_{},_{o}, )\) and \(\)_Partially Observable Expected Predictive Information Gain (PO-EPIG)_: \(_{_{}}(^{eval},Y|_{}, _{o},^{eval},)\). Corollary 1 states that these metrics provide a higher uncertainty reduction than their vanilla counterparts. We use Monte-Carlo for estimation (see Appendix C).

### Predictive models in the PO setting

Our derivations are based on a distribution perspective, considering different numbers of conditioned variables. For instance, when calculating PO-EIG, expressed as \(_{_{}}(,Y|_{},_{o}, )\), it is necessary to compute the distribution \(p_{}(y|_{o},_{})\). Here, \(_{o}\) could vary in length from one instance to another and \(_{}\) varies based on the number of features considered for computing the uncertainty reduction metric. In practical terms, this means that the predictive model, attempting to approximate this distribution, must effectively handle inputs with varying variables and lengths.

To address this challenge, we employ GSMs to impute the missing information to enable predictive models that expect fixed-size inputs. This imputation is separated in two different steps (1) _conditioning_ and (2) _marginalization_. Essentially, when evaluating the uncertainty reduction of an unobserved subset of features \(_{}\) considered for acquisition, we _condition_ on this subset \(_{}\) and \(_{o}\) (the observed features), _marginalizing_ over the remaining subset of unobserved features \(_{^{}}\) (where \(_{}_{^{}}\) is the set of all unobserved features). This approximation process is mathematically formalized as follows, with supplementary visual aids provided in Figure 8 of Appendix C.3:

\[p_{}(y|_{o},_{})= p_{}(y|_{o},_{},_{^{}})p_{}(_{^{}}|_{o}, _{})=_{p_{}(_{^{}}|_{o}, _{})}[p_{}(y|)], \]

Here the predictive model simulates the behavior, wherein the predictive model only has access to \(_{o}\) and \(_{}\) but it is computed using a model _as it would have all the features_. The marginalization step is essential for accurate metric estimation in the pool set and can also be applied during training. However, to reduce costs, we use GSM to impute features not acquired in the training set.

```
1:\(P=[\ ],F=[\ ]\)
2:for\(i[I]\)do
3:\(^{*}=[J]\)
4:while\(r(i,^{*})\)do
5:\(^{*}=_{v^{*}}_{i,^{*} v}\) \(r(i,^{*} v)\)
6:\(^{*}=^{*} v^{*}\)
7:endwhile
8:\(P.add(_{i,^{*}})\), \(P.add(^{*})\)
9:endfor
10:\(i^{*}=_{i[I]}P[i]\), \(j^{*}=F[i^{*}]\)
11:Return: \((i^{*},j^{*})\)
```

**Algorithm 1** Acquisition processfeatures until the constraint \(r\) in Eq. (2) is satisfied, in order \((J^{2})\). The acquisition process is summarized in Algorithm 1, with feature selection steps highlighted in teal. Appendix C.3 provides details on an efficient approach to computing the _marginalization_ step necessary for estimating \(_{i,}\). This efficiency can be further improved by selecting the most informative samples, followed by the application of Algorithm 1 (see Appendix D). For a comprehensive overview, including cost analyses, and details on GSM training and sampling, refer to Appendix D.

### Large Language Models as Generative Surrogate Models

**LLMs as GSMs.** For the scenarios outlined in POCA, we specify the following desiderata for GSMs: (P1) generative capability, (P2) ability to learn from partially observed data, (P3) sample efficiency, and (P4) seamless integration of mixed-type variables. We argue that LLMs are well-suited to meet these criteria due to their \(\) generative capabilities and flexibility in training under \(\) arbitrary conditioning contexts . Moreover, recent research highlights their exceptional performance in \(\) few-shot settings  and their generative capabilities applied to \(\) tabular data comprising mixed-type attributes . These strengths provide strong justification for focusing our research on LLMs as GSMs. However, **any** imputation method that fulfills these criteria may also serve as a suitable GSM, as further discussed in Appendix G.

We use LLMs as GSMs leveraging the unlabelled information via **Supervised Fine-Tuning (SFT)**. When working with tabular data, we serialize rows of the data, thereby converting it to natural language. For example, a set of features is serialized as _"Age is 25, Gender is Female,..., Blood pressure is 0.57"_. The LLM is then used to predict unobserved features based on available information. To achieve this, we utilize SFT on the LLM with the available observed features. The training data can encompass all unlabeled data, including historical and pool set data. The process entails generating random masks to form an input, \(m_{o}\), and an output, \((1-m)_{o}\), for SFT across all available data. This empowers the LLM to predict missing information by leveraging various combinations of observed features.3 For more details, refer to Appendix F.2.

**Analysis of GSMs.** The effectiveness of \(\)POCA in partially observed settings is closely tied to the GSM's ability to approximate the distribution of unobserved features. Two primary factors influence the accuracy of this approximation: **(1) the approximation capacity of the GSM** and **(2) intrinsic characteristics of the dataset**. A detailed examination of these factors is provided in Appendix J.

## 4 Experiments

We evaluate \(\)POCA across three dimensions 4: First, in the case that all features are acquired, we demonstrate that \(\)POCA acquisition metrics are more informative in selecting instances with informative features than AL metrics. Second, we present a synthetic experiment accompanied by theoretical insights. Finally, we explore scenarios with budget constraints demonstrating that \(\)POCA on more challenging scenarios.

Comparing \(\)POCA with the current AL models is complex, as the latter are designed for fixed-size inputs. To address this challenge, we developed _Scenario 1_ (see visual aid in Appendix H). This scenario involves dividing each instance in the pool set into the same observed and unobserved feature sets. We specifically select half of the features to remain unobserved, chosen by their high relevance to the predictive task as identified by a preliminary RF. It is important to note that while \(\)POCA methods can handle any form of missing data, _Scenario 1_ ensures a fair comparison by allowing AL models to operate without any modification, which could bias our evaluation. This scenario presumes the availability of a historical unlabeled dataset for training the GSM, using instances that include data on unobserved features. In practical applications, the pool set can often serve as the training set itself, representing a more realistic scenario we may encounter. We refer to this setting as _Scenario 2_. Results for this scenario are presented in Appendix I, where GSM is trained on partially observed data, while vanilla AL employs deterministic imputation to manage this case.

We select Magic, Adult, Housing, Cardio, and Banking tabular datasets based on their use in AL , tabular generation , LLM-based classification , and relation with potential real-worldapplications (Appendix A). These datasets have diverse characteristics: sample size, number of features, number of categorical, and numerical variables. We prioritize datasets with over 1000 samples to guarantee sufficient samples for the pool set. We showcase results using a RF trained with 100 estimators. We start training with two fully observed samples per class, conduct 150 acquisition cycles, repeat each experiment over 60 seeds, and display a 95% confidence interval. We train Mistral7B-Instruct-v0.3 using 8 Monte-Carlo samples for generative imputation.

### Need for POCA: Shortfalls of Active Learning

**Objective.** To assess the need for more generalized methodologies such as \(\)POCA, we analyze the performance of PO-EIG, a partially observed extension of BALD (EIG)--the most widely used metric in active learning literature. Additionally, we incorporate EPIG into the study, a recently developed active learning metric within the 1 family. According to our theoretical framework (see Corollary 1), _PO-metrics_ outperforms their vanilla counterparts in terms of uncertainty reduction. Our goal is to examine whether this uncertainty reduction leads to improved downstream performance when all features are acquired based on the same information, \(}\), or, in other words, if the selected instances possess features that are more relevant.

**Setup.** To ensure a fair comparison, we evaluated _PO-metrics_ and _Vanilla-metrics_ under _Scenario 1_, using Random and Oracle as reference baselines. Here, _Oracle_ represents the _Vanilla-metrics_ acquisition metric, but with access to all features. Ideally, when GSM functions optimally, the performance of PO-EIG should align with that of _Oracle_.

**Analysis**. The first thing to note is that EIG metrics computed with partially observed features can be significantly worse than simple baselines like random as shown in Magic dataset from Figure 3 (top). Figure 3 (top) demonstrates that PO-EIG generally either _outperforms_ or worst case matches their fully observable counterparts BALD across all datasets. A similar behavior is observed for PO-EPIG, which generally outperforms their vanilla metric counterpart. This suggests that an increase in uncertainty reduction translates into an increase in downstream performance. While PO-EIG and PO-EPIG metrics consistently outperform baselines, they occasionally fall short of oracle performance, notably in the Housing datasets. This may stem from two factors: Firstly, the GSM has poor prediction performance on the unobserved data due to insufficient data or model capacity. Secondly, even with adequate capacity and data, weak correlation between unobserved data and the target hinders the acquisition process. We study these factors in Appendix J. We note that it is non-trivial to quantify the GSM's capability or correlations of unobserved data to the target. Thus, the practical implication is that both _PO-metrics_ should be preferred in PO settings, providing a performance boost or at least matching their _vanilla_ counterparts. Additionally, in Appendix I.1, we include other relevant Active Learning metrics that, while not fitting into the family of studied metrics, also demonstrate performance gains with the proposed framework.

\(}\) First, AL metrics computed on partially observed features can dramatically fail for selecting relevant instances. Second, PO-EIG and PO-EPIG generally _outperform_ or match fully observed counterparts.

Figure 3: PO-EIG and PO-EPIG computed across diverse datasets - showing they either outperform or match their fully observed counterpart in terms of predictive performance

### Theoretical insights

**Objective.** We investigate the implications of our theoretical findings (Eq. (7)) on the acquisition process; determining whether a weak correlation between unobserved features and the target, results in a small gap between PO-EIG and BALD. We also explore how correlation affects performance.

**Setup:** We create an intuitive synthetic 2D experimental setup (Figure 4) with a variable target. The target is determined linearly with varying slopes, leading to different correlations with the features. Our chosen features--\(X_{1}\), \(X_{2}\), and \(X_{3}\)--represent data along the x-axis, y-axis, and a Gaussian category, respectively. Introducing the Gaussian category injects stochasticity into the marginalization process, ensuring non-trivial solutions. The observed feature is \(X_{1}\), with possible acquisition of \(X_{2}\), \(X_{3}\). We examine three scenarios: 1) Low Corr(\(X_{2}\),\(Y\)), where the class depends solely on \(X_{1}\) due to vertical slope; 2) High Corr(\(X_{2}\),\(Y\)), where the class depends solely on \(X_{2}\), rendering \(X_{1}\) irrelevant; and 3) Mid. Corr(\(X_{2}\),\(Y\)), where \(X_{1}\) has some impact. Note, we evaluate acquisition metrics and performance until the convergence of the oracle (BALD with all features)

**Analysis.** Figure 5A empirically validates that PO-EIG is always equal to or greater than BALD, consistent with our theoretical insights (Eq. (7)). Figure 5B illustrates the evolution of the metric gap between PO-EIG and BALD under varying correlations between \(X_{2,3}\) and \(Y\). In low correlation scenarios (orange line), the gap diminishes towards the acquisition's end, aligning with Corollary 1 where both metrics should converge when \(\!\!\!_{2,3}|x_{1,\,}\), i.e., when the unobserved features don't impact generalization. Initially, the gap exists as the model learns from data the redundancy of unobserved features. The same figure shows larger correlation leads to a wider gap, observed most notably in the large correlation scenario (purple) and moderately in the medium correlation scenario (teal). Figure 5 shows that, generally, the degree of problem correlation provides a proxy correlation with acquisition performance. For example, the purple line exhibits the largest difference between BALD and EIG. Particularly in low correlation scenarios, the performance difference between PO-EIG and BALD is negligible across the acquisition (orange line).

### Cost-aware active learning

**Objective:** We evaluate the performance of \(\)POCA (specifically PO-EIG) under budget-constrained feature acquisition, aiming to determine if acquiring only a subset of features, denoted as \(\), offers an advantageous trade-off in performance. This selective acquisition approach enables acquiring a larger number of instances within the same budget. Furthermore, we aim to show that imputation alone cannot fully replace the need for direct data acquisition.

**Setup.** We use the Magic dataset as a case study to examine the impact of cost constraints on predictive performance and the feature acquisition process. To facilitate this assessment, we introduce costs associated with both features and labels. For simplicity and visualization clarity, we assume the cost of an instance to be 1, representing the sum of the costs for all features and the label, with each feature assigned an equal cost. This setup allows us to analyze four distinct approaches: (1) the Vanilla acquisition metric (EIG), (2) PO-EIG, (3) PO-EIG with a maximum feature acquisition limit of 60%, and (4) PO-EIG with unrestricted feature acquisition. We evaluate the performance of these approaches in three ways: by accuracy based on acquired instances (Figure 6, left), by performance relative to the budget utilized (assuming no label costs) (Figure 6, middle), and by performance with varying label costs under a fixed total budget of 50 (Figure 6, right).

**Analysis.** Figure 6 (left) illustrates that acquiring fewer features generally results in decreased performance; however, it still outperforms the EIG baseline. While limited feature acquisition impacts performance, it allows for a more efficient budget allocation across instances, enabling the acquisition of a larger instance pool. This trend is visible in Figure 6 (middle), where performance is plotted against the total budget spent, assuming no label cost. Here, methods focusing on selective feature acquisition excel, as they gather more overall information through increased instance count and key features.

Figure 4: Synthetic dataset

Figure 5: Comparing PO-EIG and BALD.

Figure 6 (right) demonstrates that the optimal PO-EIG method depends on feature acquisition cost: when cost is heavily weighted toward feature acquisition (right histogram), the best method is PO-EIG with \(20\%\) of features acquired, whereas a \(50\%\) label cost favors PO-EIG with \(60\%\) feature acquisition. While these findings might suggest that acquiring fewer features and imputing the rest is optimal for maximizing instances, this approach may introduce noise into the training set, potentially biasing the model. To explore this, we analyze model performance at different levels of feature acquisition in Figure 7, with varying levels of pool data, using the full pool set for training (excluding non-acquired features). As shown on the y-axis, acquiring more features enhances performance. When the budget is unlimited, acquiring all available data is preferable; however, in practice, this may not be feasible, making POCA approaches advantageous.

\(\) First, \(\)POCA metrics (PO-EIG) can be more cost-effective than common active learning metrics. Second, imputation is useful for missing data but shouldn't replace data acquisition.

## 5 Discussion

We introduce and formalize POCA a data acquisition framework, addressing the vital but underexplored challenge of partially observed settings. Through \(\)POCA, a practical implementation of this framework, we demonstrate the feasibility of acquiring unobserved features and labels based on those partially observed features, using more generalized AL utility metrics -- computed by estimating features generated using an LLM-based GSM. Our results over various scenarios are substantially more effective than alternatives -- of substantial value for data acquisition in cost-restrictive environments. We hope the POCA framework and our subsequent findings will spur additional work to advance data acquisition in partially observed settings.

**Limitations.** Our work focuses on the values of features, providing a general framework where restrictions are the main source of constraints in terms of acquisition. However, we do not assess how these restrictions are selected, which could be a promising area for future research. We also note that we use LLMs in the context of data acquisition. Like any GSM, LLMs can indeed exhibit biases that affect the acquisition process. In this study, we did not consider this issue, and it represents an interesting avenue for future work. If necessary, current debiasing techniques can be applied.

**Practical consideration and future work.** (1) In the PO setting with data "missingness," GSM imputation is essential for acquisition. Future work could quantify uncertainty  to assess GSM efficacy. (2) LLM capability also impacts acquisition; while we use a 7B-parameter model, larger models could further enhance performance, though this is beyond our current scope.