# Geometry-Aware Adaptation for Pretrained Models

Nicholas Roberts, Xintong Li, Dyah Adila, Sonia Cromp,

**Tzu-Heng Huang, Jitian Zhao, Frederic Sala**

University of Wisconsin-Madison

{nicki1iroberts, fredsala}@cs.wisc.edu

{xli2224, adila, cromp, thuang273, jzhao326}@wisc.edu

###### Abstract

Machine learning models--including prominent zero-shot models--are often trained on datasets whose labels are only a small proportion of a larger label space. Such spaces are commonly equipped with a metric that relates the labels via distances between them. We propose a simple approach to exploit this information to adapt the trained model to reliably predict new classes--or, in the case of zero-shot prediction, to improve its performance--without any additional training. Our technique is a drop-in replacement of the standard prediction rule, swapping \(\) with the Frechet mean. We provide a comprehensive theoretical analysis for this approach, studying (i) learning-theoretic results trading off label space diameter, sample complexity, and model dimension, (ii) characterizations of the full range of scenarios in which it is possible to predict any unobserved class, and (iii) an optimal active learning-like next class selection procedure to obtain optimal training classes for when it is not possible to predict the entire range of unobserved classes. Empirically, using easily-available external metrics, our proposed approach, Loki, gains up to 29.7% relative improvement over SimCLR on ImageNet and scales to hundreds of thousands of classes. When no such metric is available, Loki can use self-derived metrics from class embeddings and obtains a 10.5% improvement on pretrained zero-shot models such as CLIP.

## 1 Introduction

The use of pretrained models is perhaps the most significant recent shift in machine learning. Such models can be used out-of-the-box, either to predict classes observed during pretraining (as in ImageNet-trained ResNets ) or to perform zero-shot classification on any set of classes . While this is exciting, label spaces are often so huge that models are unlikely to have seen _even a single point_ with a particular label. Without additional modification, pretrained models will simply fail to reliably predict such classes. Instead, users turn to fine-tuning--which requires additional labeled data and training cycles and so sacrifices much of the promise of zero-shot usage.

How can we adapt pretrained models to predict new classes, without fine-tuning or retraining? At first glance, this is challenging: predictive signal comes from labeled training data. However, _relational_ information between the classes can be exploited to enable predicting a class even when there are no examples with this label in the training set. Such relational data is commonly available, or can be constructed with the help of knowledge bases, ontologies, or powerful foundation models .

How to best exploit relational structure remains unclear, with a number of key challenges: We might wish to know what particular subset of classes is rich enough to enable predicting many (or all) remaining labels. This is crucial in determining whether a training set is usable or, even with the aid of structure, insufficient. It is also unclear how approaches that use relational information interact with the statistical properties of learning, such as training sample complexity. Finally, performing adaptation requires an efficient and scalable algorithm.

This work addresses these challenges. It proposes a simple and practical approach to learning in structured label spaces, with theoretical guarantees. First, we offer a simple way to translate the soft outputs (i.e., probability vectors) produced by _any_ supervised learning model into a more general model that can exploit geometric information for label structure. In other words, our approach, called Loki,1 is a simple _adaptor_ for pretrained models. Loki can be applied via a fixed linear transformation of the model outputs. Loki's simplicity makes it applicable to a broad range of settings while enabling very high-cardinality predictions subject to a potentially small model output budget--we provide a visualization of this key idea in Figure 1.

Theoretically, we provide a rich set of results for the metric-based adaptation setting. First, we introduce a learning-theoretic result in terms of the sample complexity of the pretrained model. It captures a key tradeoff between the number of classes and metric structure, the problem dimensionality, and the number of samples used to train the model prior to adaptation. Next we exhaustively study the properties of training sets, determining for a wide class of relational structures the minimum number (and structure) of subsets that enable reliable prediction. Finally we show how to exploit this result in an active learning-like approach to selecting points that will improve deficient training datasets.

Experimentally, we show that using structural information improves prediction in high-cardinality settings. We demonstrate the strength of the active learning-based approach to dataset expansion over random baselines. Finally, and most excitingly, we show that even in zero-shot models like CLIP, where it is possible to produce probability vectors over any possible class, the use of our adaptor leads to a **19.53%** relative improvement.

## 2 Background

First, we introduce the problem setting, notation, and mathematical tools that we will use. Afterward, we discuss how Loki relates to prior work.

Problem SettingAs in conventional supervised learning, we have a dataset \((x_{1},y_{1}),,(x_{n},y_{n})\) drawn from a distribution on \(\), where \(\) and \(\) are the input and label spaces. In our setting, \(N:=||\) is finite but large; often \(N n\)--so that many labels will simply not be found in the training dataset. We let \(\) with \(||=K\) be the set of observed labels. For convenience of notation, we also define \(:=[_{i}]_{i=1}^{K}\), \(:=[y_{i}]_{j=1}^{N}\) to be the vectors of elements in \(\) and \(\).

In addition to our dataset, we have access to a relational structure on \(\). We assume that \(\) is a metric space with metric (distance) \(d:^{2}\); \(d\) encodes the relational structure of the label space. Specifically, we model this metric space using a graph, \(G=(,)\) where \(^{2}_{+}\) is a set of edges relating the labels and the standard shortest-path distance \(d:^{2}_{ 0}\). In addition to its use in prediction, the metric \(d\) can be used for evaluating a model by measuring, for example, \(_{i=1}^{n}d^{2}(f(x_{i}),y)\)--the analogue of the empirical square loss.

Frechet Mean EstimatorDrawing on ideas from structured prediction [5; 35], we use a simple predictor that exploits the label metric. It relies on computing the _Frechet mean_, given by

\[m_{}():=*{arg\,min}_{y}_{i=1}^{K} _{i}d^{2}(y,_{i}), \]

Figure 1: Classification regions in the probability simplex of 3-class classifiers faced with a 100-class problem. The probability simplex using \(*{arg\,max}\) prediction can only output one of three classes. Loki uses the entire probability vector to navigate the class metric space, leading to more prediction regions. (Left) regions from \(*{arg\,max}\) prediction. (Centers, Right) classification regions from Loki.

where \(_{ 0}^{K}\) is a set of weights. The Frechet mean generalizes the barycenter to metric spaces and is often used in geometric machine learning .

Locus of the Frechet mean.The _locus of the Frechet mean_ is the set of all Frechet means under different weights . We write it as \(():=_{^{K-1}}m_{}()\).

\(()\) can be thought of the set of all labels in \(\) that are reachable by the Frechet mean given \(\{_{i}\}_{i=1}^{K}\) and different choices of its parameter \(\). Intuitively, we can think of the locus for a given dataset as describing how usable it is for predicting beyond the observed labels. Trivially, if \(\{_{i}\}_{i=1}^{K}=\), then \(()=\). We are primarily interested in the case in which \(\{_{i}\}_{i=1}^{K}\) yet we still have \(()=\), or that \(|()|\) is at least sufficiently large.

### Relation to Prior work

Loki is primarily related to two areas: zero-shot learning and structured prediction.

Zero-Shot LearningLike zero-shot learning (ZSL), Loki is capable of predicting unobserved classes. Our framework is most closely related to _generalized_ ZSL, which uses side information to predict both observed and unobserved classes. Many types of external knowledge are used in ZSL, including text [28; 38; 9], attributes [22; 23; 7], knowledge graphs [42; 34; 10], ontologies , and logical rules . Our work is most closely related to ZSL approaches that rely on knowledge graph information. Often, ZSL methods that use knowledge graph information rely on the use of graph neural network architectures [42; 17; 18]. _However, we note that these architectures can be heavyweight and can be challenging to scale up to extremely large graphs, whereas Loki does not have architectural requirements and scales linearly in the size of the graph when \(K\) is small._

Structured PredictionStructured prediction (SP) operates in label spaces that are endowed with algebraic or geometric structure [1; 21]. This includes problems such as predicting permutations , non-Euclidean and manifold regression [31; 36], and learning on graphs . Loki is the most immediately related to the latter, however, any finite metric space can be represented as a graph, which further lends to the flexibility of our approach. Even in discrete structured prediction settings, the cardinality of the label space may be combinatorially large. _As such, Loki can be viewed as a simple method for adapting classifiers to structured prediction._

The Frechet mean has been used in structured prediction--but in approaches requiring training. In , \((x)=_{y}_{i=1}^{n}_{i}(x)d^{ 2}(y,_{i})\), where \((x)=(K+n I)^{-1}K_{x}\). \(K\) is the kernel matrix for a kernel \(k:\), so that \(K_{i,j}=k(x_{i},x_{j})\), \((K_{x})_{i}=k(x,x_{i})\). \(\) is a regularization parameter. In other words, the weight \(w_{i}\) corresponding to \(_{i}\) is the average produced by solving a kernel regression problem at all points \(x_{k}\) in the dataset where \(y_{k}=_{i}\). It has also used in weak supervision (WS) for metric-equipped label spaces [41; 37], where the goal is to produce labeled data for training structured prediction models.

## 3 Framework

We introduce our framework--Loki. We show how Loki can be used to adapt any supervised classifier over a set of \(K\) classes to a much richer set of possible class predictions. It does so by weighting the Frechet mean by the classifier's per-class prediction probabilities or logits, allowing it to predict any class in the locus of the Frechet mean--potentially far more classes than the initial \(K\). Next, we show how Loki can be expressed as a _fixed_ linear transformation of a model's outputs. Finally, we show that Loki relates to standard classification.

### Loki: Adapting Pretrained Models

We describe our approach to adapting pretrained classifiers-trained on a set of classes \(\)--to the metric geometry of the label space \(\), enabling the prediction of unobserved classes.

We model unobserved classes \(\) using the Frechet mean among observed classes weighted by their _prediction probabilities_\(P(y=_{i}|xy)\). We denote the vector of model outputs as \(_{y|x}:=[_{_{i}|x}]_{i=1}^{K}=[P(y=_{i}|xy)]_{i=1}^{K}\). Then predictions using Loki are given by

\[ m_{}(_{y|x})=*{arg\,min}_{y }_{i=1}^{K}_{_{i}|x}d^{2}(y,_{i}).\]

### Loki as a linear transformation of model outputs

Most standard classifiers output a vector of prediction probabilities, \(_{y|x}\), whose entries correspond to the confidence of predicting a specific class. Predictions are typically given by \(*{arg\,max}_{i[K]}(_{y|x})_{i}\). Loki generalizes this prediction rule when viewed as a linear transformation of \(_{y|x}\). Consider the Loki prediction rule \( m_{}(_{y|x})=*{arg\,min}_{y }_{i=1}^{K}_{_{i}|x}d^{2}(y,_{i})= *{arg\,max}_{j[N]}(_{y|x})_{j}\), where \(_{j,i}=[-d^{2}(y_{j},_{i})]\); \(^{N K}\) is the matrix of negative squared distances between the observed classes and the rest of the label space. Thus Loki can be used within standard classification pipelines when the model output \(_{y|x}\) is multiplied by the fixed matrix \(\).

### Generalizing Standard Classification

We provide a simple intuition for our approach. The fact that Loki reduces to standard classification among the observed classes has several implications. This includes the idea that under our framework, forms of few-shot, zero-shot, hierarchical, and partial label learning all reduce to standard classification when additional metric information is introduced.

Generalizing the arg max prediction ruleIn the absence of this metric information--a situation that we model using the complete graph and setting \(=\)--our framework also recovers standard classification. Indeed, both in terms of error modeling and in terms of inter-class similarity, the intuition of standard classification and of the 0-1 loss are captured well by the unweighted complete graph--simply treat all differences equally. This graph is given as \(K_{N}:=(,^{2}\{1\})\)--i.e., every label is equidistant from every other label. Plugging this into Loki, we obtain the following:

\[ m_{}(_{y|x})=*{arg\,min}_{y }_{i=1}^{K}_{_{i}|x}d^{2}(y,_{i})= *{arg\,min}_{y}_{i=1}^{K}_{_{i }|x}\{y_{i}\}=*{arg\,max}_{i[K]}_{_{i}|x},\]

which is exactly the standard classification prediction rule.

Generalizing the 0-1 loss via the expected squared distance_The expected squared distance_\([d^{2}(y,)]\)_is the standard loss function for many structured prediction problems_. Note that **accuracy fails in such settings**--since it cannot distinguish between small and large errors. This is most clearly seen in the extreme case of regression, where test accuracy will virtually always be zero no matter how good a trained model is. At the other extreme, the complete graph, this loss function becomes the standard 0-1 loss: \([d^{2}(y,)]=[\{y\}]\). As an adaptor for structured label spaces, we use the empirical version of this loss to evaluate Loki. Note that the expected squared distance subsumes other metrics as well. For example, when \(=\), we can derive the standard MSE by setting \(d(y,)=|y-|\), which is just the standard L1 distance metric. Other scores such as recall at top-k can be similarly obtained at the cost of \(d(,)\) being a true metric. In other words, \([d^{2}(y,)]\) is an very general metric that supports any metric space, and we use it throughout this work.

## 4 Theoretical Results

Challenges and OpportunitiesThe \(*{arg\,max}\) of per-class model probabilities is a ubiquitous component of classification pipelines in machine learning. In order to predict unobserved classes using metric space information, Loki replaces this standard component. As a simple but significant change to standard pipelines, Loki opens up a new area for fundamental questions. There are three main flavors of theoretical questions that arise in this setting:

1. How does the performance of Loki change as a function of the number of samples?
2. What minimal sets of observed classes are required to predict any class in the metric space?
3. How can we acquire new classes that maximize the total number of classes we can predict?

[MISSING_PAGE_FAIL:5]

**Definition 4.3** (Trivial locus cover).: If \(=\), then \(\) is the trivial locus cover.

This Definition captures the notion of observing all of the classes in the label space. Here, all of the elements of \(\) are trivially reachable using Loki.

**Definition 4.4** (Nontrivial locus cover).: A locus cover \(\) is nontrivial if \(\).

Loki is more useful and interesting when faced with a nontrivial locus cover--under Definition 4.4, we can use some subset of classes \(\) to predict any label in \(\).

**Definition 4.5** (Minimum locus cover).: Given a set \(\), if \(\) is the smallest set that is still a locus cover, then it is a minimum locus cover.

In cases involving an extremely large number of classes, it is desirable to use Loki on the smallest possible set of observed classes \(\) such that all labels in \(\) can still be predicted. Definition 4.5 characterizes these situations--later, we obtain the minimum locus covers for all trees and grid graphs. It is worth noting that the minimum locus cover need not be unique for a fixed graph.

**Definition 4.6** (Identifying locus cover).: Given a set \(\), if \(\) is a locus cover where \(\,y,\;\,^{||-1}\) such that \(m_{}()=\{y\}\), then \(\) is an identifying locus cover.

The Frechet mean need not be unique--as an \(\), it returns a set of minimizers. In certain metric spaces, the minimum locus cover can yield large sets of minimizers--this is undesirable, as it makes predicting a single class challenging. Definition 4.6 appeals to the idea of finding some set of classes for which the Frechet mean _always_ returns a unique minimizer--this is desirable in practice, and in some cases, moreso than Definition 4.5.

**Definition 4.7** (Pairwise decomposable).: Given \(\), \(()\) is called pairwise decomposable when it holds that \(()=_{_{1},_{2}}(\{_{1}, _{2}\})\).

In many cases, the locus can be written in a more convenient form--the union of the locus of pairs of nodes. We refer to this definition as pairwise decomposability. Later, we shall see that pairwise decomposability is useful in computing the locus in polynomial time.

TreesMany label spaces are endowed with a tree metric in practice: hierarchical classification, in which the label space includes both classes and superclasses, partial labeling problems in which internal nodes can represent the prediction of a set of classes, and the approximation of complex or intractable metrics using a minimum spanning tree. We show that for our purposes, trees have certain desirable properties that make them easy to use with Loki--namely that we can easily identify a locus cover that satisfies both Definition 4.5 and Definition 4.6. Conveniently, we also show that any locus in any tree satisfies Definition 4.7.

We first note that the leaves of any tree yield the minimum locus cover. This is a convenient property--any label from any label space endowed with a tree metric can be predicted using Loki using only classifiers trained using labels corresponding to the leaves of the metric space. This can be especially useful if the tree has long branches and few leaves. Additionally, for tree metric spaces, the minimum locus cover (Definition 4.5) is also an identifying locus cover (Definition 4.6). This follows from the construction of the weights in the proof of Theorem A.4 (shown in the Appendix) and the property that all paths in trees are unique. Finally, we note that any locus in any tree is pairwise decomposable--the proof of this is given in the Appendix (Lemma A.5). We will see later that this property yields an efficient algorithm for computing the locus.

Phylogenetic TreesImage classification datasets often have a hierarchical tree structure, where only the leaves are actual classes, and internal nodes are designated as superclasses--examples include the ImageNet  and CIFAR-100 datasets . Tree graphs in which only the leaf nodes are labeled are referred to as phylogenetic trees . Often, these graphs are weighted, but unless otherwise mentioned, we assume that the graph is unweighted.

For any arbitrary tree \(T=(,)\), the set of labels induced by phylogenetic tree graph is \(=(T)\). We provide a heuristic algorithm for obtaining locus covers for arbitrary phylogenetic trees in Algorithm B.1 (see Appendix). We prioritize adding endpoints of long paths to \(\), and continue adding nodes in this way until \(()=\). Similarly to tree metric spaces, any phylogenetic tree metric space is pairwise decomposable. We prove the correctness of Algorithm B.1 and pairwisedecomposability of phylogenetic trees in the Appendix (Theorem A.6 and Lemma A.7). Later, we give algorithms for computing the set of nodes in an arbitrary locus in arbitrary graphs--if the locus is pairwise decomposable, the algorithm for doing so is efficient, and if not, it has time complexity exponential in \(K\). Due to the pairwise decomposability of phylogenetic trees, this polynomial-time algorithm to compute \(()\) applies.

Grid GraphsClasses often have a spatial relationship. For example, classification on maps or the discretization of a manifold both have spatial relationships--grid graphs are well suited to these types of spatial relationships. We obtain minimum locus covers for grid graphs satisfying Definition 4.5, but we find that these are not generally identifying locus covers. On the other hand, we give an example of a simple identifying locus cover satisfying Definition 4.6. Again, we find that grid graphs are in general pairwise decomposable and hence follow Definition 4.7.

We find that the pair of vertices on furthest opposite corners yields the minimum locus cover. While the set of vertices given by Theorem A.8 (found in the Appendix) satisfies Definition 4.5, this set does not in general satisfy Definition 4.6. This is because the path between any two vertices is not unique, so each minimum path of the same length between the pair of vertices can have an equivalent minimizer. On contrast, the following example set of vertices satisfies Definition 4.6 but it clearly does not satisfy Definition 4.5. _Example_: Given a grid graph, the set of all corners is an identifying locus cover. On the other hand, the vertices given by Theorem A.8 can be useful for other purposes. Lemma A.9 (provided in the Appendix) shows that subspaces of grid graphs can be formed by the loci of pairs of vertices in \(\). This in turn helps to show that loci in grid graphs are pairwise decomposable in general (see Lemma A.10 in the Appendix).

The Complete GraphThe standard classification setting does not use relational information between classes. As before, we model this setting using the complete graph, and we show the expected result that in the absence of useful relational information, Loki cannot help, and the problem once again becomes standard multiclass classification among observed classes. To do so, we show that there is no nontrivial locus cover for the complete graph (Theorem A.11 in the Appendix).

### Label Subspaces in Practice

While it is desirable for the set of observed classes to form a minimum or identifying locus cover, it is often not possible to choose the initial set of observed classes a priori--these are often random. In this section, we describe the more realistic cases in which a random set of classes are observed and an active learning-based strategy to choose the next observed class. The aim of our active learning approach is, instead of randomly selecting the next observed class, to actively select the next class so as to maximize the total size of the locus--i.e., the number of possible classes that can be output using Loki. Before maximizing the locus via active learning, we must first address a much more basic question: can we even efficiently compute the locus?

Computing the LocusWe provide algorithms for obtaining the set of all classes in the locus, given a set of classes \(\). We show that when the locus is pairwise decomposable (Definition 4.7), we can compute the locus efficiently using a polynomial time algorithm. When the locus is not pairwise decomposable, we provide a general algorithm that has time complexity exponential in \(||\)--we are not aware of a more efficient algorithm. We note that any locus for every type of graph that we consider in Section 4.2 is pairwise decomposable, so our polynomial time algorithm applies. Algorithms B.2 and B.3 along with their time complexity analyses can be found in the Appendix.

Large Locus via Active Next-Class SelectionWe now turn to actively selecting the next class to observe in order to maximize the size of the locus. For this analysis, we focus on the active learning setting when the class structure is a tree graph, as tree graphs are generic enough to apply to a wide variety of cases--including approximating other graphs using the minimum spanning tree. Assume the initial set of \(K\) observed classes are sampled at random from some distribution. We would like to actively select the \(K+1\)st class such that \(|()|\) with \(=\{\}_{i=1}^{K+1}\) is as large as possible.

**Theorem 4.8**.: _Let \(T=(,)\) be a tree graph and let \(\) with \(K=||\). Let \(T^{}\) be the subgraph of the locus \(()\). The vertex \(v\) that maximizes \(|(\{v\})|\) is the solution to the following optimization problem: \(*{arg\,max}_{y()}d(y,b)\) s.t. \(b_{in}T^{}\) and \((y,b)\{b\}()\). where \(_{in}T^{}\) is the inner boundary of \(T^{}\) (all vertices in \(T^{}\) that share an edge with vertices not in \(T^{}\))._This procedure can be computed in polynomial time--solving the optimization problem in Theorem 4.8 simply requires searching over pairs of vertices. Hence we have provided an efficient active learning-based strategy to maximize the size of the locus for trees.

## 5 Experimental Results

In this section, we provide experimental results to validate the following claims:

1. Loki improves performance of zero-shot foundation models even with no external metric.
2. Loki adapts to label spaces with a large number of unobserved classes.
3. The active approach given in Theorem 4.8 yields a larger locus than the passive baseline.
4. The same active approach yields better performance on ImageNet.
5. With Loki, calibration can improve _accuracy_, even with no external metric.

### Loki Improves Zero-Shot Models

We evaluate the capability of Loki to improve upon zero-shot models where all classes are observed.

SetupOur experiment compares the zero-shot prediction performance of CLIP  on CIFAR-100  to CLIP logits used with Loki. First, we consider the setting in which no external metric relating the labels is available, and instead derive internal metric information from Euclidean distances between text embeddings from the models using their respective text encoders. Second, we consider three external metric spaces for use with Loki: the complete graph \(K_{100}\) and two phylogenetic trees: the default CIFAR-100 superclasses  and WordNet .

ResultsThe results of this experiment are given in Table 1. When no external metric information is available, Loki still outperforms CLIP-like models that use the standard prediction rule--in other words, Loki seems to unconditionally improve CLIP. As expected, under the complete graph, our method becomes equivalent to the standard prediction mechanism used by CLIP. On the other hand, Loki outperforms CLIP when using the default CIFAR-100 tree hierarchy and even more so when using the WordNet geometry, with a \(\) relative improvement in mean squared distance over the CLIP baseline. We postulate that the strong performance using WordNet is due to the richer geometric structure compared to that of the default CIFAR-100 hierarchy.

### Loki on Partially-Observed Label Spaces

To validate our approach on partially observed label spaces, we evaluate the performance of adapting a logistic classifier trained on SimCLR embeddings of ImageNet , 5-NN models trained on a 9,419 class subset of the PubMed dataset,2 and the 325,056-class LSHTC dataset .

   Model & Metric Space & \(\) & Loki & Relative Improvement \\  CLIP-RN50  & Internal & 0.2922 & **0.2613** & **10.57\%** \\  CLIP-ViT-L-14  & Internal & 0.1588 & **0.1562** & **1.63\%** \\  ALIGN  & Internal & 0.1475 & **0.1430** & **3.02\%** \\  CLIP-RN50  & \(K_{100}\) & 0.5941\({}^{*}\) & 0.5941\({}^{*}\) & 0.0\%\({}^{*}\) \\ CLIP-RN50  & Default tree & 7.3528 & **7.1888** & **2.23\%** \\ CLIP-RN50  & WordNet tree & 24.3017 & **19.5549** & **19.53\%** \\   

* \({}^{*}\) methods equivalent under the complete graph as Loki reduces to \(\) prediction.

Table 1: **CIFAR-100. Improving CLIP predictions using Loki. Results are reported as \([d^{2}(y,)]\) in the respective metric space. CLIP-like zero-shot models can be improved using Loki even without access to an external metric, and internal class embedding distances are used. When an external metric is available, Loki outperforms CLIP using the default CIFAR-100 hierarchy and WordNet.**SetupFor ImageNet, we use the WordNet phylogenetic tree as the metric space . In this setting, we sample random subsets of size \(K\) of the 1000 ImageNet classes and compare a baseline one-vs-rest classifier to the same classifier but using Loki to adapt predictions to classes beyond the original \(K\). We conduct two sets of experiments. In the first, we sample \(K\) classes uniformly, while in the second, we adopt a more realistic sampler--we sample from a Gibbs distribution: \(P(|)=(- d(,^{c}))\), where \(^{c}=m_{}(_{N})\) is the centroid of the metric space, \(\) is the concentration parameter around the centroid, and \(Z\) is the normalizer. While the Gibbs distribution sampler is more realistic, it is also the more challenging setting--classes which have low probability according to this distribution are less likely to appear in the locus. For PubMed, we derive our metric from Euclidean distances between SimCSE class embeddings . Finally for LSHTC, we summarize the default graph by randomly selecting nodes and merging them with their neighbors until we obtain a graph with 10,000 supernodes representing sets of classes.

ResultsFigure 2 shows the mean squared distances compared to the baseline one-vs-rest classifiers, across various settings of \(K\). We find that Loki always significantly outperforms the baseline, even in the more challenging setting of sampling according to a Gibbs distribution. Tables 2 and 3 show our improvements when using Loki over the baseline 5-NN models. While Loki consistently yields an improvement on PubMed and LSHTC, the improvement is more dramatic on LSHTC.

### Large Loci via Active Learning

We evaluate our active next class selection approach on increasing the locus size. We expect that compared to passive (random) selection, our active approach will lead to larger loci, and in practice, a larger set of possible classes that can be predicted using Loki while observing fewer classes during training.

SetupWe compare with a baseline that randomly selects the next class. We first generate a synthetic random tree with size \(N\) and fix an initial \(K\). In active selection, we use the approach described in Theorem 4.8 to select the next node. As a passive baseline, we randomly sample (without replacement) the remaining nodes that are not in \(\).

   \(K\) & 5-NN & 5-NN + Loki \\ 
100 & 1.68666 & **1.42591** \\
250 & 1.52374 & **1.47801** \\
500 & 1.64074 & **1.45921** \\   

Table 2: **PubMed. Loki improves baseline for all settings of \(K\). The metric space is Euclidean distances applied to SimCSE embeddings.**

Figure 2: (Top left) **ImageNet. Mean squared distances under uniform class sampling and under the Gibbs distributionâ€”Loki improves upon the baseline SimCLR one-vs-rest classifier. (Top right) **Synthetic. Active class selection consistently leads to larger loci compared to uniform sampling.** (Bottom) **Active selection on ImageNet.** Active class selection improves performance on ImageNet.

ResultsThe results are shown in Figure 2. We set \(N=100\) and the initial \(K=3\), then we average the result over 10 independent trials. The active approach consistently outperforms the random baseline as it attains a larger locus with fewer nodes selected.

### Improving Performance via Active Learning

Next, we evaluate the our active next class selection approach on improving error on ImageNet. While we found that the the active approach indeed leads to an increased locus size compared to the passive baseline, we expect that this increased locus size will lead to improved performance.

SetupWe randomly sample 500 ImageNet classes, and using the WordNet metric space, we use our active approach to iteratively sample 50 more classes. We compare this to a passive baseline in which the 50 classes are sampled randomly. We repeat this experiment over 10 independent trials.

ResultsFigure 2 shows that our active approach yields improved error over the passive baseline. The gap between the active approach and the passive baseline widens with more selection rounds.

### Improving Accuracy via Calibration

Finally, we evaluate the effect of calibrating the Softmax outputs on the performance of Loki.

SetupWe calibrate via Softmax temperature scaling  using CLIP on CIFAR-100. We do not use an external metric space, and instead use Euclidean distance applied to the CLIP text encoder.

ResultsThe reliability diagram in Figure 3 shows that the optimal Softmax temperature for Loki is both close to the default temperature used by CLIP and to the optimally-calibrated temperature. In Figure 3 (right), we find that appropriate tuning of the temperature parameter _can lead to improved accuracy with CLIP_, even when no external metric space is available.

## 6 Conclusion

In this work, we proposed Loki--a simple adaptor for pretrained models to enable the prediction of additional classes that are unobserved during training by using metric space information. We comprehensively answered the space of new questions that arise under Loki in terms of learning, optimal metric space settings, and a practical active selection strategy. Experimentally, we showed that Loki can be used to improve CLIP even without external metric space information, can be used to predict a large number of unobserved classes, and a validation of our active selection strategy.

Figure 3: **CLIP on CIFAR-100 with no external metric.** (Left) Reliability diagrams across a range of Softmax temperatures, highlighting the CLIP default temperature, the optimal temperature for Loki, and the minimizer of the Expected Calibration Error (ECE). All three are well-calibrated. (Center) Tradeoff between optimizing for ECE and the expected squared distance. As with the reliability diagrams, the CLIP default temperature, the Loki-optimal temperature, and the ECE-optimal temperature are similar. (Right) Tradeoff between optimizing for zero-one error and the expected squared distance. Temperature can be tuned to improve _accuracy_ when using Loki.