# GeoLRM: Geometry-Aware Large Reconstruction Model for High-Quality 3D Gaussian Generation

Chubin Zhang\({}^{1,3}\) Hongliang Song\({}^{3}\) Yi Wei\({}^{2}\)

Yu Chen\({}^{3}\) Jiwen Lu\({}^{2}\) Yansong Tang\({}^{1,}\)

\({}^{1}\)Tsinghua Shenzhen International Graduate School, Tsinghua University

\({}^{2}\)Department of Automation, Tsinghua University

\({}^{3}\)Alibaba Group

{zcb24, y-wei19}@mails.tsinghua.edu.cn,

{hongliang.shl, chenyu.cheny}@alibaba-inc.com,

lujiwen@tsinghua.edu.cn, tang.yansong@sz.tsinghua.edu.cn.

\({}^{}\) corresponding author

###### Abstract

In this work, we introduce the Geometry-Aware Large Reconstruction Model (GeoLRM), an approach which can predict high-quality assets with 512k Gaussians and 21 input images in only 11 GB GPU memory. Previous works neglect the inherent sparsity of 3D structure and do not utilize explicit geometric relationships between 3D and 2D images. This limits these methods to a low-resolution representation and makes it difficult to scale up to the dense views for better quality. GeoLRM tackles these issues by incorporating a novel 3D-aware transformer structure that directly processes 3D points and uses deformable cross-attention mechanisms to effectively integrate image features into 3D representations. We implement this solution through a two-stage pipeline: initially, a lightweight proposal network generates a sparse set of 3D anchor points from the posed image inputs; subsequently, a specialized reconstruction transformer refines the geometry and retrieves textural details. Extensive experimental results demonstrate that GeoLRM significantly outperforms existing models, especially for dense view inputs. We also demonstrate the practical applicability of our model with 3D generation tasks, showcasing its versatility and potential for broader adoption in real-world applications. The project page: [https://linshan-bin.github.io/GeoLRM/](https://linshan-bin.github.io/GeoLRM/).

## 1 Introduction

In fields ranging from robotics to virtual reality, the quality and diversity of 3D assets can dramatically influence both user experience and system efficiency. Historically, the creation of these assets has been a labour-intensive process, demanding the skills of expert artists and developers. While recent years have witnessed groundbreaking advancements in 2D image generation technologies, such as diffusion models  which iteratively refine images, their adaptation to 3D asset creation remains challenging. Directly applying diffusion models to 3D generation  is less than satisfactory, primarily due to a dearth of large-scale and high-quality data. DreamFusion  innovatively optimize a 3D representation  by distilling the score of image distribution from pre-trained image diffusion models . However, this approach lacks a deep integration of 3D-specific knowledge, such as geometric consistency and spatial coherence, leading to significant issues such as the multi-head problem and the inconsistent 3D structure. Additionally, these methods require extensive per-scene optimizations, which severely limits their practical applications.

The introduction of the comprehensive 3D dataset Obiayverse  brings significant advancements for this field. Utilizing this dataset, researchers have fine-tuned 2D diffusion models to produce images consistent with 3D structures . Moreover, recent innovations  have combined these 3D-aware models with large reconstruction models (LRMs)  to achieve rapid and accurate 3D image generation. These methods typically employ large transformers or UNet models that convert sparse-view images into 3D representations in a single forward step. While they excel in speed and maintaining 3D consistency, they confront two primary limitations. Firstly, previous works utilize triplanes  to represent the 3D models, wasting lots of features in regions devoid of actual content and involving dense computations during rendering. This _violates the sparse nature of 3D_ as our analysis shows that the visible portions of the 3D models in the Obiayverse dataset constitute only about 5% of the overall spatial volume. Though Gaussian-based methods  may use pixel-aligned Gaussians for better efficiency, this representation is incapable of recovering the unseen area and thus heavily relies on the input images. Secondly, previous works tend to _overlook the explicit geometric relationships between 3D and 2D images_, which results in ineffective processing. The tri-plane or pixel-aligned Gaussian tokens do not correspond to a specific space in 3D, thus being unable to utilize the projection relationship between 3D points and images. In other words, they conduct dense attention between the 3D queries and the image keys. This leads to the fact that these methods tend to reconstruct 3D with sparse view inputs but cannot achieve better performance with denser inputs.

To address these challenges, we introduce the geometry-aware large reconstruction model (GeoLRM) for 3D Gaussian generation. Our method centres on a 3D-aware reconstruction transformer that eschews conventional representations like triplanes or pixel-aligned Gaussians in favour of a direct interaction within the 3D space. However, directly generating 3D Gaussians in the whole 3D space requires huge memory costs. To this end, we first propose a specialized proposal network to predict an occupancy grid from input images. Only the occupied voxels will be further processed to generate 3D Gaussian features. The proposed transformer replaces the dense cross attention with deformable cross attention . By projecting the input 3D tokens onto the corresponding image planes, these tokens only focus on the most relevant features, which greatly improves the effectiveness.

Figure 1: Image to 3D using GeoLRM. Initially, a 3D-aware diffusion model, specifically SV3D , transforms an input image into multiple views. Subsequently, these views are processed by our GeoLRM to generate detailed 3D assets. **Unlike other LRM-based approaches, GeoLRM notably improves as the number of input views increases.**

We trained our GeoLRM on the Objaverse dataset rendered by  and tested it on the Google Scanned Objects . By integrating geometric principles, our model not only outperforms existing methods with the same number of inputs but also makes it possible to work with denser image inputs. Significantly, the model efficiently handles up to 21 images (even more if necessary), yielding superior 3D models in comparison to those generated from fewer images. Leveraging this capability, we integrated GeoLRM with SV3D  for high-quality 3D model generation.

In summary, our contributions are as follows:

* We introduce a two-stage pipeline that leverages the sparse nature of 3D data, resulting in a sparse 3DGS token representation suitable for extension to high resolution.
* We fully exploit the projection relationship between 3D points and 2D images, significantly reducing the space complexity of attention mechanisms in LRMs, thus enabling denser image input configurations.
* To the best of our knowledge, GeoLRM is the first to process dense inputs using LRM, potentially paving the way for integrating video generation models into 3D AIGC applications.

## 2 Related Work

### Optimization-based 3D reconstruction

3D reconstruction from multi-view images has been extensively studied in computer vision for decades. While traditional methods like SfM [68; 58; 45] and MVS [46; 16] provide basic reconstruction and calibration, they lack robustness and expressiveness. Recent advancements leverage learning-based methods for better performance. Among these methods, NeRF  stands out for its capability of capturing high-frequency details. Following works [2; 3; 34; 77; 8; 53; 4] further improve its performance and speed. Though NeRF has made a great improvement, the need to query tons of points during the rendering process makes it hard for real-time applications. 3D Gaussians  solves this problem by explicitly expressing a scene with 3D Gaussians and utilizing an efficient rasterization pipeline. These methods involve a per-scene optimization process and require dense multi-view images for a good reconstruction.

### Large Reconstruction Model

Different from optimization-based 3D reconstruction methods, large reconstruction models [18; 22; 54; 74; 65; 82; 64] are able to reconstruct 3D shapes in a feed-forward way. As the pioneer work of this area, the LRM  illustrates that the transformer backbone can effectively leverage the power of large-scale datasets and translate image tokens into implicit 3D triplanes under multi-view supervision. Beyond LRM, Instant3D  improves reconstruction quality with sparse-view inputs. It employs a two-stage paradigm, which first generates four views with the diffusion model and then regresses NeRF  from generated multi-view images. Instead of NeRF, InstantMesh  utilizes mesh representation to reconstruct 3D objects, which adopts a differentiable iso-surface extraction module. However, many works [54; 82; 74; 71] choose 3D Gaussians  as the outputs. GRM  proposes a transformer network to translate pixels to the set of pixel-aligned 3D Gaussians while LGM  uses an asymmetric UNet to predict and fuse 3D Gaussians. Compared with these methods, our GeoLRM projects multi-view features to the 3D space with cross-view attention mechanisms, which explicitly explores geometric knowledge.

### 3D generation

Early methods [6; 7; 15; 35; 51; 73; 37] in 3D generation area utilize 3D GANs to generate 3D-aware contents. Although some methods [32; 32; 85; 30; 10; 49; 80] replace 3D GANs with 3D diffusion models for high-quality generation, their generalization ability is bounded by the limited training data. Recently, proposed in DreamFusion , score distillation sampling (SDS) requires no 3D data and is able to leverage the great power of 2D text-to-image diffusion models [44; 43; 42]. Specifically, it optimizes a randomly-initialized 3D model and diffuses the render images with a pretrained diffusion model. As the follow-up works [63; 9; 26; 61; 55; 76; 27; 78; 25; 23; 41], many methods have been proposed to accelerate the optimization process or improve 3D generation quality. Differentwith SDS-based methods, Zero-1-to-3  fine-tunes the 2D diffusion models on a large-scale synthetic dataset to change the camera viewpoint of a given image. Similar to Zero-1-to-3, many other works [47; 60; 48; 75; 29; 67; 31; 69] aim to synthesize multi-view consistent images. Our method can reconstruct 3D contents based on these synthesis multi-view images.

## 3 Methodology

### Overview

Figure 2 illustrates the pipeline of our proposed method. Our approach takes a set of images \(\{I^{i}\}_{i=1}^{N}\) with their corresponding intrinsic \(\{K^{i}\}_{i=1}^{N}\) and extrinsic \(\{T^{i}\}_{i=1}^{N}\) as input. Initially, we encode input images into hierarchical image features and predict an occupancy grid with a proposal transformer. Each occupied voxel within this grid is considered a 3D anchor point. These 3D anchor points are then processed by a reconstruction transformer, refining their geometry and retrieving textural details. The proposal and reconstruction transformers share the same model architecture, which is further discussed in Section 3.2. The outputs of the reconstruction transformer are decoded into Gaussian features with a shallow MLP for rendering. Loss functions are described in Section 3.3.

### Model Architecture

Our model architecture features a hierarchical image encoder for extracting high and low-level image feature maps along with a geometry-aware transformer for lifting 2D features into 3D representations.

Hierarchical Image EncoderOur method integrates both high and low-level features to enhance model performance. For high-level features, we utilize DINOv2 , which excels in single-image 3D tasks . To capture low-level features, we combine Plucker ray embeddings and RGB values. The Plucker ray parameterizes each ray corresponding to a pixel by \(=(,)\), with \(\) representing the ray's direction and \(\) its origin [50; 75]. These embeddings, denoted as \(R^{v}\) for each image \(I^{v}\), are concatenated with the RGB values of the image. This combined data is then integrated through a

Figure 2: **Pipeline of the proposed GeoLRM**, a geometry-powered method for efficient image to 3D reconstruction. The process begins with the transformation of dense tokens into an occupancy grid via a Proposal Transformer, which captures spatial occupancy from hierarchical image features extracted using a combination of a convolutional layer and DINOv2 . Sparse tokens representing occupied voxels are further processed through a Reconstruction Transformer that employs self-attention and deformable cross-attention mechanisms to refine geometry and retrieve texture details with 3D to 2D projection. Finally, the refined 3D tokens are converted into 3D Gaussians for real-time rendering.

convolution layer. The encoding processes are succinctly described by the equations:

\[_{H}^{v} =(I^{v}), \] \[_{L}^{v} =((I^{v},R^{v})), \]

where \(_{H}^{v}\) and \(_{L}^{v}\) represent the high and low-level feature maps of image \(I^{v}\), respectively.

Geometry-aware TransformerThe geometry-aware transformer aims to efficiently lift image features to 3D. The proposal transformer and reconstruction transformer are both instances of this architecture. Previous methods  use tri-planes or pixel-aligned Gaussians to represent 3D contents. However, these data structures make it hard to utilize the projection relationships, causing dense computations. Instead, we use 3D anchor points, which serve as proxies for their surrounding points, significantly reducing the number of points we need to process. As detailed in Figure 2, each transformer block contains a self-attention layer, a deformable cross-attention layer and a feed-forward network (FFN). The model takes \(N\) anchor point features \(_{A}=\{_{i}\}_{i=1}^{N}\) as input tokens. Each token \(_{i}\) comprises the coordinate of the corresponding point and a shared learnable feature.

For the self-attention layer, a crucial problem is how to inject positional information into the sparse 3D tokens. We extend the Rotary Positional Embedding (RoPE)  to 3D conditions for relative positional embedding. For a query \(}\) and a key \(}\) at absolute position \(\) and \(\), we ensure that the inner product of embedded values reflects only the relative position information \(-\). A direct yet promising way is splitting the features into three parts and applying RoPE  on each part with x, y, and z positions respectively.

As we can locate each anchor point in the 3D space, a possible way to lift 2D features to 3D is to project them to the feature maps with known poses and average the corresponding features. However, this method assumes an accurate anchor position, an equal contribution of all images and a good 3D correspondence of input images, which is often impractical, especially in 3D generation tasks. To tackle these issues, we employ deformable attention  for a robust fusion of image features. Given a 3D anchor point feature \(_{i}\), its spatial coordinate \(_{i}\) and multiple feature maps \(\{^{v}\}_{v=1}^{V}\), the deformable attention mechanism is formulated as:

\[(_{i},_{i},\{^{v}\}_{v=1}^{V})=_{ v=1}^{V}w_{v}[_{k=1}^{K}A_{k}^{v}_{iv}+ _{ivk}], \]

where \(k\) indexes the sampled keys and \(K\) is the total sampled key numbers. \(_{iv}\) is the projected 2D coordinate on feature map \(^{v}\) and \(_{ivk}\) is the sampled offset. \(\) indicates the interpolation operation. \(A_{k}\) is the attention weight predicted from \(_{i}\). \(w_{v}\) is a per-view weight derived from the feature it weights. Notably, the prediction of \(_{ivk}\) allows the network to correct the geometry error of anchor points and the inconsistency of input images; The \(w_{v}\) enables different importance levels for each image. To further enhance the representation ability of the model, this mechanism is extended to multi-head and multi-scale conditions.

Given input tokens \(_{A}^{in}\), the transformer block enhances these tokens through a series of sophisticated transformations described as follows:

\[_{A}^{self} =_{A}^{in}+((_ {A}^{in})), \] \[_{A}^{cross} =_{A}^{self}+(( _{A}^{self}),\{(_{H}^{v},_{L}^{v})\}_{v=1}^{V }),\] (5) \[_{A}^{out} =_{A}^{cross}+((_{ A}^{cross})). \]

This design introduces several improvements over the original transformer architecture . By incorporating RMSNorm  for normalization and SiLU  for activation, we achieve more stable training dynamics and better performance.

Post-processingThe proposal network takes a low-resolution dense grid (\(16^{3}\)) as anchor points. The output is upsampled to a high-resolution grid (\(128^{3}\)) with a linear layer. This grid is formulated to represent the occupancy probability of the corresponding area (\([-0.5,0.5]^{3}\)). The reconstruction transformer takes occupied voxels as anchor points. Each output token \(_{i}\) is decoded into multiple 3D Gaussians \(\{_{ij}\}_{j=1}^{M}\) with a multilayer perceptron. The 3D Gaussian \(_{ij}\) is parameterized by the offset \(_{ij}\) regarding the anchor points, 3-channel RGB \(_{ij}\), 3-channel scale \(_{ij}\), 4-channel rotation quaternion \(_{ij}\), and 1-channel opacity \(_{ij}\). We employ activation functions to limit the range of the offset, scale and opacity for better training stability similar to :

\[_{ij} =(_{ij}^{}) o_{}, \] \[_{ij} =(_{ij}^{}) s_{},\] (8) \[_{ij} =(_{ij}^{}), \]

where \(o_{},s_{}\) are predefined maximum values of offsets and scales. Given target camera views \(\{_{t}\}_{t=1}^{T}\), the 3D Gaussians can be further rendered into images \(\{_{t}\}_{t=1}^{T}\), alpha masks \(\{_{t}\}_{t=1}^{T}\) and depth maps \(\{_{t}\}_{t=1}^{T}\) through Gaussian splatting .

### Training Objectives

We employ a two-stage training mechanism for our model. In the first stage, we train the proposal transformer using 3D occupancy ground truth. This stage presents a challenge as it involves a highly unbalanced binary classification task; only about 5% of the voxels are occupied. To address this imbalance, we employ a combination of binary cross-entropy loss and the scene-class affinity loss, as proposed in , to supervise the training process. For the generation of ground truth data, see A.1.

For the second stage, we supervise the rendered \(T\) images, alpha masks and depth maps with corresponding ground truth:

\[=_{t=1}^{T}(_{}( _{t},I_{t})+_{}(_{t},M_{t})+0.2_{ }(_{t},D_{t},I_{t})), \] \[_{}(_{t},I_{t})=||_{t}-I_{t}|| _{2}+2_{}(_{t},I_{t}),\] (11) \[_{}(_{t},M_{t})=||_{t}-M_{t} ||_{2},\] (12) \[_{}(_{t},D_{t},I_{t})=_{t}|}||(- I_{t})(1+|_{t}-D_{t}|) ||_{1}, \]

where \(_{}\) is the perceptual image patch similarity loss , \(|_{t}|\) is the total number of pixels in \(|_{t}|\), \( I_{t}\) is the gradient of the current RGB image and \(\) is the element-wise multiplication operation. As demonstrated in , applying a logarithmic penalty and weighting the per-pixel depth errors with the image gradients result in a smoother geometric representation.

## 4 Experiments

### Datasets

**G-buffer Obiayverse (GObjaverse) :** Used for training. Derived from the original Obiayverse  dataset, GObjaverse includes high-quality renderings of albedo, RGB, depth, and normal images. These images are generated through a hybrid technique combining rasterization and path tracing. The dataset comprises approximately 280,000 normalized 3D models scaled to fit within a cubic space of \([-0.5,0.5]^{3}\). GObjaverse employs a diverse camera setup involving:

* Two orbital paths yielding 36 views per model. This includes 24 views at elevations between 5\({}^{}\) and 30\({}^{}\) (incremented by 15\({}^{}\) rotations) and 12 views at near-horizontal elevations from -5\({}^{}\) to 5\({}^{}\) (with 30\({}^{}\) rotation steps).
* Additional top and bottom views for comprehensive spatial coverage.

**Google Scanned Objects (GSO) :** Used for evaluation, this dataset is rendered similarly to GObjaverse. A random subset of 100 objects is selected to streamline the evaluation process.

**OmniObject3D :** Also used for evaluation, this dataset is consistently rendered like GObjaverse. A random subset of 100 objects is chosen for efficient evaluation.

### Implementation details

Our model features 330 million parameters distributed across two distinct image encoders and two transformers. The first encoder processes geometry with the 6-layer proposal transformer, while the second focuses more on textures crucial with the 16-layer reconstruction transformer. During training, we maintain a maximum number of transformer input tokens of 4k and randomly select 8 views from a possible 38 for supervision. From these 8 views, we randomly select 1 to 7 views as inputs to predict the remaining views. This flexibility in view selection not only tests the robustness of our method but also mimics real-world scenarios where complete data may not always be available. Both input and rendering resolutions are maintained at 448x448 pixels. At the testing and inference stages, we use a resolution of 512x512 to align with existing methods. Besides, the number of input tokens is extended to 16k during testing, showcasing its scalability without the need for fine-tuning. Detailed information on our model's architecture and training procedures can be found in Section A.3.

### Quantitative Results

We evaluated the quality of reconstructed assets from sparse view inputs by analyzing both 2D visual and 3D geometric aspects on the GSO and OmniObject3D dataset . Visual quality was assessed by comparing rendered views to ground truth images using metrics such as PSNR, SSIM, and LPIPS. Geometric accuracy was evaluated by aligning our models to the ground truth coordinate systems and measuring discrepancies using Chamfer Distance and F-Score at a threshold of 0.2, with point samples totalling 16,000 from the ground truth surfaces. Our method was quantitatively compared against established baselines, including LGM , CRM , and InstantMesh . We avoided comparisons with proprietary methods due to the unavailability of their test splits. Similarly, we excluded comparisons with OpenLRM  and TripoSR  as these methods are tailored for single image inputs, which would be unfair to compare with.

Our approach achieved state-of-the-art performance in four out of the five metrics studied. Although InstantMesh showed slightly higher LPIPS on the GSO dataset, attributed to its mesh-based smoothing capabilities, our method demonstrated superior geometric accuracy, benefiting from explicit modelling of the 3D-to-2D relationship.

In another experiment, outlined in Table 3, we observed a notable trend: our model's performance consistently improves with more input views while maintaining low computational costs. This indicates robust scalability, a critical feature for practical applications. In contrast, the performance of InstantMesh , does not follow this pattern. Specifically, InstantMesh shows a decline in performance when the input views increase to 12. This degradation could be due to two primary factors. First, the low-resolution tri-planes may reach their maximum capacity to represent details. Second, the model tends to oversmooth details when handling a large volume of image tokens. Our approach strategically addresses these issues. We employ an extendable sequence of 3D tokens that can be dynamically adjusted to fit the resolution requirements. Additionally, our model features deformable attention mechanisms that intelligently focus on the most pertinent information, preventing the loss of critical details.

   Method & PSNR \(\) & SSIM \(\) & LPIPS \(\) & CD \(\) & FS \(\) \\  LGM & 21.94 & 0.824 & 0.203 & 0.256 & 0.787 \\ CRM & 23.12 & 0.855 & 0.175 & 0.204 & 0.810 \\ InstantMesh & 23.86 & 0.860 & 0.139 & 0.178 & 0.834 \\ Ours & **24.74** & **0.883** & **0.134** & **0.156** & **0.863** \\   

Table 2: Quantitative results on OmniObject3D . **Bold** and underline denote the highest and second-highest scores, respectively.

   Method & PSNR \(\) & SSIM \(\) & LPIPS \(\) & CD \(\) & FS \(\) & Inf. Time (s) & Memory (GB) \\  LGM & 20.76 & 0.832 & 0.227 & 0.295 & 0.703 & **0.07** & 7.23 \\ CRM & 22.78 & 0.843 & 0.190 & 0.213 & 0.831 & 0.30 & 5.93 \\ InstantMesh & 23.19 & 0.856 & **0.166** & 0.186 & 0.854 & 0.78 & 23.12 \\ Ours & **23.57** & **0.872** & 0.167 & **0.167** & **0.892** & 0.67 & **4.92** \\   

Table 1: Quantitative results on Google Scanned Objects (GSO) , where we used six views for inputs and four for evaluation. Inference time and memory usage account only for the reconstruction process. **Bold** and underline denote the highest and second-highest scores, respectively.

[MISSING_PAGE_FAIL:8]

### Ablation Study

In this part, We provide ablation studies for the key designs of our method as shown in Table 4. Due to the limited computational sources, the ablation is done using a smaller reconstruction model (12 layers) and lower resolution (224x224).

Hierarchical Image EncoderOur ablation study underscores the critical role of hierarchical image features in reconstruction tasks, which necessitate both high-level semantic information (e.g., object identity and arrangement) and low-level texture information (e.g., surface patterns and colors). As illustrated in Figure 5, the absence of high-level features leads to model instability, while omitting low-level features results in a loss of textural detail. This dual requirement emphasizes the model's reliance on a comprehensive feature set for accurate image reconstruction. We also performed an ablation study regarding the Plucker ray embeddings in the low-level encoder. These coordinates assist the model in learning camera directions, contributing to an improved performance.

   Method & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\  W/o Plücker rays & 20.64 & 0.826 & 0.244 \\ W/o low-level features & 20.29 & 0.817 & 0.246 \\ W/o high-level features & 15.85 & 0.798 & 0.289 \\ W/o 3D RoPE & 20.52 & 0.827 & 0.224 \\ Fixed \# input views & **20.97** & **0.839** & 0.220 \\ Full model & 20.73 & 0.831 & **0.216** \\    
    &  &  &  \\  & PSNR \(\) & SSIM \(\) & PSNR \(\) & SSIM \(\) & PSNR \(\) & SSIM \(\) \\  Fixed \# input views & 19.72 & 0.822 & 20.85 & 0.833 & 21.43 & 0.838 \\ Full model & **19.94** & **0.835** & **21.16** & **0.840** & **22.04** & **0.853** \\   

Table 4: Ablation study of some key designs. Models are tested on the GSO dataset . Upper: 6 input views and 4 testing views. Lower: different input views. **Bold** and underline denote the highest and second-highest scores, respectively.

Figure 4: Qualitative comparison concerning scalability in input views.

Figure 5: Effects of excluding high-level and low-level features in the image encoder.

3D RoPEIn transformer-based architectures, the role of positional embeddings is critical for accurately interpreting sequence data positions. A key question arises: With the reconstruction transformer employing deformable cross-attention to elevate 2D features to 3D, is positional embedding still necessary? Our ablation studies confirm its necessity. Notably, 3D RoPE significantly enhances the model's ability to handle longer sequences. For instance, increasing the sequence length from 4k to 16k elements, models equipped with 3D RoPE exhibited a PSNR improvement of 0.4, compared to a 0.2 improvement in models lacking 3D RoPE. This observation aligns with the 1D RoPE .

Dynamic InputThe ablation study demonstrates a decrease in performance when employing our dynamic input view strategy compared to the fixed 6-input view setting when the training and testing phases were consistent. Despite this, the dynamic input strategy enhances the model's ability to generalize across different input configurations. This adaptability is critical for handling more complex scenarios, aligning with our primary objectives.

Deformable attentionAs shown in Table 5, the ablation results indicate that increasing the number of sampling points in the deformable attention generally improves performance. Given the trade-off between computational cost and performance gain, we find that using 8 sampling points strikes the best balance.

## 5 Conclusion

In this paper, we present GeoLRM, a geometry-aware large reconstruction model designed to improve the efficiency and quality of 3D generation. Our approach distinguishes itself from previous methods by effectively utilizing the inherent sparsity of 3D structures and explicitly integrating geometric relationships between 3D and 2D images. The GeoLRM framework employs a 3D-aware transformer architecture that predicts 3D Gaussians through a sophisticated coarse-to-fine methodology. Initially, a proposal network estimates coarse occupancy grids, which serve as foundational 3D anchor points for subsequent refinement. The second stage leverages deformable cross-attention to enhance the 3D structure, integrating detailed textural information. Extensive experiments validate that GeoLRM can process higher resolutions and accommodate denser image inputs, outperforming existing models in terms of detail and accuracy. This innovation demonstrates significant potential for real-world applications, particularly in domains where dense view inputs can enhance output quality and user experience. GeoLRM's ability to handle up to 21 images efficiently underscores its scalability and adaptability, paving the way for integration with advanced video generation technologies.

## 6 Limitation

While GeoLRM achieves impressive reconstruction quality, it does so through a two-stage process, which is not inherently end-to-end. This segmentation can lead to the accumulation of errors. The reliance on a proposal network is currently indispensable due to the computational intensity of processing Gaussian points across the entire 3D space. This necessity introduces potential inefficiencies and constraints that could hinder real-time applications. Future research will focus on developing an end-to-end solution that integrates these stages seamlessly, reducing error propagation and optimizing processing time. By addressing these limitations, we aim to enhance the model's robustness and applicability across a broader range of 3D generation tasks.

   Method & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\ 
0 sampling points & 19.52 & 0.802 & 0.265 \\
4 sampling points & 20.21 & 0.819 & 0.238 \\
8 sampling points & 20.73 & 0.839 & 0.220 \\
16 sampling points & **20.80** & **0.846** & **0.219** \\   

Table 5: Ablation study of deformable attention. ‘0 sampling points’ means directly using the projected points without any deformation. **Bold** and underline denote the highest and second-highest scores, respectively.