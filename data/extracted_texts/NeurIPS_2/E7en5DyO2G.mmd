# Bayesian Online Natural Gradient (BONG)

Matt Jones

University of Colorado

mcjones@colorado.edu &Peter Chang

MIT

gyuyoung@mit.edu &Kevin Murphy

Google DeepMind

kpmurphy@google.com

###### Abstract

We propose a novel approach to sequential Bayesian inference based on variational Bayes (VB). The key insight is that, in the online setting, we do not need to add the KL term to regularize to the prior (which comes from the posterior at the previous timestep); instead we can optimize just the expected log-likelihood, performing a single step of natural gradient descent starting at the prior predictive. We prove this method recovers exact Bayesian inference if the model is conjugate. We also show how to compute an efficient deterministic approximation to the VB objective, as well as our simplified objective, when the variational distribution is Gaussian or a sub-family, including the case of a diagonal plus low-rank precision matrix. We show empirically that our method outperforms other online VB methods in the non-conjugate setting, such as online learning for neural networks, especially when controlling for computational costs.

## 1 Introduction

Bayesian methods for neural network (NN) training aim to minimize the Kullback-Leibler divergence between true and estimated posterior distributions. This is equivalent to minimizing the variational loss (or negative ELBO)

\[()=-_{ q_{}}[ p( |)]+D_{}(q_{}|p_{0}) \]

Here \(\) are the network parameters, \(\) are the variational parameters of the approximate posterior \(q_{}()\), \(\) is the training dataset, and \(p_{0}()\) is the prior. The two terms in the variational loss correspond to data fit and regularization to the prior, the latter being analogous to a regularizer \(r()=- p_{0}()\) in traditional point estimation methods like SGD.

An important set of approaches learns the variational parameters by gradient descent on \(()\)(Blundell et al., 2015). More recently Khan and colleagues (Khan et al., 2018; Khan and Rue, 2023; Shen et al., 2024) have proposed using the natural gradient \(_{}^{-1}_{}()\) where \(_{}\) is the Fisher information matrix of the variational family evaluated at \(q_{}\). Natural gradient descent (NGD) is often more efficient than vanilla GD because it accounts for the intrinsic geometry of the variational family (Amari, 1998). Khan and Rue (2023) call this approach the "Bayesian Learning Rule" or BLR. Using various choices for the variational distribution, generalized losses replacing negative log-likelihood, and other approximations, they reproduce many standard optimization methods such as Adam, and derive new ones.

We study Bayesian NN optimization in online learning, where the data are observed in sequence, \(_{t}=\{(_{k},_{k})_{k=1}^{t}\}\), and the algorithm maintains an approximate posterior \(q_{_{t}}(_{t}) p(_{t}|_{t})\), which it updates at each step. Fast updates (in terms of both computational speed and statistical efficiency) are critical for many online learning applications (Zhang et al., 2024). To allow for nonstationarity in the datastream, we include a time index on \(_{t}\), to represent that the parameters may change over time, as is standard for approaches based on state-space models and the extended Kalman filter (see e.g., (Sarkka and Svensson, 2023)). The belief state is updated recursively usingthe prior \(q_{_{t|t-1}}\) derived from the previous time step so that the variational loss becomes

\[(_{t})=-_{_{t} q_{_{t}}}[  p(_{t}|_{t},_{t})]+D_{}q_{_{t}}|q_{_{t|t-1}} \]

One option for this online learning problem is to apply NGD on \((_{t})\) at each time step, iterating until \(_{t}\) converges before consuming the next observation. Our first contribution is a proposal for skipping this inner loop by (a) performing a single natural gradient step with unit learning rate and (b) omitting the \(D_{}\) term in Eq. (2) so that learning is based only on expected loglikelihood:

\[_{t}=_{t|t-1}+_{_{t|t-1}}^{-1}_{ _{t|t-1}}_{_{t} q_{_{t|t-1}}}[ p (_{t}|_{t},_{t})] \]

These two modifications work together: instead of regularizing toward the prior explicitly using \(D_{}q_{_{t}}|q_{_{t|t-1}}\), we do so implicitly by using \(_{t|t-1}\) as the starting point of our single natural gradient step. This may appear as a heuristic but we prove in Proposition 4.1 that it yields exact Bayesian inference when \(q_{}\) and \(p(|,)\) are conjugate and \(q_{}\) is an exponential family with natural parameter \(\). Thus our proposed update can be viewed as a relaxation of the Bayesian update to the non-conjugate variational case. As is common in work on variational inference, we view the result for the conjugate case as a motivating foundation that ensures our method is exact in certain simple settings. The experiments reported in Section 5 and Appendix B complement the theory by showing our method also works well in more general settings. We call Eq. (3) the Bayesian online natural gradient (bong).

Our second contribution concerns ways of computing the expectation in Eqs. (1) to (3). This is intractable for NNs, even for variational distributions that are easy to compute, since the likelihood takes the form \(p(_{t}|_{t},_{t})=p(_{t}|f(_{t},_{t}))\) with \(f(_{t},_{t})\), representing the function computed by the network, is a complex, nonlinear function of \(_{t}\). Many previous approaches have approximated the expected loglikelihood by sampling methods which add variance and computation time depending on the number of samples (Blundell et al., 2015; Shen et al., 2024). We propose a deterministic, closed-form update that applies when the variational distribution is Gaussian (or a sub-family) and the likelihood is an exponential family with natural parameter \(f(_{t},_{t})\) and mean parameter \(h(_{t},_{t})\) (e.g., for classification, \(f\) returns the vector of class logits, \(h\) returns class probabilities, and \(h=(f)\)). This update can be derived in two equivalent ways. First, we use a local linear approximation of the network \(h(_{t},_{t})_{t}(_{t})\)(Immer et al., 2021) and a Gaussian approximation of the likelihood \((_{t}|_{t}(_{t}),_{t})\)(Ollivier, 2018; Tronarp et al., 2018). Under these assumptions the expectation in Eq. (3) can be calculated analytically. Alternatively, we use a different linear approximation \(f(_{t},_{t}) f_{t}(_{t})\) and a delta approximation \(q_{_{t|t-1}}(_{t})_{_{t|t-1}}(_{t})\) where \(_{t|t-1}=_{q_{_{t|t-1}}}[_{t}]\) is the prior mean, so that the expectation in Eq. (3) is replaced by a plugin prediction. The linear(\(h\))-Gaussian approximation is previously known but the linear(\(f\))-delta approximation is new, and we prove in Proposition 4.2 that they yield the same update, which we call linearized bong, or bong-lin. Finally, we discuss different ways of approximating the Hessian of the objective, which is needed for NGD.

Our bong framework unifies several existing methods for Bayesian online learning, and it offers new algorithms based on alternative variational families or parameterizations. We define a large space of methods by combining 4 different update rules with 4 different ways of computing the relevant expected gradients and Hessians and 3 different variational families (Gaussians with full, diagonal, and diagonal-plus-low rank precision matrices). We conduct experiments systematically testing how these factors affect performance. We find support for all three principles of our approach-- NGD, implicit regularization to the prior, and linearization-- in terms of both statistical and computational efficiency. Code for our experiments is available at [https://github.com/petergchang/bong/](https://github.com/petergchang/bong/).

## 2 Related work

Variational inference approximates the Bayesian posterior from within some suitable family in a way that bypasses the normalization term (Zellner, 1988; Jordan et al., 1999). A common choice for the variational family is a Gaussian. For online learning, the exact update equations for Gaussian variational filtering are given by the rvga method of (Lambert et al., 2021). This update is implicit but can be approximated by an explicit rvga update which we show arises as a special case of bong. Most applications of Gaussian VI use a mean-field approximation defined by diagonal covariance, which scales linearly with model size. More expressive but still linear in the model size are methodsthat express the covariance (Tomczak et al., 2020) or precision (Mishkin et al., 2018; Lambert et al., 2023; Chang et al., 2023) as a sum of diagonal and low rank matrices (DLR). In this paper, we consider variational families defined by full covariance, diagonal covariance, and DLR covariance.

For NNs and other complicated models, even the variational approximation can be intractable, so methods have been developed to approximately minimize the VI loss. Bayes by backprop (bbb)(Blundell et al., 2015) learns a variational distribution on NN weights by iterated GD on the VI loss of Eq. (1). They focus on mean-field Gaussian approximations but the approach also applies to other variational families. Here we adapt bbb to online learning to compare to our methods.

The Bayesian learning rule (blr) replaces bbb's GD with NGD (Khan and Rue, 2023). Several variants of blr have been developed including von and vogn for a mean-field Gaussian prior (Khan et al., 2018) and slang for a dlr Gaussian (Mishkin et al., 2018). blr has also been used to derive versions of many classic optimizers including SGD, RMSprop and Adam (Khan et al., 2018; Khan and Rue, 2023; Lin et al., 2024; Shen et al., 2024). Although blr has been applied to online learning, we are particularly interested in Bayesian filtering including in nonstationary environments, where observations must be processed one at time and updates are based on the posterior from the previous step, often in conjunction with parameter dynamics. We therefore develop filtering versions of blr to compare to bong, some of which reduce to von, vogn and slang in the batch setting, while others are novel. We also note blr is a mature theory including several clever tricks we have not yet incorporated into our framework.

Khan and Rue (2023) observe that conjugate updating is equivalent to one step of blr with learning rate 1. This is similar to our Proposition 4.1 except that blr retains the KL term in the variational loss. blr and bong agree in this case because the gradient of the KL is zero on blr's first iteration: \(_{=_{t|-1}}D_{}(q_{}|q_{_{t|-1}})=0\). Therefore bong can be seen as a special case of blr with one update step per observation and learning rate 1. Our contribution is to recognize that doing a single update step allows the KL term to be dropped entirely, yielding a substantially simpler algorithm which our experiments show also performs better.

While blr allows alternative losses in place of the NLL in Eq. (2), we can also replacing the KL term with other divergences (Knoblauch et al., 2022). Our approach fits within that "generalized VB" framework in that it drops the divergence altogether. Our approach of implicitly regularizing to the prior using a single NGD step is also similar to the implicit MAP filter of (Bencomo et al., 2023) which performs truncated GD from the prior mode. The principal difference is they perform GD on model parameters (\(_{t}\)) while we do NGD on the variational parameters (\(_{t}\)). Thus bong maintains a full prior and posterior while IMAP is more concerned with how the choice of optimizer can substitute for explicit tracking of covariance.

We show two other ways to derive the bong update in Appendix D, one of which is to replace the expected NLL in Eq. (2) with a linear approximation and solve the resulting equation exactly. Several past works have taken this approach, arriving at updates similar to ours. Cherief-Abdellatif et al. (2019) study streaming variational Bayes and propose solving Eq. (2) with a linearized expected NLL. When the variational family is an exponential family their update becomes NGD (Khan and Lin, 2017) and matches the bong update. Hoeven et al. (2018) show how mirror descent can be derived as a special case of Exponential Weights (Littlestone and Warmuth, 1994), which is closely related to Bayesian updating. The resulting algorithm is similar to bong and follows from linearizing the NLL instead of expected NLL, with an additional delta assumption at the prior mean. Lyu and Tsang (2021) study relaxed block-box optimization where the objective is \(_{}_{ q_{}}[f()]\) for some target function \(f\). They use a mirror descent formulation with linearized expected loss and KL regularizer and show the resulting update is NGD on expected loss, formally equivalent to our BONG update. From the perspective of this prior work, our contribution is to express the bong update simply as NGD on the expected NLL, motivated by replacing the KL with implicit regularization, and to show how this yields a variety of known and novel algorithms for Bayesian filtering.

EKF applications to NNs apply Bayesian filtering using a local linear approximation of the network, leading to simple closed form updates (Singhal and Wu, 1989; Puskorius and Feldkamp, 1991). The classic EKF assumes a Gaussian observation distribution but it has been extended to other exponential families (e.g. for classification) by matching the mean and covariance in what we call the conditional moments EKF (cm-ekf)(Ollivier, 2018; Tronarp et al., 2018). Applying a KL projection to diagonal covariance yields the variational diagonal EKF (vd-ekf)(Chang et al., 2022).

Alternatively, projecting to diagonal plus low rank precision using SVD gives Lo-fi(Chang et al., 2023). We derive all these methods as special cases of bong-lin. Further developments in this direction include the method of (Titsias et al., 2024) which does Bayesian filtering on only the final weight layer, and WoLF(Duran-Martin et al., 2024) which achieves robustness to outliers through data-dependent weighting of the loglikelihood.

## 3 Background

We study online supervised learning where the agent receives input \(_{t}^{D}\) and observation \(_{t}^{C}\) on each time step, which it aims to model with a function \(f_{t}(_{t})=f(_{t},_{t})\) such as a NN with weights \(_{t}^{P}\). The predictions for \(_{t}\) are given by some observation distribution \(p(_{t}|f_{t}(_{t}))\). For example, \(f\) may compute the mean for regression or the class logits for classification.

We work in a Bayesian framework where the agent maintains an approximate posterior distribution over \(_{t}\) after observing data \(_{t}=\{(_{k},_{k})_{k=1}^{t}\}\). The filtering posterior \(q_{_{t}}(_{t}) p(_{t}|_{t})\) is approximated within some parametric family indexed by the variational parameter \(_{t}\). We allow for nonstationarity by assuming \(\) changes over time according to some dynamic model \(p(_{t}|_{t-1})\). By pushing the posterior from step \(t-1\) through the dynamics we obtain a prior for step \(t\) given by \(q_{_{t|t-1}}(_{t}) p(_{t}|_{t-1})\). For example suppose the variational posterior from the previous step is Gaussian, \(q_{_{t-1}}(_{t-1})=(_{t-1}|_{ t-1},_{t-1})\), and the dynamics model is an Ornstein-Uhlenbeck process, as proposed in prior work (Kurle et al., 2020; Titsias et al., 2024) to handle non-stationarity, i.e., the dynamics model has the form \(_{t}(_{t}_{t-1}+(1-_{t})_{0},_{t})\), where \(_{t}=(1-_{t}^{2})_{0}\) is the covariance of the noise process, \(0_{t} 1\) is the degree of drift, and \(p(_{0})=(_{0},_{0})\) is the prior. In this case, the parameters of the prior predictive distribution are \(_{t|t-1}=_{t}_{t-1}+(1-_{t})_{0}\) and \(_{t|t-1}=_{t}^{2}_{t-1}+_{t}\). In general the predict step may require approximation to stay in the variational family (e.g., if the dynamics are nonlinear). In this paper, our focus is the update step from \(_{t|t-1}\) to \(_{t}\) upon observing \((_{t},_{t})\), so for simplicity we assume constant (static) parameters, i.e., \(p(_{t}|_{-1})=(_{t}-_{t-1})\) (equivalently \(_{t}=1\)), so \(_{t|t-1}=_{t-1}\); however, our method can trivially handle non-stationary parameters.

Variational inference seeks an approximate posterior that minimizes the KL divergence from the exact Bayesian update from the prior. In the online setting this becomes

\[_{t}^{*}=*{arg\,min}_{}D_{}q_{ }(_{t})|Z_{t}^{-1}q_{_{t|t-1}}(_{t})\,p (_{t}|f_{t}(_{t}))=*{arg\,min}_{ }_{t}() \]

where \(_{t}\) is the online VI loss defined in Eq. (2), and the normalization term \(Z_{t}\) (which depends on \(_{t}\)) drops out as an additive constant. Our goal is an efficient approximate solution to this variational optimization problem.

We will sometimes assume the variational posterior \(q_{}\) is an exponential family distribution with natural parameter \(\) so that \(q_{_{t}}(_{t})=_{t}^{*}T(_{ t})-(_{t})\), with log-partition function \(\) and sufficient statistics \(T(_{t})\). Assuming \(\) is strictly convex (which holds in the cases we study) there is a bijection between \(_{t}\) and the dual (or expectation) parameter \(_{t}=_{_{t}_{t}}[T(_{t})]\). Classical thermodynamic identities imply that the Fisher information matrix has the form \(_{_{t}}=_{t}/_{t}\). This has important implications for NGD on exponential families (Khan and Rue, 2023) because it implies that for any function \(\) defined on the variational parameter space the natural gradient wrt natural parameters \(_{t}\) is the regular gradient wrt the dual parameters \(_{t}\), i.e., \(_{_{t}}^{-1}_{_{t}}=_{_{ t}}\).

## 4 Methods

We propose to approximate the variational optimization problem in Eq. (4) using the bong update in Eq. (3). When \(q_{}\) is an exponential family, the fact that the natural gradient wrt the natural parameters \(_{t}\) is the regular gradient wrt the dual parameters \(_{t}\) implies an equivalent mirror descent form (see Appendix D for further analysis of bong from the MD perspective):

\[_{t}=_{t|t-1}+_{_{t|t-1}}_{_{t} q_{_{t|t-1}}}[ p(_{t}|_{t},_{t})] \]

This is NGD with unit learning rate on the variational loss in Eq. (2) but ignoring the \(D_{}q_{}|q_{_{t|t-1}}\) term. In this section we first prove this method is optimal when the model is conjugate and then describe extensions to more complex cases of practical interest.

### Conjugate case

Our approach is motivated by the following result which states that bong matches exact Bayesian inference when the variational distribution and the likelihood are conjugate exponential families:

**Proposition 4.1**.: _Let the observation distribution (likelihood) be an exponential family with natural parameter \(_{t}\) (where \(T_{l}(_{t})=_{t}\) is the sufficient statistics for the likelihood and \(A(_{t})\) is the log-partition function)_

\[p_{t}(_{t}|_{t})=(_{t}^{}_{ t}-A(_{t})-b(_{t})) \]

_and let the prior be the conjugate exponential family_

\[q_{_{t|t-1}}(_{t})=(_{t|t-1}^{}T (_{t})-(_{t|t-1})) \]

_with \(T(_{t})=[_{t};-A(_{t})]\). Then the exact Bayesian update agrees with Eq. (5)._

The proof is in Appendix C. Writing the natural parameters of the prior as \(_{t|t-1}=[_{t|t-1};_{t|t-1}]\), we show the Bayesian update and bong both yield \(_{t}=_{t|t-1}+_{t}\) and \(_{t}=_{t|t-1}+1\). Intuitively, we are just accumulating a sum of the observed sufficient statistics, and a counter of the sample size (number of observations seen so far).

### Variational case

In practical settings the conjugacy assumption of Proposition 4.1 will not be met, so Eqs. (3) and (5) will only approximate the Bayesian update. In this paper we restrict to Gaussian variational families. We refer to the unrestricted case as FC (full covariance), defined by the variational distribution

\[q_{_{t|t-1}}(_{t})=(_{t}|_{t|t-1},_{t|t-1}) \]

where \(_{t|t-1}\) can be any positive semi-definite (PSD) matrix. The natural and dual parameters are \(=(^{-1},-(^{-1}))\) and \(=(,(^{}+))\). Appendix E.1.1 shows that Eq. (5) translated back to \((,)\) gives the following bong update for the FC case:

\[_{t} =_{t|t-1}+_{t}_{_{t} q_{_{t|t-1}}}[_{_{t}} p(_{ t}|f_{t}(_{t}))]}_{_{t}} \] \[_{t}^{-1} =_{t|t-1}^{-1}-_{_{t } q_{_{t|t-1}}}[_{_{t}}^{2} p(_{t}|f_{ t}(_{t}))]}_{_{t}} \]

which matches the explicit update in the RVGA method of (Lambert et al., 2021).

### Monte Carlo approximation

The integrals over the prior \(q_{_{t|t-1}}\) in Eqs. (9) and (10) are generally intractable and must be approximated. One option is to use Monte Carlo, in what we call bong-mc. Given \(M\) independent samples \(}_{t}^{(m)} q_{_{t|t-1}}\), we estimate the expected gradient \(_{t}=_{_{t} q_{_{t|t-1}}}[_{ {}_{t}} p(_{t}|f_{t}(_{t}))]\) and expected Hessian \(_{t}=_{_{t} q_{_{t|t-1}}} _{_{t}}^{2} p(_{t}|f_{t}(_{t}) )\) as the empirical means

\[_{t}^{-}}} =_{m=1}^{M}}_{t}^{(m)}, }_{t}^{(m)}=_{_{t}=}_{t}^{(m)}}  p(_{t}|f_{t}(_{t})) \] \[_{t}^{}}}} =_{m=1}^{M}}_{t}^{(m)}, }_{t}^{(m)}=_{_{t}=}_{t}^{(m)}} ^{2} p(_{t}|f_{t}(_{t})) \]

We use \(^{}}}}\) only for small models. Otherwise we use empirical Fisher (Section 4.5).

### Linearized bong

As an alternative to bong-mc, we propose a linear approximation we call bong-lin that yields a deterministic and closed-form update. Assume the likelihood is an exponential family as in Proposition 4.1 but with natural parameter predicted by some function \(f_{t}(_{t})=f(_{t},_{t})\):

\[p(_{t}|_{t},_{t})=(f_{t}(_{t})^{ }_{t}-A(f_{t}(_{t}))-b(_{t})) \]We also define the dual (moment) parameter of the likelihood as \(h_{t}(_{t})=[_{t}|f_{t}(_{t})]\). In a NN, \(f_{t}\) and \(h_{t}\) are related by the final response layer. For example in classification \(f_{t}\) and \(h_{t}\) give the class logits and probabilities, with \(h_{t}(_{t})=(f_{t}(_{t}))\), with \(_{t}\) being the one-hot encoding.

We now define two methods for approximating the expected gradient \(_{t}\) and expected Hessian \(_{t}\), based on linearizing the predictive model at the prior mean \(_{t|t-1}\) in terms of either \(f_{t}(_{t})\) or \(h_{t}(_{t})\), and then prove their equivalence.

The **linear(\(h\))-Gaussian** approximation (Ollivier, 2018; Tronarp et al., 2018) linearizes \(h_{t}(_{t})\)

\[_{t}(_{t}) =}_{t}+_{t}(_{t}-_{t|t-1}) \] \[}_{t} =h_{t}(_{t|t-1})\] (15) \[_{t} =}{_{t}}_{|_{t }=_{t|t-1}} \]

and approximates the likelihood by a Gaussian with variance based at \(_{t|t-1}\)

\[_{t}^{}(_{t}|_{t})=(_{t}| _{t}(_{t}),_{t}),_{t}= [_{t}|_{t}=_{t|t-1}] \]

The **linear(\(f\))-delta approximation** linearizes \(f_{t}(_{t})\) and maintains the original exponential family likelihood distribution in Eq. (13)

\[_{t}(_{t}) =f_{t}(_{t|t-1})+_{t}(_{t}-_ {t|t-1}) \] \[_{t} =}{_{t}}_{|_{t} =_{t|t-1}}\] (19) \[_{t}^{}(_{t}|_{t}) (_{t}(_{t})^{}_{ t}-A(_{t}(_{t}))-b(_{t})) \]

It also uses a plug-in approximation that replaces \(q_{_{t|t-1}}(_{t})\) with a point mass \(_{_{t|t-1}}(_{t})\) so that the expected gradient and Hessian are approximated by their values at the prior mean, i.e., \(_{_{t}=_{t|t-1}}_{t}^{}( _{t}|_{t})\) and \(_{_{t}=_{t|t-1}}^{2}_{t}^{}( {y}_{t}|_{t})\), rather than being sampled.

**Proposition 4.2**.: _Under a Gaussian variational distribution, the linear(\(h\))-Gaussian and linear(\(f\))-delta approximations yield the same values for the expected gradient and Hessian_

\[_{t}^{} =_{t}^{}_{t}^{-1}(_{t}-}_{t}) \] \[_{t}^{} =-_{t}^{}_{t}^{-1}_{t} \]

See Appendix C for the proof. The main idea for the \(_{t}^{}\) part is that the linear-Gaussian assumptions make the gradient linear in \(_{t}\) so the expected gradient equals the gradient at the mean. The main idea for the \(_{t}^{}\) part is that eliminating the Hessian of the NN requires different linearizing assumptions for the Gaussian and delta approximations, and the remaining nonlinear terms (from the log-likelihood in Eq. (13)) agree because of the property of exponential families that the Hessian of the log-partition \(A\) equals the conditional variance \(_{t}\).

Applying Proposition 4.2 to Eqs. (9) and (10) gives the bong-lin update for a FC Gaussian prior

\[_{t} =_{t|t-1}+_{t}(_{t}-}_{t}) \] \[_{t} =_{t|t-1}-_{t}_{t}_{t| t-1}\] (24) \[_{t} =_{t|t-1}_{t}^{}( _{t}+_{t}_{t|t-1}_{t}^{})^{-1} \]

where \(_{t}\) is the Kalman gain matrix (see Appendix E.1.2). This matches the cm-ekf(Tronarp et al., 2018; Ollivier, 2018).

### Empirical Fisher

The methods in Sections 4.3 and 4.4 require explicitly computing the Hessian of the loss (mc-hess) or the Jacobian of the network (lin-hess). These are too expensive for large models or high-dimensional observations. Instead we can use an empirical Fisher approximation that replaces the Hessian with the outer product of the gradient (see e.g, (Martens, 2020)).

  Name & Eqs. \\  mc-hess & (11), (12) \\ lin-hess & (21), (22) \\ mc-ef & (11), (26) \\ lin-ef & (28), (29) \\  

Table 1: The 4 Hessian approximations.

For the mc-ef variant, we make the following approximation:

\[_{t}^{}=-}_{t}^{(1:M)}}_{t}^{(1:M)^{}} \]

where \(}_{t}^{(1:M)}=[}_{t}^{(1)},,}_{t}^{(M )}]\) is the \(P M\) matrix of gradients from the MC samples.

We can also consider a similar approach for the lin-ef variant that is Jacobian-free and sampling-free. Note that if \(}_{t}\) were the true value of \([_{t}|_{t}]\) (i.e., if the model were correct) then we would have \([(_{t}-}_{t})(_{t}-}_{t})^{ }]=_{t}\), implying \([_{t}^{}(_{t}^{}) ^{}]=-_{t}^{}\). This suggests using

\[_{t}^{} =_{_{t}=_{t|t-1}}[- (_{t}-h_{t}(_{t}))^{}_{t}^{-1}( _{t}-h_{t}(_{t}))] \] \[=((_{t})}{ _{t}})^{}_{_{t}=_{t|t-1}}_{t}^{-1}( _{t}-h_{t}(_{t|t-1}))=_{t}^{}\] (28) \[_{t}^{} =-_{t}^{}(_{t}^{}) ^{} \]

where Eq. (29) is the EF approximation to Eq. (22).

A more accurate EF approximation is possible by sampling virtual observations \(}_{t}\) from \(p(|f_{t}(}_{t}^{(m)}))\) or \(p(|f_{t}(_{t|t-1}))\) and using them for the gradients in Eq. (26) or Eq. (29) (respectively) (Martens, 2020; Kunstner et al., 2020). However, in our experiments we use the actual observations \(_{t}\) which is faster and follows previous work (e.g., (Khan et al., 2018)).

### Update rules

In addition to the four ways of approximating the expected Hessian (summarized in Table 1), we also consider four variants of bong, based on what kind of loss we optimize and what kind of update we perform, as we describe below. See Table 2 for a summary.

**bong** (Bayesian online natural gradient) performs one step of NGD on the expected log-likelihood. We set learning rate to \(_{t}=1\) since this is optimal for conjugate models. The update (for an exponential variational family) is as in Eq. (5):

\[_{t}=_{t|t-1}+_{_{t|t-1}}_{_{t} q_{_{t|t-1}}}[ p(_{t}|_{t},_{t})] \]

**bog** (Bayesian online gradient) performs one step of GD (instead of NGD) on the expected log-likelihood. We include a learning rate \(\) because GD does not have the scale-invariance of NGD:

\[_{t}=_{t}+_{t}_{_{t}}_{_{t} q_{_{t}}}[ p(_{t}|f_{t}(_{t}))] \]

**blr** (Bayesian learning rule, (Khan and Rue, 2023)) uses NGD (like bong) but optimizes the VI loss using multiple iterations, instead of optimizing the expected NLL with a single step. When modified to the online setting, blr starts an inner loop at each time step with \(_{t,0}=_{t|t-1}\) and iterates

\[_{t,i}=_{t,i-1}+_{t}_{_{t,i-1}} _{_{t,i-1}}_{_{t} q_{_ {t,i-1}}}[ p(_{t}|f_{t}(_{t}))]-D_{}q_{ _{t,i-1}}|q_{_{t|t-1}} \]

For an exponential variational family this can be written in mirror descent form

\[_{t,i}=_{t,i-1}+_{t}_{_{t,i-1}} _{_{t} q_{_{t,i-1}}}[ p(_{t}|f_{t} (_{t}))]-D_{}q_{_{t,i-1}}|q_{_{ t|t-1}} \]

**bbb** (Bayes By Backprop, (Blundell et al., 2015)) is like blr but uses GD instead of NGD. When adapted to online learning, it starts each time step at \(_{t,0}=_{t|t-1}\) and iterates with GD:

\[_{t,i}=_{t,i-1}+_{t}_{_{t,i-1}} _{_{t} q_{_{t,i-1}}}[ p(_{t}|f_{t}( _{t}))]-D_{}q_{_{t,i-1}}|q_{_{t|t -1}} \]

  Name & Loss & Update \\  bong & \([]\) & NGD(\(I=1\)) \\ bog & \([]\) & GD(\(I=1\)) \\ blr & VI & NGD(\(I 1\)) \\ bbb & VI & GD(\(I 1\)) \\  

Table 2: The 4 update algorithms.

### Variational families and their parameterizations

We investigate five variational families for the posterior distribution: (1) FC Gaussian using natural parameters \(=(^{-1},-^{-1})\), (2) FC Gaussian using central moment parameters \(=(,)\), (3) diagonal Gaussian using natural parameters \(=(^{-2},-^{-2})\) (using elementwise exponents and products), (4) diagonal Gaussian using central moment parameters \(=(,^{2})\), and (5) DLR Gaussian with parameters \(=(,,})\) and precision \(^{-1}=+}}^{}\) where \(^{P P}\) is diagonal and \(}^{P}\) with \(R P\). The moment parameterizations are included to test the importance of using natural parameters per Proposition 4.1. The diagonal family allows learning of large models because it scales linearly in the model size \(P\). DLR also scales linearly but is more expressive than diagonal, maintaining some of the correlation information between parameters that is lost in the mean field (diagonal) approximation (Lambert et al., 2023; Mishkin et al., 2018; Chang et al., 2023).

Optimizing the bong objective wrt \((,,})\) using NGD methods is challenging because the Fisher information matrix in this parameterization cannot be efficiently inverted. Instead we first derive the update wrt the FC natural parameters (leveraging the fact that the prior \(^{-1}_{t|t=1}\) is DLR to make this efficient), and then use SVD to project the posterior precision back to low-rank form, following our prior lo-fi work (Chang et al., 2023). However, if we omit the Fisher preconditioning matrix and use GD as in bog and bbb, we can directly optimize the objective wrt \((,,})\) (see Appendix E.5).

### Overall space of methods

Crossing the four algorithms in Table 2, the four methods of approximating the Hessian in Table 1, and the five variational families yields 80 algorithms. Table 3 shows the 36 based on the three tractable Hessian approximations and the three variational families that use natural parameters. Update equations for all the algorithms are derived in Appendix E. Pseudocode is given in Appendix A.

    & &  \\  Update & Hessian &  &  &  \\  BONG & MC-EF & \(O(MP^{2})\) [rwga] & \(O(MP)\) & \(O((R+M)^{2}P)\) \\ BLR & MC-EF & \(O(I^{3})\) & \(O(IMP)\) [von] & \(O(I(R+M)^{2}P\) [slang] \\ BOG & MC-EF & \(O(P^{3})\) & \(O(MP)\) & \(O(RMP)\) \\ BBB & MC-EF & \(O(I^{3})\) & \(O(IMP)\) [bbb] & \(O(IR(R+M)P)\) \\  BONG & LIN-HESS & \(O(CP^{2})\) [cm-ekf] & \(O(C^{2}P)\) [vd-ekf] & \(O((R+C)^{2}P)\) [lo-fi] \\ BLR & LIN-HESS & \(O(I^{3})\) & \(O(IC^{2}P)\) & \(O(I(2R+C)^{2}P)\) \\ BOG & LIN-HESS & \(O(P^{3})\) & \(O(C^{2}P)\) & \(O(C(C+R)P)\) \\ BBB & LIN-HESS & \(O(IP^{3})\) & \(O(IC^{2}P)\) & \(O(I(C+R)RP)\) \\  BONG & LIN-EF & \(O(P^{2})\) & \(O(P)\) & \(O(R^{2}P)\) \\ BLR & LIN-EF & \(O(IP^{3})\) & \(O(IP)\) & \(O(IR^{2}P)\) \\ BOG & LIN-EF & \(O(P^{3})\) & \(O(P)\) & \(O(RP)\) \\ BBB & LIN-EF & \(O(IP^{3})\) & \(O(IP)\) & \(O(IR^{2}P)\) \\   

Table 3: Time complexity of the algorithms. \(P\): params, \(C\): observation dim, \(M\): MC samples, \(I\): iterations, \(R\): DLR rank. We assume \(P\{I,C,R,M\}\) so display only the terms of leading order in \(P\). Time complexities for mc-hess algorithms (not shown) are always at least as great as for the corresponding mc-ef. Full (full covariance) and Diag (diagonal covariance) columns indicate natural parameters; corresponding algorithms using moment parameters have the same complexities except bog-fc.mom which is \(O(MP^{2})\) for mc-ef, \(O(CP^{2})\) for lin-hess, and \(O(P^{2})\) for lin-ef. [Names] correspond to the following existing methods (or variants thereof) in the literature: rvga: (Lambert et al., 2021) (explicit update version); von: (Khan et al., 2018b) (modified for online); slang: (Mishkin et al., 2018) (modified for online); bbb: (Blundell et al., 2015) (modified for online and uses moment parameters); cm-ekf: (Ollivier, 2018; Tronarp et al., 2018); vd-ekf: (Chang et al., 2022); lo-fi: (Chang et al., 2023).

Experiments

This section presents our primary experimental results. These are based on mnist (\(D=784\), \(N_{}=60\)k, \(N_{}=10\)k, \(C=10\) classes) [LeCun et al., 2010]. See Appendix B for more details on these experiments, and more results on MNIST and other datasets. We focus on training on a prefix of the first \(T=2000\) examples from each dataset, since our main interest is in online learning from potentially nonstationary distributions, where rapid adaptation of a model in response to a small number of new data points is critical.

Our primary evaluation objective is the negative log predictive density (NLPD) of the test set as a function of the number of training points observed so far.1 It is defined as \(_{t}=-}}_{i^{}} [ p(_{i}|f(_{i},_{t}))q_{_{t}}( _{t})_{t}]\). We approximate this integral in two main ways: (1) using Monte Carlo sampling2,or (2) using a plugin approximation, where we replace the posterior \(q_{_{t}}(_{t})\) with a delta function centered at the mean, \((_{t}-_{t})\).

## 6 Conclusions, limitations and future work

Our experiment results show benefits of bong's three main principles: NGD, implicit regularization to the prior, and linearization. The clear winner across datasets and variational families is bong-lin-hess, which embodies all three principles. Blr-lin-hess nearly matches its performance but is much slower. Several of the best-performing algorithms are previously known (notably cm-ekf and lo-fi) but we explain these results within a systematic theory that also offers new methods (including blr-lin-hess).

Bong is motivated by Proposition 4.1 which applies only in the idealized setting of a conjugate prior. Nevertheless we find it performs well in non-conjugate settings. On the other hand our experiments are based on relatively small models and datasets. It will be important to test how our methods scale up, especially using the promising DLR representation.

Figure 2: Performance on MNIST using Lin-MC posterior predictive, where the posterior is computed using bong with different variational families, namely diagonal (natural and moment), blr-1, blr-10.

Figure 1: Performance on MNIST using Lin-MC posterior predictive, where the posterior is computed using bong, bog, bbb and blr and the 3 tractable Hessian approximations with blr-10 variational family.