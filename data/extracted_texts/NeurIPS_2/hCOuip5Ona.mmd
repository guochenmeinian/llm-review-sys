# Continuous Partitioning for Graph-Based Semi-Supervised Learning

Chester Holtz

Halicioglu Data Science Institute

University of California San Diego

La Jolla, CA

chholtz@ucsd.edu

&Pengwen Chen

Department of Applied Mathematics

National Chung Hsing University

South District, Taichung, Taiwan

pengwen@email.nchu.edu.tw

Zhengchao Wan

Department of Mathematics

University of Missouri

Columbia, MO

zwan@missouri.edu

&Chung-Kuan Cheng

Department of Computer Science

University of California San Diego

La Jolla, CA

ckcheng@ucsd.edu

&Gal Mishne

Halicioglu Data Science Institute

University of California San Diego

La Jolla, CA

gmishne@ucsd.edu

###### Abstract

Laplace learning algorithms for graph-based semi-supervised learning have been shown to suffer from degeneracy at low label rates and in imbalanced class regimes. Here, we propose CutSSL: a framework for graph-based semi-supervised learning based on continuous nonconvex quadratic programming, which provably obtains _integer_ solutions. Our framework is naturally motivated by an _exact_ quadratic relaxation of a cardinality-constrained minimum-cut graph partitioning problem. Furthermore, we show our formulation is related to an optimization problem whose approximate solution is the mean-shifted Laplace learning heuristic, thus providing new insight into the performance of this heuristic. We demonstrate that CutSSL significantly surpasses the current state-of-the-art on \(k\)-nearest neighbor graphs and large real-world graph benchmarks across a variety of label rates, class imbalance, and label imbalance regimes. Our implementation is available on github1.

## 1 Introduction

In semi-supervised learning (SSL), a learner is given access to a partially-labeled training set consisting of labeled examples and unlabeled examples. The goal in this setting is to learn a predictor that is superior to a predictor that is trained using the labeled examples alone. This framework is motivated by the high cost of obtaining annotated data on practical problems. For problems where very few labels are available, the geometry of the unlabeled data can be used to significantly improve the performance of classic machine learning models. A seminal work in graph-based semi-supervised learning is Laplace learning , which seeks a harmonic function that extends provided labels overthe unlabeled vertices (a _harmonic extension_ of the labeled vertices). Laplace learning and its variants have been widely applied in semi-supervised and graph-structured learning [37; 36; 2; 20].

However, recent work [26; 1] has demonstrated that 'vanilla' Laplace learning methods exhibit poor prediction error at low label rates, primarily due to degenerate estimates near the decision boundary--although this can be partially addressed in practice via a simple mean-shift of the predictions . Furthermore, in the context of classification, the labels often correspond to elements from a discrete set. In this setting, typically a thresholding operation is applied post hoc to the harmonic extension to map from continuous predictions to the discrete set of labels. This thresholding can further exacerbate the aforementioned degeneracy. Furthermore, when class sizes are imbalanced, approximate heuristics are often employed to ensure satisfaction of the volume (class-cardinality) prior [10; 19]. In particular,  apply efficient auction algorithms  to volume-constrained semi-supervised learning. They do this by iteratively solving linearizations of a quadratic problem under volume and box constraints to reach a local solution. The authors claim convergence to a local solution due to monotonic decrease of the objective, although local solutions may be poor and convergence can be slow.

In this paper, we formulate cardinality-constrained semi-supervised learning as a nonconvex quadratic program, a generalization of the assignment problem. Notably, in contrast to semidefinite or spectral relaxations, our formulation is exact in that it has a \(0-1\) minimizer corresponding to a maximally smooth discrete label assignment. Furthermore, our method has connections to spectral graph partitioning and the mean-shift Laplace learning heuristic.

Our method involves solving a sequence of quadratic programs via Alternating Direction Method of Multipliers (ADMM) , which exhibits robust convergence guarantees, having also been applied successfully to Quadratic Assignment Problems and similar Semi-Definite Program (SDP)-based relaxations of nonconvex Boolean constrained problems . As we will show, the ADMM iterates can be solved efficiently, particularly when the underlying graph is sparsely connected.

Our contributions are as follows:

1. We introduce a framework derived from an exact formulation of a cardinality-constrained graph partitioning problem with supervision, _a non-degenerate_ problem in the unlabeled data limit. We also prove sufficient conditions for exact recovery of integer solutions.
2. Numerically, we develop a scalable and convergent iterative method based on ADMM and demonstrate superior performance and scalability in low, medium, high, and imbalanced label rate and data regimes on \(k\)-NN graphs (MNIST, Fashion-MNIST, and Cifar-10) as well as Planetoid citation networks and large-scale (160k-2.5M nodes) OGB graphs compared to state-of-the-art graph-based SSL algorithms.
3. We describe a connection to Laplace learning, and derive a simple and intuitive explanation for the efficacy of the mean-shift heuristic.

## 2 Preliminaries

In this section, we review graph semi-supervised learning, Laplace learning, the combinatorial minimum cut problem, and an associated continuous extension.

### Laplace learning

Let \(V=\{v_{1},v_{2},,v_{M}\}\) denote the \(M\) vertices of the graph \(G\) with weight matrix \(W\) whose entries \(w_{ij} 0\) are the edge weights between \(v_{i}\) and \(v_{j}\). We assume the graph is symmetric, i.e., \(w_{ij}=w_{ji}\). We define the degree \(d_{i}=_{j=1}^{n}w_{ij}\). Without loss of generality, we assume the first \(m\) vertices \(l=\{v_{1},v_{2},,v_{m}\}\) are assigned labels \(\{_{1},_{2},,_{m}\}\), where \(0<m M\). In the context of \(k\)-class classification we take each \(_{i}\) to be one of the \(k\) standard basis vectors \(\{_{1},_{2},,_{k}\}\) of the form \(_{i}=(0, 0,1,0,,0)\), i.e. a one-hot row vector. Let \(n\) denote the number of unlabeled vertices, i.e. \(n=M-m\). The problem of graph-based semi-supervised learning is to smoothly propagate the labels over the unlabeled vertices \(u=\{v_{m+1},v_{m+2},,v_{M}\}\).

Given a graph and a set of labeled vertices, the solution of Laplace learning , is the minimizer of the following quadratic program with label constraints \((X_{0})_{i}=y_{i}\):

\[_{X_{0}^{M k}}\{(X_{0}^{}X_{ 0}):(X_{0})_{i}=y_{i},\;1 i m\}, \]

where \(X_{0}^{M k}\), \(\) is the combinatorial graph Laplacian given by \(=D-W\), where \(D=(W_{M})\) is a diagonal matrix whose elements are the node degrees, The prediction for vertex \(v_{i}\) is determined by a heuristic of thresholding the largest component of \((X_{0})_{i}\):

\[*{arg\,max}_{j\{1,,k\}}\{(X_{0})_{ij}\}. \]

Note that Laplace learning is also called _label propagation (LP)_, since the Laplace equation associated with (1), can be solved by repeatedly replacing \((X_{0})_{i}\) with a weighted average of its neighbors.

Let \(=[L_{l}&L_{lu}\\ L_{ul}&L_{u}]\) and \(X_{0}=[X_{l}\\ X_{u}]\) where \(l\) and \(u\) correspond to labeled and unlabeled indices, respectively. For brevity, we denote \(X=X_{u}\) and \(Y=X_{l}\). The solution \(\) satisfies the linear system

\[L_{u}=B:=-L_{ul}Y. \]

Although Laplace learning and related algorithms work well when the number of labeled examples is sufficient, these algorithms generally suffer from two drawbacks: (1.) they typically solve a relaxation such that rather than predicting categorical values (e.g., binary one hot-encodings), they assign a real value for each class. A heuristic (usually thresholding) is necessarily applied to determine the predicted categorical label. (2.) In the low label rate regime, Laplace learning degenerates, yielding homogeneous predictions. Thus, thresholding results in poor prediction. In contrast, we propose an approach derived from a _non_-degenerate problem and coupled with a method to ensure exact combinatorial predictions, removing the need for heuristic thresholding.

### Graph partitioning

Given a graph \(G=(W,V)\), consider the discrete, cardinality-constrained graph partitioning problem

\[_{P V}\{(P,P^{c}):=_{v_{i} P}_{v_{j} P ^{c}}w_{ij}\}\;\;|P|=m_{1} \]

Critically, we note that this combinatorial problem is non-degenerate when \(m n\), if \(m_{1}<n\), due to the cardinality constraint on the solution . Algorithms and solutions to this problem have been thoroughly studied, particularly in the context of neighborhood graphs and for formulations that replace the constraint \(|P|=m_{1}\) with a penalty on the imbalance between partitions.

The applications of graph partitioning-inspired methods to data science are well-known, particularly various relaxations of graph cut problems, including spectral methods [31; 4], combinatorial algorithms [21; 13], and semi-definite programs (SDPs) [16; 25]. However, key drawbacks of these methods include the inexactness of the relaxation (spectral methods), high computational price due to a large number of constraints (SDPs), or slow convergence (combinatorial algorithms).

On the other hand, while exact continuous nonconvex formulations for graph partitioning have been studied , as far as we are aware, no attempts have been made to design _efficient_ algorithms amenable to \(k\)-way partitioning of large-scale datasets _with partial label information_. In particular, the \(k\)-way graph cut generalization was studied in . The developed algorithms are based on a piecewise linear refinement of an initial, continuous-valued solution to one that is integer-valued and a local minimizer of a certain nonconvex relaxation (Theorem 2.1 in their paper). These algorithms guarantee an integer solution that exhibits a valid cut with objective value greater than that of the continuous-valued extension used to initialize their method. However, note that these algorithms are prohibitively expensive, e.g., potentially requiring exhaustive search over \(3^{n^{2}}\) feasible "path matrices".

### Graph cuts and continuous quadratic programming

Here we restate the discrete graph partitioning problem in a form more amenable for continuous quadratic optimization, i.e. as a continuous optimization problem over \(^{M}\). Later, we will see that this problem bears similarities to the Laplace learning problem in that the two share the same objective, but the graph partitioning problem involves additional constraints.

For simplicity, we first introduce a bipartitioning framework, \(V_{0}():=\{v_{i}:_{i}=0\}\), \(V_{1}():=\{v_{j}:_{j}=1\}\) characterized by a binary vector \(\{0,1\}^{M}\). Equivalently, each binary vector \(\) determines an edge set \(E_{x}=\{(i,j):_{i}=0,_{j}=1\}\) connecting two subsets \(V_{0}()\), \(V_{1}()\). The cardinality-constrained min-cut solution is the binary vector \(\) satisfying \(_{i}_{i}=m_{1}\)

\[*{arg\,min}_{\{0,1\}^{M}}\{(_{M}- )^{}W\}=*{arg\,min}_{ \{0,1\}^{M}}_{(i,j) E_{x}}w_{ij}. \]

The equality is due to both terms being equal, i.e. \((_{M}-)^{}W=_{(i,j) E_{x}}w_{ij}\), over the set of binary vectors. Next, note that when \(\) is binary, \((_{M}-)^{}S=0\) for any diagonal matrix \(S\). Hence, min-cut is equivalent to

\[_{\{0,1\}^{M}}\{(_{M}-)^{}(W+S) \} \]

The theoretical aspects of this perturbed problem were investigated in . However, an algorithm to solve this problem was left as future work.

For completeness, we review the multi-partition generalization before proposing an efficient iterative scheme in Section 3. In particular, the choice of \(S\) is characterized in  as a way to provide a tighter relaxation from the set of binary vectors to the real-valued box set \(^{M}\). Essentially, the term \(x^{}Sx\) acts as a regularizer. For instance, take \(S=I\). The maximizer of the quadratic \(x^{}Sx=x^{}x=||x||_{2}^{2}\) on the simplex \(\{x\ :\ _{i}x_{i}=1\}\) are the extreme points of the simplex, i.e. the one-hot vectors. Hager and Krylyuk  rigorously proved that the larger the entries of \(S\), the tighter the relaxation, but the number of stationary points grows. The key condition that must be satisfied by \(S\) is stated in Theorem 2.1 of : \(S_{ii}+S_{jj} 2w_{ij}\) which ensures that the min-cut problem with cardinality constraint on \(V_{1}\) coincides with the following _continuous_ quadratic optimization problem,

\[_{^{M}}(_{M}-)^{}(W+S) 0_{i} 1,_{M}^{ }=m_{1}=|V_{1}| \]

Given \(S 0_{n n}\), an essential question is how to compute a solution to (7). Note that it is no longer a convex problem. Intuitively, given some quadratic (e.g., parameterized by \(W\)) with a minimum in the interior of the simplex, perturbing \(W\) by \(S\) results in a new, indefinite or concave quadratic with a minimum at a vertex of the simplex.

Our proposed framework is based on constructing a homotopy path, e.g. with \(S=sD\) for some parameter \(s\) in (7), to find high-quality stationary points at extreme points of the simplex. In other words, our proposed method computes \(x(s_{0})\) for some small \(s_{0}\), and then uses \(x(s_{0})\) as initialization to reach \(x(s)\) for a larger value \(s\), until solutions are within some small radius \(\) of an integral solution. By controlling the magnitude of \(s\), we control the number of critical points that trap iterative methods.

We illustrate these principles in Figure 1, where we visualize the level sets of the \(2\)-d quadratic \(f(x,y)=(x-0.8)^{2}-(y-0.2)^{2}-s(x^{2}+y^{2})\) on the simplex \(\{(x,y)^{2}:x+y=1\}\) for various choices of \(s\). Note that when \(s=0\), the quadratic is convex and the minimum lies in the interior of the simplex. As \(s\) increases, the quadratic becomes indefinite, and eventually concave on the simplex (i.e. when \(s=10\)). As \(s\) increases, the local minima on the simplex gravitate towards the "corners" (one-hot labels) until both "corners" become local minima in the extreme case.

## 3 Graph Partitioning for SSL

In this section, we generalize semi-supervised bi-partitioning (\(2\)-class SSL) to \(k\)-way partitioning (multi-class SSL). This yields a constrained optimization problem over \(n k\) real-valued matrices. We generalize the previous framework to incorporate label information, introduce the conditions necessary for recovering binary-valued solutions, and develop our CutSSL algorithm.

### Semi-supervised \(k\)-way partitioning

Before introducing label information, we transition from the bipartitioning to the \(k\)-way partitioning setting. The following maximization problem, introduced in , is interpreted as the maximization of the sum of intra-partition edge weights.

\[_{X^{M k}}(X^{}(W+S)X)\ X^{ }_{M}=,\ X_{k}=_{M},\ X 0. \]

Here, the entries of the vector \(^{k}\) denote the given cardinalities of _each_ class and \(_{k}^{}=M\), the number of all examples.

To motivate the introduction of labels to the cardinality-constrained cut problem, we relate the objective of (8) with the _perturbed_ Laplacian quadratic form:

**Proposition 3.1** (Equivalent binary minimizers).: _Let \(H_{S}(X)=tr(X^{}(-S)X)\) be the quadratic form associated with the Laplacian, and let \(G_{S}\) be the objective of (8), \(G_{S}=tr(X^{}(W+S)X)\). Then, we have that_

\[\{G_{S}(X):X\}=\{H_{S}(X):X\}\]

_for all \(X_{}\), where \(=\{X^{M k}:X_{i,j}\{0,1\}, X^{}_{M}=\}\)._

Proof.: Observe that

\[H_{S}(X)+G_{S}(X)= X,DX=||D_{M}||_{1} \]

is a constant and the minimizers and maximizers coincide. 

The Laplace learning solution (3) alongside this proposition motivates the following Laplacian quadratic objective with perturbation \(S\), which we term CutSSL. This is our key formulation.

\[_{X^{n k}}\{F_{s}(X):= {tr}(X^{}(L_{u}-S)X)-(X^{}B)\} \] \[\ X^{}_{n}=,\ X _{k}=_{n},\ X 0,\]

In practice for the homotopy path, we take \(S=sD\), where we have for any feasible \(X_{}\),

\[F_{s}(X_{})=F_{0}(X_{})-s||D^{1/2}X_{}||_{F}^{2}=F_{0}(X_{ })-s||Dm||_{1}\]

Thus, any binary minimizer of (10) with \(s=0\) is also a binary minimizer with \(s>0\). For brevity denote \(L_{u,s}=L_{u}-sD\). For real-valued \(X\), we have that the objective is \(F_{s}(X)=F_{0}(X)-s||D^{1/2}X||_{F}^{2}\). The quadratic term of the objective \(F_{s}\) satisfies

\[(X^{}L_{u,s}X)=(X^{}((1-s)D_{u}-W_{u})X)=(1-s)_{ i}(d_{u})_{i}||X_{i}||_{2}^{2}-_{ij}(w_{u})_{ij}(X_{i},X_{j}).\]

Thus, we have the following natural interpretation:

* When \(s=0\), the objective mirrors Laplace learning.
* When \(0 s 1\), the constrained minimizer of the quadratic encourages solutions with predictions at unlabeled high-degree vertices to have small norm.
* When \(s>1\), \(L_{u,s}\) could be indefinite or negative definite. The minimizers are extreme points of the feasible set and correspond exactly to labels.

Figure 1: Effect of perturbing \(L\) by \(S\) on minimizers of (10) on the simplex. Shade of level-curves denotes descent direction. Blue dots denote unique minimizers in the simplex. As the magnitude of the perturbation increases, the minimzers gravitate towards the extreme points of the simplex.

[MISSING_PAGE_FAIL:6]

```
0: Laplacian \(L\), labels \(B\), initialization \(X_{0}\)
0: One-hot label predictions \(X\)
1:functionADMM(\(_{u,s},X_{0}\))
2:\(X X_{0}\)
3:while not converged do
4:\(X_{t+1}=_{u,s}^{-}[B+T_{t}-_{n}_{1}^{}-_{2} _{k}^{}-_{t}\)\(\)\(_{1},_{2}\) given by (15)
5:\(T_{t+1}(_{t}+X_{t+1},0)\)\(\) Projection of \(T\)
6:\(_{t+1}=_{t}+X_{t+1}-T_{t+1}\)\(\) update multipliers
7:endwhile
8:return\(X_{t}\)
9:endfunction
```

**Algorithm 1** CutSSL

### Convergence and complexity of CutSSL

The convergence of the standard two-block ADMM for convex and nonconvex problems has been thoroughly established in the literature [6; 8]. Note that when \(s=0\), (10) is convex and the feasible set is non-empty. An optimal solution exists. The ADMM iterations for (11) produce a sequence \(\{(X_{t},T_{t},_{t})\}\), where \(X_{t}\) is guaranteed to satisfy the equality constraints, the entries of \(T_{t}\) are guaranteed to be positive and \(_{t}\) is the multiplier for equality constraint.

Generally, and as we show in the Appendix, ADMM converges to a KKT point of (11) if and only if the objective, primal, and dual residuals converge  as \(t\), i.e.

\[r_{p}^{t+1}=||X_{t}-T_{t}||_{2}, r_{d}^{t+1}=||T_{t+1}-T_{t}||_{2} \]

The iterations stop when the norms of these residuals are within specified tolerance levels. \(_{p} 0\), \(_{d} 0\). One may speed up the empirical rate of convergence of ADMM by adjusting a step-size parameter associated with the Lagrangian. See the Appendix for details.

Partitioning poses a challenge from an optimization perspective, primarily due to the nonconvexity of the quadratic objective. For example, the Poisson MBO method  is locally convergent in the limit and incurs complexity \(O(TR)\), where \(T\) is a bound on the number of iterations the procedure is run and \(O(R)\) is the cost of solving a Laplacian system. Likewise, the volume MBO method presented in  also claims local convergence and complexity \(O(TNV((V)+N)C)\) for some constant \(C\). Preconditioned conjugate gradient can be applied to find solutions to Laplacian-like systems in nearly linear time (linear in \(M\)) . Thus, the complexity of our method is dominated by the primal variable update \(O(TR)\) (step 4 in Alg. 1).

## 4 Mean-shifted Laplace learning

In the previous section, we introduced a graph-cut-inspired framework for graph-based semi-supervised learning. Here we consider a related problem formulation to (10) such that we do not limit our solutions to lie on the simplex, and set \(s=0\). This is equivalent to imposing a cardinality constraint on Laplace learning (1):

\[_{X^{n k}}\{(X^{}L_{u}X )-(X^{}B)\}X^{}_{n}= \]

This comparison is justified with empirical evidence on MNIST. At solutions to (10) with \(s=0\), we observe (1.) the inequality constraints \(X 0\) are _inactive_ (2.) the multiplier \(_{2}\) associated with the constraint \(X_{k}=_{n}\) has infinitesimal norm (empirically order of \(10^{-9}\)) (3.) the solutions to (18) and (10) differ in norm by a tiny amount (order of \(10^{-4}\)).

We will show that an approximate solution to this problem is a previously known heuristic, mean-shift Laplace learning. Our analysis of this problem provides new evidence to explain the empirical performance of this heuristic, while also revealing that it is suboptimal in some sense.

While Laplace learning (3) exhibits excellent results at medium and higher label rates, recent work [10; 11] has identified that the solution to the Laplace learning problem degenerates to a constant as \(m M\), i.e., as the number of unlabeled vertices is significantly larger than the number of labeled vertices. Specifically, in the low label rate regime, \(\) asymptotically depends only on the degrees of the labeled vertices of the graph : \(_{i}_{w}:=^{n}d_{i}y_{i}}{_{j=1}^{n}d_{j }}\), thus, thresholding as in (2) results in a poor solution, concentrated on a single class.

In the nonasymptotic regime, however, \(\) is not exactly a constant, and earlier work has demonstrated that shifting \(\) such that the column-means are zero (i.e. the _mean-shift_) empirically corrects this issue. This is justified in Calder et al. , von Luxburg et al.  via a random walk argument.

### Mean-shift as an approximate solution for cardinality-constrained SSL

The problem in (18) is convex since the matrix \(L_{u}\) is a principal submatrix of \(\), a positive semidefinite matrix. Additionally, the set \(C_{}=\{X:X^{}_{n}=\}\) is convex and nonempty. Mean-shifted Laplace learning is one heuristic to solve this problem, by performing the the following pair of steps:

1) **Linearization**: Solve \(L_{u}X=B\) to get \(\). This is equivalent to vanilla Laplace learning.

2) **Projection**: Project \(\) onto the set \(C_{m}\):

\[}=*{arg\,min}_{X}\{\|X-\|_{F}^ {2}\ X^{}_{n}=\}, \]

Applying Lagrange multipliers yields \(}_{ij}=_{ij}-(_{i=1}^{n}_{ij}- _{j})\). Thus, mean-shift is the projection \(}_{ij}\) when \(=_{k}\).

This is only an approximate solution. However, it is straightforward to characterize the optimal solutions of (18), which can be decomposed into the sum of two terms: \(X=Z+_{n}^{}\). The first term is associated with the solution to (18) for \(=0\), i.e. \(_{n}^{}Z=0\). Denote the projection \(P=I-_{n}_{n}^{}\) onto the set of \(n k\) matrices with mean-zero columns. Note that \(Z\) is the minimizer of \((Z^{}PL_{u}PZ)-(Z^{}(PB))\) and satisfies the system \(Z=(PL_{u}P)^{}PB\). The second term \(_{n}^{}\) is a constant term to correct for the true value of \(\) so that \(_{n}^{}X=\). Thus, it is clear that to resolve the suboptimality of the mean-shift heuristic, one must apply mean-shift to the Laplace learning solution when the _labels have been shifted_. This can be solved with Projected Conjugate Gradient (PCG). In the following proposition, we characterize optimal solutions to (18) in terms of the mean-shift heuristic. Naturally, we expect that as the number of labeled vertices increases, the gap becomes smaller. The proof is provided in the appendix.

**Proposition 4.1**.: _Let \(X^{*}^{n k}\) be a solution to Laplace learning (18), \(^{n k}\) be the solution to (3), and \(}^{n k}\) be the mean-shift heuristic (28). Let \((L_{u})=}(L_{u})}{_{}(L_{u})}\) and \(}=_{n}\). Then, \(X^{*}\) is a rank-one perturbation of \(}\) and \(||X^{*}-}||)}{}||^{}- }^{}||+||

_and_ the labels is a superior heuristic. However, we note that while gains in accuracy of \(1-2\%\) are consistent in the very low label-rate regime, the improvement becomes marginal at higher label rates. In contrast, we demonstrate that CutSSL, which avoids degenerate solutions via recovery of integer solutions, has consistent improvement in performance across label rates.

## 5 Experiments

We evaluate our method on three image datasets: MNIST , Fashion-MNIST  and Cifar-10 , using pretrained autoencoders as feature extractors as in Calder et al. , see appendix for details. We also evaluate cutSSL on various real-world networks, including the Cora citation network and the OGB  Arxiv and Product networks.

**Image datasets** We construct a graph over the latent feature space. We used all available data to construct the graph, with \(n=70,000\) nodes for MNIST and Fashion-MNIST, and \(n=60,000\) nodes for Cifar-10. The graph was constructed as a \(k\)-nearest neighbor graph with Gaussian edge weights given by \(w_{ij}=(-4||v_{i}-v_{j}||^{2}/d_{k}(v_{i})^{2})\), where \(v_{i}\) are the latent variables for image \(i\), and \(d_{k}(v_{i})\) is the distance in the latent space between \(v_{i}\) and its \(k^{}\) nearest neighbor. We used \(k=10\) in all experiments and symmetrize \(W\) by replacing \(W\) with \((W+W^{})\).

We compare to the state-of-the-art unconstrained graph-based SSL algorithm Poisson Learning  as well as variants based on the MBO procedure  to incorporate knowledge of class sizes. We include vanilla Laplace learning, Laplace learning with the mean-shift heuristic, and exact Laplace learning with relaxed cardinality constraints presented in Section 4. We also include a method inspired from spectral relaxations of the min-cut problem. Laplacian Eigenmaps-SSL  performs graph-based SSL by learning a linear predictor using the principal eigenvectors of the graph Laplacian as features. We note that our method is directly comparable to Volume- and Poisson-MBO, which also incorporate knowledge of class sizes.

Adopting a homotopy method implies that we solve a sequence of quadratic programs with \(s_{0}=0\) and use the associated solution to initialize a subsequent problem with \(s>0\). In practice, we set \(s=0.1\) for all experiments. The procedure presented in Alg. 1 is run for \(100\) iterations. In Table 1 we present the accuracy of our method on Cifar-10 across various label rates (MNIST and fMNIST results are in the appendix). CutSSL outperforms all methods across all label rates. In particular, we outperform the state-of-the-art by \(2.9\%\) when only a single sample from each class is provided and by \(1.9\%\) in the large label-rate regime.

   Cifar-10 \# Labels per class & **1** & **3** & **5** & **10** & **4000** \\  Laplace/LP  & 10.4 (1.3) & 11.6 (2.7) & 14.1 (5.0) & 21.8 (7.4) & 80.9 (0.0) \\ Mean Shift Laplace/LP & 40.9 (4.1) & 49.5 (3.4) & 51.3 (2.9) & 57.0 (2.1) & 81.0 (0.4) \\ Exact Laplace/LP (ours) & 41.6 (4.3) & 50.1 (2.9) & 51.6 (2.6) & 57.3 (1.9) & 81.1 (0.2) \\ LE-SSL  & 36.2 (0.1) & 50.2 (4.3) & 54.7 (3.4) & 59.4 (2.3) & 80.1 (0.9) \\ Sparse LP  & 13.1 (2.9) & 13.4 (2.6) & 18.4 (2.1) & 19.8 (1.9) & 71.3 (0.1) \\ Poisson  & 40.7 (5.5) & 49.9 (3.4) & 53.8 (2.6) & 58.3 (1.7) & 80.3 (0.9) \\  Volume-MBO  & 38.0 (7.2) & 50.1 (5.7) & 55.3 (3.8) & 59.2 (3.2) & 75.1 (0.2) \\ Poisson-MBO  & 41.8 (6.5) & 53.5 (4.4) & 57.9 (3.2) & 61.8 (2.2) & 80.1 (0.3) \\ CutSSL (ours) & **44.7 (5.9)** & **56.1 (3.7)** & **59.7 (3.1)** & **64.5 (1.7)** & **82.0 (0.2)** \\   

Table 1: Cifar-10. Average accuracy over 100 trials with standard deviation in brackets.

   ogbn-arxiv, \(k=40\) & Acc. (Rate=1) & Runtime (s) & Cut (\(10^{3}\)) & Acc. (Rate=tr) \\  Poisson  & 45.72\% & **79.34** & 71.13 & 50.11\% \\ GraphHop-NN  & 27.36\% & 1878.39 & 94.16 & 34.47\% \\ Poisson-MBO  & 46.94\% & 341.12 & 72.09 & 51.7\% \\
**CutSSL (ours)** & **52.37\%** & 122.04 & **70.43** & **68.21\%** \\   

Table 2: A citation network with \(169,343\) vertices and \(40\) labels (“Rate=tr” means all train data).

**Citation networks :** In Figure 2 we plot accuracy and label rate for the Cora and Citeseer citation networks, demonstrating that our method generalizes beyond \(k\)-NN graphs. Notably, for Cora, we outperform PoissonMBO by \(7.5\%\) at 1 label per class (see appendix). We note that the trend persists across datasets and label rates. Interestingly, LE-SSL  performs significantly worse across label rates. This implies that searching near the set of binary vectors is critical to achieving good predictors.

**Large-scale networks:** To demonstrate the scalability of our method, we provide results at 1-label rate for OGB networks Arxiv (Table 2) and Products (Table 3). These networks contain up to 2 million vertices and \(47\) classes. We demonstrate state-of-the-art performance in this regime, with an improvement of \( 5\%\) for both. Note that CutSSL also significantly outperforms a graph neural network-based method, GraphHop , designed for low label rates and which also exploits node feature information. CutSSL has double the accuracy with an order of magnitude less run-time.

**Label/class imbalance:** In the appendix, we also present results demonstrating performance under label and class imbalance. Our method exhibits superior robustness when either the label rate is imbalanced (different classes have different number of labels, see Tab. 5) or when the underlying partition sizes are imbalanced (different classes have different number of samples, see Tab. 6).

**Additional experiments:** In the appendix (section C.2), we demonstrate the efficacy of CutSSL at classifying samples with small "margin". This is in contrast to spectral and Laplace learning-type algorithms are known to produce predictions that "bleed" across the cut boundary. We conduct ablative studies on the graph construction and choice of \(S\) (section C.4). Convergence of CutSSL and recovery of integer solutions is empirically demonstrated in Figure 6.

## 6 Conclusion

We have proposed a novel formulation of graph-based semi supervised learning as a nonconvex continuous quadratic program that bears similarities to the mean-shift Laplace learning heuristic and graph cuts. We have presented an iterative method to solve a sequence of these problems to recover _discrete_ predictions. Numerically, we have demonstrated that our approach consistently outperforms state-of-the-art methods on semi-supervised learning problems at low, medium, and high label rates and in imbalanced class regimes. Future work includes a rigorous analysis of exact cut-based methods for graph-based semi-supervised learning. Of particular interest are the asymptotic behavior and consistency of cut-based methods for graph-based SSL.