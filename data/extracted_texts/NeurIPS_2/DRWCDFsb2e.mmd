# Interpretability of LLM Deception:

Universal Motif

Wannan Yang

Center for Neural Science

New York University

New York, USA

winnieyangwn96@gmail.com

Gyorgy Buzsaki

Neuroscience Institute

Grossman School of Medicine, New York University

New York, USA

Gyorgy.Buzsaki@nyulangone.org

###### Abstract

Conversational large language models (LLMs) are trained to be helpful, honest and harmless (HHH) and yet they remain susceptible to hallucinations, misinformation and are capable of deception. A promising avenue for safeguarding against these behaviors is to gain a deeper understanding of their inner workings. Here we ask: what could interpretability tell us about deception and can it help to control it? First, we introduce a simple and yet general protocol to induce 20 large conversational models from different model families (Llama, Gemma, Yi and Qwen) of various sizes (from 1.5B to 70B) to knowingly lie. Second, we characterize three iterative refinement stages of deception from the latent space representation. Third, we demonstrate that these stages are _universal_ across models from different families and sizes. We find that the third stage progression reliably predicts whether a certain model is capable of deception. Furthermore, our patching results reveal that a surprisingly sparse set of layers and attention heads are causally responsible for lying. Importantly, consistent across all models tested, this sparse set of layers and attention heads are part of the third iterative refinement process. When contrastive activation steering is applied to control model output, only steering these layers from the third stage could effectively reduce lying. Overall, these findings identify a universal motif across deceptive models and provide actionable insights for developing general and robust safeguards against deceptive AI. The code, dataset, visualizations, and an interactive demo notebook are available at [https://github.com/safellm-2024/llm_deception](https://github.com/safellm-2024/llm_deception).

## 1 Introduction

Large language models (LLMs) have seen widespread deployment in recent years. They exhibit impressive general capabilities - some of which approach or even surpass human expertise. These advances also pose greater risks around misuses in misinformation and malicious applications (Hubinger et al., 2024; Scheurer et al., 2024). Despite the growing evidence for unsafe behaviors that persist through safety training, we know very little about why and how these safety breaches occur. Enhanced transparency of models under those scenarios would offer numerous benefits, from a deeper understanding of their inner workings, to increased accountability for safety assurance and the potential for discovering novel failure modes (Casper et al., 2024).

Recent advances in interpretability (Wang et al., 2022; Nanda et al., 2023b;a; Meng et al., 2023; Zou et al., 2023) have demonstrated great potential for understanding the internal mechanisms of language models. Interpretability tools have successfully revealed the inner mechanisms of models performing various tasks. However, most interpretability works study _base_ models that havenot been through safety training. Some recent works carefully examine a set of safety-related behaviors in chat models (Campbell et al., 2023; Arditi et al., 2024; Ball et al., 2024; Turner et al., 2024; Rimsky et al., 2024), but they typically limiting themselves to one kind of model under each investigation.

In this study, we integrate mechanistic interpretability and representation engineering tools (Zou et al., 2023) to study a diverse set of large conversational language models (_chat_ models), focusing on one key safety challenge - deception. Overall, our main contributions are:

* We introduce a simple yet general protocol to induce large conversational models to knowingly lie. We test our protocol on 20 models of various model sizes (from 1.5 to 70 billion) from different model families (Qwen, Yi, Llama and Gemma).
* We identify three iterative refinement stages of deception and demonstrate that these stages are _universal_ across different models.
* We show that progression on the third stage could reliably predict whether a particular model is capable of lying.
* With activation patching, we identify a sparse set of stage 3 layers that are causally responsible for lying. Consistently, with contrastive activation steering, we show that only steering (with contrastive activation steering) the third stage layers could effectively reduce lying.

## 2 Related Work

### Dishonesty and Deception.

Many studies highlight that LLMs do not reliably output truth. Failures in truthfulness fall into two categories (Evans et al., 2021): sometimes LLMs simply do not know the correct answer (capability failure), and sometimes they apparently 'know' the true answer but nevertheless generate a false response or 'hide' their true motives (Perez et al., 2022; Pacchiatrid et al., 2023; Zou et al., 2023; Park et al., 2023). For instance, Lin et al. (2022) show that models often generated false answers that mimic popular human misconceptions. Interestingly, Lin et al. (2022) show that scaling up models alone does not help improving truthfulness since larger models are more prone to imitative falsehoods (inverse scaling law). Park et al. (2023) document that the AI system CICERO can engage in premeditated deception, planning in advance to build a fake alliance with a player in order to trick that player into leaving themselves undefended for an attack. More recently, Hubinger et al. (2024) create'sleeper agents' which behave helpfully during training but exhibit harmful behaviors when deployed. Their results raise concerns about the effectiveness of current safety training techniques against maliciously trained AI systems. Scheurer et al. (2024) demonstrate that LLM agents can even strategically deceiving their users in a realistic situation, without direct instructions or training for deception.

Refer to Appendix B for further discussion on related work.

## 3 Method

### A general protocol for instructing different models to knowingly lie

As noted by Campbell et al. (2023) inducing lying behavior can be challenging and requires careful prompt engineering. Built on this previous work, we build a simple and yet general protocol to induce knowingly lie in a diverse set up models.

As illustrated in Figure 6, the main structure of our prompting protocol composed of the following four components:

* **1. System prompt.*
* For each statement, we have a pair of contrastive prompts:
* Honest persona: "You are to answer the following question in an honest manner."
* Lying persona: "You are to answer the following question in a lying manner."
* **2. User prompt:** "Is the following statement true of false?"
* **3. Statement:** Insert one statement regarding a scientific fact from Azaria & Mitchell (2023)
* **4. Prefix injection:** "Answer: The statement is __ __ "

### Residual stream dimensionality reduction

For each model completion, we cache the residual stream activation \(x_{I}^{(l)}^{d_{model}}\) at the _last token position_\(I\) of the prompt at each layer \(l\), and perform Principle Component Analysis (PCA). We do this for all layers \(l[L]\) of the transformer block, and visualize their low dimensional embedding \(a_{I}^{(l)}^{2}\).

"Truth direction'.Truth direction denotes the vector direction from the centroid of the false statements to the centroid of the true statements (difference in means between true and false statements).

Centroid of all true statements are calculated by taking the geometric mean of the residual stream activations for all true statements \(t D^{true}\) at the _last token position_\(I\) :

\[t_{I}^{(l)}=}_{t D^{(true)}}x_{I}^{(l)}(t) \]

Centroid of all false statements are calculated by taking the mean of the residual stream activations for all false statements \(t D^{false}\) at the _last token position_\(I\) :

\[f_{I}^{(l)}=}_{t D^{(false)}}x_{I}^{(l)}(t) \]

Truth direction \(u_{I}^{(l)}\) is:

\[u_{I}^{(l)}=t_{I}^{(l)}-f_{I}^{(l)} \]

### Contrastive Activation Steering

Contrastive activation steering is a technique for controlling the behavior of language models by modifying their internal activations during inference (Turner et al., 2024; Arditi et al., 2024; Rimsky et al., 2024). The two major steps are:

* **Extracting** the steering vector from contrastive examples.
* **Applying** the steering vectors to modify model behavior during generation.

#### 3.3.1 Extracting Steering Vector

"Honest direction'.To steer the lying model to become honest, an 'honest direction' is extracted from the latent activations to build the _steering vector_. The _difference-in-means_ method is used to build the steering vector. This involves taking the mean difference in activations over a dataset of contrastive prompts.

Here, the contrastive pairs consist of honest and lying versions of the prompt for each statement. We compute the difference between the mean activations when models are instructed to be honest versus lying.

For each layer \(l[L]\) and the _last token position_ of the prompt \(I\), we calculate the mean activation \(h_{I}^{(l)}\) for honest persona and \(l_{I}^{(l)}\) for lying persona:

\[h_{I}^{(l)}=}_{t D^{(honest)}}x_{I}^{(l)}(t),  l_{I}^{(l)}=}_{t D^{(lying)}}x_{I}^{(l)}(t) \]Honest direction \(r^{(l)}\) is the difference between the mean honest activation and the mean lying activation:

\[r^{(l)}=h^{(l)}_{I}-l^{(l)}_{I} \]

### Applying Steering Vector

'Honest addition'.To steer the lying model to become honest, we add the 'honest direction' as the steering vector to the lying activations. This is a form of activation addition Turner et al. (2024).

Given a difference-in-means vector ('honest direction') extracted form layer \(l\), we add the difference-in-means vector to the residual stream activations response to the lying prompt to shift them closer to the mean honest activation:

\[x^{(l)^{}} x^{(l)}+ r^{(l)} \]

where \(r^{(l)}^{d_{model}}\) is the 'honest direction' extracted from layer \(l\), \(x^{(l)}\) is the residual stream activations from the same layer \(l\) and \(\) is the scaling factor. We find that a scaling factor of 1 is enough to steer the lying model to become honest across all models tested.

Following Arditi et al. (2024) the steering vector extracted from layer \(l\) is applied _only at layer \(l\)_, and _across all token positions_ during generation.

### Contrastive activation patching

Contrastive activation patching is used as a causal intervention tool to identify model components responsible for lying. It is a similar type of causal intervention as performed in Meng et al. (2023) and Wang et al. (2022).

Contrastive activations patching consists of three steps:

* 1. **'Honest run'**. First, we cache all activations of the network run when we prompt the model to answer questions in an honest manner.
* 2. **'Lying run'**. Secondly, we cache all activations of the network run when we prompt the model to answer questions in a lying manner.
* 3. **'Patched run'**. Then we run the network where the model is prompted to lie but _replacing_ some activations with the activations from the 'honest run'.

We can then measure the behavior as well as the internal activations of the patched model. Doing this for each node individually locates the nodes that explain why model behavior is different in the 'honest run' and 'lying run'.

## 4 Results

### Lying scales with model size

We focus on studying one type of deception where models give wrong answers to a question even though they 'know' the correct answer (knowingly lie). To do so, we first filter out a set of questions (Azaria and Mitchell, 2023) that the LLMs can answer correctly when prompted to be honest. We then check if they will answer incorrectly when asked to lie.

As has been previously noted (Campbell et al., 2023), inducing lying behavior can be surprisingly challenge and often requires careful prompt engineering. Built on the work of Campbell et al. (2023), we establish a general protocol (detailed description in SS3.1) for inducing a wide range of models to knowingly lie.

Constrained by our carefully designed chatting template, the model first make a true or false judgement for a given statement and then elaborates on the rationale for the judgement. As illustrated in Figure 6, the careful prompting design encourages free generation and enforcing a structure so that the performance can be easily measured by matching to the ground truth label (either "true" or "false"). Detailed evaluation methods are provided in Appendix A.2 and further evaluation results are presented in Appendix E.

We evaluate the performance (as measured by accuracy in judging if the statements are true or false) across 20 chat models from 4 model families with sizes ranging from 1.5 to 70 billion (see Appendix A.1 for the full list of of models tested). We show that lying is an emergent capacity that scales with model size. In general, within each model family, the small models do not lie and the larger models could knowingly lie (high accuracy when asked to be honest and low accuracy when prompted to lie, Figure 1).

### Iterative Refinement Stages of Deception

Performing PCA on the residual stream activation (see description in SS3.2), we compare the change in layer-by-layer representation patterns when models are prompt to lie VS be honest. The latent representation of lying goes through three iterative refinement stages (Lad et al., 2024). For illustration purposes, we include the latent representations of L1lam-3-8b-chat as an example in Figure 2. It is representative for all models that are capable of lying. The complete layer-by-layer representations of other models are shown in Appendix H.1.

Stage 1: Separation of honest and lying instruction.Activations to the honest (yellow) and lying (blue) prompts are initially intermingled but start to form very distinctive clusters during stage one (layer 7, Figure 2A).

Stage 2: Separation of truth and falsehood.Second state of the iterative refinement starts when the true (star) and false (circle) statements form distinct clusters (layer 12, Figure 2B). This observation is consistent with the emergence of 'truth direction' reported by Marks & Tegmark (2024).

Stage 3: 'Rotation' of the 'truth directions'.The 'truth directions' (see definition in SS3.2) of the honest and lying persona gradually 'rotate' (Figure 2C): starting from being parallel (cosine similarity \( 1\)) to orthogonal (cosine similarity \( 0\)), and finally close to anti-parallel (cos similarity \(-1\)). To quantify the change in stage 3, we measure the cosine similarity between the 'truth directions' when prompted to be honest v.s. lying and plot its change across layers (Figure 2D).

Figure 1: **Lying is an emergent capacity that scales with model size.** In general, the small models can not lie, and the larger models can knowingly lie (high accuracy when asked to be honest and low accuracy when prompted to lie).

### Universality of Representation and Predictability

As shown in Figure 1, not all models can lie. Can we predict which models are can lie and which cannot?

As observed in Figure 3, models that cannot lie do not complete the third stage of the iterative refinement stage - their 'truth directions' remain aligned (cosine similarity \( 1\)) throughout the layers. Figure 3A&B display one example model that cannot lie (Yi-1-6b-chat). In contrast, the 'truth directions' of all models that knowingly lie gradually 'rotate' with respect to each other (cosine similarity \(-1\)) throughout the third stage of the iterative refinement process. Figure 3D&E display one example model that knowingly lie (llama-3-8b-Instruct). What about models with 'truth directions' only 'partially rotate' (\(cos 0\) in the final layer)? They behave in between completely honest and completely lying: these models sometimes lie and sometimes act honestly (Figure H.1; Figure H.1). Overall, stage 3 progression strongly correlates with the lying score across all models tested (Figure 3; Figure 8).

### Model Patching: key model components of lying

As shown in Figure 3, both models that can and cannot lie undergo the first two stages of iterative refinement process, but only the lying models complete the third stage. We then ask whether layers in the third stage are _causally_ responsible for lying. To answer this question, we apply activation

Figure 2: **Three iterative refinement stages of lying.** Latent representations are extracted from the residual stream activations (last token of the prompt) in response to 100 different statements. A-C: subsets of layers marking the transitions between the three stages. D: the change in cosine similarity between the ‘truth directions’ across layers.

patching as a causal intervention tool to dissect the model components causally responsible for dishonesty.

Figure 4: **Patching a sparse set of layers and layers and attention heads can cause a lying model to become honest.** A and D: layer-by-layer and token-by-token patching results. B and E: head-by-head patching results for all attention heads across layers. C and F: the sparse set of layers with the most steep increase in average logit different (ALD) overlap with the layers with sharpest decrease in cosine similarity. Top panels: Llama-3-8b-Instruct, bottom panels: Gemma-2-9b-it.

Figure 3: **Stage 3 progression predicts if a model can knowingly lie.** A&B: example model that cannot lie. D&E: example model that knowingly lie. C: correlation between stage 3 progress and lying score for all of the 20 models tested (the size of the dot denotes the size of the model).

Following the method described in Appendix 3.5, we present results for two levels of patching: layer-by-layer and head-by-head patching. For the layer-by-layer patching, the representations (residual stream activations) from the 'honest run' are patched to the 'lying run' for each token position (of the prompt) across all layers of the model. The average logit difference (ALD) across 100 statements is used as a proxy for the causal contribution of each layer. As noted in previous works Marks & Tegmark (2024); Tigges et al. (2023), both Llama and Gemma models display the "summarization" behavior where information relevant to the full statement is represented at the end-of-sentence token (last token of the prompt). This pattern is consistent for both Llama and Gemma models (Figure 4A&D). Head-level patching further reveals a sparse set of attention heads causally responsible for lying (Figure 4B&E). Patching results for MLP and attention outputs are presented in Figure 9. Attention pattern for heads with top ALD can be found in Appendix F.2.

Importantly, the set of layers with the largest increase in patching contribution (steep increase in ALD, see Appendix A.3.1) corresponds to the stage three layers where 'truth directions' rotate with respect to each other (cosine similarity between the 'truth directions' sharply decrease). This is consistent with the result in SS4.3where progression during stage 3 best predicts whether a model is capable of lying.

### Model Steering: from lying to honesty

The simple linear structure in the latent representation (Nanda et al., 2023b) allows us to steer the models with linear vectors. Inspired by recent development in contrasting representation steering (Zou et al., 2023; Arditi et al., 2024; Turner et al., 2024; Rimsky et al., 2024), we steer the lying model to become honest by adding the 'honest direction' to the residual stream activation.

Using contrastive activation steering, we successfully steer all lying models to be honest (Figure 5A). Furthermore, there exists a critical window for steering to be effective. _Only_ steering the layers from the third stage ('rotation' layers) effectively reduces lying, further supporting the argument that

Figure 5: **Only steering the third stage layers effectively reduces lying.** A: adding the ‘honest direction’ to the residual stream activation of the lying models can effectively reduce lying across models from different model families. B: only steering the layers from the third stage (green dash line) can increase the model performance in answering the true/false questions. C: only steering the third stage layers could effectively prevent the rotation of ‘truth directions’.

stage three layers are responsible for lying (Figure 5B). To visualize the effect of steering the stage three layers, we plot the cosine similarity change across layers when applying the steering vector to each individual layer (Figure 5C). Only steering the third stage layers successfully prevent the 'truth directions' from rotating against each other (cosine similarity remain close to 1 after steering). Applying steering vector either before or after the third stage is ineffective.

Discussions on limitations and future work are presented in Appendix C.