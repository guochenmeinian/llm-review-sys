# A Specialized Semismooth Newton Method for Kernel-Based Optimal Transport

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Kernel-based optimal transport (OT) estimation is an alternative to the standard plug-in OT estimation. Recent works suggested that kernel-based OT estimators are more statistically efficient than plug-in OT estimators when comparing probability measures in high-dimensions . However, the computation of these estimators relies on the short-step interior-point method for which the required number of iterations is known to be _large_ in practice. In this paper, we propose a nonsmooth equation model for kernel-based OT estimation and show that it can be efficiently solved via a specialized semismooth Newton (SSN) method. Indeed, by exploring the special problem structure, the per-iteration cost of performing one SSN step can be significantly reduced in practice. We also prove that our algorithm can achieve a global convergence rate of \(O(1/)\) and a local quadratic convergence rate under some standard regularity conditions. Finally, we demonstrate the effectiveness of our algorithm by conducing the experiments on both synthetic and real datasets.

## 1 Introduction

Optimal transport (OT) theory  has provided a principled framework for comparing probability distributions. It has been extensively adopted in machine learning and related fields, with examples including generative modeling , classification and clustering , and domain adaptation , see also the monograph . It has also had an impact in applied areas such as neuroimaging  and cell trajectory prediction .

**Curse of Dimensionality.** In many real application problems, the OT cost is computed for squared Euclidean distance on the sampled distributions with \(n\) observations (leading to the 2-Wasserstein distance). It is known that OT estimation suffers from the curse of dimensionality : the standard plug-in estimator, which consists in computing the OT distance between the sampled distributions with \(n\) observations, converges to the OT distance between true distributions at a rate of \(O(n^{-1/d})\), which degrades exponentially in the dimension \(d\). This rate can be improved to \(O(n^{-1/2d})\) when true distributions are different  but it is still problematic in a high-dimensional regime. This issue can be a barrier to its adoption in machine learning since various application problems arising from image processing and bioengineering are high-dimensional. Practitioners have long been aware of such limitations and proposed efficient computational schemes that not only improve computational complexity but also carry out statistical regularization.

**Regularization.** In this context, two threads have been investigated to regularize the OT distance: entropic regularization  or low-dimensional projection . For the former approach, the sample complexity of entropic OT is bounded by \(O(^{-d/2}n^{-1/2})\) for a regularization parameter \(>0\). For the latter approach, the sample complexity of projection OT is bounded by \(O(n^{-1/k})\) for an integer-valued projection dimension \(k d\). Even though these boundsattain the dimension-free dependence on \(n\), they deteriorate when \(\) is small or \(k\) is large, either of which is needed to study the sample complexity of OT , and which plays a role in real applications.

**Leveraging Smoothness.** A recent line of works have focused on the _wavelet-based OT estimators_ under a strong smoothness condition [63; 26; 15; 34]. Although these estimators are minimax optimal from a statistical viewpoint, they are algorithmically intractable . In contrast, a specific entropic regularized OT estimator is computationally tractable but still suffers from the curse of dimensionality when the dimension is sufficiently large . Recently, Vacher et al.  has closed this statistical-computational gap by designing a kernel-based estimator relying on kernel sums-of-squares (SoS) and showed that it can be computed by a short-step interior-point method with polynomial-time complexity guarantee. However, the short-step interior-point method is well known to be ineffective for large number of iterations required as the sample size increases, diminishing their value from both statistical and practical viewpoints1. In this context, Muzzellec et al.  proposed to use the relaxation model and solve it using gradient-based methods. However, the relaxation model may not be a good approximation for kernel-based OT estimator, thereby lacking any statistical guarantee.

**Goal:** While there is an ongoing debate in the OT literature on the merits of computing the plug-in OT estimators v.s. kernel-based OT estimators, we adopt the perspective that Vacher et al.  does introduce a fairly novel approach and we believe that it is worth studying if the kernel-based OT estimation can provide leads for practical use. The goal of this paper is therefore to facilitate the computational aspect by designing new algorithms, and to figure out whether that estimator's theoretical claims is also supported by practical relevance. The statistical analysis of kernel-based OT estimation itself, e.g., the proper choice of penalty parameters, is beyond the scope of this paper.

**Contribution:** In this paper, we propose a nonsmooth equation model for computing kernel-based OT estimators and show that it has a special problem structure, allowing it to be solved in an efficient manner using semismooth Newton method [37; 47; 46; 58].

We first propose a nonsmooth equation model for computing the kernel-based OT estimator and define an approximate OT value, which allows us to carry out a finite-time analysis of the algorithm. Then, we propose a specialized semismooth Newton method for computing the kernel-based OT estimator and prove a global convergence rate of \(O(1/)\) (Theorem 3.3) and a local quadratic convergence rate under standard regularity conditions (Theorem 3.4). Notably, we significantly reduce the per-iteration computational cost by exploiting the special problem structure. Finally, we conduct the experiments to evaluate our algorithm on both synthetic and real datasets. Experimental results demonstrate its efficiency for solving the kernel-based OT estimation.

**Organization.** The remainder of the paper is organized as follows. In Section 2, we present the nonsmooth equation model for computing the kernel-based OT estimators and define the optimality notion based on the residual map. In Section 3, we propose and analyze the specialized semismooth Newton (SSN) algorithm for computing the kernel-based OT estimators and prove that our algorithm achieves the convergence rate guarantee in both global and local sense. In Section 4, we conduct the experiments on both synthetic and real datasets, demonstrating that our algorithm can effectively compute the kernel-based OT estimators and is more efficient than short-step interior-point methods. In Section 5, we conclude this paper. In the supplementary material, we provide further background materials on SSN methods, additional experimental results, and missing proofs for key results.

## 2 Preliminaries and Technical Background

In this section, we present the basic setup for the kernel-based optimal transport (OT) estimation and propose a nonsmooth equation model for its computation.

### Kernel-based OT estimation

We formally define the OT distance and review the kernel-based OT estimation . Indeed, the OT distance with strong smooth distributions can be estimated at a dimension-free statistical rate with high probability by solving a suitably defined optimization model.

Let \(X\) and \(Y\) be two bounded domains in \(^{d}\) and let \((X)\) and \((Y)\) be the set of Borel probability measures in \(X\) and \(Y\). Suppose that \((X)\), \((Y)\) and \((,)\) is the set of couplings between \(\) and \(\), the OT distance  is given by

\[(,):=(_{(,)}_{X Y} \|x-y\|^{2}\;d(x,y)).\]

Its dual formulation is stated as follows,

\[_{u,v C(^{d})}_{X}u(x)d(x)+_{Y}v(y)d(y), \|x-y\|^{2} u(x)+v(y),(x,y) X Y,\]

where \(C(^{d})\) is the space of continuous functions on \(^{d}\). Note that the supremum can be attained and the corresponding optimal dual functions \(u_{}\) and \(v_{}\) are referred to as the Kantorovich potentials . This problem is delicate to solve since \(\|x-y\|^{2} u(x)+v(y)\) needs to be satisfied on a continuous set \(X Y\). A natural approach is to take \(n\) points \(\{(_{1},_{1}),,(_{n},_{n})\} X  Y\) and consider the constraints \(\|_{i}-_{i}\|^{2} u(_{i})+v(_{i})\) for all \(1 i n\). However, it can not leverage the smoothness of potentials , yielding an error of \((n^{-1/d})\). Vacher et al.  has overcome this difficulty by replacing the inequality constraints with equality constraints that are equivalent and considering the equality constraints over \(n\) points. Following their works, we impose the following assumption on the support sets \(X,Y\) and the densities of \(\) and \(\).

**Assumption 2.1**: _Let \(d 1\) be the dimension and let \(m>2d+2\) be the order of smoothness. Then, we assume that (i) the support sets \(X,Y\) are convex, bounded, and open with Lipschitz boundaries; (ii) the densities of \(,\) are finite, bounded away from zero and \(m\)-times differentiable._

Assumption 2.1 guarantees that the potentials \(u_{}\) and \(v_{}\) have a similar order of differentiability , leading to an effective way to represent \(u\) and \(v\) via a _reproducing Kernel Hilbert space_ (RKHS) . In particular, we define \(H^{s}(Z):=\{f L^{2}(Z)\|f\|_{H^{s}(Z)}:=_{|a| s}\|D^{a}f\|_{L^{ 2}(Z)}<+\}\) and remark that \(H^{s}(Z) C^{k}(Z)\) for any \(s>+k\), where \(k 0\) is integer-valued. This implies that \(H^{m+1}(X)\), \(H^{m+1}(Y)\) and \(H^{m}(X Y)\) are RKHS under Assumption 2.1 and they are associated with three bounded continuous feature maps \(_{X}:X H^{m+1}(X)\), \(_{Y}:Y H^{m+1}(Y)\) and \(_{XY}:X Y H^{m}(X Y)\). For simplicity, we let \(H_{X}=H^{m+1}(X)\), \(H_{Y}=H^{m+1}(Y)\) and \(H_{XY}=H^{m}(X Y)\). Vacher et al. [59, Corollary 7] shows that (i) \(u_{} H_{X}\) and \(v_{} H_{Y}\) with

\[_{X}u(x)d(x)= u,w_{}_{H_{X}},\;_{X}v(y)d(y)=  v,w_{}_{H_{Y}},\]

where \(w_{}=_{X}_{X}(x)d(x)\) and \(w_{}=_{Y}_{Y}(y)d(y)\) are _kernel mean embeddings_; (ii) \(A_{}^{+}(H_{XY})\)2 exists and satisfies the equality constraint as follows:

\[\|x-y\|^{2}-u_{}(x)-v_{}(y)=_{XY}(x,y),A_{ }_{XY}(x,y)_{H_{XY}}\,.\]

Putting these pieces yields a representation theorem for estimating the OT distance. Indeed, under Assumption 2.1, the dual OT problem is equivalent to the RKHS-based problem given by

\[_{u,v,A}& u,w_{}_{H_{X}}+ v,w_ {}_{H_{Y}},\\ &\|x-y\|^{2}-u(x)-v(y)=_{XY}(x,y),A_{ XY}(x,y)_{H_{XY}}\,. \]

The above equation offers two advantages: (i) The equality constraint can be well approximated under Assumption 2.1; (ii) RKHS allow the kernel trick: computing parameters are expressed in terms of _kernel functions_ that correspond to

\[k_{X}(x,x^{})=_{X}(x),_{X}(x^{})_{H_{X}},  k_{Y}(y,y^{})=_{Y}(y),_{Y}(y^{})_{H_{ Y}},\]

and

\[k_{XY}((x,y),(x^{},y^{}))=_{XY}(x,y),_{XY}(x^{ },y^{})_{H_{XY}},\]

where the kernel functions are explicit and can be computed in \(O(d)\) given the samples. The final step is to approximate Eq. (2.1) using the data \(x_{1},,x_{n_{}}\) and \(y_{1},,y_{n_{}}\), and the filling points \(\{(_{1},_{1}),,(_{n},_{n})\} X  Y\). Indeed, we define \(=}}_{i=1}^{n_{}}_{x_{i}}\) and \(=}}_{i=1}^{n_{}}_{y_{i}}\), and use \( u,w_{}_{H_{X}}+ v,w_{}_{H_{Y}}\) instead of \( u,w_{}_{H_{X}}+ v,w_{}_{H_{Y}}\) where \(w_{}=}}_{i=1}^{n_{}}_{X }(x_{i})\) and \(w_{}=}}_{i=1}^{n_{}}_{Y} (y_{i})\). We also impose _the penalization terms_ for \(u\), \(v\), and \(A\) to alleviate the error induced by sampling the corresponding equality constraints. Then, the resulting problem with regularization parameters \(_{1},_{2}>0\) is summarized as follows:

\[_{u,v,A}& u,w_{}_{H_{X}}+  v,w_{}_{H_{Y}}-_{1}(A)-_{2}(\|u \|_{H_{X}}^{2}+\|v\|_{H_{Y}}^{2}),\\ &\|_{i}-_{i}\|^{2}-u(_{i})-v( _{i})=_{XY}(_{i},_{i}),A_{XY}( {x}_{i},_{i})_{H_{XY}}\,. \]

Focusing on the case \(n_{}=(n)\), we let \(_{}\) and \(_{}\) be the unique maximizers of Eq. (2.2). Then, the estimator for \((,)\) we consider corresponds to

\[}^{n}=_{},w_{}_{H_{X}}+ _{},w_{}_{H_{Y}}. \]

**Remark 2.2**: _It follows from Vacher et al. [59, Corollary 3] that the norm of empirical potentials can be controlled using \(_{1}=(n^{-1/2})\) and \(_{2}=(n^{-1/2})\) in high probability sense, leading to the sample complexity bound: \(|}^{n}-(,)|=(n^{-1/2})\). In comparison with plug-in estimators, the kernel-based OT estimators are better when the sample size is small and the dimension is high._

Note that Eq. (2.2) is an infinite-dimensional optimization problem and is thus difficult to be solved. Thanks to Vacher et al. [59, Theorem 15], we have that the dual problem of Eq. (2.2) can be presented in a finite-dimensional space and the strong duality holds true. Indeed, we define \(Q^{n n}\) with \(Q_{ij}=k_{X}(_{i},_{j})+k_{Y}(_{i},_{j})\), and \(z^{n}\) with \(z_{i}=w_{}(_{i})+w_{}(_{i})-_{2}\| _{i}-_{i}\|^{2}\), and \(q^{2}=\|w_{}\|_{H_{X}}^{2}+\|w_{}\|_{H_{Y}}\), where we have

\[w_{}(_{i})=}}_{j=1}^{n_{}}k_{X}(x_{j},_{i}), w_{}(_{i})=}}_{j=1}^{n_{}}k_{Y}(y_{j},_{i}),\]

and

\[\|w_{}\|_{H_{X}}^{2}=}^{2}}_{1 i,j  n_{}}k_{X}(x_{i},x_{j}),\|w_{}\|_{H_{Y}}^{2}= }^{2}}_{1 i,j n_{}}k_{Y}(y_{ i},y_{j}).\]

We define \(K^{n n}\) with \(K_{ij}=k_{XY}((_{i},_{i}),(_{j},_{j}))\) and \(R\) as an upper triangular matrix for the Cholesky decomposition of \(K\). We let \(_{i}\) be the \(i^{}\) column of \(R\). Then, the dual problem of Eq. (2.2) reads:

\[_{^{n}}}^{}Q -}^{}z+}{4_{2}}, \,\,_{i=1}^{n}_{i}_{i}_{i}^{}+_{1} I 0. \]

Suppose that \(\) is one minimizer, we have

\[^{n}=}{2_{2}}-}_{i=1 }^{n}_{i}(w_{}(_{i})+w_{}(_{i })).\]

To our knowledge, the existing method proposed for solving Eq. (2.4) is a short-step interior-point method for which the required number of iterations is known to be large when \(n\) is large, which is necessary to guarantee small statistical error. To avoid this issue, Muzellec et al.  proposed solving an unconstrained relaxation model which allows for the application of gradient-based methods. However, the estimators obtained from solving such relaxation model lack any statistical guarantee.

### Nonsmooth equation model and optimality condition

For simplicity, we define the operator \(:^{n n}^{n}\) and its adjoint \(^{}:^{n}^{n n}\) by

\[(X)= X,_{1}_{1}^{}\\ \\  X,_{n}_{n}^{},^{}( )=_{i=1}^{n}_{i}_{i}_{i}^{}.\]

We present the optimality notion for Eq. (2.4) as follows:

**Definition 2.1**: _A point \(^{n}\) is an optimal solution of Eq. (2.4) if we have \(^{*}()+_{1}I 0\) and \(}^{}Q-} ^{}z+}{4_{2}}} ^{}Q-}^{}z+}{4 _{2}}\) for all \(\) satisfying that \(^{*}()+_{1}I 0\)._

Clearly, Eq. (2.4) can be reformulated as the following optimization problem given by

\[_{^{n}}_{X 0}\ } ^{}Q-}^{}z+}{4 _{2}}- X,^{*}()+_{1}I. \]

We denote \(w=(,X)\) as a vector-matrix pair and let \(R:^{n}^{n n}^{n} ^{n n}\) be given by

\[R(w)=}Q-}z-(X)\\ X-_{^{n}_{+}}(X-(^{*}()+_{1}I)). \]

where \(^{n}_{+}=\{X^{n n}:X 0\}\). Then, we can measure the optimality of \(w\) via appeal to the quantity \(\|R(w)\|\) and shows that the notion is the same as used in Definition 2.1.

**Proposition 2.3**: _A point \(\) is an optimal solution of Eq. (2.4) if and only if \(=(,)\) satisfies \(R()=0\) for some \( 0\)._

Proposition 2.3 shows that we can compute the kernel-based OT estimators by solving the nonsmooth equation model \(R(w)=0\). The optimality criterion based on the residual map \(R()\) allows for a global convergence rate analysis for our specialized semismooth Newton method.

## 3 Algorithm and Convergence Analysis

In this section, we derive our algorithm and provide a convergence rate analysis. The key idea here is to apply the regularized semismooth Newton (SSN) method for solving \(R(w)=0\) and improve the computation of each SSN step by exploring the special structure of generalized Jacobian. We also safeguard the regularized SSN method by min-max method to achieve a global rate.

**Generalized Jacobian.** We first examine the special structure of the generalized Jacobian of \(R(w)\). Indeed, by using the definition of \(^{n}_{+}\), we have \(_{^{n}_{+}}(Z)=P_{}_{}P^{}_{}\) where

\[Z=P P^{}=(P_{} P_{})_{ }&0\\ 0&_{}P^{}_{}\\ P^{}_{}, \]The following proposition characterizes the residual map given in Eq. (2.6) and its generalized Jacobian matrix. It also guarantees that the SSN method is suitable to solve \(R(w)=0\).

**Proposition 3.1**: _The residual map \(R\) given in Eq. (2.6) is strongly semismooth._

**Regularized SSN step.** We then discuss how to compute the Newton direction efficiently. In particular, at a given iterate \(w_{k}\), we compute a Newton direction \( w_{k}\) by solving the equation

\[(_{k}+_{k})[ w_{k}]=-r_{k}, \]

where \(_{k} R(w_{k})\), \(r_{k}=R(w_{k})\) and \(\) is an identity operator. The regularization parameter is chosen as \(_{k}=_{k}\|r_{k}\|\) for stabilizing the semismooth Newton method in practice. From a computational point of view, it is not practical to solve the linear system in Eq. (3.2) exactly. Thus, we seek an approximation step \( w_{k}\) by solving Eq. (3.2) approximately such that

\[\|(_{k}+_{k})[ w_{k}]+r_{k}\|\{1, \|r_{k}\|\| w_{k}\|\}, \]

where \(0<,<1\) are some positive constants and \(\|\|\) is defined for a vector-matrix pair \(w=(,X)\) (i.e., \(\|w\|=\|\|_{2}+\|X\|_{F}\) where \(\|\|_{2}\) is Euclidean norm and \(\|\|_{F}\) is Frobenius norm).

Since \(_{k}\) in Eq. (3.2) is nonsymmetric and its dimension is large, we consider applying the Schur complement trick to transform Eq. (3.2) into a smaller symmetric system. If we vectorize the vector-matrix pair \( w\)3, the operators \((Z)\) and \(\) can be expressed as matrices:

\[M(Z)=^{}^{n^{2} n^{2}}, A= _{1}^{}_{1}^{}\\ \\ _{n}^{}_{n}^{}^{n n^{2}},\]

where \(=P P\) and \(=(())\).

We next provide a key lemma on the matrix form of \(_{k}+_{k}I\) at a given iterate \(w_{k}=(_{k},X_{k})\).

**Lemma 3.2**: _Given an iterate \(w_{k}=(_{k},X_{k})\), we compute \(Z_{k}=X_{k}-(^{}(_{k})+_{1}I)\) and use Eq. (3.1) to obtain \(P_{k}\), \(_{k}\), \(_{k}\) and \(_{k}\). We then obtain \(_{k}\), \(_{k}=P_{k} P_{k}\) and \(_{k}=((_{k}))\). Then, the matrix form of \(_{k}+_{k}I\) is given by_

\[(J_{k}+_{k}I)^{-1}=C_{1}BC_{2},\]

_where_

\[C_{1}=I&0\\ -T_{k}A^{}&I, C_{2}=I&+ 1}(A+AT_{k})\\ 0&I,\]

_and_

\[B=(}Q+_{k}I+AT_{k}A^{})^{-1}&0\\ 0&+1}(I+T_{k}),\]

_with \(T_{k}=_{k}L_{k}_{k}^{}\) where \(L_{k}\) is a diagonal matrix with \((L_{k})_{ii}=)ii}{_{k}+1-(_{k})_{ii}}\) and \((_{k})_{ii}(0,1]\) is then denoted as the \(i^{}\) diagonal entry of \(_{k}\)._

As a consequence of Lemma 3.2, the solution of Eq. (3.2) can be obtained by solving one certain symmetric linear system with the matrix \(}Q+_{k}I+AT_{k}A^{}\). We remark that this system is well-defined since both \(Q\) and \(AT_{k}A^{}\) are positive semidefinite and the coefficient \(_{k}\) is chosen such that \(}Q+_{k}I+AT_{k}A^{}\) is invertible. This also shows that Eq. (3.2) is well-defined.

We define \(_{k}\) and \(\) as the operator form of \(T_{k}=_{k}L_{k}_{k}^{}\) and \(Q\) and write \(r_{k}=(r_{k}^{1},r_{k}^{2})\) explicitly where \(r_{k}^{1}^{n}\) and \(r_{k}^{2}^{n n}\). Then, we have

\[(a)=-I&+1}(A+AT)\\ 0&I(r_{k})\{a^{1 }=-r_{k}^{1}-+1}((r_{k}^{2}+_{k}[r_{k}^{2}])), \\ a^{2}=-r_{k}^{2}..\]

The next step consists in solving a new symmetric linear system and is given by

\[()=(}Q+_{k}I+AT_{k}A ^{})^{-1}&0\\ 0&+1}(I+T_{k})(a),\]

which leads to

\[\{^{1}=(}+ _{k}+_{k}^{})^{-1}a^{1},\\ ^{2}=+1}(a^{2}+_{k}[a^{2}])..\]

Compared to Eq. (3.2) whose matrix form has size \((n^{2}+n)(n^{2}+n)\), we remark that the one in the step above is smaller with the size of \(n n\) and can be efficiently solved by conjugate gradient (CG) method or symmetric quasi-minimal residual (QMR) method . The final step is to compute the Newton direction \( w_{k}=( w_{k}^{1}, w_{k}^{2})\) as follows,

\[( w_{k})=I&0\\ -TA^{}&I()\{ []{ll} w_{k}^{1}=^{1},\\  w_{k}^{2}=^{2}-_{k}[^{}(^{1})]. .\]

It remains to provide an efficient manner to compute \(_{k}[]\). Since \(_{k}\) is defined as the operator form of \(T=_{k}L_{k}_{k}^{}\), we have

\[_{k}[S]=P_{k}(_{k}(P_{k}^{}SP_{k}))P_{k}^{},\]

where \(_{k}\) is determined by \(_{k}\) and \(_{k}\). Indeed, we have

\[_{k}=E_{_{k}}&_{_{k}_ {k}}\\ _{_{k}_{k}}&0_{k}= }E_{_{k}_{k}}&_{_{k}_{k}}\\ _{_{k}_{k}}^{}&0,\]

where \(_{ij}=}{_{k}+1-_{ij}}\) for all \((i,j)_{k}_{k}\). Following Zhao et al. , we use the decomposition \(_{k}[S]=G+G^{}\) where \(U=P_{k}(:,_{k})^{}S\) and

\[G=P_{k}(:,_{k})(}(UP_{k}(:,_{k}))P_{k}(:,_ {k})^{}+_{_{k}_{k}}(UP_{k}(:,_{k}) )P_{k}(:,_{k})^{}).\]

The number of flops required to compute \(_{k}[S]\) is \(8|_{k}|n^{2}\). For the case of \(|_{k}|>_{k}\), we compute \(_{k}[S]\) via \(_{k}[S]=}S-P_{k}((}E-_{k})( P_{k}^{}SP_{k}))P_{k}^{}\) using \(8|_{k}|n^{2}\) flops. This demonstrates that we can obtain an approximate solution of Eq. (3.2) efficiently whenever \(|_{k}|\) or \(|_{k}|\) is small. We present the scheme for computing an approximate Newton direction in Algorithm 1.

```
1:Input:\(,,_{2}_{1}>0\), \(_{0}<1\), \(_{1},_{2}>1\) and \(,>0\).
2:Initialization:\(v_{0}=w_{0}^{n}_{+}^{n}\) and \(_{0}>0\). Set \(k=0,1,2,\)do
3: Update \(v_{k+1}\) from \(v_{k}\) using one-step EG.
4: Select \

**Main scheme.** We summarize the complete scheme of our new algorithm in Algorithm 2. Indeed, we generate a sequence of iterates by alternating between extragradient (EG) method [17; 6] and the aforementioned regularized SSN method.

Note that we maintain one auxiliary sequence of iterates \(\{v_{k}\}_{k 0}\). This sequence is directly generated by the EG method for solving the min-max optimization problem in Eq. (2.5) and is used to safeguard the regularized SSN method to achieve a global convergence rate. More specifically, we start with \(v_{0}=w_{0}^{n}_{n}^{*}\) and perform the \(k^{}\) iteration as follows,

1. Update \(v_{k+1}\) from \(v_{k}\) using one-step EG.
2. Update \(_{k+1}\) from \(w_{k}\) using one-step regularized SSN.
3. Set \(w_{k+1}=_{k+1}\) if \(\|R(_{k+1})\|\|R(v_{k+1})\|\) and \(w_{k+1}=v_{k+1}\) otherwise.

In our experiment, we find that the main iterates are mostly generated by regularized SSN steps and the whole algorithm converges at a superlinear rate. This phenomenon is quite intuitive: if the initial point is sufficiently close to one nondegenerate optimal solution, the regularized SSN method can achieve the similar quadratic convergence rate (cf. Theorem 3.4) as shared by other SSN methods in the existing literature [35; 18; 1]. The detailed analysis will be provided in the appendix.

**Main results.** We establish the convergence guarantee of Algorithm 2 in the following theorems.

**Theorem 3.3**: _Suppose that \(\{w_{k}\}_{k 0}\) is a sequence of iterates generated by Algorithm 2. Then, the residuals of \(\{w_{k}\}_{k 0}\) converge to 0 at a rate of \(1/\), i.e., \(\|R(w_{k})\|=O(1/)\)._

**Theorem 3.4**: _Suppose that \(\{w_{k}\}_{k 0}\) is a sequence of iterates generated by Algorithm 2. Then, the residuals of \(\{w_{k}\}_{k 0}\) converge to 0 at a quadratic rate if the initial point \(w_{0}\) is sufficiently close to \(w^{*}\) with \(R(w^{*})=0\) and every element of \( R(w^{*})\) is invertible._

**Remark 3.5**: _In the context of constrained convex-concave min-max optimization problem, Cai et al.  proved the \(O(1/)\) last-iterate convergence rate of the EG, matching the lower bounds [24; 23]. Since the kernel-based OT estimation can be solved as a min-max problem, the global convergence rate in Theorem 3.3 demonstrates the efficiency of Algorithm 2. It remains unclear whether or not we can improve the convergence result by exploring special structure of Eq. (2.5)._

## 4 Experiments

We present the results of experiments that evaluate the kernel-based OT estimation with our algorithm. The baseline approach is the short-step interior-point method ; we exclude the gradient-based method  from our experiment since it only solves the relaxation model. All the experiments were conducted on a MacBook Pro with an Intel Core i9 2.4GHz and 16GB memory.

Following the setup in Vacher et al. , we draw \(n_{}\) samples from \(\) and \(n_{}\) samples from \(\), where \(\) is a mixture of 3 \(d\)-dimensional Gaussian distributions and \(\) is a mixture of 5 \(d\)-dimensional Gaussian distributions. Then, we sample \(n\) filling samples from a \(2d\) Sobol sequence. We also set the bandwidth \(^{2}=0.01\) and parameters \(_{1}=\) and \(_{2}=}}}\). Focusing on the case of \(d=1\) (i.e., 1-dimensional setting), we report the visualization results in Figure 1 and 2 and find that the inferred OT map will be closer the the true OT map as the number of filling points and data samples increase.

Figure 1: Visualization of the OT map with \(n_{}=n\{50,100,200\}\).

By varying the dimension \(d\{2,5,10\}\), we also report the computation efficiency results in Figure 3. It indicates that the our new algorithm is more efficient than the IPM as the number of filling points increases, with smaller variance in computation time (seconds).

The experiments comparing kernel-based OT estimators with plug-in OT estimators on synthetic datasets have been conducted before [59; 38] and the results demonstrate that the kernel-based OT estimators behave better when the number of samples is small. Here, we repeat such experiment but using the real-world 4i datasets from Bunne et al. , which contains single-cell perturbed responses, and which include the unperturbed cells and cells subject to drug perturbations. Our experiments are conducted on 15 datasets with different drug perturbations.

Due to space limit, we defer the results to Appendix G (see Figure 4). We can see that the kernel-based OT estimators computed by our algorithm achieve satisfactory performance and behave better in most cases when the number of training samples is small; in particular, they better on 6 datasets, comparable on 5 datasets and worse on 4 datasets. Note that \(\) computes the entropic regularized plug-in OT estimators and is heavily optimized to effectively handle noisy data. Therefore, it would be no surprise that \(\) outperforms our algorithm when the number of training samples is sufficient. However, the kernel-based OT estimation still provides a fairly effective alternative when the number of training samples is small, which is consistent with the previous observations on synthetic data [59; 38]. Our results also validate the effectiveness of our algorithm for computing kernel-based OT estimators.

## 5 Concluding Remarks

In this paper, we propose a nonsmooth equation model for computing kernel-based OT estimators and show that it has a special problem structure, allowing it to be solved in an efficient manner using semismooth Newton method. In particular, we propose a specialized semismooth Newton method that achieves low per-iteration computational cost by exploiting the special problem structure, and prove a global sublinear convergence rate and a local quadratic convergence rate under standard regularity conditions. Preliminary experimental results on synthetic datasets show that our algorithm is more efficient than the short-step interior-point method , and the results on real data demonstrate the effectiveness of our algorithm. Future work includes the applications of kernel-based OT estimators to deep generative models and other real-world problems.

Figure 3: Comparisons of mean computation time of IPM and our algorithm on CPU time.

Figure 2: Visualization of the constraint with \(n_{}=n\{50,100\}\). The right one is ground truth.