# Quantification of Uncertainty with Adversarial Models

Kajetan Schweighofer\({}^{*}\) Lukas Aichberger\({}^{*}\) Mykyta Ielanskiy\({}^{*}\)

Gunter Klambauer Sepp Hochreiter

ELLIS Unit Linz and LIT AI Lab, Institute for Machine Learning,

Johannes Kepler University Linz, Austria

\({}^{*}\)Joint first authors

###### Abstract

Quantifying uncertainty is important for actionable predictions in real-world applications. A crucial part of predictive uncertainty quantification is the estimation of epistemic uncertainty, which is defined as an integral of the product between a divergence function and the posterior. Current methods such as Deep Ensembles or MC dropout underperform at estimating the epistemic uncertainty, since they primarily consider the posterior when sampling models. We suggest Quantification of Uncertainty with Adversarial Models (QUAM) to better estimate the epistemic uncertainty. QUAM identifies regions where the whole product under the integral is large, not just the posterior. Consequently, QUAM has lower approximation error of the epistemic uncertainty compared to previous methods. Models for which the product is large correspond to adversarial models (not adversarial examples!). Adversarial models have both a high posterior as well as a high divergence between their predictions and that of a reference model. Our experiments show that QUAM excels in capturing epistemic uncertainty for deep learning models and outperforms previous methods on challenging tasks in the vision domain.

## 1 Introduction

Actionable predictions typically require risk assessment based on predictive uncertainty quantification (Apostolakis, 1991). This is of utmost importance in high stake applications, such as medical diagnosis or drug discovery, where human lives or extensive investments are at risk. In such settings, even a single prediction has far-reaching real-world impact, thus necessitating the most precise quantification of the associated uncertainties. Furthermore, foundation models or specialized models that are obtained externally are becoming increasingly prevalent, also in high stake applications. It is crucial to assess the robustness and reliability of those unknown models before applying them. Therefore, the predictive uncertainty of given, pre-selected models at specific test points should be quantified, which we address in this work.

We consider predictive uncertainty quantification (see Fig. 1) for deep neural networks (Gal, 2016; Hullermeier and Waegeman, 2021). According to Vesely and Rasmuson (1984), Apostolakis (1991), Helton (1993), McKone (1994), Helton (1997), predictive uncertainty can be categorized into two types. First, _aleatoric_ (Type A, variability, stochastic, true, irreducible) uncertainty refers to the variability when drawing samples or when repeating the same experiment. Second, _epistemic_ (Type B, lack of knowledge, subjective, reducible) uncertainty refers to the lack of knowledge about the true model. Epistemic uncertainty can result from imprecision in parameter estimates, incompleteness in modeling, or indefiniteness in the applicability of the model. While aleatoric uncertainty cannot be reduced, epistemic uncertainty can be reduced by more data, better models, or more knowledge about the problem. We follow Helton (1997) and consider epistemic uncertainty as the imprecision or variability of parameters that determine the predictive distribution. Vesely and Rasmuson (1984)calls this epistemic uncertainty "parameter uncertainty", which results from an imperfect learning algorithm or from insufficiently many training samples. Consequently, we consider predictive uncertainty quantification as characterizing a probabilistic model of the world. In this context, aleatoric uncertainty refers to the inherent stochasticity of sampling outcomes from the predictive distribution of the model and epistemic uncertainty refers to the uncertainty about model parameters.

Current uncertainty quantification methods such as Deep Ensembles (Lakshminarayanan et al., 2017) or Monte-Carlo (MC) dropout (Gal and Ghahramani, 2016) underperform at estimating the epistemic uncertainty (Wilson and Izmailov, 2020; Parker-Holder et al., 2020; Angelo and Fortuin, 2021), since they primarily consider the posterior when sampling models. Thus they are prone to miss important posterior modes, where the whole integrand of the integral defining the epistemic uncertainty is large. We introduce Quantification of Uncertainty with Adversarial Models (QUAM) to identify those posterior modes. QUAM searches for those posterior modes via adversarial models and uses them to reduce the approximation error when estimating the integral that defines the epistemic uncertainty.

Adversarial models are characterized by a large value of the integrand of the integral defining the epistemic uncertainty. Thus, they considerably differ to the reference model's prediction at a test point while having a similarly high posterior probability. Consequently, they are counterexamples of the reference model that predict differently for a new input, but explain the training data equally well. Fig. 1 shows examples of adversarial models which assign different classes to a test point, but agree on the training data. A formal definition is given by Def. 1. It is essential to note that adversarial models are a new concept that is to be distinguished from other concepts that include the term 'adversarial' in their naming, such as adversarial examples (Szegedy et al., 2013; Biggio et al., 2013), adversarial training (Goodfellow et al., 2015), generative adversarial networks (Goodfellow et al., 2014) or adversarial model-based RL (Rigter et al., 2022).

Our main contributions are:

* We introduce QUAM as a framework for uncertainty quantification. QUAM approximates the integral that defines the epistemic uncertainty substantially better than previous methods, since it reduces the approximation error of the integral estimator.
* We introduce the concept of adversarial models for estimating posterior integrals with non-negative integrands. For a given test point, adversarial models have considerably different predictions than a reference model while having similarly high posterior probability.
* We introduce a new setting for uncertainty quantification, where the uncertainty of a given, pre-selected model is quantified.

## 2 Current Methods to Estimate the Epistemic Uncertainty

Definition of Predictive Uncertainty. Predictive uncertainty quantification is about describing a probabilistic model of the world, where aleatoric uncertainty refers to the inherent stochasticity of sampling outcomes from the predictive distribution of the model and epistemic uncertainty refers to the uncertainty about model parameters. We consider two distinct settings of predictive uncertainty quantification. Setting **(a)** concerns with the predictive uncertainty at a new test point expected under all plausible models given the training dataset (Gal, 2016; Hullermeier and Waegeman, 2021). This definition of uncertainty comprises how differently possible models predict (epistemic) and how

Figure 1: Adversarial models. For the red test point, the predictive uncertainty is high as it is far from the training data. High uncertainties are detected by different adversarial models that assign the red test point to different classes, although all of them explain the training data equally well. As a result, the true class of the test point remains ambiguous.

confident each model is about its prediction (aleatoric). Setting **(b)** concerns with the predictive uncertainty at a new test point for a given, pre-selected model. This definition of uncertainty comprises how likely this model is the true model that generated the training dataset (epistemic) (Apostolakis, 1991; Helton, 1997) and how confident this model is about its prediction (aleatoric).

As an example, assume we have initial data from an epidemic, but we do not know the exact infection rate, which is a parameter of a prediction model. The goal is to predict the number of infected persons at a specific time in the future, where each point in time is a test point. In setting (a), we are interested in the uncertainty of test point predictions of all models using infection rates that explain the initial data. If all likely models agree for a given new test point, the prediction of any of those models can be trusted, otherwise we can not trust the prediction regardless of which model is selected in the end. In setting (b), we have selected a specific infection rate from the initial data as parameter for our model to make predictions. We refer to this model as the given, pre-selected model. However, we do not know the true infection rate of the epidemic. All models with infection rates that are consistent with the initial data are likely to be the true model. If all likely models agree with the given, pre-selected model for a given new test point, the prediction of the model can be trusted.

### Measuring Predictive Uncertainty

We consider the predictive distribution of a single model \(p(,)\), which is a probabilistic model of the world. Depending on the task, the predictive distribution of this probabilistic model can be a categorical distribution for classification or a Gaussian distribution for regression. The Bayesian framework offers a principled way to treat the uncertainty about the parameters through the posterior \(p() p()p()\) for a given dataset \(\). The Bayesian model average (BMA) predictive distribution is given by \(p(,)\ =_{}p(, })p(})}\). Following Gal (2016); Depeweg et al. (2018); Smith and Gal (2018); Hullermeier and Waegeman (2021), the uncertainty of the BMA predictive distribution is commonly measured by the entropy \([p(,)]\). It refers to the total uncertainty, which can be decomposed into an aleatoric and an epistemic part. The BMA predictive entropy is equal to the posterior expectation of the cross-entropy \([\,,\,]\) between the predictive distribution of candidate models and the BMA, which corresponds to setting (a). In setting (b), the cross-entropy is between the predictive distribution of the given, pre-selected model and candidate models. Details about the entropy and cross-entropy as measures of uncertainty are given in Sec. B.1.1 in the appendix. In the following, we formalize how to measure the notions of uncertainty in setting (a) and (b) using the expected cross-entropy over the posterior.

Setting (a): Expected uncertainty when selecting a model.We estimate the predictive uncertainty at a test point \(\) when selecting a model \(}\) given a training dataset \(\). The total uncertainty is the expected cross-entropy between the predictive distribution of candidate models \(p(,})\) and the BMA predictive distribution \(p(,)\), where the expectation is with respect to the posterior:

\[_{} [p(,})\,,\,p( {x},)]\,p(})\,} \ =\ [p(,)]\] \[=\ _{}[p(,})] \,p(})\,}\ +\ [Y\,;\,W,]\] \[=\ }[p(, })]\,p(})\,}}_ {}\ +\ }_{}(p(, })\,\|\,p(,))\,p(} )\,}}_{}\.\]

The aleatoric uncertainty characterizes the uncertainty due to the expected stochasticity of sampling outcomes from the predictive distribution of candidate models \(p(,})\). The epistemic uncertainty characterizes the uncertainty due to the mismatch between the predictive distribution of candidate models and the BMA predictive distribution. It is measured by the mutual information \([\,;]\), between the prediction \(Y\) and the model parameters \(W\) for a given test point and dataset, which is equivalent to the posterior expectation of the KL-divergence \(_{}(\,\|\,)\) between the predictive distributions of candidate models and the BMA predictive distribution. Derivations are given in appendix Sec. B.1.

Setting (b): Uncertainty of a given, pre-selected model.We estimate the predictive uncertainty of a given, pre-selected model \(\) at a test point \(\). We assume that the dataset \(\) is produced according to the true distribution \(p(,^{*})\) parameterized by \(^{*}\). The posterior \(p(})\) is an estimate of how likely \(}\) match \(^{*}\). For epistemic uncertainty, we should measure the difference between the predictive distributions under \(\) and \(^{*}\), but \(^{*}\) is unknown. Therefore, we measure the expected difference between the predictive distributions under \(\) and \(}\). In accordance with Apostolakis (1991) and Helton (1997), the total uncertainty is therefore the expected cross-entropy between the predictive distributions of a given, pre-selected model \(\) and candidate models \(}\), any of which could be the true model \(^{*}\) according to the posterior:

\[_{}[p(,)\,,\,p( ,})]\,p(})\; } \] \[=\;[p(,)]}_{}\;+\;}_{}(p( ,)\,\|\,p(,}))\;p(} )\;}}_{}\;.\]

The aleatoric uncertainty characterizes the uncertainty due to the stochasticity of sampling outcomes from the predictive distribution of the given, pre-selected model \(p(,)\). The epistemic uncertainty characterizes the uncertainty due to the mismatch between the predictive distribution of the given, pre-selected model and the predictive distribution of candidate models that could be the true model. Derivations and further details are given in appendix Sec. B.1.

### Estimating the Integral for Epistemic Uncertainty

Current methods for predictive uncertainty quantification suffer from underestimating the epistemic uncertainty (Wilson and Izmailov, 2020; Parker-Holder et al., 2020; Angelo and Fortuin, 2021). The epistemic uncertainty is given by the respective terms in Eq. (1) for setting (a) and Eq. (2) for our new setting (b). To estimate these integrals, almost all methods use gradient descent on the training data. Thus, posterior modes that are hidden from the gradient flow remain undiscovered and the epistemic uncertainty is underestimated (Shah et al., 2020; Angelo and Fortuin, 2021). An illustrative example is depicted in Fig. 2. Posterior expectations as in Eq. (1) and Eq. (2) that define the epistemic uncertainty are generally approximated using Monte Carlo integration. A good approximation of posterior integrals through Monte Carlo integration requires to capture all large values of the non-negative integrand (Wilson and Izmailov, 2020), which is not only large values of the posterior, but also large values of the KL-divergence.

Variational inference (Graves, 2011; Blundell et al., 2015; Gal and Ghahramani, 2016) and ensemble methods (Lakshminarayanan et al., 2017) estimate the posterior integral based on models with high posterior. Posterior modes may be hidden from gradient descent based techniques as they only discover mechanistically similar models. Two models are mechanistically similar if they rely on the same input attributes for making their predictions, that is, they are invariant to the same input attributes (Lubana et al., 2022). However, gradient descent will always start by extracting input attributes that are highly correlated to the target as they determine the steepest descent in the error

Figure 2: Model prediction analysis. Softmax outputs (black) of individual models of Deep Ensembles (a) and MC dropout (b), as well as their average output (red) on a probability simplex. Models were selected on the training data, and evaluated on the new test point (red) depicted in (c). The background color denotes the maximum likelihood of the training data that is achievable by a model having a predictive distribution (softmax values) equal to the respective location on the simplex. Deep Ensembles and MC dropout fail to find models predicting the orange class, although there would be likely models that do so. Details on the experimental setup are given in the appendix, Sec. C.2.

landscape. These input attributes create a large basin in the error landscape into which the parameter vector is drawn via gradient descent. Consequently, other modes further away from such basins are almost never found (Shah et al., 2020; Angelo and Fortuin, 2021). Thus, the epistemic uncertainty is underestimated. Another reason that posterior modes may be hidden from gradient descent is the presence of different labeling hypotheses. If there is more than one way to explain the training data, gradient descent will use all of them as they give the steepest error descent (Scineca et al., 2022).

Other work focuses on MCMC sampling according to the posterior distribution, which is approximated by stochastic gradient variants (Welling and Teh, 2011; Chen et al., 2014) for large datasets and models. Those are known to face issues to efficiently explore the highly complex and multimodal parameter space and escape local posterior modes. There are attempts to alleviate the problem (Li et al., 2016; Zhang et al., 2020). However, those methods do not explicitly look for important posterior modes, where the predictive distributions of sampled models contribute strongly to the approximation of the posterior integral, and thus have large values for the KL-divergence.

## 3 Adversarial Models to Estimate the Epistemic Uncertainty

Intuition.The epistemic uncertainty in Eq. (1) for setting (a) compares possible models with the BMA. Thus, the BMA is used as reference model. The epistemic uncertainty in Eq. (2) for our new setting (b) compares models that are candidates for the true model with the given, pre-selected model. Thus, the given, pre-selected model is used as reference model. If the reference model makes some prediction at the test point, and if other models (the adversaries) make different predictions while explaining the training data equally well, then one should be uncertain about the prediction. Adversarial models are plausible outcomes of model selection, while having a different prediction at the test data point than the reference model. In court, the same principle is used: if the prosecutor presents a scenario but the advocate presents alternative equally plausible scenarios, the judges become uncertain about what happened and rule in favor of the defendant. We use adversarial models to identify locations where the integrand of the integral defining the epistemic uncertainty in Eq. (1) or Eq. (2) is large. These locations are used to construct a mixture distribution that is used for mixture importance sampling to estimate the desired integrals. Using the mixture distribution for sampling, we aim to considerably reduce the approximation error of the estimator of the epistemic uncertainty.

Mixture Importance Sampling.We estimate the integrals of epistemic uncertainty in Eq. (1) and in Eq. (2). In the following, we focus on setting (b) with Eq. (2), but all results hold for setting (a) with Eq. (1) as well. Most methods sample from a distribution \(q(})\) to approximate the integral:

\[v\;=\;_{}_{}(p(,)\, \|\,p(,}))\,p(})\; }\;=\;_{},,})}{q(})}\;q(})\;}\;, \]

where \(u(,,})=_{}(p(, )\,\|\,p(,}))p(})\). As with Deep Ensembles or MC dropout, posterior sampling is often approximated by a sampling distribution \(q(})\) that is close to \(p(})\). Monte Carlo (MC) integration estimates \(v\) by

\[=_{n=1}^{N},,}_{n})}{q( }_{n})}\;,}_{n} q(})\;. \]

If the posterior has different modes, the estimate under a unimodal approximate distribution has high variance and converges very slowly (Steele et al., 2006). Thus, we use mixture importance sampling (MIS) (Hesterberg, 1995). MIS utilizes a mixture distribution instead of the unimodal distribution in standard importance sampling (Owen and Zhou, 2000). Furthermore, many MIS methods iteratively enhance the sampling distribution by incorporating new modes (Raftery and Bao, 2010). In contrast to the usually applied iterative enrichment methods which find new modes by chance, we have a much more favorable situation. We can explicitly search for posterior modes where the KL divergence is large, as we can cast it as a supervised learning problem. Each of these modes determines the location of a mixture component of the mixture distribution.

**Theorem 1**.: _The expected mean squared error of importance sampling with \(q(})\) can be bounded by_

\[_{q(})}[(\;-\;v)^{2}] _{q(})}[(,,} )}{q(})})^{2}]\;. \]Proof.: The inequality Eq. (5) follows from Theorem 1 in Akyildiz and Miguez (2021), when considering \(0 u(,,})\) as an unnormalized distribution and setting \(=1\). 

Approximating only the posterior \(p(})\) as done by Deep Ensembles or MC dropout is insufficient to guarantee a low expected mean squared error, since the sampling variance cannot be bounded (see appendix Sec. B.2).

**Corollary 1**.: _With constant \(c\), \(_{q(})}[(\ -\ v)^{2}] 4c^{2}/N\) holds if \(u(,,}) c\ q(})\)._

Consequently, \(q(})\) must have modes where \(u(,,})\) has modes even if the \(q\)-modes are a factor \(c\) smaller. The modes of \(u(,,})\) are models \(}\) with both high posterior and high KL-divergence. We are searching for these modes to determine the locations \(}_{k}\) of the components of a mixture distribution \(q(})\):

\[q(})\ =\ _{k=1}^{K}_{k}\ (} \;;}_{k},)\, \]

with \(_{k}=1/K\) for \(K\) such models \(}_{k}\) that determine a mode. Adversarial model search finds the locations \(}_{k}\) of the mixture components, where \(}_{k}\) is an adversarial model. The reference model does not define a mixture component, as it has zero KL-divergence to itself. We then sample from a distribution \(\) at the local posterior mode with mean \(}_{k}\) and a set of shape parameters \(\). The simplest choice for \(\) is a Dirac delta distribution, but one could use e.g. a local Laplace approximation of the posterior (MacKay, 1992), or a Gaussian distribution in some weight-subspace (Maddox et al., 2019). Furthermore, one could use \(}_{k}\) as starting point for SG-MCMC chains (Welling and Teh, 2011; Chen et al., 2014; Zhang et al., 2020, 2022). More details regarding MIS are given in the appendix in Sec. B.2. In the following, we propose an algorithm to find those models with both high posterior and high KL-divergence to the predictive distribution of the reference model.

Adversarial Model Search.Adversarial model search is the concept of searching for a model that has a large distance / divergence to the reference predictive distribution and at the same time a high posterior. We call such models "adversarial models" as they act as adversaries to the reference model by contradicting its prediction. A formal definition of an adversarial model is given by Def. 1:

**Definition 1**.: _Given are a new test data point \(\), a reference conditional probability model \(p(,)\) from a model class parameterized by \(\), a divergence or distance measure \((,)\) for probability distributions, \(\ >0\), \(\ >0\), and a dataset \(\). Then a model with parameters \(}\) that satisfies the inequalities \( p()- p(}) \) and \(\ (p(,),p(,}))\) is called an \((,)-\)adversarial model._

Adversarial model search corresponds to the following optimization problem:

\[_{}\ (p(,)\,,\,p( ,\ +\ ))\  p( )\ -\  p(\ +\ )\ \ . \]

We are searching for a weight perturbation \(\) that maximizes the distance \((,)\) to the reference distribution without decreasing the log posterior more than \(\). The search for adversarial models is restricted to \(\), for example by only optimizing the last layer of the reference model or by bounding the norm of \(\). This optimization problem can be rewritten as:

\[_{}\ (p(,)\,,\,p( ,\ +\ ))\ +\ c\ ( p(\ +\  )\ -\  p()\ +\ ). \]

where \(c\) is a hyperparameter. According to the _Karush-Kuhn-Tucker (KKT) theorem_(Karush, 1939; Kuhn and Tucker, 1950; May, 2020; Luenberger and Ye, 2016): If \(^{*}\) is the solution to the problem Eq. (7), then there exists a \(c^{*} 0\) with \(_{}(^{*},c^{*})=\) (\(\) is the Lagrangian) and \(c^{*}\ ( p()- p(+^{*} )-)=0\). This is a necessary condition for an optimal point according to Theorem on Page 326 of Luenberger and Ye (2016).

We solve this optimization problem by the penalty method, which relies on the KKT theorem (Zangwill, 1967). A penalty algorithm solves a series of unconstrained problems, solutions of which converge to the solution of the original constrained problem (see e.g. Fiacco and McCormick (1990)). The unconstrained problems are constructed by adding a weighted penalty function measuring the constraint violation to the objective function. At every step, the weight of the penalty is increased, thus the constraints are less violated. If exists, the solution to the constraint optimization problem is an adversarial model that is located within a posterior mode but has a different predictive distribution compared to the reference model. We summarize the adversarial model search in Alg. 1.

```
0: Adversarial model \(}\) with maximum \(_{}\) and \(_{} 0\)
0: Test point \(\), training dataset \(=\{(_{k},_{k})\}_{k=1}^{K}\), reference model \(\), loss function \(l\), loss of reference model on the training dataset \(_{}=_{k=1}^{K}l(p(_{k},),_{k})\), minimization procedure MINIMIZE, number of penalty iterations \(M\), initial penalty parameter \(c_{0}\), penalty parameter increase scheduler \(\), slack parameter \(\), distance / divergence measure \((,)\).
1:\(};\;\;};\;\;c  c_{0}\)
2:for\(m 1\) to \(M\)do
3:\(_{}_{k=1}^{K}l(p( _{k},}),_{k})\;-\;(_{}\;+\;)\)
4:\(_{}-(p(,)\;,\;p( ,}))\)
5:\(_{}+c\;_{}\)
6:\(}((}))\)
7:if\(_{}\) larger than all previous and \(_{} 0\)then
8:\(}}\)
9:\(c(c)\)
10:return\(}\)
```

**Algorithm 1** Adversarial Model Search (used in QUAM)

Practical Implementation.Empirically, we found that directly executing the optimization procedure defined in Alg. 1 tends to result in adversarial models with similar predictive distribution for a given input across multiple searches. The vanilla implementation of Alg. 1 corresponds to an _untargeted_ attack, known from the literature on adversarial attacks (Szegedy et al., 2013; Biggio et al., 2013). To prevent the searches from converging to a single solution, we optimize the cross-entropy loss for one specific class during each search, which corresponds to a _targeted_ attack. Each resulting adversarial model represents a local optimum of Eq. (7). We execute as many adversarial model searches as there are classes, dedicating one search to each class, unless otherwise specified. To compute Eq. (4), we use the predictive distributions \(p(,})\) of all models \(}\) encountered during each penalty iteration of all searches, weighted by their posterior probability. The posterior probability is approximated with the negative exponential training loss, the likelihood, of models \(}\). This approximate posterior probability is scaled with a temperature parameter, set as a hyperparameter. Further details are given in the appendix Sec. C.1.

## 4 Experiments

In this section, we compare previous uncertainty quantification methods and our method QUAM in a set of experiments. First, we assess the considered methods on a synthetic benchmark, on which it is feasible to compute a ground truth epistemic uncertainty. Then, we conduct challenging out-of-distribution (OOD) detection, adversarial example detection, misclassification detection and selective prediction experiments in the vision domain. We compare (1) QUAM, (2) cyclical Stochastic Gradient Hamiltonian Monte Carlo (cSG-HMC) (Zhang et al., 2020), (3) an efficient Laplace approximation (Laplace) (Daxberger et al., 2021), (4) MC dropout (MCD) (Gal and Ghahramani, 2016) and (5) Deep Ensembles (DE) (Lakshminarayanan et al., 2017) on their ability to estimate the epistemic uncertainty. Those baseline methods, especially Deep Ensembles, are persistently among the best performing uncertainty quantification methods across various benchmark tasks (Filos et al., 2019; Ovadia et al., 2019; Caldeira and Nord, 2020; Band et al., 2022)

### Epistemic Uncertainty on Synthetic Dataset

We evaluated all considered methods on the two-moons dataset, created using the implementation of Pedregosa et al. (2011). To obtain the ground truth uncertainty, we utilized Hamiltonian Monte Carlo (HMC) (Neal, 1996). HMC is regarded as the most precise algorithm to approximate posterior expectations (Izmailov et al., 2021), but necessitates extreme computational expenses to be applied to models and datasets of practical scale. The results are depicted in Fig. 3. QUAM most closely matches the uncertainty estimate of the ground truth epistemic uncertainty obtained by HMC and excels especially on the regions further away from the decision boundary such as in the top left and bottom right of the plots. All other methods fail to capture the epistemic uncertainty in those regions as gradient descent on the training set fails to capture posterior modes with alternative predictive distributions in those parts and misses the important integral components. Experimental details and results for the epistemic uncertainty as in Eq. (2) are given in the appendix Sec. C.3.

### Epistemic Uncertainty on Vision Datasets

We benchmark the ability of different methods to estimate the epistemic uncertainty of a given, pre-selected model (setting (b) as in Eq. (2)) in the context of (i) out-of-distribution (OOD) detection, (ii) adversarial example detection, (iii) misclassification detection and (iv) selective prediction. In all experiments, we assume to have access to a pre-trained model on the in-distribution (ID) training dataset, which we refer to as reference model. The epistemic uncertainty is expected to be higher for OOD samples, as they can be assigned to multiple ID classes, depending on the utilized features. Adversarial examples indicate that the model is misspecified on those inputs, thus we expect a higher epistemic uncertainty, the uncertainty about the model parameters. Furthermore, we expect higher epistemic uncertainty for misclassified samples than for correctly classified samples. Similarly, we expect the classifier to perform better on a subset of more certain samples. This is tested by evaluating the accuracy of the classifier on retained subsets of a certain fraction of samples with the lowest epistemic uncertainty (Filos et al., 2019; Band et al., 2022). We report the AUROC for classifying the ID vs. OOD samples (i), the ID vs. the adversarial examples (ii), or the correctly classified vs. the misclassified samples (iii), using the epistemic uncertainty as score to distinguish the two classes respectively. For the selective prediction experiment (iv), we report the AUC of the accuracy vs. fraction of retained samples, using the epistemic uncertainty to determine the retained subsets.

Mnist.We perform OOD detection on the FMNIST (Xiao et al., 2017), KMNIST (Clanuwat et al., 2018), EMNIST (Cohen et al., 2017) and OMNIGLOT (Lake et al., 2015) test datasets as OOD datasets, using the LeNet (LeCun et al., 1998) architecture. The test dataset of MNIST (LeCun et al., 1998) is used as ID dataset. We utilize the aleatoric uncertainty of the reference model (as in Eq. (2)) as a baseline to assess the added value of estimating the epistemic uncertainty of the reference model. The results are listed in Tab. 1. QUAM outperforms all other methods on this task, with Deep Ensembles being the runner up method on all dataset pairs. Furthermore, we observed, that only the epistemic uncertainties obtained by Deep Ensembles and QUAM are able to surpass the performance of using the aleatoric uncertainty of the reference model.

ImageNet-1K.We conduct OOD detection, adversarial example detection, misclassification detection and selective prediction experiments on ImageNet-1K (Deng et al., 2009). As OOD dataset, we use ImageNet-O (Hendrycks et al., 2021), which is a challenging OOD dataset that was explicitly created to be classified as an ID dataset with high confidence by conventional ImageNet-1K classifiers. Similarly, ImageNet-A (Hendrycks et al., 2021) is a dataset consisting of natural adversarial exam

Figure 3: Epistemic uncertainty as in Eq. (1) for two-moons. Yellow denotes high epistemic uncertainty. Purple denotes low epistemic uncertainty. HMC is considered as ground truth (Izmailov et al., 2021) and is most closely matched by QUAM. Artifacts for QUAM arise because it is applied to each test point individually, whereas other methods use the same sampled models for all test points.

[MISSING_PAGE_FAIL:9]

outperforms MC dropout even under a very limited computational budget. Furthermore, training a single additional ensemble member for Deep Ensembles requires more compute than evaluating the entire ImageNet-O and ImageNet-A datasets with QUAM when performed on all \(1000\) classes.

## 5 Related Work

Quantifying predictive uncertainty, especially for deep learning models, is an active area of research. Classical uncertainty quantification methods such as Bayesian Neural Networks (BNNs) (MacKay, 1992; Neal, 1996) are challenging for deep learning, since (i) the Hessian or maximum-a-posterior (MAP) is difficult to estimate and (ii) regularization and normalization techniques cannot be treated (Antoran et al., 2022). Epistemic neural networks (Osband et al., 2021) add a variance term (the epinet) to the output only. Bayes By Backprop (Blundell et al., 2015) and variational neural networks (Oleksienko et al., 2022) work only for small models as they require considerably more parameters. MC dropout (Gal and Ghahramani, 2016) casts applying dropout during inference as sampling from an approximate distribution. MC dropout was generalized to MC dropconnect (Mobiny et al., 2021). Deep Ensembles (Lakshminarayanan et al., 2017) are often the best-performing uncertainty quantification method (Ovadia et al., 2019; Wursthorn et al., 2022). Masksembles or Dropout Ensembles combine ensembling with MC dropout (Durasov et al., 2021). Stochastic Weight Averaging approximates the posterior over the weights (Maddox et al., 2019). Single forward pass methods are fast and they aim to capture a different notion of epistemic uncertainty through the distribution or distances of latent representations (Bradshaw et al., 2017; Liu et al., 2020; Mukhoti et al., 2021; van Amersfoort et al., 2021; Postels et al., 2021) rather than through posterior integrals. For further methods and a general overview of uncertainty estimation see e.g. Hullermeier and Waegeman (2021); Abdar et al. (2021) and Gawlikowski et al. (2021).

## 6 Conclusion

We have introduced QUAM, a novel method that quantifies predictive uncertainty using adversarial models. Adversarial models identify important posterior modes that are missed by previous uncertainty quantification methods. We conducted various experiments on deep neural networks, for which epistemic uncertainty is challenging to estimate. On a synthetic dataset, we highlighted the strength of our method to capture epistemic uncertainty. Furthermore, we conducted experiments on large-scale benchmarks in the vision domain, where QUAM outperformed all previous methods.

Searching for adversarial models is computationally expensive and has to be done for each new test point. However, more efficient versions can be utilized. One can search for adversarial models while restricting the search to a subset of the parameters, e.g. to the last layer as was done for the ImageNet experiments, to the normalization parameters, or to the bias weights. Furthermore, there have been several advances for efficient fine-tuning of large models (Houlsby et al., 2019; Hu et al., 2021). Utilizing those for more efficient versions of our algorithm is an interesting direction for future work.

Nevertheless, high stake applications justify this effort to obtain the best estimate of predictive uncertainty for each new test point. Furthermore, QUAM is applicable to quantify the predictive uncertainty of any single given model, regardless of whether uncertainty estimation was considered during the modeling process. This allows to assess the predictive uncertainty of foundation models or specialized models that are obtained externally.

Figure 4: Inference speed vs. performance. MCD and QUAM evaluated on equal computational budget in terms of forward pass equivalents on ImageNet-O (left) and ImageNet-A (right) tasks.