# SuperVLAD: Compact and Robust Image Descriptors

for Visual Place Recognition

 Feng Lu\({}^{1,2}\)1  Xinyao Zhang\({}^{1}\)1  Canming Ye\({}^{1}\)1  Shuting Dong\({}^{1,2}\)

**Lijun Zhang\({}^{3}\)  Xiangyuan Lan\({}^{2,4}\)2  Chun Yuan\({}^{1}\)2 \({}^{1}\)**

\({}^{1}\)Tsinghua Shenzhen International Graduate School, Tsinghua University

\({}^{2}\)Pengcheng Laboratory \({}^{3}\)CIGIT, Chinese Academy of Sciences

\({}^{4}\)Pazhou Laboratory (Huangpu)

{lf22@mails,yuanc@sz}.tsinghua.edu.cn lanxy@pcl.ac.cn

Equal contribution.Corresponding authors.

###### Abstract

Visual place recognition (VPR) is an essential task for multiple applications such as augmented reality and robot localization. Over the past decade, mainstream methods in the VPR area have been to use feature representation based on global aggregation, as exemplified by NetVLAD. These features are suitable for large-scale VPR and robust against viewpoint changes. However, the VLAD-based aggregation methods usually learn a large number of (_e.g._, 64) clusters and their corresponding cluster centers, which directly leads to a high dimension of the yielded global features. More importantly, when there is a domain gap between the data in training and inference, the cluster centers determined on the training set are usually improper for inference, resulting in a performance drop. To this end, we first attempt to improve NetVLAD by removing the cluster center and setting only a small number of (_e.g._, only 4) clusters. The proposed method not only simplifies NetVLAD but also enhances the generalizability across different domains. We name this method **SuperVLAD**. In addition, by introducing ghost clusters that will not be retained in the final output, we further propose a very low-dimensional **1-Cluster VLAD** descriptor, which has the same dimension as the output of GeM pooling but performs notably better. Experimental results suggest that, when paired with a transformer-based backbone, our SuperVLAD shows better domain generalization performance than NetVLAD with significantly fewer parameters. The proposed method also surpasses state-of-the-art methods with lower feature dimensions on several benchmark datasets. The code is available at [https://github.com/lu-feng/SuperVLAD](https://github.com/lu-feng/SuperVLAD).

## 1 Introduction

Visual Place Recognition (VPR) is a task that aims at quickly estimating the coarse geographical location of a place image (_i.e._, query) by retrieving the most similar images from a geo-tagged database . It has garnered significant attention in both computer vision and robotics communities, driven by its wide applications in augmented reality  and robot localization , etc. However, VPR still faces some challenges: On the one hand, images captured at the same place may vary dramatically due to the changes in conditions (_e.g._, lighting and weather) and viewpoints. On the other hand, multiple different places can show high similarity, which may lead to perceptual aliasing . It is quite challenging to address these issues simultaneously, especially for VPR methods that use compact global descriptors to represent the place images.

The VPR is typically implemented with the image retrieval approaches . The place images are first described with global features and then the nearest neighbor search is performed over the database to get the matched place images of the query. The global descriptors are usually obtained by applying the aggregation methods, such as Bag of Words  and Vector of Locally Aggregated Descriptors (VLAD) , to process local (or patch) features. Although some hierarchical (_i.e._two-stage) VPR methods improve robustness under challenging environments by cross-matching local features in the query and candidate images for re-ranking, they incur considerable additional computational latency and memory footprint. Taking a step back, the first stage of these hierarchical methods still requires the use of global features to retrieve candidates. As such, representing place images with compact and robust global descriptors is always the most essential and important issue in VPR.

In the VPR methods based on neural networks, the NetVLAD  and GeM pooling  are the most commonly used aggregation/pooling methods. The former aggregates the feature maps extracted by neural networks with a trainable generalized VLAD layer. Its main difference from the vanilla VLAD  lies in its differentiable soft assignment and better performance. However, it still constructs the output vector by summing the residuals of descriptors assigned to each cluster (like VLAD), which is calculated by subtracting the learned cluster center from the local descriptors. When training data distributions differ, the learned cluster centers also vary. Even if we use a well-generalized backbone to extract the local features of two images, the differences in cluster centers can lead to completely different results in the similarity of the output global features (see Fig. 1). In other words, when there is a domain gap between the training and inference data, the cluster center learned on the training set is usually not suitable for inference, which can cause performance drops . Besides, the global descriptor yielded by NetVLAD has a large dimension. Even with dimensionality reduction methods like PCA (or other learnable linear projections), the dimension often remains notably higher compared to the output of pooling methods. As a result, some VPR approaches requiring very low-dimensional features use GeM pooling to yield global descriptors at the cost of certain performance.

To address the above issues, we propose a compact and robust global descriptor for VPR, named SuperVLAD. Our method uses the transformer-based backbone to produce initial patch features. Similar to NetVLAD, SuperVLAD calculates the weights of assigning patch features to clusters in the soft-assignment way. The key difference is that SuperVLAD does not compute cluster centers. Instead, we directly perform a weighted summation of the local features assigned to each cluster to achieve the purpose of aggregating local features. This means SuperVLAD only needs to learn the assignment of local features to clusters, without needing to learn the cluster centers, which directly improves its domain generalization. Meanwhile, we set a small number of clusters (an order of magnitude lower than vanilla NetVLAD), resulting in more compact global descriptors. Furthermore, by introducing supernumerary ghost clusters during the soft assignment and retaining one real (useful) cluster in the final output vector, we obtain a 1-cluster VLAD descriptor with extremely low dimensionality and promising performance. Our work brings the following **contributions**:

(1) We propose a SuperVLAD aggregation method that does not require cluster centers to produce robust global descriptors. It can mitigate the performance degradation in NetVLAD caused by the training and testing data bias, while having fewer parameters (_i.e._, more lightweight). Besides, it can use only a small number of clusters to yield compact descriptors.

Figure 1: VLAD and SuperVLAD similarity measures under different clusterings (Voronoi cells). Orange triangles and blue diamonds depict local descriptors from two different images. In (a) and (b), orange and blue arrows are the sum of residuals (for VLAD). With different training data distributions, the different cluster centers are yielded, causing opposite similarity results using cosine similarity (or normalized L2 distance). Compared to VLAD, our SuperVLAD, as shown in (c) and (d), simply calculates the distance between the weighted sum of local features directly, freeing from the impact of cluster centers. Thus, only minor changes will occur when dealing with two different distributions.

(2) We also design a 1-cluster VLAD method by employing supernumerary ghost clusters during the soft assignment. It produces very low-dimensional features similar to the pooling methods. Compared to the same-dimensional GeM feature or class token, it shows notable performance advantages.

(3) We conduct extensive experiments using various transformer-based backbones, demonstrating the effectiveness of our method. Furthermore, our SuperVLAD with the DINOv2 backbone outperforms state-of-the-art (SOTA) methods on several benchmark datasets with lower-dimensional descriptors.

## 2 Related Work

In the early development of VPR, the methods predominantly relied on hand-crafted local features such as SURF [6; 13]. These local descriptors were then aggregated into global features using algorithms such as Bag of Words , Fisher Vector , and VLAD [28; 53; 5; 32]. The resulting global feature vectors were subsequently used to perform a nearest neighbor search over the database to retrieve the most similar images. As deep learning techniques have advanced, the representation capability of deep features has been widely recognized in the VPR community [51; 4; 29; 11; 12; 44; 18; 20; 58; 60; 52; 33; 1; 2; 21; 7; 10]. Arandgolovic et al. designed a pioneering architecture that combines deep neural networks with the proposed differentiable VLAD aggregation approach called NetVLAD. Likewise, other traditional aggregation algorithms were also transformed into differentiable modules as the aggregation layer of neural networks for end-to-end training [26; 45; 61]. Despite remarkable performance over global max (or average) pooling, NetVLAD-related methods [4; 29; 34; 21] tended to have high-dimensional feature representations, which can be a drawback in terms of real-time performance . To address this, the Generalized Mean (GeM) pooling  was considered a simple alternative that obtains low-dimensional global features. This method simply extended global average pooling by using the \(p\)-norm of local features instead of the average, where \(p\) is also a trainable parameter. Additionally, Berton et al. established an open-source benchmark to fairly compare these standard global-retrieval-based VPR approaches under a unified framework.

Although global-retrieval-based methods have achieved reasonable performance, most of them exhibit limited robustness in challenging environments and are susceptible to perceptual aliasing. Two common methods to improve the robustness of VPR systems are leveraging temporal consistency constraints and spatial consistency constraints. The first method matches image sequences (_i.e._, maintains temporal continuity) [43; 16; 22; 37; 19] to achieve robust VPR with extreme condition changes. The second method usually involves a two-stage VPR process [25; 24; 8; 55; 40; 67; 38; 41], where it first retrieves the top-k candidate images in the database with global features and then re-ranks these candidates by matching local features with spatial consistency verification. However, these methods always introduce extra consumption in computational latency or/and memory footprint.

In the last two years, with the emergence of purpose-built large-scale VPR training datasets [7; 1] and pre-trained foundation models [62; 46], some robust global-retrieval-based VPR methods have been proposed. CosPlace  and EigenPlaces  cast the training of VPR as a classification problem and trained the VPR model on the San Francisco eXtra Large (SF-XL) datasets. MixVPR  incorporated the deep features with the multi-layer perceptrons and trained the model with the Multi-Similarity loss  on the large-scale and appropriate supervised GSV-Cities  dataset. These works all have achieved outstanding performance using only the CNN models. Other works [30; 41; 39; 27] were based on visual foundation models and achieved better results. DINOv2  has been the most widely used foundation model in VPR. It is a ViT-based  model trained on the large-scale curated LVD-142M dataset using a self-supervised strategy and can offer powerful visual features for downstream tasks. SelaVPR  proposed a hybrid global-local adaptation method to adapt the DINOv2 model for two-stage VPR. CricaVPR  also adapted DINOv2 as the backbone and proposed a cross-image correlation-aware representation learning method to enhance the robustness of image features. A closely related work to our SuperVLAD is the SALAD work , which used DINOv2 as the backbone and presented a novel aggregation algorithm to improve NetVLAD. SALAD redefined the soft assignment of local features in NetVLAD as an optimal transport problem and employed the Sinkhorn algorithm  to solve it. Different from this work, our SuperVLAD maintains the soft-assignment way of NetVLAD but effectively solves the issue caused by the learned cluster centers in NetVLAD being unsuitable for various inference data distributions.

## 3 Methodology

Fig. 2 shows the overview of SuperVLAD. We first use a backbone to extract local features (Sec. 3.1), then aggregate them with SuperVLAD (Sec. 3.2). 1-Cluster VLAD is an alternative to get more compact descriptors (Sec. 3.3) and cross-image encoder is optional to boost performance (Sec. 3.4).

### Local Features Extraction

The Vision Transformer (ViT)  and its variants possess superiority in capturing long-range feature dependencies and have shown remarkable performance on various computer vision tasks , including VPR . In this work, we use ViT (or its variants) as the backbone for feature extraction. ViT initially divides the image into \(N\) patches and linearly projects them into \(D\)-dim patch embeddings \(x_{p}^{N D}\), after which it prepends a learnable class token to \(x_{p}\) as \(x_{0}^{(N+1) D}\). Following the addition of positional embeddings, \(x_{0}\) is input into a sequence of transformer encoder layers to generate the feature representation. The final output of ViT includes one class token and \(N\) patch tokens. We directly discard the former and use the latter as local features to be input to the subsequent SuperVLAD aggregation layer, thereby obtaining the final global descriptor of the place image.

### SuperVLAD Layer

SuperVLAD produces the global descriptor of an image by first associating all local features of this image with \(K\) clusters and subsequently aggregating these features into each cluster. The basic process of the SuperVLAD layer is similar to that of NetVLAD , so we start by introducing our work from NetVLAD (and VLAD ).

For NetVLAD, given \(N\) local descriptors \(\{_{i}\}(_{i}^{D})\) of an image as input, and \(K\) cluster centers \(\{_{k}\}(_{k}^{D})\) as parameters, it computes a matrix \(V^{K D}\) as the image representation. The \(k\)-th row in the matrix \(V\) accumulates the (weighted) residuals \((_{i}-_{k})\) of local descriptors assigned to cluster \(_{k}\). More formally, the (\(k,j\))-th element in the matrix \(V\) is computed as follows:

\[V_{k,j}=_{i=1}^{N}a_{k}(_{i})(x_{i,j}-c_{k,j} ), \]

where \(x_{i,j}\) and \(c_{k,j}\) are the \(j\)-th element of the \(i\)-th local descriptor and \(k\)-th cluster center, respectively. \(a_{k}(_{i})\) is the weight of the local descriptor \(_{i}\) assigned to the cluster \(_{k}\). In VLAD, the assignment is hard, _i.e._, \(a_{k}(_{i})\) equals 0 or 1. In contrast, NetVLAD replaces it with the soft assignment and computes \(a_{k}(_{i})\) as:

\[a_{k}(_{i})=_{i}-_{k}\|^{2}}}{_{k^{}}e^{-\|_{i}-_ {k^{}}\|^{2}}}, \]

Figure 3: Unlike VLAD, since the parameters \(_{k}\) and \(b_{k}\) used for soft-assignment in NetVLAD are decoupled from cluster center \(_{k}\), \(_{k}\) does not necessarily coincide with the true centroid of the cluster (Voronoi cell). Its robustness against domain shift can be improved to some extent. SuperVLAD completely eliminates the need for cluster centers and avoids their negative impact.

Figure 2: Illustration of the proposed SuperVLAD layer. It aggregates the patch tokens output by the transformer-based backbone and produces a \(K D\) vector as the global descriptor. Note that the VLAD core of SuperVLAD has no cluster center, which is the main difference from NetVLAD.

Expanding the squares in Eq. 2, we can readily observe the cancellation of the term \(e^{-\|_{i}\|^{2}}\) between the numerator and the denominator, yielding the following form:

\[a_{k}(_{i})=_{k}^{T}_{i}+b_{k }}}{_{k^{}}e^{_{k^{}}^{T}_{i}+b_{k^{ }}}}, \]

where vector \(_{k}=2_{k}\) and scalar \(b_{k}=-\|_{k}\|^{2}\). In the implementation of NetVLAD, \(\{_{k}\}\), \(\{b_{k}\}\), and \(\{_{k}\}\) are set as three independent sets of trainable parameters (while VLAD has only \(\{_{k}\}\)). That is, \(\{_{k}\}\) and \(\{b_{k}\}\) are actually decoupled from \(\{_{k}\}\). As shown in Fig. 3, NetVLAD, with its greater flexibility than VLAD, has some potential to alleviate the performance drop caused by domain shift . However, NetVLAD retains cluster centers, which would be hardly suitable for various data distributions in inference as it is learned on one training set . Since the assignment of local features has been decoupled from the cluster centers in NetVLAD (in VLAD, it is based on the distance to the cluster center), we can directly remove the cluster center and aggregate the first-order statistics of local features (rather than residuals) assigned into each cluster. So, the matrix \(V\) in SuperVLAD can be formulated as

\[V_{k,j}\,=\,_{i=1}^{N}a_{k}(_{i})x_{i,j}\,=\,_{i =1}^{N}_{k}^{T}_{i}+b_{k}}}{_{k^{}}e^{ _{k^{}}^{T}_{i}+b_{k^{}}}}x_{i,j}. \]

For each cluster, SuperVLAD has the parameters \(_{k}\) and \(b_{k}\)1, compared to NetVLAD with the parameters \(_{k}\), \(b_{k}\), and \(_{k}\), and VLAD with the parameter \(_{k}\). This makes SuperVLAD not affected by the cluster center \(_{k}\) and provides greater flexibility compared to NetVLAD. Besides, the number of parameters in SuperVLAD is about half less than that in NetVLAD. It is worth noting that Eq. 4 is different from applying the attention mechanism to local features. Our approach involves assigning features to clusters (rather than applying attention to features). More specifically, given a feature, the sum of the weights of assigning it to all clusters is the constant 1. However, for a given cluster, the sum of the weights of assigning all features to it is not fixed (for attention, it is the constant 1). The recent work SALAD  also sums the local features with the soft-assignment weights to get the global descriptor. However, its soft assignment relies on clusters (the so-called optimal transport between clusters and local features), _i.e._, not free from clusters, and requires iterative computations to solve. Its parameters are no less than NetVLAD. These are different from our lightweight and low-computation SuperVLAD. Besides, In SuperVLAD we set only a small number of clusters, namely 4, which makes the output compact without additional dimensionality reduction techniques. As shown in Fig. 2, the main module of SuperVLAD is a 1x1 convolution (conv) and softmax for soft-assignment, and a VLAD core for aggregation. Unlike NetVLAD, the VLAD core of SuperVLAD has no parameters, and all trainable parameters exist only in the conv layer. The matrix \(V\) is finally intra-normalized, flattened into a vector, and entirely L2-normalized as the output global descriptor.

Moreover, we also absorb GhostVLAD, which extends NetVLAD by introducing "ghost" clusters. Specifically, it adds additional \(G\) ghost clusters that are used for the soft assignment in the same way as the original \(K\) clusters, but the ghost clusters are disregarded in the aggregation process and do not directly contribute to the final output. Ghost clusters can be used to correspond to useless objects in VPR, such as sky, ground, dynamic objects, and so forth. In SuperVLAD, the number of ghost clusters is set to only one (_i.e._, \(G=1\)).

### 1-Cluster VLAD

Due to the high dimensionality of the descriptors output by NetVLAD, some SOTA VPR methods use learnable linear projections or GeM pooling for low-dimensional representations, _e.g._, CosPlace , EigenPlaces , and MixVPR . The GeM pooling is particularly popular because it easily produces global representations with the same dimensionality as the pooled local/patch features, although it may be lacking in performance. Here, we design a 1-cluster VLAD algorithm that can produce very low dimensional descriptors, which has the same output dimensionality as GeM but performs better. Specifically, we only need to set the number of useful clusters in SuperVLAD to 1, and the number of ghost clusters to greater than 1, (we set it to 2), _i.e._, \(K\)=1 and \(G\)=2, to get the 1-cluster VLAD. This means that all descriptors representing objects relevant to VPR are assigned to the same cluster, and their weighted sum (the weight is \(a_{1}(_{i})\)) is used as the final global descriptor.

### Cluster-wise Cross-image Interaction

This subsection introduces an optional process for SuperVLAD (not for 1-Cluster VLAD). To further enhance the performance, we draw inspiration from the cross-image encoder proposed in CricaVPR , which can use the cross-image variations as a cue to guide the representation learning and produce more robust image features through cross-image interaction in a batch. However, directly using the transformer encoder to process the entire SuperVLAD vector would incur significant memory and running time overhead, proportional to the square of the vector size. To overcome this issue, we first split the SuperVLAD descriptors by clusters and then input them into the cross-image encoder ( _i.e._, two stacked transformer encoders) for cluster-wise cross-image interaction. That is, this encoder will model the correlation between the aggregated features belonging to the same cluster of all images in a batch. We then concatenate the aggregated features belonging to different clusters of an image and L2 normalize again to get the final descriptor of this image.

## 4 Experiments

### Datasets and Performance Evaluation

We conduct experiments on several VPR benchmark datasets, which cover various challenges in VPR such as viewpoint changes, condition changes, and perceptual aliasing. Table 1 provides a concise summary of them. **Pitts30k** mainly exhibits severe viewpoint changes in the urban environment. **MSLS** is a comprehensive dataset comprising images collected in urban, suburban, and natural scenes over 7 years, and encompasses a wide range of visual changes (viewpoint and condition changes). **Nordland** exhibits seasonal changes in natural and suburban scenes. **SPED** consists of low-quality and high-scene-depth images captured from diverse scenes with various condition changes. More details are in Appendix G.

In our experiments, we employ the Recall@N (_i.e._, R@N) to assess the recognition performance of VPR methods. This metric quantifies the percentage of queries for which at least one of the N retrieved reference images falls within a predefined threshold of the ground truth. We set the threshold to 25 meters for Pitts30k and MSLS, \( 10\) frames for Nordland, unique counterpart for SPED, following common evaluation procedures .

### Implementation Details

Here we describe the implementation details of training the DINOv2-based SuperVLAD model on the GSV-Cities  dataset for SOTA comparison. More details of training the other transformer-based models for the ablation study are in Appendix E. We implement our experiments on two NVIDIA GeForce RTX 3090 GPUs using PyTorch. The size of the input image is 224x224 in training (322x322 in inference) and the token dimension of the DINOv2-base backbone is 768. The descriptor dimension of SuperVLAD is 3072 and that of 1-cluster VLAD is 768. We only fine-tune the last four transformer encoder layers (freeze the previous layers) of the DINOv2 backbone. For the loss function, we utilize the multi-similarity loss  and set its hyperparameters as in . The model training is performed using the Adam optimizer with an initial learning rate of 0.00005, halved every 3 epochs. Considering that the cross-image encoder is not initialized, we use a larger learning rate (0.0001) to train it separately. Each training batch consists of 120 places, with 4 images per place, resulting in a total of 480 images. An inference batch consists of 8 images (except for the SPED dataset where the batch size is 4). The training process is terminated when the performance on MSLS-val does not improve for three epochs. The actual number of effective training epochs is 7, and the training time is 81.6 minutes.

### Comparisons with State-of-the-Art Methods

In this section, we compare the proposed SuperVLAD with several excellent VPR methods, including seven one-stage methods using global feature retrieval (like ours): NetVLAD , SFRS ,

    &  &  \\   & & Database & Queries \\  Pitts30k-test & urban, panorama & 10,000 & 6816 \\  MSLS-val & urban, suburban & 18,871 & 740 \\  Nordland & natural, seasonal & 27,592 & 27,592 \\  SPED-test & various scenes & 607 & 607 \\   

Table 1: Summary of the evaluation datasets.

CosPlace , MixVPR , Eigenplaces , CricaVPR , and SALAD , as well as two two-stage methods with re-ranking: TransVPR  and SelaVPR . Note that MixVPR, CricaVPR, SALAD, and our SuperVLAD use the same training dataset, _i.e._, GSV-Cities. Meanwhile, CosPlace and EigenPlaces are trained on the purpose-built extra large-scale (SF-XL) datasets. Additionally, the latest works SelaVPR, CricaVPR, and SALAD all use the foundation model DINOv2 as the backbone (SelaVPR using DINOv2-large and others using DINOv2-base) and achieve the SOTA performance on the VPR task. So here we follow them and use the DINOv2-base backbone. The details of the above methods can be seen in Appendix H. Table 2 shows the quantitative results and our SuperVLAD achieves the best results on all these datasets.

The methods based on the DINOv2 backbone, including SelaVPR, CricaVPR, SALAD, and our SuperVLAD, all achieve excellent performance and outperform the remainder methods on these datasets with diverse challenges. These methods all fine-tune DINOv2 in different ways, which shows that based on the powerful feature representation capability of DINOv2, coupled with appropriate fine-tuning, it is sufficient to cope with most challenges in the VPR task. However, our method uses a more compact feature representation (than CricaVPR and SALAD) and outperforms other methods on the four datasets. In particular, it achieves 93.2% R@1 on the SPED dataset, demonstrating significant advantages compared to other global-retrieval-based methods and two-stage methods. This indicates the high robustness of our method to handle condition variations in datasets containing low-quality and high-scene-depth images. Additionally, both CricaVPR and SuperVLAD use the cross-image encoder and thus achieve significantly better performance than other methods on Pitts30k and Nordland, which are able to provide different images of the same place in a batch and mutually improve condition invariance and viewpoint invariance . However, such benefit cannot be obtained on MSLS-val and SPED (all query images from different places), our approach still outperforms all other methods (and is obviously better than CricaVPR).

    &  &  &  &  &  &  \\   & & & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\  NetVLAD  & VGG16 & 32768 & 81.9 & 91.2 & 93.7 & 53.1 & 66.5 & 71.1 & 6.4 & 10.1 & 12.5 & 70.2 & 84.5 & 89.5 \\ SFRS  & VGG16 & 4096 & 89.4 & 94.7 & 95.9 & 69.2 & 80.3 & 83.1 & 16.1 & 23.9 & 28.4 & 80.2 & 92.6 & 95.4 \\ TransVPR  & / & / & 89.0 & 94.9 & 96.2 & 86.8 & 91.2 & 92.4 & 63.5 & 68.5 & 70.2 & 85.7 & 90.9 & 91.8 \\ CosPlace  & VGG16 & 512 & 88.4 & 94.5 & 95.7 & 82.8 & 89.7 & 92.0 & 58.5 & 73.7 & 79.4 & 75.5 & 87.0 & 89.6 \\ MixVPR  & ResNet50 & 4096 & 91.5 & 95.5 & 96.3 & 88.0 & 92.7 & 94.6 & 76.2 & 86.9 & 90.3 & 84.7 & 92.3 & 94.4 \\ EigenPlaces  & ResNet50 & 2048 & 92.5 & 96.8 & 97.6 & 89.1 & 93.8 & 95.0 & 71.2 & 83.8 & 88.1 & 70.2 & 83.5 & 87.5 \\ SelavVPR  & DINOv2-l & / & 92.8 & 96.8 & 97.7 & 90.8 & 96.4 & 97.2 & 87.3 & 93.8 & 95.6 & 89.5 & 94.6 & 95.9 \\ CricaVPR  & DINOv2-b & 4096 & 94.9 & 97.3 & **98.2** & 90.0 & 95.4 & 96.4 & 90.7 & 96.3 & 97.6 & 91.4 & 95.2 & 96.2 \\ SALAD  & DINOv2-b & 8448 & 92.5 & 96.4 & 97.5 & **92.2** & 96.4 & 97.0 & 89.7 & 95.5 & 97.0 & 92.1 & 96.2 & 96.5 \\ SuperVLAD & DINOv2-b & 3072 & **95.0** & **97.4** & **98.2** & **92.2** & **96.6** & **97.4** & **91.0** & **96.4** & **97.7** & **93.2** & **97.0** & **98.0** \\   

Table 2: Comparison to state-of-the-art methods on four VPR benchmark datasets. The best results are highlighted in **bold** and the second are underlined. The descriptor dimensionalities of two-stage methods are not displayed.

Figure 4: Qualitative results. In these four challenging examples (covering viewpoint variations, condition variations, dynamic objects, etc.), our SuperVLAD successfully retrieves the right database images, while other methods get the wrong results.

Fig. 4 qualitatively shows the superior performance of our SuperVLAD in some difficult scenes. These examples exhibit various challenges, such as drastic condition changes, viewpoint changes, and occlusions (caused by dynamic objects). Other methods mostly return similar images from different places, _i.e._, suffer from perceptual aliasing and fail to get the right results. However, our SuperVLAD retrieves the correct reference images, showing high robustness against these challenges.

Fig. 5 simultaneously shows the R@1 on Pitts30k, the inference time of a single image, and the descriptor dimensionality. Among the four methods (SFRS, MixVPR, SALAD, and SuperVLAD), MixVPR uses the CNN backbone and the feature mixing method to get global descriptors, which achieves the shortest inference time. The remaining three methods use VLAD-related methods to get global features, and our SuperVLAD achieves the fastest inference speed among them. Although SFRS is based on the CNN model, it uses NetVLAD with 64 clusters to obtain high-dimensional image features and then uses PCA for dimensionality reduction (time-consuming). SALAD and our SuperVLAD are based on the foundation model DINOv2. However, SALAD uses the Sinkhorn algorithm to iteratively compute optimal transport assignment between clusters and local features, which takes more inference time than our method (17.8ms vs. 13.5ms). Besides, the number of parameters in our SuperVLAD aggregator is much lower than that of SALAD (less than 3/1000), as shown in Table 3. Our SuperVLAD uses a lightweight and low-compute aggregation layer to get the compact global descriptor (without extra dimensionality reduction), which simultaneously has fast inference speed, low-dimensional descriptors, and excellent recognition performance.

### Ablation Study

In this section, we conduct a series of ablation experiments to demonstrate the effectiveness of the proposed SuperVLAD and 1-Cluster VLAD. The cross-image encoder is not used by default in ablation experiments. The SuperVLAD using the cross-image encoder is denoted as SuperVLAD\({}^{}\).

**Effect of SuperVLAD.** We first validate the effectiveness of SuperVLAD by comparing it to NetVLAD. To be fair, we do not use the cross-image encoder in SuperVLAD, and apply multiple different transformer-based models as the backbone, including the ViT  and CCT  models pre-trained on ImageNet , and the foundation model DINOv2  (that is also a ViT model) pre-trained on the large-scale curated dataset. We conduct experiments with three training sets (MSLS, Pitts30k, and GSV-Cities) and two test sets (MSLS and Pitts30k). To validate that SuperVLAD has superior generalization ability over NetVLAD in addressing the domain shift issue mentioned above, we focus on the cross-domain inference performance of models. For example, the performance of a model trained on Pitts30k when tested on MSLS, or vice versa. The results are shown in Table 4. Note that Pitts30k contains only urban scene images, while MSLS covers urban, suburban, and natural scenes, and GSV-Cities is a large-scale training set with accurate supervision and diverse visual variations. Thus, the performance of a model trained on Pitts30k and tested on MSLS best reflects domain generalization, while the reverse is less indicative. The model trained on GSV-Cities encounters almost no domain drift issues on these test sets (Pitts30k and MSLS-val). The experimental results largely align with these facts and the above theory. Training the SuperVLAD model (based on CCT and DINOv2) on Pitts30k and testing it on Pitts30k yields similar results to NetVLAD. However, when tested on MSLS, it shows an absolute R@1 improvement of 6.8% (with CCT) and 2.4% (with DINOv2), respectively. The SuperVLAD model trained on MSLS using ViT

Figure 5: The comparison of some global-retrieval-based methods in Recall@1 (on Pitts30k), inference time (ms/single image), and descriptor dimensionality. The diameter of each dot is proportional to the descriptor dimension. Our SuperVLAD gets the best R@1 with the most compact descriptor.

   Method & Total (M) & Trainable (M) & Aggregator (M) \\  SALAD & 88.0 & 29.8 & 1.4 \\  SuperVLAD & 86.6 (+11.0) & 28.4 (+11.0) & 0.0038 \\   

Table 3: The number of parameters of SALAD and SuperVLAD that both use the DINOv2-base backbone. The value in parentheses is the number of parameters in the optional cross-image encoder.

exhibits similar test results to NetVLAD on MSLS-val but still has a slight advantage when tested on Pitts30k. These results demonstrate that our SuperVLAD has better domain generalization than NetVLAD. Since the GSV-Cities dataset covers the various visual variations in VPR and provides accurate supervision for fine-tuning, both models trained on it perform very well on Pitts30k and MSLS-val. However, this does not mean that our SuperVLAD has no advantage over NetVLAD with such a large-scale VPR training set. SuperVLAD achieves the same performance as NetVLAD with fewer parameters and computations, _e.g._, offering equivalent performance at a lower cost.

**Effect of ghost clusters** and **the performance of 1-Cluster VLAD.** To investigate the impact of ghost clusters, we conduct two sets of experiments based on CCT and DINOv2 to compare the performance with and without ghost clusters. All models also do not use the cross-image encoder. The CCT-based model is trained on Pitts30k and the DINOv2-based model is trained on GSV-Cities. The results are shown in Table 5. When based on DINOv2, whether using ghost clusters or not achieves excellent performance, with little to no difference in their performance (without using ghost clusters even slightly better). However, based on CCT and trained on Pitts30k, the use of ghost clusters resulted in a certain improvement. This indicates that the ghost cluster is still necessary. More importantly, we leverage it to achieve 1-Cluster VLAD. We compared the proposed 1-Cluster VLAD with two commonly used methods (GeM pooling and class token) that can output very compact features of the same dimension as local features (as does our 1-Cluster VLAD). The results, as shown in Table 6, demonstrate the obvious superiority of 1-Cluster VLAD compared to the other two methods. So, our 1-Cluster VLAD can be used as a new choice for very low-dimensional descriptors.

**Effect of the number of clusters.** To investigate the impact of the number of clusters on global descriptors, we conducted two sets of experiments using SuperVLAD with and without the cross-image encoder. All models utilize DINOv2 as the backbone and are trained on GSV-Cities. Results (see Table 7) indicate that even with a small number of clusters, there is no significant performance decrease. This is primarily attributed to the powerful feature representation capability of the transformer-based foundation model DINOv2, while the large-scale VPR training dataset GSV-Cities offers appropriate supervision for fine-tuning. Even with a small number of clusters, it allows for reasonable and effective classification/clustering of objects that are relevant to place recognition (_e.g._, buildings, vegetation). Unlike most VLAD-related methods that use dozens of clusters, our experiments show

   &  &  \\   & & R@1 R@5 R@10 & R@1 R@5 R@10 \\   & 89.5 & 95.0 & 96.3 & 85.4 & 93.0 & 94.3 \\  & 91.4 & **96.2** & **97.4** & 88.4 & 95.1 & **96.4** \\
1-ClusterVLAD & **91.6** & **96.2** & **97.4** & **90.4** & **95.3** & **96.4** \\  

Table 6: Comparison of the very low-dimensional global descriptors with the same dimensions as the local descriptors. That is, all methods produce 768-dim global descriptors (using DINOv2-base backbone). All models are trained on GSV-Cities.

   &  &  &  &  \\   & & & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\  NetVLAD &  &  & 79.0 & 90.9 & 94.1 & 75.7 & 89.5 & 91.8 \\ SuperVLAD & & & 79.4(+-0.4) & 91.9(+-1.0) & 94.7(+-0.6) & 75.9(+-0.2) & 88.8(-0.7) & 91.8(+0.0) \\  NetVLAD &  &  & 84.7 & 93.0 & 95.3 & 55.4 & 68.5 & 73.1 \\ SuperVLAD & & & 84.7(+-0.0) & 93.0(+-0.0) & 95.2(-0.1) & 62.2(+-0.6) & 73.1(+-0.6) & 77.3(+-0.2) \\  NetVLAD &  &  & 89.4 & 95.7 & 96.9 & 72.7 & 84.7 & 87.2 \\ SuperVLAD & & & 89.4(+-0.0) & 95.8(+-0.1) & 97.1(+-0.2) & 75.1(+-0.2) & 84.6(-0.1) & 87.4(+-0.2) \\  NetVLAD &  &  & 92.3 & 96.8 & 97.7 & 91.6 & 96.2 & 96.8 \\ SuperVLAD & & & 92.6(+-0.3) & 96.4(-0.4) & 97.5(-0.2) & 92.2(+0.6) & 95.9(-0.3) & 96.8(+0.0) \\  

Table 4: Comparison of NetVLAD and SuperVLAD with different backbones and training sets. Since the token dimension of CCT is only half of ViT, models based on it have 8 clusters, while models based on ViT/DINOv2 have 4 clusters. The values in parentheses indicate the change in results of SuperVLAD relative to NetVLAD: red for increase, green for decrease, and black for no change.

   &  &  \\   & & R@1 R@5 R@10 & R@1 R@5 R@10 \\  CCT-SV-ng & 84.1 & 92.9 & 95.1 & 60.3 & 72.2 & 76.9 \\ CCT-SV & **84.7** & **93.0** & **95.2** & **62.2** & **73.1** & **77.3** \\  DINOv2-SV-ng & 92.4 & **96.5** & **97.6** & **92.4** & **96.4** & **96.9** \\ DINOv2-SV & **92.6** & 96.4 & 97.5 & 92.2 & 95.9 & 96.8 \\  

Table 5: Comparison of SuperVLAD with and without the ghost cluster. “SV” is short for SuperVLAD. The methods with the “-ng” suffix are those without the ghost cluster. Specifically, DINOv2-SV is the model based on DINOv2 and trained on GSV-Cities as detailed in Table 4.

that just 4 clusters can achieve robust VPR in most cases, which provides compact global descriptors without the need for additional dimensionality reduction techniques.

**Effect of the cross-image encoder.** Table 7 also presents the comparison of SuperVLAD with and without cross-image encoder (_i.e._, our cluster-wise cross-image interaction). It is evident that using a cross-image encoder consistently leads to performance improvements. However, the performance enhancement on Pitts30k is significantly greater than that on MSLS. This is because Pitts30k can provide a batch of images with different viewpoints from the same place during inference, which allows different images to directly improve the robustness of each other through the cross-image interaction, while MSLS cannot. Considering that conditions like the Pitts30k dataset might not be available in practical application, we consider the cross-image encoder as an optional component for performance enhancement. It is worth noting that, even without it, our method still achieves performance comparable to the SOTA methods.

## 5 Conclusions

In this paper, we introduced SuperVLAD, a compact and robust global descriptor for VPR. By eliminating the need for cluster centers and utilizing a small number of clusters, SuperVLAD achieved improved domain generalization and produced more compact descriptors. We also designed a 1-cluster VLAD descriptor with extremely low dimensionality by introducing supernumerary ghost clusters during the soft assignment. Experimental results on various transformer-based backbones validated the effectiveness of SuperVLAD, producing more robust features than NetVLAD with a lighter aggregation layer. Furthermore, the DINOv2-based SuperVLAD also outperformed SOTA methods on several VPR benchmark datasets with more compact global descriptors.

**Limitations & Future Work.** While our study presents some improvements in VPR, particularly in enhancing domain generalization ability, reducing the dimensionality of global descriptors, and reducing the number of parameters for the aggregation layer, we acknowledge two limitations of our work: **Firstly**, the current success of SuperVLAD relies on the use of the transformer backbone. Although our experimental results provided in Appendix C demonstrate that SuperVLAD still has certain advantages over NetVLAD when using a CNN backbone with a small number of clusters, it is not as good as NetVLAD in more commonly used settings. This can be seen as a limitation of SuperVLAD. **Secondly**, our method uses the cross-image encoder to further enhance performance, similar to CricaVPR . It is necessary to set the inference batch size to an appropriate value. Setting it to 1 directly renders the cross-image encoder ineffective during inference, resulting in a performance drop due to the gap between training and inference. Since we can only use a single-frame query in some practical applications, we compromise to treat the cross-image encoder as an optional module that is not used (during both training and inference) when multiple-frame inference is not feasible. Even without the cross-image encoder, our SuperVLAD can achieve good performance. Some additional discussion on the limitations of SuperVLAD is in Appendix B. In future work, we will try to address the above limitations, and further assess the long-term stability and potential performance changes of the model, which are important for practical deployments.