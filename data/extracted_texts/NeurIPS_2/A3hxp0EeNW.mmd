# Generative Modelling of Structurally Constrained Graphs

Manuel Madeira

EPFL, Lausanne, Switzerland

manuel.madeira@epfl.ch

&Clement Vignac

EPFL, Lausanne, Switzerland &Dorina Thanou

EPFL, Lausanne, Switzerland &Pascal Frossard

EPFL, Lausanne, Switzerland

###### Abstract

Graph diffusion models have emerged as state-of-the-art techniques in graph generation; yet, integrating domain knowledge into these models remains challenging. Domain knowledge is particularly important in real-world scenarios, where invalid generated graphs hinder deployment in practical applications. Unconstrained and conditioned graph diffusion models fail to guarantee such domain-specific structural properties. We present ConStruct, a novel framework that enables graph diffusion models to incorporate hard constraints on specific properties, such as planarity or acyclicity. Our approach ensures that the sampled graphs remain within the domain of graphs that satisfy the specified property throughout the entire trajectory in both the forward and reverse processes. This is achieved by introducing an edge-absorbing noise model and a new projector operator. ConStruct demonstrates versatility across several structural and edge-deletion invariant constraints and achieves state-of-the-art performance for both synthetic benchmarks and attributed real-world datasets. For example, by incorporating planarity constraints in digital pathology graph datasets, the proposed method outperforms existing baselines, improving data validity by up to 71.1 percentage points.

## 1 Introduction

Learning how to generate realistic graphs that faithfully mirror a target distribution is crucial for tasks such as data augmentation in network analysis or discovery of novel network structures. This has become a prominent problem in diverse real-world modelling scenarios, ranging from molecule design  and inverse protein folding  to anti-money laundering  or combinatorial optimization . While the explicit representation of relational and structural information with graphs encourage their widespread adoption in numerous applications, their sparse and unordered nature make the task of graph generation challenging.

In many real-world problems, we possess a priori knowledge about specific properties of the target distribution of graphs. Incorporating such knowledge into generative models is a natural approach to enforce the generated graphs to comply with the domain-specific properties. Indeed, common generative models, even when conditioned towards graph desired properties, fail to offer guarantees. This may however become particularly critical in settings where noncompliant graphs can lead to real-world application failures. Many of these desired properties are edge-related, i.e., constraints in the structure of the graph. For example, in digital pathology, graphs extracted from tissue slides are planar . Similarly, in contact networks between patients and healthcare workers within hospitals, the degrees of healthcare workers are upper bounded to effectively prevent the emergence of superspreaders and mitigate the risk of infectious disease outbreaks . In graph generation,diffusion models have led to state-of-the-art performance [79; 64; 7], in line with their success on other data modalities [75; 29]. However, constrained generation still lags behind its unconstrained counterpart: despite the remarkable expressivity of graph diffusion models, constraining them to leverage specific graph properties remains a particularly challenging task .

In this paper, we propose ConStruct, a constrained graph discrete diffusion framework that induces specific structural properties in generative models. Our focus lies on a broad family of structural properties that hold upon edge deletion, including graph planarity or absence of cycles or triangles, for example. ConStruct operates within graph discrete diffusion, where both node and edge types lie in discrete state-spaces [79; 27; 64; 10]. Notably, ConStruct is designed to preserve both the forward and reverse processes of the diffusion model within distribution with respect to a specified structural property. To accomplish this, we introduce two main components: an edge absorbing noise model and an efficient projector of the target property. The former casts the forward process as an edge deletion process and the reverse process as an edge insertion process. Simultaneously, the projector ensures that the inserted edges in the reverse process, predicted by a trained graph neural network, do not violate the structural property constraints. We theoretically ground the projector design by proving that it can retrieve the optimal graph under a graph edit distance setting. Additionally, we further enhance its efficiency by leveraging incremental constraint satisfaction algorithms, as opposed to their full graph versions, and a blocking edge hash table to avoid duplicate constraint property satisfaction checks. These two components enable a reduction in computational redundancy throughout the reverse process.

We empirically validate the benefit of promoting the match of distributions between the training and generative processes in terms of sample quality and constraint satisfaction on a set of benchmark datasets, outperforming unconstrained methods. We demonstrate the flexibility of ConStruct by testing it with three distinct structural properties constraints: graph planarity, acyclicity and lobster components. To further illustrate the utility of ConStruct to real-world applications, we evaluate the performance of our model in generating biologically meaningful cell-cell interactions, represented through planar cell graphs derived from digital pathology data. We focus on the generation of simple yet medically significant tertiary lymphoid structures [44; 18; 58; 28; 69]. Our experiments demonstrate a significant improvement in cell graph generation with ConStruct compared to unconstrained methods , notably achieving an increase of up to 71.1 percentage points in terms of cell graph va

Figure 1: Constrained graph discrete diffusion framework. The forward process consists of an edge deletion process driven by the edge-absorbing noise model, while the node types may switch according to the marginal noise model. At sampling time, the projector operator ensures that sampled graphs remain within the constrained domain throughout the entire reverse process. In the illustrated example, the constrained domain consists exclusively of graphs with no cycles. We highlight in gray the components responsible for preserving the constraining property.

lidity. These results open new venues for innovative data augmentation techniques and novel instance discovery, addressing a key challenge in digital pathology and real-world applications in general.1

## 2 Related Work

By decomposing the graph generation task into multiple denoising steps, graph diffusion models have gained prominence due to their superior generative performance in the class of methods that predict the full adjacency matrix at once (e.g., VAEs [40; 74; 78; 37], GANs [16; 42; 54], and normalizing flows [48; 53; 47; 51]). Diverse diffusion formulations have emerged to address various challenges in the graph setting, encompassing score-based approaches [60; 38; 85] and discrete diffusion [79; 27; 64]. They have also been employed as intermediate steps in specific generative schemes, such as hierarchical generation through iterative local expansion .

The explicit incorporation of structural information (beyond local biases typical of GNNs) has been shown to be an important prior for enhancing the expressiveness of one-shot graph generative models. For example, in the GAN setting, SPECTRE  conditions on graph spectral properties to capture global structural characteristics and achieve improved generative performance. Graph diffusion models are similarly amenable to conditioning techniques [79; 31], which, despite enabling the guidance of the generation process towards graphs with desired properties, do not guarantee the satisfaction of such properties. In contrast, autoregressive models can ensure constraint satisfaction through validity checks at each iteration, effectively addressing this challenge. Although graph diffusion models can leverage formulations that are invariant to permutations, thus avoiding the sensitivity to node ordering that characterizes autoregressive approaches [87; 46; 14], they still lag behind in ensuring constraint satisfaction.

Previous graph diffusion approaches to address this challenge can be categorized according to the nature of the state spaces they assume. In the continuous case, aligned with successful outcomes in other data modalities , PRODIGY  offers efficient guidance for pre-trained models by relaxing adjacency matrices and categorical node features into continuous spaces, subsequently finding low-overhead projections onto the constraint-satisfying set at each reverse step. This approach can impose structural and molecular properties for which closed-form projections can be derived. However, it does not guarantee constraint satisfaction, facing a trade-off between performance and constraint satisfaction due to mismatched training and sampling distributions. This challenge arises from the continuous relaxation approach, which, while effective within the plug-and-play controllable diffusion framework, imposes an implicit ordering between states that can yield suboptimal graph representations when remapping to the inherently discrete graph space. Additionally, the proposed projection operators cannot be derived for some combinatorial constraints over the graph structure that are frequently encountered in real-world scenarios, such as planarity and acyclicity.

Then, in discrete state-spaces, EDGE  leverages a node-wise maximum degree hard constraint due to its degree guidance but it is limited to this particular property. Similarly, GraphARM , a graph autoregressive diffusion model, allows for constraint incorporation in the autoregressive manner. However, this method requires learning a node ordering, a task that is at least as complex as isomorphism testing. Therefore, to the best of our knowledge, ConStruct consists of the first constrained graph discrete diffusion framework covering a broad class of structural (potentially combinatorial) constraints.

## 3 Constrained Graph Diffusion Models

We now introduce our framework on generative modelling for structurally constrained graphs. We first present the graph diffusion framework and then focus on the new components for constrained graph generation.

### Graph Diffusion Models

We first introduce the mathematical notation adopted in the paper.

NotationWe define a graph as \(G=(X,E)\), where \(X\) and \(E\) denote the sets of attributed nodes and edges, respectively. We consider the node and edge features to be categorical and to lie in the spaces \(\) and \(\) of cardinalities \(b\) and \(c\), respectively. Thus, \(x_{i}\) denotes the node attribute of node \(i\) and \(e_{ij}\) the edge attribute of the edge between nodes \(i\) and \(j\). With \(^{k}=\{=(v_{1},,v_{k}) v_{i}\{0,1\},_{i=1 }^{k}v_{i}=1\}\), their corresponding one-hot encodings are then \(_{i}^{b}\) and \(_{ij}^{c+1}\), since we consider the absence of edge between two nodes as an edge type ("no edge" type). These are stacked in tensors \(\{0,1\}^{n b}\) and \(\{0,1\}^{n n(c+1)}\), respectively. So, equivalently to the set notation, we also have \(G=(,)\). Additionally, we define the probability simplex, \(^{k}=\{(_{0},_{1},,_{k-1})^{k} _{i} 0i,\ _{i=0}^{k-1}_{i}=1\}\).

We then recall the core components of generative models based on graph diffusion, a state-of-the-art framework in several applications . Graph diffusion models are composed of two main processes: a _forward_ and a _reverse_ one. The forward process consists of a Markovian noise model, \(q\), with \(T\) timesteps, that allows to progressively perturb a clean graph \(G\) to its noisy version \(G^{t}\), where \(t\{1,,T\}\). This process is typically modelled independently for nodes and edges. The reverse process consists of the opposite development, starting from a fully noisy, \(G^{T}\), and iteratively refining it until a new clean sample is generated. This process uses a denoising neural network (NN), the only learnable part of the diffusion model. The NN is trained to predict a probability distribution over node and edge types of the clean graph \(G\). After its training, we combine the NN prediction with the posterior term of the forward process to find the distribution \(p_{}(G^{t-1}|G^{t})\), from where we sample a one-step denoised graph. The reverse process results from applying this sampling procedure iteratively until we arrive to a fresh new clean graph \(G^{0}\). Both processes are illustrated in Figure 1.

In some tasks, we are interested in generating instances of a specific class of graphs that conform to well-defined structural properties and align with the training distribution. Importantly, these structural properties do not fully define the underlying distribution; rather, the model must still learn this distribution within the specific class of graphs. This approach becomes particularly crucial in scenarios where we possess domain knowledge but lack sufficient data for an unconstrained model to capture strict dependencies, allowing us to reduce the task's hypothesis space. This need also applies to many real-world applications, where generated graphs become irrelevant if they do not meet certain conditions, as they may be infeasible or lack physical meaning (e.g., in drug design). Despite the remarkable expressivity of graph diffusion models, incorporating such constraints into their generative process remains a largely unsolved problem.

### Constrained Graph Discrete Diffusion Models

We now introduce ConStruct, a framework that efficiently constrains graph diffusion models based on structural properties. Constraining graph generation implies guaranteeing that such target structural properties are not violated in the generated graphs. We build on graph discrete diffusion due to its intrinsic capability to effectively preserve fundamental structural properties (e.g., sparsity) of graphs throughout the generative process .

A successful way of imposing constraints to diffusion models in continuous state-spaces consists of constraining the domain where the forward and reverse processes occur . However,

Figure 2: Projector operator. At each iteration, we start by sampling a candidate graph \(^{t-1}\) from the distribution \(p_{}(G^{t-1}|G^{t})\) provided by the diffusion model. Then, the projector step inserts in an uniformly random manner the candidate edges, discarding those that violate the target property, \(P\), i.e., acyclicity in this illustration. In the end of the reverse step, we find a graph \(G^{t-1}\) that is guaranteed to comply with such property.

constraining domains over graphs, which are inherently discrete, poses a challenging combinatorial problem. Instead, we propose to constrain the graph generative process with specific structural properties. In our approach, we explore the broad class of graph structural properties that hold under edge deletion, namely _edge-deletion invariant_ properties.

**Definition 3.1**.: (Edge-Deletion Invariance) _Let \(P\) be a boolean-valued application defined on graphs, referred to as a property. \(P\) is said to be edge-deletion invariant if, for any graph \(G\) and any subset of edges \( E\), it satisfies:_

\[P(G)= P(G^{})=,\ G^{ }=(X,E).\]

Many properties that are observed in real-world scenarios are edge-deletion invariant. For example, graph planarity is observed in road networks , chip design , biochemistry  or digital pathology . In evolutionary biology  or epidemiology , we find graphs that must not have cycles. Additionally, if we consider the extensions of discrete diffusion to directed graphs (e.g., Asthana et al. ), there are several domains where graph acyclicity is critical: neural architecture search, bayesian network structure learning , or causal discovery . Also, maximum degree constraints are quite common in the design of contact networks [32; 1]. Finally, it is worth noting that Definition 3.1 is extendable to continuous graph-level features through a binary decision (e.g., by thresholding continuous values into boolean values).

Provided that the training graphs satisfy the target structural properties, ConStruct enforces these properties in the generated graphs by relying on two main components: an edge-absorbing noise model and a projector. These two components are described in detail below.

### Edge-deletion Aware Forward Process

Our goal is to design a forward process that yields noisy graphs that necessarily satisfy the target property. This process is typically modelled using transition matrices. Thus, \([_{X}^{t}]_{ij}=q(x^{t}=j|x^{t-1}=i)\) corresponds to the probability of a node transitioning from type \(i\) to type \(j\). Similarly, for edges we have \([_{E}^{t}]_{ij}=q(e^{t}=j|e^{t-1}=i)\). These are applied independently to each node and edge, yielding \(q(G^{t}|G^{t-1})=(^{t-1}_{X}^{t},^{t-1}_{E}^{t})\). Consequently, we can directly jump \(t\) timesteps in the forward step through the categorical distribution given by:

\[q(G^{t}|G)=(}_{X}^{t},}_{E}^ {t}), \]

with \(}_{X}^{t}=_{X}^{1}_{X}^{t}\) and \(}_{E}^{t}=_{E}^{1}_{E}^{t}\). Noising a graph amounts to sampling a graph from this distribution. For the nodes, we use the marginal noise model  due to its great empirical performance. Importantly, to preserve the constraining structural property throughout the forward process, and, consequently, throughout the training algorithm (see Algorithm 1, in Appendix A.1), we propose the utilization of an _edge-absorbing noise model_. This noise model forces each edge to either remain in the same state or to transition to an absorbing state (which we define to be the no-edge state) throughout the forward process. This edge noise model poses the forward as an edge deletion process, converging to a limit distribution that yields graphs without edges. Therefore, we obtain the following transition matrices:

\[_{X}^{t} =^{t}+(1-^{t})_{b}_{X}^{ }\] \[_{E}^{t} =_{}^{t}+(1-_{}^{t} )_{c}_{E}^{}, \]

where \(^{t}\) and \(_{}^{t}\) transition from 1 to 0 with \(t\) according to the popular cosine scheduling  and the mutual-information-based noise schedule (\(^{t}=1-(T+t+1)^{-1}\)) , respectively. The vectors \(_{b}\{1\}^{b}\) and \(_{c}\{1\}^{c+1}\) are filled with ones, and \(_{X}^{}^{b}\) and \(_{E}^{}^{c+1}\) are row vectors filled with the marginal node distribution and the one-hot encoding of the no-edge state, respectively.

### Structurally-Constrained Reverse Process

The reverse process of the diffusion model is fully characterized by the distribution \(p_{}(G^{t-1}|G^{t})\). We detail how to build it from the predictions of a denoising graph neural network, \(_{}\), and the posterior term of the forward process in Appendix A.2. Importantly, the latter imposes the reverse process as an edge insertion process, yet does not necessarily ensure the target structural property. To handle that, we propose an intermediate procedure for each reverse step. Provided a noisy graph at timestep \(t\), we do not accept directly \(^{t-1}\), sampled from \(p_{}(G^{t-1}|G^{t})\), as the one step denoised graph. Instead, we iteratively insert the newly added edges to \(^{t-1}\) in a random order, discarding the ones that lead to the violation of the target property. Therefore, we only have \(G^{t-1}=^{t-1}\) if none of the candidate edges breaks the target property. We refer to the operator that outputs \(G^{t-1}\) provided \(^{t-1}\) and \(G^{t}\) by discarding the violating edges as the _projector_. Its implementation is illustrated in Figure 2 and described in Algorithm 2, in Appendix A.3. Importantly, this procedure merely interferes with the sampling algorithm (refer to Algorithm 3, in Appendix A.3) and ensures that the diffusion model training remains unaffected, fully preserving its efficiency.

Despite its algorithmic simplicity, the design of our projector is theoretically motivated by the result below. We denote the graph edit distance  with uniform cost between two graphs \(G_{1}\) and \(G_{2}\) by \((G_{1},G_{2})\) (see Definition B.1).

**Theorem 1**.: (Simplified) _Let \(^{t-1}=(P,^{t-1},G^{t})\) be the set of all possible one-step denoised graphs outputted by ConStruct. If we define \(G^{*}\) as any optimal solution of:_

\[_{G}(^{t-1},G), \]

_where \(=\{G|P(G)=True,G G^{t}\}\) and \(\) is the set of all unattributed graphs, then \(G^{*}\) can be recovered by our projector, i.e., \(G^{*}^{t-1}\)._

The relationship between the projector, the candidate element \(^{t-1}\) (the instance we aim to project onto a constrained set) and the specified target property \(P\) (defining the constrained set) can be analogized to the conventional projection operator in continuous state spaces. However, while projection in continuous spaces is typically straightforward, this is not the case for discrete state spaces, where, for instance, there often lacks an inherent notion of order between different states. In particular, projecting into an arbitrary subclass of graphs is a complex general combinatorial problem to which there is no efficient solution. For example, finding the maximum planar subgraph of a given graph is NP-hard . Therefore, the novelty of our method is introduced by considering an additional dependency on \(G^{t}\): to make such problem efficiently approachable, we use the previous iterate, \(G^{t}\), which we know by construction that verifies the target property, as a reference. This information is added into the optimization problem through the formulation of the set \(C\). Importantly, this formulation is consistent with the designed noise for the diffusion model, as it complies with the reverse process as an edge insertion process (i.e., \(G^{t} G^{t-1}\)). The complete version of this theorem and extensions for specific constraints can be found in Appendix B.

Importantly, the utilization of the projector breaks the independent sampling of new edges since the insertion of an edge now depends on the order by which we insert them at a given timestep. This sacrifices the tractability of an evidence lower bound for the diffusion model's likelihood. In exchange, it conserves all the sampled graphs throughout the reverse process in the constrained domain. Therefore, the edge-absorbing noise model and the projector jointly ensure that the graph distributions of the training and sampling procedures match, within the predefined constrained graph domain. With these blocks in place, we are now able to both train and sample from the constrained diffusion model.

### Implementation Improvements

We further enhance the efficiency of the sampling algorithm with the two improvements detailed below.

Blocking Edge Hash TableThroughout the reverse process, we keep in memory the edges that have already been rejected in previous timesteps (higher \(t\)). Therefore, once an edge is rejected, it is blocked throughout the rest of the reverse process. This prevents the repetition of redundant constraint satisfaction checks since we know _a priori_ that inserting a previously rejected edge would lead to constraint violation. We store this information in a hash table, where both the lookup and update operations are \(O(1)\), causing minor overhead. Since we only perform the validity check, of complexity \(O(V)\), once for each edge - if it is a candidate edge, we either insert it or block it -, it incurs a \(O(n^{2}V)\) overhead throughout the full reverse process. Note that we lose any dependency on the number of timesteps of the reverse process, which is typically the limiting factor in diffusion models efficiency due to its required high values (\(T 10^{3}\)).

Incremental AlgorithmsOur reverse process consists solely of edge insertion steps, making it well-suited for the application of incremental algorithms. These algorithms efficiently check whether newly added edges preserve the target property by updating and checking smartly designed representations of the graph. This approach contrasts with full graph counterparts, leading to significant efficiency gains by reducing redundant computation. For instance, while the best full planar testing algorithm is \(O(n)\), its fastest known incremental test has amortized running time of \(O((,n))\), where \(\) is the total number of operations (edge queries and insertions), and \(\) denotes the inverse-Ackermann function  (often considered "almost constant" complexity). More details for different properties in Appendix C.

At each reverse step, the denoising network makes predictions for all nodes and pairs of nodes. This results in \(O(n^{2})\) predictions per step. Thus, the complexity of the sampling algorithm of the underlying discrete diffusion model is \(O(n^{2}T)\). In addition, the complexity overhead imposed by the projector is \(O(NV)\). Here, \(V\) represents the complexity of the property satisfaction algorithm and \(N\) is the total number of times this algorithm is applied throughout the reverse process. So, in total, we have \(O(n^{2}T+NV)\). Our analysis in Appendix C shows that incremental property satisfaction algorithms have notably low complexity. For instance, in cases like acyclicity, lobster components, and maximum degree, we have \(V=O(|E_{}|)\). Since the projector adds one edge at a time, we have \(V=O(1)\). Additionally, since the blocking edge hash table limits us to perform at most one property satisfaction check per newly proposed edge (either we have never tested it or it is already blocked), \(N\) corresponds to the total number of different edges proposed by the diffusion model across the whole reverse process. A reasonable assumption is that the model proposes \(N=O(|E|)\) edges throughout the reverse process, with \(|E|\) referring to the number of edges of the clean graph. This is for example true if the model is well trained and predicts the correct graph. Most families of graphs are sparse, meaning that \(O(|E|/n^{2}) 0\) as \(n\). For example, planar and tree graphs can be shown to satisfy \(|E|/n^{2}=O(1/n)\). Thus, we necessarily have \(N n^{2}\). For these reasons, we directly find \(O(NV) O(n^{2}T)\), highlighting the minimal overhead imposed by the projector compared to the discrete diffusion model. This explains the low runtime overhead observed for ConStruct, as detailed in Appendix D.3 (9% for graphs of the tested size). Therefore, we can conclude that asymptotically \(O(n^{2}T+NV)=O(n^{2}T)\), i.e., the projector overhead becomes increasingly negligible relative to the diffusion algorithm itself as the graph size increases, highlighting the scalability of our method.

## 4 Experiments

In this section, we first explore the flexibility of ConStruct to accommodate different constraints in synthetic unattributed graph datasets. Then, we test its applicability to a real-world scenario with digital pathology data.

### Synthetic Graphs

SetupWe focus on three synthetic datasets with different structural properties: the _planar_ dataset , composed of planar and connected graphs; the _tree_ dataset , composed of connected graphs without cycles (tree graph); and the _lobster_ dataset , composed of connected graphs without cycles, where no node is more than 2 hops away from a backbone path (lobster graph). We follow the splits originally proposed for each of the datasets: 80% of the graphs are used in the training set and the remaining 20% are allocated to the test set. We use 20% of the train set as validation set. Statistics of these datasets are shown in Appendix E. As the graphs in these datasets are unattributed, we can specifically isolate ConStruct's capability of incorporating structural information in comparison to previously proposed methods, which are described in Appendix E.2. From here on, we use DiGress+ to denote the DiGress model with the added extra features described in Appendix A.1 and HSpectre to refer to the model proposed by Bergmeister et al. .

Regarding performance metrics, we follow the evaluation procedures from Martinkus et al. . We assess how close the distributions of different graph statistics computed from the generated and test sets are. To accomplish that, we compute the Maximum Mean Discrepancy (MMD)2 for the node degrees (Deg.), clustering coefficients (Clus.), orbit count (Orbit), eigenvalues of the normalized graph Laplacian (Spec.), and statistics from a wavelet graph transform (Wavelet). To summarize this set of metrics, we compute the ratios against the corresponding metrics from the training set and then average them (Ratio). We also compute the proportion of generated graphs that are non-isomorphic to each other (Unique), the proportion that are non-isomorphic to any graph in the training set (Novel),

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

ResultsConStruct outperforms all baselines across all summary evaluation metrics (shown in light gray in Table 2) for cell graph generation on both datasets. Unlike the synthetic datasets, here the structural distribution is conditioned on the node types, which is inherently a more complex task. This complexity contributes to the poor performance of the several unconstrained models. Constraining the edge generation process allows to significantly alleviate this modelling complexity, highlighting the benefits of ConStruct in such scenarios. We emphasize the substantial improvement in the V.U.N. of the generated graphs, with values approaching 100% using our framework, which aligns with the main motivation behind the proposed method. Interestingly, it also promotes the generation of more connected graphs. Finally, the 1-hop baseline model, while capturing the node type dependencies to some extent, as illustrated by the MMD on the components of \(\), completely fails to capture structure-based dependencies.

Additionally, we carry out some experiments for molecular datasets in Appendix G: we explore the utilization of planarity for constrained molecular generation and showcase how ConStruct can be used for controlled generation. Finally, we explore likelihood-based variants of ConStruct, as well as some ablations to the projector in Appendix H.

## 5 Limitations and Future Directions

In our work, we cover edge-deletion invariant properties. However, ConStruct can be easily extended to also handle edge-insertion invariant properties (i.e., properties that hold upon edge insertion). This extension can be useful in domains where constraints such as having at least \(n\) cycles in a graph are important. To achieve this, we can simply "invert" the proposed framework: design the transition matrices with the absorbing state in an existing edge state (instead of the no-edge state) and a projector that removes edges progressively (instead of inserting them) while conserving the desired property.

In the particular context of molecular generation, Appendix G illustrates that, while purely structural constraints can guide the generation of molecules with specific structural properties (e.g., acyclicity), for general properties shared by all molecules (e.g., planarity) they are too loose. In contrast, autoregressive models thrive in such setting due to the possibility of molecular node ordering (e.g., via canonical SMILES) and the efficient incorporation of _joint node-edge_ constraints (e.g., valency). Therefore, although it consists of a fundamentally different setting than the one considered in this paper, incorporating joint node-edge constraints into ConStruct represents an exciting future direction.

Additionally, the induced sparsity created by the edge-absorbing noise model presents opportunities for further exploitation. By leveraging this sparsity, future extensions of ConStruct could enhance sampling efficiency and improve the underlying diffusion model's scalability for generating larger graphs.

## 6 Conclusion

In this paper, we introduced ConStruct, a framework that allows to integrate domain knowledge via structural constraints into graph diffusion models. By constraining the diffusion process based on a diverse set of geometric properties, we enable the generation of realistic graphs in scenarios with limited data. To accomplish that, we leverage an edge-absorbing noise model and a projector operator to ensure that both the forward and reverse processes preserve the sampled graphs within the constrained domain and, thus, maintain their validity. Despite its algorithmic simplicity, our approach overcomes the arbitrarily hard problem of projecting a given graph into a combinatorial subspace in an efficient and theoretically grounded manner. Through several experiments on benchmark datasets, we showcase the versatility of ConStruct across various structural constraints. For example, in digital pathology datasets, our method outperforms existing approaches, bringing the validity of the generated graphs close to 100%. Overall, ConStruct opens new avenues for integrating domain-specific knowledge into graph generative models, thereby paving the way for their application in real-world scenarios.