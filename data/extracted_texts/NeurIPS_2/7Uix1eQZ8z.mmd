# A State Representation for Diminishing Rewards

Ted Moskovitz

Gatsby Unit, UCL

ted@gatsby.ucl.ac.uk &Samo Hromadka

Gatsby Unit, UCL

samo.hromadka.21@ucl.ac.uk &Ahmed Touati

Meta

atouati@meta.com &Diana Borsa

Google DeepMind

borsa@google.com &Maneesh Sahani

Gatsby Unit, UCL

maneesh@gatsby.ucl.ac.uk

A common setting in multitask reinforcement learning (RL) demands that an agent rapidly adapt to various stationary reward functions randomly sampled from a fixed distribution. In such situations, the successor representation (SR) is a popular framework which supports rapid policy evaluation by decoupling a policy's expected discounted, cumulative state occupancies from a specific reward function. However, in the natural world, sequential tasks are rarely independent, and instead reflect shifting priorities based on the availability and subjective perception of rewarding stimuli. Reflecting this disjunction, in this paper we study the phenomenon of diminishing marginal utility and introduce a novel state representation, the \(\) representation (\(\)R) which, surprisingly, is required for policy evaluation in this setting and which generalizes the SR as well as several other state representations from the literature. We establish the \(\)R's formal properties and examine its normative advantages in the context of machine learning, as well as its usefulness for studying natural behaviors, particularly foraging.

## 1 Introduction

The second ice cream cone rarely tastes as good as the first, and once all the most accessible brambles have been picked, the same investment of effort yields less fruit. In everyday life, the availability and our enjoyment of stimuli is sensitive to our past interactions with them. Thus, to evaluate different courses of action and act accordingly, we might expect our brains to form representations sensitive to the non-stationarity of rewards. Evidence in fields from behavioral economics  to neuroscience  supports this hypothesis. Surprisingly, however, most of reinforcement learning (RL) takes place under the assumptions of the Markov Decision Process [MDP; \(4\)], where rewards and optimal decision-making remain stationary.

In this paper, we seek to bridge this gap by studying the phenomenon of _diminishing marginal utility_ in the context of RL. Diminishing marginal utility (DMU) is the subjective phenomenon by which repeated exposure to a rewarding stimulus reduces the perceived utility one experiences. While DMU is thought to have its roots in the maintenance of homeostatic equilibrium (too much ice cream can result in a stomach ache), it also manifests itself in domains in which the collected rewards are abstract, such as economics ($10 vs. $0 is perceived as a bigger difference in value than $1,010 vs. $1,000), where it is closely related to risk aversion . While DMU is well-studied in other fields, relatively few RL studies have explored diminishing reward functions , and, to our

Figure 1.1: Diminishing rewards.

knowledge, none contain a formal analysis of DMU within RL. Here, we seek to characterize both its importance and the challenge it poses for current RL approaches (Section 3).

Surprisingly, we find that evaluating policies under diminishing rewards requires agents to learn a novel state representation which we term the \(\)_representation_ (\(\), Section 4). The \(\) generalizes several state representations from the RL literature: the _successor representation_[SR; 10], the _first-occupancy representation_[FR; 11], and the _forward-backward representation_[FBR; 12], adapting them for non-stationary environments. Interestingly, despite the non-stationarity of the underlying reward functions, we show that the \(\) still admits a Bellman recursion, allowing for efficient computation via dynamic programming (or approximate dynamic programming) and prove its convergence. We demonstrate the scalability of the \(\) to large and continuous state spaces (Section 4.1), show that it supports policy evaluation, improvement, and composition (Section 5), and show that the behavior it induces is consistent with optimal foraging theory (Section 6).

## 2 Preliminaries

Standard RLIn standard RL, the goal of the agent is to act within its environment so as to maximize its discounted cumulative reward for some task \(T\). Typically, \(T\) is modeled as a discounted MDP, \(T=(,,p,r,,_{0})\), where \(\) is the state space, \(\) is the set of available actions, \(p:()\) is the transition kernel (where \(()\) is the space of probability distributions over \(\)), \(r:\) is the reward function, \([0,1)\) is a discount factor, and \(_{0}()\) is the distribution over initial states. Note that the reward function is also frequently defined over state-action pairs \((s,a)\) or triples \((s,a,s^{})\), but to simplify notation we mostly consider state-based rewards. All of the following analysis, unless otherwise noted, can easily be extended to the \((s,a)\)- or \((s,a,s^{})\)-dependent cases. The goal of the agent is to maximize its expected _return_, or discounted cumulative reward \(_{t}^{t}r(s_{t})\). The role of the discount factor is twofold: from a theoretical standpoint, it ensures that this sum is finite for bounded rewards, and it induces myopia in the agent's behavior. To simplify notation, we will frequently write \(r(s_{t}) r_{t}\) and \(^{||}\) as the vector of rewards for each state. The agent acts according to a stationary policy \(:()\). For finite MDPs, we can describe the expected transition probabilities under \(\) using a \(||||\) matrix \(P^{}\) such that \(P^{}_{s,s^{}}=p^{}(s^{}|s)_{a}p(s^{}|s,a )(a|s)\). Given \(\) and a reward function \(r\), the expected return associated with taking action \(a\) in state \(s\) is

\[Q^{}_{r}(s,a)=_{}[_{k=0}^{}^{k}r_{t+k} s_{t}=s,a_{t}=a]=_{s^{} p^{}(|s)} [r_{t}+ Q^{}_{r}(s^{},(s^{}))]. \]

The \(Q^{}_{r}\) are called the state-action values or simply the \(Q\)-values of \(\). The expectation \(_{}[]\) is taken with respect to the randomness of both the policy and the transition dynamics. For simplicity of notation, from here onwards we will write expectations of the form \(_{}[|s_{t}=s,a_{t}=a]\) as \(_{}[|s_{t},a_{t}]\). The recursive form in Eq. (2.1) is called the _Bellman equation_, and it makes the process of estimating \(Q^{}_{r}\)--termed _policy evaluation_--tractable via dynamic programming. In particular, successive applications of the _Bellman operator_\(^{}Q r+ P^{}Q\) are guaranteed to converge to the true value function \(Q^{}\) for any initial real-valued \(||||\) matrix \(Q\). Once a policy has been evaluated, _policy improvement_ identifies a new policy \(^{}\) such that \(Q^{}_{r}(s,a) Q^{^{}}_{r}(s,a),\ (s,a) Q^{}_{r}(s,a)\). Helpfully, such a policy can be defined as \(^{}(s)*{argmax}_{a}Q^{}_{r}(s,a)\).

Value DecompositionOften, it can be useful for an agent to evaluate the policies it's learned on new reward functions. In order to do so efficiently, it can make use of the _successor representation_[SR; 10], which decomposes the value function as follows:

\[V^{}(s)=_{}[_{k 0}^{k}r_{t+k}s_{t} ]=^{}_{}[_{k 0 }^{k}(s_{t+k})s_{t}]}_{ M^{}(s)}, \]

where \((s_{t})\) is a one-hot representation of state \(s_{t}\) and \(M^{}\), the SR, is an \(||||\) matrix such that

\[M^{}(s,s^{})_{}[_{k 0}^{k} (s_{t+k}=s^{})s_{t}]=_{}[ (s_{t}=s^{})+ M^{}(s_{t+1},s^{})s_{t} ]. \]Because the SR satisfies a Bellman recursion, it can be learned in a similar manner to the value function, with the added benefit that if the transition kernel \(p\) is fixed, it can be used to instantly evaluate a policy on a task determined by any reward vector \(\). The SR can also be conditioned on actions, \(M^{}(s,a,s^{})\), in which case multiplication by \(\) produces the \(Q\)-values of \(\). The SR was originally motivated by the idea that a representation of state in the brain should be dependent on the similarity of future paths through the environment, and there is evidence that SR-like representations are present in the hippocampus . A representation closely related to the SR is the _first-occupancy representation_[FR; 11], which modifies the SR by only counting the first visit to each state:

\[F^{}(s,s^{})_{}[_{k 0}^{k} (s_{t+k}=s^{},s^{}\{s_{t},,s_{t+k-1}\}) |s_{t}]. \]

The FR can be used in the same way as the SR, with the difference that the values it computes are predicated on ephemeral rewards--those that are consumed or vanish after the first visit to a state.

Policy CompositionIf the agent has access to a set of policies \(=\{\}\) and their associated SRs (or FRs) and is faced by a new task determined by reward vector \(\), it can instantly evaluate these policies by simply multiplying: \(\{Q^{}(s,a)=^{}M^{}(s,a,)\}\). This process is termed _generalized policy evaluation_[GPE; 15]. Similarly, _generalized policy improvement_ (GPI) is defined as the identification of a new policy \(^{}\) such that \(Q^{^{}}(s,a)_{}Q^{}(s,a)\  s,a \). Combining both provides a way for an agent to efficiently _compose_ its policies \(\)--that is, to combine them to produce a new policy without additional learning. This process, which we refer to as GPE+GPI, produces the following policy, which is guaranteed to perform at least as well as any policy in \(\):

\[^{}(s)*{argmax}_{a}_{} ^{}M^{}(s,a,). \]

## 3 Diminishing Marginal Utility

Problem StatementMotivated by DMU's importance in decision-making, our goal is to understand RL in the context of the following class of non-Markov reward functions:

\[r_{}(s,t)=(s)^{n(s,t)}(s),(s), \]

where \(n(s,t)\) is the agent's visit count at \(s\) up to time \(t\) and \((s)\) describes the reward at the first visit to \(s\). \((s)\) therefore encodes the extent to which reward diminishes after each visit to \(s\). Note that for \((s)==1\) we recover the usual stationary reward given by \(\), and so this family of rewards strictly generalizes the stationary Markovian rewards typically used in RL.

DMU is ChallengingAn immediate question when considering reward functions of this form is whether or not we can still define a Bellman equation over the resulting value function. If this is the case, standard RL approaches still apply. However, the following result shows otherwise.

**Lemma 3.1** (Impossibility; Informal).: _Given a reward function of the form Eq. (3.1), it is impossible to define a Bellman equation solely using the resulting value function and immediate reward._

We provide a more precise statement, along with proofs for all theoretical results, in Appendix B. This result means that we can't write the value function corresponding to rewards of the form Eq. (3.1) recursively only in terms of rewards and value in an analogous manner to Eq. (2.1). Nonetheless, we found that it _is_ in fact possible to derive \(a\) recursive relationship in this setting, but only by positing a novel state representation that generalizes the SR and the FR, which we term the \(\)_representation_ (\(\)R). In the following sections, we define the \(\)R, establish its formal properties, and demonstrate its necessity for RL problems with diminishing rewards.

## 4 The \(\) Representation

A Representation for DMUWe now the derive the \(\)R by decomposing the value function for rewards of the form Eq. (3.1) and show that it admits a Bellman recursion. To simplify notation, we use a single \(\) for all states, but the results below readily apply to non-uniform \(\). We have

\[V^{}(s)=[_{k=0}^{}^{k}r_{}(s_{t+k},k) s_{t}=s]=}^{}[_{ k=0}^{}^{k}^{n_{t}(s_{t+k},k)}(s_{t+k})s_{t}=s ]}_{_{}^{}(s)}, \]

where we call \(_{}^{}(s)\) the \(\)_representation_ (\(\)), and \(n_{t}(s,k)_{j=0}^{k-1}(s_{t+j}=s)\), is the number of times state \(s\) is visited from time \(t\) up to--but not including--time \(t+k\). Formally:

**Definition 4.1** (\(\)).: _For an MDP with finite \(\) and \(^{||}\), the \(\) representation is given by \(_{}^{}\) such that_

\[_{}^{}(s,s^{})[_{k=0}^{ }(s^{})^{n_{t}(s^{},k)}^{k}(s_{t+k}=s ^{})s_{t}=s] \]

_where \(n_{t}(s,k)_{j=0}^{k-1}(s_{t+j}=s)\) is the number of times state \(s\) is visited from time \(t\) until time \(t+k-1\)._

We can immediately see that for \(=0\), the \(\) recovers the FR (we take \(0^{0}=1\)), and for \(=1\), it recovers the SR. For \((0,1)\), the \(\) interpolates between the two, with higher values of \(\) reflecting greater persistence of reward in a given state or state-action pair and lower values of \(\) reflecting more ephemeral rewards.

The \(\) admits a recursive relationship:

\[_{}^{}(s,s^{}) =[_{k=0}^{}^{n_{t}(s^{},k)} ^{k}(s_{t+k}=s^{})s_{t}=s]\] \[}{{=}}[(s_{t }=s^{})+^{n_{t}(s^{},1)}_{k=1}^{}^{n _{t+1}(s^{},k)}^{k-1}(s_{t+k}=s^{})s_{t}=s]\

### Continuous State Spaces

When the state space is large or continuous, it becomes impossible to use a tabular representation, and we must turn to function approximation. There are several ways to approach this, each with their own advantages and drawbacks.

Feature-Based RepresentationsFor the SR and the FR, compatibility with function approximation is most commonly achieved by simply replacing the indicator functions in their definitions with a _base feature function_\(:^{D}\) to create _successor features_[SFs; 15; 16] and _first-occupancy features_[FFs; 11], respectively. The intuition in this case is that \(\) for the SR and the FR is just a one-hot encoding of the state (or state-action pair), and so for cases when \(||\) is too large, we can replace it with a compressed representation. That is, we have the following definition

**Definition 4.3** (SFs).: _Let \(:^{D}\) be a base feature function. Then, the successor feature (SF) representation \(_{1}:^{D}\) is defined as \(_{1}^{}(s,a)_{}[_{k=0}^{} ^{k}(s_{t+k})s_{t},a_{t}]\) for all \(s,a\)._

One key fact to note, however, is that due to this compression, all notion of state "occupancy" is lost and these representations instead measure feature accumulation. For any feasible \(\) then, it is most natural to define these representations using their recursive forms:

**Definition 4.4** (\(\)F).: _For \(\) and bounded base features \(:^{D}\), the \(\)-feature (\(\)F) representation of state \(s\) is given by \(_{}^{}\) such that_

\[_{}^{}(s)(s)(1+_{s^ {} p^{}(|s)}_{}^{}(s^{}))+(1- (s))_{s^{} p^{}(|s)}_{}^{ }(s^{}). \]

In order to maintain their usefulness for policy evaluation, the main requirement of the base features is that the reward should lie in their span. That is, a given feature function \(\) is most useful for an associated set of reward functions \(\), given by

\[=\{r^{D}r(s,a)=^{}(s,a)\; s,a \}. \]

However, Barreto et al.  demonstrate that good performance can still be achieved for an arbitrary reward function as long as it's sufficiently close to some \(r\).

Set-Theoretic FormulationsAs noted above, computing expectations of accumulated abstract features is unsatisfying because it requires that we lose the occupancy-based interpretation of these representations. It also restricts the agent to reward functions which lie within \(\). An alternative approach to extending the SR to continuous MDPs that avoids this issue is the _successor measure_[SM; 19], which treats the distribution of future states as a measure over \(\):

\[M^{}(s,a,X)_{k=0}^{}^{k}(s_{t+k} X  s_{t}=s,a_{t}=a,) X, \]

which can be expressed in the discrete case as \(M^{}=I+ P^{}M^{}\). In the continuous case, matrices are replaced by their corresponding measures. Note that SFs can be recovered by integrating: \(_{1}^{}(s,a)=_{s^{}}M^{}(s,a,ds^{})(s^{})\). We can define an analogous object for the \(\) as follows

\[_{}^{}(s,X)_{k=0}^{}^{n_{t}(X,k)} ^{k}(s_{t+k} X s_{t}=s,) \]

Figure 4.1: **The \(\) interpolates between the FR and the SR.** We visualize the \(\)s of the bottom left state for the depicted policy for \(\{0.0,0.5,1.0\}\).

where \(n_{t}(X,k)_{j=0}^{k-1}_{s_{t+j}}(X)\). However, this is not a measure because it fails to satisfy additivity for \(<1\), i.e., for measurable disjoint sets \(A,B\), \(_{}^{}(s,A B)<_{}^{}(s,A)+_{}^{} (s,B)\) (Lemma B.4). For this reason, we call Eq. (4.8) the \(\)_operator_ (\(\)O). We then minimize the following squared Bellman error loss for \(_{}^{}\) (dropping sub/superscripts for concision):

\[()=_{s_{t},s_{t+1},s^{} }[((s_{t},s^{})-(s_{t+1},s^{ }))^{2}]-2_{s_{t}}[(s_{t},s_{t})] \] \[+2(1 -)_{s_{t},s_{t+1}}[(s_{t})(s_{t},s_{t}) (s_{t+1},s_{t})],\]

where \(\) and \(\) are densities over \(\) and \(_{}^{}(s)_{}^{}(s)()\) in the discrete case, with \(_{}^{}\) parametrized by neural network. \(\) indicates a stop-gradient operation, i.e., a target network. A detailed derivation and discussion are given in Appendix H. While \(\) can be any training distribution of transitions we can sample from, we require an analytic expression for \(\). Eq. (4.9) recovers the SM loss of Touati and Ollivier  when \(=1\).

## 5 Policy Evaluation, Learning, and Composition under DMU

In the following sections, we experimentally validate the formal properties of the \(\) and explore its usefulness for solving RL problems with DMU. The majority of our experiments center on navigation tasks, as we believe this is the most natural setting for studying behavior under diminishing rewards. However, in Appendix I we also explore potential for the \(\)'s use other areas, such as continuous control, even when rewards do not diminish. There is also the inherent question of whether the agent has access to \(\). In a naturalistic context, \(\) can be seen as an internal variable that the agent likely knows, especially if the agent has experienced the related stimulus before. Therefore, in subsequent experiments, treating \(\) as a "given" can be taken to imply the agent has prior experience with the relevant stimulus. Further details for all experiments can be found in Appendix E.

### Policy Evaluation

In Section 4, we showed that in order to perform policy evaluation under DMU, an agent is required to learn the \(\). In our first experimental setting, we verify this analysis empirically for the policy depicted in Fig. 4.1 with a rewarded location in the state indicated by a \(g\) in Fig. 5.1 with \(_{true}=0.5\). We then compare the performance for agents using different values of \(\) across dynamic programming (DP), tabular TD learning, and \(\) TD learning with Laplacian features. For the latter two, we use a linear function approximator with a one-hot encoding of the state as the base feature function. We then compute the \(Q\)-values using the \(\) with \(\{0.5,1.0\}\) (with \(=1\) corresponding to the SR) and compare the resulting value estimates to the true \(Q\)-values. Consistent with our theoretical analysis, Fig. 5.1 shows that the \(\) with \(=_{true}\) is required to produce accurate value estimates.

### Policy Learning

To demonstrate that \(\) is useful in supporting policy improvement under diminishing rewards, we implemented modified forms of \(Q\)-learning  (which we term \(Q_{}\)-learning) and advantage

Figure 5.1: **The \(\) is required for accurate policy evaluation.** Policy evaluation of the policy depicted in Fig. 4.1 using dynamic programming, tabular TD learning, and \(\) TD learning produces the most accurate value estimates when using the \(\) with \(=_{true}\). Results are averaged over three random seeds. Shading indicates standard error.

actor-critic  and applied them to the TwoRooms domain from the NeuroNav benchmark task set  (see Fig. 5.2). For a transition \((s_{t},a_{t},s_{t+1})\), we define the following operator:

\[_{}^{}(s_{t})(1+ (s_{t+1},a_{t+1}))+(1-(s_{t}))(s_{t+1},a_{ t+1}), \]

where \(a_{t+1}=*{argmax}_{a}Q_{}(s_{t},a)=*{argmax}_ {a}^{}(s_{t},a)\). This is an improvement operator for \(Q_{}\). The results in Fig. 5.2 show that \(Q_{}\)-learning outperforms standard \(Q\)-learning (\(=1\)) for diminishing rewards, and that the "correct" \(\) produces the best performance. To implement A2C with a \(\) critic, we modified the standard TD target in a similar manner as follows:

\[_{}V(s_{t})=r(s_{t})+(V(s_{t+1})+(-1)^{}((s_{t})_{}(s_{t+1}))), \]

where \(\) were one-hot state encodings, and the policy, value function, and \(\) were output heads of a shared LSTM network . Note this target is equivalent to Definition 4.4 multiplied by the reward (derivation in Appendix E). Fig. 5.2 shows again that correct value targets lead to improved performance. Videos of agent behavior can be found at lambdarepresentation.github.io.

### Policy Composition

As we've seen, DMU problems of this form have an interesting property wherein solving one task requires the computation of a representation which on its own is task agnostic. In the same way that the SR and FR facilitate generalization across reward functions, the \(\) facilitates generalization across reward functions with different \(\)s.The following result shows that there is a benefit to having the "correct" \(\) for a given resource.

**Theorem 5.1** (Gpi).: _Let \(\{M_{j}\}_{j=1}^{n}\) and \(M\) be a set of tasks in an environment \(\) and let \(Q^{_{j}^{*}}\) denote the action-value function of an optimal policy of \(M_{j}\) when executed in \(M\). Assume that the agent uses diminishing rate \(\) that may differ from the true environment diminishing rate \(\). Given estimates \(^{_{j}}\) such that \(\|Q^{_{j}^{*}}-^{_{j}}\|_{}\) for all \(j\), define_

\[(s)*{argmax}_{a}_{j}^{_{j}}(s,a).\]

_Then,_

\[Q^{}(s,a)_{j}Q^{_{j}^{*}}(s,a)- (2+|-|\|r\|_{}+).\]

Note that for \(=1\), we recover the original GPI bound due to Barreto et al.  with an additional term quantifying error accrued if incorrectly assuming \(<1\).

Tabular NavigationWe can see this result reflected empirically in Fig. 5.3, where we consider the following experimental set-up in the classic FourRooms domain . The agent is assumed to be given or have previously acquired four policies \(\{_{0},_{1},_{2},_{3}\}\) individually optimized to reach rewards located in each of the four rooms of the environment. There are three reward locations \(\{g_{0},g_{1},g_{2}\}\) scattered across the rooms, each with its own initial reward and all with \(=0.5\). At the

Figure 5.2: **The \(\) is required for strong performance. We apply a tabular \(Q\)-learning-style algorithm and deep actor-critic algorithm to policy optimization in the TwoRooms domain. The blue locations indicate reward, and the black triangle shows the agent’s position. Results are averaged over three random seeds. Shading indicates standard error.**

beginning of each episode, an initial state \(s_{0}\) is sampled uniformly from the set of available states. An episode terminates either when the maximum reward remaining in any of the goal states is less than \(0.1\) or when the maximum number of steps \(H=40\) is reached (when \(=1\), the latter is the only applicable condition). For each of the four policies, we learn \(\)Rs with \(\{0.0,0.5,1.0\}\) using dynamic programming and record the returns obtained while performing GPE+GPI with each of these representations over 50 episodes. Bellman error curves for the \(\)Rs are shown in Fig. 5, and demonstrate that convergence is faster for lower \(\). In the left panel of Fig. 5, we can indeed see that using the correct \(\) (0.5) nets the highest returns. Example trajectories for each agent \(\) are shown in the remaining panels.

Pixel-Based NavigationWe verified that the previous result is reflected in larger scales by repeating the experiment in a partially-observed version of FourRooms in which the agent receives \(128 128\) RGB egocentric observations of the environment (Fig. 5, left) with \(H=50\). In this case, the agent learns \(\)Fs for each policy for \(\{0.0,0.5,1.0\}\), where each \(\)F is parameterized by a feedforward convolutional network with the last seven previous frames stacked to account for the partial observability. The base features were Laplacian eigenfunctions normalized to \(\), which which were shown by Touati et al.  to perform the best of all base features for SFs across a range of environments including navigation.

## 6 Understanding Natural Behavior

Naturalistic environments often exhibit diminishing reward and give insight into animal behavior. The problem of foraging in an environment with multiple diminishing food patches (i.e., reward states) is of interest in behavioral science . The cornerstone of foraging theory is the marginal value theorem [MVT; 28, 29], which states that the optimal time to leave a patch is when the patch's reward rate matches the average reward rate of the environment. However, the MVT does not describe which patch to move to once an agent leaves its current patch. We show that the \(\)O recovers MVT-like behavior in discrete environments and improves upon the MVT by not only predicting _when_ agents should leave rewarding patches, but also _where_ they should go. Moreover, we provide a scheme for _learning_\(\) alongside the \(\)O using feedback from the environment.

Figure 5: **Pixel-Based GPI.** Performance is strongest for agents using the correct \(=0.5\). PCA on the learned features in each underlying environment state shows that the \(\)Fs capture the value-conditioned structure of the environment.

Figure 4: **Tabular GPI.** (Left) Average returns obtained by agents performing GPE+GPI using \(\)Rs with \(\{0.0,0.5,1.0\}\) over 50 episodes. Error bars indicate standard error. (Right) Sample trajectories. Agents with \(\) set too high overstay in rewarding states, and those with \(\) too low leave too early.

To learn the \(\)O, we take inspiration from the FBR  and use the following parametrization: \(_{}^{_{*}}(s,a,s^{})=F(s,a,z)^{}B(s^{})(s^{ })\) and \(_{z}(s)=*{argmax}_{a}F(s,a,z)^{}z\), where \(\) is a density with full support on \(\), e.g., uniform. We then optimize using the the loss in Eq. (4.9) under this parameterization (details in Appendix E). Given a reward function \(r:\) at evaluation time, the agent acts according to \(*{argmax}_{a}F(s,a,z_{R})^{}z_{R}\), where \(z_{R}=_{s}[r(s)B(s)]\). Because the environment is non-stationary, \(z_{R}\) has to be re-computed at every time step. To emulate a more realistic foraging task, the agent learns \(\) by minimizing the loss in Eq. (H.1) in parallel with the loss

\[()=_{s_{t},s_{t+1}}[(s_{t} =s_{t+1})(r(s_{t+1})- r(s_{t}))^{2}],\]

which provably recovers the correct value of \(\) provided that \(\) is sufficiently exploratory. In Appendix H.1 we provide experiments showing that using an incorrect value of \(\) leads to poor performance on tabular tasks. In Fig. 6.1 we show that the agent learns the correct value of \(\), increasing its performance. We illustrate the behavior of the \(\)O in an asymmetric environment that has one large reward state on one side and many small reward states (with higher total reward) on the other. Different values of \(\) lead to very different optimal foraging strategies, which the \(\)O recovers and exhibits MVT-like behavior (see Appendix H.2 for a more detailed analysis). Our hope is that the \(\)R may provide a framework for new theoretical studies of foraging behavior and possibly mechanisms for posing new hypotheses. For example, an overly large \(\) may lead to overstaying in depleted patches, a frequently observed phenomenon .

## 7 Conclusion

LimitationsDespite its advantages, there are several drawbacks to the representation which are a direct result of the challenge of the DMU setting. First, the \(\) is only useful for transfer across diminishing reward functions when the value of \(\) at each state is consistent across tasks. In natural settings, this is fairly reasonable, as \(\) can be thought of as encoding the type of the resource available at each state (i.e., each resource has its own associated decay rate). Second, as noted in Section 4.1, the \(\) does not admit a measure-theoretic formulation, which makes it challenging to define a principled, occupancy-based version compatible with continuous state spaces. Third, the \(\) is a prospective representation, and so while it is used to correctly evaluate a policy's future return under DMU, it is not inherently memory-based and so performs this evaluation as if the agent hasn't visited locations with diminishing reward before. Additional mechanisms (i.e., recurrence or frame-stacking) are necessary to account for previous visits. Finally, the \(\) is dependent on an episodic task setting for rewards to reset, as otherwise the agent would eventually consume all reward in the environment. An even more natural reward structure would include a mechanism for reward _replenishment_ in addition to depletion. We describe several such candidates in Appendix J, but leave a more thorough investigation of this issue to future work.

In this work, we aimed to lay the groundwork for understanding policy evaluation, learning, and composition under diminishing rewards. To solve such problems, we introduced--and showed the necessity of--the \(\)_representation_, which generalizes the SR and FR. We demonstrated its usefulness for rapid policy evaluation and by extension, composition, as well as control. We believe the \(\) represents a useful step in the development of state representations for naturalistic environments.

Figure 6.1: \(\)**O trained via FB.****a)**\(\) values of two agents in FourRooms, one which learns \(\) and one which does not. **b)** Performance of the two agents from (**a**). Learning \(\) improves performance. **c)** Reward structure and starting state of the asymmetric environment. **d)** Trajectory of an agent with \(=1\). The optimal strategy is to reach the high reward state and exploit it _ad infinitum_. **e)** Trajectory of an agent with \(=0.1\). The optimal strategy is to exploit each reward state for a few time steps before moving to the next reward state.