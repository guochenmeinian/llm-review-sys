# BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices

Anka Reuel

Stanford University

Amelia Hardy

Stanford University

Chandler Smith

Northeastern University

Max Lamparth

Stanford University

Malcolm Hardy

Stanford University

Mykel J. Kochenderfer

Stanford University

###### Abstract

AI models are increasingly prevalent in high-stakes environments, necessitating thorough assessment of their capabilities and risks. Benchmarks are popular for measuring these attributes and for comparing model performance, tracking progress, and identifying weaknesses in foundation and non-foundation models. They can inform model selection for downstream tasks and influence policy initiatives. However, not all benchmarks are the same: their quality depends on their design and usability. In this paper, we develop an assessment framework considering 46 best practices across an AI benchmark's lifecycle and evaluate 24 AI benchmarks against it. We find that there exist large quality differences and that commonly used benchmarks suffer from significant issues. We further find that most benchmarks do not report statistical significance of their results nor allow for their results to be easily replicated. To support benchmark developers in aligning with best practices, we provide a checklist for minimum quality assurance based on our assessment. We also develop a living repository of benchmark assessments to support benchmark comparability, accessible at betterbench.stanford.edu.

## 1 Introduction

AI systems are rapidly advancing and proliferating . The increasing integration of AI, and in particular foundation models (FMs) , into decision-making systems has significantly amplified its impact and has showcased both benefits [9; 39; 57; 66] and risks [2; 75; 44; 86; 45; 30; 70]. Given the importance of correctly assessing a model's capabilities and potential harms, AI evaluation is an essential discipline . Current evaluation approaches include both internally (e.g., private testing on proprietary data) and externally developed techniques (e.g., scoring on public benchmarks) [74; 27; 73; 48; 32].

Following the work of , we define a benchmark "as a particular combination of a dataset or sets of datasets [...], and a metric, conceptualized as representing one or more specific tasks or sets of abilities, picked up by a community of researchers as a shared framework for the comparison of methods" . Using benchmarks to facilitate comparison, measure performance, track progress, and identify weaknesses has become a standard practice. For example, benchmarks are widely used by model developers to report performance and compare models upon release [3; 8], and as part of policy initiatives to support third-party model evaluations, such as as part of the UK AI Safety Institute's_Inspect_ framework for evaluating large language models (LLMs)  or Article 51 of the EU AI Act . However, the fidelity of this approach depends entirely on the benchmarks' quality, where we define a _high-quality_ benchmark as one that is interpretable, clear about its intended purpose and scope, and that is usable. To date, no structured assessment for the quality of AI benchmarks, including both FM and non-FM benchmarks, has been published, and no comparative analysis has been conducted to understand quality differences between widely used AI benchmarks. To address these gaps, our paper:

* [noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
* Presents a novel AI benchmark assessment framework evaluating the quality of AI benchmarks based on 46 criteria derived from expert interviews and domain literature
* Scores 16 foundation model (FM) and 8 non-FM benchmarks (full list in App. D), finding quality differences across both categories
* Provides insights into prevalent issues in current AI benchmarking practices based on our assessment
* Creates a checklist for minimum quality assurance to support benchmark developers in aligning with best practices
* Makes available a living repository2 of benchmark assessments for users to analyze benchmarks' quality and appropriateness for their usage contexts. 
We structure the paper as follows: Sec. 2 explores benchmarking in AI and other fields. Sec. 3 describes our assessment development, which combined literature and expert interviews, and details our benchmark scoring procedure. Sec. 4 presents our framework's criteria, focusing on aspects under developers' control to promote better benchmarks. Sec. 5 lists additional context-dependent design considerations. Sec. 6 reports findings from applying our framework to 24 benchmarks. Finally, Sec. 7 and Sec. 8 explore implications for future evaluations and discuss our work's scope and limitations. We further outline open challenges with AI benchmarking in App. A, involved stakeholders in App. B, and the AI benchmark lifecycle in App. C.

## 2 Related Work

### AI Benchmarking Practices and Challenges

Our literature review of AI benchmarking practices identifies two primary concerns: what a benchmark measures and how this measurement is used. Regarding what a benchmark measures,  find that current benchmarks for LLMs are insufficient for assessing these models capabilities. A frequent concern in this context is the validity of evaluations . Similarly,  finds

Figure 1: Five stages of the benchmark lifecycle. A detailed description can be found in App. C.

that the rapid advancement of AI models threatens benchmarks' utility, as a large fraction of these evaluations are near saturation.  and  both address the narrow scope of existing benchmarks, with  advocating for approaches intended to reduce the socio-technical gap that exists between the capabilities that benchmarks are able to measure and the ability of models to meet user needs in downstream applications. With respect to how evaluations are used,  critiques the tendency of AI practitioners to overgeneralize benchmark results, highlighting how these scores present an inherently reductive view of model performance.

In addition, the community has also recognized the importance of data curation and documentation in the context of evaluations.  put forth the idea of data cards as standardized documentation framework for datasets and  develop a framework and checklist for best practices in data curation. Finally, the FAIR principles  outline best practices for digital data access, based on the principles of _Findability_, _Accessibility_, _Interoperability_, and _Reuse_. While these efforts support the adoption of best practices in the context of data, they are insufficient for assessing AI benchmarks, which extend data with infrastructure and evaluation methods, requiring additional guidelines to support the development of high-quality benchmarks and the decision-making of benchmark users.

Hence, our work builds on and expands these guidelines, with the aim of advancing the analysis of AI benchmarking by presenting a first-of-its-kind framework for the assessment of both foundation model and non-foundation model benchmarks. Unlike prior studies, such as  and , which focus on identifying limitations in limited contexts and scopes, our approach offers practical tools, empowering developers to address shortcomings and directly enhance benchmark quality: Our assessment spans a wider range of criteria across the benchmark lifecycle, from design (e.g., have domain experts been involved in the development?) to implementation (e.g., is the evaluation script available?), documentation (e.g., is the applicable license specified?), and maintenance (e.g., is a feedback channel available for users?). We give an overview of all our criteria in Sec. 4 and explain, justify, and provide scoring details for each criterion in App. K. We further provide a checklist of best practices derived from our analysis (App. J), offering guidance for improving AI benchmarks, rather than merely highlighting issues.

### Benchmarking Best Practices in Other Fields

Our work is informed by benchmarking practices from fields beyond AI, ranging from transistor hardware  to environmental quality  to bioinformatics , and we identify common themes regarding what constitutes an effective benchmark. Where applicable, we incorporate these best practices into our assessment (Sec. 4):

**Designing for downstream utility.** Many of the papers reviewed discuss the importance of a benchmark's tasks being designed with real world applications in mind.  considers the best benchmarks to be situation-specific,  defines an ideal test set as one which reflects real world data,  proposes that benchmarks should be adapted to their intended applications, and  suggests that benchmarks be designed to fit the diversity of downstream use cases.  emphasizes the importance of guaranteeing that tested methods only use information available in a practical setting and recommends checking that a benchmark simulates the envisioned usage.

**Ensuring validity.** A frequent concern with benchmarking is the validity of evaluations . In educational testing,  outline a framework to ensure validity by providing guidelines for effective evidence collection.  outline what and how evidence can be collected and how it should be interpreted for tests "of attributes for which there is no adequate criterion" . Measures that are used in other fields further include choosing a large test set to promote the statistical significance of results  and updating a benchmark over time to prevent developers from overfitting it .  also notes that the methods or approaches being evaluated should not be used to create the gold standard dataset.

**Prioritizing score interpretability.** highlights that benchmarks are particularly important when a wide variety of tools are available and it is difficult for non-specialists to distinguish between them. Interpretability is important in not only selecting tools, but also deciding between benchmarksthemselves. Effective benchmarks must provide transparent information regarding the procedural details of their experiments  and goals of the evaluation . They should clearly describe the benchmark's purpose and scope, as these are fundamental to its design and implementation . Regarding scope,  states that for environmental quality applications, benchmarks should never be the basis of final decisions. With this in mind, they identify misleading benchmarks as the worst-case scenario. Furthermore, they state that a benchmark should not present its results as absolutes, instead ensuring that its evaluations are understandable inputs for decision makers .

**Guaranteeing accessibility.** A good benchmark is easy to obtain and use . If a benchmark is run computationally, then its data and scripts must be available for results to be reproducible .

## 3 Methodology

Our benchmark assessment consists of 46 criteria based on our literature review and interviews with five primary groups of stakeholders. These groups, who also present the user personas of our assessment, are described in detail in App. B. Through our interview process, we defined a five-stage benchmark lifecycle and identified objectives along it. In this section, we discuss our methodology for identifying stakeholders, developing criteria, and assessing benchmarks. A detailed flow diagram of our methodology can be found in App. H.

**Step 1: Mapping the space.** Initially, we surveyed the existing benchmark landscape (Sec. 2). Based on this review, we identified five stakeholder groups who present the user personas of our assessment (App. B). To understand their objectives with respect to benchmarking, we conducted unstructured interviews with representatives of all stakeholder groups, including 20+ policymakers, model developers, benchmark developers, model users, and AI researchers. During this process, we developed a five-stage model of the benchmark lifecycle (Fig. 5 and App. C) and mapped both the benchmarking objectives of the stakeholders and their communicated use cases for a benchmark assessment (App. B).

**Step 2: Translation to criteria.** Based on Step 1, we identified tasks and objectives for each stage of the AI benchmark lifecycle and translated them into concrete criteria. We categorized these as: (a) criteria controlled by the benchmark developer where the authors and interviewees reached a normative consensus, (b) criteria controlled by the benchmark developer but context-dependent, difficult for an external party to assess, or both and (c) aspects either outside the benchmark developer's control or requiring further research. The assessment in Sec. 4 is limited to category (a) criteria. We cover considerations in (b) in Sec. 5, and those in (c) in App. A.

**Step 3: Validating the assessment.** Initially, three authors independently scored the same benchmark to calibrate the assessment and identify potential misinterpretations of the criteria. We adapted and clarified scoring guidelines (App. K) to address differing interpretations and uncertainties. To validate our assessment, we shared it with members of all stakeholder groups and revised it based on their feedback. Finally, we verified that our assessment, which in itself can be considered a benchmark, met all of our defined criteria, where applicable (App. J.2).

**Step 4: Structuring the assessment.** We evaluated 16 FM and 8 non-FM benchmarks. We prioritized commonly used benchmarks, such as those that were recently reported by model developers  and aim to expand the number of assessed benchmarks continuously on our website _betterbench.stanford.edu_. Since our assessment considers varying information sources (official websites, papers, GitHub repositories published by the benchmark developers3) that do not follow a standard structure, we manually evaluated all benchmarks. At least two authors independently reviewed each benchmark. They subsequently had to reach a consensus on the final score and a third reviewer could be called to make the final decision if a consensus could not be reached (this case did not occur).

**Step 5: Scoring.** We scored benchmarks on a discrete 0/5/10/15-point scale for each criterion: 15 for fully meeting, 10 for partially meeting, 5 for mentioning without fulfilling, and 0 for neither referencing nor satisfying the criterion. Average scores were calculated for each benchmark lifecycle stage (design, implementation, documentation, and maintenance). An aggregate usability score, representing the weighted average of the implementation, documentation, and maintenance scores, was also introduced (see App. G for scoring details). We consider a mean score of 10 or higher to indicate a reasonably good benchmark for each aggregated scoring category, as it signifies that, on average, the benchmark at least partially fulfills all assessment criteria within the respective category.

**Step 6: Platform for continuous updates.** Finally, we develop a supplementary website4 to continuously publish assessment results using the scoring methodology in App. G, given the rapid development of new AI benchmarks. The website includes a community feedback channel for submitting new AI benchmarks and correcting previously posted scores if benchmarks are updated or stakeholders disagree with our evaluation. This provides benchmark users with an accessible, up-to-date database of existing benchmarks and their quality, enabling quick analysis of the most suitable benchmark for their application context.

## 4 Assessment Criteria

We separate our assessment criteria according to the phase of the benchmark lifecycle during which they would be fulfilled. Although the retirement stage is within the developer's control, we do not include specific criteria for this phase within the current framework, because we cannot assess the retirement of active benchmarks. App. K contains full explanations, justifications, and scoring guidelines for each of the 46 criteria.

### Benchmark Design

Benchmarks should clearly describe their goals and scope . This includes defining the tested capability or characteristic, describing how the tested capability translates to the benchmark task, and stating how knowing about the tested concept is helpful in real-world applications . These design choices should be informed by considering use cases and user personas for the benchmark, involving domain experts, and integrating domain literature . Clearly stating how the benchmark is different from related existing AI benchmarks is necessary to help benchmark users decide the applicability of a benchmark to their use case. A benchmark's measurements must be interpretable , which requires an informed choice of performance metric(s) and a description of how the benchmark score should or shouldn't be interpreted . Including floors, ceilings, human performance levels, and random performance levels for the chosen metric(s) further assists users in understanding a model's score . If addressing input sensitivity and providing a validated automatic evaluation are possible, these measures enhance a benchmark's robustness and accessibility .

Figure 2: Overview of assessment criteria for the benchmark design stage.

Criteria in the implementation stage focus on the availability of necessary code and infrastructure and the inclusion of key engineering features. To ensure reproducibility and scrutiny , a benchmark should provide working evaluation code, and make its evaluation data, prompts, or dynamic test environment accessible. A script should be available to replicate initial published results. In domains where models are often accessed via API, such as NLP, an ideal benchmark supports the evaluation of both API-based and local models. A benchmark can minimize the risks of contamination and gamification by including a globally unique identifier or encrypting evaluation instances. This is especially important for testing models that rely on web-scraped training data. Including a _training_on_test_set_ task allows determining whether a model's training data included benchmark examples . As an additional measure, specifying clear release requirements informs users how to preserve the integrity of test results .

### Benchmark Documentation

Providing comprehensive and accessible documentation is crucial for the practicability and interpretation of benchmarks . Key information about a benchmark should be readily available and include documentation of benchmark construction processes , data collection  or test environment design, and its test tasks and their rationale . Clearly documenting evaluation metric(s) and reporting the statistical significance of results is necessary so that users can understand a benchmark's actual signal . To provide context and prevent misinterpretation, developers should document normative assumptions about benchmark properties and discuss the limitations of their benchmark. A benchmark's codebase should contain a requirements file, a quick-start guide or demo code, a description of code file structure and contents, and in-line comments within all relevant files. Having a benchmark's paper accepted at a peer-reviewed venue signals external scrutiny and adherence to certain standards. Lastly, developers should specify the applicable license to provide legal clarity and enable, e.g., commercial use.

Figure 4: Overview of assessment criteria for the benchmark documentation stage.

Figure 3: Overview of assessment criteria for the benchmark implementation stage.

An optimally designed, implemented, and documented benchmark will cease to be useful if it is not maintained. Developers should regularly check code usability and maintain a feedback channel for users to report issues or suggest improvements. Providing contact details of a person responsible for the benchmark facilitates communication and support. Alternatively, if a benchmark is not maintained anymore, authors should include a corresponding statement indicating that the benchmark was retired in any official benchmark artefacts.

## 5 Other Design Considerations

This section presents design considerations for benchmark developers that were excluded from our assessment because their appropriateness is context-dependent, they are not easily verifiable, or both. Our aim with this list is to promote conscious design decisions regarding these considerations.

**General vs. specific benchmarks.** Benchmark developers must decide whether to prioritize general or abstract knowledge and skills or specific contexts and domains. Broad concept benchmarks may contribute to understanding foundational characteristics of models, but often face challenges in real-world applicability and reliable testing (see App. A).

**Detecting small improvements.** Benchmarks should be designed so that a 1% improvement can be reliably detected . As  states, "the more difficult it is to detect small amounts of progress, the more difficult it becomes to make iterative progress on a benchmark." Practically, this is likely dependent on evaluation data size and task diversity.

**Multi-modal assessment.** As multi-modal models become increasingly common, benchmark developers may want to consider designing tasks to assess the capabilities they want to test across modalities. Additional design considerations for multi-modal assessments include the increased complexity of mapping a tested concept to different modalities and the different output formats of the tested models .

**Versioning.** Minor updates (e.g., removing faulty prompts) should be clearly indicated via _task versioning_. Major updates require releasing new _benchmark versions_, as exemplified by the AgentBench v0.1 and v0.2 releases .

**Dynamic vs. static benchmarks.** Dynamic benchmarks may better address quick saturation (App. A) and contamination (App. A) issues but reduce result comparability and are easier to implement for some tasks (e.g., adding numbers) than others. Static benchmarks, on the other hand, tend to suffer from the issues outlined above.

**Gameability.** An ideal benchmark is resilient to attempts to boost task performance without improving the fundamental capability being tested . Existing benchmarks have been shown to be vulnerable to manipulation . Specific guidelines have been proposed to prevent cheating and ensure evaluations reflect genuine model performance .

**Positionality statement.** Positionality statements5 are a reflective account common in social sciences research. In them, researchers acknowledge how their background, experiences, and biases may have influenced their work. If developers believe such factors significantly impacted their benchmark's construction, they may provide a positionality statement for increased context and transparency.

Figure 5: Overview of assessment criteria for the benchmark maintenance stage.

## 6 Quantitative Results

In this section, we present our assessment results.6 Tab. 1 showcases the average scores per benchmark lifecycle stage, showing that for both FM and non-FM benchmarks, the implementation stage tends to be the weakest area, followed by maintenance. All criteria averages are reported in App. F. Some criteria have not been fulfilled by almost any benchmark (e.g., _Standardized metadata is included_). Notably, both benchmark types are particularly weak for criteria supporting the reproducibility and interpretation of results: benchmarks get an average score of 3.75 on _Including a script to replicate results_ and an average score of 5.62 on _Reporting statistical significance_.

While individual benchmark or criteria scores are deterministic, we can analyze statistical fluctuations across categories and benchmarks. Fig. 7 compares the design and usability scores of FM and non-FM benchmarks. The overall average design score across all benchmarks is 10.7, and the weighted average usability score is 8.7. The difference in mean design and usability scores between FM and non-FM benchmarks is not statistically significant (95% confidence level), see Fig. 8 in App. E. Furthermore, we find statistically significant correlations between the design and usability scores for FM benchmarks alone and all benchmarks combined at the 95% confidence level (Tab. 2). This suggests that, in both cases, benchmarks with poorer design tend to also be less usable, and vice versa.

## 7 Discussion

**Not all benchmarks are of the same quality.** Model developers frequently report performance on benchmarks that vary significantly in quality. For instance, the widely-used MMLU benchmark scored the lowest in our assessment (weighted average: 5.5), while GPQA scored significantly higher (weighted average: 11.0). However, recent communications introducing models like GPT-4 , Claude-3 , and Gemini  report results on both benchmarks without explicitly acknowledging their limitations or quality differences. This practice may be driven by the assumed expectation that reviewers want to see a wide range of metrics and the belief that readers should determine the most relevant metrics for their needs. The lack of clear guidance on AI benchmark quality and limitations may lead to incorrect conclusions about a model's performance, even if developers do not intend to

    & **FM** & **Non-FM** & **All** \\ 
**Pearson \(\)** & 0.721 & 0.318 & 0.655 \\
**p-value \(p\)** & 0.001 & 0.487 & 0.001 \\   

Table 2: Pearson correlation coefficient for FM, Non-FM, and All benchmarks between the design and usability (weighted average of implementation, documentation, and maintenance stages) score as in Fig. 7.

  
**Stage** & **FM** & **Non-FM** & **All** \\ 
**Design** & 10.6 & 11.1 & 10.7 \\
**Implementation** & 5.5 & 7.4 & 6.1 \\
**Documentation** & 10.3 & 9.9 & 10.1 \\
**Maintenance** & 9.1 & 10.8 & 9.7 \\   

Table 1: Benchmark lifecycle scores averaged over the 24 assessed benchmarks separated for FM, non-FM, and All benchmarks combined.

Figure 6: Average and individual scores of all assessed benchmarks per lifecycle stage.

mislead users. The UK AI Safety Institute's _Inspect_ framework  similarly includes both MMLU  and GPQA , potentially resulting in misleading evaluations. This is problematic because governments increasingly rely on evaluations for AI regulations and may use frameworks like _Inspect_ or individual benchmarks .

**Most benchmarks fail to distinguish signal and noise.** Benchmark developers should not only report a single result for a model but also re-run their evaluation  with, e.g., different random seeds or sampling temperatures, and report the mean and variance for these intra-model evaluations. As benchmarks are primarily used to compare models, users must know the intra-model variance of a benchmark to determine whether observed inter-model variances are genuine performance differences or arise from noisy results. If intra-model variance bounds are tight and inter-model variance bounds are wide, benchmark users can conclude that there are genuine performance differences between models. However, if both intra- and inter-variance bounds are wide, statistical analysis is required to discern noise and actual signal. Yet, 14 out of 24 benchmarks did not perform multiple evaluations of the same model or report statistical significance or uncertainty of results.

**Insufficient implementation limits reproducibility and scrutiny of benchmarks.** Our analysis reveals that scores for implementation stage criteria are the lowest across all assessed benchmarks. Notably, 17 out of 24 benchmarks do not provide easy-to-run scripts to replicate the results reported in the initial paper, and 4 out of 24 only provide scripts to replicate part of the results. This lack of accessibility hinders reproducibility and limits users' ability to scrutinize the benchmarking process. In a field where reproducibility is a significant concern , providing materials to reproduce results is crucial for validating benchmark findings.

**Small changes can lead to significant improvements in overall benchmark practices.** Many of the criteria we have identified for improving AI benchmarks are relatively easy to implement, even for existing benchmarks. For example, adding code documentation and and a point of contact are not time consuming to add, yet can significantly enhance usability, accountability, and ease of use.

**Necessity for higher benchmark development standards.** As evidenced by the strong discrepancies in AI benchmark quality we found (Sec. 6 and App. F), there is a need to introduce additional checks for benchmarking practices to ensure a minimum quality standard for AI benchmarks. We assume that benchmark developers do not intentionally construct insufficient benchmarks, but rather do so due to limited knowledge of what constitutes a good benchmark. By providing a checklist of best practices (App. J.1), we aim to make it easy for benchmark developers to adopt these recommendations and

Figure 7: Design and usability score for all 24 assessed benchmarks. The usability score is the weighted average of the implementation, documentation, and maintenance scores. Benchmarks were split into foundation model and non-foundation model benchmarks, depending on the model group theyâ€™re targeting.

improve the quality of their benchmarks. In addition, some of the criteria we have identified in our expert interviews and from reviewing evaluation practices in other fields, such as including a build status in GitHub repositories that assesses whether the last commit successfully passed defined unit tests , were relatively unknown and only implemented by 3 out of 24 benchmarks. Other criteria, like using globally unique identifiers or encrypting evaluation instances to avoid data contamination, have been pioneered by only a few of the assessed benchmarks  but have not yet gained widespread adoption. By incorporating these criteria into our assessment, we aim to encourage benchmark developers to adopt these best practices in the field of AI benchmarking.

## 8 Limitations

Our assessment assigns equal weight to all criteria, despite their varying levels of effort required for fulfillment and differing contributions to overall benchmark quality. The scoring system differentiates only four score categories to enable relatively objective evaluation through clear-cut criteria (App. K and App. G), but may miss nuances within each category. For example, a benchmark barely fulfilling a criterion and one almost entirely fulfilling it would receive the same 10-point score. Given the equal weighting and scoring, benchmark developers could potentially "game" the assessment by focusing on easily fulfilled criteria. However, we believe that even if a developer only implements easy-to-implement criteria, the resulting benchmark will still be of higher quality than one not meeting any criteria, thus fulfilling our work's goal. Furthermore, assessing the construct validity of a benchmark and determining whether its approach to assessing a concept is truly effective would presumably require in-depth analysis by domain experts in the respective fields, which is beyond the scope of this assessment. Instead, we aim to provide benchmark developers with a blueprint for minimum quality assurances. Finally, our framework is intended for public benchmarks and future work is needed to extend it to private ones.

## 9 Impact Statement

By releasing the first systematic assessment framework for AI benchmarks, we aim to encourage benchmark developers to construct higher-quality benchmarks and to contribute to community efforts to make AI evaluations more practicable and transparent. Higher-quality benchmarks resulting from the adoption of our framework and checklist can lead to better-informed model selection for downstream tasks, potentially reducing risks and improving outcomes in high-stakes applications. Our living repository of benchmark assessments promotes transparency and comparability, allowing benchmark users to make informed decisions when choosing benchmarks. However, there is a potential risk of misinterpretation of our results; our assessment only provides minimum quality assurances and is not sufficient to assess the suitability of a benchmark for a concrete use case. The outputs of our evaluation do not contain sensitive or harmful content, but users may encounter such content during a benchmark assessment depending on the benchmark's data. While we do not anticipate direct safety risks from releasing our framework, we acknowledge that strict adherence to some of our proposed criteria, such as the involvement of domain experts, may unequally impact researchers based on their access to resources and connections, potentially hindering the development of benchmarks from a broader range of research institutions and underrepresented communities, which could limit diversity in benchmark creation.