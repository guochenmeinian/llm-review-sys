# Attention boosted Individualized Regression

Guang Yang

Department of Data Science

College of Computing

City University of Hong Kong

guang.yang@my.cityu.edu.hk

&Yuan Cao

Department of Statistics and Actuarial Science

School of Computing and Data Science

The University of Hong Kong

yuancao@hku.hk

&Long Feng

Department of Statistics and Actuarial Science

School of Computing and Data Science

The University of Hong Kong

lfeng@hku.hk

Long Feng is the corresponding author.

###### Abstract

Different from classical one-model-fits-all strategy, individualized models allow parameters to vary across samples and are gaining popularity in various fields, particularly in personalized medicine. Motivated by medical imaging analysis, this paper introduces a novel individualized modeling framework for matrix-valued data that does not require additional information on sample similarity for the individualized coefficients. Under our framework, the model individualization stems from an optimal internal relation map within the samples themselves. We refer to the proposed method as Attention boosted Individualized Regression, due to its close connections with the self-attention mechanism. Therefore, our approach provides a new interpretation for attention from the perspective of individualized modeling. Comprehensive numerical experiments and real brain MRI analysis using an ADNI dataset demonstrated the superior performance of our model.

## 1 Introduction

Model-based machine learning methods have advanced significantly and become essential in modern data analysis. From linear regression to deep neural networks, most approaches fundamentally follow an one-model-fits-all strategy, meaning that parameters of a well-trained model are fixed and do not change for different samples. However, in fields like medical diagnosis and treatment design, it is important to explore and apply individualized models with parameters tailored to each sample, adapting to their unique features. Due to the heterogeneity among instances, individualized models are expected to provide more accurate predictions and personalized interpretations, which are their main advantages.

Individualized modeling has been extensively investigated in research, with the earliest example possibly being the varying coefficient models  in statistics community. A varying coefficient model usually includes an additional variable and represents the varying coefficient as a function of this extra variable. These models have been applied and adapted in various contexts. For instance,  explored spatial modeling using a spatially varying coefficient process. In a similar vein,  considered varying coefficient models in image response regression and proposed using deep neural networks to estimate the varying coefficients.

Beyond varying coefficient models, recent studies have also incorporated prior knowledge of sample similarity to regulate sample-specific coefficients. The fundamental assumption is that the similarity among coefficients for different samples relies on the sample similarity, meaning that the more similar the samples, the closer their coefficients. For instance,  tackled personalized medical models using a multi-task learning approach called FORMULA, assuming that models for similar patients are close and achieving this through Laplacian regularization.  developed the localized Lasso, which assumes a known weighted network over samples that reflects the distance in parameter space.  loosened the aforementioned prior assumption by considering additional covariates and assuming the existence of some measurement of similarity corresponding to similarity in parameter space. Moreover, they constrained the matrix of personalized parameters to be low-rank, so closeness in loadings implies closeness in parameters. While effective in various contexts, these methods heavily rely on the prior knowledge about parameter similarity, which might not be readily available in numerous real-world applications.

This paper aims to develop a novel individualized modeling framework for matrix-valued data, without the need for additional information on sample similarities. In our framework, model individualization is derived from the heterogeneity inherent in the samples themselves. Specifically, we seek to find an optimal sample-specific internal relation map to enhance model fitting and interpretation. The sample-specific relation map allows us to capture the local dependence between patches within each matrix input, thereby enhancing prediction performance and model interpretability.

It is worth noting that the proposed individualized modeling framework with sample-specific internal relation map is highly connected to the self-attention mechanism , which has demonstrated its exceptional performance in various field, including natural language processing, computer vision, and more. Due to such connection, we named the proposed framework Attention boosted Individualized Regression (AIR). Therefore, our approach could also provide a new interpretation for attention from the perspective of individualized modeling.

We should emphasize that our proposed approach is particularly well-suited for applications in personalized medicine and brain connectomics analysis, which initially motivated us to study individualized modeling. In recent years, the field of brain connectomics has experienced rapid growth due to advancements in medical imaging technology. This area of study focuses on examining comprehensive maps of connections within the human brain, playing a vital role in cognitive neuroscience, clinical diagnosis, and more. Brain networks can be represented by relation matrices, often established based on connections among regions of interest (ROIs). Besides sample features, the internal relationships within each sample may also influence relevant responses. This consideration has been addressed in the literature, such as [18; 9]. In this context, differentiated internal relations can emphasize heterogeneity among subjects, providing individual-level information about brain connectivity. This effect supports the use of internal relations in individualized models and further contributes to personalized medicine.

## 2 Attention boosted individualized regression

Given any matrix \(\), we first introduce a matrix reshaping operator that allows us to explore the internal relations within \(\). Let \(\) have dimensions \(D_{1} D_{2}\) and let \(d_{1},d_{2}\) be factors of \(D_{1},D_{2}\). Define \((p_{1},p_{2})=(D_{1}/d_{1},D_{2}/d_{2})\). We can now define the operator \(_{(d_{1},d_{2})}():^{D_{1} D_{2}} ^{(p_{1}p_{2})(d_{1}d_{2})}\) as a mapping from \(\) to

\[_{(d_{1},d_{2})}()=[(_{1,1}^{d_{1}, d_{2}}),,(_{p_{1},p_{2}}^{d_{1},d_{2}}) ]^{}, \]

where \(_{j,k}^{d_{1},d_{2}}\) represents the \((j,k)\)-th block of \(\) with size \(d_{1} d_{2}\). The operator \(_{(d_{1},d_{2})}()\) essentially vectorizes each of the \(p_{1} p_{2}\) block (of size \(d_{1} d_{2}\)) and stacks the vectorized blocks together. Denote the inverse operation of \(_{(d_{1},d_{2})}()\) as \(_{(d_{1},d_{2})}^{-1}()\). A special case occurs when \((d_{1},d_{2})=(1,D_{2})\), in which case we have \(_{(1,D_{2})}()=_{(1,D_{2})}^{-1}()=\). It is worth noting that this reshaping operation has also been applied in attention mechanisms, allowing us to examine the relations or correlations among the \(p_{1} p_{2}\) patches.

Suppose we observe \(n\) samples with scalar outcomes \(y_{i}\) and matrix inputs \(_{i}^{}^{D_{1} D_{2}}\) for \(i=1,,n\). Given a block size \((d_{1},d_{2})\), we first reshape the original images to obtain \(_{i}=_{(d_{1},d_{2})}(_{i}^{})^{p  d}\), where \(p=p_{1}p_{2}\) and \(d=d_{1}d_{2}\). Then we consider the following individualized linear regression model with coefficient matrices varying across samples

\[y_{i}=_{i},_{i}+_{i},\ \ i=1,,n, \]

where \(_{i}^{p d}\) is the unknown individualized coefficient matrix for \(i\)-th sample and \(_{i}\) is the noise term. Note that the reshaping operation \(_{(d_{1},d_{2})}()\) is one-to-one. Thus, model (2) is equivalent to \(y_{i}=_{i}^{()},_{i}^{()}+ _{i}\), with \(_{i}^{()}=_{(d_{1},d_{2})}^{-1}(_{i})\). As previously mentioned, model (2) type of individualized regression has been studied under various constraints on the individualized coefficients, such as [10; 6; 24; 25].

In this paper, we propose to model \(_{i}\) with two components: a homogeneous coefficient \(\) reflecting common effects and a heterogeneous coefficient \(_{i}\) containing individualized information. Specifically,

\[_{i}=+_{i},\ \ i=1,,n. \]

For the heterogeneous coefficients, we further assume that they share an unknown common factor \(\) across samples,

\[_{i}=_{i}^{}. \]

Here, \(_{i}\) represents unknown sample-specific factors serving as a re-weighting matrix to aggregate the coefficients in \(\), where the transpose is to better connect with self-attention mechanism later. The matrix \(^{p d}\) can be viewed as a base coefficient matrix for the heterogeneous effects. Clearly, additional constraints on the individual factor \(_{i}\) are necessary to ensure the identifiability of the model. The choice of factor \(_{i}\) may vary depending on the purpose. In this paper, we propose an internal-relation-boosted individualized factor for matrix-valued inputs. Specifically, we consider \(_{i}^{p p}\) of the form

\[_{i}=g(_{i}_{i}^{}), \]

where \(^{d d}\) is an unknown matrix to be learned, while \(g():^{p p}^{p p}\) is a known function that preserves dimension, of which different forms to be discussed. It is worth recalling that \(_{i}=_{(d_{1},d_{2})}(_{i}^{}) ^{p d}\) is the reshaped matrix. The reshaping operation (1) enables us to calculate the "generalized correlation" between patches through (5). When fixing \(=_{d}\) and setting \(g()\) as the identity function, \(_{i}=_{i}_{i}^{}\) reduces to standard covariance matrix of patches within \(i\)-th sample when \(_{i}\) is properly centered.

In the formulation (5), the individualized matrix \(_{i}\) is designed to capture the internal relationships among the \(p\) rows of reshaped matrix (or \(p\) patches of original matrix) for each sample. Relations between two vectors can be measured in different ways, such as correlation, similarity, distance, etc. Our formulation of (5) is motivated by the rotation correlation introduced by . For any two vectors \(\) and \(\), the rotation correlation is defined as

\[_{}^{},\]

where the matrix \(\) is usually required to be orthogonal. This rotational correlation aims to find the maximized correlation between \(\) and \(\) with the best possible rotation. When \(\) is the identity matrix and \(\|\|_{2}=\|\|_{2}=1\), the rotation correlation reduces to standard Pearson correlation. We note that the \((j,k)\)-th element of the sample-specific factor can be written as \(\{_{i}\}_{jk}=\{_{i}\}_{j}.\{_{i}\}_{k}^{}\), where \(\{_{i}\}_{j}\). and \(\{_{i}\}_{k}\). are the \(j\)-th and \(k\)-th rows of \(_{i}\), respectively. In other words, \(\{_{i}\}_{jk}\) is related to the rotation correlation between \(\{_{i}\}_{j}\). and \(\{_{i}\}_{k}\). However, our goal is not to maximize the correlation between \(\{_{i}\}_{j}\). and \(\{_{i}\}_{k}\). but to find the optimal rotation that achieves the best fitting for the responses.

Combining (2) to (5), we obtain our individualized model in the following form

\[y_{i}=_{i},}_{}+ _{i},g(_{i}_{i}^{})^{} {D}}_{}+_{i}. \]

Here, \(^{p d}\), \(^{p d}\), and \(^{d d}\) are the coefficient matrices that need to be learned. The decomposition of (6) allows us to understand and assess the individuation degree of each sample and the entire model. At the sample level, a larger magnitude of the heterogeneous part indicates that the sample is more distinctive, affected by its internal relations. At the model level, the larger the magnitude of the homogeneous part, the closer the model is to an ordinary linear model, and vice versa. Naturally, achieving a proper balance between the two parts contributes to a better model fit.

We shall note that model (6) could be easily extended to a generalized linear model (GLM) setting to accommodate other types of outcomes. For example, by allowing certain link function \(f()\), we may consider a GLM of the form \(f((y_{i}))=_{i},_{i}\). Then, the coefficients \(_{i}\) could still be modeled as in (3) to (5).

To learn the coefficients \(\), \(\) and \(\), we propose the following penalized minimization problem

\[_{,,} _{i=1}^{n}(y_{i}-_{i},_{i })^{2}+_{1}\|\|_{F}^{2}+_{2}\|\|_{F}^{ 2},\] (7) s.t. \[_{i}=+g(_{i}_{i}^{})^{},\;\|\|_{F}=1,\]

where \(\|\|_{F}\) is the Frobenius norm and \(_{1}\) and \(_{2}\) are regularization parameters to balance the homogeneous and heterogeneous effects. Besides, a norm constraint for \(\) is also required due to identifiability consideration. We defer to Section 4 for the computation of (7).

## 3 Individualized regression and attention

We refer to our individualized modeling as Attention boosted Individualized Regression due to its connections with the self-attention mechanism. The self-attention mechanism was proposed in the seminal work , and the Transformer model based on it has demonstrated exceptional performance in natural language processing, computer vision, and other fields. In this section, we establish the connection between the proposed model (6) and the self-attention mechanism.

Given the input \(^{n d}\), the Scaled Dot-Product Attention mechanism computes the output using \(^{n d_{k}}\), \(^{n d_{k}}\), and \(^{n d_{v}}\), representing query, key, and value, respectively. The three essential components are linearly transformed from \(\) by

\[=_{Q},\;=_{K},\;= _{V}\]

with corresponding weight matrices \(_{Q}\), \(_{K}\), and \(_{V}\). Incorporating a softmax function for normalization, the Scaled Dot-Product Attention is defined as

\[f()=(^{}}{}}). \]

In the attention mechanism, the first part \((^{}}{}})\) essentially computes the pairwise similarity between queries and keys, normalized by a combination of scaling and row-wise softmax. With the resulting attention map, the output is obtained by reweighing the pairs' values. The attention map is at the core of the attention mechanism, as it provides an individualized map that captures information on pairwise similarity within each sample.

Moreover, the attention mechanism (8) could also be expressed in a row-wise form. Let \(=(,,)\) be the output of the attention function. Further let \(_{i},_{i},_{i}\) and \(_{i}\) be the \(i\)-th row of \(\), \(\), \(\), and \(\), respectively. Then, (8) is equivalent to

\[_{i}=^{N}(_{i}^{} _{j}/})_{j}}{_{j=1}^{N}(_{i}^{} _{j}/})}. \]

This form clearly highlights that the basis of the weights in the attention map is formed by vector correlation. In fact, the dot-product-based pairwise similarity is derived from a nonlinear transformation of the correlation between pairs. Beyond softmax function, normalization in attention could also be accomplished using a general function \(g()\). As a result, we obtain the following generalized attention

\[f()=g(^{}). \]The attention mechanism in the form of (10) with a nonlinear function \(g()\) can face computational challenges, as the direct computation of attention maps requires significant resources to handle \(n n\) matrices. To address the computation issues, several recent works have emerged, such as sparse transformers , efficient transformers , and more. Linear attention mechanisms have been studied as a subcategory, which can dramatically decrease complexity from quadratic to linear.  proposed a linear attention boosted on the first-order Taylor expansion of the exponential part in the softmax function, i.e., \((^{}) 1+^{}\).  presented the linearized attention using kernel functions, which measure the similarity between \(\) and \(\) through \(()^{}()\). In this case, \(()\) represents a specific kernel function.  introduced Linformer, which leverages the low-rankness of the attention map to reduce complexity to linear. Notably,  considered linear \(()=/n\) as scaling normalization, consequently,

\[f()=^{}. \]

Linear attention mechanisms are efficient because they bypass the need to compute \(n n\) matrices by using associative multiplication, reducing complexity from \(O(n^{2})\) to \(O(n)\). While on the other hand, experiments show that Linear attentions does not result in a significant compromise in performance.

Now we demonstrate the connections between our individualized regression model (6) and the self-attention mechanism. We let the homogeneous coefficient \(=\) and focus on the model

\[y_{i}=_{i},g(_{i}_{i}^{})^{} +_{i}. \]

**Proposition 3.1**.: _Suppose the model (12) holds and matrices \(\) and \(\) in model (12) could be decomposed as below_

_(I)_ \(=_{Q}_{K}^{}\) _for two matrices_ \(_{Q},_{K}^{d d_{k}}\) _with_ \(d_{k} d\)_,_

_(II)_ \(=_{V}^{}\) _for two matrices_ \(,_{V}^{d d_{v}}\) _with_ \(d_{v} d\)_._

_Then, the following equation holds for each sample \(_{i}\)_

\[_{i},\ g(_{i}_{i}^{})^{} = g(_{i}_{i}^{})_{i},\  , \]

_where_

\[_{i}=_{i}_{Q},\ \ _{i}=_{i}_{K},\ \ _{i}=_{i}_{V}.\]

_Remark 3.2_.: Proposition 3.1 establishes the connection between our individualized regression model (12) and the self-attention mechanism (10). We shall note that the product of the query and key \(_{i}_{i}^{}=_{i}_{i}^{}\) essentially acts as an internal relation map, capturing the inter-dependence between local patches. By applying an appropriate function \(g()\), we can obtain the normalized sample-specific internal relation map. Furthermore, the value matrix \(_{i}\) can be enhanced by multiplying with such relation map. The final outcome is obtained as the inner product of the aggregated features and the coefficient matrix.

It is important to note that the two conditions in the proposition are mild, as they correspond to the low-rank assumptions: (I) \(() d_{k}\) and (II) \(() d_{v}\). In particular, (I) is consistent with Theorem 1 in , which demonstrated that the self-attention mechanism, i.e., the attention matrix, is low-rank. Moreover, if assumption (II) is not considered, (13) becomes equivalent to the simplified Vision Transformer in  without considering the value.

Conversely, the equivalence (13) also helps understand our model from the perspective of the self-attention mechanism. Since the tuple \((_{Q},_{K},_{V})\) represents embedding projections in self-attention, \(=_{Q}_{K}^{}\) is equivalent to a composite embedding that is adaptive and determines the attention map. Meanwhile, \(=_{V}^{}\) represents a projected regression coefficient that can be learned as a whole. The heterogeneous coefficients \(_{i}=g(_{i}_{i}^{})^{}\) can be considered as an aggregation of base coefficients through the attention map, contributing to model interpretation. If we set \(g()\) as the identity function, (13) simplifies to linear attention, thus enjoying the computational advantages of linear attention.

We should also note that with the growing popularity of transformers in natural language processing, self-attention-based architectures have begun to be introduced in computer vision, encompassingvarious visual tasks such as detection, segmentation, and generation. However, we mainly discuss their initial involvement in regression and classification tasks [23; 3; 5]. In particular,  directly applied a pure transformer to address the image classification problem and proposed Vision Transformer (ViT). ViT treats images as sequences by dividing them into fixed-size patches and processes them using a transformer architecture. ViT comprises two main components: the Encoder and the Classifier. In the transformer encoder, each attention map is computed for each image based on patch-wise similarity. The embedded patches are then followed by a multilayer perceptron head that serves as a regressor/classifier. Although some details are not discussed, this simplification helps to understand the connection with our model. More recently,  proposed simplifying transformer blocks. By removing skip connections, value parameters, sequential sub-blocks, and normalization layers, the simplified transformer has the potential to achieve fewer parameters and faster training.

## 4 Computation

In this section, we demonstrate the computation of the penalized minimization problem (7). From now on, we shall focus on the special case where \(g(x)=x\) is the identity function, which corresponds to linear attention. Namely, the model is

\[y_{i}=_{i},+_{i}^{}_{i}^{} +_{i}. \]

In this context, we develop an alternating minimization algorithm and highlight its benefits compared to gradient-based ones. First, we observe that the heterogeneous part in model (14) satisfies

\[_{i},_{i}^{}_{i}^{} =_{i}^{}_{i}^{}_{i}, =_{i}_{i}^{}_{i}, . \]

Moreover, let \(=()\) and \(=()\) be the vectorization of \(\) and \(\). It holds that

\[_{i}^{}_{i}^{}_{i},= _{i},^{},_{i}=(_{i}^{}_{i})_{i}^{} \]

and \(\) denotes the Kronecker product. Clearly, (16) displays a bilinear form. We start our algorithm by initializing \(\) as the top left singular vector of \(_{i=1}^{n}y_{i}_{i}\). Formally,

\[}^{(0)}=_{u}(_{i=1}^{n}y_{i}_{i} ), \]

where \(_{u}()\) represents the top left singular vector of a matrix.

Now we introduce our alternating minimization algorithm. Denote \(}^{(t)}\), \(}^{(t)}\) and \(}^{(t)}\) as the iterates in \(t\)-th loop. According to (15), we alternatively update \((}^{(t)},}^{(t)})\) and \(}^{(t)}\) as below.

Given \(}^{(t-1)}\), denote \(_{i}^{(t-1)}=_{i}}^{(t-1)}_{i}^{}_{i}\). Then \((}^{(t)},}^{(t)})\) can be updated by

\[(}^{(t)},}^{(t)})=*{ argmin}_{,}_{i=1}^{n}(y_{i}-[ _{i},_{i}^{(t-1)}]\!,[,\;] )^{2}+_{1}\|\|_{F}^{2}+_{2}\|\|_{F}^{2}. \]

Clearly, (18) can be seen as a ridge-like regression with two levels of penalization on distinct coefficients, which has an explicit solution shown in Section A in the appendix.

Given \((}^{(t)},}^{(t)})\), then \(}^{(t)}\) can be updated by

\[}^{(t)} =*{argmin}_{}_{i=1}^{n} y_{i}-_{i},}^{(t)}- _{i}^{}}^{(t)}_{i}^{}_{i}, ^{2},\] (19) \[}^{(t)} =}^{(t)}/\|

Theoretical analysis

In this section, we provide theoretical guarantees for our Attention boosted Individualized Regression. Specifically, we show that \(^{(t)}\) and \(^{(t)}\) obtained by alternating minimization algorithm converge to the true counterparts at a geometric rate. To simplify analysis, we focus on the heterogeneous part of model (14), although our results can be extended to more general cases. Suppose that

\[y_{i}=_{i},_{i}^{}_{i}^{}+ _{i}. \]

Let \(=()\) and \(=()\), the optimization problem could be written as

\[_{,}_{i=1}^{n}y_{i}- _{i}^{}_{i}_{i}^{},^{} }^{2}+_{2}\|\|_{2}^{2}. \]

which is non-convex on \(\) and \(\). For the rearranged images \(_{i}\) for \(i=1,,n\), we define

\[=(\{(_{1}^{}_{1}) _{1}^{}\},\ ,\ \{(_{n}^{} _{n})_{n}^{}\})^{}. \]

Here each row of \(\) represents a transformed sample. For the new feature matrix \(\), we suppose the following RIP condition.

_Condition 5.1_.: _(Restricted Isometry Property)_ For each integer \(=1,2,\), a matrix \(^{n q_{1}q_{2}}\) is said to satisfy the \(r\)-RIP condition with constant \(_{r}(0,1)\), if for all \(^{q_{1} q_{2}}\) of rank at most \(r\), it holds that

\[(1-_{r})\|\|_{F}^{2} 1/n\|()\|_{2}^{2} (1+_{r})\|\|_{F}^{2}. \]

The Restricted Isometry Property (RIP) was initially introduced by  for sparse vector recovery and subsequently extended by  for low-rank matrices, as in Condition 5.1. Many random matrices with an adequately large number of independent observations, such as Gaussian or sub-Gaussian matrices, satisfy the RIP condition . In our analysis, we require that \(\) defined in (23) satisfies the \(2\)-RIP condition with constant \(_{2}\).

To evaluate the estimation error of parameters, we consider an angle-based distance between two matrices. Formally, for any two matrices \(\) and \(\) with the same dimension, we define the distance as \((,)=,^{2}/(\| \|_{2}^{2}\|\|_{F}^{2})}\). This distance metric corresponds to the squared sine value after vectorization, that is, \((,)=(,)\), where \(=()\) and \(=()\). Now we are ready to present our main theorem.

**Theorem 5.2**.: _Suppose model (21) holds and solved by alternating minimization algorithm. Assume that \(\) satisfies 2-RIP Condition 5.1 with a constant \(_{2}\). Denote \(_{0}=(}^{(0)},)\) as the initial distance. Let \(_{1}=_{0}/2+3_{2}/(1-3_{2})\) and \(_{2}=_{0}/2+(3_{2}+_{2})/(1-3_{2}+_{2})\) and assume \(_{1},_{2}<1\). And \(_{1},_{2}\) are noise related terms. Suppose \(_{1}_{0}+_{1}_{0}\) and Theorem 5.3 suggests that the prediction error decreases in a similar manner as the estimation errors in Theorem 5.2. It is important to note that the error bounds in both theorems are dependent on suitable initialization. We employ spectral initialization as shown in (17), which has been proven to have an error closely approximating the true value.

## 6 Simulation

We conduct extensive simulation studies to evaluate the performance of our Attention boosted Individualized Regression compared to related methods in this section. Besides, ablation studies are deferred to Section B.1 in the appendix to show the advantage of combining the homogeneous and heterogeneous parts. Throughout the simulation, we assume that the data is generated according to the model (6). The size of the images is set to \(28 28\), with a sample size of 4000 for training and 1000 for testing. The noise \(_{i}\) follows an i.i.d. \((0,1)\). The coefficient matrices \(^{}\) and \(^{}\) are generated as two circles depicted in Figure 1. For the images \(_{i}^{}\), we assume that internal relations exist among blocks of size \(4 4\) within each image, where two blocks at random locations are correlated. The entries in \(_{i}^{}\) follow i.i.d \((0,1)\), while the correlated blocks are generated using the two methods below.

Case 1: With specific \(\), the internal relations are subject to (5). Consider a low-rank \(=2_{1}_{1}^{}+1_{2}_{2}^{}\) where \(_{1},_{2}\) and \(_{1},_{2}\) are random vectors with entries subject to i.i.d. \((0,1)\). Then, the correlated blocks are generated as \(_{1}\) plus noise vectors with i.i.d. entries from \((0,0.25)\).

Case 2: Without specific \(\), the internal relations are Pearson correlation coefficients. Given a random vector \(\) as a base with i.i.d. entries from \((0,1)\), the correlated blocks are also generated as \(\) plus noise vectors with i.i.d. entries from \((0,0.25)\). Then \(_{i}\) is taken as the correlation matrix where \((j,k)\)-th element of \(_{i}\) is the Pearson correlation coefficient between \(j\)-th and \(k\)-th blocks within \(_{i}^{}\).

Furthermore, we consider different levels of model individualization and investigate the effects on model performance. To this end, we define the degree of individuation (DI) of model (6) by the relative total magnitude of the heterogeneous part and homogeneous part. Specifically, \(=^{n}(_{i},_{i})^{2}/_{i=1}^{n}(_{i},)^{2}}\).

The performance of AIR is compared with four competing methods, including, low-rank matrix regression [LRMR, 27], tensor regression with lasso penalty [TRLasso, 28], Deep Kronecker Network [DKN, 7], and Vision Transformer [ViT, 5], respectively. Implementation details are provided in Section B.2 in the appendix. Of note is that we cannot implement several individualized regression methods  as they require additional information of unknown variables. We evaluate prediction performance of different methods, measured by the root mean squared error (RMSE) on test set: \(})_{i=1}^{n_{}}(_{i}^{ }-y_{i}^{})^{2}}\). The average and standard error of 100 repetitions are reported in Table 1, and the estimated coefficients of different methods are illustrated in Figure 1 and 4.

The numerical results indicate that AIR outperforms all other methods, with the advantage increasing as the degree of individuation becomes greater. Figure 1 and 4 demonstrate that AIR, when solved by our algorithm, can accurately recover the shape of the true parameters. It is worth noting that in Case 2, even though our model is mis-specified with no explicit \(\) exists, AIR still performs well. Common-model methods such as LRMR, TRLasso, and DKN tend to estimate the sum of the true coefficients for both parts. On the other hand, ViT typically requires a large number of samples and is thus not as effective due to the limited sample size.

    &  &  \\   & & AIR & LRMR & TRLasso & DKN & ViT \\   & 0.5 & 4.422 (0.130) & 6.616 (0.020) & 8.215 (0.021) & 4.886 (0.018) & 18.429 (0.049) \\  & 1.0 & 8.102 (0.325) & 13.101 (0.040) & 14.655 (0.044) & 7.028 (0.032) & 18.351 (0.047) \\  & 2.0 & 10.599 (0.816)

## 7 Real data analysis

In this section, we analyze the relationship between cognitive assessment scores and brain MRI data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). The ADNI is a study on Alzheimer's disease (AD) that includes clinical, genetic, and imaging data, covering AD patients, individuals with mild cognitive impairment (MCI), and healthy controls. We collected a total of 1059 subjects from ADNI 1 and GO/2 phases with Mini-Mental State Examination (MMSE) score and brain MRI. The MMSE score measures a patient's cognitive impairment which can assist in the diagnosis of AD. Brain MRI were carefully preprocessed following a standard pipeline involving denoising, registration, skull-stripping and so on and were resized to tensors of size \(48 60 48\) for computation efficiency. Then we extracted 10 middle coronal slices for each subject, resulting in images of size \(48 48\). Two samples are shown in the first column in Figure 2.

We compare the methods described in the simulation section by 5-fold cross-validation in test RMSE, of which average and standard error are presented in Table 2. AIR exhibits the best prediction performance among all methods, of which the significance can be shown by paired t-test. Furthermore, Figure 2 compares estimations of different methods while illustrates the individualized estimations from AIR for two different subjects, including the heterogeneous effect \(}^{}\) and significant internal relations. To screen significant internal relations for each subject, we summarize relations of each node in the internal relation matrix \(}_{i}\) and select top 5 as significant nodes. Subsequently, we mark these nodes at corresponding locations in the original sample by red boxes and show their relations by a chord diagram. For example, the block (4, 5) in sample 1 has the strongest relations, and is related to both (6, 4) and (6, 5), indicating the important relations between corpus callosum and hippocampus. We also note that after separating heterogeneous effect, the homogeneous effect \(}^{}\) highlights regions of the hippocampus, which have been acknowledged in medical literature as a crucial substructure associated with Alzheimer's disease . By this means, we can find important regions and relations among them for each subject, which is potential to help personalized treatment. In contrast, other methods do not reveal clear shapes and fail to offer valuable interpretations.

   AIR & LRMR & TRLasso & DKN & ViT \\ 
**3.145 (0.019)** & 3.715 (0.008) & 3.292 (0.023) & 3.261 (0.017) & 3.282 (0.025) \\   

Table 2: Prediction errors of different methods.

Figure 1: Case 1 simulation results with DI \(=1.0\). The first three columns show true parameters and estimations from AIR. The last two columns show estimations from other methods except ViT, as it has no explicit coefficient matrix. An additional OLS estimation is added for reference.

## 8 Discussion

In this paper, we present an Attention boosted Individualized Regression model that emphasizes internal relationships within samples and is based on the concept of rotation vector correlation. Our method is specifically tailored for data with heterogeneous internal relationships. By concentrating on the internal relations within samples, our approach effectively addresses the complex and heterogeneous nature of data, making it highly beneficial for various fields, particularly, brain imaging analysis and personalized medicine. On the other hand, we realize that the AIR framework also has limitations. First, its capability to handle general data is more or less restricted. When there are minimal heterogeneous effects, its performance will be similar to an ordinary linear model. Second, as discussed earlier, our framework could be viewed as a simplified version of the Vision Transformer; however, such simplifications may also reduce its approximation power for more complex scenarios. Furthermore, this paper primarily investigates the linear form of AIR. Although the linear form performs well in the cases of interest, it remains worthwhile to explore the generalization of the model in future work.