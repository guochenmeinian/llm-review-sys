# Test-time Training for Matching-based

Video Object Segmentation

Juliette Bertrand\({}^{*1,2}\) Giorgos Kordopatis-Zilos\({}^{*1}\) Yannis Kalantidis\({}^{2}\) Giorgos Tolias\({}^{1}\)

\({}^{1}\)VRG, FEE, Czech Technical University in Prague \({}^{2}\)NAVER LABS Europe

###### Abstract

The video object segmentation (VOS) task involves the segmentation of an object over time based on a single initial mask. Current state-of-the-art approaches use a memory of previously processed frames and rely on matching to estimate segmentation masks of subsequent frames. Lacking any adaptation mechanism, such methods are prone to test-time distribution shifts. This work focuses on matching-based VOS under distribution shifts such as video corruptions, stylization, and sim-to-real transfer. We explore test-time training strategies that are agnostic to the specific task as well as strategies that are designed specifically for VOS. This includes a variant based on mask cycle consistency tailored to matching-based VOS methods. The experimental results on common benchmarks demonstrate that the proposed test-time training yields significant improvements in performance. In particular for the sim-to-real scenario and despite using only a single test video, our approach manages to recover a substantial portion of the performance gain achieved through training on real videos. Additionally, we introduce DAVIS-C, an augmented version of the popular DAVIS test set, featuring extreme distribution shifts like image-/video-level corruptions and stylizations. Our results illustrate that test-time training enhances performance even in these challenging cases. Project page: [https://jbertrand89.github.io/test-time-training-vos/](https://jbertrand89.github.io/test-time-training-vos/)

Figure 1: **Test-time training significantly improves performance in the presence of distribution shifts for the task of video object segmentation (VOS). _Left:_ Example using a model trained on synthetic videos from BL-30K  and tested on a real video from DAVIS . _Right:_ Example using a model trained on DAVIS  and YouTube-VOS  and tested on a cartoonized video from the corrupted DAVIS-C benchmark which we introduce in this work. Second-to-bottom row: Results obtained using the STCN  approach. Bottom row: Results after test-time training with the proposed _mask cycle consistency_ loss (\(tt\)-MCC) using the single ground-truth mask provided for the first video frame.**Introduction

In one-shot video object segmentation (VOS), we are provided with a single segmentation mask for one or more objects in the first frame of a video, which we need to segment across all video frames. It is a dense prediction task over time and space, and therefore, collecting training data is highly demanding. Early VOS methods design foreground/background segmentation models that operate on single-frame input and require a two-stage training process [4; 29]. During the off-line stage, the model is trained to segment a variety of foreground objects. During the on-line stage for a specific test video, the model is fine-tuned to adapt to the objects of interest indicated in the provided mask.

Recently, _matching-based_ methods like STCN  or XMem  have shown impressive performance on the common VOS benchmarks. These methods propagate the segmentation mask from a memory of previously predicted masks to the current frame. They only involve an off-line training stage where the model learns how to perform matching and propagation. Lacking any test-time adaptation mechanism, however, such methods are highly prone to test-time distribution shifts.

Our goal is to improve the generalization of matching-based VOS methods under distribution shifts by fine-tuning the model at test time through a single test video. Such one-shot adaptation is a form of _test-time training_ (TTT), a research direction that is lately attracting much attention in classification tasks and a promising way for generalizing to previously unseen distributions. We argue that TTT is a great fit for matching-based VOS, not only because of the additional temporal dimension to exploit, but also because we are given a ground-truth label for a frame of the test example.

In this work, we focus on two cases of test-time distribution shift for VOS: a) when training on synthetic data and testing on real data, also known as _sim-to-real transfer_, and b) when the test data contain image/video corruption or stylization. We explore three different ways of performing TTT: i) a task-agnostic approach that borrows from the image classification literature and performs entropy minimization [34; 46], ii) an approach tailored to the STCN architecture that employs an auto-encoder loss on the mask provided for the first frame and does not exploit any temporal information, and iii) a mask cycle consistency approach that is tailored to matching-based VOS, and utilizes temporal cycle consistency of segmentation masks to update the model. As we show, these different test-time training strategies do not work equally well, and tailoring TTT to the task and the methods is important and not trivial.

Our experiments demonstrate that some of the proposed approaches constitute a powerful way of generalizing to unseen distributions. When starting from a model trained only on synthetic videos, test-time training based on mask cycle consistency improves STCN by +22 and +11 points in terms of \(\&\) score on the two most popular VOS benchmarks, YouTube-VOS and DAVIS, respectively. What is even more impressive is that by simply using test-time training, we recover the bulk of the performance gain that training the original model on real videos brings: for YouTube-VOS and DAVIS, TTT recovers 82% and 72% of the performance gains, respectively, that training on real videos brings compared to synthetic ones, but _without requiring any video annotations during off-line training_.

We also study the performance of TTT in the presence of image-level or video-level corruptions and stylization. To that end, we follow a process similar to ImageNet-C  and create DAVIS-C, a version of the DAVIS test set with 14 corruptions and style changes at three strength levels. We evaluate models with and without TTT on DAVIS-C and analyze how performance changes as the distribution shift increases. Our results show that TTT significantly improves the performance of STCN by more than 8 points in terms of \(\&\) score for the hardest case of corruption/stylization. A qualitative example for both types of distribution shift is shown in Figure 1.

## 2 Related Work

Foreground/background models with online fine-tuning for VOS.Early approaches train a segmentation model to separate the foreground object from the background. The offline training stage is followed by an online supervised fine-tuning stage that uses the mask of the first frame for adapting the model [4; 29]. The fine-tuning process is improved by including pseudo-masks derived from highly confident predictions  or by additionally including hard-negative examples . Maninis et al.  additionally incorporate semantic information about the underlined object categories via an external instance-aware semantic segmentation model. Other methods first produce a coarse segmentation result and then refine it by the guidance of optical flow , the appearance cue only , or temporal consistency . Hu et al.  utilize an RNN model to exploit the long-term temporal structures, while others additionally incorporate optical flow information . The heavy cost of fine-tuning is reduced by meta-learning in the work of Xiao et al.  or Meinhardt and Leal-Taixe .

Matching-based VOS.Most recent approaches build upon a matching-based prediction model, which is trained offline and neither requires nor incorporates any fine-tuning stage. The current test frame is matched to either the first annotated frame , a propagated mask , or a memory containing both . Early methods use simple matching process  or are inspired by classical label propagation , while different methods improve the design of the matching , the type and extent of the memory used , or exploit interactions between objects or frames , among other aspects.

Hybrid and other VOS methods.Mao et al.  borrow from both families of approaches, _i.e_. matching-based and those that require online fine-tuning, and jointly integrate two models, but, unlike our work, the matching model is not updated at test time. Some methods differ from matching-based in the way they capture the evolution over time, _e.g_. by RNNs  or by spatio-temporal object-level attention .

Cycle consistency in VOS.Matching-based methods require dense video annotations for training, which is a limitation. To dispense with the need for mask annotations during training, cycle consistency of pixel or region correspondences over space and time has been successfully used by prior work . This is an unsupervised loss that is also appropriate for test-time training. In our case, we tailor cycle consistency to the task of matching-based VOS, and propose a _supervised_ consistency loss that operates on masks by taking advantage of the provided mask at test-time in order to better adapt to the object of interest. Mask consistency based on the first frame is used by prior work during model training. Khoreva et al.  synthesize future frames using appearance and motion priors, while in our case, instead of synthesizing, we use mask predictions of the existing future video frames. Li et al.  and HODOR  use a temporal cycle consistency loss on a the first frame mask during training that allows learning from videos with a single segmentation mask. In contrast, we utilize the mask frame that is provided at test-time for the first frame of the test video and we are able to outperform HODOR and other state-of-the-art methods for multiple test-time distribution shifts.

Test-time training.A family of approaches adapts the test example to the training domain. Kim et al.  chose the appropriate image augmentation that maximizes the loss according to a loss prediction function, while Xiao et al.  updates the features of the test sample by energy minimization with stochastic gradient Langevin dynamics. Another family of approaches adapts the model to the test domain. Entropy minimization is a common way to update either batch-normalization statistics only  or the whole model . Other self-supervised objectives include rotation prediction , contrastive learning , or spatial auto-encoding . In our work, we move beyond the image domain and introduce mask cycle consistency as an objective to adapt the model specifically for video object segmentation applications. Azimi et al.  evaluate some of the aforementioned TTT techniques on top of self-supervised dense tracking methods under several distribution shifts on videos. Another recent work , concurrent to ours, also studies TTT in the video domain but for a classification task; we instead fully tailor the TTT on the VOS task and use temporal _mask_ consistency as our loss.

## 3 Test-time training for matching-based VOS

In this section, we first formulate the task of Video Object Segmentation (VOS) and briefly describe the basic components of the matching-based VOS methods we build on . We then present three ways for test-time training: a task-agnostic baseline based on entropy minimization that has been highly successful in other tasks and two VOS-specific variants that leverage the fact that we are provided with a ground-truth mask at test time; one using an auto-encoder loss and another a temporal cycle-consistency loss on the masks.

[MISSING_PAGE_FAIL:4]

equivalent to a cross-attention operation between the test frame features (queries) and the memory frame features (keys) that aggregates memory mask features (values). Finally, a mask decoder \(d_{m}\) is used to obtain the predicted mask \(_{j}=d_{m}([_{j},u_{j}])\), where \([_{j},u_{j}]^{W H(D_{e}+D_{m})}\) is the concatenation of the two representations along the depth dimension.

In this work, we assume that we have access to a matching-based VOS model, trained on real or synthetic data with mask annotations. These models do not generalize well to extreme distribution shifts not encountered during supervised training. Our goal is to improve their performance in such cases by fine-tuning the parameters of the model at test time.

### Test-time training

We explore three different losses to perform TTT for the case of matching-based VOS: (i) based on entropy minimization (\(tt\)_-Ent_), which forms a paradigm transferred from the image domain (image classification in particular) and is a task- and method-agnostic loss, (ii) based on an auto-encoder (\(tt\)_-AE_) that operates on segmentation masks, which is an approach tailored for the STCN architecture, (iii) based on mask cycle consistency (\(tt\)_-MCC_), which is more general and appropriate for a variety of matching-based methods. \(tt\)-MCC exploits the matching-based nature of STCN and includes mask propagation via a memory of masks, while the \(tt\)-AE variant does not include the matching step between multiple frames.

Given a test video, we optimize the parameters of the model using the provided mask and either only the first frame (for \(tt\)-AE) or all its frames (\(tt\)-Ent, \(tt\)-MCC). The test-time loss is optimized, the model parameters are updated, and the model is used to provide predictions for the specific test video. Then, the model is re-initialized back to its original parameters before another test video is processed.

For the two TTT variants that use multiple frames, _i.e_. \(tt\)-MCC and \(tt\)-Ent, we follow the process used during the off-line training phase of STCN and XMem and operate on randomly sampled frame triplets and octuplets, respectively. In the following, we describe the case of triplets which is extended to larger sequences in a straightforward way. All the triplets include the first video frame, while the second (third) triplet frame is uniformly sampled among the \(s\) frames that follow the first (second) triplet frame in the video. For triplet \(x_{0},x_{i},x_{j}\) with \(0<i<j\), the first predicted mask is given by \(_{i}=f(x_{i};\{(x_{0},m_{0})\})\), and the second by \(_{j}=f(x_{j};\{(x_{0},m_{0}),(x_{i},_{i})\})\), _i.e_. the second frame is added to the memory before predicting the last mask of the triplet. Multiple triplets are sampled during test-time training. The per-pixel losses are averaged to form the frame/mask loss. We denote the cross entropy loss between a ground-truth mask \(m\) and a predicted mask \(\) by \(_{CE}(m,)\).

#### 3.2.1 Entropy (\(tt\)-Ent) loss

We start from a task- and method-agnostic loss based on Entropy (\(tt\)-Ent) minimization, a common choice for test-time training on image classification . In this case, we force the model to provide confident predictions for the triplets we randomly sample. The entropy of mask \(_{i}\) is denoted by \(H(_{i})\) and the loss for a particular triplet is \(L_{H}=0.5*(H(_{i})+H(_{j}))\), which is minimized over multiple triplets. Prior work on image classification avoids the trivial solution by using a batch of test examples  or augmenting the input example and minimizing the entropy over the average prediction of the augmented frames . In the task of semantic segmentation, batches or augmentations are not required since it is a dense prediction task and we have multiple outputs to optimize, _i.e_. a prediction per pixel. In the case of VOS, we have even more outputs due to the temporal dimension and because we are sampling triplets among many frames. However, similar to Wang et al. , we observe that training only the batch normalization parameters is a requirement for \(tt\)-Ent to work.

#### 3.2.2 Mask auto-encoder (\(tt\)-AE) loss

The STCN model includes a mask encoder and a mask decoder to decode the mask representation coming from the cross-attention process. We repurpose these components for test-time training and compose an auto-encoder that reconstructs the input mask. The auto-encoder input is the provided ground-truth mask, _i.e_. \(m_{0}\), and the loss is given by \(L_{AE}=_{CE}(m_{0},d_{m}([e_{m}(m_{0}),e_{x}(x_{0})]))\). Note that, together with the mask encoder that gets optimized to better represent the specific object shape, we also optimize the frame encoder with the \(tt\)-AE loss, since the mask representation is concatenated with the frame representation in the input of the mask encoder .

[MISSING_PAGE_FAIL:6]

## 4 Experiments

In this section, we first describe the training and implementation details. Then, we introduce the DAVIS-C benchmark and report our experimental results on four datasets and two distribution shifts: sim-to-real transfer and corrupted/stylized test data.

Initial models. We start from two publicly available models for STCN. STCN-BL30K is the model trained on synthetic data from BL-30K . STCN-DY is a model initialized by STCN-BL30K and further trained using the training videos of DAVIS and YouTube-VOS, which include approximately 100K frames annotated with segmentation masks. We use the same two models for XMem, denoted by XMem -BL30K and XMem -DY, respectively.

Training details.We use learning rates \(10^{-5}\) and \(10^{-6}\) for models STCN-BL30K/ XMem -BL30K and STCN-DY/ XMem -DY, respectively since their training data differ significantly. Jump step \(s\) for sampling training frames is set to \(10\). For each test example, we train the models with \(tt\)-MCC and \(tt\)-Ent for 100 iterations and with \(tt\)-AE for 20, using the Adam  optimizer and a batch size of 4 sequences for STCN and 1 for XMem. We consider all target objects in the ground truth mask during TTT, and the raw videos are used with no augmentations. We develop our methods on top of the publicly available STCN2 and XMem3 implementations.

### Datasets and evaluation

We report results on the two most commonly used benchmarks for video object segmentation, the DAVIS-2017 validation set  and the YouTubeVOS-2018 validation set . We further report results on the validation set of the recent MOSE  dataset, which includes challenging examples with heavy occlusion and crowded real-world scenes. Finally, we introduce DAVIS-C, a variant of the DAVIS test set that represents distribution shifts via corruptions and stylization. We evaluate all methods using the widely established \(\&\) measure similar to prior work [9; 6].

DAVIS-C: corrupted and stylized DAVIS.To model distribution shifts during the testing, we process the set of test videos of the original DAVIS dataset to create DAVIS-C. This newly created test set offers a test bed for measuring robustness to corrupted input and generalization to previously unseen appearance. It is the outcome of processing the original videos to introduce image-level and video-level corruption and image-level appearance modification. We perform 14 different transformations to each video, each one applied at three different strengths, namely _low_, _medium_, and _high_ strength, making it a total of 42 different variants of DAVIS.

Figure 4: **The 14 types of corruption in DAVIS-C. Corruptions are depicted at “medium” strength.**For the image-level corruptions, we follow the paradigm of ImageNet-C and adopt 7 of their corruptions, namely gaussian noise, brightness, contrast, saturate, pixelate, defocus blur, and glass blur. The different transformation strengths are obtained by using severity values 3, 4, and 5 from ImageNet-C. For the video-level corruptions, we introduce constant rate factor (CRF) compression for three increasing values of compression and motion blur by averaging every 2, 3, and 4 consecutive frames. For the image-level transformations, we create cartoonization using the mean shift algorithm by increasing the amount of smoothing and image-stylization  using four different styles. For stylization, we do not vary the strength, but we preserve the original video colors (low and medium strength) while switching to those of the style image (high strength), making it a more drastic appearance shift. Frame examples for all cases are shown in Figure 4.

Some of the transformations in DAVIS-C do not perfectly represent video distortions. Nevertheless, several of the transformations constitute common real-world video edits (contrast, brightness, crf compression, cartoonization, stylization). We believe DAVIS-C provides a valuable benchmark to study video understanding under distribution shift.

### Results

Sim-to-real transfer.In Figure 4(a), we report results with and without test-time training on four datasets for the case of sim-to-real transfer. All three TTT variants bring gains consistently across datasets, with \(tt\)-MCC improving performance significantly, _i.e_. a relative gain of more than 39%, 15% and 15% for YouTube-VOS, DAVIS and MOSE, respectively. Moreover, we see that naively applying task-agnostic entropy minimization test-time training brings only a small percentage of the gains one can get. It is also worth noting that the gains from TTT are so high that performance using the mask cycle consistency variant becomes comparable to the performance of a model trained on video data. Specifically, \(tt\)-MCC recovers 82%, 72% and 44% of the performance gains that training with all the training videos from YouTube-VOS and DAVIS combined brings. This is very promising in cases where annotated data are scarce but synthetic data is widely available.

Corrupted test examples.We report results for DAVIS-C in Figures 4(b), 5, and the right side of Table 1. Figure 4(b) clearly shows how \(tt\)-MCC outperforms STCN for all corruption levels, with the gains increasing more as corruption strength increases. The entropy minimization approach does not offer any gains, while \(tt\)-AE is effective but worse than \(tt\)-MCC. In Figure 5, we report the \(\&\) score separately per video of DAVIS-C at the highest strength. We see that the gains come from many transformations and examples, while we see that TTT is able to improve most videos where the original \(\&\) score was in the lower side of the spectrum. Finally, on the right side of Table 1, we further report results on DAVIS-C for XMem and see that gains persist for one more state-of-the-art method. Interestingly, from Table 1, we also see that the performance gain of XMem over STCN disappears for the sim-to-real transfer case for YouTube-VOS, as well as for the extreme case a model trained only on synthetic videos is tested on corrupted inputs.

Figure 5: **VOS performance under distribution shifts.**_Left:_ performance of STCN-BL30K before and after test-time training for the sim-to-real transfer case on four datasets. _Right:_ performance of STCN-DY on DAVIS-C for input corruptions with different strength levels.

Comparison to the state of the art.In Table 1, we additionally report results for our \(tt\)-MCC variant when starting from the state-of-the-art XMem  method. Once again, the gains are consistent, for both cases. It is worth noting that from the fourth column of the table, we can see that \(tt\)-MCC is also able to slightly improve the case when there is no test-time distribution shift, _i.e_. we report 1.4 and 0.4 higher \(\&\) score on DAVIS for STCN-DY and XMem -DY, respectively. In the table, we also compare our method to HODOR  method that uses cycle consistency during the offline training stage and reports results when training without real videos. Our STCN-BL30K model with TTT outperforms HODOR by **+3.6** and **+8.3**\(\&\) score, on DAVIS and YouTube-VOS respectively.

Impact of sampling longer sequences.Regarding the sequence length used during test-time training, we adopt the setup used by the base methods during their training phase, _i.e_. frame triplets for STCN and octuplets for XMem. We experiment with quadruplets or quintuplets for mask cycle consistency on top of STCN which achieve performance of 81.7 and 82.4, respectively, on DAVIS versus 81.1 for triplets. This gain in performance, however, comes with a significant increase in test-time training time, _i.e_. 33% and 66%, respectively.

Impact of changing the sampling strategy.Additionally to changing the sequence length, another way to affect the size of the temporal context is by the value of the jump step, _i.e_. the interval used to sample frames \(x_{i}\) and \(x_{j}\) for a training triplet. Test-time training with \(tt\)-MCC over the STCN-BL30K model achieves 81.1 on DAVIS, with the jump step \(s\) parameter set to 10 by default. Varying the jump step from 1, 2, 25, and 50, achieves 80.6, 81.0, 80.8, and 79.0, respectively. Neither sampling too close to the first frame nor sampling frames too far from each other is the optimal choice. Switching to random sampling, without any step as a restriction, causes a performance drop to 79.7, while sampling only within the last 5% or 10% of the video also causes a drop to 79.8 and 79.7, respectively.

    &  &  \\  & DAVIS & DAVIS-C & YouTube-VOS & MOSE & no corr. & low & med & high & avg \\  HODOR  & 77.5\({}^{}\) & – & 71.7\({}^{}\) & – & 81.3\({}^{}\) & 69.0 & 64.5 & 55.2 & 65.0 \\  STCN  & 70.4 & 41.7 & 57.3 & 38.9 & 85.3 & 76.6 & 72.6 & 58.8 & 73.3 \\ STCN + \(tt\)-MCC (ours) & 81.1 & **70.1** & **79.4** & **44.9** & 86.7 & 78.3 & 75.6 & 67.3 & 77.0 \\  XMem  & 78.1 & 53.9 & 65.6 & 40.9 & 87.7 & 80.4 & 77.3 & 69.4 & 78.7 \\ XMem + \(tt\)-MCC (ours) & **82.1** & **70.1** & 78.9 & 44.7 & **88.1** & **81.7** & **78.9** & **72.2** & **80.2** \\   

Table 1: **Comparison to the state-of-the-art** for two cases of test-time distribution shift. Left part: Results when using models trained without real videos. The HODOR model is trained on COCO  images, while STCN and XMem on BL-30K. Right part: Results on DAVIS-C for different levels of corruption. HODOR, STCN and XMem are fine-tuned with DAVIS and YouTube-VOS videos. \({}^{}\) Results from .

Figure 6: **Test-time training on DAVIS-C**. We plot the \(\&\) score separately per video before (triangles) and after \(tt\)-MCC (circles) for the STCN-DY model on the 14 corruptions of the proposed DAVIS-C benchmark. We report results for the variants with the highest corruption strength. A red (grey) vertical line denotes that performance drops (increases) with test-time training.

Across all strategies and hyperparameter values, we see noticeable improvements using \(tt\)-MCC; the base model without TTT, merely achieves 70.4 \(\&\) score on DAVIS.

Impact of training iterations.For all results we use a fixed number of 100 iterations during TTT. In Figure 7, we present the performance improvement of \(tt\)-MCC for all 30 DAVIS videos (one curve per video) for a varying number of iterations. One can clearly see that optimal performance is achieved at different iterations per video, and we believe that adaptive early stopping is a promising future direction that may boost the overall improvement even further.

Impact on inference time.We discuss the effect that test-time training has on inference speed, by measuring average inference time over all DAVIS validation videos. The vanilla STCN model requires roughly 2 seconds per video. For \(tt\)-MCC, \(tt\)-Ent, and \(tt\)-AE, it takes 67, 29, and 5.3 seconds, respectively, for 100 iterations. Since TTT is run per video and its speed is only affected by the number of iterations, per frame timings vary with video length, _i.e._ the TTT overhead reduces as video length increases. It is worth noting here that all timings presented above are estimated with a non-optimized TTT implementation that leaves a lot of space for further optimization.

Reducing TTT overhead.As discussed, TTT induces a significant overhead at test time, which is an aspect not often discussed in the relevant literature. The TTT overhead is reduced by training for less iterations with only a minor decrease in performance. As we show Table 3 of the Appendix, using 20 instead of 100 iterations is even advantageous in cases, particularly for larger datasets like YouTube-VOS and MOSE with a model trained on real videos and without a distribution shift. In a real-world application, applying \(tt\)-MCC on all test examples might seem infeasible due to time overheads. Nevertheless, it is very useful for cases of extreme test-time distribution shifts. In scenarios involving a few out-of-distribution examples, where rapid adaptation of the current model is desired for improved performance, \(tt\)-MCC constitutes a straightforward and cost-effective solution, that is far more efficient than retraining the base model, which typically requires approximately 12.5 hours on 2 A100 GPUs to train STCN.

Memory requirements.During inference, STCN requires roughly 8GB of GPU RAM. Test-time training on top of STCN with \(tt\)-AE, \(tt\)-Ent and \(tt\)-MCC requires 8,16 and 23.5GB, respectively, _i.e._ it can still fit in a modest GPU. Note that the calculations for TTT are computed when using a batch size of 4 (the default). Memory requirements can be further reduced by using a smaller batch size, if needed, without significant change in performance.

## 5 Conclusions

In this work we show that test-time training is a very effective way of boosting the performance of state-of-the-art matching-based video object segmentation methods in the case of distribution shifts. We propose a mask cycle consistency loss that is tailored to matching-based VOS and achieves top performance. Its applicability goes beyond the two methods used in this work, as it is compatible with the general family of matching-based VOS methods [36; 60; 56; 1]. We report very strong gains over top performing methods for both sim-to-real transfer and the case of corrupted test examples. We also show that achieving such gains is not trivial and that it is important to tailor the test-time training method to the task and method at hand. A limitation of the proposed approach is the lack of a way for performing early stopping, and selecting the best iteration to stop training, a very promising direction for future work.

Figure 7: **Change in \(\&\) score during TTT** compared to the score before TTT. Each curve is one video and is normalized wrt. the maximum improvement/decline observed over 200 iterations.