# Bringing regularized optimal transport to lightspeed:

a splitting method adapted for GPUs

 Jacob Lindback

EECS, KTH

Stockholm, Sweden

jlindbac@kth.se Zesen Wang

EECS, KTH

Stockholm, Sweden

zesen@kth.se Mikael Johansson

EECS, KTH

Stockholm, Sweden

mikaelj@kth.se

###### Abstract

We present an efficient algorithm for regularized optimal transport. In contrast to previous methods, we use the Douglas-Rachford splitting technique to develop an efficient solver that can handle a broad class of regularizers. The algorithm has strong global convergence guarantees, low per-iteration cost, and can exploit GPU parallelization, making it considerably faster than the state-of-the-art for many problems. We illustrate its competitiveness in several applications, including domain adaptation and learning of generative models.

## 1 Introduction

Optimal transport (OT) is an increasingly important tool for many ML problems. It has proven successful in a wide range of applications, including domain adaptation , learning generative models , smooth ranking and sorting schemes , and long-tailed recognition . The versatility of OT stems from the fact that it provides a flexible framework for comparing probability distributions that incorporates the geometric structure of the underlying sample space . Early on, practitioners were directed to LP solvers with poor scalability to solve OT problems, but this changed dramatically with the introduction of entropic regularization and the Sinkhorn algorithm for OT. Sinkhorn's simple parallelizable operations resolved a computational bottleneck of OT and enabled the solution of large-scale problems at "lightspeed" . Despite the computational advantages of entropic regularization, many applications depend on sparse or structured transportation plans, and the bias that entropic regularization introduces can significantly impact the performance of the downstream task being solved. In such settings, other structure-promoting regularizers are typically preferred (see e.g. ). However, to the best of our knowledge, no framework exists that handles general regularizers for OT in a unifying way with a numerical efficiency that is comparable to Sinkhorn. To this end, we study OT for a broad class of regularizers and develop an algorithm with similar computational performance as Sinkhorn. Further, we benchmark our method against the state-of-the-art, showing that our algorithm achieves up to 100x speed-up for several applications.

For discrete probability distributions, solving regularized OT problems amounts to finding a solution to the optimization problem

\[*{minimize}_{X^{m n} _{+}}& C,X+h(X)\\ & X_{n}=p,\ X^{ }_{m}=q. \]

Here \(p\) and \(q\) are non-negative vectors that sum to \(1\), \(C\) is a non-negative cost matrix, and the regularizer \(h\) is a function that promotes structured solutions. To solve (1) fast, it is important that the large number of decision variables and the non-negativity constraint on the transportation plan are handled in a memory-efficient way. It is also crucial to manage the non-smoothness induced by the non-negativity constraints without altering the complexity of the algorithm. It is common tpractice to solve the OT problem by considering its dual since many regularization terms give rise to dual problems with structure that can be exploited . Most notably, entropic regularization, i.e. \(h(X)=_{ij}X_{ij} X_{ij}\), where \(>0\), enables deriving a simple alternating dual ascent scheme, which coincides with the well-known Sinkhorn-Knopp algorithm for doubly stochastic matrices . An advantage of this scheme is that it is easy to parallelize and has a low per-iteration cost . However, decreasing the regularization parameter \(\) will slow down the convergence of the algorithm and ultimately render the algorithm numerically unstable, which is particularly noticeable when low-precision arithmetic is used . Conversely, increasing \(\) makes the transportation plan blurrier - which can be problematic in applications when sparse solutions are desired .

For more general regularizers, it is often difficult to develop fast algorithms via the dual problem. However, if the regularizer is strongly convex, the dual (or semi-dual) will be smooth, and standard gradient-based methods can be used. This is computationally tractable when the gradients of the dual can be computed efficiently . Besides potentially expensive gradient computations, just as for Sinkhorn, lower regularization parameters will slow down the algorithm. Furthermore, regularizers that are not strongly convex cannot be handled in this framework. Therefore, we propose a different technique to solve (1) for a broad class of regularizers, including many non-smooth and non-strongly convex functions. Using the Douglas-Rachford splitting technique, we derive an algorithm with strong theoretical guarantees, that solves a range of practical problems rapidly and accurately. In particular, it efficiently handles regularizers that promote sparse transportation plans, in contrast to entropic regularization.

### Contributions

We make the following contributions:

* We adapt the Douglas-Rachford splitting technique to regularized optimal transport, extending the recent DROT algorithm proposed in .
* Focusing on a broad class of regularizers which includes quadratic regularization and group lasso as special cases, we demonstrate global convergence guarantees and an accelerated local linear convergence rate. This extends the available theoretical results for related OT solvers and captures the behavior of the iterates observed in practice.
* We develop an efficient GPU implementation that produces high-quality optimal transport plans faster than the conventional Sinkhorn algorithm. We then show how the proposed solver can be used for domain adaption and to produce solutions of better quality up to 100 times faster than the state-of-the-art when implemented on GPU.

### Related Work

The Sinkhorn algorithm is arguably one of the most popular OT solvers, and it can often find approximate solutions fast, even for large-scale OT problems. Many extensions and improvements have been proposed, including variations with improved numerical stability  and memory-efficient versions based on kernel operations . However, to our best knowledge, no framework exists for general regularizers that enjoy comparable computational properties. As an example, a standard approach for group-lasso regularization is to linearize the regularization term and use Sinkhorn iteratively . Although this approach is fairly generic, it requires that the transportation plan is recovered in every iteration, adding significant computational overhead. For strongly convex regularizers, such as quadratically regularized OT, several dual and semi-dual methods have been proposed, e.g. , with stochastic extensions  and non-convex versions to deal with cardinality constraints . However, these methods are significantly harder to parallelize, rendering them slower than Sinkhorn for larger problems . Moreover, just as for Sinkhorn, both convergence rates and numerical stability properties deteriorate significantly with lower regularization parameters.

A promising research direction that has recently attracted interest in the community is to consider splitting methods as an alternative to Sinkhorn for OT problems. For instance, an accelerated primal-dual method for OT and Barycenter problems was proposed in , proximal splitting for a particular OT discretization was explored in , and an algorithm for unregularized OT based on Douglas-Rachford splitting was developed in . The convergence of splitting methods is well-studied even for general convex problems . For Douglas-Rachford splitting, tight global linear convergence rates can be derived under additional smoothness and strong convexity assumptions . Global linear rates have also been established for certain classes of non-smooth and non-strongly convex problems [41; 1]. Moreover, Douglas-Rachford splitting can benefit from the inherent sparsity of the problem at hand. For certain classes of problems, these algorithms can identify the correct sparsity pattern of the solution in finitely many iterations, after which a stronger local convergence rate starts to dominate [23; 31]. This paper contributes to this line of research, by introducing a splitting method for regularized OT with strong theoretical guarantees and unprecedented computational performance.

#### Notation

For any \(X,Y^{m n}\), we let \( X,Y:=(X^{}Y)\) and \(\|X\|_{F}:=\). \(^{m n}_{+}\) and \(^{m n}_{-}\) denote the set of \(m n\) matrices with non-negative entries and non-positive entries respectively. We let \(_{S}\) denote the indicator function over a closed convex set \(S\), i.e \(_{S}(X)=0\) if \(X S\) and \(_{S}(X)=\) if \(X S\), and the relative interior of a set \(S\) is denoted \(S\). The projection onto \(^{m n}_{+}\) is denoted \([]_{+}\), which sets all negative entries of the inputted matrix to zero. The subdifferential of an extended-real valued function \(h\) is denoted \( h(X)\), and its proximal operator is defined \(_{ph}(x)=_{z}h(z)+ {2}\|z-x\|^{2}\), where \(>0\)

## 2 Regularized Optimal Transport

We consider regularized OT problems on the form (1) where the function \(h\) is _sparsity promoting_ in the sense that its value does not increase if an element of \(X\) is set to zero.

**Definition 2.1** (Sparsity promoting regularizers).: \(h:^{m n}\{+\}\) _is said to be sparsity promoting if, for any \(X^{m n}\), \(h(X) h(X_{s})\) for every \(X_{s}^{m n}\) with \((X_{s})_{ij}\{0,\,X_{ij}\}\)._

Notice that this function class does not necessarily induce sparsity. For instance \(h=0\), or \(h=\|\|_{F}^{2}\) meet the conditions of Definition 2.1. Besides these two examples, the class of sparsity promoting functions include, but are not limited to, the following functions.

* \(h(X)=_{g}\|X_{g}\|_{F}\) (group lasso OT)
* \(h(X)=_{ij}X_{ij}(X_{ij}/)-(X_{ij}^{2}+^{2})^{1/2}\) (hypentropic regularization)
* \(h(X)=_{ij}w_{ij}|X_{ij}|\), where \(w_{ij} 0\) (weighted \(_{1}\)-regularization)
* \(h(X)=_{(ij)}_{X_{ij}=0}(X)\) (constrained OT)

Conic combinations of sparsity promoting functions are also sparsity promoting. Moreover, many regularized OT problems, such as Gini-regularized OT  and regularizers on the form \(\|X-A\|_{F}^{2}\), can be converted to only involve sparsity promoting regularizers. Note that the negative Shannon entropy \( X_{ij} X_{ij}\) is not sparsity promoting.

In this paper, we will develop a scalable algorithm for solving OT problems with sparsity-promoting regularizers. Besides strong theoretical guarantees, the algorithm is numerically stable and can be implemented efficiently on a GPU. Our approach builds on the recently proposed DROT algorithm for unregularized OT . We extend the DROT algorithm to regularized OT problems, improve the theoretical convergence guarantees, and develop an extended GPU kernel. The resulting algorithm solves regularized OT problems faster, with higher precision, and in a more generic fashion than the state-of-the-art. To explain the algorithm, we review the splitting method for OT developed in .

### Douglas-Rachford splitting for OT

Douglas-Rachford splitting is a technique for solving optimization problems on the form

\[*{minimize}_{x^{n}}\;f(x)+g(x) \]

using the fixed point update \(y_{k+1}=T(y_{k})\), where

\[T(y)=y+_{ g}(2_{ f}(y )-y)-_{ f}(y) \]and \(>0\) is a stepsize parameter. If \(y^{}\) is a fixed point of \(T\) then \(x^{}=_{ f}(y^{})\) is a solution to (2). Often a third iterate, \(z_{k}\), is introduced, and the Douglas-Rachford iterations are expressed as

\[x_{k+1}=_{ f}(y_{k}), z_{k+1}=_{  g}(2x_{k+1}-y_{k}) y_{k+1}=y_{k}+z_{k+1}-x_{k+1}. \]

As long as \(f\) and \(g\) in (2) are closed and convex (possibly extended-real valued), and an optimal solution exists, the DR-splitting method converges to a solution (see, e.g. Theorem 25.6 in ). Under additional assumptions on \(f\) and \(g\), the method has even stronger convergence guarantees . For a given problem that can be formulated on the form (2), there are typically many ways to partition the problem between \(f\) and \(g\). This must be done with care: a poor split can result in an algorithm with a higher per-iteration cost than necessary or one which requires more iterations to converge than otherwise needed. It is often difficult to achieve both simultaneously, which poses a challenging trade-off between the iterate complexity and the per-iteration cost of the resulting algorithm.

To facilitate the derivation of the algorithm, we let \(=\{X^{m n}:X_{n}=p,\,X^{}_{m}=q\}\) and introduce the indicator functions \(_{_{+}^{m n}}\) and \(_{}\), that correspond to the constraints of (1). Recognizing that the projection onto \(\) is tractable (by invoking recently derived formulas for matrix projections ) the authors of  proposed the splitting

\[f(X)= C,X+_{_{+}^{m n}}(X)\ \ \ \ \ g(X)=_{}(X)\]

and demonstrated that the update (3) can be simplified to

\[X_{k+1}=[Y_{k}- C]_{+}, Y_{k+1}=X_{k+1}+_{k+1}_{n}^{ }+_{m}_{k+1}^{} \]

where \(_{k}\) and \(_{k}\) are vectors given via the iterative scheme:

\[r_{k+1}=X_{k+1}_{n}-p, s_{k+1}=X_{k+1}^{} _{m}-q,_{k+1}=f^{}r_{k+1}/(m+n),\] \[_{k+1}=_{k}-_{k+1}, a_{k+1}=a_{k}-r_{k+1},  b_{k+1}=b_{k}-s_{k+1},\]

\[_{k+1}=(a_{k}-2r_{k+1}+(2_{k+1}-_{k})_{m})/n,\] \[_{k+1}=(b_{k}-2s_{k+1}+(2_{k+1}-_{k}) _{n})/m. \]

The most expensive operations in (6) are the matrix-vector products in the \(r\) and \(s\) updates. Nonetheless, these operations can efficiently be parallelized on a GPU. Further, notice that \(Y_{k}\) can be eliminated by substituting it in the \(X\)-update of (5), giving

\[X_{k+1}=[X_{k}+_{k}_{n}^{}+_{m}_{k}^{}-  C]_{+}.\]

Hence, the splitting scheme only needs to update the transportation plan \(X_{k}\) and the ancillary vectors and scalars in (6).

## 3 DR-splitting for regularized OT

When extending the DR-splitting scheme to the regularized setting, it is crucial to incorporate \(h\) in one of the updates of (4) in a way that makes the resulting algorithm efficient. Specifically, if we let

\[f(X)= C,X+_{_{+}^{m n}}(X)+h(X),g(X)=_{}(X), \]

the first update of (4) reads:

\[X_{k+1}=_{ f}(Y_{k})=_{ h+_ {_{+}^{m n}}}(Y_{k}- C).\]

This update is efficient given that \(_{ h+_{_{+}^{m n}}}()\) is easy to compute. This is indeed the case when \(h\) is a sparsity promoting. We present the result in Lemma 3.1.

**Lemma 3.1**.: _Let \(f(X)= C,X+_{_{+}^{m n}}(X)+h(X)\) where \(h(X)\) is sparsity promoting, closed, convex and proper when \(X_{+}^{m n}\). Then \(_{ f}(X)=_{ h}([X- C]_{ +})\)._

Lemma 3.1 enables us to integrate the regularizer into the algorithm with only a minor update. Using the split of (7), the updated Douglas-Rachford algorithm, which we will refer to as RDROT, follows:

\[X_{k+1}=_{ h}([Y_{k}- C]_{+}), Y_{k+1}=X_ {k+1}+_{k+1}_{n}^{}+_{m}_{k+1}^{}. \]Under the assumption that \(h\) is closed, convex, and proper over the feasible set of (1), Theorem 25.6 in  guarantees that RDROT converges to a solution of (1). Before we derive even stronger convergence properties of the algorithm, we describe a few specific instances in more detail.

**Quadratic Regularization.** Letting \(h(X)=\|X\|_{F}^{2}\) yields

\[X_{k+1}=(1+)^{-1}[Y_{k}- C]_{+} \]

An attractive feature of quadratic regularization is that it shares similar limiting properties as entropic regularization when used in OT-divergences . In contrast to entropically regularized OT, letting \( 0\) does not lead to numerical instability in our method. Furthermore, the quadratic term makes the objective of (1) strongly convex which renders its optimal solution unique. This can be helpful when OT is used to define a loss function for e.g. training generative models  or adjusting for long-tailed label distributions , since it preserves some sparsity and renders the OT cost differentiable (see Section 3.3 for details).

**Group Lasso Regularization.** With \(h(X)=_{g}\|X_{g}\|_{F}\), where \(\) is a collection of disjoint index sets, the RDROT update becomes

\[_{k+1}=[Y_{k}- C]_{+}, X_{k+1,g}=[1-_{k+1,g}\|_{F}}]_{+}_{k+1,g}, g . \]

This regularizer has been used extensively for OT-based domain adaptation . The rationale is that each unlabeled data point in the test (target) domain should only be coupled to data points of the training (source) domain that share the same label. This can be accomplished by organizing the data so that the rows of \(X\) correspond to data points in the training domain and the columns to points in the test domain, and then using a sparsity-inducing group-lasso regularizer in (1). In this setting, \(\) is a collection of sets, each specifying which columns correspond to a particular label. Consequentially, this regularizer will promote solutions that map each data point in the test domain to a single label.

### Rate of convergence

It is well-known that DR-splitting finds an \(\)-accurate solution in \(O(1/)\) iterations for general convex problems . This is a significant improvement over Sinkhorn's iteration complexity of \(O(1/^{2})\), but the Sinkhorn iterations are very cheap to execute. A key contribution of  was the development of a GPU kernel that executes the DR updates so quickly that the overall method is faster than Sinkhorn for many problem instances. They also demonstrated a global linear convergence by exploiting the geometrical properties of LPs, but the convergence factors are very difficult to quantify or use for step-size tuning. As illustrated in Figure 1, and discussed in more detail below, neither the global linear rate nor the \(1/k\)-rate are globally tight. Instead, the stopping criterion decays as \(O(1/k)\) during the initial steps of the RDROT algorithm, but then begins to converge at a fast linear rate. Hence, existing convergence analyses that neglect this local acceleration are unsatisfactory.

In this section, we will develop a convergence analysis for DR splitting on regularized OT problems that explains this observed behavior. Based on previous work on local convergence properties of

Figure 1: An illustration of the convergence behavior of the algorithm for a quadratically regularized problem. The stopping criterion and the number of positive entries were computed in every iteration. Notice that the iteration number at which the sparsity pattern stabilizes is aligned with when the linear convergence starts. In this example, the stepsize is chosen to make this effect more evident.

DR-splitting , we establish that our algorithm (8) identifies the true sparsity pattern of the solution in finitely many iterations. When the sparsity pattern is identified, a fast linear rate typically starts to dominate over the sublinear rate. To derive these guarantees, we need one additional assumption on the optimal solution of (1).

**Assumption 1**.: _Let \(Y^{}\) be a fixed point of (3) such that \(Y_{k} Y^{}\), and let \(X^{}=_{ f}(Y^{})=_{ h}([Y^{}- C]_{+})\). We then assume that:_

\[Y^{}-X^{}  f(X^{}). \]

Assumption 1 can be seen as a stricter version of the traditional optimality condition. It is a common assumption in analyses of active constraint identification that can be traced back to . We consider this assumption to be fairly weak since it holds for most relevant OT problems, except for some very specific cost matrices. A more elaborate discussion is included in the supplementary material.

Under the regularity conditions of (11), we derive the following two convergence results.

**Theorem 1**.: _Let \(h(X)\) be sparsity promoting, convex, closed, and twice continuously differentiable for \(X>0\). If Assumption 1 holds, then there is a \(K 1\) such that for all \(k K\), \(X_{k,\,ij}=0\) if and only if \(X^{}_{ij}=0\)._

For our problem, this result implies that there is a \(K\) after which the sparsity pattern does not change. To prove this, we first show that the sparsity-promoting and smoothness assumptions imply that \(f\) and \(g\) are partly smooth with respect to two manifolds and then invoke results presented in . Moreover, Theorem 1 allows us to derive the following local convergence rate guarantees:

**Theorem 2**.: _Assume that the conditions stated in Theorem 1 hold. If \(h\) is locally polyhedral, then RDROT enjoys a local linear rate: When \(k K\), then_

\[\|X_{k+1}-X^{}\|_{F} r^{k-K}\|Y_{K}-Y^{}\|_{F}, r(0,\,1).\]

_Moreover, the optimal rate is independent of the stepsize \(\)._

The regularizer \(h\) is locally polyhedral, for instance, in the unregularized setting, and when weighted \(_{1}\) regularization is used (i.e. \(h(X)=_{ij}w_{ij}|X_{ij}|\), where \(w_{ij} 0\)). Therefore, one can expect a sublinear rate until the correct sparsity pattern of the transportation plan is identified. From that point onward, a fast linear rate will dominate. The local rate holds for many other regularizers beyond locally polyhedral ones, which has also been observed in experiments (see, _e.g._,  and ). However, when the local polyhedrality assumption is relaxed, one must account for the Hessian information of the regularizer, which makes it difficult to quantify a rate using this proof technique.

Stepsize selectionThe stepsize implicitly determines how the primal residual descent is balanced against the descent in the duality gap. When the cost has been normalized so that \(\|C\|_{}=1\), a stepsize that performs well regardless of \(h\) is \(=2(m+n)^{-1}\). This stepsize was proposed for DR-splitting on the unregularized problem in . We use this stepsize in all our numerical experiments and observe that our method is consistently competitive without any additional tuning.

InitializationA reasonable initialization of the algorithm is to let \(X_{0}={pq}^{}\), and \(_{0}=_{m}\), \(_{0}=_{n}\). However, when using the stepsize specified above, this will result in \(X_{k}=_{m n}\) for \(1 k N\), where \(N=O((m,n))\). By initializing \(_{0}=(3(m+n))^{-1}(1+m/(m+n))_{m}\), and \(_{0}=(3(m+n))^{-1}(1+n/(m+n))_{n}\), one skips these first \(N\) iterations, resulting in a considerable speed-up the algorithm, especially for larger problems. The derivation is added to the supplementary material. There is potential for further improvements (see e.g. ), since \(^{-1}_{k}\), \(^{-1}_{k}\) are related to dual potentials. But for simplicity, we use this strategy throughout the paper.

Stopping criteriaOne drawback of the Sinkhorn algorithm is that its theoretically justified suboptimality bounds are costly to compute from the generated iterates. This is usually addressed by only computing the primal residuals \(\|X_{k}_{m}-p\|\), and \(\|X_{k}^{}_{n}-q\|\) every \(M\) iterations and terminate when both residuals are less than a chosen tolerance. In contrast, our algorithm computes the primal residuals in every iteration (see \(r_{k}\) and \(s_{k}\) in (6)), so evaluating the stopping criterion does not add any extra computations. Moreover, the iterates \(_{k}\) and \(_{k}\) will, in the limit, be proportional to the optimal dual variables, and can hence be used to estimate the duality residual and duality gap in every iteration. For more details, we refer to the supplementary material.

### GPU implementation

Similar to the approach developed in , the RDROT algorithm can be effectively parallelized on a GPU, as long as the regularizer is sparsity promoting and its prox-mapping is easy to evaluate. We have developed GPU kernels for RDROT with quadratic and group-lasso regularization, respectively1. Below, we provide an overview of how the DROT-kernel is adapted for the regularized case. More details of the implementation are included in the supplementary material.

For the GPU implementation, the main computation time is spent on updating the \(X\) matrix as described in (8). Compared with DROT , the RDROT kernel is also required to evaluate the prox-mapping. Specifically, for the given regularizers, the following additional operations are needed:

* **Quadratic regularization:** Each element of \([Y_{k}- C]_{+}\) needs to be rescaled with a constant to update \(X_{k}\) (see (9)). This only requires a minor change and results in practically the same per-iteration-cost as for the unregularized case.
* **Group-lasso regularization:** The norm of each group, i.e \(_{k+1,g},g\) is required to the determine a group-specific scaling factor (see (10)). To this end, the following additional steps are required: (1) a reduction for gathering the square of the elements and computing the scale for the group; and (2) a broadcast to apply the scale to \(_{k+1,g}\). For small problem sizes, the reduction and the broadcast can be done within a thread block via its shared memory. For large problem sizes, when the elements in one group can not fit in a single thread block, the results of the reduction are stored in global memory, and an additional kernel function is introduced to apply the scale for the group. This overhead increases the per-iteration cost a little, but we show in our experiments that it is still significantly faster compared to other methods.

The remaining parts of the algorithm are for updating vectors, which have much smaller time complexity than the \(X\) matrix updates. These are handled with an ancillary kernel.

### Backpropagation via Pytorch and Tensorflow wrappers

In many applications, e.g. [17; 35; 29], it is useful to use the solution of (1) to construct a loss function. Most notably, the optimal value of (1) is directly related to the Wasserstein distance between \(p\) and \(q\), which is a natural measure of the proximity between distributions with many advantageous properties (see  for an extensive treatment). When training generative models, for example, the objective is to transform a simple distribution into one that resembles the data distribution. It is therefore natural to use the Wasserstein distance to construct loss functions, see e.g . But in order to backpropagate through such loss functions, one must differentiate the optimal value of (1) with respect to the cost matrix \(C\) (which is parameterized by both a data batch and a generated batch of samples). Using Fenchel duality, we can show that the optimal solution \(X^{}\) is a gradient (or Clarke subgradient) of the loss, regardless of the regularizer. By applying a similar argument to the dual problem, one can show that \(^{-1}(^{};^{})\) is a reasonable gradient estimate of the OT cost with respect to the marginals \((p;q)\). Hence, one can use RDROT for both the forward and backward pass and do not need to lean on memory-expensive unrolling schemes or computationally expensive implicit differentiation techniques. We include further details on the derivation in the supplementary material. To facilitate using RDROT for DL, we wrapped our GPU OT solvers as PyTorch and TensorFlow extensions that feature fast automatic differentiation. The extensions can easily be integrated with networks that parameterize cost matrices or marginals. In a forward pass, our extension solves the regularized OT problem using the cost matrix and marginals given at the current iteration and stores the approximate solution \(X^{}_{k}\) and \(^{-1}(^{};^{})\) for the backward pass. The aforementioned technique is subsequently used in the backward pass to efficiently obtain an approximate gradient, which is used to rescale the gradients of the weights of the remaining network.

## 4 Numerical experiments

All experiments mentioned below are performed using a single NVIDIA Tesla V100 GPU.

### GPU performance of RDROT with quadratic regularization

To illustrate the numerical advantages of the algorithm, we compared the GPU implementation of RDROT for quadratically regularized OT (which we will refer to as QDROT) with a dual ascent method (L-BFGS), proposed in , which we implemented in PyTorch with all tensors loaded on the GPU. Although there are several other solvers for quadratic OT, such as the semi-dual method proposed in  and the semi-smooth Newton method from , we have omitted them from our experiments since their associated gradient/search direction computations scale poorly with data size and are difficult to parallelize. In our benchmark, we simulated 50 data sets of size \(500 500\) and \(5000 5000\), respectively. We generated cost matrices with \(C_{ij}=\|_{i}-_{j}\|_{2}^{2}\), where \(_{i}\), \(_{j}\) are simulated 2D Gaussian variables with random parameters specific for each data set. We used the quadratic regularization term \(((m+n)/2)\|X\|_{F}^{2}\) with \(\) ranging between \(0.0005\) to \(0.2\). For QDROT, we included \(=0\) for reference. We ran the algorithms for each dataset and regularization parameter until they reached an accuracy level of \(0.0001\). For completeness, we also benchmark a PyTorch implementation of QDROT. The results are displayed in Figure 2, in which it is clear that our method is substantially faster than the L-BFGS method. Furthermore, when the GPU kernel is used, an even more significant speedup is achieved compared to the L-BFGS method. Further details and additional experiments with other dataset sizes are included in the supplementary material.

### Group Lasso and Domain Adaptation

OT is the backbone of many domain adaption techniques since it provides a natural framework to estimate correspondences between a source domain and a target domain. These correspondences can subsequently be used to align the respective domains to improve the test accuracy. Specifically, for a given source data set \(\{(_{s}^{i},y^{i})\}_{i=1}^{m}\) and a target set \(\{_{t}^{j}\}_{j=1}^{n}\), where \(\{y^{i}\}_{i=1}^{m}\) are data labels, we define the cost matrix elements \(C_{ij}=d(_{s}^{i},_{t}^{j})\) using a positive definite function \(d\). By computing a transportation plan \(X\) associated with (1) and cost matrix \(C\), one can adapt each data point in the source set into the target domain via \(_{s,a}^{i}=}_{j=1}^{m}X_{ij}_{t}^{j}\). It has been shown that group-lasso regularization can improve the adaptation  (cf. the discussion in SS3). To handle the regularizer, it is customary to linearize it and iteratively solve updated OT problems with Sinkhorn [9; 10]. The current implementation of group-lasso OT in the Python OT package POT uses this approach . Besides the computational cost of iteratively reconstructing transportation plans, entropic regularization tends to shrink the support of the adapted training data and hence underestimate the true spread in the target domain. This is thoroughly discussed in  and illustrated in Figure 3 where we adapt data sets with different regularizers. Of course, the shrinkage can partially be addressed by decreasing the regularization parameter, but this tends to slow down that algorithm and may even lead to numerical instability. To shed some light on this trade-off, we simulated \(50\) datasets with two features, 1500 training samples, and 1000 test samples. The target domain was simulated via a random affine transformation of the source domain. Each set had 2 unique labels that were uniformly distributed among the instances. The labels in the test set were only used for validation. The cost \(C_{ij}=\|_{s}^{i}-_{t}^{j}\|^{2}\) was used and normalized so that \(\|C\|_{}=1\). To compare the performance of

Figure 2: Comparison between the RDROT (QDROT) using the GPU kernel, a PyTorch version, and an L-BFGS method applied to the dual for different quadratic regularization parameters run on GPU. 50 datasets of two different sizes were simulated.

the algorithms, we computed the time taken to reach a tolerance of \(10^{-4}\) and evaluted the Wasserstein distance between the adapted samples of each label and the corresponding samples of the test set. This means that the better the alignment, the lower the aggregated distance. For the method using entropic regularization, we varied the regularization parameters \(0.001\) to \(10\), and the group lasso regularization was set to \(0.001\). The results are presented in Table 1. Indeed, increasing the entropic regularization parameter to speed up the conditional gradient methods will worsen the alignment between the source domain and the target domain, just as illustrated in Figure 3. We also note that our method consistently outperforms the alternative methods, both in terms of adaptation quality and in terms of time to reach convergence. Additional experiments with other problem sizes and regularization are consistent with Table 1, see the supplementary.

In addition to the experiments above, we compare the per-iteration costs for several OT solvers in the supplementary material. These strengthen our claim further that our algorithm achieves "lightspeed" even for large problem sizes.

#### Training of generative models

As an illustration of the efficiency of the GPU version of the algorithm, we use our PyTorch wrapper to implement the Minibatch Energy Distance , a loss function for training generative models via OT. The network consists of a generator, and a network that learns a cost function, and the OT-based Minibatch Energy distance to quantify the similarity between data batches and generated artificial samples. We performed experiments with image generation on the MNIST and CIFAR10 datasets, with some of the generated samples shown in Figure 4. The details of the experiments are included in the supplementary material. The resulting generator gives sharp images, and the training time was better or comparable with alternative frameworks, despite the low amount of regularization used.

## 5 Conclusions

The ability to solve large regularized optimal transport problems quickly has the potential to improve the performance of a wide range of applications, including domain adaption, learning of generative models, and more. In this paper, we have shown that the Douglas-Rachford splitting technique can handle a broad class of regularizers in an integrated and computationally efficient manner. Using a carefully chosen problem split, we derived an iterative algorithm that converges rapidly and reliably,

   &  &  & \) dist. \(\)} \\  & Ent & GL & Median & \(q10\) & \(q90\) & Median & \(q10\) & \(q90\) \\  GLSK  & 1e-3 & 1e-3 & 3.77 & 3.68 & 8.90 & 0.311 & 0.0657 & 6.48 \\ GLSK  & 1e-1 & 1e-3 & 3.73 & 3.71 & 3.77 & 8.24 & 3.47 & 31.6 \\ GLSK  & 1e+2 & 1e-3 & 1.86 & 1.86 & 1.88 & 52.8 & 10.9 & 311 \\  GLDROT (ours) & - & 1e-3 & 0.0619 & 0.0475 & 0.0771 & **0.0576** & 0.0262 & 0.182 \\ GLDROT (ours) & - & 5e-3 & **0.0384** & 0.0306 & 0.0474 & 0.0879 & 0.0358 & 0.274 \\  

Table 1: Performance benchmark of RDROT for Group-Lasso regularization (GLDROT) against the conditional-gradient based Sinkhorn algorithm. Median and \(10\)th and \(90\)th percentile statistics are included. Our method is competitive in terms of both speed and adaption quality.

Figure 3: A toy example illustrating the effect strongly convex regularizations have on the support of the resulting adapted domains. Pink and purple points mark the labeled training set and the gray points the test domain. Notice how the Sinkhorn-based method, as well as the Quadratic Regularized method transfers the source domain samples towards the center of the target domain, while our approach transfers the samples in a way that matches the true variation of the test data

and whose operations are readily parallelizable on GPUs. The performance of our algorithm is supported by both theoretical arguments and extensive empirical evaluations. In particular, the experiments demonstrate that the resulting algorithms can be up to two orders of magnitude faster than the current state-of-the-art. We believe that this line of research has the potential to make regularized OT numerically tractable for a range of tasks beyond the ones discussed in this paper.