# Provably (More) Sample-Efficient Offline RL with Options

Xiaoyan Hu

Department of Computer Science and Engineering

The Chinese University of Hong Kong

Hong Kong SAR, China

xyhu21@cse.cuhk.edu.hk

&Ho-fung Leung

Independent Researcher

Hong Kong SAR, China

ho-fung.leung@outlook.com

###### Abstract

The options framework yields empirical success in long-horizon planning problems of reinforcement learning (RL). Recent works show that options improves the sample efficiency in _online_ RL where the learner can actively explores the environment. However, these results are no longer applicable to scenarios where exploring the environment online is risky, e.g., automated driving and healthcare. In this paper, we provide the first analysis of the sample complexity for offline RL with options, where the agent learns from a dataset without further interaction with the environment. We propose the **PE**ssimistic **V**alue **I**eration for Learning with **O**ptions (PEVIO) algorithm and establish near-optimal suboptimality bounds (with respect to the novel information-theoretic lower bound for offline RL with options) for two popular data-collection procedures, where the first one collects state-option transitions and the second one collects state-action transitions. We show that compared to offline RL with actions, using options not only enjoys a faster finite-time convergence rate (to the optimal value) but also attains a better performance (when either the options are carefully designed or the offline data is limited). Based on these results, we analyze the pros and cons of the data-collection procedures, which may facilitate the selection in practice.

## 1 Introduction

Planning in long-horizon tasks is challenging in reinforcement learning (RL) (Co-Reyes et al., 2018; Eysenbach et al., 2019; Hoang et al., 2021). A line of study proposes to accelerate learning in these tasks using temporally-extended actions (Fikes et al., 1972; Sacerdoti, 1973; Drescher, 1991; Jiang et al., 2019; Nachum et al., 2019; Machado et al., 2021; Erraqabi et al., 2022). One powerful approach is the _options_ framework introduced by Sutton et al. (1999), where the agent interacts with the environment with closed-loop policies called _options_. Empirical success (Tessler et al., 2017; Vezhnevets et al., 2017) shows that options help achieve sample-efficient performance in long-horizon planning problems.

To provide a theoretical guarantee to the options framework, recent works have focused on the sample complexity of RL with options in the _online_ setting, where the agent continuously explores the environment and learns a _hierarchical policy_ to select options. Brunskill and Li (2014) establish a PAC-like sample complexity of RL with options in the semi-Markov decision processes (SMDPs), where temporally-extended actions are treated as indivisible and unknown units. Later, Fruit and Lazaric (2017) provide the first regret analysis of RL with options under the Markov decision processes (MDPs) framework. While their proposed algorithm attains a sublinear regret, it requires prior knowledge of the environment, which is not usually available in practice. To address this problem, Fruit et al. (2017) propose an algorithm that does not require prior knowledge, yet achievesa near-optimal regret bound. However, these results are inapplicable to many real-world scenarios where online exploration is not allowed. For example, it has been argued that in healthcare (Gottesman et al., 2019) and automated driving (Shalev-Shwartz et al., 2016), learning in an online manner is risky and costly. In these scenarios, _offline_ learning, where the agent learns a policy from a dataset, is preferred. We note that there is a line of studies on the sample complexity of offline RL with primitive actions only (i.e., without the use of options) (Levine et al., 2020; Fu et al., 2020; Rashidinejad et al., 2021). Unfortunately, to the best of our knowledge, there have been no results reported on the offline RL with options.

In this paper, we make the following contributions. First, we derive a novel information-theoretic lower bound, which generalizes the one for offline learning with actions. Second, we propose the **PE**ssimistic **V**alue **I**teration for Learning with **O**ptions (PEVIO) algorithm and derive near-optimal suboptimality bounds for two popular data-collection procedures, where the first one collects state-option transitions and the second one collects state-action transitions. More importantly, we show that options facilitate more sample-efficient learning in both the finite-time convergence rate and actual performance. To shed light on offline RL with options in practice, we discuss the pros and cons of both data-collection procedures based on our analysis.

## 2 Related Work

Learning with OptionsBuilding upon the theory of _semi-Markov decision processes_ (SMDPs) (Bradtke and Duff, 1994; Mahadevan et al., 1997), Sutton et al. (1999) propose to learn with options. Following their seminal work, learning with options has been widely studied in the function approximation setting (Sorg and Singh, 2010) and hierarchical RL (Igl et al., 2020; Klassarov and Precup, 2021; Wulfmeier et al., 2021). Discovering useful options has also been the subject of extensive research (Stolle and Precup, 2002; Riemer et al., 2018; Mankowitz et al., 2018; Harb et al., 2018; Hiraoka et al., 2019; Bagaria et al., 2021). Despite its empirical success, there have been fairly limited studies on the sample efficiency of learning with options. Brunskill and Li (2014) analyze the sample complexity bound for an RMax-like algorithm for SMDPs. Fruit and Lazaric (2017) derive the first regret analysis of learning with options. They propose an algorithm that attains sublinear regret in the infinite-horizon average-reward MDP while requiring prior knowledge of the environment. Later, Fruit et al. (2017) remove this requirement.

Offline RLIn the offline setting, a dataset that is collected by executing a _behavior policy_ in the environment is provided, and the agent is asked to learn a near-optimal policy using only this dataset. A key challenge in offline RL is the insufficient coverage of the dataset (Wang et al., 2021), which is also known as distributional shift (Chen and Jiang, 2019; Levine et al., 2020). To address this problem, the previous study on sample-efficient learning assumes uniform coverage of the dataset (Liu et al., 2018; Chen and Jiang, 2019; Jiang and Huang, 2020; Yang et al., 2020; Xie and Jiang, 2020; Uehara et al., 2020; Qu and Wierman, 2020; Yin et al., 2021). This assumption is relaxed in recent works by pessimism principle (Xie et al., 2021; Rashidinejad et al., 2021; Jin et al., 2021).

## 3 Preliminaries

### Episodic MDP with Options

Let \(()\) denote the probability simplex on space \(\) and \([N]:=\{1,,N\}\) for any positive integer \(N\). An episodic MDP with options is a sextuple \(=(,,,H,,r)\), where \(\) is the state space, \(\) the (primitive) action set, \(\) the finite set of options, \(H\) the length of each episode, \(=\{P_{h}:()\}_{ h[H]}\) the transition kernel, \(r=\{r_{h}:\}_{h[H]}\) the deterministic reward function.1 We define \(S:=||\), \(A:=||\), and \(O:=||\). A (Markov) _option_\(o\) is a pair \((^{o},^{o})\) where \(^{o}=\{_{h}^{o}:()\}_{h[H]}\) is the option's policy and \(^{o}=\{_{h}^{o}:\}_{h[H]}\) is the probability of the option's _termination_. For convenience, we define \(_{H+1}^{o}(s)=1\) for all \((s,o)\), i.e., any option is terminated after the end of an episode. We assume that the initial state \(s_{1}\) is _fixed_.2 Upon arriving at state \(s_{h}\) at any timestep \(h[H]\), if \(h=1\) (at the beginning of an episode), the agent selects option \(o_{1}_{1}(|s_{1})\), where \(=\{_{h}:()\}_{h[H]}\) is a _hierarchical policy_ to select an option at each state. Otherwise (\(h 2\)), the agent first terminates option \(o_{h-1}\) with probability \(_{h}^{o_{h-1}}(s_{h})\). If option \(o_{h-1}\) is terminated, she then selects a new option \(o_{h}_{h}(|s_{h})\) according to the hierarchical policy \(\). If option \(o_{h-1}\) is not terminated, the agent continues to _use_ option \(o_{h-1}\) at timestep \(h\), i.e., \(o_{h}=o_{h-1}\). After that, the agent takes action \(a_{h}_{h}^{o_{h}}(|s_{h})\), receives a reward \(r_{h}:=r_{h}(s_{h},a_{h})\), and transits to the next state \(s_{h+1} P_{h}(|s_{h},a_{h})\). An episode terminates at timestep \(H+1\). A special case is that an action \(a\) is an option \(o\), such that \(_{h}^{o}(a|s)=1\) and \(_{h}^{o}(s)=1\) for any \((h,s)[H]\). For convenience, we use the notation \(=\) to represent that each option corresponds to an action, which is the case in RL with primitive actions.

To define the \(Q\)-function and the value function, we introduce some useful notations.3 Let \(T=\{T_{h}:([H-h+1]) \}_{h[H]}\) and \(U=\{U_{h}:[0,H]\}_{h[H]}\) denote the _option transition function_ and the _option utility function_, respectively. Particularly, for any \((h,s,o)[H]\), the option transition function \(T_{h}(s^{}|s,o,)\) is the probability that the agent uses option \(o\) at state \(s\) at timestep \(h\), reaches state \(s^{}\) at timestep \(h+\) without terminating option \(o\) in these \(\) timesteps, and finally terminates option \(o\) at state \(s^{}\) at timestep \(h+\). The option utility function \(U_{h}(s,o)\) is the expected cumulative reward within timesteps that the option is used without being terminated. Given any arbitrary series of functions \(\{y_{h}:\}_{h[H]}\), define the operator \([T_{h}y_{h+}](s,o):=_{s^{}}T_{h}(s^{}|s,o, )y_{h+}(s^{})\) for any \((s,o,)[H-h+1]\). In the following, we derive the \(Q\)-function and the value function for learning with options. (The detailed proof can be found in Appendix B.)

**Theorem 1** (\(Q\)-function and value function).: _For any hierarchical policy \(\) and \((h,s,o)[H]\), the \(Q\)-function is given by_

\[Q_{h}^{}(s,o):= _{}[_{h^{}=h}^{H}r_{h^{}}(s_{h^ {}},a_{h^{}})s_{h}=s,o_{h}=o]=U_{h}(s,o)+_{ [H-h+1]}[T_{h}V_{h+}^{}](s,o) \]

_and the value function is given by_

\[V_{h}^{}(s):= _{}[_{h^{}=h}^{H}r_{h^{}}(s_{h ^{}},a_{h^{}})s_{h}=s,o_{h}_{h}(|s_{h})] =_{o}_{h}(o|s)Q_{h}^{}(s,o) \]

_where \(V_{H+1}^{}(s)=Q_{H+1}^{}(s,o)=0\) for any \((s,o)\)._

Intuitively, the first term \(U_{h}(s,o)\) of the \(Q\)-function is the expected reward within timesteps that option \(o\) is used without being terminated, and the second term \(_{[H-h+1]}[T_{h}V_{h+}^{}](s,o)\) corresponds to the expected reward within timesteps after option \(o\) is terminated and a new option is selected according to \(\). It can be shown that there exists an optimal (and deterministic) hierarchical policy \(^{*}=\{^{*}_{h}:\}_{h[H]}\) that attains the optimal value function, i.e., \(V_{h}^{*}(s)=_{}V_{h}^{}(s)\) for all \((h,s)[H]\)(Sutton et al., 1999).

### Offline RL with Options

We consider learning with options in the offline setting. That is, given a dataset \(\) that is collected by an experimenter through interacting with the environment, the algorithm outputs a hierarchical policy \(\). The sample complexity is measured by the _suboptimality_, i.e., the shortfall in the value function of the hierarchical policy \(\) compared to that of the optimal hierarchical policy \(^{*}\), which is given by

\[_{}(,s_{1}):=V_{1}^{*}(s_{1})-V_{1}^{ }(s_{1}) \]

To derive a novel information-theoretic lower bound of \(_{}\), we first define some useful notations. For any hierarchical policy \(\), we denote by \(^{}=\{^{}_{h}:\}_{h [H]}\) its _state-optionoccupancy measure_. That is, \(_{h}^{}(s,o)\) is the probability that the agent selects a particular option \(o\) at state \(s\) at timestep \(h\) (either when \(h=1\) or when the option \(o_{h-1}\) used at the timestep \(h-1\) is terminated) when following the hierarchical policy \(\). With a slight abuse of the notation, we denote by \(_{h}^{}(s):=_{o}_{h}^{}(s,o)\) the _state occupancy measure_ for any \((h,s)[H]\). Further, we define

\[Z_{}^{}:=_{h,s}_{h}^{}(s),\;_{}^{}:=_{h,s,o}[_{h}^{}(s,o)>0] \]

where \([]\) is the indicator function. Intuitively, \(Z_{}^{}\) is the expected number of timesteps to alternate a new option and \(_{}^{}\) is the maximal number of state-option pairs that can be visited, when following the hierarchical policy \(\). The following proposition shows that options facilitate temporal abstraction and reduction of the state space.

**Proposition 1**.: _For any hierarchical policy \(\), we have that \(Z_{}^{} H\). If \(\) is deterministic, i.e., \(=\{_{h}:\}_{h[H]}\), we further have that \(_{}^{} HS\). All the above equalities hold when \(=\)._

Next, we derive a novel information-theoretic lower bound of \(_{}\). The detailed proof can be found in Appendix C.

**Theorem 2** (Information-theoretic lower bound).: _Let \(=\{_{h}:()\}_{h[H]}\) denote any hierarchical behavior policy to collect the dataset. Define the class of problem instances_

\[(C^{},z^{*},^{*}):= (M,):\;$ of an episodic MDP $M$}\] \[_{h,s,o}^{^{*}}(s,o)}{ _{h}^{}(s,o)} C^{},Z_{}^{^{*}} z ^{*},_{}^{^{*}}^{*}}.\]

_Suppose that \(C^{} 2\), \(z^{*} 1\), and \(^{*} z^{*} S\), where \( x:=\{n:n x\}\) is the largest integer no greater than \(x\). Then, there exists an absolute constant \(c_{0}\) such that for any offline algorithm that outputs a hierarchical policy \(\), if the number of episodes_

\[K C^{}Hz^{*}^{*}}{^{2}}\]

_then there exists a problem instance \((M,)(C^{},z^{*},^{*})\) on which the hierarchical policy \(\) suffers from \(\)-suboptimality, that is,_

\[_{M}[_{_{1}}(,s)]\]

_where the expectation \(_{M}\) is with respect to the randomness during the execution of \(\) within MDP \(M\)._

Theorem 2 shows that, when dataset \(\) sufficiently covers the trajectories induced by \(^{*}\), i.e., \(_{h,s,o}_{h}^{^{*}}(s,o)/_{h}^{}(s,o) C^{}\), at least \((C^{}Hz_{}^{*}_{}^{*}/ ^{2})\) episodes are required to learn an \(\)-optimal hierarchical policy from dataset \(\). Note that when \(=\), it recovers the lower bound \((H^{3}SC^{*}/^{2})\) for offline RL with primitive actions, where \(C^{*}\) is the concentrability defined therein.

## 4 The PEVIO Algorithm

Inspired by the Pessimistic Value Iteration (PEVI) algorithm (Jin et al., 2021), we propose the **PE**ssimistic **V**alue **I**teration for Learning with **O**ptions (PEVIO) in Algorithm 1. Given a dataset \(\) and the corresponding **O**ffline **O**ption **E**valuation (OOE) subroutine, whose details are specified in Sections 5.1 and 5.2, PEVIO outputs a hierarchical policy \(=\{_{h}:()\}_{h [H]}\).

To estimate the \(Q\)-function, given a dataset \(\), PEVIO first constructs \((,,)\) by the OOE subroutine (line 3). Specifically, \(_{h}\) and \(_{h}\) are the empirical counterparts of \(T_{h}\) and \(U_{h}\) presented in the \(Q\)-function given by Equation (1), respectively. In addition, \(\) is a penalty function computed based on dataset \(\). We remark that the OOE subroutine varies when different data-collecting proceduresare considered and we provide the details in Sections 5.1 and 5.2, respectively. Given \(_{h}\), \(_{h}\), and \(_{h}\), the estimated \(Q\)-function \(_{h}\) is the derived (lines 6 and 7). Particularly, \(_{h}\) computed in line 6 can be seen as first replacing \(U_{h}\) and \(T_{h}\) with their empirical counterparts \(_{h}\), \(_{h}\) in Equation (1), and then subtracting the penalty function \(_{h}\). Further, a hierarchical policy \(_{h}\) is constructed greedily with \(_{h}\) (line 10), where \( f(),g()_{}:=_{o}f(o)g(o)\) for any arbitrary functions \(f,g\) defined on \(\). Finally, given \(_{h}\) and \(_{h}\), the corresponding estimated value function \(_{h}\) is computed (line 11). To analyze the suboptimality of the hierarchical policy \(\) output from PEVIO, we first provide the the following definition, which motivates the design of the penalty function \(\).

**Definition 1** (\(\)-uncertainty quantifier for dataset \(\)).: The penalty function \(=\{_{h}:^{+}\}_{h [H]}\) output from the OOE subroutine in Algorithm 1 (line 3) is said to be a \(\)-uncertainty quantifier with respect to \(_{}\) if the following event

\[= \{|_{h}(s,o)-U_{h}(s,o)+_{=1}^{H-h+1}[( _{h}-T_{h})_{h+}](s,o)| \] \[_{h}(s,o)(h,s,o)[H] \}\]

satisfies that \(_{}() 1-\), where \(_{}\) is the joint distribution of the data collecting process.

In other words, the penalty function \(\) is a \(\)-uncertainty quantifier if it upper bounds the estimation errors in the empirical option transition function \(\) and the empirical option utility function \(\). Next, we show that the suboptimality of \(\) output from PEVIO is upper bounded if \(\) is a \(\)-uncertainty quantifier. (The detailed proof can be found in Appendix D.)

**Theorem 3** (Suboptimality of learning with options using dataset \(\)).: _Let \(\) denote the hierarchical policy output by Algorithm 1. Suppose that \(=\{_{h}\}_{h[H]}\) output from the OOE subroutine is a \(\)-uncertainty quantifier. Conditioned on the successful event \(\) defined in Equation (5), which satisfies that \(_{}() 1-\), we have that_

\[_{}(,s_{1}) 2_{h=1}^{H} _{^{*}}[_{h}(s_{h},o_{h})|s_{1}] \]

_where \(_{^{*}}[g(s_{h},o_{h})]=_{(s,o)}_{h}^{^{*}}(s,o)g(s, o)\) for any \(h[H]\) and arbitrary function \(g:\)._

_Remark 1_.: Since the temporal structure of learning with options is much more complex than learning with actions, PEVIO is significantly different from the algorithms proposed for offline RL with primitive actions, such as PEVI (Jin et al., 2021) or VI-LCB (Xie et al., 2021), despite sharing a similar intuition. First, in terms of the algorithm design, PEVI and VI-LCB estimate (one-step) transition kernel and reward function to compute the \(Q\)-function of a state-action pair. However, by Equation (1), the \(Q\)-function of a state-option pair depends on multi-step transitions and rewards.

Hence, it is challenging to design the OOE subroutine and analyze the estimated \((,,)\) for options. Indeed, if the dataset contains \((s,o,u)\) (state-option-utility) tuples, then \((T,U,)\) can be estimated similarly to the case of learning with primitive actions. However, if the dataset contains only \((s,a,r)\) (state-action-reward) tuples, then it remains elusive to estimate and analyze \((T,U,)\). Second, in terms of the suboptimality analysis, previous works on offline RL with primitive actions rely on the extended value difference lemma (Cai et al., 2020, Lemma 4.2), which also depends on the one-step temporal structure of actions and cannot be directly applied to our setting. Hence, to derive Theorem 3, it is non-trivial to generalize the extended value difference lemma to the options framework (See Lemma 8 in the Appendices).

## 5 Data-Collection and Suboptimality Analysis

In this section, we consider two data-collection procedures that are widely deployed in the options literature. The first one collects state-option-utility tuples (dataset \(_{1}\)) and similar datasets are utilized in the work of Zhang et al. (2023). The second one collects state-action-reward tuples (dataset \(_{2}\)) and is studied in a line of works (Ajay et al., 2021; Villecroze et al., 2022; Salter et al., 2022). Intuitively, dataset \(_{1}\) requires smaller storage and enables efficient evaluation of the options, while dataset \(_{2}\) provides richer information on the environment and even facilitates the evaluation of new options. For each dataset, we design the corresponding OOE subroutine and derive a suboptimality bound for the PEVIO algorithm. Based on these results, we further discuss the advantages and the disadvantages of both data-collection procedures, which sheds light on offline RL with options in practice.

### Learning from State-Option Transitions

We consider dataset \(_{1}:=\{(s^{k}_{t^{k}_{t}},o^{k}_{t^{k}_{t}},u^{k}_{t^{k}_{t}})\}_ {i[j^{k}],k[K]}\) consisting of state-option-utility tuples, which is collected by the experimenter's interaction with the environment for \(K\) episodes using a hierarchical behavior policy \(=\{_{h}:()\}_{h[H]}\). More precisely, at timestep \(t^{k}_{i}\) of the \(k\)th episode, the experimenter selects a new option \(o^{k}_{t^{k}_{t}}\), uses it for \((t^{k}_{i+1}-t^{k}_{i})\) timesteps, collects a cumulative reward of \(u^{k}_{t^{k}_{t}}\) within these \((t^{k}_{i+1}-t^{k}_{i})\) timesteps, and finally terminates this option at state \(s^{k}_{t^{k}_{t^{k}_{t+1}}}\) at timestep \(t^{k}_{i+1}\). For convenience, we define \(t^{k}_{j^{k}+1}:=H+1\) for any \(k[K]\).

Let \(a b:=\{a,b\}\) for any pair of integers \(a,b\). When dataset \(_{1}\) is available, the OOE subroutine in Algorithm 1 is given by Subroutine 2. Particularly, Subroutine 2 incorporates the data splitting technique (Xie et al., 2021) (line 2). That is, given dataset \(_{1}\), the algorithm randomly splits it into \(H\) subdatasets \(\{_{1,h}\}_{h[H]}\). Then \(_{h}\) and \(_{h}\) are constructed using subdataset \(_{1,h}\) (lines 4-7).

To derive the suboptimality for \(\) output from PEVIO, we follow the previous study and make a standard assumption on the coverage of dataset \(_{1}\).

**Assumption 1** (Single hierarchical policy concentrability for dataset \(_{1}\)).: The experimenter collects dataset \(_{1}\) by following a hierarchical behavior policy \(=\{_{h}:()\}_{h[H]}\). There exists some deterministic optimal hierarchical policy \(^{*}\) such that

\[C_{1}^{}:=_{h,s,o}}_{h}(s, o)}{^{}_{h}(s,o)} \]

(with the convention \(0/0=0\)) is finite.

In other words, Assumption 1 states that dataset \(_{1}\) sufficiently covers the trajectories of state-option-utility tuples induced by some deterministic optimal hierarchical policy \(^{*}\). We derive an upper bound of \(_{_{1}}\) in the following theorem. (The detailed proof can be found in Appendix E.)

**Theorem 4** (Suboptimality for dataset \(_{1}\)).: _Under Assumption 1, with probability at least \(1-\), we have that_

\[_{_{1}}(,s_{1})(^{}H^{3}Z_{}^{*}_{ }^{*}}{K}}) \]_where \(Z^{*}_{}:=Z^{^{*}}_{}\) and \(^{*}_{}:=^{^{*}}_{}\)._

Compared to the lower bound in Theorem 2, suboptimality bound (8) is near-optimal except for an extra factor of \(H\).4 More importantly, it shows that learning with options enjoys a faster convergence rate to the optimal value than learning with primitive actions. Recall that the VI-LCB algorithm (Xie et al., 2021) that learns with primitive actions attains the suboptimality bound \((SC^{*}/K})\), where \(C^{*}\) is the concentrability defined therein. When ignoring the concentrability parameters, the suboptimality bound (8) is smaller since \(Z^{*}_{} H\) and \(^{*}_{} HS\).

_Remark 2_.: While, in the worst case, both \(Z^{*}_{}\) and \(^{*}_{}\) can scale with \(H\) and \(HS\), respectively, we note that in many long-horizon planning problems, they often scale with the _number of sub-tasks_, which are greatly smaller, especially for tasks that enables temporal abstraction and the reduction of the state space. For example, while the route-planning task of going from City A to City B by transportation takes thousands of primitive actions to finish, it can be efficiently solved by decomposing into the following sub-tasks: (1) going to the airport/train station in City A; (2) taking transportation to City B; and (3) reaching the final destination in City B, for which options are designed. In this case, both \(Z^{*}_{}\) and \(^{*}_{}/S\) may only scale as \(o(H)\). In other words, options facilitate more sample-efficient learning through temporal abstraction, i.e., sticking to an option until a sub-task is finished. Another concrete example is solving a maze, where options are often designed to move agents to bottleneck states (Simsek and Barto, 2004; Solway et al., 2014; Machado et al., 2017) that connect different densely connected regions of the state space, e.g., doorways. In this case, while the number of option switches may grow proportionally to \(H\), i.e., \(Z^{*}_{}/H=O(1)\), the number of states to switch options can be greatly smaller than \(S\), i.e., \(^{*}_{}/H=o(s)\). That is to say, options help improve the sample complexity by the reduction of the state space.

Further, we show that learning with options attains a better performance than learning with primitive actions, when either the options are carefully designed or the offline data is limited.

**Corollary 1** (Better performance).: _Let \(_{_{1}}(,s_{1}):=V^{*,}_{1} (s_{1})-V^{}_{1}(s_{1})\), where \(V^{*,}\) is the optimal value function defined for the primitive actions. Ignoring the concentrability parameters, we have that \(_{_{1}}(,s_{1})(SC^{*}/K})\) attained by the VI-LCB algorithm (Xie et al., 2021), when either the options are carefully designed (i.e., \(_{}(s_{1}):=V^{*,}_{1}(s_{1})-V^{*}_{1}(s_{1})=0\)) or 

[MISSING_PAGE_FAIL:8]

\(\}_{h[H]}\) denote the state-action distribution of the behavior policy \(\) used by the experimenter. That is, \(d^{}_{h}(s,a)\) is the probability that the agent takes action \(a\) at state \(s\) at timestep \(h\). Similarly, we make the following assumption on dataset \(_{2}\).

**Assumption 2** (Single hierarchical policy concentrability for dataset \(_{2}\)).: The experimenter collects dataset \(_{2}\) by following an arbitrary behavior policy \(\). There exists some deterministic optimal hierarchical policy \(^{*}\) such that

\[C^{}_{2}:=_{h,s,o}_{h m H,(s^{},a^{}) ^{m}_{h,s,o}}}(s,o)}{d^{}_{m}(s^{ },a^{})} \]

(with the convention \(0/0=0\)) is finite.

Intuitively, Assumption 2 states that dataset \(_{2}\) sufficiently covers the trajectories of state-action-reward tuples induced by the optimal hierarchical policy \(^{*}\). Next, we derive an upper bound of \(_{_{2}}\) under Assumption 2. The detailed proof can be found in Appendix F.

**Theorem 5** (Suboptimality for dataset \(_{2}\)).: _Under Assumption 2, with probability at least \(1-\), we have that_

\[_{_{2}}(,s_{1})(}_{2}H^{3}SZ^{*}_{}^{*}_{}}{K}}+SOC^{}_{2}}{K}) \]

_which translates to_

\[(}_{2}H^{3}SZ^{*}_{} ^{*}_{}}{K}})\]

_when \(K\) of dataset \(_{2}\) is sufficiently large, i.e., \(K(C^{}_{2}H^{5}S^{9}A^{2}O^{2}/(Z^{*}_{} ^{*}_{}))\), where \(Z^{*}_{}=Z^{^{*}}_{}\) and \(^{*}_{}=^{^{*}}_{}\)._

While, in general, suboptimality bound (10) does not compare favorably against the suboptimality \((SC^{*}/K})\) attained by the VI-LCB algorithm that learns with primitive actions, we argue that it can be better in long-horizon problems where the horizon \(H\) is much greater than the cardinality of the state space \(S\).

### Further Discussion

We analyze the pros and cons of both data-collection procedures, which sheds light on offline RL with options in practice. Compared to \(_{2}\), dataset \(_{1}\) requires smaller storage and enjoys faster convergence to the optimal value, which is further illustrated as follows.

* **Storage:** For dataset \(_{2}=\{(s^{k}_{k},a^{k}_{h},r^{k}_{h})\}_{h[H],k[K]}\), the storage is simply \(HK\). However, for dataset \(_{1}=\{(s^{k}_{i^{k}_{i}},o^{k}_{i^{k}_{i}},u^{k}_{i^{k}})\}_{i [j^{k}],k[K]}\), its expected size is \(K Z^{}_{} HK\), where \(\) is the hierarchical behavior policy. Therefore, in the case of a small \(Z^{}_{}\), dataset \(_{1}\) requires much smaller storage than \(_{2}\) (on average).
* **Suboptimality:** Ignoring the concentrability, the suboptimality bound (10) for dataset \(_{2}\) is worse than the suboptimality bound (8) for dataset \(_{1}\) by a factor of \(\), which is introduced when estimating the option transition function and the option utility function using only the information of the primitive actions.

However, since dataset \(_{2}\) contains more information on the environment than \(_{1}\), it has a weaker requirement on the behavior (hierarchical) policy and allows the evaluation of new options, which is illustrated as follows.

* **Concentrability:** Recall that the suboptimality bounds for both datasets build upon the sufficient coverage assumptions, i.e., Assumption 1 for \(_{1}\) and Assumption 2 for \(_{2}\). While they are generally incomparable (as dataset \(_{2}\) can be collected by an arbitrary behavior policy), we focus on the case that both datasets are collected by the same hierarchical behavior policy \(\). Particularly, it can be shown that Assumption 2 is weaker than Assumption 1. Indeed, if \(\)covers the trajectories of state-option-utility tuples induced by \(^{*}\) (i.e., Assumption 1 holds), then it must have covered the trajectories of state-action-reward tuples induced by \(^{*}\) (i.e., Assumption 2 holds). However, the opposite does not hold in general and we provide such an example in Appendix H.
* **Evaluation of New Options:** In the options literature, a popular task is offline option discovery (Ajay et al., 2021; Villecroze et al., 2022), i.e., designing new and useful options from the dataset. Therefore, an important problem is whether these new options can be evaluated through the dataset. We argue that dataset \(_{2}\) yields greater flexibility than \(_{1}\) in this case. Again, we assume that both datasets are collected by the same hierarchical behavior policy \(\). Unfortunately, one cannot use dataset \(_{1}\) to evaluate any \((h,s,o)\) that is not visited by \(\), i.e., \(_{h}^{}(s,o)=0\), let along evaluating the new options. However, this is not the case for dataset \(_{2}\). In fact, any \((h,s,o)\) can be evaluated if the visiting state-action pairs are also reachable by \(\), i.e., \(_{h m H,(s^{},a^{})_{h,s,o}^{m}}1/d_{m} ^{}(s^{},a^{})<\). An interesting problem is how to leverage the results in this paper to facilitate offline option discovery, which we shall research in the future work.

## 6 Conclusions

In this paper, we provide the first analysis of the sample complexity for offline RL with options. A novel information-theoretic lower bound is established, which generalizes the one for offline RL with actions. We derive near-optimal suboptimality bounds of the **PE**ssimistic **V**alue **I**eration for Learning with **O**ptions (PEVIO) algorithm for two popular data-collection procedures. Our results show that options facilitate more sample-efficient learning than primitive actions in offline RL in both the finite-time convergence rate to the optimal value and the actual performance.