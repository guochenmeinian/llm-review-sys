# LLMs Can Evolve Continually on Modality for

\(\)-Modal Reasoning

Jiazuo Yu\({}^{1}\), Haomiao Xiong\({}^{1}\), Lu Zhang\({}^{1,}\)1, Haiwen Diao\({}^{1}\), Yumzhi Zhuge\({}^{1}\),

**Lanqing Hong\({}^{2}\)**, **Dong Wang\({}^{1}\)**, **Huchuan Lu\({}^{1}\)**, **You He\({}^{3}\)**, **Long Chen\({}^{4}\)**

\({}^{1}\)Dalian University of Technology, \({}^{2}\)Huawei Noah's Ark Lab

\({}^{3}\)Tsinghua University, \({}^{4}\)The Hong Kong University of Science and Technology

yujiazuo@mail.dlut.edu.cn, zhanglu@dlut.edu.cn

###### Abstract

Multimodal Large Language Models (MLLMs) have gained significant attention due to their impressive capabilities in multimodal understanding. However, existing methods rely heavily on extensive modal-specific pretraining and joint-modal tuning, leading to significant computational burdens when expanding to new modalities. In this paper, we propose **PathWeave**, a flexible and scalable framework with modal-**path** switching and expansion abilities that enables MLLMs to continually evolve on modalities for \(\)-modal reasoning. We leverage the concept of Continual Learning and develop an incremental training strategy atop pre-trained MLLMs, enabling their expansion to new modalities using uni-modal data, without executing joint-modal pretraining. In detail, a novel Adapter-in-Adapter (AnA) framework is introduced, in which uni-modal and cross-modal adapters are seamlessly integrated to facilitate efficient modality alignment and collaboration. Additionally, an MoE-based gating module is applied between two types of adapters to further enhance the multimodal interaction. To investigate the proposed method, we establish a challenging benchmark called **C**ontinual **L**earning of **M**odality (MCL), which consists of high-quality QA data from five distinct modalities: image, video, audio, depth and point cloud. Extensive experiments demonstrate the effectiveness of the proposed AnA framework on learning plasticity and memory stability during continual learning. Furthermore, PathWeave performs comparably to state-of-the-art MLLMs while concurrently reducing parameter training burdens by 98.73%. Our code locates at [https://github.com/JiazuoYu/PathWeave](https://github.com/JiazuoYu/PathWeave).

## 1 Introduction

With recent advances in artificial intelligence, Large Language Models (LLMs) have demonstrated impressive capacities in language understanding and reasoning. The success of LLMs  has spurred researchers to develop Multimodal LLMs (MLLMs) by integrating additional input for multimodal tasks, such as image-text understanding , audio recognition  and 3D question answering . Aided by large-scale image-text paired data from the Internet , vision LLMs have become a thriving area in the research community. The typical framework comprises a visual encoder, a frozen or trainable LLM, and a projection module for vision-language alignment. Through stepwisely pretraining on large-scale image-text pairs and instruction tuning on specific datasets, vision LLMs exhibit promising generalization abilities on downstream applications such as detection , grounding , and captioning . Subsequently, the LLM-based framework and training pipeline of vision LLMs serve as the basis and drive the extension to other modalities, including video , audio , and point cloud . However, these modalspecific LLMs that inject single-modal data into language models struggle to tackle the challenge of perceiving different modalities like us humans.

To address this issue, recent approaches  extend the architecture and training strategies of modal-specific MLLMs, and try to integrate multiple modalities into a unified system. Some early attempts  utilize specific projection modules to align image, video, and audio encoders into a frozen LLM. However, a complex training process is usually required to enhance cross-modal alignment, involving separate pretraining on uni-modal data and joint fine-tuning on multimodal data. Subsequent attempts try to enhance the scalability of MLLMs by unifying the architecture and simplifying the training process. For instance, X-InstructBLIP  proposes a unified projection architecture for all modalities and constructs high-quality instruction tuning data to simplify modal-specific customization and pretraining. OneLLM  leverages a unified encoder and projection module and introduces an incremental pretraining strategy to achieve parameter unification for a wide range of modalities. While effective, most approaches still rely on joint-modal optimization that is high-resource demanding (see Figure 1 (a)). When expanded to new modalities, the models have to re-access all the historical data and repeat the complete training process, limiting the continual extension of MLLMs.

In this paper, we propose **PathWeave**\(\), a flexible and scalable framework with modal-**path** switching and expansion capabilities that enables MLLMs to continually **ev**olve on modality for \(\)-modal reasoning. PathWeave leverages the concept of Continual Learning (CL) and forms an incremental training pipeline on uni-modal data, eliminating the necessity for joint-modal pretraining or finetuning. To this end, we employ a pre-trained vision LLM  as the interface and propose a novel Adapter-in-Adapter (AnA) framework, allowing efficient extension and alignment for other modalities. We set two types of adapters in AnA, uni-modal and cross-modal, and seamlessly incorporate them to boost modality alignment and collaboration during incremental learning. Specifically, the uni-modal adapters are progressively added to the interface and optimized on the corresponding modality data, which will be frozen once trained. Meanwhile, we insert in-adapters into the previous uni-modal adapters to form cross-modal adapters, allowing the effective integration between historical knowledge and ongoing modality. Additionally, an MoE-based gating module is implemented between uni-modal and cross-modal adapters to further enhance multimodal collaboration. As shown in Figure 1 (b), our PathWeave can be flexibly implemented on the pretrained MLLMs and efficiently expand to more modalities in an incremental manner.

To evaluate the proposed PathWeave, we establish a challenging benchmark, namely **C**ontinual **L**earning of multi-**M**odality (**MCL**). It consists of data from five distinct modalities: image, video, depth, audio, and point cloud. In our setting, the modalities data are incrementally fed to the MLLMs. Thus, we leverage the commonly-used metrics from  to investigate the precision on newly learned modalities. Furthermore, we introduce a metric to measure the forgetting rate in MCL to demonstrate the effectiveness of the proposed AnA strategy on historical modality memorization. Finally, we conduct extensive experiments to compare with state-of-the-art continual learning approaches, demonstrating that PathWeave is effective at incorporating multimodal data in an incremental manner. Moreover, our method achieves comparable performance with state-of-the-art MLLMs without requiring joint-modal pretraining or fine-tuning.

In summary, our contributions are summarized as follows:

* We present an efficient and scalable framework, PathWeave, which enables MLLM to progressively expand on multiple modalities, without the need for joint-modal pretraining.

Figure 1: Comparisons of Different Multimodal LLMs: (a) The normal multimodal methods  require unified sampling across multi-modal. (b) Our proposed incremental MLLMs learns each modality sequentially without joint-modal datasets.

* We introduce a novel adapter-in-adapter framework that seamlessly integrates uni-modal and cross-modal adapters to enhance modality alignment and interaction during incremental learning.
* We establish a challenging MCL benchmark with well-defined evaluation metrics. Extensive results demonstrate the effectiveness of PathWeave on modality plasticity and memorization during continual learning. Furthermore, PathWeave performs on par with state-of-the-art MLLMs while reducing parameter training burdens by at least 98.73%.

## 2 Related Work

**Multimodal Large Language Models.** In recent years, researchers have been exploring the potential of LLMs in multimodal perceptions, such as visual question answering [5; 14] and captioning [6; 24]. This leads to the rapid development of Multimodal LLMs [6; 5; 21; 22]. For example, LLaVA  utilizes a simple linear layer to project visual information into language space, ending LLMs the ability to perceive natural scenes. Subsequently, several methods attempt to expand the supported modalities of LLMs by modifying architecture designs or training strategies. For instance, X-LLM  and Chatbridge  use modal-specific modules to extract features for multiple modalities and exploit modal-specific projection layers for multimodal alignment on a frozen LLM. However, a complex training process is usually required to enhance cross-modal alignment, which involves separate pretraining on uni-modal data and joint instruction tuning on multimodal data. Later, X-InstructBLIP  proposes a unified projection architecture (Q-former) for all modalities and collects large-scale, high-quality instruction tuning data to eliminate the need for uni-modal pretraining. OneLLM  explores parameter unification by introducing a unified encoder and projection module for a wide range of modalities. Although an incremental pretraining strategy is proposed to alleviate the high resource demand of cross-modal alignment, OneLLM still relies on cross-modal finetuning on large-scale instruction datasets. In contrast to these methods, we incorporate the continual learning concept into MLLMs and propose an incremental training strategy to allow MLLMs' modal expansion by finetuning on uni-modal data, without requiring joint-modal pretraining or finetuning. Among these approaches, X-InstructBLIP  is highly related to our method, as it separately tunes Q-former to align multimodal into a uniform system. However, our method designs an adapter-based expansible framework that significantly reduces the parameter training burdens by at least 98.73%.

**Continual Learning in Foundational Models.** Continual Learning (CL) has been applied to large foundational models [25; 26; 27; 28], allowing them to continually acquire new knowledge. To address the forgetting issue in CL, significant efforts  have been made, including data replay, regularization constraints, and dynamic frameworks. Data replay-based methods [30; 31; 32; 33; 34] retain the historical data in a memory bank and mix them with new data to execute the general training process. However, the redundant historical data would incur increasing resource demand during lifelong learning. Regularization-based methods add explicit regularization terms on weights [35; 36; 37] or data [38; 39; 40; 41] to achieve a balance between historical and new tasks, which are usually used as an auxiliary trick in data-replay or dynamic methods. In contrast, dynamic methods [28; 26; 42; 43; 44] exhibit impressive expansible abilities by incrementally adding new parameters into a shared interface. Recently, the dynamic frameworks have been combined with efficient tuning techniques to achieve efficient, cost-friendly continual learning on visual-textual domain [28; 27; 26]. This inspires us to eliminate joint-modal pertaining from MLLMs by developing an efficient, scalable framework where new modalities are incrementally involved by accessing uni-modal data. To this end, we propose an adapter-in-adapter framework, which incorporates uni-modal and cross-modal adapters for efficient modality alignment and collaboration.

**Transfer learning.** In the realm of Natural Language Progressing (NLP), fine-tuning large-scale models (_e.g.,_ 175B GPT-3 ) imposes significant burdens in both parameter complexity and time consumption. As a result, transfer learning methods [45; 46; 47; 48] have gained significant attention to facilitate the efficient adaption of LLMs on downstream applications. The techniques usually activate a small set of parameters on the frozen models while achieving comparable performance with fully-finetuned approaches. Among these methods, LoRA  reduces the trainable parameters through low-rank matrix decomposition, leading to the generalization of the pre-trained model on diverse downstream tasks. The success of LoRA further promotes the development of parameter-efficient transfer learning of MLLMs [5; 49; 50] and uni-modal continual learning approaches [27; 26; 51]. However, these methods cannot be directly applied to fix the proposed MCL task due to the significant variations in modality spaces. In this paper, we propose a modality continual learning method that incorporates adapter-based dynamic architecture on a frozen LLM, allowing efficient adaption and flexible expansion of new modalities in an incremental manner.

## 3 PathWeave

### Preliminaries

Continual learning can empower large-scale foundation modals to constantly acquire new knowledge without accessing the entire historical data. We introduce this concept into MLLMs to form an incremental training strategy on uni-modal data called Continual Learning on Modality (MCL), eliminating the necessity of modal-specific pertaining and joint-modal datasets. Given a set of \(M\) modalities \(\{^{m}\}_{m=1}^{M}\), we enforce LLMs to sequentially access and learn on each modality for question answering. Here, each modality \(^{m}\) contains \(N^{m}\) datasets, which can be represented as \(^{m}=\{^{m}_{i}\}_{i=1}^{N^{m}}\). More specifically, \(^{m}_{i}=\{^{m}_{i},^{m}_{i},^{m}_{i}\}\) denotes the \(i\)-\(th\) data of the \(m\)-\(th\) modality \(^{m}\), in which \(,\) and \(\) are text instruction, modality samples, and answering, respectively.

### Framework Overview

This work presents PathWeave, an efficient and extensible framework that empowers MLLMs to constantly evolve on modalities, without requiring modal-specific pretraining. Considering the complicity of training MLLMs from scratch, we start from a pretrained vision LLM and align other modalities in an incremental manner. The overall framework of PathWeave is illustrated in Figure 2. Specifically, we build the PathWeave on X-InstructBLIP , providing a unified Q-Former architecture for various modalities. Given the samples from \(m\)-\(th\) modality, a modal-specific encoder \(E_{m}\) pretrained on the corresponding modality is first exploited for feature extraction. Then, the Q-Former \(Q\) takes the input of modality feature, learnable query \(q_{m}\), and instruction embedding \(I_{m}\) for

Figure 2: Overall framework of PathWeave. We start from a pretrained vision LLM  and progressively expand new modalities on it without acquiring historical data. Given input samples from **modality \(\)**, we first exploit a frozen encoder (\(E_{m}\)) for feature extraction and leverage Q-Former to achieve multimodal alignment with LLMs. Then, the Adapter-in-Adapter (AnA) module is implemented in Q-Former to achieve flexible modal-path switching and expansion. In detail, the uni-modal adapters (\(^{m}\)) are implemented in parallel to facilitate new modal plasticity, which will be frozen once trained. While the cross-modal adapters (\(}^{m}\)) are formed by inserting a set of in-adapters (\(\{^{m}_{i}\}_{i=1}^{m-1}\)) into the learned uni-adapters to enhance the collaboration of historical knowledge. Additionally, an MoE-based gating module (\(^{m}\)) is implemented among uni-adapters to adaptively multimodal integration in input space.

multimodal alignment on a frozen LLM. It is worth noting that the initial modality \(^{0}\) is predefined as images, as we leverage the pretrained X-InstructBLIP to facilitate the alignment of subsequent modalities. As a result, the entire parameter of the encoder, Q-Former, and LLM will be frozen during continual learning. To achieve continual learning on modalities, we propose Adapter-in-Adapter (AnA), a dynamically expansible framework atop MLLMs, enabling the efficient integration of new modalities by executing uni-modal instruction tuning. The AnA consists of uni-modal and cross-modal adapters to boost modality alignment and collaboration along the modality sequence. In detail, the uni-modal adapters (\(^{m}\)) are implemented in parallel in Q-Former to efficiently adapt to new modalities, which will be frozen once trained to "memorize" the historical modalities. Meanwhile, the cross-modal adapters (\(}^{m}\)) are constructed by inserting a set of in-adapters (\(\{^{m}_{i}\}_{i=1}^{m-1}\)) into previously learned uni-adapters to acquire their knowledge for ongoing modality, which will be removed accordingly when testing former modalities. Furthermore, an MoE-based gating module is implemented between uni-adapter and cross-adapted for further multimodal integration.

### Adapter-in-Adapter

X-InstructBLIP  utilizes Q-Former as a unified framework to extend MLLMs' capabilities on more diverse modality reasoning, eliminating the need for modal-specific pretraining. However, instruction tuning on uni-modal data is implemented on separated Q-Formers, which leads to significant computational costs and parameter burdens when integrating more modalities. Recently, some attempts  have demonstrated that adapters with few parameters can enhance the adaption of foundation modal on downstream tasks. Inspired by this, we leverage an effective transfer learning technique, LoRA , to serve as the basic unit of our AnA framework, enabling the efficient adaption of subsequent modalities during incremental learning.

**Uni-modal Adapters.** Given the current modality \(^{m}\), we implement uni-modal adapters \(^{m}\) in the pretrained Q-Former for new modal alignment. The adapters \(^{m}\) are inserted into different linear layers \(l\) of pretrained model in parallel. The output of layer \(l\) with adapters \(^{m}\) can be expressed as:

\[^{m}_{l}=Q_{l}(^{m}_{l})+^{m}_{l}(^{m}_{l}), \]

where \(^{m}_{l}\) and \(^{m}_{l}\) are the input and output embedding of \(l\)-\(th\) layer when aligning \(m\)-\(th\) modality. \(^{m}_{l}\) is the adapter of \(m\)-\(th\) modality in \(l\) layer, and \(^{m}()=^{m}_{u}(^{m}_{d}())\), where \(_{u}\) and \(_{d}\) are the up and down projection of adapter. The uni-modal adapters are effective at acquiring modal-specific knowledge. Besides, the parallel architecture of adapters endows our system with the capabilities to flexibly switch and expand to diverse modalities.

**Cross-modal Adapters.** The uni-modal adapters are effective at preserving the uni-modal knowledge and alleviating the forgetting issue in long-term learning. Based on it, we introduce a modal-special in-adapter module (\(^{m}_{i}\)) to form a cross-modal adapter (\(}^{m}\)), which can help the ongoing modality learn previous knowledge and encourage inter-modality collaboration. Specifically, the in-adapters are inserted into the previously learned uni-modal adapters to effectively acquire the learned knowledge without reactivating their parameters. Then, the output of \(l\)-\(th\) layer \(^{m}_{l}\) after adding In-Adapter \(^{m}_{i}\) can be reformulated as:

\[^{m}_{l}=Q_{l}(^{m}_{l})+_{i=1}^{m-1}}^{i}_{l} (^{m}_{l})+^{m}_{l}(^{m}_{l}), \]

where \(}^{i}()=^{i}_{i}(^{m}_{i}(^{i}_{d}())),i[1,m-1]\) represents the cross-modal adapters for current modality \(^{m}\). \(^{m}_{i}\) is the in-adapter that is inserted into \(i\)-\(th\) frozen uni-adapters \(^{i}\), which is a single linear layer with the dimension of adapters' low rank. The uni-modal and cross-modal adapters collaborate to facilitate the new modality alignment and cross-modal integration during incremental learning. Furthermore, the proposed in-adapter serves as a plug-and-play module that will not affect the performance of previously learned adapters, thereby effectively alleviating the modality forgetting.

**MoE-based Gating.** Cross-modal adapters rely on in-adapters to effectively leverage historical knowledge to boost the alignment of ongoing modality. However, the output of cross-modal and uni-modal adapters are treated equally in the original Q-Former. Considering the significant gap between distinct modalities, this simple integration strategy might pose performance degradation affected by the interfering information from other modalities. To address this issue, we propose an MoE-based gating module between cross-modal and uni-modal adapters for adaptive multimodal integration. Our MoE-based gating \(^{m}\) automatically assigns weights of paths \(^{m}\) of different cross-modal adapters and uni-modal adapter to produce outcomes tailored to each modality \(^{m}\). The paths \(\{^{m}\}_{m=1}^{M}\) include the previous cross-modal adapters with the current in-adapter and current uni-modal adapter. Therefore, each linear's output \(^{m}\) after adding MoE-based gating \(^{m}\) in AnA module can be computed as:

\[^{m}_{l}=Q_{l}(^{m}_{l})+_{i=1}^{m}W^{m}_{i}_{i}(^{m}_{l}), \]

where \(W^{m}=\{W^{m}_{i}\}_{i=1}^{N_{E}}\) represents the gating weights assigned by \(^{m}\), dictating the contribution of each adapter's path \(^{m}\). The gating weights are then computed as follows:

\[W^{m}=Softmax(^{m}(^{m})), \]

where \(^{m}\) projects each token of embeddings \(\) to a 1-D vector indicating each modality's likelihood of functioning. It is worth noting that we do not set the \(Topk\) hyper-parameter here. By default, the knowledge of each modality will provide a reference for the current modality. The \(Softmax()\) function normalizes these weights to emphasize the modality-branch contribution. Finally, the output \(^{m}_{l}\) of AnA with MoE-based gating can be expressed as:

\[^{m}_{l}=Q^{m}_{l}(^{m}_{l})+_{i=1}^{m-1}W^{i}}^{i}(^{m}_{l})+W^{m}^{m}(^{m}_{l}). \]

## 4 Continual Learning on Modality

**MCL Benchmark.** We establish a challenging benchmark, Continual Learning on Modality (MCL), which consists of multimodal high-quality QA data to evaluate the effectiveness of our method on continual uni-modal finetuning. These datasets are collected from five distinct modalities: image, video, depth, audio and point cloud. Based on this benchmark, our PathWeave is trained and tested along the multimodal sequence without requiring modal-specific pretraining or joint-modal finetuning. More details of the dataset list and size for each modality are illustrated in Table A6 of the Appendix.

**MCL Metrics.** We formulate the metrics from two aspects to evaluate the proposed MCL strategy on multimodal reasoning. On the one hand, we use the general metrics from MLLMs [20; 21] to investigate the model's overall performance on learned new modalities. On the other hand, we modify the conventional metrics of continual learning to verify the performance of our method on "catastrophic forgetting". Specifically, for each modality and dataset, suppose \(S^{n}_{m,i}\) represents the evaluation score on \(n\)-\(th\) datasets of modality \(^{i}\) after training on modality \(^{m}\). We redefine the forgetting rate  to measure the degree of forgetting \(F_{m}\) on all old modalities after each modality stage \(m\):

\[F_{m}=_{i=0}^{m-1}F^{N_{i}}_{m,i}, \]

where \(F^{N_{i}}_{m,i}\) is the average forgetting across \(N_{i}\) datasets of modality \(i\) after modality \(m\) training, and \(N_{i}\) is the number of datasets in modality \(i\). And the \(F^{N_{i}}_{m,i}\) are defined:

\[F^{N_{i}}_{m,i}=}_{n=1}^{N_{i}}_{0 j<m}(S^{n}_{j,i})- S^{n}_{m,i}. \]

In addition, we define the forgetting \(^{n}_{i}\) for the \(n\)-\(th\) dataset in modality \(i\) during the training of all modalities:

\[^{n}_{i}=_{m=i+1}^{M}_{0 j<m}(S^{n}_{j,i})-S^{ n}_{m,i}. \]

To measure the overall performance on learned modalities, we further report the average scores of across \(N_{m}\) datasets of modality \(m\) after training on \(m\) modality, it can be expressed as:

\[T_{m}=}_{n=1}^{N_{m}}S^{n}_{m,m}. \]

And the performance on learned modalities \(^{n}_{i}\) for the \(n\)-\(th\) dataset in modality \(i\) can be expressed as \(^{n}_{i}=S^{n}_{i,i}\).

## 5 Experiments

### Implementation Details

Our method is built on the LAVIS library's framework  atop the Vicuna v1.1 7b . The input preprocessing method remains consistent with X-InstructBLIP . We optimize our model on 4\(\)A800 GPUs (80GB) using AdamW  with \(_{1}=0.9\), \(_{2}=0.999\), and a weight decay of \(0.05\). Our initial pre-trained model is the image modality model of X-InstructBLIP . During training, the unified incremental module, consisting of Q-former and LLM projection, is continuously trained in the order of image, video, audio, depth, and 3D modalities. During testing, the learnable query and modality encoder are kept modality-specific. The CL methods compared below maintain consistent settings with our method. More details are provided in the Appendix A.2.

### Comparison with State-of-the-art Methods

Transfer Learning on New Modality.As shown in Table 1 and 2, we conduct experiments on existing traditional CL methods under our proposed MCL setting. We report the average expansion capability for each modality, which is represented as \(T_{m}\) and indicates the scalability in the new modality. The inference datasets are in-domain, which is involved in model training, and additional results of out-of-domain are provided in the supplementary material. Continual-FT, which refers to continuous learning of each modality without incorporating anti-forgetting strategies, exhibits the best expansion ability due to fine-tuning all parameters but inevitably leads to catastrophic forgetting. In contrast, the methods of L2 Reg&WE , WISE-FT  and EProj  effectively alleviate forgetting by parameter regularization and ensemble, but it is difficult for them to transfer new modality. As shown in Table 1, when performing transfer learning on new modalities with significant data distribution gaps from the images, these methods under-perform ours by at least 38 points on the Audio modality and 66 points on the 3D modality. Furthermore, as shown in Table 2, our method surpasses the current best methods by over 29 points in the average transfer learning metrics across in-domain datasets. This demonstrates that our approach can effectively prevent forgetting while flexibly extending to new modalities with substantial data distribution differences.

and 2. The results show that continually full finetuning pre-trained modal suffers from catastrophic forgetting. WiSE-FT  and L2 Reg&WE  achieve some effectiveness in combating forgetting via parameter regularization and ensemble. However, the constraint of parameters limits their transfer learning on new modalities. In contrast, the EProj  and our method achieve anti-forgetting by freezing model parameters. However, the scalability of the EProj  is significantly lower than our method, especially in the audio and 3D modes. It indicates that our method achieves an optimal balance between anti-forgetting and effective expansion compared to other methods.

**Comparison with Existing MLLMs.** Table 3 shows the comparison between our approach and state-of-the-art multimodal QA methods in terms of training parameters, required data, training times, GPU usage, and relevant multimodal QA metrics. Among these methods, we unify the settings to ensure fairness in the Times and GPU metrics by only training on the instruction tuning stage, setting all batchsize to 4, and keeping the LLMs of BLIP-based X-LLM and ChatBridge frozen. It can be seen that our method demonstrates a significant advantage in parameter efficiency compared to X-InsructBLIP  and OneLLM , reducing parameter training burdens by at least 98.73%. Moreover, compared with OneLLM , X-LLM , and ChatBridge , our approach does not necessitate pre-training and instruction tuning with all joint-modal datasets to adapt to multimodal language reasoning tasks. Our method offers flexible scalability and requires considerably less training data than other methods. The results of the three QA tasks involving video, audio, and 3D, as shown in Table 3, indicate that our approach maintains flexibility without significantly compromising model performance. More experiments are provided in the Table A11 of Appendix.

### Ablation Study

**Ablation Study of the In-Adapter and MoE-based gating.** We conduct detailed ablation studies on different parts of the proposed method, as shown in Table 4 and 5. Table 4 shows the average performance \(T_{m}\) of transfer learning in each modality. It can be seen that our final method demonstrates increasingly significant performance improvements compared to others when faced with continual modality changes. For instance, as we further extend to depth and 3D modalities, the collaborative synergy between MoE-based gating and In-Adapter becomes increasingly apparent. In addition, Table 5 demonstrates that compared to directly using the incremental adapter method, our approach improves the average performance of transfer learning across all datasets by 4.3 points. When removing the In-Adapter or MoE-based gating, the model's transfer learning performance of transfer learning across all datasets decreases by at least 1.1 points and 4.0 points. It indicates the effectiveness of our proposed In-Adapter and MoE-based gating, which enhance inter-modal interactions and modulate cross-modal knowledge.

**Analysis of the Benefit from Previous Modalities.** Figure 3 presents the ablation study on the ability to transfer learning based on different knowledge of modalities. As shown in Figure 3 (a), our method enhances the scalability of audio modality after incorporating additional video modality training. It indicates that our designed method can extract knowledge from the other adapter to enhance the learning of the current modality. In addition, when more than one modality is additionally introduced, our method can still enhance new generalization by modulating inter-modal knowledge and fine

    &  &  &  \\   & \(T_{2^{}}\)\(\) & \(T_{2^{}}\)\(\) & \(T_{3^{}}\)\(\) & \(T_{3^{}}\)\(\) & \(T_{4^{}}\)\(\) & \(T_{4^{}}\)\(\) \\  Continual-Adapter & 51.17 & 40.28 & 75.75 & 49.10 & 68.00 & 51.05 \\ w/o MoE-based gating & 43.77 & 39.35 & 76.40 & 49.80 & 69.50 & 49.70 \\ w/o In-Adapter & 52.47 & 40.78 & 79.50 & 50.25 & 71.35 & 52.60 \\ Ours & **56.63** & **42.90** & **83.35** & **52.20** & **73.45** & **53.70** \\   

Table 4: Ablation study of different parts for the influence of the each modalities’ performance. We label the best and second methods with **bold** and underline styles.

   Method & Params & All Modal & Data Size & Times\(\) & GPU\(\) & MSVD QA & Clotho Caps & Modelnet Cls \\  X-InsructBLIP  & 189.91M+ & ✗ & 27.78M+ & 0.34s/it & 28.7G & 51.7 & 29.4 & 62.8 \\ OnclLid  & 77B+ & ✓ & 1007M+ & 0.83s/it & 64.8G & 56.5 & 29.1 & - \\ X-LLM  & 189.91M+ & ✓ & 17.2M+ & 0.34s/it & 28.7G & - & - & - \\ ChatBridge  & 7B+ & ✓ & 4.4M+ & 0.34s/it & 28.7G & 45.3 & 26.2 & - \\ Ours & 0.8\(\)24M & ✗ & 23.2M+ & 0.23s/it & 13.1G & 48.2 & 28.6 & 59.5 \\   

Table 3: Comparison with state-of-the-art methods on training parameters, data requirements and some performance. “All Modal” indicates whether fine-tuning on all modality datasets is included. “\(\)” represents the same hyperparameters and training settings of different methods for fair comparison.

[MISSING_PAGE_FAIL:9]