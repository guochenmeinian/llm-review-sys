# Understanding Emergent Abilities of Language Models from the Loss Perspective

Zhengxiao Du\({}^{1,2}\), Aohan Zeng\({}^{1,2}\), Yuxiao Dong\({}^{2}\), Jie Tang\({}^{2}\)

\({}^{1}\)Zhipu AI \({}^{2}\)Tsinghua University

{zx-du20,zah22}@mails.tsinghua.edu.cn

###### Abstract

Recent studies have put into question the belief that emergent abilities  in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities. In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute. We demonstrate that the Transformer models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks, with a fixed data corpus, tokenization, and model architecture. We also discover that a model exhibits emergent abilities on certain tasks--regardless of the continuity of metrics--when its pre-training loss falls below a specific threshold. Before reaching this threshold, its performance remains at the level of random guessing. This inspires us to redefine emergent abilities as those that manifest in models with lower pre-training losses, highlighting that these abilities cannot be predicted by merely extrapolating the performance trends of models with higher pre-training losses.

## 1 Introduction

Scaling of language modes (LMs) on both model and data sizes has been shown to be effective for improving the performance on a wide range of tasks , leading to the widespread adoption of LM applications, e.g., ChatGPT. The success of such scaling is guided by scaling laws , which study the predictability of pre-training loss given the model and data sizes.

While scaling laws focus on the pre-training loss, the scaling effect on the performance of downstream tasks has thus far less studied. Emergent abilities  are defined as abilities that present in larger LMs but not present in smaller one. The existence of such abilities is recently challenged for two reasons. First, small LMs trained on a sufficient amount of high-quality data can outperform large models on tasks with claimed emergent abilities . For example, LLaMA-13B with less compute  can outperform GPT-3 (175B) on MMLU , due to more training tokens and improved data-filtering methods. Second, Schaeffer et al.  claim that emergent abilities appear due to the nonlinear or discontinuous metrics selected to evaluate certain datasets, rather than from a fundamental change in larger models.

The Chinchilla scaling laws  show that different combinations of model sizes and data sizes can lead to different pre-training losses even with the same training compute. Consequently, the pre-training loss can naturally better represent the learning status of LMs than the model or data sizes. However, the relationship between the loss of an LM and its performance on downstream tasks is not yet well understood. Existing literature has either focused on the transfer learning paradigm  or constrained its study to single models, tasks, or prompting methods .

In this work, we propose to study emergent abilities from the perspective of pre-training loss instead of model size or training compute. To examine the relationship between the pre-training loss of LMs and their performance, we pre-train more than 30 LMs of varied model and data sizes from scratch, using a fixed data corpus, tokenization, and model architecture. Their downstream performance is evaluated on \(12\) diverse datasets covering different tasks, languages, prompting types, and answer forms. We demonstrate that the pre-training loss of an LM is predictive of its performance on downstream tasks, regardless of its model size or data size. The generality of this conclusion is further verified by extracting and observing the performance and loss relationship of the open LLaMA  and Pythia  models.

Over the course, we find that performance on certain downstream tasks only improves beyond the level of random guessing when the pre-training loss falls below a specific threshold, i.e., emergent abilities. Interestingly, the loss thresholds for these tasks are the same. When the loss is above this threshold, performance remains at the level of random guessing, even though performance on other tasks continues to improve from the outset. To exclude the impact of discontinuous metrics [46; 61], we evaluate the emergent performance increase using continuous metrics and show that the emergent abilities persist across both discontinuous and continuous metrics.

Based on these observations, we define the emergent abilities of LMs from the perspective of pre-training loss: an ability is emergent if it is not present in language models with higher pre-training loss, but is present in language models with lower pre-training loss. According to the loss scaling laws [22; 28], the pre-training loss is a function of model size, data size, and training compute. Therefore, the new emergent abilities can also account for the previously-observed emergent abilities in terms of model size or training compute.

The advantage of the new definition lies in its ability to better capture the tipping points in training trajectories when LMs acquire emergent abilities. Once again , the existence of emergent abilities suggests that we cannot predict all the abilities of LMs by simply extrapolating the performance of LMs with higher pre-training loss. Further scaling the model and data size to lower the pre-training loss may enable new abilities that were not present in previous LMs.

## 2 Does Pre-training Loss Predict Task Performance?

We study the relationship between the performance of the language models (LMs) on 12 downstream tasks and the pre-training loss. We pre-train LMs of different model sizes (300M, 540M, 1B, 1.5B, 3B, 6B, and 32B) on varied numbers of tokens with fixed data corpus, tokenization, and architecture. In addition, we leverage the open LLaMA  models (7B, 13B, 33B, and 65B) to validate our observations.

  
**Dataset** & **Task** & **Prompting Type** & **Answer Form** & **Metric** \\    \\  TriviaQA  & Closed-book QA & Few-shot & Open-formed & EM \\ HellaSwag  & Commonsense NLI & Zero-shot & Multi-choice & Accuracy \\ RACE  & Reading Comprehension & Few-shot & Multi-choice & Accuracy \\ WinoGrande  & Coreference Resolution & Zero-shot & Multi-choice & Accuracy \\ MMLU  & Examination & Few-shot & Multi-choice & Accuracy \\ GSM8K  & Math Word Problem & Few-shot CoT & Open-formed & EM \\    \\  NLPCC-KBQA & Closed-book QA & Few-shot & Open-formed & EM \\ ClozeT  & Commonsense NLI & Zero-shot & Multi-choice & Accuracy \\ CLUEWSC  & Coreference Resolution & Zero-shot & Multi-choice & Accuracy \\ C3  & Reading Comprehension & Few-shot & Multi-choice & Accuracy \\ C-Eval  & Examination & Few-shot & Multi-choice & Accuracy \\ GSM8K-Chinese & Math Word Problem & Few-shot CoT & Open-formed & EM \\   

Table 1: English and Chinese datasets evaluated in the experiment, and their task types, prompting types, answer forms and metrics. For prompting type, we refer to the chain-of-thought prompting  as few-shot CoT and the original in-context learning prompting  as few-shot.

It is not straightforward that the loss of LMs decides the performance on downstream tasks. Generally the performance is decided by the probability to predict the ground truth \(y\) given the prompt \(x\), i.e. \(p(y|x)\). The probability can be written as a function of the cross entropy loss:

\[p(y|x)=(-(y|x)) \]

where \((y|x)\) is the cross entropy loss of the LM given the context \(x\) and the target \(y\). While \((y|x)\) has the same form as the pre-training loss \(L\), they are not equal. First, the pre-training loss is an average of all the tokens in all the documents pre-trained on. According to our empirical observation, the losses of different documents are not uniform. Second, if \(x\) and similar documents do not exist in the pre-training corpus, \((y|x)\) is the generalization loss, which is often related to other factors beyond the training loss, such as the model size. For example, in computer vision, a highly over-parameterized models often improve over an under-parameterized models in test performance when both models converge on the training data [14; 7].

### Pre-training Setting

All the models are pre-trained on a mixture of English and Chinese corpus. The ratio of English to Chinese is 4:1 in the pre-training corpus. The model architecture is similar to LLaMA  with two differences: we use grouped-query attention  to replace the multi-query attention and we apply rotary position embedding on half the dimensions of the query and key vectors. More details can be found in Appendix A.

### Evaluation Tasks

To present a comprehensive demonstration, we evaluate the pre-trained models on 12 datasets across different tasks and prompting types in both English and Chinese. The six task types include:

**Closed-book QA:** Answering questions about the real world based solely on the pretrained knowledge. We use TriviaQA  for English. For Chinese, we build a closed-book QA dataset based on NLPCC-KBQA  dataset following the TriviaQA format.

**Commonsense Natural Language Inference (NLI):** Selecting the most likely followup given an event description. We use the HellaSwag dataset  for English and the ClozeT dataset in Yao et al.  for Chinese.

**Reading comprehension:** Reading a given article or paragraph and answering questions about it. We use RACE  for English and C3  for Chinese. Both are based on multi-choice questions.

**Coreference Resolution:** Given a sentence with pronouns, determine which pronoun refers to which entity. We use the WinoGrande dataset  for English and the CLUEWSC dataset  for Chinese.

**Examination:** Multiple-choice questions in examinations. For English, we use MMLU , which includes mathematics, US history, computer science, law, and more. For Chinese, we use C-Eval  which ranges from humanities to science and engineering.

**Math Word Problem**: Solving real-life, situational and relevant problems using mathematical concepts. For English we use the GSM8K  dataset. For Chinese, we translate the questions and answers in GSM8K to Chinese, namely GSM8K-Chinese.

The prompting types cover few-shot , zero-shot, and few-shot chain-of-thought (CoT) . The datasets are summarized in Table 1.

### Pre-training Loss vs. Performance

In the first experiment, we train three models with 1.5B, 6B, and 32B parameters and observe their behaviors until trained on 3T, 3T, and 2.5T tokens, respectively. The training hyperparameters are shown in Table 4 (Appendix).

We evaluate the performance of intermediate training checkpoints. The checkpoints are saved around every 43B tokens during pre-training. We plot the points of task performance (\(y\)-axis) and training 

[MISSING_PAGE_FAIL:4]

* Interestingly, we find that the overall training loss is a good predictor of performance on both English and Chinese tasks, although it is computed on a mixture of English and Chinese tokens. This implies that the learning dynamics of English and Chinese tokens are likely very similar during multilingual pre-training.

### Training Token Count vs. Performance

Following the empirical experiments in scaling laws , we further pre-train 28 relatively smaller models with different numbers of training tokens. The model sizes range from 300M, to 540M, 1B, 1.5B, 3B, and to 6B, while the numbers of pre-training tokens range from 33B to 500B. Varying the number of pre-training tokens is necessary since to achieve optimal performance we need to set the cosine learning rate schedule to reach the minimum at the corresponding token count . The number of tokens used and hyperparameters for all models are shown in Table 5 (Appendix).

On each line, each data point represents the performance and pre-training loss of the corresponding model pre-trained completely from scratch with the certain token count (and learning rate schedule). We can see that similar to the observations from Figure 1, the data points of different models sizes and training tokens largely fall on the same trending curves. In other words, _the LMs with the same pre-training loss regardless of token count and model size exhibit the same performance on the 12 downstream tasks_.

Another similar observation is that the performance curves on MMLU, C-Eval, GSM8K, and GSM8K-Chinese do not yield an uptrend, meaning that the performance of these models on these four tasks are close to random (with fewer than 500B tokens). For simplicity, we only plot the performance of the latest checkpoint in each training in Figure 2. The complete performance curves with intermediate checkpoints of each model, in which we can observe the same trend but larger variance, are shown in Figure 5 (Appendix).

Figure 2: **The performance-vs-loss curves of smaller models pre-trained with different numbers of training tokens**. Each data point is the loss (\(x\)-axis) and performance (\(y\)-axis) of the final checkpoint of one model, i.e., each point corresponds to one model trained from scratch. We mark the results of random guess in black dashed lines.

### LLaMA's Loss vs. Performance

To validate the generality of our observations, we analyze two different model series with required information made publicly available, i.e., LLaMA  and Pythia . Compared to our models, LLaMA uses a pre-training corpus that excludes Chinese documents, leverages a different pre-training framework , and adopts a slightly different model architecture. Since the intermediate checkpoints of LLaMA are not available, we extract the pre-training loss and corresponding performance on six question answering and commonsense reasoning tasks from the figures in its original paper, and plot the points in Figure 3.

Excitingly, most data points from the LLaMA models with different sizes (7B, 13B, 33B, 65B) fall on the same upwards trend. This observation further confirm our conclusion that the model's pre-training loss can predict its performance on downstream tasks, regardless of model size and token count. Note that there is only one exception at the early stage of LLaMA-65B. We can see that when the training loss is higher than 1.8, LLaMA-65B performs worse than smaller models with the same training loss. Without access to its intermediate checkpoints, we unfortunately cannot further analyze the result. One possible explanation is that they use exponential smoothing on either the loss or downstream performance plots. Exponential smoothing would perturb the earlier points more than other points, potentially leading to this effect. Note that the outliers only constitute the initial 10% training tokens. The results for Pythia are shown in Appendix F, which also support our conclusion.

Observed from previous experiments and analysis, we can conclude that the pre-training loss is a good indicator of LMs' performance on downstream tasks, independent of model sizes, training tokens, languages, and pre-training frameworks.

## 3 Analysis of Different Tasks and Metrics

### Performance Trends of Different Tasks

In Figures 1 and 2, we can separate the datasets into two groups: First, on TriviaQA, HellaSwag, RACE, WinoGrande, NLPCC-KBQA, ClozeT, CLUEWSC, and C3, the performance improves smoothly with decreased pre-training loss from the very beginning. Second, on MMLU, C-Eval, GSM8K, and GSM8K-Chinese, the performance remains flat when the loss is higher than a certain threshold. Once the pre-training loss is lower than this threshold, the performance starts to improve.

Figure 3: **The performance-vs-loss curves of LLaMA.** The values of performance and training loss are extracted from the figures in the original LLaMA paper . Note that the LLaMA2 paper  does not cover such figures with related information.

The correlation coefficients in Table 2 also reveal the difference: coefficients in the first group are close to -1, indicating strong correlations, while correlations in the second group are weaker.

Take MMLU as an example of the second group, when the pre-training loss is higher than 2.2, the accuracy remains around 25%. Since each question in MMLU has four options, this means the model prediction is no better than random guessing. However, when the pre-training loss drops below 2.2, the accuracy increases as the loss decreases, similar to the trend observed in the first group of tasks. The performance trends of C-Eval, GSM8K, and GSM8K-Chinese follow a similar pattern. Despite differences in languages, tasks, prompting types, and answer forms among the four datasets are different, the thresholds for performance improvement are surprisingly all around 2.2.

RACE in the first group has a prompting format similar to MMLU: both consist of multi-choice examination questions with in-context demonstrations, but their performance curves are quite different. We hypothesis that it is the task difficulty that makes the difference. Tasks of the first group of datasets are easier than those of the second group. For example, RACE requires the model to select correct answers for questions about a given article, and HellaSwag lets the model to select the possible followup of a situation based on commonsense. In contrast, MMLU and C-Eval consist of questions designed for high school, college, or professional examinations, requiring a broader range of knowledge. GSM8K and GSM8K-Chinese are math word problems that were previously considered as impossible to be solved by pre-trained language models before Chain-of-Thought prompting.

The phenomenon can be related to grokking, which describes the improvement of performance from the random chance level to perfect generalization . Power et al.  find that this improvement can occur well past the point of overfitting. In pre-training, the models are usually underfitting instead of overfitting overall. Since the pre-training corpus is a mixture of different documents, it is possible that the model already fits some patterns--such as numerical addition--in the data, while still underfitting the overall corpus.

Certainly, the observations on the second groups of datasets can also be related to emergent abilities , that is, abilities that only present in large models. According to the scaling law , with the number of training tokens fixed, the pre-training loss follows a power law with respect to model sizes. In other words, there is a monotonic relationship between model size and pre-training loss. For the second group of tasks, there is a threshold of model sizes that corresponds to the tipping point in the pre-training loss. When the model size exceeds this threshold, the model can exhibit performance above the random chance level.

Figure 4: **The performance-vs-loss curves of different metrics on MMLU and C-Eval. Accuracy: discontinuous; CorrectChoiceProb and BrierScore: continuous. We mark the result of random guess in black dashed lines.**

### Influence of Different Metrics

Schaeffer et al.  propose an alternative explanation of emergent abilities of LMs, that is, emergent abilities appear due to the researchers' choice of nonlinear or discontinuous metrics. The accuracy on multi-choice questions (e.g., MMLU) is discontinuous, since the score on a question is either 1 or 0. To validate this claim, we examine the intermediate checkpoints on MMLU and C-Eval with continuous metrics rather than discontinuous accuracy used in the original benchmarks. The first metric is the predicted probability of the correct answer, denoted as CorrectChoiceProb. The second one is the Brier Score  used in Schaeffer et al. :

\[=_{i=1}^{N}_{j=1}^{C}(y_{ij}-_{ij })^{2} \]

where \(_{ij}\) is the predicted probability of sample \(i\) for class \(j\) and \(y_{ij}\) is the ground truth probability. The metric measures the prediction error and a lower value indicates better performance.

We plot the results measured by different metrics on MMLU and C-Eval in Figure 4. All three metrics--accuracy, correct choice probability, and Brier Score--show emergent performance improvements (value increase for the first two and decrease for the third) when the pre-training loss drops below a certain threshold. The Brier Score also decreases when the pre-training loss is above the threshold. However, the decrease of Brier Score does not always represent improvements on the task, since the Brier Score is related to not only the predicted probability of the correct answer but also the predicted probabilities of the incorrect answers. We find that the distribution of the correct answers is uniform in the four options in MMLU and C-Eval. The best Brier Score for a context-free predictor is achieved by always giving uniform probability to all the options. In this case, the Brier Score is equal to 0.75. Therefore, the performance in terms of Brier Score is no better than random guess before the loss reaches the threshold. This observation further confirms our previous conclusion. We discuss the contrary observations of Schaeffer et al.  and Xia et al.  in Appendix C.

We conclude that emergent abilities of language models occur when the pre-training loss reaches a certain tipping point, and continuous metrics cannot eliminate the observed tipping point.

## 4 Defining Emergent Abilities from the Loss Perspective

In previous sections, we show that 1) the pre-training loss is predictive of the performance of language modes on downstream tasks, and 2) some tasks exhibit emergent performance improvements from the random guess level when the pre-training loss drops below a certain threshold regardless of model size, token count, and continuity of metrics. Based on these observations, we give a new definition of emergent abilities from the pre-training loss perspective:

**Definition**.: _An ability is emergent if it is not present in models with higher pre-training loss but is present in models with lower pre-training loss._

The normalized performance on an emergent ability as a function of the pre-training loss \(L\) is:

\[f(L)&L<\\ 0& \]

where \(f(L)\) is a monotonically decreasing function of \(L\), \(\) is the threshold, and the normalized performance of random guess is 0.

Next we will show how the new definition can be related to previously observed emergent abilities . In Henighan et al. , they give the scaling relation for the loss with model size \(N\) when the number of training tokens \(D\) is fixed:

\[L(N)=L_{}+(}{N})^{_{N}} \]

where \(L_{}\) is the irreducible loss, and \(_{N}\) is the coefficient. The equation shows that the loss of language models follows a power-law plus a constant. Combining Equation (3) and Equation (4), we can get the normalized performance as a function of the model size \(N\)

\[f(L_{}+(}{N})^{_{N}} )&N N_{0}(-L_{})^{-}}\\ 0& \]From this equation, we can explain the emergent abilities observed in Wei et al. : when model sizes are smaller than \(N_{0}(-L_{})^{-1/_{N}}\), the normalized performance is zero. When model sizes exceed \(N_{0}(-L_{})^{-1/_{N}}\), the increase in model size leads to a decrease of pre-training loss and an improvement in normalized performance.

## 5 Related Work

**Relationship of Pre-training Loss and Task Performance.** In the transfer learning setting, i.e. the language model is pre-trained on the general corpus and fine-tuned on supervised data of specific tasks, Tay et al.  find that models with the same pre-training loss can have different downstream performance after finetuning, due to inductive bias in model architectures such as Transformers and Switch Transformers. Tay et al.  further study the effect of model shapes on downstream fine-tuning. Liu et al.  also study the effect of inductive bias of model sizes and model algorithms on the relationship of pre-training loss and downstream performance after fine-tuning, but their theory only applies in the saturation regime, where the models are close to minimal possible pre-training loss. Instead, large language models today are generally under-trained , far from the saturation regime. Overall, these studies focus on the pretrain-finetune paradigm, in which inductive bias helps improve transferability, while we study prompted performance of large language models without finetuning . For the prompted performance of large language models, Xia et al.  claim that perplexity is a strong predictor of in-context learning performance, but the evidence is limited to the OPT model  and a subset of BIG-Bench . Instead, Shin et al.  find that low perplexity does not always imply high in-context learning performance when the pre-training corpus changes. Gadre et al.  fits the relation of perplexity and the top-1 error averaged over many natural language tasks with a power law. Instead, we focus on the different relations of tasks and a small part of tasks that show emergency trends.

**Emergent abilities.** Wei et al.  propose the idea of emergent abilities, abilities that only present in large language models. This is similar to the claim of Ganguli et al.  that it is more difficult to predict the capacities of language models than to predict the pre-training loss. The existence of emergent abilities has been challenged. Hoffmann et al.  show that smaller language models trained with sufficient data can outperform undertrained larger language models, supported by follow-up models . On the other hand, Schaeffer et al.  claim that emergent abilities are due to the discontinuous metrics used for evaluation, also found in Xia et al. . Similarly, Hu et al.  propose to predict the performance of emergent abilities with the infinite resolution evaluation metric. In this paper we prove the existence of emergent abilities from the perspective of pre-training loss, even with continuous metrics.

## 6 Conclusion

Our paper proposes a new definition of emergent abilities of language models from the perspective of pre-training loss. Empirical results show that the pre-training loss is a better metric to represent the scaling effect of language models than model size or training compute. The performance of emergent abilities exhibits emergent increase when the pre-training loss falls below a certain threshold, even when evaluated with continuous metrics.

The new definition offers a precise characterization of the critical junctures within training trajectories where emergent abilities manifest. It encourages future studies to investigate the shifts in language models at these junctures, which facilitate the development of new capabilities.

## 7 Limitation

We study the relationship of pre-training loss and task performance across model sizes, training tokens, tasks, languages, prompting types, and answer forms. Factors we have not considered are model architectures and training algorithms. We analyze the performance-loss curves of LLaMA and Pythia with slightly different architectures, and find that the relationship holds for all the models. But there are fundamentally different model architectures, such as routed Transformers , and non-Transformer architectures  beyond our consideration. Both our models and LLaMA use AdamW optimizer , while there are other optimizers for language model pre-training .

The disadvantage of studying emergent abilities in the lens of pre-training loss is that the pre-training loss is affected by the tokenizer and the distribution of pre-training corpus. The values of pre-training loss of language models trained on different corpus are not directly comparable. One possible solution is to evaluate different language models on a public validation set with the normalized perplexity  to account for the different vocabulary sizes.

The paper should not be considered as a push to expand model sizes and data sizes of language models beyond current scales. It is not guaranteed that new tipping points emerge in larger scales. Also, instruction tuning  can improve the zero-shot performance of language models on unseen tasks, including MMLU and GSM8K.