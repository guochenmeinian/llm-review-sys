# Star-Agents: Automatic Data Optimization with LLM Agents for Instruction Tuning

Hang Zhou\({}^{1,2}\), Yehui Tang\({}^{2}\), Haochen Qin\({}^{2}\), Yujie Yang\({}^{2}\), Renren Jin\({}^{1}\),

**Deyi Xiong\({}^{1}\)1, Kai Han\({}^{2}\)1, Yunhe Wang\({}^{2}\)1 \({}^{1}\)**College of Intelligence and Computing, Tianjin University, Tianjin, China.

\({}^{2}\)Huawei Noah's Ark Lab.

{zhouhang25, yehui.tang, qinhacohenl, yangyujie26}@huawei.com,

{rrjin, dyxiong}@tju.edu.cn, {kai.han, yunhe.wang}@huawei.com.

###### Abstract

The efficacy of large language models (LLMs) on downstream tasks usually hinges on instruction tuning, which relies critically on the quality of training data. Unfortunately, collecting high-quality and diverse data is both expensive and time-consuming. To mitigate this issue, we propose a novel Star-Agents framework, which automates the enhancement of data quality across datasets through multi-agent collaboration and assessment. The framework adopts a three-pronged strategy. It initially generates diverse instruction data with multiple LLM agents through a bespoke sampling method. Subsequently, the generated data undergo a rigorous evaluation using a dual-model method that assesses both difficulty and quality. Finaly, the above process evolves in a dynamic refinement phase, where more effective LLMs are prioritized, enhancing the overall data quality. Our empirical studies, including instruction tuning experiments with models such as Pythia and LLaMA, demonstrate the effectiveness of the proposed framework. Optimized datasets have achieved substantial improvements, with an average increase of 12% and notable gains in specific metrics, such as a 40% improvement in Fermi, as evidenced by benchmarks like MT-bench, Vicuna bench, and WizardLM testset. Codes will be released soon1.

## 1 Introduction

The research and development of natural language understanding and generation have been dramatically accelerated with the emergence and prevalence of LLMs . These models have been extensively applied in a wide range of scenarios, e.g., question answering and text generation, significantly enhancing downstream task performance due to their exceptional ability to follow instructions . Such an instruction-following capability is primarily acquired through a process known as instruction tuning , where LLMs are fine-tuned on instruction data. It is hence widely acknowledged that the quality of instructions plays a pivotal role .

Historically, the creation of instruction data for training LLMs has heavily relied on the expertise of human annotators, as evidenced by substantial scholarly contributions . While expert-driven data generation assures the production of high-quality instructions, the enormous volume of data necessary for effective training renders this method economically untenable. In response, recent efforts have shifted towards the utilization of LLMs to automatically generate instructions, thereby mitigating the reliance on costly human annotation . Concurrently,there is a growing emphasis on the generation and selection of challenging examples, grounded in the belief that more complex and difficult instructions can substantially elevate model capabilities [22; 17].

Despite the clear advantages of using LLMs for data generation, several challenges persist in this strategy. Primarily, previous efforts often depend on a single LLM, resulting in data that may lack stylistic variety  and encompass a limited range of difficulty levels, which may not be ideal for all models. Additionally, there is a trend towards the creation of exceedingly complex instructions [19; 44; 18], which may surpass the operational capabilities of models with small parameter scale, thereby hindering their ability to fully capitalize on the data's potential for performance enhancement.

To address the aforementioned challenges, we propose the **Star-Agents** framework, an advanced automatic data optimization system specifically designed to learn and refine instruction samples with suitable complexity and diversity for target LLMs. The framework consists of three main components. First, to increase the diversity of generated data, an instruction data rewriting process involving multiple advanced LLM agents is proposed. This process samples different LLM agents for rewriting instructions and responses separately (referred to as agent-pairs). Next, to select high-quality samples, the generated data undergo a dual-model evaluation function with appropriate complexity as the selection metric. Finally, to balance data diversity and quality, the sampling probability of agent-pairs is adjusted and evolved based on the composite scores of the selected data, identifying agent-pairs that generate high-quality data.

Extensive experiments are conducted to evaluate instruction-following capabilities of LLMs on a variety of benchmark datasets, including MT-bench , Vicuna-bench , and the WizardLM testset . Instruction tuning experiments with LLMs such as Pythia and LLaMA, demonstrate the effectiveness of the Star-Agents framework. LLMs trained on data generated by Star-Agent outperform those (the same LLMs) trained on the Evol-Instruct dataset  or data selected according to the Instruction-Following Difficulty (IFD) metric . Significantly, the optimized datasets have resulted in an average performance improvement of 12%, with some metrics such as Fermi demonstrating gains of over 40%.

## 2 Related Work

Our work is related to both instruction data generation and selection. We briefly review these topics within the constraint of space.

Instruction Data GenerationDatasets like Dolly  and OpenAssistant  are built from human-generated instruction data. The ShareGPT dataset, built from conversations between humans and ChatGPT, has been effectively used to improve the instruction-following performance of fine-tuned models . Both Self-Instruct  and Alpaca  leverage the generation capabilities of GPT-3 to expand seed instructions. The generated instructions undergo filtering to eliminate low-quality instructions while the kept instructions are used to fine-tune the model to enhance the model's ability to respond to instructions. Baize  proposes a self-dialogue framework, using questions from popular Q&A websites as starting topics, then having LLMs converse with themselves. CAMEL  introduces a role-playing framework where LLMs discuss a given topic when playing a role as either "user" or "assistant". UltraChat  uses real-world named entities combined with various text-writing tasks to generate diverse and high-quality multi-turn dialogues for LLMs. Lion  introduces the concept of adversarial distillation, using the Imitation-Discrimination-Generation stages to iteratively generate data, refine existing instructions, and produces more complex and diverse instructions to expand the capabilities of the student model. Evol-Instruct  uses five manually designed prompts to explicitly guide LLM in rewriting existing simple instructions into more complex ones. The WizardLM model, trained with Evol-Instruct, ranks highly on MT-Bench , highlighting the importance of data quality in training effective LLMs.

Instruction Data SelectionWith the aforementioned methods, it is not difficult to use LLMs to generate large instruction tuning datasets at low cost. However, for instruction-tuned language models, data quality is more crucial than quantity. In this aspect, ALPAGASUS  evaluates the effectiveness of instruction data by leveraging ChatGPT. INSTAG  automatically generates tags for instruction samples with ChatGPT and keeps diversity by selecting subsets with more tags. Cherry LLM  pioneers the self-guided approach, using the IFD metric to measure the difficulty for an LLM to learn an instruction sample. This allows to select instruction samples that significantly enhance training efficiency without resorting to an external model. DEITA  first uses ChatGPT to evaluate the complexity and quality of samples, then assesses the diversity of samples based on the distance between model embeddings, thereby guaranteeing complexity, quality, and diversity in the subset. LIFT  first guides GPT-4 to generate challenging instructions to expand the data distribution and then uses dimensionality reduction and row variance analysis to select representative high-quality data, where GPT-4 generates a quality score for each instruction. LESS  first stores the gradient features of samples in the dataset, then calculates the similarity between a small number of samples from the target task and the training data samples. Based on the calculated similarity scores, it selects the training samples whose gradient features are most similar to those of the target task samples as the fine-tuning instances. Data selection not only improves training efficiency but also prevents low-quality or poison data from undermining model performance by filtering them out .

## 3 Star-Agents

The aim of our research is to construct a high-quality dataset \(T\) of tailored complexity for the target LLM through the enhancement of an initial seed dataset \(S=(I_{i},R_{i})_{i=1}^{N}\), consisting of instruction-response pairs \((I,R)\).

To this end, we introduce the Star-Agents Framework, depicted in Figure 1, which is segmented into three steps. The first step leverages a spectrum of advanced LLMs, each trained independently. These models are engaged in a dynamic interaction to generate a diverse data candidate set \(D(S_{i})\) by sampling agent-pair derived from \(S_{i}\) as detailed in Section 3.1. Following this, we apply a dual

Figure 1: The diagram of the Star-Agents Framework. Step 1 is designed to gather diverse instructions and responses as shown in Appendix A.3. Step 2 focuses on selecting high-quality, tailored data from the data collected in Step 1. Finally, Step 3 aims to enhance the effectiveness and efficiency of the data generation process by evolving the Star-Agents framework.

model evaluation strategy \(()\) to meticulously extract the most suitable data from \(D(S_{i})\), aiming to substantially elevate the target model's performance. This process is elaborated in Section 3.2. To enhance the effectiveness and efficiency of the Star-Agents framework in generating tailored data, we have developed an evolutionary strategy for the Star-Agents, as discussed in Section 3.3. After these three steps, a tailored high-quality dataset \(T\) is obtained from the seed dataset, which is formulated as:

\[T=\{_{d D(S_{i})}(D(S_{i}))\ |\ i=1,2,,N\}. \]

### Generating Diverse Data

To improve the instruction-tuned model, it is crucial to assemble a high-quality and diverse instruction dataset . Traditional methods often use a single LLM, such as ChatGPT, for data enrichment. In contrast, our approach employs multiple LLMs to avoid monotonous data distribution. This multifaceted strategy also addresses the limitations and risks of quality degradation on domain-specific tasks associated with using a single model. To counter these challenges, we propose to use an Agent-Pair strategy.

Agent-Pair.Utilizing a spectrum of LLMs, each trained with discrepant setting, facilitates the generation of varied responses to given instructions. This diversity is crucial for synthesizing a dataset characterized by high richness .

The Star-Agents framework strategically pairs different LLMs to rewrite the instructions in the seed dataset and generate new responses to increase the diversity. With agent-pair \((A_{j}^{I},A_{k}^{R})\), a new instruction data can be generated as follows:

\[f_{j,k}(I_{i},R_{i})=(A_{j}^{I}(I_{i}),A_{k}^{R}(R_{i})), \]

where \(A^{I}\) and \(A^{R}\) represent the agents that rewrites the instruction and response to the instruction, respectively.

Given the high cost of deploying all agent-pairs, a feasible solution to balance cost and agent diversity is to sample a subset of agent-pairs from the Star-Agents for data generation. Equation 3 formulates this process, where \(D\) is collected dataset generated by performing \(f\) over all sampled pairs \((A_{j}^{I},A_{k}^{R})\) of instruction agents \(A_{j}^{I}\) and response agent \(A_{k}^{R}\) with sampling probabilities \(p_{jk}\):

\[D(S_{i})=\{f_{j_{1},k_{1}}(S_{i}),,f_{j_{M},k_{M}} (S_{i})\ |\ (j_{m},k_{m}) p_{jk},m=1,2,,M\}, \]

\(M\) is number of agent-pairs sampled for a single seed sample. The sampling probability \(p_{jk}\) is initialized as a uniform distribution and will be updated using the method described in Subsection 3.3 during data generation. Meanwhile, an Instruction Memory Bank that stores high-quality instructions will be updated. To ensure the lower bound of data quality, each iteration will consistently call a fixed set of agent-pairs, referred to as base agent-pairs.

### Evaluating Tailored Data via a Dual-model Strategy

Identifying and selecting tailored data from a diverse dataset is crucial for enhancing model performance, especially since the presence of low-quality data can impede model functionality. It is acknowledged that data samples that are lengthy, complex, and challenging significantly benefit the instruction tuning process .

Nevertheless, too complex instruction data may be not necessarily benefit model performance. We have observed that for models with 14M and 70M parameters as illustrated in Figure 2, the Evol-Instruct dataset, though more challenging than the Alpaca dataset, results in diminished model performance. This suggests that intricate examples may surpass the capabilities of small models and be harmful for model performance, despite the advantages of using complex data for large models.

Dual-model Evaluation.To address the issue mentioned above, we propose to use a larger model to evaluate the difficulty of data instances together with the evaluation from a smaller model (target LLM), hence termed as dual-model evaluation. Inspired by Cherry LLM , we employ the IFD metric to measure the degree of difficulty a data sample presents to the target model, which is calcuated as

\[(I_{i},R_{i})=|}_{w R_{i}}  P(w|I_{i}))}{(-|}_{w R_{i}} P(w) )}. \]

We assume that for the same sample, stronger model yields a smaller IFD score. When the IFD scores of the two models are close to each other, it indicates that the sample is either too simple or too complex, which is not contributive to effective learning. However, when their IFD scores differ significantly, it indicates that the data is sufficiently complex for the smaller model but still within the capability range of the stronger model. This is a tailored complexity for facilitating learning. The above data assessment method is illustrated at Figure 3 and formulated as

\[_{}^{i}=_{}(I_{i},R_{i})- _{}(I_{i},R_{i})}{ (_{}(I_{i},R_{i})-_{} (I_{i},R_{i}))}. \]

Noising data can be endowed with high score since the dual-model metric considers only the relative complexity with the neglect of generation quality. To address this issue, we utilize an LLM as referee for data sample scoring. This involves comparing each data sample in the same batch of diverse data samples generated by selected agent-pairs against a base data sample generated by base agent-pairs. There are three potential outcomes: the base data sample is better, the diverse data sample is better, or a tie, as shown in Appendix A.1. These outcomes are quantitatively assigned as quality scores, thereby avoiding collecting noising instruction samples:

\[_{}=0,&,\\ 1,&,\\ 0.5,&. \]

Finally, the evaluation scores from both the LLM and the dual-model evaluation are combined to compute a final composite score:

\[=_{}_{}. \]

This score determines the overall quality and suitability of data for enhancing the model's capabilities. The highest scoring data sample is then selected into dataset \(T\) and Instruction Memory Bank as detailed in Section 3.3, ensuring that the chosen dataset maximizes potential improvements in model performance.

### Evolving Star Agents

As mentioned in Section 3.1, we use the joint probability of instruction agents and response agents to regulate the invocation of each agent-pair. Considering the abilities and specialities of each LLM vary, however, sampling each agent-pair with the same probability is not optimal. We hence use the score from Section 3.2 to dynamically evolve the sampling probability. Additionally, since the generation performance of agent-pairs is task-dependent, we also propose an Instruction Memory Bank to select the most suitable agent-pair for particular tasks.

Agent-Pair Sampling Evolution.Section 3.2 has introduced the score \(\), which effectively estimates the quality of generated samples. During each iteration, if the generated samples are of high quality, we will increase the sampling probability of the selected agent-pair, which is updated as follows:

\[_{jk}&=p_{jk}+( I_{i},R_{i}),\\ p_{jk}&_{jk}}{_{j,k} _{jk}}. \]

The updated sampling probability for the agent-pair of the \(j\)-th instruction agent and \(k\)-th response agent that successfully process the \(i\)-th data sample will be used in the next iteration, where \(\) denotes the evolution rate.This formula adjusts the sampling probabilities based on the effectiveness demonstrated by agent-pairs in generating relevant data. Iterative updates ensure that as the synthesis process advances, the probability of selecting more effective agent-pairs increases, while less effective pairs are gradually phased out.

Instruction Memory Bank Evolution.We establish an Instruction Memory Bank storing high-quality instructions aiming to accelerate sampling and relate the evolution with task data. When processing a data sample \((I_{i},R_{i})\), we perform a query in the Instruction Memory Bank for \(I_{i}\), retrieving the top \(n\) closest matches according to embedding similarity. The associated agent-pairs, identified as highly proficient for tasks similar to \(I_{i}\), are then sampled. We sample \(l\) agent-pairs from this pool using normalized probabilities to generate diverse data. Moreover, to foster the creation of a diverse dataset, additional \(M-l\) agent-pairs are sampled from the remaining pool using their respective probabilities to assist in data synthesis. As a result, \(M\) new samples are generated and then feed for data assessment. Subsequently, the Instruction Memory Bank will continuously evolve by incorporating tailored high-quality data, which get high scores as introduced in Section 3.2.

## 4 Experiments

We conducted extensive experiments to evaluate the proposed Star-Agents framework. A wide range of LLMs, benchmark datasets were used in our experiments to guarantee the robustness of our evaluation.

### Setups

Datasets.In alignment with the WizardLM , we adopted the Supervised Fine-Tuning (SFT) dataset, designated as the Evol-Instruct dataset, which consists of 70,000 instruction-response pairs. The instructions in this dataset were refined using "In-Depth Evolving" and "In-Breadth Evolving" methods, which were tailored to enhance the base instructions by adding intricate details or expanding the overall scope, respectively. To guarantee the fidelity of the data, ChatGPT was also integrated as generator into the refinement process. The quality of the instruction data from the Evol-Instruct dataset has been validated as superior [44; 25]; hence, our research continues to leverage these refined instructions. Employing the Star-Agents framework, our study invokes multiple LLMs to generate diverse and high-quality responses for these instructions. For further enriching our comparative analysis, we employed the Alpaca dataset , comprising 52,000 instruction-following samples. This dataset, developed under the self-instruct paradigm, utilizes the ChatGPT2 instead of text-davinci-003 for a fair comparison .

During our experiments, we integrated as generator a diverse array of LLMs, as detailed in Table 1. Our hypothesis posits that models from different development teams possess unique capabilities, yielding rich responses to identical prompts due to the diversity in their training data and strategies. For instance, the Phi2  employed 1.4T tokens of meticulously curated textbook-like data without undergoing Reinforcement Learning with Human Feedback (RLHF) while the Gemma  was trained on 6T tokens primarily sourced from English web documents, mathematical content, and code, with subsequent fine-tuning through SFT and RLHF. To ensure the diversity and quality of generated data, we assembled LLMs trained by different teams, widely regarded for their exceptional performance. In pursuit of fostering the generation of data across varying levels of difficulty, the utilized LLMs range from 2.7B to 14B parameters, including even larger models via API access. For a fair comparison with the Evol-Instruct dataset, the most capable model employed was the ChatGPT, which was also used for generating responses within the Evol-Instruct dataset. Notably, the ChatGPT was also served as evaluator to compute the comparison score \(_{}\).

Benchmarks.To rigorously evaluate the instruction-following capabilities of AI models, we utilized three widely used benchmarks: MT-bench, Vicuna-bench, and the WizardLM testset. Specifically, MT-bench and Vicuna-bench are designed to test the models' competencies in various complex cognitive tasks, including mathematics, reasoning, complex format handling, and writing through both multi-turn and single-turn dialogues. The WizardLM testset, conversely, extends the evaluation to encompass diverse fields such as technology, biology, and law. It also features varied difficulty levels to facilitate a more nuanced comparison of models' performance disparities. Following established protocols, we employed the Fast-Chat  to assess model performances, with GPT-4 acting as the judge model.

Baselines.For baseline comparisons, we employed the Pythia-1B and Llama-2-7B, both trained using the Evol-Instruct datasets. The Alpaca datasets were also referenced for comparative analysis, alongside IFD  and Random select as an additional comparison for data selection methods.

Implementation Details.We fine-tuned our models (Pythia-1B and Llama-2-7B) over three epochs using the Adam optimizer, with an initial learning rate of \(2 10^{-5}\), a maximum token count of 2048, and a batch size of 64. For the Star-Agents, 10 agent-pairs were employed.

### Main Results

GPT-4 Automatic EvaluationBased on the findings summarized in Table 2, comprehensive training sessions were conducted for the Pythia-1B and Llama-2-7B models utilizing three distinct datasets: Alpaca, Evol-Instruct, and the optimally refined _Star Instruct_ datasets. The latter was developed through the application of Star-Agents, which are derivatives of the Evol-Instruct datasets. Through comparative analyses with other contemporary state-of-the-art models, we observe that the SFT-aligned models employing the _Star Instruct_ datasets consistently outperform nearly all aligned counterparts, across all evaluated model families.

   Model Famliy & Model Size & Data Size & Method & Source \\  Phi  & 2.7B & 1.4T & Pretrain & Microsoft \\ ChatGLM  & 6B & 1T+ & SFT \& RLHF & Zhipu AI \\ Gemma  & 7B & 6T & SFT \& RLHF & Google \\ Mistral  & 7B & - & SFT & Mistral \\ Qwen  & 14B & - & SFT \& RLHF & Alibaba \\ ChatGPT & - & - & SFT \& RLHF & OpenAI \\   

Table 1: Typical LLMs utilized in Star-Agents.

Notably, at the 1B scale, models trained with the _Star Instruct_ dataset demonstrate significant superiority, surpassing baselines across diverse evaluation datasets. Remarkably, in comparison to models trained with the Evol-Instruct dataset, those utilizing _Star Instruct_ achieve an average absolute improvement of approximately 0.45, which is corresponding to a performance enhancement of about 12%. Additionally, when compared to models trained with the Alpaca dataset, our framework achieves an absolute improvement of 1 point, thereby affirming that the _Star Instruct_ dataset is particularly well-suited for the Pythia-1B model, significantly boosting its operational efficacy. Additionally, within the 7b model category, the Llama-2-7B-star_instruct outperforms the sRecycled-Wiz-7B-v2 , which is trained on the Evol-Instruct dataset enhanced by Selective Reflection-Tuning. Figure 3(a) illustrates the Llama-2-7B-star_instruct's performance enhancements across nine metrics, with notable substantial improvements in math, coding and fermi problem-solving, where improvements surge up to 40%. A similar phenomenon can be observed in Figure 3(b). Additionally, comparative examples of single-turn and multi-turn dialogues are provided in A.2, and the performance on the Open LLM Leaderboards of LLMs can be found in A.4.

  
**Model** & **Vicuna-Bench** & **WizardLM testset** & **MT-Bench** & **Average** \\   \\  Pythia-1B  & 1.68 & 1.34 & 1.17 & 1.40 \\ OPT-1.3B  & 2.49 & 1.64 & 1.12 & 1.75 \\ Sheared-LLaMA-1.3B  & 2.73 & 1.86 & 1.59 & 2.06 \\ Pythia-1B-alpaca & 4.14 & 2.97 & 2.20 & 3.10 \\ Pythia-1B-evol_instruct & 5.07 & 3.55 & 2.56 & 3.73 \\ Pythia-1B-IFD  & 4.60 & 3.21 & 1.98 & 3.26 \\ Pythia-1B-Random & 5.13 & 3.39 & 2.35 & 3.62 \\ Pythia-1B-star_instruct & **5.93** & **3.90** & **2.69** & **4.17** \\   \\  Llama-2-7B  & - & - & 3.95 & - \\ zephyr-beta-stf  & - & - & 5.32 & - \\ mpt-7B-chat  & - & - & 5.45 & - \\ XGen-7B-8k-Inst  & - & - & 5.55 & - \\ sRecycled-Wiz-7B-v2  & - & - & 5.56 & - \\ Llama-2-7B-alpaca & 6.33 & 5.08 & 3.63 & 5.01 \\ Llama-2-7B-evol_instruct & 7.27 & 6.57 & 5.21 & 6.35 \\ Llama-2-7B-star_instruct & **8.24** & **6.87** & **5.74** & **6.95** \\   

Table 2: Results of different models on Vicuna-bench, WizardLM testset and MT-Bench.

Figure 4: Radar plot of detailed scores for Llama-2-7B-star_instruct against the major baseline on different subtasks of (a) Vicuna-Bench and (b) MT-Bench.

### Ablation Study

Main Components.As illustrated in Table 3, we conducted ablation experiments on the three principal components within the Star-Agents framework. Results indicate that models using solely diversified datasets with random sampling yield a bit lower performance than the baseline. This occurs because the baseline employs data generated by ChatGPT, which is of high quality. In contrast, the diversified datasets draw from a variety of sources, making it challenging to ensure uniformly high quality. Thus, random sampling may introduce low-quality data, leading to diminished model performance. The inclusion of a data selection module subsequently leads to a recovery in model performance, suggesting that this module effectively selects high-quality data suitable for the model. Integration of the evolution strategy also provides a significant improvement, demonstrating that the evolution module can effectively select the most appropriate data generation agent-pairs from a complex array of candidate agent-pairs.

Selection Method.As demonstrated in Table 4, we evaluated a range of conventional selection methods, including both random selection and strategies informed by the IFD . Our dual-model selection strategy significantly outperforms these approaches. Compared to random selection, our method achieves a significant improvement, registering an improvement exceeding 0.5 points on average across a variety of test sets. When compared with the IFD approach, our enhancement approaches a 0.9 point. These findings robustly validate the effectiveness of our dual-model selection strategy, illustrating its superior performance in refining model selection precision using diverse evaluation metrics.

Evolution.As depicted in Figure 5, we analyzed the sampling probability curves of typical agent-pairs throughout an iterative evolutionary process. Initially, each agent-pair began with a sampling probability of approximately 10%. Due to its robust performance, the Mistral-ChatGPT receives consistent rewards, which leads to a gradual increase in its sampling probability. By the completion of about 70,000 iterations, this probability has escalated to 30%. In stark contrast, the Phi2-ChatGPT undergoes a steady decline over the same period, with its sampling probability ultimately plummeting to near zero as it is progressively phased out. Concurrently, the ChatGLM3-ChatGPT exhibits a relatively stable trajectory, albeit with a slight downward trend. Evolutionary trajectories present significant discrepancy indicating different generation suitability of different generators on different tasks, where all the differences are captured by our evolution mechanism.

    & Components & & **Average Score** \\  Diversity & Data selection & Evolution & \\  ✓ & ✓ & ✓ & **4.17** \\ ✓ & ✓ & ✕ & 3.97 \\ ✓ & ✕ & ✕ & 3.62 \\ ✕ & ✕ & ✕ & 3.73 \\   

Table 3: Impact of different components.

Figure 5: Evolution of the typical Agent-Pairs.

  
**Model** & **Average Score** \\  Pythia-1B-eyed instruct & 3.73 \\ Pythia-1B-IFD  & 3.26 \\ Pythia-1B-Random & 3.62 \\ Pythia-1B-star\_instruct & **4.17** \\   

Table 4: Impact of the selection method.

Conclusion

In this paper, we have presented the Star-Agents framework, an automated system for optimizing data to be optimally challenging for target LLMs. This framework has been applied to the open-source SFT datasets, and we conduct training sessions on a variety of model families, adjusting the data to enhance its efficacy. Our empirical investigations include a series of instruction tuning experiments that utilize both multiple baselines and specially optimized datasets on well-known models such as Pythia and LLaMA. Extensive experiments confirm the substantial impact of our method: the optimized tailored datasets result in an average performance enhancement of approximately 12%, with certain metrics, especially those involved in Fermi problem tasks exhibiting increases exceeding 40%, as substantiated by results on benchmarks such as MT-bench, Vicuna bench, and the WizardLM testset. These findings underscore the premise that strategically diverse and tailored data can profoundly improve model alignment and performance. In conclusion, our research details a highly effective automated framework that significantly augments dataset functionality, thus fostering more efficient model alignment.

Limitations.Our approach achieves remarkable performance improvements on single-turn instruction datasets. However, it has not yet been evaluated on multi-turn conversations. We hence leave the evaluation on multi-turn instruction datasets and validation on datasets with domain-specific instructions to our future work.