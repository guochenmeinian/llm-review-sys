# MGDD: A Meta Generator for

Fast Dataset Distillation

 Songhua Liu Xinchao Wang

National University of Singapore

songhua.liu@u.nus.edu, xinchao@nus.edu.sg

Corresponding Author.

###### Abstract

Existing dataset distillation (DD) techniques typically rely on iterative strategies to synthesize condensed datasets, where datasets before and after distillation are forward and backward through neural networks a massive number of times. Despite the promising results achieved, the time efficiency of prior approaches is still far from satisfactory. Moreover, when different sizes of synthetic datasets are required, they have to repeat the iterative training procedures, which is highly cumbersome and lacks flexibility. In this paper, different from the time-consuming forward-backward passes, we introduce a generative fashion for dataset distillation with significantly improved efficiency. Specifically, synthetic samples are produced by a generator network conditioned on the initialization of DD, while synthetic labels are obtained by solving a least-squares problem in a feature space. Our theoretical analysis reveals that the errors of synthetic datasets solved in the original space and then processed by any conditional generators are upper-bounded. To find a satisfactory generator efficiently, we propose a meta-learning algorithm, where a meta generator is trained on a large dataset so that only a few steps are required to adapt to a target dataset. The meta generator is termed as _MGDD_ in our approach. Once adapted, it can handle arbitrary sizes of synthetic datasets, even for those unseen during adaptation. Experiments demonstrate that the generator adapted with only a limited number of steps performs on par with those state-of-the-art DD methods and yields \(22\) acceleration.

## 1 Introduction

Dataset distillation (DD) introduced by Wang _et al._ aims to compress an original dataset \(\) into a much smaller synthetic set \(\), such that the performance of a neural network, trained with the condensed dataset \(\), is similar to the network trained with \(\). The derived synthetic datasets not only save the cost of storage and transmission but also significantly reduce the computational resources and time required by training models using original datasets. As such, DD finds its application across a wide spectrum of domains and is receiving increasing attention from the community.

The typical paradigm of DD is to optimize \(\) in an iterative loop, as shown in Fig. 1(a). In each iteration, a new network is sampled, leveraged by which a matching metric is calculated for \(\) and \(\), and the matching loss is then back-propagated through the network to update \(\). Recently, a large number of approaches have been dedicated to exploring advanced matching objectives to improve the training performance of distilled datasets, including matching training performance via meta learning , matching feature regression performance , matching training gradients , matching training trajectories , and matching feature statistics .

Although impressive results have been achieved and it has been demonstrated that neural networks trained by a synthetic dataset with even only 1 image per class can yield reasonable performanceon real data, the iterative optimization process adopted in existing works results in significant computational overhead. As shown in Fig. 2, for FRePo , the method with the best performance requires above 3 hours to obtain a synthetic dataset with 1 image per class for CIFAR100 , let alone RTP , which is a back-propagation-through-time method and requires over 9 days for optimization. Such a dramatic latency makes existing approaches hard to be applied in scenarios requiring high efficiency, like handling data streams. Moreover, when the memory budget for a synthetic dataset changes, existing methods have to repeat the time-consuming optimization for the different sizes of \(\), which lacks flexibility.

To alleviate the drawbacks of the conventional iterative forward-backward process, in this paper, we propose a generative dataset distillation approach, where the optimization loop is replaced by a single feed-forward propagation, as shown in Fig. 1(b). Specifically, given an initialization of synthetic samples in \(\), we first obtain the corresponding synthetic labels by analytically solving a least-squares problem in a feature space. Then, a generator is adopted to transfer the initialized samples to the final ones. Our theoretical analysis indicates that \(\) solved in an original space can be transferred to the final result by any conditional generator with an upper-bounded error, which validates the feasibility of this pipeline.

Then, the key problem of our framework lies in finding a suitable generator as quickly as possible for the feed-forward generation process of \(\). To this end, we propose a method called _MGDD_, where a meta generator is learned with a meta learning algorithm on a large database like ImageNet . Trained in a learning-to-learn fashion, the meta generator is optimized such that only a small number of adaptation steps are required for a target dataset unseen in the meta learning. Experiments demonstrate that our approach yields \(22\) acceleration 2 and comparable performance with existing state-of-the-art methods, as shown in Fig. 2. Beyond that, the generator once adapted can also handle unseen sizes of \(\) during adaptation, which improves the flexibility of cross-size generalization in existing DD methods significantly. We also validate that MGDD gets competitive performance on target datasets with large domain shifts from those seen in the meta learning.

Our contributions can be summarized from the following three aspects:

* We propose an innovative feed-forward generation fashion for DD without backward propagation after adaptation which significantly improves the efficiency of existing methods;
* We introduce MGDD which uses a meta-learning algorithm to learn a meta generator and helps the generator adapt to a target dataset rapidly;
* The proposed method achieves significant acceleration and improvement in the flexibility of cross-size generalization for existing DD approaches with comparable performance.

## 2 Related Works

Unlike conventional efficient learning schemes that mainly focus on lightening models [9; 34; 55; 54; 10; 18; 19; 17], dataset distillation (DD) looks into data compression: given a real large dataset \(\), DD aims at a smaller synthetic dataset \(\) which can match the training performance of \(\). The seminal work by Wang _et al._ proposes a meta learning approach to model this objective: in meta training, a network is trained with the current \(\) for multiple times, while in meta test, the loss for the updated network is evaluated on \(\), which is then back-propagated through the bi-level optimization

Figure 1: Illustration of previous back-propagation-based and our generative fashions after adaptation for dataset distillation.

to update \(\). The following work by Deng _et al._ improves performances by adopting momentum during meta training.

Considering the concerns on memory and time complexity of unrolling the computational graph in meta learning, a variety of works introduce various surrogate matching objectives to update \(\). Zhao _et al._ propose to match training gradients of \(\) with those of \(\), and following researches [20; 27; 16; 57; 45] focus on improving the classical gradient-matching objective. Beyond the single-step gradient, Cazenavette _et al._ and subsequent works [4; 8] consider regulating multi-step training effects and propose matching training trajectories. Without the necessity of calculating higher order gradients, the distribution matching methods [59; 50] minimize the distance between feature statistics of \(\) and \(\), and result in satisfactory computational efficiency. Another branch of methods [37; 38; 61; 32; 33] turn to kernel ridge regression (KRR) model to improve the efficiency of the seminal meta learning based solution, since KRR enjoys the analytical form of solution, which gets rid of the meta-training process and yields best trade-off between performance and efficiency.

Different from works focusing on objectives of DD, some other researches explore methods of synthetic data parameterization, where synthetic samples can be stored in some other formats except the raw one to improve the data efficiency, and raw samples are recovered via some functions during down-stream training, _e.g._, data augmentation , up-sampling , linear transformation , and neural networks [30; 26; 3; 49].

For a thorough introduction to dataset distillation, we refer readers to the recent surveys for this area [56; 13; 43; 28]. For existing methods, no matter what objectives and parameterizations are adopted, they all rely on an intensive loop of forward-backward propagation through a massive number of neural networks. Although a concurrent work  also adopts a similar pre-training and adaptation pipeline, it still relies on an iterative loop to solve an initialization of synthetic data. Different from prior works, we innovatively introduce a feed-forward fashion for dataset distillation in this paper. In fact, our method is orthogonal to different training objectives and data parameterizations, and can be built upon any combination of them. In this paper, we consider the KRR-based objective thanks to its favorable performance and computational efficiency and experiment with both raw and down-sampled parameterizations.

## 3 Methods

In this section, we elaborate on the technical methods of the proposed MGDD pipeline. We first introduce in Sec. 3.1 some preliminary information related to the matching objective. Then, for the main method, according to the overview of the whole pipeline in Fig. 1(b), the final synthetic labels and samples are derived by solving a least-squares problem and a conditional generator given initial synthetic data, which would be illustrated in detail in Sec. 3.2 and 3.3 respectively. Finally in Sec. 3.4, we analyze the feasibility of this two-step pipeline theoretically.

### Preliminary

Let us denote datasets before and after distillation as \(=(X_{t},Y_{t})\) and \(=(X_{s},Y_{s})\) respectively, where \(X_{t}^{n_{t} d}\), \(Y_{t}^{n_{t} c}\), \(X_{s}^{n_{s} d}\), \(Y_{s}^{n_{s} c}\), \(n_{t}\) and \(n_{s}\) are number of real and synthetic data respectively, \(d\) and \(c\) are input and output dimensions respectively. Typically, for the RGB image classification task, \(d\) is equal to \(h w 3\), \(c\) is the number of classes, and \(Y_{t}\) is organized in the one-hot format. For the objective of DD, in this paper, we mainly consider the KRR-based methods in neural feature spaces [61; 32] due to its overall superior performance in terms of accuracy and efficiency. Specifically, assume there is an optimal neural network \(f_{^{*}}\) to projects \(X_{t}\) and \(X_{s}\) to a feature space with \(p\) dimensions and \(n_{s} n_{t}<p\). The prediction error on \(\) for the optimal KRR

Figure 2: Results of different DD methods on CIFAR100 with 1 image per class. Our MGDD achieves state-of-the-art efficiency and at least comparable performance. DS denotes down-sampling parameterization.

parameter solved by \(\), denoted as \(W_{s}^{^{*}}\) is adopted as the loss function:

\[(;^{*})=\|f_{^{*}}(X_{t})W_{s}^{^{*}}-Y_{ t}\|_{2}^{2}=\|f_{^{*}}(X_{t})f_{^{*}}(X_{s})^{}(f_{^{*}}(X_{s })f_{^{*}}(X_{s})^{})^{-1}Y_{s}-Y_{t}\|_{2}^{2}. \]

In practice, since the optimal parameter \(^{*}\) is unknown, it is approximated by different random initializations  or alternately optimization with \(\).

### Solving Synthetic Labels

Through Eq. 1, we can find that the loss in a neural space \(\) is upper-bounded by the distance between parameters solved by \(\) and \(\):

\[(;)=\|f_{}(X_{t})W_{s}^{^ {*}}-Y_{t}\|_{2}^{2} =\|f_{}(X_{t})W_{s}^{}-f_{^{*}}(X_{t})f_{ }(X_{t})^{}Y_{t}\|_{2}^{2}\] \[=\|f_{}(X_{t})W_{s}^{}-f_{}(X_{t})W_{t}^{ }\|_{2}^{2}\|f_{}(X_{t})\|_{2}^{2}\|W_{s}^{}-W_{t}^{ }\|_{2}^{2} \] \[=\|f_{}(X_{t})\|_{2}^{2}\|f_{}(X_{s})^{}Y_{s}-W _{t}^{}\|_{2}^{2},\]

where \({}^{}\) denotes the pseudo-inverse of a matrix. In our MGDD framework, synthetic samples \(X_{s}\) are initialized as some random real samples in \(X_{t}\). Given a fixed \(X_{s}\) and a random network \(f_{}\), the upper bound in Eq. 2 forms a least-squares problem with respect to synthetic labels \(Y_{s}\), which can be minimized by an analytically optimal solution:

\[Y_{s}^{*}=f_{}(X_{s})W_{t}^{}=f_{}(X_{s})f_{}(X_{t})^{ }(f_{}(X_{t})f_{}(X_{t})^{})^{-1}Y_{t}. \]

\(Y_{s}^{*}\) obtained with Eq. 3 serves as final synthetic labels. In the next subsection, we will introduce the generation of synthetic samples conditioned on their initialization.

### Learning a Synthetic Sample Generator

Conditioned on initialized synthetic samples \(X_{s}\), a generator \(g_{}\) is adopted to predict the final synthetic data \(X_{s}^{*}\), where the parameter \(\) can encode useful information of the target dataset \(\) and the optimal neural space parameterized by \(^{*}\). We expect that the generator can acquire such knowledge through a fast learning process within a limited number of training steps. To this end, we propose a learning-to-learn algorithm based on MAML , where a meta generation network is learned to optimize the performance of the network adapted for a few steps from the meta one.

Specifically, to ensure the generality of the meta generator for different target datasets, we perform the training algorithm on ImageNet1k , a large-scale dataset for image classification. In each training iteration, a subset of all classes is randomly sampled from it to mimic different target datasets that the generator may encounter in practice. And the meta generator is learned in a bi-level learning framework including a meta-training loop and a meta-testing step, and the meta-testing loss is back-propagated through the computational graph of meta-training steps to update the parameter of the meta generator.

In each meta-training and meta-testing step, from the selected classes, we randomly sample a batch of real images as \(\) and initialize synthetic data \(X_{s}\) with part of them. With a random and fixed neural network as \(f_{}\), the synthetic labels \(Y_{s}^{*}\) are solved via Eq. 3. Then, the final synthetic samples \(X_{s}^{*}\) are predicted by the current generator in a forward pass, and \(=(X_{s}^{*},Y_{s}^{*})\) is evaluated by the loss \((;^{*})\) in Eq. 1. In this paper, following Loo _et al._, we approximate the optimal neural parameter \(^{*}\) via random sampling in different steps from the distribution for initialization. The loss signal is back-propagated to the current generator and the meta generator to update parameters in meta-training and meta-testing respectively. It is worth noting that in different meta-training and meta-testing times, we use different sizes of synthetic data, which enhances the cross-size generalization ability on target datasets since the meta-testing losses on sizes probably unseen during meta-training are optimized. The main algorithm is summarized in Alg. 1. Given a trained meta generator, a limited number of adaptation steps are performed for a target dataset. The procedure of adaptation is similar to the meta-training loop in Alg. 1.

As for the architecture of the generator, in this paper, we adopt a simple encoder-decoder model, where the encoder consists of three convolutional blocks with two average pooling layers while the decoder has a symmetric structure. Notably, we observe in practice that it is beneficial for different sizes of synthetic datasets to adopt different transfer functions. Taking various sizes into consideration, we concatenate additional size-embedding channels to the bottle-necked layer of the generator, inspired by the design of the position embedding in Transformer models  and the time-step embedding in diffusion models [15; 39; 41]. Please refer to the appendix for detailed architecture configurations.

### Theoretical Analysis

There are two key steps in the proposed MGDD framework: solving the optimal synthetic labels \(Y^{*}_{s}\) as introduced in Sec. 3.2 and generating the corresponding synthetic samples \(X^{*}_{s}\) as introduced in Sec. 3.3. Define the error \((;)\) in Eq. 1 using the fixed \(X_{s}\) and the optimal \(Y_{s}\) in Eq. 3 with the projection function of \(f_{}\) as \(\). The reason we pursue the optimal \(Y_{s}\) is that the final \(X^{*}_{s}\) is transferred from the initial \(X_{s}\), whose error in the optimal neural space parameterized by \(^{*}\) is upper-bounded by \(\), as derived in the following theorem.

**Theorem 1**.: _Given \(f_{}:^{d}^{p}\), \(f_{^{*}}:^{d}^{p}\), \(X_{t}^{n_{t} d}\), \(Y_{t}^{n_{t} c}\), \(X_{s}^{n_{s} c}\), \(d<p\), \(Y^{*}_{s}\) obtained by Eq. 3 with the optimal \(((X_{s},Y^{*}_{s});)\) denoted as \(\), and an arbitrary transfer function \(g_{}\) parameterized by \(\) taking \(X_{s}\) as input, the transferred \(g_{}(X_{s})\) yields an upper-bounded loss \(((g_{}(X_{s}),Y^{*}_{s});^{*})\)._

Proof.: We first rewrite the given condition:

\[((X_{s},Y^{*}_{s});)=\|f_{}(X_{t})W^{}_{s}-Y_{t} \|_{2}^{2}=\|f_{}(X_{t})f_{}(X_{s})^{}(f_{}(X_{s})f_{ }(X_{s})^{})^{-1}Y^{*}_{s}-Y_{t}\|_{2}^{2}=. \]

Then, we have:

\[((g_{}(X_{s}),Y^{*}_{s});^{*}) =\|f_{^{*}}(X_{t})W^{^{*}}_{s}-Y_{t}\|_{2}^{2} \] \[=\|f_{^{*}}(X_{t})f_{^{*}}(g_{}(X_{s}))^{}(f _{^{*}}(g_{}(X_{s}))f_{^{*}}(g_{}(X_{s}))^{})^{-1}Y ^{*}_{s}-Y_{t}\|_{2}^{2}\] \[\|f_{^{*}}(X_{t})W^{^{*}}_{s}-f_{}(X_{t})W^ {}_{s}\|_{2}^{2}+\|f_{}(X_{t})W^{}_{s}-Y_{t}\|_{2}^{2}\] \[=\|f_{^{*}}(X_{t})W^{^{*}}_{s}-f_{}(X_{t})W^{ }_{s}\|_{2}^{2}+,\]

where the inequality is based on the triangle inequality and the last equation is due to Eq. 4. 

Theorem 1 indicates that to achieve feed-forward dataset distillation, we do not need to design a neural network taking the whole original dataset \(\) as input. We can in fact adopt a conditional generation function that transfers the initial synthetic samples to the desired ones, which has an error upper bound as shown in Theorem 1. Since the upper bound is related to the error in the original space parameterized by \(\), it is crucial to solve the optimal synthetic labels with respect to \(\) in the first step. Also, we notice from Eq. 5 that the optimal generator is dependent on \(\) and \(^{*}\). Given that \(^{*}\) is intractable and can only be approximated by iterative sampling, we build a meta learning algorithm in Alg. 1 for the MGDD framework to enforce an efficient process to model this dependency in only a few steps.

## 4 Experiments

### Implementing Details

As discussed in the previous section, there are 3 stages in the proposed MGDD, including training, adaptation, and inference stages. In the training stage, we aim at a meta generator \(g_{}\) and adopt Alg. 1 to train \(g_{}\) on a large dataset. In this paper, to ensure that the meta generator can acquire general knowledge for fast adaptation on a target dataset, we use ImageNet1k , a large-scale image recognition dataset popular in the computer vision and machine learning communities, as the dataset for meta learning. There are roughly 1,280,000 images in a total of 1,000 classes. We resize all images to the \(32 32\) resolution. In each outer loop of Alg. 1, we randomly select 100 classes at most and a batch of data with 2,000 images in maximal from the selected class as a current target dataset \(\), with which we initialize a synthetic dataset \(\) with 1,000 samples at most in each inner step. The training objective is based on Eq. 1 and the implementation follows the open-source code of FRePo . For computational efficiency, the generator processes each sample independently and the configuration of the architecture can be found in the appendix. The meta generator is trained by the Adam optimizer  and the learning rate \(\) is set as \(10^{-5}\). The learning rate in meta-training is set as \(10^{-4}\) and the number of meta-training steps \(T\) is \(5\). The meta generator is trained with 200,000 meta-testing iterations. We use a cloud server with a single A40 GPU for meta learning and a workstation with a single 3090 GPU for the subsequent adaptation. The meta learning takes roughly 2 days while the time cost of adaptation is analyzed in Tab. 1.

[MISSING_PAGE_FAIL:7]

is applicable to datasets with larger resolutions. It can also be adapted for datasets with different numbers of input channels with minor modifications. In the appendix, we provide more experimental results of such cross-resolution and cross-channel-number generalization cases.

### Empirical Studies

In this part, we focus on some interesting properties of the proposed MGDD method, including cross-dataset, cross-objective, cross-architecture, cross-synthetic-initialization, cross-parameterization, and cross-IPC settings. More studies including cross-resolution, cross-channel-number, and cross-class-number that cannot fit into the main content are put in the appendix.

**Cross-Dataset Generalization:** MGDD proposed in this paper is expected to be generalized to any downstream target datasets, including those unseen and with large domain shifts from datasets used in the meta learning. To evaluate the cross-dataset generalization performance of MGDD, we conduct experiments on more datasets including one domain generalization dataset PACS , two medical classification datasets PathMNIST and BloodMNIST , and one fine-grain image classification dataset CUB200 . PACS contains 9,991 images from 7 classes and 4 domains: Photo (P), Art Painting (A), Cartoon (C), and Sketch (S). The style variations across the 4 domains are drastic. We perform dataset distillation on each domain both independently and jointly, which formulates a 28-class dataset. PathMNIST and BloodMNIST contain 107,180 images of 9 classes and 17,092 images from 8 classes respectively. We also combine them together to form a 17-class dataset denoted as PathBloodMNIST. CUB200 contains 6,000 images of 200 bird species. We process all the images to the \(32 32\) resolution in the RGB format and compare the performance with the FRePo baseline  and a generator from scratch instead of the meta generator, under 1,000 and 2,000 steps as well as full convergence. The quantitative results shown in Tab. 2 validate the robustness of MGDD under various datasets and domain shifts.

**Cross-Objective Generalization:** By default, both the meta-learning and adaptation objectives used in this paper for MGDD are the KRR objective in Eq. 1 following FRePo . Empirically, we find that it is also feasible to adopt different objectives for adaptation. Here, we switch the adaptation objective to DC  and DM  respectively. The optimization steps for both baselines and our method are set as 2,000. As shown in Tab. 3, our methods yield consistent improvement over different baselines in a limited number of optimization steps.

**Cross-Architecture Generalization:** Adapted on an original architecture, a satisfactory generator is expected to produce results that are also valid to train networks with different structures, namely cross-architecture generalization. Tab. 4 shows the performance on CIFAR10, where ConvNet is used in adaptation while AlexNet , VGG11 , and ResNet18  are used as unseen architectures for evaluation. The results indicate that the accuracy improvement on the original model still holds for unseen structures.

**Cross-Initialization Generalization:** We find that the single-initialization scheme used in Tab. 1 may lead to over-fitting of the generator to the adopted single initialization. As shown in Tab. 5, if we change the initialization of synthetic data, the performance would drop dramatically. Fortunately, multi-initialization is an alternative to account for this drawback, wherein each adaptation step, a new initialization of the synthetic dataset is sampled from the original dataset. Tab. 5 indicates that multi-initialization typically requires more adaptation steps for convergence and can perform on par with single-initialization. It is useful when samples in the synthetic dataset are concerned with user privacy, given that visualization results produced by the KRR objective are somehow realistic, as illustrated in Fig. 6 and . In such cases, replacing the current dataset with the results of another

    &  &  \\  IPC & Method & ConvNet & AlexNet & VGG & ResNet \\    & Baseline & 49.6\(\)0.1 & 44.5\(\)0.7 & 33.0\(\)0.1 & 31.8\(\)1.6 \\  & Ours & 58.9\(\)0.4 & 55.1\(\)0.4 & 35.9\(\)0.6 & 32.7\(\)0.8 \\   & Baseline & 26.8\(\)0.7 & 23.4\(\)0.3 & 16.9\(\)0.1 & 15.1\(\)0.8 \\  & Ours & 42.6\(\)0.3 & 39.6\(\)0.8 & 22.9\(\)0.6 & 19.1\(\)1.3 \\   & Baseline & 62.0\(\)0.4 & 59.2\(\)0.3 & 48.7\(\)1.1 & 48.2\(\)0.4 \\  & Ours & 66.8\(\)0.2 & 62.8\(\)0.2 & 50.9\(\)0.7 & 52.4\(\)1.2 \\   

Table 4: Performance of different network architectures on CIFAR10.

   Dataset &  &  \\  IPC & 1 & 10 & 50 & 1 & 10 \\   FRePo & 26.8\(\)0.7 & 49.6\(\)0.1 & 62.0\(\)0.4 & 10.1\(\)0.3 & 29.6\(\)0.2 \\ Ours w FRePo & 42.6\(\)0.3 & 58.9\(\)0.4 & 66.8\(\)0.2 & 20.8\(\)0.2 & 32.2\(\)0.3 \\  DC & 24.7\(\)0.4 & 43.1\(\)0.3 & 56initialization would help solve the problem efficiently, without the necessity to re-run the whole optimization loop of existing methods.

**Cross-Parameterization Generalization:** Beyond different training objectives, the proposed MGDD is also orthogonal with different synthetic data parameterization tricks. In Tab. 6, we consider storing \(2\) down-sampled synthetic images instead of the raw ones. Thus, \(4\) synthetic samples can be stored given the same storage budget. We find that the simple strategy can lead to additional performance gain for relatively small budgets. The observation is consistent with previous works [20; 30; 7].

**Cross-IPC Generalization:** One crucial benefit of the MGDD is the cross-IPC generalization. Once the generator is adapted for a target dataset, when the storage budget changes, we do not need to perform the optimization again, unlike previous solutions relying on iteratively updating synthetic datasets. To demonstrate the cross-IPC generalization performance, we conduct experiments on CIFAR10 and adapt the meta generator using multi-initialization, with 10 and 50 IPCs for 2,000 steps. The adapted generator is evaluated on unseen IPCs 20, 30, and 40. The results are shown in the red curve of Fig. 4, where the generator produces satisfactory synthetic datasets with unseen IPCs.

To make the generator aware of the sizes of synthetic datasets, we concatenate size embedding channels to features at the middle layer of the generator. To understand how the embedding works, we remove these channels and conduct the same evaluation. As shown in the yellow curve of Fig. 4, the performance degrades without size embedding. In Fig. 6, we visualize some samples before and after the adapted generator on CIFAR10 with 1 and 50 IPCs. We can observe that the results are quite different: for small sizes, the generated results are vague, while for large sizes the results are more realistic, and their styles are also different. Thus, it is reasonable to take different sizes of synthetic datasets into consideration in the inference stage.

We also try training the generator from scratch instead of the meta model on the target dataset. As shown in Fig. 4, the worse performance in the green curve indicates that meta-learning is crucial for finding a satisfactory initial checkpoint for downstream adaptation.

Moreover, we also evaluate the performance on higher IPCs like 100 and 200, and the results are still encouraging compared with random real samples, which indicates that our method can serve as an alternative when the computational resource cannot support optimization for larger IPCs directly.

Furthermore, to demonstrate the effectiveness of analytical labels, we replace them with vanilla one-hot labels in synthetic datasets and the performance is shown in the blue curve. The considerable performance drop indicates the importance of minimizing the error in an original space via analytical labels, which is consistent with the theoretical analysis in Theorem 1.

### Application: Continual Learning

Continual learning (CL) aims to learn from a stream of data, where historical and future data are unavailable when learning the current data batch. One important issue is catastrophic forgetting : a model tends to forget knowledge acquired in previous data when learning on newly-coming data. Focusing on this drawback, many works introduce a buffer with limited memory to store core data and knowledge of past experience for future use . Dataset distillation benefits this field by generating informative samples  to prevent forgetting as much as possible.

In this paper, we evaluate the CL performance of the proposed MGDD on CIFAR100, following the same protocol of , where all 100 classes are divided into 10 tasks randomly with 10 classes for each task. For each task, a buffer with 20 images for each class is allowed for synthetic data. We first try adapting the generator on each new task from the meta model for 2,000 steps and the performance is shown in the yellow curve in Fig. 5. Alternatively, we can choose to adapt the generator from the checkpoint of the previous task, which has already learned some global knowledge of full data and yields better performance, as shown in the red curve. Notably, it is also feasible to only adapt the generator on the first task and the remaining tasks directly adopt this generator to output synthetic data. As shown in the blue curve, with the most significant flexibility, the performance is still comparable with the FRePo baseline  shown in the green curve, which suggests great practical value for processing data streams.

## 5 Conclusion

In this paper, we propose MGDD, a novel feed-forward paradigm for dataset distillation (DD). Specifically, in our pipeline, synthetic labels are obtained by solving a least-squares problem equipped with an analytical solution, and synthetic samples are transferred from their initial results by a conditional generator instead of taking the whole original training dataset as input. Theoretical derivation indicates an error upper bound of the proposed framework. On the one hand, unlike existing DD approaches requiring time-consuming forward-backward iterations through a massive number of networks, MGDD generates distilled results with a generator adapted rapidly from a meta generator, which improves the efficiency of DD significantly. On the other hand, existing techniques have to repeat the whole iterative algorithms for different sizes of synthetic datasets, while MGDD can perform inference flexibly on various sizes once adapted to the target dataset. Focusing on the efficiency of adaptation on target datasets, we propose a meta-learning algorithm to train a meta generator, such that it can acquire knowledge of target datasets sufficiently in only a few steps. Experiments demonstrate that the proposed MGDD performs on par with existing state-of-the-art DD baselines under \(22\) acceleration. It also exerts strong cross-size generalization ability even on sizes of synthetic datasets unseen during adaptation. Future works may explore advanced feed-forward fashions of DD, focusing on generation pipelines, training algorithms, and network architectures, making improvements on the cross-dataset, cross-size, and cross-architecture generalization.

Figure 6: Visualizations of samples before and after generator on CIFAR10 with 1 and 50 IPC.