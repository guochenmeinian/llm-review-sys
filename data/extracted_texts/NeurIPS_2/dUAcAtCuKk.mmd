# RecKoning: Reasoning through Dynamic Knowledge Encoding

Zeming Chen\({}^{1}\)  Gail Weiss\({}^{1}\)  Eric Mitchell\({}^{2}\)  Asli Celikyilmaz\({}^{3}\)  Antoine Bosselut\({}^{1}\)

EPFL\({}^{1}\)  Stanford University\({}^{2}\)  Meta AI Research\({}^{3}\)

{zeming.chen, gail.weiss, antoine.bosselut}@epfl.ch

eric.mitchell@cs.stanford.edu  aslic@meta.com

###### Abstract

Recent studies on transformer-based language models show that they can answer questions by reasoning over knowledge provided as part of the context (i.e., in-context reasoning). However, since the available knowledge is often not filtered for a particular question, in-context reasoning can be sensitive to distractor facts, additional content that is irrelevant to a question but that may be relevant for a different question (i.e., not necessarily random noise). In these situations, the model fails to distinguish the necessary knowledge to answer the question, leading to spurious reasoning and degraded performance. This reasoning failure contrasts with the model's apparent ability to distinguish its contextual knowledge from all the knowledge it has memorized during pre-training. Following this observation, we propose teaching the model to reason more robustly by folding the provided contextual knowledge into the model's parameters before presenting it with a question. Our method, Reckoning, is a bi-level learning algorithm that teaches language models to reason by updating their parametric knowledge through back-propagation, allowing them to answer questions using the updated parameters. During training, the inner loop rapidly adapts a copy of the model weights to encode contextual knowledge into its parameters. In the outer loop, the model learns to use the updated weights to reproduce and answer reasoning questions about the memorized knowledge. Our experiments on three diverse multi-hop reasoning datasets show that Reckoning's performance improves over the in-context reasoning baseline (by up to 4.5%). We also find that compared to in-context reasoning, Reckoning generalizes better to longer reasoning chains unseen during training, is more robust to distractors in the context, and is computationally more efficient when multiple questions are asked about the same knowledge.

## 1 Introduction

Consider the sentence: "John is David's dad, and Tom is John's dad". Concluding that Tom is David's grandfather involves _reasoning_ about the information in the sentence. Specifically, it requires understanding the direct information, or _contextual knowledge_, given in the sentence: the stated relationships between John, David, and Tom; and combining it with our existing, _commonsense knowledge_ of the world: someone's dad is their grandfather. Achieving such logical reasoning automatically has long been a goal of AI .

The example above demonstrates two necessary abilities required for successful reasoning: first, holding large amounts of commonsense or general knowledge about the world, and second, processing and combining new information with existing knowledge. Transformer-based large language models have shown a remarkable capacity for the first of these abilities, repeatedly being demonstrated to memorize large amounts of data, or _parametric knowledge_, in their weights .

For the second, recent work showed that transformers fine-tuned to predict answers over a concatenated context ("The cow is big; If something is big then it chases the dog; If the cow chases the dog then the cow sees the rabbit") and question ("Did the cow see the rabbit?") achieve high performance on reasoning tasks where all necessary knowledge is given in the context . We refer to this general setting as _in-context reasoning_ (ICR).

In real-world question-answering settings , large amounts of contextual knowledge may be provided at once, and the information may not be perfectly filtered for a specific question. Unfortunately, in-context reasoning is highly sensitive to _distractors_: additional facts that are not relevant to a question (e.g., "The cow is round" for the above example). Indeed, when fine-tuning and evaluating GPT-2  for ICR, we find that adding distractors to the context drops performance from \(99.4\%\) to only \(70.9\%\) accuracy for the same questions (SS4.2). This sensitivity to distractors in contextual knowledge contrasts with GPT-2's apparent robustness to distractors in parametric knowledge: for any specific example, most of the training data seen by GPT-2--which forms its parameters--is likely to be completely irrelevant to that example. Naturally, we wonder whether presenting contextual knowledge in the same way as memorized knowledge, by encoding it into a model's parameters, will improve the reasoning abilities of transformer-based language models.

In this work, we propose a novel bi-level optimization algorithm, Reckoning, that learns to memorize (and reason) over facts (i.e., knowledge) by performing inference-time parameter updates using gradients computed from a language modeling loss on those facts. The updated model is then used to answer any questions about those facts. Our training framework involves two nested loops: the inner loop performs fast adaptations from a set of initial weights to memorize a set of external knowledge through a few gradient updates, and the outer loop optimizes those same initial weights such that the updated model will solve reasoning problems associated with the memorized knowledge. In other words, the outer loop learns optimal _meta-parameters_ that can rapidly memorize and successfully reason over contextual knowledge, allowing knowledge memorization to be optimized directly for downstream reasoning. At inference time, instead of including external knowledge in the input sequence as the prefix to a question prompt, the model can encode it in its parameters through gradient updates and then reason over its updated parametric knowledge to reach a conclusion.

We evaluate Reckoning on two synthetic multi-hop reasoning datasets: ProofWriter  and CLUTRR-Systematic-Generalization (CLUTRR-SG) , and one real-world dataset, FOLIO , comparing against a fine-tuned ICR (FT-ICR) baseline that uses the same underlying model. Our results show that Reckoning consistently outperforms the FT-ICR baseline on each benchmark, demonstrating that it successfully learns to answer multi-hop reasoning questions as desired. In particular, we find that Reckoning more successfully generalizes to adversarial settings, such as the presence of distractor facts and the introduction of longer reasoning chains at inference time. Finally, while the inference-time gradient updates make Reckoning slower to process new knowledge than a typical ICR forward pass, our run-time analysis shows that Reckoning is more efficient when answering multiple questions about a shared knowledge set. This speedup occurs because Reckoning only needs to encode the knowledge once to answer multiple questions about it. Overall, we demonstrate that Reckoning is an effective algorithm for reasoning through dynamic and controllable knowledge encoding, overcoming an observed weakness in the common reasoning setting and providing multiple additional benefits.

Figure 1: Our algorithm, Reckoning, solves reasoning problems by encoding external contextual knowledge into a modelâ€™s parameters through gradient updates. At inference time, Reckoning performs a few parameter updates using the gradients of a language modeling loss to encode the relevant facts. Then, the updated model answers the question using only its implicit knowledge.

## 2 Background

NotationWe use \(f:\) to refer to parameterised functions in which \(\) is the set of possible inputs and \(\) are their possible weights (parameters). We use \(f_{}:x f(x,)\) to easily refer to any \(f\) with a given set of parameters \(\). We describe reasoning problems using tuples \((,,y^{*},Y)\) such that \(y Y\) is the correct answer for the question \(\) given facts \(\), and use \(\) to refer to sets of such problems. When it is clear from context, we drop \(Y\) and use only \((,,y^{*})\).

Language Modeling and MemorizationIn the causal language modeling (CLM) objective, a parameterized model \(f_{}\) is trained to estimate the conditional probabilities of each token in a sequence given its predecessors: \(p(x_{t}|x_{<t})\). Specifically, we train \(f_{}\) to approximate \(p\) using the CLM loss:

\[_{}(f_{},)=-_{t=1}^{T} f_{}(x_{t}|x_{1},...,x_{t-1}). \]

This training objective allows language models to _memorize_ individual training examples [10; 11], and we will exploit this ability to memorize and draw on contextual knowledge in our work.

Transformers as Soft ReasonersIn natural language _reasoning_ tasks, we are given reasoning problems \((,,y^{*},Y)\) in natural language and attempt to recover the correct answer \(y^{*}\) from the context \(\), question \(\), and possible answers \(Y\) alone. In _in-context reasoning_, language models \(f_{}\) trained with a CLM objective are applied to this task by selecting as the response the answer \(y Y\) with a maximum probability according to the model's next-token prediction from the concatenated context and question: \(y=_{y^{} Y}f_{}(y^{}||;)|\). Previous works show that, after relevant supervised fine-tuning, transformer language models can achieve high performance in this setting [17; 29; 74], though this degrades significantly in the presence of irrelevant facts (_distractors_) .

## 3 Method

Addressing these challenges, we propose Reckoning (**RE**asoning through dynami**C** **Kn**O**wledge **e**N**c**od**ING**), which solves reasoning problems by memorizing the provided contextual knowledge, and then using this encoded knowledge when prompted with downstream questions. Specifically, Reckoning uses bi-level optimization to learn a set of meta-parameters primed to encode relevant knowledge in a limited number of gradient steps. The model can then use its updated weights to solve reasoning problems over this knowledge, _without further presentation of the knowledge itself_.

Overview: InferenceGiven a reasoning problem \((,,y,Y)\), we initialize our model with weights copied from a set of meta-parameters \(\) and perform a constant number \(N\) of gradient descent steps on these with the goal of minimizing the CLM objective on the knowledge set \(\). This allows the model to memorize \(\) in its updated parameters, which we refer to as \(}_{N}^{}\). Next, we pass the question \(\) to the model, using \(f_{}_{N}^{}}\) to obtain a distribution over \(Y\), and taking as output the answer \(y Y\) with the highest probability. For this method to consistently output the ground truth \(y^{*}\), we seek a set of optimal meta-parameters \(^{*}\) that can quickly memorize (i.e., learn) the given knowledge in a way that then allows accurate reasoning when queried about the knowledge downstream.

Figure 2: The two-stage training process of Reckoning with an inner and outer loop.

[MISSING_PAGE_FAIL:4]

We compare our method against the following baselines: (1) a fine-tuned model that performs a forward pass on only the question without access to the knowledge (**No-Facts**), (2) a fine-tuned model that performs a forward pass on only the knowledge without access to the question (**No-Question**), (3) a model trained using Reckoning with random knowledge that is not relevant to the questions (**Random-Facts**), and (4) an ICR baseline that concatenates the knowledge \(\) with the question \(\) in a single context and is trained using supervised learning to predict the answer (**FT-ICR**). Our first three baselines sanity-check whether any surface-level patterns in the questions and facts can be exploited to make accurate predictions. The last baseline compares Reckoning to the conventional way of reasoning with language models. Unless stated otherwise, we use the GPT-2-small  model (\(\)124M parameters) as our initialization and refer by Reckoning to our method trained with the multi-task objective. We compute each score from the average across three different runs. For more details on the implementation, datasets, and examples, see Appendix A and Appendix C.

### Multi-hop Reasoning Performance

Main ResultsWe first evaluate whether Reckoning learns to perform reasoning in the base setting. A model is given a set of supporting facts (without distractors) and a question (or hypothesis) as input and begins by performing a few CLM learning steps on the facts. Then, the updated model reads **only** the question and generates an answer. To answer correctly, the model must reason over both facts and the question, meaning it must encode the facts during the inner loop such that multi-hop reasoning can be performed over them later.

We train our models and the fine-tuned ICR (FT-ICR) baselines with both the single-task (\(_{}\)) and multi-task (\(_{}+_{}\)) objectives. For multi-task (MT) training, the model learns to answer the question and generate its relevant knowledge in the outer loop. Table 1 shows the evaluation results on question answering (or hypothesis classification). For all hop numbers in ProofWriter and CLUTRR-SG, multi-task Reckoning outperforms the best result of all baselines (consistently obtained by multi-task FT-ICR) by an average of \(1\%\). We conclude that Reckoning can effectively solve reasoning problems through its updated parametric knowledge and do so better than existing baselines. The multi-task objective is crucial for this success: not only is Reckoning's performance consistently higher (by an average of \(2.8\%\) over the two datasets and their hop counts) when using the multi-task rather than single-task (ST) objective, but it also under-performs both FT-ICR baselines when trained with only the single-task objective. The multi-task objective also improves FT-ICR consistently (average \(1.8\%\)), though it is not enough to beat the multi-task Reckoning. In all further experiments, we consider only Reckoning and FT-ICR with a multi-task objective.

Generalizing to Longer Reasoning ChainsOur first experiments assume an alignment between the number of reasoning hops in the questions in the training and test set. However, we may not be able to train on all \(n\)-hop reasoning questions we encounter in the wild, and we rarely know the number of reasoning hops in a question _a priori_. Consequently, we also measure the generalization capacity of our model to questions with hop numbers unseen during training. We compile _interpolation_ (fewer hops than the train set) and _extrapolation_ (more hops than the train set) test sets from the CLUTRR-SG dataset. Again, we train models individually on 2-hop, 4-hop, and 6-hop examples and evaluate these three sets of models on the test sets, which contain 2-10-hop reasoning questions. Figure 3 shows that both Reckoning models and ICR baselines retain high performance on the interpolation test sets but exhibit decreasing performance as the number of hops increases. Importantly, though, Reckoning outperforms FT-ICR on all test sets regardless of the number of training hops, with the highest difference being more than 10% in every training setting (15%, 30%, 10%, respectively). These performance gains when testing on extrapolation data suggest that training with Reckoning better generalizes to examples with high OOD hop counts than in-context reasoning (ICR).

    &  &  \\  Method & 2-h & 3-h & 5-h & 2-h & 4-h & 6-h \\  No-Facts & 64.1 & 63.0 & 64.2 & 0.0 & 8.8 & 8.9 \\ No-Question & 66.2 & 67.0 & 65.2 & 35.7 & 36.4 & 28.7 \\ Random-Facts & 64.1 & 63.0 & 64.2 & 0.0 & 1.3 & 2.5 \\  FT-ICRST & 98.4 & 98.8 & 97.8 & 97.4 & 91.3 & 89.1 \\ FT-ICRMT & 99.4 & 99.2 & 99.6 & 98.1 & 96.9 & 90.3 \\  ReckoningST & 98.3 & 98.3 & 99.1 & 96.0 & 90.2 & 91.2 \\ ReckoningMT & **99.5** & **99.7** & **99.8** & **98.3** & **97.6** & **94.8** \\   

Table 1: Label accuracy of Reckoning on ProofWriter and CLUTRR-SG, compared to FT-ICR baselines where the supporting facts are given as part of the input. MT marks models trained with the multi-task objective, which optimizes both question-answering and knowledge memorization.

Does Reckoning's performance depend on the number of inner loop gradient steps?In Reckoning, the model performs multi-hop reasoning over facts by encoding facts using multiple gradient steps in the inner loop optimization (SS3). Naturally, this process prompts the question of whether there is a correlation between the number of reasoning hops and the number of gradient steps needed to reliably encode the knowledge (i.e., problems with more reasoning hops require more gradient steps in the inner loop to encode the facts). In Figure 4, we show for CLUTRR-SG that as the number of inner loop steps increases, the label accuracy of the outer-loop task also increases. Furthermore, when considering the performance gains for reasoning with 6 inner loop steps (i.e., knowledge encoding) as opposed to one, we observe that this gap is much more pronounced for 4-hop (\(42.3\%\)) and 6-hop (\(34.7\%\)) reasoning than it is for 2-hop reasoning (\(5.9\%\)). These results show that problems requiring more hops of reasoning also greatly benefit from more steps of inner loop knowledge encoding.

### Reasoning with Distractors

In cases where multiple questions must be answered about the same knowledge set, some knowledge that is relevant to one question will likely be irrelevant to another question. For example, in Table 7, the fact "Charlie is White." is not needed to answer the question "Harry is red?". Thus, it is important to evaluate the robustness of Reckoning when there exists irrelevant information (i.e., distractors) in the knowledge set. In this experiment, we analyze Reckoning's ability to focus on the correct knowledge and ignore distractors when answering questions. We use ProofWriter as the evaluation dataset since it already has a setting with distractors included in the knowledge. For systematic analysis, we gradually add distractors to the context (starting from 2 and finishing at _all_ possible

Figure 4: Multi-hop reasoning performance as a function of the number of inner loop steps (x-axis), with each line focusing (by training and testing) on CLUTRR-SG with a different number of hops.

Figure 5: Robustness under distractors for ProofWriter. Each of the three plots corresponds to training and testing on a subset of questions in ProofWriter with a different number of hops (2,3,5-hops). Each bar corresponds to the number of distractors in the knowledge sets for those questions.

Figure 3: System generalization evaluation on CLUTRR-SG. From left to right, the models are trained on 2-hop, 4-hop, and 6-hop CLUTRR-SG data portions. We evaluate the model on 2-10 hop test sets. The higher the hops, the more facts a question has, and the more difficult that question is.

distractors, of which there are an average of 7 per question). We train Reckoning and the baseline using the multi-task objective, where the model must (1) recall all of the facts and rules relevant to the question and (2) predict the conclusion based on the correct knowledge. In this case, we adapt training such that for each question \(\), the outer-loop (Equation (5)) CLM loss is only computed with respect to the relevant facts from \(\), thereby learning to recall only relevant facts during training.

In Figure 5, we see that Reckoning's performance is consistently more robust under distractors than the FT-ICR baseline. When we include all of the distractors in the context, Reckoning achieves a significantly higher average label accuracy (82.5%) across hops than the baseline (70.9%), as computed by the average of the 3 considered hop depths. Additionally, compared to performance with no distractors, Reckoning's performance only drops 17.1% while the baseline performance drops 28.6%, thereby exhibiting a better ability to disentangle the correct knowledge from the distractors.

Finally, we also explore Reckoning's generalizability to models with a larger parameter size. We scale up the language model we used, GPT-2-small (124M), to GPT-2-XL (1.5B) by adopting a parameter efficient finetuning method _LoRa_. For simplicity, we only evaluate the models on the most difficult settings, i.e., ProofWriter-5-hop with all the distractors. With GPT-2-XL-LoRA, in-context reasoning achieves 65% accuracy on the test set, while our Reckoning model achieves 70.2% accuracy, a 5% performance gain. This result suggests that Reckoning's advantages in the presence of distractors hold even as models scale in size.

### Generalization to Real-World knowledge

To investigate how generalizable our method is to real-world knowledge beyond the synthetic setting, we evaluate Reckoning on a more real-world multi-hop logical reasoning task, FOLIO , and report the result in Table 2. The dataset has a rich vocabulary, diverse logic patterns, and abundant language variations. It has been shown to challenge LLMs in both supervised fine-tuning and in-context learning settings. We fine-tune the GPT-2 model following the in-context reasoning setting as the baseline. As before, we train the GPT-2 model and Reckoning using the multi-task objective. We also compare to more advanced baselines, including GPT-3.5 (text-davinci-003 ) and ChatGPT(gpt-3.5-turbo 2), two popular large language models with around 175B parameters. For these two large models, we evaluate both in the zero-shot and few-shot settings. In the few-shot setting, we prompt the model with 8 single-task examples randomly sampled from the training set to perform in-context learning. We find that Reckoning's performance (which is initiated here from GPT-2) is better than the GPT-2 in-context reasoning baseline. Compared to the two advanced large language models, Reckoning outperforms them by a significant margin (12% 0-shot and 7% 8-shot). We conclude that Reckoning is effective and significantly benefits reasoning tasks using real-world knowledge.

### Run-time Analysis

One of the advantages of Reckoning is the ability to memorize a large set of knowledge \(\) and answer multiple related questions about that knowledge at a little extra cost per question. Specifically, in contrast to ICR, Reckoning can encode \(\) once and answer multiple questions without needing to reprocess it for each question asked. To test whether Reckoning could be a more efficient method for inference in this setting, we measure the wall-clock time (in seconds) of the complete inference pipeline of Reckoning vs. ICR. For this experiment, we use a synthetic reasoning dataset in which \(\) is a sequence of random letters, and the question \(\) asks for the most frequent letter in the context. The total number of tokens in each example is 1024: 7 for \(\), 1 for the answer, and the remaining 1016 for \(\), broken into 8 "facts".

  
**Model** & **Acc.** \\  Random & 33.3 \\  GPT-2 & 53.3 \\ GPT-3.5 (text-davinci-003) 0-shot & 45.1 \\ GPT-3.5 (text-davinci-003) 8-shot & 52.9 \\ ChatGPT (gpt-3.5-turbo) 0-shot & 40.0 \\ ChatGPT (gpt-3.5-turbo) 8-shot & 42.6 \\  Reckoning & **54.9** \\   

Table 2: Evaluation results on FOLIO. We compare Reckoning against the FT-ICR baseline with GPT-2 and two popular large language models.

The FT-ICR baseline receives a sequence including all 8 facts and the question. In contrast, Reckoning receives the 8 facts as a batch of eight segments of 127 tokens and encodes them in parallel in the inner loop. In the outer loop, the model only receives the question or a batch of questions. We focus on two settings: (1) inference time for a single question and (2) inference time when answering multiple questions. In the multiple-question setting, we set the number of questions to 18 (the same as in ProofWriter). For Reckoning, the inference process includes the inner-loop knowledge encoding and the final forward pass to encode the question. We set the number of inner loop gradient steps to 1 and 4. In Table 3, we see that when answering a single question, Reckoning does not perform inference faster than in-context reasoning. However, Reckoning shows significant advantages under a multi-question setting. Both the 1-step inner loop and the 4-step inner loop are faster than the baseline. Since Reckoning encodes the knowledge in model parameters, it does not need to reprocess the knowledge for a related question and is more efficient. We run this experiment on 1 RTX 3090 GPU.3

### Memorizing Knowledge

In Table 1, we saw that training Reckoning with a multi-task (MT) outer loop objective improved over training with the single-task (ST) objective, potentially because the MT objective improves the model's ability to memorize the knowledge in the inner loop. To validate our hypothesis, we analyze Reckoning's performance in reproducing memorized knowledge.

First, we show in Table 4 the inner loop average loss (\(_{}\)) and average change (\(_{}\)) (from first inner loop evaluation to last) on validation examples from the 5-hop ProofWriter data. We see that the average inner loop loss for Reckoning\({}_{}\) is much higher than Reckoning\({}_{}\), and indeed starts out much higher as well. This shows that the ST outer loop objective, which optimizes the model only for question answering, does not learn to encode the knowledge in the inner loop by _memorizing_ it. In contrast, the MT objective forces the model to learn to memorize the knowledge, too: we observe that Reckoning\({}_{}\) minimizes the inner loop loss as it processes the knowledge. This pattern is also shown in the average inner-loss difference (\(_{}\)): the inner loop loss decreases more after the gradient updates when trained with the MT objective. Next, we report in Table 5 the model's ability to reproduce memorized facts correctly under a multi-task setting, as measured by an exact match score between the reproduced facts and the gold facts. 4 We evaluate on the ProofWriter dataset both with and without distractors in the context and compare the results to the FT-ICR baseline.

The results show that Reckoning\({}_{}\) can successfully (average exact match score of \(99.3\%\)) recover the relevant facts from its model parameters when the context does not include any distractors. Note that this is comparable to the FT-ICR baseline, for which the task is much easier as it can directly attend to and copy the facts from input, while Reckoning\({}_{}\) no longer has direct access to them. When the context includes distractors, both Reckoning and FT-ICR struggle to identify and reproduce _only_ the relevant facts. However, the per

   Method & \(_{CLM}\) & \(_{CLM}\) \\  Reckoning\({}_{}\) & 10.74 & 9.55 \\ Reckoning\({}_{}\) & 0.167 & 12.61 \\   

Table 4: Average inner loop validation loss: final (\(_{CLM}\)) and difference from start to finish (\(_{CLM}\)).

    & **Wall-clock Time (s)** \\  _Single question_ & \\  FT-ICR & 0.1887 \\ Reckoning\({}_{1step}\) & 0.2532 \\ Reckoning\({}_{4step}\) & 0.9664 \\  _Multiple questions (18)_ & \\  FT-ICR & 2.0436 \\ Reckoning\({}_{1step}\) & 0.6228 \\ Reckoning\({}_{4step}\) & 1.4839 \\   

Table 3: Wall clock run-time, in seconds, of the fine-tuned ICR baseline and Reckoning.

formance for FT-ICR (average \(49.4\%\)) drops far below that of Reckoning (\(73.6\%\)), demonstrating that Reckoning is much better at disentangling the relevant knowledge from the distractors.

Finally, we show that Reckoning with a multi-task objective is also more robust to distractors as it trains the model to reproduce only the facts that would be relevant to a particular question we ask in the outer loop. As in Section 4.2, we use the ProofWriter dataset and, for each question, add all the distractors to the context. We train the model using the multi-task objective and report the label accuracy. While in Table 1, we originally saw a \( 1\%\) improvement from training with a multi-task objective on ProofWriter with no distractors, we see a much more significant performance gap in Figure 6 (\( 18.2\%\)) when distractors are available. We also note that the performance of the single-task model is essentially _random_ (see the Random-Facts baseline from Table 1). By learning _how_ to memorize knowledge in the inner loop to recall relevant facts in the outer loop, the model also learns how to encode facts more robustly over them.

## 5 Related Work

Logical Reasoning Datasets and BenchmarksAs a central building block of human cognition and intelligence , logical reasoning has been a long-pursued topic in the field of AI . Logical reasoning, in general, can be categorized in a trichotomy of deductive, inductive, and abductive reasoning . Multiple datasets have been published that evaluate neural models' ability on these three types of logical reasoning . Initially, logical reasoning tasks focused on hypothesis classification, where, given a theory consisting of multiple facts and rules, a model would determine whether the hypothesis was correct. Recently, transformer-based language models have been directly used to solve this task in synthetic , real-world , and adversarial  settings. However, simply predicting whether the hypothesis is valid does not elucidate whether the model correctly reasons over the provided knowledge. To better analyze and interpret the reasoning process of language models, new tasks focus on generating the valid proof that explains the model's decision . Our proposed method, Reckoning, is optimized for the hypothesis classification reasoning task and evaluates on many of these datasets .

Logical Reasoning over Natural LanguageHistorically, automatic logical reasoners used symbolic systems and formal languages as a knowledge representation . However, these systems were hard to scale up due to the knowledge-acquisition bottleneck and the brittleness of formal representation . With recent advances in transformer-based language modeling  and self-supervised pre-training , a novel paradigm for logical reasoning emerged, where pre-trained language models (PLMs) could be used as soft reasoners over knowledge expressed in natural language. Natural language as a knowledge representation allowed PLMs to handle raw input with diverse formats , resulting in PLMs being applied to various types of deductive , abductive , and inductive  reasoning tasks. However, language models as soft reasoners also showed structural weaknesses, as their performance dropped on complex logical operations , and their reasoning process was not interpretable . Consequently, a new line of work uses neuro-symbolic methods to combine the best of both language models and symbolic reasoning . Specifically, the interpretability gap motivated modular and step-wise reasoning systems that use PLMs as intermediate modules  to generate reasoning steps (e.g., proofs). In contrast to these works, our method Reckoning dynamically encodes natural language knowledge into the model parameters, thereby reasoning by mixing contextual knowledge with pre-encoded parametric knowledge and allowing the model to determine a conclusion based on its updated parametric knowledge.

Model EditingWhile our motivations are grounded in research on machine reasoning, our methods are more often used in the area of model editing. Model editing is a method to edit a model's parameters to correct its errors or update the model. Several works propose hypernetwork-based methods to edit knowledge in a model by predicting updates conditioned on new factual statements  or transforming the gradients from new provided facts  to make local edits to a model. Other

Figure 6: Reckoning performance when trained with a single-task and a multi-task objective under distractors. When trained with the multi-task objective, the model learns to memorize and reason over the relevant facts.

approaches focus on more direct edits of model behavior, such as directly modifying neuron outputs , localizing distinct feed-forward layers that are responsible for factual recall, and modifying these weights , and performing weight updates across multiple layers to perform simultaneous edits . Similarly, our method also rapidly edits the model parameters to add knowledge. However, our bi-level framework optimizes model edits for the reasoning task in the outer loop, allowing the model to learn to quickly memorize knowledge that can support the model's reasoning ability.

Language Models as Knowledge BasesOur work learns to reason by dynamically encoding contextual knowledge in the parameters of language models before answering questions about them. Previous studies have found that LLMs can store real-world facts learned during pre-training . Learning these facts during pre-training allows language models to be prompted  or adapted  to produce these facts on-demand. However, LLM knowledge is latent and hard to identify or control. The model generation is sensitive to specific words or phrases. LLMs emit knowledge encoded in the parameters only when prompted appropriately . It is also difficult to inject or update knowledge for LLMs , and the memorization of knowledge in LLMs is not optimized toward their reasoning ability. In our work, we seek to find a way to add knowledge to LLMs in a controllable and adaptive way that can benefit downstream reasoning applications.

## 6 Conclusion

We present Reckoning, a bi-level learning framework for multi-hop reasoning that encodes knowledge verbalized using natural language into a model's parameters through gradient updates. During training, the inner loop encodes the contextual knowledge into the model parameters by backpropagating a language modeling loss. In the outer loop, given only the question as input, the model solves reasoning problems using the memorized knowledge. Through bi-level optimization, Reckoning finds a set of meta-parameters that allows it to perform quick knowledge-based updates for reasoning. Our experiments show that Reckoning learns to reason only by relying on its parametric knowledge after the external knowledge has been encoded. Using a multi-task objective that jointly optimizes reasoning and knowledge memorization in the outer loop, Reckoning outperforms ICR baselines that are trained to encode external knowledge as part of the context. Through our analysis, we show that Reckoning is more generalizable to problems with longer reasoning chains, less susceptible to irrelevant distractor knowledge, and that Reckoning is more efficient than the baseline when answering multiple questions that require common knowledge.