# Auxiliary Losses for Learning Generalizable Concept-based Models

Ivaxi Sheth

CISPA-Helmholtz Center for Information Security

ivaxi.sheth@cispa.de

Work done when the author was at Mila, Ecole de technologie superieure.

Samira Ebrahimi Kahou

Ecole de technologie superieure, Mila, CIFAR AI Chair

samira.ebrahimi-kahou@etsmtl.ca

###### Abstract

The increasing use of neural networks in various applications has lead to increasing apprehensions, underscoring the necessity to understand their operations beyond mere final predictions. As a solution to enhance model transparency, Concept Bottleneck Models (CBMs) have gained popularity since their introduction. CBMs essentially limit the latent space of a model to human-understandable high-level concepts. While beneficial, CBMs have been reported to often learn irrelevant concept representations that consecutively damage model performance. To overcome the performance trade-off, we propose cooperative-Concept Bottleneck Model (coop-CBM). The concept representation of our model is particularly meaningful when fine-grained concept labels are absent. Furthermore, we introduce the concept orthogonal loss (COL) to encourage the separation between the concept representations and to reduce the intra-concept distance. This paper presents extensive experiments on real-world datasets for image classification tasks, namely CUB, AwA2, CelebA and TIL. We also study the performance of coop-CBM models under various distributional shift settings. We show that our proposed method achieves higher accuracy in all distributional shift settings even compared to the black-box models with the highest concept accuracy.

## 1 Introduction

The recent advances in deep learning have impacted various domains such as computer vision , natural language processing , and speech recognition . Recently, the deployment of large models  has led to various concerns regarding privacy and safety since machine learning models are often considered black boxes. With the increasing use of such deep learning models in daily human life and their wide deployment, it is essential to understand model behaviors beyond final prediction. Since neural networks are considered opaque decision-makers, inaccurate decisions by models in applications such as medicine  or autonomous driving  lead to catastrophe for humans. To understand the inner workings of such black-box neural networks, the field of XAI  has emerged in recent times. Concept Bottleneck Models (CBMs)  are a family of neural networks that enable human interpretable explanations.

Concept-based models introduce a bottleneck layer before the final prediction. This bottleneck layer consists of human-interpretable concept predictions. For example, in the context of images of animals, these concepts could be "mane" in the case of a lion or "black and white stripes" in the case of a zebra.

Concept bottleneck models (CBMs) map input images to interpretable concepts, which in turn are used to predict the label. The intermediary concept prediction allows a human _supervisor_ to interpret and understand the concepts influencing the label prediction. In addition to explainability, CBMs offers an interesting paradigm that allows humans to interact with explanations. During inference, a supervisor can query for explanations for a corresponding label, and if it observes an incorrect concept-based explanation then the supervisor can provide feedback.

While CBMs present benefits with models' explainability, Mahinpei et al.  have shown that concept representations of CBM may result in information leakage that deteriorates predictive performance. It is also noted that CBM may not lead to semantically explainable concepts . Such bottlenecks may result in ineffective predictions that could prevent the use of CBMs in the wild.

Along with model transparency, another challenge that modern neural networks face is robustness to distributional shifts . Deep learning models fail to generalize in real-world applications where datasets are non-iid . The absence of a comprehensive study examining the behavior of CBMs under distributional shifts is a significant limitation, potentially impeding their application in real-world scenarios.

In this work, we propose **cooperative-CBM** (coop-CBM) model aimed at addressing the performance gap between CBMs and standard black-box models. Coop-CBM uses an auxiliary loss that facilitates the learning of a rich and expressive concept representation for downstream task. To obtain orthogonal and disentangled concept representation, we also propose **concept orthogonal loss** (COL). COL can be applied during training for any concept-based model to improve their concept accuracy. Our main contributions are as follows:

* We proposed a multi-task learning paradigm for Concept Bottleneck Models to introduce inductive bias in concept learning. Our proposed model coop-CBM improves the downstream task accuracy over black box standard models.
* Using the concept orthogonal loss, we introduce orthogonality among concepts in the training of CBMs.
* We perform an extensive evaluation of the generalisation capabilities of CBMs on three different distribution shifts.
* We looked at using human uncertainty as a metric for interventions in CBMs during test-time.

## 2 Related Works

Concept-based ModelsEarly concept-based models that involved the prediction of concepts prior to the classifier were widely used in few-shot learning settings [9; 62]. Other works propose to predict human-specified concepts with statistical modeling [28; 29]. Unsupervised concept learning methods use a concept encoder to extract the concepts and relevance network for final predictions [4; 43]. Although these methods are useful in the absence of pre-defined concepts, they do not enable effective interventions. Concept whitening  was introduced as a method to plug an intermediate layer in place of the batch normalization layer of a Convolutional Neural Network (CNN) to assist the model in concept extraction. CBM  extends the idea by decomposing the task into two stages: concept prediction through a neural network from inputs, and then target prediction from the concepts. Many works have proposed models built on CBMs to either improve the downstream task accuracy [44; 60; 26] or mitigate the concept leakage [33; 20]. There has been a line of work extending CBMs to real-world applications such as medical imaging [58; 13], autonomous driving  and reinforcement learning  CBMs require annotated concepts which poses a challenge for their applications to large-scale image datasets. Yuksekgonal et al.  propose using concept activation vectors  and Oikarinen et al.  used multimodal models such as CLIP  to annotate concepts for CBMs. Although these either require concept bank or suffer from pretrained model's biases .

Alternative lossesThe training of CBM and its variants typically involves the use of Cross Entropy (CE) loss. Several variants of the CE loss have been explored in the past to improve the discriminative power of learned feature representations of data [19; 46; 56; 39]. Ranasinghe et al. , Vorontsov et al.  introduce the use of orthogonality in feature space to encourage inter-class separation and intra-class clustering. Our work builds upon  by introducing orthogonality constraints in the concept feature space.

## 3 Method

### Background

Consider a standard supervised learning setting for a classification task, where models are trained on a dataset \(=\{x_{i},y_{i}\}_{i=1}^{N}\) with \(N\) data samples. Standard models aim to predict the true distribution \(p_{}(y|x)\) from an input \(x\). Although such a setting has been proven effective on vision benchmarks, users are unaware of the detailed inner workings of the model. Therefore, CBMs introduces intermediate prediction of human-understandable concepts before the model prediction.

In the _supervised concept-based model_ setting, the dataset uses additional labeled concept vectors \(c_{i}\{0,1\}^{a}\) where each element indicates the presence of one of \(a\) high-level concepts. This allows supervised concept learning in addition to target learning. Following a simplistic causal graph for data generation, \(y c x\), CBMs consist of two models. The first model \(f_{X C}\) maps the input image \(x\) to concepts \(c\), while the second model \(g_{C Y}\) maps the concepts \(c\) to the label \(y\).

 CBMs can be categorized by their method of training \(g_{C Y}\) from the obtained concept representations \(f(c|x)\). This could be done in the following manner: _jointly_, where both \(f_{X C}\) and \(g_{C Y}\) are trained simultaneously end-to-end, _sequentially_, where \(f_{X C}\) is trained first, after which \(g_{C Y}\) is trained using \(p_{f}(c|x)\) representations, and finally _independently_, where \(f_{X C}\) and \(g_{C Y}\) are trained individually and then combined.

InterventionsInterventions are a core motivator of CBMs. The bottleneck model allows for interventions by editing the concept predictions. Since CBMs consider correcting the predicted concepts through interventions during test-time, the corrected concepts are not back-propagated through \(f_{X C}\) and \(g_{C Y}\). During test-time intervention the predicted concepts can be modified by a supervisor to their ground truth values, leading to "adjusted" concepts prediction. We represent the predicted concepts as \(=p_{f}(c|x)\) and the modified concepts as \(\). We consider test-time interventions as an important aspect of explainable models in safety-critical applications. We hypothesize that model-supervisor interaction must lead to the development of a symbiotic relationship between the model and the expert. Here, the expert learns about the potential causation between a concept and its corresponding label, and the model learns true concept values from the expert. We attempt to shine a light on these test-time interventions by simulating realistic scenarios by introducing human uncertainty.

### Coop-CBM

While CBMs provide concept explanations behind a prediction, it has been observed that this can come at the expense of lower model accuracy compared to black-box standard models . In this work, we propose a concept-based architecture, coop-CBM to improve the performance of CBMs on downstream classification tasks.

MotivationThe different training paradigms in CBMs introduced by Koh et al.  give rise to differences in their concept representations, \(p_{f}(c|x)\). Koh et al.  reports that joint CBMs have the highest task accuracy among the different CBM training procedures albeit still lower than standard models. Intuitively, this suggests that joint CBMs, which train both concept predictor and task predictor simultaneously, are able to encode the information about the task label \(y\) into concept labels \(c\) better than sequential and independent CBMs. In the case of joint CBMs, backpropagation of task loss through the concept predictor aids the overall model in giving more accurate predictions. In this work, we aim to leverage such "soft" information

Figure 1: Coop-CBM model that consists of an encoder, concept learner \(f\), auxiliary label learner \(h\), and task label learning \(g\). The encoder transforms input data into a feature representation, which is used by \(f\) to predict high-level concepts and \(h\) to predict a supplemental auxiliary label, and finally \(g\) predicts the final task label conditioned on the concepts only.

about the task to improve accuracy. Coop-CBM aims to leverage soft label information in concept predictors to better align the concept predictions to the corresponding label.

ModelCoop-CBM introduces a multi-task setting before the final prediction. Along with predicting the concepts \(c\), we introduce the prediction of task labels in the concept predictor. This allows the model to learn relevant signals and inductive biases of downstream tasks in the concept learning phase. In essence, we now have model \(f_{X C}\) that predicts concepts \(c\) from input \(x\) and new model \(h_{X Y}\) that predicts label \(y\) from input \(x\). This enables the model to learn relevant knowledge about the task that could be absent in the bottleneck concepts \(c\). Although this setting makes the model interpretable, since corresponding concepts to a label can be obtained, one loses the causal \(x c y\) property. Additionally, it does not allow test-time interventions, which is a key application of concept-based models that facilitates human-model interactions. Therefore, to maintain the original properties of CBM, coop-CBM uses a label predictor \(g\) which takes the predicted concepts \(c\) from \(f_{X C}\) as input and gives the final label \(y\) as output. Hence to avoid confusion, we call the label prediction from \(h\)_immediate label_, \(y^{}\), and the final task label, \(y\). It must be noted that \(f\) and \(h\) share all but the last linear layer. Therefore the concept predictor \(f\) and \(g\) parameters \(\), \(\) are trained in the following manner:

\[=*{}_{}[*{ argmax}_{}[\ p(c,y^{}|x;)]]=*{}_{}[*{argmax}_{}[\ p_{f}(c|x;) +\ p_{h}(y^{}|x;)]] \]

\[=*{}_{}[*{ argmax}_{}[\ p(y|c;)]] \]

Therefore coop-CBM is trained using a linear combination of three different CE losses: \(_{}\) as concept loss, \(_{^{}}\) as immediate label loss and \(_{}\) as task prediction loss.

\[*{arg\,min}[_{C}(f(x),c)+_{y^{}}(h(x ),y)+_{y}(g(f(x),y)] \]

In summary, we argue that the introduction of an immediate label introduces the concept predictor to learn meaningful information about the task while still being interpretable. The \(h_{X Y^{}}\) model intuitively acts as a regularizer for meaningful concept prediction.

Mutual information perspectiveWe hypothesize that by using coop-CBM, the concept predictor acquires better knowledge about \(y\). In particular, this can be beneficial when fine-grained concept annotations are not available. We, therefore, suspect that the mutual information (MI) between the input image, concept representations, and the label becomes richer and more expressive as compared to CBM . One way to quantify this is by visualising the MI planes throughout the training, similar to Zarlenga et al. .

### Concept Orthogonal Loss

Following the current CBM literature, we use cross-entropy loss to train each of the models, \(f_{X C}\), \(h_{X Y^{}}\) and \(g_{C Y}\) in coop-CBM. In model \(f\), each concept is learned via independent and separate classifiers. Given their binary representation, it is intuitive to improve the embedding space of concepts by increasing separability. To do so, we introduce the concept orthogonal loss (COL). By incorporating COL into the training process, we aim to enhance the overall separability of the concept embeddings, leading to improved performance and interpretability of the coop-CBM model.

MotivationDue to the variations in training strategies employed in different CBM models, the resulting concept representations can exhibit varying levels of accuracy. The concept accuracy refers to how effectively the learned concept representations align with the ground truth or human-defined concepts. Coop-CBM was concerned with the predictive performance of the task, but here we focus on the concept label accuracy. Higher concept label accuracy signifies improved interpretability. As observed by Koh et al. , the concept accuracy of joint CBMs models is lower than other variants because the concepts learned are not completely independent of each other, also called leakage by . Hence, increasing the inter-concept distance and intra-concept clustering throughout the concept vector for the entire dataset can allow the model to learn beyond co-dependent concept representations.

COLIn addition to CE loss for learning concepts, we introduce novel concept orthogonal loss by conditioning orthogonality constraints on concept feature space. The disadvantage of CE loss is that it does not set a specific distance or separation between different concept representations in the feature space. Consider the CE loss for each concept prediction:

\[_{CE}(c,)=_{c_{i}}^{c_{N}}-c_{i}(_{i})-( 1-c_{i})(1-_{i}) \]

Using this traditional CE loss for each concept, in Equation 4 we are essentially minimizing the difference between the predicted probability distribution and the true probability distribution of the binary concepts. The model, \(f_{c_{i}}\) attempts to learn probability distribution for when a concept is active or inactive respectively. CE does not explicitly enforce separation between concepts.

With concept orthogonal loss (COL), we enforce the separation in the latent representation of the concepts. Our aim with COL is to group similar features together while ensuring that features belonging to different concept classes do not overlap with each other. COL \(_{COL}\) enforces inter-concept orthogonality and intra-concept clustering. We define inter-concept separation and intra-concept similarity as \(d_{1}\) and \(d_{2}\) respectively. We enforce orthogonality constrain via cosine similarity. We define the \(_{COL}\) loss in the shared last layer, \(q\) of coop-CBM before concept and auxiliary label predictions. We enforce COL constraints within each batch, \(B\).

\[d_{1}=_{i,j B\\ c_{i}^{2}=c_{j}^{3}\\ a A}^{T}q_{j}}{||q_{i}||\ ||q_{j}||};\ d_{2}=_{ i,j B\\ c_{i}^{2} c_{j}^{3}\\ a A}^{T}q_{j}}{||q_{i}||\ ||q_{j}||} \]

where \(||.||\) denotes the Frobenius norm.

Using the cosine distances, \(d_{1}\) and \(d_{2}\), we simultaneously aim to increase the distance between different latent concept representations and decrease the distance between representations from the same concept. We can introduce a hyperparameter, \(\) to accordingly give weightage to either \(d_{1}\) or \(d_{2}\). The similarity loss \(d_{1}\) between the feature representation of two samples corresponding to the same concept aims to push the \(d_{1}\) towards \(1\) which means that the feature representations of same concept should be as similar as possible. As for the dissimilarity loss, the goal is to push the loss towards \(0\), which enforces that the feature representations of different class samples should be as dissimilar as possible. Therefore we consider the absolute value of \(d_{2}\).

\[_{COL}=(1-d_{1})+|d_{2}| \]

It is important to note that CE loss is applied to each concept binary classification task, which measures the difference between the predicted class probabilities and the true labels. The introduction of COL encourages the network to learn features that are both discriminative and non-redundant among concepts at an intermediary network level. By combining the COL and CE losses, the network is trained to learn discriminative that separate each concept and useful features for classifying when a concept is active. A benefit of COL is it can be universally any concept-based model to encourage orthogonality between different concepts.

In this section, we introduce two auxiliary losses, one to improve the task accuracy using multi-task setting and the other to improve concept representation in latent space, leading to improved concept accuracy. The final loss is a linear combination (\(,,\) are hyperparameters for weighting in Equation 7) of concept and task losses along with immediate and concept orthogonal losses.

\[[_{C}(f(x),c)+_{y^{}}(h(x),y)+ _{y}(g(c),y)+_{COL}(q)] \]

### Interventions

Koh et al.  demonstrated the potential of CBMs for facilitating human-model interaction and improving task performance during inference. But it can be time-consuming and costly to have domain experts go over each concept, hence some of the recent and concurrent works proposed to use uncertainty as a metric to select interventions.

We propose a lightweight approach that strategizes the supervisor-model interaction. Our method is intuitive and considers three aspects of intervention:1. Uncertainty of concept prediction - CUS represents the confidence of the model to predict latent concepts, \(f_{X C}\).
2. Supervisor confidence for concept correction - SCS represents the reliance on the supervisor to intervene and subsequently correct the concepts accurately.
3. Importance of concept for label prediction - CWS denotes the significance of each concept for the subsequent downstream task.

Chauhan et al.  propose to optimize interventions over a small validation set using CUS. In comparison, we consider access to the validation set unrealistic. Shin et al. , Sheth et al.  evaluated interventions more comprehensively and studies the behavior of CBMs during inference by selecting CUS and CWS metrics. We additionally take into account a supervisor's confidence in domain knowledge, SCS and their expertise in correcting the concepts. Concurrent work  also looked at human uncertainty for CBMs in depth. Unlike previous works that evaluate test-time interventions on the test splits of respective datasets, we also analyze test-time interventions in OOD setting in the Appendix E.

## 4 Experiments

For our evaluation we consider several image classification benchmark datasets. Our work performs an in-depth empirical analysis of the effectiveness of concept-based models in the presence of different distributional shifts simulating real-world scenarios where data is diverse.

BaselinesWe consider the models proposed by Koh et al.  as our baseline. Additionally, we compare our performance with recent concept-based models that are built on CBM [60; 20]. Due to biases introduced during automatic concept acquisition as mentioned by the authors of Yukekgonul et al. , we consider it to be an unfair baseline to compare generalization properties. They also vary in the number of concepts considered which can also damage the performance and are limited by either the presence of concept bank or application (CLIP will fail to generate concepts for TIL dataset), making an unfair comparison.

DatasetsWe use Caltech-UCSD Birds-200-2011 (CUB)  dataset for the task of bird identification. Every dataset image contains 312 binary (eg: beak color, wing color) concepts. We additionally use Animals with Attributes 2 (AwA2)  dataset for the task of animal classification. The dataset contains 85 binary concepts. We use all of the subsets of the Tumor-Infiltrating Lymphocytes (TIL)  dataset for cancer cell classification.

## 5 Results and Analysis

The primary metric used for the downstream classification task is accuracy. We use the same metric to evaluate the effectiveness of the intervention. We first evaluate the different model performances on the test data split of respective datasets and report the task accuracy \(g_{C Y}\) for coop-CBM model variants.

Coop-CBM improves task accuracyThe evaluation of the performance of a model is based on the final prediction accuracy. In Table 1, we compare the performance of coop-CBM against other baseline models. We first observe that CBMs experienced a significant drop in performance compared

   Model type & CUB & AwA2 & TIL \\  Standard _[No concepts]_ & \(82.3\) & \(96.2\) & \(51.1\) \\  & \( 0.2\) & \( 0.1\) & \( 0.9\) \\ Independent CBM  & \(76.0\) & \(94.9\) & \(47.4\) \\  & \( 0.4\) & \( 0.3\) & \( 1.0\) \\ Sequential CBM  & \(76.3\) & \(94.6\) & \(47.9\) \\  & \( 0.2\) & \( 0.2\) & \( 0.9\) \\ Joint CBM  & \(80.1\) & \(95.4\) & \(49.6\) \\  & \( 0.1\) & \( 0.1\) & \( 0.7\) \\ CEM  & \(82.5\) & \(96.2\) & \(51.3\) \\ CBM-AR  & \( 0.2\) & \( 0.1\) & \( 1.3\) \\  & \( 0.4\) & \( 0.0\) & \( 1.0\) \\  Coop-CBM (ours) & \(83.6\) & \(96.6\) & \(53.4\) \\  & \( 0.3\) & \( 0.1\) & \( 0.8\) \\  & \(\) & \(\) & \(\) \\  & \( 0.2\) & \( 0.1\) & \( 0.9\) \\   

Table 1: Model accuracy on CUB, AwA2 and TIL datasetsto the standard model that did not use concepts. Our proposed model, coop-CBM with immediate label prediction achieves state of art accuracy and statistically significant results on every dataset. We have observed a significant improvement in the performance of the CUB (\(+1.8\%\) increase from the standard model) and TIL(\(+3.1\%\) increase from the standard model) datasets. This finding is important as it suggests that machine learning models can be designed to overcome the high accuracy vs interpretability tradeoff. Our performance can be further boosted by introducing orthogonality among different concepts. It must be noted that CUB is a fairly densely annotated dataset, which might not always be realistic, hence we also benchmark our model by training on a fraction of concept sets. We also observe a similar trend in results in concept-scarce settings (see Appendix D.2). This suggests that our method is robust to concept selection, which can be beneficial in scenarios where the number of available concepts is limited or expensive to obtain.

COL improves concept accuracyPreviously in Table 1, we observed that adding concept orthogonal loss to coop-CBM improved its downstream accuracy, in Table 2, we study the impact of adding COL to baseline concept models. Our experiments show that adding COL improves the concept accuracy by a significant margin, especially in joint CBM, CEM, and CBM-AR settings. A known pitfall of CBMs is, _concept leakage_ could be potentially prevented by increasing the separation between their concept representations. By maximizing the inner product between the concept embeddings of different concepts, we can ensure that each concept is represented in a separate and distinct direction in the embedding space. This helps in preventing the models from relying on irrelevant concepts. Further to intuitively understand the differences in the concept representation of our model, we compute the histogram for the predicted concept logits. From Figure 3 we see that Coop-CBM+COL minimizes the concept loss better (with help from the auxiliary loss which aids representation learning), which results in clearer separation of logits.

Clipping concept values to avoid concept leakageFurther, we employ clipping of concept prediction proposed by Mahinepei et al.  to further mitigate information leakage in 3 through two experiments. For the first experiment, we trained the model by clipping the predicted concept values to "hard" labels. Second, we trained the model as we have described earlier in the paper (using soft labels) and evaluated the test set by clipping to "hard" labels. From the above experiments, we conclude that the model is able to learn a good representation of the concepts without necessarily leaking information.

Accounting for Human uncertainty for interventionsAs discussed earlier, higher concept accuracy also improves the test-time interventions as seen in Figure 2. While other works used concept weights and uncertainty as metrics to select the interventions, our work introduces a more realistic setting by introducing human uncertainty additionally. The previous works do not account for human error or certainty. Albeit human uncertainty is difficult to quantify since it is often subjective, we use the concept visibility data in the CUB dataset to quantify the confidence score, SCS. The

   Model type & w/o COL & w COL \\  Independent CBM  & \(\) & \(\) \\  & \( 0.0\) & \( 0.1\) \\ Sequential CBM  & \(\) & \(\) \\  & \( 0.0\) & \( 0.1\) \\ Joint CBM  & \(\) & \(\) \\  & \( 0.1\) & \( 0.2\) \\ CEM  & \(\) & \(\) \\  & \( 0.2\) & \( 0.1\) \\ CBM-AR  & \(\) & \(\) \\  & \( 0.1\) & \( 0.1\) \\  Coop-CBM (ours) & \(\) & \(\) \\  & \( 0.2\) & \( 0.2\) \\   

Table 2: Concept prediction accuracy for each model before and after adding COL for CUB dataset

    &  &  \\   & Std & Expl & Exp2 & Std & Exp1 & Exp2 \\  Coop-CBM & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  & \( 0.3\) & \( 0.2\) & \( 0.5\) & \( 0.8\) & \(\) & \( 1.1\) \\ +COL & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  & \( 0.2\) & \( 0.2\) & \( 0.6\) & \( 0.9\) & \( 0.9\) & \( 0.8\) \\   

Table 3: Testing for information leakage in our proposed model. Std - standard conditions when joint probabilities are learned to predict the final task, no clipping. Exp1 - During training, we clipped the predicted concept values to “hard” labels. Exp2 - During the evaluation, we clipped the predicted concept values to “hard” labels.

annotators rank visibility from \(1-4\), \(1\) being 'not visible' and \(4\) being 'definitely visible'. Figure 2 presents an accuracy curve with an increasing number of interventions on the vanilla CBM model during test-time for proper and fair comparison to . It suggests that considering SCS leads to the most meaningful interventions. On intervention comparison between different concept-based models, we observe that coop-CBM leads to significant interventions that improve downstream performance, especially in TIL dataset where both image descriptive and non-descriptive concepts are present.

In summary, the coop-CBM model and the addition of the concept orthogonal loss help to improve both task and concept accuracy without sacrificing interpretability, which was a common tradeoff in previous methods. This result demonstrates the potential for concept-based models to be more effective in human-AI interactions, especially in domains where expert intervention and interpretability are critical such as healthcare.

### Performance under distribution shifts

#### 5.1.1 Background spurious correlations

Shortcut-based biases  exist in many datasets, where deep learning can easily learn spurious features. In the presence of shortcuts, the model can learn to use spurious features to approximate the true distribution of the labels, as opposed to learning core features. It can be of particular interest to evaluate the performance of concept-based models in the presence of shortcuts. Furthermore, using explainable concepts to facilitate human-model interaction could help reduce the impact of these biases.

The shortcut we consider here is a spurious correlation to the background color in CUB dataset. Loosely following the experimental setup of , we correlate the background of a species of bird to its corresponding label. For the dataset, we segment the bird images and add a colored background to all of the images. Each class here is correlated to a randomly generated color background with a probability of \(80\%\) for the train set. The in-domain test set contains images with similar color background probability as the train set while the correlation in the out-domain test set is reduced to \(30\%\). We also consider the hair color-based shortcut induced in Large-scale CelebFaces Attributes (CelebA) dataset where we focus on gender classification between males and females. We aggregate and then construct a modified version of CelebA that learns the shortcut of blonde hair color with women similar to .

   &  &  \\   & In & Out & In & Out \\  Standard & \(\) & \(27.7\) & \(\) & \(76.3\) \\  & \( 0.3\) & \( 2.8\) & \( 0.5\) & \( 1.8\) \\ Independent CBM  & \(81.1\) & \(30.9\) & \(93.8\) & \(76.2\) \\  & \( 0.5\) & \( 2.3\) & \( 0.6\) & \( 1.9\) \\ Sequential CBM  & \(81.2\) & \(30.8\) & \(93.8\) & \(76.1\) \\  & \( 0.5\) & \( 22.2\) & \( 0.6\) & \( 1.9\) \\ Joint CBM  & \(83.6\) & \(32.9\) & \(94.7\) & \(80.1\) \\  & \( 0.4\) & \( 3.2\) & \( 0.5\) & \( 1.7\) \\ CEM  & \(84.0\) & \(34.0\) & \(95.2\) & \(81.0\) \\  & \( 0.4\) & \( 2.9\) & \( 0.4\) & \( 1.7\) \\ CBM-AR  & \(82.6\) & \(33.6\) & \(95.9\) & \(79.7\) \\  & \( 0.6\) & \( 2.5\) & \( 0.5\) & \( 2.0\) \\    } & \) & \(35.4\) & \(95.4\) & \(81.3\) \\  & \( 0.3\) & \( 2.5\) & \( 0.6\) & \( 1.9\) \\ \(+\)**COL** & \(85.8\) & \(\) & \(95.9\) & \(\) \\  & \( 0.4\) & \( 2.7\) & \( 0.7\) & \( 2.7\) \\  

Table 4: Accuracy of different models under distributional shift - background spurious correlation.

Figure 2: L\(\)R:1) Accuracy vs intervention graph using joint CBM while including supervisor uncertainty. 2)Accuracy vs intervention graph in presence of incorrect interventions by the supervisor using joint CBM. 3) Comparing different model’s random interventions on the TIL dataset. 4) Comparing different model’s random interventions on the CUB dataset.

[MISSING_PAGE_FAIL:9]

In Section 5.1.1 and Section 5.1.2, we conducted experiments concerned with distribution shifts in image space, in this section, we introduce the evaluation of CBMs by simulating distribution shifts in the concept space. To investigate the potential risks of spurious correlations in concept models, we introduced Gaussian noise to the binary concepts. By altering the standard deviation (\(\)) of the Gaussian noise, we effectively correlated the shortcut (here noise level) with the image through the concept. To simulate a more realistic setting, instead of adding distinct noise \(\) for each class species, we aggregate random groups of species and add same \(\) to them. In our experiment for the CUB dataset, we add \(10\) different levels of noise (simulated by \(\)) to groups of 20 species labels (200 total classes). For AwA2, we create groups of \(10\) classes. This approach allowed us to simulate the possibility of introducing unintended correlations between the concepts and the images. By studying the effects of these correlations on the performance of the concept models, we gain insights into the robustness and reliability of the models in handling contaminated concepts. We observe that by introducing a separation between different concepts through COL, our model performs significantly better than the rest of the baselines.

## 6 Future work and Limitations

In this work, we introduced coop-CBM, a novel concept-driven method to balance AI model interpretability and accuracy. We utilized the Concept Orthogonal Loss (COL) to improve concept learning and applied coop-CBM to various datasets, achieving better generalization, robustness to spurious correlations, and improved accuracy-interpretability trade-offs.

However, our approach has limitations. It relies on labeled concept vectors, which can be challenging in domains with limited annotations and face biases in concept annotation methods. A potential future work could be to extend it to methods that do not assume concept label [58; 38]. Further, we used accuracy as a metric to evaluate concept leakage, in the future, it would be interesting to explore other metrics beyond the accuracy of concept prediction. A future extension of COL could be to evaluate which concepts should be explicitly orthogonalized. We recognize that our model has a few hyperparameters to be optimized. Furthermore, our model assumes that learned concepts align closely with human notions, but this alignment isn't always perfect, affecting comprehensibility. Future research could improve the accuracy of concept-based models by providing meaningful explanations and incorporating additional evaluation metrics. Another potential direction could be to assess the mutual information and therefore establish theoretical grounding to describe the superior performance of coop-CBM.

## 7 Discussion and Conclusion

In this work we proposed two significant contributions to the paradigm of concept-based models. First, we introduced a multi-task model that predicts an intermediary task label along with concept prediction. This is particularly helpful when dense and relevant concept annotation is absent, such as in TIL dataset. Second, we introduced orthogonality constrain in the concept representation space during training via concept orthogonal loss. This loss increases inter-concept separation and decreases intra-concept distance. For both of our proposed methods, we perform extensive experiments on diverse datasets and different distributional shifts. We observe that the bottleneck layer before the final prediction enables concept-based models to exhibit robustness to spurious correlations in the background. Coop-CBM along with COL achieves state-of-art performance for both task accuracy and concept accuracy. Our work indicates that coop-CBM and COL have a strong ability to adapt and generalize well across diverse datasets and real-world scenarios.

   Model type & CUB & AwA2 \\  Independent CBM  & \(69.7\\  0.6\\ \) & \(80.1\\  0.4\\ \) \\ Sequential CBM  & \(69.6\\  0.5\\ \) & \(80.3\\  0.2\\ \) \\ Joint CBM  & \(71.0\\  0.4\\ \) & \(81.3\\  0.4\\ \) \\ CEM  & \(71.2\\  0.6\\ \) & \(81.9\\  0.3\\ \) \\ CBM-AR  & \(71.1\\  0.4\\ \) & \(81.5\\  0.3\\ \) \\ Coop-CBM (ours) & \(71.9\\  0.6\\ \) & \(82.5\\  0.2\\ \) \\ \(+\) COL & \(\\  0.4\\ \) & \(\\  0.2\\ \) \\   

Table 6: Accuracy of different models under distributional shift-noise concept correlation Acknowledgements

We would like to thank Vincent Michalski and the reviewers for engaging in discussions on the earlier version of the paper. The authors would like to thank Google, CIFAR (Canadian Institute for Advanced Research) and NSERC (Natural Sciences and Engineering Research Council of Canada) for supporting and funding the research and Digital Research Alliance of Canada for the compute support.