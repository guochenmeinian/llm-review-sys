# M\({}^{2}\)Hub: Unlocking the Potential of Machine Learning

for Materials Discovery

 Yuanqi Du1,* Yingheng Wang1,* Yining Huang2 Jianan Canal Li3 Yanqiao Zhu4

**Tian Xie5 Chenru Duan6 John M. Gregoire7 Carla P. Gomes1 1 Cornell 2 Northwestern 3 UCB 4 UCLA 5 MSR AI4Science 6 Microsoft Quantum 7 Caltech * Equal Contribution**

###### Abstract

We introduce M\({}^{2}\)Hub, a toolkit for advancing machine learning in materials discovery. Machine learning has achieved remarkable progress in modeling molecular structures, especially biomolecules for drug discovery. However, the development of machine learning approaches for modeling materials structures lag behind, which is partly due to the lack of an integrated platform that enables access to diverse tasks for materials discovery. To bridge this gap, M\({}^{2}\)Hub will enable easy access to materials discovery tasks, datasets, machine learning methods, evaluations, and benchmark results that cover the entire workflow. Specifically, the first release of M\({}^{2}\)Hub focuses on three key stages in materials discovery: virtual screening, inverse design, and molecular simulation, including 9 datasets that covers 6 types of materials with 56 tasks across 8 types of material properties. We further provide 2 synthetic datasets for the purpose of generative tasks on materials. In addition to random data splits, we also provide 3 additional data partitions to reflect the real-world materials discovery scenarios. State-of-the-art machine learning methods (including those are suitable for materials structures but never compared in the literature) are benchmarked on representative tasks. Our codes and library are publicly available at [https://github.com/yuanqidu/M2Hub](https://github.com/yuanqidu/M2Hub).

## 1 Introduction

With the methodological advancements in machine learning, an increasing number of machine learning models have been developed and applied to solve scientific problems, from simulating molecular systems with millions of particles to predicting accurate protein structures Zhang et al. (2018); Jumper et al. (2021). The primary focus of machine learning in the chemical sciences has remained in the domain of molecular structures, (bio)molecules including small molecules, proteins, RNAs, etc. Atz et al. (2021); Rives et al. (2021); Townshend et al. (2021). However, materials constitute a large portion of the chemical space which have been significantly less studied, especially in the machine learning community. Among scientific problems, materials discovery plays a vital role in driving innovations and progress across various fields spanning energy, electronics, healthcare, and sustainability Sanchez-Lengeling and Aspuru-Guzik (2018); Gomes et al. (2021). However, the traditional trial-and-error approach to materials discovery is expensive and time-consuming. Over decades, classical machine learning methods have already been widely applied in assisting materials discovery, Schmidt et al. (2019) yet the impact of machine learning for solid state materials lags behind its efficacy in other areas of chemical science.

Witnessing the success of machine learning in solving grand challenges in science Wang et al. (2018); Jumper et al. (2021), one of the key ingredients is the infrastructure that supports the machine learning community to build the machine learning workflow: data preparation/processing, model development, performance evaluation, and model improvement based on the evaluation feedback. While efforthas been made to make materials datasets available to the machine learning community Blaiszik et al. (2019); Dunn et al. (2020); Clement et al. (2020); Qayyum et al. (2022); Durdy et al. (2023), existing work mainly focus on providing data servers that allow the users to query materials data and predefined benchmark sets. However, to bridge the gap between the molecular and solid state materials, we identify the need for a unified platform to facilitate the development of machine learning approaches for materials discovery purpose, including (1) centralized data sources with diverse materials, property and task types, (2) clear problem formulations, (3) realistic problem settings (e.g. data split), and (4) appropriate benchmarks and transparent comparisons with prior methods.

In order to address the aforementioned challenges, we establish M\({}^{2}\)Hub, which integrates and connects different machine learning building blocks in the materials discovery (Fig. 1). The cornerstone of M\({}^{2}\)Hub is a benchmark that incorporates several key aspects: (i) it integrates three key tasks: virtual screening, molecular dynamics simulation, and inverse design, which are translated using machine learning constructs such as materials representation learning, machine learning forcefield, and generative materials design; (ii) it is underpinned by a curated set of 11 datasets that encompass 6 types of materials with 56 tasks across 8 distinct material properties. In addition to the standard random split, we have included 3 realistic (out-of-distribution) data splits to enhance the robustness of model evaluation; (iii) a distinctive feature of our benchmark is the emphasis on the generative design of materials. For this, we provide machine learning formulations, evaluation metrics, and oracle functions to facilitate further research and development in this area; (iv) finally, our benchmarks evaluate not only the commonly used material representation learning methods, but also those designed for non-periodic molecular structures. These methods are applied to 13 representative tasks for material property prediction.

The flow of this paper is as follows: we introduce the background and related work on developing machine learning methods for materials discovery in Sec. 2; we present the overview of M\({}^{2}\)Hub including problem formulation, dataset curation, data processing, evaluation and oracle function for inverse design in Sec. 3; in Sec. 4, we detail the implemented machine learning models, benchmarking results, observations, and insights emerging from the results.

## 2 Background

### Materials Representation Learning

Material representation learning refers to representing materials structures in an expressive and machine-readable format for downstream studies, from property prediction to materials generation. Recent advances in graph neural networks bring a wave of representing materials structures as graphs where nodes represent atoms and edges represent bonds or interactions. A line of works has been proposed to adapt this structured inductive bias into deep learning models.CGCNN Xie and Grossman (2018) introduces a multi-edge graph representation to capture periodicity. MEGNet Chen et al.

Figure 1: M\({}^{2}\)Hub: Materials discovery meets Machine learning. A-F on the left figure demonstrates machine learning approaches used in each stage of the materials discovery pipeline on the right figure (dashed lines denote currently unavailable experiment-related tasks).

 unifies molecules and crystal structures representations by graph neural networks representing each atom as node and interaction/bond between atoms as edge. More recently, ALIGNN considers both atomistic and line graphs which externally capture angular information Choudhary and DeCost . Equivariant graph neural networks (e.g., E3NN Geiger and Smidt ) have also been applied thanks to its root-translational equivariance property. This task has also attracted increasing interests through years Kaba and Ravanbakhsh , Yan et al. , DAS et al. , Lin et al. , Das et al. .

### Machine Learning Forcefields

Molecular dynamics simulation has become an essential tool to understand the microscopic dynamical behaviors of molecular systems. It is worth noting there is a common trade-off between two popular diagrams, empirical forcefields and _ab initio_ molecular dynamics. Empirical forcefields often rely on the hand-crafted parameters which are efficient yet inaccurate, while ab initio molecular dynamics rely on quantum-mechanical calculations which are precise but inefficient. Inspired by recent advances of deep learning in automated parameters learning and transferability, a large amount of works has been developed to learn machine learning forcefields from quantum-mechanical data to strike a balance between accuracy and efficiency. Specifically, it is expected to be more accurate than empirical forcefields and more efficient than quantum-mechanical calculations. Most representative work include DeepMD Zhang et al. , ANI-1 Smith et al. , and NeuqIP Batzner et al. .

### Materials Inverse Design

Designing new materials structures is a long-standing challenge, often known as the inverse design problem, in materials science Du et al. , Manica et al. . Before deep generative models have been applied to this problem, traditional computational methods often leverage quantum mechanical search over the possible stable materials including random search, evolutionary algorithm, element substitutions over known materials Glass et al. , Pickard and Needs , Hautier et al. . One line of works leverages a learned force field to minimize the energy of the structure to reach a stable material Deringer et al. . Later, deep generative models have been applied to this problem where the models aim to model the distribution of the known crystal structures and learn to sample new structures from it. Early work leverage 3D voxel representation but it is nontrivial to fit atom from the generated voxels Hoffmann et al. , Noh et al. . Later work instead leverage atomic representation directly Zhao et al. . G-SchNet Gebauer et al.  instead proposes an auto-regressive model that generates each atom in a sequential way. Notably, it remains largely unexplored for efficient and controllable crystal structure generation with machine learning methods. A recent work Xie et al.  builds upon the recent success of diffusion models on images and adapts them for crystal structure generation and optimization in an iterative refinement manner instead of one-shot or auto-regressive sequential generation.

## 3 Overview of M\({}^{2}\)Hub

### Problem Formulation

Material representation.Material structures can be represented as a set of atoms \(M=(m_{0},m_{1},,m_{N})\) in 3D space with atom types \(H=(h_{0},h_{1},,h_{N})^{N K}\) and atomic positions \(X=(x_{0},x_{1},,x_{N})^{N 3}\) where \(N\) denotes number of atoms and \(K\) denotes number of atom types (C, O, Fe, Al, etc.). Most materials are crystal structures which periodically repeat their unit cells in 3D space. In such cases, lattice vectors \(L=(l_{1},l_{2},l_{3})^{3 3}\) are utilized to describe the periodicity in 3D space. Note \(L\) is not rotation invariant, 6 invariant lattice parameters (lengths of lattice parameters and angles between them) can also

Figure 2: Regular workflow for studying materials with machine learning approaches (green colored steps denote materials science expertise and blue colored steps denote machine learning expertise.

be used to describe the lattice \((l_{a},l_{b},l_{c},,,)\). Overall, a material is denoted as \(M=(H,X,L)\) if it is periodic and otherwise \(M=(H,X)\).

Material graph representation.Regardless of periodicity, materials structures can be naturally represented as graphs \(G=(,)\), where \(\) is a set of vertices and \(=\{e_{ij}(k_{1},k_{2},k_{3})|i,j\{1,2,,N\},k_{1},k_{2},k_{ 3}\}\) is a set of \(D\) edges (\(k_{1}\), \(k_{2}\), \(k_{3}\) denotes the translation of the unit cell using lattice vector \(L\), none if not periodic); \(H^{N K}\) denotes the node features; \(X^{N 3}\) denotes the atomic positions; \(E^{D F}\) denotes \(F\) edge features (such as bonds or distances between each pair of nodes). The graph connections can be determined in multiple ways such as distance threshold, detailed in Sec. 3.3.

Predictive tasks.Predictive tasks often have paired input material \(M\) and label \(Y\), where given an input material \(M\), we aim to predict the expected label as \(p(Y|M)\). The label \(Y\) could be of various format such as binary, scalar, vector and distribution, detailed in Sec. 3.4. Both material representation learning and machine learning forcefield are considered as predictive tasks.

Generative tasks.Generative tasks could be divided into two parts: (1) distribution learning and (2) goal-oriented generation. Given a set of \(J\) materials \(=\{M_{i}\}_{i=1}^{J}\), distribution learning aims to learn the distribution of \(p(M)\) and sample new materials \(M_{} p(M)\). Goal-oriented generation aims to sample molecules fulfilling specific design targets (often defined by an oracle function \(f(M)\) such that \(M^{}=_{M p(M)}f(M)\).

### Data Description

We aim to curate a set of datasets covering the diversity of material, property, and task types and data amounts which will enable a variety of perspectives for machine learning model developments.

#### 3.2.1 Material types

**Inorganic bulks** refer to solid substances that lacks carbon-hydrogen bonds, that is, substances that are not organic compounds, such as metals, alloys, ceramics, and minerals. They are typically large-scale structures and are used in various applications, ranging from construction materials to electronics, due to their robustness and electrical/thermal conductivity.

**Organic crystals** are composed of carbon-based molecules arranged in a highly ordered pattern. They exhibit distinct molecular structures and are often used in the field of optoelectronics, such as organic light-emitting diodes (OLEDs), due to their unique optical properties. They can range in size from microscopic to macroscopic.

**Organic molecules** are individual carbon-based compounds that can be small in size, consisting of a few atoms, or large, such as polymers. They have diverse chemical structures and are widely used in pharmaceuticals, plastics, and organic electronics due to their flexibility in design and functionality.

**Bulk-adsorbate interfaces** refer to the boundary between a bulk material and an absorbed species, such as gases or liquids, on its surface. They are typically at the nanoscale and play a crucial role in various fields, including catalysis, gas sensing, transport, and energy storage, by influencing the interaction and reactivity of the absorbed species with the material.

   Data name & Materials type & Dim & PBC & Prop. type & Task type & \# tasks & \# data & Size & Method \\  Mathzschn & inorganic bulk & 37 (T, T) & elec./incach./absorb./optther. & scalar & 8 & 312–137.527 & 27.88–28.27 & Sim. \\ OMOF & metal-organic framework & 30 (T, T) & elec./incach. & scalar & 1 & \(>\)20.00 & 113.67\(\)26.86 & Sim. \\ OCO2 & bulk-adsorbate interface & 30 (T, T) & p. & energetic & scalar/vector & 3 & 640.081,732.40\(\)36 & Sim. \\ OMBB & organic crystal & 30 (T, T) & elec. & scalar/vector & 3 & 640.081,732.40\(\)36 & Sim. \\ DFT3D & inorganic bulk & 30 (T, T) & elec./incach./absorb. & scalar & 1 & 25.500 & 83.20\(\)26.55 & Sim. \\ DFT2D & inorganic bulk & 30 (T, T) & elec./incach./absorb. & scalar & 29 & 55.722 & 9.59\(\)8.04 & Sim. \\ DFT2D & inorganic bulk & 30 (T, T) & e4h & scalar & 1 & 636 & 7.19\(\)4.34 & Sim. \\ EDOS-FOOS & inorganic bulk & 30 (T, T) & elec./incach. & 1/D dist. & 2 & 48.469 & 9.50\(\)8.45 & Sim. \\ mm4M & transition metal complex & 30 (F, F) & elec. & scalar & 8 & 86.665 & 6.99\(\)97.01 & Sim. \\ OMB & organic molecules & 30 (F, F) & elec. & scalar & 12 & \(>\)134.00 & 18.01\(\)22.94 & Sim. \\ Carbon24 & inorganic bulk & 30 (T, T) & N/A & N/A & 1 & 10,153 & 9.21\(\)3.58 & N/A \\ Perox5 & inorganic bulk & 30 (T, T) & N/A & N/A & 1 & 18,928 & 5.00\(\)0.00 & N/A \\   

Table 1: Curated materials datasets in M\({}^{2}\)Hub. Materials type and property type are detailed in Sec. 3.2.1 and Sec. 3.2.2; dim refers to data dimensionality; PBC refers to peroxide boundary condition; method refers to how properties are obtained (sim short for simulation).

**Transition metal complexes** are coordination compounds consisting of central transition metal atom(s) surrounded by ligands. They exhibit unique electronic and magnetic properties and are commonly used in catalysis, medicine, and material science due to their ability to undergo redox reactions and flexible and tunable coordination environment with organic ligands.

**Metal-organic frameworks (MOFs)** are crystalline materials composed of metal ions or clusters coordinated with organic linkers. MOFs poss a high surface area and tunable porious structure, making them useful in applications such as gas storage, separations, and catalysis. They can range in size from microscopic crystals to bulk materials.

#### 3.2.2 Property types

**Electrical properties** refer to the characteristics of a material related to its ability to conduct or resist the flow of electric current. These properties include conductivity, resistivity, and dielectric constant, which determine how well a material can conduct or insulate against electrical charges.

**Mechanical properties** describe how a material behaves under applied forces or loads. These properties include strength, stiffness, ductility, toughness, and elasticity. They determine how the material responds to stress, strain, and deformation, and are essential in understanding its structural integrity and performance.

**Stability** refers to a material's ability to maintain its properties and resist degradation over time. It encompasses chemical stability (resistance to chemical reactions or corrosion), thermal stability (ability to withstand high temperatures), and mechanical stability (ability to resist physical changes or mechanical stress).

**Optical properties** pertain to a material's interaction with light. These properties include absorption, reflection, transmission, and emission of light. Optical properties determine a material's color, transparency, opacity, and light-emitting capabilities, and are crucial in fields such as optics, photonics, and display technologies.

**Thermal properties** describe how a material conducts, stores, and dissipates heat. These properties include thermal conductivity, specific heat capacity, thermal expansion coefficient, and thermal diffusivity. Thermal properties influence a material's ability to transfer heat, its response to temperature changes, and its behavior in thermal management applications.

**Energetic properties** are computational models that use machine learning algorithms to predict the behavior of materials at the atomic or molecular level. They employ large datasets to learn the relationships between atomic arrangements and energies, enabling the simulation and understanding of complex material systems.

**Semiconductor properties** refer to the electrical behavior of materials that exhibit an intermediate conductivity between conductors and insulators. These materials can be controlled to selectively allow or impede the flow of electrons, making them ideal for electronic devices. Semiconductor properties are characterized by parameters such as band gap, carrier mobility, and doping concentration, and are crucial in the design and functionality of transistors, diodes, and integrated circuits.

#### 3.2.3 Datasets

**Materials Project (MP)**Jain et al. (2013) (license: CC-BY-4.0) is a database that curates inorganic materials with computed properties including but not limited to thermal, electrical, mechanical, etc.

**MatBench**Dunn et al. (2020) (MIT license) is a benchmark that provides a standardized framework for evaluating and comparing the performance of different machine learning models on various materials science tasks. It curates data from multiple sources with MP as a main source. However, they do not provide machine learning ready data preparation nor implemented machine learning models and workflow.

**Quantum MOF Database (QMOF)**Rosen et al. (2022) (license: CC-BY-4.0) is a comprehensive database that focuses on metal-organic frameworks (MOFs) with quantum-chemical properties. The MOFs are optimized by DFT derived from both experimental and hypothetical MOF databases.

Organic Materials Database (OMDB) Borysov et al. (2017) (open access but no license specified) is a repository of organic materials. The properties are calculated using DFT for crystal structures contained in the COD database (in Appendix B additional data sources).

**Joint Automated Repository for Various Integrated Simulations (JARVIS)** Choudhary et al. (2020) (license: GNU v3.0) is a database that integrates materials data from various sources, including quantum mechanical calculations, materials simulations, machine learning predictions and high-throughput databases. Our datasets DFT3D, DFT2D and EDOS-PDOS are all from JARVIS database.

**Open Catalyst (OC)** Chanussot et al. (2021) (MIT license) is a database focused on catalytic materials. It includes three tasks: Structure to Energy and Forces (S2EF), Initial Structure to Relaxed Structure (IS2RS) and Relaxed Energy (IS2RE).

**Transition Metal Quantum Materials Database (tmQM)** Balcells and Skjelstad (2020) (license: CC BY-NC 4.0) is a comprehensive database focused on transition metal-based materials. It compiles experimentally derived and computationally predicted data on the structure, composition, and electronic properties of transition metal compounds.

**Quantum Machines 9** (QM9) Ramakrishnan et al. (2014) (open access but no license specified) comprises small organic molecules up to 9 heavy atoms with 12 quantum chemical properties.

**Carbon24** Pickard (2020) (license: CC-BY-4.0) is a synthetic dataset that includes materials made up by carbon atoms but with different structures obtained by _ab initio_ random structure searching.

**Perov5** Castelli et al. (2012, 2012) (license: CC-BY-4.0) is a synthetic dataset that includes perovskite materials with the same structure but different compositions.

### Machine Learning Ready Dataset Preparation

**Raw data format** The raw data format for both molecules and crystals is 3D structures and atomic types. Other features (such as angular information) can be derived from them.

**Machine learning ready data format** As explained in Sec. 3.1, a machine learning ready format for materials includes _atomic types_ denote the atomic number of a given atom and are often converted to one-hot embeddings; _atomic coordinates_ denote the positions of a given atom and often need to be used careful if equivariance needs to be guaranteed; _edge features_ denote information attached to each edge which often include bond types, interatomic distances, etc.

**Graph construction** Three common graphs are constructed to represent the materials: _multi-graph construction_ is a common way to represent materials as graphs which considers edges with repeated atoms (outside the unit cell) as multiple edges with the same atom; _line graph construction_ for materials representation is first proposed in Choudhary and DeCost (2021) which a bond adjacency graph (i.e. line graph) is constructed to capture the bond and angular information.

**Data split** The test scenarios are often out-of-distribution of the training set. While previously common use data split is random split, it is crucial to develop data splits that mimic the real scenarios: _composition split_ (e.g. AxBy vs AzBy) refers to splitting the dataset with same materials compositions but varying ratios; _system split_ (e.g. AB, AC vs ABC) refers to splitting the dataset with unseen materials systems; _time split_ refers to splitting the dataset into training, validation and test set by the date when the materials are published. Note that time split is only available for MP dataset now as the publication information for each material structure is provided in MP.

### Evaluations

#### 3.4.1 Predictive Evaluations

The evaluation metrics for predictive tasks depend on the type of prediction label: (i) **scalar value prediction**: common evaluation metrics include R\({}^{2}\), mean absolute error, and mean squared error; (ii) **classification**: common evaluation metrics are accuracy and Area under the ROC Curve (AUC-ROC) score; (iii) **vector/tensor value prediction**: common evaluation metrics are similar to scalar value prediction, including R\({}^{2}\), mean squared error, and mean absolute error. However, the distance measurement between two vector or tensor values may need to take into account the symmetry, e.g., rotation invariance, such that the distance between the rotated crystal structure and the original structure is zero; (iv) **distribution prediction**: common evaluation metrics include cosine similarity, KL divergence, Wasserstein distance, etc.

#### 3.4.2 Generative Evaluations

Evaluating generative tasks has been a notoriously challenging problem in machine learning. The evaluation metrics can generally be divided into three categories: (i) **reconstruction**: This evaluates the performance of the generative methods in reconstructing the exact material in the training set. (ii) **basic requirement**: This assesses the minimum requirement for the generated materials, such as structure or composition validity. (iii) **distribution**: This measures whether the generative model is capable of learning the data distribution (in terms of structure, property, etc.) in the training set, and whether it can interpolate or generalize to unseen materials.

We include all three types of evaluations metrics developed by Xie et al. (2022): _Materials match_ is a reconstruction metric to check if the generated material reconstructs structure in the test set. Following Xie et al. (2022), this is done using _StructureMatcher_ from _pymatgen_Ong et al. (2013) which considers the match of two materials considering invariances. _Validity_ is a basic metric to check if the generated materials are valid. Following Court et al. (2020), a material structure is valid if the shortest distance between any pair of atoms is greater than 0.5A. _Structure coverage_ is a distribution metric to check if the generated material structures cover the training distribution. We follow Xie et al. (2022) to utilize the CrystalNN fingerprint Zimmermann and Jain (2020) and normalized MagPie fingerprint Ward et al. (2016) to define the structure and composition distance, respectively. _Property statistics_ is a distribution metric to check if the properties of generated materials are close to those in the training dataset.

### Oracle Functions for Generative Materials Design

In our efforts to facilitate the generative design of materials, we have established two categories of oracle functions. Drawing inspiration from oracle functions designed for drug discovery via machine learning Huang et al. (2021), we initially offer a **fingerprint (FP)-based oracle function**. This function utilizes conventional materials descriptors in tandem with classical machine learning algorithms to predict properties of interest. More specifically, we have pre-trained a random forest model for each material property prediction task across 13 different datasets as proposed by Dunn et al. (2020). Consequently, by harnessing the pre-trained models with extracted features based on the Sine Coulomb Matrix and MagPie featurization algorithms, we can predict the properties of an input material. While the predictive accuracy of these classical, FP-based materials descriptors may not rival that of deep learning-based models, we underscore the importance of their inclusion. Their utilization enables generalization where rules apply and mitigates the risk of biasing the optimization process towards deep learning. Our second oracle function is **structure-based oracle function** which aids in selecting an appropriate substrate for a given material (film). By taking into account their respective structures, we have incorporated an oracle function that matches a film with a list of substrates. Specifically, this method analyzes the compatibility between a thin film and various potential substrates, particularly in terms of crystallographic orientation, matching area, potential strain, and elastic energy. This is achieved by loading the structural information of the film and substrates from respective files, then calculating and grouping matches based on substrate Miller indices. Each match, characterized by a minimum match area, is recorded with relevant details such as the substrate formula, orientations of the film and substrate, and, if available, elastic energy and strain. Then the most suitable matches--those with the smallest matching area--for each substrate orientation are identified. This method ultimately returns a list of all matches, providing a comprehensive overview of how well the film could potentially fit on each substrate. Details can be found in Appendix E.

## 4 Benchmarking Machine Learning Models

### Existing approach

A burgeoning amount of machine learning models have been developed for learning molecular representations suitable for a variety of downstream tasks, especially machine learning potential and molecular property prediction Wu et al. (2018); Ramakrishnan et al. (2014); Chmiela et al. (2017).

However, most of existing work focus on molecules without periodicity. Around the same time, another branch of work motivated directly by modeling crystal structures have been developed. We implement and benchmark models from both branches to facilitate the development of new methods in realization of both directions. Specifically, we detail them below and summarize them in Table 2.

**Learning on crystal structures.**_CGCNN_ Xie and Grossman (2018) is a E(3) invariant graph neural network that leverages pairwise distances as edge features. _ALIGNN_ Choudhary and DeCost (2021) is an E(3) invariant graph neural network that builds an extra line graph to explicitly encode the bond angle information in addition to the original atomistic graph similar to CGCNN.

**Learning on molecular structures.**_SchNet_ Schutt et al. (2018) is an E(3) invariant graph neural network that leverages pairwise distances with a continuous filter convolution to construct the message. _EGNN_ Satorras et al. (2021) is an E(3) equivariant graph neural network that leverages relative positions between each pair of nodes and pairwise distances as the message function to update both invariant and equivariant features. _DimeNet++_ Gasteiger et al. (2020) is an E(3) invariant graph neural network that introduces bond angles to improve expressiveness. However, it requires triplet of atom representations to model the bond angle. _GemNet_ Gasteiger et al. (2021) is an SE(3) invariant graph neural network that leverages dihedral angles for better expressiveness. However, it requires learning on quadruplet representations of atoms. _Equiformer_ Liao and Smidt is an SE(3)/E(3) equivariant graph transformer network. Equiformer equips previous transformers with equivariant operations such as tensor product to learn equivariant features built from irreducible representations. _LEFTNet_ Du et al. (2023) is an SE(3)/E(3) equivariant graph neural network based on equivariant local frames. LEFTNet first scalarizes vector and tensor features during message passing and convert them back by tensorizing the scalars through the equivariant frames proposed in ClofNet Du et al. (2022). LEFTNet introduces a local structure encoding and frame transition encoding components to further improve the expressiveness.

### Experiment Set-ups

We build on top of the Open Catalyst Project (OCP) which provides reproducible implementations of commonly used 3D graph neural networks with benchmarks on OC datasets Chanussot et al. (2021). We further implement CGCNN, ALIGNN, EGNN, Equiformer and LEFTNet as they are not included in OCP. We test all the methods on a list of 13 representative tasks from our benchmarks with three data splits (random, composition and system). We mostly use the default hyperparameters provided in the open-source code of each method and reported them in Appendix F. As OC20 and QM9 have been largely adopted in the community, we directly take the results and report in Appendix C. Most of our experiments are conducted on single 16GB V100s while some experiments with memory-intensive models on single 80GB A100s.

### Results and Discussions

Several observations can be gleaned from our benchmark results as shown in Table 3: (i) **performance (observation 1)**: despite the competitive performance of advanced equivariant graph neural networks, invariant models such as DimeNet++ and ALIGNN continue to be among the state-of-the-art methods; (ii) **efficiency**_(observation 2)_: there is a significant variation in efficiency across the benchmarked models. ALIGNN, DimeNet++, GemNet, and Equiformer, as illustrated in Table 4, have particularly slow runtimes. LEFTNet presents a desirable balance of accuracy and efficiency; (iii) **data split**_(observation 3)_: more realistic data splits indeed increase the challenge of the task, particularly the system split. However, this trend does not hold for all properties, with dielectric being an exception;

   Method & Representation & Symmetry & Graph construction & Angular \\  CGCNN Xie and Grossman (2018) & Graph & Perm. + E(3) Inv. & Multi-graph & None \\ ALIGNN Choudhary and DeCost (2021) & Graph & Perm. + E(3) Inv. & Multi-graph + Line graph & Explicit \\ SchNet Schutt et al. (2018) & Graph & Perm. + E(3) Inv. & Multi-graph & None \\ EGNN Satorras et al. (2021) & Graph & Perm. + E(3) Equiv. & Multi-graph & Implicit \\ DimeNet++ Gasteiger et al. (2020) & Graph & Perm. + E(3) Inv. & Multi-graph & Explicit \\ GemNet Gasteiger et al. (2021) & Graph & Perm. + SE(3) Inv. & Multi-graph & Explicit \\ Equiformer Liao and Smidt & Graph & Perm. + E(3)/SE(3) Equiv. & Multi-graph & Implicit \\ LEFTNet Du et al. (2023) & Graph & Perm. + E(3)/SE(3) Equiv. & Multi-graph & Implicit \\   

Table 2: Representative work in modeling molecular and crystal structures.

(iv) **material type** _(observation 4)_: the performance trends across various models remain consistent for a given material property. For instance, for the bandgap property, organic crystals (OMDB) demonstrate the smallest values, followed by metal-organic frameworks (QMOF), while inorganic bulk materials (MP) exhibit the largest values.

## 5 Conclusion, Limitation and Future Outlook

In this paper, we introduce M\({}^{2}\)Hub as a comprehensive platform for machine learning development in materials discovery. M\({}^{2}\)Hub is a toolkit that consists of problem formulation, data downloading, data processing, machine learning methods implementations, machine learning training and evaluation procedures, and benchmark results. We cover not only the commonly considered predictive tasks on materials but also provides tools to enable the study of generative tasks on materials. Specifically, we curate 9 datasets constructed by 6 types of materials with 65 tasks across 8 property types for the predictive task. We further provide 2 synthetic datasets for the purpose of generative tasks on materials. We design 3 extra challenging and realistic data split schemes in addition to previously used random split. We believe M\({}^{2}\)Hub will serve as an essential role in machine learning for materials discovery with datasets, infrastructures and benchmarks.

**Limitation**: Despite we formulate the materials discovery pipeline in the machine learning language supported by datasets, infrastructures and benchmarks, most of the tasks do not involve experiments. However, in reality, experiment is the golden standard to test new materials. It remains a challenge to develop datasets and benchmarks for machine learning models to grow in assisting the experiment phase of materials discovery such as phase demixing and experiment planning (related work is summarized in Appendix A).

**Future work**: There are multiple future directions to extend M\({}^{2}\)Hub is to improve the usability for materials science community (similar to previous work Ward et al. (2018), Jacobs et al. (2020)), e.g. collect pre-trained models Xia et al., Wang et al. (2021, 2021), benchmark machine learning models on specific tasks Kong et al. (2022), Bai et al. (2023), etc.

    & CGCNN & ALIGNN & SchNet & EGNN & DimeNet++ & GemNet & Equiformer & LEFTNet \\  pdos & 68s & 623s & 77s & 87s & 158s & 203s & 713s & 117s \\ e\_form & 8572s & 41343s & 10589s & 12591s & 35622s & 40801s & 62344s & 13797s \\ qmof bandgap & 678s & 2277s & 512s & 1336s & 5240s & 4572s & 27884s & 2405s \\ average ranking & 1.33 & 6 & 1.67 & 3 & 5.67 & 6 & 8 & 4.33 \\   

Table 4: Benchmark the efficiency of machine learning models with materials in different sizes (pdos\(\)10, e\_form\(\)30, qmof\(\)100) on a single V100 GPU (each row with same batch size except when exceeding the maximum memory, running time for 10 epochs).

    & Methods

**Maintenance**: We are committed to maintaining and extending the usability of this toolkit to wider machine learning and materials science community. We plan to maintain this toolkit by adding new datasets, new tasks and evaluations, new oracle functions, and pre-trained models as this community continues to grow to benefit more researchers.

## 6 Acknowledgement

This project is partially supported by the Eric and Wendy Schmidt AI in Science Postdoctoral Fellowship, a Schmidt Futures program; the National Science Foundation (NSF), the Air Force Office of Scientific Research (AFOSR); the Department of Energy; and the Toyota Research Institute (TRI).