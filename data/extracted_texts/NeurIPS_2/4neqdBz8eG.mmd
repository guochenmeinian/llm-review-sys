# Rethinking Weight Decay for Robust Fine-Tuning of Foundation Models

Junjiao Tian

Georgia Institute of Technology

jtian73@gatech.edu

&Chengyue Huang

Georgia Institute of Technology

chuang475@gatech.edu

&Zsolt Kira

Georgia Institute of Technology

zkira@gatech.edu

###### Abstract

Modern optimizers such as AdamW, equipped with momentum and adaptive learning rate, are designed to escape local minima and explore the vast parameter space. This exploration is beneficial for finding good loss basins when training from scratch. It is not necessarily ideal when resuming from a powerful foundation model because it can lead to large deviations from the pre-trained initialization and, consequently, worse robustness and generalization. At the same time, strong regularization on all parameters can lead to under-fitting. We hypothesize that selectively regularizing the parameter space is the key to fitting and retraining the pre-trained knowledge. This paper proposes a new weight decay technique, Selective Projection Decay (SPD), that selectively imposes a strong penalty on certain layers while allowing others to change freely. Intuitively, SPD expands and contracts the parameter search space for layers with consistent and inconsistent loss reduction, respectively. Experimentally, when equipped with SPD, Adam consistently provides better in-distribution generalization and out-of-distribution robustness performance on multiple popular vision and language benchmarks. Code available at [https://github.com/GT-RIPL/Selective-Projection-Decay.git](https://github.com/GT-RIPL/Selective-Projection-Decay.git).

## 1 Introduction

Modern optimizers, such as Adam , LARS , and LAMB  usually include momentum and adaptive learning rates. They help optimizers avoid local minima and accelerate learning  to explore wider parameter spaces. However, we hypothesize that this behavior is not always beneficial for fine-tuning from a _well_ pre-trained foundation model, especially when fine-tuning a few layers is already sufficient for fitting the target data . Several prior works have found that unnecessary exploration will lead to large deviation from the initialization and worse robustness , and constraining the deviation can improve a model's generalization on in-distribution (ID) data and robustness to out-of-distribution (OOD) data 1. For example, L2-SP  imposes a regularization term on the distance between the current and pre-trained models. More recently, TPGM  and FTP  propose to learn different hard constraints for each layer. These new works have demonstrated impressive results on benchmarks. However, they are either difficult to tune, specialized to specific settings, or require significant computation and storage overhead. _This motivates us to ask whether a simple few-liner solution exists for this fundamental problem._We propose re-examining the existing methods and summarizing their findings to find this solution. Starting from the simplest: L2-SP . Specifically, L2-SP adds an L2 regularization term to the original objective function. Formally,

\[()=}()+\|- _{0}\|_{2}^{2} \]

where \(\) denotes the model parameters, \(_{0}\) the initialization, \(}()\) the original objective function, and \(\) the hyper-parameter for regularization strength. When \(_{0}=\), L2-SP reduces to an ordinary weight decay. This simple method should be effective enough to constrain the model, as our experiments show it can reduce the deviation between the fine-tuned and pre-trained models (Sec. 4.1). However, it is held back by an important design choice: the penalty is always applied to all model parameters. Our empirical results identify that a large \(\) prevents every layer from deviating too much and leads to poor fitting, while a small \(\) cannot provide enough regularization. This significantly limits the otherwise effective design (Sec. 4.1). So, _what is missing in this algorithm?_

Recent works in robust fine-tuning and parameter-efficient fine-tuning (PEFT) have shown that customizing constraints for each layer and _selectively_ choosing layers for fine-tunning can improve robustness [10; 11; 6]. Inspired by these findings, we hypothesize that selectively imposing the regularization to different layers is the key. Therefore, we propose a simple _selective_ version of L2-SP weight decay: selective projection decay (SPD). This new algorithm innovates in two aspects: a **selection condition** and a **regularization strength ratio**. The former determines when to apply regularization to a layer, and the latter determines the strength of regularization for intuitive hyperparameter tuning. Specifically, we derive the selection condition from hyper-optimization [15; 16; 17] by treating the condition as an optimizable parameter (Sec. 3.3), and the regularization strength ratio by re-writing L2-SP as a projection operation (Sec. 3.4). Intuitively, when the condition is met, the algorithm imposes _large_ regularizations on selected layers. This allows the algorithm to avoid unnecessary deviation and simultaneously fit into the fine-tuning data. We test SPD on large-scale computer vision, and NLP benchmarks with popular foundation models and test ID and OOD performance on various distribution and domain shifts. SPD achieves SOTA performance while being much simpler than other competing methods. Our contributions are:

* We propose a selective projection decay, a selective variant of the popular L2-SP/weight decay regularization methods, for robust fine-tuning of large foundation models. We show that selectivity is important to make regularization effective.
* We conduct a detailed study of ID and OOD performance on image classification and semantic segmentation with natural distribution and domain shifts. SPD improves ID and OOD performance on these benchmarks.
* We show that SPD consistently improves the performance of PEFT methods (e.g. LoRA  and adapters ) on 8 common sense reasoning language tasks with LLaMA-7B (-13B).

## 2 Related Works

**Robust Fine-Tuning with Distance Regularization.** Constraining the distance or deviation between the fine-tuned and pre-trained models has been studied in several prior works. L2-SP  explicitly adds an L2 norm penalty on the deviation and shows improved ID generalization for fine-tuning. MARS-SP  studies different forms of norms as the penalty. It shows that the Matrix Row Sum (MARS) norm can be a superior alternative to the L2 norm. These two methods impose "soft" penalties and can be less effective . Instead, LCC  proposes constraining the deviation through direct projection on the parameters, which also enforces a hard constraint on the Lipschitz continuity of the fine-tuned model. However, LCC is hard to tune because the projection radius is not an intuitive hyper-parameter. Furthermore, using a single projection constraint for all layers is not an ideal strategy . More recently, TPGM  proposes to automatically learn the constraints in LCC during fine-tuning, customizing a different projection radius for each layer through a bi-level optimization scheme. FTP  further improves the computation efficiency of TPGM by adopting hyper-optimization [15; 16; 17] in its computation. Nevertheless, FTP is still difficult to control because hyper-optimization requires a secondary optimizer with additional optimization hyper-parameters, and the learned regularization can be too strong with no intuitive way to adjust. In contrast, SPD is a much simpler and more intuitive method, which can be implemented with just a few lines of code. The superior controllability makes SPD potentially applicable to more applications.

**Parameter Efficient Fine-Tuning (PEFT).** PEFT methods such as adapters [9; 8] and LoRA  have been proposed to reduce training memory usage and computation complexity. Recent works have found that PEFT methods also provide good robustness because they modify fewer parameters and retain more knowledge of the pre-trained models . Surgical fine-tuning  concludes that fine-tuning a selective few layers can improve ID generalization. These new works motivate us to re-evaluate L2-SP and weight decay, often uniformly applied to all layers. We identify that the inferior performance of the simple methods is because of this uniformity, which exhibits a strong trade-off between fitting and regularization. Other robust fine-tuning methods, such as LP-FT  and FLYP , focus on feature distortion. We will review them in the Appendix 8.1.

**Other Robust Fine-Tuning Methods.** WiSE-FT  discovers that linearly interpolating between the fine-tuned and pre-trained models after fine-tuning can improve out-of-distribution robustness. This demonstrates that a closer distance to the pre-trained model can improve robustness. However, it only applies to models with zero-shot capabilities. Another orthogonal line of research for robust fine-tuning focuses on feature distortion. LP-FT  shows that fine-tuning with a randomly initialized head layer distorts learned features. It proposes a simple two-stage method to train the head layer first and then fine-tune the entire model. FLYP  shows that fine-tuning a foundation model using the same objective as pre-training can better preserve the learned features. Our contribution is an optimization method to penalize the derivation between the fine-tuned and pre-trained models explicitly during fine-tuning, which is orthogonal to them.

## 3 Methods

In this section, we first provide an overview of the Selective Projection Decay (SPD) method and then describe the intuition behind SPD with a numerical example. Finally, we provide a concrete mathematical motivation for our method's algorithmic design.

### Selective Projection Decay (SPD)

**Formulation.** SPD is a regularization technique that penalizes significant deviation from the pre-trained model. We motivate the formulation from an existing method: L2-SP  (Eq. 1). L2-SP adds a distance penalty on the deviation between the fine-tuned and pre-trained models. The penalty is applied to all model parameters at all times. A large \(\) prevents every layer from deviating too much and empirically leads to poor fitting, while a small \(\) cannot provide enough regularization. This significantly limits the otherwise effective design. We propose a _selective_ version of this simple technique: selective projection decay (SPD). We will examine L2-SP and SPD in Alg. 1 and Alg. 2.

**Notations.** We follow the notations in prior works [1; 21]. Let \(m_{t},v_{t}\) denote the moving average of the gradient and squared gradient, \(_{1},_{2}\) their hyper-parameters, and \(\) the learning rate.

Alg. 1 shows the Adam optimizer with the L2-SP regularization in Eq. 1. The effects of the regularization are highlighted in blue, also shown in Eq. 2. Intuitively, the regularization leads to an interpolation-like equation2. If the product \(=1\), then \(_{t}_{0}\) and if \(=0\), then \(_{t}_{t}\), where \(_{0}\) and \(_{t}\) denote the initialization and the updated model _without_ regularization.

\[_{t}_{t}-(_{t}- _{0}) \]

Alg. 2 shows the proposed SPD. There are two changes compared to Alg. 1.

* a condition, \[c_{t}=-g_{t}^{}(_{t-1}-_{0}).\] (3)
* a new interpolation-like equation with a multiplier, \(r_{t}\), replacing the learning rate \(\), \[_{t}_{t}- r_{t}(_{t}- _{0}).\] (4)Compared to L2-SP, SPD only imposes a penalty when the _condition_ is met (\(c_{t}<0\)), and the strength of the penalty is controlled by a hyper-parameter \(\) and an analytical quantity _deviation ratio_\(r_{t}\), which we will introduce later.

```
Initialize\(m_{0} 0\), \(v_{0} 0\), \(t 0\) While\(_{t}\) not converged \(t t+1\) \(g_{t}_{}}(_{t-1})\) \(m_{t}_{1}m_{t-1}+(1-_{1})g_{t}\) \(v_{t}_{2}v_{t-1}+(1-_{2})g_{t}^{2}\) Bias Correction \(}}{1-_{1}^{}}\), \(}}{1-_{2}^{}}\) Update \(_{t}_{t-1}-}}{}+}\) \(_{t}_{t}-(_{t}- _{0})\)
```

**Algorithm 1**Adam with L2-Regularization

### Intuition Behind SPD

**SPD prioritizes layers with consistent improvement.** SPD adds regularization on layers that meet the condition \(c_{t}<0\) to slow their growth. The condition is determined by the sign of the inner product between two vectors. One vector is the negative gradient direction \((-g_{t})\), i.e., the descent direction, and the other is the current progress direction \((_{t-1}-_{0})\). The inner product between them measures the _alignment_ between the _vanilla3_ update direction and the progress so far. When the inner product is positive, the current progress direction generally points to a low loss region, and following it will lead to consistent loss reduction. Conversely, if the inner product is negative, the current progress direction will likely lead to a higher loss region, indicating inconsistent improvement. In this case, SPD will impose a penalty to slow down updates for those layers. Recall that modern optimizers use momentum to escape local minima and explore wider regions. Without this penalty, the model will likely head towards the higher loss region to overcome it. SPD chooses to slow down these layers and prioritizes layers with more consistent loss reduction. We will motivate this strategy in a principled manner and validate it in our experiments.

```
Initialize\(m_{0} 0\), \(v_{0} 0\), \(t 0\),\(c_{0} 0\) While\(_{t}\) not converged \(t t+1\) \(g_{t}_{}}(_{t-1})\) \(m_{t}_{1}m_{t-1}+(1-_{1})g_{t}\) \(v_{t}_{2}v_{t-1}+(1-_{2})g_{t}^{2}\) Bias Correction \(}}{1-_{1}^{}}\), \(}}{1-_{2}^{}}\) Update \(_{t}_{t-1}-}}{}+}\) \(c_{t}=-g_{t}^{}(_{t-1}-_{0})\) If\(c_{t}<0\) : \(_{t}_{t}- r_{t}(_{t}-_{0})\)
```

**Algorithm 2**Adam with Selective L2-Reg.

### Deriving \(c_{t}\) from Hyper-Optimization

Previously, we explained the intuition behind SPD. Specifically, we interpreted the condition \(c_{t}\) as a measure of alignment and a test of update consistency. Nevertheless, there is a more profound reason why the quantity \(c_{t}\) is a natural choice for selective regularization. In this section, we motivate SPD from a more mathematical perspective.

**Hyper-Optimization Setup.** Hyper-optimization is a technique to optimize hyper-parameters inside an optimizer . They treat the hyper-parameters as trainable parameters and optimize them using another gradient-based optimizer. Let's start from the vanilla Adam with L2-SP algorithm (Alg. 1) and treat the regularization strength hyper-parameter \(\) as a trainable parameter. To update \(\), we need to obtain its gradient by taking a derivative w.r.t. \(\) after applying it.

\[:=(_{t})}{}=(_{t})}{_{t}}^{}}{}=*-g_{t+1}^{}(_{t}-_{0}). \]

**Selection Condition \(c_{t}\).** Intuitively, if the quantity \(\) is negative, applying the update in gradient descent will increase the value of \(\), thus increasing the regularization strength of L2-SP. Conversely, a positive quantity will decrease the regularization strength. Therefore, the \((-g_{t+1}^{}(_{t}-_{0}))\) determines the change of regularization strength in the hyper-optimization of \(\). Formally, we define the condition \(c_{t}\) as,

\[c_{t}:=-g_{t+1}^{}(_{t}-_{0}) \]For memory efficiency, we use \((_{t}-_{0})\) instead of \((_{t}-_{0})\) because both vectors point in the same direction and won't affect the sign of \(c_{t}\). This allows us to discard \(_{t}\). Otherwise, we need to keep an additional copy in memory. In summary, when \(c_{t}<0\), we apply a regularization for that layer as shown in Alg. 2. This calculation is done for each layer, and the regularization is selectively applied.

**Alternative Interpretation:** We just interpreted the selection condition \(c_{t}\) in SPD as a measure of consistency between the current heading direction and the gradient direction. This perspective is more valid when the algorithm has accumulated some updates, i.e., \(\|_{t}-_{0}\|_{2} 0\), and less justified when a heading has not been established at the beginning of training. To analyze this, we discuss the behavior SPD from the perspective of _stochastic_ optimization when \(\|_{t}-_{0}\|_{2}\) is small at the beginning of training in the Appendix 8.1.

### Deriving \(r_{t}\) from Projection

The selection condition \(c_{t}\) determines when to apply regularization to which layers. However, one remaining question is the strength of regularization, which is not intuitive to tune. To overcome this, we introduced an analytical quantity, the deviation ratio \(r_{t}\), in Eq. 2 and Alg. 1. In this section, we will motivate it from the perspective of projection.

**L2-SP is projection.** Projection onto a norm ball is common in constrained optimization. While L2-SP is not a constrained optimization problem, its operation bears similarity to projection. Suppose we project a model \(_{t}\) to an \(_{2}\)-norm ball with radius \(\) centered around its initialization \(_{0}\). The equation of projection is the following,

\[_{p}=_{0}+_{t}-_{ 0}\|_{2}\}}*(_{t}-_{0}). \]

Equivalently, we can rewrite the equation as,

\[_{p}=_{t}-(1-_{t}-_{0}\|_{2}\}})*(_{t}-_{0}). \]

Now, we can equate this equation to the highlighted L2-SP equation in Eq. 2 and Alg. 1, we can see that if \(=(1-_{t}-_{ 0}\|_{2}\}})\), the regularization is equivalent to projection with radius \(\).

**Deviation Ratio \(r_{t}\).** This equivalence inspires us to define a deviation ratio \(r_{t}\):

\[r_{t}=-_{t-1}\}}{_{t}} \]

where \(_{t}:=\|_{t}-_{0}\|_{2}\) and \(_{t}:=\|_{t-1}-_{0}\|_{2}\) denote the current deviation (before regularization) and the previous deviation from the initialization \(_{0}\), respectively. We use \(r_{t}\) in SPD (Alg. 2) to replace the learning rate \(\) in L2-SP (Alg. 1) to make hyper-parameter (\(\)) tuning more intuitive. Specifically, suppose the hyper-parameter \(=1\), then the regularization in SPD is:

\[_{t}_{t}--_{t-1} \}}{_{t}}(_{t}-_{0})=_{0}+}{ \{_{t-1},_{t}\}}*(_{t}-_{0}). \]

Intuitively, with \(=1\), the regularization in SPD is equivalent to projection with a radius equal to the previous deviation if the current deviation is larger. In summary:

* **No regularization** (\(=0\)): the projection radius is \(\|_{t}-_{0}\|_{2}\), meaning no projection.
* **Weak regularization** (\(1>0\)): the projection radius lies between \(\|_{t}-_{0}\|_{2}\) and \(\|_{t-1}-_{0}\|_{2}\). Within this range, all layers will expand or remain unchanged.
* **Strong regularization** (\(>1\)): the projection radius lies between \(0\) and \(\|_{t-1}-_{0}\|_{2}\). In this range, it's possible that regularized layers can contract.

We recommend starting with \(=1\) and adjusting the strength according to the specific needs.

### Compatibility with PEFT methods.

As shown in Alg. 2, SPD retains a copy of the pre-trained model in memory. This adds additional memory requirements to the overhead of vanilla optimizers. While this is practical for moderate-sized models, as fine-tuning focuses more and more on large models, additional memory requirements become undesirable. Fortunately, in extremely large models, the prevalent fine-tuning strategy is parameter-efficient fine-tuning (PEFT), such as LoRA , series adapters , and parallel adapters . SPD is naturally compatible with these methods without the additional memory. Intuitively, SPD selectively projects the current model towards the pre-trained initialization. PEFT methods generally initialize new parameters to add to the original model weights. To recover the behavior of SPD, we can instead project the new parameters towards the _origin_, equivalent to a selective version of regular weight decay, i.e., replacing \(_{0}\) with \(\) in Alg. 2. Consequently, this does not require a memory copy of the pre-trained model. It consistently improves PEFT fine-tuning for large language models on common sense reasoning benchmarks in Sec. 4.4.

For example, LoRA decomposes a linear layer \(h=W_{t}x\) into two components, where \(h^{m 1}\), \(W_{t}^{m n}\) and \(x^{n 1}\) are the output, weights, and input of this layer.

\[h=W_{t}x=(W_{0}+ W_{t})x W_{0}x+W_{up}W_{down}x \]

where \(W_{0}^{m n}\), \(W_{up}^{m r}\) and \(W_{down}^{r n}\) are the pre-trained model, up-projection and down-projection matrices. If \(r\{m,n\}\), (\(W_{up}W_{down}\)) is a low-rank approximation of \( W_{t}\). To regularize the overall deviation \(\|W_{t}-W_{0}\|_{2}\), it suffices to regularize \(\|W_{up}W_{down}\|_{2}\) to be close to zero. In this case, SPD acts as selective weight decay on \(W_{up}\) and \(W_{down}\) individually.

In summary, we propose selective projection decay (SPD) to impose strong regularization on layers during fine-tuning selectively. As shown in Fig. 1, SPD regularizes the deviation of the fine-tuned model from the pre-trained model \(\|W_{t}-W_{0}\|_{2}\) for full fine-tuning and the deviation from the origin \(\| W_{t}\|_{2}\) for PEFT fine-tuning.

## 4 Experiments

We test Selective Projection Decay on a diverse set of benchmarks, architectures, and tasks to demonstrate its effectiveness. We will test both ID generalization and OOD robustness across various domain and distribution shifts.

**Image Classification.** We first analyze the behavior of SPD on conventional image classification datasets DomainNet  and ImageNet . We use a CLIP ViT-Base model for both experiments as the pre-trained initialization . Specifically, DomainNet consists of images from several domains with 345 classes. We fine-tune on one domain and test on all domains. ImageNet is a large-scale dataset with 1000 classes. We fine-tune on ImageNet and test on ImageNet and four variants, namely ImageNet-V2 , ImageNet-A , ImageNet-R , and ImageNet-S .

**Semantic Segmentation.** We further test SPD on the PASCAL-Context semantic segmentation dataset . Following prior works [30; 11], we use a Swin ViT-Tiny , pre-trained on ImageNet-22K, and Segformer  segmentation architecture. To construct the OOD datasets, we follow the popular natural robustness literature  and apply four representative image corruptions (fog, defocus blur, Gaussian noise, and brightness) with 5 severity each. We fine-tune on the clean segmentation data and test on clean and corrupted data.

Figure 1: **Selective Projection Decay (SPD) imposes regularization on layers selectively during fine-tuning. It regularizes \(\|W_{t}-W_{0}\|_{2}\) for full fine-tuning and \(\| W_{t}\|_{2}\) for PEFT fine-tuning.**

[MISSING_PAGE_FAIL:7]

and significantly improves OOD performance while matching the best ID performance. Under SPD, the correlation coefficient between OOD performance and deviation is \(-0.96\), which indicates a strong negative correlation between the two quantities, i.e., smaller deviation and higher OOD accuracy. This experiment shows that selective regularization is superior to uniform regularization.

**Training Details.** We use the vision transformer public repository for DEIT  to fine-tune all methods. We use \(=1\) for all Adam-SPD results in Tab. 1. More details are in Appendix 8.4.

### ImageNet Experiments

**SPD outperforms more complicated works on image classification.** Following the training recipe from the prior work , we fine-tune a CLIP ViT-Base model on ImageNet using Adam-SPD. We use the same hyper-parameters as the prior work and only adjust the regularization hyperparameter in SPD. In Tab. 3, we observe that Adam-SPD provides the best ID performance (strong ID generalization) and best average OOD performance (strong OOD robustness) on four ImageNet variants. SPD achieves a level of competitive performance with just a few lines of code. SPD's simplicity and strong performance show that selective regularization is a fundamental improvement for robust fine-tuning.

**Training Details.** For Adam-SPD, we fine-tune the model with a learning rate of \(3e-5\) and \(=1.4\). The regularization hyper-parameter is found through cross-validation, and the model with the best ID validation accuracy is taken. More details are in Appendix 8.4.

   Hyper-Parameter \(\) & 1e-1 & 1e-2 & 6e-3 & 3e-3 & 1e-3 & 6e-4 & 3e-4 & 1e-4 & 1e-5 & 1e-6 & 1e-7 & 0.0 \\  Deviation & 0.03 & 0.14 & 0.18 & 0.24 & 0.34 & 0.39 & 0.46 & 0.53 & 0.58 & 0.58 & 0.58 & 0.59 \\  OOD & 14.90 & 37.20 & 39.43 & 40.52 & 41.13 & 41.76 & 40.52 & 41.26 & 41.35 & 41.73 & 40.62 & 41.34 \\  ID & 27.25 & 69.74 & 73.

### PASACAL Dense Semantic Segmentation

**SPD outperforms more complicated works on semantic segmentation.** The same trend is observed on semantic segmentation in Tab. 4. Again, SPD achieves the best ID generalization and OOD robustness across four different corruptions. This shows that proper regularization is not only important for achieving strong ID generalization (performance on the test set) but also for strong OOD robustness (performance on distribution shifted test sets) to domains shift (Tab. 3) and distribution shift such as natural corruptions (Tab. 4). The model fine-tuned with SPD is consistently more robust across different levels of corruption and severity.

**Training Details.** For Adam-SPD, we fine-tune the model with a learning rate of \(1e-4\) and \(=2.2\). The regularization hyper-parameter is found through cross-validation, and the model with the best ID validation accuracy is taken. More details are in Appendix 8.4.

### LLaMA PEFT Fine-Tuning Experiments

**SPD is compatible and consistently improves PEFT methods.** Previous experiments have shown that SPD imposes effective regularization for full fine-tuning. Furthermore, SPD can also improve the performance of PEFT methods. We fine-tune LLaMa-7B (-13B) models on the Commonsense-170k dataset . As shown in Tab. 5, SPD consistently improves regular fine-tuning with AdamW, which uses a uniform weight decay for all tested PEFT methods. This demonstrates that selective regularization benefits full fine-tuning and PEFT fine-tuning. Combined with its simplicity, SPD can potentially improve generalization and robustness for more tasks in deep learning.

**Training Details.** We follow the training code released by a prior work . We report the best performance from the original paper and compare them with Adam-SPD. More details are in Appendix 8.4.

    & ID &  &  \\  & Clean & Fog & Defocus & Gaussian & Brightness & OOD Avg. & ID \(\) (\%) & OOD \(\) (\%) \\  Vanilla FT & 66.03 & 56.72 & 38.04 & 23.21 & 58.03 & 44.00 & 0.00 & 0.00 \\ Adapter  & 71.85 & 69.36 & 50.94 & 37.43 & 68.26 & 56.50 & 8.82 & 28.40 \\ Birit  & 70.31 & 67.00 & 46.39 & 30.61 & 66.22 & 52.56 & 6.49 & 19.44 \\ L2-SP  & 73.47 & 69.87 & 49.20 & 39.10 & 68.61 & 56.70 & 11.27 & 28.85 \\ MARS-SP  & 66.24 & 56.97 & 37.29 & 21.82 & 58.27 & 43.59 & 0.32 & -0.94 \\ LLRD  & 72.09 & 68.13 & 46.18 & 37.28 & 66.30 & 54.47 & 9.18 & 23.79 \\ TPGM  & 72.56 & 69.51 & 50.88 & 38.62 & 68.82 & 56.96 & 9.89 & 29.44 \\ FTP  & 73.79 & 71.10 & 52.63 & 40.25 & 69.81 & 58.45 & 11.76 & 32.83 \\  Adam-SPD & **74.27** & **71.74** & **53.41** & **44.17** & **70.92** & **60.06** & **12.47** & **36.50** \\   

Table 4: Pascal Semantic Segmentation Results with SWIN-Tiny transformers (ImageNet21K pretrained). Performance is measured by mIoU\(\). SPD improves OOD robustness compared to vanilla fine-tuning without regularization and L2-SP by 36.5\(\%\) and \(5.8\%\), respectively.

   PEFT & LLM & Optimizer & BoolQ & PIQA & S1QA & HellaSwag & WinoGrande & ARC-e & ARC-e & OBOA & Avg. \\   & LLaMA\({}_{T2B}\) & AdamW & 63.0 & 79.2 & 76.3 & 67.9 & 75.7 & 74.5 & 57.1 & 72.4 & 70.8 \\  & Adam-SPD (1.0) & **68.3** & **80.4** & **77.4** & **81.6** & **79.7** & **79.4** & **63.5** & **78.4** & **76.1** \\   & LLaMA\({}_{T2B}\) & AdamW & 67.9 & 76.4 & **78.8** & 69.8 & 78.9 & 73.7 & 57.3 & 75.2 & 72.3 \\  & Adam-SPD (1.0) & **68.8** & **80.9** & 78.3 & **82.0** & **80.8** & **80.0** & **63.1** & **78.0** & **76.5** \\   & LLaMA\({}_{T2B}\) & AdamW & 68.9 & 80.7 & 77.4 & 78.1 & 78.8 & 77.8 & 61.3 & 74.8 & **74.7** \\  & Adam-SPD (0.7) & **69.1** & **82.8** & **78.9** & **84.8** & **80.7** & **80.9** & **65.8** & **79.2** & **77.8** \\   & LLaMA\({}_{13B}\) & AdamW & 72.1 & 83.5 & 80.5 & 80.5 & **83.7** & 82.8 & 68.3 & 82.4 & 80.5 \\  & Adam-SPD (1.2) & **72.9** & **85.6** & **80.7** & **92.0** & **83.7** & **85.6** & **71.6** & **85.6** & **82.2** \\   

Table 5: Accuracy comparison of LLaMA-7B (-13B) with different adapters and optimizers on eight commonsense reasoning datasets. SPD consistently improves fine-tuning performance on multiple PEFT methods across all datasets. Note that AdamW employs uniform weight decay by default.

### Visual Question Answering (VQA) Experiments

**SPD shows competitiveness across ID, near OOD, and far OOD datasets on multimodal tasks.** Apart from uni-modal tasks, SPD outperforms other baselines on multi-modal tasks. We fine-tune PaliGemma-3B model on VQAv2  dataset with LoRA. In Tab. 6, SPD improves vanilla fine-tuning and other robust fine-tuning methods, achieving best ID and average OOD performance w.r.t. distribution shifts across single modalities such as vision, question, answer and combinations of multiple modalities. We also show the performance evaluation for both near and far OOD datasets. SPD is consistently more robust under different types and degrees of distribution shifts.

**Training Details.** For Adam-SPD, we fine-tune the model with a learning rate of \(1e-3\) and \(=0.5\). The regularization hyper-parameter is found through cross-validation, and the model with the best ID validation accuracy is taken. More details are in Appendix 8.4.

## 5 Limitations

SPD is a selective regularization technique explicitly designed for fine-tuning. While it can be theoretically used for pre-training, it will likely lead to poor performance because it will hinder the training of some layers. For fine-tuning, it works well because the pre-trained foundation model is _assumed_ to be a good initialization, and only small changes in a selected few layers can lead to a good local minimum. Furthermore, the level of performance gain depends on how well the foundation models are exposed to the fine-tuning and OOD data distributions during pre-training. For example, in the DomainNet experiment (Tab. 1), fine-tuning a CLIP ViT model on any other domain does not have reasonably good OOD robustness on the Quickdraw domain. One can deduce that Quickdraw is not well represented in the pre-training data of CLIP ViT.

## 6 Conclusion

Fine-tuning differs from training from scratch because it starts from a good initialization. Therefore, effective regularization is critical to retaining the knowledge of the pre-trained foundation model while fitting a model to the target distribution. We identified that 1) regularization is necessary to keep the fine-tuned model close to its initialization and maintain robustness; 2) uniform regularization can hurt model fitting if regularization is too strong. In this paper, we proposed selective projection decay (SPD), a selective version of the popular weight decay/L2-SP regularization method. With an additional few lines of code, SPD can be integrated into existing optimizers and performs selective regularization. It demonstrates superior regularization performance on different tasks and modalities in our experiments.

    & ID &  &  \\   &  &  &  &  &  & \\  & VQAv2 & FV-VQA & CV-VQA & VQA-Rephrasing & VQA-CP v2 & VQA-CE & A/VQA & Text/VQA & Vir-Viz & OK-VQA \\  Zero-Shot & 54.42 & 63.95 & 44.72 & 50.10 & 54.29 & 30.68 & 30.46 & 14.86 & 16.84 & 28.60 \\ Vanilla FT(LoRA) & 86.29 & 94.43 & **69.36** & 78.90 & 86.21 & 71.73 & 49.82 & 42.08 & 22.92 & 48.30 \\ Linear Prob. & 78.24 & 87.83 & 63.87 & 69.61 & 78.48 & 61.66 & 42.90 & 29.61 & 18.80 & 42.27 \\ L2P-FL(LoRA) & 85.97 & 93.30 & 65.93 & 76.49 & 86.16 & 72.73 & 45.68 & 31.41 & 19.01 & 43.27 \\ WiSE-FT(LoRA) & 71.36 & 85.06 & 64.55 & 66.42 & 70.89 & 48.74 & 43.95 & 36.98 & 22.41 & 42.35 \\ Adam-SPD(LoRA) & **87.39** & **95.25** & 68.85 & **79.48** & **87.27** & **73.52** & **50.90** & **43.56** & **23.05** & **50.11** \\   

Table 6: Visual Question Answering Result using PaliGemma-3B. SPD outperforms baselines across ID, near OOD and far OOD datasets using LoRA. Note that L2-SP reduces to Vinilla FT with AdamW under LoRA.

Acknowledgement

This work was supported by ONR grant N00014-18-1-2829.