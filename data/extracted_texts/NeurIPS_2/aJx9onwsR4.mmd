# Predicting the Performance of Foundation Models via Agreement-on-the-Line

Rahul Saxena\({}^{*}{}^{1}\)  Taeyoun Kim\({}^{*}{}^{1}\)  Aman Mehra\({}^{*}{}^{1}\)  Christina Baek\({}^{1}\)

Zico Kolter\({}^{1,2}\)  Aditi Raghunathan\({}^{1}\)

Carnegie Mellon University\({}^{1}\), Bosch Center for AI\({}^{2}\)

{rsaxena2, taeyoun3, amanmehr, kbaek, zkolter, radii}@cs.cmu.edu

###### Abstract

Estimating the out-of-distribution performance in regimes where labels are scarce is critical to safely deploy foundation models. Recently, it was shown that ensembles of neural networks observe the phenomena "agreement-on-the-line", which can be leveraged to reliably predict OOD performance without labels. However, in contrast to classical neural networks that are trained on in-distribution data from scratch for numerous epochs, foundation models undergo minimal finetuning from heavily pretrained weights, which may reduce the ensemble diversity needed to observe agreement-on-the-line. In our work, we demonstrate that when lightly finetuning multiple runs from a _single_ foundation model, the choice of randomness during training (linear head initialization, data ordering, and data subsetting) can lead to drastically different levels of agreement-on-the-line in the resulting ensemble. Surprisingly, only random head initialization is able to reliably induce agreement-on-the-line in finetuned foundation models across vision and language benchmarks. Second, we demonstrate that ensembles of _multiple_ foundation models pretrained on different datasets but finetuned on the same task can also show agreement-on-the-line. In total, by careful construction of a diverse ensemble, we can utilize agreement-on-the-line-based methods to predict the OOD performance of foundation models with high precision.

## 1 Introduction

Foundation models (FM), or large models first pretrained on open world data then finetuned or prompted for a specific downstream task, have proven to be powerful solutions for many common machine learning problems. A notable trait about FMs is that they are far more robust to distribution shift than other deep learning approaches -- across image and language benchmarks, they suffer a smaller performance degradation on out-of-distribution (OOD) data, that may vary substantially from the in-distribution (ID) finetuning data . From clinical decision-making in different hospitals to navigating robots through unseen terrains, FMs are increasingly utilized for tasks prone to distribution shift. However, evaluating these models in OOD settings remains difficult: in many cases, acquiring labels for OOD data is costly and inefficient, while unlabled OOD data is much easier to collect. Although the field has explored other means for estimating OOD accuracy without labeled data, they are not ideal for FMs. A reliable FM performance estimator has the following desirable properties. First, the method must be computationally efficient to account for FMs' large model size. Second, FMs are leveraged for many different tasks (e.g., classification, question-answering, regression), so the method should also be versatile across tasks. Third, as we will see, methods for finetuned FMs may require _different model assumptions from neural networks trained from scratch_.

Recently,  proposed a promising method for estimating the OOD accuracy of deep networks using the _agreement_ between pairs of these classifiers (i.e., how often two classifiers make the same prediction). For distribution shifts where models observe a strong linear correlation in ID versus OOD accuracy - a common phenomenon in vision and language benchmarks  - a strong linear correlation also holds for ID versus OOD agreement with extremely similar slopes and intercepts. These effects are referred to as accuracy-on-the-line (ACL) and agreement-on-the-line (AGL) respectively, and together they provide a simple method for estimating OOD accuracy via unlabeled data alone. Namely, without any OOD labels, we can instead measure the linear fit of ID versus OOD agreement as a proxy for the linear fit of ID versus OOD accuracy. With this linear fit, we can verify whether ACL holds by using the correlation strength of agreement's linear trend and estimating each model's OOD accuracy by linearly transforming ID accuracy. This simple approach has shown to reliably predict the OOD accuracy of models within a few percentage points across classification and question-answering tasks.

Figure 1: The ID vs OOD lines for accuracy (orange) and agreement (blue) for various datasets and fine-tuned ensembles. Each blue dot corresponds to a member of the ensemble and represents the ID (x) and OOD (y) accuracy. Each orange dot corresponds to a pair of these members and represents the ID (x) and OOD (y) agreement. From CIFAR10 to CIFAR10C “Pixelate” in linear probed CLIP, MNLI to SNLI in full fine-tuned OPT, and SQuAD to SQuAD-Shifts “Amazon” in full fine-tuned GPT2, we observe that randomly initializing the head as the diversity source for generating ensembles (columns) shows the closest agreement linear fit to accuracy.

Unfortunately, while the method has several practical advantages, it is unclear whether finetuned FMs also observe the necessary AGL phenomena. Intuitively, a prerequisite to observing AGL is a _diverse ensemble_ of classifiers. Since OOD accuracy falls below ID accuracy, if the linear trend in ID versus OOD agreement is to match that of accuracy, models must also _agree much less OOD than ID_. For this to happen, errors between any two models must be sufficiently decorrelated.  observes AGL in ensembles of neural networks trained for hundreds of epochs from scratch where it is conceivable that the stochasticity between training runs leads to large divergences in the weight space, and corresponding models have diverse OOD predictions. However, in the case of finetuned FMs, models are much closer in the weight space. FMs are often either linear probed over the same pretrained weights or full finetuned for a few epochs with a small learning rate and intuitively, such _light_ finetuning may lead to models that "revert back" to their pretrained behavior to make highly correlated predictions OOD.

This raises the question: can we enforce AGL in this paradigm of lightly finetuning heavily pretrained models? In this work, we conduct an extensive study across several modalities, e.g., CLIP-based image classification and LLM-based question-answering, and training regiments, e.g., full finetuning and linear probing, to understand when AGL holds for finetuned FMs. We first investigate whether AGL appears in an ensemble of finetuned models from a _single_ base FM. To collect a deep ensemble, the following sources of diversity can be injected into the finetuning process: 1) random initialization of the linear head; 2) random data ordering; and 3) random data subsetting. We find that not every source of diversity during fine-tuning on ID data manifests in sufficient diversity OOD, breaking the matching linear fits in ID versus OOD accuracy and agreement. Interestingly, finetuning models from _different random initializations of the linear head consistently induces AGL_ across benchmarks. In contrast, neural networks trained from scratch observe AGL irrespective to these diversity sources.

Second, we show that finetuned models from _multiple_ different base FMs can be leveraged for AGL-based performance estimation. As base FMs can be pretrained with different datasets, architectures, and training regiments, the linear trends in ID versus OOD accuracy and agreement may break altogether in such ensembles. Indeed previous works indicate that on vision tasks, FMs pretrained on different image corpora can have different levels of OOD robustness for the same ID performance [17; 43; 51]. On the contrary, we find that on language tasks, FMs pretrained on different text corpora observe both AGL and ACL across question-answering and text classification tasks.

In total, we develop simple techniques for applying AGL-based performance estimation methods to predict the OOD performance of foundation models. We demonstrate that the AGL phenomenon is not limited to ensembles of neural networks trained from scratch. By simply finetuning FMs from random initializations of the linear head, we can observe the phenomena in FMs across a wide variety of tasks (classification, question-answering) and modalities (vision, language) and training procedures (linear probing, full finetuning). We find that AGL is the only method to accurately estimate the performance of finetuned FMs across all tasks, surpassing other performance estimation baselines by a significant margin as large as \(20\%\) mean absolute percentage error.

## 2 Background and related work

### Setup

We are interested in evaluating models that map an input \(x\) to a discrete output \(y\). In particular, we finetune foundation models. For a base model \(\), let \(f()\) denote a finetuned version of \(\). In this work, we consider a variety of foundation models: GPT2 , OPT , Llama2 , BERT , and CLIP .

Finetuning strategies.We have access to labeled data from some distribution \(_{}\) that we use for obtaining \(f()\) from \(\). In this work, we consider the following standard finetuning procedures.

1. **Linear probing (LP):** Given features from the base model \(_{}\), we train a linear head \(v\) such that the final classifier maps the score \(v^{}_{}(x)\) to a predicted class. We randomly initialize \(v\) and update \(v\) via gradient steps on a suitable loss function. The base model parameters remain frozen. We refer to \(v\) as either a linear probe (classification), or span prediction head (question-answering) depending on the task.

2. **Full finetuning (FFT):** We update all parameters of the backbone \(_{}\) and the linear head \(v\) _using a small learning rate_. When infeasible to update all parameters, we perform _low-rank adaptation_ (LoRA)  to reduce the number of trainable parameters while still effectively updating the feature extractor \(_{}\). In this work, we do not distinguish between LoRA and FFT as they conceptually achieve the same effect, and seem to show similar empirical trends in our studies.

OOD performance estimation.Given access to a labeled validation set from \(_{}\) and _unlabeled_ samples from a related but different distribution \(_{}\), our goal is to estimate performance on \(_{}\). We consider the standard performance metrics for various tasks: Accuracy \(_{0 1}:\) for classification, and Exact Match \(_{}:\) and Macro-averaged F1 score \(_{}:\) for question-answering. We use \(\) to denote the appropriate metric in the context.

### Background on OOD accuracy estimation

There is rich literature on OOD performance estimation for deep networks, with a variety of proposed approaches. Initial works focused on upper bounding the degree of distribution shift through data and/or model dependent metrics, e.g., uniform convergence bounds using \(\)-divergence . However, these bounds tend to be loose for deep networks . The following works try to estimate the performance exactly.

For classification,  leverage the model's confidence to predict the OOD performance. Since deep models are typically overconfident, these models are first calibrated in-distribution by temperature scaling. Similar methods are uncertainty quantification works that directly calibrate models under distribution shift . Confidence based methods are commonly utilized in practice, and favorable for foundation models as they are computationally light and model-agnostic. However, they often fail for large shifts  and are often well-defined for accuracy but not other common metrics like F1 score. These can be limiting factors for foundation models which are applied to a broad array of tasks. Still, as they are the most common estimation methods, we utilize them as the baselines in our work.

 also measure model behavior on known auxiliary tasks to understand model behavior under the distribution shift at hand. However, these approaches tend to be overfit to specific datasets or modalities. Similar to AGL, there are prediction methods that utilize information from ensembles. Oftentimes a separate "reference" ensemble is trained on some objective to predict the performance of a "target" model . These methods have a higher computational cost than AGL. Although AGL also requires at least 3 models to compute agreement, these models only undergo generic finetuning. Thus, it is a better suited approach for evaluating foundation models, especially if off-the-shelf finetuned models are readily available, e.g., from Huggingface (see Section 4).

Overall, there is growing attention towards understanding the safety and reliability of foundation models. To understand the effective robustness of FMs under distribution shift, recent works focus on studying the "accuracy-on-the-line" phenomena  (details in next subsection) and designing benchmarks that expose different failure modes of large models . However, unsupervised OOD performance estimation is underexplored in this modern setting, in terms of new methods and the transferability of old methods to large pretrained models.

### Accuracy and agreement on the line

We are interested in adapting the method "agreement-on-the-line" (AGL)  for OOD estimation as it obtains state-of-the-art performance estimation across several distribution shifts. AGL is based on an earlier observation called "accuracy-on-the-line" (ACL) -- across common distribution shift benchmarks, there is a strong linear correlation between the ID and OOD performance of models . ACL can also be observed in FMs for image classification, e.g., CIFAR10C , ImageNetV2 , FMoW-WILDS , and question-answering, e.g., SQuAD-Shifts . However, ACL does not always hold, e.g., Camelyon-WILDS  and SearchQA .

While ACL is a striking phenomenon, it does not immediately provide a practical method to estimate OOD performance--computing the linear fit of ID versus OOD accuracy requires labeled samples from \(_{}\). Alternatively, we can estimate this linear trend exactly using only the agreement between neural networks . Formally, given a pair of models \(f_{1}\) and \(f_{2}\) that map inputs to labels, accuracy and agreement is defined as

\[(f_{i})=_{x,y}[(f_{i}(x),y)],\ \ (f_{1},f_{2})=_{x,y}[(f_{1}(x),f_{2}(x))], \]

where \(\) is the appropriate performance metric of interest. While accuracy requires access to the ground truth labels \(y\), agreement only requires access to unlabeled data and a pair of models.  observes that when ID versus OOD accuracy is strongly linearly correlated between neural networks, i.e., ACL, then the ID versus OOD agreement of pairs of these models also observe a strong linear correlation with the _same_ linear slope and bias. Furthermore, when accuracies do not show a linear correlation, agreements also do not. This coupled phenomenon is dubbed "agreement-on-the-line" (AGL).

To use AGL for OOD performance estimation, one may obtain the slope and bias of the agreement line with unlabeled data, and then estimate the OOD performance by linearly transforming the ID validation performance. Specifically, with a collection of models \(=\{f_{1},f_{2},...,f_{n}\}\), AGL suggests that ID versus OOD accuracy observe a strong linear correlation if and only if ID versus OOD agreement observes a strong linear correlation and when they do, the slopes and biases match: \( f_{i},f_{j}\) where \(i j\)

\[^{-1}(_{}(f_{i}))&=a ^{-1}(_{}(f_{i}))+b\\ ^{-1}(_{}(f_{i},f_{j}))&=a ^{-1}(_{}(f_{i},f_{j}))+b \]

\(^{-1}\) is the probit transform used to induce a better linear fit as used in [2; 39]. Provided access to \(_{}(f_{i})\), \(_{}(f_{i},f_{j})\), \(_{}(f_{i},f_{j})\)\( i\), \(j\), we can estimate \(_{}(f_{i})\) for all \(f_{i}\). We refer the reader to  for formal AGL-based performance estimation algorithms (ALine-S and ALine-D), which we also provide in Appendix A.1.1.

## 3 Predicting OOD performance: single base foundation model

We first evaluate whether AGL appears in an ensemble of multiple finetuned runs of a _single_ base foundation model. This would enable precise OOD performance estimates for each ensemble member. A practitioner may naively gather a finetuned ensemble by training a couple runs with different seeds or hyperparameters. However, an overriding concern is that even with some randomness in the finetuning process, linear probing or light full-finetuning over the same base model may lead to solutions with very correlated predictions. We extensively evaluate the following methods of introducing diversity into the finetuning process to see what approach (if any) can lead to AGL.

1. **Random linear heads** We initialize the last layer of the network (i.e., the linear head) randomly, instead of via some zero-shot or pre-specified manner.
2. **Data ordering** We present the same training data to each model but shuffle the order of the data, i.e., model observes different minibatches.
3. **Data subsetting** We i.i.d. sample \(p\%\) subset of the data to train over. In the main body, we report models trained on independently sampled \(10\%\) of the training data, other proportions of \(30\%\) and \(50\%\) are reported in Appendix A.4.

We perturb one source of diversity at a time and study whether AGL occurs in each resulting model ensemble. For each setting, we also vary the number of training epochs to collect models with different ID performance, which is necessary to obtain a meaningful linear correlation in accuracy. The additional randomness induced by different training epochs does not affect the observations we make. We use at most four A6000's for all experiments except for linear probing where we use one RTX 8000.

### VLM-based Image Classification

We first investigate the effect of diversity source on AGL behavior for vision benchmarks. For image classification, a common pipeline is to finetune over a CLIP  pretrained foundation model.

CLIP Linear ProbingWe finetune over OpenCLIP ViT-B/32 model trained on LAION-2B . Given its well-established zero-shot capabilities, a popular method of finetuning CLIP is to simply employ linear probing on top of the CLIP representation. We take particular interest in evaluating the OOD performance of an ensemble of linear models trained on top of frozen base model representations.

DatasetsWe evaluate ensembles on synthetic corruptions (CIFAR10C, CIFAR100C, ImageNetC), dataset replication shifts (CIFAR10.1, ImageNetV2), style shifts (OfficeHome), geographical and temporal shifts (FMoW-WILDS, iWildCam-WILDS), and interlaboratory shifts in medicine (Camelyon17-WILDS). iWildCam-WILDS exhibits weak ACL and Camelyon17-WILDS doesn't exhibit any ACL . We test on iWildCam-WILDS and Camelyon17-WILDS to verify AGL's negative condition, i.e., when the linear correlation does not exist in ID versus OOD accuracy, it also does not exist in agreement.

ResultsAcross vision benchmarks, linear probed CLIP models observe ACL, i.e. there is a strong linear correlation in the ID versus OOD performance. Similarly, on the same datasets, we can observe a corresponding strong linear correlation in agreement across ensembles injected with diversity in linear head initialization, data ordering, and data subsetting (Figures 1). However, we find that only ensembles with diverse initialization leads to AGL where the linear trend of agreement and accuracy have a matching slope and bias (Figure 2). See Appendix A.3.2 for results on other datasets. In model ensembles obtained by data ordering and data subsetting, we observe a consistent trend where the agreement trend observe a much higher slope _close to the diagonal \(y=x\) line_. These results are not specific to linear probing alone. In full finetuned CLIP models, we also observe that random linear heads induce the most reliable AGL behavior (See Appendix A.3). Note that this setting is still notably different from  where models are heavily trained for _tens to hundreds of epochs_ often with a large learning rate, which causes AGL behavior to be more robust to the source of diversity used to induce the ensemble (See Appendix A.3.4).

### LLM-based Question-Answering and Text Classification

We conduct a similar systematic investigation of AGL in finetuned runs of a single base _language_ model. Similar to CLIP linear probing, we find that AGL cannot be observed without random head initialization in language models evaluated on text classification and extractive question-answering tasks. While we mostly focus on tasks that require a linear head during finetuning, we also conduct a diversity study on generative tasks where the base model is finetuned directly in Appendix A.6.1.

  ID & OOD \\  CIFAR10  & CIFAR10C , CIFAR10.1  \\ CIFAR100  & CIFAR100C  \\ ImageNet  & ImageNetC , CIFAR10.1  \\ FMoW ID  & FMoW OOD  \\ iWildCam ID  & iWildCam OOD  \\ Camelyon17 OOD  & Camelyon17 OOD  \\ OfficeHome  & All (ID, OOD) pairings of domains at, CLipArt, Product, Real World \\ MNL1  & MNL1-Mismatched , SNL1  \\ SQuAD  & SQuAD-Shifts  \\  

Table 1: We evaluate models on the following distribution shift benchmarks.

Figure 2: In ensembles with diverse random initializations, ACL and AGL holds across benchmarks in linear probed CLIP models. Similar to , neither ACL nor AGL holds for the Camelyon17-WILDS

Full Finetuned Language ModelsWe evaluate over a collection of \(450\) full finetuned runs of several base FMs: GPT2-Medium  and OPT-125M . Models are full finetuned for up to \(20\) epochs with a small learning rate (\( 1e^{-6}\)). Hyperparameters specifics can be found in Appendix A.2. We do not conduct a linear probing study for question-answering as it leads to poorly performing models. For text classification, we also conduct a linear probing study in Appendix A.5.

DatasetsWe test models on a text classification shift from MNLI  in the GLUE benchmark  to MNLI-Mismatched  and SNLI . We also evaluate extractive question-answering models on the shift from SQuAD v1.1  to SQuAD-Shifts .

ResultsWe evaluate models on accuracy for text classification and F1 score for question-answering. Similar to our findings in CLIP, in both text classification and question-answering benchmarks, ensembles of full finetuned LLMs observe AGL when models are trained from different randomly initialized linear or span heads while data ordering and data subsetting observe an agreement trend closer to the diagonal \(y=x\) line (Figure 1 and Appendix A.5). We note that with full finetuning, the differences in AGL behavior between diversity sources are not as stark as with linearly probed models. In some sense, how model diversity is achieved becomes increasingly less important for observing AGL as the base model parameters also diverge, with ensembles of models heavily trained from scratch at the extreme .

### Summary and Implications

Across image and language modalities, we demonstrate that ensembles of finetuned FMs can also observe agreement-on-the-line similar to heavily trained CNN's . In both domains and regardless of the fine-tuning strategy, e.g. FFT and LP, or metric, e.g. F1 and Accuracy, employed, the diversity induced via random head initialization yields AGL, while the diversity induced via data reordering or data subsetting does not. This phenomenon can be observed across hyperparameters (Appendix A.7) and different PEFT methods (Appendix A.8). With a single heavily pre-trained base FM, one may think that light finetuning leads to downstream models with highly correlated behavior under distribution shift. However, simply randomly initializing the linear head alone induces sufficiently decorrelated models for observing AGL. The diversity in the ensemble becomes important when predicting the OOD performance of models using downstream AGL-based methods. In Table 2, we show that AGL-based methods can only accurately predict the OOD performance of models in ensembles with diverse initialization, and cannot with data subsetting or ordering.

Furthermore, our findings contrast previous work that suggest AGL is a neural-network specific phenomenon , unlike ACL which is model agnostic . Specifically,  report that linear models trained on top of the flattened CIFAR10 images do not observe AGL. However, we find that, on top of CLIP features, _linear models can exhibit AGL_ with random initialization. Previous work on the Generalization Disagreement Equality  contend that data subsetting leads to the most diversity in model predictions for deep ensembles (neural networks, random forests). Specifically, in-distribution, the agreement rate between pairs of models was shown to equal their expected accuracy in ensembles obtained by data subsetting, while those that vary random initialization has slightly higher agreement . On the other hand, in our problem setting of _out-of-distribution_ datasets on FMs, we found that ensembles induced by different random initialization achieves AGL, while data ordering or subsetting cannot. Our setting is different from previous literature in two ways: (1) AGL studies the OOD agreement rate relative to their ID agreement, in contrast to the GDE phenomenon which only regards the models' ID agreement. We hypothesize that random initialization is much more important for observing the right levels of OOD agreement. (2) Models

  
**Source of Diversity** & CIFAR10C & SQuAD-Shifts Amazon & SQuAD-Shifts Reddit & SNLI \\  Random Linear Heads & **14.64** & **6.34** & **3.48** & **11.70** \\ Data Ordering & 37.01 & 10.30 & 9.59 & 15.40 \\ Data Subsetting & 35.85 & 16.21 & 13.94 & 15.50 \\   

Table 2: ALine-D MAPE (\(\%\)) of different sources of diversity for CLIP linear probing and GPT2-Medium/OPT-125M full finetuning. We average the score over all corruptions for CIFAR10C.

are only lightly fine-tuned or linearly probed, unlike deep ensembles trained from scratch. Diversity sources may behave differently in this circumstance.

## 4 Predicting OOD performance: multiple foundation models

Alternatively, we consider ensembling _multiple_ base foundation models. First, ACL may not hold because the base models are heavily pretrained on different data corpuses. This may cause respective downstream models to have different ID versus OOD accuracy trends or "effective robustness" . On vision tasks, for example, linear probing over CLIP, EfficientNet , ViT , and BYOL  observe varying robustness trends . Second, even when ACL does hold, it is unclear whether the ensembles will also observe AGL. Here the problem is different from the single base model setting: any pair of foundation models finetuned from different base models may agree _too little_, or OOD agreement rate may vary across model pairs depending on the similarity of the pretraining corpus, breaking the linear correlation of agreement entirely. Yet, we observe that for language models and tasks, ensembles of finetuned FMs from a wide range of base models _observe both ACL and AGL_.

ModelsWe finetune models from OPT-125M, OPT-350M, OPT-1.3B , GPT2, GPT2-Medium, GPT2-Large, GPT2-XL , GPT-Neo-135M , Llama2-7B , Alpaca-7B , and Vicuna-7B . We fully finetune OPT and GPT models and LoRA finetune Llama, Alpaca, and Vicuna. These models are pretrained on different mixtures of BookCorpus , Stories , PILE , CCNews v2 corpus, and PushShift.io Reddit . Alpaca and Vicuna are instruction-finetuned over Llama2.

### Results

We investigate the AGL behavior of an ensemble of foundation models finetuned from diverse base models in Figure 3 for question-answering. First note that base LLM models pretrained on different text corpora lead to finetuned models that lie on the _same linear trend in accuracy_. Unlike the different accuracy trends observed by different vision foundation models , we suspect that the pretraining datasets for the language models in our study observe much more homogeneity. Second, the ID

Figure 3: AGL can be observed between models finetuned from different base models (Llama, GPT, OPT) for the F1 score for question-answering shift (SQuAD to SQuAD-Shifts) and accuracy for text classification (MNLI-Matched to MNLI-Mismatched and SNLI). SQuAD-Shifts New Wiki, SQuAD-Shifts NYT, and MNLI Mismatched show little drop in OOD performance because the distribution shift is small compared to the corresponding ID dataset. Nonetheless, we observe that AGL holds regardless of the degree of distribution shift.

versus OOD agreement between pairs of models in this ensemble, including those between different base foundation models, is also strongly correlated and the slope and intercept closely matches that of accuracy. In other words, ensembles of different base models also observe AGL without any special regularization for ensemble diversity. The same holds for generative QA tasks (Appendix A.6.2).

## 5 Estimating OOD Accuracy using AGL in Diverse Ensembles

By constructing a diverse ensemble of foundation models, we can leverage AGL to extract precise estimates of model performances under distribution shift. We construct ensembles by collecting models trained from randomly-initialized heads (Section 3) and different base models (Section 4). For image classification, our model collection consists just linear models over CLIP representations. For text classification and question-answering, we include GPT, OPT, and Llama models individually finetuned from differently initialized heads. In Table 3, we compare the Mean Absolute Percentage Error (MAPE) of AGL-based prediction algorithms, ALine-S and ALine-D , to other baselines.

We compare against confidence based methods ATC , AC  and DOC-Feat  and Naive Agreement which directly uses agreement between model pairs . For confidence based methods, we first temperature scale the models using ID validation data, and pick the lower error rate from the estimations obtained with and without temperature scaling. However, there are several limitations when naively applying confidence baselines to estimate performance on question-answering, as they estimate classification accuracy. First, there is no easy analogous formulation of confidence baselines for the F1 score, so we estimate the exact-match score instead for fair comparison. On the other hand, AGL can predict performance across metrics accuracy, F1, and exact-match. Second, extractive question-answering is a joint classification task where models predict both the start and end token index of the answer span in the context. More details for how we calibrate baselines for this setting is provided in Appendix A.1.2.

Because ALine-S and ALine-D only provide estimation guarantees where the coefficient of determination \(R^{2}\) of the linear fit in agreement is strong (), we filter out datasets with low \(R^{2} 0.95\). These shifts include iWildCam-WILDS, Camelyon-WILDS, and a few corruptions in CIFAR10C, CIFAR100C, and ImageNetC. We evaluate ALine-S/D for these failure cases in Appendix A.1.3. Across datasets with a high \(R^{2}\) in agreement, ALine-S and ALine-D provide precise OOD performance estimates in finetuned FMs, surpassing other baselines by a large margin. This is noteworthy, especially for shifts where the agreement line is significantly off \(y=x\), further lending to the utility of this method. Furthermore, they perform better on the question-answering task SQuAD, with the next best confidence method achieving as large as \(20\%\) higher error.

   OOD Dataset & ALine-D & ALine-S & Naive Agr & ATC & AC & DOC-Feat \\  CIFAR10C\({}^{*}\) & 5.44 & **4.73** & 17.39 & 6.90 & 11.49 & 11.91 \\ CIFAR10.1 v6 & **1.95** & 1.99 & 16.95 & 2.60 & 4.93 & 5.36 \\ CIFAR100C\({}^{*}\) & 7.17 & **6.79** & 17.66 & 8.18 & 17.58 & 14.96 \\ ImageNetC\({}^{*}\) & 15.03 & **14.17** & 32.27 & 15.90 & 22.83 &

## 6 Limitations

Estimating the out-of-distribution performance of foundation models has rapidly grown in importance, especially as these models are increasingly deployed in real-world use cases. Our work focuses on a promising method to enable deployers to reduce the harm of machine learning systems when they encounter OOD inputs. However, deployers should be careful to not use AGL as the only signal for OOD performance. The correlation between agreement and accuracy is not guaranteed to hold for all distribution shifts, so other metrics should additionally be used to monitor model performance. In particular for foundation models, we observe that careful choices during fine-tuning is required to observe AGL. In fact, if different pretrained checkpoints are evaluated zero-shot, without any fine-tuning, AGL is not able to reliably predict the performance of FMs (Appendix A.3.5). Furthermore, while we studied AGL closely for a wide array of classification/QA benchmarks, there remains other important downstream tasks such as long-form generation that we leave for future study. We also do not provide any theoretical guarantees to back our empirical findings, such as the importance of random initialization, which we leave for future work.

## 7 Conclusion

We develop methods for extending AGL to foundation models to enable OOD performance prediction in this emerging paradigm. We find that utilizing AGL for performance estimation requires a careful tuning of ensemble diversity. Unlike the original paradigm of AGL, where models observed tens or hundreds of epochs of training on the ID dataset, we find that randomness in specific optimization choices, especially linear head initialization, is crucial for foundation models. In fact, in contrast to , we find that linear models can also observe AGL, specifically in the CLIP representation space, suggesting that AGL may not be a neural network specific phenomena. Our conclusion on AGL also sheds light on the robustness of foundation models. First, our experiments show that light finetuning alone can corrupt models to have diverse behaviors. Next, in contrast to vision models, where previous works show different forms of pretraining lead to different slopes in the linear correlations , we find that all the language models we evaluate, e.g., OPT, GPT2, GPT2-Neo, Alpaca, Llama, and Vicuna lie on the _same_ accuracy line. This is particularly intriguing because it goes against the common wisdom that the pretraining data influences the models "effective robustness". We leave these questions for future analysis.