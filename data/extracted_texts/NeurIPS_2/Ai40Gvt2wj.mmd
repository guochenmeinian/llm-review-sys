# Rethinking \(\mathrm{SO}(3)\)-equivariance with Bilinear Tensor Networks

# Rethinking \((3)\)-equivariance with Bilinear Tensor Networks

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Many datasets in scientific and engineering applications are comprised of objects which have specific geometric structure. A common example is data inhabiting a representation of \((3)\) scalars, vectors, and tensors. One way to exploit prior knowledge of the structured data is to enforce \((3)\)-equivariance. While general methods for handling arbitrary \((3)\) representations exist, they can be computationally intensive and complicated to implement. We show that by judicious symmetry breaking, we can efficiently increase the expressiveness of a network operating on these representations. We demonstrate the method on an important classification problem from High Energy Physics known as _b-tagging_. In this task, we find that our method achieves a \(2.7\) improvement in rejection score over standard methods.

## 1 Introduction

In many Machine Learning (ML) applications, at least some of the data of interest have specific geometric structure. For example, position measurements from LiDAR imaging, the configuration of atoms in molecular potentials, and measurements of particle momenta are all cases where the data are naturally represented as spatial 3-vectors. However, classical Neural Network (NN) architectures are not well suited to this sort of data; for instance, the standard Multi Level Perceptron would require that all information, spatial or otherwise, must be collapsed into a flat list of features as input to the network. In this case, the spatial nature of the data, while not lost, is not communicated _a priori_ nor enforced _post hoc_.

More recently, developments in the field of Representation Learning have shown that _equivariant_ NNs are a natural way to accommodate structured data, and in many cases lead to substantially improved algorithms. Very informally, a function (such as a NN) is called equivariant if the output transforms similarly to the input.

Convolutional Neural Networks (CNNs) are the prototypical example of this. CNNs exploit the fact that image data can be most naturally represented as data on a discrete 2-dimensional grid. This data structure is associated with the representation of the group of discrete translations. The standard CNN layer takes advantage of this by operating on input grid (pixel) data with discrete translation operations, and returning outputs on a similar grid structure. Because the output of each layer has the same representational structure as the input, it is straightforward to build very deep representations without destroying the prior spatial structure of the data, simply by stacking CNN layers. The result, of course, is that CNNs have completely revolutionized the field of computer vision.

We specifically consider the case of continuous scalar and 3-dimensional vector point data, as may be encountered in many point-cloud datasets. For these data, the natural group associated with their representation is \((3)\), the set of 3D rotations. Therefore, one strategy to incorporate this structureinto a neural architecture is to enforce equivariance _w.r.t._\((3)\), and several such architectures have been proposed [1; 2; 3]. In general, these approaches achieve equivariance either by defining a spherical convolutional operation [3; 1], or by constraining the network's operations to maintain strict representational structure [2; 4].

Our method follows the latter approach, but in a much simpler way. Rather than concerning ourselves with arbitrary \((2+1)\)-dimensional representations, we consider only a few physically relevant representations: scalars, vectors, and order-2 tensors. For these three representations, it is straightforward to enumerate the options for linear neuron layers. We also want our network to be able to exchange information between different representations. The Clebsh-Gordon theory prescribed in other methods provides the most general method for projecting arbitrary tensor products between representations back into irreducible representations, However, once again we take a similar approach, and instead introduce a simple _Tensor Bilinear Layer_, a subset of the CG space that consists of commonly known and physically intuitive operations, such as the vector dot product and cross product.

Importantly, we propose a novel method that allows us to relax equivariance requirements when an axial symmetry is present, by allowing the global \((3)\) symmetry to be locally broken down to \((2)\). These looser conditions allow us to design of models that enforce only the instantaneously relevant equivariance, and allows the network to learn more expressive functions at each layer. We show that this kind of equivariant neuron is generally only possible with the introduction of order-2 tensor representations, but we provide an efficient implementation for vector-valued networks that constructs only the minimal tensors required.

To illustrate a real-world application to data with an axial symmetry, we introduce a common problem from the field of High Energy Physics (HEP), described in Sec. 2. In Sec. 3, we describe the modular elements of our method, from which a wide variety of neural architectures may be composed. In Sec. 4, we describe a specific architecture based on Deep Sets  which will serve as a baseline model, and we illustrate how to adapt this architecture using our approach. In Sec. 5, we describe the simulated data used for training and evaluation, and describe the results of a progressive implementation of the modules developed herein. Finally, we offer concluding remarks in Sec. 6.

### Related Work

From the field of High Energy Physics, there has been much work in applying various DL approaches to jet tagging [6; 7; 8; 9] in general and b-tagging in particular [10; 11]. The present work seeks to build on this effort by offering novel neural architectures that can be adapted into next-generation applications.

From the field of Machine Learning, there have been numerous prior works on SO(3) equivariant models [1; 2; 3; 4]. In general, these approaches depend on Clebsh-Gordon (CG) decomposition and/or sampling from spherical harmonics. While our approach is more similar to the CG method, it is simpler and more relevant for the task at hand. Moreover, we innovate on the the space of allowed equivariant operations by relaxing the global SO(3) symmetry which is relevant for our particular application.

### Novel Developments

The main innovation of this paper is to expand the set of linear equivariant maps in the special case where there is a "special" direction in space, which may change from sample to sample. In this case, it is possible to maintain global SO(3) equivariance, while breaking the per-layer equivariance condition down to a locally-defined SO(2) symmetry, which is parameterized by the special direction. We also innovate by introducing a simpler method of forming SO(3)-equivariant nonlinearities, by simply introducing familiar bilinear operations on spatial representations such as scalars, vectors, and tensors. In addition to the nonlinearity provided by the bilinear operations, we also introduce simple nonlinear activation functions on the vector and tensor representations, which we find helps stabilize training and improve performance.

Lastly, from the physics perspective, we propose a significant departure from standard practice, by stipulating that our b-tagging should be provided with raw 3-dimension position and momentum information, as this is the only way to ensure that SO(3)/SO(2) equivariance is exploited.

While we demonstrate our methods using a specific architecture based on Deep Sets , we expect these innovations can be useful in many other applications. Given the modularity and strictly defined input and output representations of each layer, these elements could be used to augment other neural architectures such as convolutional, graph, and transformers as well.

## 2 B-jet Identification at LHC Experiments

In HEP experiments, such as ATLAS  and CMS  at CERN, _b-jets_ are a crucial signal for studying rare phenomena and precision physics at the smallest scales of nature. A _jet_ is a collimated spray of hadronic particles originating from energetic quarks or gluons produced in high energy particle collisions. A _b-jet_ is a jet which specifically originates from a \(b\)-quark; when these quarks hadronize, they form metastable B-mesons which travel some distance from the collision origin before decaying, emitting particles from a secondary, displaced vertex.

Charged particles originating from these vertices are measured with tracking detectors and are often referred to as _tracks_. Due to the displacement of the secondary vertex, when track trajectories originating from B-meson decays are extrapolated backwards, they are generally not incident to the origin. Therefore, we instead measure the distance to the point of closest approach; this is often referred to as the _track impact parameter_, which is a 3-vector quantity that we denote with \(\).

In most applications, only the transverse and longitudinal components, \(d_{0}\) and \(z_{0}\), of this impact parameter are examined . The magnitude of these projections is the most distinctive feature that indicates whether a particular jet originated from a \(b\)-quark.

The inspiration for this work was the observation that the physical processes which govern how particles within a jet are produced and propagated are largely invariant with respect to rotations about the _jet axis_, denoted \(}\). This is the unit vector in the direction of the aggregate jet's momentum vector. On the other hand the standard \(b\)-tagging observables \(d_{0}\) and \(z_{0}\) have no well-defined transformation rule under rotations, _i.e._ they are not part of a covariant representation.

Previous works  have demonstrated that networks which exploit this natural \((2)\) symmetry can greatly improve performance, but these methods all essentially rely on reducing the problem to vectors in a 2-dimensional plane. In order to obtain an equivariant representation in the case of \(b\)-jets, we must consider the full 3-dimensional structure of the impact parameter, which transforms as a vector under general rotations \(R\). In addition to the 3-dimensional impact parameter \(\), we also have information about the track's momentum \(\) and various scalar quantities such as the particle's charge, energy, and a limited identification of the particle type.

In the next section, we will describe modular neural elements that can solve this problem, by allowing a network to admit a global \((3)\) symmetry which preserves the scalar and vector representations, while also breaking \((3)\) down to the more physically appropriate \((2)\) whenever possible.

## 3 Network Elements

Our proposed method depends on three modular elements, described in detail in the following subsections. The overall strategy begins by mirroring what has proved to work for NNs in general: we interleave simple linear (or affine) layers with nonlinear activation functions, in order to learn powerful models. For an equivariant network, we first need to identify a set of linear equivariant maps suitable for the symmetry at hand. In our case, we come up with two sets of such maps: a global \((3)\)-equivariant affine layer, and a locally \((2)_{}\)-equivariant linear layer.

Since we also require our network to mix between its scalar, vector, and tensor representations, we introduce an equivariant _bilinear_ layer. Lastly, we define \((3)\) equivariant nonlinear activations for each output representation.

In Sec. 4, we demonstrate how to combine these elements into a complete neural architecture. This architecture is based on the Deep Sets  architecture suitable for variable-length, permutation-invariant data.

### \((2)_{}\)-equivariant Linear Layers

A well-known way to ensure equivariance _w.r.t._ any group is to broadcast the neural action across the representational indices of the data [15; 16]. That is, the neural weight matrix simply forms linear combinations of the features in their representation space. In general, it is helpful to add a bias term, but care must be taken to select one that preserves equivariance.

The simplest example of this is for a collection of \(F\) scalar input features, \(\{s_{i}\}\), mapping to a collection of \(K\) output features. The scalar has no representational indices, so this simply amounts to the standard affine1 network layer

\[y_{i}=W_{ij}s_{j}+b_{i} \]

where the learnable parameters \(W_{ij}\) and \(b_{i}\) are the neural weights and bias terms, respectively. In the vector case, we may generalize to

\[_{i}=W_{ij}_{j}\,;\;\;_{i}=0\,. \]

Note that the equivariance condition for vector-valued functions \(f(R)=Rf()\) implies that \(R=\) for arbitrary rotation \(R\); hence, the bias vector must be zero. Finally, the analogous case for order-2 tensors is:

\[Y_{i}=W_{ij}T_{j}+B_{i}\,;\;\;B_{i}=b_{i}I\,, \]

where again we have learnable scalar parameters \(b_{i}\). In this case, the equivariance condition is \(f(RTR^{T})=Rf(T)R^{T}\), which implies that \(RBR^{T}=B\), _i.e._\(B\) must commute with arbitrary \(R\). Therefore, \(B\) must be proportional to the identity tensor \(I\).

The above neurons are purely isotropic in \((3)\). However, as discussed in Sec. 1, for our problem we have prior knowledge that the distribution is symmetric about a specific axis. At worst, having only isotropic operations can over-regularize the network by imposing too much structure, and at best it might be harder for the network to spontaneously learn about the axial symmetry. We therefore consider the most general linear map is equivariant _w.r.t._ the axial symmetry. Since this is a lesser degree of symmetry, the network should have greater freedom in choosing linear maps.

#### 3.1.1 Vector Case

Let \(}\) be a unit vector (in our application, the jet's momenutrix) which is instantaneously fixed per batch input. The rotations about this axis define a proper subgroup \(S_{}}(3)\) where we identify \(S_{}}(2)\). We therefore refer to this subgroup as \((2)_{}}(3)\); the distinction being that \((2)_{}}\) fixes a representation on \(^{3}\) which depends on \(}\).

The set of all linear \((2)_{}}\)-equivariant maps is exactly the set of matrices \(A\) which commute with arbitrary \(R_{}}(2)_{}}\), which are of the form

\[A=(a}}^{T}+b(I-}} ^{T}))R^{}_{}}()\,, \]

for arbitrary learnable parameters \(=(a,b,)\). The first two terms represent anisotropic scaling in the directions parallel and perpendicular to \(}\), respectively. The third term represents any other arbitrary rotation about the \(}\) axis, parameterized by a single angle \(\).

Because \(A\) commutes with all \(R_{}}(2)_{}}\), the linear layer defined by

\[_{i}=A_{_{ij}}_{j} \]

is \((2)_{}}\)-equivariant, for arbitrary parameters \(_{ij}\). The isotropic linear neuron of Eq. 1 corresponds to the special case \(a_{ij}=b_{ij}\), \(_{ij}=0\).

#### 3.1.2 Tensor Case

In order for a tensor-valued linear map \(L\) to be equivariant, we require that \(L(R_{}}TR_{}}^{T})=R_{}}(LT)R_{ }}^{T}\). Note that in the case of full \((3)\) equivariance, the only option is for \(L\) to be proportional to theidentity. Without loss of generality, we may assume the order-4 tensor \(L\) can be written as a sum of terms \(A B\) for some order-2 tensors \(A,B\). The tensor product acts on an order-2 tensor \(T\) as \((A B)T:=ATB^{T}\). Taking \(L\) to be of this form (up to linear combinations), the equivariance condition reads \(A(R_{}T_{}^{T})B^{T}=R_{}(ATB^{T})R_{}^ {T}\). This is satisfied when both \(A\) and \(B\) commute with \(R_{}\); we have already identified the set of such matrices in Eq. 4. Therefore, we define the action of the tensor-valued \((2)_{}\) linear layer by:

\[Y_{i}=A_{_{ij}}T_{j}A_{_{ij}}^{T}+b_{i}I\,, \]

where the parameters \((_{ij},_{ij})\) are the neural connections and we also allow for an affine bias term parameterized by \(b_{i}\), which is proportional to the identity tensor and hence also equivariant.

### Tensor Bilinear Operations

So far we have provided two means for working with data in the \((3)\) scalar, vector, and order-2 tensor representations. However, we also desire a means for allowing information between the different representations to be combined and mixed.

The most general approach to this is addressed by Clebsh-Gordon theory . But we adopt a simpler approach, wherein we take advantage of the familiar representations of our data and employ common bilinear operations such as dot products and cross products for vectors2. This allows the network to create a mixing between different representations. The operations considered are enumerated schematically in Fig. 1. In order to form these terms, the bilinear layer requires that the scalar, vector, and tensor inputs \((s,,T)\) all have the same size, \(2F\), in their feature dimension, and that the size is a multiple of two. We then split the features into groups of two: \(s_{a}=\{s_{i}\}_{i=1..F}\), \(s_{b}=\{s_{i}\}_{i=F+1..2F}\), and define similarly \(_{a,b}\) and \(T_{a,b}\).

After effecting all of the options from Fig. 1, the layer returns scalar, vector, and tensor outputs with \(3F\) features each.

### \((3)\)-equivariant Nonlinear Activations

For the scalar features, any function is automatically equivariant. Therefore, for these features we use the well-known ReLU activation function, although any alternative nonlinearity would also work.

In the vector and tensor cases, care must be taken to ensure equivariance. For the vector case, we state a simple theorem:

**Theorem 3.1**: _For any vector-valued function \(f:^{3}^{3}\) which satisfies \(f(R)=Rf()\) for all \(R(3)\), there exists a scalar function \(\) such that_

\[f()=(|x|)}\,,\]

_where \(}=/|x|\) when \(|x|>0\) and \(}=\) otherwise._

Figure 1: A schematic diagram of the bilinear layer with mixing between different representations.

In other words, we may chose an arbitrary, nonlinear function \(\) which acts only on the vector magnitude, and the layer must leave the direction of the input unchanged. This leaves many possibilities; after some experimentation, we found the following activation, which we call Vector ReLU (\(\)), works well:

\[():=&|v|<1\\ /|v|&. \]

The \(\) activation is analogous to the standard rectified linear unit, except that the transition from linear to constant happens at a fixed positive magnitude rather than zero. We found that in particular, the saturating aspect of \(\) greatly helps to stabilize training, as otherwise the vector features tend to coherently grow in magnitude, leading to exploding gradients.

For the order-2 tensor case, we note here that the tensor analog to Theorem 3.1 is much more nuanced, and in general depends on three principal invariants \(_{1},_{2},_{3}\). For simplicity, we define the Tensor ReLU (\(\)) completely analogously to the vector case, and leave a more complete analysis of tensor nonlinearities to future work:

\[(T):=T&||T||_{F}<1\\ T/||T||_{F}&. \]

## 4 Benchmark Architectures

We now have defined the four modular elements which provide the appropriate equivariant operations. In order to evaluate the practical effects of these modules, we define a benchmark architecture that is based on the Deep Sets architecture, also referred to as a Particle Flow Network (PFN)  in the field of HEP. The PFN is a commonly-used architecture for this sort of problem in real-world applications such as at the ATLAS experiment.

We will first define the standard PFN architecture, which will serve as our baseline in experiments. Then, we describe a modified version at the module level using the analogous equivariant operations in place of the standard neural network layers.

### Particle Flow Network

The basic structure of the PFN  is based on the Deep Sets  architecture, and will serve as our baseline. It is of the form:

\[(\{p_{k}\})=F(_{k=1}^{P}(p_{k}))\,. \]

where \(:^{F}^{L}\) and \(F:^{L} Y\) are arbitrary continuous functions parameterized by neural networks. \(L\) is the dimension of the latent embedding space in which the particles are aggregated and \(P\) is the number of particles in an observed jet. \(Y\) represents the relevant output space for the task at hand; since our task is classification, we consider \(Y=\).

The input features \(\{p_{k}\}\) represent the observed track particles within the jet. These features include:

* The jet 3-momentum in detector coordinates, \((p_{T}^{(J)},^{(J)},^{(J)})\)
* The 3-momentum of each particle track in _relative_ detector coordinates, \((p_{T}^{k},^{k},^{k})\)
* The track impact parameters of each particle \((d_{0}^{k},z_{0}^{k})\)
* The particle's charge \(q\) and particle type \(\{\}\)

For each jet, we allow up to \(P=30\) particle tracks; inputs with fewer than \(30\) particles are padded with zeros. We also repeat the jet 3-momentum over the particle axis and concatenate with the rest of the per-particle features. The discrete particle type feature is embedded into \(3\) dimensions. After concatenating all features, the input to the PFN is of shape \((*,P,F)\) where \(F=12\) is the feature dimension.

The subnetworks \(\) and \(F\) are simple fully-connected neural networks. \(\) consists of two hidden layers with \(128\) units each, and ReLU activation. The output layer of \(\) has \(L\) units and no activation applied. The \(F\) network consists of three hidden layers with \(128\) units each and ReLU activations.

The final output layer has two units with no activation, in order to train with a categorical cross entropy objective.

### Vector and Tensor PFN

We now adapt the basic PFN architecture and promote it to what we term a Vector PFN (VPFN) or Tensor PFN (TPFN), according to the highest representation included. The overall architecture is of the same form as Eq. 9; we will simply modify the detailed implementation of the \(\) and \(F\) sub-networks.

The first change is that the input features now belong strictly to one of the three \((3)\) representations: scalar, vector, or order-2 tensor:

\[(\{(s,,T)_{k}\})=F(_{k=1}^{P}(s_{k}, _{k},T_{k})) \]

In general, the number of features in any of the representation channels are independent. The features for the TPFN experiments include:

* The jet 3-momentum in Cartesian coordinates \((p_{x}^{(J)},p_{y}^{(J)},p_{z}^{(J)})\)
* The 3-momentum of each particle track \(^{k}\)
* The 3-position of the track's point of closest approach to the origin \(^{k}\)
* The charge and particle type of each track, as described in Sec. 4.1

As before, we replicate the jet momentum across the particle index, and we embed the particle type into 3 dimensions, resulting in \(F_{s}=4\) scalar and \(F_{v}=3\) vector features. Since there are no observed tensor features for particle tracks, we synthesize an initial set of features to act as a starting point for the tensor operations. This is done by taking the outer product between all combinations of the three available vector features, resulting in \(F_{t}=9\) features.

We now have \(:^{F_{s} 3F_{v} 9F_{t}}^{L  3L 9L}\), where \(F_{s},F_{v},F_{t}\) are the number of scalar, vector, and tensor inputs, respectively. A single layer of \(\) is formed as shown in Fig. 2, by combining in sequence the Affine, \((2)_{}\)-Linear, Bilinear, and Nonlinear modules described in Sec. 3. The network consists of two hidden and one output layer. Each hidden Affine layer of the \(\) network contains \(2F=128\) features per representation, which results in \(3F=192\) features after the Bilinear layer. The output of the \(\) sub-network had \(L\) features, and there is no Bilinear or Nonlinear layers applied.

The \(F\) network is built similarly to the \(\) network, except that it has three hidden tensor layers. In lieu of an output layer, after the hidden tensor layers, the \(F\) network computes the square magnitude of each vector and tensor feature, in order to create a final set of \(3 3F\) scalar invariant features. The scalar features are concatenated, passed through two more hidden layers with 128 units each and ReLU activations, and finally to an output layer with two units and no activation.

## 5 Experiments

To train b-tagging algorithms, we must use Monte Carlo simulations of particle collision events, as this is the only way to get sufficiently accurate ground truth labels. The optimization of these algorithms is commonly studied by experiments such as ATLAS and CMS, which use highly detailed proprietary detector simulation software, and only limited amounts data are available for use outside of the collaborations.  There are also some community-generated datasets available for benchmarking , however none of these publicly-available datasets contain the key information that our method leverages for its unique equivariant approach. Specifically, our model requires the full 3-dimensional displacement vector of each track's impact parameter, whereas the existing datasets only retain the transverse and longitudinal projections \(d_{0}\) and \(z_{0}\). Therefore, we have created a new dataset for b-jet tagging benchmarks, to be made publicly available. The data is generated using standard Monte Carlo tools from the HEP community.

We begin by generating inclusive QCD and \(t\) events for background and signal, respectively, using Pythia8. Pythia handles sampling the matrix element of the hard processes at \(=13TeV\)the parton shower, and hadronization. The hadron-level particles are then passed Delphes, a fast parametric detector simulator which is configured to emulate the CMS detector at the LHC.

After detector simulation, jets are formed from reconstructed EFlow objects using the anti-\(k_{T}\)[23; 24] clustering algorithm with radius parameter \(R=0.5\). Only jets with \(p_{T}>90\)GeV are considered. For the signal sample, we additionally only consider jets which are truth-matched to a B-meson. Finally, the highest-\(p_{T}\) jet is selected and the track and momentum features are saved to file.

The training dataset consists of a balanced mixture of signal and background with a total of 1M jets. The validation and test datasets contain 100k signal jets each. Due to the high degree of background rejection observed, we must generate a substantially larger sample of background events for accurate test metrics, so the validation and test datasets contain 300k background jets each.

### Results

To quantify the performance of our model, we consider the following metrics in our experiments. First, the loss function used in the training is sparse categorical cross entropy, which is also used in the validation dataset. We also consider the area under the ROC curve (AUC) as an overall measure of the performance in signal efficiency and background rejection. We also consider the background rejection at fixed efficiency points of \(70\%\) and \(85\%\), labeled by \(R_{70}\) and \(R_{85}\), respectively. Background rejection is defined as the reciprocal of the false positive rate at the specified true positive rate.

A summary of a variety of experiments is given in Table 1. The numbers in the table represent the median test score over 10 training runs, where the test score is always recorded at the epoch with the lowest validation loss. The quoted uncertainties for the rejections are the inter-quartile range.

### Discussion

Table 1 shows that the family of models with only vector representations can indeed improve over the baseline, provided that we include at least the bilinear layer allowing the vector and scalar representations to mix. Moreover we find that adding the the \((2)\) linear operations gives the vector network access to a minimal set of order-2 tensors, \(R_{}}\), \(}}^{T}\), and \(I\) to enable it to exploit the axial symmetry of the data.

Figure 2: A schematic diagram of the DeepSets-adapated Tensor PFN.

In the case of the tensor family of models, there is a less substantial improvement when adding the \((2)\) linear layer. We expect that this is because the network with only bilinear operations is, at least in theory, able to learn the relevant operations on its own. Nonetheless, there is some improvement when adding this layer, so it would be reasonable to include both unless computational constraints are a concern.

Finally, we note that neither family of models performs even as well as the baseline, when no bilinear operations are allowed. This clearly demonstrates the effectiveness of a network which can mix \((3)\) representations.

## 6 Conclusion

In this work, we have introduced four modules of neural network architecture that allow for the preservation of \((3)\) symmetry. The Tensor Particle Flow Network (TPFN) shows promising results in our dataset, yielding up to \(2.7\) improvement in background rejection, compared to the simple Particle Flow baseline model. We emphasize that the overall architecture of the PFN and TPFN are nearly identical; the improvement is entirely due to a drop-in replacement of standard neural layers with our covariant and bilinear layers. We also note that in our approach, the TPFN outputs a scalar which is rotationally invariant. However, it is also possible to obtain a covariant output by simply not apply the scalar pooling operations. This could be useful for many other applications, such as regression tasks, where covariant predictions are desired.

Moreover, we show that second-order tensor representations are required in order to exploit a locally-restricted class of equivariance with respect to the axial rotations \((2)\)j. When computational constraints are a concern, it is possible to recover most of the performance of the Bilinear Tensor Network, by restricting it to a faster Bilinear Vector Network with the appropriate \((2)\) equivariant linear layer.

While the example application demonstrated here is of particular interest to the field of HEP, we expect our method can have great impact in other ares where vector-valued point cloud data is used. Finally, we note that while we demonstrated the modular elements of the TBN on a simple Deep Sets / PFN type network, it should also be possible to use these modules for creating equivariant Graph and attention based networks.

 Model & \(_{70}\) & Impr.(\(_{70}\)) & \(_{85}\) & Impr.(\(_{85}\)) \\   Baseline (PFN) & \(436 15\) & – & \(112 3\) & – \\   Vector PFN & \(1047 85\) & \(140\%\) & \(235 12\) & \(110\%\) \\ 
**Tensor PFN** & \(\) & \(\) & \(\) & \(\) \\ 

Table 1: Test metrics for training experiments on progressive model architectures. \(_{70}\) and \(_{85}\) indicate the test rejection at 70% and 85% signal efficiency, respectively. The percentage relative improvement in these metrics is also shown. Values shown are the median result over at least 10 training runs, per model type; errors quoted on rejection figures are the inter-quartile range.