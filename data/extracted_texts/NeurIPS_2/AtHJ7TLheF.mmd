# Calibrate and Boost Logical Expressiveness of GNN Over Multi-Relational and Temporal Graphs

Dingmin Wang

University of Oxford

dingmin.wang@cs.ox.ac.uk

Equal contribution, listed in alphabetical order.

Yeyuan Chen

University of Michigan

yeyuanch@umich.edu

Equal contribution, listed in alphabetical order.

###### Abstract

As a powerful framework for graph representation learning, Graph Neural Networks (GNNs) have garnered significant attention in recent years. However, to the best of our knowledge, there has been no formal analysis of the logical expressiveness of GNNs as Boolean node classifiers over multi-relational graphs, where each edge carries a specific relation type. In this paper, we investigate \(_{2}\), a fragment of first-order logic with two variables and counting quantifiers. On the negative side, we demonstrate that the R\({}^{2}\)-GNN architecture, which extends the local message passing GNN by incorporating global readout, fails to capture \(_{2}\) classifiers in the general case. Nevertheless, on the positive side, we establish that R\({}^{2}\)-GNN models are equivalent to \(_{2}\) classifiers under certain restricted yet reasonable scenarios. To address the limitations of R\({}^{2}\)-GNN regarding expressiveness, we propose a simple _graph transformation_ technique, akin to a preprocessing step, which can be executed in linear time. This transformation enables R\({}^{2}\)-GNN to effectively capture any \(_{2}\) classifiers when applied to the "transformed" input graph. Moreover, we extend our analysis of expressiveness and _graph transformation_ to temporal graphs, exploring several temporal GNN architectures and providing an expressiveness hierarchy for them. To validate our findings, we implement R\({}^{2}\)-GNN and the _graph transformation_ technique and conduct empirical tests in node classification tasks against various well-known GNN architectures that support multi-relational or temporal graphs. Our experimental results consistently demonstrate that R\({}^{2}\)-GNN with the graph transformation outperform the baseline methods on both synthetic and real-world datasets. The code is available at [https://github.com/hdmmblz/multi-graph](https://github.com/hdmmblz/multi-graph).

## 1 Introduction

Graph Neural Networks (GNNs) have become a standard paradigm for learning with graph structured data, such as knowledge graphs Park et al. (2019); Tena Cucala et al. (2021); Wang et al. (2023) and molecules Hao et al. (2020); Gasteiger et al. (2021); Guo et al. (2021). GNNs take as input a graph where each node is labelled by a feature vector, and then they recursively update the feature vector of each node by processing a subset of the feature vectors from the previous layer. For example, many GNNs update a node's feature vector by combining its value in the previous layer with the output of some aggregation function applied to its _neighbours_' feature vectors in the previous layer; in this case, after \(k\) iterations, a node's feature vector can capture structural information about the node's \(k\)-hop neighborhood. GNNs have proved to be very efficient in many applications like knowledge graph completion and recommender systems. Most previous work on GNNs mainly revolves around findingGNN architectures (e.g. using different aggregation functions or graph-level pooling schemes) which offer good empirical performance Kipf and Welling (2016); Xu et al. (2018); Corso et al. (2020). The theoretical properties of different architectures, however, are not yet well understood.

In Xu et al. (2018), the authors first proposed a theoretical framework to analyze the expressive power of GNNs by establishing a close connection between GNNs and the Weisfeiler-Lehman (1-WL) test for checking graph isomorphism. Similarly, Geerts and Reutter (2022) provides an elegant way to easily obtain bounds on the separation power of GNNs in terms of the Weisfeiler-Leman (k-WL) tests. However, the characterization in terms of the Weisfeiler-Lehman test only calibrates distinguishing ability. It cannot answer _which Boolean node classifier can be expressed by GNNs._ To this end, Barcelo et al. (2020) consider a class of GNNs named ACR-GNNs proposed in Battaglia et al. (2018), where the update function uses a "global" aggregation of the features of all nodes in the graph in addition to the typical aggregation of feature vectors of neighbour nodes. Then, the authors of the paper prove that in the single-relational 2 scenario, ACR-GNNs can capture every Boolean node classifier expressible in the logic \(_{2}\).

However, most knowledge graphs need multiple relation types. For example, in a family tree, there are multiple different relation types such as "father" and "spouse". In this paper, we consider the abstraction of a widely used GNN architecture called R-GCN Schlichtkrull et al. (2018), which is applicable to multi-relational graphs. Following Barcelo et al. (2020), we define R\({}^{2}\)-GNN as a generalization of R-GCN by adding readout functions to the neighborhood aggregation scheme. We show that although adding readout functions enables GNNs to aggregate information of isolated nodes that can not be collected by the neighborhood-based aggregation mechanism, R\({}^{2}\)-GNN are still unable to capture all Boolean node classifiers expressible as formulas in logic \(_{2}\) in multi-relational scenarios if applied "directly" to the input. This leaves us with the following questions: (1) Are there reasonable and practical sub-classes of multi-relational graphs for which \(_{2}\) can be captured by R\({}^{2}\)-GNN? (2) Is there some simple way to encode input graphs, so that all \(_{2}\) node classifiers can be captured by R\({}^{2}\)-GNN for all multi-relational graphs?

In this paper, we provide answers to the above questions. Moreover, we show that our theoretical findings also transfer to temporal knowledge graphs, which are studied extensively in Park et al. (2022) and Gao and Ribeiro (2022). In particular, we leverage the findings from Gao and Ribeiro (2022) which shows that a temporal graph can be transformed into an "equivalent" static multi-relational graph. Consequently, our results, originally formulated for static multi-relational graphs, naturally extend to the domain of temporal knowledge graphs. Our contributions are as follows:

* We calibrate the logic expressiveness of R\({}^{2}\)-GNN as node classifiers over different sub-classes of multi-relational graphs.
* In light of some negative results about the expressiveness of R\({}^{2}\)-GNN found in the multi-relational scenario, there is a compelling need to boost the power of R\({}^{2}\)-GNN. To address this challenge, we propose a _graph transformation_ and show that such a transformation enables R\({}^{2}\)-GNN to capture each classifier expressible as a \(_{2}\) formula in all multi-relational graphs.
* We expand the scope of expressiveness results and graph transformation from static multi-relational graphs to _temporal_ settings. Within this context, we propose several temporal GNN architectures and subject them to a comparative analysis with frameworks outlined in Gao and Ribeiro (2022). Ultimately, we derive an expressiveness hierarchy.

## 2 Preliminaries

### Multi-relational Graphs

A _multi-relational_ graph is a \(4\)-tuple \(G=(V,,P_{1},P_{2})\), where \(V\), \(P_{1}\), \(P_{2}\) are finite sets of _nodes_, _types_ and _relations_ (a.k.a, unary/binary predicates)3, respectively, and \(\) is a set of triples of the form \((v_{1},p_{2},v_{2})\) or \((v,type,p_{1})\), where \(p_{1} P_{1}\), \(p_{2} P_{2}\), \(v_{1},v_{2},v V\), and \(type\) is a special symbol.

Next, given arbitrary (but fixed) finite sets \(P_{1}\) and \(P_{2}\) of unary and binary predicates, respectively, we define the following three kinds of graph classes:

[MISSING_PAGE_FAIL:3]

In the definitions (Equations (1) and (2)), the functions can be set as any functions, such as matrix multiplications or QKV-attentions. Most commonly used GNN such as R-GCN (Schlichtkrull et al. ) and R-GAT (Busbridge et al. ) are captured (upper-bounded) within our R-GNN frameworks. Other related works, such as (Barcelo et al. , Huang et al. , Qiu et al. ) also use intrinsically the same framework as our R-GNN/R\({}^{2}\)-GNN, which has been widely adopted and studied within the GNN community. We believe that analyzing these frameworks can yield common insights applicable to numerous existing GNNs

GNN-based Boolean node classifierIn order to translate the output of a GNN to a Boolean value, we apply a Boolean classification function \(CLS:^{d}\{true,false\}\), where \(d\) is the dimension of the feature vectors \(_{v}^{L}\). Hence, a Boolean node classifier based on an R\({}^{2}\)-GNN \(\) proceeds in three steps: (1) encode the input multi-relational graph \(G\) as described above, (2) apply the R\({}^{2}\)-GNN, and (3) apply \(CLS\) to the output of the R\({}^{2}\)-GNN. This produces a \(true\) or \(false\) value for each node of \(G\). In what follows, we abuse the language and represent a family of GNN-based Boolean node classifiers by the name of the corresponding GNN architecture; for example, R\({}^{2}\)-GNN is the set of all R\({}^{2}\)-GNN-based Boolean node classifiers.

### Logic \(_{2}\) Formulas

In this paper, we focus on the logic \(_{2}\), a fragment of first-order logic that only allows formulas with at most two variables, but in turn permits to use _counting quantifiers_. Formally, given two finite sets \(P_{1}\) and \(P_{2}\) of _unary_ and _binary_ predicates, respectively, a \(_{2}\) formula \(\) is inductively defined according to the following grammar:

\[::=A(x) r(x,y) ^{ n}y()A P_{1}r P_{2} \]

where \(x/y\) in the above rules can be replaced by one another. But please note that \(x\) and \(y\) are the only variable names we are allowed to use (Though we can reuse these two names). In particular, a \(_{2}\) formula \(\) with exactly one free variable \(x\) represents a Boolean node classifier for multi-relational graphs as follows: a node \(v\) is assigned to \(true\) iff the formula \(_{v}\) obtained by substituting \(x\) by \(v\) is satisfied by the (logical) model represented by the multi-relational graph. Similarly as the GNN-based Boolean node classifiers, in what follows, we abuse the language and represent the family of \(_{2}\) Boolean node classifiers by its name \(_{2}\).

### Inclusion and Equality Relationships

In this paper, we will mainly talk about inclusion/non-inclusion/equality/strict-inclusion relationships between different node classifier families on certain graph classes. To avoid ambiguity, we give formal definitions of these relationships here. These definitions are all quite natural.

**Definition 2**.: _For any two sets of node classifier \(A,\)\(B\), and graph class \(\), We say:_

* \(A B\) _on_ \(\)_, iff for any node classifier_ \(a A\)_, there exists some node classifier_ \(b B\) _such that for all graph_ \(G\) _and_ \(v V(G)\)_, it satisfies_ \(a(G,v)=b(G,v)\) _(Namely,_ \(a\) _and_ \(b\) _evaluate the same for all instances in_ \(\)_). It implies_ \(B\) _is more expressive than_ \(A\) _on_ \(\)_._
* \(A B\) _on_ \(\)_, iff the above condition in item_ 1 _doesn't hold._
* \(A B\) _on_ \(\)_, iff_ \(A B\) _but_ \(B A\)_. It implies_ \(B\) _is_ _strictly more expressive than_ \(A\) _on_ \(\)_._
* \(A=B\) _on_ \(\)_, iff_ \(A B\) _and_ \(B A\)_. It implies_ \(A\) _and_ \(B\) _has the same expressivity on_ \(\)_._

## 3 Related Work

The relationship between first-order logic and the Weisfeiler-Lehman test was initially established by Cai et al. . Subsequently, more recent works such as Xu et al. , have connected the Weisfeiler-Lehman test with expressivity of GNN. This line of research has been followed by numerous studies, including Maron et al. , which explore the distinguishability of GNNs using the Weisfeiler-Lehman test technique. In particular, Barcelo et al.  introduced the calibration of logical expressivity in GNN-based classifiers and proposed a connection between \(_{2}\) and R\({}^{2}\)-GNN in single-relational scenario. This led to the emergence of related works, such as Huang et al. , Geerts and Reutter , and Qiu et al. , all of which delve into the logical expressivity of GNNs. Moreover, the theoretical analysis provided in Gao and Ribeiro  has inspired us to extend our results to temporal graph scenarios.

## 4 Logic expressiveness of \(^{2}\)-GNN in multi-relational graphs

Our analysis begins with the observation that certain Boolean classifiers can be represented as \(_{2}\) formulas, but remain beyond the expressiveness of any \(^{2}\)-GNN (and consequently, any R-GNN or R-GCN). An illustrative example of this distinction is provided in Figure 1. In this example, we make the assumption that \(P_{1}\) is empty, thereby ensuring that all nodes in both \(G_{1}\) and \(G_{2}\) possess identical initial feature vectors. Additionally, \(P_{2}\) is defined to comprise precisely two relations, namely, \(p_{1}\) and \(p_{2}\). It is evident that no \(^{2}\)-GNN can distinguish the node \(a\) in \(G_{1}\) from node \(a\) in \(G_{2}\) - that is, when an \(^{2}\)-GNN performs the neighbour-based aggregation, it cannot distinguish whether the \(p_{1}\)-neighbour of \(a\) and the \(p_{2}\)-neighbour of \(a\) are the same. Moreover, the global readout aggregation cannot help in distinguishing those nodes because all nodes have the same feature vector.

We proceed to formalize this intuition and, in the reverse direction, offer a corresponding result. We demonstrate that there exist Boolean classifiers that fall within the scope of \(^{2}\)-GNN but elude capture by any \(_{2}\) formula.

**Proposition 3**.: \(_{2}^{2}\)_-GNN and \(^{2}\)-GNN \(_{2}\) on some universal graph class._

We prove Proposition 3 in the Appendix. Here, we give some intuition about the proof. The first result is proved using the example shown in Figure 1, which we have already discussed. To show \(^{2}\)-GNN \(_{2}\), we construct a classifier \(c\) which classifies a node into true iff _the node has a larger number of \(r_{1}\)-type neighbors than that of \(r_{2}\)-type neighbors_. We can prove that we can easily construct an \(^{2}\)-GNN to capture \(c\). However, for \(_{2}\), this cannot be done, since we can only use counting quantities expressing that there exist at most or at least a specific number of neighbours connected via a particular relation, but our target classifier requires comparing indefinite numbers of neighbours via two relations. Thus, we proceed by contradiction, assume that there exists a \(_{2}\) classifier equivalent to \(c\), and then find two large enough graphs with nodes that cannot be distinguished by the classifier (but can be distinguished by \(c\)).

In some real-world applications, it is often possible to find an upper bound on the size of any possible input graph or to ensure that any input graph will contain at most one relation between every two nodes. For this reason, we next present restricted but positive&practical expressiveness results on bounded and simple graph classes.

**Theorem 4**.: \(_{2}^{2}\)_-GNN on any simple graph class, and \(_{2}^{2}\)-GNN on some simple graph class._

The key idea of the construction is that we will first transform the \(_{2}\) formula into a new form which we call _relation-specified_\(_{2}\) (an equivalent form to \(_{2}\), see more details in our Appendix), and then we are able to construct an equivalent \(^{2}\)-GNN inductively over the parser tree of the transformed formula.

Having Theorem 4, one may wonder about the inclusion relationship of \(^{2}\)-GNN and \(_{2}\) in the backward direction. In Proposition 3, we showed that for arbitrary universal graph classes, this inclusion relationship fails. However, given a bounded graph class, we can show that for each \(^{2}\)-GNN Boolean node classifier, one can write an equivalent \(_{2}\) classifier. An intuition about why this is the case is that all graphs in a bounded graph class will have at most \(n\) constants, for some known \(n\), so for each \(^{2}\)-GNN classifier, we can construct an equivalent \(_{2}\) classifier with a finite number of sub-formulas to recover the features obtained at different layers of \(^{2}\)-GNN.

**Theorem 5**.: \(^{2}\)_-GNN \(_{2}\) on any bounded graph class, and \(^{2}\)-GNN \(_{2}\) on some bounded graph class._

Figure 1: Multi-edge graphs \(G_{1}\) and \(G_{2}\), and a \(_{2}\) formula \((x)\) that distinguishes them; \((x)\) evaluates node \(a\) in \(G_{1}\) to \(true\) and node \(a\) in \(G_{2}\) to \(false\).

Combining Theorem 4 and Theorem 5, we have the following corollary.

**Corollary 5.1**.: \(R^{2}\)_-\(G\!N\!N=_{2}\) on any bounded simple graph class._

At Last, one may be curious about the complexity of logical classifier in Theorem 5. Here we can give a rather loose bound as follows:

**Theorem 6**.: _For any bounded graph class \(_{b}\). Suppose any \(G_{b}\) has no more than \(N\) nodes, and \(_{b}\) has unary predicate set \(P_{1}\) and relation (binary predicate) set \(P_{2}\). Let \(m_{1}:=|P_{1}|,\)\(m_{2}:=|P_{2}|\), then for any node classifier \(c\), suppose \(c\) can be represented as an \(R^{2}\)-GNN with depth (layer number) \(L\), then by Theorem 5 there is a \(_{2}\) classifier \(\) equivalent to \(c\) over \(_{b}\), and the following hold:_

* _The quantifier depth of_ \(\) _is no more than_ \(L\)_._
* _The size of_ \(\) _(quantified by the number of nodes of_ \(\)_'s parse tree) is no more than_ \(2^{2f(L)}\)_, where_ \(f(L):=2^{2^{2(N+1)f(L-1)}}\)_,_\(f(0)=O(2^{2^{2(m_{1}+m_{2})}})\)_._

The key idea of Theorem 6 is the following: First, by Lemma 27 in our appendix, the combination of **ALL**\(_{2}\) logical classifiers with quantifier depth no more than \(L\) can already distinguish accepting and rejecting instances of \(c\). Then by Proposition 26 (This is a key point of this bound; please refer to our appendix), We know the number of intrinsically different bounded-depth \(_{2}\) classifiers is finite, so we only need to get an upper bound on this number. Finally, we can get the desired bound by iteratively using the fact that a boolean combination of a set of formulas can be always written as DNF (disjunctive normal form). The tower of power of two comes from \(L\) rounds of DNF enumerations. Although the bound seems scary, it is a rather loose bound. We give a detailed proof of Theorem 6 in the appendix along with the proof of Theorem 5.

## 5 R\({}^{2}\)-GNN capture \(_{2}\) over transformed multi-relational graphs

As we pointed out in the previous section, one of the reasons why R\({}^{2}\)-GNN cannot capture \(_{2}\) classifiers over arbitrary universal graph classes is that in multi-relational graphs, they cannot distinguish whether information about having a neighbour connected via a particular relation comes from the same neighbour node or different neighbour nodes. Towards solving this problem, we propose a _graph transformation_\(F\) (see Definition 7), which enables R\({}^{2}\)-GNN to capture all \(_{2}\) classifiers on multi-relational graphs. Similar transformation operations have also been used and proved to be an effective way to encode multi-relational graphs in previous studies, e.g., MGNNs Tena Cucala et al. (2021), Indigo Liu et al. (2021) and Time-then-Graph Gao and Ribeiro (2022).

**Definition 7**.: _Given a multigraph \(G=(V,,P_{1},P_{2})\), the transformation \(F\) will map \(G\) to another graph \(F(G)=(V^{},^{},P^{}_{1},P^{}_{2})\) with changes described as follows:_

* _for any two nodes_ \(a,b V\)_, if there exists at least one relation_ \(p P_{2}\) _between_ \(a\) _and_ \(b\)_, we add two new nodes_ \(ab\) _and_ \(ba\) _to_ \(V^{}\)_._
* _we add a new unary predicate {_primal_} and two new binary predicates {_aux1,aux2_}. Hence,_ \(F(P_{1}):=P^{}_{1}=P_{1}\{\}\)_, and_ \(F(P_{2}):=P^{}_{2}=P_{2}\{\}\)_. For each node_ \(v^{} V^{}\)_,_ \((v^{})=1\) _iff_ \(v^{}\) _is also in_ \(V\)_; otherwise,_ \((v^{})=0\)_;_
* _for each triplet of the form_ \((a,\!p_{2},b)\) _in_ \(\)_, we add to_ \(^{}\) _four new triples:_ \((ab,\!,a)\)_,_ \((ba,\!,b)\) _and_ \((ab,\!,ba)\) _as well as_ \((ab,\!p_{2},ba)\)_._

An example is in Figure 2. We can see that after applying the _graph transformation_, we need to execute two more hops to propagate information from node \(a\) to node \(b\). However, now we are able to distinguish whether the information about different relations comes from the same node or different nodes. This transformation can be implemented and stored in linear time/space complexity \(O(|V|+||)\), which is very efficient.

Figure 2: Graph Transformation.

**Definition 8**.: _Given a classifier \(\) and a transformation function \(F\), we define \( F\) to be a new classifier, an extension of \(\) with an additional transformation operation on the input graph._

With _graph transformation_\(F\), we get a more powerful class of classifiers than R\({}^{2}\)-GNN. We analyze the logical expressiveness of R\({}^{2}\)-GNN \( F\) in multi-relational graphs, which means first transform a graph \(G\) to \(F(G)\) and then run an R\({}^{2}\)-GNN on \(F(G)\). We will see in the following that this transformation \(F\) boosts the logical expressiveness of R\({}^{2}\)-GNN prominently.

**Theorem 9**.: \(R^{2}\)_-GNN \( R^{2}\)-GNN \( F\) on any universal graph class._

**Theorem 10**.: \(_{2} R^{2}\)_-GNN \( F\) on any universal graph class._

Theorem 9 demonstrates that R\({}^{2}\)-GNN with _graph transformation_\(F\) have more expressiveness than R\({}^{2}\)-GNN; and Theorem 10 shows the connection between \(_{2}\) and R\({}^{2}\)-GNN equipped with _graph transformation_\(F\). We depict their relations in Figure 3. Theorem 9 is a natural result since no information is lost in the process of transformation, while Theorem 10 is an extension on Theorem 4, whose formal proofs can be found in the Appendix. As for the backward direction, we have the result shown in Theorem 11.

**Theorem 11**.: \(R^{2}\)_-GNN \( F_{2}\) on any bounded graph class._

The proof of the theorem is relatively straightforward based on previous results: by Theorem 5, it follows that R\({}^{2}\)-GNN \( F_{2} F\) on any bounded graph class. Then, it suffices to prove \(_{2} F_{2}\), which we do by using induction over the quantifier depth.

By combining Theorem 10 and Theorem 11, we obtain Corollary 11.1, stating that \(_{2}\) and R\({}^{2}\)-GNN \( F\) have the same expressiveness with respect to bounded graph classes. Corollary 11.1 does not hold for arbitrary universal graph classes, but our finding is nevertheless exciting because, in many real-world applications there are upper bounds over input graph size.

**Corollary 11.1**.: \(R^{2}\)_-GNN \( F=_{2}\) on any bounded graph class._

To show the strict separation as in Figure 3, we can combine Proposition 3 and theorems 4 and 9 and Theorem 10 to directly get the following:

**Corollary 11.2**.: \(R^{2}\)_-GNN \( R^{2}\)-GNN \( F\) on some universal graph class, and \(_{2} R^{2}\)-GNN \( F\) on some simple graph class._

One may think after transformation \(F\), the logic \(_{2} F\) with new predicateds becomes stronger as well. However by a similar proof as for Theorem 10 and Lemma 28, we can actually show \(_{2} F_{2}\) always holds, so \(F\) won't bring added power for \(_{2}\). However, it indeed make R\({}^{2}\)-GNN strictly more expressive.

## 6 Temporal Graphs

As stated in Gao and Ribeiro (2022), a temporal knowledge graph, composed of multiple snapshots, can consistently undergo transformation into an equivalent static representation as a multi-relational graph. Consequently, this signifies that our theoretical results initially devised for multi-relational graphs can be extended to apply to temporal graphs, albeit through a certain manner of transfer.

Following previous work Jin et al. (2019); Pareja et al. (2020); Park et al. (2022); Gao and Ribeiro (2022), we define a temporal knowledge graph as a set of graph "snapshots" distributed over a sequence of **finite** and **discrete** time points \(\{1,2,,T\}\). Formally, a temporal knowledge graph is a set \(G=\{G_{1},,G_{T}\}\) for some \(T\), where each \(G_{t}\) is a static multi-relational graph. All these \(G_{t}\) share the same node set and predicate set.

In a temporal knowledge graph, a relation or unary fact between two nodes might hold or disappear across the given timestamps. For example, a node \(a\) may be connected to a node \(b\) via a relation \(p\) in the first snapshot, but not in the second; in this case, we have \((a,p,b)\) in \(G_{1}\) not in \(G_{2}\). To keep track of which relations hold at which snapshots, we propose _temporal predicates_, an operation which we define in Definition 12.

**Definition 12**.: _Given a temporal graph \(G=\{G_{1},,G_{T}\}\), where each \(G_{t}\) is of the form \((V_{t},_{t},P_{1},P_{2})\), temporal predicates are obtained from \(G\) by replacing, for each \(t\{1,,T\}\) and each \(p P_{2}\), each triple \((v_{a},p,v_{b})_{t}\) with \((v_{a},p^{t},v_{b})\), where \(p^{t}\) is a fresh predicate, unique for \(p\) and \(t\). Similarly, each unary fact \((v_{a},q)_{t},q P_{1}\) should be replaced by \((v_{a},q^{t})\)._

Note that temporalising introduces \(T|P|\) new predicates in total. By _temporalizing predicates_, we assign a superscript to each predicate and use it to distinguish relations over different timestamps.

**Definition 13**.: _Given a temporal knowledge graph \(G=\{G_{1},,G_{T}\}\), the collapse function \(H\) maps \(G\) to the static graph \(H(G)\) obtained by taking the union of graphs over all timestamps in the temporalization of \(G\)._

As we have proved in Section 5, for multi-relational graphs, R\({}^{2}\)-GNN with _graph transformation_ is more powerful than the pure R\({}^{2}\)-GNN. Here, we transfer these theoretical findings in multi-relational graphs to the setting of temporal knowledge graphs. To be more specific, after _temporalizing predicates_, we apply a _graph transformation_ to each graph snapshot.

**Definition 14**.: _We define \(F^{T}\) to be the temporal graph transformation that takes any temporal knowledge graph as input, applies graph transformation to each snapshot and outputs. Specially, non-primal nodes, aux1 and aux2 edges added in any snapshot should be added into all snapshots._

R\({}^{2}\)-TgnnGao and Ribeiro (2022) casts node representation in temporal graphs into two frameworks: _time-and-graph_ and _time-then-graph_. Due to space constraints, we refer interested readers to Gao and Ribeiro (2022) for more details about the two frameworks. Here, we define a more general GNN-based framework abbreviated as R\({}^{2}\)-TGNN, where each R\({}^{2}\)-TGNN is a sequence \(\{_{t}\}_{t=1}^{T}\), where each \(_{t}\) is an R\({}^{2}\)-GNN model.Given a temporal knowledge graph \(G=\{G_{1},,G_{T}\}\), where \(G_{t}=(V_{t},_{t},P_{1},P_{2})\) for each \(t\{1,,T\}\). The updating rule is as follows:

\[_{v}^{t}=_{t}G_{t},\!v,\!^{t} _{v}^{t}=[I_{G_{t}}(v):_{v}^{t-1}],  v V(G_{t}) \]

where \(I_{G_{t}}(v)\) is the one-hot initial feature vector of node \(v\) at timestamp \(t\), and \(_{t}(G_{t},\!v,\!^{t})\) calculates the new feature vector of \(v\) by running the R\({}^{2}\)-GNN model \(_{t}\) on \(G_{t}\), but using \(^{t}\) as the initial feature vectors. As shown in Theorem 15, R\({}^{2}\)-TGNN composed with \(F^{T}\) have the same expressiveness as _time-then-graph4_, while being more powerful than _time-and-graph_.

**Theorem 15**.: _time-and-graph \(\) R\({}^{2}\)-TGNN \( F^{T}=\) time-then-graph._

We also establish the validity of Theorem 16, which asserts that R\({}^{2}\)-TGNN with _graph transformation_ maintains the same expressive power, whether it is applied directly to the temporal graph or to the equivalent collapsed static multi-relational graph

**Theorem 16**.: \(R^{2}\)_-TGNN \( F^{T}=\) R\({}^{2}\)-GNN \( F H\)_

We also prove a strict inclusion that R\({}^{2}\)-TGNN \(\) R\({}^{2}\)-TGNN\( H\). Finally we get the following hierarchy of these frameworks as in Figure 6. the proof of Theorem 17 is in the appendix.

**Theorem 17**.: _The following hold:_

* \(R^{2}\)_-GNN_ \(\)__\(R^{2}\)_-GNN_ \( H\)__\(R^{2}\)_-TGNN_ \( F H\)_=_ \(R^{2}\)_-TGNN_ \( F^{T}\)_= time-then-graph._
* \(time\)_-and-graph_ \(\)__\(R^{2}\)_-TGNN_ \( F^{T}\)_._
* \(R^{2}\)_-TGNN_ \(\)__\(time\)_-and-graph._

Figure 4: Hierarchic expressiveness.

## 7 Experiment

We empirically verify our theoretical findings for multi-relational graphs by evaluating and comparing the testing performance of R\({}^{2}\)-GNN with _graph transformation_ and less powerful GNNs (R-GNN and R\({}^{2}\)-GNN). We did two groups of experiments on synthetic datasets and real-world datasets, respectively. Details for datasets generation and statistical information as well as hyper-parameters can be found in the Appendix.

### Synthetic Datasets

We first define three simple \(_{2}\) classifiers

\[_{} ^{ 2}y(p_{1}^{1}(x,y) Red^{1}(y)) ^{ 1}y(p_{1}^{2}(x,y) Blue^{2}(y))\] \[_{} ^{}y( p_{1}^{2}(x,y)_{1} (y))_{}^{ 2}y(p_{1}^{1}(x,y)  p_{1}^{2}(x,y))\]

Besides, we define another complicate \(_{2}\) classifier denoted as \(_{4}\) shown as follows:

\[_{}_{3 t 10}(^{  2}y(Black^{t}(y) Red^{t-1}(y) Blue^{t-2}(y)) p_{1}^{t}(x,y)  p_{2}^{t-1}(x,y) p_{3}^{t-2}(x,y)^{t}(y))\] \[^{t}(y)^{ 2}x(p_{1}^{t} (x,y) Red^{t}(x))^{ 1}x(p_{2}^{t-1}(x,y) Blue^{t-2}(x))\]

For each of them, we generate an independent dataset containing 7k multi-relational graphs of size up to 50-1000 nodes for training and 500 multi-relational graphs of size similar to the train set. We tried different configurations for the aggregation functions and evaluated the node classification performances of three temporal GNN methods (R-TGNNs, R\({}^{2}\)-TGNNs and R\({}^{2}\)-TGNNs \( F^{T}\)) on these datasets.

We verify our hypothesis empirically according to models' actual performances of fitting these three classifiers. Theoretically, \(_{1}\) should be captured by all three models because the classification result of a node is decided by the information of its neighbor nodes, which can be accomplished by the general neighborhood based aggregation mechanism. \(_{2}\) should not be captured by R-TGNN because the use of \( p_{1}^{2}(x,y)\) as a guard means that the classification result of a node depends on the global information including those isolated nodes, which needs a global readout. For \(_{3}\) and \(_{4}\), they should only be captured by R\({}^{2}\)-TGNNs \( F^{T}\). An intuitive explanation for this argument is that if we _temporalise predicates_ and then collapse the temporal graph into its equivalent static multi-relational graph using \(H\), we will encounter the same issue as in the Figure 1. Thus we can't distinguish expected nodes without _graph transformation_.

Results for temporal GNN methods and static GNN methods on four synthetic datasets can be found in Table 1. We can see that R\({}^{2}\)-GNN with _graph transformation_ achieves the best performance. Our theoretical findings show that it is a more expressive model, and the experiments indeed suggest that the model can exploit this theoretical expressiveness advantage to produce better results. Besides, we can also see that R\({}^{2}\)-TGNN \( F H\) and R\({}^{2}\)-TGNN \( F^{T}\) achieve almost the same performance, which is in line with Theorem 16.

   \(_{2}\) classifier & }\)} & }\)} & }\)} & }\)} \\ Aggregation & sum & max & mean & sum & max & mean & sum & max & mean & sum & max & mean \\   \\  R-TGNN & 100 & 60.7 & 65.4 & 61.0 & 51.3 & 52.4 & 93.7 & 82.3 & 84.4 & 83.5 & 60.0 & 61.3 \\ R\({}^{2}\)-TGNN & 100 & 63.5 & 66.8 & 93.1 & 57.7 & 60.2 & 94.5 & 83.3 & 85.9 & 85.0 & 62.3 & 66.2 \\ R\({}^{2}\)-TGNN \( F^{T}\) & **100** & 67.2 & 68.1 & **99.0** & 57.6 & 62.2 & **100** & 88.8 & 89.2 & **98.1** & 73.4 & 77.5 \\  

### Real-world Datasets

For real-world static multi-relational graphs benchmarks, we used AIFB and MUTAG from Ristoski and Paulheim (2016). Since open source datasets for the node classification on temporal knowledge graphs are rare, we only tried one dataset Brain-10 Gao and Ribeiro (2022) for temporal settings.5

For static multi-relational graphs, we compare the performances of our methods with RGCN Schlichtkrull et al. (2018). Note that RGCN assigns each node an index and the initial embedding of each node is initialised based on the node index, so the initialisation functional is not permutation-equivariant Chen et al. (2019) and RGCN cannot be used to perform an isomorphism test. However, from Table 3, we can see that R\({}^{2}\)-GNN with _graph transformation_ still achieves the highest accuracy while being able to be used for the graph isomorphism test. Besides, R\({}^{2}\)-GNN \( F\) also performs better compared with both R-GNN and R\({}^{2}\)-GNN. This again suggests that the extra expressive power gained by adding a _graph transformation_ step to R\({}^{2}\)-GNN can be exploited by the model to obtain better results.

For temporal graphs, Gao and Ribeiro (2022) have classified existing temporal models into two categories, _time-and-graph_ and _time-then-graph_, and shown that _time-then-graph_ models have better performance. We choose five models mentioned in Gao and Ribeiro (2022) as our baseline and include the best accuracy of the dataset Brain-10 reported in Gao and Ribeiro (2022). As we expected, R\({}^{2}\)-TGNNand R\({}^{2}\)-TGNN \( F^{T}\) achieve better performance than that of the baseline models and R-TGNN scoring to Table 2. However, we observed that although in theory, R\({}^{2}\)-TGNN \( F^{T}\) has stronger expressive power than R\({}^{2}\)-TGNN, we did not see an improvement when using R\({}^{2}\)-TGNN \( F^{T}\) (\(0.8\%\) accuracy drop). To some extent, it may show that some commonly used benchmarks are inadequate for testing advanced GNN variants. Similar phenomena have also been observed in previous works Chen et al. (2019), Barcelo et al. (2020).

## 8 Conclusion

We analyze expressivity of R\({}^{2}\)-GNNs with and without _graph transformation_ in multi-relational graphs under different situations. Furthermore, we extend our theoretical findings to the temporal graph setting. Our experimental results confirm our theoretical insights, particularly demonstrating the state-of-the-art performance achieved by our _graph transformation_ technique.

    &  &  \\  & sum & max & mean & sum & max & mean \\  R-GNN & **91.7** & 73.8 & 82.5 & **76.5** & 63.3 & 73.2 \\ R\({}^{2}\)-GNN & **91.7** & 73.8 & 82.5 & **85.3** & 62.1 & 79.5 \\ R\({}^{2}\)-GNN \( F\) & **97.2** & 75.0 & 89.2 & **88.2** & 65.5 & 82.1 \\ R-GCN & **95.8** & 77.9 & 86.3 & **73.2** & 65.7 & 72.1 \\   

Table 3: Results on two static multi-relational graphs.

    &  &  &  \\  & & & sum & max & mean \\  GCRN-M2 & time-and-graph & Seo et al. (2018) & 77.0 & 61.2 & 73.1 \\ DCRNN & time-and-graph & Li et al. (2018) & 84.0 & 70.1 & 66.5 \\ TGAT & time-then-graph & Xu et al. (2020) & 80.0 & 72.3 & 79.0 \\ TGN & time-then-graph & Rossi et al. (2020) & 91.2 & **88.5** & 89.2 \\ GRU-GCN & time-then-graph & Gao and Ribeiro (2022) & 91.6 & 88.2 & 87.1 \\  R-TGNN & – & – & 85.0 & 82.3 & 82.8 \\ R\({}^{2}\)-TGNN & – & – & **94.8** & 82.3 & 91.0 \\ R\({}^{2}\)-TGNN \( F^{T}\) & – & – & 94.0 & 83.5 & **92.5** \\   

Table 2: Results on temporal graphs.

Acknowledgements

The authors extend their gratitude to Bernardo Cuenca Grau and David Tena Cucala for their valuable insights, stimulating discussions, and support.