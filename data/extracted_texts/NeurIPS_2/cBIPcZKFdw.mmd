# Strategic Apple Tasting

Keegan Harris

Carnegie Mellon University

Pittsburgh, PA 15213

keeganh@cmu.edu &Chara Podimata

MIT & Archimedes/Athena RC

Cambridge, MA 02142

podimata@mit.edu &Zhiwei Steven Wu

Carnegie Mellon University

Pittsburgh, PA 15213

zstevenwu@cmu.edu

###### Abstract

Algorithmic decision-making in high-stakes domains often involves assigning _decisions_ to agents with _incentives_ to strategically modify their input to the algorithm. In addition to dealing with incentives, in many domains of interest (e.g. lending and hiring) the decision-maker only observes feedback regarding their policy for rounds in which they assign a positive decision to the agent; this type of feedback is often referred to as _apple tasting_ (or _one-sided_) feedback. We formalize this setting as an online learning problem with apple-tasting feedback where a _principal_ makes decisions about a sequence of \(T\)_agents_, each of which is represented by a _context_ that may be strategically modified. Our goal is to achieve sublinear _strategic regret_, which compares the performance of the principal to that of the best fixed policy in hindsight, _if the agents were truthful when revealing their contexts_. Our main result is a learning algorithm which incurs \(}()\) strategic regret when the sequence of agents is chosen _stochastically_. We also give an algorithm capable of handling _adversarially-chosen_ agents, albeit at the cost of \(}(T^{(d+1)/(d+2)})\) strategic regret (where \(d\) is the dimension of the context). Our algorithms can be easily adapted to the setting where the principal receives _bandit_ feedback--this setting generalizes both the linear contextual bandit problem (by considering agents with incentives) and the strategic classification problem (by allowing for partial feedback).

## 1 Introduction

Algorithmic systems have recently been used to aid in or automate decision-making in high-stakes domains (including lending and hiring) in order to, e.g., improve efficiency or reduce human bias . When subjugated to algorithmic decision-making in high-stakes settings, individuals have an incentive to _strategically_ modify their observable attributes to appear more qualified. Such behavior is often observed in practice. For example, credit scores are often used to predict the likelihood an individual will pay back a loan on time if given one. Online articles with titles like _"9 Ways to Build and Improve Your Credit Fast"_ are ubiquitous and offer advice such as "pay credit card balances strategically" in order to improve one's credit score with minimal effort . In hiring, common advice ranges from curating a list of keywords to add to one's resume, to using white font in order to "trick" automated resume scanning software . If left unaccounted for, such strategic manipulations could result in individuals being awarded opportunities for which they are not qualified for, possibly at the expense of more deserving candidates. As a result, it is critical to keep individuals' incentives in mind when designing algorithms for learning and decision-making in high-stakes settings.

In addition to dealing with incentives, another challenge of designing learning algorithms for high-stakes settings is the possible _selection bias_ introduced by the way decisions are made. In particular, decision-makers often only have access to feedback about the deployed policy from individuals that have received positive decisions (e.g., the applicant is given the loan, the candidate is hired to the job and then we can evaluate how good our decision was). In the language of online learning, this type of feedback is known as _apple tasting_ (or _one-sided_) feedback. _When combined, these two complications (incentives & one-sided feedback) have the potential to amplify one other, as algorithms can learnonly when a positive decision is made, but individuals have an incentive to strategically modify their attributes in order to receive such positive decisions, which may interfere with the learning process._

### Contributions

We formalize our setting as a game between a _principal_ and a sequence of \(T\)_strategic agents_, each with an associated _context_\(_{t}\) which describes the agent. At every time \(t\{1,,T\}\), the principal deploys a _policy_\(_{t}\), a mapping from contexts to binary _decisions_ (e.g., whether to accept/reject a loan applicant). Given policy \(_{t}\), agent \(t\) then presents a (possibly modified) context \(_{t}^{}\) to the algorithm, and receives a decision \(a_{t}=_{t}(_{t}^{})\). If \(a_{t}=1\), the principal observes _reward_\(r_{t}(a_{t})=r_{t}(1)\); if \(a_{t}=0\) they receive no feedback. \(r_{t}(0)\) is assumed to be known and constant across rounds.1 Our metric of interest is _strategic regret_, i.e., regret with respect to the best fixed policy in hindsight, _if agents were truthful when reporting their contexts_.

Our main result is an algorithm which achieves \(()\) strategic regret (with polynomial per-round runtime) when there is sufficient randomness in the distribution over agents (Algorithm 1). At a high level, our algorithm deploys a linear policy at every round which is appropriately shifted to account for the agents' strategic behavior. We identify a _sufficient_ condition under which the data received by the algorithm at a given round is "clean", i.e. has not been strategically modified. Algorithm 1 then online-learns the relationship between contexts and rewards by only using data for which it is sure is clean.

In contrast to performance of algorithms which operate in the non-strategic setting, the regret of Algorithm 1 depends on an exponentially-large constant \(c(d,)(1-)^{-d}\) due to the one-sided feedback available for learning, where \(d\) is the context dimension and \((0,1)\) is a parameter which represents the agents' ability to manipulate. While this dependence on \(c(d,)\) is insignificant when the number of agents \(T\) (i.e. is very large), it may be problematic for the principal whenever \(T\) is either small or unknown. To mitigate this issue, we show how to obtain \((d T^{2/3})\) strategic regret by playing a modified version of the well-known _explore-then-commit_ algorithm (Algorithm 2). At a high level, Algorithm 2 "explores" by always assigning action \(1\) for a fixed number of rounds (during which agents do not have an incentive to strategize) in order to collect sufficient information about the data-generating process. It then "exploits" by using this data learn a strategy-aware linear policy. Finally, we show how to combine Algorithm 1 and Algorithm 2 to achieve \((\{c(d,),d T^{2/3}\})\) strategic regret whenever \(T\) is unknown.

While the assumption of stochastically-chosen agents is well-motivated in general, it may be overly restrictive in some specific settings. Our next result is an algorithm which obtains \((T^{(d+1)/(d+2)})\) strategic regret when agents are chosen _adversarially_ (Algorithm 4). Algorithm 4 uses a variant of the popular Exp3 algorithm to trade off between a carefully constructed set of (exponentially-many) policies . As a result, it achieves sublinear strategic regret when agents are chosen adversarially, but requires an exponentially-large amount of computation at every round.

Finally, we note that while our primary setting of interest is that of one-sided feedback, all of our algorithms can be easily extended to the more general setting in which the principal receives _bandit feedback_ at each round, i.e. \(r_{t}(0)\) is not constant and must be learned from data. To the best of our knowledge, we are the first to consider strategic learning in the contextual bandit setting.

### Related work

**Strategic responses to algorithmic decision-making** There is a growing line of work at the intersection of economics and computation on algorithmic decision-making with incentives, under the umbrella of _strategic classification_ or _strategic learning_ focusing on online learning settings , causal learning , incentivizing desirable behavior , incomplete information . In its most basic form, a principal makes either a binary or real-valued prediction about a strategic agent, and receives _full feedback_ (e.g., the agent's _label_) after the decision is made. While this setting is similar to ours, it crucially ignores the one-sided feedback structure present in many strategic settings of interest. In our running example of hiring, full feedback would correspond to a company not offering an applicant a job, and yet still getting to observe whether they would have been a good employee! As a result, such methods are not applicable in our setting. Concurrent work  studies the effects of bandit feedback in the related problem of _performative prediction_, which considers data distribution shifts at the _population level_ in response to the deployment of a machine learning model. In contrast, our focus is on strategic responses to machine learning models at the _individual level_ under apple tasting and bandit feedback. Ahmadi et al.  study an online strategic learning problem in which they consider "bandit feedback" _with respect to the deployed classifier_. In contrast, we use the term "bandit feedback" to refer to the fact that we only see the outcome when for the action/decision taken.

**Apple tasting and online learning** Helmbold et al.  introduce the notion of apple-tasting feedback for online learning. In particular, they study a binary prediction task over "instances" (e.g., fresh/rotten apples), in which a positive prediction is interpreted as accepting the instance (i.e. "tasting the apple") and a negative prediction is interpreted as rejecting the instance (i.e., _not_ tasting the apple). The learner only gets feedback when the instance is accepted (i.e., the apple is tasted). While we are the first to consider classification under incentives with apple tasting feedback, similar feedback models have been studied in the context of algorithmic fairness , partial-monitoring games , and recidivism prediction . A related model of feedback is that of _contaminated controls_, which considers learning from (1) a treated group which contains only _treated_ members of the agent population and (2) a "contaminated" control group with samples from the _entire_ agent population (not just those under _control_). Technically, our results are also related to a line of work in contextual bandits which shows that greedy algorithms without explicit exploration can achieve sublinear regret as long as the underlying context distribution is sufficiently diverse .

**Bandits and agents** A complementary line of work to ours is that of _Bayesian incentive-compatible_ (BIC) exploration in multi-armed bandit problems . Under such settings, the goal of the principal is to _persuade_ a sequence of \(T\) agents with incentives to explore across several different actions with bandit feedback. In contrast, in our setting it is the principal, not the agents, who is the one taking actions with partial feedback. As a result there is no need for persuasion, but the agents now have an incentive to strategically modify their behavior in order to receive a more desirable decision/action.

**Other related work** Finally, our work is broadly related to the literature on learning in repeated Stackelberg games , online Bayesian persuasion , and online learning in principal-agent problems . In the repeated Stackelberg game setting, the principal (leader) commits to a mixed strategy over a finite set of actions, and the agent (follower) best-responds by playing an action from a finite set of best-responses. Unlike in our setting, both the principal's and agent's payoffs can be represented by matrices. In contrast, in our setting the principal commits to a pure strategy from a continuous set of actions, and the agent best-responds by playing an action from a continuous set. In online Bayesian persuasion, the principal (sender) commits to a "signaling policy" (a random mapping from "states of the world" to receiver actions) and the agent (receiver) performs a posterior update on the state based on the principal's signal, then takes an action from a (usually finite) set. In both this setting and ours, the principal's action is a policy. However in our setting the policy is a linear decision rule, whereas in the Bayesian persuasion setting, the policy is a set of conditional probabilities which form an "incentive compatible" signaling policy. This difference in the policy space for the principal typically leads to different algorithmic ideas being used in the two settings. Strategic learning problems are, broadly speaking, instances of principal-agent problems. In contract design, the principal commits to a contract (a mapping from "outcomes" to agent payoffs). The agent then takes an action, which affects the outcome. In particular, they take the action which maximizes their expected payoff, subject to some cost of taking the action. The goal of the principal is to design a contract such that their own expected payoff is maximized. While the settings are indeed similar, there are several key differences. First, in online contract design the principal always observes the outcome, whereas in our setting the principal only observes the reward if a positive decision is made. Second, the form of the agent's best response is different, which leads to different agent behavior and, as a result, different online algorithms for the principal.

## 2 Setting and background

We consider a game between a _principal_ and a sequence of \(T\)_agents_. Each agent is associated with a _context_\(_{t}^{d}\), which characterizes their attributes (e.g., a loan applicant's credit history/report). At time \(t\), the principal commits to a _policy_\(_{t}:\{1,0\}\), which maps from contexts to binary _decisions_ (e.g., whether to accept/reject the loan application). We use \(a_{t}=1\) to denote the the principal's positive decision at round \(t\) (e.g., agent \(t\)'s loan application is approved), and \(a_{t}=0\) to denote a negative decision (e.g., the loan application is rejected). Given \(_{t}\), agent \(t\)_best-responds_ by strategically modifying their context within their _effort budget_ as follows:

**Definition 2.1** (Agent best response; lazy tiebreaking).: _Agent \(t\) best-responds to policy \(_{t}\) by modifying their context according to the following optimization program._

\[^{}_{t} _{^{}}\ \{_{t}( ^{})=1\}\] \[s.t. \|^{}-_{t}\|_{2}\]

_Furthermore, we assume that if an agent is indifferent between two (modified) contexts, they choose the one which requires the least amount of effort to obtain (i.e., agents are lazy when tiebreaking)._

In other words, every agent wants to receive a positive decision, but has only a limited ability to modify their (initial) context (represented by \(_{2}\) budget \(\)).2 Such an effort budget may be induced by time or monetary constraints and is a ubiquitous model of agent behavior in the strategic learning literature (e.g., ). We focus on _linear thresholding policies_ where the principal assigns action \((^{})=1\), if and only if \(,^{}\) for some \(^{d}\), \(\). We refer to \(,^{}_{t}=\) as the _decision boundary_. For linear thresholding policies, the agent's best-response according to Definition 2.1 is to modify their context in the direction of \(/\|\|_{2}\) until the decision-boundary is reached (if it can indeed be reached). While we present our results for _lazy tiebreaking_ for ease of exposition, all of our results can be readily extended to the setting in which agents best-respond with a "trembling hand", i.e. _trembling hand tiebreaking_. Under this setting, we allow agents who strategically modify their contexts to "overshoot" the decision boundary by some bounded amount, which can be either stochastic or adversarially-chosen. See Appendix D for more details.

The principal observes \(^{}_{t}\) and plays action \(a_{t}=_{t}(^{}_{t})\) according to policy \(_{t}\). If \(a_{t}=0\), the principal receives some known, _constant_ reward \(r_{t}(0):=r_{0}\). On the other hand, if the principal assigns action \(a_{t}=1\), we assume that the reward the principal receives is linear in the agent's _unmodified_ context, i.e.,

\[r_{t}(1):=^{(1)},_{t}+_{t} \]

for some _unknown_\(^{(1)}^{d}\), where \(_{t}\) is i.i.d. zero-mean sub-Gaussian random noise with (known) variance \(^{2}\). Note that \(r_{t}(1)\) is observed _only_ when the principal assigns action \(a_{t}=1\), and _not_ when \(a_{t}=0\). Following Helmbold et al. , we refer to such feedback as _apple tasting_ (or _one-sided_) feedback. Mapping to our lending example, the reward a bank receives for rejecting a particular loan applicant is the same across all applicants, whereas their reward for a positive decision could be anywhere between a large, negative reward (e.g., if a loan is never repaid) to a large, positive reward (e.g., if the loan is repaid on time, with interest).

The most natural measure of performance in our setting is that of _Stackelberg regret_, which compares the principal's reward over \(T\) rounds with that of the optimal policy _given that agents strategic_.

**Definition 2.2** (Stackelberg regret).: _The Stackelberg regret of a sequence of policies \(\{_{t}\}_{t[T]}\) on agents \(\{_{t}\}_{t[T]}\) is_

\[_{}(T):=_{t[T]}r_{t}(^{*} (}_{t}))-_{t[T]}r_{t}(_{t}(^{}_{t}))\]

_where \(}_{t}\) is the best-response from agent \(t\) to policy \(^{*}\) and \(^{*}\) is the optimal-in-hindsight policy, given that agents best-respond according to Definition 2.1._

A stronger measure of performance is that of _strategic regret_, which compares the principal's reward over \(T\) rounds with that of the optimal policy _had agents reported their contexts truthfully_.

**Definition 2.3** (Strategic regret).: _The strategic regret of a sequence of policies \(\{_{t}\}_{t[T]}\) on agents \(\{_{t}\}_{t[T]}\) is_

\[_{}(T):=_{t[T]}r_{t}(^{*}( _{t}))-_{t[T]}r_{t}(_{t}(^{}_{t}))\]

_where \(^{*}(_{t})=1\) if \(^{(1)},_{t} r_{0}\) and \(^{*}(_{t})=0\) otherwise._

[MISSING_PAGE_FAIL:5]

_for exploration_; a growing area of interest in the online learning literature (see references in Section 1.2). Moreover, such assumptions often hold in practice. For example, in the related problem of (non-strategic) contextual bandits (we will later show how our results extend to the strategic version of this problem), Bietti et al.  find that a greedy algorithm with no explicit exploration achieved the second-best empirical performance across a large number of datasets when compared to many popular contextual bandit algorithms. In our settings of interest (e.g. lending, hiring), such an assumption is reasonable if there is sufficient diversity in the applicant pool. In Section 4 we show how to remove this assumption, albeit at the cost of worse regret rates and exponential computational complexity.

At a high level, our algorithm (formally stated in Algorithm 1) relies on three key ingredients to achieve sublinear strategic regret:

1. A running estimate of \(^{(1)}\) is used to compute a linear policy, which separates agents who receive action \(1\) from those who receive action \(0\). Before deploying, we shift the decision boundary by the effort budget \(\) to account for the agents strategizing.
2. We maintain an estimate of \(^{(1)}\) (denoted by \(}^{(1)}\)) and only updating it when \(a_{t}=1\) and we can ensure that \(_{t}^{}=_{t}\).
3. We assign actions "greedily" (i.e. using no explicit exploration) w.r.t. the shifted linear policy.

**Shifted linear policy** If agents were _not_ strategic, assigning action \(1\) if \(}^{(1)}_{t},_{t} r_{0}\) and action \(0\) otherwise would be a reasonable strategy to deploy, given that \(}^{(1)}_{t}\) is our "best estimate" of \(^{(1)}\) so far. Recall that the strategically modified context \(_{t}^{}\) is s.t., \(\|_{t}^{}-_{t}\|\). Hence, in Algorithm 1, we shift the linear policy by \(\|}^{(1)}\|_{2}\) to account for strategically modified contexts. Now, action \(1\) is only assigned if \(}^{(1)}_{t},_{t}\| }^{(1)}\|_{2}+r_{0}\). This serves two purposes: (1) It makes it so that any agent with unmodified context \(\) such that \(}^{(1)}_{t},<r_{0}\) cannot receive action \(1\), no matter how they strategize. (2) It forces some agents with contexts in the band \(r_{0}}^{(1)}_{t},<\| }^{(1)}\|_{2}+r_{0}\) to strategize in order to receive action \(1\). **Estimating \(^{(1)}\)** After playing action \(1\) for the first \(d\) rounds, Algorithm 1 forms an initial estimate of \(^{(1)}\) via ordinary least squares (OLS). Note that since the first \(d\) agents will receive action \(1\) regardless of their context, they have no incentive to modify and thus \(_{t}^{}=_{t}\) for \(t d\). In future rounds, the algorithm's estimate of \(^{(1)}\) is only updated whenever \(_{t}^{}\) lies _strictly_ on the positive side of the linear decision boundary. We call these contexts _clean_, and can infer that \(_{t}^{}=_{t}\) due to the lazy tiebreaking assumption in Definition 2.1 (i.e. agents will not strategize more than is necessary to receive the positive classification).

**Condition 3.2** (Sufficient condition for \(^{}=\)).: _Given a shifted linear policy parameterized by \(^{(1)}^{d}\), we say that a context \(^{}\) is clean if \(^{(1)},^{}>\|^{(1)}\|_ {2}+r_{0}\)._

**Greedy action assignment** By assigning actions greedily according to the current (shifted) linear policy, we are relying on the diversity in the agent population for implicit exploration (i.e., to collect more datapoints to update our estimate of \(^{(1)}\)). As we will show, this implicit exploration is sufficient to achieve \(}()\) strategic regret under Assumption 3.1, albeit at the cost of an exponentially-large (in \(d\)) constant which depends on the agents' ability to manipulate (\(\)).

We are now ready to present our main result: strategic regret guarantees for Algorithm 1 under apple tasting feedback.

**Theorem 3.3** (Informal; detailed version in Theorem B.1).: _With probability \(1-\), Algorithm 1 achieves the following performance guarantee:_

\[(T)}( c_{ 1}(d,) c_{2}(d,)}T(4dT/)})\]

_where \(c_{0}\) is a lower bound on the density ratio as defined in Assumption 3.1, \(c_{1}(d,):=_{ U^{d}}() (}{d^{2}})\) for sufficiently large \(d\) and \(c_{2}(d,):=_{ U^{d}}[^{2}]](--^{2} )^{3}\), where \([i]\) denotes the \(i\)-th coordinate of a vector \(\).4_

Proof sketch.: Our analysis begins by using properties of the strategic agents and shifted linear decision boundary to upper-bound the per-round strategic regret for rounds \(t>d\) by a term proportional to \(\|}_{t}^{(1)}-^{(1)}\|_{2}\), i.e., our instantaneous estimation error for \(^{(1)}\). Next we show that

\[\|}_{t}^{(1)}-^{(1)}\|_{2} ^{t}_{s}_{s}1\!\{_{s} ^{(1)}\}\|_{2}}{_{min}(_{s=1}^{t}_{s}_{s}^{}\!\!\!1\!\{_{s}^{(1)}\})}\]

where \(_{min}(M)\) is the minimum eigenvalue of (symmetric) matrix \(M\), and \(_{s}^{(1)}=\{}_{s}^{(1)},_{s }\|}_{s}^{(1)}\|_{2}+r_{0}\}\) is the event that Algorithm 1 assigns action \(a_{s}=1\) and can verify that \(_{s}^{}=_{s}\). We upper-bound the numerator using a variant of Azuma's inequality for martingales with subgaussian tails. Next, we use properties of Hermitian matrices to show that \(_{min}(_{s=1}^{t}_{s}_{s}^{}\!\!\!1\!\{ _{s}^{(1)}\})\) is lower-bounded by two terms: one which may be bounded vs.h.p. by using the extension of Azuma's inequality for matrices, and one of the form \(_{s=1}^{t}_{min}(_{s-1}[_{s}_{s}^{ }\!\!\!1\!\{_{s}^{(1)}\}])\), where \(_{s-1}\) denotes the expected value conditioned on the filtration up to time \(s\). Note that up until this point, we have only used the fact that contexts are drawn i.i.d. from a _bounded_ distribution.

Using Assumption 3.1 on the bounded density ratio, we can lower bound \(_{min}(_{s-1}[_{s}_{s}^{}\!\!\!1\! \{_{s}^{(1)}\}])\) by \(_{min}(_{U^{d},s-1}[_{s}_{s}^{}\!\! \!1\!\{_{s}^{(1)}\}])\), _where the expectation is taken with respect to the uniform distribution over the \(d\)-dimensional ball_. We then use properties of the uniform distribution to show that \(_{min}(_{U^{d},s-1}[_{s}_{s}^{}\!\!\!1 \!\{_{s}^{(1)}\}])(c_{0} c(d,))\). Putting everything together, we get that \(\|}_{t}^{(1)}-^{(1)}\|_{2} (c_{0} c(d,))^{-1}\) with high probability. Via a union bound and the fact that \(_{t[T]}} 2T\), we get that \((T)}( c(d, )})\). Finally, we use tools from high-dimensional geometry to lower bound the volume of a spherical cap and we show that for sufficiently large \(d\), \(c_{1}(d,)(}{d^{2}})\). 

### High-dimensional contexts

While we typically think of the number of agents \(T\) as growing and the context dimension \(d\) as constant in our applications of interest, there may be situations in which \(T\) is either unknown or small. Under such settings, the \(}{{c}}(d,)\) dependence in the regret bound (where \(c(d,)=c_{1}(d,) c_{2}(d,)\)) may become problematic if \(\) is close to \(1\). This begs the question: "Why restrict the OLS estimator in Algorithm 1 to use only clean contexts (as defined in Condition 3.2)?" Perhaps unsurprisingly, we show in Appendix B that the estimate \(}^{(1)}\) given by OLS will be inconsistent if even a constant fraction of agents strategically modify their contexts.

Given the above, it seems reasonable to restrict ourselves to learning procedures which only use data from agents for which the principal can be sure that \(^{}=\). Under such a restriction, it is natural to ask whether there exists some sequence of linear polices which maximizes the number of points of the form \((^{}_{t},r_{t}(1))\) for which the principal can be sure that \(^{}_{t}=_{t}\). Again, the answer is no:

**Proposition 3.4**.: _For any sequence of linear policies \(\{_{t}\}_{t}\), the expected number of clean points is:_

\[_{_{1},,_{T} U^{d}}[_{t[T] }\{_{t},_{t}> \|_{t}\|_{2}\}]=c_{1}(d,) T\]

_when (initial) contexts are drawn uniformly from the \(d\)-dimensional unit sphere._

The proof follows from the rotational invariance of the uniform distribution over the unit sphere. Intuitively, Proposition 3.4 implies that any algorithm which wishes to learn \(^{(1)}\) using clean samples will only have \(c_{1}(d,) T\) datapoints in expectation. Observe that this dependence on \(c_{1}(d,)\) arises as a direct result of the agents' ability to strategize. We remark that a similar constant often appears in the regret analysis of BIC bandit algorithms (see Section 1.2). Much like our work,  find that their regret rates depend on a constant which may be arbitrarily large, depending on how hard it is to persuade agents to take the principal's desired action in their setting. The authors conjecture that this dependence is an inevitable "price of incentive-compatibility". While our results do not rule out better strategic regret rates in \(d\) for more complicated algorithms (e.g., those which deploy non-linear policies), it is often unclear how strategic agents would behave in such settings, both in theory (Definition 2.1 would require agents to solve a non-convex optimization with potentially no closed-form solution) and in practice, making the analysis of such nonlinear policies difficult in strategic settings.

We conclude this section by showing that polynomial dependence on \(d\) is possible, at the cost of \(}(T^{2/3})\) strategic regret. Specifically, we provide an algorithm (Algorithm 3) which obtains the following regret guarantee whenever \(T\) is small or unknown, which uses Algorithm 1 and a variant of the explore-then-commit algorithm (Algorithm 2) as subroutines:

**Theorem 3.5** (Informal; details in Theorem B.13).: _Algorithm 3 incurs expected strategic regret_

\[[(T)]=}(\{ }{(1-)^{d/2}},d T^{2/3}\}),\]

_where the expectation is taken with respect to the sequence of contexts \(\{_{t}\}_{t[T]}\) and random noise \(\{_{t}\}_{t[T]}\)._

The algorithm proceeds by playing a "strategy-aware" variant of explore-then-commit (Algorithm 2) with a doubling trick until the switching time \(^{*}=g(d,)\) is reached. Note that \(g(d,)\) is a function of both \(d\) and \(\), _not_\(c_{0}\). If round \(^{*}\) is indeed reached, the algorithm switches over to Algorithm 1 for the remaining rounds.

```
Input : Time horizon \(T\), failure probability \(\)  Set \(T_{0}\) according to Theorem B.9  Assign action \(1\) for the first \(T_{0}\) rounds  Estimate \(^{(1)}\) as \(}^{(1)}_{T_{0}}\) via OLS for \(t=T_{0}+1,,T\)do  Assign action \(a_{t}=1\) if \(}^{(1)}_{T_{0}},_{t} \|}^{(1)}_{T_{0}}\|_{2}\) and action \(a_{t}=0\) otherwise
```

**ALGORITHM 2**Explore-Then-Commit

**Extension to bandit feedback** Algorithm 1 can be extended to handle bandit feedback by explicitly keeping track of an estimate \(}^{(0)}\) of \(^{(0)}\) via OLS, assigning action \(a_{t}=1\) if and only if \(}^{(1)}_{t}-}^{ (0)}_{t},^{}_{t}\|}^{(1)}_{t}-}^{(0)}_{t}\|_{2}\), and updating the OLS estimate of \(}^{(0)}\) whenever \(a_{t}=0\) (since agents will not strategize to receive action 0). Algorithm 3 may be extended to bandit feedback by "exploring" for twice as long in Algorithm 2, in addition to using the above modifications. In both cases, the strategic regret rates are within a constant factor of the rates obtained in Theorem 3.3 and Theorem 3.5.

## 4 Beyond stochastic contexts

In this section, we allow the sequence of initial agent contexts to be chosen by an (oblivious) _adversary_. This requires new algorithmic ideas, as the regression-based algorithms of Section 3 suffer _linear_ strategic regret under this adversarial setting. Our algorithm (Algorithm 4) is based on the popular EXP3 algorithm . At a high level, Algorithm 4 maintains a probability distribution over "experts", i.e., a discretized grid \(\) over carefully-selected policies. In particular, each grid point \(^{d}\) represents an "estimate" of \(^{(1)}\), and corresponds to a slope vector which parameterizes a (shifted) linear threshold policy, like the ones considered in Section 3. We use \(a_{t,}\) to refer to the action played by the principal at time \(t\), had they used the linear threshold policy parameterized by expert \(\). At every time-step, (1) the adversary chooses an agent \(_{t}\), (2) a slope vector \(_{t}\) is selected according to the current distribution, (3) the principal commits to assigning action \(1\) if and only if \(_{t},_{t}^{}\|_{t }\|_{2}\), (4) the agent strategically modifies their context \(_{t}_{t}^{}\), and (5) the principal assigns an action \(a_{t}\) according to the policy and receives the associated reward \(r_{t}(a_{t})\) (under apple tasting feedback).

Algorithm EXP4, which maintains a distribution over experts and updates the loss of _all_ experts based on the current action taken, is not directly applicable in our setting as the strategic behavior of the agents prevents us from inferring the loss of each expert at every time-step . This is because if \(_{t}^{}_{t}\) under the thresholding policy associated with expert \(\)), it is generally not possible to "back out" \(_{t}\) given \(_{t}^{}\), which prevents us from predicting the counterfactual context the agent would have modified to had the principal been using expert \(^{}\) instead. As a result, we use a modification of the standard importance-weighted loss estimator to update the loss of _only the policy played by the algorithm_ (and therefore the distribution over policies). Our regret guarantees for Algorithm 4 are as follows:

**Theorem 4.1** (Informal; detailed version in Theorem C.1).: _Algorithm 4 incurs expected strategic regret \([(T)]=}(T^{(d+1)/(d+2)})\)._

We remark that Algorithm 4 may be extended to handle settings in which agents are selected by an _adaptive_ adversary by using EXP3.P  in place of EXP3.

Proof sketch.: The analysis is broken down into two parts. In the first part, we bound the regret w.r.t. the best policy on the grid. In the second, we bound the error incurred for playing policies on the grid, rather than the continuous space of policies. We refer to this error as the _Strategic Discretization Error_ (\(SDE(T)\)). The analysis of the regret on the grid mostly follows similar steps to the analysis of EXP3 / EXP4. The important difference is that we shift the reward obtained by \(a_{t}\), by a factor of \(1+\), where \(\) is a (tunable) parameter of the algorithm. This shifting (which does not affect the regret, since all the losses are shifted by the same fixed amount) guarantees that the losses at each round are non-negative and bounded with high probability. Technically, this requires bounding the tails of the subgaussian of the noise parameters \(_{t}\).

We now shift our attention to bounding \(SDE(T)\). The standard analysis of the discretization error in the non-strategic setting does not go through for our setting, since an agent may strategize very differently with respect to two policies which are "close together" in \(_{2}\) distance, depending on the agent's initial context. Our analysis proceeds with a case-by-case basis. Consider the best expert \(^{*}\) in the grid. If \(a_{t,^{*}}=^{*}(_{t})\) (i.e., the action of the best expert matches that of the optimal policy), there is no discretization error in round \(t\). Otherwise, if \(a_{t,^{*}}^{*}(_{t})\), we show that the per-round \(SDE\) is upper-bounded by a term which looks like twice the discretization upper-bound for the non-strategic setting, plus an additional term. We show that this additional term must always be non-positive by considering two subcases (\(a_{t,^{*}}=1\), \(^{*}(_{t})=0\) and \(a_{t,^{*}}=0\), \(^{*}(_{t})=1\)) and using properties about how agents strategize against the deployed algorithmic policies.

**Algorithm 4**EXP3 with strategy-aware experts (EXP3-SAE)

**Computational complexity** While both Algorithm 1 and Algorithm 3 have \((d^{3})\) per-iteration computational complexity, Algorithm 4 must maintain and update a probability distribution over a grid of size exponential in \(d\) at every time-step, making it hard to use in practice if \(d\) is large. We view the design of computationally efficient algorithms for adversarially-chosen contexts as an important direction for future research.

**Extension to bandit feedback** Algorithm 4 may be extended to the bandit feedback setting by maintaining a grid over estimates of \(^{(1)}-^{(0)}\) (instead of over \(^{(1)}\)). No further changes are required.

## 5 Conclusion

We study the problem of classification under incentives with apple testing feedback. Such one-sided feedback is often what is observed in real-world strategic settings including lending and hiring. Our main result is a "greedy" algorithm (Algorithm 1) which achieves \(}()\) strategic regret when the initial agent contexts are generated _stochastically_. The regret of Algorithm 1 depends on a constant \(c_{1}(d,)\) which scales exponentially in the context dimension, which may be problematic in settings for which the number of agents is small or unknown. To address this, we provide an algorithm (Algorithm 3) which combines Algorithm 1 with a strategy-aware version of the explore-then-commit algorithm using a doubling trick to achieve \(}(\{}{c_{1}(d,)},d T^{2/3 }\})\) expected strategic regret whenever \(T\) is unknown. Finally, we relax the assumption of stochastic contexts and allow for contexts to be generated adversarially. Algorithm 4 achieves \(}(T^{})\) expected strategic regret whenever agent contexts are generated adversarially by running EXP3 over a discretized grid of strategy-aware policies, but has exponential-in-\(d\) per-round computational complexity. All of our results also apply to the more general setting of bandit feedback, under slight modifications to the algorithms. There are several directions for future work:

**Unclean data** The regret of Algorithm 1 depends on a constant which is exponentially large in \(d\), due to the fact that it only learns using clean data (Condition 3.2). While learning using unclean data will generally produce an inconsistent estimator, it would be interesting to see if the principal could leverage this data to remove the dependence on this constant. Alternatively, lower bounds which show that using unclean data will not improve regret would also be interesting.

**Efficient algorithms for adversarial contexts** Our algorithm for adversarially-chosen agent contexts suffers exponential-in-\(d\) per-round computational complexity, which makes it unsuitable for use in settings with high-dimensional contexts. Deriving polynomial-time algorithms with sublinear strategic regret for this setting is an exciting (but challenging) direction for future research.

**More than two actions** Finally, it would be interesting to extend our algorithms for strategic learning under bandit feedback to the setting in which the principal has _three or more_ actions at their disposal. While prior work  implies an impossibility result for strategic regret minimization with three or more actions, other (relaxed) notions of optimality (e.g., sublinear _Stackelberg_ regret; recall Definition 2.2) may still be possible.