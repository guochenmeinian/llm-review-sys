# A Theory of Multimodal Learning

Zhou Lu

Princeton University

zhoul@princeton.edu

###### Abstract

Human perception of the empirical world involves recognizing the diverse appearances, or'modalities', of underlying objects. Despite the longstanding consideration of this perspective in philosophy and cognitive science, the study of multimodality remains relatively under-explored within the field of machine learning. Nevertheless, current studies of multimodal machine learning are limited to empirical practices, lacking theoretical foundations beyond heuristic arguments. An intriguing finding from the practice of multimodal learning is that a model trained on multiple modalities can outperform a finely-tuned unimodal model, even on unimodal tasks. This paper provides a theoretical framework that explains this phenomenon, by studying generalization properties of multimodal learning algorithms. We demonstrate that multimodal learning allows for a superior generalization bound compared to unimodal learning, up to a factor of \(O(),\) where \(n\) represents the sample size. Such advantage occurs when both connection and heterogeneity exist between the modalities.

## 1 Introduction

Even before the common era, the concept of viewing an object as the collection of its appearances, has already sprouted in early philosophy. The Buddha, in the 'Diamond Sutra', separated the essence of universe from various modalities such as sight, sound, smell, taste and touch. Two centuries ago, Immanuel Kant made a further step, positing that humans perceive only the representations of 'noumena' from the empirical world. He wrote:

_"And we indeed, rightly considering objects of sense as mere appearances, confess thereby that they are based upon a thing in itself, though we know not this thing in its internal constitution, but only know its appearances, viz., the way in which our senses are affected by this unknown something. - Prolegomena"_

From this perspective, human cognition of the world, may therefore be considered effectively equivalent to the multiple modalities of the underlying objects. The importance of multimodality extends beyond metaphysics to everyday life: children learning languages often rely on illustrations, and even mathematicians benefit from visual aids.

However, machine learning, which could be seen as the cognition of computer systems, has not fully harnessed the power of multimodality. Multimodal machine learning, which processes and learns from data with multiple modalities, remained relatively under-explored until recently. Despite the impressive success of multimodal learning in empirical applications, such as Gato  and GPT-4 , the corresponding theoretical understanding is largely absent, often limited to heuristics.

A fascinating observation from empirical multimodal learning is that a model trained with multiple modalities can outperform a finely-tuned unimodal model, even on population data of the same unimodal task. It's not immediately clear why multimodality offers such an advantage, considering that the trained model's focus is spread across different modalities.

While it seems challenging to outperform unimodal learning asymptotically when sufficient data is available, multimodal learning can still provide an edge under a fixed data budget. Different modalities might focus on different aspects of an object, and for a specific classification problem, one modality may require a smaller sample complexity. This phenomenon often occurs with large models handling many tasks and a vast amount of training data, suggesting that:

Training across tasks learns a common connection between modalities efficiently, allowing the model to adapt to the modality with the smallest sample complexity.

An intuitive example of how multiple modalities help is learning parametric sine functions. The samples come in the form of

\[x(0,1],y= x,z=(1/y),\]

where \(x,y\) are the two modalities and \(z\) is the label. Given data from both modalities the learning problem is trivial, even with a single training data point, while learning solely on \(x\) is hard albeit there is a bijective mapping between \(x,y\). From a perspective of VC-dimension, there is a gap between the class of linear functions \(\{ x\}\) and the class of parametric sine functions \(\{(1/ x)\}\), in that the former one has VC-dimension 1 while the latter one has infinite VC-dimension. More details will be provided later.

The theory problem we study in this paper, is thus how to formalize the above heuristic with provable guarantees. To this end, we examine generalization bounds of a simple multimodal ERM algorithm, which involves two parallel stages: learning a predictor \(\) based on multimodal training data, and learning a connection \(\) that maps one modality to another with potentially unlabeled data. During inference, the composition \(\) is used to perform prediction on unimodal population data.

In this setting, we prove that the learnt unimodal predictor \(\) can achieve vanishing generalization error against the best multimodal predictor \(f^{*}\) as if given multiple modalities, whenever \(\) is expressive enough to realize the training data. In addition, such generalization bound depends on the complexities of both hypothesis classes \(,\) separately, better than unimodal approaches which typically involve the complexity of \(\) or a worst-case complexity of \(\), up to an \(O()\) factor where \(n\) denotes the size of training data. On the other hand, we show a separation between multimodal and unimodal learning, by constructing a hard instance learnable by multimodal learning, in which no matter what hypothesis class is chosen for the unimodal learning problem, it's either under-expressive or over-expressive and thus incurs constant error. Putting the two pieces together, our theory suggests that with both connection and heterogeneity, multimodal learning is provably better than unimodal learning.

The paper is organized as follows. In section 2 we formalize the setting of multimodal learning and provide a motivating example. Section 3 proves a generalization upper bound of the two-stage multimodal ERM algorithm on semi-supervised multitask learning problems. The lower bound on the separation between multimodal and unimodal learning is given in section 4, then we discuss the limitations of this paper and future directions in section 5.

### Related Works

**Theoretical Multimodal Learning**: while empirical multimodal learning has shown significant progress, theoretical studies are relatively sparse, lacking a firm foundation. Prior theoretical investigations often focus on specific settings or incorporate additional assumptions. Some of these studies adopt an information-theoretic perspective, proposing algorithms based on total correlation or utilizing partial information decomposition to quantify relationships between modalities [35; 17]. Other studies approach the problem from a multi-view setting, typically assuming that each view alone suffices for prediction [40; 1; 11; 34].

An important work in theoretical multimodal learning is , which also considered the advantage of multimodal learning in generalization and is the first and probably the only general theoretical result in this field so far. In particular, they considered the population risk of a representation learning based approach where learning under different subsets of modalities is performed on the same ERM objective with shared hypothesis classes. They proved that the gap between the population risks of different subsets of modalities is lower bounded by the difference between what they called the latent representation quality, which is the best achievable population risk with the learnt representation on the chosen subset of modalities.

There are two limitations in this result: 1, there is no quantitative characterization on how large the gap between latent representation qualities can be; 2, the comparison is not-only instance-dependent, but also carried over the same hypothesis classes and doesn't exclude the possibility that the smaller subset of modalities could potentially use a different class to bypass the gap, making the lower bound somewhat restricted. We strengthen this result by showing the gap can be as large as \((1)\) (Theorem 7), even if we allow the smaller subset of modalities to use any hypothesis class. They also showed an upper bound on the excess population risk via a standard representation learning analysis, which involves the complexity of a composition of hypothesis classes \(\), while our analysis decouples the complexities of hypothesis classes, leading to an improved upper bound up to a factor of \(O()\).

A recent work of  made a more fine-grained study on multimodal learning, analyzing the benefit of contrastive loss in training dynamics. They considered the aspect of optimization instead of generalization for a particular problem, focusing on the setting of a linear data-generating model. They proved that the use of contrastive loss is both sufficient and necessary for the training algorithm to learn aligned and balanced representations.

**Empirical Multimodal Learning**: the inception of multimodal learning applications dates back to the last century, initially devised to enhance speech recognition using both vision and audio [39; 25]. Multimedia is another aspect in which multimodal learning inspired new methods for indexing and searching [10; 19]. With the development of deep learning and its success in computer vision  and natural language processing , people start studying deep multimodal learning in related tasks such as generating one modality from the other [8; 15; 30]. For a more comprehensive introduction of multimodal learning, we refer the readers to the excellent survey paper , see also [29; 13; 18].

Recently, the power of multimodal learning was carried out in large-scale generalist models. In , the training data includes a wide variety of modalities such as image, text, robotics and so on. The resulting model is reported to be able to beat fine-tuned unimodal models in some tasks. In a more recent ground-breaking result, the super-large language model GPT-4  makes use of not only text data available on internet, but also data from other modalities such as audio, demonstrating excellent capabilities in integrating knowledge from multiple domains.

**Representation Learning**: this field, closely related to our work, focuses on learning a common underlying representation across multiple tasks. The typical framework of representation learning involves solving an ERM problem with a composition of hypotheses \(f_{t} g\), where \(f_{t}\) is task-specific while \(g\) is the common representation. Generalization bounds of representation learning usually involve the complexity of \(\) or a worst-case complexity of \(\).

Starting from Baxter's study  which gave theoretical error bounds via covering numbers on the inductive bias learning approach , a long line of work has followed, each improving upon and generalizing the previous results [6; 2; 21; 7; 20; 28; 27].

For more recent works we detail several representative ones here. The work of  studied both representation learning and transfer learning in the setting of multitask learning, achieving dimension independent generalization bounds with a chain rule on Gaussian averages . For the problem of transfer learning,  improved the leading term of  by a \(O()\) factor under a task diversity assumption, while  obtained a similar bound under low-dimension and linear function assumptions.  analyzed a generalized setting called contrastive learning inspired by the success of empirical language models.

## 2 Setting

In this paper, we consider a straightforward yet non-trivial case of two modalities to ensure clarity in our presentation. Formally, we denote the set of possible observations \(\) to be \((,,)\). Here, each element \(s\) constitutes a pairing of inputs from both modalities \(x^{q},y^{k}\) and their associated label \(z\), thus forming a tuple \((x,y,z)\). Assuming without loss of generality that both \(\) and \(\) are contained within their respective Euclidean unit balls. Given a probability measure \(\) on \(\) and a loss function \(\), the performance of a learning algorithm \(\) is measured by the population loss if we interpret \(\) as a function, namely

\[_{(x,y,z)}((x,y),z).\]To leverage the hidden correlation between different modalities, we aim to learn both a connection function \(g\) bridging the modalities and a prediction function \(f\) that accepts inputs from both modalities. Although we focus on learning a connection from \(\) to \(\) for simplicity, a symmetrical approach can handle the reverse direction.

In particular, we will consider learning algorithms as a composition of functions \((x,y)=f(x,g(x))\), where \(f\) and \(g\) represent the hypothesis classes for both functions. This form is most general and common practical forms such as fusion \((g(x),h(y))\) can be subsumed by the general form.

The goal is to identify \(,\) using multi-modal training data, to minimize the excess population risk

\[_{(x,y,z)}((x,(x)),z)-_{f }_{(x,y,z)}(f(x,y),z). \]

In this context, we compare with the optimal predictor \(f^{*}\) as if given complete observations of both modalities, because our objective is to achieve a performance comparable to the best predictor given both modalities. The reason is that such a predictor could have a significant advantage over any unimodal predictor that does not learn these connections (either explicitly or implicitly).

We seek statistical guarantees for \(\) and \(\). To provide generalization bounds on the excess population risk, we require a complexity measure of hypothesis classes, defined as follows.

**Definition 1** (Gaussian average).: Let \(Y\) be a non-empty subset of \(^{n}\), the Gaussian average of \(Y\) is defined as

\[G(Y)=_{}[_{y Y}_{i=1}^{n}_{i}y_{i}]\]

where \(_{i}\) are iid standard normal variables. Similarly, we can define the function version of Gaussian average. Let \(\) be a function class from the domain \(\) to \(^{k}\), and \(X=\{x_{1},...,x_{n}\}\) be the set of input sample points. We define the Gaussian average of the class \(\) on sample \(X\) as:

\[G((X))=_{}[_{g}_{i=1}^{k }_{j=1}^{n}_{i,j}g_{i}(x_{j})],\]

where \(_{i,j}\) are iid standard normal variables.

We make the following Lipschitz assumption on the class \(\) and the loss function \(\), which is standard in literature.

**Assumption 1**.: _We assume that any function \(f:^{q+k}\) in the class \(\) is \(L\)-Lipschitz, for some constant \(L>0\). The loss function \(\) takes value in \(\), and is 1-Lipschitz in the first argument for every value of \(z\)._

### A Motivating Example

To introduce our theoretical findings, we present a straightforward but insightful example, illustrating the circumstances and mechanisms through which multimodal learning can outperform unimodal learning. Despite its simplicity and informal nature, this example captures the core concept of why multimodal learning requires both connection and heterogeneity, and we believe it is as vital as the more formal statements that follow.

Consider the problem where \(==(0,1]\). Any potential data point \((x,y,z)\) from \(\) is governed by a parameter \(^{*}(0,1]\), such that

\[y=^{*}x,z=(1/y).\]

The loss function choice is flexible in this case, and any frequently used loss function like the \(_{1}\) loss will suffice.

Suppose we have prior knowledge about the structure of the problem, and we select \(=g(x)= x|(0,1]\) and \(=(1/x)\) as our hypothesis classes. If we have sampled data from both modalities, we can easily learn the correct hypothesis via Empirical Risk Minimization (ERM): simply take any \((x,y,z)\) sample and compute \(=y/x\).

However, if we only have sampled data with the \(\) modality concealed, there could be multiple \(\) values that minimize the empirical loss, making the learning process with \(\) significantly more challenging. To formalize this, we can calculate the Gaussian averages for both scenarios. In the multimodal case, the Gaussian average of \(\) is zero since it's a singleton. \(G((X))\) can be upper bounded by

\[G((X))=_{}[_{(0,1]}_{i= 1}^{n}_{i}x_{i}]_{}[|_{i=1}^{n} _{i}x_{i}|]=O().\]

In contrast, the Gaussian average of \(\) is larger by a factor of \(\)

\[G((X))=_{}[_{(0,1 ]}_{i=1}^{n}_{i}(})]_{ }[_{i=1}^{n}|_{i}|]=(n), \]

for some sample \(X\) (we leave the proof in the appendix), see also .

This separation in Gaussian averages implies that unimodal learning can be statistically harder than multimodal learning, even if there exists a simple bijective mapping between \(x,y\). We summarize the intrinsic properties of multimodal learning leading to such separation as follows:

**Heterogeneity:**: multimodal data is easier to learn than unimodal data.
**Connection:**: a mapping between multiple modalities is learnable.

Thereby, the superiority of multi-modality can be naturally decomposed into two parts: a model trained with multi-modal data performs comparably on uni-modal population data as if multi-modal data is provided (connection), a model trained and tested with multi-modal data outperforms any model given only uni-modal data (heterogeneity).

We note that both connection and heterogeneity are crucial to achieving such advantage: connection allows efficiently learning of \(\) from \(\), while heterogeneity guarantees that learning with \(\) is harder than learning with both \(,\). Lacking either one can lead to ill-conditioned cases: when \(x y\) the connection is perfect while there is no heterogeneity and thus no need to learn anything about \(\) at all. When \(x\) is a random noise the heterogeneity is large while there can't be any connection between \(,\), and it's impossible to come up with a non-trivial learner solely on \(\).

**Remark 2**.: The example above can be converted to be hard for each single modality. Any potential data point \((x,y,z)\) is now generated by three parameters \(c(0,1),_{1}(1,2),_{2}(-2,-1)\), under the constraint that \(_{1}+_{2} 0\), and \((x,y,z)\) is of form \((c_{1},c_{2},c(_{1}+_{2}))\). The hypothesis classes are now \(=\{g(x)= x,(-1,0)(0,1)\}\) and \(=(1/x)\). For any uni-modal data \(x=c_{1}\), the range of ratio \((x+y)/x\) is \((1-2/_{1},0)(0,1-1/_{1})\). This range is a subset of \((-1,0)(0,1)\) and we have that \((|1-2/_{1}|,|1-1/_{1}|) 1/4\). As a result, \(G((X))\) in this case is at least \(1/4\) of that in the simpler example, thus the term remains \((n)\). On the other hand, we have that \((|1-2/_{1}|,|1-1/_{1}|) 1\), so \(G((X))=O()\) holds still. The same argument holds for \(\) similarly.

## 3 The Case of Semi-supervised Multitask Learning

The efficacy of practical multimodal learning often hinges on large models and extensive datasets, with the majority potentially being unlabeled. This is especially true when the training data encompasses a broad spectrum of tasks. Given these conditions, we are interested in the power of multimodality within the realm of semi-supervised multitask learning, where the model leverages substantial amounts of unlabeled data from various tasks.

Consider the following setup for semi-supervised multitask multimodal learning. The training data is taken from a number of tasks coming in the form of a multi-sample, partitioned into two parts \(S,S^{}\). The labeled sample \(S\) takes form \(S=(S_{1},...,S_{T})\) where \(S_{t}=(s_{t1},...,s_{tn})_{t}^{n}\), in which \(_{t}\) represents the probability measures of the \(T\) different tasks from which we draw the independent data points \(s_{ti}=(x_{ti},y_{ti},z_{ti})\) from. The unlabeled sample \(S^{}\) takes a similar form, that \(S^{}=(S^{}_{1},...,S^{}_{T})\) where \(S^{}_{t}=((x_{t1},y_{t1}),...,(x_{tm},y_{tm}))_{t,(x,y)}^{m}\), drawn independently of \(S\), here by \(_{t,(x,y)}\) we denote the marginal distribution of \(_{t}\). We assume \(S^{}\) has a larger size \(m n\) than the labeled sample \(S\).

Using \(S^{}\), we aim to learn a connection function, and with \(S\), we learn a predictor that leverages both modalities. A common approach to this learning problem is empirical risk minimization (ERM). In particular, we solve the following optimization problem, where the predictors \(_{t}\) on both modalities are learnt via an ERM on the labeled sample \(S\),

\[_{1},..,_{T}=_{f_{1},...,f_{T}} {1}{nT}_{t=1}^{T}_{i=1}^{n}(f_{t}(x_{ti},y_{ti}),z_{ti}).\]

Meanwhile, the connection \(\) is learned by minimizing the distance to the true input, using the unlabeled sample \(S^{}\) instead:

\[=_{g}_{t=1}^{T}_{i=1}^ {m}\|g(x_{ti}^{})-y_{ti}^{}\|.\]

Our focus is not on solving the above optimization problems (as modern deep learning techniques readily address ERM) but rather on the statistical guarantees of the solutions to these ERM problems.

To measure the performance of the solution \(,_{1},..,_{T}\) on the modality \(\), we define the task-averaged excess risk as follows:

\[L(,_{1},...,_{T})=_{t=1}^{T}_{( x,y,z)_{t}}(_{t}(x,(x)),z)-_{f} _{t=1}^{T}_{(x,y,z)_{t}}(f_{t}(x,y),z).\]

In order to bound the excess risk, it's crucial to require the class \(\) to contain, at least, an approximate of a "ground truth" connection function, which maps \(x\) to \(y\) for any empirical observation. Later we will show that such requirement is inevitable, which can be seen a fundamental limit of our theoretical model.

**Definition 3** (Approximate realizability).: We define the approximate realizability of a function class \(\) on a set of input data \(S=\{(x_{1},y_{1}),...,(x_{n},y_{n})\}\) as

\[(,S)=_{g}_{i=1}^{n}\|g( x_{n})-y_{n}\|.\]

When the set \(S\) is labeled, we abuse the notation \((,S)\) to denote \((,(X,Y))\) for simplicity.

We have the following theorem that bounds the generalization error of our ERM algorithm in terms of Gaussian averages and the approximate realizability.

**Theorem 4**.: _For any \(>0\), with probability at least \(1-\) in the drawing of the samples \(S,S^{}\), we have that_

\[L(,_{1},...,_{T})}{nT}_{t=1}^{T} G((_{t},_{t}))+L}{mT}G((X ^{}))+L(,S^{})+(8L+4)},\]

_where \((_{t},_{t})\) denotes the set of \(\{x_{ti},(x_{ti})|i=1,...,n\}\)._

**Remark 5**.: It's important to note that the Gaussian average is typically on the order of \(O()\) when \(N\) is the sample size and \(d\) is the intrinsic complexity of the hypothesis class, such as the VC dimension. If we treat \(d\) as a constant, for most hypothesis classes in machine learning applications, the term \(G((X^{}))\) typically scales as \(O()\) and each term \(G((_{t},_{t}))\) scales as \(O()\). In practice, learning the connection \(g\) is often more challenging than learning a predictor \(f_{t}\), so it's encouraging to see the leading term \(G((X^{}))/mT\) decay in both \(m,T\).

Theorem 4 asserts that the ERM model trained with multi-modal data, achieves low excess risk on uni-modal test data to the optimal model as if multi-modal test data is provided, when connection is learnable. This result can be naturally extended to accommodate multiple modalities in a similar way. In this case, the ERM algorithm would learn a mapping from a subset of modalities to all modalities, which involves only one hierarchy as in the two-modality case, thus our analysis naturally carries over to this new setting.

### Necessity of A Good Connection

Recall that in the upper bound of Theorem 4, all the terms vanish as \(n,T\) tend to infinity, except for \(L(,S^{})\). It's therefore important to determine whether the term is a defect of our analysis or a fundamental limit of our theoretical model. Here we present a simple example showing that the dependence on approximate realizability is indeed inevitable.

Let \(==\{0,1\}\) and \(n,T 2\). Each probability measure \(_{t}\) is determined by a Boolean function \(b_{t}:\{0,1\}\{0,1\}\), and for each observation \(s_{t}\), the label \(z_{t}=b_{t}(y_{t})\) is purely determined by \(y_{t}\). In particular, the four possible observations

\[(0,0,b_{t}(0)),(1,0,b_{t}(0)),(0,1,b_{t}(1)),(1,1,b_{t}(1))\]

happen with the same probability for any \(t\).

For the hypothesis classes, \(\) includes all Boolean functions \(g:\{0,1\}\{0,1\}\), while \(\) includes all 1-Lipschitz functions \(^{2}\). It's straightforward to verify that

\[L(,S)=}{nT}(-^{c_{0}}_{i}|}{2c_{0}})+}{nT}(- {|_{i=1}^{c_{1}}_{i}|}{2c_{1}}),\]

where \(_{i}\) are iid Rademacher random variables, and \(c_{0},c_{1}\) denotes the number of observations with \(x=0\) and \(x=1\) respectively. The loss function \(\) is set as \(|f(x,y)-z|\).

The simplest version of Bernstein inequality states that for any \(>0\) and \(m^{+}\)

\[(|_{i=1}^{m}_{i}|>) 2e^{- }{2(1+)}},\]

therefore with probability at least \(\), we have that \(|c_{0}-c_{1}| 3\) since \(|c_{0}-c_{1}|\) itself can be written in the form of \(|_{i=1}^{nT}_{i}|\).

Condition on \(|c_{0}-c_{1}| 3\), we have that \(c_{0},c_{1}+2\). Using the Bernstein inequality again, with probability at least \(\), it holds \(|_{i=1}^{c_{0}}_{i}| 8}\) and similarly for \(c_{1}\). Putting it together, with probability at least \(\), the term \(L(,S)\) can be lower bounded as

\[L(,S)-}{}.\]

On the other hand, the population loss of any \(f(x,g(x))\) composition is clearly at least \(\) because the label \(z\) is independent of \(x\), while the population loss of \(\{f_{t}(x,y)\}\) with the choice of \(f_{t}(x,y)=b_{t}(y)\) is zero. As a result the excess risk on population is at least \(\) which doesn't scale with \(n,T\). When \(n,T\) are large enough, the term \(L(,S)\) matches the optimal achievable excess risk.

## 4 The Role of Heterogeneity

So far we have demonstrated that as long as a good connection indeed exists, learning with multimodal training data using a simple ERM algorithm yields a unimodal model which is guaranteed to perform as well as the best model \(f_{t}^{*}(x,y)\) with both modalities. The story is not yet complete. In order to explain the empirical phenomenon we're investigating, we still need to determine in what circumstance learning with multimodal data is strictly easier than unimodal learning. A good connection itself isn't sufficient: in the case of \(y x\) which admits a perfect connection function, bringing \(\) into consideration apparently gives no advantage.

To address this question, we move our eyes on heterogeneity, another fundamental property of multimodal learning that describes how modalities diverge and complement each other. Intuitively, heterogeneity can potentially lead to a separation between learnability in the following way: learning from a single modality is much harder than learning from both modalities, in the sense that it requires a much more complicated hypothesis class. As a result, either the sample complexity is huge due to a complicated hypothesis class, or the hypothesis class is so simple that even the best hypothesis performs poorly on population.

Consequently, we compare not only the best achievable population risks, but also the generalization errors. For unimodal learning denote \(\) as the hypothesis class, we consider the ERM solution

\[=*{arg\,min}_{g G}_{i=1}^{n}(g(x),z).\]

The generalization error of \(\) can be bounded via Gaussian average of the hypothesis class \(\) in the following way if we denote \(g^{*}=_{g}_{(x,y,z)}(g(x),z)\):

\[_{(x,y,z)}((x),z)-_{(x,y,z)} (g^{*}(x),z)((X))}{n}),\]

which is tight in general without additional assumptions. For multimodal learning, \(\) and \(f^{*}\) can be defined in the same way.

We are going to show that, either the intrinsic gap of risk between unimodality and multimodality

\[_{(x,y,z)}(g^{*}(x),z)-_{(x,y,z)}(f^ {*}(x,y),z) \]

is substantial, or the Gaussian average gap is large. This implies a separation between multimodal learning and unimodal learning, when heterogeneity is present. Consequently, we define the heterogeneity gap as follow.

**Definition 6** (Heterogeneity gap).: Given a fixed hypothesis class \(\) and number of sample \(n 2\), the heterogeneity gap between w.r.t some distribution \(\) and hypothesis class \(\) is defined as

\[H(,)=[(X))}{n}+_{(x,y,z)}(g^{*}(x),z)]-[(X,Y)) }{n}+_{(x,y,z)}(f^{*}(x,y),z)].\]

The above definition measures the population risks between learning with a single modality \(\) or with both modalities \(,\). When \(H(,)\) is large, unimodal learning is harder since ERM is arguably the optimal algorithm in general. As long as Theorem 4 admits a low risk, the heterogeneity gap itself directly implies the superiority of multi-modality by definition. Therefore, the only question left is whether such desired instance (large heterogeneity gap + perfect connection) actually exists.

To this end, the following theorem provides the existence of such instance, proving that our theory is indeed effective. Let's sightly abuse the notation \(L()\) to denote the excess population risk of \(\) which is the output of the unimodal ERM. We have the following lower bound w.r.t heterogeneity gap.

**Theorem 7**.: _There exist \(,,\) and a class \(U\) of distributions on \((,,)\), such that_

\[, n^{+}, U,s.t.\ H(, )=(1),_{X}[L()]=(1).\]

_Meanwhile, let \(,\) denote the outputs of the multimodal ERM algorithm in section 3, we have that_

\[,s.t.\  U, n^{+},L(, )=0.\]

**Remark 8**.: We compare with the work of . They showed a similar separation in terms of the intrinsic gap (the difference between optimal hypotheses), under a representation learning framework. We make a step further by taking the Gaussian average (how hard to learn the optimal hypothesis) into consideration, which is crucial to the \((1)\) gap in the more general setting.

Theorem 7 shows the existence of hard instances, where not only the heterogeneity gap is large, but the difference between actual risks is also substantial. It implies that under certain circumstances, multimodal learning is statistically easy, while unimodal learning incurs constant error no matter what hypothesis classes are used.

To sum up, our theory demonstrates that the superiority of multi-modality can be explained as the combined impact of connection and heterogeneity: when connection (Theorem 4) and heterogeneity (Theorem 7) both exist, multimodal learning has an edge over unimodal learning even if tested on unimodal data, providing an explanation to the empirical findings. Nevertheless, our theory also suggests a simple principle potentially useful for guiding empirical multimodal learning:

1. Collect numerous unlabeled multimodal data. Learn a connection via generative models.
2. Learn a predictor based on a modest amount of labeled multimodal data.

Such framework can be easily carried out with modern deep learning algorithms, for example we can learn the connection by generative models .

### Comparison with Representation Learning

It's possible to learn \(,\) and minimize 1, based on observations of a single modality \(\), via representation learning. In particular, representation learning solves the following unimodal ERM problem by treating \(y\) as an unknown representation of \(x\), on a labeled sample \(S\) where \(y\) is hidden

\[,_{1},..,_{T}=_{g,f_{1},..,f_ {T}}_{t=1}^{T}_{i=1}^{n}(f_{t}(x_{ti},g( x_{ti})),z_{ti}).\]

Unfortunately, although this representation learning method uses the same set of hypotheses, it leads to a worse sample complexity bound. Such method fails to exploit two essential parts of semi-supervised multimodal learning, namely the rich observation of unlabeled data \(S^{}\), and separate learning of \(f,g\). Failing to utilize \(S^{}\) will lead to a worse factor \(G((X))/nT\) which scales as \(1/\), while in Theorem 4 the factor \(G((X^{}))/mT\) scales as \(1/\) which is much smaller than \(1/\).

Failing to exploit the "explicit representations" \(y\) from the training data requires learning a composition \(f g\) from scratch, which typically leads to a worst case Gaussian average term, for example in  they have \(_{g}G((S(g)))\) instead where \(S(g)=\{(x_{ti},g(x_{ti}))\}\) is a random set induced by \(g\). As a comparison, our approach decouples the learning of \(f,g\), and the Gaussian average term \(G((_{t},_{t}))\) is only measured over the "instance" \(\) which can be smaller. In fact, \(G((_{t},_{t}))\) can be smaller than \(_{g}G((S(g)))\) up to a factor of \(O()\), see the example in appendix.

## 5 Limitations and Future Directions

In this paper we propose a theoretical framework on explaining the empirical success of multimodal learning, serving as a stepping stone towards more in-depth understanding and development of the theory. Nevertheless, as a preliminary study on the relatively unexplored field of theoretical multimodal learning, our result comes with limitations, and potentially opportunities for future research as well. We elaborate on these points below.

**More natural assumptions**: the current set of assumptions, while generally consistent with practical scenarios, does not fully align with them. Although most assumptions are satisfied in practice, the assumption on \(\) containing only Lipschitz functions, is restrictive: the predictor class \(\) typically comprises deep neural networks. Future work could seek to overcome the Lipschitz assumption, which is not only fundamental to our results but also a cornerstone in representation learning theory.

**Hypothesis-independent definitions**: our study characterizes multimodal learning through two central properties: connection and heterogeneity. However, their current definitions, \((,S)\) and \(H(,)\), depend on both the data and the hypothesis class \(\). It's interesting to see if we can develop theories with hypothesis-independent definitions, such as mutual information or correlation. A potential difficulty is that such statistical notions are not totally aligned with the perspective of machine learning, for example it could happen that the two modalities are independent with zero mutual information, while there exists a trivial connection mapping.

**Fine-grained analysis**: our theory, which focuses on the statistical guarantees of ERM solutions, is fairly abstract and does not make specific assumptions about the learning problem. To study other more concrete multimodal learning algorithms, for example the subspace learning method, we need more fine-grained analysis which takes the particular algorithm into account.

**More realistic examples**: while the example in section 2.1 provides theoretical justification, it remains somewhat artificial and diverges from typical multimodal learning problem structures. Ideally, we would like to see examples that closely mirror real-world scenarios. For instance, can we progress beyond the current example to provide one that captures the inherent structures of NLP and CV data?

**Benefit in optimization**: our work demonstrates the advantages of multimodal learning primarily in terms of generalization properties. However, optimization, another crucial aspect of machine learning, could also benefit from multimodality. One potential direction to explore is the possibility that multimodal data is more separable and thus easier to optimize. We provide a simple examplein the appendix that demonstrates the existence of linearly separable multimodal data, where the decision boundary for any single modality is arbitrary.

## 6 Conclusion

In this paper we study multimodal learning from a perspective of generalization. By adopting classic tools in statistic learning theory on this new problem, we prove upper and lower bounds on the population risk of a simple multimodal ERM algorithm. Compared with previous works, our framework improves the upper bound up to an \(O()\) factor by decoupling the learning of hypotheses, and gives a quantitative example in the separation between multimodal and unimodal learning. Our results relate the heuristic concepts connection and heterogeneity to a provable statistical guarantee, providing an explanation to an important phenomenon in empirical multimodal learning, that a multimodal model can beat a fine-tuned unimodal one. We hope our result, though being a preliminary step into a deeper understanding of multimodal learning, can shed some light on the directions of future theoretical studies.