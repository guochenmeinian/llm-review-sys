# Algorithm 3

The Pick-to-Learn Algorithm: Empowering Compression for Tight Generalization Bounds and Improved Post-training Performance

Dario Paccagnan

Dept. of Computing

Imperial College London

d.paccagnan@imperial.ac.uk &Marco C. Campi

Dip. di Ingegnerira dell'Informazione

Universita di Brescia

marco.campi@unibs.it &Simone Garatti

Dip. di Elettronica Informazione e Bioingegneria

Politecnico di Milano

simone.garatti@polimi.it

###### Abstract

Generalization bounds are valuable both for theory and applications. On the one hand, they shed light on the mechanisms that underpin the learning processes; on the other, they certify how well a learned model performs against unseen inputs. In this work we build upon a recent breakthrough in _compression theory_(Campi & Garatti, 2023) to develop a new framework yielding tight generalization bounds of wide practical applicability. The core idea is to embed any given learning algorithm into a suitably-constructed meta-algorithm (here called Pick-to-Learn, P2L) in order to instill desirable compression properties. When applied to the MNIST classification dataset and to a synthetic regression problem, P2L not only attains generalization bounds that compare favorably with the state of the art (test-set and PAC-Bayes bounds), but it also learns models with better post-training performance.

## 1 Introduction

Machine learning has achieved remarkable results in the last decades, with successful stories ranging from digit recognition (Dosovitskiy et al., 2020), to protein folding (Jumper et al., 2021), traffic prediction (Li et al., 2017), medical diagnosis (Kononenko, 2001), and beyond. Contrary to that, the understanding of the mechanisms underpinning generalization, and the availability of bounds for its evaluation, is still limited. This is unfortunate because generalization bounds are important both to develop trust in learning methods as well as to allow for a fair comparison among alternatives, which is relevant to hyper-parameter tuning.

Among the various tools that have been introduced to provide generalization bounds, we recall the VC-dimension (Vapnik & Chervonenkis, 1971), Radamacher complexity (Bartlett & Mendelson, 2002), mutual information (Xu & Raginsky, 2017), sharpness (Keskar et al., 2017), compression schemes (Littlestone & Warmuth, 1986; Floyd & Warmuth, 1995; Graepel et al., 2005), and PAC-Bayes approaches (Dziugaite & Roy, 2017; Perez-Ortiz et al., 2021). Each of these frameworks is applicable to specific contexts. For example, the approach based on the VC-theory requires that a model is selected from a class with finite VC-dimension, while compression schemes are applicable provided the existence of an informative subset of data points from which the model can be reconstructed. It has also to be said that the level of precision of the available bounds is highly problem-dependent and they often take a significant margin from the actual performance.

While bounds based on the VC-dimension, and some other aforementioned approaches, are often conservative (resulting in untight, or even vacuous, evaluations), the sharpest approach to evaluate generalization across various learning domains is still that based on test-set bounds (Langford, 2005). However, the test-set approach requires holding out a portion of the training set for testing, which makes it data-inefficient with negative impact on the post-training performance. In view of these considerations, it would be highly desirable to develop a new general-purpose technique able to set the data free in their double role of (i) _delivering the information that is needed to learn_, while also (ii) _providing evaluations on the attained generalization result_.

In this paper, we move towards the goal expressed in (i) and (ii). To this aim, we build upon a recent breakthrough in the field of compression schemes, (Campi & Garatti, 2023). Therein, the authors have established new and tight bounds for the so-called probability of change of compression that hold under certain properties: under the property of _preference_, tight upper bounds have been obtained that link the probability of change of compression to the size of the compressed set1; moreover, under a so-called _non-associativity_ property and a property of _non-concentrated mass_, a lower bound has also been obtained. Hence, under these additional properties, the change of compression is put in sandwich between two bounds, which are shown to rapidly converge one on top of the other as the number of data points increases. The interest of these results in the context of statistical learning lies on the fact that some learning algorithms naturally define a compression scheme and the probability of change of compression can be used to bound the probability of misclassification/misprediction. However, the challenge is that many other learning algorithms (including most algorithms for deep learning) do not exhibit any compression property.

With this paper, we give new, significant, thrust to the above findings. Specifically, we present a _meta-algorithm_, called Pick-to-Learn (P2L), that incorporates any learning algorithm given as a black box and _makes it_ into a compression scheme, so licensing the use of results from compression to establish generalization bounds in virtually any learning problem. More precisely, the meta-algorithm constructs a loop around the original learning algorithm that elects at each iteration a new data point to be included in the set of those used for training. Independently of the inner algorithm (which can be any algorithm, e.g., GD - Gradient Descent - for regression), we show that the meta-algorithm possesses the properties of _preference_, _non-associativity_ and _non-concentrated mass_ by which, as mentioned above, one can secure extremely tight bounds on the change of compression. By further linking the change of compression to the probability of misclassification (or misprediction, in regression problems), powerful and informative generalization bounds are obtained without requiring the use of test-sets. When using our approach on various setups, of which we here present the MNIST classification problem and a synthetic regression problem, we find that P2L returns bounds that are comparable, or superior, to those attained via test-set and PAC-Bayes bounds, while it learns models with better post-training performance.

The idea and the theoretical apparatus behind P2L is presented for the first time in this paper, which also provides a complete set of proofs (given in the appendices) to establish the ensuing generalization results. On the other hand, it has to be said that the usage of the meta-algorithm P2L requires the user to choose a rule by which a new data point is selected at each iteration of the external loop, a choice that is problem-dependent and cannot be part of the general theory. Therefore, the present contribution lays the groundwork of a general new methodology and we expect that this methodology will thrive in the coming few years by adapting it to various specific contexts.

While our demonstrative focus in this paper is on classification and regression, P2L bears a promise of applicability to any data-driven learning problem, including large-scale constrained optimization. Interestingly, in recent years, selection rules to iteratively find "core" examples in the training dataset have been studied in various learning problems, see, e.g., (Toneva et al., 2018; Paul et al., 2021; Yang et al., 2022). These works differ from the present contribution both in their motivations (pruning the training dataset is motivated by computational issues) and results (none of these works provide a compression scheme according to the classical definition and certainly they do not enforce any preference property). Nonetheless, these selection rules can be adopted in the external loop of P2Land we envisage a synergy of our results with these methods. This opens exciting perspectives for future research.

**Structure of the paper.** After introducing the mathematical preliminaries (Section 2), we present the meta-algorithm P2L (Section 3) and its generalization results (Section 4). For the sake of generality, these sections adopt an _abstract viewpoint_ that accommodates various learning frameworks. Applications to MNIST classification and synthetic regression can be found in Sections 5 and 6. Appendix A contains the proof of the main result and various additional results, including an extended version of P2L. Appendix B instead includes some implementation details and additional material for the MNIST application. Finally, Appendix C provides a further numerical experimentation to study the interpretability of the results of P2L. The source code necessary to reproduce all our numerical results can be found in the Supplementary Material, and at at [https://github.com/dario-p/P2L](https://github.com/dario-p/P2L).

## 2 Mathematical Preliminaries

We use \(z\) to denote an example, which is an element from a generic set \(\). For instance, in supervised classification, \(z\) is an input-label pair \((x,y)\) and \(=\{1,2,,M\}\); instead, in supervised regression, \(=\). We assume to have access to \(N\) examples, which we collect in a dataset \(D\); that is, \(D=\{z_{1},z_{2},,z_{N}\}\). Throughout, we model \(D\) as a _multiset_.2 This is motivated by the fact that P2L's output will not depend on the order of appearance of the examples but it will account for (possibly) repeated observations. With multisets, the set operations \(,,,\) extend from their definitions for sets in an obvious way.3 All multisets encountered in our mathematical derivations have a finite number of elements and \(||\) denotes the cardinality where each element is counted as many times as is its multiplicity. The multiset of examples \(D\) is modeled as a realization of \(=\{_{1},_{2},,_{N}\}\), where \(_{1},_{2},,_{N}\) are independent and identically distributed (i.i.d.) random elements taking value in \(\) and defined over a probability space \((,,)\).4

We wish to utilize \(D\) to construct a hypothesis from a hypothesis space \(\), which can be any generic space. For example, if we are tasked with making predictions on the label/output of previously unseen inputs, \(\) can be the space of classifiers/predictors obtained by suitably parameterized neural networks. We assume to be given a _learning algorithm_\(L\) that maps a _list_ of examples to a hypothesis (this is the inner black-box of P2L). Unlike multisets, lists come with a positional order of their elements and our approach works rigorously with learning algorithms \(L\) whose returned hypothesis either depends on this positional order or does not. Considering a list (as opposed to a multiset) as input to \(L\) gives a setup that embraces the most general perspective. This is important as many commonly employed algorithms, e.g., SGD for the training of neural networks, are of the first type.

## 3 The Meta-Algorithm P2L

As anticipated, our goal is that of utilizing a given learning algorithm \(L\) as a building block to construct a meta-algorithm (P2L) that induces a compression scheme with desired properties, regardless of whether the initial learning algorithm has such properties. For this, we need two ingredients: (i) an initial hypothesis \(h_{0}\); (ii) a suitable _criterion of appropriateness_, and a corresponding _appropriateness threshold_. The criterion of appropriateness quantifies to _what extent_ a given hypothesis \(h\) is appropriate for an example \(z\), while the threshold is used to assess - in the form of a yes/no answer - _whether_ the given hypothesis \(h\) is deemed sufficiently appropriate for \(z\). We formalize the idea of criterion of appropriateness, and the corresponding threshold, by introducing a _hypothesis-dependent total order_\(_{h}\) over the extended set \(_{}=\{\}\), where Stop is an external element that is added to \(\) for algorithmic reasons. Given a hypothesis \(h\) (our meta-algorithm iterates over subsequent choices of \(h\) before exiting and \(_{h}\) will be used at each iteration with the current \(h\)), \(_{h}\) orders the elements in \(\) and, in particular, allows us to determine the element that is the least appropriate. The outside element Stop is instead used to model the appropriateness threshold: \(h\) is "enough appropriate" for \(z\) if \(z_{h}\) while it is not appropriate enough if Stop\(_{h}z\). Note that the two conditions cannot be satisfied simultaneously because \(z\) (recall that Stop is an outside element) and \(_{h}\) is a total order. While remarking the generality of this setting, we provide two examples for concreteness and clarity of presentation.

**Example 3.1** (Classification).: Consider a binary classification problem where \(z=(x,y)\) with \(x\) and \(y\{0,1\}\). Commonly employed hypotheses (e.g., neural networks) return the probability that a feature vector is mapped to either of the two classes. A possible criterion measuring the appropriateness of \(h\) for \(z\) is given by the cross-entropy between the label \(y\) and the probability distribution returned by \(h\) in correspondence of \(x\), (Bishop & Nasrabadi, 2006)[Sec. 4.3.4]. Then, the appropriateness threshold can be used to certify if \(z\) is correctly classified by \(h\) by assessing if the cross-entropy is above or below the threshold \(-(0.5)\). This setting is captured by the following total order: for \(z_{1},z_{2}\), we define \(z_{1}_{h}z_{2}\) if the cross-entropy of \(z_{1}\) is no bigger than the cross-entropy of \(z_{2}\) (and ties are broken according to any given rule), while Stop\(_{h}z\) if \(z\) has cross-entropy larger than \(-(0.5)\) and \(z_{h}\) otherwise.

**Example 3.2** (Regression).: In regression, \(z=(x,y)\) with \(x\) and, for example, \(y\). In this setting, a hypothesis is typically a predictor that maps \(x\) into an estimate \((x)\) for \(y\). A possible criterion measuring the appropriateness of \(h\) for \(z\) is given by \(|y-(x)|\), while the appropriateness threshold can be specified by requiring that \(|y-(x)|\) for a given \(\) for which the predictions are sufficiently informative. In this case, the hypothesis-dependent total order can be defined as follows. For \(z_{1},z_{2}\), \(z_{1}_{h}z_{2}\) when \(|y_{1}-(x_{1})||y_{2}-(x_{2})|\) (when \(|y_{1}-(x_{1})|=|y_{2}-(x_{2})|\) but \(z_{1} z_{2}\) the tie can be broken according to any given rule); otherwise, Stop\(_{h}z\) when \(|y-(x)|>\) and \(z_{h}\) when \(|y-(x)|\).

We are now ready to introduce the meta-algorithm P2L (Algorithm 1). P2L works by iteratively feeding the learning algorithm \(L\) with a growing list of training examples taken from \(D\), while terminating when the current hypothesis is deemed sufficiently appropriate (as assessed by the appropriateness threshold) for all the remaining examples that have not been used for training. If this is not the case, P2L appends to the current training examples the example among those not yet used for which the current hypothesis is least appropriate (according to the criterion of appropriateness). P2L returns both the multiset \(T\) of the employed training examples and the final hypothesis \(h\).

```
1:Initialize:\(T=\), \(h=h_{0}\), \(=_{h_{0}}(D_{})\)
2:while\(\)do
3:\(T T\{\}\)\(\) Augment \(T\)
4:\(h L([T]_{})\)\(\) Learn hypothesis
5:\(_{h}(D_{} T)\)\(\) Compute max
6:endwhile
7:return\(h\), \(T\)\(\) Hypothesis \(h\) and multiset \(T\)
```

**Algorithm 1**\((D)\) - The meta-algorithm P2L

Formally, given \((L,h_{0},_{h})\), the meta-algorithm P2L is a map from a dataset \(D\) to two objects: an hypothesis \(h\), and a multiset \(T D\). We denote this with \((h,T)=(D)\). P2L is composed of an initialization, and a main loop where three steps are performed. See Algorithm 1, where \(D_{}=D\{\}\) and \([T]_{}\) is the list of the elements in \(T\) with positional order of each element corresponding to the iteration in which that element is selected by P2L.

In the initialization, we let the multiset of training examples be empty, the hypothesis \(h\) be \(h_{0}\), and compute the maximal element in \(D_{}=D\{\}\) according to \(_{h_{0}}\), which we denote with \(=_{h_{0}}(D_{})\).5 In the main loop, we first check if the maximal element is Stop. If so, the meta-algorithm terminates (thus our naming it Stop). Else, the training multiset is augmented with \(\). We then produce a new hypothesis by running the learning algorithm on \([T]_{}\), i.e., we set \(h L([T]_{})\). Finally, we compute the maximal element in \(D_{} T\) according to the new hypothesis \(h\), that is, \(_{h}(D_{} T)\), and repeat. When the meta-algorithm terminates, it returns both the hypothesis \(h\), and the multiset \(T\).

We conclude with two important remarks. First, note that the choice of the learning algorithm \(L\), ordering \(_{h}\), and initial hypothesis \(h_{0}\) are arbitrary. Interestingly, strong generalization guarantees can be secured at this high level of abstraction (Theorem 4.2) and this allows one to tackle multiple learning problems, including both classification and regression (Sections 5 and 6). Second, observe that the generalization guarantees we will provide rely crucially on the fact that the meta-algorithm

[MISSING_PAGE_FAIL:5]

More precisely, we consider a binary version of the MNIST dataset introduced in the seminal work of (Dziugaite & Roy, 2017) for the specific purpose of comparing the generalization guarantees of PAC-Bayes and other approaches, and later employed in, e.g., (Rivasplata et al., 2019). In this problem, the digits 0-4 and 5-9 are mapped to the labels 0 and 1. To classify the inputs, we employ a fully connected feed-forward neural network with three hidden layers each with 600 nodes and ReLu activation functions. The input has 784 nodes and the output has two nodes which are passed to a softmax function. This architecture, employed in the above-cited works, is used across all our experiments.

MNIST consists of a training dataset containing 60000 examples and a test dataset with 10000 examples. In our experiments, we train all models only on 1000 samples from the training dataset at a time. We particularly care of this "small" dataset setting, in which one has to make the most of the data for both training and assessing the generalization. This is a setting of interest to various fields where data are a limited, possibly costly, resource.6 Specifically, we shuffle the original MNIST training dataset and extract 60 disjoints datasets with 1000 data points each. All approaches we compare are run on the resulting 60 datasets, which are used to both train the network and provide generalization bounds. The _full_ test dataset, containing 10000 examples, is never used in any training phase. It is instead used to evaluate the actual post-training performance of each trained model. We

Figure 2: Top row: Average bounds on the risk (dashed) and misclassification on the test dataset (solid) \(\) one standard deviation for P2L \(\), test-set \(\), and PAC-Bayes \(\) with a confidence of \(=0.035\) (left) and \(=0.001\) (right). The solid markers denote the _best bounds_ achieved and their corresponding post-training performance. Bottom row: Distribution of the upper bounds on the risk and misclassification on the test dataset for the models achieving the _best bounds_ for P2L \(+\), test-set \(+\), and PAC-Bayes \(+\). Precisely, each point in these figures corresponds to the pair (_generalization bound_, _actual misclassification level_) achieved in one of the \(60\) partitions of the MNIST training dataset we use. The means are indicated with a solid diamond, circle, and triangle, respectively.

now present the specific algorithmic choices made for P2L. Due to space limitations, the details concerning the test-set approach and PAC-Bayes can be found in Appendix B.1.

**P2L.** As discussed, P2L is fully specified once a learning algorithm \(L\), an initial hypothesis \(h_{0}\), and a hypothesis-dependent total ordering \(_{h}\) with a criterion of stop are defined. For the learning algorithm \(L\), we choose the widely employed GD (gradient descent) with momentum, which we run for \(200\) epochs with a learning rate of \(0.01\), momentum of \(0.95\), dropout probability of \(0.2\). For the sake of clarity, we remark that each time GD is called by P2L, GD iterates until convergence (or until reaching the allowed number of epochs) so as to perform the minimization of the empirical risk. As initialization of GD, we take the \(h\) returned at the previous iteration of P2L.7 Regarding \(h_{0}\), it is clear that when starting from an educated guess the choice of the worst example to be inserted in \(T\) tends to be more meaningful, allowing P2L to terminate with a smaller \(T\) and, thus, a better generalization bound. To this purpose, one can use a portion of each training dataset to pretrain \(h_{0}\) (using GD again), while P2L and, therefore, the ensuing generalization bound rely on the remaining part of the training dataset. In our computation, we experiment with multiple sizes of the pretraining portion. As for \(_{h}\), given an hypothesis \(h\), i.e., a neural network, we use the total order induced by the cross-entropy of its output as discussed in Example 3.1 (ties are broken according to the lexicographic order of the examples' bit representation). This choice ensures that the statistical risk of Definition 4.1 corresponds to the probability of misclassification, and thus a bound on the statistical risk coincides with a bound on the misclassification. The final bound we present on the misclassification is that of the main result in Theorem 4.2, where the number of samples equals \(N\) minus those used to pretrain the initial hypothesis \(h_{0}\).

**Experimental results.** The results are presented in Figure 2 for different values of the confidence. The top row depicts, the average (over the 60 trials) generalization bounds as well as the average misclassification levels on the test dataset (post-training performances) with their dispersion for the three approaches. The values are plotted as functions of the portion of the dataset (called train/pretrain portion) used to train the the model in GD+test-set, to pretrain the prior distribution in PAC-Bayes, and to pretrain the initial hypothesis \(h_{0}\) in P2L. The joint distribution of the returned bound and actual post-training misclassification level for the best models (corresponding to train/pretrain portion equal to \(0.5\), \(0.6\), \(0.7\) for P2L, PAC-Bayes, and test-set approach, respectively) are shown in the bottom row. The average bounds and misclassification levels for these best models are also presented in Table 1 for \(=0.035\). For the sake of completeness, Table 1 also reports the average running times of one execution (i.e., for one dataset with 1000 examples) of the three approaches (computational resource: Apple MacBook Pro with M1 Pro CPU and 32Gb of ram).

**Conclusions.** Four important observations are in order. First, it is evident that, in the present application, the implementation of P2L with GD outperforms the PAC-Bayes approach with respect to both the provided upper bound on the risk and the post-training performance on the test dataset. This is true not just for the best model learned with each approach (cfr. the solid diamonds and the solid triangles in Figure 2), but it holds uniformly across all train/pretrain portions we tried (cfr. the green and red curves). Notably, the model returned by P2L+GD when using only 10% of the data to pretrain the initial hypothesis \(h_{0}\) has a risk bound that is comparable to that of the _best_ PAC-Bayes model (whose prior is trained on 60% of the data) while it achieves a better post-training performance. Second, P2L+GD provides best risk bounds that are comparable to those of GD+test-set approach, albeit slightly inferior (\(0.143\) vs \(0.129\) for \(=0.035\) and \(0.163\) vs \(0.155\) for \(=0.001\)). However, P2L+GD provides better post-training performance uniformly across all train/pretrain portions. As a matter of fact, for any choice of pretrain portion, P2L+GD's performance is equal to that obtained when GD is run on the whole data set (\(N=1000\)), for which the test-set approach cannot provide

  & Bound on risk & Risk on the test dataset & Difference & Average running time \\  P2L & 0.143 & 0.072 & 0.071 & 2m 1s \\ Test-set & 0.129 & 0.079 & 0.050 & 0m 5s \\ Pac-Bayes & 0.177 & 0.088 & 0.089 & 4m 1s \\ 

Table 1: Risk of _best models_ for \(=0.035\).

any meaningful bound since no data are left for testing the model. This is a clear indication of the important feature that _P2L utilizes all data to jointly learn a good model and provide a risk bound._ Third, the post-training performance of PAC-Bayes and GD+test-set approaches are similar. This suggests that in PAC-Bayes the training of the posterior does not exploit the additional available data to improve the model, but rather to certify it, in a similar vein as in the test-set approach. This fact has also been observed recently in (Loft et al., 2022)[Fig 1(a)]. Fourth, as for the tightness of the bounds (i.e., the difference between the upper bounds and risk on the test dataset), GD+test-set approach provides the tightest results, while P2L+GD and PAC-Bayes approaches alternate depending on the specific train/pretrain portion selected.

## 6 Application to a synthetic regression problem

In this section we apply our methodology to a synthetic regression problem in order to showcase the flexibility of the approach introduced in Section 4.

Specifically, we consider \(100\) distinct training datasets with \(N=200\) examples \((x,y)\) and then another \(100\) distinct training datasets with \(N=500\) examples \((x,y)\). The input \(x[-0.5,1.5]\) is extracted uniformly at random and the output is \(y=f(x)+e\) with \(f(x)=(2.5 x)/(2.5 x)\) and \(e\) extracted according to a normal distribution with zero mean and standard deviation \(0.05\), see Figure 3. For each dataset, our objective is twofold: finding a model that fits the data well, and providing a bound on the probability that the model mispredicts the output by more than a threshold \(=0.1\) (bounds of this sort matters in relation to many applications).

In pursuing this goal, we compare the generalization bounds and post-training performances attained using P2L and test-set bounds. For illustration purposes, in all our experiments we consider a simple network architecture comprising one input node, one output node, and one hidden layer with six nodes, each equipped with a \(\) activation function. To evaluate the post-training performance, we use an additional test dataset with \(20000\) examples. The value of \(\) is set to \(0.035\).

**P2L.** In deploying P2L we use GD as learning algorithm \(L\), and, similarly to the MNIST example, we experiment with initial hypotheses \(h_{0}\) trained on different fractions of the training dataset (including the null fraction in which case \(h_{0}\) is the network with all weights set to \(0\)). We fix the same total order \(_{h}\) of Example 3.2, with \(=0.1\). In this context, the risk introduced in Definition 4.1 measures the probability that the output \(y\) is mispredicted by more than \(\), thus giving us a guarantee on the quality of the model. The generalization bound we use is that in Theorem 4.2, where the number of samples equals \(N\) minus those used to pretrain the initial hypothesis \(h_{0}\). We use a learning rate of \(0.1\), momentum of \(0.95\), \(1000\) epochs, and no dropout.

**Test-set approach.** We apportion each dataset in two, with one portion used for training, and the other for deriving the generalization bounds. We run GD for 1000 epochs, perform a grid search over learning rates [0.001, 0.005, 0.01], momentum [0.9, 0.95] (no dropout), and select those giving the best generalization bound. As in the MNIST example (see Appendix B.1), we use the binomial test-set bound of (Langford, 2005)[Thm 3.3] with a number of samples equal to \(N\) minus those used for training.

**Experimental results.** The first two panels of Figure 4 present the average (over the \(100\) trials) upper bounds on the risk and the average risk on the test dataset jointly with their dispersion, as a function of the data portion used to train the model (GD+test-set) or to pretrain the initial hypothesis (P2L). The third and fourth panels depict the distribution of the upper bound and of the risk on the test dataset

Figure 3: An instance of dataset used for regression. The crosses are examples \((x,y)\). The black line depicts the function \(f(x)=(2.5 x)/(2.5 x)\) around which noise is added. The red dashed and blue solid lines represent the fit obtained with a choice of train/pretrain portion equal \(0.3\) (best) for P2L and test-set approaches. The red shaded region depicts a tube of radius \(0.1\) around the red model. The risk of the red model is the probability that an unseen example falls outside the shaded region.

for the data proportion 0.3, returning the best learned models for both P2L and the test-set approach. Their averages are compared in Table 2, which also includes the average runtime (computational resource: Apple MacBook Pro with M1 Pro CPU and 32Gb of ram).

**Conclusions.** First, it appears evident that, in this synthetic regression problem, the application of P2L with GD provides superior results to those achieved by GD with the test-set approach, regardless of the size of the datasets used for training. This holds true jointly for the upper bound and the risk on the test dataset uniformly across all train/pretrain portions (cfr. the red and blue dashed lines, similarly for the solid ones). We ascribe this result to the fact that P2L does not set aside data for testing, and yet it also provides rigorous evaluations of the risk. Second, the test-set approach provides bounds that are closer to the risk it incurs on the test dataset for \(N=500\), while this effect is less clear with \(N=200\). Third, as expected, when the size of the training dataset grows, both the upper bounds and the risk on the test dataset improve.

To conclude, it is fair to notice that GD does not pursue directly the goal of obtaining a prediction error smaller than \(\), and this may justify the large gap in the post-training performance between

  & Bound on risk & Risk on the test dataset & Difference & Average running time \\  P2L & 0.172 & 0.064 & 0.108 & 1.18s \\ Test-set & 0.250 & 0.173 & 0.077 & 0.21s \\ 

Table 2: Risk of _best models_ for \(N=200\), \(=0.035\)

Figure 4: Top row: Average bounds on the risk (dashed) and risk on the test dataset (solid) \(\) one standard deviation for P2L \(\) and test-set \(\) approaches, with \(N=200\) (left) and \(N=500\) (right). The solid markers denote the best average bounds and the corresponding risks on the test dataset. Bottom row: Empirical distribution of the bound and risk on the test dataset for the data split returning the best average bound for P2L \(+\) and test-set \(+\). Their means are indicated with a solid diamond and circle, respectively. Note that the test-set approach is not considered for zero train portion, as it always requires a non-zero amount of data to train the model. Observe that three instances in the test-set case (out of one hundred) in the bottom row violate the upper bound. This is in line with the choice of \(=0.035\), suggesting that the bound will fail, on average, on \(3.5\) datasets out of one hundred.

the two approaches also when almost all the dataset is used for training in the test-set approach. Although GD is used as inner algorithm \(L\) in P2L, it seems that a good performance with respect to the chosen appropriateness criterion can be obtained thanks to the structure of the meta-algorithm. This is another interesting feature of P2L.

## 7 Conclusions, Limitations and Future research

We have proposed a novel framework called P2L to provide virtually any learning algorithm with sharp generalization bounds. Our approach is based on making a given learning algorithm into a compression scheme with desirable properties, thus enabling the use of powerful generalization results. Numerical results show that P2L is capable of learning hypotheses with post-training performances and generalization bounds equal or superior to the state of the art.

**Computational aspects.** While P2L requires learning a hypothesis over a training set of increasing size, and thus might be less efficient than learning the hypothesis only once over the full dataset, it is important to look at it from the appropriate perspective: we are concerned with settings where data is limited or costly to acquire (see, e.g., Footnote 6 and ensuing discussion) while computations are performed off-line and fast execution does not represent a primary concern (a setup also considered in other recent work, e.g., (Foong et al., 2021)).

**Source of conservatism.** In our framework, as revealed by the proof of Theorem 4.2, the risk is controlled by bounding the probability of change of compression (as defined in Equation (2) in Appendix A.1). Notably, the mismatch between the risk and the probability of change of compression is the _only_ source of conservatism our approach needs to resolve since the upper and lower bounds on the probability of change of compression from Theorem A.4 are _extremely tight_ - see Remark A.9. The magnitude of this conservatism is determined by the choices we make in specializing the proposed meta-algorithm. For a given learning algorithm \(L\), these choices entail selecting an initial hypothesis \(h_{0}\) and an hypothesis-dependent total order used to select which data points are fed to the learning algorithm \(L\). Our experimenting with multiple initial hypotheses (trained with different portions of the dataset) was _solely_ geared at reducing this gap. Indeed, after training \(h_{0}\), P2L allows the data to "freely speak", and thus improve the resulting hypothesis, as it can be appreciated from the fact that the misclassification on the test dataset for P2L is constant across _all_ prior/train portions (see Figure 2, top row). This is in stark contrast with the test-set approach and even PAC-Bayes. In the former, data are either used to train the model _or_ to provide a risk bound. In the latter, data employed to train the posterior are effectively used to compute a risk bound as opposed to significantly improving the quality of the prior (see first row in Figure 2). We conclude noting that our choices of the initial hypothesis and total order is but one of many possible. Depending on the specific learning problem, other choices can be made (see for examples the last part of the Introduction). We believe that the overall fact that the theoretical apparatus in our approach clearly identifies the sole source of conservatism will put us and others in the position to build upon the P2L framework beyond this work.