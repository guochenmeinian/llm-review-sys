# Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances

Mikhail Khodak

CMU

khodak@cmu.edu

&Edmond Chow

Georgia Tech.

&Maria-Florina Balcan

CMU

&Ameet Talwalkar

CMU

###### Abstract

Solving a linear system \(=\) is a fundamental scientific computing primitive for which numerous solvers and preconditioners have been developed. These come with parameters whose optimal values depend on the system being solved and are often impossible or too expensive to identify; thus in practice sub-optimal heuristics are used. We consider the common setting in which many related linear systems need to be solved, e.g. during a single numerical simulation. In this scenario, can we sequentially choose parameters that attain a near-optimal overall number of iterations, without extra matrix computations? We answer in the affirmative for Successive Over-Relaxation (SOR), a standard solver whose parameter \(\) has a strong impact on its runtime. For this method, we prove that a bandit online learning algorithm--using only the number of iterations as feedback--can select parameters for a sequence of instances such that the overall cost approaches that of the best fixed \(\) as the sequence length increases. Furthermore, when given additional structural information, we show that a _contextual_ bandit method asymptotically achieves the performance of the _instance-optimal_ policy, which selects the best \(\) for each instance. Our work provides the first learning-theoretic treatment of high-precision linear system solvers and the first end-to-end guarantees for data-driven scientific computing, demonstrating theoretically the potential to speed up numerical methods using well-understood learning algorithms.

## 1 Introduction

The bottleneck subroutine in many science and engineering computations is a solver that returns a vector \(\) approximating the solution \(^{-1}\) to a linear system. A prominent example is in partial differential equations (PDEs), whose solutions often involve solving sequences of high-dimensional linear systems, often to very high precision . As a result, a vast array of solvers and preconditioners have been developed, many of which have tunable parameters; these can have a strong, quantifiable effect on runtime, e.g. via their impact on condition numbers or the spectral radius of an iteration matrix . There is a long literature analyzing these algorithms, and indeed for some problems we have a strong understanding of the optimal parameters for a given matrix. However, computing them can sometimes be more costly than solving the original system, leading to an assortment of heuristics for setting good parameters .

Our goal will be to provide an alternative to (possibly suboptimal) heuristics by taking advantage of the fact that, in practice, we often solve many linear systems at a time. A natural approach is to treat these instances as data to be passed to a machine learning (ML) algorithm; in particular, due to the sequential nature of many scientific computing tasks, the framework of online learning  provides a natural language for reasoning about it. For example, if we otherwise would solve a sequence of \(T\) linear systems \((_{1},_{1}),,(_{T},_{T})\) using a given solver with a fixed parameter, can we use ML to do as well as the best choice of that parameter, i.e. can we _minimize regret_? Or, if the matrices are all diagonal shifts of single matrix \(\), can we learn the functional relationship between the shift \(c_{t}\) and the optimal solver parameter for \(_{t}=+c_{t}_{n}\), i.e. can we predict using _context_?We investigate these questions for the Successive Over-Relaxation (SOR) solver, a generalization of Gauss-Seidel whose relaxation parameter \((0,2)\) dramatically affects the number of iterations (c.f. Figure 1, noting the log-scale). SOR is well-studied and often used as a preconditioner for Krylov methods (e.g. conjugate gradient (CG), as a basis for semi-iterative approaches, and as a multigrid smoother. Analogous to some past setups in data-driven algorithms [8; 36], we will sequentially set relaxation parameters \(_{t}\) for SOR to use when solving each linear system \((_{t},_{t})\). Unlike past theoretical studies of related methods [31; 12; 11], we aim to provide _end-to-end_ guarantees--covering the full pipeline from data-intake to efficient learning to execution--while minimizing dependence on the dimension (\(n\) can be \(10^{6}\) or higher) and precision (\(1/\) can be \(10^{8}\) or higher). We emphasize that we do _not_ seek to immediately improve the empirical state of the art, and also that existing research on saving computation when solving sequences of linear systems (recycling Krylov subspaces, reusing preconditioners, etc.) is complementary to our own, i.e. it can be used in addition to the ideas presented here.

### Core contributions

We study two distinct theoretical settings, corresponding to views on the problem from two different approaches to data-driven algorithms. In the first we have a deterministic sequence of instances and study the spectral radius of the iteration matrix, the main quantity of interest in classical analysis of SOR . We show how to convert its asymptotic guarantee into a surrogate loss that upper bounds the number of iterations via a quality measure of the chosen parameter, in the style of _algorithms with predictions_. The bound holds under a _near-asymptotic_ condition implying that convergence occurs near the asymptotic regime, i.e. when the spectral radius of the iteration matrix governs the convergence. We verify the assumption and show that one can learn the surrogate losses using only bandit feedback from the original costs; notably, despite being non-Lipschitz, we take advantage of the losses' unimodal structure to match the optimal \(}(T^{2/3})\) regret for Lipschitz bandits . Our bound also depends only logarithmically on the precision and not at all on the dimension. Furthermore, we extend to the diagonally shifted setting described before, showing that an efficient, albeit pessimistic, contextual bandit (CB) method has \(}(T^{3/4})\) regret w.r.t. the instance-optimal policy that always picks the best \(_{t}\). Finally, we show a similar analysis of learning a relaxation parameter for the more popular (symmetric SOR-preconditioned) CG method.

Our second setting is _semi-stochastic_, with target vectors \(_{t}\) drawn i.i.d. from a (radially truncated) Gaussian. This is a reasonable simplification, as convergence usually depends more strongly on \(_{t}\), on which we make no extra assumptions. We show that the expected cost of running a symmetric variant of SOR (SSOR) is \(()()\)-Lipschitz w.r.t. \(\), so we can (a) compete with the optimal number of iterations--rather than with the best upper bound--and (b) analyze more practical, regression-based CB algorithms [26; 55]. We then show \(}(})\) regret when comparing to the single best \(\) and \(}(T^{9/11})\) regret w.r.t. the instance-optimal policy in the diagonally shifted setting using a novel, Chebyshev regression-based CB algorithm. While the results do depend on the dimension \(n\), the dependence is much weaker than that of past work on data-driven tuning of a related regression problem .

**Remark 1.1**.: _Likely the most popular algorithms for linear systems are Krylov subspace methods such as CG. While an eventual aim of our line of work is to understand how to tune (many) parameters of (preconditioned) CG and other algorithms, SOR is a well-studied method and serves as a meaningful starting point. In fact, in Appendix A.3 we show that our near-asymptotic analysis extends directly, and in the semi-stochastic setting there is a natural path to (e.g.) SSOR-preconditioned CG, as it can be viewed as computing polynomials of iteration matrices where SSOR just takes powers. Lastly, apart from its use as a preconditioner and smoother, SOR is still sometimes preferred for direct use as well [27; 61; 37; 63; 64]._

### Technical and theoretical contributions

By studying a scientific computing problem through the lens of data-driven algorithms and online learning, we also make the following contributions to the latter two fields:

1. Ours is the first comparison of two leading theoretical approaches to data-driven algorithms applied to the same problem. While the algorithms with predictions  approach in Section 2 takes better advantage of the existing scientific computing literature to obtain (arguably) more interpretable and dimension-independent bounds, data-driven algorithm design  competes directly with the quantity of interest in Section 3 and enables provable guarantees for modern CB algorithms.
2. For algorithms with predictions, our approach of showing near-asymptotic performance bounds may be extendable to other iterative algorithms, as we demonstrate with CG. We also show that such performance bounds on a (partially-observable) cost function are learnable even when the bounds themselves are too expensive to compute.
3. In data-driven algorithm design, we take the novel approach of proving continuity of _the expectation of_ a discrete cost, rather than showing dispersion of its discontinuities  or bounding predicate complexity .
4. We introduce the idea of using CB to sequentially set instance-optimal algorithmic parameters.
5. We show that standard discretization-based bandit algorithms are optimal for sequences of adversarially chosen _semi-Lipschitz_ losses that generalize regular Lipschitz functions (c.f. Appendix C).
6. We introduce a new CB method that combines SquareCB  with Chebyshev polynomial regression to get sublinear regret on Lipschitz losses (c.f. Appendix D).

### Related work and comparisons

We discuss the existing literature on solving sequences of linear systems [51; 57; 25], work integrating ML with scientific computing to amortize cost [2; 4], and past theoretical studies of data-driven algorithms [31; 11] in Appendix A. For the latter we include a detailed comparison of the generalization implications of our work with the GJ framework . Lastly, we address the baseline of approximating the spectral radius of the Jacobi iteration matrix.

## 2 Asymptotic analysis of learning the relaxation parameter

We start this section by going over the problem setup and the SOR solver. Then we consider the asymptotic analysis of the method to derive a reasonable performance upper bound to target as a surrogate loss for the true cost function. Finally, we prove and analyze online learning guarantees.

### Setup

At each step \(t=1,,T\) of (say) a numerical simulation we get a linear system instance, defined by a matrix-vector pair \((_{t},_{t})^{n n}^{n}\), and are asked for a vector \(^{n}\) such that the norm of its _residual_ or _defect_\(=_{t}-_{t}\) is small. For now we define "small" in a relative sense, specifically \(\|_{t}-_{t}\|_{2}\|_{t }\|_{2}\) for some _tolerance_\((0,1)\); note that when using an iterative method initialized at \(=_{n}\) this corresponds to reducing the residual by a factor \(1/\), which we call the _precision_. In applications it can be quite high, and so we will show results whose dependence on it is at worst logarithmic. To make the analysis tractable, we make two assumptions (for now) about the matrices \(\): they are symmetric positive-definite and consistently-ordered (c.f. Hackbusch , Definition 4.23). We emphasize that, while not necessary for convergence, both are standard in the analysis of SOR ; see Hackbusch [32; Criterion 4.24] for multiple settings where they holds.

To find a suitable \(\) for each instance in the sequence we apply Algorithm 3 (SOR), which at a high-level works by multiplying the current residual \(\) by the inverse of a matrix \(_{}\)--derived from the diagonal \(\) and lower-triangular component \(\) of \(\)--and then adding the result to the current iterate \(\). Note that multiplication by \(_{}^{-1}\) is efficient because \(_{}\) is triangular. We will measure the cost of this algorithm by the number of iterations it takes to reach convergence, which we denote by \((,,)\), or \(_{t}()\) for short when it is run on the instance \((_{t},_{t})\). For simplicity, we will assume that the algorithm is always initialized at \(=_{n}\), and so the first residual is just \(\).

Having specified the computational setting, we now turn to the learning objective, which is to sequentially set the parameters \(_{1},,_{T}\) so as to minimize the total number of iterations:

\[_{t=1}^{T}_{t}(_{t})=_{t=1}^{T} (_{t},_{t},_{t}) \]

Figure 1: **Left:** comparison of different cost estimates. **Center-left:** mean performance of different parameters across forty instances of form \(+_{n}\), where \(c(2,6)\). **Center-right:** the same but for \(c(1/2,3/2)\), which is relatively higher-variance. In both cases the dashed line indicates instance-optimal performance, the matrix \(\) is a discrete Laplacian of a \(100 100\) square domain, and the targets \(\) are truncated Gaussians. **Right:** asymptocity as measured by the difference between the spectral norm at iteration \(k\) and the spectral radius, together with its upper bound \((1-(_{}))\).

To set \(_{t}\) at some time \(t>1\), we allow the learning algorithm access to the costs \(_{s}(_{s})\) incurred at the previous steps \(s=1,,t-1\); in the literature on online learning this is referred to as the _bandit_ or _partial feedback_ setting, to distinguish from the (easier, but unreasonable for us) _full information_ case where we have access to the cost function \(_{s}\) at every \(\) in its domain.

Selecting the optimal \(_{t}\) using no information about \(_{t}\) is impossible, so we must use a _comparator_ to obtain an achievable measure of performance. In online learning this is done by comparing the total cost incurred (1) to the counterfactual cost had we used a _single_, best-in-hindsight \(\) at every timestep \(t\). We take the minimum over some domain \((0,2)\), as SOR diverges outside it. While in some settings we will compete with every \((0,2)\), we will often algorithmically use \([1,_{}]\) for some \(_{}<2\). The upper limit ensures a bound on the number of iterations--required by bandit algorithms--and the lower limit excludes \(<1\), which is rarely used because theoretical convergence of vanilla SOR is worse there for realistic problems, e.g. those satisfying our assumptions.

This comparison-based approach for measuring performance is standard in online learning and effectively assumes a good \(\) that does well-enough on all problems; in Figure 1 (center-left) we show that this is sometimes the case. However, the center-right plot in the same figure shows we might do better by using additional knowledge about the instance; in online learning this is termed a _context_ and there has been extensive development of contextual bandit algorithms that do as well as the best fixed policy mapping contexts to predictions. We will study an example of this in the _diagonally shifted_ setting, in which \(_{t}=+c_{t}_{n}\) for scalars \(c_{t}\); while mathematically simple, this is a well-motivated structural assumption in applications . Furthermore, the same learning algorithms can also be extended to make use of other context information, e.g. rough spectral estimates.

### Establishing a surrogate upper bound

Our first goal is to solve \(T\) linear systems almost as fast as if we had used the best fixed \(\). In online learning, this corresponds to minimizing _regret_, which for cost functions \(_{t}:\) is defined as

\[_{}(\{_{t}\}_{t=1}^{T})=_{t=1}^{T }_{t}(_{t})-_{}_{t=1}^{T}_{t}( ) \]

In particular, since we can upper-bound the objective (1) by \(_{}(\{_{t}\}_{t=1}^{T})\) plus the optimal cost \(_{}_{t=1}^{T}_{t}()\), if we show that regret is _sublinear_ in \(T\) then the leading-order term in the upper bound corresponds to the cost incurred by the optimal fixed \(\).

Many algorithms attaining sublinear regret under different conditions on the losses \(_{t}\) have been developed . However, few handle losses with discontinuities--i.e. most algorithmic costs--and those that do (necessarily) need additional conditions on their locations . At the same time, numerical analysis often deals more directly with continuous asymptotic surrogates for cost, such as convergence rates. Taking inspiration from this, and from the algorithms with predictions idea of deriving surrogate loss functions for algorithmic costs , in this section we instead focus on finding _upper bounds_\(U_{t}\) on \(_{t}\) that are both (a) learnable and (b) reasonably tight in-practice. We can then aim for overall performance nearly as good as the optimal \(\) as measured by these upper bounds:

\[_{t=1}^{T}_{t}(_{t})_{t=1}^{T}U_{t}(_{t}) =_{}(\{U_{t}\}_{t=1}^{T})+_{} _{t=1}^{T}U_{t}()=o(T)+_{}_{t=1}^{T}U_{t}() \]

A natural approach to get a bound \(U_{t}\) is via the _defect reduction matrix_\(_{}=_{n}-(/+)^{-1}\), so named because the residual at iteration \(k\) is equal to \(_{}^{k}\) and \(\) is the first residual. Under our assumptions on \(\), Young  shows that the spectral radius \((_{})\) of \(_{}\) is a (nontrivial to compute) piecewise function of \(\) with a unique minimum in \([1,2)\). Since we have error \(\|_{}^{k}\|_{2}/\|\|_{2}\|_{ }^{k}\|_{2}\) at iteration \(k\), \((_{})=_{k}[k]{\|_{}^{k}\| _{2}}\) asymptotically bounds how much the error is reduced at each step. It is thus often called the _asymptotic convergence rate_ and the number of iterations is said to be roughly bounded by \(_{})}\) (e.g. Hackbusch [32, Equation 2.31b]). However, while it is tempting to use this as our upper bound \(U\), in fact it may not upper bound the number of iterations at all, since \(_{}\) is not normal and so in-practice the iteration often goes through a transient phase where the residual norm first _increases_ before decreasing [60, Figure 25.6].

Thus we must either take a different approach or make some assumptions. Note that one can in-fact show an \(\)-dependent, finite-time convergence bound for SOR via the energy norm [32, Corollary 3.45], but this can give rather loose upper bounds on the number of iterations (c.f. Figure 1 (left)). Instead, we make the following assumption, which roughly states that convergence always occurs _near_ the asymptotic regime, where nearness is measured by a parameter \((0,1)\):

**Assumption 2.1**.: _There exists \((0,1)\) s.t. \(\ \) the matrix \(_{}=_{n}-(/+)^{-1}\) satisfies \(\|_{}^{k}\|_{2}((_{})+(1-( _{})))^{k}\) at \(k=_{\|_{}^{k+1}\|_{2}<\|\|_{2}}i\)._

This effectively assumes an upper bound \((_{})+(1-(_{}))\) on the empirically observed convergence rate, which gives us a measure of the quality of each parameter \(\) for the given instance \((,)\). Note that the specific form of the surrogate convergence rate was chosen both because it is convenient mathematically--it is a convex combination of 1 and the asymptotic rate \((_{})\)--and because empirically we found the degree of "asymptocity" as measured by \(\|_{}^{k}\|_{2}^{1/k}-(_{})\) for \(k\) right before convergence to vary reasonably similarly to a fraction of \(1-(_{})\) (c.f. Figure 1 (right)). This makes intuitive sense, as the parameters \(\) for which convergence is fastest have the least time to reach the asymptotic regime. Finally, note that since \(_{k}\|_{}^{k}\|_{2}^{1/k}=(_{})\), for every \(>0\) there always exists \(k^{}\) s.t. \(\|_{}^{k}\|_{2}((_{})+)^{k}\)\(\ k k^{}\); therefore, since \(1-(_{})>0\), we view Assumption 2.1 not as a restriction on \(_{}\) (and thus on \(\)), but rather as an assumption on \(\) and \(\). Specifically, the former should be small enough that \(_{}^{t}\) reaches that asymptotic regime for some \(i\) before the criterion \(\|_{}^{k}\|_{2}\|\|_{2}\) is met; for similar reasons, the latter should not happen to be an eigenvector corresponding to a tiny eigenvalue of \(_{}\) (c.f. Figure 2 (left)).

Having established this surrogate of the spectral radius, we can use it to obtain a reasonably tight upper bound \(U\) on the cost (c.f. Figure 1 (left)). Crucially for learning, we can also establish the following properties via the functional form of \((_{})\) derived by Young :

**Lemma 2.1**.: _Define \(U()=1+_{})+(1- (_{})))}\), \(=+(1-)\{^{2},_{}-1\}\), and \(^{*}=1+^{2}/(1+})^{2}\), where \(=(_{n}-^{-1})\). Then the following holds:_

1. \(U\) _bounds the number of iterations and is itself bounded:_ \((,,)<U() 1+\)__
2. \(U\) _is decreasing towards_ \(^{*}\)_, and_ \(}\)_-Lipschitz on_ \(^{*}\) _if_ \(}\) _or_ \(^{2}}(1-})\)__

Lemma 2.1 introduces a quantity \(=+(1-)\{^{2},_{}-1\}\) that appears in the upper bounds on \(U()\) and in its Lipschitz constant. This quantity will in some sense measure the difficulty of learning: if \(\) is close to 1 for many of the instances under consideration then learning will be harder. Crucially, all quantities in the result are spectral and do not depend on the dimensionality of the matrix.

### Performing as well as the best fixed \(\)

Having shown these properties of \(U\), we now show that it is learnable via Tsallis-INF , a bandit algorithm which at each instance \(t\) samples \(_{t}\) from a discrete probability distribution over a grid of \(d\) relaxation parameters, runs SOR with \(_{t}\) on the linear system \((_{t},_{t})\), and uses the number of iterations required \(_{t}(_{t})\) as feedback to update the probability distribution over the grid. The scheme is described in full in Algorithm 1. Note that it is a relative of the simpler and more familiar Exp3 algorithm , but has a slightly better dependence on the grid size \(d\). In Theorem 2.1, we bound the cost of using the parameters \(_{t}\) suggested by Tsallis-INF by the total cost of using the best fixed parameter \(\) at all iterations--as measured by the surrogate bounds \(U_{t}\)--plus a term that increases sublinearly in \(T\) and a term that decreases in the size of the grid.

**Theorem 2.1**.: _Define \(_{t}=_{t}+(1-_{t})\{_{t}^{2},_{}-1\}\), where \(_{t}=(_{n}-_{t}^{-1}_{t})\) and \(_{t}\) is the minimal \(\) satisfying Assumption 2.1 and the second part of Lemma 2.1. If we run Algorithm 1 using SOR initialized at \(=_{n}\) as the solver, \(_{[i]}=1+(_{}-1)\) as the parameter grid, normalization \(K}}\) for \(_{}=_{t}_{t}\), and step-size \(=1/\) then the expected number of iterations is bounded as_

[MISSING_PAGE_EMPTY:6]

\(\) can be described as coming from a radially truncated normal distribution. Note also that the exact choice of truncation was done for convenience; any finite bound \( n\) yields similar results.

We also make two other changes: (1) we study _symmetric_ SOR (SSOR) and (2) we use an absolute convergence criterion, i.e. \(\|_{k}\|_{2}\), not \(\|_{k}\|_{2}\|_{0}\|_{2}\). Symmetric SOR (c.f. Algorithm 8) is very similar to the original, except the linear system being solved at every step is now symmetric: \(}_{}=_{} ^{-1}_{}^{T}\). Note that the defect reduction matrix \(}_{}=_{n}-}_{ }^{-1}\) is still not normal, but it _is_ (non-orthogonally) similar to a symmetric matrix, \(^{-1/2}}_{}^{1/2}\). SSOR is twice as expensive per-iteration, but often converges in fewer steps, and is commonly used as a base method because of its spectral properties (e.g. by the Chebyshev semi-iteration, c.f. Hackbusch [32, Section 8.4.1]).

### Regularity of the expected cost function

We can then show that the expected cost \(_{}(,,)\) is Lipschitz w.r.t. \(\) (c.f. Corollary G.1). Our main idea is the observation that, whenever the error \(\|}_{}^{k}\|_{2}\) falls below the tolerance \(\), randomness should ensure that it does not fall so close to the threshold that the error \(\|}_{^{}}^{k}\|_{2}\) of a nearby \(^{}\) is not also below \(\). Although clearly related to dispersion , here we study the behavior of a continuous function around a threshold, rather than the locations of the costs' discontinuities.

Our approach has two ingredients, the first being Lipschitzness of the error \(\|}_{}^{k}\|_{2}\) at each iteration \(k\) w.r.t. \(\), which ensures \(\|}_{^{}}^{k}\|_{2}(, +(|-^{}|)]\) if \(\|}_{}^{k}\|_{2}<\|}_{^{}}^{k}\|_{2}\). The second ingredient is anti-concentration, specifically that the probability that \(\|}_{}^{k}\|_{2}\) lands in \((,+(|-^{}|)]\) is \((|-^{}|)\). While intuitive, both steps are made difficult by powering: for high \(k\) the random variable \(\|}_{}^{k}\|_{2}\) is highly concentrated because \((}_{}) 1\); in fact its measure over the interval is \((|-^{}|/(}_{})^{k})\). To cancel this, the Lipschitz constant of \(\|}_{}^{k}\|_{2}\) must scale with \((}_{})^{k}\), which we can show because switching to SSOR makes \(}_{}^{k}\) is similar to a normal matrix. The other algorithmic modification we make--using absolute rather than relative tolerance--is so that \(\|}_{}^{k}\|_{2}^{2}\) is (roughly) a sum of i.i.d. \(^{2}\) random variables; note that the square of relative tolerance criterion \(\|}_{}^{k}\|_{2}^{2}/\|\|_{2}^{2}\) does not admit such a result. At the same time, absolute tolerance does not imply an a.s. bound on the number of iterations if \(\|\|_{2}\) is unbounded, which is why we truncate its distribution.

Lipschitzness follows because \(|_{}()-_{}(^{})|\) can be bounded using Jensen's inequality by the probability that \(\) and \(^{}\) have different costs \(k l\), which is at most the probability that \(\|}_{}^{k}\|_{2}\) or \(\|}_{^{}}^{l}\|_{2}\) land in an interval of length \((|-^{}|)\). Note that the Lipschitz bound includes an \(}()\) factor, which results from \(}_{}^{k}\) having stable rank \( n\) due to powering. Regularity of \(_{}\) leads directly to regret guarantee for the same algorithm as before, Tsallis-INF:

**Theorem 3.1**.: _Define \(_{}=_{t}(_{t})\) to be the largest condition number and \(_{}=_{t}(_{n}-_{t}^{-1}_{t})\). Then there exists \(K=()\) s.t. running Algorithm 1 with SSOR has regret_

\[_{t=1}^{T}_{t}(_{t})-_{[1, _{}]}_{t=1}^{T}_{t}() 2K+T}{ _{}^{4}d}}{}} \]

Setting \(d=(K^{2})\) yields a regret bound of \((^{2}})\). Note that, while this shows convergence to the true optimal parameter, the constants in the regret term are much worse, not just due to the dependence on \(n\) but also in the powers of the number of iterations. Thus this result can be viewed as a proof of the asymptotic (\(T\)) correctness of Tsallis-INF for tuning SSOR.

Figure 2: **Left:** solver cost for b drawn from a truncated Gaussian v.s. b a small eigenvector of \(_{1.4}\). **Center-left:** cost to solve 5K diagonally shifted systems \(_{t}=+-3}{20}_{n}\) for \(c_{t}(2,6)\). **Center-right:** total SSOR-preconditioned CG iterations taken while solving the 2D heat equation with a time-varying diffusion coefficient (used as context) on different grids, as a function of the linear system dimension. **Right:** (smoothed) parameters chosen at each timestep of one such simulation, overlaid on a contour plot of the cost of solving the system at step \(t\) with parameter \(\) (c.f. Appendix H).

```
0: solver \(:^{n n}^{n} _{>0}\), instance sequence \(\{(_{t},_{t})\}_{t=1}^{T}^{n n} ^{n}\),  context sequence \(\{c_{t}\}_{t=1}^{T}[c_{},c_{}+C]\), learning rate \(>0\), parameter grid \(^{d}\),  Chebyshev polynomial features \(:[c_{},c_{}+C]^{m+1}\), normalizations \(K,L,N>0\) for\(t=1,,T\)do \(_{i}|,|_{i}| }{}=i}{}^{t-1}(( ,(c_{s}))-)^{2}\ \ i[d]\)// update models \(_{[i]}_{i},(c_{t})\ \ i[d]\)// compute model predictions \(i^{*}_{[d]}|_{[i]}|\)\(_{[i]}_{[i]}-_{[i^{*}]})}\ \ i i^{*}\)// compute probability of each action \(_{[i^{*}]} 1-_{i i^{*}}_{[i]}\) sample \(i_{t}[d]\) w.p. \(_{[i_{t}]}\) and set \(_{t}=_{[i_{t}]}\)// sample action \(k_{t}(_{t},_{t},_{t})-1\)// run solver and update cost
```

**Algorithm 2**ChebC: SquareCB with a follow-the-leader oracle and polynomial regressor class.

### Chebyshev regression for diagonal shifts

For the shifted setting, we can use the same approach to prove that \(_{}(+c_{n},,)\) is Lipschitz w.r.t. the diagonal offset \(c\) (c.f. Corollary G.2); for \(n=O(1)\) this implies regret \(}(T^{3/4})\) for the same discretization-based algorithm as in Section 2.4. While optimal for Lipschitz functions, the method does not readily adapt to nice data, leading to various smoothed comparators ; however, as we wish to compete with the true optimal policy, we stay in the original setting and instead highlight how this section's semi-stochastic analysis allows us to study a very different class of bandit algorithms.

In particular, since we are now working directly with the cost function rather than an upper bound, we are able to utilize a more practical regression-oracle algorithm, SquareCB . It assumes a class of regressors \(h:[c_{},c_{}+C][d]\) with at least one function that perfectly predicts the expected performance \(_{}(+c_{n},, _{[i]})\) of each action \(_{[i]}\) given the context \(c\); a small amount of model misspecification is allowed. If there exists an online algorithm that can obtain low regret w.r.t. this function class, then SquareCB can obtain low regret w.r.t. any policy.

To apply it we must specify a suitable class of regressors, bound its approximation error, and specify an algorithm attaining low regret over this class. Since \(m\) terms of the Chebyshev series suffice to approximate a Lipschitz function with error \(}(1/m)\), we use Chebyshev polynomials in \(c\) with learned coefficients--i.e. models \(,(c)=_{j=0}^{m}_{[j]}P_{j}(c)\), where \(P_{j}\) is the \(j\)th Chebyshev polynomial--as our regressors for each action. To keep predictions bounded, we add constraints \(|_{[j]}|=(1/j)\), which we can do without losing approximation power due to the decay of Chebyshev series coefficients. This allows us to show \((dm T)\) regret for Follow-The-Leader via Hazan et al. [33, Theorem 5] and then apply Foster & Rakhlin [26, Theorem 5] to obtain the following guarantee:

**Theorem 3.2** (Corollary of Theorem D.4).: _Suppose \(c_{}>-_{}()\). Then Algorithm 2 with appropriate parameters has regret w.r.t. any policy \(f:[c_{},c_{}+C]\) of_

\[_{t=1}^{T}_{t}(_{t})-_{t=1}^{T}_{t}(f(c_{t}))}(d+}{m}+ }{d}) \]

Setting \(d=(T^{2/11})\) and \(m=(T^{3/11})\) yields \(}(T^{9/11})\) regret, so we asymptotically attain instance-optimal performance, albeit at a rather slow rate. The rate in \(n\) is also worse than e.g. our semi-stochastic result for comparing to a fixed \(\) (c.f. Theorem 3.1), although to obtain this the latter algorithm uses \(d=()\) grid points, making its overhead nontrivial. We compare ChebCB to the Section 2.4 algorithm based on Tsallis-INF (among other methods), and find that, despite the former's worse guarantees, it seems able to converge to an instance-optimal policy much faster than the latter.