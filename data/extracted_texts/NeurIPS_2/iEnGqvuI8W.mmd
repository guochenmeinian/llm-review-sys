# Estimating shape distances on neural representations with limited samples

Dean A. Pospisil\({}^{1}\)  Brett W. Larsen\({}^{2,3,4}\)  Sarah E. Harvey\({}^{2,3}\)  Alex H. Williams\({}^{2,3}\)

\({}^{1}\)Princeton University, Princeton, NJ, 08544; dp4846@princeton.edu

\({}^{2}\)New York University, Center for Neural Science, New York, NY, 10003

\({}^{3}\)Flatiron Institute, Center for Computational Neuroscience, New York, NY, 10010

\({}^{4}\)Flatiron Institute, Center for Computational Mathematics, New York, NY, 10010

{brettlarsen, sharvey, awilliams}@flatironinstitute.org

###### Abstract

Measuring geometric similarity between high-dimensional network representations is a topic of longstanding interest to neuroscience and deep learning. Although many methods have been proposed, only a few works have rigorously analyzed their statistical efficiency or quantified estimator uncertainty in data-limited regimes. Here, we derive upper and lower bounds on the worst-case convergence of standard estimators of _shape distance_--a measure of representational dissimilarity proposed by Williams et al. . These bounds reveal the challenging nature of the problem in high-dimensional feature spaces. To overcome these challenges, we introduce a new method-of-moments estimator with a tunable bias-variance tradeoff. We show that this estimator achieves superior performance to standard estimators, particularly in high-dimensional settings. Thus, we lay the foundation for a rigorous statistical theory for high-dimensional shape analysis, and we contribute a new estimation method well-suited to practical scientific settings.

## 1 Introduction

Many approaches have been proposed to quantify similarity in neural network representations. Some popular methods include canonical correlations analysis , centered kernel alignment [CKA; 13], representational similarity analysis [RSA; 14], and shape metrics . Each of these approaches takes in a set of high-dimensional measurements from two networks--e.g., hidden layer activations or measured biological responses--and outputs a (dis)similarity score. Shape distances additionally satisfy the triangle inequality, thus enabling downstream algorithms for clustering and nearest-neighbor regression that leverage metric space structure . These measures have numerous applications including comparisons of artificial and biological systems [10; 24], comparisons of neural activity across different animal species , quantifying how hidden layer activity differs across deep network architectures [18; 19], and many more [see 11, for review]

In many practical settings, these measures must be estimated over a finite set of sampled networks inputs. However, with the noteworthy exception of research on RSA [4; 22; 28], there is little work on quantifying uncertainty (e.g. through confidence intervals) on estimators of representational similarity. This poses a serious obstacle to adoption of these methods, particularly in experimental neuroscience where there is a hard limit on the number of conditions that can be feasibly sampled [23; 29].

We address these concerns in the context of measuring shape distances between neural representations [Fig. 1; 30]. First we obtain analytic upper and lower bounds on the accuracy of typical "plug-in estimates" of shape distance as as a function of the number of samples, \(M\), and the dimension of the representation, \(N\). We then propose a new method-of-moments estimator with an explicit and tunable tradeoff between estimator bias and variance to overcome the limitations of the plug-in estimator.

## 2 Results

We begin by reviewing generalized shape distances and the standard plug-in estimator (extended background can be found in App. A) Based on our theoretical characterization of the plug-in estimator in App. B, we find that plug-in estimates rapidly converge onto their expected value, but the expected error decays moderately slowly (i.e. the estimators have low variance and high bias). We thus introduce a method-of-moments estimator with tunable bias (Sec. 2.2) to overcome these shortcomings. We characterize the behavior of both estimators on synthetic (Sec. 2.3) and neural data (App. D.2). Finally, we discuss the implications of our results in Sec. 3.

### Problem Setting

We consider initially a simple setting where each neural network is a deterministic map (for the stochastic setting, see appendix A.2). A collection of \(K\) neural systems can then be viewed as a set of functions, each denoted \(h_{i}:^{N}\) for \(i\{1,,K\}\). Here, \(\) is a feature space and \(N\) can be interpreted as the number of neurons in each system (e.g. the size of a hidden layer in an artificial network, or the number of recorded neurons in a biological experiment).

Motivated by the shape theory literature [9; 30], we consider estimating the _Procrustes size-and-shape distance_, \(\), and _Riemannian shape distance_, \(\), between neural representations. Let \(h_{i}\) and \(h_{j}\) denote neural systems that are mean-centered and bounded:

\[[h_{i}()]=[h_{j}()]= \|h_{i}()\|_{2},\|h_{j}()\|_{2}<B \]

for some constant \(B>0\). Here, the expectations are taken over \( P\), for some distribution \(P\) over network inputs. The Procrustes and Riemannian shape distances can be defined [App. D in 30]:

\[(h_{i},h_{j}) =_{(N)}[h_{i}()-h_{j}()]_{2}^{2}} \] \[(h_{i},h_{j}) =_{(N)}^{-1}([h_ {i}()^{-1.5mu }h_{j}()]}{[h_{i}( )^{-1.5mu }h_{i}()][h_{j}()^{ -1.5mu }h_{j}()]}}) \]

where \((N)\) denotes the set of \(N N\) orthogonal matrices.

Figure 1: **(A) Classical shape distances  can be used to provide a rotation-invariant distance between neural representations . Given two labelled points clouds in \(N\)-dimensional space (_left_ and _middle_), the distance is computed after an optimal orthogonal transformation is chosen to align the point clouds (_right_). In this visual example the point clouds trace out a low-dimensional manifold. (B) Heatmap shows the covariances \((_{ii},_{jj})\) and cross-covariance \((_{ij})\) of the 3D representations in panel A. Shape distances can be re-expressed in terms of these quantities (see eq. 2.5, 2.6). (C) Our ability to estimate the shape distance is related to \(M\), the number of stimuli. As \(M\) increases (_left_ to _right_) the number of sampled points along the underlying manifold increases, and we are better able to resolve shape differences between the representations.**It is well-known that the optimal orthogonal alignment in eqs. (2.2) and (2.3) can be identified in closed form (App. A). Leveraging this, we can use the covariance and cross-covariance matrices,

\[_{ii}=[h_{i}()h_{i}()^{ }],_{jj}=[h_{j}()h_{j}()^{}],_{ij}= [h_{i}()h_{j}()^{}], \]

to reformulate the squared Procrustes distance and cosine shape similarity:

\[^{2}(h_{i},h_{j}) =[_{ii}]+[ _{jj}]-2\|_{ij}\|_{*} \] \[(h_{i},h_{j}) =_{ij}\|_{*}}{[ _{ii}][_{jj}]}} \]

where \(\|_{ij}\|_{*}\) denotes the nuclear norm (or Shatten 1-norm) of the cross-covariance matrix.

Suppose we are given \(M\) independent and identically distributed network inputs \(_{1},,_{M} P\). We can estimate the generalized shape distances by substituting the empirical covariances:

\[}_{ii}=}h_{i} (_{m})h_{i}(_{m})^{},\;}_{jj}=}h_{j}(_{m})h_{j}(_{m})^{},\;}_{ij}= }h_{i}(_{m})h_{j}( _{m})^{} \]

to approximate the true covariances appearing in eqs. (2.5) and (2.6). Thus,

\[^{2}(h_{i},h_{j}) =[}_{ii}]+ [}_{jj}]-2\|}_{ij}\|_{*} \] \[(h_{i},h_{j}) =}_{ij}\|_{*}}{[}_{ii}][}_{jj}]}} \]

define plug-in estimators for the squared Procrustes and cosine Riemannian shape distances.

### A new estimator with controllable bias

The plug-in estimator of \(\|_{ij}\|_{*}\) has low variance but large and slowly decaying bias (see theorems B.2 and B.1). Here we develop an alternative estimator that is nearly unbiased.

First, note that the eigenvalues of \(_{ij}_{ij}^{}\) correspond to the squared singular values of \(_{ij}\). Thus, \([(_{ij}_{ij}^{})^{1/ 2}]=\|_{ij}\|_{*}\), and so we can reduce our problem to estimating the trace of \((_{ij}_{ij}^{})^{1/2}\), which is symmetric. Leveraging ideas from a well-developed literature , we proceed to define the \(p^{}\) moment of this matrix as:

\[W_{p}=[(_{ij}_{ij}^{ })^{p}]=_{n=1}^{N}_{n}^{p} \]

where \(_{1},,_{N}\) denote the eigenvalues of \(_{ij}_{ij}^{}\). Now, for any function \(f:\) and symmetric matrix \(\) with eigenvalues \(_{1},,_{N}\), we define1\([f()]=_{i}f(_{i})\). So long as \(f\) is reasonably well-behaved, we can approximate it using a truncated power series with \(P\) terms. Thus, with \(=_{ij}_{ij}^{}\) and \(f(x)=\):

\[\|_{ij}\|_{*}=[(_{ij}_{ij}^{})^{1/2}]_{n=1}^{N}_{p=0}^{P}_{p} _{n}^{p}=_{p=0}^{P}_{p}_{n=1}^{N}_{n}^{p}=_{p= 0}^{P}_{p}W_{p} \]

where \(_{0},,_{P}\) are scalar coefficients.

In summary, we can estimate \(\|_{ij}\|_{*}\) by (a) specifying an estimator of the top eigenmoments, \(W_{1},,W_{P}\), and (b) specifying a desired set of scalar coefficients \(_{0},,_{P}\). To estimate the eigenmoments, we adapt procedures described by Kong and Valiant  to obtain unbiased estimates for each moment, \(_{1},,_{P}\) (see App. C). To select the scalar coefficients, we propose an optimization procedure that trades off between bias and variance in the estimate of \(\|_{ij}\|_{*}\). Our starting point is the usual bias-variance decomposition:

\[[(\|_{ij}\|_{*}-_{p}_{p}_{p })^{2}]=([\|_{ij}\|_{*}-_{p }_{p}_{p}])^{2}+[_{p} _{p}_{p}]. \]Since \([_{p}]=W_{p}=_{n}_{n}^{p}\), the first term above (i.e. the "bias") simplifies and is upper-bounded:

\[([\|_{ij}\|_{*}-_{p}_{p}_{p} ])^{2}=(_{n}(_{n}^{1/2}-_{p}_{p} _{n}^{p}))^{2}_{0 x 1}(N(x^{1/2}- _{p}_{p}x^{p}))^{2}\]

The inequality follows from replacing each term in the sum over \(n\) with the worst case approximation error of the polynomial expansion (given here as the maximization over \(x\)). Thus, we seek to:

\[,,_{P}}{}_{0 x 1 }(N(x^{1/2}-_{p}_{p}x^{p}))^{2}+_{p,p^{ }}_{p}_{p^{}}(_{p},_ {p^{}}). \]

We estimate \((_{p},_{p^{}})\) by bootstrapping--i.e. the empirical covariance of these statistics across re-sampled datasets where \(_{1:M}\) are sampled with replacement. Given this estimate of covariance, eq. (2.13) can be cast as a convex quadratic program and the maximal bias can be bounded to a user defined limit at the expense of variance (see App. C.2). We use the maximal bias (eq. 2.13, term 1) and variance (eq. 2.13, term 2) to form approximate confidence intervals (see App. C.3).

### Validation on synthetic data

We validate our method-of-moments estimator (section 2.2) on simulated representations jointly sampled from a multivariate normal distribution. We consider estimating the cosine shape similarity, \(\), defined in eq. 2.6. Our estimator of \(\|_{ij}\|_{*}\) is the principle novelty; thus, it is informative to understand its properties in isolation. To achieve this, in our experiments we use the ground truth covariance of \(_{p}\) (instead of an estimate from a bootstrap) and use the ground truth values of \([_{ii}]\) and \([_{jj}]\). To draw data for our simulations, we set the eigenvalues of the \(_{ii}\) and the singular values of \(_{ij}\) to a ground truth nuclear norm and similarity score. To demonstrate the estimators accuracy across the space of orthogonal transformations we apply a random orthogonal rotation matrix to each population's covariance in each new parameter setting.

We first compared the bias of the plug-in estimator to that of the moment-based estimator across a range of ground truth shape similarity values (Fig. 2A). As expected from our intuition discussed in App. B.1, the plug-in estimator (blue line) tends to grossly inflate estimated similarity when ground truth similarity is low (left side of plot). The moment-based estimator (orange line), in contrast, performs reasonably well over the full range of simulations, at the cost of modest increases in estimator variance (blue vs orange error bars).

Next, we fixed the ground truth similarity at 0.2 and studied the effect of sample size, \(M\) (Fig. 2B).The moment estimator (constrained to 5% bias) maintains small bias even with small \(M\), at the cost of high variance (large orange error bars). Increasing \(M\) quickly reduces the variance of the estimator. A similar story emerges when we fix \(M\) and vary the ambient dimension \(N\) (Fig. 2C). As the

Figure 2: Validation of estimator on synthetic data. **(A)** The moment based estimator (orange) compared to plug-in estimator (blue) in simulation with standard deviation bars calculated across simulations. Estimators are evaluated at 20 linearly spaced ground truth similarity score values. **(B)** Effect of increasing sample size when moment estimator is constrained to have a bias less than 5 %. **(C)** Effect of increasing dimensionality. **(D)** Demonstration of conservative confidence intervals that account for variance and maximal bias of moment estimator. We do not include CIs for the plug in estimator (implied by theorem B.1) because for small sample sizes, the theoretical bounds on estimator bias always contain far more than the entire allowable interval (\(\)).

dimensionality increases, the the plug-in estimator bias quickly explodes. In contrast, the moment estimator (here constrained to 10% bias) has roughly constant bias; however, it's variance grows with \(N\). Thus our estimator bias outperforms the plug-in sample size is low and dimensionality is high.

Finally, an important property of the moment-based estimator is our ability to compute approximate confidence intervals (CI) (see App. C.3). We demonstrate 95% CIs across simulations in Figure 2D (shaded orange region). These CIs are conservative, the true shape score is not within the CI's for only 2.3% of simulations. Results on neural data can be found in App. D.2.

## 3 Discussion

There is a vast literature of papers that utilize or develop measures of representational similarity between neural networks (see 11, for review), and recent works have shown interest in leveraging representational distances that satisfy the triangle inequality . Yet, the statistical properties of these shape distance measures appears understudied. Here, rigorously analyzed of "plug-in" estimates of shape distance in high-dimensional, noisy, and sample-limited regimes. Our analysis showed that these estimates _(a)_ tend to over-estimate representational similarity when the true similarity is small and _(b)_ require a large number of samples, \(M\), to overcome this bias in high dimensional regimes. Theorems B.1 and B.2 provide precise guarantees on the worst-case performance of plug-in estimators, which should guide the design of biological experiments and analyses of their statistical power.

An equally important contribution of our work is to provide a practical method to _(a)_ reduce the bias of plug-in estimators of shape distance, _(b)_ quantify uncertainty in shape distance estimates, and _(c)_ enable practicioners to explicitly trade off estimator bias and variance. When employed on a biological dataset published by Stringer et al. , we find that shape similarity estimates are highly uncertain, revealing the challenging nature of the problem in high dimensions and with noisy data. Importantly, this degree of uncertainty is not obvious from the procedures and plug-in estimates advertised by existing work on this subject.

Both theoretical and methodological aspects of our work may be of broader interest beyond the immediate subject of shape distance estimation. We have seen that estimating the nuclear norm of the cross-covariance, \(\|_{ij}\|_{*}\), is the key challenge in our problem. Estimating the spectrum of cross-covariance matrices is a topic of contemporary interest , and further exploring the connections between this problem and shape distance estimation is an intriguing direction. Similarly, the method-of-moments estimator presented in section 2.2 is broadly applicable to generalized trace estimation . While others have used polynomial expansions in this context , a key novelty of our approach is the selection of coefficients with a tunable parameter that explicitly trades off estimator bias and variance. A more typical approach would be to choose these coefficients based on a Chebyshev polynomial expansion. While elegant, we believe our procedure for tuning these coefficients will be more relevant to scientific applications where samples are limited (such as neural data) and practitioners desire finer-scale control.

In summary, our work is one of the first to rigorously interrogate the statistical challenges of estimating shape distances in high-dimensional spaces. While shape distances can be well-behaved in certain settings (e.g. in artificial networks where a very large number of inputs can be sampled), our theoretical results and empirical observations underscore the challenging nature of this problem, suggesting the need for carefully designed biological experiments and estimation procedures.