# Self-supervised Object-Centric Learning for Videos

Gorkay Aydemir\({}^{1}\) Weidi Xie\({}^{3,4}\) Fatma Guney\({}^{1,2}\)

\({}^{1}\) Department of Computer Engineering, Koc University \({}^{2}\) KUIS AI Center

\({}^{3}\) CMIC, Shanghai Jiao Tong University \({}^{4}\) Shanghai AI Laboratory

{gaydemir23, fguney}@ku.edu.tr weidi@sjtu.edu.cn

###### Abstract

Unsupervised multi-object segmentation has shown impressive results on images by utilizing powerful semantics learned from self-supervised pretraining. An additional modality such as depth or motion is often used to facilitate the segmentation in video sequences. However, the performance improvements observed in synthetic sequences, which rely on the robustness of an additional cue, do not translate to more challenging real-world scenarios. In this paper, we propose the first fully unsupervised method for segmenting multiple objects in real-world sequences. Our object-centric learning framework spatially binds objects to slots on each frame and then relates these slots across frames. From these temporally-aware slots, the training objective is to reconstruct the middle frame in a high-level semantic feature space. We propose a masking strategy by dropping a significant portion of tokens in the feature space for efficiency and regularization. Additionally, we address over-clustering by merging slots based on similarity. Our method can successfully segment multiple instances of complex and high-variety classes in YouTube videos.1

## 1 Introduction

Given a video sequence of a complex scene, our goal is to train a vision system that can discover, track, and segment objects, in a way that abstracts the visual information from millions of pixels into semantic components, _i.e._, objects. This is commonly referred to as object-centric visual representation learning in the literature. By learning such abstractions of the visual scene, the resulting object-centric representation acts as fundamental building blocks that can be processed independently and recombined, thus improving the model's generalization and supporting high-level cognitive vision tasks such as reasoning, control, _etc._.

The field of object-centric representation learning in computer vision has made significant progress over the years, starting from synthetic images , and has since shifted towards in-the-wild image  and real-world videos . In general, existing approaches typically follow an autoencoder training paradigm , _i.e._, reconstructing the input signals with certain bottlenecks, with the hope to group the regional pixels into semantically meaningful objects based on the priors of architecture or data. In particular, for images, low-level features like color, and semantic features from pretrained deep networks, are often used to indicate the assignment of pixels to objects, while for videos, additional modalities/signals are normally incorporated, such as optical flow , depth map , with segmentation masks directly available from the discontinuities. Despite being promising, using additional signals in videos naturally incurs computation overhead and error accumulation, for example, optical flow may struggle with static or deformable objects and large displacements between frames, while depth may not be readily available in generic videos and its estimation can suffer in low-light or low-contrast environments.

In this paper, we introduce SOLV, a self-supervised model capable of discovering multiple objects in real-world video sequences without using additional modalities [2; 36; 40; 17] or any kind of weak supervision such as first frame initialization [40; 17]. To achieve this, we adopt axial spatial-temporal slot attentions, that first groups spatial regions within frame, then followed by enriching the slot representations using additional cues from interactions with neighboring frames. We employ masked autoencoder (MAE)-type training, with the objective of reconstructing dense visual features from masked inputs. This approach has two benefits: first, it acts as an information bottleneck, forcing the model to learn the high-level semantic structures, given only partial observations; second, it alleviates memory constraints, which helps computational efficiency. Additionally, due to the complexity of visual scenes, a fixed number of slots often leads to an over-segmentation issue. We address this issue by merging similar slots with a simple clustering algorithm. Experimentally, our method significantly advances the state-of-the-art without using information from additional modalities on a commonly used synthetic video dataset, MOVi-E  and a subset of challenging Youtube-VIS 2019  with in-the-wild videos (Fig. 1).

To summarize, we make the following contributions: (i) We propose a self-supervised multi-object segmentation model on real-world videos, that uses axial spatial-temporal slots attention, effectively grouping visual regions with similar property, without relying on any additional signal; (ii) We present an object-centric learning approach based on masked feature reconstruction, and slot merging; (iii) Our model achieves state-of-the-art results on the MOVi-E and Youtube-VIS 2019 datasets, and competitive performance on DAVIS2017.

## 2 Related Work

Object-centric Learning:The field of unsupervised learning of object-centric representations from images and videos has gained considerable attention in recent years. Several approaches have been proposed to address this problem with contrastive learning [31; 41; 84] or more recently with reconstruction objectives. An effective reconstruction-based approach first divides the input into a set of region identifier variables in the latent space, _i.e._ slots, that bind to distinctive parts corresponding to objects. Slot attention has been applied to both images [26; 5; 10; 12; 18; 50; 89; 19; 27; 15; 52; 69] and videos [35; 71; 28; 40; 33; 75; 90; 43]. However, these methods are typically evaluated on synthetic data and struggle to generalize to real-world scenarios due to increasing complexity. To address this challenge, previous works explore additional information based on the 3D structure [8; 58; 32] or reconstruct different modalities such as flow  or depth . Despite these efforts, accurately identifying objects in complex visual scenes without explicit guidance remains an open challenge. The existing work relies on guided initialization from a motion segmentation mask [1; 2] or initial object locations [40; 17]. To overcome this limitation, DINOSAUR  performs reconstruction in the feature space by leveraging the inductive biases learned by recent self-supervised models . We also follow this strategy which has proven highly effective in learning object-centric representations in real-world data without any guided initialization or explicit supervision.

Object Localization from DINO Features:The capabilities of Vision Transformers (ViT)  have been comprehensively investigated, leading to remarkable findings when combined with self-supervised features from DINO . By grouping these features with a traditional graph partitioning method [57; 68; 81], impressive results can be achieved compared to earlier approaches [76; 77].

Figure 1: We introduce SOLV, an object-centric framework for instance segmentation in real-world videos. This figure depicts instance segmentation results of first, middle, and last frames of videos on Youtube-VIS 2019 . Without any supervision both in training and inference, we manage to segment object instances accurately.

Recent work, CutLER , extends this approach to segment multiple objects with series of normalized cuts. The performance of these methods without any additional training shows the power of self-supervised DINO features for segmentation and motivates us to build on it in this work.

**Video Object Segmentation:** Video object segmentation (VOS) [86; 4; 22; 23; 42; 38; 44; 56; 59; 61; 72; 78; 79; 88] aims to identify the most salient object in a video without relying on annotations during evaluation in unsupervised setting [73; 21; 47; 53] and only annotation of the first frame in semi-supervised setting . Even if the inference is unsupervised, ground-truth annotations can be used during training in VOS [11; 16; 63; 54; 6]. Relying on labelled data during training might introduce a bias towards the labelled set of classes that is available during training. In this paper, we follow a completely unsupervised approach without using any annotations during training or testing.

Motion information is commonly used in unsupervised VOS to match object regions across time [36; 9; 51; 66]. Motion cues particularly come in handy when separating multiple instances of objects in object-centric approaches. Motion grouping  learns an object-centric representation to segment moving objects by grouping patterns in flow. Recent work resorts to sequential models while incorporating additional information [70; 40; 17; 1]. In this work, we learn temporal slot representations from multiple frames but we do not use any explicit motion information. This way, we can avoid the degradation in performance when flow cannot be estimated reliably.

## 3 Methodology

In this section, we start by introducing the considered problem scenario, then we detail the proposed object-centric architecture, that is trained with self-supervised learning.

### Problem Scenario

Given an RGB video clip as input, _i.e._, \(_{t}=\{_{t-n},,_{t},_{t+n}\}^{(2n+1) H W 3}\), our goal is to train an object-centric model that processes the clip, and outputs all object instances in the form of segmentation masks, _i.e._, discover and track the instances in the video, via _self-supervised learning_, we can formulate the problem as :

\[_{t}=(_{t};)=_{} _{}_{}(_{t}) \]

where \(_{t}^{K_{t} H W}\) refers to the output segmentation mask for the middle frame with \(K_{t}\) discovered objects. After segmenting each frame, we perform Hungarian matching to track the objects across frames in the video. \((;)\) refers to the proposed segmentation model that will be detailed in the following section. Specifically, it consists of three core components (see Fig. 2), namely, visual encoder that extracts frame-wise visual features (Section 3.2.1), spatial-temporal axial binding that first groups pixels into slots within frames, then followed by joining the slots across temporal frames (Section 3.2.2), and visual decoder that decodes the spatial-temporal slots to reconstruct the dense visual features, with the segmentation masks of objects as by-products (Section 3.2.3).

### Architecture

Generally speaking, our proposed architecture is a variant of Transformer, that can be trained with simple masked autoencoder, _i.e._, reconstructing the full signals given partial input observation. However, unlike standard MAE  that recovers the image in pixel space, we adopt an information bottleneck design, that first assigns spatial-temporal features into slots, then reconstructs the dense visual features from latent slots. As a result, each slot is attached to one semantically meaningful object, and the segmentation masks can be obtained as by-products from reconstruction, _i.e._, without relying on manual annotations.

#### 3.2.1 Visual Encoder

Given the RGB video clip, we divide each image into regular non-overlapping patches. Following the notation introduced in Section 3.1, _i.e._, \(_{t}=\{_{t-n},,_{t},_{t+n}\}^{(2n+1) N(3P^{2})}\), where \(N=HW/P^{2}\) is the number of tokens extracted from each frame with patches of size \(P\). The visual encoder consists of token drop and feature extraction, as detailed below.

**Token Drop:** As input to the encoder, we only sample a subset of patches. Our sampling strategy is straightforward: we randomly drop the input patches with some ratio for each frame,

\[^{}_{t}=\{^{}_{t-n},,^{ }_{t+n}\}=\{(_{t-n}),, (_{t+n})\}^{(2n+1) N ^{}(3P^{2})}, N^{}<N \]

where \(N^{}\) denotes the number of tokens after random sampling.

**Feature Extraction:** We use a frozen Vision Transformer (ViT)  with parameters initialized from DINov2 , a self-supervised model that has been pretrained on a large number of images:

\[=\{_{t-n},,_{t+n}\}=\{ _{}(^{}_{t-n}),,_{}(^{}_{t+n})\}^{(2n+1) N ^{} D} \]

where \(D\) refers to the dimension of output features from the last block of DINov2, right before the final layer normalization.

**Discussion:** Our design of token drop serves two purposes: _Firstly_, masked autoencoding has been widely used in natural language processing and computer vision, acting as a proxy task for self-supervised learning, our token drop can effectively encourage the model to acquire high-quality visual representation; _Secondly_, for video processing, the extra temporal axis brings a few orders of magnitude of more data, processing sparsely sampled visual tokens can substantially reduce memory budget, enabling computation-efficient learning, as will be validated in our experiments.

#### 3.2.2 Spatial-temporal Binding

After extracting visual features for each frame, we first spatially group the image regions into slots, with each specifying a semantic object, _i.e_., discover objects within single image; then, we establish the temporal bindings between slots with a Transformer, _i.e_., associate objects within video clips.

\[_{}()=_{}(_{ }(_{t-n}),,_{}( _{t+n}))^{K D_{}} \]

**Spatial Binding (\(_{}\)):** The process of spatial binding is applied to each frame **independently**. We adopt the invariant slot attention proposed by Biza et al. , with one difference, that is, we use a shared initialization \(_{}\) at each time step \(\{t-n,,t+n\}\). Specifically, given features after token drop at a time step \(\) as input, we learn a set of initialization vectors to translate and scale the input position encoding for each slot separately with \(K\) slot vectors \(^{j}^{D_{}}\), \(K\) scale vectors

Figure 2: **Overview. In this study, we introduce SOLV, an autoencoder-based model designed for object-centric learning in videos. Our model consists of three components: (i) Visual Encoder for extracting features for each frame using \(_{}\); (ii) Spatial-temporal Binding module for generating temporally-aware object-centric representations by binding them spatially and temporally using \(_{}\) and \(_{}\), respectively; (iii) Visual Decoder for estimating segmentation masks and feature reconstructions for the central frame with \(_{}\), after merging similar slots using \(_{}\)**\(^{j}_{s}^{2}\), \(K\) position vectors \(^{j}_{p}^{2}\), and one absolute positional embedding grid \(_{}^{N 2}\). We mask out the patches corresponding to the tokens dropped in feature encoding (Section 3.2.1) and obtain absolute positional embedding for each frame \(\) as \(_{,}=(_{} )^{N^{} 2}\). Please refer to the original paper  or the Supplementary Material for details of invariant slot attention on a single frame. This results in the following set of learnable vectors for each frame:

\[_{}=\{(^{j},^{j}_{s},^{ j}_{p},_{,})\}_{j=1}^{K} \]

Note that these learnable parameters are shared for all frames and updated with respect to the dense visual features of the corresponding frame. In other words, slots of different frames start from the same representation but differ after binding operation due to interactions with frame features. The output of \(_{}\) is the updated slots corresponding to independent object representations at frame \(\):

\[\{^{j}_{}\}_{j=1}^{K}=_{}(_{ })^{K D_{}},\{t-n,,t+n\} \]

In essence, given that consecutive frames typically share a similar visual context, the use of learned slots inherently promotes temporal binding, whereby slots with the same index can potentially bind to the same object region across frames. Our experiments demonstrate that incorporating invariant slot attention aids in learning slot representations that are based on the instance itself, rather than variable properties like size and location.

**Temporal Binding (\(_{}\)):** Up until this point, the model is only capable of discovering objects by leveraging information from individual frames. This section focuses on how to incorporate temporal context to enhance the slot representation. Specifically, given the output slots from the spatial binding module, denoted as \(\{\{^{j}_{t-n}\}_{j=1}^{K},,\{^{j}_{t+n}\}_{j=1 }^{K}\}^{(2n+1) K D_{}}\), we apply a transformer encoder to the output slots with same index across different frames. In other words, the self-attention mechanism only computes a \((2n+1)(2n+1)\) affinity matrix across \(2n+1\) time steps. This approach helps to concentrate on the specific region of the image through time. Intuitively, each slot learns about its future and past in the temporal window by attending to each other, which helps the model to create a more robust representation by considering representations of the same object at different times.

To distinguish between time stamps, we incorporate learnable temporal positional encoding onto the slots, _i.e._, all slots from a single frame receive the same encoding. As the result of temporal transformer, we obtain the updated slots \(\) at the target time step \(t\):

\[=_{}()^{K D _{}} \]

#### 3.2.3 Visual Decoder

The spatial-temporal binding process yields a set of slot vectors \(^{K D_{}}\) at target frame \(t\) that are aware of temporal context. However, in natural videos, the number of objects within a frame can vary significantly, leading to over-clustering when a fixed number of slots is used, _i.e._, as we can always initialize slots in an over-complete manner.

To overcome this challenge, we propose a simple solution for slot merging through Agglomerative Clustering. Additionally, we outline the procedure for slot decoding to reconstruct video features, which resembles a masked auto-encoder in feature space.

\[_{}()=_{}_{ }() \]

**Slot Merging (\(_{}\)):** As expected, the problem of object segmentation is often a poorly defined problem statement in the self-supervised scenario, as there can be multiple interpretations of visual regions. For instance, in an image of a person, it is reasonable to group all person pixels together or group the person's face, arms, body, and legs separately. However, it is empirically desirable for pixel embeddings from the same object to be closer than those of different objects. To address this challenge, we propose to merge slots using Agglomerative Clustering (AC), which does not require a predefined number of clusters. As shown in Fig. 3, we first compute the affinity matrix between all slots based on cosine similarity, then use this affinity matrix to cluster slots into groups, and compute the mean slot for each resulting cluster:

\[^{}=_{}()^{ K_{t} D_{}}, K_{t} K \]

[MISSING_PAGE_FAIL:6]

recent works such as Karazija et al. , Bao et al. , and Seitzer et al. . This approach also allows us to compare to state-of-the-art image-based segmentation methods, such as DINOSAUR . For real-world datasets, we use the mean Intersection-over-Union (mIoU) metric, which is widely accepted in segmentation, by considering only foreground objects. To ensure temporal consistency of assignments between frames, we apply Hungarian Matching between the predicted and ground-truth masks of foreground objects in the video, following the standard practice .

### Results on Synthetic Data: MOVi-E

The results on the MOVi-E dataset are presented in Table 1, with methods divided based on whether they use an additional modality. For example, the sequential extensions of slot attention, such as SAVi  and SAVi++ , reconstruct different modalities like flow and sparse depth, respectively, their performance falls behind other approaches despite additional supervision. On the other hand, STEVE  improves results with a transformer-based decoder for reconstruction. PPMP  significantly improves performance by predicting probable motion patterns from an image with flow supervision during training. MoTok  outperforms other methods, but relies on motion segmentation masks to guide the attention maps of slots during training, with a significant drop in performance without motion segmentation. The impressive performance of DINOSAUR  highlights the importance of self-supervised training for segmentation. Our method stands out by using the spatial-temporal slot binding, and autoencoder training in the feature space, resulting in significantly better results than all previous work (see Fig. 5).

### Results on Real Data: DAVIS17 and Youtube-VIS 2019

Due to a lack of multi-object segmentation methods evaluated on real-world data, we present a baseline approach that utilizes spectral clustering on the DINOv2 features . To define the number of clusters, we conduct an oracle test by using the ground-truth number of objects. After obtaining masks independently for each frame, we ensure temporal consistency by applying Hungarian Matching between frames based on the similarity of the mean feature of each mask. However, as shown in Table 2, directly clustering the DINOv2 features produces poor results despite the availability of privileged information on the number of objects.

   Model & +Modality & FG-ARI \(\) \\  SAVi  & Flow & 39.2 \\ SAVi++  & Sparse Depth & 41.3 \\ PPMP  & Flow & 63.1 \\ MoTok  & Motion Seg. & 66.7 \\  MoTok  & & 52.4 \\ STEVE  & - & 54.1 \\ DINOSAUR  & - & 65.1 \\ Ours & & **80.8** \\   

Table 1: **Quantitative Results on MOVi-E.** This table shows results in comparison to the previous work in terms of FG-ARI on MOVi-E.

    &  &  &  \\   & & mIoU \(\) & FG-ARI \(\) & mIoU \(\) & FG-ARI \(\) \\  DINOv2  + Clustering & \#Objects & 14.34 & 24.19 & 28.17 & 16.19 \\ OCLR  & Flow & **34.60** & 14.67 & 32.49 & 15.88 \\ Ours & - & 30.16 & **32.15** & **45.32** & **29.11** \\   

Table 2: **Quantitative Results on Real-World Data.** These results show the video multi-object evaluation results on the validation split of DAVIS17 and a subset of the YTVIS19 train split.

Figure 5: **Qualitative Results on MOVi-E.**

We also compare our method to the state-of-the-art approach OCLR , which is trained using a supervised objective on synthetic data with ground-truth optical flow. OCLR  slightly outperforms our method on DAVIS17, where the optical flow can often be estimated accurately, that greatly benefits the segmentation by allowing it to align with the real boundaries, resulting in better mIoU performance. However, our method excels at detecting and clustering multiple objects, even when the exact boundaries are only roughly located, as evidenced by the higher FG-ARI. The quality of optical flow deteriorates with increased complexity in YTVIS videos, causing OCLR  to fall significantly behind in all metrics. The qualitative comparison in Fig. 6 demonstrates that our method can successfully segment multiple objects in a variety of videos. Additional qualitative results can be found in the Supplementary Material.

### Ablation Study

**On the Effectiveness of Architecture:** We conducted an experiment to examine the impact of each proposed component on the performance. _Firstly_, we replaced the spatial binding module, \(_{}\), with the original formulation in Slot Attention . _Secondly_, we removed the temporal binding module, \(_{}\), which prevents information sharing between frames at different time steps. _Finally_, we eliminated the slot merging module, \(_{}\).

We can make the following observations from the results in Table 3: (i) as shown by the results of Model-A, a simple extension of DINOSAUR  to videos, by training with DINOv2 features and matching mask indices across frames, results in poor performance; (ii) only adding slot merging (Model-B) does not significantly improve performance, indicating that over-clustering is not the primary issue; (iii) significant mIoU gains can be achieved with temporal binding (Model-C) or

   \#Slots & Merging & mIoU \(\) & FG-ARI \(\) \\   & ✗ & 43.29 & 27.73 \\  & ✓ & 44.90 & 28.52 \\   & ✗ & 39.90 & 22.55 \\  & ✓ & **45.32** & **29.11** \\   & ✗ & 36.04 & 20.20 \\  & ✓ & 43.19 & 27.94 \\   

Table 4: **Number of Slots.** The effect of varying the number of slots with/without slot merging.

Figure 6: **Qualitative Results of Multi-Object Video Segmentation on YTVIS19.** We provide the first, middle, and last frames along with their corresponding segmentation after the Hungarian Matching step. Our model is proficient in accurately identifying objects in their entirety, irrespective of their deformability or lack of motion. This capability is highlighted in contrast to OCLR , whose results are displayed in the third row for comparison.

spatial binding (Model-D), which shows the importance of spatially grouping pixels within each frame as well as temporally relating slots in the video. Finally, the best performance is obtained with Model-E in the last row by combining all components.

**Number of Slots:** We perform an experiment by varying the number of slots with and without slot merging in Table 4. Consistent with the findings in previous work , the performance is highly dependent on the number of slots, especially when slot merging is not applied. For example, without slot merging, increasing the number of slots leads to inferior results in terms of both metrics, due to the over-segmentation issue in some videos. While using slot merging, we can better utilize a larger number of slots as can be seen from the significantly improved results of 8 or 12 slots with slot merging. This is also reflected in FG-ARI, pointing to a better performance in terms of clustering. We set the number of slots to 8 and use slot merging in our experiments. We provide a visual comparison of the resulting segmentation of an image with and without slot merging in Fig. 4. Our method can successfully merge parts of the same object although each part is initially assigned to a different slot.

**Token Drop Ratio:** We ablated the ratio of tokens to drop in Fig. 7. As our motivation for the token drop is two-fold (Section 3.2.1), we show the effect of varying the token drop ratio on both performance and memory usage. Specifically, we plot mIoU on the vertical axis versus the memory consumption on the horizontal axis. We report the memory footprint as the peak in the GPU memory usage throughout a single pass with constant batch size. Fig. 7 confirms the trade-off between memory usage and performance. As the token drop ratio decreases, mIoU increases due to less information loss from tokens dropped. However, this comes at the cost of significantly more memory usage. We cannot report results by using all tokens, _i.e_. token drop ratio of 0, due to memory constraints. Interestingly, beyond a certain threshold, _i.e_. at 0.5, the token drop starts to act as regularization, resulting in worse performance despite increasing the number of tokens kept. We use 0.5 in our experiments, which reaches the best mIoU with a reasonable memory requirement. In summary, the token drop provides not only efficiency but also better performance due to the regularization effect.

**Visual Encoder:** We conducted experiments to investigate the effect of different visual encoders in Table 5. We adjust the resize values to maintain a consistent token count of 864 for different architectures. These findings underscore the critical role of pretraining methods. Please see Supplementary for a qualitative comparison. Notably, the DINov2 model  outperforms its predecessor, DINO , in terms of clustering efficiency. On the other hand, models pretrained using supervised methods results in the weakest performance, with a substantial gap in the FG-ARI metric when compared to self-supervised pretraining methods. Furthermore, no specific patch size offers a clear advantage within the same pretraining type. For instance, DINO pretraining with ViT-B/8 exhibits superior performance in the FG-ARI metric but lags behind its 16-patch-sized counterpart in the mIoU metric.

## 5 Discussion

We presented the first fully unsupervised object-centric learning method for scaling multi-object segmentation to in-the-wild videos. Our method advances the state-of-the-art significantly on commonly used simulated data but more importantly, it is the first fully unsupervised method to report state-of-the-art performance on unconstrained videos of the Youtube-VIS dataset.

This work takes a first step towards the goal of decomposing the world into semantic entities from in-the-wild videos without any supervision. The performance can always be improved with better

   Model & mIoU & FG-ARI \(\) \\  Supervised ViT-B/16 & 37.58 & 19.94 \\ DINO ViT-B/8 & 39.53 & 24.70 \\ DINO ViT-B/16 & 41.91 & 24.01 \\ DINov2 ViT-B/14 & **45.32** & **29.11** \\   

Table 5: **Visual Encoder**. The effect of different types of self-supervised pretraining methods and architectures for feature extraction.

features from self-supervised pretraining as shown in the comparison of different DINO versions. While we obtain significant performance boosts from self-supervised pretraining, it also comes with some limitations. For example, our method can locate objects roughly, but it fails to obtain pixel-accurate boundaries due to features extracted at patch level. Furthermore, nearby objects might fall into the same patch and cause them to be assigned to the same slot. Our slot merging strategy, although has been proved effective as a simple fix for the issue of over-segmentation, remains not differentiable. Future work needs to go beyond that assumption and develop methods that can adapt to a varying number of objects in a differentiable manner.

## 6 Acknowledgements

Weidi Xie would like to acknowledge the National Key R&D Program of China (No. 2022ZD0161400).