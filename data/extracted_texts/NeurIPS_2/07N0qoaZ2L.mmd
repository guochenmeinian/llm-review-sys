# Improved Analysis for Bandit Learning in Matching Markets

Fang Kong

Southern University of Science and Technology

kongf@sustech.edu.cn

&Zilong Wang

Shanghai Jiao Tong University

wangzilong@sjtu.edu.cn

&Shuai Li

Shanghai Jiao Tong University

shuaili8@sjtu.edu.cn

Corresponding author.

###### Abstract

A rich line of works study the bandit learning problem in two-sided matching markets, where one side of market participants (players) are uncertain about their preferences and hope to find a stable matching during iterative matchings with the other side (arms). The state-of-the-art analysis shows that the player-optimal stable regret is of order \(O(K T/^{2})\) where \(K\) is the number of arms, \(T\) is the horizon and \(\) is the players' minimum preference gap. However, this result may be far from the lower bound \((\{N T/^{2},K T/\})\) since the number \(K\) of arms (workers, publisher slots) may be much larger than that \(N\) of players (employers in labor markets, advertisers in online advertising, respectively). In this paper, we propose a new algorithm and show that the regret can be upper bounded by \(O(N^{2} T/^{2}+K T/)\). This result removes the dependence on \(K\) in the main order term and improves the state-of-the-art guarantee in common cases where \(N\) is much smaller than \(K\). Such an advantage is also verified in experiments. In addition, we provide a refined analysis for the existing centralized UCB algorithm and show that, under \(\)-condition, it achieves an improved \(O(N T/^{2}+K T/)\) regret.

## 1 Introduction

The two-sided matching market problem has been extensively studied in the literature due to its wide range of applications like labor market, school admission, house allocation, and online advertising . There are two sides of participants in the market, such as the employers and workers in the labor market, advertisers and publishers in online advertising. Each participant on the one side has a preference ranking over the other side. The concept of stability, which characterizes the equilibrium state of the market where no participant wants to break up the current matching relationship and find another partner, has attracted great interest from researchers . Achieving stability is critical for ensuring the long-term viability of the market.

A rich line of works  study how to find a stable matching in the market. Most of them assume the preference ranking of each market participant is known beforehand, which we refer to as the _offline_ setting. However, in real applications, the knowledge of the preferences may be uncertain. For example, in the labor market, employers usually do not know the working abilities of workers before being matched, and advertisers also do not know the exact conversion rate of placing the advertisement in a publisher slot. This makes the traditional algorithms unavailable tofind an exact stable matching. With the emergence of online market platforms such as the online labor market UpWork and TaskRabbit as well as online advertising platforms where employers or advertisers have many similar tasks, market participants are able to learn their unknown preferences during iterative matchings with the other side of agents.

Multi-armed bandit (MAB) is a classic framework that characterizes the learning process during iterative interactions . It considers the setting with single player on one side and multiple arms on the other side. At each round, the player selects an arm and receives a reward. The player has unknown preferences over arms and would learn this knowledge based on the collected rewards. To accumulate as many rewards as possible, the player faces the dilemma of exploration and exploitation. The former selects arms with less observations while the latter focuses on arms with better historical performances. How to balance the exploration and exploitation trade-off is the key of bandit algorithm design. The upper confidence bound (UCB) , Thompson sampling (TS) , and explore-then-commit (ETC)  are common strategies in MAB to achieve this objective.

Liu _et al._ introduce the bandit learning problem in matching markets and try to provide theoretical guarantees. Two sides of agents in the market can be modeled as players and arms. Without loss of generality, denote \(N\) and \(K\) as the number of players and arms, respectively. It is worth noting that this work and all of the following works assume \(N K\) to ensure each player has a chance to be matched. In this problem, the objective is to find a stable matching and minimize the stable regret for each player, which is defined as the difference between the reward of the stable arm and that the player receives during the horizon. Since there may be more than one stable matching, they mainly focus on the players' most preferred one corresponding to the player-optimal stable matching and the least preferred one corresponding to the player-pessimal stable matching. Note that players receive more rewards in the player-optimal stable matching and thus the former objective is the most desirable. Liu _et al._ first study a centralized setting where a central platform would compute allocations for players to avoid conflicts. Both ETC and UCB-type algorithms are proposed for this setting. The former achieves a player-optimal stable regret guarantee with prior knowledge of players' minimum preference gap \(\) and the latter can only ensure to reach the player-pessimal stable matching. Motivated by real applications where the central platform may not always exist, a rich line of works then study the decentralized case where no platform coordinates players' behavior . This line of works again only achieve guarantees for player-pessimal stable regret . Table 1 compares settings and regrets among these works. Until recently, Zhang _et al._ and Kong and Li  independently derive algorithms that have polynomial player-optimal stable regret and show the upper bound is \(O(K T/^{2})\). However, this result may be still far from the lower bound \((\{N T/^{2},K T/\})\) since \(K\) is usually much larger than \(N\) such as that the number of workers (publisher slots) is usually much larger than that of employers in labor markets (advertisers in online advertising, respectively).

In this paper, we try to provide more efficient algorithms and improve the results over existing works. The detailed contribution can be summarized as follows: (1) We propose an algorithm named adaptively explore-then-Gale-Shapley (AETGS) with elimination. State-of-the-art works  explicitly separate the exploration and exploitation processes, which can lead to unnecessary regret, as exploring certain preference rankings may not contribute to the exploitation process. To avoid excessive exploration, our AETGS with elimination algorithm integrates the players' learning process into the GS steps. Players adaptively switch between exploration and exploitation and promptly eliminate sub-optimal arms. (2) We prove that the player-optimal stable regret of AETGS with elimination can be upper bounded by \(O(N^{2} T/^{2}+K T/)\). This is the first result that removes the dependence on \(K\) in the main regret order term and improves existing works in common cases where \(N\) is much smaller than \(K\). We also conduct experiments to show the advantages of the algorithm. (3) We refine the analysis of the centralized UCB algorithm in Liu _et al._ for markets satisfying the \(\)-condition. By investigating the preference hierarchy structure of the \(\)-condition, we demonstrate that the stable matching converges sequentially from player 1 to player \(N\). Through inductive analysis over players, we establish an \(O(N T/^{2}+K T/)\) regret upper bound, which improves the original result for this algorithm in this specific market.

## 2 Related Work

The problem of bandit learning in matching markets is first introduced by Das and Kamenica . They study the special case where both sides of agents have the same preferences and propose some empirical methods to solve the problem. Liu _et al._ first theoretically formulate this problem and provide an upper bound for the stable regret of players. They propose a centralized explore-then-commit (ETC) algorithm and upper confidence bound (UCB) algorithm, which obtain an \(O(K T/^{2})\) player-optimal stable regret and \(O(NK T/^{2})\) player-pessimal stable regret, respectively. It is worth noting that the former ETC algorithm requires knowledge about \(\) to ensure the algorithmic operation. Due to the generality, the following works focus on the decentralized setting. Liu _et al._ and Kong _et al._ propose the UCB and TS-type algorithm for general decentralized markets, respectively. Such a setting is much more challenging and both of them only achieve \(O((N^{4})N^{5}K^{2}^{2}(T)/^{2})\) upper bound for the player-pessimal stable regret.

To improve the stable regret guarantee, a line of research studies some special markets with unique stable matching in which case the player-optimal stable matching is equivalent to the player-pessimal one. Sankararaman _et al._ propose the UCB-D3 algorithm based on the assumption of serial dictatorship, i.e., all arms share the same preferences, and obtain an \(O(NK T/^{2})\) regret upper bound. To investigate the problem hardness, they also derive a lower bound \((\{N T/^{2},K T/\})\) under this assumption. Basu _et al._ consider more general \(\)-condition setting for unique stable matching. They propose the UCB-D4 algorithm and also achieve the \(O(NK T/^{2})\) regret bound. Later, Maheshwari _et al._ study the market satisfy

ing \(\)-reducible condition and proposes a communication-free algorithm. Their regret bound has an exponential dependence on the number of market participants. Recently, researchers have developed algorithms that can achieve player-optimal stable regret guarantees without assuming unique stable matching. Both Zhang _et al._ and Kong and Li  propose ETC-type algorithms that achieve \(O(K T/^{2})\) player-optimal stable regret. Wang and Li  studies the matching markets with serial dictatorship and obtains the \(O(N T/^{2}+K T/)\) regret. Table 1, compares our proposed algorithm with these related works in terms of their corresponding settings and theoretical guarantees.

There are also other works considering unknown preferences in matching markets. Wang _et al._ study a many-to-one market where an arm can accept multiple players. Jagadeesan _et al._ consider online matching markets with monetary transfers. Min _et al._ investigate Markov matching markets where state transitions occur during the matching process and players' rewards depend on the current state. Other studies have focused on non-stationary rewards, such as Muthirayan _et al._, Ghosh _et al._, who propose robust algorithms to mitigate the impact of reward disturbances. Additionally, several studies have explored the problem of offline matching market learning. Dai and Jordan  propose approaches that leverage historical data to design optimal matching or recommend participants on both sides.

## 3 Setting

This paper considers the problem of bandit learning in two-sided matching markets. Denote \(=\{p_{1},p_{2},,p_{N}\}\) as the player set and \(=\{a_{1},a_{2},,a_{K}\}\) as the arm set. Let \(N\) and \(K\) be the number of players and arms, respectively. To ensure that each player can be matched with an arm, we follow previous works and assume \(N K\).

For each player \(p_{i}\), its preference towards arm \(a_{j}\) can be portrayed by an absolute utility \(_{i,j}(0,1]\). For any pair of arms \(a_{j}\) and \(a_{j^{}}\), \(_{i,j}>_{i,j^{}}\) indicates that player \(p_{i}\) prefers arm \(a_{j}\) over \(a_{j^{}}\). Following previous works for matching markets , players are assumed to have distinct preferences over different arms, i.e., \(_{i,j}_{i,j^{}}\) for any \(a_{j} a_{j^{}}\). In practice, players' preferences which correspond to workers' abilities and the publisher's conversion rates are typically unknown and can be learned through the interactive matching process. On the other side, each arm \(a_{j}\) also has a fixed and distinct preference utility \(_{j,i}\) over each player \(p_{i}\), and \(_{j,i}>_{j,i^{}}\) means that arm \(a_{j}\) prefers player \(p_{i}\) over \(p_{i^{}}\). As in labor markets where workers usually know their preferences over employers based on the payments and task types, the preferences of arms are assumed to be known beforehand .

At each round \(t=1,2,\), each player \(p_{i}\) proposes to an arm \(A_{i}(t)\). For each arm \(a_{j}\), denote \(A_{j}^{-1}(t)=\{p_{i}:A_{i}(t)=a_{j}\}\) as the set of players who selects arm \(a_{j}\) at round \(t\). When more than one player selects \(a_{j}\), it accepts its most-preferred one in \(A_{j}^{-1}(t)\), i.e. \(a_{j}\) will match with \(p_{i}_{p_{i} A_{j}^{-1}(t)}_{j,i}\). If a player \(p_{i}\) is successfully matched with arm \(A_{i}(t)\), it will receive a random reward \(X_{i}(t)\) characterizing its matching experience, which we assume is a 1-subgaussian random variable with expectation \(_{i,A_{i}(t)}\). Otherwise, \(p_{i}\) is rejected by its proposed arm and only gets reward \(X_{i}(t)=0\). Denote \(_{i}(t)\) as the final matched arm of player \(p_{i}\) at round \(t\). Then \(_{i}(t)=A_{i}(t)\) if \(p_{i}\) is accepted by the arm \(A_{i}(t)\) and we simply set \(_{i}(t)=\) if \(p_{i}\) is rejected.

Stability is a key property of a matching in two-sided markets . A matching \((t)=\{(i,_{i}(t)):i[N]\}\) is stable if no market participant wants to break up its current matching relationship and find a new partner. Formally speaking, there is no player-arm pair \((p_{i},a_{j})\) such that \(_{i,j}>_{i,_{i}(t)}\) and \(_{j,i}>_{j,_{i}^{-1}(t)}\). It is worth noting that there may be multiple stable matchings in the market. Denoted \(M=\{m:m\}\) as the set of all stable matchings. It is shown that there exists a stable matching \(m^{*} M\) such that all players are matched with their most preferred stable arm , i.e., \(_{i,m^{*}_{j}}_{i,m_{i}}\) for any \(m M,i[N]\). Given a specified horizon \(T\), the learning objective is to minimize the player-optimal stable regret for each player \(p_{i}\) which is defined as the difference between the cumulative reward received by being matched with \(m^{*}_{i}\) and the cumulative reward received by \(p_{i}\) over \(T\) rounds:

\[Reg_{i}(T)=[_{t=1}^{T}(_{i,m^{*}_{i}}-X_{i}(t) )]\,.\]Here, the expectation is taken over by the randomness of the reward generation and the randomness inherent in the player's strategy.

For completeness, we introduce the procedure of the offline Gale-Shapley (GS) algorithm, which would be useful when describing the algorithmic details. The offline GS algorithm is a classic algorithm to find the player-optimal stable matching when both sides of the market participants know their exact preference rankings. Following offline GS, each player proposes to the arm one by one based on its preference ranking. Until no rejection happens, the final matching is exactly the player-optimal stable matching . Specifically, at the first step, all players propose to their most preferred arm. Arms would accept their most preferred player among those who propose to it and reject others. Then players who are rejected at previous steps would then propose to their next preferred arm. And arms still reject the players who propose to it except for their most preferred one. Such a process continues until no rejection happens.

For convenience, we also define some useful notations that quantify the hardness of the learning problem in matching markets and are used in the later analysis.

**Definition 3.1**.: For each player \(p_{i}\), denote \(_{i}\) as \(p_{i}\)'s preference ranking and let \(_{i,k}\) as \(p_{i}\)'s the \(k\)-th preferred arm in its ranking. With a little abuse of notation, let \(_{i}(a_{j})\) represent the rank of arm \(a_{j}\) in \(p_{i}\)'s preference. For each player \(p_{i}\) and arm \(a_{j} a_{j^{}}\), let \(_{i,j,j^{}}=|_{i,j}-_{i,j^{}}|\) be the preference gap of \(p_{i}\) between \(a_{j}\) and \(a_{j^{}}\). Define \(=_{i,k[_{i}(m_{i}^{*})]}_{i,_{i,k},_{i,k +1}}\) as the minimum preference gap between the arm ranked the first \((_{i}(m_{i}^{*})+1)\)-th among all players. Further, define \(_{N}=_{i,k[N]}_{i,_{i,k},_{i,k+1}}\) as the minimum preference gap between the arm ranked the first \((N+1)\)-th among all players.

## 4 Algorithm for General Markets

In this section, we propose an algorithm called adaptively explore-then-Gale-Shapley (AETGS) with elimination. For simplicity, we present the centralized version of the algorithm in Algorithm 1 from view of player \(p_{i}\). The discussion on how to extend it to a decentralized version is deferred to later subsections.

In general, AETGS with elimination is an adaptive version of the GS algorithm. Since players do not know their preference rankings, they need to learn this knowledge by exploring arms (Line 4). To reduce the regret during exploration, players would adaptively eliminate sub-optimal arms (Line 6-8). Until they find their most preferred arm among available arms, they will stop exploration and focus on this arm (Line 9-11). And once the player finds this arm is occupied by a more preferred player, it would re-start exploration to find the next preferred arm (Line 12-19).

Specifically, each player \(p_{i}\) still maintains \(_{i,j}(t)\) and \(T_{i,j}(t)\) to represent the empirical mean and the number of observations on each arm \(a_{j}\) at the end of round \(t\). To determine whether an arm is more preferred than another, it maintains a confidence interval for each arm \(a_{j}\) with upper bound \(_{i,j}(t):=_{i,j}(t)+(t)}\) and lower bound \(_{i,j}(t):=_{i,j}-(t)}\). When \(T_{i,j}=0\), they will be initialized as \(+\) and \(-\), respectively. And once the confidence intervals of the two arms are disjoint, it can regard the arm with a higher empirical mean to be more preferred (Line 1). To be consistent with the offline GS, each player maintains \(_{i}\) to represent the set of arms that have rejected \(p_{i}\) during previous steps. In the beginning, it is initialized as an empty set. And we use \(_{i}\) to represent the available arms with the potential to be the stable arm of \(p_{i}\), which is initialized as \(_{i}\). For convenience, denote \(_{i}\) as the exploration status of player \(p_{i}\). \(_{i}=\) means that \(p_{i}\) still needs to explore arms in \(_{i}\) to determine its most preferred arm. And \(_{i}=\) means that \(p_{i}\) already finds its most preferred arm and now focuses on this arm (Line 2).

To reduce the regret suffered during exploration, players would update \(_{i}\) and eliminate sub-optimal arms in real-time (Line 6-8). Here to avoid collision during round-robin exploration, we would maintain \(_{i}\) such that it contains no less than \(N\) arms if \(p_{i}\) still has not determined its most preferred one. Thus the union of the available arm set over all players with \(_{i}=\) contains more than \(N\) arms and the round-robin exploration over \(_{i}\) for each such player \(p_{i}\) can be carried out without collisions. For completeness, we defer how to arrange players' explorations in later discussions. And once there exists an arm in \(_{i}\) that can be regarded to be optimal, \(p_{i}\) will set the exploration status \(_{i}\) to be \(\) and update the exploration arm set \(_{i}\) to only contain this optimal arm. For convenience, denote \(A_{i}\) as this arm (Line 9-11).

The update of the available arm set should not only depend on \(p_{i}\)'s own observations but also on the other market participants. Specifically, if a player \(p_{i^{}}\) determines \(A_{i^{}}\) as its most preferred arm, then the final stable player of \(A_{i^{}}\) would be the same as or more preferred than \(p_{i^{}}\). So if \(A_{i^{}}\) prefers \(p_{i^{}}\) than \(p_{i}\), then \(A_{i^{}}\) would not be the stable arm of \(p_{i}\) and there is no need for \(p_{i}\) to explore \(A_{i^{}}\) anymore. In this case, \(p_{i}\) deletes arm \(A_{i^{}}\) from its available set and update \(_{i}\) (Line 12-19). It is worth noting that this operation may incorporate the eliminated arms again in \(_{i}\). This is reasonable as the previously eliminated arm may be more preferred than the current arms in \(_{i}\) after the deletion operation. And if the deleted arm is \(p_{i}\)'s current most preferred arm, it will mark \(_{i}\) as \(\) and restart exploration to find the next most preferred one (Line 15-17).

### Theoretical Results.

**Theorem 4.1**.: _Following Algorithm 1, the player-optimal stable regret for each player \(p_{i}\) satisfies_

\[Reg_{i}(T) O(N^{2} T/^{2}+K T/)\,.\]

Due to the space limit, the proof of Theorem 4.1 is deferred to Appendix A. The following are discussions on the detailed implementation as well as the novelty of the result.

Arrangement of the round-robin exploration process. Recall that the number of available arms of each player is always larger than \(N\) based on Line 6 and we assume players can explore their available arms in a round-robin manner without conflict. We now propose an arrangement by letting players explore the available arms in units of \(N\) to guarantee this property. Specifically, in every \(2N\) rounds, each player selects the \(N\) available arms with the fewest observations (randomly breaks ties) and explores them in a round-robin way. It can be shown by contradiction that there exists an assignment such that each player can successfully match with their respective \(N\) arms once during these \(2N\) rounds (Lemma B.2 in Appendix), which only doubles the original regret without influencing the regret order. This guarantees that after every \(2N\) rounds, the observation count difference among all available arms is at most \(1\). Players would perform arm elimination and optimal arm identification (Line 6 and 9) in the end of each \(2N\) rounds. Therefore, compared to the timely eliminating/deleting of arms, this approach ensures that each player will select each arm at most one additional time during each exploration cycle before the player finds the optimal one. Since each player may restart exploration (Line 12-13) up to \(N^{2}\) times (each of \(N\) players can focus on \(N\) arms), this scenario leads to an additional \(O(N^{2}K)\) constant regret and does not influence the final regret order.

Extension to the decentralized setting.For simplicity, we present the algorithm in a centralized manner. It can also be extended to the decentralized version where no central platform coordinates players' selections. Specifically, we divide the total horizon into several phases and the length of each grows exponentially, i.e., the lengths of phases are \(2,4,8,\). At the end round of each phase, players would decide whether to eliminate sub-optimal arms as Line 6-8, whether to determine one arm as the most preferred one and update the exploration status as Line 9-11. After the end of each phase, players would communicate their current exploration status, update their deletion set and available arm set, re-update the exploration status as Line 12-19, and then communicate their updated available arm set to determine the round-robin exploration process in the next phase. If there exists a player whose exploration status becomes \(\) from \(\) during communication, the phase length would re-start from \(2\) and grows exponentially since new arms may be required for exploration. If the final exploration status of all players is \(\) and their optimal arms are different, the next phase would continues until the end of the interaction. The detailed implementation of communication is deferred to the next paragraph. Based on the communication, players with \(_{i}=\) can have a pre-agreed protocol to explore arms in their available arm set in a round-robin manner without collision as discussed in the last paragraph. Within each phase, they just round-robin explore arms and collect observations but do not make any decision on arms' optimality. If \(L\) observations on a sub-optimal arm are enough to decide its sub-optimality in the centralized version, then this arm would be eliminated at the end of the corresponding phase with the selected time to be at most \(2L\) due to the exponentially increasing phase length. So the regret in this decentralized version is at most two times as that suffered in the centralized version.

This paragraph describes the implementation of the communication procedure. Recall that players need to communicate their exploration status and available arm set (calculated by subtracting the deletion and eliminating set from \(\)) at the end of each phase. For the phase length, recall that it grows exponentially until a player's exploration status becomes \(\) from \(\) and a player updates \(_{i}\) from \(\) as \(\) only when its most preferred arm is occupied by a higher-priority player (Line 15). As shown by Lemma A.3, each player may occupy \(N\) arms, so such event happens at most \(N^{2}\) times. And when all players find their unique optimal arm which requires \(O(N^{2} T/^{2})\) times, the phase would continue until the end of the interaction. Above all, the total number of phases is of order \(O(N^{2}(N^{2} T/^{2}))\). For the detailed communication procedure, as phase 1 in Kong and Li , players can first estimate their unique indices and we assume the matching results are public as . During the communication block of each phase, players sequentially transmit their data based on their indices, received by others through matching outcomes. Specifically, in the corresponding round, player \(p_{i}\) selects the focused arm if \(_{i}=\) and nothing otherwise, incurring an \(O(N^{3}(N^{2} T/^{2}))\) cost for status communication in all phases. For deletion (eliminating) sets, \(p_{i}\) first selects the arm with index \(k\) to indicate it will transmit \(k\) arms and then sequentially selects these \(k\) arms. The communication cost on the arm set size is \(O(N^{3}(N^{2} T/^{2}))\). Recall that players delete arms only when a higher-priority player focuses on this arm, so \(N\) players focus on at most \(N\) arms before reaching stability and each player deletes up to \(N\) arms. Also, each player can eliminate up to \(K-N\) arms during each exploration and would re-start exploration for at most \(N^{2}\) times. Thus the communication cost on the deletion (eliminating) arms is \(O(N^{3}K)\) and the total communication cost is \(O(N^{3}(N^{2} T/^{2})+N^{3}K)\), which is not the main order of the regret.

Key idea of removing the dependence on \(K\).Balancing the exploration-exploitation trade-off is the key to achieving low regret. Previous efforts were devoted to addressing pessimal stable regret  and uniqueness assumptions  using classic UCB and TS strategies. Until recently, Zhang _et al._ and Kong and Li  show that ETC-type strategies better fit this problem. Specifically, players first uniformly explore arms to learn the complete preference ranking of the top \(N\) arms, and then use the GS procedure for exploitation to find the player-optimal stable matching. However, such a method may over-explore and cause unnecessary regret. The reason is that to learn the first \(N\)-ranked arms, each sub-optimal arm \(a_{j}\) must be selected \(O( T/_{i,_{i,N},j}^{2})\) times to be distinguished from the \(N\)-ranked arm. And each time selecting this arm, the player pays \(_{i,m_{i}^{*},j}\) regret. The mismatch between the paid regret and the difference to be figured out results in \(O(K T/_{N}^{2})\) regret.

In contrast to the existing approach, we present a more adaptive perspective that integrates the learning process into each GS step. To avoid additional regret, players do not need to estimate their complete preference. Instead, they would start exploitation once the optimal available arm is identified. And if this arm is occupied by a higher-priority player, this player would restart exploration to find the next preferred one. To avoid the additional cost while exploring the optimal arm, we design a more efficient way to let players promptly eliminate \(K-N\) sub-optimal arms and only maintain the remaining \(N\) arms to guarantee no collision. For the eliminated arm \(a_{j}\), the reward difference to be figured out is at least \(_{i,m^{*}_{i},j}\), which matches the regret when selecting this arm. So the regret caused by these eliminated arms is \(O(K T/)\), avoiding dependence on \(K\) in the main order.

Recall Kong and Li  propose an adaptively-explore-then-deferred-acceptance (AETDA) algorithm for more general many-to-one markets with responsiveness. Compared with their realization in the one-to-one setting, our algorithm shares the same idea of balancing exploration and exploitation but further introduces the elimination operation to avoid unnecessary selections. Compared with their \(O(N^{2}K T/^{2})\) result in the reduced one-to-one setting, our Theorem 4.1 removes the dependence on \(K\) in the main order.

Discussion on the definition of gaps. Recall that our \(\) is defined as the minimum preference difference among arms that ranked in the first \((_{i}(m^{*}_{i})+1)\)-th positions (\(_{4}\)), while the lower bound in Sankararaman _et al._ for markets with serial dictatorship depends on \(\) that is defined as the minimum preference difference between the arm ranked \(_{i}(m^{*}_{i})\) and the arm ranked \(_{i}(m^{*}_{i})+1\) (\(_{1}\) in Table 1, respectively). It is an open problem whether the lower bound should depend on our \(_{4}\) in general markets. Here we would like to discuss that the knowledge of \(_{4}\) is important to learn the true stable matching. Consider a market with \(4\) players and \(5\) arms. The preference rankings of players are \(p_{1}:a_{1}>a_{2}>a_{3}>a_{4}>a_{5};p_{2}:a_{2}>a_{3}>a_{1}>a_{4}>a_{5};p_{3}:a _{3}>a_{1}>a_{2}>a_{4}>a_{5};p_{4}:a_{1}>a_{2}>a_{3}>a_{4}>a_{5}\) and the preference rankings of arms are \(a_{1}:p_{2}>p_{3}>p_{4}>p_{1};a_{2}:p_{3}>p_{4}>p_{1}>p_{2};a_{3}:p_{4}>p_{1}>p_ {2}>p_{3};a_{4}:p_{1}>p_{2}>p_{3}>p_{4};a_{5}:p_{1}>p_{2}>p_{3}>p_{4}\). In this market, the player-optimal stable matching is \(\{(p_{1},a_{4}),(p_{2},a_{1}),(p_{3},a_{2}),(p_{4},a_{3})\}\). However, if player \(p_{1}\) has collected enough observations to identify \(_{4}\) in general markets. Here we would like to discuss that the knowledge of \(_{4}\) and wrongly estimate the to learn the true stable matching. Consider a market with \(4\) players and \(5\) arms. The preference rankings of players are \(p_{1}:a_{1}>a_{2}>a_{3}>a_{4}>a_{5};p_{2}:a_{2}>a_{3}>a_{1}>a_{4}>a_{5};p_{3}:a _{1}>a_{2}>a_{4}>a_{5};p_{4}:a_{1}>a_{2}>a_{3}>a_{4}>a_{5}\) and the preference rankings of arms are \(a_{1}:p_{2}>p_{3}>p_{4}>p

## 6 Centralized UCB Algorithm for Markets with \(\)-condition

In this section, we provide a new analysis for the centralized UCB algorithm in markets satisfying \(\)-condition. The algorithm is first introduced by Liu _et al._. For completeness, we present the full algorithm in Algorithm 2. At each round, players submit their UCB rankings to the centralized platform (Line 3). The platform runs the GS algorithm (based on players' submitted rankings) and returns the partner to each player (Line 4).

```
1:\(N,K\).
2: Initialize: \(_{i,j}(0)=0,T_{i,j}(0)=0,_{i,j}(1)=\), \( i[N],j[K]\).
3:for round \(t=1,2,,T\)do
4: Receive rankings \(:=\{_{i}\}_{i[N]}\) according to the decreasing order of \(\{_{i,j}(t)\}_{j[K]}, i[N]\);
5:\(A_{i}(t)\)Gale-Shapley (\(,\)) for each player \(p_{i}\);
6: Observe \(X_{i}(t)\), and update \(_{i,j}(t)\), \(T_{i,j}(t)\), \(_{i,j}(t+1)\) for each \(p_{i},a_{j}\);
7:endfor
```

**Algorithm 2** centralized UCB

In the following, we introduce the \(\)-condition. Conditions guaranteeing the unique stable matching have been widely studied in the offline setting  and also the online setting to improve the learning efficiency . Among these conditions, the \(\)-condition is shown to be the weakest sufficient one  and incorporate the conditions studied in existing works .

Let \(\) denote a pair of permutations of \([N]\) and \([K]\). Then \([N]_{}=\{Q_{1}^{()},,Q_{N}^{()}\}\) and \([K]_{}=\{q_{1}^{()},,q_{K}^{()}\}\) denote permutations of the ordered sets \([N]\) and \([K]\), respectively. The \(j\)-th player in \([N]_{}\) is the \(Q_{j}^{()}\)-th player in \([N]\), and the \(k\)-th arm in \([K]_{}\) is the \(q_{k}^{()}\)-th arm in \([K]\). Then we can define the \(\)-condition below.

**Definition 6.1**.: The \(\)-condition is satisfied if there is a stable matching (\(},}\)), a left-order of players and arms s.t. \( i[N]_{l}, j>i,j[K]_{l}:_{i,j^{*}_{l}}>_{i,j}\) where \(j^{*}_{i}\) is the partner of player \(p_{i}\) in stable matching (\(},}\)), and a (possibly different) right-order of players and arms s.t. \( j<i N,q_{j}[K]_{r},Q_{i}[N]_{r}:_{q_{j},Q_{i^{*}_{q_{j}}} }>_{q_{j},Q_{i}}\). Here similarly, \(i^{*}_{q_{j}}\) is the partner of arm \(a_{q_{j}}\) in stable matching (\(},}\)).

Without loss of generality, we consider the identity of players and arms is just the left order, i.e., \([N]=[N]_{l}\) and \([K]=[K]_{l}\). Thus we only deal with player order \(Q_{i}^{(r)}=Q_{i}\) and arm order \(q_{j}^{(r)}=q_{j}\), for \(i[N],j[K]\) in the rest of the paper. Under \(\)-condition, it is easy to inductively verify that for any \(i[N]\), the player \(p_{i}\) is matched with arm \(a_{i}\), and the player \(p_{Q_{i}}\) is matched with the arm \(a_{q_{i}}\) in the unique stable matching .

Figure 1: Experimental comparisons of our AETGS-E with ETGS, ML-ETC and Phased ETC in one-to-one decentralized markets with \(N=3\) players and \(K=10\) arms.

### Theoretical Results

We analyze the regret for the centralized UCB algorithm under \(\)-condition.

**Theorem 6.2**.: _When preferences of participants satisfy \(\)-condition, following Algorithm 2, the stable regret for each player \(p_{i}\) satisfies_

\[Reg_{i}(T) O(N T/_{N}^{2}+K T/)\,.\]

The centralized UCB algorithm is proposed by Liu _et al._ and shown to have \(O(NK T/^{2})\) player-pessimal stable regret for general markets. We provide a new analysis for markets satisfying \(\)-condition which removes the dependence of \(K\) in the regret. Due to the space limit, we discuss the key idea of the proof below and defer the detailed proof to Appendix C.

We investigate the preference structure of \(\)-condition to obtain the improved analysis. For player \(p_{i}\), its regret is due to selecting sub-optimal arm \(a_{k}\) with \(_{i,k}<_{i,i}\). Arm \(a_{k}\) will be selected by \(p_{i}\) when its UCB value is higher than \(p_{i}\)'s stable matched arm \(a_{i}\), which time is bounded by \(O( T/_{i,i,k}^{2})\), and one \(_{i,i,k}\) on the denominator can be eliminated when multiplying \(_{i,i,k}\) to compute regret. This contributes \(O(K T/)\) regret since there are at most \(K-1\) sub-optimal arms. It is worth noting that arm \(a_{k}\) will also be selected by \(p_{i}\) if \(p_{i}\) is rejected by \(a_{i}\) in the GS algorithm. Recall that under \(\)-condition, there is a right order \(Q_{i^{}}=i[N]_{r}\) for player \(p_{i}\), such that \( i^{}>i^{},Q_{i^{}}[N]_{r}:_{q_{i^{ }},Q_{i^{}}}_{q_{i^{}},Q_{i^{}}}\), which means arm \(a_{q_{i^{}}}=a_{i}\) can only prefer players \(p_{Q_{1}},p_{Q_{2}},,p_{Q_{i^{}-1}}\) than player \(p_{Q_{i^{}}}=p_{i}\). Thus \(p_{i}\) is rejected by \(a_{i}\) only when these players select \(a_{i}\), and \(a_{i}\) is sub-optimal for those players. To bound the regret of \(p_{i}\) when being rejected, we just need to bound the exploration times of these players \(p_{Q_{1}},p_{Q_{2}},,p_{Q_{i^{}-1}}\) on arm \(a_{i}\). However, the exploration time of a single player \(p_{Q_{}}\) with \(1 i^{}-1\) on \(a_{i}\) can not be trivially bounded by \(O( T/_{N}^{2})\) since \(p_{Q_{}}\) may have to select arm \(a_{i}\) after rejected by its stable arm \(a_{q_{}}\) in offline GS, where \(a_{q_{}}\) might be selected by \(p_{Q_{1}},,p_{Q_{-1}}\). This leads to a recursion form. We control this term using the fact that when a player is rejected by its stable matched arm in the GS, it can date back to a higher right-order player wrongly over-estimate its preference for a sub-optimal arm. This key observation and the definition of \(\) make it possible to derive the final \(O(N T/_{N}^{2})\) bound.

## 7 Conclusion

In this paper, we investigate the problem of whether a tighter bound can be derived for the bandit learning problem in two-sided matching markets. For the general one-to-one matching markets, we try to improve the learning efficiency of the existing algorithms. By integrating the offline GS procedure into the online learning process and carefully designing the elimination strategy, we show that the player-optimal stable regret can be upper bounded by \(O(N^{2} T/^{2}+K T/)\). This result removes the dependence on \(K\) in the main order term of existing works and improves the state-of-the-art result  in common cases where the number of players is much smaller than that of arms. An experiment is conducted to verify its advantage over other baselines in such markets. We also present a novel analysis for the centralized UCB algorithm in markets satisfying \(\)-condition and derive an improved \((N T/_{N}^{2}+K T/)\) regret upper bound.

One significant future direction is to investigate the optimality of algorithms. Although the dependence on \(N,K,T\) in Theorem 6.2 matches the lower bound, the definition of \(\) differs. It remains unclear how the upper bound changes with the same \(\). Furthermore, since the lower bound provided by  applies only to special markets, and the learning problem in general markets is more challenging due to the complex preference structure, determining whether an algorithm can perform better in general markets is still an open problem.