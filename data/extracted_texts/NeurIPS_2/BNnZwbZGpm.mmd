# Provably Faster Algorithms for Bilevel Optimization

via Without-Replacement Sampling

 Junyi Li and Heng Huang

Department of Computer Science, Institute of Health Computing

University of Maryland College Park

College Park, MD, 20742

junyili.ai@gmail.com, henghuanghh@gmail.com

###### Abstract

Bilevel Optimization has experienced significant advancements recently with the introduction of new efficient algorithms. Mirroring the success in single-level optimization, stochastic gradient-based algorithms are widely used in bilevel optimization. However, a common limitation in these algorithms is the presumption of independent sampling, which can lead to increased computational costs due to the complicated hyper-gradient formulation of bilevel problems. To address this challenge, we study the example-selection strategy for bilevel optimization in this work. More specifically, we introduce a without-replacement sampling based algorithm which achieves a faster convergence rate compared to its counterparts that rely on independent sampling. Beyond the standard bilevel optimization formulation, we extend our discussion to conditional bilevel optimization and also two special cases: minimax and compositional optimization. Finally, we validate our algorithms over both synthetic and real-world applications. Numerical results clearly showcase the superiority of our algorithms.

## 1 Introduction

Bilevel optimization  has received a lot of interest recently due to its wide-ranging applicability in machine learning tasks, including hyper-parameter optimization , meta-learning  and personalized federated learning . A bilevel optimization problem is a type of nested two-level problems as follows:

\[_{x^{p}}h(x) f(x,y_{x})y_{x}=*{arg \,min}_{y^{d}}g(x,y), \]

which includes an outer problem \(f(x,y)\) and a inner problem \(g(x,y)\). The outer problem \(f(x,y)\) relies on the solution \(y_{x}\) of the inner problem \(g(x,y)\). Eq. (1) can be solved through gradient descent: \(x_{t+1}=x_{t}- h(x_{t})\), with \(\) be the stepsize and \( h(x_{t})\) be gradient at the state \(x_{t}\). Specially, the gradient \( h(x)\) is a function of the inner problem minimizer \(y_{x}\), first order derivatives \(_{x}f(x,y)\), \(_{y}f(x,y)\), \(_{y}g(x,y)\) and second order derivatives \(_{xy}g(x,y)\), \(_{y^{2}}g(x,y)\). In particular, \(y_{x}\) can be solved though gradient descent: \(y_{t+1}=y_{t}-_{y}g(x,y_{t})\) with \(\) be the stepsize and \(_{y}g(x_{t},y_{t})\) be gradient at the iterate \((x_{t},y_{t})\). In the standard setup, the outer and inner problems are defined over two datasets \(_{u}=\{_{i},i[m]\}\) and \(_{l}=\{_{j},i[n]\}\), respectively:

\[f(x,y)=_{i=1}^{m}f(x,y;_{i})g(x,y)=_{j=1}^{n}g(x,y;_{j}),\]

Naturally, we sample from the datasets to estimate the first and second-order derivatives in the hyper-gradient formulation. And to guarantee convergence, previous approaches require the examplesbe _mutually independent_ even at the same state \((x,y)\). For example, two different examples \(_{1}\) and \(_{2}\) are sampled to estimate \(_{y}g(x,y)\) and \(_{y^{2}}g(x,y)\), respectively. This leads to extra computational cost in practice. Indeed, two backward passes are required to evaluate \(_{x}f(x,y)\) and \(_{y}f(x,y)\), while five backward passes are needed to evaluate \(_{y}g(x,y)\), \(_{y^{2}}g(x,y)\) and \(_{xy}g(x,y)\) for a given state of \((x,y)\). In contrast, one backward pass are needed to evaluate the former and two backward passes for the latter if not sampling independently for each property. This leads to notable computational cost difference for the current large-scale machine learning models . Therefore, it is beneficial to explore other example-selection strategies beyond independent sampling. More specifically, we aim to answer the following question in this work: _Can we solve the bilevel optimization problem without using independent sampling?_

Although example-selection is under-explored for bilevel optimization, it has been well studied in the single level optimization literature . For single level problems: \(_{x^{p}}f(x)=_{i=1}^{n}f(x;_{i})\), we iteratively perform the stochastic gradient step \(x_{t+1}=x_{t}- f(x_{t},_{t})\), where the sample gradient is used to estimate the full gradient and the example \(_{t}\) is sampled according to some order. More specifically, the sampling schemes are roughly divided into two categories: _with-replacement_ sampling and _without-replacement_ sampling. Traditional stochastic gradient descent typically employs with-replacement sampling to ensure an unbiased estimation of the full gradient in each iteration. In contrast, the without-replacement sampling has biased estimation at each iteration. Despite this, when averaging the gradients of consecutive examples, without-replacement sampling can approximate the full gradient more efficiently . In fact, any permutation-based without-replacement sampling has that the gradient estimation error contracts at rate of \(O(K^{-2})\) where \(K\) is the number of steps, while stochastic gradient descent only has a rate of \(O(K^{-1})\). Inspired by the properties of without-replacement sampling demonstrated in the single-level optimization, we design the algorithm named WiOR-BO (and its variants) which performs without-replacement sampling for bilevel optimization. Naturally, WiOR-BO does not require the independent sampling as in previous methods. In fact, our algorithm enjoys favorable convergence rate. As shown in Table 1, WiOR-BO converges faster compared with their counterparts (_e.g._ StocBiO ) using independent sampling. Actually, WiOR-BO gets comparable convergence rate with methods using more complicated variance reduction techniques (_e.g._ MRBO ). In Table 2, we perform a more comprehensive comparison with other variance reduction techniques and in Section C of Appendix, we further compare WiOR-BO with these methods.

The **contributions** of our work can be summarized as:

1. We propose **WiOR-BO** which performs without-sampling to solve the bilevel optimization problem and converges to an \(\) stationary point with a rate of O(\(^{-3}\)). This rate improves over the O(\(^{-4}\)) rate of its counterparts (_e.g._ stocBiO ) using independent sampling.
2. We propose **WiOR-CBO** for the conditional bilevel optimization problems and show that our algorithm converges to an \(\) stationary point with a rate of O(\(^{-4}\)). This rate improves over the O(\(^{-6}\)) rate of its counterparts (_e.g._ DL-SGD ) using independent-sampling.

   & **Algorithm** & **\(\)** & **\(\)** & \(\)** & \(\) \\   & BSA  & \(O(^{-6})\) & \(O(^{-6})\) & \(O(^{-4})\) & \(O(^{-4})\) \\  & TTSA  & \(O(^{-5})\) & \(O(^{-3})\) & \(O(^{-5})\) & \(O(^{-5})\) \\  & StocBiO  & \(O(^{-6})\) & \(O(^{-4})\) & \(O(^{-4})\) & \(O(^{-4})\) \\  & SOBA  & \(O(^{-4})\) & \(O(^{-4})\) & \(O(^{-4})\) & \(O(^{-4})\) \\  & AIDITP  & \(O(max(m,n)^{-2})\) & \(O(max(m,n)^{-2})\) & \(O(max(m,n)^{-2})\) & \(O(max(m,n)^{-2})\) \\  & **WiOR-BO (Ours)** & \(O((max(m,n)^{-3})\) & \(O(max(m,n)^{-3})\) & \(O(max(m,n)^{-3})\) & \(O(max(m,n)^{-3})\) \\   & DL-SGD  & \(O(^{-4})\) & \(O(^{-6})\) & \(O(^{-4})\) & \(O(^{-4})\) \\  & RT-MLMC  & \(O(^{-4})\) & \(O(^{-4})\) & \(O(^{-4})\) & \(O(^{-4})\) \\  & **WiOR-CBO (Ours)** & \(O((max(m,n)^{-3})\) & \(O(max(m,n)^{-4})\) & \(O(n(max(m,n))^{-3})\) & \(O((max(m,n))^{2-4})\) \\  

Table 1: **Comparisons of the Bilevel Opt. & Conditional Bilevel Opt. algorithms for finding an \(\)-stationary point. We include comparison with stochastic gradient descent type of methods and a more comprehensive comparison including other acceleration methods can be found in Table 2 of Appendix. An \(\)-stationary point is defined as \(\| h(x)\|\). \(Gc(f,)\) and \(Gc(g,)\) denote the number of gradient evaluations _w.r.t._\(f(x,y)\) and \(g(x,y)\); \(JV(g,)\) denotes the number of Jacobian-vector products; \(HV(g,)\) is the number of Hessian-vector products. \(m\) and \(n\) are the number of data examples for the outer and inner problems, in particular, \(n\) is the maximum number of inner problems for conditional bilevel optimization. Our methods have a dependence over example numbers \((max(m,n))^{q}\), \(q\) is a value decided by without-replacement sampling strategy and can have value in \(\) (A herding-based permutation  can let \(q=0\)).**3. We customize our algorithms to the special cases of minimax and compositional optimization problems and demonstrate similar favorable convergence rates.

**Notations.**\(\) (\(_{x}\)) denotes full gradient (partial gradient, over variable \(x\)), higher order derivatives follow similar rules. \([n]\) represents sequence of integers from 1 to \(n\). \(O()\) is the big O notation, and we hide logarithmic terms.

## 2 Related Works

**Bilevel Optimization** has its roots tracing back to the 1960s, beginning with the regularization method proposed by , and then followed by many research works [14; 47; 43; 55]. In the field of machine learning, similar implicit differentiation techniques were used to solve Hyper-parameter Optimization [28; 6; 12]. Exact solutions for Bilevel Optimization solve the inner problem for each outer variable but are inefficient. More efficient algorithms solve the inner problem with a fixed number of steps, and use the 'back-propagation through time' technique to compute the hyper-gradient [13; 35; 16; 40; 45]. Recently, single loop algorithms [17; 22; 25; 26; 52; 9; 30; 24] are introduced to perform the outer and inner updates alternatively. Besides, other aspects of the bilevel optimization are also investigated:  studied a type of conditional bilevel optimization where the inner problem depends on the example of the outer problem; [53; 56] studied the Hessian-free bilevel optimization and proposed an algorithm with optimal convergence rate;  proposed an efficient single loop algorithm for the general non-convex bilevel optimization; [54; 31] studies the bilevel optimization in the federated learning setting.

**Sample-selection** in stochastic optimization has been extensively studied for the case of single level optimization problems [2; 3; 19; 38]. Beyond the with-replacement sampling adopted by SGD, various without-replacement strategies were also studied in the literature. The random-reshuffling strategy shuffles the data samples at the start of each training epoch, and its property was first studied in  via the non-commutative arithmetic-geometric mean conjecture and followed by [20; 19]. [44; 36] studied the shuffling-once strategy, where the data samples are only shuffled at the start of the training once;  investigated data echoing where one data sample is repeatedly used. Quasi-Monte Carlo sampling is also applied in stochastic gradient descent [5; 34]. Recently, [34; 33] proposed to use average gradient error to study the convergence of various example order strategies and then proposed a herding-based sampling strategy  to proactively minimize the average gradient error.

## 3 Without-Replacement Sampling in Bilevel Optimization

In this work, we focus on the following finite-sum bilevel optimization problem:

\[_{x^{p}}h(x) f(x,y_{x})=_{i=1}^{m}f( x,y_{x};_{i}),y_{x}=*{arg\,min}_{y^{d}}g(x,y)=_{j= 1}^{n}g(x,y;_{j}) \]

where both the outer and the inner problems are defined over a finite dataset, and we denote them as \(_{u}=\{_{i},i[m]\}\) and \(_{l}=\{_{j},j[n]\}\), respectively. Under mild assumptions, the hyper-gradient of \(h(x)\) is:

\[ h(x)=_{x}f(x,y_{x})-_{xy}g(x,y_{x})u_{x},\;u_{x}=_{y ^{2}}g(x,y_{x})^{-1}_{y}f(x,y_{x}) \]

Our objective is to minimize \(h(x)\) by exploiting Eq. (3) with examples from \(_{u}\) and \(_{l}\) at each step. More specifically, for a given example order \(\) denoting the following example-selection sequence as \(\{_{t}^{}_{u}\}\) and \(\{_{t}^{}_{l}\}\) for \(t[T]\), where \(T\) denotes the length of the sequence and can be arbitrarily large. We then perform the following update steps iteratively for \(t[T]\):

\[y_{t+1} =y_{t}-_{t}_{y}g(x_{t},y_{t};_{t}^{}) \] \[u_{t+1} =u_{t}-_{t}(_{y^{2}}g(x_{t},y_{t};_{t}^{})u_{t} -_{y}f(x_{t},y_{t};_{t}^{}))\] (4b) \[x_{t+1} =x_{t}-_{t}(_{x}f(x_{t},y_{t};_{t}^{})-_{ xy}g(x_{t},y_{t};_{t}^{})u_{t}) \]

where Eq. (4) adopts the single-loop  update. More specifically, Eq. (4a) performs gradient descent to estimate the minimizer \(y_{x}\); Eq. (4b) performs gradient descent to estimate \(u_{x}\) defined in Eq. (3); and Eq. (4c) perform the gradient descent step given the estimation of \(y_{x}\) and \(u_{x}\).

Note that at each step \(t\), we use a pair of examples \(\{_{t}^{},_{t}^{}\}\) to estimate the involved properties and require only three backward passes, _i.e._\( f(x_{t},y_{t};_{t}^{})\), \( g(x_{t},y_{t};_{t}^{})\) and \((_{y}g(x_{t},y_{t};_{t}^{}))\). In contrast, if we independently sample examples for each property, seven backward passes are needed. For large-scale machine learning models where back-propagation is expensive, our method can lead to significant computation saving in practice.

Equation (4) is independent of the specific order of samples. It can accommodate both with-replacement and without-replacement sampling strategies. In particular, we consider two most widely-used without-replacement sampling methods: _random-reshuffling_ and _shuffle-once_. For random-reshuffling, we set \(T=R lcm(m,n)\), where \(lcm(m,n)\) denotes the least common multiple of \(m\) and \(n\) is a constant. More specifically, we generate \(R lcm(m,n)/m\) random permutations of examples in \(_{u}\) then concatenate them to get \(\{_{t}^{}_{u}\}\), and we follow similar procedure to generate \(\{_{t}^{}_{u}\}\). In algorithm 1, we show our **WiOR-BO** algorithm for solving Eq. (2) with random-reshuffling sampling. Note that in Line 4 of Algorithm 1, \(_{t}()\) is the projection operation to an \(t\)-size ball and we perform projection to make \(u_{t}\) be bounded during updates. Shuffle-once is another widely used without-replacement sampling strategy, where a single permutation is generated and reused for both \(_{u}\) and \(_{l}\), and we can modify Line 3 of Algorithm 1 to incorporate shuffle-once. Finally, compared to independent sampling-based algorithm such as stocBiO , we require extra space to generate and store the orders of examples, but this cost is negligible compared to memory and computational cost of training large scale models.

```
1:Input: Initial states \(x_{l}^{0}\), \(y_{l}^{0}\) and \(u_{l}^{0}\); learning rates \(\{_{i}^{r},_{i}^{r},_{i}^{r}\},i[I],r[R]\), \(I(m,n)\) denotes the least common multiple of \(m\) and \(n\).
2:for epochs \(r=1\)to\(R\)do
3: Randomly sample \(I/m\) permutations of outer dataset and concatenate them to have \(\{_{i}^{},i[I]\}\), sample \(I/n\) permutations of inner dataset and concatenate them to have \(\{_{i}^{},i[I]\}\);
4: Set \(y_{0}^{r}=y_{l}^{r-1}\), \(u_{0}^{r}=_{}(u_{I}^{r-1})\) and \(x_{0}^{r}=x_{I}^{r-1}\)
5:for\(i=0\)to\(I-1\)do
6:\(y_{i+1}^{r}=y_{i}^{r}-_{i}^{r}_{y}g(x_{i}^{r},y_{i}^{r};_{i} ^{})\); \(u_{i+1}^{r}=u_{i}^{r}-_{i}^{r}(_{y^{2}}g(x_{i}^{r},y_{i}^{r};_{ i}^{})u_{i}^{r}-_{y}f(x_{i}^{r},y_{i}^{r};_{i}^{}))\);
7:\(x_{i+1}^{r}=x_{i}^{r}-_{i}^{r}(_{x}f(x_{i}^{r},y_{i}^{r};_{i}^{ })-_{xy}g(x_{i}^{r},y_{i}^{r};_{i}^{})u_{i}^{r})\);
8:endfor
9:endfor
```

**Algorithm 1** Without-Replacement Bilevel Optimization (WiOR-BO)

### Conditional Bilevel Optimization

In Eq. (2), we assume the outer dataset \(_{u}\) and the inner dataset \(_{l}\) are independent with each other. However, it is also common in practice that the inner problem not only relies on the outer variable \(x\), but also the current outer example. This type of problems is known as conditional (contextual) bilevel optimization problems  in the literature and we consider the finite setting as follows:

\[_{x^{p}}\,h(x) f(x,y_{x})= _{i=1}^{m}f(x,y_{x}^{(_{i})},_{i}),\ y_{x}^{(_{i})}= *{arg\,min}_{y^{d}}\,g^{(_{i})}(x,y)=}}_{j=1}^{n_{_{i}}}g(x,y;_{_{i},j}), \]

Note that in Eq. (5), the outer problem is defined on a finite dataset \(_{u}=\{_{i},i[m]\}\), and for each example \(_{i}_{u}\), we have a inner problem \(g^{(_{i})}(x,y)\) defined on a dataset \(_{l,_{i}}=\{_{_{i},j},j[n_{_{i}}]\}\). The hyper-gradient of Eq. (5) is:

\[ h(x)=_{i=1}^{m}_{x}f(x,y_{x}^{ _{i}};_{i})-_{xy}g^{(_{i})}(x,y_{x}^{_{i}})u_{x}^{_{i}} ,u_{x}^{_{i}}=_{y^{2}}g^{(_{i})}(x,y_{x})^{-1}_{y}f(x, y_{x};_{i}), \]

The key difference of Eq. (6) with Eq. (3) is that \(y_{x}^{_{i}}\) and \(u_{x}^{_{i}}\) are defined for each example \(_{i}_{u}\). To minimize \(h(x)\) in Eq. (5), we assume a sample order \(\) denoting the following example selection sequence as \(\{_{t}^{}_{u},t[T]\}\), where \(T\) denotes the example sequence length and can be arbitrarily large. We then perform the following update steps iteratively:

\[x_{t+1}=x_{t}-_{t}(_{x}f(x_{t},_{t};_{t}^{})-_{xy}g^{ _{t}^{}}(x_{t},_{t})_{t}) \]

where \(_{t}\) and \(_{t}\) are estimations of \(y_{t}^{_{t}^{}}\), \(u_{t}^{_{t}^{}}\). We get them by performing the following rules \(T_{l}\) steps over the sample order \(\{_{_{t}^{},t_{l}}^{}_{l,_{t}^{}},t_{l}[T _{l}]\}\}\):

\[y_{t_{l}+1}=y_{t_{l}}-_{t_{l}}_{y}g(x_{t},y_{t_{l}};_{_{t} ^{},t_{l}}^{}),u_{t_{l}+1}=u_{t_{l}}-_{t_{l}}(_{y^{2}}g(x_{t}, y_{t_{l}},_{_{t}^{},t_{l}}^{})u_{t_{l}}-_{y}f(x_{t},y_{t_{l} };_{t}^{})) \]

where both the outer variable state \(x_{t}\) and sample \(_{t}^{}\) are fixed. We then set \(_{t}=y_{T_{l}}\) and \(_{t}=u_{T_{l}}\) in Eq. (7). Note that Eq. (7) and Eq. (8) perform a double-loop update rule . Since the inner problem relies on the example of the outer problem, we compute \(y_{x}^{_{t}}\) and \(u_{x}^{_{t}^{}}\) for each new state \(x\) and sample \(_{i}\) instead of reusing them as in the single loop update of Eq. (4).

Note that an important feature of our algorithm is that we select samples based on an example order \(\) at each step instead of sampling independently. Similar to the unconditional bilevel optimization, various example orders can be incorporated. For random-reshuffling, we set \(T=Rm\) with \(R\) be a constant. Then we generate \(R\) random permutations of examples in \(_{u}\) and concatenate them to get the example order \(\{_{t}^{}_{u},t[T]\}\). Similarly, we concatenate \(S\) random permutations of the inner dataset \(_{l,_{t}}\) to get \(\{_{_{t}^{},t_{l}}^{}_{l,_{t}^{}},t_{l}[ T_{l}]\}\}\) with a sequence length \(T_{l}=S n_{_{i}}\). We summarize this in Algorithm 2 and denote it as **WiOR-CBO**. We slightly abuse the notation to replace \(n_{_{i}}\) with \(n_{i}\) in the inner loop update for better clarity.

```
1:Input: Initial state \(x^{0}\), learning rates \(\{_{i}^{}\},i[m],r[R]\).
2:for epochs \(r=1\)to \(R\)do
3: Randomly sample a permutation of outer dataset to have \(\{_{i}^{},i[m]\}\) and set \(x_{0}^{r}=x_{m}^{r-1}\) or \(x_{0}\) if \(r=1\).
4:for\(i=0\)to\(m-1\)do
5: Initial states \(y^{0}\) and \(u^{0}\), learning rates \(\{_{j}^{s},_{j}^{s}\},j[n_{i}],s[S]\).
6:for\(s=1\)to\(S\)do
7: Sample a permutation of inner dataset to have \(\{_{_{i},j}^{},j[n_{i}]\}\), set \(y_{0}^{s}=y_{n_{i}}^{s-1}\) and \(u_{0}^{s}=_{t}(u_{n_{i}}^{s-1})\) or \(y_{0}^{s}=y_{0}\) and \(u_{0}^{s}=u_{0}\) if \(s=1\).
8:for\(j=0\)to\(n_{i}-1\)do
9:\(y_{j+1}^{s}=y_{j}^{s}-_{j}^{s}_{y}g(x_{t}^{r},y_{j}^{s};_{t_ {j}}^{})\);
10:\(u_{j+1}^{s}=u_{j}^{s}-_{j}^{s}(_{y^{2}}g(x_{t}^{r},y_{j}^{s};_{t _{j}}^{})u_{j}^{s}-_{y}f(x_{t}^{r},y_{j}^{s};_{i}^{}))\);
11:endfor
12:endfor
13:\(x_{t+1}^{r}=x_{t}^{r}-_{i}^{r}(_{x}f(x_{i}^{r},y_{n_{i}}^{S};_{i} ^{})-_{xy}g^{_{t}^{}}(x_{t}^{r},y_{n_{i}}^{S})u_{n_{i}}^{S})\);
14:endfor
15:endfor
```

**Algorithm 2** Without-Replacement Conditional Bilevel Optimization (WiOR-CBO)

### MiniMax and Compositional Optimization Problems

Our discussion of without-replacement sampling for bilevel optimization can be customized for two important nested optimization problems: _compositional optimization_ and _minimax optimization_ problems, as they can be viewed as a special type of bilevel optimization problem. Suppose we let the outer problem only rely on \(y\), set the inner problem as \(g(x,y)=\|y-r(x)\|^{2}\). Consider the finite case of \(f(y)\) and \(r(x)\), we have the following compositional optimization problem:

\[_{x^{p}}h(x)_{i=1}^{m}f(r(x);_{i}) =_{i=1}^{m}f(_{j=1}^{n}r(x;_{j});_{i}) \]

Then we can derive the hyper-gradient as: \( h(x)=_{x}r(x)u_{x},\) where \(u_{x}=_{y}f(r(x)),\) meanwhile, we have \(_{y}g(x,y)=y-r(x)\). Then we can instantiate Algorithm 1 to get an algorithm for compositional optimization problem using without-replacement sampling (Algorithm 3 of Appendix). Similar to the SCGD algorithm , we track the exponential moving average of the inner state 

[MISSING_PAGE_FAIL:6]

Assumption 4.5 measures how well the sample gradients approximate the full gradient over an interval on average. Similar assumptions are used to study the sample order in single level optimization problems . If samples are selected independently at each step, we get \(=1\) and \(C=A\) based on Assumption 4.3 and 4.4. For any permutation-based order (_e.g._ random-reshuffling and shuffle-once), we can show the assumption holds with \(=2\) and \(C=(m,n) A\). Note that the \(C\) depends on the number of samples \(m\) (\(n\)), this dependence can be reduced to \(((m,n))^{0.5}\) for sufficiently small learning rates . More recently,  proposed a herding-based algorithm which explicitly optimizes the gradient error in Assumption 4.5 and shows that we can reach \(=2\) and \(C=O(A)\).

**Bilevel Optimization**. We are ready to show the convergence of Algorithm 1. Firstly, we denote the following potential function \(_{t}_{t}=h(x_{t})+_{y}\|y_{t}-y_{x_{t}}\|^ {2}+_{u}\|u_{t}-u_{x_{t}}\|^{2}\), where \(_{y}\) and \(_{u}\) are constants that relates to the smoothness parameters of \(h(x)\). Then we have:

**Theorem 4.6**.: _Suppose Assumptions 4.1-4.4 are satisfied, and Assumption 4.5 is satisfied with some \(\) and \(C\) for the example order we use in Algorithm 1. We choose learning rates \(_{t}=\), \(_{t}=c_{1}\), \(_{t}=c_{2}\) and denote \(T=R I\) be the total number of steps. Then for any pair of values (E, k) which has \(T=E k\) and \(}\), we have:_

\[_{e=0}^{E-1}\| h(x_{ke})\|^{2}+ 2^{2}k^{-}C^{2}\]

_where \(c_{1}\), \(c_{2}\), \(_{0}\), \(\) are constants related to the smoothness parameters of \(h(x)\), \(\) is the initial sub-optimality, \(R\) and \(I\) are defined in Algorithm 1._

To reach an \(\) stationary point, we choose \(=}\), and for \((E,k)\), we choose: \(E=_{0}}{^{2}}\) and \(k=(^{2}C^{2}}{^{2}})^{1/}\), which means we need to choose the number of epochs \(R==_{0}(4^{2}C^{2})^{1/}}{Ie^{ 2+2/}}\). Specialily, if we choose examples independently at each step, we have \(C=A\) and \(=1\), then we need \(T=O(^{-4})\) steps to reach an \(\)-stationary point. This recovers the rate of stocBiO algorithm  and SOBA . Next, if we use some permutation-based without-replacement sampling strategy with has \(C=((m,n))^{q} A\) (\(q\)) and \(=2\), then we need \(T=O(((m,n))^{q}^{-3})\). In particular, by adopting a herding-based permutation as in , we get \(q=0\), and our WiOR-BO strictly outperforms the independent sampling-based stocBiO and SOBA algorithms.

**Conditional Bilevel Optimization**. Next, we study the convergence rate of Algorithm 2. Besides Assumptions 4.3 and 4.4, we need the bias of sample hyper-gradient be bounded too:

**Assumption 4.7**.: For all examples \(_{i} D_{u}\) and states \(x^{p}\), there exists an upper bound A such that the sample gradient errors satisfy \(\| h(x;_{i})- h(x)\| A\).

We also augment Assumption 4.5 to include the case of \( h(x)\) (Seem Appendix B.1 for more details.). Then we are ready to show the convergence rate, we use the potential function \(_{t}=h(x_{t})\) and have:

**Theorem 4.8**.: _Suppose Assumptions 4.1-4.4 and 4.7 are satisfied, and Assumption 4.5 is satisfied with some \(\) and \(C\) for the example order we use in Algorithm 2. We choose learning rates \(_{t}==\), \(_{t}==\), \(_{t}==\) and denote \(T=R m\) be the total number of outer steps and \(T_{l}=S(n_{i})\) be the maximum inner steps. Then for any pair of values (E, k) which has \(T=E k\) and \(T_{l}=E_{l} k\), we have:_

\[_{e=0}^{E-1}\| h(x_{ke})\|^{2}}{kE }+C_{u}(1-)^{E_{l}}_{u}+C_{y}(1-)^{E_{l}}_{y}+(C^{{}^{}})^{2}C^{2}k^{-}\]

_where \(C_{u}\), \(C_{y}\), \(C^{{}^{}}\), \(\) are constants related to the smoothness parameters of \(h(x)\), \(_{h}\), \(_{u}\), \(_{y}\) are the initial sub-optimality, \(R\) and \(S\) are defined in Algorithm 2._

To reach an \(\) stationary point, we choose: \(E=}{^{2}},\) and \(,k=(^{}})^{2}C^{2}}{^{2}})^{1/}\) and \(E_{l}=O(log(^{-1}))\). Then the sample complexity of Algorithm 2 is \(EE_{l}k^{2}=O(C^{4/}^{-(2+4/)})\), where we omit the logarithmic term \(E_{l}\). Specially, if we choose examples independently at each step, then we have sample complexity \(O(^{-6})\) steps for an \(\)-stationary point. This recovers the rate of DL-SGD algorithm . Next, if we use some permutation-based without-replacement sampling strategy,then we have sample complexity \(O(((m,n))^{2q}^{-4})\), which can match the state-of-art algorithm RT-MLMC  if we choose an example order with \(q=0\).

**MiniMax and Compositional Optimization**. As special cases of Bilevel Optimization, we can also show that Algorithm 3, Algorithm 4 and Algorithm 5 converge under similar rate. Please see Corollary B.21 and Corollary B.20 for more details.

## 5 Applications and Numerical Experiments

In this section, we verify the effectiveness of the proposed algorithm through a synthetic invariant risk minimization task and two real world bilevel tasks: Hyper-Data Cleaning and Hyper-representation Learning. The code is written in Pytorch. Our experiments were conducted on servers equipped with 8 NVIDIA A5000 GPUs. Experiments are averaged over five independent runs, and the standard deviation is indicated by the shaded area in the plots.

### Invariant Risk-Minimization

In this section, we consider a synthetic invariant risk minimization task . More specifically, we perform logistic regression over a set of examples \(\{(c_{i},b_{i}),i[M]\}\) with \(c\) be the input and \(b\) be the target. However, we does not have access to \(c\) but a set of noisy observations \(\{c_{j,i},j n_{i}\}\) for each \(c_{i}\). To learn the coefficient \(x\) between \(c\) and \(b\), we optimize the following objective:

\[_{x^{p}}_{i=1}^{m}(1+(-b_ {i}y_{x}^{(i)})),\] \[y_{x}^{(i)}=*{arg\,min}_{y^{d }}}_{j=1}^{n_{i}}\|y-c_{j,i}x\|^{2}\]

This is a conditional bilevel optimization problem and we can solve it using our WiOR-CBO. We generate synthetic data to test the effects of different sample order (sample generation process is described in Appendix A). More specifically, we compare independent-sampling (WiR-CBO), shuffle-once (WiOR-CBO (SO)) and random-reshuffling (WiOR-CBO (RR)). As shown in Figure 1, the two without-replacement sampling methods outperforms the independent sampling one.

### Hyper-Data Cleaning

In the Hyper-Data Cleaning task, we are given noisy training dataset whose labels are corrupted by noise, meanwhile, we have a validation set whose labels are not corrupted. Our target is then using the clean validation samples to identify corrupted samples in the training set. More specifically, we learn optimal weights for training samples such that a model learned over the weighted training set performs well on the validation set. We can formulate this task as a bilevel optimization problem (Eq. (2)). A mathematical formulation of the task is included in Appendix A.

**Dataset and Baselines.** We construct datasets based on MNIST . For the training set, we randomly sample 40000 images from the original training dataset and then randomly perturb a fraction of labels of samples. For the validation set, we randomly select 5000 clean images from the original training dataset. In our experiments, we test our WiOR-BO algorithm (Algorithm 1), with the shuffle-once (WiOR-BO-SO) and random-reshuffling (WiOR-BO-RR) sampling; additionally, we also consider the following non-adaptive bilevel algorithms as baselines: reverse , stocBiO , BSA , AID-CG , MRBO , VRBO  and WiR-BO (the variant of our WiOR-BO using independent sampling, which is similar to the single loop algorithms such as SOBA , AmIGO  and FLSA ). We perform grid search for hyper-parameters and report the best results.

We summarize the results in Figure 2. In the figure, we compare the validation loss and F1 score _w.r.t._ both Hyper-iterations and running time. As shown in the figure, the two variants of our algorithm WiOR-BO-SO and WiOR-BO-RR get similar performance and they both outperform other baselines. In particular, note that WiR-BO differs with WiOR-BO-SO (RR) only over the example order, but the

Figure 1: Comparison of different sampling strategies for the Invariant Risk-Minimization task.

later converges much faster in terms of running time, this is partially due to less number of backward passes needed by without-replacement sampling per iteration.

### Hyper-Representation Learning

In this section, we consider Hyper-representation learning task. In this task, we learn a hyper-representation of the data such that a linear classifier can be learned quickly with a small number of samples. We consider the Omniglot  and MiniImageNet  data sets. This task can be viewed as a conditional bilevel optimization problem (Eq. (2)) and a mathematical formulation of the task is included in Appendix A.

**Dataset and Baselines.** The details of the datasets are included in Appendix A and we consider N-way-K-shot classification task following . In our experiments, we test our WiOR-CBO (Algorithm 2), using the shuffle-once (WiOR-BO-SO) and random-reshuffling (WiOR-BO-RR) sampling. Besides, we compare with the following baselines: DL-SGD  and RT-MLMC . Note that our WiOR-CBO can also use independent sampling, and it has similar performance as DL-SGD. We perform grid search of hyper-parameters for each method and report the best results.

We summarize the experimental results for the Omniglot dataset in Figure 3 and we defer the results for MiniImageNet to the Appendix A. As shown in the figure, our WiOR-CBO SO/RR outperforms DL-SGD and is comparable to the state-of-the-art algorithm RT-MLMC.

## 6 Conclusion

In this work, we investigated example-selection of bilevel optimization. Beyond the classical independent sampling, we assumed an example-order based on without-replacement sampling, such as random-reshuffling and shuffle-once. We proposed the WiOR-BO algorithm for bilevel optimization and show the algorithm converges to an \(\)-stationary point with rate \(O(^{-3})\). After that, we also discussed the conditional bilevel optimization problems and introduced an algorithm with convergence rate of \(O(^{-4})\). As special cases of bilevel optimization, we studied the minimax and

Figure 3: **Comparison of different algorithms for the Hyper-Representation Task over the Omniglot Dataset.** From Left to Right: 5-way-1-shot, 5-way-5-shot, 20-way-1-shot, 20-way-5-shot.

Figure 2: **Comparison of different algorithms for the Hyper-data Cleaning task.** The top two plots show validation error/F1 score vs Number of Hyper-Iterations and the bottom two plots show validation error/F1 score vs Running Time. The fraction of the noisy samples is 0.6.

compositional optimization problems. Finally, we validated the efficacy of our algorithms through one synthetic and two real-world tasks; the numerical results show the superiority of our algorithms.