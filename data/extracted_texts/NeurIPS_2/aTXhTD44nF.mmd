# USDC: A Dataset of User Stance and Dogmatism in Long Conversations

**Anonymous Author(s)**

Affiliation

Address

email

Identifying user's opinions and stances in long conversation threads on various topics can be extremely critical for enhanced personalization, market research, political campaigns, customer service, conflict resolution, targeted advertising and content moderation. Hence, training language models to automate this task is critical. However, to train such models, gathering manual annotations has multiple challenges: 1) It is time-consuming and costly; 2) Conversation threads could be very long, increasing chances of noisy annotations; and 3) Interpreting instances where a user changes their opinion within a conversation is difficult because often such transitions are subtle and not expressed explicitly. Inspired by the recent success of large language models (LLMs) for complex natural language processing (NLP) tasks, we leverage Mistral Large and GPT-4 to automate the human annotation process on the following two tasks while also providing reasoning: i) User Stance classification, which involves labeling a user's Stance of a post in a conversation on a five-point scale; ii) User Dogmatism classification, which deals with labeling a user's overall opinion in the conversation on a four-point scale. The majority voting on zero-shot, one-shot, and few-shot annotations from these two LLMs on 764 multi-user Reddit conversations helps us curate the USDC dataset. USDC is then used to finetune and instruction-tune multiple deployable small

Figure 1: Sample Reddit conversation on “Capitalism vs. Socialism” with Stance (for every comment \(\{c_{i}\}_{i=1}^{6}\)) and Dogmatism (for every author \(\{a_{j}\}_{j=1}^{3}\)) labels from Mistral Large and GPT-4. The submission content favors to socialism and examines how the authors position their opinions regarding socialism vs. capitalism.

language models for the 5-class stance and 4-class dogmatism classification tasks. We make the code and dataset publicly available 1.

## 1 Introduction

Understanding the user's (or author's) opinion in a conversation is a fundamental aspect of successful interpersonal interactions, and it is essential for developing better interpersonal communication skills, empathy development, and informed decision-making. This user understanding is particularly relevant in the context of dogmatism, a phenomenon observed in various areas such as politics, religion, culture, intellect, and science, where rigid adherence to beliefs often hinders open-mindedness and empathy (Rokeach, 1954). Advertisers can target their campaigns more effectively by aligning with the opinions and stances of potential customers. Companies can use this information for market research to tailor products and services to meet consumer needs and preferences. Political groups can gauge public reaction to policies and campaigns and adjust their strategies accordingly. Identifying differing opinions can help conflict resolution by understanding the perspectives of all parties' perspectives. Society can promote tolerance and maintain social harmony by recognizing and respecting diverse opinions.

Fig. 1 shows a sample Reddit conversation on the topic of _Capitalism vs. Socialism_. We refer to an author's initial post (containing title and body) as a submission. Multiple authors can then share their opinions as comments on the submission. Specifically this example contains 6 comments \(\{c_{i}\}_{i=1}^{6}\) from 3 authors \(\{a_{j}\}_{j=1}^{3}\). We also show stance and dogmatism predictions from two large language models (LLMs): Mistral Large and GPT-4. Some authors like \(a_{1}\) change their views during the discussion based on the beliefs or opinions of others. At the beginning of the dialogue, we note that author \(a_{1}\) is somewhat favoring socialism (in submission and \(c_{2}\)). But the author shifts their stance to somewhat favors capitalism (in \(c_{4}\)) after considering the viewpoints of author \(a_{2}\) in comments \(c_{1}\) and \(c_{3}\), illustrating author \(a_{1}\)'s firm yet open-minded approach. On the other hand, author \(a_{3}\) seems very flexible based on their comment \(c_{5}\). Understanding conversations requires understanding the fine-grained topics being discussed and the dynamic viewpoints of the individual users.

Given the importance of understanding these user dynamics in conversations, training language models to perform this task automatically at scale is critical. While numerous datasets are available for analyzing individual user posts (Fast & Horvitz, 2016; Sakketou et al., 2022), typically through random subsampling of posts or selecting posts with a limited number of tokens, the exploration of a specific user's opinion across each post within an entire conversational thread remains under-explored.

Crowdsourcing is one possible approach to address the need for a suitable dataset. However, a significant limitation in manually annotating datasets for user opinions is the time-consuming nature of the process, as annotators must read entire conversations to label each user's post, making data acquisition costly. Additionally, manual annotation often faces challenges related to quality, as accurately labeling opinions requires understanding demographic details and domain-specific knowledge. Given these limitations, achieving a comprehensive and accurate set of user opinions corresponding to posts about a topic often requires multiple annotators or iterative rounds of annotation. Since users could change their opinion (often times with subtle transitions and not with explicit statements) within a conversation, tracking such changes across multiple users manually becomes very cumbersome.

Recently, large language models (LLMs), especially those built on Transformer architectures (Vaswani et al., 2017) and pretrained on large datasets, have resulted in state-of-the-art accuracies on several complex natural language processing (NLP) tasks (Brown et al., 2020; Chung et al., 2024). LLMs are also being frequently used for dialog response generation (Zhang et al., 2020; Bao et al., 2019; Roller et al., 2021; Adiwardana et al., 2020). Given the complex and cumbersome nature of conversation understanding, we hypothesize that LLMs can be effective in capturing nuances involved in understanding user opinions and their shifts in multi-user conversational contexts. Also, since these models possess long-range memory capabilities, we believe that they can reason over extended conversational threads involving numerous participants, as good as human annotators, if not better.

In this work, we leverage LLMs like Mistral Large and GPT-4 to perform two tasks: i) User Stance classification, which involves labeling a user's stance of a post in a conversation on a five-pointscale; ii) User Dogmatism classification, which deals with labeling a user's overall opinion in the conversation on a four-point scale. Besides the class labels, we also obtain reasoning behind these labels from these LLMs. We experiment with these two models as human-like annotators to generate user opinions in full-length, multi-user Reddit conversations in a zero-shot, one-shot as well as few-shot setup. Thus, overall for every sample, we obtain six annotations ({Mistral Large, GPT-4} \(\) {zero-shot, one-shot, few-shot}). Fig. 2 presents our LLM-based annotation pipeline for user-level Stance and Dogmatism tasks. We consider majority voting over these six as our final annotations. Overall, this helps us curate our USDC (a dataset of user stance and dogmatism in conversations) dataset, which consists of 764 multi-user conversations from 22 subreddits, including 1,528 user-level dogmatism samples and 9,618 stance samples across all posts from selected users. Overall, the annotations in the dataset highlight specific user opinions in each post related to stance, track opinion fluctuations leading to a dogmatic nature, and provide reasoning about why users hold specific opinions.

USDC addresses several weaknesses of existing post-level stance and dogmatism datasets. First, the full-length multi-user conversation aspect of USDC enables it to capture contextual and opinion shifts of multiple users. This feature allows it to serve as both an instruction-tuning user opinion dataset and an evaluation benchmark. We believe that the ability to perform instruction tuning for user opinions at a large scale can bridge the gap between open-source and commercial user trait understanding models. Additionally, the in-context learning annotations using state-of-the-art LLMs in USDC make it a more comprehensive measure of how current LLMs understand complex tasks like capturing opinions. This aspect makes it a valuable resource, especially for social media agents seeking deeper insights into user behavior.

In this work, we utilize our USDC dataset to finetune as well as instruction-tune open-source LLMs for generating stance and dogmatism labels for users. We experiment with three pretrained small language models (SLMs) like LLaMA-2-7B, LLaMA-3-8B, and Falcon-7B. We also experiment with four instruction-tuned SLMs like LLaMA-2-chat-7B, LLaMA-3-8B-instruct, Vicuna-7B-v.1.5, and Falcon-7B-instruct. We report weighted F1 scores obtained using these models for both the tasks.

We make the following contributions: 1) We contribute USDC (a dataset of user stance and dogmatism in conversations) dataset consisting of 764 multi-user conversations labeled with 1,528 user-level dogmatism samples and 9,618 stance samples. 2) We report initial results for the stance and dogmatism detection tasks using seven small language models for the UDSC dataset. We find that stance detection performance improves with instruction-tuning (F1-score of 56.2) compared to finetuning (F1-score of 54.9). However, dogmatism detection performs worse with instruction-tuning (F1-score of 49.2) compared to fine-tuning (F1-score of 51.4), highlighting the complexity of this task. 3) We make the code and dataset publicly available1. Also, the finetuned and instruction-tuned models are made available as well.

Figure 2: Generating annotations using LLMs: We pass the entire conversation for each Reddit thread in JSON format. The JSON highlights the top two authors who posted the most comments, alongside annotation guidelines for stance and dogmatism labels in the system prompt.

Related Work

**Opinion fluctuations in user conversations.** Our work is closely related to previous studies (Fast and Horvitz, 2016; Sakketou et al., 2022), which explore Stance and Dogmatism at the post level, where posts are randomly sampled from conversation threads. Fast and Horvitz (2016) predicted user dogmatism on randomly sampled Reddit posts from conversations, with each post limited to 200-300 characters. One major limitation of this work is the unavailability of a public dataset and missing annotator demographic details. Sakketou et al. (2022) created the post-level Stance dataset, SPINOS, where each post is considered independent, and submission posts are missing while annotators label the data. Additionally, the quality of the dataset is not validated due to missing demographic details of these annotators. Our work overcomes the limitations of previous studies and presents Stance detection for posts and Dogmatism labels of users in conversations, considering the entire context, while preserving submission IDs. Hence, our dataset provides clear user-level posts and dogmatism data, which are useful for modeling dynamic user representations.

**Generating annotations for NLP tasks using Large Language Models** Our work also relates to a growing body of literature suggesting that large language models can perform similarly to human annotators in labeling complex NLP tasks (Zhou et al., 2022; Zhang et al., 2023; Bansal and Sharma, 2023; Lowmanstone et al., 2023; Wadhwa et al., 2023; Honovich et al., 2023; Zheng et al., 2024; Ye et al., 2022a; Meng et al., 2022). Several studies have explored LLM-based annotation generation in zero-shot or few-shot task settings (Ye et al., 2022a; Meng et al., 2022; Ye et al., 2022b), while others have compared pairs of language models to assess the quality of annotations generated by these LLMs (Zheng et al., 2024). However, these studies focused on generating annotations for NLP tasks such as sentiment analysis, natural language inference (Gilardi et al., 2023; Alizadeh et al., 2023), or creating synthetic dialogues, but only for dyadic conversations (Lee et al., 2023). Our approach complements these previous studies by focusing on generating annotations of user opinions in complex multi-user conversations.

## 3 USDC Dataset Curation

In this section, we will discuss three main things: 1) Collection of Reddit conversations, 2) Obtaining LLM annotations, and 3) Inter-annotator agreement with LLMs as annotators.

### Collection of Reddit Conversation Threads

**Initial crawl.** We crawl an year (2022) worth of multi-user conversation data from 22 subreddits of Reddit 2 using praw API 3. This dataset includes submissions and all associated user comments. Each submission, which serves as the initial message of the conversation, contains a title and content body. This is followed by comments and replies to the submission or other comments. Overall, we crawled 3,619 Reddit conversations across the 22 subreddits. A sample Reddit conversation is displayed in Fig. 1.

**Quality filtering of conversations.** Since submission content on Reddit can sometimes include videos, we perform the following filtering steps. 1) We only consider submissions where the content is text. 2) We remove conversations with [deleted] tags and empty content. 3) We exclude conversations where the posts were either discarded by users or removed by moderators.

Reddit user conversations can be very long and we observed up to 591 comments in a single crawled conversation data. Considering the maximum sequence length allowed by various language models, we retained only those conversations that contain at least 20 and at most 70 comments. Considering conversations with fewer than 20 comments results in too few comments to accurately gauge user opinions based on small samples. Further, we ensure that at least two users covering \(\)50% of the comments in the conversations. We did not remove any comments or reduce the post length in the selected conversations. Out of the initial 3,619 conversations, these filtering steps result into 764 conversations getting selected. Table. 4 in the Appendix shows detailed subreddit level statistics.

### Obtaining LLM Annotations

**Representing Reddit conversations in JSON format.**

To create the prompt, we follow the nested hierarchical structure of Reddit conversations to maintain the context. Specifically, we maintain a JSON structure for each conversation, where each author has their post IDs, and comments or replies are available in the body section. An example of a Reddit conversation in JSON format is provided in Appendix D. Note that the JSON explicitly includes the top-2 authors who posted the most comments in the conversation, as well as their respective post IDs. Our emphasis on these top-2 users (covering 47% posts of total posts on average) aimed at accurately assigning Stance and Dogmatism labels, acknowledging the challenge of modeling a user's opinion belief based on a very number of posts within a conversation.

**Using LLMs as human-like annotators.** To annotate the position (or Stance) of a user towards a subreddit topic at each post and the overall opinion (or Dogmatism level) of a user in a conversation, we employ two well-known commercialized API-based LLMs: GPT-4 (OpenAI, 2023) and Mistral Large (Jiang et al., 2024). OpenAI GPT-4 is a decoder-based language model which features a context window of 32k to 128k tokens. Mistral Large features a context window of 32k tokens. Additionally, we also examined other versions of these models, such as GPT-3.5 and Mistral-small and medium, but found that these models failed to produce annotations in the desired format. We briefly discuss these limitations in Section 6.

For both GPT-4 and Mistral Large, we supplied a system prompt that contains the definition of Stance and Dogmatism, guidelines for annotating each user conversation, and the necessary labels for Stance and Dogmatism, as shown in Fig 2. The system prompt is detailed in the Appendix B. Along with the system prompt, we provided a user prompt comprising the entire user conversation in a structured JSON format, as discussed above. Additionally, we prompted the model to generate reasoning for each label, explaining why the LLMs assigned a particular label to a specific user post. We used zero-shot, one-shot, and few-shot settings to get the LLM-based annotations. For the few-shot setting, we added two examples in the prompt. Samples of generated outputs using GPT-4 in zero-shot, one-shot, and few-shot settings are shown in Appendix E.1, E.2, E.3 respectively. Similarly, samples of generated outputs using Mistral Large in zero-shot, one-shot, and few-shot settings are shown in Appendix E.4, E.5, E.6 respectively.

**Annotation tasks.** We prompt the LLMs to perform two annotation tasks: 1) Stance detection, which determines if a user comment or post is _Strongly In Favor_, _Strongly Against_, _Stance Not Inferrable_, _Somewhat In Favor_, or _Somewhat Against_ towards specific subreddit submission content; 2) Dogmatism identification, which evaluates the user's overall opinion in conversation and categorizes them into one of four categories: _Firm but Open_, _Open to Dialogue_, _Flexible_ or _Deeply Rooted_. This assessment reveals whether a user is open to changing their beliefs or remains steadfast in their opinions based on interactions with other users.

**Addressing LLM response and JSON parsing failures.** Sometimes the LLMs got confused with the author IDs and missed Stance labels for some author IDs (Fig. 3 (left)). Sometimes, there were minor errors in key naming ('label' vs 'body' in Fig. 3 (right)). For each LLM setting, we observed

Figure 3: Failure cases of LLMs: Mistral Large few-shot output (left), the ids (‘f6mmzx1’,‘f6mma88”) were mismatched with generated ids (‘f9mmzx1’,‘f9mna88”), GPT-4 zero-shot output (right), the key **“label”** was mismatched with generated key “body”.

such errors in around 15 cases on average. We manually fixed such JSON parse errors and missing Stance labels for some author IDs.

**Majority voting.** After obtaining six annotations ([Mistral Large, GPT-4]\(\){zero-shot, one-shot, few-shot}) for each sample, we aggregate using majority voting to determine the final gold annotations for the Stance and Dogmatism tasks. Fig. 4 presents the class distributions for both the annotation tasks. Additionally, we present the class distributions obtained from each model with the three settings (zero-shot, one-shot and few-shot) for two tasks in Appendix Figs. 5 and 6 respectively.

### Inter-annotator Agreement with LLMs as Annotators

As the quality of labeling on subjective tasks is challenging, we validated the inter-annotator agreement (IAA) between the six LLMs (GPT-4 Zero-shot, GPT-4 One-shot, GPT-4 Few-shot, Mistral Large Zero-shot, Mistral Large One-shot, and Mistral Large Few-shot) for the Stance as well as Dogmatism tasks. We perform IAA using two approaches: i) Cohen's kappa score (Cohen, 1960) and ii) Fleiss' kappa score (Fleiss, 1971). Cohen's kappa measures the agreement between two raters, while Fleiss' kappa extends this to multiple raters. Hence, we employed Cohen's kappa for pairwise comparisons and Fleiss' kappa for overall agreement across all models.

Fig. 7 in Appendix shows the pairwise Cohen's kappa values for both Stance and Dogmatism tasks. We observe that Cohen's kappa values range from 0.36 to 0.72 for Stance and 0.31 to 0.61 for dogmatism, indicating moderate agreement between the models. Broadly kappa values are higher for model pairs within a family (GPT-4 or Mistral large). Thus, the large variance in the kappa scores is not due to the various in-context learning settings (ZS, OS, FS) but rather due to architectural differences.

The overall Fleiss' kappa value was calculated as 0.485 for Stance and 0.435 for Dogmatism, suggesting moderate agreement among all six models. Comparing LLM IAA with previous studies, we observe that for dogmatism, the LLM IAA of 0.435 matches with 0.44 as mentioned in Fast & Horvitz (2016). Similarly, for Stance, the LLM IAA of 0.485 is much higher than 0.34 as reported in Sakketou et al. (2022). It is important to note that previous studies on Stance and Dogmatism datasets were created on post-level data with limited token lengths, whereas our work focuses on entire user conversations. This suggests that LLMs can be considered as competent annotators for complex subjective tasks. However, the moderate agreement levels indicate potential areas for improvement and align with the observed performance variations among the models.

## 4 Training Small Language Models

In this section, we briefly discuss the small language models that we experiment with. We also discuss their finetuning and instruction tuning details.

Figure 4: Distribution of class labels for Stance (left) and Dogmatism (right) tasks. These class labels are determined by majority voting across GPT-4 and Mistral Large models.

### Small Language Models

we train three pretrained small language models (LLaMA-2-7B, LLaMA-3-8B, Falcon-7B) and four instruction-tuned small language models (LLaMA-2-chat-7B, LLaMA-3-8B-instruct, Vicuna-7B-v.1.5, and Falcon-7B-instruct). We finetune as well as instruction tune these models using the proposed USDC dataset. We use pretrained models checkpoints from Hugging Face. All of these LLMs have context length of 4096 tokens.

**LLaMA** models (Touvron et al., 2023a) are decoder-only LLMs trained on 1.6 trillion tokens from a mixture of corpora including C4, English CommonCrawl, Wikipedia, Github, and more. We use two versions of models in our study: LLaMa-2-7B (Touvron et al., 2023b) and LLaMa-3-8B and their instruction tuned variants.

**Falcon** models (Almazrouei et al., 2023) are decoder-only LLMs trained on \(\) 1 trillion tokens of text, with a particular emphasis on the RefinedWeb corpus. For Falcon, we use both the pretrained and instruction tuned 7B parameter variants in our study.

**Vicuna** model (Chiang et al., 2023) is finetuned from the LLaMA 7B model on approximately 70K user-shared conversations gathered from ShareGPT.com and we used the 7B parameter variants.

### Experimental Setup

**Train-test setup.** We conducted both finetuning and instruction-tuning of small language models. For this purpose, we divided the dataset of 764 conversations into train (\(\) 75%) and test splits (\(\) 25%). The training dataset comprised 564 conversations, including 1128 samples of Dogmatism labels and 7520 samples of Stance labels. Conversely, the testing dataset consisted of 200 conversations, with 400 samples of Dogmatism labels and 1831 samples of Stance labels across two authors posts.

**Implementation details for reproducibility.** All experiments were conducted on a machine equipped with an NVIDIA A100 GPU with 80 GB of GPU RAM, partitioned into two devices of 40 GB each. We employed 4-bit quantization with normalized floating precision (nf4) from the bitsandbytes library 4. Additionally, we utilized LoRA (Hu et al., 2021) with a rank of 64 and an alpha value of 16 during task-based instruction tuning. Finally, we use PEFT (Parameter Efficient Finetuning) 5 library to train large language models with SFTT (Supervised Finetuning Trainer) setting. To further enhance performance, we divided the training dataset into a validation set, comprising a randomly chosen 10% subset from the training set, used exclusively for hyperparameter tuning. More details about bitsandbytes, PEFT and SFTT parameters are reported in Appendix.

### Finetuning and Instruction Tuning of Small Language Models (SLMs)

**Finetuning of SLMs.** For Stance classification, we treat each user post as an independent sample. In contrast, for Dogmatism classification, we consider the entire user conversation as a single sample by concatenating all the threads from a user in that conversation. To load the pretrained SLMs, we perform 4-bit quantization, apply the LoRA technique (Hu et al., 2021), and fine-tune the models with SFTT before saving the fine-tuned model. For finetuning, we used prompt for Stance classification as shown in Fig. 8 (see Appendix). Similarly, Fig. 9 (see Appendix) displays prompt for Dogmatism identification.

**Instruction tuning of SLMs.** We instruction tune the SLMs on user conversations along with their gold labels from the training part of the USDC dataset. For instruction tuning, we use the same prompt as used for LLMs to generate the USDC dataset (also shown in Appendix B). Similar to finetuning, we use same train-test splits for instruction tuning.

## 5 Results

**Do SLMs finetuned with task-specific LLM annotations accurately perform Stance and Dogmatism tasks on user opinions?** We show the weighted F1 of various SLMs finetuned with task-specific LLM annotations on the stance and dogmatism detection tasks on the USDC test set in Table 1. Wereport AUC scores and other qualitative analysis in Appendix F (Fig. 11 and 12). We make the following observations from these results: 1) For both tasks, the majority voting labels as ground truth, has a relatively high performance, scoring above 50% weighted F1-score across several models. 2) LLaMa-3 models (LLaMA-3-8B and LLaMA-3-8B-instruct) perform better across both the tasks. 3) For GPT-4 annotations, in most cases, SLMs finetuned with few-shot annotations outperform those trained with zero and one-shot annotations. For Mistral Large annotations, typically SLMs finetuned with one-shot annotations performs the best. 4) Specifically, for Stance detection task, Vicuna-7B-v.1.5 finetuned using few-shot annotations is the best model trained with GPT-4 annotations. Similarly, LLaMA-3-8B-instruct finetuned with one-shot annotations is the best model trained with Mistral Large annotations. 5) For the Dogmatism detection task, LLaMA-3-8B-instruct finetuned using few-shot annotations is the best model trained with GPT-4 annotations. Similarly, LLaMA-2-chat-7B finetuned with one-shot annotations is the best model trained with Mistral Large annotations. 6) Overall, we observe that instruction tuned SLMs perform better than the pretrained SLMs.

**Do SLMs instruction-tuned with task-specific LLM annotations perform better than SLMs finetuned with task-specific LLM annotations for the Stance and Dogmatism tasks?** We show the weighted F1 of various SLMs instruction-tuned with task-specific LLM annotations, on the stance and dogmatism detection tasks on the USDC test set in Table 2. We report AUC scores and other qualitative analysis in Appendix F (see Fig. 13). We make the following observations from these results: 1) SLMs with instruction-tuning result in higher weighted F1-scores than SLMs with finetuning for stance detection, while SLMs with finetuning outperform SLMs with instruction-tuning in dogmatism detection. 2) Contrary to results in Table 1, Table 2 demonstrates that using majority voting labels as ground truth, SLM instruction-tuning yields relatively high performance only for the stance detection task, but not for the dogmatism detection. 3) Similar to results in Table 1, LLaMA-3 models (LLaMA-3-8B and LLaMA-3-8B-instruct) perform better across both tasks. Additionally, GPT-4 annotations yield the best results in the few-shot setting, while Mistral Large annotations perform best in the one-shot setting.

Overall, we draw the following conclusions when comparing SLM finetuning and instruction-tuning: (1) Since dogmatism detection is inherently a more complex and varied task than stance detection, the model might struggle to generalize from the instructional data. (2) The system prompt used in finetuning is much simpler than the original system prompt for instruction-tuning, making it challenging to handle the context length for longer conversations. We perform an error analysis to further analyze the results in the next subsection.

**Error Analysis** Table 3 illustrates the confusion matrix for stance detection for LLaMa-3-8B finetuning and instruction-tuning. We make the following observations this table: 1) For both finetuning and instruction-tuning, there is a significant misclassification between "Somewhat Against" and "Somewhat In Favor," as well as between "Somewhat In Favor" and "Stance Not Inferrable." These overlaps suggest challenges in distinguishing moderate stances, indicating a need for enhanced

   &  &  \\   &  &  &  &  \\   & **2S** & **0S** & **1S** & **2S** & **0S** & **2S** & **0S** & **1S** & **2S** & **0S** & **1Sfeature representation and clearer class definitions to improve model performance. We report the confusion matrix for dogmatism detection task in Appendix Fig. 10. Fig. 10 shows significant misclassifications, especially for the "Deeply Rooted" and "Flexible" labels, with both having zero accuracy and F1-scores. On the other hand, the model performs moderately better for "Firm but Open" and "Open to Dialogue" classes with accuracies of 48.7% and 64.4% respectively. The confusion matrix also indicates substantial confusion to distinguish between intermediate levels of dogmatism, such as "Firm but Open" and "Open to Dialogue". The area under the ROC curve (AUC) is a measure of the model's ability to distinguish between classes. Hence, we further report the ROC curve which shows the trade-off between the true positive rate (TPR) and false positive rate (FPR) for each class for stance and dogmatism tasks, see Figs. 11 and 12 in Appendix F.

**Verification using Human Interaction.** Due to the time-consuming nature of the manual annotation process, we perform human annotations on the set of 200 test conversations. In the forms for human annotations, we displayed the top 2 author's Reddit posts from the conversation, along with the submission title and content. We also provided a link to the original Reddit URL for annotators to look at the full conversation. We provided detailed annotation guidelines (similar to the ones mentioned in the prompt in Appendix B) to instruct human annotators in carrying out these tasks. Here is a sample Google form6. With three human annotators on a sample of 10 conversations, the agreement of majority labels (i.e., USDC test set labels) with human labels is 0.56 for the stance detection task and 0.45 for the dogmatism task. The annotators included two males and one female, affiliated with both academia and industry, aged between 20 and 40, and were very familiar with Reddit topics.

## 6 Conclusion

In this paper, we focused on the problems of 5-class stance and 4-class dogmatism classification in long conversations. Using LLMs as human-like annotators, we introduced USDC, a large-scale dataset of user stance and dogmatism in conversations. This is achieved by providing detailed annotation guidelines in the system prompt and full-length conversation as user prompt. Commercialized API-based LLMs generate author-level stance and dogmatism labels via zero, one and few-shot settings. The full-length multi-user conversation aspect of USDC allows it to capture the contextual and opinion shifts of multiple users in a conversation. We believe that the ability to perform finetuning or instruction tuning SLMs for user opinions at a large scale can bridge the gap between SLMs and commercial LLMs for understanding user traits. While finetuning SLMs shows F1-score on both stance and dogmatism tasks, the F1-score remains below 60% (54.9% for Stance and 51.4% for Dogmatism). On the other hand, instruction tuning of SLMs only improves F1-score performance on stance, not the dogmatism task. Further, the performance still falls short of 60%, with weighted F1-scores of 56.2% for stance and 49.2% for dogmatism. These findings indicate that there is still significant room for improvement in understanding user opinions from a text segment.

**Limitations.** We plan to extend this work along the following directions in the future. 1) We performed this work on English conversations only. It would be nice to extend this to multi-lingual conversations and verify how accurately SLMs and LLMs perform on the Stance and Dogmatism tasks in the multi-lingual scenario. 2) We analyzed user dogmatism based on their posts within a single conversation. This approach could be extended to include posts across multiple conversations and utilize similar profile information if available. 3) We analyzed dogmatism information for only the top two authors. This was mainly because considering more authors increases the output generation length, and we were constrained by our budget. This implies that our current models have not been evaluated for authors who do not post frequently.

    & & &  & & \\   & SOA & SOIF & SNII & SGA & SIF \\   & SOA & 151 & 132 & 34 & 44 & 2 \\   & SOIF & 93 & **537** & 113 & 17 & 14 \\   & SNI & 23 & 78 & **259** & 5 & 0 \\   & SCA & 52 & 35 & 13 & 115 & 17 \\   & SIF & 18 & 50 & 12 & 25 & 27 \\  

Table 3: Confusion matrix for LLMa-3-8B Stance detection models on USDC test set: finetuning (left) and instruction-tuning (right). SOA: Somewhat Against, SOIF: Somewhat In Favor, SNI: Stance Not Inferrable, SGA: Strongly Against, SIF: Strongly In Favor.