# Diffusion Policies Creating a Trust Region for

Offline Reinforcement Learning

 Tianyu Chen   Zhendong Wang   Mingyuan Zhou

The University of Texas at Austin

{tianyuchen, zhendong.wang}@utexas.edu

mingyuan.zhou@mccombs.utexas.edu

###### Abstract

Offline reinforcement learning (RL) leverages pre-collected datasets to train optimal policies. Diffusion Q-Learning (DQL), introducing diffusion models as a powerful and expressive policy class, significantly boosts the performance of offline RL. However, its reliance on iterative denoising sampling to generate actions slows down both training and inference. While several recent attempts have tried to accelerate diffusion-QL, the improvement in training and/or inference speed often results in degraded performance. In this paper, we introduce a dual policy approach, Diffusion Trusted Q-Learning (DTQL), which comprises a diffusion policy for pure behavior cloning and a practical one-step policy. We bridge the two polices by a newly introduced diffusion trust region loss. The diffusion policy maintains expressiveness, while the trust region loss directs the one-step policy to explore freely and seek modes within the region defined by the diffusion policy. DTQL eliminates the need for iterative denoising sampling during both training and inference, making it remarkably computationally efficient. We evaluate its effectiveness and algorithmic characteristics against popular Kullback-Leibler divergence-based distillation methods in 2D bandit scenarios and gym tasks. We then show that DTQL could not only outperform other methods on the majority of the D4RL benchmark tasks but also demonstrate efficiency in training and inference speeds. The PyTorch implementation is available at [https://github.com/TianyuCodings/Diffusion_Trusted_Q_Learning](https://github.com/TianyuCodings/Diffusion_Trusted_Q_Learning).

## 1 Introduction

Reinforcement learning (RL) centers on developing a policy to make sequential decisions by interacting with an environment, aiming to maximize the total rewards accumulated over a trajectory (Wiering and Van Otterlo, 2012; Li, 2017). Offline RL addresses these challenges by enabling the training of an RL policy from fixed datasets of previously collected data, without further interactions with the environment (Lange et al., 2012; Fu et al., 2020). This approach leverages large-scale historical data, mitigating the risks and costs associated with live environment exploration. However, offline RL introduces its own set of challenges, primarily related to the distribution shift between the data on which the policy was trained and the data it encounters during evaluation (Fujimoto et al., 2019). Additionally, the limited expressive power of policies that may not adequately capture the multimodal nature of action behaviors is also a concern.

To mitigate distribution shifts, popular approaches include weighted regression, such as IQL (Kostrikov et al., 2021) and AWAC (Nair et al., 2020), aimed at extracting viable policies from historical data. Alternatively, behavior-regularized policy optimization techniques are employed to constrain the divergence between the learned and in-sample policies during training (Wu et al., 2019). Notable examples of this strategy include TD3-BC (Fujimoto and Gu, 2021), CQL (Kumar et al.,2020), and BEAR (Kumar et al., 2019). These methods primarily utilize either Gaussian or deterministic policies, which have faced criticism for their limited expressiveness. Recent advancements have incorporated generative models to enhance policy representation. Variational Autoencoders (VAEs) (Kingma and Welling, 2013) and Generative Adversarial Networks (GANs) (Goodfellow et al., 2020) have been introduced into the offline RL domain, leading to the development of algorithms such as BCQ (Fujimoto et al., 2019) and GAN-Joint (Yang et al., 2022). Moreover, diffusion models have recently emerged as the most prevalent tools for achieving expressive policy frameworks (Janner et al., 2022; Wang et al., 2022; Chen et al., 2023; Hansen-Estruch et al., 2023; Chen et al., 2022), demonstrating state-of-the-art performance on the D4RL benchmarks. Diffusion Q-Learning (DQL) (Wang et al., 2022) applies these policies for behavior regularization, while algorithms such as IDQL (Hansen-Estruch et al., 2023) leverage diffusion-based policies for policy extraction.

However, optimizing diffusion policies for rewards in RL is computationally expensive due to the need for iteratively denoising to generate actions during both training and inference. Recently, distillation has become a popular technique for reducing the computational costs of diffusion models, \(e.g.\), score distillation sampling (SDS) (Poole et al., 2022) and variational score distillation (VSD) (Wang et al., 2024) in 3D generation, and Diff-Instruct (Luo et al., 2024), Distribution Matching Distillation (Yin et al., 2023), and Score identity Distillation (SiD) (Zhou et al., 2024) in 2D. These advancements distill the iterative denoising process of diffusion models into a one-step generator. SRPO (Chen et al., 2023) employs SDS (Poole et al., 2022) in the offline RL field by incorporating a Kullback-Leibler (KL) divergence-based behavior regularization loss to reduce training and inference costs. Another related work, IDQL (Hansen-Estruch et al., 2023), selects action candidates from a diffusion behavior-cloning policy and requires a 5-step iterative denoising process to generate multiple candidate actions (ranging from 32 to 128) during inference, which remains computationally demanding. Unlike previous approaches, our paper introduces a diffusion trust region loss that moves away from focusing on distribution matching; instead, it emphasizes establishing a safe, in-sample behavior region. We then simultaneously train dual policies: a diffusion policy for pure behavior cloning and a one-step policy for actual deployment. The one-step policy is optimized based on two objectives: the diffusion trust region loss, which ensures safe policy exploration, and the maximization of the Q-value function, guiding the policy to generate actions in high-reward regions. We elucidate the differences between our diffusion trust region loss and KL-based behavior distillation in Section 3 empirically and theoretically. Our method consistently outperforms KL-based behavior distillation approaches. We provide more discussions on related work in Appendix A.

In summary, we propose DTQL with a diffusion trust region loss. DTQL achieves new state-of-the-art results in majority of D4RL (Fu et al., 2020) benchmark tasks and demonstrates significant improvements in training and inference time efficiency over DQL (Wang et al., 2022) and related diffusion-based methods.

## 2 Diffusion Trusted Q-Learning

Below, we first introduce the preliminaries of offline RL and basics of diffusion policies for our modeling. We then propose a new diffusion trust region loss which inherently avoids exploring out-of-distribution actions and hence enables safe and free policy exploration. Finally, we introduce our algorithm Diffusion Trusted Q-Learning (DTQL), which is efficient and well-performed.

### Preliminaries

In RL, the environment is typically defined within the context of a Markov Decision Process (MDP). An MDP is characterized by the tuple \(M=\{S,,p_{0}(),p(^{}|,),r(, {a}),\}\), where \(S\) denotes the state space, \(\) represents the action space, \(p_{0}()\) is the initial state distribution, \(p(^{}|,)\) is the transition kernel, \(r(,)\) is the reward function, and \(\) is the discount factor. The objective is to learn a policy \(_{}(|)\), parameterized by \(\), that maximizes the cumulative discounted reward \([_{t=0}^{}^{t}r(_{t},_{t})]\). In the offline setting, instead of interacting with the environment, the agent relies solely on a static dataset \(=\{,,r,^{}\}\) collected by a behavior policy \(_{}(|)\). This dataset is the only source of information for the agents.

### Diffusion Policy

Diffusion models are powerful generative tools that operate by defining a forward diffusion process to gradually perturb a data distribution into a noise distribution. This model is then employed to reverse the diffusion process, generating data samples from pure noise. While training diffusion models is computationally inexpensive, inference is often costly due to the need for iterative refinement-based sequential denoising. In this paper, we only train a diffusion model and avoid using it for inference, thus significantly reducing both training and inference times.

The forward process involves initially sampling \(_{0}\) from an unknown data distribution \(p(_{0})\), followed by the addition of Gaussian noise to \(_{0}\), denoted by \(_{t}\). The transition kernel \(q_{t}(_{t}|_{0})\) is given by \(_{t}=_{t}_{0}+_{t}\), where \(_{t}\) and \(_{t}\) are predefined, and \(\) represents random Gaussian noise.

The objective function of the diffusion model aims to train a predictor for denoising noisy samples back to clean samples, represented by the optimization problem:

\[_{}_{t,_{0},(0,)}[ w(t)\|_{}(_{t},t)-_{0}\|_{2}^{2}] \]

where \(w(t)\) is a weighted function dependent only on \(t\). In offline RL, since our training data is state-action pairs, we train a diffusion policy using a conditional diffusion model as follows:

\[()=_{t,(0,),( _{0},)}[w(t)\|_{}(_{t},t|)-_{0}\|_{2}^{2}] \]

where \(_{0},\) are the action and state samples from offline datasets \(\), and \(_{t}=_{t}_{0}+_{t}\). Following previous work [Chen et al., 2023, Hansen-Estruch et al., 2023, Wang et al., 2022a], \((_{t},t|)\) can be considered an effective behavior-cloning policy.

The ELBO ObjectiveThe diffusion denoising loss is intrinsically connected with the evidence lower bound (ELBO). It has been demonstrated in prior studies [Ho et al., 2020, Song et al., 2021, Kingma et al., 2021, Kingma and Gao, 2024] that the ELBO for continuous-time diffusion models can be simplified to the following expression (adopted in our setting):

\[ p(_{0}|)(_{0}|)=- _{t(0,1),(0,)} [w(t)\|_{}(_{t},t|)-_{0}\|_{2}^{2}]+c, \]

where \(_{t}=_{t}_{0}+_{t}\), \(w(t)=-(t)}{t}\), and the signal-to-noise ratio \((t)=^{2}}{_{t}^{2}},c\) is a constant not relevant to \(\). Since we always assume that the \((t)\) is strictly monotonically decreasing in \(t\), thus \(w(t)>0\). The validity of the ELBO is maintained regardless of the schedule of \(_{t}\) and \(_{t}\).

Kingma and Gao  generalized this theorem stating that if the weighting function \(w(t)=-v(t)(t)}{t}\), where \(v(t)\) is monotonic increasing function of \(t\), then this weighted diffusion denoising loss is equivalent to the ELBO as defined in Equation 3. The details of how to train the diffusion policy, including the weight and noise schedules, will be discussed in Section 4.3.

### Diffusion Trust Region Loss

We found that optimizing diffusion denoising loss from the data perspective with a fixed diffusion model can intrinsically discourage out-of-distribution sampling and lead to mode seeking. For any given \(\) and a fixed diffusion model \(_{}\), the loss is to find the optimal generation function \(_{}(|)\) that can minimize the diffusion-based trust region (TR) loss:

\[_{}()=_{t, (0,),,_{}_{}(| )}[w(t)\|_{}(_{t}_{}+_{t},t| {s})-_{}\|_{2}^{2}], \]

where \(_{}(|)\) is a one-step generation policy, such as a Gaussian policy.

**Theorem 1**.: _If policy \(_{}\) satisfies the ELBO condition of Equation 3, then the Diffusion Trust Region Loss aims to maximize the lower bound of the distribution mode \(_{_{0}} p(_{0}|)\) for any given \(\)._

Proof.: For any given state \(\)

\[_{_{0}} p(_{0}|) _{}_{_{}_{}( |)}[ p(_{}|)]_{}_{_{}_{}(|)}[(_{ }|)]\] \[=_{}_{t(0,1),(0,),_{}_{}( |)}[w(t)\|_{}(_{t}_{}+_{t},t|)-_{}\|_{2}^{2}]+c\]

Then, during training, we consider all states \(\) in \(\). Thus, by taking the expectation over \(\) on both sides and setting \(t(0,1)\), we derive the loss described in Equation 4.

By definition of the mode of a probability distribution, we know minimizing the loss given by Equation 4 aims to maximize the lower bound of the mode of a probability. Unlike other diffusion models that generate various modalities by optimizing \(\) to learn the data distribution, our method specifically aims to generate actions (data) that reside in the high-density region of the data manifold specified by \(_{}\) through optimizing \(\). Thus, the loss effectively creates a trust region defined by the diffusion-based behavior-cloning policy, within which the one-step policy \(_{}\) can move freely. If the generated action deviates significantly from this trust region, it will be heavily penalized.

**Remark 1**.: _For any given \(\), assuming that our training set consists of a finite number of samples \(\{^{1}_{0},,^{n}_{0}\}\), this implies that \(p(|)\) is represented by a mixture of Dirac delta distributions:_

\[p(|)=_{i=1}^{n}(-^{i}_{0})\]

_This indicates that all actions \(^{i}_{0}\) appearing in the training set have a uniform probability mass. Therefore, the generated action \(_{}\) can be any one of the actions in \(\{^{1}_{0},,^{n}_{0}\}\) to minimize \(_{}()\) in Equation 4, since all of them are modes of the data distribution._

**Remark 2**.: _This loss is also closely connected with Diffusion-GAN (Wang et al., 2022b) and EB-GAN (Zhao et al., 2016), where the discriminator loss is considered as:_

\[D(_{}|)=\|((_{})|)- {a}_{}\|_{2}^{2}\]

_In our model, the process of adding noise, \(_{t}_{}+_{t}\), functions as an encoder, and \(_{}(|)\) acts as a decoder. Thus, this loss can also be considered as a discriminator loss, which determines whether the generated action \(_{}\) resembles the training dataset._

**Remark 3**.: _By Theorem 1, the trust region can be defined using the conditional log-likelihood. Specifically, for a given state \(\), the trust region for an action is defined as the set \(\{ p() threshold\}\), where the conditional log-likelihood is approximated by the diffusion loss. The threshold can be adjusted by tuning the hyperparameter \(\) during the optimization of the final loss (Eq. 5)._

This approach makes the generated action \(_{}\) appear similar to in-sample actions and penalizes those that differ, thereby effectuating behavior regularization. Thus, a visualization of the toy examples (Figure 1) can help better understand how this loss behaves. The generated action \(_{}\) will incur a small diffusion loss when it resembles a true in-sample action and a high diffusion loss if it deviates significantly from the true in-sample action.

### Diffusion Trusted Q-Learning

We motivate our final algorithm from DQL (Wang et al., 2022a), which utilizes a diffusion model as an expressive policy to facilitate accurate policy regularization, ensuring that exploration remains within a safe region. Q-learning is implemented by maximizing the Q-value function at actions sampled from

Figure 1: Diffusion trust region loss. The first column shows how the training behavior dataset looks. Columns 2-6 display the diffusion trust region loss on different actions with varying magnitudes of Gaussian noise. We can observe that the trust regions captured by the diffusion model at a given \(t\) are consistent with the high-density regions of the noisy data at that specific \(t\). For example, when \(t\) is small, the diffusion loss is minimal where the true action lies, and high in all other locations.

the diffusion policy. However, sampling actions from diffusion models can be time-consuming, and computing gradients of the Q-value function while backpropagating through all diffusion timesteps may result in a vanishing gradient problem, especially when the number of timesteps is substantial.

Building on this, we introduce a dual-policy approach, Diffusion Trusted Q-Learning (DTQL): a diffusion policy for pure behavior cloning and a one-step policy for actual deployment. We bridge the two policies through our newly introduced diffusion trust region loss, detailed in Section 2.3. The diffusion policy ensures that behavior cloning remains expressive, while the trust region loss enables the one-step policy to explore freely and seek modes within the region designated by the diffusion policy. The trust region loss is optimized efficiently through each diffusion timestep without requiring the inference of the diffusion policy. DTQL not only maintains an expressive exploration region but also facilitates efficient optimization. We further discuss the mode-seeking behavior of the diffusion trust region loss in Section 3. Next, we delve into the specifics of our algorithm.

Policy Learning.Diffusion inference is not required during training or evaluation in our algorithm; therefore, we utilize an unlimited number of timesteps and construct the diffusion policy \(_{}\) in a continuous-time setting, based on the schedule outlined in EDM (Karras et al., 2022). Further details are provided in Section 4.3. The diffusion policy \(_{}\) can be efficiently optimized by minimizing \(()\) as described in Equation (2). Furthermore, we can instantiate one typical one-step policy \(_{}(|)\) in two cases, Gaussian \(_{}(|)=(_{}(),_{}( ))\) or Implicit \(_{}=_{}(,), (0,)\). Then, we optimize \(_{}\) by minimizing the introduced diffusion trust region loss and typical Q-value function maximization, as follows.

\[_{}()=_{}()-_{,_{}_{}(|)}[Q_{ }(,_{})], \]

where \(_{}()\) serves primarily as a behavior-regularization term, and maximizing the Q-value function enables the model to preferentially sample actions associated with higher values. Here we use the double Q-learning trick (Hasselt, 2010) where \(Q_{}(,_{})=(Q_{_{1}}(,_{}),Q_{ _{2}}(,_{}))\). If a Gaussian policy is used, an additional negative log likelihood (NLL) term, \(-_{,}[_{}(|)]\), should be introduced to preserve the policy's entropy and encourage exploration during training. This aspect is particularly crucial for diverse and sparse reward RL tasks. The empirical results of the NLL term will be discussed in Section 4.4.

Q-Learning.We utilize Implicit Q-Learning (IQL) to train a Q function by maintaining two Q-functions \((Q_{_{1}},Q_{_{2}})\) and one value function \(V_{}\), following the methodology outlined in IQL (Kostrikov et al., 2021).

The loss function for the value function \(V_{}\) is defined as:

\[_{V}()=_{(,)}[L_{2}^{ }((Q_{_{1}^{}}(,),Q_{_{2}^{}}(,))-V_{}())], \]

where \(\) is a quantile in \(\), and \(L_{2}^{}(u)=|-(u<0)|u^{2}\). When \(=0.5\), \(L_{2}^{}\) simplifies to the \(L_{2}\) loss. When \(>0.5\), \(L_{}\) encourages the learning of the \(\) quantile values of \(Q\).

The loss function for updating the Q-functions, \(Q_{_{i}}\), is given by:

\[_{Q}(_{i})=_{(,,^{} )}[||r(,)+*V_{}(^{})-Q_{ _{i}}(,)||^{2}], \]

where \(\) denotes the discount factor. This setup aims to minimize the error between the predicted Q-values and the target values derived from the value function \(V_{}\) and the rewards. We summarize our algorithm in Algorithm 1.

``` Input:\(\),As we do not have access to the log densities of the fake and true conditional distributions of actions, the loss itself cannot be calculated directly. However, we are able to compute the gradients. The gradient of \( p_{}(_{}|)\) can be estimated by the diffusion model \(_{}(|)\), and the gradient of \( p_{}(_{}|)\) can also be estimated by a diffusion model trained from fake action data \(_{}\). For more details, please refer to Appendix C.

KL divergence is employed in this context with the goal of capturing **multiple modalities** of the data distribution. We evaluated this loss function using a 2D toy task to gain a deeper understanding of its capability to capture multiple modalities of the dataset, as illustrated in Figure 2.

We further investigate the differences between our trust region loss, \(_{}\), and the KL-based behavior distillation loss within the context of policy improvement. As illustrated in Figure 1, \(_{}\) ensures that the generated action \(_{}\) remains within the action manifold of the in-sample dataset. Coupled with the gradient of the Q-function, this allows actions to move freely within the in-sample data manifold while gravitating toward high-reward regions, which correspond to the **single modality** present in the dataset.

Conversely, \(_{}()\) seeks to align the distribution of \(_{}(|)\) with that of \(_{}(|)\), thereby encouraging coverage of **multiple modalities**, unlike \(_{}\). Covering a wide range of modalities is particularly beneficial in image generation, where diversity among generated images is essential. However, this characteristic is less advantageous in reinforcement learning (RL) contexts, where typically a single, highest-reward action is optimal for a given state. Additionally, maximizing the Q function often results in a more deterministic policy by favoring the highest-reward paths, potentially discarding alternative actions. From this perspective, \(_{}\) demonstrates a stronger mode-seeking capability compared to \(_{}\).

To visualize how these two different behavior losses work with policy improvement, we use 2D bandit scenarios. We designed a scenario shown in Figure 3; for additional settings, please refer to Appendix F.1. In the designed 25 Gaussian setting, all four corners have the same high reward. \(_{}\) encourages the policy to randomly select one high reward mode without promoting covering all of them. In contrast, \(_{}\) tries to cover all high-density and high-reward regions and, as a byproduct, introduces artifacts that appear as data connecting these high-density regions. This could partially be due to the smoothness constraint of neural networks. The same situation occurs in a Swiss roll dataset where the high reward region is the center of the data; \(_{}\) adheres closely to the high reward region, while \(_{}\) includes some suboptimal reward regions.

In addition to testing on 2D bandit scenarios, we also evaluated the performance of two losses on the Mujoco Gym Medium task. Consistent with our previous findings, the behavior-regularization loss \(_{}()\) consistently outperformed \(_{}()\) in terms of achieving higher rewards. The results are presented in Table 5, and the training curves are depicted in Figure 8 in Appendix F.2.

Figure 2: Green points represent the datasets we trained on. Red points are generated by \(_{}\), trained using \(_{}\). This demonstrates that the KL loss encourages the generation process to cover multiple modalities of the dataset.

**Connection and Difference with SDS and SRPO**  SDS was first proposed in DreamFusion (Poole et al., 2022) for 3D generation, using the gradient of the loss form (adopted in our setting):

\[_{}_{}=_{t,,} [w(t)(_{}(_{t},t|)-)_{t}}{}] \]

where \(_{t}=_{t}_{}+_{t}\) and \(_{}\) is the noise-prediction diffusion model. This loss is utilized by SRPO (Chen et al., 2023) in offline RL.

Considering the gradient of \(_{}()\) in Equation 4, and acknowledging the equivalence between noise-prediction and data-prediction diffusion models with only a modification in the weight function \(w(t)\), we can reformulate the loss in noise-prediction form by:

\[_{}() =_{t,,}[w^{}(t)\| _{}(_{t},t|)-\|_{2}^{2}] \] \[_{}_{}() =2_{t,,}[w^{}(t)( _{}(_{t},t|)-)(_{t},t|)}{_{t}}_{t}}{}] \]

The primary distinction between the gradient of our method, as shown in Equation 11, and that of SDS/SRPO, detailed in Equation 9, lies in the inclusion of a Jacobian term, \((_{t},t|)}{_{t}}\). This Jacobian term, identified as the score gradient in SiD by Zhou et al. (2024), is notably absent from most theoretical discussions and was deliberately omitted in previous works, with DreamFusion (Poole et al., 2022) and SiD being the sole exceptions.

DreamFusion reported that the gradient depicted in Equation 11 fails to produce realistic 3D samples. Similarly, SiD observed its inadequacy in generating realistic images. These findings align with our Theorem 1, which demonstrates that this gradient primarily targets the mode and does not sufficiently account for diversity-- an essential factor in both 3D and image generation.

In high-dimensional generative models, modes often differ significantly from typical image samples, as discussed by Nalisnick et al. (2018). DreamFusion observed that the gradient from Equation 9, which is based on a KL loss, effectively promotes diversity. However, while diversity is crucial in image and 3D generation, it might be of lesser importance in offline RL. Consequently, SRPO's use of the SDS gradient, which is tailored for diverse generation, may result in suboptimal performance compared to our diffusion trust region loss. This assertion is supported by empirical results on the D4RL datasets, as discussed in Section 4.1.

## 4 Experiments

In this section, we evaluate our method using the popular D4RL benchmark (Fu et al., 2020). We further compare our training and inference efficiency against other baseline methods. Additionally, an ablation study on the negative log likelihood (NLL) term and one-step policy choice is presented. Details regarding the training of the diffusion model and its structural components are also discussed.

Figure 3: We tested the differential impact of \(_{}\) and \(_{}\) on behavior regularization, using a trained Q-function for policy improvement. Red points represent actions generated from the one-step policy \(_{}\).

HyperparametersIn D4RL benchmarks, for all Antmaze tasks, we incorporate an NLL term, while for other tasks, this term is omitted. Additionally, we adjust the parameter \(\) for different tasks. Details on hyperparameters and implementation are provided in Appendices D and E.

### D4RL Performance

In Table 1, we evaluate the D4RL performance of our method against other offline algorithms. Our selected benchmarks include conventional methods such as TD3+BC (Fujimoto and Gu, 2021) and IQL (Kostrikov et al., 2021), along with newer diffusion-based models like Diffusion QL (DQL) (Wang et al., 2022), IDQL (Hansen-Estruch et al., 2023), and SRPO (Chen et al., 2023).

In the D4RL datasets, our method (DTQL) outperformed all conventional and other diffusion-based offline RL methods, including DQL and SRPO, across all tasks. Moreover, it is 10 times more efficient in inference than DQL and IDQL; and 5 times more efficient in total training wall time compared with IDQL (see Section 4.2).

**Remark 4**.: _We would like to highlight that the SRPO method (Chen et al., 2023) reported results on Antmaze using the "-v2" version, which differs from the "-v0" version employed by prior methods such as DQL (Wang et al., 2022) and IDQL (Hansen-Estruch et al., 2023), to which it was compared. This version discrepancy, not explicitly stated in their paper, is evident upon inspection of SRPO's official codebase 1. The variation between the -v2" and -v0" datasets significantly impacts algorithm performance. To ensure a fair comparison, we utilize the "-v0" environments consistent with established baselines. We employed the official SRPO code on Antmez-v0 and maintained identical hyperparameters used for Antmaze-v2. Additionally, we conducted experiments with our

  
**Gym** & **BC** & **Onestep RL** & **TD3+BC** & **DT** & **CQL** & **IQL** & **DQL** & **IDQL** & **SRPO** & **Ours** \\  halfcheetah-medium-v2 & 42.6 & 48.4 & 48.3 & 42.6 & 44.0 & 47.4 & 51.1 & 51.0 & **60.4** & **57.9**\(\) 0.13 \\ hopper-medium-v2 & 52.9 & 59.6 & 59.3 & 67.6 & 58.5 & 66.3 & 90.5 & 65.4 & **95.5** & **99.6**\(\)0.87 \\ walker2d-medium-v2 & 75.6 & 81.8 & 83.7 & 74.0 & 72.5 & 78.3 & **87.0** & 82.5 & 84.4 & **98.4**\(\)0.13 \\ halfcheetah-medium-replay-v2 & 36.3 & 38.1 & 44.6 & 36.0 & 45.2 & 44.2 & 47.8 & 45.9 & **51.4** & **50.9**\(\)0.11 \\ hopper-medium-replay-v2 & 18.1 & 97.5 & 60.9 & 82.7 & 95.0 & 94.7 & 101.3 & 92.1 & **101.2** & **100.0**\(\)0.13 \\ walker2d-medium-replay-v2 & 26.0 & 49.5 & 81.8 & 66.6 & 77.2 & 73.9 & **95.5** & 85.1 & 84.6 & 88.5 \(\)2.16 \\ halfcheetah-medium-exppert-v2 & 55.2 & 93.4 & 90.7 & 86.8 & 91.6 & 86.7 & **96.8** & **95.9** & **92.2** & **92.7**\(\) 0.2 \\ hopper-medium-exppert-v2 & 52.5 & 103.3 & 98.0 & 107.6 & 105.8 & 91.5 & **111.1** & **108.6** & 100.1 & **100.3**\(\) 1.49 \\ walker2d-medium-exppert-v2 & 101.9 & **113.0** & **110.1** & 107.1 & 107.1 & **109.4** & **109.6** & **110.1** & **112.7** & **114.0** & **110**\(\) 0.07 \\ 
**Gym Average** & 51.9 & 76.1 & 75.3 & 74.7 & 77.6 & 77.0 & **88.0** & 82.1 & **87.1** & **88.7** \\ 
**Antmaze** & **BC** & **Onestep RL** & **TD3+BC** & **DT** & **CQL** & **IQL** & **DQL** & **IDQL** & **SRPO** & **Ours** \\  antmaze-unze-v0 & 54.6 & 64.3 & 78.6 & 59.2 & 74.0 & 87.5 & **93.4** & **94.0** & **90.8** & **94.8**\(\)0.10 \\ antmaze-unze-dverse-v0 & 45.6 & 60.7 & 71.4 & 53.0 & **84.0** & 62.2 & 66.2 & **80.2** & **90.9** & 78.8 \(\)1.83 \\ antmaze-medium-play-v0 & 0.0 & 10.6 & 0.0 & 0.0 & 61.2 & 71.2 & 76.6 & **84.5** & 73.0 & 79.6 \(\) 1.8 \\ antmaze-medium-diverse-v0 & 0.0 & 3.0 & 0.2 & 0.0 & 53.7 & 70.0 & 78.6 & **84.8** & 65.2 & **82.2**\(\) 1.71 \\ antmaze-large-play-v0 & 0.0 & 0.0 & 0.0 & 0.0 & 15.8 & 39.6 & 46.4 & 63.5 & 38.8 & 52.0 \(\) 2.23 \\ antmaze-large-diverse-v0 & 0.0 & 0.0 & 0.0 & 0.0 & 14.9 & 47.5 & 56.6 & **67.9** & 33.8 & 54.0 \(\) 2.23 \\ 
**Antmaze Average** & 16.7 & 20.9 & 27.3 & 18.7 & 50.6 & 63.0 & 69.6 & **79.1** & 30.1 & 73.6 \\ 
**Adroit Tasks** & **BC** & **BCQ** & **BEAR** & **BRAC-v** & **BRAC-v** & **REM** & **CQL** & **IQL** & **DQL** & **Ours** \\  pen-human-v1 & 25.8 & 68.9 & -1.0 & 8.1 & 0.6 & 5.4 & 35.2 & **71.5** & **72.8** & 64.1\(\)2.97 \\ pen-cloned-v1 & 38.3 & 44.0 & 26.5 & 1.6 & -2.5 & -1.0 & 27.2 & 37.3 & 57.3 & **81.3**\(\)3.04 \\ 
**Adroit Average** & 32.1 & 56.5 & 12.8 & 4.9 & -1.0 & 2.2 & 31.2 & 54.4 & 65.1 & **72.7** \\ 
**Kitchen Tasks** & **BC** & **BCQ** & **BEAR** & **BRAC-v** & **AWR** & **CQL** & **IQL** & **DQL** & **Ours** \\  kitchen-complete-v0 & 33.8 & 8.1 & 0.0 & 0.0 & 0.0 & 0.0 & 43.8 & 62.5 & **84.0** & **80.8**\(\)1.06 \\ kitchen-partial-v0 & 33.8 & 18.9 & 13.1 & 0.0 & 0.0 & 15.4 & 49.8 & 46.3 & 60.5 & **74.4**\(\)0.25 \\ kitchen-mixed-v0 & 47.5 & 8.1 & 47.2 & 0.0 & 0.0 & 10.6 & 51.0 & 51.0 & **62.6**\(\)0.59 \\ 
**Kitchen Average** & 38.4 & 11.7 & 20.1 & 0.0 & 0.0 & 8.7 & 48.2 & 53.3 & **69.0** & **71.8** \\   

Table 1: The performance of Our methods and SOTA baselines on D4RL Gym, AntMaze, Adroit, and Kitchen tasks. Results for our methods correspond to the mean and standard errors of normalized scores over 50 random rollouts (5 independently trained models and 10 trajectories per model) for Gym tasks, which generally exhibit low variance in performance, and over 500 random rollouts (5 independently trained models and 100 trajectories per model) for the other tasks. Our method outperforms all prior methods by a clear margin on most of domains. The normalized scores is recorded by the end of training phase. Numbers within 5 % of the maximum in every individual task are highlighted.

algorithm on the Antmaze-v2 environment using the same hyperparameters as in the Antmaze-v0 setup but extended the training epochs, as detailed in Table 6 in Appendix F._

### Computational Efficiency

We further examine the training and inference performance relative to other diffusion-based offline RL methods. An overview of this performance, using _antmaze-umaze-v0_ as a benchmark, is presented in Table 2. Our method requires less training time per epoch than DQL and SRPO, yet more than IDQL. However, while IDQL necessitates 3000 epochs, DTQL operates efficiently with only 500 epochs, considerably reducing the overall training duration.

As depicted in Figure 4, the extended training time per epoch for our method results from the requirement to train an additional one-step policy, a step not needed by IDQL. Although SRPO also incorporates a one-step policy, our method achieves greater efficiency in training the diffusion policy. Unlike SRPO, which requires several ResNet blocks for effective performance, our approach utilizes only a 4-layer MLP, further curtailing the training time. Additional details on total training wall time are provided in Appendix F.4.

For inference time, our method performs comparably to SRPO, as both utilize a one-step policy. The slightly higher inference time results from our use of a stochastic policy, which requires resampling after each forward pass of the neural network. Additionally, we employ the stochastic max Q trick, similar to that used in DQL Wang et al. (2022). However, our method achieves a tenfold increase in inference speed over DQL and IDQL, which require 5-step iterative denoising to generate actions. All experiments were performed on a server equipped with eight RTX-A5000 GPUs, each with 24GB of memory.

**Remark 5**.: _For total training time, SRPO trains 1000 epochs for the one-step policy while training 1500 epochs for the diffusion policy and Q function. DTQL requires 50 epochs of pretraining. Implement details are in Appendix D._

### Diffusion Training Schedule

For training the diffusion policy as described in Equation 2 and the diffusion trust region loss in Equation 4, we utilize the diffusion weight and noise schedule outlined in EDM (Karras et al., 2022). Although EDM does not satisfy the ELBO condition stipulated in Equation 3--a fact established in Kingma and Gao (2024)--we adopted it due to its demonstrated enhancements in perceptual generation quality, as evidenced by metrics such as the Frechet Inception Distance (FID) and Inception Score in the field of image generation. Kingma and Gao (2024) also attempted to modify the EDM weight schedule to be monotonically increasing, but this did not lead to better FID for image generation. Thus, we retain EDM as our continuous training schedule. For completeness, the details of the EDM schedule are discussed in Appendix B.

  
**antmaze-umaze-v0** & **DQL** & **IDQL** & **SRPO** & **Ours** \\  Training time (s per 1k steps) & 24.13 & 17.57 & 24.71 & 21.83 \\ Inference time (s per trajectory) & 3.03 & 3.04 & 0.22 & 0.35 \\ Training epochs & 1000 & 3000 & 1000 & 500 \\ Total training time (hours) & 6.70 & 14.64 & 9.42 & 3.33 \\   

Table 2: Training and Inference time required for different algorithms in D4RL _antmaze-umaze-v0_ tasks. Every single experiment is conducted with the same PyTorch backend and run on a single RTX-A5000 GPU.

Figure 4: Training time required for different algorithms in D4RL _antmaze-umaze-v0_ tasks. All experiments are conducted with the same PyTorch backend and the same computing hardware setup.

### Ablation Studies

One-step Policy ChoiceWe chose to use a Gaussian policy for all our experiments instead of an implicit or deterministic policy because the Gaussian policy is flexible and provides a convenient way to control entropy when needed. When there is no need to maintain entropy, the Gaussian policy quickly degenerates to a deterministic policy, where the variance approaches zero, as indicated in Figures 4(b) and 4(d).

Negative Log Likelihood TermAs mentioned in Section 2.4, we incorporate an NLL term \(-_{,}[(|)]\) into the loss function in Equation 5 to maintain exploration and policy entropy during training when using a Gaussian policy. We conducted an ablation study to assess its impact on the final rewards and the of the Gaussian policy, taking _antmaze-unmaze-v0_ and _antmaze-large-diversev0_ as examples. As observed in Figure 5, for the less complex task _antmaze-unmaze-v0_, adding the NLL term does not significantly enhance the final score but does stabilize the training process (see Figure 4(a)). However, for more complex tasks like _antmaze-large-diverse-v0_, the addition of the NLL term markedly increases the final score. We attribute this improvement to the ability of the NLL term to maintain high entropy during training, thus preserving exploration capabilities, as shown in Figures 4(b) and 4(d).

## 5 Conclusion and Limitation

In this work, we present DTQL, which comprises a diffusion policy for pure behavior cloning and a practical one-step policy. The diffusion policy maintains expressiveness, while the diffusion trust region loss introduced in this paper directs the one-step policy to explore freely and seek modes within the safe region defined by the diffusion policy. This training pipeline eliminates the need for iterative denoising sampling during both training and inference, making it remarkably computationally efficient. Moreover, DTQL achieves state-of-the-art performance across the majority of tasks in the D4RL benchmark. Some limitations of DTQL include the potential for improvement in its benchmark performance. Additionally, some design aspects of the one-step policy could benefit from further investigation. Currently, our experiments are primarily conducted in an offline setting. It would be interesting to explore how this method can be extended to an online setting or adapted to handle more complex inputs, such as images. Additionally, rather than focusing solely on point estimation of rewards, it would be beneficial to estimate the distribution of rewards, as recommended by previous studies in distributional reinforcement learning (Bellemare et al., 2017; Barth-Maron et al., 2018; Yue et al., 2020).