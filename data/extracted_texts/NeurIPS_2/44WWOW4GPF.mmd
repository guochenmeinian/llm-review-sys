# Learning symmetries via weight-sharing with doubly stochastic tensors

Putri A. van der Linden\({}^{1,}\)1  Alejandro Garcia-Castellanos\({}^{1}\)   Sharvaree Vadgama\({}^{1}\)

**Thijs P. Kuipers\({}^{2,3}\)   Erik J. Bekkers\({}^{1}\)**

\({}^{1}\)Amsterdam Machine Learning Lab, University of Amsterdam

\({}^{2}\)Department of Biomedical Engineering and Physics, Amsterdam UMC, the Netherlands

\({}^{3}\)Department of Radiology and Nuclear Medicine, Amsterdam UMC, the Netherlands

###### Abstract

Group equivariance has emerged as a valuable inductive bias in deep learning, enhancing generalization, data efficiency, and robustness. Classically, group equivariant methods require the groups of interest to be known beforehand, which may not be realistic for real-world data. Additionally, baking in fixed group equivariance may impose overly restrictive constraints on model architecture. This highlights the need for methods that can dynamically discover and apply symmetries as soft constraints. For neural network architectures, equivariance is commonly achieved through group transformations of a canonical weight tensor, resulting in weight sharing over a given group \(G\). In this work, we propose to _learn_ such a weight-sharing scheme by defining a collection of learnable doubly stochastic matrices that act as soft permutation matrices on canonical weight tensors, which can take regular group representations as a special case. This yields learnable kernel transformations that are jointly optimized with downstream tasks. We show that when the dataset exhibits strong symmetries, the permutation matrices will converge to regular group representations and our weight-sharing networks effectively become regular group convolutions. Additionally, the flexibility of the method enables it to effectively pick up on partial symmetries.

## 1 Introduction

Equivariance has emerged as a beneficial inductive bias in deep learning, enhancing performance across a variety of tasks. By constraining the function space to adhere to specific symmetries, models not only generalize better but also achieve greater parameter efficiency . For instance, integrating group symmetry principles into generative models has enhanced sample generation and efficient learning of data distributions, particularly benefiting areas such as vision and molecular generation .

The most well-known and transformative models in equivariant deep learning are convolutional neural networks (CNNs) , which achieve translation equivariance by translating learnable kernels to every position in the input. This design ensures that the weights defining the kernels are shared across all translations, so that if the input is translated, the output features are correspondingly translated; in other words, _equivariance_ is achieved through _weight sharing_. In the seminal work by Cohen and Welling , this concept was extended to generalize weight-sharing under any discrete group of symmetries, resulting in the group-equivariant CNN (G-CNN). G-CNNs enable G-equivariance to a broader range of symmetries, such as rotation, reflection, and scale , thereby expanding the applicability of CNNs to more complex data transformations.

However, the impact of G-CNNs is closely tied to the presence of specific inductive biases in the data. When exact symmetries, such as E(3) group symmetries in molecular point cloud data, are known to exist, G-CNNs excel. Yet, for many types of data, including natural images and sequence data, these exact symmetries are not present, leading to overly constrained models that can suffer in performance [32; 24; 2; 31]. In scenarios with limited data and for certain critical downstream tasks, having appropriate inductive biases becomes even more crucial.

To avoid overly constraining models, symmetries must be chosen carefully to match those in the input data. This requires prior knowledge of these symmetries, which may not always be available. Furthermore, different symmetries at different scales can coexist, making manual determination of these symmetries highly impractical. To address this, multiple works have proposed partial or relaxed G-CNNs [24; 6; 2; 32]. Such models are initialized to be fully equivariant to some groups and learn from the data to partially break equivariance on a per-layer basis where necessary. However, these methods still require specifying which symmetries to include and can only achieve equivariance to subsets of these symmetries.

In this work, we tackle the challenge of specifying group symmetries upfront by introducing a general weight-sharing scheme. Our method can represent G-CNNs as a special case but is not limited to exact equivariance constraints, offering greater flexibility in handling various symmetries in the data. Inspired by the idea that group equivariance for finite groups can be achieved through weight-sharing patterns on a set of base weights , we propose learning the symmetries directly from the data on a per-layer basis, requiring no prior knowledge of the possible symmetries.

We leverage the fact that _regular_ group representations act as permutations and that the expectation of random variables defined over this set of permutations is a doubly stochastic matrix . This implies that regular partial group transformation can be approximated by a stack of doubly stochastic matrices which essentially act as (soft) permutation matrices. Consequently, we learn a set of doubly stochastic matrices through the Sinkhorn operator , resulting in weight-sharing under learnable group transformations.

We summarize our contributions as follows:

* We propose a novel weight-sharing scheme that can adapt to group actions when certain symmetry transformations are present in the data, enhancing model flexibility and performance.
* We present empirical results on image benchmarks, demonstrating the effectiveness of our approach in learning relevant weight-sharing schemes when there are clear symmetries.
* The proposed method outpaces models configured with known symmetries in environments where they are only partially present. Moreover, in the absence of predefined symmetries, it adeptly identifies effective weight-sharing patterns, matching the performance of fully flexible, non-weight-sharing models.
* We provide analyses of the learned symmetries on some controlled toy settings.

## 2 Related work

Partial or relaxed equivarianceMethods such as [24; 6; 2] learn partial equivariance by learning distributions over transformations, and thereby aim to learn partial or relaxed equivariances from data by sampling some group elements more often than others. [32; 33] relax equivariance by introducing learnable equivariance-breaking components. [11; 31] Relax equivariance constraints by parameterizing layers as (linear) combinations of fully flexible, non-constrained components and constrained equivariant components. Finally, several works model soft invariances through learning the amount of data augmentation in the data or model relevant for a given task, either through learned distributions on the group or (automatic) hyperparameter selection [6; 13; 22]. However, these methods require pre-specified sets of symmetry transformations and/or group structure to be known beforehand. In contrast, we aim to pick up the relevant symmetry transformations during training.

Symmetry discovery methodsSeveral methods have been developed for symmetry discovery by leveraging the infinitesimal generators of symmetry groups, as articulated through the Lie algebra [10; 36; 37; 21]. Works such as [10; 36; 21] focus on discovering symmetries associated with general linear actions. In contrast,  extends these concepts to encompass non-linear symmetry transformations, offering capabilities for discovering partial symmetries within the data.

[26; 20] Learn the group structure via (irreducible) group representations.  Proposed to learn the Fourier transform of finite compact commutative groups and their corresponding bispectrum by learning to separate orbits on our dataset. This approach can be extended to non-commutative finite groups leveraging advanced unitary representation theory . However, these methods are constrained to finite-dimensional groups and require specific orbit-predicting datasets. In contrast, our approach learns a relaxation of _regular group representations_--as opposed to irreducible representations. Moreover, our approach is not merely capable of learning symmetries, it subsequently utilizes them in a regular group-convolution-type architecture.

Weight-sharing methodsPrevious studies have demonstrated that equivariance to finite groups can be achieved through weight-sharing schemes applied to model parameters. Notably, the works in , , and  provide foundational insights into this approach. In , weight-sharing patterns are learned by using a matrix that operates on flattened canonical weight tensors, effectively inducing weight sharing. They additionally prove that for finite groups, there are weight-sharing matrices capable of implementing the corresponding group convolution. However, their approach requires learning these patterns through meta-learning and modeling the weight-sharing matrix as an unconstrained tensor. In contrast, our method learns weight sharing directly in conjunction with the downstream task and enforces the matrix to be doubly stochastic, thereby representing soft permutations by design.

 Presents an approach closely aligned with ours, where a weight-sharing scheme is learned that is characterized by row-stochastic entries. Their method involves both inner- and outer-loop optimization and demonstrates the ability to uncover relevant weight-sharing patterns in straightforward scenarios. However, their approach does not support joint optimization of the canonical weights and weight-sharing pattern, and they acknowledge difficulties in extending their method to higher input dimensionalities. Unlike , we enforce both row and column stochasticity. Additionally, we can optimize for the sharing pattern and weight tensors jointly, and successfully apply our approach to more interesting data domains such as image processing.

In , group actions are integrated directly into the learning process of the downstream task. This method involves learning a set of generator matrices that operate via matrix multiplication on flattened input vectors. However, this approach constrains the operators to members of finite cyclic groups, which inherently limits their ability to represent more complex group structures. Furthermore, this restriction precludes the possibility of modeling partial equivariances, reducing the flexibility and applicability of the model to more diverse or complex scenarios.

## 3 Background

We begin by revisiting group convolutional methods in the context of image processing, followed by their relation to weight-sharing schemes. We then proceed to briefly cover the Sinkhorn operator, which is the main mechanism through which we acquire weight-sharing schemes. Some familiarity with group theory is assumed, and essential concepts will be outlined in the following.

GroupsWe are interested in (symmetry) groups, which are algebraic constructs that consist of a set \(G\) and a group product--which we denote as a juxtaposition--that satisfies certain axioms, such as the existence of an _identity_ element \(e G\) such that for all \(g G\) we have \(eg=ge=g\), _closure_ such that for all \(g,h G\) we have \(gh G\), the existence of an _inverse_\(g^{-1}\) for each \(g\) such that \(g^{-1}g=e\), and _associativity_ such that for all \(g,h,i G\) we have \((gh)i=g(hi)\).

RepresentationsIn the context of geometric deep learning , it is most useful to think of groups as transformation groups, and the group structure describes how transformations relate to each other. Specifically, _group representations_\(:G GL(V)\) are concrete operators that transform elements in a vector space \(V\) in a way that adheres to the group structure (they are group homomorphisms). That is, to each group element \(g\), we can associate a linear transformation \((g) GL(V)\), with \(GL(V)\) the set of linear invertible transformations on vector space \(V\).

Group convolutionConcretely, such representations can be used to define group convolutions. Consider feature maps \(f:X^{D}\) over some domain on which a group action is defined, i.e., over a _\(G\)-space_. E.g., for images (signals over \(X=^{2}\)) we could consider the group \(G=(^{2},+)\) of translations, which acts on \(X\) via \(gx=x+y\), with \(g=(y)\) a translation by \(y^{2}\). While the group \(G\) merely defines how two transformations \(g,h G\) applied one after the other correspond to a net translation \(gh G\), a representation \(\) concretely describes how data is transformed. E.g., signals \(f:X\) can be transformed via the _left-regular representation_\([(g)f](x):=f(g^{-1}x)\), which in the case of images and the translation group is given by \([(g)f](x)=f(x-y)\).

In general, group convolution is defined as transforming a base kernel under every possible group action, and for every transformation taking the inner product with the underlying data via

\[} (k_{G}f)(g)=(g)k,f\,, \]

with \(,\) denoting the inner product. For images, which are essentially functions over the group \(G=(^{2},+)\), and taking \( k,f:=_{X}k(x)f(x)x\) the standard inner product, Eq. (1) boils down to the standard _cross-correlation_ operator2:

\[} (k f)(g) =_{G}k(g^{-1}h)f(h)h \] \[(k f)(x) =_{^{2}}k(x^{}-x)f(x^{})x^{ }\,. \]

Semi-direct product groupsWhen equivariance to larger symmetry groups is desired, e.g. in the case of \(G=SE(2)\) roto-translation equivariance for images with domain \(X=^{2}\), a _lifting convolution_ can be used to generate signals over the group \(G\). In essence it is still of the form of (2), however integration is over \(X\) instead of over \(G\):

\[} (k f)(g) =_{X}k(g^{-1}x)f(x)x \] \[(k f)(x,) =_{^{2}}k(^{-1}(x^{}-x))f(x^{ })x^{}\,, \]

with \(g=(x,)(^{2},+) SO(2)\). The roto-translation group is an instance of a semi-direct product group (denoted with \(\)) between the translation and rotation group, which has the practical benefit that a stack of rotated kernels can be precomputed , and the translation part efficiently be taken care of via optimized Conv2D operators. Namely via \((k f)(x,_{i})=[k_{i},f]\), with \(k_{i}:=k(_{i}^{-1}x)\). This trick can also be applied for full group convolutions (2).

## 4 Method

Our objective is to uncover the underlying symmetries of datasets whose exact symmetries may not be known, in a manner that is both parameter-efficient and free from rigid group constraints. To achieve this, we re-examine regular representations and their critical role in generating various instantiations of the fundamental group convolution equation (1). Moving from the continuous setting to concrete instantiations, we derive weight-sharing from a finite-dimensional vector of base weights by interpreting regular representations as permutations. We further analyze the characteristics of this weight-sharing approach, proposing the use of doubly stochastic matrices. This analysis forms the foundation for developing weight-sharing layers that adaptively learn dataset symmetries.

### Weight-sharing through permutations

In the current section, we first establish the connection between representations, weight-sharing and permutations. We then proceed to provide practical instantiations as ingredients for the proposed weight-sharing convolutional layers.

[MISSING_PAGE_FAIL:5]

activation functions so as not to break group equivariance. Such activations in practice are not as effective as the classic element-wise activations such as ReLU : they may introduce discretization artifacts that disrupt exact equivariance, ultimately constraining the method's expressivity . Hence, when learning weight-sharing schemes--as is our objective--it is preferred to achieve weight-sharing using regular representations without the risk of breaking equivariance by using standard activation functions.

### Learnable doubly stochastic tensors

Having established the link between regular representations and soft permutations, we now motivate the use of doubly stochastic matrices as natural candidates for their implementation. Specifically, we utilize the fact that the expected value of random variables over this set of permutations yields a doubly stochastic matrix .

Doubly stochastic matricesLet \(_{}\) (\(_{n}\)) denote respectively the system of infinite (\(n n\)) doubly stochastic matrices, i.e. matrices \(S\{s_{ij}:i,j=1,2,...,(n)\}\) such that \(_{j}s_{ij}=1\), and \(_{i}s_{ij}=1\). Let \(_{}\) (\(_{n}\)) denote respectively the system of infinite (\(n n\)) permutation matrices, i.e. matrices \(P\{p_{ij}\{0,1\}:i,j=1,2,...(,n)\}\) such that \(_{j}p_{ij}=1\), and \(_{i}p_{ij}=1\). Note that for any permutation tensor \(^{|G||X||X|}\), where \(X\) is the domain of the signal that is transformed by the group \(G\), then \(_{g}_{|X|}\) for every \(g G\).

Then, by Birkhoff's Theorem , and its extension to infinite dimensional matrices, commonly called Birkhoff's Problem 111 , we have that any convex combination of permutation matrices will be equal to a doubly stochastic matrix, i.e,

\[_{P_{n}}(P)P=S_{n},_{P_{n}}(P)=1( n\{+ \})\]

where \((P)\) gives a probability measure supported on a finite subset of the set of permutation matrices \(\). Therefore we may state that

Using doubly stochastic matrices, we can model approximate equivariance as defined in .

I.e., let \(S\) be a random variable over \(\{_{g}_{|X|} g G\}\) with a finitely supported probability measure \([S=_{g}]=(_{g})\) for every \(g G\), then \(=[S]=_{g G}[S=_{g}]_ {g}\) is a doubly stochastic matrix. We want to note that \(\) can be seen as a generalization of the _convolution matrix_ presented in .

Sinkhorn operatorThe Sinkhorn operator  transforms an arbitrary matrix to a doubly stochastic one through iterative row and column normalization, provided that the number of iterations

Figure 1: Kernel stacks are acquired through a learned weight-sharing scheme applied to a set of flattened base kernels.

is large enough. That is, initialize a tensor \(^{N N}\), then it will converge to a doubly stochastic tensor via the following algorithm:

\[S^{0}()=()\,, 28.452756ptS^{l}()=T_{c}(T _{r}(S^{l-1}()))\,, 28.452756pt_{N} =_{l}S^{l}()\,, \]

with \(T_{c}\) and \(T_{r}\) the normalization operators over the rows and columns, respectively, defined as \(T_{c}=X_{N}_{N}^{T}}_{_{c}()}\) and \(T_{r}=_{N}_{N}^{T}}_{_{r}()}\), where \(\) denotes elementwise division, \(_{c}()\), \(_{r}()\) perform column-wise and row-wise summation, respectively.

Our proposal: Weight Sharing Convolutional Neural NetworksHaving established the foundational elements, we now define our _Weight Sharing Convolutional Neural Networks_ (WSCNNs). We let \(_{i}^{l}^{|X||X|}\) be a collection of learnable parameters that parametrize the _representation stack_ of the layer \(l\) as \(^{l}=(S^{K}(_{0}^{l}),S^{K}(_{1}^{l}),...,S^{K}(_{ N}^{l}))^{T}^{|G||X||X|}\). i.e., we parameterize this tensor as a stack of \(|G|\) approximate doubly stochastic \(|X|\)-dimensional matrices, wherein stochasticity is enforced via \(K\) applications of the Sinkhorn operator. We also define a set of _learnable base weights_\(^{l}^{|X| C_{out} C_{in}}\). The WSCNN layer is then simply given by (7) with \(\) and \(\) respectively replaced by \(^{l}\) and \(^{l}\).

We further note that on image data \(|X|\) can be large, making the discrete matrix form implementation computationally demanding. Hence, we consider semi-direct product group parametrizations for \(G\), in which we let \(G\) be of the form \((^{n},+) H\), with \(H\) a learnable (approximate) group. Then, the representation stacks will merely be of shape \(|H||X^{}||X^{}|\), with \(|H||G|\) the size of the sub-group and \(|X^{}|\) the number of pixels that support the convolution kernel. A WSCNN layer is then efficiently implemented via a Conv2D[\(\), \(^{l}^{l}\)]. For group convolutions (after the lifting layer) the representation stacks will be of shape \(|H|(|X^{}||H|)(|X^{}||H|)\). Computational scaling requirements can be found in Appendix C.4.

## 5 Experiments

We first demonstrate that the proposed weight sharing method can effectively pick up on useful weight sharing patterns when trained on image datasets with different equivariance priors. We then proceed to show the method can effectively handle settings where partial symmetries are present in the data, and further analyze the learned weight-sharing structures on a suite of toy datasets. Model architectures, regularizers (\(\), \(\)) and design choices can be found in Appendix C.4 and C.1, respectively. An analysis of computational requirements can be found in Appendix C.4

### Image datasets and equivariance priors

We assess the efficacy of our proposed weight-sharing scheme in recognizing data symmetries through experiments on datasets subjected to various data augmentations. Specifically, we evaluate our model on MNIST images that have been rotated (with full \(SO(2)\) rotations) and scaled (with scaling

Figure 3: Comparison of \(C_{4}\) representations and the representation stack learned by the lifting layer on the rotated MNIST dataset. Top: learned representations. Bottom: permutations for \(C_{4}\) on \(d=25\).

Figure 2: Learned kernels from the lifting layer of WSCNN, applied to rotated MNIST and reshaped to \([,C_{out}]\). Since \(_{1}\) is set as the identity operator, the first column displays the raw kernels.

factors between \([0.3,1.0]\)). We categorize these datasets based on their data symmetry characteristics: MNIST with rotation and scaling as datasets with known symmetries, and CIFAR-10 with flips as a dataset with unknown symmetries.

For RotatedMNIST, we regard a C4-group convolutional model as a benchmark since it has been equipped with a subgroup of the underlying data symmetries a priori. Additionally, we contrast our results with a non-constrained CNN model which has double the number of channels, allowing for free optimization without symmetry constraints. As such, our evaluations are benchmarked against two distinct models: 1) a group convolutional model that is equivariant to discrete rotations, embodying fixed equivariance constraints, and 2) a standard CNN that adheres only to translational equivariance, without additional constraints.

This experimental setup positions the standard CNN as the most flexible model lacking predefined inductive biases. In contrast, the group convolutional neural network (GCNN) is characterized by fixed weight sharing, while our proposed weight-sharing CNN (WSCNN) introduces a semi-flexible, learnable weight-sharing mechanism. Note that we explicitly distinguish between the number of free model and kernel parameters and the additional parameters introduced by our weight sharing scheme throughout results (marked by \(+\)).

Results can be found in Tab. 1. When there is a clear misalignment between the model and data symmetries, the constraints imposed by the model hinder performance, as demonstrated by the C4-GCNN on the scaled MNIST dataset. Notably, our proposed method consistently achieves high performance across all datasets without requiring fixed group specifications. Furthermore, visual inspection of the learned kernels indicates that WSCNN adapts to the underlying data symmetries by effectively rotating kernels, as shown in Figure 2. Additionally, analysis of the learned representation stack reveals that it closely resembles elements of \(C_{4}\) permutations, further demonstrating the model's capability to internalize and replicate data transformations (see Figure 3 and Appendix B.3).

Additionally, we test the model on CIFAR-10, representing a dataset with possibly more complex/unknown symmetry structures, and CIFAR-10 with horizontal flips (which cannot be represented by \(C_{N}\) transformations). Results can be found in Tab. 2, with detailed training and model specifications available in Appendix C.4. We compare against the (possibly misspecified) C4-GCNN, and several unconstrained CNNs models with varying number of channels: 1) **CNN-32** matched in free

  
**Model** & **\# Params** & **Sharing Scheme** &  \\  & & & Rotations & Scaling \\  CNN & 412 K & \(Z_{2}\) & \(\) & \(\) \\ GCNN & 103 K & \(Z_{2} C_{4}\) & \(\) & \(97.50.15\) \\  WSCNN + norm & 410 K (+ 122 K) & Learned & \(97.56.07\) & \(\) \\ WSCNN + norm + ent & 410 K (+ 122 K) & Learned & \(\) & \(\) \\   

Table 1: Test accuracy on MNIST for both rotation and scaling transformations. Additional parameters induced by weight-sharing are marked (+). Parameter counts denoted in millions (M) or thousands (K). Best-performing models (equivalent within \(<1\%\)) marked **bold**.

  
**Model** & **\# Params** & **\# Elements** & **Accuracy** \\  CNN-32 & 428 K & - & \(70.50 0.62\) \\ CNN-64 & 1.66 M & - & \(76.29 0.57\) \\ CNN-128 & 6.5 M & - & \(\) \\ GCNN & 1.63 M & 4 & \(76.72 0.26\) \\  WSCNN + norm & 1.63 M (+ 468 K) & 4 & \(\) \\ WSCNN + norm + ent & 1.63 M (+ 468 K) & 4 & \(76.80 1.40\) \\   

Table 2: Test accuracy on CIFAR-10. Number of elements denotes the number of group elements used in group convolutional models. Additional parameters induced by weight-sharing are marked (+). Parameter counts denoted in millions (M) or thousands (K). Best-performing models (equivalent within \(<1\%\)) marked **bold**.

kernel size, 2) **CNN-64** matched in parameter budget, and 3) **CNN-128** matched in effective kernel size (calculated as \(|G|=4 32=128\)). Despite the kernel constraints in WSCNN, it achieves performance comparable to that of the unconstrained 128-channel CNN (within \(<1\%\) accuracy), at a significantly smaller parameter budget (2.1 M vs. 6.5 M).

### Learning partial equivariances

We show our method is able to pick up on partial symmetries by testing it on MNIST with rotations sampled from a subset of SO(2) and compare it to the C4-GCNN. Additionally, we show results on CIFAR-10 with horizontal flips, which is a commonly used train augmentation. Results can be found in Tab. 3 and Tab. 4.

Additionally, We proceed to test the model's capability to detect data symmetries by applying it to a suite of toy problems, wherein the datasets comprise noisy \(G\)-transformed samples. Details on the data generation framework are provided in Appendix B. Our testing employs a single-layer setup aimed at learning a collection of kernels that ideally match each data sample, considering inherent noise. This involves training the model to identify a set of base kernels and their pertinent transformations, effectively adapting to the variations presented by the toy problems.

## 6 Discussion and Future Work

We demonstrated a method that can effectively identifies underlying symmetries in data, even without strict group constraints. Our approach is uniquely capable of learning both partial and approximate symmetries, which more closely mirrors the complexity found in real-world datasets. Utilizing doubly stochastic matrices to adapt kernel weights for convolutions, our method offers a flexible means of learning representation stacks, accommodating both known and unknown structures within the data. This adaptability makes it possible to detect useful patterns, although these may not always be interpretable in traditional group-theoretic terms due to the absence of predefined structures in the representation stack.

Limitations include computational requirements, which scale quadratically with the size of the group and the kernel size, posing challenges in scenarios with large groups or high-resolution data. As such, in this work we have designed the representation stack to be uniform across the channel dimension. However, this prevents learning of other commonly used image transformations such as color jitter. Furthermore, the need for task-specific regularization to manage entropy scaling during the learning of representations introduces complexity in hyperparameter tuning, which can be a barrier in some applications. Additionally, we observed that representations in later layers may show minimal diversity, suggesting that further innovation in regularization strategies might be necessary to enhance the distinctiveness of learned features across different layers.

For future work, we aim to enhance our method by implementing hierarchical weight-sharing across layers and promoting group equivariance more systematically. One promising direction is to leverage the concept of a Cayley tensor, akin to , to identify and reuse learned group structures across different layers of the network. This approach would not only impose a more unified and coherent group structure within the model but also potentially reduce the computational overhead associated with learning separate representations for each layer. By encouraging a shared group structure throughout the network, we anticipate improvements in both performance and interpretability, paving the way for more robust and efficient symmetry-aware learning systems.

  
**Model** & **Rot. Range** & **Accuracy** & **Model** & **\# Params** & **\# Elements** & **Accuracy** \\  GCNN & \([0,\ 90^{}]\) & \(98.84.002\) & CNN-64 & 1.7 M & - & \(79.81.001\) \\  & \([0,180^{}]\) & \(98.72.001\) & CNN-128 & 6.5 M & - & \(82.60.001\) \\  WSCNN & \([0,\ 90^{}]\) & \(98.87.001\) & GCNN & 1.6 M & 4 & \(76.05.004\) \\  WSCNN & 1.6 M (+ 468 K) & 4 & \(82.38.003\) \\   

Table 4: Test accuracy on CIFAR-10 with horizontal flips.

  
**Model** & **Rot. Range** & **Accuracy** \\  GCNN & \([0,\ 90^{}]\) & \(98.84.002\) \\  & \([0,180^{}]\) & \(98.72.001\) \\  WSCNN & \([0,\ 90^{}]\) & \(98.87.001\) \\  & \([0,180^{}]\) & \(99.25.001\) \\   

Table 3: Test accuracy on MNIST with partial rotations.