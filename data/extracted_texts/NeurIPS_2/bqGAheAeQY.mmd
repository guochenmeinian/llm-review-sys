# Fast Encoder-Based 3D from Casual Videos via Point Track Processing

Yoni Kasten\({}^{1}\)

\({}^{1}\)NVIDIA Research

&Wuyue Lu\({}^{2}\)

\({}^{1}\)NVIDIA Research

&Haggai Maron\({}^{1,3}\)

\({}^{2}\)Simon Fraser University

&\({}^{3}\)Technion

###### Abstract

This paper addresses the long-standing challenge of reconstructing 3D structures from videos with dynamic content. Current approaches to this problem were not designed to operate on casual videos recorded by standard cameras or require a long optimization time. Aiming to significantly improve the efficiency of previous approaches, we present TracksTo4D, a learning-based approach that enables inferring 3D structure and camera positions from dynamic content originating from casual videos using a single efficient feed-forward pass. To achieve this, we propose operating directly over 2D point tracks as input and designing an architecture tailored for processing 2D point tracks. Our proposed architecture is designed with two key principles in mind: (1) it takes into account the inherent symmetries present in the input point tracks data, and (2) it assumes that the movement patterns can be effectively represented using a low-rank approximation.TracksTo4D is trained in an unsupervised way on a dataset of casual videos utilizing only the 2D point tracks extracted from the videos, without any 3D supervision. Our experiments show that TracksTo4D can reconstruct a temporal point cloud and camera positions of the underlying video with accuracy comparable to state-of-the-art methods, while drastically reducing runtime by up to 95%. We further show that TracksTo4D generalizes well to unseen videos of unseen semantic categories at inference time. Project page: _[https://tracks-to-4d.github.io_](https://tracks-to-4d.github.io_).

## 1 Introduction

Predicting 3D geometry in dynamic scenes is a challenging problem. In this problem setup, we are given access to multiple images of a scene taken sequentially, e.g., from a monocular video camera, where _both_ the content in the scene and the camera are moving. Our task is to reconstruct the dynamic 3D positions of the points seen in the images and the camera poses. This fundamental problem has gained significant interest from the research community over the years , mainly due to its important applications in many fields such as robot navigation, autonomous driving and 3D reconstruction of general environments . Importantly, in contrast to static scenes where the epipolar geometry constraints hold between the corresponding points of different views , determining the depth of a moving point from monocular views is an ill-posed problem . This causes standard Structure from Motion techniques  to be inadequate in this setup .

Previous work and limitations.Many existing approaches for the above problem make simplifying assumptions that limit their applicability to real-world scenarios. Methods based on orthographic camera models and low-rank assumptions use matrix factorization techniques , but the orthographic camera assumption might not be realistic and may cause reconstruction errors. Techniques that incorporate depth priors often require lengthy optimization processes in order to make the depth estimates across frames consistent . Other physics-based approaches make assumptionsabout rigid bones  or isometric deformable surfaces  and typically involve complex, slow optimization per video. In addition, they may require foreground-background segmentation of the moving content, which is not always easily obtained. Alternatively, some methods are specifically tailored to certain object classes like humans , restricting their domain to those limited cases. Consequently, these prior methods are either not directly applicable to casual videos, or require long optimization time per video.

Our approach.We propose TracksTo4D,1 a novel approach for fast reconstruction of sparse dynamic 3D point clouds and camera poses from casual videos. Our main idea is to train a neural network on multiple videos to learn the mapping from the input image sequence to a sequence of the scene's 3D point clouds and camera poses. After training, the trained network can be efficiently applied to new image sequences using a single feed-forward pass, avoiding costly optimization.

To enhance the method's ability to generalize across different types of videos and scenes, we made a crucial design choice: our approach processes point track tensors as input, rather than operating directly on the image sequence. Specifically, each entry \((n,p)\) in these tensors represents the 2D position of a tracked point \(p\) in a specific video frame \(n\). Our main insight is that point track tensors may exhibit more common motion patterns across casual video domains compared to image pixels. In other words, we argue that processing the raw point track data rather than scene-specific pixels or features may enable learning class and scene-agnostic internal feature representations for improved generalization. Importantly, recent advances in point tracking  enable efficiently inferring these point tracks from casual videos using pre-trained models. These two properties make point track matrices an attractive input for our learning method.

Following this design choice, we design our architecture according to two principles: (1) process point track tensors, which have a unique structure, and (2) encode meaningful prior knowledge about the reconstruction problem, as the problem is ill-posed in general. In the following, we address these desired properties.

First, we design a network architecture that can effectively and efficiently handle point track inputs. To do that, we propose a novel layer design that takes into account the symmetries of the problem: the mapping we aim to learn, from point track matrices to 3D point clouds and camera poses, preserves two natural symmetries: (i) the points being tracked can be arbitrarily permuted without affecting the problem; (ii) the frames containing these points exhibit temporal structure, adhering to an approximate time-translation symmetry. Following the Geometric Deep Learning paradigm , we build upon recent theoretical advances in equivariant learning  and integrate these two symmetries into our network architecture using dedicated attention and positional encoding mechanisms.

Figure 1: We present TracksTo4D, a method for mapping a set of 2D point tracks extracted from casual dynamic videos into their corresponding 3D locations and camera motion. At inference time, our network predicts the dynamic structure and camera motion in a single feed-forward pass. Our network takes as input a set of 2D point tracks (left) and uses several multi-head attention layers while alternating between the time dimension and the track dimension (middle). The network predicts cameras, per-frame 3D points, and per-world point movement value (right). The 3D point internal colors illustrate the predicted 3D movement level values, such that points with high/low 3D motion are presented in red/purple colors respectively. These outputs are used to reproject the predicted points into the frames for calculating the reprojection error losses. See details in the text. The reader is encouraged to watch the supplementary video visualizations.

Second, a key challenge in predicting 3D dynamic motion and camera poses from 2D point tracks is that this problem is inherently ill-posed without additional constraints . To address this, we integrate a low-rank movement assumption into our architecture, following the seminal work of  which constrained output point clouds to be linear combinations of basis elements. Specifically, given an input point track tensor, our architecture equivariantly predicts a small set of input-specific basis elements. The output point clouds at each time frame are then defined as a linear combination of these basis elements, with the coefficients also predicted by the network. Notably, the first basis is assumed to fully represent the 3D static points in the video, while the remaining basis elements capture the 3D dynamic deviations. This structure effectively restricts the predicted point clouds to have a more specific form, making the problem more constrained.

Our network is trained on a dataset of extracted point track matrices  from raw videos without any 3D supervision by simply minimizing the reprojection errors, aiming to predict output point clouds that, after undergoing a perspective projection, will return the original 2D point tracks. In our experiments, TracksTo4D is trained on the Common Pets dataset . We evaluate our method on test data with GT camera poses and GT depth information for point tracks, and demonstrate that it produces comparable results to state-of-the-art methods, while having a significantly shorter inference time by up to \(95\%\). In addition, we show the method's ability to generalize to out-of-domain videos.

Contributions.In summary, our contributions are (1) A novel modeling of the dynamic reconstruction problem via learning on point tracks without 3D supervision; (2) A novel deep learning architecture incorporating two key principles: accounting for the symmetry of the data and encoding low-rank structure in the predicted point clouds (3) Experiments demonstrating extremely fast inference time compared to baselines, accurate results, and strong generalization across other categories.

## 2 Method

Problem formulation.Given a video of \(N\) frames, let \(M^{N P 3}\) be a pre-extracted \(2D\) point tracks tensor (Fig. 1, left side). This tensor represents the two-dimensional information about a set of \(P\) world points that are tracked throughout the video. Each element in the tensor, \(M_{i,j,}\), stores three values: \((x,y,o)\) where \(x,y\) are respectively the observed horizontal and vertical locations of point \(j\) in frame \(i\), and \(o\{0,1\}\) indicates whether point \(j\) is observed in frame \(i\) or not. Our goal is to train a deep neural network to map the input point tracks tensor \(M\) into a set of per-frame camera poses \(\{R_{i}(M),_{i}(M)\}_{i=1}^{N}\) and per-frame 3D points \(\{X_{i}(M)\}_{i=1}^{N}\), where \(R_{i}(M)(3),_{i}(M)^{3},X_{i}(M) ^{P 3}\) (Fig. 1, right side).

Overview of our approach.Our method receives \(M^{N P 3}\) as input. This tensor is being processed by a neural architecture composed of multi-head attention layers where the attention is applied in an alternating fashion on the \(P\) and the \(N\) dimensions in each layer. These layers are defined in Sec. 2.1. After a composition of several such layers, the network uses the resulting features in \(^{N P d}\) to predict \(N\) camera poses in \(^{3}^{3}\) and \(N\) point clouds in \(^{N P 3}\). These \(N\) point clouds are parameterized as a linear combination of \(K\) input specific point cloud bases \(B_{1}(M), B_{K}(M)^{P 3}\). This is discussed in detail in Sec. 2.2. Our network is trained in an unsupervised way on a dataset of videos by minimizing the reprojection error and other regularization losses (Sec. 2.3) that are used to update the model parameters. Our pipeline is illustrated in Fig. 1

### Equivariant layers for point track tensors

Following the geometric deep learning paradigm, our goal is to design a neural architecture that respects the underlying symmetries and structure of the data.

Symmetry analysis.Our input is a tensor \(M^{N P 3}\) representing a sequence of \(N\) frames each with \(P\) point coordinates. This structure gives rise to two key symmetries: First, the order of the \(P\) points within each frame does not matter - in other words, permuting this axis results in an equivalent problem 2.

Formally, this axis has a permutation symmetry \(S_{P}\) where \(S_{P}\) is the symmetric group on \(P\) elements. Second, along the temporal \(N\) axis, we have an approximate translation symmetry arising from the ordered video sequence. This means that shifting the time frames is required to result in the same shift in our output. We model this with a cyclic group \(C_{N}\) of order \(N\). Both symmetries are illustrated in Fig. 2. We note that while the cyclic group assumption may not be entirely accurate, we still find it useful as it helps us to derive appropriate parametric layers for our data, similar to how the convolutional layer is derived for data with translational symmetries such as images. Taken together, the full symmetry group of the input space is the direct product \(=C_{N} S_{P}\) combining these time and point permutation symmetries, acting on \(^{N P 3}\) by \(((t,) M)_{n,p,j}=M_{t^{-1}(n),^{-1}(p),j}\) for \((t,)^{3}\). Next, we will design an architecture equivariant to \(\), to ensure that the model takes into account the symmetries above.

Linear equivariant layers.Point track tensors can be viewed as a collection of \(N\) individual point tracks, each of which exhibits translational symmetry. The scenario where an object comprises a set of elements with their own symmetry group, such as a set of images or graphs, was previously explored in . In that work, the authors characterized the general linear equivariant layer structure in such cases, termed the Deep Sets for Symmetric Elements (DSS) layer. Building on the DSS approach, our basic linear equivariant layer for the point track tensors \(M\) would take the form:

\[F(M)_{:,j}=L_{1}(M_{:,j})+_{j^{}=1}^{P}L_{2}(M_{:,j^{}}) \]

where \(L_{i}\) are linear translation equivariant function (i.e. convolutions), \(M_{:,j}^{N d}\) are the columns of \(M\) representing all the inputs for a specific tracked point, \(F(M)_{:,j}^{N d^{}}\) is the output column and \(d,d^{}\) are the input and output feature channels respectively. To construct a neural network, these layers can be interleaved with pointwise nonlinearities, similar to basic convolutional neural networks.

Implementation via transformers and positional encoding.While the linear layer design is reasonable, it may not be the optimal choice. To enhance the model, we design a new layer whose structure follows Equation (1), but incorporates nonlinear layers in the form of transformers . We note that the idea of using linear layers as inspiration for non-linear layers aligns with common practices in geometric deep learning, as described in several previous works .

Specifically, our layer \(F\) is formulated similarly to Equation (1), but instead of convolutions (\(L_{i}\)) and summations, it utilizes two self-attention mechanisms and suitable temporal positional encoding across the \(N\) dimension. Formally, our basic layer \(F:^{N P d}^{N P d^{}}\) is computed via four steps, which are described below:

\[}_{ij}=^{Q}M_{ij},\ }_{ij}= ^{K}M_{ij},\ }_{ij}=^{V}M_{ij} _{ij}=_{i^{}=1}^{N}}_{ij} }_{ij^{}})}{_{l=1}^{N}(} _{ij}}_{ij})}}_{i^{}j} \]

\[_{ij}=W^{Q}_{ij},\ _{ij}=W^{K}_{ij},\ _{ij}=W^{V}_{ij} F(M)_{ij}=_{j^{}=1}^{P} _{ij}_{ij^{}})}{_{l=1}^{P}( _{ij}_{il})}_{i^{}j^{}} \]

Here, \(M_{i,j}^{d}\) are the features associated with the \(j\)-th point in the \(i\)-th frame. The attention mechanism defined in the first equation above (2) is augmented with standard temporal positional

Figure 2: The symmetry structure of our problem. Frames (vertical) have time translation symmetry while points (horizontal) have set permutation symmetry.

encoding in the first layer and replaces the translation equivariant function \(L_{i}\) applied to the columns of \(M\) (Eq.(1)). The attention in the second equation (3) implements the set aggregation (summation) (also in Eq.(1)) applied to the rows of \(M\). As commonly done, we use transformers with 16 attention heads .

### Constraining 3D motion and camera poses via low-rank assumption

Given our 2D tracks, we aim to characterize the motion of the points by decomposing them into the global camera motion and the 3D motion of objects in the scene. The 2D motion of static scene points provides useful constraints for estimating the camera motion. However, as previously mentioned, predicting camera and dynamic 3D motion solely from 2D motion is an ill-posed problem without additional constraints . We tackle this challenge by adding two mechanisms to our architecture: (1) low-rank movement assumption; and (2) specific modeling of the static scene for camera estimation.

Low-rank movement assumption.First, motivated by classical orthographic Non-Rigid Structure from Motion , we constrain the output points to be formulated by a linear combination of input-specific basis elements. Specifically, given the input 2D point tracks, \(M^{N P 3}\), our network predicts \(K\) point clouds: \(B_{1}(M),,B_{K}(M)^{P 3}\) and \(N(K-1)\) linear coefficients,

\(\{c_{1k}(M)\}_{k=2}^{K},\{c_{Nk}(M)\}_{k=2}^{K}\), such that

\[X_{i}(M)=B_{1}(M)+_{k=2}^{K}c_{ik}(M)B_{k}(M) \]

where \(X_{i}(M)^{P 3}\) is the 3D point cloud at frame \(i\). The point clouds and coefficients are computed by taking the output of the last equivariant layer as defined in the previous section and applying invariant aggregations on the respective dimension resulting in equivariant and invariant outputs. See more details in the appendix. We note that we deliberately chose the coefficient of \(B_{1}(M)\) to be the constant \(1\), the reason is explained in the next paragraph.

Specific modeling of the static scene for camera estimation.Frequently, casual video data of dynamic scenes contains many static regions, which can be used to determine camera poses . We leverage this observation by treating the first basis element \(B_{1}(M)^{P 3}\) as a static approximation for all scene points and encourage \(B_{1}(M)\) as well as the output camera poses to explain the 2D observations according to this approximation using a "static" reprojection loss (\(_{}\), defined in the next section). We note, however, that a static point cloud is not likely to produce low reprojection errors for the non-static components, thus the reprojection error necessitates robustness to substantial errors from the non-static elements. To address this, our network predicts (equivariantly) \(P\) motion level values \(_{1}(M),,_{P}(M)_{+}\), one for each point in our dynamic point cloud, which we use to weight the reprojection errors from \(B_{1}(M)\). The main idea is to give less weight to non-static points so that the static projection loss can disregard them. Specifically, inspired by , each \(_{i}(M)\) defines a Cauchy distribution that models the reprojection errors for its associated world point, such that a world point with higher \(\) is expected to produce a wider error distribution. Empirically, as noted by , the Cauchy distribution tends to be more robust for modeling reprojection error uncertainties compared to Gaussian noise modeling . Then, \(_{}\), minimizes the negative log-likelihood under this assumption. See details in Sec. 2.3.

Comparison to supervised learning setups. A central aspect of our method is addressing the ill-posed nature of this problem through an architecture that heavily regularizes the output dynamic point clouds, enabling training without supervision. Alternatively, one could use video data with ground truth depth for supervised training, potentially allowing the output to have a more general form. Unfortunately, such supervision is both challenging to acquire and may lead to poor generalization on novel motion patterns.

### Training and losses

**Model outputs.** Given the input 2D point tracks \(M^{N P 3}\), our network produces outputs as a function of \(M\): linear bases and coefficients,\(B_{1}(M),,B_{K}(M)^{P 3},\{c_{1k}(M)\}_{k=2}^{K},,\{c_{Nk}(M)\}_{k=2}^{K} \) which define a dynamic point cloud \(X_{1}(M),,X_{N}(M)^{P 3}\),\(_{1}(M),,_{P}(M)_{+}\) movement level values, and \((R_{1}(M),_{1}(M)),,(R_{N}(M),_{N}(M)) SO(3) ^{3}\) camera poses.

We use these network outputs to define a self-supervised loss function with respect to the current network weights and \(M\) which is defined by:

\[=_{}_{}+_{ }_{}+_{}_{ }+_{}_{} \]

Reprojection loss.The reprojection loss encourages the consistency between the output 3D point clouds and camera poses, to the input 2D observations:

\[_{}=^{N}_{j=1}^{P}M_{ij}^{o} }_{i=1}^{N}_{j=1}^{P}M_{ij}^{o}(X_{ij},R_{i},_{i},M_{ij}^{xy}) \]

where \((X_{ij},R_{i},_{i},M_{ij}^{xy})\) is the reprojection error when projecting the point \(X_{ij}\) with the camera pose \((R_{i},_{i})\) with respect to the measured point \(M_{ij}^{xy}\):

\[(X_{ij},R_{i},_{i},M_{ij}^{xy})=\|^{T}( _{ij}-_{i}))_{1,2}}{(R_{i}^{T}(_{ij}- _{i}))_{3}}-M_{ij}^{xy}\| \]

Static loss.As discussed in Sec. 2.2, to better constrain the camera poses, the first predicted basis element \(B_{1}(M)^{P 3}\) defines a static (fixed in time) point cloud approximation. Our network also predicts a movement coefficient \(_{j}(M)\) for each world point that defines a zero-mean Cauchy distribution. Given \(_{j}\) and the reprojection error \(r_{ij}=(B_{1j},R_{i},_{i},M_{ij}^{xy})\)4 of the \(j^{th}\) point of \(B_{1}\) that is projected by the \(i^{th}\) camera, the negative log-likelihood of \(r_{ij}\) distributed according to the \(_{j}\)-zero-mean Cauchy distribution is proportional to:

\[(r_{ij},_{j})=(_{j}+^{2}}{_{ j}}) \]

Note, that this loss reduces the contribution of the reprojection errors for points with high \(\), but also encourages \(\) to be small, i.e. encouraging the static point cloud to represent the dynamic scene when possible. Our static loss is the mean negative log-likelihood over all observed points in all frames:

\[_{}=^{N}_{j=1}^{P}M_{ij}^{o}} _{i=1}^{N}_{j=1}^{P}M_{ij}^{o}((B_{1j},R_{i}, _{i},M_{ij}^{xy}),_{j}) \]

Regularization losses.As in  we regularize the observed points to be in front of the camera by:

\[_{}=-_{i=1}^{N}_{j=1}^{P}M_{ij}^{o}(R_{i}^{T}(_{ij}-_{i}))_{3},0) \]

We further find it beneficial to regularize the deviation from the static approximation \(B_{1}\) to be sparse for static points, i.e. points with low \(\) values:

\[_{}=_{k=2}^{K}_{j=1}^{P}}(|B_{kj1}|+|B_{kj2}|+|B_{kj3}|) \]

where \(\) is detached from the gradient calculation for this loss.

## 3 Experiments

In this section, we conduct experiments to verify our proposed network's performance on real-world casual videos. We began by training the network on specific domains and then evaluated its accuracy and running time on unseen videos from both, training and unseen domains.

**Training procedure.** We trained our network on the cat and dog partitions from the COP3D dataset , which contains a diverse set of casual real-world videos of pets. Our networks were trained from scratch three times to test our generalization capability between semantic categories: once on the cat partition, once on the dog partition, and once on both partitions combined. Training technical details are provided in the appendix. We use \(K=12\) bases in all our experiments (Sec. 2.2).

**Evaluation data.** To assess our framework's performance on pet videos, we curated a new dataset5 consisting of 21 casual videos of dogs and cats, each video containing 50 frames. These videos were captured using an RGBD (RGB-Depth) sensor. The depth maps were used as ground truth for evaluating the reconstructed structure. We extracted the cameras by running COLMAP on the images while masking out the pet areas with dilatated masks provided by . The cameras were scaled to millimeter units using the provided GT depth. Note that our network did not see this test data during training and it was not used to tune our hyperparameters.

Additionally, to evaluate our method on out-of-domain evaluation data, we used the Nvidia Dynamic Scenes Dataset . Specifically, while our network was trained on pet videos, this dataset contains other dynamic object types, e.g. human, balloon, truck, and umbrella, with a different camera motion type and a variety of motion profiles. The dataset contains 8 dynamic scenes which are captured by 12 synchronized cameras, enabling accurate depth estimation which is treated as GT for evaluating monocular depth estimation. The ground truth camera poses were calculated by  with the synchronized multiview camera rig and the ground truth dynamic masks. Similarly to  we

    &  &  & \)} & \)} &  &  &  &  & Time \\  & Dyn. & All & Dyn. & All & Dyn. & All & Dyn. & All & (mm) & (mm) & (deg) & (min) \\  D-SLAM  & - & - & - & - & - & - & - & 5.08 & 3.60 & 0.20 & 0.16 \\ ParticleSLAM  & - & - & - & - & - & - & - & 12.79 & 6.95 & 0.51 & 11.00 \\ RCVD  & 0.40 & 3.6E+0.7 & 0.43 & 0.72 & 0.75 & 0.90 & 0.92 & 0.96 & 43.95 & 25.77 &simulated 8 monocular dynamic video sequences using the camera rig, each with 24 frames, and used them for evaluation.

Evaluation results.Qualitative visualizations are presented in Fig. 3.6 We also show a visualization of the movement level values, \(\) in Fig. 4. For comparisons, we chose state-of-the-art methods that as our method, can be applied to raw casual videos that were captured by standard pinhole camera models and do not need any static or dynamic segmentation. We evaluated both, the camera poses and the structure accuracies. Comparison results for the pet-test-set and out-of-domain dataset are presented in Tables 1 and 2 respectively. The camera poses are evaluated compared to the GT, using the Absolute Translation Error(ATE), the Relative Translation Error(RTE), and the Relative Rotation Error(RRE) metrics after coordinates system alignment. We compare three training configurations of our method of training only on cats, only on dogs, and on both. As can be seen in the tables, the results are similar in all 3 cases. Our output camera poses as inferred by the network are already accurate and outperform some of the prior methods. We further show the results of our method after a single and short round of Bundle Adjustment, which makes our method better than all baselines on the pet sequences, and comparable on the out-of-domain cases.

Importantly, Tables 1 and 2 also compare the method's runtime. It can be seen that our method, even with bundle adjustment, is the fastest camera prediction method. Note that our method's runtimes include the point tracking time that is performed by  as a pre-process. In the appendix (Tab. 8), we present tracking time versus inference time, showing that most of the time is spent on tracking, while our inference is very fast. Tables 1 and 2 also show structure evaluation with the depth evaluation metrics  on the sampled point tracks. They demonstrate that our inferred structure is almost comparable to the state-of-the-art  while taking significantly shorter running times (a few seconds for our method versus more than two hours for  on pet videos). Short (0.6-5 minutes), per-sequence fine-tuning makes our method's accuracy comparable to . In the appendix, we present per-scene output accuracy, demonstrating our ability to generalize across different speed profiles. In terms of running time, our method is a bit slower than MiDaS  which only provides depth maps without cameras, but achieves much better results7. We note that in contrast to the other methods that predict the dynamic depth, ours does not use any depth-from-single-image prior.

Ablation studyTo evaluate the contribution of our different method parts we run an ablation study which is presented in Tab. 3. In this study, the training was always done on the cat partition from COP3D and evaluated on our test data which contains dogs and cats. First, we performed an ablation study on our transformer architecture by taking the architecture suggested by  ("Set of Sets") or the DSS architecture that uses only linear layers  ("DSS"). As the table shows our architecture ("Full") achieved significantly better results. To test the losses in our framework, we also evaluated the following: (1) ignoring the \(\) outputs and using regular reprojection error on \(B_{1}\) for all points ("No \(\)"); (2) removing our sparsity loss ("No \(_{}\)"); and (3) removing the static loss ("No \(_{}\)"). In all cases, the error increased whereas in the later one, the results became much worse. We further ablate the choice of \(K=12\) as the number of linear bases, by trying 2 extreme numbers of \(K=30,K=2\) (we saw no significant differences when we used nearby choices such as \(K=11\)). As can be seen in the table, when \(K=30\) the output is not regularized enough and produces a higher

    &  &  & \)\(\)} & \)\(\)} &  &  &  &  \\  & Dyn. & All & Dyn. & All & Dyn. & All & Dyn. & All & Dyn. & All & (mm) & (mm) & (deg) \\  Set of Sets & 0.27 & 0.15 & 0.60 & 0.77 & 0.87 & 0.94 & 0.97 & 0.99 & 9.86 & 5.33 & 16.87 & 5.53 & 0.39 \\ No \(_{}\) & 0.77 & 0.36 & 0.

depth error for the dynamic part. For \(K=2\) the depth is regularized but the reprojection error ("Rep. (pix.)") gets higher due to over-regularization. Overall, this study justifies our design choices ("Full").

## 4 Related Work

Simultaneous Localization and Mapping (SLAM) and Structure from Motion (SfM) SfM pipelines seek to recover static 3D structure and camera poses from unordered images.[49; 45; 42; 1, 55]. Learning-free pipelines  are effective but require repeated applications of Bundle Adjustment (BA) . [32; 9] presented a method for learning prior from a dataset of multiview image sets, to accelerate SfM pipelines by using equivariant deep networks. Monocular Simultaneous Localization and Mapping (SLAM) methods [33; 34; 13; 61; 6; 54; 67; 69; 46] extract camera poses from video sequences while defining a scene map with keyframes. These methods assume static scenes, fail to produce the cameras in scenes with large portions of dynamic motion, and cannot reproduce dynamic parts of the scene. DROID-SLAM  used synthetic data with ground truth 3D supervision for learning to predict camera poses via deep-based BA on keyframes while excluding dynamic objects. ParticleSfM  filters out 2D dynamic content for reproducing the cameras in dynamic scenes, using its pre-trained motion prediction network. Both, [48; 66] do not infer the dynamic 3D structure.

Orthographic Non-Rigid SfM (NRSfM)Bregler et al.  introduced a factorization method for computing a non-rigid structure and rotation matrices from a point track matrix, by assuming a low

Figure 4: \(\) **Visualization.** We show a visualization of the \(\) outputs of our network that are described in Sec. 2.2. In each video sequence, we show the input tracks, where each color visualizes its movement level value, \(\). Purple marks static points with low \(\) whereas red marks dynamic points with high \(\). Note, that our network did not get any direct supervision for these values, but only the raw point tracks predictions from . The \(\) visualizations for cats were produced by the model that was only trained on dogs and vice versa. We note that our model generalizes well to out-of-domain (non-pet) cases as well.

Figure 3: **Qualitative Results.** **Top.** Frames from 2 different test video sequences with point tracks marked with corresponding colors. **Bottom.** A 3D visualization of our method’s outputs, from two time stamps. The camera trajectory is present as gray frustums, whereas the current camera is marked in red. The reconstructed 3D scene points are presented in corresponding colors to the input tracks on the top. The scene is observed from the same viewpoint, enabling the visualization of the dynamic reconstructed structure.

dimensional basis model. While follow-up papers improved accuracies with different regularizations [26; 10; 36; 18] or neural representations [35; 24; 43], the orthographic camera model assumption is in general not valid for casual videos. Furthermore, these methods often assume background subtraction as a preprocessing. Even though a follow-up work proposed factorization solutions for pinhole cameras , its sensitivity to noise , makes it impractical for casual videos.

Test-time optimization for dynamic scenesRecent methods [29; 25; 64; 65] finetuned the monocular depth estimation from a pre-trained model [41; 40] using optical flow constraints , for obtaining consistent dense depth maps for a monocular video.  further optimized motion maps for handling scenes with large dynamic motion. [56; 15] use depth from single image estimations, to improve novel view synthesis in dynamic scenes.  further optimizes for the unknown camera poses together with the dynamic radiance field optimization. [37; 38] model a single deformable surface from a monocular video by applying isometric constraints. LASR , ViSER  and BANMo  optimize for a dynamic surface by assuming rigid bones and linear blend skinning weights. However, all the above-mentioned methods require per-scene optimization, resulting in slow inference. Recently,  presented the Common Pets in 3D (COP3D) dataset that contains casual, in-the-wild videos of pets, and used it to learn priors for novel view synthesis in dynamic scenes.

Point trackingThere has been a recent advance in 2D point tracking by learning [20; 11], or optimization  techniques. Concurrently,  presented a method for jointly performing 2D tracking and 3D lifting, by learning to track with depth supervision while applying an as-rigid-as-possible loss. However, their method cannot predict camera poses or identify static parts directly.

## 5 Conclusions and limitations

We presented TracksTo4D, a novel deep-learning framework that directly maps 2D motion tracks from casual videos into their corresponding dynamic structure and camera motion. Our approach features a deep learning architecture that considers the symmetries in the problem with designed intrinsic constraints to handle the ill-posed nature of this problem. Notably, our network was trained using only raw supervision of 2D point tracks extracted by an off-the-shelf method  without any 3D supervision. Yet, it implicitly learned to predict camera poses and 3D structures while identifying the dynamic parts. During inference, our method demonstrates significantly faster processing times compared to previous methods while achieving comparable accuracy. Furthermore, our network exhibits strong generalization capabilities, performing well even on semantic categories that were not present in the training data.

Limitations and future work.While our experiments demonstrated that our network is efficient, accurate, and capable of generalizing to unseen video categories, there are several limitations and future work directions that we would like to address. First, our method is limited in handling videos with rapid motion, as it depends on the accuracy of the tracking method used, specifically CoTracker , which has limitations in tracking fast movements. We observed that  often fails in automotive scenes due to high motion blur, particularly on the road, limiting our ability to perform large-scale evaluations on automotive datasets given the current tracking method performance. We note that any future improvements with point tracking in terms of accuracy and inference time will directly improve our method as well. Second, our method assumes enough motion parallax to constrain the depth values and fails to generate accurate camera poses without it. An interesting direction for future work would be to leverage depth models and 3D tracking techniques, which could potentially improve accuracy, especially in cases with minimal motion parallax. Yet, our current attempts to incorporate MiDaS-inferred depth as an additional input and apply a depth loss relative to MiDaS have not yielded any performance improvements, indicating that this approach needs further exploration. Third, while we found \(K=12\) basis elements to be effective for our evaluation set, balancing complexity reduction and motion representation, we acknowledge this fixed number may not capture all possible scene dynamics. Future work could explore automatically inferring the optimal number of bases per scene. Lastly, our network can handle up to about 1000 point tracks in 50 frames in one inference step when running on a single GPU. A possible extension to handle denser point clouds and longer videos could involve querying point tracks iteratively while maintaining a shared state, but this approach remains to be explored.

AcknowledgmentsHM is the Robert J. Shillman Fellow, and is supported by the Israel Science Foundation through a personal grant (ISF 264/23) and an equipment grant (ISF 532/23).