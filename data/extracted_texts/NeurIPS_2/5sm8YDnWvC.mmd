# Perceiving Longer Sequences With

Bi-Directional Cross-Attention Transformers

Markus Hiller, Krista A. Ehinger, Tom Drummond

School of Computing and Information Systems

The University of Melbourne, Australia

m.hiller@unimelb.edu.au

###### Abstract

We present a novel bi-directional Transformer architecture (BiXT) which scales linearly with input size in terms of computational cost and memory consumption, but does not suffer the drop in performance or limitation to only one input modality seen with other efficient Transformer-based approaches. BiXT is inspired by the Perceiver architectures but replaces iterative attention with an efficient bi-directional cross-attention module in which input tokens and latent variables attend to each other simultaneously, leveraging a naturally emerging attention-symmetry between the two. This approach unlocks a key bottleneck experienced by Perceiver-like architectures and enables the processing and interpretation of both semantics ('what') and location ('where') to develop alongside each other over multiple layers - allowing its direct application to dense and instance-based tasks alike. By combining efficiency with the generality and performance of a full Transformer architecture, BiXT can process longer sequences like point clouds, text or images at higher feature resolutions and achieves competitive performance across a range of tasks like point cloud part segmentation, semantic image segmentation, image classification, hierarchical sequence modeling and document retrieval. Our experiments demonstrate that BiXT models outperform larger competitors by leveraging longer sequences more efficiently on vision tasks like classification and segmentation, and perform on par with full Transformer variants on sequence modeling and document retrieval - but require 28% fewer FLOPs and are up to \(8.4\) faster. 1

## 1 Introduction

Much of the data we obtain when perceiving our environment can be interpreted via a division into '_what_' and '_where_'. If we consider for example the image pictured in Figure 1 on the left, we can easily describe its content by 'what' we see - the building, sky and a flag. If we were to draw conclusions on a more fine-grained level though, we would likely include more specific descriptions like "lower left corner" referring to their positions within the image - the 'where'. In other words, 'where' denotes the actual geometric location of the individual elements (e.g. pixels) and 'what' the semantic entities (e.g. objects) that collectively describe the data as a whole. Note that this similarly applies to many other modalities, like point clouds or even language where we form words via letters that together have a certain meaning.

Thanks to the few structural constraints placed on the input data paired with high performance, Transformers  have shown great capabilities in extracting both 'what' and 'where' for a range of input modalities, giving rise to significant advances across various fields such as Natural Language Processing  and Computer Vision . However, their success comes at the high cost of scaling quadratically in memory and time with the input length, practically prohibiting their use onlarger input data like point clouds, long documents, or high-resolution images when computational resources are limited.

Several approaches have since been proposed to increase their efficiency, either by changing how the computationally expensive self-attention operation is realized  or by exploiting the domain-specific structure of their data input . However, all these either face a reduction in the Transformer's performance or limit its application to only one specific type of input .

In an attempt to preserve the generality by not imposing additional constraints on the input data, Jaegle et al.  employ a small set of latent vectors as a bottleneck to extract the 'what' via one-sided (iterative) cross-attention - and require an additional decoder to draw conclusions about 'where' . While achieving linear complexity w.r.t. the input length, these 'Perceiver' architectures require between \(360\) - \(707\) GFLOPs to achieve around \(78\%\) accuracy on ImageNet1K - results that recent Transformer variants like ViT  are able to obtain at a fraction of the compute. One possible explanation for this discrepancy is that the effective working memory of Perceiver architectures is strictly limited to the latents which therefore need to compensate via increased computation, whereas conventional Transformers like ViTs leverage the (larger) number of tokens across several layers. This raises an important question: Are the appealing individual properties of these two methods mutually exclusive, or can we in fact have _the best of both worlds?_

In this paper, we set out to affirm the latter. We demonstrate that a small set of latent vectors appropriately combined with layerwise simultaneous refinement of both input tokens and latents makes it possible to pair the high performance and architectural simplicity of Transformers with the linear scaling of Perceivers - outperforming both in settings where compute is limited. We start off by investigating a naive approach: sequentially applying cross-attention to refine 'what' and 'where', one after the other. We discover that approximately symmetric attention patterns naturally emerge between latents and tokens even when both are provided with complete flexibility. In other words, for most latents ('what') that pay attention to particular tokens ('where'), these tokens in turn pay attention to exactly these latents (see Figure 1 and Section 3.1). Not only does this intuitively make sense - objects need to know 'where' they are located in the image, and image locations need to know 'what' objects are located there - it more importantly offers us a unique opportunity to save FLOPs, memory and parameters.

As we will demonstrate in Section 2, this approximate symmetry means we only need to compute the attention matrix once, reducing the involved parameters by \(}{{3}}\) to facilitate an efficient bi-directional information exchange via our proposed _bi-directional cross-attention_. Integrated into our bi-directional cross-attention Transformer architecture (BiXT), this forms a flexible and high-performing yet efficient way to process different input modalities like images, point clouds or text on a variety of instance-based (e.g. classification) or dense tasks (e.g. segmentation) - all while scaling linearly w.r.t. the input length.

In summary, our main contributions include the following:

1. We introduce a novel bi-directional cross-attention Transformer architecture (_BiXT_) that scales linearly with the input size in terms of computational cost and memory consumption, allowing us to process longer sequences like point clouds, text or images at higher resolution.

Figure 1: **Emerging patterns when attending both ways.** (a) Input image. (b) depicts the areas of the image that 4 different latents attend to, while (c) inversely shows which image regions attend to these latents (transformed into the same coordinate system for ease of interpretation). (d) displays which areas & latents are symmetrically attended to using our proposed bi-directional cross-attention.

2. We propose _bi-directional cross-attention_ as an efficient way to establish information exchange that requires computation of the attention matrix only _once_ and reduces the involved parameters by \(}{{3}}\), motivated by a naturally emerging symmetry in cross-attention and showing significant improvements over uni-directional iterative methods like Perceiver.
3. We analyze BiXT's advantage of processing longer sequences across a number of tasks using different input modalities and output structures in settings with limited computational resources - with our tiny 15M parameter model achieving accuracies up to \(83.1\%\) for classification on ImageNet1K without any modality-specific internal components, performing competitively for semantic image and point cloud part segmentation even among modality-specific approaches, and being up to \(28\%\) more efficient and \(8.4\) faster on LRA.
4. We further provide insights into BiXT's extendibility: Thanks to its simple and flexible design, modality-specific components can easily be incorporated in a plug-and-play fashion should the need arise - further improving results while trading off generality.

## 2 Perceiving via Bi-Directional Cross-Attention

We start this section by briefly revisiting the concept of attention before moving on to presenting our proposed _bi-directional cross-attention_ methodology, followed by its use within our BiXT architecture (Figure 2). Please note that we define the concepts using single-head attention for brevity instead of the actually employed multi-head attention (MHA), and all methods directly generalize to MHA.

### Background: The Attention Mechanism

While self-attention has recently gained great popularity through its use in the Transformer architecture , we will start from a slightly more general point of view: Given a source sequence \(^{N Ds}\) and a target sequence \(^{M D}\), attention aims to refine \(\) by exhaustively discovering pairwise correlations between all elements of both sequences and integrating information from the source components of interest into the target.

Formally, \(\) is linearly projected into two \(D\)-dimensional representations using learnable matrices - yielding a _key_\(_{}\!\!^{N D}\) and _value_\(_{}\!\!^{N D}\) - while \(\) is projected into one \(D\)-dimensional representation to obtain the _query_\(_{}\!\!^{M D}\). These representations are then used to compute the attention-based target refinement as

\[\!^{}\!=\!(_{}, _{},_{})\!=\!\!( _{}_{}^{}}{}) _{}, \]

with the scaled dot product \(}_{,}=}{{}}( _{}_{}^{})^{M N}\) representing the scaled pairwise similarity between target and source elements. This concept is commonly referred to as _cross-attention_ (CA) between target \(\) and source \(\). If a representation itself is to be refined given the context within, i.e. source and target are identical (\(\!=\!\)), Equation (1) reduces to the well-known _self-attention_ where the triplet key, query and value are all generated as a function of the same sequence elements.

Note that computing the similarity matrix \(}_{,}\) has computational complexity \((NM)\). For self-attention used in Transformers where \(\!=\!\) and hence \(M\!=\!N\), this yields quadratic complexity \((N^{2})\) w.r.t. the input sequence length \(N\), prohibiting its use on longer sequences when computational resources are limited. On the other hand, if cross-attention is employed with a fixed sequence length \(M\!=\!\!\!N\), the complexity becomes linear \((N)\).

### Bi-Directional Cross-Attention

Reducing the complexity of attention from quadratic to linear without impairing performance or adding constraints w.r.t. input modalities is one of the main aspects of this work. We build our approach on the previously introduced notion that most data can be interpreted as 'what' and 'where' - and both need to pay attention to the other for optimal information exchange. We represent the 'what' via a small set of \(M\) learnable _latent vectors_ and the 'where' via an input-dependent sequence of \(N\)_tokens_, respectively denoted via the subscripts \(\) and \(\) in the following and with \(M N\). Naively, one could simply apply two individual cross-attention operations sequentially - first querying information from one side and then the other by creating two _query-key-value_ triplets. However, our analyses in Section 3.1 show that symmetric tendencies in the attention patterns between latents and tokens naturally emerge during training, offering a chance to further reduce the computational requirements and to increase efficiency via our _bi-directional cross-attention_ as follows.

We start by creating _reference-value_ pairs \(_{}^{M D},_{}^{M  D}\) and \(_{}^{N D},_{}^{N  D}\) via learnable linear projection from the latent vectors and tokens, respectively. Leveraging symmetry to create bi-directional information exchange, pairwise similarities between latents and tokens are then computed via a scaled dot product as

\[}_{}=(_{}_{} ^{}}{})=}_{}^{}, \]

which is in turn used to obtain the attention-based refinement for both, the latents and tokens, via

\[_{}^{}=}_{}_{}_{}^{ }=}_{} {V}_{}. \]

Note that in addition to providing linear scaling w.r.t. to the input length \(N\), Equation (2) requires evaluating the most computationally-expensive operation, namely the similarity matrix (\((MN)\)), only **once** and allows simultaneous refinement of latents and tokens as defined in Equation (3). The implicit reuse of the _references_ as both _query_ and _key_ further reduces the parameter count of the linear projection matrices by \(}{{3}}\) compared to naive sequential cross-attention.

### BiXT - Bi-Directional Cross-Attention Transformers

Figure 2 (left) illustrates the individual components that make up our BiXT architecture. BiXT is designed in a simple symmetric, ladder-like structure allowing 'what' (latent vectors) and 'where' (tokens) to simultaneously attend to and develop alongside each other - making it equally-well suited for instance-based tasks like classification and dense tasks like semantic segmentation on a variety of input modalities. We start this section with a brief overview, followed by more detailed descriptions of the individual components.

General overview.The raw input data is first passed through a tokenization module which projects the data into an embedding sequence of length \(N\) and optionally adds positional encodings, depending on the input modality and data structure. These tokens together with a fixed set of \(M\) learnable latent vectors are then passed to the first layer's bi-directional cross-attention module for efficient refinement (details depicted in Figure 2 (right) and explained below). The latents are then further refined via latent self-attention, while the tokens are either directly passed on to the next layer (default) or optionally refined by a token refinement module which could include modality-specific components. The simultaneous ladder-like refinement of 'what' and 'where' is repeated for \(L\) layers, before the result is passed to task-specific output head(s). For instance-based tasks like classification, we simply average the set of latent vectors and attach a classification head to the output, while for tasks like segmentation that require outputs resembling the input data structure, the refined tokens are used.

Efficient bi-directional information exchange.We use bi-directional cross-attention introduced in Section 2.2 to enable \(M\) latents and \(N\) tokens to simultaneously attend to each other in a time and memory efficient way, provided \(M N\). The detailed internal structure of our module is depicted in

Figure 2: **BiXT architecture. (left) Input data passing through one layer of our Bi-Directional Cross-Attention Transformer. (right) Internal structure of proposed efficient bi-directional cross-attention.**

Figure 2 (right) and defined via Equations (2) and (3). Apart from the efficient bi-directional attention computation, it follows the common Transformer-style multi-head attention in terms of normalization, activations and processing via feed-forward networks (FFN) introduced by Vaswani et al.  and can thus be easily implemented in modern deep learning frameworks.

Three aspects are particularly worth noting here: 1) While our bi-directional attention imposes a 'hard' structural constraint of symmetry on the pair-wise similarity matrix between tokens and latents as defined in Equation (2), the actual information exchange is less strict: applying the row-wise and column-wise softmax operations to obtain the actual attention maps offers a certain degree of flexibility, since adding a constant to each element in a row keeps the resulting (latent) attention map unchanged while modulating the column-wise (token) one, and vice versa. More specifically, bi-directional CA between \(M\) latents and \(N\) tokens has in total \(M\!N\!-\!1\) degrees of freedom (_dof_), only \((M\!-\!1)\!\!(N\!-\!1)\) of which are shared - leaving \(M+N\!-\!2\)_dof_ that can be used by the network for the modulation of the (non-strictly-symmetric) information exchange (see Appendix A.3 for detailed discussion). 2) Even if the latents and tokens symmetrically attend to each other, the actual information that is transferred is created via individual value projection matrices and thus offers flexibility in terms of content. 3) While tokens cannot directly communicate with each other as is possible when using computationally expensive self-attention, this communication can still take place over two layers in our structure by using a latent vector as temporary storage in a token-latent-token sequence. Since the total number of latents is usually larger than the semantic concepts required to describe one data sample, we can expect this to be possible without impairing performance.

**Latent vector refinement.** After gathering information from the tokens, we use one multi-head self-attention operation  to further refine the information stored in the latents and provide direct information exchange with a global receptive field across latents. Note that since the number of latents \(M\) is fixed and significantly smaller than the input sequence, this operation is input-length independent and not particularly resource intensive. This step is similar to Perceiver , but we only use one instead of several self-attention operations at each layer.

**Optional token refinement.** In the majority of experiments presented in this paper, we simply pass the tokens returned by the bi-directional cross-attention to the next layer. However, our architectural structure also allows to easily include additional (e.g. data-specific) modules for further refinement in a plug-n-play manner. We demonstrate examples of this in Section 3, where we add a local refinement component exploiting grid-shaped data for semantic segmentation and a data-specific hierarchical grouping module for point cloud shape classification.

**Positional encodings.** We use additive sinusoidal positional encodings  to represent the structure of input data, which is more efficient than learnt encodings for variable input size. For simplicity, we follow previous works  and create the encodings in 32 dimensions per input axis followed by a linear projection into the model's token dimension \(D\). This method is applicable independent of the raw data's dimensions and thus easily handles data ranging from 2D images to 3D or 6D point clouds.

**Input tokenization.** Tokenization can be performed in various ways and is the only input modality-specific component in our architecture, akin to Perceiver-IO's input adapters . For image-based experiments, we follow common practice and use simple linear projection as our default tokenizer to embed image patches. For point cloud data, we simply encode the 3D or 6D points directly into embedding space using our sinusoidal positional encoder. We adhere to the guidelines of Tay et al.  for text-based hierarchical sequence modelling and document retrieval experiments on LRA.

## 3 Experimental Evaluation

The purpose of our investigations presented in the following is twofold: 1) To provide qualitative and quantitative insights into our proposed _bi-directional cross-attention_ and the underlying intuition of symmetry, and 2) to demonstrate how BiXT's ability to efficiently and effectively process longer sequences positively affects various tasks. We focus the majority of our experiments around efficient architectures in the low FLOP, memory and parameter regime, and unless otherwise stated, we use BiXT-\(\!\) with \(64\) latent vectors, embedding dimension \(192\) and \(6\) heads for all attention modules.

### Symmetric Tendencies Emerge when Attending Both Ways

We start by investigating the intuition underlying our work: When describing data like an image by asking '_what_' is in it and '_where_' things are, it intuitively makes sense that these two components are tightly interconnected, and that they will inform _aka_ pay attention to each other. To this end, we set up a naive architecture where latent vectors first query the tokens via cross-attention (CA), followed by the tokens querying the latents (i.e. using independent query-key-value triplets), before a further refinement step of the latent information via one self-attention operation - repeated over multiple layers and trained on ImageNet1K . When looking at the resulting attention patterns depicted in Figure 1, we discover that most latents pay attention to parts of the image representing one specific 'entity' like a building ((b), top-left), a flag ((b), top-right) or parts of the sky ((b), lower-right) - supporting the notion that latent vectors represent 'things'. More interestingly however, we discover in (c) that most of these image regions (tokens) are in turn also paying attention to exactly these latent vectors - showing a roughly symmetric information exchange and providing a qualitative indication that our idea of leveraging symmetry via our bi-directional architecture might be well justified. We additionally visualize the attention patterns after replacing the naive sequential CA through our efficient bi-directional one in (d), and the results look surprisingly similar - clearly indicating that our symmetrically constrained approach can achieve similar information exchange while being significantly more efficient.

### Attention - Iterative, Sequential or Bi-directional?

We aim to provide conclusive insights about the two major advantages of our proposed bi-directional attention compared to Perceiver's iterative attention: 1) Higher performance for comparable numbers of FLOPs, and 2) Ability to optionally extend the architecture via modality-specific components. We therefore choose two tasks that have also been investigated in the Perceiver paper: Image classification on ImageNet1K  and point cloud shape classification on ModelNet40 .

ImageNet classification.To provide a fair basis for comparison, we create a range of architectural configurations with iterative attention based on the insights reported by Jaegle et al. . Targeting a similar FLOP count as our BiXT tiny, we experiment with different numbers of layers, varying numbers of self-attention operations per block and with sharing all CA parameters as well as all but the first layer's (for details, see Perceiver paper and our appendix) - yielding a total of 10 architectures based on Perceiver's iterative attention. Having optimized the hyperparameters (learning rate and schedule) for each individually, we run 3 randomly seeded training runs for the best 5 configurations and report their results after training for 120 epochs in Table 1 (a) together with BiXT and the naive sequential CA variant. It is apparent that removing the bottleneck of iterative attention significantly boosts the performance, with both BiXT and sequential CA outperforming all iterative variants by a significant margin at comparable FLOP counts. Interestingly, we find the configuration with 8 blocks and 6 self-attention layers per block (sa6-d8) to achieve best performance among the iterative variants, which aligns with the 'best' configuration reported by Jaegle et al. .

Contrasting the two CA-based approaches with identical numbers of layers ('_d12_') demonstrates the clear advantage of our proposed _bi-directional CA_, requiring \(\)\(7\%\) fewer FLOPs, \(\)\(15\%\) less memory and \(5\%\) fewer parameters to achieve similar results as the sequential variant. This allows BiXT to use one additional layer at matching FLOP count, consistently outperforming the naive approach across all our experiments while being still \(7\)-\(8\%\) more memory efficient.

Table 1: **Bi-directional vs. iterative attention.** (a) Classification accuracy on ImageNet1K. All architectures use 64 latent vectors and have been trained for 120 epochs with hyperparameters individually optimized. Architectural configurations noted in brackets. †indicates sharing of all, ‡of all but the 1st layer’s cross-attention parameters. Results reported as mean and (unbiased) std-dev over 3 randomly seeded training runs (see appendix for complete results). (b) Point cloud shape classification on ModelNet40. BiXT without (_naive_) and with modality-specific components.

Point cloud shape classification.To gain further quantitative insights how bi-directional attention affects processing of other modalities, we evaluate our approach on the ModelNet40 dataset . BiXT again clearly outperforms Perceiver in terms of overall accuracy (OA) and is even competitive to other point-based methods like the seminal PointNet  (Figure 2 (b)). In contrast to Perceiver's iterative attention that gathers information exclusively in the latents, BiXT's simultaneous refinement of latents and tokens allows us to easily integrate data-specific modules for token refinement. To gauge the effect, we add the 'affine grouping' module from PointMLP  without and with hierarchical structure (i.e. point reduction). While BiXT is still outperformed by point cloud specific PointMLP, these optional modules help to boost the accuracy by up to \(3.9\%\) while trading off generality.

### Image Classification

Comparison to SOTA.Note that we focus here on efficient Transformer models in the low FLOP and/or parameter regime, with results reported in Table 2. BiXT performs favourably with default and convolutional tokenizer against the other 'vanilla' Transformers, outperforming both versions of DeiT by a significant margin (\(6.2-11.8\%\)) while being \(\!200\) more efficient than Perceiver (IO). These results are highly competitive even when compared to specialized vision-only architectures that leverage complex pyramidal multi-scale techniques, with BiXT outperforming all but one very recent method (which however requires \(29\%\) more FLOPs than our BiXT).

Increasing feature resolution and input size.We keep the patch size fixed to \(16^{2}\) while reducing the stride of our linear patch projector to increase feature resolution (see appendix for ablation on patch sizes vs. stride). Note that our BiXT/4 model can easily process \(3\),\(136\) tokens per \(224^{2}\) image thanks to linear scaling, boosting the top-1 accuracy to \(82.7\%\). Linear scaling also lets us process larger input images more efficiently - which we investigate by fine-tuning on \(384^{2}\) for 30 epochs to reduce the required computational resources. Increasing the input size further notably improves the accuracy across architectures by up to \(2.1\%\), however at the expense of higher FLOP counts. Nevertheless, BiXT shows that it is possible to achieve \(83.1\%\) on ImageNet with only \(15\)M parameters and no vision-specific internals.

Longer sequence beats model size.Most importantly, BiXT is able to _efficiently leverage longer sequences to outperform larger competitors at fewer FLOPs_: The most-recent DeiT3-S achieves \(81.4\%\) (4.6G FLOPs, 22M param), while BiXT obtains \(81.8\%\) at only \(3.6\)G FLOPs & \(15\)M parameters - see Appendix B.1 for further details.

### Dense Tasks - Semantic Image Segmentation & Point Cloud Part Segmentation

Semantic Segmentation.We investigate the transferability of our methods onto semantic image segmentation on ADE20K . We follow common practice and first integrate BiXT pretrained on ImageNet1K together with SemFPN  as decoder. Our vanilla BiXT performs competitively against other methods with similar FLOP counts, while the more vision-specific variant BiXT+LPI with local token refinement is on par with even the improved pyramidal PvTV2 and outperforms the other models of comparable complexity (Table 3). Please refer to Appendix C for more details.

  
**Architecture** &  & **Acc.** \\   _‘Generalists’ – no tokenizer, no vision-specific internals_ &  &  &  \\ Perceiver ImageNet1K  & \(70\)G & \(45\)M & \(78.0\) \\ Perceiver v2  & \(404\)G & \(42\)M & \(78.6\) \\ Perceiver-IO  & \(407\)G & \(48\)M & \(79.0\) \\  _‘vanilla’ – tokenizer, but no vision-specific internals_ &  &  &  \\ Perceiver v2 [Conv] &  &  &  &  \\ Perceiver-IO (Conv) &  &  &  &  \\ DeiT-Tt/16  & \(1.3\)G & \(6\)M & \(72.2\) \\ DeiT3-T1/16  & \(1.3\)G & \(6\)M & \(75.4\) \\ BiXT-T1/16  & \(1.7\)G & \(15\)M & \(80.1\) \\ BiXT-T1/16 (conv) & \(1.7\)G & \(15\)M & \(81.0\) \\  _‘Vision-specific derivatives, incl. multi-scale / pyramidal_ &  &  &  \\ PiT-Tt  & \(0.7\)G & \(5\)M & \(73.0\) \\ PiT-XS  & \(1.4\)G & \(11\)M & \(78.1\) \\ ViL-Ti-APE  & \(1.3\)G & \(7\)M & \(76.3\) \\ ViL-Ti-RPB  & \(1.3\)G & \(7\)M & \(76.7\) \\ PVTV1-Ti  & \(1.9\)G & \(13\)M & \(75.1\) \\ PVTV2-B1  & \(2.1\)G & \(13\)M & \(78.7\) \\ XCIT-T1  & \(2.1\)G & \(7\)M & \(77.1\) \\ XCIT-T24  & \(2.3\)G & \(12\)M & \(79.4\) \\ BiFormer  & \(2.2\)G & \(13\)M & \(81.4\) \\    
   _Going, fine w / BiXT – smaller patches, larger images_ &  &  &  &  \\ BiXT-Ti/8  & \(4.7\)G & \(15\)M & \(81.9\) \\ BiXT-T1/4 & \(16.8\)G & \(15\)M & \(82.7\) \\ BiXT-T1/16  & \(3.6\)G & \(15\)M & \(81.8\) \\ BiXT-T1/8  & \(12.5\)G & \(15\)M & \(82.8\) \\ BiXT-T1/4  & \(15.0\)G & \(15\)M & \(83.1\) \\   

Table 2: **Classification on ImageNet1K using _‘few-FLOP’_ Transformers.** Note that we focus here on efficient models in the low FLOP and/or parameter regime. Perceiver architectures are included as contrast to our bi-directional attention. All methods have been trained on input resolutions of \(224^{2}\), and \(\)\(384\) further fine-tuned on \(384^{2}\). Note that different models may have received a different optimization effort. ‘result reproduced as not reported in original work. ‘_(conv)_’ indicates the use of a convolutional tokenizer (see appendix for details).

Point Cloud Part Segmentation.Since BiXT provides a similar generality as Perceiver regarding its input data structure but additionally allows the use of the dense, local token information, we determine its suitability for the segmentation of parts of a point cloud on ShapeNetPart . The naive application of BiXT with a linear classifier directly applied to the last layer's tokens achieves a competitive class mIoU of \(83.5\%\) and outperforms other'simple' methods like seminal PointNet  (class mIoU of \(80.4\%\)), but lags slightly behind recent more complex encoder-decoder methods like PointMLP  (class mIoU of \(84.6\%\)). Including a modality-specific token-refinement module and decoder however closes the gap and lets BiXT obtain a highly competitive class mIoU of \(84.7\%\) - as always trading off performance and generality. Please refer to Appendix D for more detailed results.

### Beyond Visual Perception: Hierarchical Sequence Modeling and Document Retrieval

Up to this point, we have demonstrated BiXT's advantages on perception tasks centered around visual and 3D-structural reasoning. We now go one step further and investigate whether our claim of 'BiXT performing at the same level as a full Transformer while being more efficient' holds on tasks that are proven to require modeling of and reasoning over very long and often complex sequences. We evaluate the two tasks from the LRA benchmark with the 'longest required attention span' : _hierarchical sequence modeling_ using Long-ListOps , and _byte-level document retrieval_ using AAN . Long-ListOps tests the ability to reason hierarchically over complex sequences composed of numbers, mathematical operators and brackets - requiring models to access all tokens and model the logical structure of inputs. 'Retrieval' evaluates the ability to encode and compress sequences of 4k length for matching and retrieval, requiring reasoning over 8k tokens in total. To allow fair comparison, we follow the setup in , and train both a full Transformer model and our BiXT variant for 5 random seeds each. While both models are on par in terms of accuracy, BiXT requires up to \(28\%\)_fewer_ FLOPs and is up to \(8.4\) faster - clearly supporting our claim of significantly improving the efficiency for processing long sequences (Table 4). For additional details, please refer to the discussion in Appendix E.

    &  &  &  &  \\  &  & \)) \(\)**} &  &  \\    \\ Transf. & \(39.10\)\(\)0.57 & \(137\) & \(5175\) & \(5357\) \\ BiXT & \(39.42\)\(\)0.24 & \(103\) (-25\%) & \(16891\) (\(3.3\)) & \(23804\) (\(44.4\)) \\    \\ Transf. & \(82.34\)\(\)0.31 & \(535\) & \(751\) & \(751\) \\ BiXT & \(82.46\)\(\)0.41 & \(384\) (-28\%) & \(5703\) (\(7.6\)) & \(6325\) (\(8.4\)) \\   

Table 4: **Hierarchical Sequence Modeling and Document Retrieval** using the LRA benchmark . Samples per second indicate empirical throughput at inference time for varying specified batch sizes ‘bs’ (using one Nvidia A100).

  
**Backbone** & **FLOPs** & **\#Param** & **mIoU.** \\   _Using the Semantic FFN decoder _ & & & \\ PVTv2-BO ,  & \(25.0\)G & \(8\)M & \(37.2\) \\ ResNet18  & \(32.2\)G & \(16\)M & \(32.9\) \\ PVTv1-Ti  & \(33.2\)G & \(17\)M & \(35.7\) \\ PVTv2-Bi  & \(34.2\)G & \(18\)M & \(42.5\) \\ XGT1-T12  & – & SM & \(38.1\) \\ BiXT-Ti/16 (37) & \(31.8\)G & \(19\)M & \(39.2\) \\ BiXT-Ti/16 (37) & \(31.8\)G & \(19\)M & \(41.4\) \\ BiXT-Ti/16 (37) & \(32.4\)G & \(19\)M & \(42.4\) \\  _Simple linear predictor_ & & & \\ BiXT-Ti/16 & \(6.4\)G & \(15\)M & \(40.6\) \\ BiXT-Ti/16 (37) & \(6.4\)G & \(15\)M & \(42.3\) \\ BiXT-Ti/8 & \(23.2\)G & \(15\)M & \(42.1\) \\ BiXT-Ti/8 (37) & \(23.2\)G & \(15\)M & \(43.2\) \\   

Table 3: **Semantic Segmentation on ADE20K.** We again focus here on efficient models in the low FLOP and/or parameter regime. All methods trained on \(512^{2}\) images, and FLOPs are computed on \(512^{2}\) images as well.

### Scaling Trends - Number of Latents & Dimensions

The majority of this paper is concerned with tiny efficient models; however, it is interesting to see whether our models follow previous Transformers in terms of scaling behavior. BiXT offers an additional degree of freedom in the number of latents. We therefore provide some insights into BiXT's ImageNet1K performance changes for \(32,64\) and \(128\) latents as well as various embedding dimensions (Figure 3). As expected, accuracy increases with both larger embedding dimension and number of latents - and it is worth noting that increasing the number of latents scales quadratically in FLOPs due to the self-attention-based latent refinement while increasing the sequence length scales linearly. Note that we use shorter training schedules for this ablation, and results are intended to be interpreted relative to each other. While we chose not to run excessive hyperparameter optimization and refrain from translating to very large architectures due to the large computational requirements involved, we did not observe any signs why BiXT should not behave like other Transformer architectures in terms of scaling and performance. We therefore anticipate to see similar tendencies as reported for related attention-based architectures, but leave this to future work.

### Limitations & Discussion

Our results obtained from the investigation of iterative vs. bi-directional attention as well as our experiments across multiple tasks and modalities clearly show that bi-directional attention offers advantages in a number of settings, both in terms of performance and efficiency. However, it is worth noting that by simultaneously refining the tokens alongside the latents, BiXT does not decouple the model's depth from the input, unlike Perceiver models . Therefore, very deep BiXT variants might potentially face difficulties in settings of extremely long sequences paired with limited compute and memory. However, we suspect most such scenarios to benefit from some form of preprocessing via a modality-specific input tokenizer, similar to the input-adapter-based concept used in Perceiver-IO  - shifting most applications again into regions where BiXT performs effectively and efficiently.

Given the current popularity of natural language processing tasks, we would further like to note that BiXT in its current form is an encoder-based architecture (similar to BERT-like models), and we expect it to perform well on tasks that require understanding and modeling of entire sequences - which is what our results obtained in Section 3.5 / Table 4 on the LRA tasks indicate. However, as BiXT circumvents the expensive token self-attention of Transformers via our proposed bi-directional cross-attention, causal masking as commonly used in decoder-only methods for generative language tasks is not directly applicable to BiXT's current attention mechanism, as information from later tokens would be able to 'leak' to earlier ones via the latent refinement. One possibility to establish causality in this setup _could be_ to assign groups of tokens to specific latents by masking the bi-directional cross-attention and latent refinement accordingly (while trading off some processing resolution at training time), but we expect there to be numerous potential ways and leave this as an interesting area for future follow-up research.

Figure 3: **Scaling trends.** Ablating the influence of embedding dimension, varying numbers of latents and sequence lengths for ImageNet1K classification. All models trained with _shorter_ schedule (only 300 epochs) to save computational resources, and comparisons should therefore be performed _relative_ to each other. Red star-markers correspond to BiXT-Ti/16 (_Acc. 80.1_) from Table 2. Validation accuracy represented through solid lines, while dashed lines indicate the computational resources.

Related work

The introduction of Transformers  has helped _self-attention_ to significantly gain in popularity, despite its caveat of scaling quadratically in computational time and memory with input length. Their flexibility regarding input modality and success in Natural Language Processing (NLP)  and Computer Vision (CV)  prompted a series of works targeting more efficient versions.

Approximating the attention matrix via low-rank factorization has been employed across NLP , CV  and others , essentially avoiding the explicit computation through associativity, estimating a set of bases or using sampling - usually at the expense of performance. Others proposed to use tensor formulations  or exploit the input data structure  under the umbrella of sparsity, however limiting their use to only one specific input modality.

The line of work closest related to ours are'memory-based approaches' which employ some form of global memory to allow indirect interaction between local tokens.  propose to compose various local windowed patterns (sliding, dilated) with global attention on few 'pre-selected' and task-specific input locations for NLP tasks, while its vision derivative  provides global memory as tokens within a vision-pyramid architecture and employs four different pairwise attention operations combined with several sets of global tokens that are discarded at certain stages, introducing rather high architectural complexity.  additionally investigate the encoding of structured NLP inputs, whereas  propose a hand-crafted mix of random, window and global attention to sparsify and thus reduce attention complexity.  route information between selected tokens in a directed graph to achieve sparsity and skip computation in regions deemed irrelevant, whereas  split the input sequence and introduce dedicated latents for each chunk.  in turn use cross-attention-based dual-blocks for efficiency but combine these with merging-blocks that cast attention over the entire concatenated token sequence, introducing a shared representation space and preventing linear scaling. While these ideas of indirect local token communication via a shared global memory align with ours, BiXT realizes this goal in a much simpler and modality-independent manner when compared to the mix of highly modality-specific components, attention patterns and strategies involved in these works. Preserving generality w.r.t. the input,  use a set of learnable 'inducing points' via cross-attention to query input data, while the recent Perceiver architectures  similarly use a fixed set of latents to query input data - yet none offers the efficient simultaneous refinement of latents and tokens realized in our BiXT. Please see Appendix A.5 for some further in-detail discussion and a wider scope of related work.

## 5 Conclusion

In this paper, we presented a novel bi-directional cross-attention Transformer architecture (BiXT) for which computational cost and memory consumption scale linearly with input size, motivated by a naturally emerging symmetry in two-way cross-attention that aligns with common intuition and has been empirically demonstrated in this work. By allowing the 'what' (latent variables) and 'where' (input tokens) to attend to each other simultaneously and develop alongside throughout the architectural stages, BiXT combines Perceiver's linear scaling with full Transformer architectures' high performance in a _best-of-both-worlds_ approach. The ability to efficiently process longer sequences paired with the ease to integrate further domain-specific token refinement modules helps BiXT to outperform larger models on ImageNet1K, be up to \(80\%\) more efficient in semantic image segmentation, competitive across two point-cloud tasks, and on par with full Transformers in sequence modeling and document retrieval while requiring up to \(28\%\) less compute and being up to \(8.4\) faster.