# Simple, Scalable and Effective Clustering via One-Dimensional Projections

Moses Charikar

Stanford University

Stanford, CA

moses@cs.stanford.edu

&Monika Henzinger

Institute of Science and Technology Austria (ISTA)

Klosterneuburg, Austria

monika.henzinger@ist.ac.at

&Lunjia Hu

Stanford University

Stanford, CA

lunjia@stanford.edu

&Maximilian Votsch

Faculty of Computer Science

Doctoral School of Computer Science DoCS Vienna

University of Vienna

Vienna, Austria

maximilian.voetsch@univie.ac.at

&Erik Waingarten

Department of Computer and Information Sciences

University of Pennsylvania

Philadelphia, PA

ewaingar@seas.upenn.edu

###### Abstract

Clustering is a fundamental problem in unsupervised machine learning with many applications in data analysis. Popular clustering algorithms such as Lloyd's algorithm and \(k\)-means++ can take \((ndk)\) time when clustering \(n\) points in a \(d\)-dimensional space (represented by an \(n d\) matrix \(X\)) into \(k\) clusters. In applications with moderate to large \(k\), the multiplicative \(k\) factor can become very expensive. We introduce a simple randomized clustering algorithm that provably runs in expected time \(O((X)+n n)\) for arbitrary \(k\). Here \((X)\) is the total number of non-zero entries in the input dataset \(X\), which is upper bounded by \(nd\) and can be significantly smaller for sparse datasets. We prove that our algorithm achieves approximation ratio \((k^{4})\) on any input dataset for the \(k\)-means objective. We also believe that our theoretical analysis is of independent interest, as we show that the approximation ratio of a \(k\)-means algorithm is approximately preserved under a class of projections and that \(k\)-means++ seeding can be implemented in expected \(O(n n)\) time in one dimension. Finally, we show experimentally that our clustering algorithm gives a new tradeoff between running time and cluster quality compared to previous state-of-the-art methods for these tasks.

## 1 Introduction

Clustering is an essential and powerful tool for data analysis with broad applications in computer vision and computational biology, and it is one of the fundamental problems in unsupervised machine learning. In large-scale applications, datasets often contain billions of high-dimensional points. Grouping similar data points into clusters is crucial for understanding and organizing datasets. Because of its practical importance, the problem of designing efficient and effective clustering algorithms has attracted the attention of numerous researchers for many decades.

One of the most popular algorithms for the \(k\)-means clustering problem is Lloyd's algorithm , which seeks to locate \(k\) centers in the space that minimize the sum of squared distances from the points of the dataset to their closest center (we call this the "\(k\)-means cost"). While finding the centers minimizing the objective is NP-hard , in practice we can find high-quality sets of centers using Lloyd's iterative algorithm. Lloyd's algorithm maintains a set of \(k\) centers. It iteratively updates them by assigning points to one of \(k\) clusters (according to their closest center), then redefining the center as the points' center of mass. It needs a good initial set of centers to obtain a high-quality clustering and fast convergence. In practice, the \(k\)-means++ algorithm , a randomized _seeding_ procedure, is used to choose the initial \(k\) centers. \(k\)-means++ achieves an \(O( k)\)-approximation ratio in expectation, upon which each iteration of Lloyd's algorithm improves.1 Beyond their effectiveness, these algorithms are simple to describe and implement, contributing to their popularity.

The downside of these algorithms is that they do not scale well to massive datasets. A standard implementation of an iteration of Lloyd's algorithm needs to calculate the distance to each center for each point in the dataset, leading to a \((ndk)\) running time. Similarly, the standard implementation of the \(k\)-means++ seeding procedure produces \(k\) samples from the so-called \(D^{2}\) distribution (see Appendix A for details). Maintaining the distribution requires making a pass over the entire dataset after choosing each sample. Generating \(k\) centers leads to a \((ndk)\) running time. Even for moderate values of \(k\), making \(k\) passes over the entire dataset can be prohibitively expensive.

One particularly relevant application of large-scale \(k\)-means clustering is in approximate nearest neighbor search  (for example, in product quantization  and building inverted file indices ). There, \(k\)-means clustering is used to compress entire datasets by mapping vectors to their nearest centers, leading to billion-scale clustering problems with large \(k\) (on the order of hundreds or thousands). Other applications on large datasets requiring a large number of centers may be spam filtering , near-duplicate detection , and compression or reconciliation tasks . New algorithmic ideas are needed for these massive scales, and this motivates the following challenge:

Can we design a simple, practical algorithm for \(k\)-means that runs in time roughly \(O(nd)\), independent of \(k\), and produces high-quality clusters?

Given its importance in theory and practice, a significant amount of effort has been devoted to algorithms for fast \(k\)-means clustering. We summarize a few of the approaches below with the pros and cons of each so that we may highlight our work's position within the literature:

1. **Standard \(k\)-means++:** This is our standard benchmark. **Plus:** Guaranteed to be an \(O( k)\)-approximation ; outputs centers, as well as the assignments of dataset points to centers. **Minus:** The running time is \(O(ndk)\), which is prohibitively expensive in large-scale applications.
2. **Using Approximate Nearest Neighbor Search**: One may implement \(k\)-means++ faster using techniques from approximate nearest neighbor search (instead of a brute force search each iteration). **Plus:** The algorithms with provable guarantees, like , obtain an \(O_{}( k)\)-approximation. **Minus:** The running time is \((nd+(n())^{1+})\), depending on a dataset dependent parameter \(\), the ratio between the maximum and minimum distances between input points. The techniques are algorithmically sophisticated and incur extra poly-logarithmic factors (hidden in \(()\)), making the implementation significantly more complicated.
3. **Approximating the \(D^{2}\)-Distribution**: Algorithms that speed up the seeding procedure for Lloyd's algorithm or generate fast coresets (we expand on this below) have been proposed in . **Plus:** These algorithms are fast, making only one pass over the dataset in time \(O(nd)\). (For , there is an additional additive \(O(k^{2}d)\) term in the running time). **Minus:** The approximation guarantees are qualitatively weaker than the approximation of \(k\)-means clustering. They incur an additional additive approximation error that grows with the entire dataset's variance (which can lead to an arbitrarily large error; see Section 3). These algorithms output a set of \(k\) centers but not the cluster assignments. Naively producing the assignments would take time \(O(ndk)\).2Coresets.At a high level, coresets are a dataset-reduction mechanism. A large dataset \(X\) of \(n\) points in \(^{d}\) is distilled into a significantly smaller (weighted) dataset \(Y\) of \(m\) points in \(^{d}\), called a "coreset" which serves as a good proxy for \(X\), i.e., the clustering cost of any \(k\) centers on \(Y\) is approximately the cost of the same centers on \(X\). We point the reader to  for a recent survey on coresets. Importantly, coreset constructions (with provable multiplicative-approximation guarantees) require an initial approximate clustering of the original dataset \(X\). Therefore, any fast algorithm for \(k\)-means clustering automatically speeds up any algorithmic pipeline that uses coresets for clustering -- looking forward, we will show how our algorithm can significantly speed up coreset constructions without sacrificing approximation.

Beyond those mentioned above, many works seek to speed up \(k\)-means++ or Lloyd iterations by maintaining some nearest neighbor search data structures , or by running some first-order methods . These techniques do not give provable guarantees on the quality of the \(k\)-means clustering or on the running time of their algorithms.

Theoretical Results.We give a simple randomized clustering algorithm with provable guarantees on its running time and approximation ratio without making any assumptions about the data. It has the benefit of being fast (like the algorithms in Category C above) while achieving a multiplicative error guarantee without additional additive error (like the algorithms in Category B above).

* The algorithm runs in time \(O(nd+n n)\) irrespective of \(k\). It passes over the dataset once to perform data reduction, which gives the \(nd\) factor plus an additive \(O(n n)\) term to solve \(k\)-means on the reduced data, producing \(k\) centers and cluster assignments. On sparse input datasets, the \(nd\) term becomes \((X)\), where \((X)\) is the number of non-zero entries in the dataset. Thus, our algorithm runs in \(O((X)+n n)\) time on sparse matrices.
* The algorithm is as simple as the \(k\)-means++ algorithm while significantly more efficient. The approximation ratio we prove is \((k)\), which is worse than the \(O( k)\)-approximation achieved by \(k\)-means++ but it is purely multiplicative (see the remark below on improving this to \(O( k)\)). It does not incur the additional additive errors from the fast algorithms in .

Our algorithm projects the input points to a random one-dimensional space and runs an efficient \(k\)-means++ seeding after the projection. For the approximation guarantee, we analyze how the approximation ratio achieved after the projection can be transferred to the original points (Lemma 2.5). We bound the running time of our algorithm by efficiently implementing the \(k\)-means++ seeding in one dimension and analyzing the running time via a potential function argument (Lemma 2.4). Our algorithm applies beyond \(k\)-means to other clustering objectives that sum up the \(z\)-th power of the distances for general \(z 1\), and our guarantees on its running time and approximation ratio extend smoothly to these settings.

Improving the Approximation from \((k)\) to \(O( k)\).The approximation ratio of \((k)\) may seem significantly worse than the \(O( k)\) approximations achievable with \(k\)-means++. However, we can improve this to \(O( k)\) with an additional, additive \(O((kd) n)\) term in the running time. Using previous results discussed in Appendix A.2 (specifically Theorem A.2), a multiplicative \((k)\)-approximation suffices to construct a coreset of size \((kd)\) and run \(k\)-means++ on the coreset. Constructing the coreset is simple and takes time \((kd) n\) (by sampling from an appropriate distribution); running \(k\)-means++ on the coreset takes \((kd)\) time (with no dependence on \(n\)). Combining our algorithm with coresets, we get a \(O( k)\)-approximation in \(O((X))+O(n n)+(kd) n\) time. Notably, these guarantees cannot be achieved with the additive approximations of .

Experimental Results.We implemented our algorithm, as well as the lightweight coreset of  and \(k\)-means++ with sensitivity sampling . We ran two types of experiments, highlighting various aspects of our algorithm. We provide our code in the supplementary material. The two types of experiments are:

* **Coreset Construction Comparison**: First, we evaluate the performance of our clustering algorithm when we use it to construct coresets. We compare the performance of our algorithm to \(k\)-means++ with sensitivity sampling  and lightweight coresets . In real-world, high-dimensional data, the cost of the resulting clusters from the three algorithms is roughly the same. However, ours and the lightweight coresets can be significantly faster (ours is up to **190x** faster than \(k\)-means++,see Figure 2 and Table 1). The lightweight coresets can be faster than our algorithm (between 3-5x); however, our algorithm is "robust" (achieving multiplicative approximation guarantees).3 Additionally, we show that the clustering from lightweight coresets can have an arbitrarily high cost for a synthetic dataset. On the other hand, our algorithm achieves provable (multiplicative) approximation guarantees irrespective of the dataset (this is demonstrated in the right-most column of Figure 2). * **Direct k-means++ comparison**: Second, we compare the speed and cost of our algorithm to k-means++ as a stand-alone clustering algorithm (we also compare two other natural variants of our algorithm). Our algorithm can be up to **800x** faster than \(k\)-means++ for \(k=5000\) and our slowest variant up to **100x** faster (Table 1). The cost of the cluster assignments can be significantly worse than that of \(k\)-means++ (see Figure 3). Such a result is expected since our theoretical results show a \((k)\)-approximation. The other (similarly) fast algorithms (based on approximating the \(D^{2}\)-distribution) which run in time \(O(nd)\) do not produce the cluster assignments (they only output \(k\) centers). These algorithms would take \(O(ndk)\) time to find the cluster assignments -- this is precisely the computational cost our algorithm avoids.

We do not compare our algorithm with  nor implement approximate nearest neighbor search to speed up \(k\)-means++ for the following reasons. The algorithm in  is significantly more complicated, and there is no publicly available implementation. In addition, both  and approximate nearest neighbor search incur additional poly-logarithmic (or even \(n^{o(1)}\)-factors for nearest neighbor search over \(_{2}\)) which add significant layers of complexity to the implementation and make a thorough evaluation of the algorithm significantly more complicated. Instead, our current implementation demonstrates that a simple, one-dimensional projection and \(k\)-means++ on the line enables dramatic speedups to coreset constructions without sacrificing approximation quality.

Related Work.Efficient algorithms for clustering problems with provable approximation guarantees have been studied extensively, with a few approaches in the literature. There are polynomial-time (constant) approximation algorithms (an exponential dependence on \(k\) is not allowed) (see  for some of the most recent and strongest results), nearly linear time \((1)\)-approximations with running time exponential in \(k\) which proceed via coresets (see  and references therein, as well as the surveys ), and nearly-linear time \((1)\)-approximations in fixed / low-dimensional spaces . Our \(O(n n)\)-expected-time implementation of \(k\)-means++ seeding achieves an \(O( k)\) expected approximation ratio for \(k\)-median and \(k\)-means in one dimension. We are unaware of previous work on clustering algorithms running in time \(O(n n)\).

Another line of research has been on dimensionality reduction techniques for \(k\)-means clustering. Dimensionality reduction can be achieved via PCA based methods , or random projection . For random projection methods, it has been shown that the \(k\)-means objective is preserved up to small multiplicative factors when projecting onto \(O_{}((k))\) dimensional space. Additional work has shown that dimensionality reduction can be performed in \(O((A))\) time . To the best of our knowledge, we are the first to show that clustering objectives such as \(k\)-median and \(k\)-means are preserved up to a \((k)\) factor by one-dimensional projections.

Some works show that the \(O( k)\) expected approximation ratio for \(k\)-means++ can be improved by adding local search steps after the seeding procedure . In particular, Choo et al.  showed that adding \( k\) local search steps achieves an \(O(1/^{3})\) approximation ratio with high probability.

Several other algorithmic approaches exist for fast clustering of points in metric spaces. These include density-based methods like DBSCAN  and DBSCAN++  and the line of heuristics based on the Partitioning Around Medoids (PAM) approach, such as FastPAM , Clarans , and BanditPAM . While these algorithms can produce high-quality clustering, their running time is at least linear in the number of clusters (DBSCAN++ and BanditPAM) or superlinear in the number of points (DBSCAN, FastPAM, Clarans).

Overview of Our Algorithm and Proof Techniques

Our algorithm, which we call PRONE (PRojected ONE-dimensional clustering), takes a random projection onto a one-dimensional space, sorts the projected (scalar) numbers, and runs the \(k\)-means++ seeding strategy on the projected numbers. By virtue of its simplicity, the algorithm is scalable and effective at clustering massive datasets. More formally, PRONE receives as input a dataset of \(n\) points in \(^{d}\), a parameter \(k\) (the number of desired clusters), and proceeds as follows:

1. Sample a random vector \(v^{d}\) from the standard Gaussian distribution and project the data points to one dimension along the direction of \(v\). That is, we compute \(x^{}_{i}= x_{i},v\) in time \(O((X))\) by making a single pass over the data, effectively reducing our dataset to the collection of one-dimensional points \(x^{}_{1},,x^{}_{n}\).
2. Run \(k\)-means++ seeding on \(x^{}_{1},,x^{}_{n}\) to obtain \(k\) indices \(j_{1},,j_{k}[n]\) indicating the chosen centers \(x^{}_{j_{1}},,x^{}_{j_{k}}\) and an assignment \(:[n][k]\) assigning point \(x^{}_{i}\) to center \(x^{}_{j_{(i)}}\). Even though \(k\)-means++ seeding generally takes \(O(nk)\) time in one dimension, we give an efficient implementation, leveraging the fact that points are one-dimensional, which runs in \(O(n n)\) expected time, independent of \(k\). A detailed algorithm description is in the appendix.
3. The one-dimensional \(k\)-means++ algorithm produces a collection of \(k\) centers \(x_{j_{1}},,x_{j_{k}}\), as well as the assignment \(\) mapping each point \(x_{i}\) to the center \(x_{j_{(i)}}\). For each \([k]\), we update the cluster center for cluster \(\) to be the center of mass of all points assigned to \(x_{j_{}}\).

While the algorithm is straightforward, the main technical difficulty lies in the analysis. In particular, our analysis (1) bounds the approximation loss incurred from the one-dimensional projection in Step 1 and (2) shows that we can implement Step 2 in \(O(n n)\) expected time, as opposed to \(O(nk)\) time. We summarize the theoretical contributions in the following theorems.

**Theorem 2.1**.: _The algorithm_ PRONE _has expected running time \(O((X)+n n)\) on any dataset \(X=\{x_{1},,x_{n}\}^{d}\). Moreover, for any \((0,1/2)\) and any dataset \(X\), with probability at least \(1-\), the algorithm runs in time \(O((X)+n(n/))\)._

**Theorem 2.2**.: _The algorithm_ PRONE _achieves an \((k^{4})\) approximation ratio for the \(k\)-means objective with probability at least \(0.9\)._

To our knowledge, PRONE is the first algorithm for \(k\)-means running in time \(O(nd+n n)\) for arbitrary \(k\). As mentioned in the paragraph on improving the competitive ratio, we obtain the following corollary of the previous two theorems using a two-stage approach with a coreset:

**Corollary 2.3**.: _By using_ PRONE _as the \(\)-approximation algorithm in Theorem A.2 and running \(k\)-means++ on the resulting coreset, we obtain an algorithm with an approximation ratio of \(O( k)\) that runs in time \(O((X)+n n+(kd) n)\), with constant success probability._

Due to space constraints, all proofs of our theoretical results are deferred to the appendix, where we also generalize them beyond \(k\)-means to clustering objectives that sum up the \(z\)-th power of Euclidean distances for general \(z 1\). The following subsections give a high-level overview of the main techniques we develop to prove our main theorems above.

### Efficient Seeding in One Dimension

The \(k\)-means++ seeding procedure has \(k\) iterations, where a new center is sampled in each iteration. Since a new center may need to update \((n)\) distances to maintain the \(D^{2}\) distribution, which samples each point with probability proportional to its distance to its closest center, a naive analysis leads to a running time of \(O(nk)\). A key ingredient in the proof of Theorem 2.1 is showing that, for one-dimensional datasets, \(k\)-means++ only needs to make \(O(n n)\) updates, irrespective of \(k\).

**Lemma 2.4**.: _The \(k\)-means++ seeding procedure can be implemented in expected time \(O(n n)\) in one dimension. Moreover, for any \((0,1/2)\), with probability at least \(1-\), the implementation runs in time \(O(n(n/))\)._

The intuition of the proof is as follows: Since points are one-dimensional, we always maintain them in sorted order. In addition, each data point \(x_{i}\) will maintain its center assignment and distance \(p_{i}\) to the closest center. By building a binary tree over the sorted points (where internal nodes maintain sums of \(p_{i}^{2}\)'s), it is easy to sample a new center from the \(D^{2}\) distribution in \(O( n)\) time. The difficulty is that adding a new center may result in changes to \(p_{i}\)'s of multiple points \(x_{i}\), so the challenge is to bound the number of times these values are updated (see Figure 1 below).

To bound the total running time, we leverage the one-dimensional structure. Observe that, for a new center, the updated points lie in a contiguous interval around the newly chosen center. Once a center is chosen, the algorithm scans the points (to the left and the right) until we reach a point that does not need to be updated. This point identifies that points to the other side of it need not be updated, so we can get away without necessarily checking all \(n\) points (see Figure 1). Somewhat surprisingly, when sampling centers from the \(D^{2}\)-distribution, the expected number of times that each point will be updated is only \(O( n)\), which implies a bound of \(O(n n)\) on the total number of updates in expectation. The analysis of the fact that each point is updated \(O( n)\) times is non-trivial and uses a carefully designed potential function (Lemma C.5).

### Approximation Guarantees from One-Dimensional Projections

Our proof of Theorem 2.2 builds on a line of work studying randomized dimension reduction for clustering problems [14; 21; 11; 53]. Prior work studied randomized dimension reduction for accurate \((1)\)-approximations. Our perspective is slightly different; we restrict ourselves to one-dimensional projections and give an upper bound on the distortion.

For any dataset \(x_{1},,x_{n}^{d}\), a projection to a random lower-dimensional space affects the pairwise distance between the projected points in a predictable manner -- the Johnson-Lindenstrauss lemma which projects to \(O( n)\) dimensions being a prime example of this fact. When projecting to just one dimension, however, pairwise distances will be significantly affected (by up to \((n)\)-factors). Thus, a naive analysis will give a \((n)\)-approximation for \(k\)-means. To improve a \(c\)-approximation to a \(O( k)\)-approximation, one needs a coreset of size roughly \((c/ k)\). This bound becomes vacuous when \(c\) is polynomial in \(n\) since there are at most \(n\) dataset points.

However, although many pairwise distances are significantly distorted, we show that the \(k\)-means cost is only affected by a \((k)\)-factor. At a high level, this occurs because the \(k\)-means cost optimizes a sum of pairwise distances (according to a chosen clustering). The individual summands, given by pairwise distances, will change significantly, but the overall sum does not. Our proof follows the approach of , which showed that (roughly speaking) pairwise distortion of the \(k\) optimal centers suffices to argue about the \(k\)-means cost. The \(k\) optimal centers will incur maximal pairwise distortion \((k)\) when projected to one dimension (because there are only \(O(k^{2})\) pairwise distances among the \(k\) centers). This allows us to lift an \(r\)-approximate solution after the projection to an \(O(k^{4}r)\)-approximate solution for the original points.

**Lemma 2.5** (Informal).: _For any set \(X\) of points in \(^{d}\), the following occurs with probability at least \(0.9\) over the choice of a standard Gaussian vector \(v^{d}\). Letting \(X^{}\) be the one-dimensional projection of \(X\) onto \(v\), any \(r\)-approximate \(k\)-means clustering of \(X^{}\) gives an \(O(k^{4}r)\)-approximate clustering of \(X\) with the same clustering partition._

## 3 Experimental Results

In this section, we outline the experimental evaluation of our algorithm. The experiments evaluate the algorithms in two different ways. For each, we measure the running time and the \(k\)-means cost of the resulting solution (the sum of squares of point-to-center-assigned distances). (1) First, we evaluate our algorithm as part of a pipeline incorporating a coreset construction - the expected use case for our algorithm. (2) Second, we evaluate our algorithm by itself for approximate k-means clustering and compare it to k-means++ . As per Theorems 2.1 and 2.2, we expect our algorithm to be much faster but output an assignment of higher cost. Our goal is to quantify these differences empirically.

Figure 1: From the top to the bottom, a new center (black circle) is chosen. Every point has an arrow pointing to its closest center. The points in the dashed box are the ones that require updates.

All experiments were run on Linux using a notebook with a 3.9 GHz 12th generation Intel Core i7 six-core processor and 32 GiB of RAM. All algorithms were implemented in C++, using the blaze library for matrix and vector operations performed on the dataset unless specified differently below. We provide our code in the supplementary material for this submission.

**Datasets.** For our experiments, we use the following four datasets:

**KDD**: Training data for the 2004 KDD challenge on protein homology. The dataset consists of \(145751\) observations with \(77\) real-valued features.

**Song**: Timbre information for \(515345\) songs with \(90\) features each, used for year prediction.

**Census**: 1990 US census data with \(2458285\) observations, each with \(68\) categorical features.

**Gaussian**: A synthetic dataset consisting of \(240005\) points of dimension \(4\). The points are generated by placing a standard normal distribution at a large positive distance from the origin on each axis and sampling \(30000\) points. The points are then mirrored so the center of mass remains at the origin. Finally, 5 points are placed on the origin. This is an adversarial example for lightweight coresets , which are unlikely to sample points close to the mean of the dataset.

### Coreset Construction Comparison

Experimental Setup.Coreset constructions (with multiplicative approximation guarantees) always proceed by first finding an approximate clustering, which constitutes the bulk of the work. The approximate clustering defines a "sensitivity sampling distribution" (we expand on this in the appendix, see also ), and a coreset is constructed by repeatedly sampling from the sensitivity sampling distribution. In our first experiment, we evaluate the choice of initial approximation algorithm used to define the sensitivity sampling distribution. We compare the use of \(k\)-means++ and PRONE. In addition, we also compare the lightweight coresets of , which uses the distance to the center of mass as an approximation of the sensitivity sampling distribution. For the remainder of this section, we refer to sensitivity sampling using k-means++ as _Sensitivity_ and lightweight coresets as _Lightweight_. All three algorithms produce a coreset, and the experiment will measure the running time of the three algorithms (Table 1) and the quality of the resulting coresets (Figure 2). We implemented all algorithms in C++.

Once a coreset is constructed for each of the algorithms, we evaluate the quality of the coreset by computing the cost of the centers found when clustering the coreset (see Definition A.1). We run a state-of-the-art implementation of Lloyd's \(k\)-means algorithm from the scikit-learn library  with the default configuration (repeating 15 times and reporting the mean cost to reduce the variance). The resulting quality of the coresets is compared to a (computationally expensive) _baseline_, which runs k-means++ from the scikit-learn library, followed by Lloyd's algorithm with the default configuration on the entire dataset (repeated 5 times to reduce variance).

We evaluate various choices of \(k\) (\(\{10,100,1000\}\)) as well as coresets at various relative sizes, \(\{0.001,0.0025,0.005,0.01,0.025,0.05,0.1\}\) times the size of the dataset. We use as performance metrics (1) a relative cost, which measures the average cost of the k-means solutions returned by Lloyd's algorithm on each coreset divided by the baseline, and (2) the running time of the coreset construction algorithm.

**Results on Coreset Constructions.**_Relative cost._ Figure 2 shows the coreset size (\(x\)-axis) versus the relative cost (\(y\)-axis). Each "row" of Figure 2 corresponds to a different value for \(k\{10,100,1000\}\), and each "column" corresponds to a different dataset. Recall that the first three datasets (i.e., the first three columns) are real-world datasets, and the fourth column is the synthetic Gaussian dataset. We note our observations below:

* As expected, on all real-world data sets and all settings of \(k\), the relative cost decreases as the coreset size increases.
* In real-world datasets, the specific relative cost of each coreset construction (_Sensitivity_, _Lightweight_, and ours) depends on the dataset4, but roughly speaking, all three share a similar trend. Ours and _Sensitivity_ are very close and never more than twice the baseline (usually much better).

* The big difference, distinguishing ours and _Sensitivity_ from _Lightweight_, is the fourth column, the synthetic Gaussian dataset. For all settings of \(k\), as the coreset size increases, _Lightweight_ exhibits a minimal cost decrease and is a factor of 2.7-17x times worse than ours and _Sensitivity_ (as well as the baseline). This is expected, as we constructed the synthetic Gaussian dataset to have arbitrarily high cost with _Lightweight_. Due to its multiplicative approximation guarantee, our algorithm does not suffer this degradation. In that sense, our algorithm is more "robust," and achieves worst-case multiplicative approximation guarantees for all datasets.

_Running time._ In (the first table in) Table 1, we show the running time of the coreset construction algorithms as \(k\) increases. Notice that as \(k\) increases, the relative speedup of our algorithm and _Lightweight_ increases in comparison to _Sensitivity_. This is because our algorithm and _Lightweight_ have running time which _does not grow with \(k\)_. In contrast, the running time of _Sensitivity_ grows linearly in \(k\). In summary, our coreset construction is between **33-192x** faster than _Sensitivity_ for large \(k\). In addition, our algorithm runs about 3-5x slower than _Lightweight_, depending on the dataset. Our analysis also shows this; both algorithms make an initial pass over the dataset, using \(O(nd)\) time, but ours uses an additional \(O(n n)\) time to process.

### Direct k-Means++ Comparison

Experimental Setup.This experiment compares our algorithm and \(k\)-means++ as a stand-alone clustering algorithm, as opposed to as part of a coreset pipeline. We implemented three variants of our algorithm. Each differs in how we sample the random one-dimensional projection. The first is a one-dimensional projection onto a standard Gaussian vector (zero mean and identity covariance). This approach risks collapsing an "important" feature, i.e. a feature with high variance. To mitigate this, we implemented two _data-dependent_ variants that use the variance, resp. covariance of the data. Specifically, in the "variance" variant, we use a diagonal covariance matrix, where each entry in the diagonal is set to the empirical variance of the dataset along the corresponding feature. In the "covariance" variant, we use the empirical covariance matrix of the dataset. These variants aim to project along vectors that capture more of the variance of the data than when sampling a vector uniformly at random. Intuitively, the vectors sampled by the biased variants are more correlated with the first principal component of the dataset. For each of our algorithms, we evaluate the \(k\)-means cost of the output set \(C\) of centers when assigning points to the closest center (\(_{2}(X,C)\) in Definition A.1) and when using our algorithm's assignment (\(_{2}(X,C,)\) defined in Equation (1)).

We evaluated the algorithms for every \(k\) in \(\{10,25,50,100,250,500,1000,2500,5000\}\) and \(z=2\), for solving \(k\)-means with the \(_{2}\)-metric. When evaluating the assignment cost, we ran each of our

Figure 2: Plot of relative cost versus coreset size on our four datasets. The shaded region indicates standard error. There is no data for relative size values where the coreset size is less than \(k\).

algorithms 100 times for each \(k\) and five times when computing the nearest neighbor assignment, and we report the average cost of the solutions and the average running time. Due to lower variance and much higher runtime, k-means++ was run five times.

Results on Direct \(k\)-Means++ Comparison._Cost._ Figure 3 (on top) shows the cost of the centers found by our algorithm compared to those found by the k-means++ algorithm after computing _the optimal assignment of points to the centers computed by the algorithm_ (computing this takes time \(O(ndk)\)). That is, we compare the values of \(_{2}(X,C)\) in Definition A.1. In summary, the k-means cost of all three variants of our algorithm are roughly the same and closely match that of k-means++. On the Gaussian data set, one run of the biased algorithm failed to pick a center from the cluster at the origin, leading to a high "outlier" cost and a corresponding spike in the plot.

We also compared the k-means cost for the assignment _computed by our algorithm_ (so that our algorithm only takes time \(O(nd+n n)\) and _not_\(O(ndk)\)) with the cost of k-means++ (bottom row of Figure 3). That is, we compare the values of \(_{2}(X,C,)\) defined in Equation (1). The clustering cost of our algorithms is higher than that of k-means++. This is the predicted outcome from our theoretical results; recall Theorem 2.2 gives a \((k)\)-approximation, as opposed to \(O( k)\) from \(k\)-means++. On the real-world data sets, it is between one order of magnitude (for \(k=10\)) and two orders of magnitude (for \(k=5000\)) worse than k-means++ for our unbiased variant and between a factor 2 (for \(k=10\)) and one order of magnitude (for \(k=5000\)) worse than k-means++ for our biased and covariance variants.

    & k & 10 & 100 & 1000 \\ Dataset & Algorithm & & & & \\  Census & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  Census & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\        & k & 50 & 500 & 5000 \\ Dataset & Algorithm & & & & \\  Census & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\        & k & 50 & 500 & 5000 \\ Dataset & Algorithm & & & & \\  Census & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\        & k & 50 & 500 & 5000 \\ Dataset & Algorithm & & & & \\  Census & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\        & k & 50 & 500 & 5000 \\ Dataset & Algorithm & & & & \\  Census & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\        & k & 50 & 500 & 5000 \\ Dataset & Algorithm & & & & \\  Census & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\        & k & 50 & 500 & 5000 \\ Dataset & Algorithm & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\        & k & 50 & 500 & 5000 \\ Dataset & Algorithm & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & \\  & & & & & \\  & & & & \\    
    & k & 50 & 500 & 5000 \\ Dataset & Algorithm & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & \\  & & & & & \\  & & & & & \\  & & & & \\  & & & & & \\   

Table 1: Average speedup over sensitivity sampling across all relative sizes for constructing coresets (in the first table) and average speedup over \(k\)-means++ as a stand-alone clustering algorithm (in the second table). The tables with the full range of parameters can be found in the appendix.

Figure 3: Clustering cost of all our variants compared to \(k\)-means++. The top row shows the \(k\)-means cost, and the bottom row shows the cost of the assignment produced by our algorithm.

_Running time._ Table 1 shows the relative running time of our algorithm compared to k-means++, assuming that no nearest-center assignment is computed. Our algorithms are designed to have a running time independent of \(k\), so we can see, from the second table in Figure 1, all of our variants offer significant speedups.

* The running time of our algorithm stays almost constant as \(k\) increases while the running time of k-means++ scales linearly with \(k\). Specifically for \(k=25\), even our slowest variants have about the same running time as k-means++, while for \(k=5000\), it is at least **82x** faster, and our fastest version is up to **837x** faster over k-means++.
* The two variants can affect the quality of the chosen centers by up to an order of magnitude, but they are also significantly slower. The "variance" and "covariance" variants are slower (between 2-4x slower and up to 10x slower, respectively) than the standard variant, and they also become slower as the dimensionality \(d\) increases. We believe these methods could be further sped up, as the blaze library's variance computation routine appears inefficient for our use case.

## 4 Conclusion and Limitations

To summarize, we present a simple algorithm that provides a new tradeoff between running time and approximation ratio. Our algorithm runs in expected time \(O((X)+n n)\) to produce a \((k)\)-approximation; with additional \((kd) n\) time, we improve the approximation to \(O( k)\). This latter bound matches that of \(k\)-means++ but offers a significant speedup.

Within a pipeline for constructing coresets, our experiments show that the quality of the coreset produced (when using our algorithm as the initial approximation) outperforms the sensitivity sampling algorithm. It is slower than the lightweight coreset algorithm, but it is more "robust" as it is independent of the diameter of the data set. It does not suffer from the drawback of having an additive error linear in the diameter of the dataset, which can arbitrarily increase the cost of the lightweight coreset algorithm. When computing an optimal assignment for the centers returned by our algorithm, its cost roughly matches the cost for k-means++. When directly using the assignment produced by one variant of our algorithm, its cost is between a factor 2 and 10 worse while being up to 300 times faster.

Our experiments and running time analysis show that our algorithm is very efficient. However, the clustering quality achieved by our algorithm is sometimes not as good as other, slower algorithms. We show that this limitation is insignificant when we use our algorithm to construct coresets. It remains an interesting open problem to understand the best clustering quality (e.g., in terms of approximation ratio) an algorithm can achieve while being as efficient as ours, i.e., running in time \(O(nd+n n)\). Another interesting problem is whether other means of projecting the dataset into a \(O(1)\) dimensional space exist, which lead to algorithms with improved approximation guarantees and running time faster than \(O(ndk)\).