# Slack-Free Spiking Neural Network Formulation for Hypergraph Minimum Vertex Cover

Tam Ngoc-Bang Nguyen\({}^{1}\), Anh-Dzung Doan\({}^{1}\), Zhipeng Cai\({}^{2}\), Tat-Jun Chin\({}^{1}\)

\({}^{1}\) Australian Institute for Machine Learning, The University of Adelaide,

\({}^{2}\) Intel Labs

{tam.nb.nguyen,dzung.doan,tat-jun.chin}@adelaide.edu.au

zhipeng.cai@intel.com

###### Abstract

Neuromorphic computers open up the potential of energy-efficient computation using spiking neural networks (SNN), which consist of neurons that exchange spike-based information asynchronously. In particular, SNNs have shown promise in solving combinatorial optimization. Underpinning the SNN methods is the concept of energy minimization of an Ising model, which is closely related to quadratic unconstrained binary optimization (QUBO). Thus, the starting point for many SNN methods is reformulating the target problem as QUBO, then executing an SNN-based QUBO solver. For many combinatorial problems, the reformulation entails introducing penalty terms, potentially with slack variables, that implement feasibility constraints in the QUBO objective. For more complex problems such as hypergraph minimum vertex cover (HMVC), numerous slack variables are introduced which drastically increase the search domain and reduce the effectiveness of the SNN solver. In this paper, we propose a novel SNN formulation for HMVC. Rather than using penalty terms with slack variables, our SNN architecture introduces additional spiking neurons with a constraint checking and correction mechanism that encourages convergence to feasible solutions. In effect, our method obviates the need for reformulating HMVC as QUBO. Experiments on neuromorphic hardware show that our method consistently yielded high quality solutions for HMVC on real and synthetic instances where the SNN-based QUBO solver often failed, while consuming measurably less energy than global solvers on CPU.

## 1 Introduction

Neuromorphic computing research aims to develop computational models inspired by neural architectures found in nature. Spiking neural networks (SNN), in which a network of processing units (neurons) asynchronously transmit spike-based messages to each other , is a notable representative of the neuromorphic paradigm. Through parallelism, stochastic behavior, event-driven computing and other biologically-inspired properties, SNNs promise higher energy efficiency than conventional computing, which includes the successful artificial neural networks (ANN) .

The advent of neuromorphic computing hardware that can implement SNNs has boosted research in the area. Notable examples include IBM TrueNorth  and Intel Loihi . Rigorous benchmarking  indicate the promise of SNNs in delivering energy efficient computations, which not only benefit edge computing applications, but also reducing the energy consumption of data centers. The recent introduction of Intel Hala Point , the world's largest neuromorphic supercomputer with 1.15 billion neurons and 138.2 billion synapses, is a testament of confidence in the technology.

Two major classes of problems have been explored for SNNs: machine learning and combinatorial optimization . Representative approaches in the former include training SNNs using asynchronousvariants of backpropagation, and converting pre-trained ANNs into equivalent SNNs for deployment on neuromorphic hardware. Works in the latter develop SNNs to solve specific optimization problems, where the spike-based temporal information processing is exploited to achieve computational benefits over classical methods. Our work focuses on combinatorial optimization.

The concept of energy minimization of the Ising model underpins SNN techniques for combinatorial optimization. The task is closely related to quadratic unconstrained binary optimization (QUBO)

\[_{\{0,1\}^{N}}^{T}, \]

where \(^{N N}\) is a symmetric coefficient matrix (we restrict \(\) to integers to facilitate the SNN treatment; integers are sufficient nonetheless for the targeted combinatorial problem). Several SNNs have been developed to solve QUBO ; Sec. 3.2 will describe such a method. To bring such SNNs to bear on other combinatorial problems, the problems must first be reformulated into QUBO . For problems with feasibility constraints, penalty terms and often slack variables will be added to the QUBO objective to implement the constraints. Examples are provided below.

**Problem 1** (Minimum vertex cover (MVC)).: Let \(G=(V,E)\) be a graph with vertices \(V=\{1,,N\}\) and edges \(E=\{e_{1},,e_{K}\}\), where each \(e_{k}= i^{(k)},j^{(k)} V\) connects two vertices. A vertex cover of \(G\) is a subset \(Z\) of \(V\) such that every edge is incident with at least a vertex in \(Z\). The goal of MVC is to find the vertex cover with the smallest number of vertices. Let

\[=[z_{1},,z_{N}]^{T}\{0,1\}^{N} \]

be a binary vector whose role is to select a subset of \(V\), where \(z_{i}=1\) indicates that the \(i\)-th vertex is selected, and \(z_{i}=0\) means otherwise. MVC can be expressed as

\[_{\{0,1\}^{N}} \|\|_{1}\] (3) s.t. \[(1-z_{i^{(k)}})(1-z_{j^{(k)}})=0, k=1,,K.\]

Note that \(\|\|_{1}=\|\|_{2}\) for binary \(\) and the equality constraints in (3) are quadratic in \(\). Rewriting the equality constraints as penalty terms, we obtain the QUBO

\[_{\{0,1\}^{N}} \|\|_{2}^{2}+_{k=1}^{K}(1-z_{i^{(k)}})(1-z_ {j^{(k)}}). \]

Intuitively, solutions \(\) that violate the constraints in (3) will raise the total cost in (4) and hence be penalized. The quantity \(\) is the penalty weight that controls the degree of penalization due to constraint violations. Note that while (4) is not exactly in the form (1) due to the existence of a constant, the remaining steps to rearrange (4) to (1) are minor; see supplementary material. 

**Problem 2** (Hypergraph minimum vertex cover (HMVC)).: Let \(H=(V,F)\) be an \(r-\)uniform hypergraph with vertices \(V=\{1,,N\}\) and hyperedges \(F=\{f_{1},,f_{K}\}\), where each \(f_{k} V\) is incident with exactly \(r\) vertices, \(r 2\). A vertex cover of \(H\) is a subset \(Z\) of \(V\) such that every hyperedge is incident with at least a vertex in \(Z\). The goal of HMVC is to find the vertex cover with the smallest number of vertices. Note that HMVC reduces to MVC if \(r=2\). For each hyperedge \(f_{k}\), define a binary vector

\[^{(k)}=[b_{1}^{(k)},,b_{N}^{(k)}]^{T}\{0,1\}^{N}, \]

where \(\|^{(k)}\|_{1}=r\), and \(b_{i}^{(k)}=1\) means that vertex \(i\) is incident to \(f_{k}\) and \(b_{i}^{(k)}=0\) means otherwise. HMVC can then be written as

\[_{\{0,1\}^{N}} \|\|_{1}\] (6) s.t. \[^{(k)^{T}} 1, k=1,,K,\]

which is a 0-1 integer linear program (ILP). Since \(r\) variables (\(r 2\)) exist in each linear inequality constraint, in general it is not possible to express it as a quadratic equality constraint, _cf_. (3). Instead, the path to QUBO will involve converting each inequality into an equality constraint

\[^{(k)^{T}}-_{r^{}}^{T}^{(k)}=1, \]which requires introducing \(r^{}\) binary slack variables for each \(f_{k}\)

\[^{(k)}=[y_{1}^{(k)},,y_{r^{}}^{(k)}]^{T}\{0,1\}^{r^{ }}, \]

where \(r^{}=r-1\) and \(_{r^{}}\) is a column vector of \(1\) of size \(r^{}\). Installing the equality constraints as quadratic penalty terms, we obtain the QUBO

\[_{\{0,1\}^{N},\{^{(k)}\}_{k=1}^{K}\{0,1\}^{r^{ } K}}\|\|_{2}^{2}+_{k=1}^{K}({^{(k)}}^{T}-_{r^{}}^{T}^{(k)}-1)^{2}, \]

where \(\) is the penalty weight. See supp. material for rewriting the QUBO in form (1). 

A major difference between the QUBO reformulations for MVC and HMVC is that the latter requires slack variables, the quantity of which scales linearly with the number \(K\) and degree \(r\) of hyperedges. This significantly increases the search space and difficulty of the optimization. As we will show in Sec. 5, the existing SNN-based QUBO solver  is unable to satisfactorily solve HMVC based on (9). This presents an obstacle towards applying SNNs to a combinatorial problem with many practical applications, _e.g._, computational biology , computer network security , resource allocation  and social network analysis . Fundamentally, HMVC is a general optimization problem that encompasses several related formulations, such as set cover, hitting set, and traversal , giving it wide applicability. The issue calls for a more effective SNN for HMVC.

### Contributions

To solve HMVC more effectively, we propose an SNN that is composed of a mix of non-equilibrium Boltzmann machine (NEBM) spiking neurons  and a custom _feedback_ spiking neuron. One feedback neuron is introduced for each constraint in (6) to check for constraint violations and encourage the overall state to return to feasibility. A major benefit of our handcrafted SNN is obviating the need to reformulate HMVC as QUBO based on the penalty method, which not only avoids the usage of slack variables, but also removes the necessity to tune the penalty weight.

Results on Intel Loihi 2  indicate that our SNN significantly outperformed the QUBO approach on HMVC, in that our method consistently yielded high-quality solutions on synthetic and real problem instances where the SNN-based QUBO solver  often failed to return feasible results. Moreover, our SNN consumed measurably much less energy than global solvers on CPU.

Our work follows the spirit of other handcrafted SNNs for combinatorial optimization [8; 31; 17; 21; 36]. However, such works have mainly been targeted at constraint satisfaction problems, whereas our SNN is aimed at a graph-based optimization problem, _i.e._, HMVC (more details in Sec. 2).

## 2 Related work

Previous studies have shown that an SNN with a topology corresponding to the matrix \(\) can efficiently solve QUBO [3; 6; 14]. This enables neuromorphic computing to solve combinatorial problems that can be encoded as QUBO, such as graph partitioning . In addition, via the penalty method [29; Chap. 17], other combinatorial problems with constraints can be reformulated into QUBO . Problem 1 has discussed this for MVC, which has been evaluated on a neuromorphic processor . Other works that employed QUBO reformulation include [32; 22] who solved maximum independent set and ILP. However, more complex problems will require the introduction of slack variables, which increases the search space and difficulty for an SNN solution. Problem 2 has illustrated this for HMVC.

The majority of handcrafted SNNs for combinatorial optimization aim to solve constraint satisfaction problems (CSPs). Jonke _et al._ proposed a stochastic SNN for traveling salesman problem. Since then, several SNN solvers have been proposed for CSPs, such as, Sudoku [8; 31], graph coloring , and Boolean satisfiability problem [21; 36]. These existing approaches share a common strategy: constructing a specific SNN topology that is strongly tailored to the constraints of each CSP. This is to ensure that these SNN solvers can seek valid values for a set of variables that satisfy the specified constraints. In other words, the primary objective of these handcrafted SNNs is to find feasible solutions to the combinatorial optimization problem. Although our approach shares a similar spirit to these handcrafted SNNs in that we directly construct SNN topology using constraints, our SNN topological graph is specifically designed to seek not only a feasible solution but also the optimal one.

The QUBO formulation is amenable to Ising-specific analog and digital hardware solvers, including spintronics, memristors, and quantum annealers . However, given our focus on SNNs, we leverage the state-of-the-art Loihi 2 neuromorphic accelerator to implement the SNN solutions for both QUBO and our method, due to its high flexibility in SNN design and programmable neuron model. For a comprehensive overview of Loihi 2's capabilities, we refer readers to .

There is a large body of work on SNNs for machine learning; we refer to  for a thorough survey. While SNNs are usually less accurate than ANNs , under certain temporal dynamics, SNNs can surpass ANNs. SNNs have also been used for robotics. For instance,  implemented their SNN-based quadratic programming solver for model predictive control on Loihi 2, achieving two orders-of-magnitude gains in energy-delay product compared to CPU solvers. Another study demonstrated the applications of SNN in optical flow estimation for event cameras , which showed significant potential for real-time operations. In addition, SNNs have been applied to depth estimation for event sensors , where their method computed the optical flow on the neuromorphic chip and integrated the optical flow with camera velocity to estimate depth.

## 3 Preliminaries

An SNN can be viewed as a "program" for a neuromorphic computer . Formally, an SNN comprises a set of spiking neurons \(=\{n_{i}\}_{i=1}^{N}\) and synapses, where each synapse connects a pair of neurons. The interconnections can be summarized by a matrix \(=[w_{ij}]^{N N}\), where the element \(w_{ij}\) indicates the strength of connection between \(n_{i}\) and \(n_{j}\), and \(w_{ij}=0\) signifies an absence of connection between the pair. The architecture of an SNN and the model of the spiking neuron define the behavior of the program. Here, we describe the NEBM spiking neuron model (Sec. 3.1) and the NEBM-based SNN for QUBO (Sec. 3.2), adapted from Intel's original implementation .

### Nebm

NEBM neurons produce outputs probabilistically based on the Boltzmann distribution . An NEBM neuron \(n_{i}\) contains a binary state \(s_{i}\{0,1\}\) that indicates whether the neuron is firing (\(s_{i}=1\)) or not (\(s_{i}=0\)), and an internal state \(u_{i}\) that accumulates outputs from connected neurons.

While in theory spiking neurons operate asynchronously, practical neuromorphic computers such as Intel Loihi are fully digital devices . The continuous time dynamics of a spiking neuron are approximated using fixed-size discrete timesteps \(t\). It should be reminded that \(t\) relates to the algorithmic time of the computation rather than the time of a global synchronous clock.

Figure 1: (a) HMVC input hypergraph: a 3-uniform hypergraph with 4 vertices and 3 hyperedges. (b) SNN for QUBO (9) derived from (a). Both the variables \(z_{i}\) and introduced slack variables are NEBM neurons. The edge weights are omitted for brevity. (c) The SNN from our method for input (a). NEBM and FB neurons are in orange and green resp. Dashed circles indicate self-connections.

Based on the algorithmic time formalism, at timestep \(t\), an NEBM neuron \(n_{i}\) accumulates inputs from connected neurons into its internal state

\[u_{i}^{(t)}=u_{i}^{(t-1)}+_{j i}w_{ij} s_{j}^{(t-1)}, \]

where \( s_{j}^{(t-1)}\) is the output spike of neuron \(n_{j}\) from the previous timestep. The internal state of \(n_{i}\) is then converted to a switching probability

\[Ps_{i}^{(t)}=1=^{(t)}/T}}, \]

where \(T\) is the _temperature_. If the switching probability (11) exceeds a randomly generated threshold \(_{i}\), \(s_{i}\) is set to 1. From here, an output or delta spike is calculated

\[ s_{i}^{(t)}=s_{i}^{(t)}-s_{i}^{(t-1)}. \]

If \( s_{i}^{(t)}\) takes a non-zero value, _i.e._, the state \(s_{i}\) changes from the previous timestep, \(n_{i}\) propagates a delta spike to all its connected neurons and enters a _refractory period_ for \(r_{i}\) timesteps. Within the refractory period, the binary state \(s_{i}\) remains unchanged. Note that temperature \(T\) and length of the refractory period \(r_{i}\) are hyperparameters of NEBM neurons. For more details of NEBM, its hyperparameters and how it is implemented on Loihi 2 given hardware constraints (i.e. no division operator), we refer readers to .

### NEBM-based SNN for QUBO

The energy encoded by the neuronal states in an SNN is

\[=_{i=1}^{N}s_{i}u_{i}. \]

Following , the NEBM-based SNN algorithm to minimize the energy is summarized in Alg. 2 (see Appendix). Intuitively, the method repeatedly samples and explores the neuronal states guided by the structure of the synapses and evolving state values . The algorithm is executed on the neuromorphic hardware for \(M\) algorithmic timesteps, and the state configurations probed at the \(M\) timesteps are returned. The state configurations are evaluated off-chip, with the best one returned as the solution. Note that we employ an early version of NEBM where \(T\) is kept constant in Alg. 2, though \(T\) can be annealed to fine-tune the search. We refer interested readers to  for recent advancements of NEBM and the general-purpose NEBM-based QUBO solver.

To enable Alg. 2 to solve QUBO, following , we perform the following mapping:

* Assign each binary variable \(z_{i}\) to a neuron \(n_{i}\), and equating the state \(s_{i}\) with the value of \(z_{i}\).
* Associate the quadratic coefficients \(=[q_{ij}]^{N N}\) with the synapse weights \(\), _i.e._, \(=\). This also implies the existence of self connections for the neurons, where \(q_{ii}=w_{ii}\) is the strength of the self connection for neuron \(n_{i}\).

Fig. 0(b) illustrates the mapping. Note that the "mapping" does not imply that \(\) equates to the QUBO cost; rather, the minimization of \(\) by Alg. 2 tends to lead to the minimization of the QUBO cost. In this sense, the SNN is a _heuristic_ method to solve QUBO.

## 4 Slack-free SNN formulation for HMVC

Instead of converting HMVC to QUBO following the derivations in Problem 2 and applying the SNN described in Sec. 3.2, we develop a novel _slack-free_ SNN that directly solves HMVC. Referring to the 0-1 ILP (6), we construct our SNN, named _SF-HMVC_, as follows:

* As before, each binary variable \(z_{i}\) is encoded as the state \(s_{i}\) of an NEBM neuron \(n_{i}\).
* Each \(k\)-th constraint \(^{(k)^{T}} 1\) is represented by a neuron \(m_{k}\), whose dynamics are governed by a custom neuron model called _feedback_ or _FB_ (more details below).

* The weight matrix of our SNN which consists of \(N+K\) neurons \([s_{1},,s_{N},m_{1},,m_{K}]\) is \[=&\\ ^{T}&_{K K}^{(N+K)(N +K)},\] (14) where \(_{K K}\) is a \(K K\) matrix of zeros,
* \(=[^{(1)}^{(K)}]\{0,1\}^{N K}\) is the adjacency matrix between NEBM and FB neurons (intuitively, the \(k\)-th FB neuron \(m_{k}\) is connected to the NEBM neurons corresponding to \(z_{i}\)'s that appear in the \(k\)-th constraint); and
* \(=[f_{ij}]=^{T}^{N N}\) is the co-occurrence matrix of the NEBM neurons (intuitively, \(f_{ij}\) is high if \(z_{i}\) and \(z_{j}\) appear frequently together in the constraints, while \(f_{ii}\) is high if \(z_{i}\) appear in many constraints).

Fig. 0(c) illustrates the proposed SNN construction, while Alg. 1 summarizes the associated algorithm.

Note that Alg. 1 involves two types of neurons that implement different dynamics, and the neurons are executed in parallel on the neuromorphic hardware . The internal state for \(m_{k}\) is

\[v_{k}^{(t)}=_{i=1}^{N}a_{ik}s_{i}^{(t)} \]

where \(a_{ik}\) is the element of \(i\)-th row and \(k\)-th column of matrix \(\). An FB neuron \(m_{k}\) is deterministically activated (_i.e._, its binary state \(c_{k}\) becomes \(1\)) whenever the connected NEBM neurons are _all inactive_ (_i.e._, \(v_{k}=0\)), and vice versa. Then, a negative spike \( c_{k}\) is generated if the value of \(c_{k}\) is \(1\), so as to send excitatory signals to the connected NEMB neurons to attempt to satisfy the constraint; otherwise no spikes are generated. The point here is not to over-excite NEBM neurons. Then, the internal state of an NEBM neuron \(n_{i}\) accumulates inhibitory signals from the connected NEBM neurons and excitatory signals from the connected FB neurons, _i.e._,

\[u_{i}^{(t)}=u_{i}^{(t-1)}+_{j i}^{N}f_{ij} s_{j}^{(t-1)}+_{k =1}^{K}a_{ik} c_{k}^{(t-1)}. \]

Similar to Alg. 2, Alg. 1 is executed on the neuromorphic hardware for \(M\) algorithmic timesteps, and the best state configuration probed and evaluated off-chip is taken as the solution. Note that the probing mechanism incurs significant overhead in the execution time of current implementation , and an on-chip evaluation strategy could be implemented to address this bottleneck [24; 32].

Scalability analysisBased on the construction of the QUBO-based SNN and the proposed SF-HMVC, we can model the number of spiking neurons required to encode each SNN:

* \(n_{}()=N+(r-1)K\)
* \(n_{}()=N+K\)

As \(N,K\) and \(r\) collectively define the size of the input problem, we can conclude that SF-HMVC scales linearly whereas the QUBO-based SNN scales quadratically with respect to the problem size.

## 5 Experiments

ImplementationOur method SF-HMVC was implemented on the Loihi assembly language through Lava framework  and deployed on the neuromorphic chip of Intel Loihi 2  with \(M=1000\) algorithmic timesteps (denoted as \(\)). Using grid search, we selected temperature \(T\) from the range \(\), refractory period \(r_{i}\) from the range \(\) (note that all NEBM neurons were given the same refractory period). We set \(=2\) for QUBO ( 4) (same as ) and grid search \(=\) for QUBO ( 9). Note that we were unable to change the seed of the random number generator on Intel Loihi 2, which prevented us from repeatedly executing SNNs to obtain error bars in the solution quality. Also, we noticed that energy consumption and runtime of our method and all competitors remained fairly consistent across different runs.

### Mvc

DatasetWe employed the DIMACS benchmark dataset . DIMACS includes graph instances that are both synthetically generated and sourced from real-world applications, including coding theory, fault diagnosis, Keller's conjecture, _etc_. The original DIMACS graphs were for maximum clique problems; we converted these to MVC instances by taking the complement graphs. Only 15 MVC instances where the corresponding SNN for SF-HMVC could fit on Loihi 2 were selected.

CompetitorsSF-HMVC-Loihi was compared to the following methods:

* ILP-CPU: MVC was solved using the ILP (3) with Gurobi  and evaluated on CPU.
* QUBO-CPU: MVC was solved using QUBO (4) with Gurobi  and evaluated on CPU. We set \(=2\) (same as ).
* QUBO-Loihi: MVC was solved using QUBO (4) with NEBM-based SNN (see Sec. 3.2) and evaluated on Intel Loihi 2. We set \(T=16,r_{i}=64,=2\).

All hyperparameters of the competitors were selected using grid search. The configuration that demonstrated consistent performance across all problem instances was selected for each method. For SF-HMVC-Loihi, we set \(T=4,r_{i}=8\).

MetricsSolution quality, runtime (in seconds), and energy consumption (in Joules) were reported for all methods. Runtime and energy consumption on Intel Loihi 2's Oheo Gulch board were measured through the built-in profiler of Lava-Loihi v0.6.0 extension. Runtime on CPU was recorded using built-in functions of Gurobi and the energy usage was recorded using pyJoules  on a workstation with an Intel Core i7-11700K CPU @ 3.6GHz and 32GB RAM running Ubuntu 20.04.6 LTS.

ResultsTabs. 1 and 2 display the results. Overall, SF-HMVC-Loihi was comparable to QUBO-Loihi in all three aspects: solution quality, runtime, and energy usage (the discrepancy between them will be clearer in the next experiment). ILP-CPU and QUBO-CPU outperformed SF-HMVC-Loihi in terms of solution quality (see Tab. 1), since both ILP-CPU and QUBO-CPU were globally optimal methods. The equal solution quality of the global methods also indicated the existence of a suitable penalty weight for QUBO (4) in the grid search range of \(\). On runtime (see Tab. 2), though ILP-CPU and QUBO-CPU solved many instances in under \(1\) s, a few instances took them minutes to hours to solve. In contrast, SF-HMVC-Loihi consistently solved all instances within \(10\) s. Also, on energy usage, our method significantly outperformed ILP-CPU and QUBO-CPU.

    &  &  &  &  \\   & **Time** & **Energy** & **Time** & **Energy** & **Time** & **Energy** & **Time** & **Energy** \\  C125.9 & 0.37 & 32.41 & 1.00 & 94.53 & 3.79 & 0.09 & 3.77 & 0.05 \\ C250.9 & 3600 & 144380.56 & 3600 & \(\) & 7.36 & 0.26 & 7.37 & 0.18 \\ gen200\_p0.9\_44 & 0.1 & 8.92 & 4.60 & 387.83 & 6.03 & 0.09 & 5.18 & 0.14 \\ gen200\_p0.9\_55 & 0.03 & 2.12 & 2.59 & 203.84 & 5.98 & 0.04 & 5.17 & 0.01 \\ hamming6-2 & 0 & 0.07 & 0.01 & 0.46 & 2.09 & 0.02 & 1.84 & 0.06 \\ hamming6-4 & 0.03 & 2.25 & 0.36 & 25.83 & 2.91 & 0.07 & 1.83 & 0.06 \\ hamming8-2 & 0 & 0.19 & 0.03 & 1.53 & 7.64 & 0.04 & 7.55 & 0.04 \\ johnson8-2-4 & 0.01 & 0.41 & 0.05 & 3.16 & 1.06 & 0.08 & 0.94 & 0.02 \\ johnson8-4-4 & 0 & 0.15 & 0.26 & 18.83 & 2.26 & 0.02 & 1.99 & 0.05 \\ johnson16-2-4 & 0.5 & 3.74 & 4.46 & 316.75 & 3.71 & 0.03 & 3.72 & 0.05 \\ keller4 & 1.14 & 114.13 & 1628.92 & 185173.36 & 5.13 & 0.11 & 5.15 & 0.03 \\ MANN\_a9 & 0 & 0.97 & 0.02 & 2.24 & 1.55 & 0.07 & 1.35 & 0.01 \\ MANN\_a27 & 0.18

### Hmvc

DatasetWe generated \(11\) synthetic HMVC instances as follows: first, a set of \(N\) vertices \(V\) were created, then a set of \(K\) degree-\(r\) hyperedges \(F\) were randomly generated. The values of \(N\), \(K\) and \(r\) were selected to ensure that the SNN (for SF-HMVC) could fit on Loihi 2.

CompetitorsSF-HMVC-Loihi was compared to the following methods:

* ILP-CPU: HMVC was solved using the ILP Eq. (6) with Gurobi  and evaluated on CPU.
* QUBO-CPU: HMVC was solved using QUBO Eq. (9) with Gurobi  and evaluated on CPU. We set \(=10\).
* QUBO-Loihi: HMVC was solved using QUBO Eq. (9) with NEBM-based SNN (see Sec. 3.2) and evaluated on Intel Loihi 2 . We set \(T=32,r_{i}=8,=10\).

All hyperparameters of these competitors were selected using grid search. The configuration that demonstrated consistent performance across all problem instances was selected for each method. For SF-HMVC-Loihi, we set \(T=8,r_{i}=8\).

ResultsTabs. 3 and 4 display the results. As expected, ILP-CPU outperformed SF-HMVC-Loihi in solution quality, since ILP-CPU was a global method. Interestingly, our method occasionally found better solutions than QUBO-CPU. This was probably because, as the problem difficulty (number of variables) increases, it became more challenging for QUBO-CPU. Note that SF-HMVC-Loihi handled all instances reasonably well. In contrast, QUBO-Loihi either could not find feasible solutions, or correspoding SNN could not be embedded onto Loihi 2. This suggests that the introduction of slack variables in QUBO-Loihi made the search space and/or problem size too large.

As presented in Table 4, while ILP-CPU performed better than SF-HMVC-Loihi in terms of runtime, our method was more efficient in terms of energy consumption. Furthermore, our method significantly surpassed QUBO-CPU and QUBO-Loihi in both runtime and energy efficiency.

## 6 Limitations and conclusions

Several limitations of our work can be identified:

1. The capacity of the neuromorphic computer available to us (a single Intel Loihi 2 chip  which has 128 neuromorphic cores) was relatively low, which prevented testing of large problem instances.
2. There is a lack of public benchmarks on HMVC problems. L1 and L2 together precluded an assessment of the generalizability of the methods more practical problem instances.
3. Changing the seed of the random number generator on Intel Loihi 2 was inaccessible to us, which precluded error bars in solution quality.

  
**Instance** & \(||\) & \(||\) &  ILP- \\ CPU \\  &  QUBO- \\ CPU \\  &  QUBO- \\ Loihi \\  & 
 SF-HMVC- \\ Loihi \\  \\ 
3-uniform\_HMVC01 & 30 & 981 & 3 & 3 & 3 (68) & 3 \\
3-uniform\_HMVC02 & 30 & 2129 & 9 & 9 & 19 (1214) & 9 \\
3-uniform\_HMVC03 & 30 & 2888 & 15 & 15 & N/A & 15 \\
3-uniform\_HMVC04 & 30 & 3327 & 22 & 30 & N/A & 29 \\
3-uniform\_HMVC05 & 50 & 3506 & 5 & 5 & N/A & 5 \\
3-uniform\_HMVC06 & 50 & 6538 & 8 & 8 & N/A & 8 \\
3-uniform\_HMVC07 & 50 & 7333 & 10 & 50 & N/A & 10 \\
3-uniform\_HMVC08 & 70 & 7326 & 4 & 4 & N/A & 4 \\
3-uniform\_HMVC09 & 100 & 5979 & 20 & 20 & N/A & 20 \\
3-uniform\_HMVC10 & 200 & 7430 & 10 & 10 & N/A & 19 \\
4-uniform\_HMVC11 & 30 & 4914 & 3 & 15 & N/A & 5 \\   

Table 3: Solution quality \(\|\|_{1}\) of the HMVC methods for all synthetic HMVC instances. Infeasible solutions are indicated in red, with number of constraint violations in brackets. N/A means that the SNN was not able to be embedded into the Loihi 2 due to capacity limitations.

* SNNs are ultimately heuristic algorithms, which complicate theoretical analyses on solution quality and runtime complexity.

Despite the limitations above, the results showed clear trends of the greater scalability of the proposed method SF-HMVC, in that it was able to solve HMVC problem instances where the existing method could not. Moreover, SF-HMVC on Loihi 2 exhibited measurably lower energy consumption than global solvers on CPU, further supporting neuromorphic computing as an energy-efficient alternative.