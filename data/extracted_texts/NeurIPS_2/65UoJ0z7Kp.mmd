# SeTAR: Out-of-Distribution Detection with Selective Low-Rank Approximation

Yixia Li\({}^{1}\), Boya Xiong\({}^{2}\), Guanhua Chen\({}^{1}\), Yun Chen\({}^{3}\)

\({}^{1}\)Southern University of Science and Technology

\({}^{2}\)Shanghai University of Finance and Economics

\({}^{3}\)MoE Key Laboratory of Interdisciplinary Research of Computation and Economics,

Shanghai University of Finance and Economics

liyixia@me.com, xiongboya@163.sufe.edu.cn

chengh3@sustech.edu.cn, yunchen@sufe.edu.cn

Equal Contribution.Corresponding Authors.

###### Abstract

Out-of-distribution (OOD) detection is crucial for the safe deployment of neural networks. Existing CLIP-based approaches perform OOD detection by devising novel scoring functions or sophisticated fine-tuning methods. In this work, we propose SeTAR, a novel, training-free OOD detection method that leverages selective low-rank approximation of weight matrices in vision-language and vision-only models. SeTAR enhances OOD detection via post-hoc modification of the model's weight matrices using a simple greedy search algorithm. Based on SeTAR, we further propose SeTAR+FT, a fine-tuning extension optimizing model performance for OOD detection tasks. Extensive evaluations on ImageNet1K and Pascal-VOC benchmarks show SeTAR's superior performance, reducing the relatively false positive rate by up to 18.95% and 36.80% compared to zero-shot and fine-tuning baselines. Ablation studies further validate SeTAR's effectiveness, robustness, and generalizability across different model backbones. Our work offers a scalable, efficient solution for OOD detection, setting a new state-of-the-art in this area.

## 1 Introduction

The task of out-of-distribution (OOD) detection (Hendrycks and Gimpel, 2017; Ming et al., 2022) aims to identify whether input data comes from an unknown distribution. It has garnered significant attention in the machine learning community recently (Hendrycks et al., 2020; Xu et al., 2021; Miyai et al., 2023). While machine learning models are trained with supervised in-distribution (ID) data, they often struggle to generalize to OOD data encountered in real-world applications (Emmott et al., 2016) like autonomous vehicles and healthcare. These OOD samples pose challenges as they are not represented in the training data. Consequently, OOD detection plays a crucial role in developing reliable and trustworthy machine-learning models suitable for real-world deployment (Bai et al., 2023). It allows models to filter out and reject these awkward inputs effectively, and enables the use of curated and labeled OOD samples to further train for a more robust model in the wild.

Previous research has primarily focused on detecting OOD instances in either visual (DeVries and Taylor, 2018; Liang et al., 2018; Hendrycks et al., 2022) or textual data (Hu and Khan, 2021; Zheng et al., 2020; Zhou et al., 2021). Recently, significant progress has been made in multimodal tasks like multimodal retrieval (Li et al., 2023; Caesar et al., 2018) and image classification (Yu et al., 2022), thanks to vision-and-language pretrained (VLP) models like CLIP (Radford et al., 2021). More recentstudies have explored OOD detection with CLIP, grouped into zero-shot methods (Fort et al., 2021; Ming et al., 2022; Miyai et al., 2023b) and finetuning-based methods (Ming and Li, 2023; Tao et al., 2023; Miyai et al., 2023a). However, the zero-shot methods suffer from suboptimal performance due to potential domain gaps with ID downstream data. On the other hand, finetuning-based methods carry the risk of deconstructing the intricate representations learned by CLIP which requires a meticulously designed training strategy. Sparsification-based approaches (Sun et al., 2021; Djurisic et al., 2023) have demonstrated potential in OOD detection within CNNs, leveraging the assumption that ID and OOD samples produce distinct activation patterns. Nevertheless, their effectiveness diminishes in large-scale pre-trained models such as CLIP, where activation differences become more subtle, thereby limiting their applicability primarily to models fine-tuned on downstream ID-domain datasets.

In this work, we propose SeTAR, a training-free and effective OOD detection method by selective low-rank approximations. Low-rank approximation is to approximate a given matrix by finding a lower-rank matrix that closely resembles the original matrix. Previous research has demonstrated that using low-rank approximation matrices can achieve comparable performance to full parameters in various scenarios, as observed in tasks such as large language model (LLM) fine-tuning (Hu et al., 2022) and model pruning (Hajimolahoseini et al., 2021). These approaches typically preserve the same rank across different low-rank approximation matrices. In our work, we demonstrate that it is possible to significantly enhance the performance of OOD detection by selectively manipulating the weight matrices in the CLIP model, including the choice of weight matrices and the ratio of singular vectors to be reduced. Specifically, we propose a simple top-to-bottom and image-to-text greedy search algorithm to manipulate \(}\) in the CLIP model. Our method applies to various model backbones and does not require any additional training or new parameters. Building upon SeTAR, we further demonstrate its effectiveness for fine-tuning initialization, referred to as SeTAR+FT.

We conduct extensive evaluations and achieve state-of-the-art performance on common OOD detection benchmarks for CLIP, including the ImageNet1K and Pascal-VOC benchmarks. Compared to vanilla MCM and GL-MCM, SeTAR with the CLIP backbone reduces relatively FPR95 by 9.5% and 12.0% on average across two benchmarks, respectively. When further integrate fine-tuning into SeTAR, SeTAR+FT outperforms the state-of-the-art fine-tuning baselines LoCoOp (Miyai et al., 2023a) and LoRA (Hu et al., 2022). Moreover, we perform a comprehensive ablation study and analysis to verify and understand SeTAR. In summary, our key results and contributions:

1. We propose SeTAR, a simple yet effective OOD detection method based on selective low-rank approximation. It is training-free as it only performs post-hoc modification to weight matrices. SeTAR applies to a variety of scoring functions and model backbones. It can be readily integrated with existing zero-shot OOD detection methods.
2. We further extend SeTAR to SeTAR+FT, which demonstrates the effectiveness of SeTAR in improving the performance of finetuning-based OOD detection methods and achieving new state-of-the-art results.
3. We extensively evaluate SeTAR and SeTAR+FT across a diverse set of OOD detection tasks. It consistently outperforms baseline methods and establishes new state-of-the-art results on CLIP-based OOD detection benchmarks. On ImageNet1K, SeTAR achieves an AUROC of 91.32% with CLIP backbone and GL-MCM score. The score further increases to 92.31% when combined with the finetuning-based detection method.
4. We perform comprehensive ablation studies and empirical analyses to verify and understand SeTAR. We hope that this work will shed light on future explorations on in-depth understanding of the SeTAR method.3

## 2 Preliminaries

CLIP ArchitectureThe CLIP model (Radford et al., 2021) comprises an image encoder \(E^{v}()\) and a text encoder \(E^{t}()\), aligned via contrastive learning on web-scale image-text pairs. We focus on CLIP-ViT, where the image encoder is a Vision Transformer (ViT). Each ViT layer includes a multihead self-attention sublayer and a feed-forward sublayer. In the self-attention module, the hidden state is projected into different spaces using learnable parameters \(,W_{k},W_{v}}\). The outputs are concatenated and projected back with another linear matrix \(}\). The feed-forward module projects the hidden state into a wider space using \(}\) and then back with \(}\) after a non-linear activation (Figure 1). Given the similarity between the image and text encoder layers, we adopt consistent notations for the linear matrices in both. Each encoder also includes a linear projector \(}\) to map their representations into a shared space for contrastive learning.

Zero-shot OOD Detection with CLIPZero-shot OOD detection aims to separate ID and OOD data without an ID training dataset. Given the CLIP, the ID classes are defined by the classification task of interest, which differs from the classes used in CLIP pretraining. Accordingly, OOD is defined as classes not belonging to any ID class, making the OOD detector a binary classifier. MCM (Ming et al., 2022) and GL-MLM (Miyai et al., 2023b) are two zero-shot CLIP-based OOD detection methods. Formally, let \(\) be the test image and \(_{}=\{_{c}\}_{c=1}^{K}\) be the set of text prompts containing \(M\) ID class labels (e.g., "a photo of a [CLASS]"). The image is segmented into \(l\) image patches \(=(x_{1},...,x_{l})\). Following CLIP, we add [cls] before the image patches and use the output of [cls] from the visual projector \(}\) as the global image feature (\(h^{v}^{d}\)). The outputs of other patches are projected by the visual projector as the local image features (\(^{v}=(p_{1}^{v},...,p_{l}^{v})^{l d}\)). For the text prompt \(_{c}_{}\), we add an additional [eos] after the text tokens and use the output feature of [eos] from the textual projector \(}\) as the concept feature of ID class \(c\) (\(h_{c}^{t}^{d}\)).

The label-wise image-concept matching (IWIC) score measures how well a test image \(\) aligns with a concept \(_{c}\), using either global or local features. The global IWIC score \(s_{c}^{G}()\) is the cosine similarity between the global image feature \(h^{v}\) and the concept feature \(h_{c}^{t}\): \(s_{c}^{G}()=\_(h^{v},h_{c}^{t})\). The local IWIC score \(s_{c}^{L}()\) is the max-pooled cosine similarity between image patch features \(p_{i}^{v}\) and the concept feature \(h_{c}^{t}\): \(s_{c}^{L}()=_{i}\_(p_{i}^{v},h_{c}^{t})\). The MCM and GL-MCM scores are defined as:

\[S_{}()=_{c}^{L}( )/}}{_{c=1}^{K}e^{s_{c}^{G}()/ }}, \]

\[S_{}()=S_{}( )+_{c}^{L}()/^{}}}{ _{c=1}^{K}e^{s_{c}^{L}()/^{}}}, \]

where \(\) and \(^{}\) are the temperature hyperparameters. MCM only uses global image features, while GL-MCM additionally considers local image features. For ID data, both MCM and GL-MCM scores will be matched to one of the concept features with a high score; and vice versa. As a result, our OOD detection function can be formulated as:

\[G()=\{1&S() \\ 0&S()<,. \]

where \(S()\) is either the MCM or GL-MCM score, \(\) is the threshold value. By convention, \(G()=1\) represents the ID class and \(G()=0\) indicates the OOD class. The \(\) is chosen so that a high fraction of ID data (e.g., 95%) is above the threshold. We follow previous work (Miyai et al., 2023a) to use either MCM or GL-MCM score for OOD detection in this work.

Figure 1: The overview of SeTAR. (a) The structure of the CLIP image and text encoder. (b) The details of the feed-forward sublayer. (c) For each encoder layer, we replace the \(}\) weight matrix with its low-rank approximation \(}}\). (d) The illustration of \(\) before and after low-rank approximation. More details are in Section 3.1.

Method

We introduce SeTAR, a training-free and effective technique for improving OOD detection performance (see Figure 1). Our key idea is to perform post-hoc modification to CLIP weight matrices by selectively replacing them with their low-rank approximations. It is complementary to existing CLIP-based zero-shot OOD detection methods and could be further extended to finetuning-based methods, which we term as SeTAR+FT.

### OOD Detection with Selective Low-Rank Approximation

Low-Rank ApproximationGiven a linear matrix \(W^{m n}\), its Singular Value Decomposition (SVD) is denoted as \(W=U V^{}\), where \(U=[u_{1},u_{2},,u_{m}]^{m m}\), \(V=[v_{1},v_{2},,v_{n}]^{n n}\), and \(^{m n}\) is a matrix whose entries are all zero except for the singular values of \(W\). These singular values appear in decreasing order on the diagonal (i.e. \(_{i}^{}(W)\)). The SVD of \(W\) can be reformulated as in Equation 4. Given a hyperparameter \(r^{+}\), a rank-\(r\) approximation of \(W\) is matrix \(\) that minimizes \(\|W-\|_{2}\) and satisfies \(() r\). The optimal solution of this problem \(\) is provided by Eckart-Young-Mirsky theorem (Low-Rank Approximation, 2024) using Singular Value Decomposition (see Equation 5).

\[W=_{i=1}^{(m,n)}_{i}^{}(W)u_{i}v_{i}^{ }, \] \[=_{i=1}^{r}_{i}^{}(W)u_{i}v_{i}^{ }. \]

In this work, we will use the term **minor singular components** to refer to entries in the SVD corresponding to small singular values. These components are removed in low-rank approximation. The term of **principle singular components** is used to refer to entries in the SVD corresponding to large singular values. These components are kept in a low-rank approximation of the matrix.

OOD Detection with Selective Low-Rank ApproximationSVD-based weight pruning, particularly in noise-prone layers, can substantially reduce a network's sensitivity to minor perturbations, leading to enhanced stability and robustness (Yao et al., 2024). This stability is crucial for OOD detection, as it ensures the model's reliable performance across a wide range of inputs. Building on this, we propose a method to improve OOD detection by selectively applying low-rank approximation to weight matrices. By decomposing a weight matrix \(W\) into its singular values and vectors, we can identify and retain the principle singular components that significantly contribute to the model's performance. This approach ensures that the essential features of \(W\) are preserved while discarding the less critical minor singular components. Given a weight matrix \(W\) in CLIP (e.g., \(_{}\) or \(_{}\)), we replace the matrix with its low-rank approximation part \(\) as described in Equation 5 (see Figure 1). Given the rank reduction ratio \(\), the rank of \(\) is determined by \(r()=((1-) r(W))\). This selective low-rank approximation leverages the compact representation provided by SVD to enhance the model's ability to detect OOD instances effectively without requiring additional training. We demonstrate our method's ability to improve OOD detection (Table 1) while maintaining ID classification performance (Table 7) in Section 4.2 and Section 4.5.

HyperParameter Search AlgorithmDue to the presence of many weight matrices in CLIP, each consisting of hundreds of singular values, conducting a complete search over all combinations of low-rank approximation weight matrices is impractical. Therefore, we propose a greedy search algorithm to determine the rank reduction ratio for each weight matrix. Among all linear weight matrices in each encoder layer, we focus on \(_{}\) as it is most effective according to our preliminary experiment. For simplicity, we assume both image and text encoders have \(N\) encoder layers. As shown in Algorithm 1, we search by first enumerating all \(N\) vision encoder layers sequentially from top to bottom and then all \(N\) text encoder layers in the same way. This search order is concisely denoted as searching from \(2N\) to the first layer in CLIP. We compare different search algorithms in Section 4.4. The rank reduction ratio for each layer is the objective in SeTAR which is searched among the candidate list \(=\{_{0},_{1},,_{J}\}\) according to the loss on the validation set. We employ the LoCoOp loss (Equation 12) proposed in (Miyai et al., 2023) as our loss function. This loss requires only ID images. It contains an ID loss for ID image classification and an OOD loss to push away pseudo OOD features from the ID class text embeddings where the pseudo OOD features are from ID-irrelevant nuisances (Equation 10) (e.g., backgrounds) in CLIP's local features. We refer the readers to Miyai et al. (2023) or Appendix B for more details. For \(_{j}\), we remove \(_{j}\) (in percent) singular values along with their corresponding singular vectors to obtain the approximated matrix \(}_{}\) (Equation 5). It is worth noting that the rank reduction raio candidate list includes \(_{0}=0\), indicating that the weight matrix has the chance to remain unmodified. With the searched rank reduction ratio, the weight matrix \(_{}\) in each CLIP layer is replaced and updated with its approximation. The SeTAR can be easily applied to different ViT backbones (Table 8), by replacing the model weight matrices with their low-rank approximations in a similar approach. Then SeTAR detects the OOD data samples following MCM (Equation 1), GL-MCM (Equation 2) or other scoring-based OOD detection method with the approximated model. We provide an example procedure of the greedy search in Listing 1 for better understanding.

### OOD Detection with SeTAR-enhanced Low-rank Adaptation

**SeTAR** can be further combined with LoRA (Hu et al., 2022) as a novel low-rank adaptation method for OOD detection, which we refer to as **SeTAR+FT**. Specifically, we first apply SeTAR to the pre-trained CLIP model to obtain the reserved rank \(r\) for each weight matrix \(W\). Then we have

\[W =+B A \] \[B =_{i=r+1}^{(m,n)}^{}(W)}u_{i}\] (7) \[A =_{i=r+1}^{(m,n)}^{}(W)}v_{i}^ {} \]

where \(\) is the low-rank approximation of \(W\) found by SeTAR, with \(A\) and \(B\) being the minor singular components. During finetuning, we keep \(\) frozen and only update the low-rank matrix \(A\) and \(B\). In this way, we retain the principle singular components in the original weight matrix and only update the minor singular components.Unlike LoRA, which evenly distributes the finetuning rank budget across all layers, SeTAR+FT adjusts the rank for each layer, resulting in more effective and efficient fine-tuning (Table 2 and Figure 6). More details are provided in Section 4.3.

``` Data: Valid set \(D\). Input: Layer length \(2N\), rank reduction ratio candidates \(\) with length \(J\), loss \(_{0}\) on \(D\) WITHOUT SeTAR. Result: Rank reduction ratio list \(^{*}\) with length \(2N\). \(^{*}=_{0}\) ; // Current best loss forLayerNum \(l 2N\) to 1do \(}^{*}=_{}^{*}\)\(T^{*}[l]=0\) forcounter \(j l\)do \(r=((1-[j])

**Settings** Following existing studies (Ming et al., 2022; Miyai et al., 2023, 2023, 2020), we use CLIP ViT-B/164 (Radford et al., 2021) as our backbone. Both image and text encoders have 12 layers. More results with different backbones are in Section 4.4. The rank reduction ratio candidates range from 0 to 40% in 5% intervals. We use a temperature of \(1^{5}\), unless stated otherwise. In all experiments, we use one CLIP text prompt: "a photo of a [CLASS],", where [CLASS] is the ID class name. We set hyperparameters \(\) (Equation 12) and top-K (Equation 10) according to the specific ID datasets and backbones. Detailed settings are in Table 12, with a sensitivity analysis in Section 4.4. For SeTAR+FT and LoRA experiments, the learning rate and epoch number are set to \(1e-2\) and \(5\) for all experiments. The LoRA rank \(r\) is set to match the trainable parameters of SeTAR+FT. Detailed settings are in Table 13. We report results from three runs with seeds 3, 4, 56. All experiments are conducted on a single NVIDIA RTX 4090 GPU. The time cost for low-rank approximation with CLIP-base on the ImageNet1K validation set is about 20 minutes.

MetricsWe use the following metrics for evaluation. (1) the false positive rate (FPR95) for out-of-distribution (OOD) samples at a fixed true positive rate (TPR) of 95% for in-distribution samples, with lower values targeting better performance; and (2) the area under the receiver operating characteristic curve (AUROC) for OOD samples, with higher values indicating better performance.

BaselinesWe evaluate SeTAR against MCM (Ming et al., 2022) and GL-MCM (Miyai et al., 2023), state-of-the-art zero-shot OOD detection methods on CLIP. We also compare SeTAR+FT with fine-tuning baselines NPOS (Tao et al., 2023), CoOp (Zhou et al., 2022), LoCoOp (Miyai et al., 2023), and LoRA (Hu et al., 2022). More details are in Appendix D.

### Training-free Results

The training-free OOD detection performances are summarized in Table 1. Compared with zero-shot baselines, a salient observation is that on both MCM and GL-MCM, using SeTAR outperforms the vanilla method by a large margin across all OOD detection tasks. For example, using Pascal-VOC as ID, SeTAR yields a relatively average reduction of 12.84% FPR95 on MCM and 18.95% FPR95 on GL-MCM. Considering that SeTAR is generally applicable and training-free, these results are very encouraging. Comparing SeTAR with scoring function MCM and GL-MCM, SeTAR+GL-MCM performs better on all OOD detection tasks. However, the superiority of GL-MCM score over MCM appears to be contingent upon the choice of the model backbone. As evidenced in Table 8, SeTAR+MCM demonstrates superior performance with a relatively average FPR95 reduction of 8.30% compared to SeTAR+GL-MCM with CLIP-large as the backbone on ImageNet1K.

    &  &  &  &  &  &  &  \\   & FPR1 & AUCT & FPR1 & AUCT & FPR1 & AUCT & FPR1 & AUCT & FPR1 & AUCT & FPR1 & AUCT & FPR1 & AUCT \\  
**ImageNet1K** & & & & & & & & & & & & & & & \\
**MCM Score** & & & & & & & & & & & & & & & \\ Vanilla MCM1 [FOOTNOTE:1]**GL-MCM Score** & & & & & & & & & & & & & & \\ Vanilla GL-MCM\({}^{}\) & 15.18 & 96.71 & 30.42 & 93.09 & 38.85 & 89.90 & 57.93 & 83.63 & - & - & - & - & 35.47 & 90.83 \\ Vanilla GL-MCM\({}^{}\) & 15.34 & 96.62 & 30.65 & 93.01 & 37.76 & 90.07 & 57.41 & 83.73 & - & - & - & - & 35.29 & 90.86 \\ SeTAR & **13.36** & **96.92** & **28.17** & **93.36** & **36.80** & **90.40** & **54.17** & **84.59** & - & - & - & - & - & **33.12** & **91.32** \\ 
**Pascal-VOC** & & & & & & & & & & & & & & & \\
**MCM Score** & & & & & & & & & & & & & & & \\ Vanilla MCM\({}^{}\) & 8.20 & 98.23 & 28.60 & 94.68 & \(\) & \(\) & 51.70 & 91.45 & 51.40 & 90.94 & 54.50 & 89.02 & 38.88 & 92.86 \\ Vanilla MCM\({}^{}\) & 7.24 & 98.23 & 27.91 & 94.56 & 32.40 & 92.45 & 51.61 & 91.89 & 50.60 & 91.42 & 53.70 & 89.30 & 37.24 & 92.98 \\ SeTAR & **4.59** & **98.71** & **24.91** & **95.15** & **28.46** & **93.21** & **40.44** & **93.58** & **48.25** & **92.08** & **48.10** & **89.70** & **32.46** & **93.74** \\
**GL-MCM Score** & & & & & & & & & & & & & \\ Vanilla GL-MCM\({}^{}\) & 4.20 & 98.71 & 23.10 & 94.66 & \(\) & \(\) & 43.00 & 92.84 & 41.00 & 92.38 & 44.30 & 90.48 & 31.12 & 93.81 \\ Vanilla GL-MCM\({}^{}\) & 4.33 & 98.81 & 22.94 & 94.63 & 26.20 & 93.11 & 41.61 & 92.88 & 37.88 & 93.17 & 43.70 & 90.71 & 29.44 & 93.88 \\ SeTAR & **3.66** & **98.96** & **21.93** & **94.81** & **25.04** & **93.62** & **20.35** & **96.36** & **31.47** & **94.31** & **40.70** & **91.19** & **23.86** & **94.87** \\   

Table 1: **Training-free results of FPR95(FPR) and AUROC(AUC) compared to zero-shot baselines on CLIP-base. Bold** values represent the highest performance. \({}^{}\) is cited from Miyai et al. (2023), where \(\) represents the absence of reporting in the paper. \({}^{*}\) denotes the result of our re-run. \(-\) denotes the OOD dataset has overlapping categories with the ID dataset. We do not report standard deviations since no training is involved.

### Fine-tuning Results

In this section, we compare ScTAR+FT with fine-tuning baselines, including NPOS , CoOp , LoCoOp  and LoRA . LoCoOp is the state-of-the-art prompt-learning OOD detection method on CLIP. LoRA is a representative parameter-efficient fine-tuning method. Following previous work , we report the results on the the ImageNet1K benchmark in Table 2. We observe that SeTAR+FT outperforms all baselines on both MCM and GL-MCM scoring functions. For example, with CLIP-base as the backbone, SeTAR+FT achieves a relatively average FPR95 reduction of 3.97% and 6.67% compared to LoCoOp and LoRA. Moreover, when scaled up to CLIP-large, SeTAR+FT outperforms LoCoOp and LoRA by relatively 17.92% and 12.45% FPR95 on the same benchmark. Similar results are observed on Swin Transformer , where SeTAR+FT outperforms LoRA by relatively 17.36% and 36.80% FPR95 on MSP and Energy scoring functions, respectively. The larger improvement on Swin Transformer may stem from its reliance on ImageNet training, making it prone to overfitting and weaker at OOD detection. Our method mitigates these issues, enhancing Swin's generalization to OOD instances. These results demonstrate the effectiveness and scalability of SeTAR+FT in improving the OOD detection performance.

Furthermore, as shown in Figure 6, SeTAR+FT demonstrates faster convergence and lower loss than LoRA, especially in OOD loss, indicating that SeTAR+FT is more effective in adapting the pre-trained weights to the OOD detection task.

### Ablation Study

In this section, we conduct ablation studies with CLIP-base to understand our design choices.

Image v.s. Text modalityTable 3 shows an ablation study on the modality involved in SeTAR. As shown, the vision modality outperforms the text modality, indicating the vision modality is more dominant in enhancing the model's performance. When considering the vision modality alone and the combined vision+text modality, the latter either outperforms or achieves comparable average results to the former. Consequently, we make modifications to both the vision and text modalities in SeTAR to enhance overall performance.

Different Weight TypesIn this part, we present empirical evidence for modifying \(}\). We first compare the performance of SeTAR with different types of weight matrix in each Transformer layer, including \(}\), \(}\), \(}\), \(}\), \(}\) and \(}\). As shown in Figure 2 and Figure 3 of Appendx F, the \(X\)-axis denotes the number of weight matrixes (layers) that we have searched, while the \(Y\)-axis is the average AUROC and FPR95. The results show that \(}\) consistently outperforms other weight matrices in terms of both AUROC and FPR95. In addition to weight matrices in each transformer layer, CLIP has one projection matrix \(}\) on top of each encoder,

   Score &  &  &  \\   & FPR\(\) & AUC\(\) & FPR\(\) & AUC\(\) & FPR\(\) & AUC\(\) \\ 
**ImageNet1K** & & & & & & & \\ MCM & 40.27 & **91.24** & 42.78 & 90.50 & **40.24** & 91.05 \\ GL-MCM & **32.97** & **91.60** & 35.82 & 90.55 & 33.12 & 91.32 \\ 
**Pascal-VOC** & & & & & & \\ MCM & 33.19 & 93.45 & 33.47 & 93.42 & **32.46** & **93.74** \\ GL-MCM & 24.88 & 94.51 & 24.59 & 94.52 & **23.86** & **94.87** \\   

Table 3: **Ablation study on modality.**

    &  &  \\  & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) \\  NPOS\({}^{}\) & 42.20 & 90.43 & 36.86 & 90.37 \\ CoOp\({}^{}\) & 44.81 & 90.03 & 36.58 & 90.25 \\ LoCoOp\({}^{}\) & 40.17 & 91.53 & 33.52 & 92.14 \\ LoCoOp\({}^{}\) & 39.76\({}_{ 4.06}\) & 91.22\({}_{ 0.52}\) & 34.14\({}_{ 1.64}\) & 91.73\({}_{ 0.17}\) \\ LoRA\({}^{}\) & 41.67\({}_{ 0.14}\) & 90.85\({}_{ 0.01}\) & 34.36\({}_{ 0.11}\) & 90.88\({}_{ 0.01}\) \\ SeTAR+FT & **37.75\({}_{ 0.22}\)** & **91.85\({}_{ 0.01}\)** & **32.19\({}_{ 0.02}\)** & **33.14\({}_{ 0.05}\)** \\    & MCM Score & GL-MCM Score \\  & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) \\  LoCoOp\({}^{}\) & 40.74\({}_{ 3.00}\) & 91.13\({}_{ 0.79}\) & 46.74\({}_{ 4.19}\) & 89.32\({}_{ 0.80}\) \\ LoRA\({}^{}\) & 38.62\({}_{ 0.07}\) & 91.66\({}_{ 0.02}\) & 43.39\({}_{ 0.93}\) & **90.76\({}_{ serves to project image/text representations into a shared space. In Table 4, we compare the performance of SeTAR with and without modifying \(}\). We search \(}\) first right before searching the image/text encoder. The results show that frozen \(}\) brings a relatively reduction of \(4.20\%\) FPR95. Consequently, we keep \(}\) frozen in SeTAR.

Different Search AlgorithmsAt each step of the greedy search, SeTAR traverses the subsequent \(}\) in a predefined order and searches over different thresholds. We compare our method with two alternatives: modality-interleaved greedy search (MIS) and layer-exhaustive search (LES). MIS searches the image and text layers in an interleaved manner, while LES simultaneously searches over both layers and thresholds at each step. SeTAR-S, has linear complexity with respect to the number of model layers, similar to Table 5 presents the comparison results. SeTAR-S demonstrates better overall performance than MIS. Notably, MIS encounters limitations when the image and text towers have different layer counts (e.g., CLIP-large with 24 image layers and 12 text layers). Therefore, we choose SeTAR-S for better generalization. Compared to LES, SeTAR-S performs better in terms of both FPR95 and AUROC, as LES's locally optimal algorithm may not achieve a global optimal solution. These results validate the superiority of our top-to-bottom layer search strategy.

Different Prune StrategiesInspired from SVD, SeTAR modify the model weights by pruning the minor singular components, and retains the principle components that contribute the most to the model's performance. To validate this design, we compare SeTAR with two alternatives: principal component pruning and random the opposite approach, retaining minor components and pruning major ones. Random pruning, on the other hand, prunes weights randomly. As shown in Table 6, principle pruning suffers from a significant performance drop compared to SeTAR, while random pruning performs slightly better than principle pruning. These results demonstrate the effectiveness of SeTAR's design choice in pruning the minor components.

Sensitivity Analysis on \(\) and top-KIn this section, we present the sensitivity analysis of the hyperparameters \(\) (Figure 4) and top-K (Figure 5). As observed in Figure 4, the average AUROC remains stable at lower values and slightly decreases as \(\) increases for both SeTAR+MCM and SeTAR+GL-MCM. Notably, the optimal setting of \(\) may vary depending on the model backbone, with our experiments indicating that CLIP-large may require a larger \(\) than CLIP-base. Despite this variation, the \(\) parameter demonstrates strong transferability across datasets for the same backbone. Swapping the optimal \(\) between ImageNet1K and Pascal-VOC has a minimal performance impact, consistently outperforming the vanilla method. With the VOC-optimized \(\) on ImageNet1K, CLIP-base achieves an FPR95 of 40.91 and AUROC of 91.02, and CLIP-large reaches 46.73 FPR95 and 91.81 AUROC. Conversely, using the ImageNet1K-optimized \(\) on Pascal-VOC, CLIP-base achieves 33.18 FPR95 and 93.65 AUROC, while CLIP-large attains 44.39 FPR95 and 92.3 AUROC.

Top-K controls the number of OOD regions considered in LoCoOp loss: higher values include more OOD regions, with top-K equal to the number of ID classes covering all OOD regions, and top-K set to 0 focusing solely on ID loss. The optimal top-K depends on the number of ID categories, making it non-transferable across datasets. However, SeTAR remains robust to top-K variations, as shown in Figure 5, except at extreme values (0 or the maximum number of classes). We recommend setting top-K to around 30% of the total categories, such as 300 for ImageNet1K and 4 for Pascal-VOC. For the Swin-base model, top-K at 300 on ImageNet1K yields an FPR95 of 56.82 and AUROC of 85.68 with MSP, and an FPR95 of 52.56 and AUROC of 84.51 with Energy.

   Score &  &  &  \\   & FPR\(\) & AUC\(\) & FPR\(\) & AUC\(\) & FPR\(\) & AUC\(\) \\ 
**ImageNet1K** & & & & & & \\ MCM & 43.09 & 90.74 & 43.09 & 90.74 & **40.24** & **91.05** \\ GL-MCM & 35.29 & 90.86 & 35.29 & 90.86 & **33.12** & **91.32** \\ 
**Pascal-VOC** & & & & & & \\ MCM & 38.20 & 92.44 & 33.57 & 93.09 & **32.46** & **93.74** \\ GL-MCM & 25.36 & 93.67 & 26.20 & 94.66 & **23.86** & **94.87** \\   

Table 6: **Results for different pruning strategies.**

    &  &  &  \\   & FPR\(\) & AUC\(\) & FPR\(\) & AUC\(\) & FPR\(\) & AUC\(\) \\ 
**ImageNet1K** & & & & & & \\ MCM & 41.99 & 90.78 & 40.55 & 91.00 & **40.24** & **91.05** \\ GL-MCM & 33.90 & 91.08 & 33.36 & 91.29 & **33.12** & **91.32** \\ 
**Pascal-VOC** & & & & & & \\ MCM & 35.11 & 93.60 & 33.93 & 93.58 & **32.46** & **93.74** \\ GL-MCM & 24.48 & 94.57 & **22.87** & 94.84 & 23.86 & **94.87** \\   

Table 5: **Results for different search algorithms.** Here LES, MIS and SeTAR-S stand for layer-exhaustive search, modality-interleave greedy search, and, and the search algorithm of SeTAR.

### Analyses

Can SeTAR Improve Image Classification?To evaluate the impact of SeTAR and SeTAR+FT on classification accuracy, we present our results on ID dataset ImageNet1K and OOD datasets SUN, Places and Texture in Table 7. SeTAR effectively maintains the average accuracy, with minor variations observed across different datasets. Among the fine-tuned baselines, LoCoOp exhibits a 1% decrease in accuracy compared to Vanilla CLIP, whereas LoRA shows an improvement of 0.94%. Notably, SeTAR+FT surpasses both baselines, improving the average accuracy by 1.45% compared to Vanilla CLIP. These results highlight the efficacy of SeTAR and SeTAR+FT in improving OOD detection without compromising classification accuracy.

SeTAR is Effective on Different Architectures and Score FunctionsWe expand on Table 1 with results on ViT and CNN backbones and various score functions. For ViT-based models, we evaluate OOD detection using CLIP-large8 and Swin Transformer9Liu et al. (2021), alongside CLIP-base. The Swin Transformer Liu et al. (2022) is trained on ImageNet1K. Since it lacks a text encoder, we apply SeTAR to the image ViT only. For Swin Transformer, we use two common scoring functions: MSP Hendrycks and Gimpel (2017), which leverages softmax confidence, and the Energy score Liu et al. (2020), with \(T=0.1\) for OOD detection. We also integrate CLIP-base with the NegLabel score function Jiang et al. (2024), which uses large-scale negative labels. As shown in Table 8, SeTAR consistently outperforms baselines across all backbones and scoring functions, significantly reducing FPR95 by relatively 20.61% with the Energy score on Swin Transformer. These results demonstrate SeTAR's effectiveness in improving OOD detection for unimodal image encoders, with further confirmation from SeTAR+FT results (Table 2) across different model backbones.

We further explore SeTAR's potential on CNN architecture, and compare it with methods such as Softmax, Energy Wu et al. (2023), ReAct Sun et al. (2021), DICE Sun and Li (2022), and ASH Djurisic et al. (2023) on ResNet5010. Since ResNet lacks local features for OOD loss, we conduct experiments using only ID loss. We apply low-rank approximation to the in- and out-feature dimensions of the convolutional layers, combined with ASH for search. As shown in Table 9, SeTAR establishes new state-of-the-art results on ResNet, demonstrating its effectiveness across both ViT and CNN architectures.

Near-OOD ResultsTo further evaluate SeTAR's performance on diverse OOD tasks, we test it on a more challenging near-OOD setting using ImageNet1K as the ID dataset and SSB-Hard Vaze et al. (2022) as the OOD dataset. As shown in Table 10, SeTAR and SeTAR+FT outperform the baselines, demonstrating superior performance in near-OOD scenarios.

   Method &   &   &   &   & 
  \\  Vanilla CLIP* & 64.07 & 75.77 & 45.65 & 43.60 & 57.27 \\ LoCoOp* & 64.93 & 75.89 & 46.47 & 37.79 & 56.27 \\ LoRa* & 65.43 & 76.86 & 46.58 & **43.98** & 58.21 \\ SeTAR & 63.97 & 75.50 & 45.81 & 43.76 & 57.26 \\ SeTAR+FT & **67.02** & **77.94** & **46.64** & 43.28 & **58.72** \\   

Table 7: **Image classification results with different methods. We use ImageNet1K (IN1K) as ID dataset. * denotes the results of our re-run. The results are averaged over 3 runs.**

   Method &  FPR\(\) \\  &  AUC\(\) \\  &  Method \\  &  FPR\(\) \\  & 
 AUC\(\) \\  \\  Softmax\({}^{}\) & 66.95 & 81.99 & ASH-P\({}^{}\) & 50.32 & 89.04 \\ Energy\({}^{}\) & 58.41 & 86.17 & ASH-B\({}^{}\) & 22.73 & 95.06 \\ ReAct\({}^{}\) & 31.43 & 92.95 & ASH-S\({}^{}\) & 22.80 & 95.12 \\ DICE\({}^{}\) & 34.75 & 90.77 & SeTAR & **22.38** & **95.25** \\   

Table 8: **Results for different ViT backbones.**

   Method &  FPR\(\) \\  &  AUC\(\) \\  &  Method \\  &  FPR\(\) \\  & 
 AUC\(\) \\  \\  Softmax\({}^{}\) & 66.95 & 81.99 & ASH-P\({}^{}\) & 50.32 & 89.04 \\ Energy\({}^{}\) & 58.41 & 86.17 & ASH-B\({}^{}\) & 22.73 & 95.06 \\ ReAct\({}^{}\) & 31.43 & 92.95 & ASH-S\({}^{}\) & 22.80 & 95.12 \\ DICE\({}^{}\) & 34.75 & 90.77 & SeTAR & **22.38** & **95.25** \\   

Table 9: **Results on ResNet50**. We use ImageNet1K as the ID dataset. \({}^{}\) is cited from Djurisic et al. (2023).

Related Work

Out-of-Distribution DetectionPrevious work explores OOD detection with unimodal (DeVries and Taylor, 2018; Hendrycks and Gimpel, 2017; Hu and Khan, 2021; Zheng et al., 2020; Zhou et al., 2021) and multimodal (Fort et al., 2021; Ming et al., 2022; Tao et al., 2023; Miyai et al., 2023a) models. Numerous methodologies (Lee et al., 2018; Huang et al., 2021; Sun et al., 2022; Wang et al., 2022; Wu et al., 2023) have been developed to tackle OOD detection in computer vision. Existing CLIP-based OOD detection methods include zero-shot (Fort et al., 2021; Ming et al., 2022; Miyai et al., 2023b; Dai et al., 2023; Wang et al., 2023; Jiang et al., 2024) and fine-tuning (Ming and Li, 2023; Tao et al., 2023; Miyai et al., 2023a). Zero-shot methods like MCM (Ming et al., 2022) and GL-MCM (Miyai et al., 2023b) don't require in-distribution training data but may perform suboptimally due to domain gaps. Other approaches integrate external knowledge. For example, CLIPN (Wang et al., 2023) pre-trains a novel NO-encoder on the CC-3M dataset (Sharma et al., 2018) to empower CLIP's "no" logic for zero-shot evaluation. NegLabel (Jiang et al., 2024) demonstrates better performance than CLIPN by introducing large-scale negative labels for enhanced label scoring. Fine-tuning methods (Ming and Li, 2023; Tao et al., 2023; Miyai et al., 2023a) improve OOD detection by adapting to in-distribution data but risk damaging the pretraining representations, needing careful training strategies. CNN-based OOD detection methods, including ReAct (Sun et al., 2021), ASH (Djurisic et al., 2023), DICE (Sun and Li, 2022), CIDER (Ming et al., 2023), PALM (Lu et al., 2024), and Hopfield Boosting (Hofmann et al., 2024), have also demonstrated strong results. However, methods like ReAct and ASH rely on the assumption that ID and OOD images produce distinct activations in models trained on ID data. This assumption does not hold in large-scale pre-trained models like CLIP, where activations for ID and OOD images are not significantly different, limiting the effectiveness of such approaches in enhancing CLIP's zero-shot OOD detection capabilities. SeTAR, in contrast, offers high compatibility with various scoring functions (e.g., MCM, GL-MCM, MSP, Energy), multiple model backbones (e.g., CLIP, Swin, ResNet), and advanced OOD techniques such as NegLabel. Designed to be both lightweight and efficient, SeTAR addresses the demand for resource-efficient solutions in OOD detection.

Low-rank Approximations of Weight MatricesNeural networks trained with over-parameterization often exhibit low-rank properties (Oymak et al., 2019). These properties are utilized in both model training (Povey et al., 2018; Hu et al., 2022) and post-hoc processing (Hajimolahoseini et al., 2021; Sharma et al., 2023). In training, some works (Sainath et al., 2013; Zhang et al., 2014; Zhao et al., 2016) impose low-rank constraints, while LoRA (Hu et al., 2022) adapts pretrained LLMs to downstream tasks using trainable low-rank matrices. For post-hoc processing, pruning methods (Yu et al., 2017; Hajimolahoseini et al., 2021) reduce weight matrix ranks by retaining top-K components from SVD. While pruning preserves model behavior, performance declines with increased intervention. LASER (Sharma et al., 2023) focuses on pruning individual layers to enhance factual answering capabilities. It utilizes a simple greedy search strategy on a validation set, which is not applicable for OOD detection due to the absence of a validation set. In contrast, our approach introduces a selective rank reduction strategy specifically tailored for OOD detection. We systematically analyze and compare different greedy search techniques, evaluating their effectiveness across various layers and model backbones.

## 6 Conclusion

We propose SeTAR, a simple and effective OOD detection method using post-hoc low-rank approximation on weight matrices \(}\) with a top-down, image-to-text greedy search. SeTAR offers several advantages: (1) training-free, (2) scalable to unimodal and multimodal models, and (3) complementary to existing OOD scoring functions. Building on SeTAR, we introduce SeTAR-FT, a finetuning method that adapts the model to in-distribution data for improved OOD detection. We evaluate SeTAR and SeTAR-FT on large-scale benchmarks, including ImageNet1K and Pascal-VOC. Results show that both achieve state-of-the-art OOD detection performance. We hope our work inspires further research and contributes to more robust and reliable models.