# Greggory Heller\({}^{4}\), Severine Durand\({}^{1}\), Shawn Olsen\({}^{1}\), Stefan Mihalas\({}^{1,\dagger}\)\({}^{1}\)

Gaussian Partial Information Decomposition: Bias Correction and Application to High-dimensional Data

 Praveen Venkatesh\({}^{1,2,*}\), Corbett Bennett\({}^{1}\), Sam Gale\({}^{1}\), Tamina K. Ramirez\({}^{3}\),

**Greggory Heller\({}^{4}\), Severine Durand\({}^{1}\), Shawn Olsen\({}^{1}\), Stefan Mihalas\({}^{1,}\)\({}^{1}\)**

\({}^{1}\)Allen Institute; \({}^{2}\)University of Washington; \({}^{3}\)Columbia University;

\({}^{4}\)Massachusetts Institute of Technology

\({}^{*}\)praveen.venkatesh@alleninstitute.org; \({}^{}\)stefanm@alleninstitute.org

###### Abstract

Recent advances in neuroscientific experimental techniques have enabled us to simultaneously record the activity of thousands of neurons across multiple brain regions. This has led to a growing need for computational tools capable of analyzing how task-relevant information is represented and communicated between several brain regions. Partial information decompositions (PIDs) have emerged as one such tool, quantifying how much unique, redundant and synergistic information two or more brain regions carry about a task-relevant message. However, computing PIDs is computationally challenging in practice, and statistical issues such as the bias and variance of estimates remain largely unexplored. In this paper, we propose a new method for efficiently computing and estimating a PID definition on multivariate Gaussian distributions. We show empirically that our method satisfies an intuitive additivity property, and recovers the ground truth in a battery of canonical examples, even at high dimensionality. We also propose and evaluate, for the first time, a method to correct the bias in PID estimates at finite sample sizes. Finally, we demonstrate that our Gaussian PID effectively characterizes inter-areal interactions in the mouse brain, revealing higher redundancy between visual areas when a stimulus is behaviorally relevant.

## 1 Introduction

Neuroscientific experiments are increasingly collecting large-scale datasets with simultaneous recordings of multiple brain regions with single-unit resolution . These experimental advances call for new computational tools that can allow us to probe how multiple brain regions jointly process relevant information in a behaving animal.

Partial Information Decompositions (PIDs) offer a new method for studying how different brain regions carry task-relevant information: they provide measures to quantify the amount of _unique_, _redundant_ and _synergistic_ information that one region has with respect to another. The information itself could pertain to task-relevant variables such as stimuli, behavioral responses, or information contained in a third region. For example, we may be interested in how much information about a stimulus is communicated or shared (i.e., redundantly present) between two brain regions over time. Or, we might be interested in the extent to which one region's activity uniquely explains that of another, while excluding information corresponding to spontaneous behaviors.

Ideas such as redundancy and synergy have a long history in neuroscience, having been proposed for understanding noise correlations  and to understand differences in encoding complexity between different brain regions . PIDs have also been suggested for quantifying how much sensory information is used to execute behaviors  and for tracking stimulus-dependent information flows between brain regions . Outside of neuroscience, PID has been used to understand interactionsbetween different variables in financial markets , to quantify the relevance of different features for the purpose of feature selection in machine learning , and to define and quantify bias in the field of fair Machine Learning .

An important constraint that has limited the broader adoption of PIDs in neuroscience is the computational difficulty of estimating PIDs for high-dimensional data. Many PID definitions that are operationally well-motivated involve solving an optimization problem over a space of probability distributions: the number of optimization variables can thus be exponential in the number of neurons . This has led to the use of poorly motivated PID definitions that are easy to compute (such as the "MMI-PID" of , in works such as [9; 14; 15; 16]), or limited analyses to very few dimensions . Furthermore, due to the limited exploration of estimators for PIDs, issues such as the bias and variance of estimates have received no attention so far, to our knowledge.

In this paper, we make the following contributions:

1. We provide a new and efficient method for computing and estimating a well-known PID definition called the \(\)-PID or the BROJA-PID  on Gaussian distributions (Section 3). By restricting our attention to Gaussian distributions, we are able to significantly reduce the number of optimization variables, so that this is just quadratic in the number of neurons, rather than exponential.
2. We present a set of canonical examples for Gaussian distributions where ground truth is known, and show that our method outperforms others (Section 4).
3. We also raise (for what we believe is the first time) the issue of bias in PID estimates, propose a method for correcting the bias, and empirically evaluate its performance (Section 5).
4. Finally, we show that our Gaussian PID estimator closely agrees with ground truth, even on non-Gaussian distributions, and show an example of its use on real neural data (Section 6).

#### Related work

Our method is based on our earlier work , where we also examined PIDs for Gaussian distributions. Our current work differs in a few key aspects: (i) we estimate the PID of a different PID definition, the \(\)-PID rather than the \(\)-PID, because the \(\)-PID does not satisfy a basic property called additivity  (defined in Sec. 2); (ii) our current method provides an exact upper bound to the PID definition being computed, rather than an approximate upper bound; (iii) we now consider the problem of estimation, not just computation, and explore the issue of the bias of PID estimates; and (iv) our current method is much faster, and we demonstrate agreement with ground truth at much higher dimensionality.

Several other studies have also considered methods for efficiently estimating PIDs: Banerjee et al.  and Makkeh et al.  address computing _discrete_ PIDs, but their method does not scale to higher dimensions; Pakman et al.  estimate PIDs using copulas, but their method would also potentially be computationally prohibitive at high dimensionalities; Liang et al.  use convex optimization to directly estimate the \(\)-PID for general high-dimensional distributions, but they do not compare with ground truth at high dimensionality or examine bias in their estimates.

## 2 Background: An Introduction to PIDs and the \(\)-PID

In this section, we provide an introduction to the concept of partial information decomposition along with an illustrative example. Let \(M\), \(X\) and \(Y\) be three random variables with joint distribution \(P_{MXY}\). A PID decomposes the total mutual information between the _message_\(M\) and two _constituent_ random variables \(X\) and \(Y\) into a sum of four non-negative components that satisfy [18; 23]:

\[IM;(X,Y) =UI(M:X Y)+UI(M:Y X)+RI(M:X;Y)+SI(M:X;Y) \] \[I(M;X) =UI(M:X Y)+RI(M:X;Y)\] (2) \[I(M;Y) =UI(M:Y X)+RI(M:X;Y) \]

Here, \(I(A;B)\) is the _Shannon mutual information_ between the random variables \(A\) and \(B\), and the four terms in the RHS of (1) are respectively the information about \(M\) that is (i) _uniquely_ present in \(X\) and not in \(Y\); (ii) _uniquely_ present in \(Y\) and not in \(X\); (iii) _redundantly_ present in both \(X\) and \(Y\) and can be extracted from either; and (iv) _synergistically_ present in \(X\) and in \(Y\), i.e., information which cannot be extracted from either of them individually, but can be extracted from their interaction. For the sake of brevity, we may also refer to these partial information components as \(UI_{X}\), \(UI_{Y}\), \(RI\) and \(SI\) respectively. Notwithstanding notation, they should all properly be understood to be functions of the joint distribution \(P_{MXY}\).

Now, \(UI_{X}\), \(UI_{Y}\), \(RI\) and \(SI\) consist of four undefined quantities, subject to the three equations in (1)-(3). In addition, they are typically assumed to be non-negative, \(RI\) and \(SI\) are each constrained to be symmetric in \(X\) and \(Y\), and the functional forms of \(UI_{X}\) and \(UI_{Y}\) should be identical when exchanging \(X\) for \(Y\). Despite the number of constraints, many definitions satisfy all of them, each differing in its motivation and interpretation [18; 23; 25; 26; 27] (see [27; 28] for a review), and we need to formally define one of these partial information components to determine the other three.

**Example 1**.: Before we jump into a specific definition, we provide an intuition into what these terms mean using a simple example. Suppose \(M=[A,B,C]\), \(X=[A,B,C Z]\), and \(Y=[B,Z]\), where \(A,B,C,Z\) i.i.d. Ber(0.5).1 Then, \(X\) has 1 bit of unique information about \(M\), i.e., \(A\); \(Y\) has no unique information; \(X\) and \(Y\) both have 1 bit of redundant information, i.e., \(B\), since it can be obtained from either \(X\) or \(Y\); and \(X\) and \(Y\) have 1 bit of synergistic information, i.e., \(C\), which cannot be obtained from either \(X\) or \(Y\) individually (since \(C Z C\)), but can only be recovered when both \(X\) and \(Y\) are known. For more examples on binary variables, we refer the reader to . 

In this manuscript, we consider a definition that we refer to as the \(\)-PID2[18; 25], which is defined below. We chose to build an estimator for _this_ definition for two reasons: (i) it is a _Blackwellian PID_ definition, i.e., it has well-defined operational interpretations based on concepts from statistical decision theory (e.g., see [18; 27; 29] for details); and (ii) it satisfies many desirable properties (e.g., see [18; 30]), and in particular, a property that we call _additivity of independent components_.

**Definition 1** (\(\)-PID ).: _The unique information about \(M\) present in \(X\) and not in \(Y\) is given by_

\[(M:X Y)_{Q_{P}}I_{Q}(M;X\,|\,Y), \]

_where \(_{P}\{Q_{MXY}:Q_{MX}=P_{MX},\;Q_{MY}=P_{MY}\}\) and \(I_{Q}(\,\,;\,|\,)\) is the conditional mutual information over the joint distribution \(Q_{MXY}\). The remaining \(\)-PID components, \((M:Y X)\), \((M:X;Y)\) and \((M:X;Y)\), follow from equations (1)-(3)._

**Property 1** (Additivity of independent components).: _Suppose \(M=[M_{1},M_{2}]\), \(X=[X_{1},X_{2}]\), and \(Y=[Y_{1},Y_{2}]\), such that \((M_{1},X_{1},Y_{1})(M_{2},X_{2},Y_{2})\). Then, additivity implies that_

\[UI(M:X Y)=UI(M_{1}:X_{1} Y_{1})+UI(M_{2}:X_{2} Y_{2 }), \]

_and similarly for the other three partial information components, \(UI_{Y}\), \(RI\) and \(SI\)._

Property 1 stipulates that we should be able to compute the PIDs of two independent systems _separately_, and then add the components across both systems. In effect, additivity implies that the PID of an isolated system should not depend on the PID of another isolated system, making it an intuitive and highly desirable property (see App. A.1 for concrete examples). Of the many PID definitions examined by Rauh et al. , only the \(\)-PID satisfied additivity (as proved in ).

## 3 Computing the \(\)-PID for Gaussian Distributions

The first contribution of this paper is a method to efficiently compute bounds on the \(\)-PID for jointly Gaussian random vectors \(M\), \(X\) and \(Y\). To be precise, our method computes an upper bound for \(_{X}\) and \(_{Y}\), and lower bounds for \(\) and \(\). Similar to our earlier work , we present a new PID definition that we call the \(\)-PID, which characterizes an upper bound on the unique information of the \(\)-PID by restricting the optimization space to jointly Gaussian \(Q_{MXY}\):

**Definition 2** (\(_{G}\)-Pid).: _Let \(P_{MXY}\) be jointly Gaussian. Then, the unique information about \(M\) present in \(X\) and not in \(Y\) is given by_

\[_{G}(M:X Y)_{Q_{P}}I_{Q}(M;X\,|\, Y), \]

_where \(_{P}\{Q_{MXY}:Q_{MXY}\) jointly Gaussian, \(Q_{MX}=P_{MX},\;Q_{MY}=P_{MY}\}\) and \(I_{Q}\) is the conditional mutual information over the joint distribution \(Q_{MXY}\)._

If the optimal \(Q_{MXY}\) in the unrestricted optimization of Definition 1 happens to be Gaussian for some \(P_{MXY}\), then the \(_{G}\)-PID would be identical to the \(\)-PID for that \(P_{MXY}\). We conjecture that this happens whenever \(P_{MXY}\) is Gaussian: for example, in a similar optimization problem for computing the information bottleneck , the optimal distribution is Gaussian whenever \(P\) is Gaussian [32; 33]. We provide empirical evidence in favor of this conjecture through a sequence of examples with Gaussian \(P\) in Sec. 4, where we recover the ground truth even when \(Q\) is restricted to be Gaussian. However, we leave a theoretical examination of this conjecture for future work.

In practical terms, restricting the search space to Gaussian \(Q_{MXY}\) reduces the number of optimization variables from being exponential in the dimensionality to quadratic (see Appendix A.2), allowing us to compute the \(_{G}\)-PID for much higher dimensionalities of \(M\), \(X\) and \(Y\). In what follows, we show how the optimization problem for the \(_{G}\)-PID can be written out in closed-form and then solved using projected gradient descent.

### Notation and Preliminaries

Suppose \(M\), \(X\) and \(Y\) are jointly Gaussian random vectors of dimensions \(d_{M}\), \(d_{X}\) and \(d_{Y}\) respectively, with a joint covariance matrix given by \(_{MXY}\). We will make extensive use of the submatrices of \(_{MXY}\), so we explain their notation here:

* \(_{XY}\) will denote the \((d_{X}+d_{Y})(d_{X}+d_{Y})\) joint (auto-)covariance matrix of the vector \([X^{},Y^{}]^{}\).
* \(_{X,Y}\) (note the comma) will denote the \(d_{X} d_{Y}\) cross-covariance matrix between \(X\) and \(Y\).
* \(_{XY,M}\) will denote the \((d_{X}+d_{Y}) d_{M}\) cross-covariance matrix between the concatenated vector \([X^{},Y^{}]^{}\) and the vector \(M\).

In general, groupings of vectors without commas represent joint covariance, while a comma represents a cross-covariance between the groups on either side of the comma. The same notation will also be used for conditional covariance matrices: for example, \(_{XY|M}\) is the conditional _joint_ covariance of \((X,Y)\) given \(M\), while \(_{X,Y|M}\) is the conditional _cross_-covariance _between_\(X\) and \(Y\) given \(M\).

We will also use an equivalent notation for the joint distribution , where \(P_{MXY}\) is parameterized as a "broadcast channel" [24, Ch. 15.6] from \(M\) to \(X\) and \(Y\):

\[X=H_{X}M+N_{X} Y=H_{Y}M+N_{Y}. \]

Here, \(H_{X}_{X,M}\) and \(H_{Y}_{Y,M}\) represent _channel gain matrices_, while \(N_{X}\) and \(N_{Y}\) represent additive noise and are not necessarily independent of each other: \([N_{X}^{},N_{Y}^{}]^{}(0,_ {XY|M})\).

**Remark 1**.: Without loss of generality, we can assume that \(M\), \(X\) and \(Y\) are all zero-mean, and that \(_{M}=I\). Further, we explicitly assume that the \(X\) and \(Y\) channels are individually whitened, i.e., that \(_{X|M}=I\) and \(_{Y|M}=I\). This assumption precludes deterministic relationships between \(M\) and \(X\) or \(Y\), and is required to ensure that information quantities remain finite . 

### Optimizing the Union Information

Bertschinger et al.  showed that the minimizer for the unique information is also the minimizer for the "union information", \(I^{}(M:X;Y) UI_{X}+UI_{Y}+RI\). In other words, we can also solve the following optimization problem, which yields simpler expressions for the objective and gradient:

\[}(M:X;Y)_{Q_{MXY}}I_{Q}(M;X,Y)  Q_{MX}=P_{MX},\;Q_{MY}=P_{MY} \]

Now, suppose \(P_{MXY}\) is Gaussian with covariance \(_{MXY}^{P}\) and the solution \(Q_{MXY}\) is also assumed to be Gaussian with covariance \(_{MXY}^{Q}\). Then, the constraint in (8) implies that \(_{MX}^{Q}=_{MX}^{P}\) and \(_{MY}^{Q}=_{MY}^{P}\). In other words, \(_{M}\), \(_{X}\), \(_{Y}\), and \(_{M,XY}\) are all constant across \(P\) and \(Q\). Therefore, the only part of \(_{MXY}^{Q}\) that is variable is \(_{X,Y}^{Q}\), or equivalently, \(_{X,Y|M}^{Q}\).3 In what follows, we will drop the superscripts denoting the distribution, as this will be clear from context. Generally speaking, we will discuss the optimization problem and thus the distribution will be \(Q\).

**Proposition 1**.: _The union information for the \(_{G}\)-PID of Definition 2 is given by_

\[_{G}}_{_{X,Y|M}} I+_{M}^{-1}_{XY,M}^{}_{XY|M}^{-1} _{XY,M}_{XY|M} 0 \]_where the optimization variable \(_{X,Y|M}\) is an off-diagonal block embedded within \(_{XY|M}\); all other matrices in the objective are constants that are derived from \(_{MXY}^{}\)._

We solve the above optimization problem using projected gradient descent: we analytically derive the gradient and the projection operator for the constraint set as shown below. Then, we use the RProp algorithm [34; 35] for gradient descent, which independently adjusts the learning rates for each optimization parameter (derivations, and details of implementation and complexity are in App. A). Code for our implementation is available on GitHub , and details of the compute configuration are given in App. E.

**Proposition 2**.: _The **objective** in Proposition 1 can be simplified to_

\[f(_{X,Y|M})=I+H_{Y}^{}H_{Y}+B^{ }S^{-1}B, \]

_where \(B(H_{X}-_{X,Y|M}H_{Y})\) and \(S(I-_{X,Y|M}_{X,Y|M}^{})\)._

_The **gradient** of the objective with respect to \(_{X,Y|M}\) is given by_

\[ f(_{X,Y|M})=S^{-1}BI+H_{Y}^{}H_{Y}+B^{}S^{-1}B^{-1}B^{}S^{-1}_{X,Y|M}-H_{Y}^{ }. \]

_A **projection operator** on to the constraint set \(_{XY|M} 0\) can be obtained as follows: let \(_{XY|M}:V V^{}\) be the eigenvalue decomposition of \(_{XY|M}\), with \((_{i})\). Let \(_{i}(0,_{i})\) represent the rectified eigenvalues, and \((_{i})\). Then, define_

\[_{XY|M}  VV^{}, \] \[_{X,Y|M}^{pij} _{X|M}^{-1/2}_{X,Y|M} _{Y|M}^{-1/2}, \]

_where \(_{X|M}\), \(_{Y|M}\) and \(_{X,Y|M}\) are submatrices of \(_{XY|M}\)._

## 4 Canonical Gaussian Examples

In this section, we show how well our \(_{G}\)-PID estimator performs on a series of Gaussian examples of increasing complexity, which have known ground truth. Barrett  showed that, for Gaussian distributions, the \(\)-PID reduces to the MMI-PID (defined below), whenever \(M\) is scalar. These also happen to be cases when the optimal distribution \(Q_{MXY}\) is Gaussian , and thus the \(_{G}\)-PID should recover the ground truth. We then leverage additivity (Property 1) to combine two or more simple examples into complex ones, where ground truth continues to be known.

**Definition 3** (Minimum Mutual Information (MMI) PID).: _Let the redundant information be defined as the minimum of the two mutual informations:_

\[RI_{}(M:X;Y)=\{I(M;X),I(M;Y)\}. \]

_The remaining MMI-PID components, \(UI_{}(M:X Y)\), \(UI_{}(M:Y X)\) and \(SI_{}(M:X;Y)\), follow from equations (1)-(3)._

We first provide a Gaussian analog of Example 1 in Examples 2-4 (for \(d_{M}=d_{X}=d_{Y}=1\)). We will use the channel notation described in Equation (7). Complete derivations for these examples (and some nuances that are omitted here) are presented in App. B.

**Example 2** (Pure uniqueness: variable \(A\) from Example 1).: Suppose \(M(0,1)\), \(H_{X}=1\) and \(H_{Y}=0\), with \(N_{X},N_{Y}\,(0,1)\). Here, only \(X\) receives information about \(M\), while \(Y\) is pure noise. Thus, \(X\) has unique information about \(M\) (\(UI_{X}=I(M;X)>0\)), with no unique information in \(Y\), and no redundancy or synergy (\(UI_{Y}=RI=SI=0\)). 

**Example 3** (Pure redundancy: variable \(B\) from Example 1).: Ideally, we would set \(M(0,1)\), \(X=M\) and \(Y=M\). However, for continuous random variables, \(I(M;X)=\) when \(M=X\). So instead, we set \(M(0,1)\), \(H_{X}=1\) and \(H_{Y}=1\), with \(N_{X}(0,1)\) while \(N_{Y}=N_{X}\) (i.e., \(X=Y\), so they are both the same noisy version of \(M\)). In this case, \(X\) and \(Y\) are fully redundant since they both contain exactly the same information about \(M\). Thus, \(RI=I(M;(X,Y))>0\), while \(UI_{X}=UI_{Y}=SI=0\). 

**Example 4** (Pure synergy: variable \(C\) from Example 1).: We cannot replicate pure synergy for Gaussian variables, but we can approach it in a limit. Let \(M(0,1)\), \(H_{X}=1\) and \(H_{Y}=0\), with \(N_{X}(0,^{2})\) and \(N_{Y}=N_{X}\) (i.e., \(X=M+Y\)). Further, let \(^{2}\). In this case, \(I(M;Y)=0\)and \(I(M;X) 0\) as \(^{2}\), so \(X\) and \(Y\)_individually_ convey little to no information about \(M\). However, we can recover information about \(M\) from \(X\) and \(Y\)_together_ by taking their difference, since \(X-Y=M\). Thus, \(SI>0\), while \(UI_{Y}=RI=0\) and \(UI_{X} 0\). 

Examples 2, 3 and 4 have been provided solely for intuition. Their PIDs can be inferred directly from Equations (1)-(3). We next describe three one-dimensional examples that each have _two_ non-zero PID components. For lack of space, we only provide a brief description and defer details to App. B. We estimate the \(_{G}\)-PID (as well as the \(_{G}\)-PID  and the ground-truth MMI-PID ) for these examples and show that all three are equal (see Fig. 1).

**Example 5** (Unique and redundant information).: Let \(X\) be a noisy representation of \(M\), and let \(Y\) be a noisy representation of \(X\) with standard deviation \(_{Y|X}\). When \(Y=X\) (zero noise), this example reduces to Example 3. As \(_{Y|X}\), \(RI\) reduces while \(UI_{X}\) approaches \(I(M;X)\). 

**Example 6** (Unique and synergistic information).: Let \(M(0,1)\), \(H_{X}=1\), \(H_{Y}=0\) and \(N_{X},N_{Y}(0,^{2})\) such that their correlation is \(\). When \(^{2}\) is finite and \(=0\), this example reduces to Example 2, since there can be no synergy between \(X\) and \(Y\). As \( 1\), \(X-Y M\); so the total mutual information \(I(M;(X,Y))\), driven by synergy growing unbounded, while the unique component remains constant at \(I(M;X)\). 

**Example 7** (Redundant and synergistic information).: Let \(M(0,1)\), \(H_{X}=H_{Y}=1\) and \(N_{X},N_{Y}(0,1)\) such that their correlation is \(\). When \(<1\), \(I(M;X)\) and \(I(M;Y)\) are both equal by symmetry, and thus equal to \(RI\) (see Def. 3 for the MMI-PID, which is ground truth here). As \(\) reduces, the two channels \(X\) and \(Y\) have noisy representations of \(M\) with increasingly independent noise terms. Averaging the two, \((X+Y)/2\), will provide more information about \(M\) than either one of them individually (i.e., synergy), and thus \(SI\) increases as \(\) reduces. 

Figure 1: PID values for Examples 5, 6 and 7. The \(_{G}\)-PID and the \(_{G}\)-PID agree exactly with the MMI-PID, which is known to be the ground truth, since \(M\) is scalar .

Figure 2: Left and right panels respectively show PID values for Examples 8 and 9, which combine two scalar examples with known ground truth, using Property 1. The middle panel shows the absolute error between each PID definition and the ground truth. The \(_{G}\)-PID diverges from the \(_{G}\)- and MMI-PIDs, and is the only one that agrees with the ground truth. The \(_{G}\)-PID maintains an error less than \(10^{-7}\) bits, whereas other definitions have errors greater than \(0.1\) bits.

The next set of examples will use the examples presented above in different combinations. This ensures that, where possible, the ground truth remains known in accordance with Property 1. These examples are also designed to reveal the differences between the \(\)-PID, the MMI-PID and the \(\)-PID: in particular, they show how the MMI-PID and the \(\)-PID fail where the \(\)-PID does not. These examples use two-dimensional \(M\), \(X\) and \(Y\), i.e., \((d_{M},d_{X},d_{Y})=(2,2,2)\). A diagrammatic representation of Examples 8 and 9 is given in App. B.2.

**Example 8**.: Let \(X_{1}= M_{1}+N_{X,1}\), \(Y_{1}=M_{1}+N_{Y,1}\), \(X_{2}=M_{2}+N_{X,2}\) and \(Y_{2}=3M_{2}+N_{Y,2}\), where \(M_{1},M_{2},N_{X,i},N_{Y,i}\) i.i.d. \((0,1)\), \(i=1,2\). Here, \((M_{1},X_{1},Y_{1})\) is independent of \((M_{2},X_{2},Y_{2})\), therefore using Property 1, we can add the PID values from their individual decompositions (which each have known ground truth via the MMI-PID since \(M_{1}\) and \(M_{2}\) are scalar). Fig. 2(l) compares the \(_{G}\)-PID, the \(_{G}\)-PID and the MMI-PID for the joint decomposition of \(I(M;(X,Y))\), at different values of \(\), the gain in \(X_{1}\). Only the \(_{G}\)-PID matches the ground truth, as it is the only definition here that is additive. 

**Example 9**.: Let \(M\) and \(Y\) be as in Example 8. Suppose \(X=H_{X}\)\(R()\)\(M\), where \(H_{X}\) is a diagonal matrix with diagonal entries 3 and 1, and \(R()\) is a \(2 2\) rotation matrix that rotates \(M\) by an angle \(\). When \(=0\), \(X\) has higher gain for \(M_{1}\) while \(Y\) has higher gain for \(M_{2}\). When \(\) increases to \(/2\), \(X\) and \(Y\) have equal gains for both \(M_{1}\) and \(M_{2}\) (barring a difference in sign). Since \((M_{1},X_{1},Y_{1})\) is not independent of \((M_{2},X_{2},Y_{2})\) for all \(\), we know the ground truth only at the end-points. Nonetheless, the example shows a difference between the three definitions, as shown in Fig. 2(r). 

**Example 10**.: In this example, we test the stability of the \(_{G}\)-PID as the dimensionality, \(d d_{M}=d_{X}=d_{Y}\) increases. By Property 1, if we take two i.i.d. systems of variables \((M,X,Y)\) at dimensionality \(d\) and concatenate their respective variables, every PID component of the composite system of dimensionality \(2d\) should be double that of the original. This process can be repeated, taking two independent \(2d\)-dimensional systems and concatenating them to create a \(4d\)-dimensional system. Fig. 3 shows precisely this process starting with the system in Example 8 with \(d=2\), and continually doubling its size until \(d=1024\). The \(_{G}\)-PID accurately matches ground truth by doubling in value, and remains stable with small relative errors (shown in App. B.5). The \(_{G}\)-PID also runs much faster than the \(_{G}\)-PID on this example, providing a speed-up of more than \(1000\) at \(d=64\) (see right-most plot in Fig. 3, and extended results in App. B.4). 

**Remark 2**.: In the above examples, the ground truth referred to the \(\)-PID of Def. 1, since it was inferred using Def. 3 and additivity (Property 1). Since our method for computing the \(_{G}\)-PID recovers the ground truth \(\)-PID, we can infer that the distribution of the optimal \(Q_{MXY}\) for the \(\)-PID is in fact Gaussian in these examples. This provides empirical evidence in support of the conjecture stated in Sec. 3. 

## 5 Estimation and Bias-correction for the \(_{G}\)-PID

Having discussed how to compute the \(_{G}\)-PID and shown that it agrees well with ground truth in several canonical examples, we discuss how the \(_{G}\)-PID may be estimated from data. Given a sample

Figure 3: The first five plots on the left show PID values for Example 10. Different shadings represent different values of gain in \(X_{1}\) (\(\)) from Example 8. The \(_{G}\)-PID doubles every time \(d\) doubles as seen by the constant 45\({}^{}\) slope on the base-2 log-log plot, even when \(d_{M}=d_{X}=d_{Y}=1024\). The right-most plot shows a timing comparison between the \(_{G}\)- and the \(_{G}\)-PIDs on this example. The \(_{G}\)-PID failed for \(d>64\); the \(_{G}\)-PIDâ€™s timing performance up to \(d=1024\) is shown in App. B.4.

of \(n\) realizations of \(M\), \(X\) and \(Y\) drawn from \(P_{MXY}\), we may estimate the sample joint covariance matrix \(_{MXY}\). We therefore use the straightforward "plug-in" estimator for the \(_{G}\)-PID, by using \(_{MXY}\) in place of \(_{MXY}\) in the optimization problem in equation (9).

However, it is well-known that estimators of information-theoretic quantities suffer from large biases for moderate sample sizes . Cai et al.  characterized the bias in the entropy of a \(d\)-dimensional Gaussian random vector, for a fixed sample size \(n\).

**Proposition 3** (Bias in Gaussian entropy ).: _Suppose \(M^{d_{M}}\) has an auto-covariance matrix \(_{M}\). The entropy of \(M\) is \(H(M)=(2 e_{M})\) when \(_{M}\) is known . For the sample covariance matrix \(_{M}\), the bias is given by:_

\[(M)=_{k=1}^{d_{M}}(1-k/n). \]

For a proof, we refer the reader to [38, Corollary 2]. This result may be naturally extended to compute the bias of each of the mutual information quantities in the LHS of equations (1)-(3):

**Corollary 4** (Bias in Gaussian mutual information).: _For the joint mutual information \(I(M;(X,Y))\),_

\[M;(X,Y)=_{k=1}^{d_{M}} (1-k/n)\ +_{k=1}^{d_{X}+d_{Y}}\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!100 runs of two configurations called "Bit-of-all" (with a little bit of each PID component) and "Fully-redundant" (which has predominantly redundancy), with \(d_{M}=d_{X}=d_{Y}=10\) (details and additional setups in App. C). We find that bias correction brings the PID values closer to their true values even at small sample sizes. In App. C.3, we also include a preliminary analysis of the variance of PID estimates using bootstrap [39, Ch. 8].

## 6 Application to Simulated and Real Neural Data

So far, we have only considered the \(_{G}\)-PID when applied to _Gaussian_\(P_{MXY}\). Although Def. 2 strictly applies only to Gaussian \(P_{MXY}\), the estimation process in Sec. 5 relies only on a sample covariance matrix, which is well-defined for a wide variety of non-Gaussian distributions. Many applications where PIDs could be useful have non-Gaussian data. For instance, there is great interest in applying PIDs in neuroscience (e.g., to understand how multiple brain regions jointly encode and communicate information ), but spike-count distributions are non-Gaussian.

**Simulated neural data.** To show that our \(_{G}\)-PID estimates provide reasonable results on non-Gaussian spiking neural data, we first simulate spike-count data using Poisson random variables (following ; described in App. D.1). We evaluate the ground truth \(\)-PID for this distribution using the discrete PID estimator of Banerjee et al. . The \(_{G}\)-PID is estimated from a sample covariance matrix using \(10^{6}\) realizations of \(M\), \(X\) and \(Y\). We find that the \(_{G}\)-PID closely matches

Figure 5: A comparison of the \(_{G}\)-PID, the \(_{G}\)-PID and the MMI-PID for a multivariate Poisson system. The ground truth is a discrete \(\)-PID computed using the package of Banerjee et al. . The \(_{G}\)-PID comes closest to the ground truth (possibly because they compute the same PID definition), despite the fact that the \(_{G}\)-PID only uses the covariance matrix of the Poisson distribution, whereas the ground truth uses knowledge of the distribution itself.

Figure 6: Bias-corrected redundancy estimates for information about VISp activity that is shared between VISI and VISal: redundancy in bits (left) and redundancy as a fraction of total mutual information (right). Data spread is across 40 mice. Statistical comparisons use a two-sided Mann-Whitney-Wilcoxon test. Observe that there is greater and more sustained redundancy on flashes corresponding to image changes, which are behaviorally relevant and linked to rewards in this task.

the ground truth for a range of parameter values, despite the fact that the \(_{G}\)-PID is effectively computed on a Gaussian approximation of a Poisson distribution (Fig. 5). More examples of the \(_{G}\)-PID applied to non-Gaussian data are provided in App. D.2 (also see App. A.2). We conclude that it is reasonable to use and interpret the \(_{G}\)-PID on non-Gaussian spike count data.

**Real neural data.** We then applied our bias-corrected \(_{G}\)-PID estimator to the Visual Behavior Neuropixels dataset collected by us at the Allen Institute . We recorded over 80 mice using six neuropixels probes targeting various regions of visual cortex, while the mice were engaged in a visual change-detection task. In the task, images from a set of 8 natural scenes were presented in 250 ms flashes, at intervals of 750 ms; the image would stay the same for a variable number of flashes after which it would change to a new image. The mouse had to kick to receive a water reward when the image changed. Thus, a given image flash could be a behaviorally relevant target if the previous image was different, or not, if the previous image was the same.

We used our bias-corrected PID estimator to understand how information is processed along the visual hierarchy during this task. We estimated the \(_{G}\)-PID to understand how information contained in the spiking activity of primary visual cortex (VISp) was represented in two higher-order visual cortical areas, VISI and VISal. We aligned trials to the onset of a stimulus flash, binned spikes in 50 ms intervals and considered the top 20 principal components (to achieve reasonable estimates at these sample sizes; explained further in App. D.5) from each region in each time bin. We computed the \(_{G}\)-PID on the sample covariance matrix of these principal components (shown in Fig. 6). We found that there was a significantly larger amount of redundant information about VISp activity between VISI and VISal for stimulus flashes corresponding to an image change, compared to flashes that were not changes (Fig. 6(l)). The larger redundancy was also sustained slightly longer for flashes corresponding to changes, than non-change flashes. Both of these effects were maintained even when the redundancy was normalized by the joint mutual information, suggesting that the effect was not purely due to an increase in the total amount of information (Fig. 6(r)). Our results suggest that the visual cortex propagates information throughout the hierarchy more robustly when such information is relevant for behavior.

## 7 Discussion

In this paper, we proposed a new and efficient method for estimating the \(_{G}\)-PID for Gaussian distributions. We showed that our method recovers the ground truth and suitably corrects for bias through a series of examples. In particular, Fig. 4 showed how large the biases can be at small sample sizes, which makes bias correction particularly important in neuroscientific settings where sample sizes are often small.

We focused on Gaussian distributions, as they have historically been a starting point for many estimators (e.g., correlation is used as a measure of dependence, but zero correlation implies independence only in the Gaussian case). We were able to show ground-truth validation for our \(_{G}\)-PID estimator at high dimensionalities only thanks to the existence of closed-form results on scalar Gaussians.

While our central claims and results applied to Gaussian distributions, our method for computing PIDs did not immediately break down for distributions close to Gaussian in some limit (e.g., Poisson). The effective spiking rate used in our multivariate Poisson distribution was also not particularly large (see App. D.1); we would expect our estimates to improve for higher firing rates, since the Poisson distribution will then be more Gaussian.

**Limitations.** Our work has several limitations that require further theory and simulations to resolve, the most important of which are: (1) Our estimator is technically a bound on the PID values because we assume Gaussian optimality in Definition 2; (2) Our bias-correction method is heuristic: we do not provide a rigorous theoretical characterization of the bias of PID values.

**Broader impacts.** Our work is mainly methodological, so the scope for negative impacts depends on how the methods might be used. For example, incorrect interpretations drawn from the use of our PID estimators may affect scientific conclusions. In particular, the PID is inherently a correlational quantity and carries the same caveats: it may not be appropriate to make causal interpretations on the basis of observed PID values. Also, despite our best efforts to explore a variety of systems, we cannot tell how accurate our bias-correction method will be in novel configurations.