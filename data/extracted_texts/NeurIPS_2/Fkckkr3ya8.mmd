# Faith and Fate:

Limits of Transformers on Compositionality

 Nouha Dziri\({}^{1}\), Ximing Lu\({}^{1,2*}\), Melanie Sclar\({}^{2*}\),

Xiang Lorraine Li\({}^{1}\), Liwei Jiang\({}^{1,2}\), Bill Yuchen Lin\({}^{1}\),

**Peter West\({}^{1,2}\), Chandra Bhagavatula\({}^{1}\), Ronan Le Bras\({}^{1}\), Jena D. Hwang\({}^{1}\), Soumya Sanyal\({}^{3}\), Sean Welleck\({}^{1,2}\), Xiang Ren\({}^{1,3}\), Allyson Ettinger\({}^{1,4}\), Zaid Harchaoui\({}^{1,2}\), Yejin Choi\({}^{1,2}\)**

\({}^{1}\)Allen Institute for Artificial Intelligence \({}^{2}\)University of Washington

\({}^{3}\)University of Southern California \({}^{4}\)University of Chicago

nouhad@allenai.org, ximinglu@allenai.org, msclar@cs.washington.edu

First co-authors. \(\) Second co-authors.

###### Abstract

Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative _compositional_ tasks--multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with increased task complexity.

## 1 Introduction

_"It was the epoch of belief, it was the epoch of incrediulity." - Charles Dickens, A Tale of Two Cities_

Large-scale transformers such as ChatGPT  and GPT4  demonstrate unprecedented capabilities , even noted as "sparks of AGI" . In stark contrast, the same models sometimes struggle with simple, intuitive tasks . For instance, humans can solve 3-digit by 3-digit multiplication arithmetic after learning basic calculation rules . Yet, off-the-shelf ChatGPT and GPT4 achieve only 55% and 59% accuracies on this task, respectively (SS3).

The striking discrepancy between the impressive successes of transformer LLMs on _seemingly complex_ tasks and the astonishing failures on _seemingly trivial_ tasks spark critical open questions about how to faithfully interpret their mixed capabilities. Under what conditions do transformers succeed, fail, and why? What types of errors do they make? Can transformers uncover implicit problem-solving rules or be taught to follow reasoning paths?

Seeking thorough answers to these questions remains an open research challenge. However, we offer novel insights into the fundamental limits of transformers2, centered around _compositional_problems_ that require strict multi-hop reasoning to derive correct predictions. Applying step-by-step reasoning is fundamental to human intelligence [69; 68]. These compositional problems present compelling challenges for AI systems as they require combining basic reasoning operations to follow computational paths that arrive at unique correct solutions. In particular, we study three straightforward and flexible representative compositional tasks: long-form multiplication, logic grid puzzles (i.e., Einstein's puzzle ), and a classic dynamic programming problem.

We propose two hypotheses. **First**, transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized path matching. This contrasts with the systematic multi-step reasoning approach that learns to apply underlying _computational rules_ required for building correct answers [71; 37; 27]. Shortcut learning  via pattern-matching may yield fast correct answers when similar compositional patterns are available during training but does not allow for robust generalization to uncommon or complex examples. **Second**, due to error propagation, transformers may have inherent limitations on solving high-complexity compositional tasks that exhibit novel patterns. Errors in the early stages of the computational process can lead to substantial compounding errors in subsequent steps, preventing models from finding correct solutions.

To investigate our hypotheses, we formulate compositional tasks as _computation graphs_. These graphs break down problem-solving into submodular functional steps, enabling structured measurements of complexity and verbalization of computational steps as input sequences to language models. Moreover, we leverage information gain to predict patterns that models are likely to learn based on the underlying task distribution without the need to perform full computations within the graph.

Empirical results show that training on task-specific data leads to near-perfect performance on in-domain instances and under low compositional complexity, but fails drastically on instances outside of this region. This substantial gap suggests that systematic problem-solving capabilities do not emerge from maximum likelihood training  on input-output sequences, even when prompted or trained with human-like reasoning steps (i.e., a linearization of computation graphs; SS3.1). Models' success can be attributed, in part, to their exposure to training examples sub-graphs that involve the same computations required for solving test examples (see Section 3.2.2) In order to gain a deeper understanding of models' failures, we conduct a comprehensive analysis by decomposing their computation graphs and examining different error types. We find that while models can memorize single-step operations, they fail to compose them into correct reasoning paths, suggesting that they mostly make predictions based on shallow, rote learning rather than a deep, holistic task understanding (SS3.2.3). Importantly, we provide theoretical evidence of exponential error accumulation using abstract compositional tasks. All tasks analyzed empirically in this paper are instantiations of these abstractions (SS4). We argue that transformers could be inherently limited in solving compositionally complex tasks out-of-the-box3.

As transformers continue to make tangible real-world impacts, it is pressing to interpret their remarkable performance critically. Our work takes a realistic look at the limitations of transformers in the context of compositional tasks. To shed light on practical future steps, we identify directions for addressing these limitations, such as using transformers for tasks that could be decomposed into few reasoning steps, tasks where evaluation may afford some leniency, and using transformers in combination with planning modules or refinement methods to improve their generations. To advance language AI, fundamental innovations are required to address or complement these limitations.

## 2 Measuring Limitations of Transformers in Compositional Tasks

Human problem-solving skills can be conceptualized as a graph structure, where each vertex represents a partial solution and the edges represent operators that can be applied to modify these solutions. As we will outline next and illustrate in Figure 1, we use computation graphs and corresponding metrics to methodically evaluate transformers' reasoning abilities.

### Computation Graph Definition

Let \(A\) be a deterministic algorithm (function), and let \(_{A}\) be a set of primitives (functions) the algorithm uses in its execution. Assuming the inputs \(\) to algorithm \(A\) are given, we define \(A()\)'s static computation graph \(G_{A()}\). \(G_{A()}\ =\ (V,\,E,\,s,\,op)\) is a directed acyclic graph. Nodes \(V\)represent all variables' values during \(A\)'s execution: each node \(v V\) has a value \(s(v)\) associated. Edges \(E\) represent the function arguments involved in some computation: for each non-source node \(v V\), let \(U=\{u_{1},,u_{j}\} V^{j}\) be its parent nodes. Then, \(s(v)=f(u_{1},,u_{j})\) for some \(f_{A}\). Since each node \(v\) is uniquely defined by the computation of a single primitive \(f\), we define \(op:V_{A}\) as \(op(v)=f\).

Let \(S V\) be the source nodes of \(G_{A()}\) and without loss of generality, let \(o V\) be its sole leaf node. By definition, \(S\) and \(A()=s(o)\), representing the input and output of \(A\) respectively.

To be able to train and evaluate a language model's ability to follow algorithm \(A\) we must linearize \(G_{A()}\). Since we only consider autoregressive models, this linearization must also be a topological ordering.

### Quantifying Compositional Complexity using Graph Metrics

\(A\)'s representation as a computation graph \(G_{A()}\) enables measuring task complexity from many angles.

We define a node \(v V\)'s _layer number_ as the length of the longest path from a source node to \(v\) in the directed acyclic graph \(G_{A()}\). We then define the **reasoning depth** as the largest layer number in the graph. In computation graphs, reasoning depth is a proxy for the maximum level of multi-hop reasoning required to solve the task.

Let \(d_{S}:\ V_{0}\) be the shortest distance to any of \(G\)'s source nodes \(S V\). We define the **reasoning width** of a graph as the mode of \(\{d(v)\,:\,v V\}\). This metric aims to measure the maximum number of variables required to maintain in parallel during the computation. Relatedly, we also define the **average parallelism** of a graph as the ratio between \(|V|\) and its reasoning depth. This aims to compute the average width in computation through the graph, and not just in its mode.

### Predicting Surface Patterns through Relative Information Gain

When evaluating model performance, we may observe partially correct answers even in an overall incorrect response. To understand model strategies in these partial successes, we use Relative Information Gain to predict surface patterns that models are likely to recognize. We represent task \(T\) as a distribution \((X_{1},,X_{n},Y_{1},,Y_{m})\) and measure the amount of (normalized) information gained about an output element \(Y_{j}\) by observing a subset of input random variables \(X\{X_{1},,X_{n}\}\):

\[(Y_{j},X)=)-H(Y_{j}|X)}{H(Y_{j})} \]

RelativeIG may be used to analyze the influence of any node in the computation graph (as defined in SS2.1) with respect to a set of its ancestors; in particular, output nodes with respect to input nodes.

### Exploring Three Representative Compositional Tasks: Definitions

MultiplicationMulti-digit multiplication requires executing operations with numerical symbols based on procedural rules . This task has multiple algorithmic solutions; in constructing computation graphs, we use the well-known \(O(k_{1}k_{2})\) long-form multiplication algorithm for computing \(x y\), where \(x\) has \(k_{1} 5\) digits and \(y\) has \(k_{2} 5\) digits in base 10. See SSA.1 for data construction details.

Figure 1: Transformation of an algorithm \(A\) to its computational graph \(G_{A()}\). The depicted example is of long-form multiplication algorithm \(A\), for inputs \(=\) (i.e. computing \(7 49\)).

To instantiate \(G_{A()}\), let \(_{A}=\{\}\). Source nodes \(S\) are digits of input numbers, leaf node \(o\) is the final output, and intermediate nodes \(v\) are partial results generated during execution of the long-form multiplication algorithm (see Figure 1).

Einstein's PuzzleEinstein's puzzle is a well-known logic puzzle often used as a benchmark for solving constraint satisfaction problems . It involves a list of houses with different attributes (e.g., owner's name, pets), and the goal is to determine which attributes belong to each house by combining a set of pre-defined natural language clues or constraints. The solution to the puzzle is a matrix of size \(K M\), where \(K\) represents the number of houses and \(M\) the number of attributes. As \(K\) and \(M\) increase, synthesizing different partial solutions that satisfy individual constraints becomes highly compositionally complex. To construct the computation graph, we consider a greedy algorithm that iteratively eliminates possible solutions by filling at least one cell each time. It deterministically fills the cell(s) that requires the minimum number of clues among all current unfilled cells. We refer to this as the _elimination function_. See SSA.2 for examples, data construction, and algorithm details.

To instantiate \(G_{A()}\), let \(_{A}\!=\!\{\}\). The source nodes are the clues, all intermediate nodes are partially-filled matrices, and the output node is a fully-filled solution matrix.

Dynamic Programming ProblemDynamic programming (DP) recursively breaks down complex problems into simpler sub-problems, so problems solved using this technique are compositional. We analyze a classic relaxation of the NP-complete Maximum Weighted Independent Set problem : _Given a sequence of integers, find a subsequence with the highest sum, such that no two numbers in the subsequence are adjacent in the original sequence_. This relaxation may be solved in \(O(n)\) time using DP. See the solution in SSA.3. In the experiments, we restrict each integer to the \([-5,5]\) range.

To instantiate \(G_{A()}\), let \(_{A}=\{\}\). Source nodes are elements of the input list, and the output node is a list that for each element indicates whether it should be selected. We select an \(O(n)\) algorithm since \(G_{A()}\)'s size is proportional to \(A\)'s complexity.

## 3 Testing the Limits of Transformers: Empirical Evidence

Experimental SetupTo understand the capabilities of LLMs, we evaluate GPT3 (text-davinci-003) , ChatGPT (GPT-3.5-turbo)  and GPT4 (gpt-4)  using zero-shot, few-shot, and finetuning techniques. To enable the generation of computation graphs beyond the final answers, we use the concept of _scratchpads_. Scratchpads are a verbalization of the computation graphs (i.e., a linearized representation of a topological ordering of \(G_{A()}\)). Overall, we consider _question-answer_ and _question-scratchpad_ formats for few-shot and finetuning settings to gauge models' capabilities for learning with and without explicit reasoning. See details of additional models and experimental configurations in SSB and examples of scratchpad in SSA.

### Testing the Limits of Transformers with Zero-shot, Few-shot and Finetuning

Limits of transformers in zero- and few-shot settingsTo investigate the inherent problem-solving capabilities of LLMs, we begin by analyzing models' zero-shot and few-shot performances on our compositional tasks. As shown in Figure 2, task performances deteriorate significantly from near perfection to zero with increasing complexity when measured by either problem size (Figure 2(a))or

Figure 2: (a) **Zero-shot accuracy.** Axes refer to problem sizes (number of digits in multiplication, number of houses and attributes in puzzle, and sequence length in the DP task). Transformers’ accuracy decreases to near zero as task complexity increases, measuring task complexity by the problem size. (b) **Average parallelism** negatively correlates with accuracy.

average parallelism (Figure 2(b)).The trend remains the same for few-shot prompting (see SSB.2). These results indicate that pre-training is in fact not sufficient to teach models how to combine basic operations to solve compositional problems, especially as problems grow more complex.

Limits of transformers with question-answer trainingThe limited performance of models may be attributed to the lack of task-specific data during pre-training. To fully bring out models' potentials in solving these tasks, we next exhaustively finetune GPT3 with question-answer pairs. In multiplication and DP, we finetune models with all enumerations of questions up to the maximum problem size4 within reasonable training budget, leaving out 10% for validation and 10% for testing. In puzzles, we train on a subset of all instances up to \((K,M)(4,4)\) due to combinatorial explosion. We separately finetune GPT3 models on \(\)1.8M multiplication pairs, \(\)142K DP pairs, and \(\)41K puzzle pairs (see details in SSB.3). Additionally, to examine problems of different complexity, we consider different training splits based on the depth and width of computation graphs.

Figure 3 and Figure 4(a) show high accuracy for examples with splits seen during training, i.e., _in-domain_. However, the performance sharply declines when evaluating unseen splits during training, i.e., _out-of-domain_ (_OOD_). Similar trends hold in all tasks (see SS B.3), suggesting that systematic problem-solving capabilities do not emerge via exhaustive training on task-specific data.

Limits of transformers with explicit scratchpad trainingNext, we test whether we can explicitly teach models the required computational operations via _scratchpads_. To do so, we finetune GPT3 with question-scratchpad pairs for all tasks. We consider the same distribution splits as before. The results, presented in Figure 4(b), show that once again GPT3 achieves near-perfect performance on in-distribution, but fails entirely in generalizing to OOD cases--in particular, wider or deeper computation graphs. These results indicate that even when training directly with guidance on the computation steps, models still fail to learn component operations in a generalizable manner. This observation holds for all tasks (See details in SS B.4). Similarly, prompting transformers with question-scratchpad pairs enhances the performance compared to the zero-shot setting (refer to SS B.5). However, this performance boost diminishes to zero as complexity increases. These findings suggest that the autoregressive characteristic of transformers, which forces them to tackle problems sequentially, presents a fundamental challenge that cannot be resolved by instructing the model to generate a step-by-step solution. Instead, models depend on a greedy process of producing the next word to make predictions without a rigorous global understanding of the task.

Limits of transformers with grokkingWe explore whether extended training beyond overfitting leads to improved generalization abilities, a phenomenon known as grokking . Due to budget constraints, we only experiment on the multiplication task. Following , we fine-tune GPT3 with question-answer pairs for 420K steps and separately finetune GPT3 with question-scratchpad pairs for 30K steps. Both models' training far exceeded the point at which in-domain accuracy plateaus5. Figure 4 shows no improvement in generalization for OOD cases beyond the overfitting point, even

Figure 4: Results of training beyond the overfitting point for the multiplication task with the goal of exploring whether OOD generalization capabilities (i.e., _grokking_) arise.

Figure 3: GPT3 finetuned exhaustively on task-specific data up to a certain problem size. The **blue** region represents the in-distribution examples and the **red** region refers to OOD examples. The same trend is observed for the puzzle task (See SSB.2)

after extensive training periods. We hypothesize that the absence of grokking may be due to the level of difficulty of the task. We speculate that increased task difficulty significantly impedes learning a well-structured representation, which, according to , aligns with achieving grokking. Even if grokking were to emerge through more prolonged training, such an approach would prove inefficient and unscalable. Future work is required to accurately explain when and how grokking occurs.

### Breaking Down Successes and Failures of Transformers

#### 3.2.1 Information Gain Explains Where Transformers Partially Excel

At times transformers predict partially correct answers even when the overall response is incorrect. We speculate that this may be due to particularities in the task distribution that allow for guessing partial answers without performing the full multi-step reasoning that the task requires.

Using relative information gain (defined in SS2.3), we can predict surface patterns that a model is likely to learn and contrast them empirically. For multiplication, relative information gain shows that the first digit (two digits) of the output highly correlates with the first digit (two digits) of each input number (see SSC.1). Hence, this spurious pattern is likely to be learned by a model. Similarly, the prediction of the last digit (or two digits) of the output is observed to solely rely on the last digit (or two digits) of each input number. This pattern holds true due to the principles of modulo arithmetic, which ensures the validity of this relationship in all cases. Empirically, we verify that models indeed learn the patterns we predicted and other patterns as well (e.g., order of magnitude of the answer, number of trailing zeros for multiplication) in all the settings with and without scratchpad. See details for multiplication, plus dynamic programming task analysis in SSC.

These experiments suggest that if an output element heavily relies on a single or a small set of input features, transformers are likely to recognize such correlation during training and directly map these input features to predict the output element in testing, without going through the rigorous multi-hop reasoning and giving a false illusion of performing compositional reasoning.

#### 3.2.2 Transformers Reduce Multi-Step Compositional Reasoning into Linearized Subgraph Matching

We now explore whether models' correct predictions on unseen test data are due to learning the underlying algorithm or, instead, explainable by exposure to similar training examples. We hypothesize that, beyond simple memorization, transformers largely rely on pattern matching for solving these tasks. To test this, we calculate the average frequency with which partial computations needed to solve an instance appear in the training data, for both correctly and wrongly predicted examples.

Given a model-generated computation graph \(_{A()}\) we analyze how often the full computation of each node \(v\) is seen in training. We define \(v\)'s _full computation_ as the subgraph induced by all ancestors of \(v\) including \(v\), denoted \(FC_{_{A()}}(v)\). We say that \(FC_{_{A()}}(v)\) is seen during training if \(FC_{_{A()}}(v) FC_{G_{A(^{})}}(w)\) for some computation graph \(G_{A(^{})}\) in training, and for some \(w V\). We characterize complexity of a full computation subgraph by its depth, as defined in SS2.1.

Figure 6 shows that full computation subgraphs appear significantly more frequently in the training data for correctly predicted test examples than for incorrectly predicted ones, for both the multi

Figure 5: **GPT3 finetuning and prompting accuracy** on different data splits. Although the in-distribution performance is almost perfect, GPT3 exhibits poor generalization with increasing graph depth and width. Refer to §B.3 and §B.4 for results on the puzzle and DP tasks.

plication and DP task (both frequencies tend to zero for large depths since we ensured a disjoint train/test split). This high correlation suggests that pattern matching--and not general reasoning capabilities--may be the cause behind correct model outputs. This type of learning could be largely effective when the compositional complexity of tasks is low but it becomes less efficient when tasks are increasingly complex. This may elucidate the observed performance gain in low-complexity and in-domain cases and the striking performance drop in OOD and highly complex cases.

#### 3.2.3 What Types of Errors do Transformers Make at Different Reasoning Depths?

For clearer understanding of where transformers fall short, we analyze the types of errors that transformers make for nodes at different layers in the computation graph. For every input \(\), we compare the ground truth computation graph \(G_{A()}\) with the (possibly incorrect) model-generated computation graph \(_{A()}\). We consider a node \(v\) as having a _correct value_ if and only if \(s(v)=(v)\).6. We consider a node \(v\) to be derived from a _correct computation_ if given that \(U=\{u_{1},,u_{k}\}\) are the immediate predecessors of \(v\) in \(_{A()}\) and that \((v)=f\), we have that \(f(u_{1},,u_{k})=(v)\). Note that the notion of correct computation is independent of \(G\), and that a node \(v\) derived from a correct computation may not have the correct value if an error occurred in some of its ancestors.

We classify each node \(v\) into one of four categories. Node \(v\) is **fully correct** if \(v\) and its ancestors have correct values and are derived from correct computations. If a node \(v\) is not fully correct, its error can be of the following types: \(v\) has a **local error** if its parent nodes have correct values but \(v\) is derived from an incorrect computation (i.e., a one-hop reasoning error); \(v\) has a **propagation error** if \(v\) is derived from a correct computation but some of its parent nodes have incorrect values; \(v\) has a **restoration error** if it has a correct value but is derived from an incorrect computation.

Figure 7 shows results for few-shot GPT4 and fine-tuned GPT3 with scratchpad, with respect to graph layer number for each node. In all settings, the ratio of fully correct nodes is almost perfect but sharply decreases toward zero with increasing graph layers. Moreover, the ratio of propagation errors is usually higher than the ratio of local errors. Both phenomena suggest that models are able to correctly perform single-step reasoning, potentially due to memorizing such single-step operations during training, but fail to plan and compose several of these steps for an overall correct reasoning.

Both the DP and the puzzle tasks have a high ratio of restoration errors, suggesting memorization since correct outputs are produced despite incorrect computations. There are signs of memorization even when restoration errors are near zero: 82.3% of the final correct answers for 4-digit by 2-digit multiplications (a setting unseen during training) had at least one error in the computation graph, but still produced correct answers. These patterns are possibly due to high frequency of (input, output) multiplication pairs in the pretraining data, in contrast to intermediate reasoning steps.

Figure 6: Average frequency in which test examples’ full computations subgraph appear in the training data w.r.t. the subgraph depth, grouped by final answer.

Figure 7: Ratio of nodes in each of the four correct/error categories for each layer in computation graph. Results shown are for few-shot prompting and fine-tuning with scratchpad.

Error Propagations: The Theoretical Limits

Experiments (SS3) highlight the limitations of current transformers in handling complex, multi-step reasoning tasks. Concretely, we show that errors rapidly escalate as the problem size grows (SS3.2.3). Here, we aim to provide theoretical insights into why autoregressive transformer LLMs can perform significantly worse in compositional tasks as the problem size increases, making explicit the different ways in which compounding stochastic errors affect final performance. We argue using stylized examples that transformers may be too limited to solve compositionally complex tasks. Formal statements and full proofs are provided in SSD.

Algorithms designed to solve compositional tasks typically involve multiple independent applications of a function and/or iterated applications of the same function. A transformer executing such an algorithm acts as an estimator of these functions. In this context, we examine the probability of such an estimator reaching the correct answer as the problem size increases. We first consider a scenario where a transformer estimates an algorithm requiring \(n\) independent applications of a function:

**Proposition 4.1** (informal).: _Let \(f_{n}\) involve the combination \(h_{n}\) of \(n\) independent applications of a function \(g\). Let \(,,_{n}\) be their estimators. Assume that \(_{n}\) is a perfect estimator of \(h_{n}\) and that \(h_{n}\) has low collision, with \(c_{n}\) being an upper bound of \(h_{n}\)'s collision rate (\(c_{n}<c\)\( n\), with \(c 1\)). If \((g)=>0\) where \(\)'s errors are independent, then \((f_{n}_{n})>1-c_{n}-(1-)^{n}(1-c _{n})\). This implies that \((f_{n}_{n})\) decreases **exponentially** as \(n\) increases, with \(_{n+}(f_{n}_{n}) 1-c\). Moreover, if \(c_{n}^{n}\) for some \((0,1),>0\), \((f_{n}_{n})\) tends exponentially to \(1\) as \(n\) increases._

Prop. 4.1's proof (SSD.1) shows the rate of convergence is exponential, thus concluding that transformers will rapidly fail with increasing \(n\). Let's now analyze the iterated application function scenario.

**Proposition 4.2** (informal).: _Let \(f_{n}()=g^{n}()\) involve the repeated application of \(g\). Assume that the probability of recovering from a mistake due to the randomness of applying the estimator on an incorrect input has probability at most \(c\). If \((g)=>0\), then \((f_{n}_{n})\) decreases **exponentially** with \(n\). Precisely, \((f_{n}_{n}) 1-(1--c)^{n-1}(1- -c/(c+))\), implying \(_{n+}(f_{n}_{n}) 1-c/(c+)\)._

The argument is as follows. Let \(s_{n}:=(f_{n}=_{n})\), where \(s_{1}=1-\) by definition. Derive \(s_{n}(1--c) s_{n-1}+c\) using law of total probability. Then, prove by induction a non-recursive upper bound for \(s_{n}\) with limit \(\) when \(n+\). See formal statement and derivation in SSD.2.

Prop. 4.2's proof also shows an exponential rate of convergence. Note that if \(c\) then \(_{n+}(f_{n}_{n}) 1\). It is reasonable to assume \(c\) when \(g\) has low collision, since \(c\) represents the probability of the estimator \((y)\) arriving at the correct output \(g(x)\) by chance when given the wrong input \(y x\). More details in SSD.3.

Moreover, repeated applications of a function often imply unbounded errors: if \(g(x)\) can be expressed as an affine transformation \(Fx+c\), then it may be viewed as a first-order vector autoregression, which are known to be unstable when \(|| 1\) for at least one \(\) eigenvalue of \(F\)[31, Prop. 10.1]. While we make these arguments with affine maps, similar behaviors, possibly even more acute, could occur with nonlinear maps --but their study is beyond the scope of this paper.

In Prop. 4.2's current form, we implicitly assume that there is a single valid reasoning for each input since \(g\) is a function. We can potentially generalize this assumption with a state-transition framing, where the probability of transitioning from a valid state to an invalid one is \(\), and the probability of recovering from an invalid state is at most \(c\). See formal statement in D.2.

All tasks evaluated in the present work can be seen as instances of the results just proven. Prop. 4.1 directly applies to multiplication, since \(m\)-by-\(n\) digit multiplication can be seen as \(n\) independent instances of \(m\)-by-\(1\) digit multiplication (see Cor. D.1). Prop. 4.2 directly applies to the recursive function of the dynamic programming task, as well as to \(m\)-by-\(1\) digit multiplication, and to the puzzle through its elimination function (details in D.3). They are also all low collision settings.

Note that Prop 4.1 and 4.2 apply to any high-performant estimator of reasoning tasks. We focus on out-of-the-box transformers to align with the scope of our experiments and with the goal of framing empirical results. In SS5, we discuss how these propositions may inform future research directions.

Discussion

Collapsed Compositionality and Robustness ImplicationsTransformers today demonstrate undeniably powerful empirical results. Yet, our study suggests that they may have fundamental weaknesses in certain intellectual tasks that require true multi-step compositional operations such as multiplications and logic puzzles. Our careful study based on the computation graph and analyses demonstrates that transformers can often solve multi-step compositional problems by collapsing the depth of the compositional operations via analogical pattern matching. More broadly, our findings suggest that the strong performance of transformers should be taken with a certain grain of salt: Despite initially appearing challenging, certain tasks may not possess the inherent compositionality they seem to have. This is due to the fact that desired solutions could be readily derived from input-output sequences present in the training data, allowing for shortcut pattern matching to produce acceptable solutions. However, such an approach can ultimately result in poor generalization as shown in our study. For example, fine-tuning GPT3 on our tasks both with and without explicit reasoning graphs shows that models' learning fails to generalize beyond levels of complexity seen in training.

Theoretical Findings and their Empirical ImplicationsThe proofs presented in SS4 show that, under reasonable assumptions, the probability of incorrect predictions converges exponentially to \( 1\) for abstract compositional tasks. Importantly, these proofs apply to autoregressive LMs in general. Our insights indicate that the current configuration of transformers, with their reliance on a greedy process for predicting the next word, constrains their error recovery capability and impedes the development of a comprehensive global understanding of the task. Building on these findings, we suggest several empirical strategies for harnessing the potential of transformers. Firstly, transformers may be employed in ways that require chaining only a few compositional steps to reach a solution rather than lengthy reasoning steps (e.g., ). Secondly, transformers may be best suited for compositional tasks where evaluation metrics can afford some leniency; for example, finding approximate solutions that do not require executing the whole graph, such as identifying the most significant digit in a multiplication. Finally, we suggest augmenting transformers with planning modules as well as using refinement methods, that can iteratively improve their generations [82; 48].

Call for Broad Participation to Investigate LimitationsIdentification of limitations is an important step towards achieving greater robustness. Our study suggests fundamental limitations that impede transformers from fully mastering certain compositional operations. However, we acknowledge that due to our compute budget constraints as well as limited access to the largest language models such as GPT4, we are unable to push the empirical limits of transformers even further in terms of training data size and number of epochs. We invite the broader research community, particularly those with more extensive resources at their disposal, to investigate these possibilities further.

## 6 Related Work

Reasoning abilities in transformer LLMsRecently, transformers [11; 58; 57; 17; 16; 63; 73; 74] have demonstrated impressive reasoning abilities across a wide range of tasks, even outperforming humans in certain cases [79; 28; 15; 85]. This success has been largely attributed to the scaling effect, where larger models and training datasets result in improved performance [38; 33; 1]. However, these models have also been shown to struggle across multiple domains , including algorithmic reasoning , commonsense reasoning [62; 40], theory of mind , planning , logical reasoning , and ethical reasoning . These difficulties have motivated us to take a step back and thoroughly examine both the successes and failures of transformers from empirical and theoretical perspectives on compositional reasoning tasks.

Challenges of transformers in compositional tasksTransformers perform fairly well in single-step reasoning tasks , but face challenges when it comes to effectively combining multiple steps to solve compositionally complex problems [84; 55; 66; 81]. Recent research has focused on overcoming these limitations through various approaches. First, fine-tuning transformers to directly generate the final answer while keeping the reasoning implicit [7; 18]. Second, encouraging transformers to generate reasoning steps explicitly within a single generation [55; 80; 44; 42]. For example, Nye et al.  and Zhou et al.  used scratchpads to teach transformers how to perform algorithmic reasoning tasks such as addition by splitting the task into intermediate steps [44; 80]. Further, leveraging LLMs to generate each reasoning step iteratively via a selection and inference mechanism [20; 19; 72].

Lastly, choosing a training split that maximizes the number of observed patterns between the train and test data , or diversifying in-prompt examples to cover the maximum of patterns , ultimately enhancing generalization. The primary focus of these studies is to enhance model performance on compositional problems without striving for complete mastery. In contrast, our work explores the fundamental limits of vanilla transformers in achieving full mastery, striving for 100% performance in both in-domain and OOD settings. Our findings show that reaching full mastery is inherently challenging, providing insights into the complexities involved.

Challenges of transformers in generalizationExtensive research has been done to investigate the generalization capabilities of transformers . This encompasses various facets of generalization, including easy-to-hard generalization , length generalization , and generalization on symbolic mathematical integration . Schwarzschild et al.  and Bansal et al.  employ weight-tied neural networks to generalize from easy to hard examples. Liu et al.,  found that shallow transformers learn shortcuts during training, leading to poor OOD generalization. Razeghi et al.  revealed a positive correlation between the frequency of training terms and their test performance. Building upon this line of inquiry, we present a more rigorous examination of sub-graph matching between training and test instances for complex compositional tasks where we demonstrate how pattern matching can hinder generalization. We complement our empirical results with theoretical insights on transformers' limits.

GrokkingThe phenomena of models' gaining generalization capabilities when training significantly beyond overfitting, known as _grokking_ was recently introduced in . Subsequent works focus on characterizing when and why grokking arises:  show that perfect generalization in an arithmetic addition task happens when there is sufficient data to determine the appropriate structured representation, later extended to sparse parity in  where a sparse subnetwork of neurons is shown responsible for generalization behavior. Recently,  propose that grokking occurs when a task admits a generalizing and a memorizing solution, and the former is slower to learn. In this present work, our aim is not to explain grokking but rather to observe its emergence. We do not observe grokking arising in the context of multiplication, and we leave it to future work to explore whether this may be due to task difficulty hindering the learning of well-structured representations.

Transformers' theoretical expressivenessLin et al.  study autoregressive models' limitations from a computational complexity theory perspective. Transformer-specific work has focused on quantifying the class of problems that (not necessarily autoregressive) transformers can express assuming perfect parameters [51, 50, 14, 49, inter alia]. All tasks analyzed in our work belong to a class expressible by transformers, suggesting that known upper bound might not be tight. Importantly, Hahn  shows that transformers cannot robustly model noncounter-free regular languages even when allowing infinite precision. In contrast, our focus is on error accumulation, which enables to investigate if reasoning tasks theoretically solvable by transformers are likely to be solved by them.

Additional literature and societal impact discussion can be found in SSE.

## 7 Conclusions

On a broader scope, as transformers continue to gain widespread deployment with significant real-world impacts, it is ever more urgent to understand their successes and failures. Our study critically investigates transformers' limitations and emphasizes the need to develop models capable of robust generalization and systematic problem-solving. By examining the compositional capabilities of these models, we aspire to work towards more reliable AI systems that excel not only in tasks where abundant training examples are sufficient, but also in cases requiring precise compositional reasoning.

## 8 Limitations

We focus on analyzing compositional reasoning capabilities through the lens of computation graphs. Although they are a useful way to systematically represent rigorous reasoning processes, it is important to note that for the scratchpad approach, we are limited to only establishing a correlation between the model generation and its preceding context, as we cannot inspect the exact tokens model attends to when making the prediction. This limitation arises from our lack of access to the activations of the studied models. Furthermore, we posit that alternative approaches to linearizing reasoning processes may yield different performances and provide opportunities for further exploration.