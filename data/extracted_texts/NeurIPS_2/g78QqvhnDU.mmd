# Prioritizing Samples in Reinforcement Learning with Reducible Loss

Shivakanth Sujit

Mila, Quebec AI Institute, ETS Montreal

shivakanth.sujit@gmail.com

&Somjit Nath

Mila, Quebec AI Institute, ETS Montreal

somjitnath@gmail.com

&Pedro H.M. Braga

Mila, Quebec AI Institute, ETS Montreal, Universidade Federal de Pernambuco

pedromagalhaes.hb@gmail.com

&Samira Ebrahimi Kahou

Mila, Quebec AI Institute, ETS Montreal, CIFAR AI Chair

samira.ebrahimi.kahou@gmail.com

###### Abstract

Most reinforcement learning algorithms take advantage of an experience replay buffer to repeatedly train on samples the agent has observed in the past. Not all samples carry the same amount of significance and simply assigning equal importance to each of the samples is a naive strategy. In this paper, we propose a method to prioritize samples based on how much we can learn from a sample. We define the _learn-ability_ of a sample as the steady decrease of the training loss associated with this sample over time. We develop an algorithm to prioritize samples with high learn-ability, while assigning lower priority to those that are hard-to-learn, typically caused by noise or stochasticity. We empirically show that across multiple domains our method is more robust than random sampling and also better than just prioritizing with respect to the training loss, i.e. the temporal difference loss, which is used in prioritized experience replay. The code to reproduce our experiments can be found here.

## 1 Introduction

Deep reinforcement learning has shown great promise in recent years, particularly with its ability to solve difficult games such as Go , chess , and Atari . However, online Reinforcement Learning (RL) suffers from sample inefficiency because updates to network parameters take place at every time-step with the data being discarded immediately. One of the landmarks in the space of online RL learning has been Deep Q Networks (DQN) , where the agent learns to achieve human-level performance in Atari 2600 games. A key feature of that algorithm was the use of batched data for online learning. Observed transitions are stored in a buffer called the _experience replay_, from which one randomly samples batches of transitions for updating the RL agent.

Instead of randomly sampling from the experience replay, we propose to sample based on the _learn-ability_ of the samples. We consider a sample to be learnable if there is a potential for reducing the agent's loss with respect to that sample. We term the amount by which we can reduce the loss of a sample to be its _reducible loss_ (ReLo). This is different from vanilla prioritization in Schaul et al.

[MISSING_PAGE_FAIL:2]

A naive method of sampling is to uniformly sample all data in the buffer, however, this is inefficient because not all data is necessarily equally important. Schaul et al. (2016) proposes Prioritized Experience Replay (PER), that samples points with probabilities proportional to their TD error - which has been shown to have a positive effect on performance by efficiently replaying samples that the model has not yet learned, i.e., data points with high TD error. Each transition in the replay buffer is assigned a priority \(p_{i}\), and the transitions are sampled based on this priority. To ensure that data points, even with low TD error, are sampled sometimes by the agent, instead of greedy sampling based on TD error, the replay buffer in PER stochastically samples points with probability \(P_{i}\).

\[P_{i}=^{}}{_{j}p_{j}^{}} \]

where \([0,1)\) is a hyper-parameter introduced to smoothen out very high TD errors. Setting \(\) to 0 makes it equivalent to uniform sampling. Since sampling points non-uniformly changes the expected gradient of a mini-batch, PER corrects for this by using importance sampling (IS) weights \(w\)

\[w_{i}=(}}{P_{i}})^{} \]

where \(\) controls the amount by which the change in gradient should be corrected and \(p_{}=\) where \(N\) is the number of samples in the replay buffer. The loss attributed to each sample is weighed by the corresponding \(w_{i}\) before the gradient is computed. In practice, \(\) is either set to \(0.5\) or linearly annealed from \(0.4\) to \(1\) during training.

### Target Networks

In Eqs. 3 and 5, the target action value depends not only on the rewards but also on the value of the next state, which is not known. So, the value of the next state is approximated by feeding the next state to the same network used for generating the current \(Q\) values. As mentioned in DQN (Mnih et al., 2015), this leads to a very unstable target for learning due to the frequent updates of the \(Q\) network. To alleviate this issue, Mnih et al. (2015) introduce target networks, where the target \(Q\) value is obtained from a lagging copy of the \(Q\) network used to generate the current \(Q\) value. This prevents the target from changing rapidly and makes learning much more stable. So Eq. 3 can be suitably modified to

\[L_{}=(Q^{}(s_{t},a_{t})-(r_{t}+_{a}Q^{}(s_{t+ 1},a)))^{2} \]

respectively, where \(\) are the parameters of the target network, which are updated at a low frequency. Mnih et al. (2015) copies the entire training network \(\) to the target network, whereas Haarnoja et al. (2018) performs a soft update, where the new target network parameters are an exponential moving average (with a parameter \(\)) of the old target network parameters and the online network parameters.

### Reducible Loss

The work of Mindermann et al. (2022) proposes prioritized training for supervised learning tasks based on focusing on data points that reduce the model's generalization loss the most. Prioritized training keeps a held-out subset of the training data to train a small capacity model, \(_{ho}\) at the beginning of training. During training, this hold-out model is used to provide a measure of whether a data point could be learned without training on it. Given training data \((x_{i},y_{i})\), the loss of the hold-out model's prediction, \(_{ho}\) on a data point \(x_{i}\) could be considered an estimate of the remaining loss after training on datapoints other than \((x_{i},y_{i})\), termed the _irreducible loss_. This estimate becomes more accurate as one increases the size of the held-out dataset. The difference between the losses of the main model, \(\), and the hold-out model on the actual training data is called the _reducible loss_, \(L_{r}\) which is used for prioritizing training data in mini-batch sampling. \(L_{r}\) can be thought of as a measure of information gain by also training on data point \((x,y)\).

\[L_{r}=( x,)-( x,_{ho}) \]

### Prioritization Schemes

Alternate prioritization strategies have been proposed for improvements in sample efficiency. Sinha et al. (2020) proposes an approach that re-weights experiences based on their likelihood under the stationary distribution of the current policy in order to ensure small approximation errors on the value function of recurring seen states. Lahire et al. (2021) introduces the Large Batch Experience Replay (LaBER) to overcome the issue of the outdated priorities of PER and its hyperparameter sensitivity by employing an importance sampling view for estimating gradients. LaBER first samples a large batch from the replay buffer then computes the gradient norms and finally down-samples it to a mini-batch of smaller size according to a priority. Kumar et al. (2020) presents Distribution Correction (DisCor), a form of corrective feedback to make learning dynamics more steady. DisCor computes the optimal distribution and performs a weighted Bellman update to re-weight data distribution in the replay buffer. Inspired by DisCor, Regret Minimization Experience Replay (ReMERN) (Liu et al., 2021) estimates the suboptimality of the Q value with an error network. Yet, Hong et al. (2022) uses Topological Experience Replay (TER) to organize the experience of agents into a graph that tracks the dependency between Q-value of states.

## 3 Reducible Loss for Reinforcement Learning

While PER helps the agent to prioritize points that the model has not yet learned based on high TD error, we argue that there are some drawbacks. Data points could have _high_ TD error because they are noisy or not learnable by the model. It might not be the case that a data point with high TD error is also a sample that the model can actually learn or get a useful signal from. In supervised learning, a known failure condition of loss based prioritization schemes is when there are noisy points which can have high loss but are not useful for repeated training (Hu et al., 2021). Instead of prioritization based on the TD error, we propose that the agent should focus on samples that have higher _reducible_ TD error. This means that instead of the TD error, we should use a measure of how much the TD error can be potentially decreased, as the priority \(p_{i}\) term in Eq. 6. We contend that this is better because it means that the algorithm can avoid repeatedly sampling points that the agent has been unable to learn from and can focus on minimizing error on points that are learnable, thereby improving sample efficiency. Motivated by prioritized training in supervised learning, we propose a scheme of prioritization tailored to the RL problem.

In the context of supervised learning, learn-ability and reducible loss for a sample are well-defined as one has access to the true target. However, in RL, the true target is approximated by a bootstrapped target (TD methods) or the return obtained from that state (Monte Carlo). For policy evaluation with a fixed policy, the true value function can be obtained, however, since we are interested in control, it will be computationally intensive to capture the true value function with every change in policy as in policy iteration. Thus, to determine the learn-ability of a sample, we need access to how the targets of the sample behave and how it changes across time. Since the concepts of a hold-out dataset or model are ill-defined in the paradigm of RL, we replace them with a moving estimate of the targets. Unlike in supervised learning, where we draw i.i.d. batches from a fixed training set, the training data in RL are not i.i.d since they are generated by a changing policy. So the holdout model would need to be updated from time to time. Thus, in \(Q\) learning-based RL methods, a good proxy for the hold-out model is the target network used in the Bellman update in Eq. 8. Since the target network is only periodically updated with the online model parameters, it retains the performance of the agent on older data which are trained with outdated policies. Schaul et al. (2022) demonstrates how the policies keep changing with more training even when the agent receives close to optimal rewards. Thus, the target network can be easily used as an approximation of the hold out model that was not trained on the new samples. Therefore, we define the Reducible Loss (ReLo) for RL as the difference between the loss of the data point with respect to the online network (with parameters \(\)) and with respect to the target network (with parameters \(\)). So the Reducible Loss (ReLo) can be computed as

\[=L_{}-L_{} \]

There are similarities between ReLo as prioritization scheme in the sampling behavior of low priority points when compared to PER. Data points that were not important under PER, i.e. they have low \(L_{}\), will also remain unimportant in ReLo. This is because if \(L_{}\) is low, then as per Eq. 10, ReLo will also be low. This ensures that we retain the desirable behavior of PER, which is to not repeatedly sample points that have already been learned.

However, there is a difference in sampling points that have high TD error. PER would assign high priority to data points with high TD error, regardless of whether or not those data points are noisy or unlearnable. For example, a data point can have a high TD error which continues to remain high even after being sampled several times due to the inherent noise of the transition itself, but it would continue to have high priority with PER. Thus, PER would continue to sample it, leading to inefficient learning. But, its priority should be reduced since there might be other data points that are worth sampling more because they have useful information which would enable faster learning. The ReLo of such a point would be low because both \(L_{}\) and \(L_{}\) would be high. In case a data point is forgotten, then the \(L_{}\) would be higher than \(L_{}\), and the ReLo would ensure that these points are revisited. Thus ReLo can also help to overcome forgetting.

### Implementation

The probability of sampling a data point is related to the priority through Eq. 6 and requires the priority to be non-negative. Since \(Q\) value methods use the mean-squared error (MSE) loss, the priority is guaranteed to be non-negative. However, ReLo computes the difference between the MSE losses and it does not have the same property. This value can go to zero when the target network is updated with the main network using hard updates. However, it quickly becomes non-zero after one update. So, we should create a mapping \(f_{map}\) for the ReLo error that is monotonically increasing and non-negative for all values. In practice, we found that clipping the negative values to zero, followed by adding a small \(\) to ensure samples had some minimum probability, worked well. That is, \(p_{i}=max(,0)+\). This is not the only way we can map the negative values and we have studied one other mapping in the supplementary material. Note also that during initial training when the agent sees most of the points for the first time, it would assign low priority to all depending on the value of ReLo and the sampling would be close to random. However, the priority will never go to zero because of \(\), so we will always have a non-zero probability of drawing the sample. Once it is sampled and the loss goes down, then the priority would again increase as per ReLo and it would prioritize such points until the ReLo becomes low again. Thus, it kind of achieves the best of both worlds, where initially with less information we sample uniformly, but once ReLo increases we sample points more frequently.

ReLo is not computationally expensive since it does not require any additional training. It only involves one additional forward pass of the states through the target network. This is because the Bellman backup (i.e., the right hand side of Eq. 2) is the same for \(L_{}\) and \(L_{}\). The only additional term that needs to be computed for ReLo is \(Q_{tgt}(s_{t},a_{t})\) to compute \(L_{}\).

```
Given off-policy algorithm \(A\) with loss function \(L^{alg}\), online \(Q\) network parameters \(\), target \(Q\) network parameters \(\), replay buffer \(B\), max priority \(p_{max}\), ReLo mapping \(f_{map}\), epsilon priority \(\), training timesteps \(T\), gradient steps per timestep \(T_{grad}\), batch size \(b\). for t in 1, 2, 3, \( T\)do  Get current state \(s_{t}\) from the environment  Compute action \(a_{t}\) from the agent  Store the transition \(<s_{t},a_{t},r_{t},s_{t+1}>\) in the replay buffer \(B\) with priority \(p_{max}\). for steps in 1, 2, 3, \( T_{grad}\)do  Sample minibatch of size \(b\) from replay buffer  Compute the loss \(L^{alg}_{}\) and update the agent parameters \(\)  Compute \(L^{alg}_{ tgt}\) and calculate ReLo as per Eq. 10  Update priorities of samples in mini-batch with the new ReLo values as \(f_{map}(_{i})+\) endfor  Update target network following the original RL algorithm \(A\) endfor
```

**Algorithm 1** Computing ReLo for prioritization

In our implementation, we saw a negligible change in the computational time between PER and ReLo. ReLo also does not introduce any additional hyper-parameters that need to be tuned and works well with the default hyper-parameters of \(\) and \(\) in PER. An important point to note is that ReLo does not necessarily depend on the exact loss formulation given in Eq. 8 and can be used with the loss function \(L^{alg}_{}\) of any off-policy \(Q\) value learning algorithm. In order to use ReLo, we only have to additionally compute \(L^{alg}\) with respect to the target network parameters \(\). If the loss is just the mean square error, then ReLo can be simplified and can be represented by the difference between \(Q_{}\) and \(Q_{}\). But other extensions to off policy Q learning methods modify this objective, for example distributional learning (Bellemare et al., 2017) minimizes the KL divergence and the difference between two KL divergences can not be simplified the same way. To make ReLo a general technique that can be used across these methods, we define it in terms of \(L_{}\) and \(L_{}\). Our experiments also show that ReLo is robust to the target network update mechanism, whether it is a hard copy of online parameters at a fixed frequency (as in DQN (Mnih et al., 2015), and Rainbow (Hessel et al., 2017)) or if the target network is an exponential moving average of the online parameters (as in Soft Actor Critic (Haarnoja et al., 2018)).

## 4 Experimental Results

### GridWorld Experiments

The major goal of these Gridworld studies is to draw attention to two PER issues: subpar performance when faced with stochasticity and forgetting when faced with many tasks. We also highlight how ReLo can potentially solve both issues.

#### 4.1.1 Pitfalls of TD Error Prioritization

To illustrate the potential downsides of using PER with TD error prioritization, we created a \(7 7\) empty gridworld with a single goal. The agent always spawns at the top left corner and it has to reach the goal at the bottom right for a reward of \(+2\). The reward is zero everywhere else except, near the center of the grid, where there is another state with a reward sampled uniformly from \([-0.5,0.5]\)1 The presence of this _critical_ point would mean that during the initial phase of training, PER would keep prioritizing transitions to that state due to the unpredictable reward and the corresponding high TD error, and fail to learn about the true goal. Given a sufficient number of samples, all algorithms would ultimately converge to the optimal policy. The ReLo criterion, however, does not prioritize unlearnable points and prioritizes other points where it can receive a larger reward. Fig. 2 shows that PER samples this unlearnable point very often and with limited experience is unable to solve the task. In contrast, both uniform experience replay and ReLo do not over-sample this transition and thus can perform better in this scenario. This is further affirmed by the number of updates each algorithm makes in the neighborhood of the _unlearnable_ point as visualized in Fig. 2. Prioritization using ReLo enables the agent to sample more from the transitions leading to the main reward which explains its better performance.

Figure 1: Metrics aggregated across each benchmark based on proposed metrics from Agarwal et al. (2021). 10 seeds per environment-algorithm pair.

#### 4.1.2 Mitigating forgetting with ReLo

To study how ReLo can help reduce forgetting in RL, we design a multi task version of the gridworld. We create a \(6 6\) gridworld consisting of two rooms A and B and a goal state in each room. Task A is defined as starting in Room A and reaching the goal state in Room A, and Task B is defined similarly for Room B. There is a single gap in between the rooms, allowing the agent to explore both rooms but a time limit is introduced so that the agent can not reach both goals in one episode. During training, for the first 100k steps, the agent starts in Room A with access to Room B blocked. So during this stage, the agent observes only Task A. After 100K steps, the agent starts only in Room B, thereby no longer collecting data about Task A and must retain its ability on the task by replaying relevant transitions from the buffer. We train three agents, a baseline DQN agent, a PER agent and a ReLo agent and monitor performance on both tasks during training. We provide average success rates over 60 seeds after 1M steps in Table 1 and Appendix J. It shows how ReLo exhibits the least degradation in performance on Task A compared to the other baselines. From the training curves, we can see that there is a clear drop off in performance on Task A after 100K steps when the agent can no longer actively collect data on the task. However the ReLo agent exhibits least degradation in Task A while also outperforming the baseline and PER agent on Task B. This experiment clearly shows how ReLo helps the agent replaying relevant data points that could have been forgotten.

### Comparison of PER and ReLo

We study the effectiveness of ReLo on several continuous and discrete control tasks. For continuous control, we evaluate on 9 environments from the DeepMind Control (DMC) benchmark (Tassa et al., 2018) as they present a variety of challenging robotic control tasks with high dimensional state and action spaces. We also include 3 environments from the OpenAI Gym benchmark for continuous control. For discrete control, we use the MinAtar suite (Young and Tian, 2019) which consists of visually simpler versions of games from the Arcade Learning Environment (ALE) (Bellemare et al., 2013). The goal of MinAtar is to provide a benchmark that does not require the vast amounts of compute needed for the full ALE evaluation protocol, which usually involves training for 200M frames across 10 runs per game. This can be prohibitively expensive for researchers and thereby the MinAtar benchmark reduces the barriers present in studying deep RL research. We include scores on 24 games from the ALE benchmark for a reduced number of steps to observe if there are signs of improvement when using ReLo over PER. We provide full training curves for each environment in the supplementary material. We compare ReLo with LaBER (Lahire et al., 2021) on the DeepMind Control Suite and MinAtar benchmarks using the codebase released by the authors.

In addition to the per environment scores, we report metrics aggregated across environments based on recommendations from Agarwal et al. (2021) in Fig. 1. We can see that across a diverse set of

   Algorithm & Task A & Task B \\  Uniform & 0.43 (0.32, 0.54) & 0.40 (0.29, 0.51) \\ PER & 0.29 (0.19, 0.39) & 0.26 (0.16, 0.36) \\ ReLo & **0.63 (0.52, 0.74)** & **0.74 (0.65, 0.84)** \\   

Table 1: Performance on Forgetting Task. Confidence intervals in brackets.

Figure 2: Left: The rewards obtained after 50K episodes across 50 runs in the gridworld domain with an unlearnable point. Right: Number of times a transition containing the _unlearnable_ state and its neighbors were sampled.

tasks and domains, ReLo outperforms PER as a prioritization scheme, with higher IQM and lower optimality gap scores. This highlights the generality of ReLo.

#### 4.2.1 Dmc

In the continuous control tasks, Soft Actor Critic (SAC) (Haarnoja et al., 2018) is used as the base off-policy algorithm to which we add ReLo. SAC has an online and an exponential moving average target \(Q\) network which we use to generate the ReLo priority term as given in Eq. 10. For comparison, we also include SAC with PER to showcase the differences in performance characteristics of PER and ReLo. The results are given in Figs. 0(a). On 6 of the 9 environments, ReLo outperforms the baseline SAC as well as SAC with PER. This trend in performance is visible in the aggregated scores in Fig. 0(a) where ReLo has a higher IQM score along with a lower optimality gap when compared to SAC, SAC with PER and LaBER.

Additionally, to study the effect of stochastic dynamics in a non-gridworld setting, we conducted an experiment on stochastic versions of DMC environments. Specifically, we added noise sampled from \((0,^{2})\) to the environment rewards during training. During evaluation episodes, no noise is added to the reward. This is similar to the stochastic environments used by Kumar et al. (2020). For Quadruped Run, Quadruped Walk, Walker Run we used \(=0.1\), and for Walker Walk we used a higher level of noise (\(=1\)) since there wasn't much change in the performance when using \(=0.1\) compared to deterministic version. The results are presented in Fig. 3 and Table 2. These experiments highlight how ReLo outperforms uniform sampling and PER in stochastic dynamics while also attaining lower TD error and reinforce our claim that ReLo can effectively handle stochasticity in environments.

#### 4.2.2 OpenAI Gym Environments

In addition to the DeepMind Control Suite, we also evaluate ReLo on environments from the OpenAI Gym suite, namely HalfCheetah-v2, Walker2d-v2 and Hopper-v2 and the results are available in Fig. 0(b). There is a general trend where PER leads to worse performance when compared to the baseline algorithm, in line with previous work by Wang and Ross (2019) that shows that the addition of PER to SAC hurts performance. However, this is not the case when using ReLo as a prioritization scheme. It is very clear how the PER degrades performance while ReLo does not adversely affect the learning ability of the agent and in fact leads to higher scores in each of the tested environments. We believe that the degraded performance of PER in the Gym environments is due to instability in training caused by rapidly varying value estimates. We provide experiments to back up this claim in Appendix L. We should that early terminations in the OpenAI Gym environments cause the PER agent to have the highest variance in the estimate of the fail state distribution. These results are presented in Table 3 When we disable early terminations, the PER agent is able to learn in these environments, though it is still worse than the ReLo agent.

    & Quadruped Run & Quadruped Walk & Walker Run & Walker Walk \\  Uniform & 0.5 (0.34, 0.66) & 1.27 (1.07, 1.47) & 0.04 (0.04, 0.04) & 1.98 (1.96, 2.0) \\ PER & 5.22 (3.39, 7.05) & 0.53 (0.47, 0.59) & 0.06 (0.05, 0.07) & 3.62 (3.47, 3.78) \\ ReLo & **0.19 (0.17, 0.2)** & **0.12 (0.11, 0.12)** & **0.03 (0.02, 0.03)** & **1.94 (1.92, 1.96)** \\   

Table 2: Validation TD Error on Stochastic DMC. Performance aggregated over 5 seeds.

Figure 3: Performance on Stochastic DMC. Performance aggregated over 5 seeds.

#### 4.2.3 MinAtar

In the MinAtar benchmark, we use DQN (Mnih et al., 2015) as a baseline algorithm and compare its performance with PER and ReLo on the 5 environments in the benchmark. DQN does not have a moving average target \(Q\) network and instead performs a hard copy of the online network parameters to the target network at a fixed interval. Similar to the implementation of ReLo in SAC, we use the online and target \(Q\) network in the ReLo equation for calculating priorities. The results on the benchmark are given in Fig. 0(c). PER performs poorly on Seaquest and SpaceInvaders, with scores lower than the baseline DQN. These results are consistent with observations by Obando-Ceron and Castro (2021) which analyzed the effect of the components of Rainbow in the MinAtar environment. In contrast, ReLo consistently outperforms PER and is comparable to or better than the baseline. Our previous observation that ReLo tends to help improve performance in situations where PER hurts performance is also true here.

#### 4.2.4 Ale

As an additional test, we modified the Rainbow (Hessel et al., 2017) algorithm, which uses PER by default, to instead use ReLo as the prioritization scheme and compared it against Rainbow with PER on 24 environments from the ALE benchmark. Instead of the usual 200M frames of evaluation, we trained each agent for 2M frames to study if there are gains that can be observed in this compute-constrained setting. As shown in Fig. 0(d), we see that Rainbow with ReLo achieves better performance than Rainbow with PER. These experiments show the versatility of ReLo as a prioritization scheme.

### Analysis of TD Loss Minimization

To verify if using ReLo as a prioritization scheme leads to lower loss values during training, we logged the TD error of each agent over the course of training and these loss curves are presented in Fig. 4. As we can see, ReLo does indeed lead to lower TD errors, empirically validating our claims that using ReLo helps the algorithm focus on samples where the loss can be reduced. Another interesting point is that in Fig. 3(a), SAC with PER has the highest reported TD errors throughout training. This is due to PER prioritizing data points with high TD error which might not be necessarily learnable. Data points with higher TD error are repeatedly sampled and thus making the overall

Figure 4: Comparison of training TD error for PER and ReLo based sampling in a) DMC and b) Atari benchmarks. Calculated over 5 seeds.

   Method & Mean (CIS) & Min & Max \\  Baseline & 29.055 (-62.551, 120.662) & -87.8392 & 258.484 \\ PER & 149.982 (-76.382, 376.346) & -115.291 & 793.237 \\ ReLo & 18.225 (-0.307, 36.756) & -14.4796 & 49.37 \\   

Table 3: Variance in Value Estimate of Fail State Distribution.

losses during training higher. In contrast, ReLo addresses this problem by sampling data points that are learnable and leads to the lowest TD errors during training.

**Validation TD Error:** We also compared the validation TD errors of each method after training in Tables 2, 4, and 5. This was done by collecting \(10^{4}\) frames from the environment and computing the mean TD errors of these transitions. These results show that the validation TD errors [Li et al., 2023] obtained at the evaluation phase are actually lower for ReLo. This analysis is very similar to observations in [Li et al., 2023] where the authors find that lower validation TD error is a reliable indicator of good sample efficiency for off-policy RL algorithms. ReLo achieves lower validation TD error compared to uniform sampling and PER on DM Control Suite and Atari. When we study the correlation between validation TD error and performance in Appendix 11 and observe a strong correlation between the two metrics, confirming the findings of Li et al. . These results highlight another crucial reason for the robustness and good performance of ReLo across multiple domains.

## 5 Conclusion

In this paper, we have proposed a new prioritization scheme for experience replay, Reducible Loss (ReLo), which is based on the principle of frequently sampling data points that have the potential for loss reduction. We obtain a measure of the reducible loss through the difference in loss of the online model and a hold-out model on a data point. In practice, we use the target network in \(Q\) value methods as a proxy for a hold-out model.

ReLo avoids the pitfalls that come with naively sampling points based only on the magnitude of the loss since having a high loss does not imply that the data point is actually learnable. While alleviating this issue, ReLo retains the positive aspects of PER, thereby improving the performance of deep RL algorithms. It is very simple to implement, requiring just the addition of a few lines of code to PER, and similar to PER it can be applied to any off-policy algorithm. Since it requires only one additional forward pass through the target network, the computational cost of ReLo is minimal, and there is very little overhead in integrating it into an algorithm.

While the reducible loss can be intuitively reasoned about and tested empirically, future work should theoretically analyze the sampling differences between ReLo and PER about the kind of samples that they tend to prioritize or ignore. This deeper insight would allow us to find flaws in how we approach non-uniform sampling in deep RL algorithms similar to work done in Fujimoto et al. . It would provide an analysis of the change in learning dynamics induced by PER and ReLo.

    & Baseline & PER & ReLo \\  CheetaRun & 0.02 \(\) 0.002 & 0.03 \(\) 0.003 & 0.12 \(\) 0.033 \\ QuadrupedRun & 0.62 \(\) 0.055 & 2.24 \(\) 0.127 & 0.35 \(\) 0.067 \\ QuadrupedWalk & 3.17 \(\) 0.252 & 2.11 \(\) 0.189 & 1.07 \(\) 0.146 \\ WalkerRun & 0.08 \(\) 0.015 & 0.15 \(\) 0.018 & 0.06 \(\) 0.008 \\   

Table 4: Comparison of validation TD Errors on DM Control Suite

    & Rainbow & Rainbow+ReLo \\  Alien & 2.329 \(\) 0.408 & 2.331 \(\) 0.251 \\ Amidar & 2.157 \(\) 0.120 & 2.055 \(\) 0.143 \\ Bank Heist & 2.044 \(\) 0.222 & 2.018 \(\) 0.261 \\ Jamesbond & 1.653 \(\) 0.819 & 1.142 \(\) 0.174 \\   

Table 5: Comparison of validation TD Errors on Atari