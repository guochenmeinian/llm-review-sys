# Fine-Tuning Out-of-Vocabulary Item Recommendation with User Sequence Imagination

Ruochen Liu\({}^{1}\), Hao Chen\({}^{2}\), Yuanchen Bei\({}^{3}\), Qijie Shen\({}^{4}\),

**Fangwei Zhong\({}^{5}\)**, **Senzhang Wang\({}^{1*}\), Jianxin Wang\({}^{1}\)

\({}^{1}\)Central South University, \({}^{2}\)City University of Macau,

\({}^{3}\)Zhejiang University, \({}^{4}\)Alibaba Group, \({}^{5}\)Beijing Normal University

{ruochen, szwang\({}^{*}\), jxwang}@csu.edu.cn, sundaychenhao@gmail.com

yuanchenbei@zju.edu.cn, qjshenxdu@gmail.com, fangweizhong@bnu.edu.cn

Corresponding author

###### Abstract

Recommending out-of-vocabulary (OOV) items is a challenging problem since the in-vocabulary (IV) items have well-trained behavioral embeddings but the OOV items only have content features. Current OOV recommendation models often generate'makeshift' embeddings for OOV items from content features and then jointly recommend with the'makeshift' OOV item embeddings and the behavioral IV item embeddings. However, merely using the'makeshift' embedding will result in suboptimal recommendation performance due to the substantial gap between the content feature and the behavioral embeddings. To bridge the gap, we propose a novel **User Sequence****IMagination (USIM)** fine-tuning framework, which first imagines the user sequences and then refines the generated OOV embeddings with the user behavioral embeddings. Specifically, we frame the user sequence imagination as a reinforcement learning problem and develop a recommendation-focused reward function to evaluate to what extent a user can help recommend the OOV items. Besides, we propose an embedding-driven transition function to model the embedding transition after imaging a user. USIM has been deployed on a prominent e-commerce platform for months, offering recommendations for millions of OOV items and billions of users. Extensive experiments demonstrate that USIM outperforms traditional generative models in OOV item recommendation performance across traditional collaborative filtering and GNN-based collaborative filtering models.

## 1 Introduction

Recommendation systems, such as collaborative filtering models, learn behavioral embeddings from historical interactions to represent the behavioral characteristics of billions of users and items . For instance, the embeddings of interacted user-item pairs have higher inner products, whereas those of un-interacted pairs have lower inner products. However, besides the items with user interactions, thousands of out-of-vocabulary (OOV) items--such as short videos, photos, and posts--are generated or uploaded every second. In the age of AGI, the generation speed of AI-made OOV content, including text, images, and videos, will far exceed the speed of human consumption. To avoid being overwhelmed by the OOV content, it is essential to replicate how humans handle these items, recommending them without disrupting in-vocabulary (IV) items.

Traditional OOV recommendation models usually generate'makeshift' embeddings from the content features and then use them to recommend OOV items. The research can be classified into two categories. (a) **Generative models** aim to generate realistic embeddings. GAR  uses a generativeadversarial structure to ensure the embedding distribution of generated OOV embeddings is similar to IV embeddings. ALDI  distills knowledge from IV items to OOV items. (b) **Dropout models** increase the robustness of recommender systems. Dropout  randomly substitutes IV embeddings with "makeshift" ones to enhance system robustness. Heater  and CLCRec  further utilize a mix of experts and contrastive learning techniques to improve OOV recommendation performance.

As shown on the left of Figure 1, current recommender systems typically learn the embeddings for any given items by initializing/generating the embedding and then optimizing them through user-sequence backpropagation. However, prevalent OOV recommendation models primarily concentrate on generating improved or robust embeddings, overlooking the potential for further optimization from imagining user sequences. This oversight limits these models due to the following issues.

1. **Content-Behavior Gap**. 'Makeshift' embeddings are generated from content features, while behavioral embeddings are trained using backpropagation. The substantial difference between content features and behavioral embeddings may lead to discrepancies between IV and OOV items, impacting IV, OOV, or both. 2. **Potential Suboptimality**. Focusing only on embedding generation overlooks potential improvements from backpropagation, which could finely tune the embeddings to adapt to user preferences and current recommender systems, possibly leading to suboptimal recommendation performance and a reduction in revenue.

While imagining the sequential optimization process for OOV item embeddings shows promise, its implementation presents three challenges. 1. **Absence of Historical Interactions.** The lack of historical user interactions for OOV item embeddings hinders the definition of a clear backpropagation and optimization process. 2. **Ambiguous Imagination Objectives.** Formulating objectives, stopping criteria, and user selection for imagining OOV item interactions remains an open challenge. 3. **Navigating the Vast User Space.** Efficiently identifying suitable user sequences to imagine for OOV items within the massive user space of recommender systems.

To address these challenges, we introduce an **User Sequence Imagination (USIM)** pipeline that further optimizes the embedding of OOV items. It first imagines potential users who may interact with the OOV item and then refines the item embeddings through backpropagation. Specifically, we propose a **RL-based USIM** solution, which formulates the sequential optimization as a Markov Decision Process and introduces recommender-oriented PPO (RecPPO) to maximize the final recommendation performance of OOV items. In summary, our contributions are as follows:

* We introduce USIM, a novel approach that fundamentally addresses the Out-of-Vocabulary (OOV) problem by imagining user sequences and performing user-sequence backpropagation.
* We formally define the Reinforcement Learning (RL) formulation of USIM, including the formulation of the Markov Decision Process (MDP) for user sequence imagination, and provide the state transition function framework for user-sequence backpropagation.
* We implement USIM on a major e-commerce platform--Alibaba, successfully optimizing millions of OOV items and recommending them to billions of users. The source code is publicly available at [https://github.com/Ruochen1003/USIM](https://github.com/Ruochen1003/USIM).
* We validate the effectiveness of our approach on two benchmark datasets using both traditional collaborative filtering and graph-based collaborative filtering backbones. Extensive experiments demonstrate that USIM outperforms existing state-of-the-art OOV recommendation models in terms of OOV recommendation performance and overall recommendation quality.

Figure 1: Comparison between (a) traditional ‘makeshift’ embedding OOV recommendation framework, and (b) user sequence imagination OOV recommendation framework.

Preliminaries

Notations.Let \(\) and \(\) denote the sets of users and items, respectively. We partition \(\) into in-vocabulary (IV) items \(_{iv}\) (items with interaction history) and out-of-vocabulary (OOV) items \(_{ov}\) (items without interaction history). We denote the cardinality of these sets as \(||\), \(|_{iv}|\), and \(|_{ov}|\), respectively. For clarity in the following discussion, we also use \(_{i}\) to represent the set of users who have interacted with item \(i\).

The embedding matrices for users, IV items, and OOV items are denoted as \(_{u}^{|| d}\), \(_{iv}^{|_{iv}| d}\), and \(_{ovov}^{|_{cov}| d}\), respectively, where \(d\) represents the embedding dimension. For an individual user \(u\) and item \(i\), their corresponding embeddings are denoted as \(_{u}^{d}\) and \(_{i}^{d}\). Since OOV items lack behavioral embeddings initially, we leverage content features, denoting the content feature vector of item \(i\) as \(_{i}\).

Backpropagated Embedding of IV Items.The embeddings of IV items are initialized using standard initialization techniques such as Xavier initialization . These embeddings are subsequently optimized through backpropagation using historical interaction data [11; 4],

\[_{i}=_{i}-_{u_{i}} L(u,i), \]

where \(L\) represents the loss function (e.g., BPR loss).

Makeshift Embedding of OOV Items.Due to the absence of historical interactions for OOV items, various approaches have been proposed to generate makeshift embeddings that enable joint recommendation with IV items. These approaches can be broadly categorized into generative models [6; 5; 12] and dropout models [9; 7], which transform content features \(_{i}\) into embeddings through a generator function \(G\),

\[_{i}=G(_{i}), \]

where \(G\) is optimized using various objective functions, including similarity-based losses , adversarial losses , and knowledge distillation losses .

MDP Formulation of Back Propagation.We formulate the OOV embedding optimization process as a Markov Decision Process (MDP) to narrow the optimization gap between IV and OOV item embeddings. This formulation enables simultaneous user imagination and embedding optimization through backpropagation.

An MDP at time step \(t\) is defined by the quintuple \((,,,R,)\), where:

* \(\) represents the state space, with each state \(\) capturing the environment configuration
* \(\) denotes the action space, where each action \(a\) represents a possible agent decision
* \(:\) defines the state transition function, with \(_{t+1}=(_{t},a_{t})\)
* \(R:\) specifies the reward function,State Space.At time step \(t\), the state \(_{t}\) encapsulates the essential information required for item embedding optimization, defined as \(_{t}=[_{t},l_{t}]\). The state representation \(_{t}^{d}\), which is refined through optimization, resides in the same embedding space as \(_{i}\), and its final representation will serve as the OOV item embedding. To specifically denote the state of item \(i\) at time step \(t\), we define \(_{i,t}=[_{i,t},l_{i,t}]\). In contrast, \(_{t}\) serves as a more general denotation. The initial state presentation is obtained by the generator in Eq. (2) as \(_{i,0}=G(c_{i})\). Details can refer to Appendix B.

The temporal component \(l_{t}\) is used as a countdown mechanism, tracking the remaining optimization steps to encourage efficient convergence . Given a maximum action limit \(N\), at time step \(t\), the countdown value is computed as \(l_{t}=N-t\).

Action Space.Given state \(_{t}\), the agent selects an action \(a_{t}\) from the action space \(=\{a_{end}\}\), where \(a_{end}\) denotes the termination action. This selection process entails either imagining a user or terminating the optimization process. The action embedding \(_{a}\) corresponds to the user embedding from \(_{u}\) when \(a\), and defaults to \(\) for the termination action.

Policy Network.The agent's decision-making process is governed by policy \((_{t})\), which maps the current state \(_{t}\) to a probability distribution over possible actions. To effectively utilize existing embeddings while accommodating the special termination action, the policy distribution is given as,

\[(a_{t}|s_{t})=(1-(_{2}_{t}^{}+c)) _{a_{t}}_{1}_{t}^{})}{_{a} (_{a}_{1}_{t}^{})},&a_{t};\\ (_{2}_{t}^{}+c),&a_{t}=a_{end}, \]

where \(_{1}^{d(d+1)}\), \(_{2}^{d+1}\), and \(c\) are parameters, and \(\) represents the sigmoid function.

Reward and State Transition.The agent receives an immediate reward \(r_{t}=R(_{t},a_{t})\) after each action, guiding the optimization trajectory. To align with the imagination process, we design an efficient state transition function \(_{t+1}=(_{t},a_{t})\) that facilitates the progressive refinement from content-based to interaction-based embeddings. The experience tuples \((_{t},a_{t},r_{t},_{t+1})\) are collected in a replay buffer for subsequent training iterations.

### State Transition Function

State transition involves modifying \(_{t}\), which will ultimately be used as the OOV item embedding and each step of the state transition corresponds to optimizing \(_{t}\) using the imagined user(last action \(a_{t}\)). Therefore, to design a state transition function that aligns with this optimization process, we must

Figure 2: The overview framework of USIM. USIM fine-tunes the generated OOV item embeddings through sequential user interaction imagination, guided by exploration set construction, state transition, and a tailored reward mechanism.

first determine the objective of this optimization. Considering that most current recommendation algorithms calculate relevance scores between users and items for recommendations [11; 14; 4], we adopt the following objective as our optimization goal,

\[_{_{i}}-_{u_{t}}_{u,i}, \]

where \(_{u,i}\) is predicted score between \(_{i}\) and \(_{u}\).

Based on the above idea, we can view the user imagination process as the solution to the optimization objective (Eq. (4)). Specifically, we assume that the users imagined by the agent are the users who have interacted with this item, and by using backpropagation, we can optimize the content-based initialized embeddings. Thus, the transition function \(_{h}\) of \(_{t}\) can be written as follows,

\[_{t+1}=_{h}(_{t},a_{t})=_{t}+ _{a_{t},i}&a_{t};\\ _{t}&a_{t}=a_{end}, \]

where \(\) is a hyperparameter and can be understood as the learning rate. If \(_{a_{t},i}\) is computed using dot product, the final transition function of \(_{t}\) can be written as follows,

\[_{t+1}=_{h}(_{t},a_{t})=_{t}+ _{a_{t}}&a_{t};\\ _{t}&a_{t}=a_{end}. \]

And \(l_{t}\) can be updated as \(l_{t+1}=l_{t}-1\).

### Reward Function

The objective of the USIM is to optimize the initialized embeddings by imagining user sequences. To facilitate this process, we design a reward function to guide the reinforcement learning approach. Our reward function consists of three components.

**Embedding Alignment Reward**. We believe that the item embeddings generated by the IV model represent the best solution for our optimization process. Consequently, our objective is to closely align the final state representation with actual item embedding (i.e., the corresponding embedding from the IV model). To achieve this, we calculate the reward based on the concept of similarity,

\[R_{emb}(_{i,t},a_{t})=D(_{i,t},_{i})-D(_{i,t+1},_{i}),_{i,t+1}=_{h}(_{i,t},a_{t}), \]

where \(D(,)\) denotes the Euclidean distance between embeddings. This reward represents the change in similarity between the state representation and the actual item embedding, before and after the state transition.

**Recommendaion Performance Reward**. Although the embedding alignment reward encourages the state representation \(_{t}\) to be close to the actual item embedding, it does not differentiate between states when multiple representations are equally distant from the actual embedding. Additionally, it does not fully utilize the insights from existing user interactions. Therefore, we design a reward function based on recommendation performance as follows,

\[& R_{rec}(_{i,t},a_{t})=f(_{i,t},_{i})-f(_{i,t+1},_{i}),_{i,t+1}=_{h}(_{i,t},a_{t} ),\\ & f(_{i,t},_{i})=_{i}|}_{u_{ j}_{i}}|_{i,t}_{u_{j}}-_{i}_{u_{j }}|, \]

where \(f(_{i,t},_{i})\) represents the predictive power of state representation \(_{i,t}\) for users in \(_{i}\). The final performance reward is derived from the change in \(f\) before and after the state transition. This change represents the variation in the embedding's predictive capability.

**Step Regulation**. To encourage the agent to achieve the goal in as few steps as possible, we impose a penalty for each action it takes. Therefore, the final reward function is as follows,

\[r_{t}=R(_{i,t},a_{t})=R_{emb}(_{i,t},a_{t})+R_{rec}(_{i,t},a _{t})-p, \]

where \(p\) is a hyperparameter that represents the penalty.

### Exploration Set Construction

Given the large user base in the recommendation dataset, sampling actions solely by probability during initial reinforcement learning often results in negative rewards, slowing convergence and reducing performance. To explore actions more efficiently, we construct an exploration set according to state \(s_{t}\), comprising three components.

**Positive Action.** In the USIM framework (Section 3.2), the agent assumes that the imagined users correspond to those who have interacted with the item, so these users are included in the exploration set. While optimizing \(_{i,t}\) to match \(e_{i}\) theoretically requires only the interaction set \(_{i}\), achieving this often demands combining multiple actions \(a_{i}\), making exploration complex. To streamline this, we select users most likely to bridge \(_{i,t}\) and \(_{i}\), accelerating the process. The Positive Action set \(_{pos}\) is constructed as follows,

\[_{pos}=_{k_{1}}((e_{i}-_{i,t}),e_{u})_{i}, \]

where \(_{k_{1}}((e_{i}-_{i,t}),e_{u})\) represents the set of \(k_{1}\) users with the highest cosine similarity to \(e_{i}-_{i,t}\), forming a vector that directly points to the actual item embedding.

**Random Action**. Relying solely on the aforementioned action sets may overly restrict actions, limiting state space coverage and reducing model generalization. Moreover, sampling actions with negative rewards can also benefit training . Thus, we augment the action set by randomly selecting \(k_{2}\) actions from the remaining pool, denoted as \(_{rad}\).

**Termination Action**. To enable the agent to learn when to terminate, we also incorporate the termination action \(a_{end}\) into the final action set. This inclusion allows the agent to determine the appropriate timing for ending the optimization process.

For simplicity, we set \(k_{1}=k_{2}=k\). And exploration set \(_{samp}\) can be represented as follows,

\[_{samp}=_{pos}_{rad}\{a_{end}\}. \]

During the training phase, as the sampling range is narrowed from the full action set \(\) to a specific action set \(_{samp}\), we rewrite the Eq. (3) as follows,

\[(a_{t}|s_{t})=(1-(s_{t}^{}}+c))_{a_{t}}s_{t}^{}}\}}{_{a A_{samp}\{a _{end}\}}\{_{a}s_{t}^{}}\}},&a_{t} A_{samp} \{a_{end}\};\\ (s_{t}^{}}+c),&a_{t}=a_{end};\\ 0,&a_{t}_{samp}. \]

### Training with RecPPO

We incorporate recommendation-specific supervision signals into PPO , referring to this enhanced approach as Recommender-Oriented PPO (RecPPO), to train our USIM. In the scenario of OOV item recommendation, the optimal action following certain states is clear, allowing the cumulative expected rewards for these states to be calculated directly. When the state representation \(_{i,t}\) of a specific item is equal to its item embedding \(}\), according to our designed reward function, the expected value should be 0 because any subsequent actions, except for termination will lead to negative rewards.

So we use these supervision signals to assist in training the value network \(V_{}\), the specific loss function of the value network is defined as follows,

\[()=_{(s_{t},r_{t},s_{t+1}) B}(r_{tExperiments

We conduct comprehensive experiments on two benchmark datasets aiming to address the following three questions: **RQ1:** Can USIM achieve superior OOV item recommendation performance compared to state-of-the-art OOV item recommendation models? **RQ2:** How key components of USIM affect its performance? **RQ3:** Is the proposed USIM more effective than representative RL methods? **RQ4:** What is the tendency of performance during USIM's imagination process? **RQ5:** How does USIM perform in real-world industrial recommendations? **RQ6:** How does USIM achieve efficiency compared to other baselines?

### Experimental Setup

**Datesets**. We evaluate the performance of USIM on OOV items using the widely used datasets: CiteULike  and MovieLens . Specifically, CiteULike contains 5,551 users, 16,980 articles (items), and 204,986 interactions. MovieLens comprises 6,040 users, 3,883 movies (items), and 1,000,210 interactions. Details about these datasets are shown in Appendix D.1.

**Baselines**.To assess the effectiveness and universality of USIM, we conduct a comparative analysis with 8 leading-edge models in OOV item recommendations across two distinct datasets. These models include two main groups. (i) Dropout-based methods: DropoutNet , MTPR , Heater , and CLCRec . (ii) Generative-based methods: DeepMusic , MetaEmb , GAR , and ALDI . Details about these models are shown in Appendix D.2. To further verify the generalization ability, we adopted both the widely used collaborative filtering model MF  and GNN-based model NGCF  as the recommender, respectively.

**Evaluation Metrics**. Following the evaluation of existing OOV recommendation , we conduct three different tasks in our experiments: (1) Overall Recommendation, (2) OOV Recommendation, and (3) IV Recommendation. We employ the full-ranking evaluation approach to assess the performance of overall, IV, and OOV recommendations. Following previous works , we utilize Recall@K and Normalized Discounted Cumulative Gain (NDCG@K) as metrics.

**Implementation Details**. We implement the baselines using their officially provided version. See Appendix E for the detailed implementation. The best hyperparameters are found for each dataset. For fairness, we use the same options and follow the designs in their articles for all baselines.

    &  &  &  &  \\  & &  &  &  &  &  &  \\  & &  &  &  &  &  &  &  &  &  &  &  &  \\    } & DropoutNet & 0.0973 & 0.0768 & 0.1148 & 0.1898 & 0.2442 & 0.1485 & 0.1352 & 0.1417 & 0.1560 & 0.0941 & 0.2577 & 0.2442 \\  & MTPR & 0.1090 & 0.0822 & 0.1210 & 0.1936 & 0.2489 & 0.1427 & 0.1364 & 0.1408 & 0.1808 & 0.1021 & 0.2463 & 0.2233 \\  & Heater & 0.1149 & 0.0881 & 0.1163 & 0.1901 & 0.2578 & 0

### Main Results (RQ1)

The main comparison results of overall, IV, and OOV item recommendation results can be found in Table 1. From the results, we can have the following observations.

**USIM can generally achieve significant improvements over state-of-the-art methods on both overall and OOV item recommendations while keeping the IV item recommendation.** From the tables, we observe that the USIM achieves the highest average Recall and NDCG performance across both MF and GNN recommenders. These comparison results verify the superiority of the imagined embeddings over traditional one-step generated embeddings.

**Dropout-based baselines have the performance drop in the IV item recommendation.** We find that the USIM and the generative-based models can keep the IV item recommendation. However, dropout-based models will lead to a performance drop on IV items. This suggests that there is a difference between OOV items and IV items. Pre-training representations of IV items in advance to generate representations for OOV items may be better for retaining information for IV items.

### Ablation Study (RQ2)

To validate the effectiveness of the individual components in our model, we compared the full model against four variants: (i) _w/o ct_ removes the cosine similarity-based top-k user selection when constructing the positive action set. (ii) _w/o ra_ removes the randomly sampled actions in the exploration set. (iii) _w/o es_ does not construct an exploration set and directly explores the entire action set. (iv) _w/o pr_ removes the performance reward, only using the similarity reward in the RL processing. From the results in Table 2, we make the following observations.

**Effectiveness of components in the exploration set construction**. The _w/o ct_ result indicates that the cosine similarity-based selection of the top-\(k\) users can simplify the exploration process by quickly identifying users related to the item embedding. Then, the performance of _w/o ra_ reveals the importance of including negative samples to provide a more comprehensive exploration signal, preventing the agent from overestimating action values by only considering positive feedback. Further, the poor performance of the _w/o es_ approach highlights the challenge of effectively exploring an extremely large action space, emphasizing the need for a well-designed exploration strategy to guide the agent towards promising actions.

**Effectiveness of components in the reward function**, Removing the performance reward component leads to inferior results. This suggests that the reward function should not only consider the distance between the generated embedding and the target embedding but also explicitly incorporate the downstream recommendation performance of the generated embedding.

### Comparison with Representative RL Methods (RQ3)

In Figure 3, we compared our proposed model with our initialized MLP and two other traditional reinforcement learning methods: Wolpertinger Policy  (WP) and Hierarchical Reinforcement Learning (HRL). WP utilizes the similarity of action representations and value estimation to solve the large discrete action space problem in reinforcement learning, while HRL employs a hierarchical decomposition of action space and sub-task solution to improve exploration efficiency.

Our model significantly outperforms the other two methods in both OOV recommendation and overall recommendation. Moreover, it can be observed that the other two methods do not show

    &  &  \\ Variant &  &  &  &  \\  & Recall & NDCG & Recall & NDCG & Recall & NDCG & Recall & NDCG \\  _w/o ct_ & 0.1742 & 0.1439 & 0.2336 & 0.1411 & 0.1590 & 0.1205 & 0.2363 & 0.1198 \\ _w/o ra_ & 0.1754 & 0.1433 & 0.2341 & 0.1412 & 0.1368 & 0.1156 & 0.2411 & 0.1336 \\ _w/o es_ & 0.1436 & 0.1279 & 0.2232 & 0.1316 & 0.1221 & 0.1000 & 0.1971 & 0.1103 \\ _w/o pr_ & 0.1866 & 0.1510 & 0.2711 & 0.1596 & 0.1417 & 0.1175 & 0.2414 & 0.1344 \\ 
**USIM** & **0.1926** & **0.1530** & **0.2753** & **0.1647** & **0.1632** & **0.1245** & **0.2534** & **0.1478** \\   

Table 2: Ablation study results between USIM with its four variants on CiteULike.

apparent improvements compared to their initial states. This supports the effectiveness of our tailored exploration method in improving performance in OOV recommendations.

### Case Study (RQ4)

To investigate the performance trend of our proposed method during the optimization process, and to validate the importance of the way of sampling in the embedding optimization process, we compare USIM with the following two user selection strategies: randomly sampling users at each step, and randomly selecting from the top 20 users with the highest relevance scores at each step. The results are shown in Figure 4, and according to the result, we can draw the following conclusions.

(i) Our method outperforms the random selection of top-20 users and the entire user set in both OOV and overall scenarios, indicating it can effectively identify users beneficial for optimization. However, performance declines as more users are imagined, likely due to reduced exploration by the agent.

(ii) When randomly selecting users from the top 20, we can observe that the performance in OOV recommendation increases after the first step. This suggests that sampling from high-scoring users is more likely to select users who are beneficial to the optimization process. However, the performance then continuously declines, indicating that this approach is not suitable for all states and has limited help for the optimization process.

(iii) Randomly selecting a user at each step leads to a drastic and non-recoverable decline in performance in both the OOV and overall scenarios after the first step. This suggests that in recommendation scenarios, it is extremely difficult to sample users from the massive user set who are beneficial to the optimization, and the majority of users are highly detrimental to the optimization process.

More experimental hyperparameter analysis can be found in Appendix F.

### Online Evaluation (RQ5)

To evaluate the performance of USIM in an industrial setting, we conducted a two-week online A/B test on a major e-commerce platform with 5% of users in each group. USIM was compared against three baselines: Random, MetaEmb , and ALDI . Details about our test platform and evaluation metrics are provided in Appendix G. Table 3 presents the results of these online A/B tests.

These remarkable improvements across all metrics underscore the effectiveness of the USIM in addressing the OOV item recommendation problem in real-world recommender systems. The

Figure 4: Performance analysis of different generation methods on the CiteUlike dataset.

Figure 3: Comparing USIM with other RL methods for overall and OOV recommendation performance in CiteULike dataset.

consistent and substantial performance gains, particularly in OOV item GMV, highlight the practical impact of our approach on business outcomes in e-commerce settings.

### Efficiency Analysis (RQ6)

To evaluate the time efficiency of USIM, especially in comparison to SOTA baselines, we recorded the total training time(Training Time), total convergence epochs(Converge Epochs), Time Per Epoch, and Inference Time for USIM and each baseline on the CiteULike and MovieLens datasets. The results are presented in Table 4. Based on these results, we can draw the following conclusions:

(i) **USIM is Faster than Heater and CLCRec**: USIM computes embeddings only for OOV items, whereas Heater and CLCRec must compute embeddings for both OOV and IV items.

(ii) **USIM is Comparable with MetaEmb and ALDI**: USIM imagines sequences only for OOV items, resulting in inference times comparable to MetaEmb and ALDI.

(iii) **USIM is Efficient in Training**: By fundamentally addressing OOV recommendation, USIM converges in fewer epochs, making training more efficient.

More experiments about online recommendation efficiency can be found in Appendix H.

## 5 Conclusion

Recommending out-of-vocabulary (OOV) items is challenging due to the lack of well-trained behavioral embeddings. Current models use "makeshift" embeddings from content features, leading to suboptimal performance. We introduced the User Sequence Imagination (USIM) framework to refine OOV embeddings by imagining user sequences and incorporating behavioral embeddings. By framing this as a reinforcement learning problem and creating a recommendation-focused reward function, USIM effectively enhances OOV recommendations. Extensive experiments demonstrate its superior performance and the ablation study further illustrates the effectiveness of USIM.