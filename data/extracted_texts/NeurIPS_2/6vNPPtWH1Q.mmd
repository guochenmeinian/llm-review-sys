# Energy-based Epistemic Uncertainty

for Graph Neural Networks

Dominik Fuchsgruber, Tom Wollschlager, and Stephan Gunnemann

School of Computation, Information and Technology & Munich Data Science Institute

Technical University of Munich, Germany

{d.fuchsgruber, tom.wollschlager, s.guennemann}@tum.de

###### Abstract

In domains with interdependent data, such as graphs, quantifying the epistemic uncertainty of a Graph Neural Network (GNN) is challenging as uncertainty can arise at different structural scales. Existing techniques neglect this issue or only distinguish between structure-aware and structure-agnostic uncertainty without combining them into a single measure. We propose GEBM, an energy-based model (EBM) that provides high-quality uncertainty estimates by aggregating energy at different structural levels that naturally arise from graph diffusion. In contrast to logit-based EBMs, we provably induce an integrable density in the data space by regularizing the energy function. We introduce an evidential interpretation of our EBM that significantly improves the predictive robustness of the GNN. Our framework is a simple and effective post hoc method applicable to any pre-trained GNN that is sensitive to various distribution shifts. It consistently achieves the best separation of in-distribution and out-of-distribution data on 6 out of 7 anomaly types while having the best average rank over shifts on _all_ datasets.

## 1 Introduction

Quantifying and understanding uncertainty is crucial to developing safe and reliable machine learning systems. Many applications such as Reinforcement Learning , Active Learning  or Out-of-Distribution detection  benefit from disentangling different facets of uncertainty . Typically, one distinguishes between an irreducible _aleatoric_ component and reducible _epistemic_ factors . The former is inherent to the data, while the latter is rooted in a lack of knowledge. One way to quantify epistemic uncertainty is to define a classifier-dependent density over the data domain . High values indicate similarity to the training data and thus imply low epistemic uncertainty. Energy-based models (EBMs) induce this density by defining an energy function that assumes low values near the training data . A common choice is the logits of the classifier as their magnitude is supposed to correlate with the model confidence . However, many architectures have been shown to produce arbitrarily overconfident predictions far from the training data . The negative implications for logit-based EBMs are only scarcely discussed in the literature .

While substantial effort has been directed toward uncertainty estimation for independent and identically distributed (i.i.d.) problems , graphs have only recently received attention within the community . There, uncertainty can arise at different structural scales, e.g., from only a single node, clusters, or global properties. Previous work only accounts for this implicitly by applying techniques from i.i.d. domains to Graph Neural Networks (GNNs) . A recent approach distinguishes only between structure-aware and structure-agnostic uncertainty  while not combining them into a single measure, which is often required in downstream applications . Consequently, estimates are often only sensitive to shifts that match their structural resolution. They may overfit certain anomaly types and not be reliable in general.

We address these shortcomings and propose a novel graph-based EBM (GEBM) that is structure-aware at different levels. The core idea behind GEBM is to define energy functions at different structural abstractions and facilitate the flexibility of the EBMs to aggregate them into a single, theoretically grounded measure. Interestingly, we find that interleaving a graph diffusion process with energy marginalization gives rise to energy functions that naturally capture patterns of different granularity. As depicted in Figure 1, we utilize three energy types: (i) _Group_ energy corresponds to evidence smoothing and emphasizes anomalies clusters within the graph while not distinguishing between their type locally. (ii) _Local_ energy is more granular and sensitive to evidence disagreements in the neighborhood of a node. (iii) _Independent_ energy is fully structure-agnostic and describes patterns that are limited to individual nodes. Aggregating them using soft maximum selection induces a single energy measure that assigns high uncertainty to anomalies at various scales. A Gaussian regularizer provably ensures that GEBM converges to low confidence far from the training data.

We evaluate GEBM over an extensive suite of datasets, distribution shifts, and baselines1. It consistently exhibits state-of-the-art performance in detecting out-of-distribution (o.o.d.) instances, while other approaches are only effective in a subset of settings. On all datasets, GEBM ranks the best on average over all distribution shifts. Beyond o.o.d. detection, we discover a novel theoretical connection between EBMs and evidential models. This interpretation of GEBM enables the GNN backbone to provide accurate predictions even under severe distribution shifts.

In summary, we tackle three main deficiencies of energy-based uncertainty for graphs by:

1. Proposing GEBM, an EBM that is aware of interdependence and provides a single uncertainty estimate incorporating different structural scales.
2. Formally showing that a Gaussian regularizer mitigates the overconfidence problem of logit-based EBMs and enables GEBM to provably induce an integrable data density.
3. Showing how to interpret GEBM as an evidential approach which considerably improves the predictive robustness of the classifier under distribution shifts.

## 2 Background

**Semi-Supervised Node Classification.** We consider (semi-supervised) node classification on an attributed graph \(=(,)\) with \(n\) nodes \(\) and \(m\) edges \(\) that is represented by an adjacency matrix \(=\{0,1\}^{n n}\) and a node feature matrix \(^{n d}\). Each node has a label \(_{i}\{1, C\}\). Given the labels of training nodes, the task is to infer the labels of the remaining nodes. Lastly, we focus on homophilic graphs, i.e. edges are predominantly present between nodes of the same class.

Figure 1: Overview of the Graph Energy-based Model (GEBM). Graph-agnostic energy (uncertainty) of a trained GNN \(f_{}()\) is first regularized to mitigate overconfidence and then aggregated at a local, cluster, and structure-independent scale by interleaving energy marginalization and graph diffusion. While group energy marginalizes before diffusion, local energy is fine-grained and can pick up conflicting evidence. GEBM assigns high uncertainty to several anomaly types simultaneously.

**Uncertainty in Machine Learning.** Usually, uncertainty regarding the prediction of a model is disentangled into aleatoric and epistemic factors \(u^{}\) and \(u^{}\), respectively. The former encompasses irreducible sources of uncertainty like measurement noise and inherent ambiguities. The latter is commonly defined as the non-aleatoric components of the overall uncertainty and can in general, be reduced, for example by acquiring additional data.

**Energy-based Models.** Epistemic uncertainty is commonly defined to be anti-correlated to a classifier-dependent density \(p_{}()\) over the input space . We study Energy-based Models (EBMs)  which define a joint energy \(E_{}(,y)\) over features and labels \(p_{}(,y)(-E_{}(,y))\). Marginalization over the labels induces a feature density with normalizer \(Z_{}\).

\[p_{}()=Z_{}^{-1}_{y}(-E_{}(,y )))=Z_{}^{-1}(-E_{}()) \]

Intuitively, the energy \(E_{}()=-_{y}(-E_{}(,y))\) can be interpreted as a soft minimum of the joint energies. It is low in regions of high confidence and high elsewhere. The measure of epistemic uncertainty implied by an BBM therefore is: \(u^{}()=- p_{}()=E_{}()+\). We can also write the conditional class probabilities as:

\[p_{}(y)=(-E_{}(,y)))/ _{y^{}}(-E_{}(,y^{}))) \]

This is exactly the softmax distribution applied to \(-E_{}(,y)\) which directly connects the logits \(f_{}()^{C}\) predicted by a classifier to the joint energy by defining \(E_{}(,y):=-f_{}()_{y}\).

**Evidential Deep Learning.** In contrast to first-order methods that directly predict \(p()\), evidential methods [62; 43] instead parameterize the corresponding second-order distribution from which first-order distributions are sampled . In classification, this second-order distribution is a Dirichlet distribution with parameters \(>\) called _evidence_. They indicate the confidence of the classifier. For inference, a first-order prediction can be obtained by taking the expectation:

\[_{()}[() ]=/_{y^{}}_{y^{}} \]

## 3 Related Work

**Uncertainty Estimation for i.i.d. Data.** Disentangling uncertainty has been approached from various perspectives. A family of sampling-based approaches uses a Bayesian Information-theoretic framework  that relies on stochastic predictions derived from a posterior over model weights. The total uncertainty is defined on the mean predictor, epistemic uncertainty as the deviation of each sample from that mean, and aleatoric uncertainty as the difference of both estimates . The posterior can either be explicitly modeled by Bayesian Neural Networks (BNNs) [6; 13; 15; 18], Monte-Carlo Dropout (MCD) , or implicitly realized by ensemble methods [77; 37]. Test-time augmentation [72; 73] and Stochastic Centering  also provide samples from this posterior. Sampling-free approaches are deterministic and estimate uncertainty with a single forward pass. Evidential methods [62; 68] predict a second-order distribution from which epistemic uncertainty is derived. Distance-based approaches quantify epistemic uncertainty as similarity to the training data [46; 48; 20; 65] and are closely related to density-based uncertainty estimation. (Deep) Gaussian Processes (GPs) [52; 45; 39] use a kernel function to measure this similarity but do not disentangle epistemic and aleatoric uncertainty. Posterior Networks combine evidential and density-based approaches by predicting density-based updates to the evidence [11; 10].

**Uncertainty Estimation for Graphs.** Many of these approaches transfer to graphs: DropEdge  applies MCD to edges and GPs can utilize a structure-aware kernel [53; 83; 41; 60]. SGCN  proposes an evidential student-teacher approach while G-\(\)UQ  applies Stochastic Centering to a GNN and improves on calibration. Graph Posterior Network (GPN)  diffuses the density-based evidence of a Posterior Network but provides separate measures for structure-aware and structure-agnostic uncertainty, each of which is only effective on some distribution shifts.

**Energy-based Models.** EBMs [38; 3; 2] are typically employed in generative modelling  but have also been applied to uncertainty estimation  and anomaly detection [40; 81]. To address the overconfidence of logit-based EBMs far from training data, a Gaussian regularization term hasrecently been proposed  that we employ in our framework as well. While their work empirically validates this adjustment, we formally prove the overconfidence of logit-based EBMs to happen with high probability and Gaussian regularization to mitigate the issue. The HEAT framework  learns multiple corrected EBMs via stochastic gradient Langevin dynamics  and composes them into a single measure. In contrast, our approach does not require additional training and aggregates energy that emerges naturally at different scales in the graph from a single logit-based joint energy model. Lastly, GNNSafe  diffuses the logit-based energy of a GNN to improve its out-of-distribution detection. In contrast, our model considers energy beyond the cluster scale.

## 4 Method

We develop a simple yet effective **G**raph-**E**nergy-**B**ased **M**odel (GEBM) for post hoc epistemic uncertainty that is sensitive to a variety of distribution shifts from any pre-trained GNN.

### Energy at Different Scales

Previous work either does not distinguish between uncertainty at different structural resolutions at all  or only disentangles structure-aware and structure-agnostic factors without combining them . However, for many practical purposes, epistemic uncertainty must be quantified with a single measure . We address these issues by proposing an EBM-based uncertainty that incorporates patterns on different natural abstractions of a graph. The density induced by GEBM, \(p_{}()\), serves as a singular uncertainty measure that is sensitive to multiple distribution shifts simultaneously.

Inspired by the success of graph diffusion , we propose energy on different structural levels: (i) Graph-agnostic, for node features in isolation. (ii) Based on the evidence in the local neighborhood of a node. (iii) Within clusters in the graph. Note that defining global energy on the whole graph induces no differences at the node level and therefore we do not consider it in this work. We point to appropriate measures for a potential extension of GEBM in the existing literature .

We make the intriguing observation that interleaving a diffusion process \(P_{}:^{k}^{k}\) with the marginalization of structure-agnostic joint energy \(E_{}(,y)\) as in Equation (1) induces energy functions that describe the aforementioned natural abstraction levels. Based on this insight, we can derive definitions for three types of energy functions:

**Independent Energy.** On the finest scale, we consider energy independent of structural effects by omitting the diffusion operator. This term captures uncertainty regarding node features in isolation.

\[E_{,I}()=-_{y}(-E_{}(,y)) \]

**Local Energy.** On a coarser scale, we diffuse the joint energy _before_ marginalization. This retains local information like conflicting feature-based evidence within the neighborhood of a node as the class-specific information is marginalized _after propagation_:

\[E_{,L}()=-_{y}[P_{}(-E_{}(,y))] \]

**Group Energy.** By interchanging marginalization and diffusion, we effectively smooth the marginal evidence \(E_{}()\). Therefore, energy gets propagated predominantly within clusters of the graph.

\[E_{,G}()=-P_{}[_{y}(-E_{}(,y))] \]

Since marginalization is done _before propagation_, less local information is preserved: The energy of a node will increase when its cluster is anomalous, regardless of whether the type of anomaly matches its own. As can be seen exemplary in Figure 1, this loss of local information comes at the benefit of being less exposed to local variability within coarser clustered patterns.

Each energy type captures anomalies that affect the corresponding structural scale. We provide synthetic experiments in Appendix C.7 as an additional intuitive explanation for the aforementioned energy terms. In practice, GEBM enables practitioners to augment our framework with further, potentially task-specific, energy functions. We empirically find however that the combination of the three naturally arising energy terms already detects a broad range of distribution shifts.

### Regularizing Logit-based EBMs

Following Equation (1), EBMs imply a density over the data domain \(p_{}()\) with normalization constant:

\[Z_{}=_{}_{y}(-E_{}(,y))d {x} \]

When quantifying epistemic uncertainty with this density, the energy must be low in regions of the input feature space close to the training data, while high values should be assumed far away. To that end, \(p_{}()\) must be integrable, that is, have a finite normalizer \(Z_{}\). However, previous work has shown that piecewise affine classifiers which are the backbone of modern GNNs will converge to overconfident logits far from training data . We remark that previous studies found GNNs to be underconfident on in-distribution data [74; 75] while the aforementioned issue of overconfidence arises from high distance to training data induced by a distribution shift (see Appendix C.8). This overconfidence is problematic, as logit-based energy can suffer from the same issue and \(E_{}()\) may assume arbitrarily small values far from the training data. This contradicts the aforementioned desideratum of low confidence and the implied uncertainty measure breaks under severe distribution shifts. The normalizer of the logit-based joint energy \(Z_{}\) is not finite with a high probability and therefore the EBM can not induce an integrable density \(p_{}()\).

**Proposition 4.1**.: _Let \(f_{}:^{d}^{C}\) be a piecewise affine function and \(^{h}=_{l}^{L}Q_{l}\) be the disjoint set of polytopes on which \(f_{}\) is affine, i.e. \(f_{}()=^{(l)}+^{(l)}\) for \( Q_{l}\). Assuming the direction of the rows of each \(^{(l)}\) to be uniformly distributed, the probability that \(Z_{}\) converges decreases exponentially in the number of non-closed linear regions \(L^{}\) and classes \(C\)._

\[[Z_{}<]=(1/2)^{C L^{}}\]

We provide proofs for all claims in Appendix A. Intuitively, since \(f_{}\) behaves like an affine function in the limit \(\|\|_{2}\), its predicted logits may diverge toward \(\). If for any class the model produces overconfident predictions in one of its affine regions, the marginal energy \(E_{}()\) will diverge toward maximal confidence. As classifiers are trained to output high values for one of the classes, we expect the actual probability of a well-behaved energy function to be even lower than our theoretical bound that assumes uniform weight directions. In practice, this pathological behavior of logit-based EBMs has been observed in previous work [64; 36; 35]. We mitigate this issue by augmenting the logit-based energy and we prove that this regularization induces an integrable density.

### Our Model

Similarly to recent work on EBMs , we employ a class-conditional Gaussian prior \((_{}()_{y}|\ _{y},_{y})\) as a regularizer for the logit-based energy. We learn the parameters \(\{_{y},_{y}\}_{y=1}^{C}\) as the maximum likelihood estimates from the training instances of each class.

\[_{}(,y)=E_{}(,y)-(_{ }()_{y}_{y},_{y}) \]

The regularization strength \(>0\) and the choice of the diffusion operator \(P_{}\) are the only hyperparameters of GEBM. Each of them has an intuitive interpretation: \(\) controls the trade-off between the predictive dependency and distance-awareness of the EBM while \(P_{}\) encapsulates how information is propagated over \(\). Regularization ensures that the corresponding marginal density \(_{}()\) has a finite normalizer \(_{}\) and is therefore integrable.

**Theorem 4.2**.: _For a piecewise affine classifier \(f_{}\) as in Proposition 4.1, \(_{}\) is well-defined._

\[_{}=_{}_{y}(-E_{}(,y))\, (_{}()_{y}_{y},_{y})^{ }d<\]

This regularized joint energy diverges toward maximal uncertainty in the limit. Consequently, its induced density provides an uncertainty estimator that is reliable even far away from the training data.

**Corollary 4.3**.: _For a piecewise affine classifier \(f_{}\) as in Proposition 4.1, and any \(^{d}\) almost surely:_

\[_{}_{}()=0\]We then combine the _regularized energy_\(_{}(,y)\) at different structural scales (Equations (4) to (6)).

\[_{,}()=[(_{,I}())+(_{,L}())+(_{ ,G}())] \]

Since energy is defined per node, most diffusion processes (Appendix C.6) act on individual nodes as an affine function with a positive coefficient (see Appendix A.2). Therefore, each individual energy and their aggregate, \(_{,}\), induce an integrable density when using a linear diffusion operator \(P_{}\).

**Theorem 4.4**.: _For a linear diffusion operator \(P_{}()=+\), \(>0\) and the regularized energy \(_{}(,y)\), GEBM induces an integrable density:_

\[_{}(-_{,}() )d<\]

GEBM is lightweight and can be applied post hoc to _any_ logit-based GNN without additional training. Its induced density is a proxy for epistemic uncertainty: \(u^{}()=-_{}()\) and we compute aleatoric uncertainty as the entropy of the predictive distribution of the GNN . Following , we realize the diffusion operator as a repeated application of a label-propagation scheme (see Appendix C.6). Intuitively, this operator is a smoothing process which is only appropriate when assuming the in-distribution data to be homophilic. In the case of non-smooth (heterophilic) training data, the proposed energy would be high for in-distribution data which is undesired behaviour for an EBM-based uncertainty estimator. To disentangle effects at different structural scales, we define each of GEBM's components on structure-agnostic regularized joint energies \(_{}(,y)\). To that end, we compute the outputs of the classifier by omitting the structure _for the computation of \(_{}\) only_ by evaluating \(f_{}(,)\), i.e. setting \(=\). These outputs depend only on the node features.

### EBMs as Evidential Models

Beyond using epistemic uncertainty to detect anomalies, we also show how the joint density induced by GEBM enables predictions that are robust against distribution shifts. First, we discover a correspondence between first-order predictions of logit-based classifiers (Equation (2)) and evidential predictions (Equation (3)). This connects the joint energy \(E_{}(,)\) of an EBM to the evidence \(\):

\[(-E_{}(,)) p_{ }(,) \]

For an integrable density \(p_{}(,)\), the evidence \(\) will vanish in the limit far away from training data. Previous work on Posterior Networks  fits a normalizing flow to obtain class-conditional densities \(p_{}()\) and uses them to compute a Bayesian update to a prior evidence \(^{}\). Its extension to graphs, GPN , diffuses these structure-agnostic updates with an operator \(P_{}\). Similarly, our GEBM framework induces a normalized joint density \(_{}(,)\) through its regularized energy function at no additional cost. Therefore, it can also be interpreted as an evidential classifier that enables inference according to Equation (3).

\[^{}=^{}+P_{} (*p_{}(,))}^{)}}^{}=^{}+P_{}(_{}^{-1}*(-_{}(, ))}^{)}}\]

Here, \(\) is called an uncertainty budget and it roughly corresponds to the normalizer of the joint energy. This interpretation of EBMs recovers desirable properties of density-based evidential methods : Predictions will converge toward a prior \(^{}\) far from the training distribution. They will be less affected by anomalies in the neighborhood of a node and therefore be more robust against distribution shifts. We remark that while our framework enables this evidential inference scheme, it is also possible to instead use the backbone classifier as-is and preserve its predictive performance.

## 5 Experiments

We evaluate the efficacy of GEBM by extending the evaluation proposed in  and expose it to a suite of 7 distribution shifts from three families that cover a broad range of anomaly types.

### Setup

**Datasets and Distribution Shifts.** We use seven common benchmark datasets for node classification: The five citation datasets _CoraML_, _CoraML-LLM[4; 78]__, _Citeseer_[61; 25], _PubMed_, _Coauthor-Physics_ and _Coauthor-Computers_, and the co-purchase graphs _Amazon Photos_ and _Amazon Computers_. We expose all methods to three families of distribution shifts:

1. **Structural**. We select nodes with the lowest _homophily_ as o.o.d. and train on homophilic nodes only. This induces a shift in the local connectivity pattern of the nodes. Furthermore, we include a setting in which nodes with low _Page-Rank_ (PR) centrality  are considered o.o.d.
2. **Leave-out-Class.** We withhold nodes belonging to a subset of classes during training and reintroduce them during inference. In practice, this might, for example, correspond to a new type of user joining a social network. We either select the held-out classes randomly or pick those with the lowest average homophily to make them more dissimilar to the retained classes.
3. **Feature Perturbations**. We randomly choose a subset of nodes and perturb their features by replacing them with noise. Since most datasets have categorical bag-of-words features, we have fine-grained control over the severity of the shift in this setting. We generate _near-o.o.d._ data by sampling from a Bernoulli distribution \(\) fitted on the node features of the dataset \(\). A stronger shift is induced by fixing the success probability at a value of \(p=0.5\). Drawing node features from \((0,1)\) constitutes a domain shift (_far-o.o.d._) for categorical data.

Existing work concerns transductive node classification which enables leakage of o.o.d. information during training [64; 79]. Instead, we study the inductive setting and remove o.o.d. nodes and their edges during training, and provide results for the transductive setting in Appendix C. All results were averaged over \(5\) splits and \(5\) initializations each (for standard deviations, see Appendix C).

**Model and Baselines.** We compare GEBM to different baselines for (epistemic) uncertainty estimation: Ensembles (**GCN-Ens**), MC-dropout (**GCN-MCD**), DropEdge (**GCN-DropEdge**), a combination of MC-dropout and DropEdge (**GCN-MCD+DropEdge**) and a Bayesian GCN (**BGCN**) are sampling-based approaches. We also compare against the logit-based EBM (**GCN-Energy**), **GNNSafe**, and **HEAT** as EBM baselines. Lastly, we consider **GPN** and **SGCN** as evidential methods. For all models, including GEBM, we use the same backbone architecture (see Appendix B.2).

**Metrics.** We evaluate epistemic uncertainty by detecting distribution shifts. That is, we report the AUC-ROC and AUC-PR metrics for the binary _out-of-distribution detection_ problem of separating in-distribution (i.d.) from out-of-distribution (o.o.d.) nodes. Additionally, we report how well uncertainty correlates with erroneous (_miscalessification detection_) in Appendix C.3. Since our proposed method does not alter the softmax predictions of the backbone model, it will affect neither the aleatoric estimates nor their _calibration_. Consequently, our framework is open to additional post hoc calibration methods such as temperature scaling . For completeness, we report the Expected Calibration Error (ECE)  and the Brier score  in Appendix C.4 for the GNN backbone.

### Results

**Out-of-Distribution Detection.** Table 1 shows the performance of different aleatoric and epistemic uncertainty methods on various distribution shifts (full results in Table 6). Across all datasets and distribution shifts, our model provides the best or second-best separation of o.o.d. data from i.d. data. In practice, an estimator that is effective on all distribution shifts simultaneously is desirable. Therefore, we rank (\(\)) all estimators individually for each shift and dataset and report its average rank over all distribution shifts for each dataset. To not favor one distribution shift family, we adjust the weight such that Leave-out-Class, structural shifts, and feature perturbations contribute the same amount. We rank all epistemic uncertainty proxies both against other epistemic measures and aleatoric uncertainty separately. In both cases, Tables 2 and 7 show that our proposed EBM-based epistemic uncertainty is the only estimator that is effective under different distribution shifts at the same time. On all datasets, GEBM improves the AUC-ROC scores by \(5.8\) to \(10.9\) percentage points on average (\(16\%\)-\(32\%\) relative improvement) over a vanilla EBM, the second best-ranked estimator (Appendix C.2). Since all EBM-based approaches (GCN-EBM, GCNSafe, GEBM) are post hoc methods, they share the aleatoric uncertainty and high predictive accuracy of the backbone.

Out of all 7 distribution shifts, our framework is only less sensitive to the centrality shift. Many baselines only perform well because of their symmetric diffusion operator that biases any arbitrarysignal toward high-degree nodes regardless of the quality of the uncertainty (see Appendix C.6). While this bias could be explicitly incorporated into GEBM by defining an additional energy term, the goal of our work is not to engineer our measure toward certain downstream settings. GEBM is already sensitive to many distribution shifts as-is. Even after accounting for this setting, our estimator achieves the best average rank over all shifts. We defer further discussion to Appendix B.

**Misclassification Detection.** In alignment with previous work , we observe aleatoric uncertainty to be more effective for misclassification detection than epistemic estimates. Our method performs competitively among other epistemic measures as shown in Table 16. As previously discussed, improving aleatoric uncertainty for GNNs is beyond the scope of this work.

**Robust Evidential Inference.** As described in Section 4.4, GEBM enables evidential inference. We expose our model to feature shifts sampled from \((0,1)\) and increase the fraction of perturbed nodes. Figure 2 shows the predictive performance of the classifier induced by an evidential interpretation of the EBM at different regularization weights \(\) compared to a vanilla EBM and the evidential model, GPN. We maintain high accuracy like an evidential model, while the performance of the baselines rapidly deteriorates. The hyperparameter \(\) can be seen as interpolating between an unregularized overconfident logit-based EBM and an uncertainty-aware evidential method. While GPN requires explicit evidential training, our method enables robust inference at negligible additional cost from a pre-trained GNN. Furthermore, GPN notably sacrifices predictive capability on clean data, while GEBM enables more control over the trade-off between accuracy on clean and perturbed data.

    &  &  &  & (0,1)\) _(ar)_} &  \\  & & AUC-ROC\(\) & Acc.\(\) & AUC-ROC\(\) & Acc.\(\) & AUC-ROC\(\) & Acc.\(\) & AUC-ROC\(\) & Acc.\(\) \\  & & (Alex. / Epi) & (Alex. / Epi) & (Alex. / Epi) & (Alex. / Epi) & (Alex. / Epi) & (Alex. / Epi) & (Alex. / Epi) \\   & GCN-DE & 86.27/3.0 & 87.2 & 4.71/68.8 & 71.9 & 26.8/57.8 & 71.9 & 70.8/54.5 & 85.6 \\  & GCN-Ens & 89.87/3.9 & 90.4 & 59.4/64.1 & 75.9 & 30.4/52.7 & 75.9 & 71.5/68.5 & 91.5 \\  & GPN & 85.3/88.1 & 88.5 & 53.4/52.4 & 72.8 & 51.4/55.5 & 72.8 & 66.9/51.5 & 87.6 \\  & GCN-EBM & 89.78/9.9 & 90.3 & 59.9/58.6 & 75.8 & 31.7/25.7 & 75.8 & 71.3/71.2 & 91.2 \\  & GCN-HEAT & 89.78/1.1 & 90.3 & 59.9/64.4 & 75.8 & 31.7/76.6 & 75.8 & 71.3/70.0 & 91.2 \\  & GCN-Safe & 89.79/**1.6** & 90.3 & 59.9/53.4 & 75.8 & 31.7/35.3 & 75.8 & 71.3/73.1 & 91.2 \\  & **GCN-GEBM** & 89.79/**1.6** & 90.3 & 59.9/**94.5** & 75.8 & 31.7/**86.4** & 75.8 & 71.3/**76.7** & 91.2 \\   & GCN-DE & 76.4/61.2 & 91.7 & 52.8/51.4 & 88.5 & 41.8/51.5 & 88.5 & 62.7/52.0 & 96.6 \\  & GCN-Ens & 74.7/80.3 & 91.9 & 52.3/51.3 & 89.9 & 42.6/55.0 & 89.9 & 62.0/65.2 & 97.5 \\  & GPN & 74.5/87.3 & 88.8 & 55.3/54.9 & 86.5 & 54.2/55.5 & 86.5 & 65.9/47.0 & 97.1 \\  & GCN-EBM & 74.6/75.2 & 91.8 & 52.1/52.1 & 89.8 & 44.2/42.6 & 89.8 & 61.9/60.0 & 97.4 \\  & GCN-HEAT & 74.6/77.3 & 91.8 & 52.1/51.8 & 89.8 & 44.2/57.3 & 89.8 & 61.9/62.6 & 97.4 \\  & GCNSafe & 74.6/75.6 & 91.8 & 52.1/50.9 & 89.8 & 44.2/46.4 & 89.8 & 61.9/60.5 & 97.4 \\  & **GCN-GEBM** & 74.6/83.6 & 91.8 & 52.1/**64.2** & 89.8 & 44.2/**92.4** & 89.8 & 61.9/**68.3** & 97.4 \\   & GCN-DE & 89.2/73.7 & 91.3 & 64.3/67.9 & 88.8 & 33.7/60.3 & 88.8 & 62.9/55.7 & 97.1 \\  & GCN-Ens & 89.78/3.3 & 92.4 & 69.6/67.8 & 90.0 & 32.8/60.7 & 90.0 & 63.9/59.1 & 97.9 \\  & GPN & 73.7/88.3 & 86.6 & 54.1/59.7 & 81.7 & 53.6/59.9 & 81.7 & 61.3/45.5 & 97.9 \\  & GCN-EBM & 89.78/89.9 & 92.3 & 69.3/71.6 & 89.8 & 34.7/28.8 & 89.8 & 63.8/60.5 & 97.8 \\  & GCN-HEAT & 89.7/88.6 & 92.3 & 69.3/70.0 & 89.8 & 34.7/69.8 & 89.8 & 63.8/66.6 & 97.8 \\  & GCNSafe & 89.7/92.1 & 92.3 & 69.3/63.0 & 89.8 & 34.7/69.8 & 89.8 & 63.8/62.4 & 97.8 \\  & **GCN-GEBM** & 89.7/**92.8** & 92.3 & 69.3/**99.5** & 89.8 & 34.7/**90.0** & 89.8 & 63.8/**69.5** & 97.8 \\   

Table 1: Out-of-Distribution detection AUC-ROC (\(\)) using aleatoric or epistemic uncertainty (best and runner-up). Our epistemic measure achieves the strongest performance on most datasets and shifts while maintaining the classification accuracy of the GCN backbone.

    &  &  &  &  &  &  &  &  \\  & & & LLM & & & & Computers & Photo & CS & Physics \\  GCN-DE & 6.3/14.2 & 8.0/16.9 & 7.8/16.2 & 7.3/13.6 & 7.2/15.9 & 7.1/15.8 & 6.5/13.2 & 8.1/16.8 \\ GCN-Ens & 7.8/16.1 & 6.3/15.0 & 6.4/12.9 & 6.2/12.4 & 5.7/11.6 & 4.7/9.0 & 4.8/10.4 & 6.2/13.3 \\ GPN & 7.0/15.2 & 5.4/11.2 & 6.6/12.9 & 5.8/12.4 & 5.7/11.8 & 5.3/11.7 & 7.2/13.9 & 6.1/14.5 \\ GCN-EBM & 4.5/7.9 & 4.0/8.2 & 5.2/10.3 & 5.2/12.0 & 4.2/10.7 & 4.9/10.9 & 4.1/7.1 & 4.4/8.4 \\ GCN-HEAT & 4.4/10.4 & 5.4/12.9 & 3.8/9.2 & 5.6/12.8 & 3.5/8.6 & 4.5/10.2 & 4.2/8.3 & 4.7/10.4 \\ GCN-Safe & 5.3/9.1 & 4.4/8.1 & 5.2/9.5 & 5.4/8.9 & 5.9/13.2 & 6.4/12.9 & 5.5/10.7 & 5.8/11.7 \\
**GCN-GEBM** & **2.7/4.5** & **2.9/1.4** & **3.3/5.7** & **3.8/7.4** & **2.5/ 4.2** & **2.7/ 4.5** & **2.7/ 4.3** & **3.0/ **5.0** \\   

Table 2: Average o.o.d. detection rank (\(\)) of epistemic uncertainty versus other epistemic measures / all uncertainty measures over all distribution shifts (best and runner-up). GEBM has the best average performance rank over all distribution shifts and epistemic (and aleatoric) uncertainty estimators.

The advantage of this evidential perspective is that the evidence approaches zero under increasingly severe distribution shifts. Therefore, a fixed number of diffusion steps suffices to effectively counteract the influence of anomalous neighbors when making predictions for a node. In contrast, using diffusion at, for example, the predictive level is inadequate: As shown in Proposition 4.1, the energy is likely to diverge and, therefore, the number of diffusion steps necessary to mitigate arbitrarily severe distribution shifts is unbounded. Consequentially, we find that even compared to models like APPNP that heavily rely on graph diffusion at the predictive level, the evidential interpretation of GEBM notably improves robustness (see Figure 6).

### Ablations.

Energy at Different Scales.In Table 3, we compare GEBM to energy at specific structural scales and a variant without regularization (\(=0\)). Each of the former corresponds to one of the energies that the GEBM framework is composed of. Group energy is equivalent to a regularized logit-based EBM which is therefore also ablated implicitly. As expected, each component is sensitive to different shifts. This confirms that interleaving marginalization and diffusion captures patterns at different resolutions. As shown in Table 3, a variant of the aggregate GEBM achieves the best rank on \(7\) of \(8\) datasets. This shows that scale-awareness is the key ingredient for effective uncertainty estimation on graphs which can be further improved with energy regularization. In far-o.o.d. settings, this regularization is crucial to obtain reliable estimates as it mitigates overconfidence issues.

Energy Regularization.We also ablate the effect of regularizing the joint logit-based energy in Figure 3. At increasing distance from the training data, logit-based energy becomes overconfident. In contrast, the regularized \(E_{}\) follows the logit-based energy near the training data and is dominated by the regularizer far away. This is reflected in the clear separation between perturbed and unperturbed nodes. When computing structure-aware energy, feature corruptions affect unperturbed nodes through the diffusion operation, making a clean separation difficult. This justifies our choice to compute structure-agnostic joint energy \(_{}(,)\) and factor in the graph afterward by applying \(P_{}\).

Backbone Architecture.Our method can be applied post hoc to any logit-based GNN. Table 4 evaluates the o.o.d.-detection performance of our EBM framework using differ

    &  & \))} & (0,1)\)} & Homo. \\    } & EBM & \(89.9\) & \(67.1\) & \(26.4\) & \(71.2\) \\  & Safe & **91.6** & \(56.9\) & \(36.1\) & \(73.1\) \\  & GEBM & **91.6** & \(71.4\) & **96.6** & **76.7** \\    } & EBM & \(90.2\) & \(55.6\) & \(44.1\) & \(72.1\) \\  & Safe & \(91.5\) & \(53.2\) & \(45.4\) & \(70.3\) \\  & GEBM & \(85.0\) & \(60.9\) & **76.5** & **72.6** \\    } & EBM & \(76.5\) & \(\) & \(42.8\) & \(53.2\) \\  & Safe & \(79.0\) & \(49.3\) & \(46.2\) & \(51.2\) \\   & GEBM & \(80.7\) & \(51.4\) & \(\) & **65.4** \\    } & EBM & \(74.0\) & \(\) & \(42.2\) & \(53.2\) \\   & Safe & \(\) & \(49.2\) & \(46.6\) & \(51.1\) \\    & GEBM & \(\) & \(51.6\) & \(\) & **62.7** \\   

Table 4: O.o.d. detection AUC-ROC(\(\)) using different backbones. Our method is effective on all architectures.

ent commonly used GNN backbones: GCN , GAT (v2) [70; 8], GIN  and GraphSAGE . Standard deviations and AUC-PR are reported in Appendix C.5. Over different distribution shifts, our proposed approach consistently outperforms the logit-based EBM and the graph-specific GNNSafe variation. This shows the broad applicability of our framework that enables reliable high-quality epistemic uncertainty estimation from a large family of logit-based GNNs.

## 6 Limitations and Broader Impact

**Limitations.** As GEBM is a post hoc epistemic estimator, it does not improve aleatoric uncertainty or its calibration. In particular, the GCN backbone used in this work does not consistently achieve the strongest performance in both tasks. While the structural scales that arise naturally from graph diffusion cover many distribution shifts as-is, specific applications may require augmenting GEBM with additional energy terms, as we also observe for centrality-based shifts. While GEBM enables robust evidential inference, future work may build upon its paradigm of aggregating different structural scales in the graph for fully evidential methods. Lastly, we study homophilic node classification problems and leave an extension of GEBM beyond this setting to future work.

**Broader Impact.** GEBM enables cheap and simple uncertainty quantification for GNNs which we believe to contribute to the development of more reliable AI. Nonetheless, we encourage practitioners to actively reevaluate our measure of uncertainty to mitigate risks in safety-critical domains.

## 7 Conclusion

We propose GEBM, a simple and efficient EBM for post-hoc epistemic uncertainty estimation for GNNs. To the best of our knowledge, we are the first to consider uncertainty at different structural scales and address this gap by proposing a model that aggregates energy that naturally arises from graph diffusion. It consistently outperforms existing approaches over an extensive suite of datasets at detecting various distribution shifts. We formally and empirically confirm that logit-based EBMs suffer from overconfidence and prove that the regularized GEBM mitigates this issue by inducing an integrable data density. We exploit this property by discovering a link to evidential methods that enables the backbone to provide accurate predictions even under severe distribution shifts.