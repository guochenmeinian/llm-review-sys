# Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach

Weiyu Ma\({}^{1,2}\), Qirui Mi\({}^{1,2}\), Yongcheng Zeng\({}^{1,2}\), Xue Yan\({}^{1,2}\), Yuqiao Wu\({}^{1,2}\), Runji Lin\({}^{1,2}\),

Haifeng Zhang\({}^{1,2,4}\), Jun Wang\({}^{3}\)

\({}^{1}\) Institute of Automation, Chinese Academy of Sciences, China

\({}^{2}\) School of Artificial Intelligence, University of Chinese Academy of Sciences, China

\({}^{3}\) AI Centre, Department of Computer Science, UCL

\({}^{4}\) Nanjing Artificial Intelligence Research of IA, China

Correspondence to : \(\)haifeng.zhang@ia.ac.cn \(\), \(\)jun.wang@cs.ucl.ac.uk\(\)

###### Abstract

With the continued advancement of Large Language Models (LLMs) Agents in reasoning, planning, and decision-making, benchmarks have become crucial in evaluating these skills. However, there is a notable gap in benchmarks for real-time strategic decision-making. StarCraft II (SC2), with its complex and dynamic nature, serves as an ideal setting for such evaluations. To this end, we have developed TextStarCraft II, a specialized environment for assessing LLMs in real-time strategic scenarios within SC2. Addressing the limitations of traditional Chain of Thought (CoT) methods, we introduce the Chain of Summarization (CoS) method, enhancing LLMs' capabilities in rapid and effective decision-making. Our key experiments included: 1. LLM Evaluation: Tested 10 LLMs in TextStarCraft II, most of them defeating LV5 build-in AI, showcasing effective strategy skills. 2. Commercial Model Knowledge: Evaluated four commercial models on SC2 knowledge; GPT-4 ranked highest by Grandmaster-level experts. 3. Human-AI Matches: Experimental results showed that fine-tuned LLMs performed on par with Gold-level players in real-time matches, demonstrating comparable strategic abilities. All code and data from this study have been made publicly available at [https://github.com/histmeisha/Large-Language-Models-play-StarCraftII](https://github.com/histmeisha/Large-Language-Models-play-StarCraftII)

## 1 Introduction

Real-time strategy decision-making and long-term planning are critical AI challenges, necessitating rapid, tactical decisions and strategic adaptability over time. StarCraft II (SC2), one of the world's most popular and challenging e-sports, exemplifies these demands through its dynamic gameplay. Players must manage resources, construct bases, and command armies while making quick decisions and adapting their long-term strategies to evolving battlefield conditions. The game's layered gameplay spans economic management, military strategy, and tactical execution, making SC2 a valuable model for AI research, particularly in reinforcement learning (RL). SC2's complexity, real-time nature, and the fact that it is considered one of the hardest games in the world pose significant challenges for AI systems, requiring them to master various aspects of the game simultaneously. Further details about StarCraft II can be found in the appendix 2. Pioneering efforts, exemplifiedby DeepMind's AlphaStar , have demonstrated significant advancements in this domain, showcasing AI's growing proficiency in strategic gameplay. With the evolution of LLMs in areas like reasoning, planning, and decision-making, these models have begun to show potential in tasks traditionally dominated by RL approaches. Benchmarks such as AGENTBENCH  have been instrumental in evaluating these capabilities in multi-turn, open-ended contexts. However, despite these developments, a specific benchmark for assessing LLMs' capabilities in real-time strategy decision-making and long-term planning in environments like StarCraft II is conspicuously absent.

Therefore, we have chosen SC2 as the benchmark for evaluating the real-time strategy decision-making and long-term planning capabilities of LLMs. Given the lack of language support in existing SC2 environments, we developed TextStarCraft II. Utilizing the python-sc2 framework, TextStarCraft II converts the complex gameplay dynamics of SC2 into an interactive, text-based format. The python-sc2 interface 1 is utilized to convert game data into text, enabling LLM agents to perform macro-strategic actions through language commands. For micro-strategic actions, we implement a rule-based approach akin to that used by OpenAI Five , employing predefined Python scripts. This allows LLM agents to engage in competition against the game's built-in AI, other LLM agents, and human players through the execution of these scripted actions. Addressing the challenges posed by the intricate decision-making process in SC2, we propose the Chain of Summarization(CoS) method. This approach enhances the capacity of LLM Agents in processing complex information and making strategic decisions by incorporating single-frame and multi-frame summarization modules, each aiding in understanding the immediate game state and processing sequential data for strategy formulation and decision-making.

In this study, we conduct a thorough exploration of LLMs' application and effectiveness within SC2 via the TextStarCraft II environment. Our experimental framework includes assessing the CoS method, evaluating the performance of proprietary and fine-tuned open-source LLMs in TextStarCraft II, testing real-time human-computer interaction, and using StarCraft II-themed question-answering tasks evaluated by human experts. We also analyze the impact of varying prompts on LLMs, their strategic preferences, and the interpretability of their decision-making processes.

Our contributions are manifold:

* **TextStarCraft II Development:** We present TextStarCraft II, a novel environment that not only enables the evaluation of LLMs in strategic gaming contexts but also supports real-time human-computer interactions.
* **Chain of Summarization Method and Agent:** By introducing the CoS method and releasing an open-source agent, we offer a powerful example and a high-level interface for the community, fostering further development and interaction with TextStarCraft II.
* **Diverse Evaluations of LLMs:** Our extensive evaluations encompass testing LLMs against the game's built-in AI, assessing their understanding of StarCraft II through experts reviews, and conducting human-AI matches. These methodologies underscore LLMs' proficiency in strategic decision-making and their potential for human-like gameplay.

## 2 Background: StarCraft II

StarCraft II (SC2) is a real-time strategy (RTS) game developed by Blizzard Entertainment, renowned for its strategic depth and complexity. It features three distinct playable races: Terrans (humans with advanced technology), Protoss (a psonic species with powerful abilities), and Zerg (a biologically advanced, insectoid race), each with unique units, structures, and gameplay mechanics. The game's core elements include resource management, base construction, unit production, and tactical combat. Players must efficiently gather resources (minerals and vespene gas), construct and expand bases, compose balanced armies, and execute strategic plans while adapting to opponents' moves. This multi-faceted gameplay requires players to make rapid decisions under incomplete information, balancing short-term tactics with long-term strategy. SC2's complexity is further enhanced by its asymmetric design, where each race has distinct strengths and weaknesses, leading to a rich variety of strategies and counter-strategies. The game also incorporates elements of economics, technology progression, and territorial control, making it a comprehensive test of strategic thinking and multitasking ability.

Since its release in 2010, SC2 has been a cornerstone of competitive gaming, featuring prominently in major e-sports tournaments worldwide. The game's professional scene is characterized by high-level play, particularly from South Korean competitors, though it maintains a strong international presence. SC2's strategic complexity has made it a subject of extensive study, not only within the gaming community but also in academic fields such as artificial intelligence and cognitive science. Researchers have utilized SC2 as a testbed for developing and evaluating AI algorithms, particularly in areas of hierarchical planning, multi-agent coordination, and decision-making under uncertainty. The game's rich strategic landscape, combined with its clear victory conditions and quantifiable performance metrics, makes it an ideal environment for benchmarking AI systems against human expertise in complex, real-time domains. Moreover, SC2 has been used to study human cognition, particularly in areas of attention allocation, decision-making under time pressure, and skill acquisition. The game's replay system, which records every action taken by players, provides researchers with a wealth of data for analyzing human performance and strategy evolution over time.

## 3 Related Work

**StarCraft II Full Game AI**: StarCraft AI research, initially focused on StarCraft I with developments like BiCNet  for multi-agent coordination, has significantly advanced in the StarCraft II era. The release of PySC2  by DeepMind, coupled with Blizzard's game replays, propelled this research field. A key breakthrough was AlphaStar , which achieved Grandmaster level and defeated top players, demonstrating the potential of RL in complex environments.

Subsequent research expanded upon these foundations. Mini-AlphaStar  simplified input variables without compromising learning effectiveness. TG  and HierNet-SC2  explored efficient RL strategies, with the latter bypassing supervised pre-training. AlphaStar Unplugged  represented a leap in offline RL using human replays. TStarBotsX  and SCC  furthered federated learning approaches, achieving notable success against master and grandmaster level players.

Recent advancements include DI-star 2, which is accessible for home computer deployment, and ROA-Star , enhancing AlphaStar's training framework with goal-conditioned exploiters and refined opponent modeling techniques. ROA-Star's practical tests against professional players have shown impressive results, marking significant progress in real-time strategy AI.

**LLM Agent and Benchmark:** The introduction of GPT3.5  has significantly propelled the research on LLM agents forward. Projects such as React  and AutoGPT 3 laid the groundwork for more sophisticated implementations. Within the MineDojo environment 4, initiatives like GTM , Voyager , and others  have underscored LLM agents' adaptability to a variety of tasks and expansive open-world scenarios. Additionally, environments like TextWorld  and ALFWorld  enrich agent training by integrating text-based strategy and action execution across simulated and visual contexts, facilitating advanced generalization and adaptive learning. Additionally, optimization methods such as Reinforcement Learning with Human Feedback (RLHF) have also improved the performance of large language models.

Further advancements in multi-agent coordination and virtual social dynamics have been achieved through MetaGPT , Camel , and Generative Agents . Benchmarking platforms such as AGENTBENCH play a critical role in evaluating these developments, with AGENTBENCH examining decision-making in comprehensive, open-ended contexts.

In StarCraft II, despite the development of advanced AI agents, there remains a gap in evaluating LLMs, especially in real-time strategy and long-term planning. This led to the creation of TextStarCraft II, an environment tailored for testing LLMs in these specific aspects, filling a critical need for natural language interaction capabilities in AI research.

## 4 TextStarCraft II

TextStarCraft II provides a text-based interface for LLMs within the SC2 environment, utilizing the python-sc2 framework to translate complex gameplay into text. Key components include the **Observation-to-Text Adapter** and the **Text-to-Action Adapter**.TextStarCraft II stands out from other text-based environments like TextWorld and ALFWorld due to its more complex and dynamic gameplay. It requires agents to manage multiple aspects such as resource allocation, base building, and military strategy in real-time, challenging both their adaptability and decision-making. Additionally, the environment demands advanced language understanding to interpret and execute more open-ended and diverse commands, enhancing the need for sophisticated natural language processing capabilities. We will introduce the main components of TextStarCraft II below.

ObservationTextStarCraft II's observation space is designed to equip LLM agents with essential game insights, effectively navigating the fog of war in StarCraft II. The observations encompass six key categories:

* _Resources:_ Game resources and supply levels. ("Minerals: 500, Vespene Gas:200 ")
* _Units:_ Types and quantities of the players units.(e.g., "15 Zealots, 5 Carriers")
* _Buildings:_ Information on the players buildings.(e.g., "2 Gateway, 1 Nexus")
* _In-Process Activities:_ Ongoing construction and production data.(e.g., "Researching Wargate: 70
* _Enemy Status:_ Visible enemy units and buildings
* _Research Progress:_ Updates on the player's technological advancements.

This structured approach in the observation space enables LLM agents to efficiently process and utilize vital game data for strategic decision-making in TextStarCraft II.

ActionThere are mainly two types of actions: _Macro Actions_ and _Micro Actions_.

* _Macro Actions:_ Covering broad strategic decisions such as Training Units, Building Structures, Researching Technologies, and Other Strategic Maneuvers. Examples include:
* Building construction (e.g., "Build Cybernetics Core")
* Unit production (e.g., "Train 5 Probes (Protoss worker)")
* Research (e.g., "Research Blink Tech")
* Tactical maneuvers (e.g., "Scout with 1 probe", "Attack enemy base")
* _Micro Actions:_ Script-managed for precise placements and targeting, not directly controlled by the agent.

RewardThe reward function \(\) is crucial for aligning agent behavior with the game's objectives, assigning values of \(\{-1,0,1\}\) based on losing, drawing, or winning a match.

Game ModesTextStarCraft II enriches strategic gameplay with several modes. The **Built-in AI mode** features 10 difficulty levels, ranging from VeryEasy(LV1) to CheatInsane(LV10), and incorporates six strategic styles: RandomBuild, Rush, Timing, Power, Macro, and Air. The **Agent AI mode** enables competition against rule-based AIs and other LLM agents. The **Human mode** supports interactions with real-world players, enhancing gameplay realism.

Victory ConditionsIn our TextStarCraft2 environment, victory conditions align with those of a standard StarCraft II 1v1 game, akin to benchmarks such as AlphaStar, ROA-Star, or DI-Star. Victory requires the LLM agent to destroy all opponent structures, standardizing our benchmark with practices in various StarCraft II AI competitions.

## 5 Chain of Summarization

The Chain of Summarization (CoS) method, integral to the TextStarCraft II framework, draws inspiration from computer hardware's cache mechanisms and RL's frame skipping techniques. Serving as an enhancement to traditional Chains of Thought (CoT)  and as a standard plug-in, CoS refines strategic decision-making in StarCraft II through:

* **Information Compression:** It focuses on key data, reducing overload and sharpening strategic clarity.

* **Inference Acceleration:** This approach speeds up decision-making by providing a more comprehensive view of the game's state.
* **Global Understanding:** CoS equips LLMs with a deeper grasp of game strategies, leading to expert-level decisions.

As a versatile tool within the TextStarCraft II framework, CoS can operate both as a standalone plug-in, enhancing the environment's utility, and as a direct interface for user interaction with TextStarCraft II. This dual functionality not only showcases CoS as an exemplary model for engaging with our environment but also invites further development and customization by the community, broadening the scope of strategic AI research in gaming. CoS includes Single-Frame Summarization, Multi-Frame Summarization, and Action Extraction for the Action Queue.

Single-Frame SummarizationTo make TextStarCraft's raw observation data more comprehensible for LLMs, Single-frame Summarization compresses and extracts key information. This process, denoted as \(S_{}()\), transforms dense TextStarCraft II observations \(o\) into a condensed form \(\), described by:

\[=S_{}(o). \]

There are two approaches to this compression: a language model-based approach using few-shot learning for better alignment with game rules and a faster, rule-based approach for extraction and filtering. In our experiments, the rule-based approach is primarily used for quicker interactions.

Multi-Frame SummarizationTraditional methods query LLMs at each time step for decision-making (). However, this is inefficient for long-duration games like StarCraft II due to high computation costs and slower LLM inference. Our Multi-Frame Summarization method, inspired by caching in computer hardware and frame skipping in RL, addresses these issues. It synchronizes the quick pace of the game with LLM processing, ensuring real-time decision-making efficiency and improved comprehension in complex scenarios. Instead of constant LLM querying, we aggregate

Figure 1: **Interacting with LLM using the Enhanced Chain of Summarization (CoS) Method in TextStarCraft II. This streamlined LLM-driven gameplay. It begins with initialization, where initial game data is converted to text for processing. Next, Single-Frame and Multi-Frame Summarization refine and summarize observations into actionable insights using advanced LLM reasoning. In Directive Formulation and Action Scheduling, these insights are segmented into specific actions and queued for execution. The process concludes with Action Retrieval and Execution, where actions are implemented in the game. This cycle continually converts new data into text, enhancing LLM performance in the TextStarCraft II.**

condensed observation information \(\) for \(K\) steps into a period summary \(\), described by:

\[=S_{}(_{1},_{2},,_{K}). \]

This method enables comprehensive analysis and strategic planning through a series of steps, including situation overview, analysis, strategic planning, opponent strategy analysis, suggestion formulation, and decision-making. This process is formalized as \(v\), which is the output of \(CoT\) reasoning for summarization \(\), given by:

\[v=CoT(). \]

Action Extraction for Action QueueThe action queue forms a critical link between the Multi-Frame Summarization results, \(v\), and the TextStarCraft II environment, facilitating communication between the LLM and the game. Within \(v\), key components include analysis, suggestions, and decisions. To convert these into actionable steps, we employ regular expression matching and similarity searching in our action extractor, donated as \(Ex()\). This process populates the action queue with actions ready for execution in TextStarCraft II. From the output \(v\) of \(CoT\) reasoning, we utilize the action extractor to extract \(K\) actions, as formalized in:

\[(a_{1},a_{2},,a_{K})=Ex(v). \]

The CoS method optimizes decision-making in TextStarCraft II through a streamlined four-stage process: Initially, it sets the initial parameters and transforms the first game frame into text for subsequent analysis. Next, it distills key game observations to provide a concise snapshot of the current situation. Following this, the method translates these summaries into strategic action plans. Finally, it implements the planned actions within the game, thereby completing the decision cycle. This process is depicted in Figure 1. This approach, which updates actions every few frames, effectively manages the fast-paced dynamics of StarCraft II, thereby proving essential for real-time strategic gameplay. The pseudocode is as shown in Algorithm 1.

```
0: TextStarCraft II game environment \(env\), Chain length \(K\)
1: Set up the environment and obtain the initial raw observation \(o_{0}=env.reset()\)
2: Initialize the raw observation queue \(Q_{}\), action queue \(Q_{}\) and total reward \(R=0\)
3: Add \(K\) instances of raw observation \(o_{0}\) to the raw observation queue \(Q_{}\)
4:while\(env\) is not terminated do
5:if\((Q_{}) K\)then
6: Initialize Single-Frame Summarization Queue \(Q_{}\)\(\) CoS start
7:for raw observation \(o\) in \(Q_{}\)do
8: Perform Single-Frame Summarization \(=S_{}(o)\)
9: Add \(\) to the Single-Frame Summarization Queue \(Q_{}\)
10:endfor
11: Perform Multi-Frame summarization \(=S_{}(Q_{})\)
12: Apply Chain of Thought reasoning \(v=CoT()\)
13: extract \(K\) actions \((a_{1},a_{2},a_{K})=Ex(v)\)\(\) CoS end
14: Add the \(K\) actions \((a_{1},a_{2},a_{K})\) to the action queue \(Q_{}\)
15:endif
16: Obtain the next action \(a_{t}\) from the action queue \(Q_{}\)
17: Get the reward \(r_{t}\) and the next observation \(o_{t+1}\) from the environment \(r_{t},o_{t+1}=env.step(a_{t})\)
18: Add the raw observation \(o_{t+1}\) to the raw observation queue \(Q_{}\)
19:\(R R+r_{t}\)
20:endwhile
21:return total reward \(R\)
```

**Algorithm 1** Chain of Summarization Interaction in TextStarCraft II

## 6 Experiment

In our experiment, we detail the setup and key metrics (evaluation metrics detailed in Appendix A.2.) to evaluate macro-strategic decision-making in StarCraft II. We assess the Chain of Summarization's impact on LLM gameplay, compare various LLMs' performance, and evaluate their grasp of StarCraft II strategies. Our experiments also concludes with human-AI interaction tests.

### Performance Evaluation of Various LLMs

In this section, we assess the performance of closed-source LLMs , Llama2 70B , and fine-tuned open-source LLMs such as ChatGLM3 6B  and Qwen 1.8B  in the TextStarCraft II environment against the built-in AI at level 5. The experimental results are shown in Table 1, with the evaluation metrics detailed in Appendix A.2. We tested closed-source LLMs and the un-fine-tuned Llama2 70B using the standard CoS method. The closed-source models performed well, while Llama2 70B could not understand the task requirements and was unable to generate commands based on the given prompts.

Additionally, we fine-tuned open-source models using the entire dataset of GPT3.5-turbo-16k interaction logs with TextStarCraft II. Due to computational resource limitations, we removed the CoT component, keeping only the inputs and outputs. The results showed that all evaluated closed-source LLMs were capable of defeating the level 5 built-in AI. The fine-tuned open-source models, despite a loss in strategic diversity, still managed to overcome the level 5 AI, predominantly adopting a strategy focused on mass-producing stalkers.

Finally, we investigated the impact of data quality using the Average Population Utilization (APU) metric (detailed in Appendix A.2) to partition the dataset. The results (Table 2) demonstrated that fine-tuning on wins from the top 25% APU games yielded the highest win rate (54/100), while using the full dataset resulted in a lower win rate (28/100). This suggests that training data quality, especially the inclusion of high-performing games, significantly impacts fine-tuned model performance in TextStarCraft II.

### Assessing LLMs' Mastery of StarCraft II Concepts

We evaluated the understanding of StarCraft II by LLMs like GPT3.5 and GPT-4, focusing on their knowledge of build orders and game mechanics sourced from prominent StarCraft II forums. De

   Model & Method & Win Rate & PBR & RUR & APU & TR \\   \\  GPT3.5-Turbo-16k & Full CoS & 11/20 & 0.0781 & 7875 & 0.7608 & 0.4476 \\ GPT4-Tubor & Full CoS & 12/20 & 0.0337 & 8306 & 0.7194 & 0.3452 \\ Gemini-Pro & Full CoS & 5/20 & 0.0318 & 9284 & 0.6611 & 0.3571 \\ GLM4 & Full CoS & 4/20 & 0.0327 & 3131 & 0.6644 & 0.2904 \\ Claude2.1 & Full CoS & 3/20 & 0.0219 & 10867 & 0.6599 & 0.4312 \\   \\  Finetune-ChatGln3 6B & CoS w/o CoT & 3/20 & 0.0528 & 30356 & 0.6547 & 0.1714 \\ Finetune-Qwen 1.8b & CoS w/o CoT & 8/20 & 0.0384 & 12826 & 0.7506 & 0.2095 \\ Finetune-Qwen 7b & CoS w/o CoT & 9/20 & 0.0421 & 12276 & 0.7234 & 0.3214 \\ Finetune-Llama2 7b & CoS w/o CoT & 1/20 & 0.0469 & 12295 & 0.5752 & 0.0853 \\   

Table 1: Performance of LLMs(Vs LV5) in TextStarCraft II: Comparing models using either the full CoS or CoS without CoT. Evaluation metrics are elaborated in Appendix A.2.

   Dataset & Win Rate(Vs LV5) \\  Full Dataset (All Games) & 28/100 \\ Wins Dataset (All Wins) & 48/100 \\ Wins Dataset (Bottom 25\% APU, 75-100\%) & 29/100 \\ Wins Dataset (50-75\% APU) & 37/100 \\ Wins Dataset (25-50\% APU) & 39/100 \\ Wins Dataset (Top 25\% APU, 0-25\%) & 54/100 \\   

Table 2: Performance of models fine-tuned on various datasets in TextStarCraft II, showing win rates(Vs LV5). Datasets are differentiated based on whether they contain all games or only wins, and, for subsets of wins, based on the APU performance percentile.

spite a reasonable grasp of basic game dynamics, these models faced challenges with more complex elements like the tech tree and supply constraints.

Evaluation MethodologyTo gauge the depth of LLMs' StarCraft II knowledge, we tested models, including ChatGPT (GPT3.5), GPT-4, Claude2, and Bard, across five areas: Basic Knowledge, Racial Mechanics, Typical Strategies, Standard Build Orders, and Classic Strategies and Counterplays (detailed in Appendix G). We used a double-blind evaluation, with both Grandmaster-level human experts and GPT-4 assessing the responses to ensure unbiased scoring.

Evaluation ResultsBoth human experts and GPT-4 assessed the LLMs, illustrated in Figures 2 and Table 6, leading to the following insights: In the evaluation of LLMs on a set of complex questions, GPT-4 and ChatGPT demonstrated the best performance, with GPT-4 receiving high scores from both itself and human experts. Claude2 had mixed results, with GPT-4 rating it higher than human experts did. Bard struggled with the complex questions, receiving the lowest scores, especially on questions 3, 4, and 5. Overall, the LLMs were ranked as follows: GPT-4, ChatGPT, Claude2, and Bard. It is worth noting that GPT-4's self-assessment showed more variability compared to the more balanced evaluations provided by human experts.

### Human-AI Interaction

Following insights from Section 6.1, we evaluated the fine-tuned Qwen1.8B model's real-time interaction with human players on home PCs, requiring only 4GB of GPU memory. This model faced human players of varied skills from the Asian server, including a Grandmaster, a Gold-level player, and a novice, all playing as Zerg against the Protoss-configured LLM agent. Results, detailed in Table 3, show the LLM agent achieving competitive performance, on par with a Gold-level player. This

Figure 2: The result of Double-Blind Assessment.

   Human Player & Rank & MMR & Result \\  Player A & Pro Gamer & 5918 & 0/10 \\ Player B & GrandMaster & 5001 & 0/10 \\ Player C & Gold & 2556 & 5/10 \\ Player D & New Player & / & 10/10 \\   

Table 3: Match Results: Finetuned Qwen1.8B vs Human.

highlights the LLM's adaptability in strategic play and marks a significant step towards integrating AI into competitive gaming environments on accessible home computing setups.

## 7 Analysis

### Impact of Different Prompts

In evaluating the Chain of Summarization method using the GPT3.5-turbo-16k model in TextStarCraft II, we analyzed the effects of two different prompt types on LLM agent performance when playing as Protoss against Zerg. The results, detailed in Table 4, showed a notable improvement in performance with more complex prompts.

Simple Thought Chain:Using a basic prompt (see Prompt 1), the LLM agent could perform elementary operations like worker production, base establishment, and basic combat unit production. However, this approach was limited in developing advanced strategies, such as research upgrades or comprehensive game analysis, indicating a narrower strategic depth with simpler prompts.

Complex Thought Chain:A more intricate prompt (see Prompt 2) guided the LLM agent through a series of critical phases, including situation overview, analysis, strategic planning, and decision-making. This comprehensive approach enabled the agent to engage in advanced strategies like research upgrades, tech tree exploration, and complex military maneuvers. It proved particularly effective against higher difficulty levels (e.g., "Harder" lv 5 ), showcasing enhanced strategic capabilities.

Our analysis underscores the importance of complex prompts that replicate the thought processes of seasoned StarCraft II players. These advanced prompts are essential for LLMs to fully understand and strategically engage in the sophisticated aspects of the game.

### Policy Interpretability

Our analysis reveals a stark contrast in decision-making between AlphaStar and our LLM agent. While AlphaStar demonstrates superior micromanagement skills, it occasionally lacks rationality in its strategic choices. Conversely, the LLM agent consistently exhibits logical decision-making, as evidenced by its proactive anticipation of threats and strategic planning, detailed in Appendix E and Appendix F.

  
**Prompt** & **LV1** & **LV2** & **LV3** & **LV4** & **LV5** & **LV6** \\  Prompt 1 & 87.50 & 66.67 & 25.00 & 12.50 & 0.00 & 0.00 \\ Prompt 2 & 100.00 & 100.00 & 100.00 & 84.00 & 50.00 & 8.33 \\   

Table 4: LLM Agent Win Rates (%) vs. TextStarCraft II AI at Varied Difficulty Levels.

Figure 3: The ability to construct defensive structures and anticipate dangers in advance: Alphastar (left) vs. LLM agent (right).

Anticipating Threats:Figure 3 illustrates a stark contrast between AlphaStar and the LLM agent in their ability to anticipate and respond to potential threats. In Figure 3.a, AlphaStar overlooks the impending danger posed by enemy Oracles, failing to recognize the need for adequate defensive structures. This oversight leaves AlphaStar's base vulnerable to a devastating attack, highlighting its inability to proactively assess and mitigate potential risks. Conversely, in Figure 3.b, the LLM agent demonstrates remarkable foresight by analyzing the enemy's strategy and recommending appropriate defensive measures. The agent's log data at 05:00 game time reveals its ability to infer the enemy's Roach-centric strategy based on the presence of key structures such as the Roach Warren and Spawning Pool. Armed with this insight, the LLM agent provides a structured decision-making framework to address the identified threat:

* Enemy's Strategy: The enemy has established a Hatchery, Roach Warren, Extractor, and Spawning Pool. This indicates a potential strategy centered around Roach production and early aggression.
* Given the enemy's potential for early aggression with Roaches, we should prioritize defensive structures such as Photon Cannons and Shield Batteries. Additionally, consider scouting the enemy base to gather more information about their strategy.
* Decisions:0: <BUILD PHOTONCANNON> 1: <BUILD SHIELDBATTERY>

Flexible Unit Composition:AlphaStar often adheres to fixed unit compositions, leading to ineffective strategies in dynamic scenarios (Figure 13.a-d). In contrast, the LLM agent demonstrates adaptability in unit selection, effectively countering varied enemy tactics (Figure 13.e-h). For instance, in Figure 13.f, the Zerg (Built-in AI) employs a Hydra-Roach composition. The LLM agent, playing as Protoss in 09:00, recognizes the need to adapt its strategy based on the enemy's ground-based army. The agent's thought process, as shown in the log, reveals its ability to analyze the situation and make informed decisions:

* Enemy's Strategy: The enemy seems to be focusing on a ground-based army composition, consisting of Roaches, Swarm hosts and Hydralisks.
* Key Information: The most important aspect at this moment is our need to expand our unit composition and technology tree to counter the enemy's strategy effectively. We should prioritize unlocking advanced units and upgrades to gain an advantage. Consider researching spionic storm at the Templar Archives to deal with the enemy's ground units effectively.
* Decisions: 4: <RESEARCH PSISTORMTECH>

The LLM agent's superior performance in threat anticipation and unit composition adaptability stems from its structured, transparent decision-making process. By analyzing the situation, identifying key information, and making informed decisions based on the evolving game state, the LLM agent demonstrates a blend of human knowledge and logical reasoning. This approach enhances the interpretability of the agent's gameplay, facilitating better collaboration and strategic adaptation in complex scenarios, ultimately leading to more successful outcomes compared to AlphaStar's opaque strategies.

## 8 Discussion

TextStarCraft II enhances the use of LLMs for strategic decision-making in StarCraft II, showcasing their capabilities in adaptive strategy and crisis management. However, the frameworks reliance on rule-based scripts for micro policies, limitation to non-visual data, and a subset of the game's races may restrict the diversity and applicability of AI strategies. Additionally, resource limitations have constrained the performance capabilities of our system. Despite these challenges, TextStarCraft II establishes a new benchmark in RTS games, promoting deeper AI-human collaborative research. Future improvements will focus on integrating visual inputs, expanding race support, and optimizing resource usage to enhance strategic complexity and performance against established AI models.

Acknowledgements

This work was supported by the National Science and Technology Major Project 2022ZD0116404.

This work was supported by the StarCraft II AI5 and player communities. We extend our gratitude to Burny6 for providing detailed documentation and offering warm-hearted replies during the development process. We are grateful to several StarCraft II professional and amateur players who participated in our testing. Special thanks go to:

* Zhiyao(aka. HistTATP)Wang7 (Champion of China StarCraft II College League, Professional player of SSLT team)
* SLT.Rewhite8 (Champion of China StarCraft II College League, Professional player of SLT team)

* SLT.JoliwaLoves9 (Champion of China StarCraft II College League)

* SLT.stargazer10 (Professional player of SLT team)
* Hist.firmament11 (Champion of China StarCraft II College Tournament)

Their expertise and insights were invaluable to our research.