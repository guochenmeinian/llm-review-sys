# D-CPT Law: Domain-specific Continual Pre-Training Scaling Law for Large Language Models

Haoran Que\({}^{ 1}\), Jiaheng Liu\({}^{ 1,1}\), Ge Zhang\({}^{ 3,7}\), Chenchen Zhang\({}^{2}\), Xingwei Qu\({}^{4,7}\),

Yinghao Ma\({}^{5,7}\), Feiyu Duan\({}^{1}\), Zhiqi Bai\({}^{1}\), Jiakai Wang\({}^{2}\), Yuanxing Zhang\({}^{2}\), Xu Tan\({}^{7}\),

Jie Fu\({}^{6}\), Jiamang Wang\({}^{2}\), Lin Qu\({}^{2}\), Wenbo Su\({}^{1}\), Bo Zheng\({}^{1}\)

\({}^{1}\)Taobao & Tmall Group of Alibaba, \({}^{2}\)Alibaba Group, \({}^{3}\)University of Waterloo

\({}^{4}\)University of Manchester, \({}^{5}\)QMUL, \({}^{6}\)HKUST, \({}^{7}\)M-A-P

{quehaoran.qhr, ljh411989}@taobao.com, gezhang@umich.edu

First three authors contributed equally.Corresponding Author: Jiaheng Liu.

###### Abstract

Continual Pre-Training (CPT) on Large Language Models (LLMs) has been widely used to expand the model's fundamental understanding of specific downstream domains (e.g., math and code). For the CPT on domain-specific LLMs, one important question is how to choose the optimal **mixture ratio** between the general-corpus (e.g., Dolma, Slim-pajama) and the downstream domain-corpus. Existing methods usually adopt laborious human efforts by grid-searching on a set of mixture ratios, which require high GPU training consumption costs. Besides, we cannot guarantee the selected ratio is optimal for the specific domain. To address the limitations of existing methods, inspired by the Scaling Law for performance prediction, we propose to investigate the Scaling Law of the Domain-specific Continual Pre-Training (**D-CPT Law**) to decide the optimal mixture ratio with acceptable training costs for LLMs of different sizes. Specifically, by fitting the D-CPT Law, we can easily predict the general and downstream performance of arbitrary mixture ratios, model sizes, and dataset sizes using small-scale training costs on limited experiments. Moreover, we also extend our standard D-CPT Law on cross-domain settings and propose the **Cross-Domain D-CPT Law** to predict the D-CPT law of target domains, where very small training costs (about 1% of the normal training costs) are needed for the target domains. Comprehensive experimental results on six downstream domains demonstrate the effectiveness and generalizability of our proposed D-CPT Law and Cross-Domain D-CPT Law.

## 1 Introduction

**Continual Pre-Training (CPT)** is an essential part of training better Large Language models (LLMs). In this work, we mainly focus on Domain-specific CPT (**D-CPT**), which aims to enhance the fundamental understanding abilities of the specific downstream domains and has been widely used in existing works . In practice, for D-CPT, we usually need to collect high-quality domain-corpus to enhance the downstream performance and general-corpus to mitigate catastrophic forgetting on the general abilities . Therefore, how to determine the data composition or mixture ratio of the domain-corpus and general-corpus plays an important role in producing well-performed domain-specific LLMs. Besides, grid-searching on the mixture ratios requires heavy GPU consumption costs, and we cannot always obtain the optimal ratio under limited GPU usage. Recently, Scaling Law has been widely used for performance prediction ,which can be used to find the optimal dataset size and model size under the given GPU consumption costs. Therefore, _for D-CPT, can we find the optimal mixture ratio in the training corpus using the Scaling Law to enhance the performance of domain-specific tasks?_

To address the above question, in this work, we investigate the Scaling Law of D-CPT and propose the **D-CPT Law** to find the optimal mixture ratio with limited training costs for LLMs with different sizes. Specifically, inspired by the robust predictive ability of Scaling Law across various scales, we first perform experiments under diverse mixture ratios and several relatively small model and data scales. Following the Chinchilla Scaling Law, we then introduce the mixture ratio \(r\) into the D-CPT Law, where the parameterization is defined as follows:

\[L(N,D,r)=E+}+}{D^{}}+},r^{}=r+, \]

where \(\) is used to guarantee the stability of \(L\) when \(r\) near zero. Based on Equation 1, for a model with model size \(N\), dataset volume \(D\) and mixture ratio \(r\), we can accurately predict the validation loss \(L\). Note that when \(r\) denotes the domain-corpus mixture ratio \(r_{d}\), \(L\) means domain-corpus validation loss \(L_{d}\). Similarly, general-corpus validation loss \(L_{g}\) also follows the law relationship with the general-corpus mixture ratio \(r_{g}\). To illustrate our D-CPT Law clearly, as shown in Figure 1, we take the code domain as an example and provide the fitting results on the general and domain-specific settings, where we validate the fitting accuracy on different mixture ratios under a model with different dataset sizes \(D\). Our main contributions are summarized as follows:

(1). To show the effectiveness and generalizability of D-CPT Law, we perform extensive experiments using model sizes from 0.5B to 4B parameters, dataset sizes from 0.1B to 26B tokens, and mixture ratios from 0 to 1. The experiments show that the D-CPT law exhibits a high fitting accuracy with Huber loss  lower than 0.02 and \(R^{2}\) greater than 0.97. Besides, experiments on generalizability show that D-CPT Law not only inherits model size and dataset size generalizability following previous Scaling Law, but also precisely predicts performance for different mixture ratios.

(2). Despite the effectiveness in an in-domain setting, where we fit the D-CPT Law based on data points from one downstream domain, we also apply our D-CPT Law in the cross-domain setting, which denotes that we use the data points from multiple domains to predict the performance of unseen domains. Specifically, we first introduce the **Domain-specific Learnable Coefficient (DLC)** to denote the domain-specific parameter of each domain and integrate the DLC into the D-CPT Law. We name this new law as **Cross-Domain D-CPT Law**. In this way, if we can obtain the DLC of a new domain, we can easily derive the D-CPT Law for this new domain. In our experiments, we fit the Cross-Domain D-CPT Law using data points from 4 domains and apply the Cross-Domain

Figure 1: Illustration of the performance of **D-CPT Law**. (_Left_): The curves show the relationship between \(L_{g}\) and \(r_{g}\) under different dataset sizes \(D\) for Qwen1.5-1.8B model. CPT data are a mixture of **code-corpus** and general-corpus. Here, \(L_{g}\) represents the loss on the general-corpus validation set, while \(r_{g}\) indicates the percentage of the general corpus in the training data. The dashed curves denote the curves predicted by D-CPT Law, circular markers and star markers are fitting data points and unseen validation points, respectively. (_Right_): These curves are the corresponding results between the code-corpus validation loss \(L_{d}\) and the percentage of the code-corpus data \(r_{d}\).

D-CPT Law to predict the remaining 2 domains. The results show that DLC can represent the specific information for each downstream domain well, enabling efficient and effective fitting performance for the cross-domain setting and significantly reducing training costs for new domains.

(3). To show the real-world usages of the D-CPT Law, we apply our D-CPT Law on three important scenarios: optimal mixture on the trade-off between general and domain-specific abilities, optimal mixture for limited domain-specific data, and resource allocation setting in Figure 2 (Details are provided in Section 4.3).

## 2 Background

Following previous work , we categorize the objectives of Scaling Law as _Allocation_ and _Return_. Specifically, (1). _Allocation_: What is the optimal allocation of model size \(N\) and dataset size \(D\) given a fixed compute budget? (2). _Return_: What is the expected return on incremental resources?

The first objective on _Allocation_ is as follows:

\[*{argmin}_{N,D}L(N,D)(N,D)=C. \]

In Equation 2, given a fixed compute budget \(C\), the objective is to find the optimal model size \(N\) and dataset size \(D\) that minimize the loss. The second objective on _Return_ fundamentally depends on the generalizability of Scaling Law to accurately predict beyond the fitting data points.

Chinchilla Scaling LawHoffmann et al. propose a parameterization as follows:

\[L=E+}+}, \]

where {_E_,\(A\),_B_,_\(\),\(\)} are fitting parameters (See Appendix D for more details).

## 3 Methods

In Figure 2, D-CPT Law aims to investigate the behaviors of the Domain-specific Continual Pre-Training scenario with respect to different mixture ratios, and the objective of the D-CPT Law is to analyze an appropriate parameterization of law that represents the relationship of validation loss \(L\)

Figure 2: Illustration of **D-CPT Law** and **Cross-Domain CPT-Law** pipeline. (_Upper_): In D-CPT Law, we first collect domain-corpus and general-corpus, and conduct experiments under a small-scale experimental setup to gather empirical data points to fit the D-CPT Law. After that, we can predict the modelâ€™s performance in large-scale experimental settings. (_Lower_): In Cross-Domain CPT-Law, for an unseen downstream domain, like Physics, we can calculate its Domain-specific Learnable Coefficient value and incorporate it into the fitted Cross-Domain D-CPT Law to derive the D-CPT Law for this new domain. Based on the D-CPT Law, we introduce three application scenarios: optimal mixture on the trade-off between general and domain-specific abilities, optimal mixture for limited domain-specific data, and resource allocation in Section 4.3.

with respect to model size \(N\), dataset size \(D\), and mixture ratio \(r\). In this section, we first discuss the D-CPT Law in the in-domain setting (Section 3.1), where the fitting and testing data points are from the same domains. Then, we propose to adapt D-CPT Law to the cross-domain setting (Section 3.2), where the fitting and testing data points are from multiple domains, and introduce the Cross-Domain D-CPT Law, where a new term Domain-specific Learnable Coefficient (DLC) is used.

### D-CPT Law

As the training data includes a mixture of general-corpus and domain-corpus, we introduce two mixture ratios (i.e., general-corpus mixture ratio \(r_{g}\) and domain-corpus mixture ratio \(r_{d}\)). Correspondingly, we define two validation losses (i.e., general-corpus validation loss \(L_{g}\) and domain-corpus validation loss \(L_{d}\)). Therefore, we can derive two D-CPT Laws (i.e., \(L_{g}(N,D,r_{g})\) and \(L_{d}(N,D,r_{d})\)). For convenience, we directly use \(r\) and \(L(N,D,r)\) as default notations for D-CPT Law. Besides, in Appendix D, Chinchilla Scaling Law provides greater interpretability and clarity when compared to OpenAI Scaling Law. Thus, we choose Chinchilla Scaling Law as the foundational parameterization for D-CPT Law. In addition, since Scaling Law aims to fit data points, their parametric forms should be intrinsically related to the observed trends in the data points. Based on previous works and data trends with varying \(N\), \(D\) and \(r\), we have summarized 4 essential requirements for D-CPT Law:

* **Adaptability**: D-CPT Law is valid for values of \(r\) between 0 and 1.
* **Explicit trends**: Based on the results across varying values of \(N\), \(D\), and \(r\), we observed the following explicit trends of data points: \[<0,<0, <0.\] (4) The first two trends are consistent with the previous Chinchilla Scaling Law, and the third trend also has an intuitive explanation. A larger \(r\) indicates a higher proportion of valid corpus in the training corpus, leading to a lower \(L\). Details are provided in Appendix E.1.
* **Implicit trends**: We further discover inherent connections between \(r\), \(D\) and \(L\) as follows: \[L}{ D r}<0.\] (5) For detailed explanations, please refer to the Appendix E.2.
* **Consistency**: When \(r\) is fixed, the D-CPT Law is supposed to transform into the Chinchilla Scaling Law. In this way, D-CPT Law can inherit the excellent features of Chinchilla Scaling Law and address the issues on resource allocation discussed in Section 2.

To satisfy these requirements, we have compared multiple parameterizations in Section 4.2 and eventually, we propose the parameterization as follows:

\[L(N,D,r)=E+}+}{D^{}}+^{}}, r^{}=r+. \]

In Equation 6, {\(E\), \(A\), \(B\), \(C\), \(\), \(\), \(\), \(\),\(\)} are the fitting parameters and we use L-BFGS to fit the D-CPT Law due to its suitability for large-scale optimizations. As shown in Section 4.2, the Equation 6 can accurately fit the trends of data points at any scale and demonstrates strong performance in both effectiveness and generalizability. Besides, it effectively meets the aforementioned 4 requirements. (Please see Appendix E.3 for more details on the mathematical derivation.)

### Cross-Domain D-CPT Law

Apart from the in-domain setting for D-CPT Law, we also investigate the cross-domain setting and extend the D-CPT Law to the Cross-Domain D-CPT Law, which aims to reduce the training costs of the D-CPT Law significantly. Specifically, although the D-CPT Law collects data points using small LLMs, the GPU resource and time costs are still relatively substantial, which limits the applications of the Scaling Law. Therefore, in our Cross-Domain D-CPT Law, we first define the Domain-specific Learnable Coefficient (DLC) \(K\) for each domain, which measures the learnabilityfor a specific domain1 (See Section 4.4 for more details.). Then, we incorporate the \(K\) into the D-CPT Law and obtain the Cross-Domain D-CPT Law, which is defined as follows:

\[L(N,D,r,K)=E+}+}{D^{}}+}+}, r^{}=r+. \]

In Equation 7, \(\{E,A,B,C,F,,,,,,\}\) are fitting parameters. Thus, for an unseen domain, we only need to calculate the DLC with modest costs, which substantially increases the domain generalizability of D-CPT Law. Besides, Cross-Domain D-CPT Law has the following features:

* **Uniformity**: Once we calculate the \(K\) value of an unseen domain, we can convert Cross-domain D-CPT Law into normal D-CPT Law as follows: \[L(K=K_{0})=E_{0}+}+}{D^{}}+ {C}{r^{}}, E_{0}=E+^{}},  r^{}=r+.\] Therefore, the Cross-Domain D-CPT Law inherits all features of the D-CPT Law,
* **Monotonicity**: \(K\) denotes the learnability of a specific domain, which aligns with the intuition that a more learnable domain yields lower validation loss. Meanwhile, the Cross-domain D-CPT Law confirms a monotonic decrease with respect to \(K\), i.e., \[=-}<0.\] (8) After confirming the parameterization of Cross-domain D-CPT Law, it is essential to identify a representation for \(K\) that accurately quantifies a domain's learnability. The representation of \(K\) is supposed to be **accessible, distinct and robust**. Specifically, first, "Accessible" denotes that it is easy to obtain for an unseen domain with low costs. Second, "Distinct" indicates that \(K\) values must exhibit significant variance across domains to ensure fitting accuracy and maintain clear distinctions between domains. Third, "Robust" means that the representation of \(K\) enhances the effectiveness and generalization ability of Cross-domain D-CPT Law. In Section 4.4, we compare several variants of the representations of \(K\), and the final representation is determined as follows: \[K=}{k_{1}}+w_{2} k_{2},\] (9) where \(w_{1}\) and \(w_{2}\) are fitting parameters, \(k_{1}\) represents the initial validation loss for an unseen domain, and \(k_{2}\) denotes the rate of decline in validation loss.

## 4 Experiments

### Experimental Setup

Data SetupTo verify the effectiveness and generalizability of D-CPT Law and Cross-Domain D-CPT Law, we have prepared the 6 different downstream domains, which include Code , Math , Law , Chemistry , Music  and Medical . For general corpus, we use Dolma . All the tokens of these training datasets are sufficient, so the experiments are not performed under a data-constrained setting. Besides, we build a high-quality and held-out validation set for each domain. (See Appendix F.1 for more details.)

Model SetupWe use the Qwen-1.5 series due to its robust performance in both English and Chinese . Furthermore, Qwen-1.5 has multiple open-sourced and well-performed pre-training base models. Specifically, we select Qwen-1.5-0.5B, Qwen-1.5-1.8B, and Qwen-1.5-4B as our base models to perform the continual pre-training for multiple downstream domains.

[MISSING_PAGE_FAIL:6]

**Dataset Size Generalizability**: Our main experiments cover dataset sizes from 0.1B to 26B tokens, and we also utilize a 3-fold cross-validation approach. The data points are uniformly divided into three segments, with 2/3 used for fitting the model and the remaining 1/3 for testing. In Table 3, we report the average results across domains, and observe that \(L_{3}\) shows notably enhanced dataset size generalizability.

**Mixture ratio Generalizability**: We apply the k-fold cross-validation method across various parameterizations. Specifically, we select 7 out of 9 mixture ratios for fitting and the remaining for testing, resulting in 36 experiments per domain. For simplicity, we show average results across domains in Table 4, and observe that that \(L_{3}\) still shows significantly better performance on mixture ratio generalizability. Besides, in Figure 1, we observe that our D-CPT Law has well-performed generalizability on unseen mixture ratios.

### Usages of D-CPT Law

**Usage 1: Trade-off between general and domain-specific abilities** For D-CPT, training data is a mixture of general and domain-specific data, where \(r_{g}\) and \(r_{d}\) denote the corresponding proportions, respectively. In D-CPT Law, when \(r_{g}\) increases, the \(L_{g}\) will decrease and \(L_{d}\) will increase, indicating a trade-off between the general and domain-specific abilities of LLM. Fortunately, D-CPT Law can identify the optimal mixture ratio under any trade-off scenario. Specifically, we assume that an LLM with parameter size \(N_{0}\), it exhibits general-corpus validation loss of \(L_{g}^{0}\) and domain-corpus validation loss of \(L_{d}^{0}\) before continual pretraining. After mixing training data size of \(D_{0}\) with a ratio \(r_{d}\) of domain-specific data and \(1-r_{d}\) of general data, we obtain general-corpus validation loss \(L_{g}\) and domain-corpus validation loss \(L_{d}\) after D-CPT. Then, we can identify the optimal mixture ratio while limiting the decline in the model's general abilities within a threshold \(T\) as follows:

    &  & \)} \\   & G & D & G & D \\  \(L_{1}\) & 0.0055 & 0.0172 & 0.9521 & 0.9366 \\ \(L_{2}\) & **0.0047** & 0.0171 & 0.9663 & 0.9420 \\ \(L_{3}\) & 0.0049 & **0.0166** & **0.9711** & **0.9516** \\ \(L_{4}\) & 0.0054 & 0.0168 & 0.9680 & 0.9\[*{argmin}_{r_{d}}L_{d}(N=N_{0},D=D_{0},r_{d})-L_{g}^{0}}{L_{g}^{0}}<T, \]

where \(T\) is the threshold based on practical need. In Appendix G.1, given a fixed \(T\), a unique optimal solution \(r_{d}\) is obtained. To validate it in real scenarios, by applying D-CPT Law, we calculate the optimal domain-corpus mixture ratio \(r_{d}=0.924\) given a dataset size \(D_{0}=10B\), model size \(N_{0}=1.8B\), \(T=3\%\), domain-corpus is chemistry, and initial general validation loss value of \(L_{g}^{0}=2.8602\). Table 5 presents the results of real general-corpus validation loss and domain-corpus validation loss with respect to different domain-corpus mixture ratios, we find that the real value exactly matches the predicted values(\(L_{g\_pred}=2.9458\) and \(L_{d\_pred}=1.7284\)), domain-corpus mixture ratio exceeding 0.924 leads to a general validation loss that surpasses the 3% threshold of \(L_{g}^{0}\).

Usage 2: Optimal mixture on limited domain-specific dataGiven that domain-corpus is typically limited relative to the abundance of the general corpus, we study how to determine the optimal mixture ratio when domain-corpus is limited and general-corpus is sufficient. Specifically, for an LLM with parameter \(N_{0}\) and limited domain-corpus \(D_{d}^{0}\), we aim to minimize the domain-corpus validation loss \(L_{d}\) by selecting the optimal domain-corpus mixture ratio \(r_{d}\) as follows:

\[*{argmin}_{r_{d}}L_{d}(N=N_{0},D,r_{d}) D_{ d}=D_{d}^{0}, \]

In Equation 11, we can reach the minimum value within the \(0<r_{d}<1\) discussed in Appendix G.2. To validate it in real scenarios, we conducted experiments within the music domain by setting the model parameters \(N_{0}=1.8B\) and the domain-specific dataset size \(D_{d}=5B\). As we have data points at a large scale, we fit D-CPT Law using only data where \(D_{d}<2B\) to align with the use case scenarios. After using the D-CPT Law, we find that the optimal domain-corpus mixture ratio is 0.732. Table 6 shows the results of real domain-corpus validation loss of the music domain. We observe that \(r_{d}=0.732\) yields the lowest domain-corpus validation loss. Moreover, our predicted domain-corpus validation loss is 0.7328 when \(r_{d}=0.732\), which is close to the real value (0.7309).

Usage 3: Resource allocationD-CPT Law is consistent with Chinchilla Scaling Law under the fixed mixture ratio to address resource allocation. Specifically, how to find the optimal values of \(N\) and \(D\) given a fixed compute budget. Detailed results are shown in Appendix G.3.

### Cross-Domain D-CPT Law

In Section 3.2, we have mentioned that the learnability of a specific domain is measured by DLC (i.e., \(K\)). For Cross-Domain D-CPT Law, \(K\) must satisfy 3 core requirements: accessible, distinct, and robust. Based on these requirements, 4 different representations of \(K\) are defined as follows:

\[K_{1}=}{k_{1}}, K_{2}=w_{2} k_{2}, K_{3}=}{k_{1}}+w_{2} k_{2}, K_{4}=}{k_{1}}+w_{2} k_{2} +}{k_{3}}, \]

where {\(w_{1},w_{2},w_{3}\)} are fitting parameters. In the approximate Taylor expansion for the validation loss function near the initial points, {\(k_{1}\),\(k_{2}\),\(k_{3}\)} represent the first three coefficients. Due to the discrete nature of data points in practical scenarios, {\(k_{1}\),\(k_{2}\),\(k_{3}\)} are approximated using the variants of validation loss. Specifically, \(k_{1}\) denotes the precise value of the validation loss at the initial point, \(k_{2}\) represents the difference in validation loss close to the initial points, and \(k_{3}\) is an approximation of the second derivative of the validation loss near the initial points, details are provided in Appendix H. To compare these four representations of \(K\), we have conducted experiments in both effectiveness and generalizability aspects.

   \(r_{d}\) & 0.9 & 0.91 & 0.92 & 0.924 & 0.93 & 0.94 & 1.0 \\  \(L_{g}\) & 2.9052 & 2.9193 & 2.9376 & 2.9445 & 2.9644 & 2.9848 & 3.4667 \\ \(L_{d}\) & 1.7321 & 1.7312 & 1.7311 & 1.7291 & 1.

**Effectiveness** We utilize data points from all 6 domains for fitting and then evaluate their performance using \(R^{2}\) and Huber loss. In Table 7, we find that 4 representations of \(K\) yield comparable results in the general domain. However, \(K_{1}\) and \(K_{2}\) demonstrate a noticeable decline in domain-specific aspects. Although \(K_{4}\) slightly outperforms \(K_{3}\) in domain-specific aspects, it requires a larger number of fitting parameters. Therefore, considering the balance between fitting efficiency, fitting performance, and accessibility, We consider \(K_{3}\) to be the optimal representation. To further visualize it, Figure 5 illustrates the predicted curves in comparison to the real curves under various settings.

**Generalizability** When \(K\) is identified, Cross-Domain D-CPT Law can be transformed into D-CPT Law, we believe that the former inherits the latter's generalizability in terms of model size, dataset size, and mixture ratio. In this part, we will specifically focus on discussing the domain generalizability of Cross-Domain D-CPT Law. To evaluate domain generalizability, we use data points from 4 out of 6 domains for fitting and assign the remaining 2 domains for testing. For simplicity, we only show the averaged results across 15 combinations in Table 8. Among these 4 representations of \(K\), \(K_{3}\) exhibits the superior performance, further proving its strength.

## 5 Related Works

Scaling LawMany studies [26; 31; 27; 7; 16; 58] show a power-law relationship between performance and the increases in both the number of parameters and the size of the training data [31; 27; 18], which are crucial for large language models (LLMs [45; 51; 29; 3; 55; 35; 59; 36; 60]) and provide a predictive structure for determining the most efficient setups for expanded models using the insights gained from smaller models . Moreover, the extension of scaling laws to autoregressive generative models widens their relevance to encompass tasks beyond text [18; 25; 17]. Recently, Muennighoff et al.  studied the Scaling Law of data-constrained settings by using the full pre-training dataset across multiple epochs. Ye et al.  investigate the data-mixing scaling law for the general LLMs to improve the pretraining efficiency.

    &  & \)} &  & Accessibility \\   & G & D & G & D & G/D & G/D \\  \(K_{1}\) & 0.0675 & 0.9224 & 0.9853 & 0.9462 & + & + \\ \(K_{2}\) & 0.0612 & 1.1924 & 0.9875 & 0.8526 & + & ++ \\ \(K_{3}\) & **0.0566** & 0.3682 & **0.9889** & 0.9918 & ++ & ++ \\ \(K_{Domain-specific Continual Pre-TrainingDomain-specific Continual Pre-Training aims to continually pre-train LLMs to adapt them to new domains [23; 11; 22; 30; 19]. For example, Gururangan et al.  introduces a growing mixture of expert architecture for domain-adaptive continual pre-training. Cossu et al.  show that continually pre-trained models (RoBERTa  and BERT ) are robust against catastrophic forgetting on downstream tasks. However, the above works only investigate small encoder-decoder models on limited tasks. Recently, Gupta et al.  study different warm-up strategies for continual pertraining for better results.

## 6 Conclusion

In this work, we have investigated the Scaling Law of Domain-specific Continual Pre-Training (D-CPT), which provides a significant step forward in the optimization of training LLMs for specific downstream domains. By developing and validating the D-CPT Law, we can easily predict the optimal mixture ratio of general and domain-specific corpora, greatly reducing the previously necessary but costly grid-searching efforts. Besides, we also adapt our D-CPT Law to the cross-domain setting and introduce the Cross-Domain D-CPT Law to further reduce the efforts of fitting the D-CPT Law of new domains. Moreover, we discuss the three practical usages of our D-CPT Law. Finally, we believe our D-CPT Law is an initial investigation into quantitative prediction methods for the domain-specific continual pre-training. With its increasing focus on data engineering, we hope our exploration facilitates further quantitative studies and theoretical analyses in this research area.