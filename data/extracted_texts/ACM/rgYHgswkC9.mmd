# High-Frequency-aware Hierarchical Contrastive Selective Coding for Representation Learning on Text-attributed Graphs

High-Frequency-aware Hierarchical Contrastive Selective Coding for Representation Learning on Text-attributed Graphs

Anonymous Author(s)

###### Abstract.

We investigate node representation learning on text-attributed graphs (TAGs), where nodes are associated with text information. Although recent studies on graph neural networks (GNNs) and pretrained language models (PLMs) have exhibited their power in encoding network and text signals, respectively, less attention has been paid to delicately coupling these two types of models on TAGs. Specifically, existing GNNs rarely model text in each node in a contextualized way; existing PLMs can hardly be applied to characterize graph structures due to their sequence architecture. To address these challenges, we propose HASH-CODE, a High-frequency Aware Spectral Hierarchical Contrastive Selective Cding method that integrates GNNs and PLMs into a unified model. Different from previous "cascaded architectures" that directly add GNN layers upon a PLM, our HASH-CODE relies on five self-supervised optimization objectives to facilitate thorough mutual enhancement between network and text signals in diverse granularities. Moreover, we show that existing contrastive objective learns the low-frequency component of the augmentation graph and propose a high-frequency component (HFC)-aware contrastive learning objective that makes the learned embeddings more distinctive. Extensive experiments on six real-world benchmarks substantiate the efficacy of our proposed approach. In addition, theoretical analysis and item embedding visualization provide insights into our model interoperability.

Text Attributed Graph, Graph Neural Networks, Transformer, Contrastive Learning +
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

## 1. Introduction

Graphs are pervasive in the real world, and it is common for nodes within these graphs to be enriched with textual attributes, thereby giving rise to text-attributed graphs (TAGs) (Zhu et al., 2017). For instance, academic graphs (Zhu et al., 2017) incorporate papers replete with their titles and abstracts, whereas social media networks (Zhu et al., 2017) encompass tweets accompanied by their textual content. Consequently, the pursuit of learning within the realm of TAGs has assumed significant prominence as a research topic spanning various domains, _e.g._, network analysis (Zhu et al., 2017), recommender systems (Zhu et al., 2017), and anomaly detection (Zhu et al., 2018).

In essence, graph topology and node attributes comprise two integral components of TAGs. Consequently, the crux of representation learning on TAGs lies in the amalgamation of graph topology and node attributes. Previous works mainly adopt a cascaded architecture (Zhu et al., 2017; Zhu et al., 2017; Zhu et al., 2017; Zhu et al., 2017) (Figure 1(a)), which entails encoding the textual attributes of each node with Pre-trained Language Models (PLMs), subsequently utilizing the PLM embeddings as features to train a Graph Neural Network (GNN) for message propagation (Chen et al., 2016; Zhu et al., 2017; Zhu et al., 2017). However, as the modeling of node attributes and graph topology are segregated, this learning paradigm harbors conspicuous limitations. Firstly, the link connecting two nodes is not utilized when generating their text representations. In fact, linked nodes can benefit each other regarding text semantics understanding. For example, given a paper on "LDA" and its citation nodes which are related to topic modeling, the "LDA" can be more likely interpreted as "Latent Dirichlet Allocation" rather than "Linear Discriminant Analysis". In addition, this paradigm may yield textual embeddings that are not pertinent to downstream tasks, thereby impleting the model's ability to learn node representations suitable for such tasks. Moreover, given that the formation of the graph's topological structure is intrinsically driven by the node attribute (Zhu et al., 2017), this paradigm may adversely affect the comprehension of the graph topology.

Fortunately, recent efforts have been undertaken (Chen et al., 2016; Zhu et al., 2017; Zhu et al., 2017; Zhu et al., 2017) to co-train GNNs and LMs within a unified learning framework. For example, GraphFormers (Zhu et al., 2017) introduces GNN-nested transformers, facilitating the joint encoding of text and node features. Heterformer (Zhu et al., 2017) alternately stacks the graph aggregation module and a transformer-based text encoding module into a cohesive model to capture network heterogeneity. Despite the demonstrated efficacy of existing methods, they are encumbered by two primary drawbacks that may undermine the quality of representation learning. Firstly, these methods typically employ supervised training, necessitating a substantial volume of labeled data. However, in numerous scientific domains, labeled data are scarce and expensive to obtain (Zhu et al., 2017; Zhu et al., 2017). Secondly, these methods rely exclusively on limited optimization objectives to learn the entire model. When GNNs and LMs are jointly trained, the associated parameters are also learned through the constrained optimization objectives. It has been observed that such an optimization approach fails to capture the fine-grained correlations between textual features and graphic patterns (Zhu et al., 2017; Zhu et al., 2017). Consequently, the importance of learning graph

Figure 1. (a) An illustration of GNN-cascaded transformer. (b) An illustration of our proposed contrastive learning-empowered GNN-nested transformer. The red and green twines denote the original graph signals and the mixed LFC and HFC signals from the spectral perspective.

representations in an unsupervised or self-supervised manner is becoming increasingly paramount.

In order to tackle the aforementioned challenges, we draw inspiration from the concept of contrastive learning to enhance representation learning on TAGs. Contrastive learning (Chen et al., 2018; He et al., 2017; He et al., 2018; He et al., 2019) refines representations by drawing positive pairs closer while maintaining a distance between negative pairs. As data sparsity and limited supervision signals constitute the two primary learning obstacles associated with existing co-training methods, contrastive learning appears to offer a promising solution to both issues: it capitalizes on intrinsic data correlations to devise auxiliary training objectives and bolsters data representations with an abundance of self-supervised signals.

In practice, representation learning on TAGs with contrastive learning is non-trivial, primarily encountering the following three challenges: (1) _How to devise a learning framework that capitalizes on the distinctive data properties of TAGs?_ The contextual information within TAGs is manifested in a multitude of forms or varying intrinsic characteristics, such as tokens, nodes, or sub-graphs, which inherently exhibit complex hierarchical structures. Moreover, these hierarchies are interdependent and exert influence upon one another. How to capitalize these unique properties of TAGs remains an open question. (2) _How to design effective contrastive tasks?_ To obtain an effective node embedding that fully encapsulates the semantics, relying solely on the hierarchical topological views of TAGs remains insufficient. Within TAGs, graph topological views and textual semantic views possess the capacity to mutually reinforce one another, indicating the importance of exploring the cross-view contrastive mechanism. Moreover, the hierarchies in TAGs can offer valuable guidance in selecting positive pairs with analogous semantics and negative pairs with divergent semantics, an aspect that has received limited attention in existing research (He et al., 2019; He et al., 2019). (3) _How to learn distinctive representations?_ In developing the contrastive learning framework, we draw inspiration from the recently proposed spectral contrastive learning method (He et al., 2019), which outperforms several contrastive baselines with solid theoretical guarantees. However, we demonstrate that, from a spectral perspective, the spectral contrastive loss primarily learns the low-frequency component (LFC) of the graph, significantly attenuating the effects of high-frequency components (HFC). Recent studies suggest that the LFC does not necessarily encompass the most vital information (Chen et al., 2018; He et al., 2019), and would ultimately contribute to the over-smoothing problem (Chen et al., 2018; He et al., 2019; He et al., 2019), causing node representations to converge to similar values and impeding their differentiation. Consequently, more explorations are needed to determine how to incorporate the HFC to learn more discriminative embeddings.

To this end, we present a novel High-frequency Aware Spectral **Hierarchical Contrastive Selective Cding** framework (**HASH-CODE**) to enhance TAG representation learning. Building upon a GNN and Transformer architecture (Wang et al., 2019; He et al., 2019), we propose to jointly train the GNN and Transformer with self-supervised signals (Figure 1(b) depicts this architecture). The primary innovation lies in the contrastive joint-training stage. Specifically, we devise five self-supervised optimization objectives to capture hierarchical intrinsic data correlations within TAGs. These optimization objectives are developed within a unified framework of contrastive learning. Moreover, we propose a loss that can be succinctly expressed as a contrastive learning objective, accompanied by robust theoretical guarantees. Minimizing this objective results in more distinctive embeddings that strike a balance between LFC and HFC. Consequently, the proposed method is capable of characterizing correlations across varying levels of granularity or between different forms in a general manner.

Our main contributions are summarized as follows:

* We propose five self-supervised optimization objectives to maximize the mutual information of context information in different forms or granularities.
* We systematically examine the fundamental limitations of spectral contrastive loss from the perspective of spectral domain. We prove that it learns the LFC and propose an HFC-aware contrastive learning objective that makes the learned embeddings more discriminative.
* Extensive experiments conducted on three million-scale text-attributed graph datasets demonstrate the effectiveness of our proposed approach.

## 2. Related Work

### Representation Learning on TAGs

Representation learning on TAGs constitutes a significant research area across multiple domains, including natural language processing (Wang et al., 2019; Wang et al., 2019), information retrieval (Wang et al., 2019; Wang et al., 2019), and graph learning (Wang et al., 2019; He et al., 2019). In order to attain high-quality representations for TAGs, it is imperative to concurrently harness techniques from both natural language understanding and graph representation. The recent advancements in pretrained language models (PLM) and graph neural networks (GNN) have catalyzed the progression of pertinent methodologies.

**Seperated Training.** A number of recent efforts strive to amalgamate GNNs and LMs, thereby capitalizing on the strengths inherent in both models. The majority of prior investigations on TAGs employ a 'cascaded architecture' (He et al., 2019; He et al., 2019; He et al., 2019; He et al., 2019), in which the text information of each node is initially encoded through transformers, followed by the aggregation of node representations via GNNs. Nevertheless, these PLM embeddings remain non-trainable during the GNN training phase. Consequently, the model performance is adversely impacted by the semantic modeling process, which bears no relevance to the task and topology at hand.

**Co-training.** In an attempt to surmount these challenges, concerted efforts have been directed towards the co-training of GNNs and PLMs within a unified learning framework. GraphFormers (Wang et al., 2019) presents GNN-nested transformers, facilitating the concurrent encoding of text and node features. Heterformer (He et al., 2019) alternates between stacking the graph aggregation module and a transformer-based text encoding module within a unified model, thereby capturing network heterogeneity. However, these approaches solely depend on a single optimization objective for learning the entire model, which considerably constrains their capacity to discern the fine-grained correlations between textual and graphical patterns.

### Contrastive Learning

**Empirical Works on Contrastive learning.** Contrastive methods (Chen et al., 2018; He et al., 2019; He et al., 2019) derive representations from disparate views or augmentations of inputs and minimize the InfoNCE loss (Wang et al., 2019), wherein two views of identical data are drawn together, while views from distinct data are repelled. The acquired representation can be utilized to address a wide array of downstream tasks with exceptional performance. In the context of node representation learning on graphs, DGI (Henderson et al., 2017) constructs local patches and global summaries as positive pairs. GMI (Yang et al., 2017) is designed to establish a contrast between the central node and its local patch, derived from both node features and topological structure. MVGRL (Henderson et al., 2017) employs contrast across views and explores composition between varying views.

**Theoretical works on Contrastive Learning.** The exceptional performance exhibited by contrastive learning has spurred a series of theoretical investigations into the contrastive loss. The majority of these studies treat the model class as a black box, with notable exceptions being the work of (Yang et al., 2017), which scrutinizes the learned representation with linear models, and the research conducted by (Zhou et al., 2017) and (Wang et al., 2017), which examine the training dynamics of contrastive learning for linear and 2-layer ReLU networks. Most relevant to our research is the study by (Wang et al., 2017), which adopts a spectral graph perspective to analyze contrastive learning methods and introduces the spectral contrastive loss. We ascertain that the spectral contrastive loss solely learns the LFC of the graph.

Different from the existing works, our research represents the first attempt to contemplate the correlations inherent within the contextual information as self-supervised signals in TAGs. We endeavor to maximize the mutual information among the views of the token, node, and subgraph, which encompass varying levels of granularity within the contextual information. Our HFC-aware loss facilitates the learning of more discriminative data representations, thereby enhancing the performance of downstream tasks.

## 3. Preliminaries

In this section, we first give the definition of the text-attributed graphs (TAGs) and formulate the node representation learning problem on TAGs. Then, we introduce our proposed HFC-aware spectral contrastive loss.

### Definition (Text-attributed Graphs)

A text-attributed graph is defined as \(=(,)\), where \(=\{v_{1},,v_{N}\}\) and \(\) denote the set of nodes and edges, respectively. Let \(^{N N}\) be the adjacency matrix of the graph such that \(A_{i,j}=1\) if \(v_{j}(n)\), otherwise \(A_{i,j}=0\). Here \((.)\) denotes the one-hop neighbor set of a node. Besides, each node \(v_{i}\) is associated with text information.

### Problem Statement

Given a textual attibuted graph \(=(,)\), the task is to build a model \(f_{}:^{K}\) with parameters \(\) to learn the node embedding matrix \(F^{N K}\), taking network structures and text semantics into consideration, where \(K\) denotes the number of feature channels. The learned embedding matrix \(F\) can be further utilized in downstream tasks, _e.g._, link prediction, node classification, _etc._

### HFC-aware Spectral Contrastive Loss

An important technique in our approach is the high-frequency aware spectral contrastive loss. It is developed based on the analysis of the conventional spectral contrastive loss (Henderson et al., 2017). Given a node \(v\), the conventional spectral contrastive loss is defined as:

\[_{Spectral}(v,v^{+},v^{-})& =-2_{v,v^{+}}[f_{}(v)^{T}f_{}(v^{+})]\\ &+_{v,v^{-}}[(f_{}(v)^{T}f_{}(v^{-}))^{2}], \]

where \((v,v^{+})\) is a pair of positive views of node \(v\), \((v,v^{-})\) is a pair of negative views, and \(f_{}\) is a parameterized function from the node to \(^{K}\). Minimizing \(_{Spectral}\) is equivalent to spectral clustering on the population view graph (Henderson et al., 2017), where the top smallest eigenvectors of the Laplacian matrix are preserved as the columns of the final embedding matrix \(F\).

In Appendix A.1, we demonstrate that, from a spectral perspective, \(_{Spectral}\) primarily learns the low-frequency component (LFC) of the graph, significantly attenuating the effects of high-frequency components (HFC). Recent studies suggest that the LFC does not necessarily encompass the most vital information (Bengio et al., 2017; Chen et al., 2018), and would ultimately contribute to the over-smoothing problem (Chen et al., 2018; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018).

As an alternative of such low-pass filter, to introduce HFC, we propose our HFC-aware spectral contrastive loss as follows:

\[_{HFC}(v,v^{+},v^{-})&=-2 _{v,v^{+}}[f_{}(v)^{T}f_{}(v^{+})]\\ &+_{v,v^{-}}[(f_{}(v)^{T}f_{}(v^{-}))^{2}], \]

where \(\) is used to control the rate of HFC within the graph.

Upon initial examination, one might observe that our \(_{HFC}\) formulation closely aligns with \(_{Spectral}\). Remarkably, the primary distinction lies in the introduction of the parameter \(\). However, this is not a mere trivial addition; it emerges from intricate mathematical deliberation and is surprisingly consistent with \(_{Spectral}\) that offers a nuanced control of the HFC rate within the graph. Minimizing our \(_{HFC}\) results in more distinctive embeddings that strike a balance between LFC and HFC. Please kindly refer to Appendix A.1 for detailed discussions and proof.

## 4. Methodology

### Overview

Existing studies (Yang et al., 2017; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) mainly emphasize the effect of sequential and graphic characteristics using the supervised optimization objective alone. Inspired by recent progress with contrastive learning (Chen et al., 2018; Wang et al., 2018), we take a different perspective to characterize the data correlations by contrasting different views of the raw data.

The basic idea of our approach is to incorporate several elaborately designed self-supervised learning objectives for enhancing the original GNN and PLM. To develop such objectives, we leverage effective correlation signals reflected in the intrinsic characteristics of the input. As shown in Figure 2, for our task, we consider the information in different levels of granularity, including token, node and sub-graph, which are considered as different views of the input. By capturing the multi-view correlation, we unify these self-supervised learning objectives with the typical joint learning training scheme in language modeling and graph mining (Wang et al., 2018).

### Hierarchical Contrastive Learning with TAGs

TAGs naturally possess 3 levels in the hierarchy: token-level, node-level and subgraph-level. Based on the above GNN and PLM model,we further incorporate additional self-supervised signals with contrastive learning to enhance the representations of input data. We adopt a joint-training way to construct different loss functions based on the multi-view correlation.

#### 4.2.1. Intra-hierarchy contrastive learning

**Modeling Token-level Correlations.** We first begin with modeling the bidirectional information in the token sequence. Inspired by the masked language model like BERT (Devlin et al., 2019), we propose to use the contrastive learning framework to design a task that maximizes the mutual information between the masked sequence representation and its contextual representation vector. Specifically, for a node \(v\), given its textual attribute sequence \(x_{v}=\{x_{v1,1},x_{v2,2},...,x_{vn,T}\}\), we consider \(x_{vk,l;j}\) and \(_{k,l;j}\) as a positive pair, where \(x_{vk,l;j}\) is an \(n\)-grams spanning from \(i\) to \(j\) and \(_{k,l;j}\) is the corresponding sequence masked at position \(i\) to \(j\). We may omit the subscript \(v\) for notation simplification when it is not important to differentiate the affiliation between node and textual sequence.

For a specific query \(n\)-gram \(x_{i,j}\), instead of contrasting it indiscriminately with all negative candidates \(\) in a batch (Krizhevsky et al., 2009), we select truly negative samples for contrasting based on the supervision signals provided by the hierarchical structure in TAGs, as shown in Figure 3. Intuitively, we would like to eliminate those candidates sharing highly similar semantics with the query, while keeping the ones that are less semantically relevant to the query. To achieve this goal, we first define a similarity measure between an \(n\)-gram and a node. Inspired by (Krizhevsky et al., 2009), for a node \(v\), we define the semantic similarity between \(n\)-gram's hidden state \(h_{v_{k,j}}\) and this node's hidden state \(h_{v}\) using a node-specific dot product:

\[s(h_{x_{i,j}},h_{v})=} h_{v}}{_{h_{v}}},_{h_{v}}= } h_{v}}|h_{x_{i}}-h_{v}||_{2}}{|H_{v}||log(|H_{v}| +)},\]

where \(h_{x_{i}}\) is the hidden representation of the token \(x_{i}\), \(H_{v}\) consists of the hidden representations of the tokens assigned to node \(v\), and \(\) is a smooth parameter balancing the scale of temperature \(_{h_{v}}\) among different nodes.

On such a basis, we conduct negative sampling selection considering both the token and node hierarchies. Given the query \(n\)-gram \(x_{i,j}\), we denote its corresponding node \(v\)'s representation as \(h_{v}\). For a negative candidate, we are more likely to select it if its similarity with \(h_{v}\) is less prominent compared with other negative candidates' similarities with \(h_{v}\). Based on such an intuition, the least dissimilar negative samples \(N_{select}(h_{x_{i,j}})\) are produced for the specific query.

By using these refined negative samples, we define the objective function of token-level contrastive (TC) loss as below:

\[_{TC}=_{m=1}^{M}_{HFC}(x_{m,l;j},_{m,l;j},N_{select}(h_{x_{m,l;j}})), \]

where \(M\) is the size of the representation set and \(_{HFC}\) is our proposed HFC-aware spectral contrastive loss.

**Modeling Node-level Correlations.** Investigating the cross-view contrastive mechanism is especially important for node representation learning (Zhu et al., 2019). As mentioned before, nodes in TAGs possess

Figure 4. Modeling node-level correlations.

Figure 3. Token-level contrastive selective coding.

Figure 2. The overall architecture of HASH-CODE. With GraphFormers as our base model, we incorporate five self-supervised learning objectives based on the HFC-aware contrastive loss to capture the text-graph correlations in different granularities. Spectral contrastive loss learns the LFC while our HFC-aware loss achieves the balance between HFC and LFC.

textual attributes that can indicate semantic relationships in the network and serve as complementary to structural patterns. As shown in Figure 4, given a node \(v\), we treat its textual attribute sequence \(x_{v}\) and its direct connected neighbors \(u,\ u N_{v}\) as two different views.

The negative selective encoding strategy used in token-level correlation modeling may select those easy negative samples that contribute less and less during the training process. Inspired by (Song et al., 2018),we propose to adversarially generate the negative samples \(\) in the deep-level contrastive learning process. Specifically, we adopt ProGCL (Song et al., 2018) method to reweight the negative node samples and performing mixup operation (Song et al., 2018) to generate hard negative samples \(\). Therefore, we minimize the following Node-level Contrastive (NC) loss:

\[_{NC}=_{m=1}^{M}_{HFC}(x_{m,0},N_{m,0}, }) \]

Modeling Subgraph-level Correlations. Having modeled correlations between a node's local neighborhood and its textual features, we further consider modeling the correlations between subgraphs to cover both of the local and high-order structures of the nodes. Intuitively, nodes and their regional neighborhoods are more correlated while long-distance nodes hardly influence them. Therefore, local communities may form with the graph. This assumption is more reasonable as the size of graphs increases. Therefore, we sample a series of subgraphs including regional neighborhoods from the original graph as training data.

The most critical issue now is to sample a context subgraph, which can provide sufficient structure information for learning a high-quality representation for the central node. Here we follow the subgraph sampling based on personalized PageRank algorithm (PPR) (Kipf and Welling, 2016) as introduced in (Kipf and Welling, 2016; Leskovec et al., 2017). Considering the importance of different neighbors varies, for a specific node \(i\), the subgraph sampler \(S\) first measures the importance scores of other neighbor nodes by PPR. Given the relational information between all nodes in the form of an adjacency matrix, \(A^{N N}\), the importance score matrix \(S\) can be denoted as

\[S=(I-(1-)),\]

where \(I\) is the identity matrix and \(\) is a parameter that is always set as 0.15. \(=AD^{-1}\) denotes the column-normalized adjacency matrix, where \(D\) denotes the corresponding diagonal matrix with \(D(i,i)=_{j}A(i,j)\) on its diagonal. \(S(i,:)\) is the importance scores vector for node \(i\), indicating its correlation with other nodes.

It is noted that the importance score matrix S can be precomputed before model training starts. And we implement node-wise PPR to calculate importance scores to reduce computation memory, which makes our method more suitable to work on large-scale graphs.

For a specific node \(i\), the subgraph sampler \(S\) chooses top-k important neighbors to constitute the subgraph \(G_{i}\). The index of chosen nodes can be denoted as

\[idx=top\_rank(S(i,:),k),\]

where \(top\_rank\) is the function that returns the indices of the top k values and k denotes the size of context graphs.

The subgraph sampler \(S\) will process the original graph with the node index to obtain the context subgraph \(G_{i}\) of node \(i\). Its adjacency matrix \(A_{i}\) and feature matrix \(X_{i}\) are as follows:

\[A_{i}=A_{idx,idx}X_{i}=X_{idx,:},\]

where \(idx\) is an indexing operation. \(A_{idx,idx}\) is the rowwise and col-wise indexed adjacency matrix corresponding to the induced subgraph. \(X_{idx,:}\) is the row-wise indexed feature matrix.

**Encoding subgraph.** Given the context subgraph \(G_{i}=(A_{i},X_{i})\) of a central node \(i\), the encoder \(:^{N N}^{N F}^{N F}\) encodes it to obtain the latent representations matrix \(H_{i}\) denoted as

\[H_{i}=(A_{i},X_{i})\]

The subgraph-level representation \(s_{i}\) is summarized by a readout function, \(:^{N F}^{F}\):

\[s_{i}=(H_{i})\]

So far, the representations of subgraphs have been produced. As shown in Figure 5, to model the correlations in subgraph level, we treat two subgraphs \(s_{i}\) and \(_{i}\) that sampled from the node \(h_{i}\) and its most important neighbor node \(_{i}\) respectively as positive pairs while the rest of subgraphs \(\) are negative pairs. We minimize the following Subgraph-level Contrastive (SC) loss:

\[_{SC}=_{m=1}^{M}_{HFC}(s_{m},_{m },_{m}) \]

#### 4.2.2. Inter-hierarchy contrastive learning.

Having modeled the intra-hierarchy correlations, we further consider modeling the intra-hierarchy correlations as different hierarchies are dependent and will influence each other.

**Modeling Token-Node Correlations.** To model the token-node correlation, our intuition is to train the language model to refine the understanding of the text by GNN produced embeddings. Therefore, the language model is pushed to learn fine-grained task-aware context information. Specifically, given a sequence \(x_{v}=(x_{v,1},x_{v,2},...,x_{v,T})\), we consider \(x_{v}\) and its corresponding node representation \(h_{v}\) as a positive pair. On the other hand, for a set of node representations, we employ a function, \(\), to corrupt them to generate negative samples, denoted as

\[\{},},...,}\}=\{h_{ 1},h_{2},...,h_{M}\},\]

where \(M\) is the size of the representation set. \(\) is the random shuffle function in our experiment. This corruption strategy determines the differentiation of tokens with different context nodes, which is

Figure 5. Modeling subgraph-level correlations.

[MISSING_PAGE_FAIL:6]

baselines, the one-tower textual model (BERT) outperforms the two-tower model (Twin-BERT) as it can incorporate the information from both sides, while two-tower models can only exploit the data from a single side. However, one-tower structure has to compute the similarity between a search query and each ad one-by-one, which is not suitable for low-latency online scenario. In general, vanilla textual/graph models perform worse than GNN-cascaded transformers, which demonstrates the importance of encoding both text and network signals in text-attributed graphs.

As for GNN-cascaded transformers, Bert+GAT performs better than Bert+MeanSAGE and Bert+MaxSAGE on Product, DBLP and Wiki datasets, because the multi-head self-attention mechanism has a stronger capacity to model attributes. However, the performance of GAT is worse than that of MeanSAGE on Beauty, Sports and Toys datasets. A potential reason is that the multi-head self-attention may incorporate more noise from the attributes since they are keywords extracted from the reviews on Amazon Reviews. In general, GNN-cascaded transformers perform worse than co-training-based methods, which may be due to the node textual features are pre-existed and fixed in the training phase, leading to the limited expression capacity. AdsGNN consistently outperforms TextGNN on all datasets. This is because compared with TextGNN, the node-level aggregation model AdsGNN can capture the different roles of queries and keys, demonstrating that the tightly-coupled structure is more powerful than the loosely-coupled framework in deeply fusing the graph and textual information.

For GNN-nested transformers, Heterformer yields a larger performance improvement over Graphformers when network is more dense (_i.e._, Product and DBLP vs. Amazon datasets). By comparing our approach with all the baselines, it is clear to see that our HASH-CODE performs consistently better than them with notable advantages on six datasets. Particularly, it achieves over \(2\%\)\(\)\(4\) relative improvements over the most competitive baselines (underlined) on each of the experimental datasets. Different from these baselines, we adopt the contrastive learning to enhance the representations of the attribute, and nodes for the representation learning task, which incorporates five pre-training objectives to model multiple data correlations by our proposed HFC-aware contrastive objectives. This result also shows that the contrastive learning approach is effective to improve the performance of the co-training architecture for representation learning.

### Ablation Study (RQ2)

Our proposed HASH-CODE designs five pre-training objectives based on the HFC-aware contrastive objective. In this section, we conduct the ablation study on Product and DBLP datasets to analyze

   Datasets & Metric & MeanSAGE & GAT & Bert & Twin-Bert & Bert-MeansSAGE & Bert+MaxSAGE & Bert+GAT & TextGNN & AdaGNN & GraphFormers & Heterformer & HASH-CODE & Improv. & 788 \\   & Pq1 & 0.4071 & 0.6409 & 0.6563 & 0.6492 & 0.7240 & 0.7250 & 0.7270 & 0.7431 & 0.7623 & 0.7786 & 0.7820 & **0.7967\({}^{*}\)** & 1.888 & **799** \\  & NDCG & 0.784 & 0.7401 & 0.7911 & 0.7907 & 0.8337 & 0.8371 & 0.8378 & 0.8494 & 0.8605 & 0.8793 & 0.8861 & **0.9039\({}^{*}\)** & 2.01\% & 760 \\  & MRR & 0.6619 & 0.6627 & 0.7344 & 0.7285 & 0.7871 & 0.7832 & 0.7880 & 0.8107 & 0.8361 & 0.8430 & 0.8492 & **0.8706\({}^{*}\)** & 2.52\% & 762 \\  & Pq1 & 0.1376 & 0.1367 & 0.1528 & 0.1492 & 0.1593 & 0.1586 & 0.1544 & 0.1625 & 0.1669 & 0.1774 & 0.1739 & **0.1862\({}^{*}\)** & 4.96\% & 762 \\  & NDCG & 0.2417 & 0.2469 & 0.2702 & 0.2683 & 0.2741 & 0.2756 & 0.2726 & 0.2863 & 0.2891 & 0.2919 & 0.2911 & **0.3061\({}^{*}\)** & 4.86\% & 764 \\  & MRR & 0.2558 & 0.2549 & 0.2680 & 0.2638 & 0.2712 & 0.2759 & 0.2720 & 0.2802 & 0.2821 & 0.2893 & 0.2841 & **0.3057\({}^{*}\)** & 5.67\% & 765 \\  & Pq1 & 0.102 & 0.1088 & 0.1275 & 0.1237 & 0.1330 & 0.1311 & 0.1302 & 0.1421 & 0.1466 & 0.1548 & 0.1534 & **0.1623\({}^{*}\)** & 4.84\% & 764 \\  & Sports & NDCG & 0.2091 & 0.2116 & 0.2375 & 0.2297 & 0.2482 & 0.2478 & 0.2419 & 0.2537 & 0.2582 & 0.2674 & 0.2692 & **0.2775\({}^{*}\)** & 3.08\% & 767 \\  & MRR & 0.271 & 0.2168 & 0.2319 & 0.2296 & 0.2434 & 0.2471 & 0.2997 & 0.2612 & 0.2653 & 0.2672 & 0.2640 & **0.2754\({}^{*}\)** & 2.80\% & 768 \\  & & & & & & & & & & & & & & & & & & \\   & Pq1 & 0.1342 & 0.1330 & 0.1498 & 0.1427 & 0.1520 & 0.1536 & 0.1514 & 0.1658 & 0.1674 & 0.1703 & 0.1685 & **0.1767\({}^{*}\)** & 3.76\% & 770 \\  & NDCG & 0.2015 & 0.2028 & 0.2249 & 0.2206 & 0.2651 & 0.2486 & 0.2413 & 0.2692 & 0.2734 & 0.2852 & 0.2823 & **0.2946\({}^{*}\)** & 3.04\% & 771 \\  & MRR & 0.2173 & 0.2149 & 0.2311 & 0.2276 & 0.2509 & 0.2527 & 0.2476 & 0.2648 & 0.2715 & 0.2803 & 0.2778 & **0.2919\({}^{*}\)** & 4.14\% & 772 \\   & Pq1 & 0.4083 & 0.4931 & 0.5673 & 0.5500 & 0.6533 & 0.6596 & 0.6634 & 0.6913 & 0.7102 & 0.7267 & 0.7288 & **0.7446\({}^{*}\)** & 2.17\% & 773 \\  & NDCG & 0.6997 & 0.6981 & 0.7484 & 0.7417 & 0.8004 & 0.8059 & 0.8006 & 0.8331 & 0.8507 & 0.8565 & 0.8576 & **0.8823\({}^{*}\)** & 2.88\% & 774 \\  & MRR & 0.6314 & 0.6309 & 0.6777 & 0.6640 & 0.7266 & 0.7067 & 0.7306 & 0.7770 & 0.7805 & 0.8133 & 0.8148 & **0.8482\({}^{*}\)** & 3.44\% & 778 \\   & Pq1 & 0.2850 & 0.2826 & 0.3036 & 0.3015 & 0.3306 & 0.3264 & 0.3412 & 0.3679 & 0.3820 & 0.3922 & 0.3947 & **0.4104\({}^{*}\)** & 3.85\% & 778 \\  & MRR & 0.5189 & 0.5387 & 0.5699 & 0.5613 & 0.5730 & 0.5737 & 0.6071 & 0.6098 & 0.6155 & 0.6230 & 0.6402\({}^{*}\)** & 2.71\% & 778 \\  & MRR & 0.4411 & 0.4436 & 0.4712 & 0.4602 & 0.4980 & 0.4970 & 0.5022 & 0.5097 & 0.5134 & 0.5220 & 0.5216 & **0.5356\({}^{*}\)** & 2.61\% & 778 \\   

Table 2. Experiment results of link prediction. The results of the best performing baseline are underlined. The numbers in bold indicate statistically significant improvement (p <.01) by the pairwise t-test comparisons over the other baselines.

Figure 6. Ablation studies of different components on DBLP and Products datasets.

the contribution of each objective. We evaluate the performance of several HASH-CODE variants: (a) No-TT removes the \(_{TC}\); (b) No-TN removes the \(_{TNC}\); (c) No-TN removes the \(_{NC}\); (d) No-NS removes the \(_{NC}\); (e) No-SS removes the \(_{SC}\); (f) No-HFC replaces the HFC-aware loss with spectral contrastive loss. The results from GraphFormers are also provided for comparison. P@1 and NDCG@10 are adopted for this evaluation.

From Figure 6, we can observe that removing any contrastive learning objective would lead to the performance decrease, indicating all the objectives are useful to capture the correlations in varying levels of granularity in TAGs. Besides, the importance of these objectives is varying on different datasets. Overall, \(_{TC}\) is more important than others. Removing it yields a larger drop of performance on all datasets, indicating that natural language understanding is more important on these datasets. In addition, No-HFC performs worse than the other variants, indicating the importance of learning more discriminative embeddings with HFC.

It is clearly seen that all model variants are better than GraphFormers, which is trained only with link predication loss.

### Efficiency Analysis (RQ3)

We compare the time efficiency between HASH-CODE, and GNN-nested Transformers (GraphFormers). The evaluation is conducted utilizing an Nvidia 3090 GPU. We follow the same setting with (Wang et al., 2019), where each mini-batch contains 32 encoding instances; each instance contains one center and \(\)N neighbour nodes; the token length of each node is 16. We report the average time and memory (GPU RAM) costs per mini-batch in Table 3.

We find that the time and memory costs associated with these methods exhibit a linear escalation in tandem with the augmentation of neighboring elements. Meanwhile, the overall time and memory costs of HASH-CODE exhibit a remarkable proximity to GraphFormers, especially when the number of neighbor nodes is small. In light of the above observations, it is reasonable to deduce that HASH-CODE exhibits superior accuracy while concurrently maintaining comparable levels of efficiency and scalability when juxtaposed with GNN-nested transformers.

### In-depth Analysis (RQ4 & 5)

We continue to investigate several properties of the models in the next couple sections. To save space, we will mainly present the results here and save the details to the appendix:

* In Appendix D.1, we simulate the data sparsity scenarios by using different proportions of the full dataset. We find that HASH-CODE is consistently better than baselines in all cases, especially in an extreme sparsity level (20%). This observation implies that HASH-CODE is able to make better use of the data with the contrastive learning method, which alleviates the influence of data sparsity problem for representation learning to some extent.
* In Appendix D.2, we investigate the influence of the number of training epochs on our performance. The results show that our model benefits mostly from the first 20 training epochs. And after that, the performance improves slightly. Based on this observation, we can conclude that the correlations among different views on TAGs can be well-captured by our contrastive learning approach through training within a small number of epochs. So that the enhanced data representations can improve the performance of the downstream tasks.
* In Appendix D.3, we analyze the impact of neighbourhood size with a fraction of neighbour nodes randomly sampled for each center node. We can observe that with the increasing number of neighbour nodes, both HASH-CODE and GraphFormers achieve higher prediction accuracies. However, the marginal gain is varnishing, as the relative improvement becomes smaller when more neighbours are included. In all the testing cases, HASH-CODE maintains consistent advantages over GraphFormers, which demonstrates the effectiveness of our proposed method.
* In Appendix D.4, we visualize the input node embeddings for different target classes by t-SNE (Maaten and Hinton, 2008) to intuitively study the impact of our HFC-loss. We find that our \(_{HFC}\) helps the model learn more discriminative node embeddings compared with \(_{Spectral}\).

## 6. Conclusion

In this paper, we introduce the problem of node representation learning on TAGs and propose HASH-CODE, a hierarchical contrastive learning architecture to address the problem. Different from previous "cascaded architectures", HASH-CODE utilizes five self-supervised optimization objectives to facilitate thorough mutual enhancement between network and text signals in different granularities. We also propose a HFC-aware spectral contrastive loss to learn more discriminative node embeddings. Experimental results on various graph mining tasks, including link prediction and node classification demonstrate the superiority of HASH-CODE. Moreover, the proposed framework can serve as a building block with different task-specific inductive biases. It would be interesting to see its future applications on real-world TAGs such as recommendation, abuse detection and tweet-based network analysis.

   \#N & 3 & 5 & 10 & 20 & 50 & 100 & 200 \\  Time: GraphFormers & 63.95ms & 97.19ms & 170.16ms & 306.12ms & 714.32ms & 1411.09ms & 2801.67ms \\ Time: HASH-CODE & 67.68ms & 105.35ms & 180.03ms & 324.11ms & 754.97ms & 1573.29ms & 2962.86ms \\  Mem: GraphFormers & 1.33GiB & 1.39GiB & 1.55GiB & 1.83GiB & 2.70GiB & 4.28GiB & 7.33GiB \\ Mem: HASH-CODE & 1.33GiB & 1.39GiB & 1.55GiB & 1.84GiB & 2.72GiB & 4.43GiB & 7.72GiB \\   

Table 3. Time and memory costs per mini-batch for GraphFormers and HASH-CODE, with neighbour size increased from 3 to 200. HASH-CODE achieve similar efficiency and scalability as GraphFormers.

[MISSING_PAGE_EMPTY:9]

Improving tectual network learning with variational homophilic embeddings. Advances in Neural Information Processing Systems 23 (2019).
* Wang et al. (2021) Xiaoxu Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanti Li, and Jian Tang. 2021. KeplER: A unified model for knowledge embedding and pre-trained language representation. Transactions of the Association for Computational Linguistics 9 (2021), 176-194.
* Wang et al. (2019) Xiao Wang, Houyei, J. Chuan Shi, Bai Wang, Yanming Ye, Peng Cui, and Philip S Yu. 2019. Heterogeneous graph attention network. In _The wild world wide conference_. 2022-2032.
* Wang et al. (2021) Xiao Wang, Nian Liu, Hui Han, and Chuan Shi. 2021. Self-supervised heterogeneous graph neural network with co-contrasting learning. In _Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining_. 1726-1736.
* Wen and Li (2021) Zizin Wen and Yuanhui Li. 2021. Toward understanding the feature learning process of self-supervised contrastive learning. In _International Conference on Machine Learning_. PMLR, 11115-11122.
* Wu et al. (2016) Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Kelvin, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. _arXiv preprint arXiv:1608.04146_ (2016).
* Xie et al. (2012) Jun Xia, Liang Wu, Gu Wang, Jines Chen, and FangXie. 2012. Proget: Rethinking hand negative mining in graph contrastive learning. In _International Conference on Machine Learning_. PMLR, 23432-23436.
* Xu et al. (2021) Minghao Xu, Hang Wang, Bingbing Ni, Hongyu Guo, and Jian Tang. 2021. Self-supervised graph-level representation learning with local and global structure. In _International Conference on Machine Learning_. PMLR, 11588-11588.
* Xu et al. (2019) Zeman Xu, Qinliang Su, Xiaojun Quan, and Weijia Zhang. 2019. A deep neural information fusion architecture for textual network embeddings. _arXiv preprint arXiv:1908.11057_ (2019).
* Yang et al. (2015) Cheng Yang, Zhiyuan Liu, Del Zhu, Maosong Sun, and Edward Y Chung. 2015. Network representation learning with rich text information. In _IJCAI_, Vol. 2015. 2111-2117.
* Tang et al. (2021) Junnan Tang, Zheng Liu, Shihao Xiao, Chaozhou Li, Defu Lian, Sanjay Agrawal, Anui Singh, Guangzhong Sun, and Xing Xie. 2021. GraphFormers: CNN-nested transformers for representation learning on textual graph. _Advances in Neural Information Processing Systems 3_ (2021), 2898-28180.
* Tsamagas et al. (2022) Michihiro Tsamagas, Jure Leskovec, and Perey Liang. 2022. LinkBERT: Pretraining Language Models with Document Links. In _Proceedings of the 6th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_. 8003-8016.
* Yaunaga et al. (2017) Michihiro Yaunaga, Rui Zhang, Khaitibi Mechi, Ayush Pareek, Krishnan Srinivasan, and Dragomir Radev. 2017. Graph-based neural multi-document summarization. _arXiv preprint arXiv:1706.06081_ (2017).
* Ying et al. (2018) Kue Ying, Ruining He, Kaifeng Chen, Peng, Hosomhotakai, William L Hamilton, and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale recommender systems. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_. 974-983.
* Zhang et al. (2019) Chuxu Zhang, Donglin Song, Chao Huang, Ananthram Swami, and Nitch V Chandra. 2019. Heterogeneous graph neural network. In _Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining_. 733-903.
* Zhang et al. (2019) Chuxu Zhang, Ananthram Swami, and Nitesh V Chawik. 2019. Shre: Representa-let learning for semantic-associated heterogeneous networks. In _Proceedings of the twelfth ACM international conference on web and data mining_. 600-608.
* Zhang et al. (2016) Chao Zhang, Guangyu Zhou, Yuan Yuan, Hongliu Zhang, Yu Zheng, Zhao Kaplan, Shaween Wang, and Jiawei Han. 2016. Geobarts: Real-time local event detection in geo-tagged tweet streams. In _Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval_. 513-522.
* Zhang et al. (2017) Hongyi Zhang, Moustafa Cisz, Yuan N Dauphin, and David Lopez-Paz. 2017. Inicpp: Beyond empirical risk minimization. _arXiv preprint arXiv:1701.00412_ (2017).
* Zhang et al. (2020) Jiawei Zhang, Haapeng Zhang, Congying Xia, and Li Sun. 2020. Graph-bert: Only attention is needed for learning graph representations. _arXiv preprint arXiv:2001.05140_ (2020).
* Zhao et al. (2022) Jun Zhao, Meng Qi, Chaozhou Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and Jian Tang. 2022. Learning on Large-scale Text-attributed Graphs via Variational Inference. _arXiv preprint arXiv:2210.07097_ (2022).
* Zhou et al. (2020) Kun Zhou, Hui Wang, Wayne Xin Zhao, Yitao Zhu, Siyuan Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. 53-rec self-supervised learning for sequential recommendation with mutual information maximization. In _Proceedings of the 29th ACM international conference on information & knowledge management_. 1939-1902.
* Zhu et al. (2021) Jason Zhu, Yanling Cui, Yunming Liu, Hao Sun, Xue Li, Markus Pelger, Tianqi Yang, Liangliuie Zhang, Rudei Zhang, and Huashu Zhao. 2021. Textgnn: Improving text encoder via graph neural network in sponsored search. In _Proceedings of the Web Conference 2021_. 2848-2857.
* Zhu et al. (2021)

## Appendix A Theoretical analysis of HFC

### Background: Spectral Clustering

Given a graph \(=(,)\), with adjacency matrix \(A\), the Laplacian matrix of the graph is defined as \(L=D-A\), where \(D=diag(d_{1},...,d_{N})\) is the diagonal degree matrix (\(d_{i}=_{j}A_{i,j}\)). Then the symmetric normalized Laplacian matrix is defined as \(L_{sym}=D^{-LD^{-}}\). As \(L_{sym}\) is real symmetric and positive semidefinite, therefore it can be diagonalized as \(L=U U^{T}\)(Granovich, 2017). Here \(U^{N N}=\{u_{1},...,u_{N}\}\), where \(u_{i}^{N}\) denotes the \(i\)-th eigenvector of \(L_{sym}\) and \(=diag(_{1},...,_{N})\) is the corresponding eigenvalue matrix. To partition the graph, spectral clustering (Grabbiek et al., 2016; Grabbiek et al., 2016) computes the first K eigenvectors and creates a feature vector \(f_{K,}^{K}\) for each node \(:[1,K]\), \(f_{K,}(k)=u_{k}()\), which is in turn used to obtain K clusters by K-means or hierarchical clustering, etc.

An analogy between signals on graphs and usual signals (Grabbiek et al., 2016) suggests to interpret the spectrum of \(L_{sym}\) as a Fourier domain for graphs, hence defining filters on graphs as diagonal operators after change of basis with \(U^{-1}\). It turns out that the features \(f_{K,}\) can be obtained by ideal low-pass filtering of the Delta function \(_{}\) (localized at node a). Indeed, let \(l_{K}\) be the step function where \(l_{K}(i)=1\) if \(<_{K}\) and \(0\) otherwise. We define \(l_{K}\) the diagonal matrix for which \(l_{K}(i,i)=l_{K}(_{i})\). Then we have: \(l_{K,}=l_{K}U^{-1}_{}^{K}\), where we fill the last \(N-K\) values with \(0\)'s. Therefore, spectral clustering is equivalent to clustering using low-pass filtering of the local descriptors \(_{}\) of each node \(\) of the graph \(\).

### Spectral Contrastive Loss Revisited

To introduce spectral contrastive loss (Grabbiek et al., 2016), we give the definition of population view graph (Grabbiek et al., 2016) first.

**Population View Graph.** A population view graph is defined as \(=(,)\), where the set of nodes comprises all augmented views \(\) of the population distribution, with \(w_{,^{}}\) the edge weights of the edge connecting nodes \(x,x^{}\) that correspond to different views of the same input datapoint. The core assumption made is that this graph cannot be split into a large number of disconnected subgraphs. This set-up aligns well with the intuition that in order to generalize, the contrastive notion of "similarity" must extent beyond the purely single-instance-level, and must somehow connect distinct inputs points.

**Spectral Contrastive Loss.** Using the concept of population view graph, spectral contrastive loss is defined as:

\[(x,x^{+},x^{-},f_{}) =-2_{x,x^{+}}[f_{}(x)^{T}f_{}(x^{+})]\] \[+_{x,x^{+}}[(f_{}(x)^{T}f_{}(x^{-}))^{2}], \]

where \((x,x^{+})\) is a pair of views of the same datapoint, \((x,x^{-})\) is a pair of independently random views, and \(f_{}\) is a parameterized function from the data to \(^{K}\). Minimizing spectral contrastive loss is equivalent to spectral clustering on the population view graph, where the top smallest eigenvectors of the Laplacian matrix are preserved as the columns of the final embedding matrix \(F\).

### HFC-aware Spectral Contrastive Loss

As discussed in Appendix A.2, the spectral contrastive loss only learns the low-frequency component (LFC) of the graph from a spectral perspective, where the effects of high-frequency components (HFC) are much more attenuated. Recent studies have indicated that the LFC does not necessarily contain the most crucial information; while HFC may also encode useful information that is beneficial for the performance (Brockman et al., 2017; Grabbiek et al., 2016). In this regard, merely using the spectral contrastive loss cannot adequately capture the varying significance of different frequency components, thus constraining the expressiveness of learned representations and producing suboptimal learning performance. How to incorporate the HFC to learn a more discriminative embedding still requires explorations.

In image signal processing, the Laplacian kernel is widely used to capture high-frequency edge information for various tasks such as image sharpening and blurring (Grabbiek et al., 2016). As its counterpart in Graph Signal Processing (GSP) (Grabbiek et al., 2016), we can multiply the graph Laplacian matrix \(L\) with the input graph signal \(x^{N}\), (_i.e._, \(h=Lx\)) to characterize its high-frequency components - the frequencies that carry sharply varying signal information across edges of graph. On the contrary, when highlighting the LFC, we would subtract the term \(Lx\) which emphasizes more on HFC from the input signal \(x\), _i.e._, \(z=x-Lx\).

It should be noted that the above operation corresponds to a fixed low-pass filter in the spectral domain, where higher weights are specified for LFC. However, in practice, LFC may not always be useful, and HFC can also provide complementary insights for learning (Brockman et al., 2017; Grabbiek et al., 2016), especially when the label information is not smooth across edges. Additionally, the HFC of the input graph signal would be unavoidably too much weakened compared with the lower ones with fixed filters, leading to the well-known over-smoothing problem (Grabbiek et al., 2016). As discussed in Appendix A.1, spectral clustering is equivalent to clustering using a low-pass filter on each node of the graph. Henceforth, the feature vectors learned by the spectral contrastive loss is LFC of the population view graph. In this regard, the fixed low-pass filters largely limit the fitting capability of contrastive learning and its variants for learning discriminative node representations. As a consequence, it is vital to capture the varying importance of frequencies in the filter to preserve more useful information and alleviate over-smoothing issues.

As an alternative of the traditional low-pass filter, a simple and elegant solution to introduce HFC is to assign a single parameter to control the rate of high-frequency substraction.

\[z=x- Lx=(I- L)x,\]

where \(I\) is the identity matrix. We thus obtain the kernel \(I- L\) that contains HFC.

Following (Grabbiek et al., 2016), we consider the following matrix factorization based objective for eigenvectors:

\[_{F^{N K}}_{mf}(F) =\|(I- L)-FF^{T}\|_{F}^{2}\] \[=((1-)I+_{L,f}(}{} }}-f_{}(x_{i})^{T}f_{}(x_{j})))^{2}, \]

where \(w_{x}=_{x^{}}w_{,^{}}\) is the total weights associated to view \(x\). By the classical low-rank approximation theory (Eckart-Young-Mirsky theorem (Grabbiek et al., 2016)), minimizer \(F\) possesses eigenvectors of HFC-aware kernel \(I- L\) as columns and thus contains both the LFC and HFC of the population view graph.

Lemma 1 (_HFC-aware spectral contrastive loss.) Denote \(p_{x}\) is the \(x\)-th row of \(F\). Let \(p_{x}=w_{x}^{1/2}f_{0}(x)\). Then, the loss function \(_{mf}(F)\) is equivalent to the following loss function for \(f_{0}\), called HFC-aware spectral contrastive loss, up to an additive constant:_

\[_{mf}(F)=_{HFC}(f_{0})+const, \]

_where_

\[_{HFC}(f_{0}) =-2_{x,x^{+}}[f_{0}(x)^{T}f_{0}(x^{+})]\] \[+_{x,x^{-}}[(f_{0}(x)^{T}f_{0}(x^{-}))^{2}] \]

Proof.: We expand \(_{mf}(F)\) and obtain

(13) \[_{mf}(F) =((1-)I+_{i,j}(,x_{j}}}{}}}}}-f_{0}(x_{i})^{T}f_{0}(x_{j})))^{2}\] \[=const-2_{i,j}[(1-)I+,x_{j}}}{ }}}}}]f_{0}(x_{i})^{T}f_{0}(x_{j})\] \[+_{i,j}(f_{0}(x_{i})^{T}f_{0}(x_{j}))^{2}\] \[=\{const-2_{i,j}1-+,x_{j}}}{}}}}}f_{0}(x_{i})^{T}f_{0}( x_{j})\\ +_{i,j}(f_{0}(x_{i})^{T}f_{0}(x_{i}))^{2},i j\\ .\] \[=\{const-2_{i,j}1-+,x_{j}}}{}}}}}f_{0}(x_{i})^{T}f_{0}(x_{j})\\ +_{i,j}(f_{0}(x_{i})^{T}f_{0}(x_{j}))^{2},i j\\ .\] \[=\{const-2_{i,j}1-+,x_{j}}}{}}}}}f_{0}(x_{i})^{T}f_{0}(x_{j}) \\ +_{i,j}(f_{0}(x_{i})^{T}f_{0}(x_{j}))^{2},i j\\ .\] \[=\{const-2_{i,j}1-+,x_{j}}}{}}}}}f_{0}(x_{i})^{T}f_{0}(x_{j}) \\ +_{i,j}(f_{0}(x_{i})^{T}f_{0}(x_{j}))^{2},i j\\ .\] \[=\{const-2_{i,j}1-+,x_{j}}}{}}}}}f_{0}(x_{i})^{T}f_{0}(x_{j}) \\ +_{i,j}(f_{0}(x_{i})^{T}f_{0}(x_{j}))^{2},i j\\ .\] \[=\{const-2_{i,j}1-+,x_{j}}}{}}}}}f_{0}(x_{i})^{T}f_{0}(x_{j}) \\ +_{i,j}(f_{0}(x_{i})^{T}f_{0}(x_{j}))^{2},i j\\ .\] \[=\{const-2_{i,j}1-+,x_{j}}}{}}}}}f_{0}(x_{i})^{T}f_{0}(x_{j}) \\ +_{i,j}(f_{0}(x_{i})^{T}f_{0}(x_{j}))^{2},i j\\ .\] \[=\{const-2_{i,j}1-+,x_{j}}}{}}}}}f_{0}(x_{i})^{T}f_{0}(x_{j}) \\ +_{i,j}(f_{0}(x_{i})^{T}f_{0}(x_{j}))^{2},i j\\ .\] \[=\{const-2_{i,j}1-+,x_{j}}}{}}}}}f_{0}(x_{i})^{T}f_{0}(x_{j}) \\ +_{i,j}(f_{0}(x_{i})^{T}f_{0}(x_{j}))^{2},i j\\ .

* TextGNN : This model incorporates the text and graph information with a node-level aggregator, in which the query encoders share the same parameters.
* AdsGNN : This model also utilizes a node-level aggregator to aggregate the graph information at different levels, and introduces domain-specific pre-training and knowledge-distillation techniques to improve model performance.
* **Third**, the state-of-the-art co-training-based methods that enables the joint encoding of text and node features for the node representation learning on TAGs.
* GraphFormers : This is the state-of-the-art GNN-nested transformer model, which has graph-based propagation and aggregation in each transformer layer.
* Heterformer : This model alternately stacks the graph-attention-based neighbor aggregation module and the transformer-based text and neighbor joint encoding module to facilitate thorough mutual enhancement between network and text signals.

### Summary of HASH-CODE's workflow

```
Input: The input graphs \(G\) (consist of the center node \(n\) and its neighbours) Output: The embedding for the center node \(h_{o}\). for each text \(g G\)do \(H_{g}^{l}^{p}(H_{g}^{g})\); // Get the initial token-level embeddings. endfor \(l=1,...,L-1\)do \(Z_{g}^{l}(z_{g}^{l}|g G)\); // Gather node-level embeddings to GNN \(_{g}^{l}(Z_{g}^{l})\); // Graph aggregation in GNN for\(l=1,...,5\)do \(Z_{g}^{l}(Z_{g}^{l},H_{g}^{l})\); // Hierarchical contrastive learning for mutually reinforce the textual and graphic patterns endfor for each text \(g G\)do \(_{g}^{l}(Z_{g}^{l},H_{g}^{l})\); // Get contrastive graph-augmented token-level embeddings \(H_{g}^{l+1}^{l}(_{g}^{l})\); // Text encoding in Transformer endfor endfor return\(h_{o}^{l}}{2}\)
```

**Algorithm 1** HCL-TAG's Workflow

## Appendix C Node classification

**Settings.** In node classification, we train a 2-layer MLP classifier to classify nodes based on the output node representation embeddings of each method. The experiment is conducted on DBLP. Following , we select the most frequent 30 classes in DBLP. Also, we study both transductive and inductive node classification to understand the capability of our model comprehensively. For transductive node classification, the model has seen the classified nodes during representation learning (using the link prediction objective), while for inductive node classification, the model needs to predict the label of nodes not seen before. We separate the whole dataset into train set, validation set, and test set in 7:1:2 in all cases and each experiment is repeated 5 times in this section with the average performance reported.

**Results.** Table 4 demonstrates the results of different methods in transductive and inductive node classification. We observe that: (a) our HASH-CODE outperforms all the baseline methods significantly on both tasks, showing that HASH-CODE can learn more effective node representations for these tasks; (b) GNN-nested transformers generally achieve better results than GNN-cascaded transformers, which demonstrates the necessity of introducing graphic patterns in modeling textual representations; (c) HASH-CODE generalizes quite well on unseen nodes as its performance on inductive node classification is quite close to that on transductive node classification. Moreover, HASH-CODE even achieves higher performance in inductive settings than the baselines do in transductive settings.

## Appendix D In-Depth Analysis

### Data Sparsity Analysis

Conventional representation learning methods require a considerable amount of training data, thus they are likely to suffer from the data sparsity issues in real-world applications. This problem can be alleviated by our method because the proposed contrastive learning approach can better utilize the data correlation from input. We simulate the data sparsity scenarios by using different proportions of the full dataset, i.e., 20%, 40%, 60%, 80%, and 100%.

Figure 7 shows the evaluation results on Product and Sports datasets. As we can see, the performance substantially drops when less training data is used. While, HASH-CODE is consistently better than baselines in all cases, especially in an extreme sparsity level (20%). This observation implies that HASH-CODE is able to make better use of the data with the contrastive learning method, which alleviates the influence of data sparsity problem for representation learning to some extent.

    &  &  \\  & P@1 & NDCG & P@1 & NDCG \\  MeanSAGE & 0.5186 & 0.7231 & 0.5152 & 0.7197 \\ GAT & 0.5208 & 0.7196 & 0.5126 & 0.7146 \\ Bert & 0.5493 & 0.7506 & 0.5310 & 0.7485 \\ Twin-Bert & 0.5291 & 0.7440 & 0.5248 & 0.7431 \\  Bert+MeanSAGE & 0.6731 & 0.7637 & 0.6413 & 0.7494 \\ Bert+MaxSAGE & 0.6705 & 0.7752 & 0.6587 & 0.7599 \\ Bert+GAT & 0.6849 & 0.7801 & 0.6689 & 0.7619 \\  TextGNN & 0.6820 & 0.7753 & 0.6380 & 0.7716 \\ AdsGNN & 0.6882 & 0.7790 & 0.6624 & 0.7737 \\ GraphFormers & 0.6919 & 0.7929 & 0.6791 & 0.7993 \\ Heterformer & 0.6924 & 0.7957 & 0.6746 & 0.8079 \\  HASH-CODE & **0.7116** & **0.8198** & **0.6961** & **0.8170** \\  _Improv._ & 2.77\% & 3.03\% & 2.50\% & 1.13\% \\   

Table 4. Experiment results of transductive and inductive node classification on DBLP dataset. (HASH-CODE marked in bold, the best baseline underlined). HASH-CODE outperforms all baselines, especially the ones based on GNN-nested transformers.

### Influence of Training Epochs Number

Our approach consists of co-training with GNNs and Transformers. During the training stage, our model can learn the enhanced representations of the attribute and node for the representation learning task. The number of training epochs will affect the performance of the downstream task. To investigate this, we train our model with a varying number of epochs and fine-tune it on the downstream task.

Figure 8 presents the results on Product and Sports datasets. We can see that our model benefits mostly from the first 20 training epochs. And after that, the performance improves slightly. Based on this observation, we can conclude that the correlations among different views (i.e., the graph topology and textual attributes) can be well-captured by our contrastive learning approach through training within a small number of epochs. So that the enhanced data representations can improve the performance of the downstream tasks.

### Influence of Neighbor Size

We analyze the impact of neighbourhood size with a fraction of neighbour nodes randomly sampled for each center node (using DBLP for illustration). The link prediction results are shown in Figure 9. We can observe that with the increasing number of neighbour nodes, both HASH-CODE and Graphformers achieve higher prediction accuracies. However, the marginal gain is varnishing, as the relative improvement becomes smaller when more neighbours are included. In all the testing cases, HASH-CODE maintains consistent advantages over GraphFormers, which demonstrates the effectiveness of our proposed method.

### HFC-aware Embedding Visualization.

To intuitively study the impact of our HFC-loss, we visualize the input node embeddings for different target classes by t-SNE (van der Maaten and Hinton, 2008).

Figure 8. Performance (P@1) comparison w.r.t. different numbers of training epochs on DBLP and Product datasets. HASH-CODE benefits mostly from the first 20 training epochs, thus the correlations among different views can be well-captured by our approach through training within a small number of epochs.

Figure 7. Performance (P@1) comparison w.r.t. different sparsity levels on DBLP and Product datasets. The performance substantially drops when less training data is used, while HASH-CODE is consistently better than baselines in all cases, especially in an extreme sparsity level (20%).

[MISSING_PAGE_FAIL:15]