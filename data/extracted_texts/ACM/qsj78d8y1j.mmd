# No-Regret Algorithms in non-Truthful Auctions with Budget and ROI Constraints

Anonymous Author(s)

###### Abstract.

Advertisers are increasingly using automated bidding to optimize their ad campaigns on online advertising platforms. Autobidding allows an advertiser to optimize her objective subject to various constraints. In this paper, we design online autobidding algorithms to optimize value subject to ROI and budget constraints.

We consider an item is being auctioned in each of \(T\) rounds. We focus on one buyer with budget and ROI constraints in the stochastic setting: her value and highest competing bid faced are drawn i.i.d. from some unknown (joint) distribution in each round. We design low-regret bidding algorithms that bid on behalf of this buyer. Our main result is an algorithm with full information feedback (i.e., the highest competing bid is revealed after each round) that guarantees a near-optimal \(()\) regret with respect to the best Lipschitz function that maps values to bids. The class of Lipschitz bidding functions is rich enough to best respond to many correlation structures between value and highest competing bid, e.g., positive or negative correlation. Our result applies to a wide range of auctions, most notably any mixture of first- and second-price auctions. In addition, our result holds for both value-maximizing buyers and quasi-linear utility-maximizing buyers.

We also study the bandit setting, where the algorithm only observes whether the bidder wins the auction or not. In this setting, we show an \((T^{2/3})\) regret lower bound for first-price auctions, showing a significant disparity between the full information and bandit settings. We also design an algorithm with a regret bound of \((T^{3/4})\) when the value distribution is known and is independent of the highest competing bid.

repeated auctions, online learning, first-price, budget constraint, ROI constraint

## 1. Introduction

With the growth of online advertising markets in terms of both complexity and scale, advertisers are increasingly turning towards autobidding to optimize their ad campaigns on online advertising platforms. Autobidding allows an advertiser to use an optimization algorithm to generate bids on her behalf, rather than manually bidding for each ad query. The advertiser provides high-level goals and constraints to the autobidder, which bids on her behalf in order to optimize her objective, while satisfying her constraints.

In this paper, we study the problem of designing algorithms for autobidding on behalf of a buyer. We consider a stochastic setting with \(T\) rounds, in each of which one item is sold via an auction. In each round, the information of this round, including the buyer's value and the highest competing bid, are drawn i.i.d. from some unknown (joint) distribution. The autobidder submits a bid to the auction based on her value and the history. If the bid is at least the highest competing bid, the bidder wins the current round and pays a price. The bidder has a budget constraint that limits the total payment, as well as a Return-on-Investment (ROI) constraint which requires that the total value in the winning rounds is at least a fraction of the total payment. These are the two most common constraints that bidders have in practice. In particular, ROI constraint captures similar constraints used in practice like target cost-per-acquisition (tCPA) and target return-on-ad-spend (tROAS)1. Our goal is to design online bidding algorithms that maximize the bidder's objective subject to both budget and ROI constraints. To quantify an algorithm's performance, we use (additive) regret against the objective value obtained by the best bidding strategy that knows the underlying distribution.

There has been a lot of recent work on the problem of designing algorithms for autobidding in stochastic settings. One line of work  focuses on truthful auctions (e.g., second-price), which is proved to be much easier than non-truthful auctions due to technical reasons that we discuss later. A different line of work focuses on non-truthful auctions, with either a weak benchmark for regret, namely the best constant pacing (also sometimes called uniform bidding), where the bid is proportional to the value , or have regret bounds that scale with the number of values and bids, which can be uncountably many .

In this paper, we study the problem of bidding in non-truthful auctions and design no-regret algorithms against a stronger benchmark than the best constant pacing - our algorithms have low regret compared to the _best Lipschitz bidding function that maps values to bids_. Due to the generality of Lipschitz functions this benchmark can best-respond to a range of different correlations between the buyer's value and the highest competing bid, e.g. positive correlation for some values and negative correlation for others.

**Our results and techniques.** We first consider the full-information setting where the bidder observes the highest-competing bid at the end of each round. We prove that there is an algorithm that can get near-optimal regret with respect to the best Lipschitz bidding function. The main result for this setting is as follows:

**Theorem 1.1** (Informal version of Theorem 4.1).: _There is an algorithm that achieves \(()^{2}\) regret while satisfying both the budget and ROI constraints, with respect to the best Lipschitz bidding function. The result applies to various classes of auctions (see Assumption 3.1) including first-price auctions, second-price auctions and a hybrid of both. The result applies to both value and quasi-linear utility maximizing bidders._

To the best of our knowledge, this is the first algorithm that achieves near-optimal regret bounds against the best Lipschitz bidding function for non-truthful auctions under budget and/or ROI constraints.2 Our result applies to any input distribution under mild assumptions (see Section 2).

Our algorithm is based on the primal/dual framework for online learning with constraints (Gran et al., 2015; Gran et al., 2016; Gran et al., 2017; Gran et al., 2018; Gran et al., 2019). In this framework, to manage global constraints, the 'core' algorithm deploys two competing algorithms, each optimizing an unconstrained objective. On one hand, the primal algorithm picks an action (subsequently used in the actual bidding problem) to maximize a function similar to the Lagrangian of the problem. On the other hand, the dual algorithm picks Lagrangian multipliers to minimize the same function. Guarantees for this sequential unconstrained stochastic zero-sum game imply the guarantees for the original constrained problem.

While the dual algorithm uses a standard instance of Online Gradient Descent to pick the scalars that represent the Lagrangian multipliers, designing the primal algorithm is often much more complicated and requires knowledge specific to the original problem. We develop the primal algorithm for our main result in Section 3.

**Main Technical Challenges.** Below we list some of the main technical challenges that we need to tackle and give a brief outline of our approach to solving them.

_Lagrangian Maximization in Non-Truthful Auctions._ To better explain the challenge, we first consider the problem where the auction used is a second-price auction. The part of the Lagrangian function that depends on the primal algorithm's bid \(b\) takes the following form (for either value or quasi-linear utility maximization): \(r(b)=1[b d]\) (\( v- d\)),where \(v\) is the player's value, \(d\) is the (unknown) highest competing bid, and \(,\) are arbitrary non-negative numbers that depend on the Lagrange multipliers. Maximizing the above function turns out to be straightforward: using \(b^{*}=v\) implies4\(r(b^{*})=( v- d)^{*}\), which guarantees maximum reward. Since \(b^{*}\) does not depend on the highest competing bid \(d\), the primal algorithm can pick this bid to guarantee zero regret for maximizing the Lagrangian; this subsequently leads to low regret guarantees for the original problem with constraints.

In contrast to truthful auctions, for non-truthful auctions, the bid that maximizes the Lagrangian cannot be calculated without the highest competing bid. Therefore, the learner needs to learn the best function that maps values to bids. However, learning the best such function is unrealistic since it might be non-monotone and discontinuous. Instead, we focus on a class of functions with specific structures. Such a class used in previous work is the class of pacing multipliers, \(_{}\), that map values to bids by multiplying by a constant number. Instead, we focus on the more general class of Lipschitz continuous functions, \(_{}\), which provide a much stronger benchmark to compete against, even in very simple settings where values and highest competing bids are independent. For example, if the highest competing bid is constant and the value is not, the best response is a fixed bid, which cannot be expressed by the class of pacing multipliers. In Appendix B we give a concrete example of this and include some additional discussion on the limitations of the pacing multiplier class \(_{}\).

The increased expressivity and complexity of \(_{}\) over \(_{}\) can also be observed when considering finite approximations of them. \(_{}\) can be approximated with accuracy \(\) using a set of size \((1/)\). If this approximation results in \((T)\) error over \(T\) rounds (this is not trivial, see our discussion on that next), along with many more simplifying assumptions, using standard online learning algorithms we get \((T+)\) regret (\(\) is the regret of using \(K\) different actions); optimizing over \(\) we get \(()\) regret. In contrast, approximating \(_{}\) with \(\) accuracy requires a set of size \(((1/))\), leading to \((T+)\) regret. This is \(}(T^{2/3})\) if optimized over \(\), which is suboptimal.

The near-optimal \(}()\) regret is achieved by utilizing the structure implied by the finite subset of \(_{}\), similar to (Gran et al., 2016; Gran et al., 2017). More specifically, we create a tree where the functions of the finite subset of \(_{}\) are the leaves and smaller distance between two leaves implies more similarity between the two corresponding functions. This allows us to enhance the standard regret guarantees of learning algorithms to get the improved result, found in Section 3.3.

_Discretization Error and Safe bid._ Our algorithms are based on discretizing the bidding space of real numbers. However, two bids that are similar do not necessarily lead to similar reward, as the reward of a round is not a continuous function of the bid. This has been solved in previous works (e.g. Fikioris and Tardos (Fikioris and Tardos, 2018); Han et al. (Han et al., 2018)) for first-price by noticing that using bid \(b+\) instead of \(b\) still wins the auction and the price paid can only be \(\) more. However, in this work we face one additional challenge: since our primal algorithms aim to maximize the Lagrangian, the reward of bid \(b+\) might be negative, making bid \(b\) much better if it _does not win_ the auction. This means that the error of discretizing our action space is harder to handle. We tackle this by defining a general way of transforming bids to "safe bids" that guarantee non-negative reward that is at least as good as the original bid (Assumption 3.1), which is crucial to getting optimal regret rates.

_Time-Varying Range._ The Lagrangian function that the primal algorithm aims to maximize depends on the Lagrangian multipliers picked by the dual algorithm. Thus the primal algorithm's guarantees need to hold even against an adaptive adversary since no assumptions can be made for the dual algorithm's behavior, which adapts to the primal's decisions. While this challenge is not new to online learning algorithms, a new problem that we face is that the Lagrange multipliers control the range of the objective that the primal algorithm has to maximize. For technical reasons (which we discuss in Section 2), we cannot a priori upper bound these multipliers. This means that the primal algorithm needs to maximize a function whose range is time-varying and unknown. We develop algorithms that tackle this problem and offer regret bounds that match the bounds of algorithms that know this range in advance. We first solve this problem in Section 3.2 and use a technique that is very general and, we believe, is of independent interest.

_From Standard Regret to Interval Regret._ The 'core' algorithm requires that the primal and dual algorithms have low interval regret, i.e, low regret in every interval of rounds. This is not automatically achieved by classic algorithms, e.g., the Hedge algorithm (Henderson, 1983) has linear interval regret. In Section 3.4, we offer a black-box reduction to reduce the problem of standard regret minimization to interval regret minimization with only \(}()\) error, which also works for the above time-varying range problem.

**Bandit Information.** In Section 5 we consider the bandit information setting where the algorithm only observes whether the bid wins the auction or not and the price she pays if she wins. In sharp contrast to the full-information setting, we prove an \((T^{2/3})\) regret lower bound for first-price auctions even in a simple setting when the value is constant. While this is known for quasi-linear utility maximization Balseiro et al. (2015), no results are known for value maximization. Our lower bound is materialized in a very simple setting, as showcased in the theorem that follows.

Theorem 1.2 (Informal version of Theorem 5.1).: _No algorithm can always guarantee \(o(T^{2/3})\) regret in value-maximizing first-price auctions with bandit information, even when the value is constant, the budget is \((T)\), and there is no ROI constraint._

Our lower bound is based on a distribution of highest competing bids that has the following property: for (almost) every pair of values in the support, there is an optimal bidding strategy that uses only those values. A small adversarial modification in the distribution at a certain value ensures that (a) bidding at any other value is sub-optimal and (b) the bidder wastes many rounds on sub-optimal bids before finding the optimal one. This construction is inspired by the \((T^{2/3})\) lower bound of (Balseiro et al., 2015) for revenue maximization in posted-price mechanisms without constraints.

To complement our lower bound in Theorem 1.2, we present a \(}(T^{3/4})\) regret upper bound in Theorem F.3.

**Tight satisfaction of the ROI constraint.** We remark that all our regret upper bounds satisfy the ROI constraint exactly but are based on similar results that approximately satisfy the ROI constraint (i.e. have sublinear violation with high probability). In Section 4, we present a black box reduction to turn any algorithm with approximate satisfaction into one with exact satisfaction.

Finally, we note that the focus of our \(}()\) regret bounds in the full information setting (Section 3) is regret minimization. To get this optimal information theoretic bound our algorithms require exponential running time. In Appendix G, we present algorithms that require polynomial time to run and offer the same guarantees as Theorem 1.1 when the values and highest competing bids are independent across rounds.

### Related work

The most relevant paper to ours is Castiglioni et al. (2017). The algorithm designed in our paper is based on the primal/dual framework in (Ballesteros, 2017); we briefly introduce the framework in Section 2. They also use the framework to design algorithms for bidding in first-price auctions with budget and ROI constraints, albeit only for a finite number of values and bids: their regret bound is \(}()\) against the best bid per value, where \(n\) is the number of values and \(m\) is the number of bids. In addition, their algorithm satisfies the ROI constraint only approximately. In contrast, our results apply to continuous distributions and strictly satisfy the ROI constraint.

**Online bidding in non-truthful auctions.**Lucier et al. (2018) design an algorithm for bidding in first and second price auctions under budget and ROI constraints, that implies welfare guarantees when used by every player (extending the result of Gaitonde et al. (2018)). In addition, (Luo et al., 2018) prove that their algorithm, when used in a stochastic environment, has \(}(T^{7/8})\) regret against the class of pacing multipliers while satisfying both constraints strictly. Fikioris and Tardos (2018) also focus on welfare guarantees in first-price auctions when budgeted players use arbitrary algorithms that have no-regret against the class of pacing multipliers. In addition, they design a full information algorithm that has \(}()\) regret with respect to the same class in the stochastic environment. Finally, Wang et al. (2018) study online learning in first-price auctions with budgets but focus only on the independent values and highest competing bids.

We defer further discussion about related work in Appendix A, were we discuss online learning in truthful auctions, online bidding without constraints, and online learning with or without budget constraints.

## 2. Preliminaries

We consider the setting where a single bidder participates in \(T\) sequential auctions. In each round \(t[T]\) there is a single item being sold; a pair \((v_{t},d_{t})\) is drawn i.i.d. from some unknown (joint) distribution \(\), where \(v_{t}\) indicates the bider's value, and \(d_{t}\) is the highest competing bid5. The bidder submits a bid \(b_{t}\) based on her value \(q_{t}\). The bidder wins this round if her bid is at least6 the highest competing bid \(d_{t}\); we denote \(x_{t}=1\)\([b_{t} d_{t}]\). If the bidder wins the auction, then she pays a price \(p_{t}=p(b_{t},d_{t})\), where \(p(b,d)\) is the payment function. For example, for first-price auctions, \(p(b,d)=b\); for second-price auctions, \(p(b,d)=d\); for any combination of the two auctions, \(p(b,d)=q b+(1-q) d\) for some \(q\), we note that the payment function \(p\) is fixed across all rounds and known to the bidder.

At the end of each round \(t\), the bidder observes information about that round depending on the feedback model. In the full-information setting, the bidder observes the highest competing bid \(d_{t}\) with which she can compute the outcome for any possible bid at this round. In the bandit-information setting, the bidder only observes whether she wins the auction or not (i.e. \(x_{t}=1\)\([b_{t} d_{t}]\)) and the payment \(p_{t}\) if she wins. Our results hold for different objectives of the bidder, who wants to maximize \(_{t[T]}u_{t}\), where \(u_{t}\) is her per-round utility. The focus of our paper is value-maximizing, where \(u_{t}=v_{t}x_{t}\), but our results also hold when the bidder has a quasi-linear utility, where \(u_{t}=x_{t}(v_{t}-vp_{t})\) for some \(v\).

We assume that the bidder has a budget \(B\). This is a strict upper bound on her total payment. Namely, it must hold that her total payment after \(T\) rounds is at most \(B\), i.e. \(_{t[T]}x_{t}p_{t} B\). We define \(=\) and note that w.l.o.g. we can assume that \( 1\): any \( 1\) implies that the bidder is effectively not budget constraint, since \(p(,) 1\). The bidder must also satisfy a Return-On-Investment (ROI) constraint: her total value in the winning rounds must be at least a fraction of her total payment: \(_{t}x_{t}v_{t}_{t}x_{t}p_{t}\), for some \( 1\). For the ROI constraint, we often allow approximate satisfaction where \(_{t}x_{t}(v_{t}-vp_{t})-V\) and \(V\) is the violation amount, often \(V=}()\). W.l.o.g., we assume that \(=1\); any other \(\) can be handled by rescaling the values7.

**Benchmark.** We want low regret when competing against a class of bidding functions \(\) that map values to bids. More specifically, we assume that for every \(f\), \(f\) maps \(\) to \(\) and we want the player's resulting utility to be close to her utility if she knew the distribution \(\) in advance and she bid using the best fixed functions from \(\). Since the bidder has to satisfy certain constraints, the best response to a distribution \(\) might be a distribution of functions over \(\), not a single function. Our benchmark is the maximum expected average-per-round utility of the best distribution of functions from \(\) that satisfies the constraints in expectation. For example, for a value maximizing player in first price auctions, i.e., \(u_{t}=v_{1}[b_{t} d_{t}]\) and \(p_{t}=b_{t}\) we have

\[=_{F()}& *{}_{F F(u_{d}) D}[o  1[f(v) d]]\\ &&*{}_{F F (u_{d}) D}[f(v) 1[f(v) d]]\\ &*{}_{F F(u_{d}) D} [(v-f(v)) 1[f(v) d]] 0 \]

For simplicity, we assume that the function \(f(v)=0\) always belongs in \(\), making (1) always feasible. \(T\) is an upper bound for the achievable total expected utility of any algorithm that satisfies the constraints \(\). We design algorithms that have low regret with respect to \(T\) (where \(\) is defined analogously depending on the buyer's objective and auction format).

**Primal/dual framework.** We now briefly describe the primal/dual framework where a constrained problem is solved by having two learning algorithms, the primal and the dual, compete against each other in a sequential unconstrained zero-sum game. Specifically we will look at the results of (Bartlett and Barthelem, 1989) who develop such a framework for budget and ROI constraints. We first discuss the assumptions required on the input distribution and how these relate to our setting of learning in sequential auctions. We then present the guarantees that the primal algorithm must satisfy to get guarantees for the original constrained problem. For simplicity, the rest of this section focuses on value maximization and presents all the assumptions in the context of auctions. We refer the reader to their paper for a more comprehensive description of their techniques.

First, we illustrate the need of some assumptions on the distribution \(\) that generates \(v_{t},d_{t}\). Specifically, we assume that there exists a bidding function that on expectation leads to \(\) more value than payment, for some \( 0\). Formally,

\[ f:*{}_{(u,d) }[(v-pf(v),d)[f(v) d] ]. \]

Intuitively, this assumption implies that a learner who makes wrong decisions and violates her ROI constraint can recover this in later rounds. This assumption is similar to the one in (Bartlett and Barthelem, 1989), who examine value maximization in repeated truthful auctions. Their assumption is the same as (2) but for \(f(v)=v\). This might seem stronger, since it implies (2) when the function \(f(v)=v\) is contained in \(\). However, the reverse also holds for truthful auctions, since \(f(v)=v\) is optimal for maximizing quasi-linear utility.

We now show the basics of the primal/dual framework and the guarantees the primal needs to satisfy to get guarantees for the original problem. We first define the following function: \(_{t}(b,,)=[b d_{t}](v_{t}-  b+ v_{t}-)+\). This function is inspired by the optimization problem in (1) (and would analogously be defined for other objectives/pricing functions). \(,\) are Lagrange multipliers that correspond to the budget and ROI constraint, respectively. The core algorithm of (Bartlett and Barthelem, 1989) runs two algorithms, the primal algorithm that picks bids and the dual that picks Lagrange multipliers. On every round \(t\), the primal (resp. dual) algorithm picks \(b_{t}\) (resp. \((_{t},_{t})\)) aiming to maximize (resp. minimize) \(_{t}(b_{t},_{t},_{t})\). Given their actions, each algorithm faces some regret. To get regret guarantees for the original problem (value maximization under constraints) the primal and dual algorithms must have with high probability low _interval regret_, i.e., have low regret over every interval \([_{1},_{2}][T]\). Before formally defining this, we point out one subtle detail.

In general, regret bounds depend on the range of values that the objective function takes. This range of the primal algorithm in our setting can be as high as \(2_{t}+_{t}+1\) in round \(t\). This depends on the dual algorithm's actions \(_{t},_{t}\), meaning we cannot know \(_{t}\{2_{t}+_{t}+1\}\) in advance. One solution to this is to explicitly upper bound \(_{t}\) and \(_{t}\). To get meaningful regret guarantees such an upper bound would be \(_{t},_{t}\). However, calculating \(\) requires \(\), as seen in (2), which is unknown.

(9) show that the above issue can be circumvented, if the primal algorithm satisfies stronger _interval regret_ bounds: with high probability, for every interval of rounds, the regret in those rounds is small with respect to the best fixed action in that interval and the maximum Lagrange multipliers seen so far. To formally define this, first define \(M_{t}=_{t^{} t}\{2_{t^{}}+_{t^{}}+1\}\). We require that the primal algorithm picks bids \(b_{1},,b_{T}\) such that for every \((0,1]\), with probability at least \(1-\) it holds that for all \([_{1},_{2}][T]\) :

\[_{f}_{t=_{1}}^{_{2}}_{t}f(v_{t }),_{t},_{t}-_{t=_{1}}^{_{2}}_{t}(b_{ t},_{t},_{t})*{}_{}(T,M_{}) \]

Given the above guarantee for the primal and some mild conditions on \(\), we get the guarantees for the original problem. More specifically, we require that the dependence of \(M\) in \(*{}_{}(T,M)\) is not worse than quadratic. Using this condition, both the regret of the original problem and the violation of the ROI constraint are at most \(*{}_{}(T,,)\). Formally, we have the following theorem.

**Theorem 2.1** (Theorem 6.9 of (Bartlett and Barthelem, 1989), adapted to auctions).: _Assume (2) is true for \(>0\), the dual algorithm is Online Gradient Descent, and (3) holds with \(*{}_{}(T,M)M^{2} *{}_{}(T,1)\). Then, for every \(>0\), with probability at least \(1-4\), the regret of the core algorithm and the ROI violation is each at most \(*{}_{}T,,+\)._

First, we note that (Bartlett and Barthelem, 1989) require a slightly more general condition than the one in (2) for \(>0\); we present this in detail in Appendix C. Second, we note that if \(||=K\) was finite, we had bandit feedback, and a known upper bound for \(M_{t}\) for \(M_{t}\) then existing techniques would allow us to get \(*{}_{}(T,M_{t})=}M_{t} \). In the setting where there is no such upper bound, (Bartlett and Barthelem, 1989) offer an algorithm with \(*{}_{}(T,M_{t})=}M_{t} ^{2}\). While this regret bound satisfies the requirements of Theorem 2.1, the quadratic dependence on \(M_{t}\) is sub-optimal, making the resulting regret bound of the core algorithm to be proportional to \(^{2}}\). One of the contributions of the following sections is a general technique to get linear dependence of \(M_{t}\) in \(*{}_{}(T,M_{t})\), similar to knowing \(M_{t}\) in advance. This leads to much improved regret bounds that scale with \(\), which is especially important when \(\) and \(\) are small.

## 3. Primal algorithm designs with full information

In this section, we design a primal algorithm that satisfies (3) for sequential auctions. Our goal is to pick bids to maximize the Lagrangian \(_{t}(b_{t},_{t},_{t})\). For every round \(t\), we define for simplicity \(r_{t}()\) to be part of the Lagrangian that depends on the bid:

\[r_{t}(b)=1\{b d_{t}\}(_{t}b_{t}-_{t}b(b,d_{t})) \]

where \(d_{t}\) is the highest competing bid, \(a_{t}\) is the value, \(p(b,d_{t})\) is the payment of bid \(b\), and \(_{t}\), \(_{t}\) are non-negative numbers that depend on the Lagrange multipliers \(_{t}\) (for budget constraint) and \(_{t}\) (for ROI constraint) of round \(t\). For value maximizing, \(_{t}=1+_{t}\) and \(_{t}=_{t}+_{t}\). For quasi-linear utility, \(_{t}=1+_{t}\) and \(_{t}=v+_{t}+_{t}\) for some \(v\). To present general results, we assume that \(_{t},_{t}\) are arbitrarily picked by an adaptive adversary with \(_{t}>0\) and \(_{t} 0\). We emphasize the lack of an upper bound on \(_{t},_{t}\) and recall (3), which requires that the regret scales with the largest \(_{t},_{t}\) seen so far. We overload the notation of \(r_{t}()\) to also take as an argument a bidding function \(f:\), in which case \(r_{t}(f)=r_{t}(f(a_{t}))\).

Our results apply to a wide range of price functions \(p(,)\) that include any combination of first and second price. In particular, \(p(,)\) needs to satisfy the following assumption which we explain after its formal statement.

**Assumption 3.1**.: The pricing function \(p(b,d)\) satisfies: (i) \(p(0,0)=0\); (ii) \(p(,d)\) is non-decreasing and \(1\)-Lipschitz continuous8 for all \(d\); (iii) for every \(t\), there exists a "safe" bid \(b_{t}^{c}\) so that

* \(r_{t}(b_{t}^{c}) 0\) for all \(d_{t}\).
* \(b_{t}^{c}\) is a function of \(a_{t},_{t},_{t}\) but not \(d_{t}\).
* for every bid \(b\) such that \(_{d_{t}}r_{t}(b)<0\), then for all \(d_{t}\) it holds \(r_{t}(b_{t}^{c}) r_{t}(b)\).

While Conditions (i) and (ii) are standard assumptions on the payment function, Condition (iii) is less straightforward. In short, the safe bid guarantees that our algorithm will never use \(b_{t}\) with \(r_{t}(b_{t})<0\). For every \(t\), we require that the function \(r_{t}(b)\) has a "safe" bid \(b_{t}^{c}\) so that: (a) \(b_{t}^{c}\) guarantees a non-negative reward, (b) \(b_{t}^{c}\) can be calculated using the information known before bidding, and (c) \(b_{t}^{c}\) guarantees reward that is at least the reward of any other bid which has a negative reward for some \(d_{t}\). We emphasize that (c) states that \(r_{t}(b_{t}^{c}) r_{t}(b)\) for all \(d_{t}\) as long as \(r_{t}(b)<0\) for some \(d_{t}\). In Appendix D.1, we show that any mixture of first and second price auction satisfies Assumption 3.1. In particular, the safe bid of Condition (iii) in this case is \(b_{t}^{c}=\{}{_{t}}_{t},1\}\).

### Overview of the Primal Algorithm Design

We now state the main result of the section: There is an algorithm that has low interval regret with high probability. Our regret guarantee is with respect to the class of all \(L\)-Lipschitz continuous functions for any \(L 1\), which we denote with \(\). In addition, our regret bound in rounds up to \(t\) scale linearly with respect to the highest \(,\) seen so far: \(U_{}=_{t}\{_{t},_{t}\}\). Finally, we note that our algorithm takes into advantage the fact that \(_{t},_{t}\) are known before bidding in round \(t\). This is important for various calculations, e.g., calculating the safe bid \(b_{t}^{o}\) of a round. Outside of that, however, \(_{t},_{t}\) are picked adversarially and are not known before round \(t\).

**Theorem 3.2**.: _Let \(\) be the set of all \(L\)-Lipschitz continuous functions from \(\) to \(\) for some \(L 1\). Assume that the payment function satisfies Assumption 3.1. Assume that \(_{t},_{t},d_{t}\) are picked by an adaptive adversary and \(_{t},_{t}\) are revealed after round \(t-1\). \(U_{_{t}}=_{t_{t}}\{_{t},_{t}\}\). Then there exists an algorithm that generates bids \(b_{1},,b_{T}\) such that for all \(>0\), with probability at least \(1-\) it holds that for all intervals \([_{1},_{2}][T]\)_

\[_{f}_{t=1}^{_{2}}r_{t}(f)-_{t=1}^{_{2}}r_{t} (b_{t})U_{_{2}}( T+).\]

Theorem 3.2 satisfies (3) as well as the conditions of Theorem 2.1 getting the following theorem.

**Theorem 3.3**.: _There is an algorithm for value or quasi-linear utility maximization when the payment function satisfies Assumption 3.1, such that for every \(>0\), with probability at least \(1-\) the algorithm has regret against the class of \(L\)-Lipschitz continuous functions and ROI violation at most \(( T+)\)._

We note that the above algorithm, while providing an optimal (up to \(( T)\)) information-theoretic bound, runs in exponential time. In Appendix G we present a polynomial time algorithm with matching regret which requires \(a_{t},d_{t}\) to be independent.

There are a couple of technical challenges in order to get low regret against \(\) and prove Theorem 3.2. First, even for in-expectation regret bound, we cannot directly use a standard algorithm like Hedge, since \(\) contains infinite actions. Instead, we work with finite approximations of \(\); for accuracy \(>0\) let \(_{}\) such that

\[ f, f_{}_{}: f(u ) f_{}(b) f(0)+,\  \]

i.e., for every \(f\) in the original set \(\) there exists some function \(f_{}\) in the new set \(_{}\) such that \(f_{}\) is at least \(f\), but is never greater by more than \(\). Previous work shows that there is always an \(_{}\) with \(|_{}|((L/))\). (30, Corollary 2.7.2)

The above bound on the cardinality of \(_{}\) is exponential in \(1/\). As explained in the introduction, using a standard online learning algorithm could only prove \((T^{2/3})\) regret bounds. However, as Theorem 3.2 suggests, we can get much stronger \(}()\) regret bounds. We solve this issue by utilizing the structure of \(\), similar to (Gilard et al., 2017; Gilard et al., 2017). We create a hierarchical tree structure where the leaves of the tree are the functions of \(\). Leaves whose distance is small represent bidding functions that are close in \(L_{}\) distance. Next, a non-leaf node above the leaf nodes can calculate a bidding function by combining the bidding functions of its children. Because its children are very 'close,' we can design algorithms so that the output has small regret with respect to its best child. Similarly, every non-leaf node does the same with its children, with nodes closer to the root having larger regret. This results in \(()\) regret instead of \((T^{2/3})\). We develop this tree algorithm in Section 3.3.

In Section 3.2, we develop the algorithm that is used by the non-leaf nodes and that utilizes the proximity of its children's bids. We present this algorithm in the general language of online learning. The structure that this algorithm takes advantage of is that there is a "good" action: it is at most \(\) sub-optimal compared to any other action, in every round. This leads to \(()\) regret,which offers a great improvement over the regret of \(()\) that Hedge has when \( 1\).

Another technical challenge we face is regarding the discretization error. A naive assumption is that using \(_{}\) instead of \(\) leads to \(()\) error every round. However, this is not the case: Let \(f,f_{}\) as described in (5). Using \(f_{}\) instead of \(f\) in a round \(t\) results in at most \(_{t}\) error if bidding \(f(_{t})\) wins the auction (since \(f_{}(_{t})\) also wins). However, if \(f(_{t})<d_{t} f_{}(_{t})\) it might be the case that \(r_{t}(f)=0 r_{t}(f_{})\). For example, consider first-price with \(_{t}=_{t}=1\), \(q_{t}=\), \(f(_{t})=1-\), \(f(_{t})=1\), and \(d_{t}=1-/2\), in which case \(r_{t}(f_{})=-(1-) r_{t}(f)=0\).

The safe bid of Assumption 3.1 solve this discretization issue. If a bid \(b+\) is in danger of leading to a negative reward (i.e., \(_{d_{t}}r_{t}(b+)<0\)), we can use the safe bid instead to guarantee at least as good reward for any \(d_{t}\), ensuring \(()\) less reward than \(r_{t}(b)\). We present this in Section 3.3. We note that one can circumvent this issue when maximizing quasi-linear utility with no constraints (i.e., when \(_{t}=_{t}=1\)) by making sure that the class \(\) contains only functions such that \(f() v\), which is the solution of (Gardner, 2017). However, we cannot limit \(\) in such a way since \(_{t},_{t}\) change dynamically.

Finally, to complete the proof of Theorem 3.2, in Section 3.4, we reduce the problem of bounding interval regret to bounding normal regret. The whole structure of our algorithm (including the primal/dual framework) is shown in Fig. 1.

Our resulting primal algorithm works by chaining multiple sub-algorithms, as shown in Fig. 1. The standard way to employ this chaining is to use each algorithm's outputting action. However, since our goal is a regret bound with high probability, the success of every algorithm is conditioned on the success of all its sub-algorithms. This creates noise in the high probability bounds, scaling with the number of sub-algorithms.

To overcome this challenge, we employ a different technique. Instead of an action, each algorithm outputs the distribution from which it would have sampled its action. These distributions satisfy a regret bound with probability 1, making chaining multiple algorithms much more stable since the sampling of an action happens only once and not for every sub-algorithm.

### Time-varying Ranges and Good Actions

In this section we develop the algorithm that we need for the non-leaf nodes of the tree algorithm, which takes advantage of the proximity of the rewards of its actions. We present the algorithm in the general language of online learning when there is a set \([K]\) of actions and an arbitrary reward function \(r_{t}()\) for every round \(t\) which the learner observes after round \(t\).

There has been extensive work on this setting, under various different assumptions that make the problem easier or harder. The contribution of this section, Theorem 3.5, is twofold. First, our algorithm is agnostic to the future range of the rewards. Specifically, we assume that the reward of round \(t\) is in the range \([0,U_{t}]\) for some adversarially chosen \(U_{t}>0\) and the learner observes \(U_{t}\) when picking her action \(a_{t}\) in round \(t\). We assume that \(U_{1} U_{2} U_{T}=U\). If \(U\) is known in advance, then using Hedge leads to regret that scales linearly with \(U\). We achieve the same dependency without knowing \(U\). This also improves the quadratic dependency on \(U\) of previous work (Gardner, 2017, Theorem 8.1).

Our second contribution in online learning when there is a \(\)-good action. The formal definition is in Definition 3.4, but simply put a \(\)-good action is at most \( U_{t}\) sub-optimal compared to any other action in every round \(t\). Definition 3.4 extends the original definition of (Gardner, 2017) for time-varying reward ranges \(U_{1},,U_{T}\).

**Definition 3.4** (Good action).: For rewards \(r_{t}:[K][0,U_{t}]\), a \(\)-good action \(g[K]\) satisfies \(r_{t}(g) r_{t}(a)- U_{t}\), \( a[K],t[T]\).

Taking \(U_{t}=1\) yields the definition of (Gardner, 2017). Note that \(\) since \(0 r_{t}(a) U_{t}\).

We now present the algorithm for the above setting. Our algorithm is the Hedge algorithm but with a carefully selected step size \(_{t}\). In particular, the probability of playing action \(a\) in round \(t\) is proportional to \((_{t}R_{t-1}(a))\), where \(R_{t-1}(a)=_{t^{} t-1}r_{t^{}}(a)\) and \(_{t} 1/U_{t}\). We believe that this step size is of independent interest and can be used in any online learning setting to get regret bounds for time-varying ranges that match classical ones. Our regret bound is \((U)\), which matches the one in (Gardner, 2017), without assuming \(U_{t}=U\) for every \(t\) and that \(U\) is known. The full algorithm can be found in Algorithm 1.

```
Input: Total rounds \(T\), actions \([K]\), sub-optimality of good action \(\)  Initialize cumulative reward of each action \(R_{0}(a)=0 a[K]\) for\(t[T]\)do  Receive range \([0,U_{t}]\) and calculate step size \(_{t}=}}\)  Calculate probability distribution \(_{t}(a)=(_{t}R_{t-1}(a))/_{a^{}}( _{t}R_{t-1}(a^{})) a[K]\)  Sample and play action \(a_{t}_{t}(a)\)  Receive rewards \(r_{t}:[K][0,U_{t}]\)  Update cumulative rewards \(R_{t}(a)=r_{t}(a)+R_{t-1}(a) a[K]\)  end for
```

**ALGORITHM 1**Hedge for time-varying ranges and good actions

We now present our regret bound. As mentioned before, we bound the regret of the action distributions \(p_{1}(),,p_{T}()\) that Algorithm 1 generates instead of the regret of the sampled actions. This implies a matching bound on the expected regret. Using standard concentration inequalities we can get bounds with high probability. However, the most useful application of outputting distributions instead of actions is that chaining multiple algorithms becomes easier. This allows for stronger high-probability guarantees: the overall regret bound is dependent only on one sampling process, the one performed by the top-level algorithm.

Figure 1. The algorithm structure of the entire primal/dual framework in our setting.

**Theorem 3.5**.: _Assume that an adaptive adversary picks the reward function \(r_{t}:[K][0,U_{t}]\) in every round \(t\), where \(U_{1} U_{T}\). Assume that there is a \(\)-good action, with \(}{{T}}\). Then the action distributions \(p_{1},,p_{T}\) of Algorithm 1 guarantee \([T]\)_

\[_{a[K]}_{t[]}r_{t}(a)-_{t[]}_{a[K]}p_{t}(a )r_{t}(a) 4U_{}.\]

Algorithm 1 dynamically adapts to the time varying reward ranges due to \(_{t} U/U_{t}\). Because of this, if the adversary picks \(U_{t} U_{t-1}\), our algorithm adapts to the new ranges and picks the action of round \(t\) accordingly. We believe this technique is very versatile and can be used to modify existing no-regret algorithms to work in the space of time-varying reward ranges. We show this in the bandit information setting in Theorem F.4. We defer the full proof of Theorem 3.5, along with the proofs of the other results of this section to Appendix D. In Appendix D we also include a high probability version of the above theorem, Theorem D.2 with \((U)\) additional error. To retain regret that depends on \(T\) instead of \(T\), we cannot directly apply a standard concentration inequality and instead show that our rewards have low variance that depends on \(\) to get the improved bound.

### Algorithm for Lipschitz Bidding Functions

In this section we present the result that has low regret compared to the best \(L\)-Lipschitz function in the class \(\), when maximizing the reward function \(r_{t}()\) as defined in (4).

The biggest novelty of this section is making sure that "similar" bidding functions result in similar reward. To that end, let \(f,f_{}:\) such that \(f(v) f_{}(v) f(v)+\) for all \(v\). Bidding function \(f\) is meant to represent an arbitrary function from \(\) while \(f_{}\) is meant to represent a function from \(_{t}\), the discrete cover of \(\). We want to have low error when using \(f_{}\) instead of \(f\).

As we discussed in Section 3.1, \(f_{}\) might lead to \(r_{t}(f_{}) r_{t}(f_{})\) if \(f_{}(v_{t}) d_{t}>f(v_{t})\). We solve this issue by ensuring the bids we use never lead to negative reward, using Assumption 3.1. In our learning algorithm, each "action" (e.g. the \([K]\) actions in Theorem 3.5) represents a bidding function \(f_{}_{t}\). However, using the action that represents \(f_{}\) will not lead to bidding \(f_{}(v_{t})\). Instead, when bid \(f_{}(v_{t})\) could lead to negative reward for some \(d_{t}\) (recall \(d_{t}\) is unknown when bidding) then we replace that bid with \(b_{t}^{o}\), the safe bid of that round. Because of Assumption 3.1, we can both calculate when bid \(f_{}(v_{t})\) could lead to negative reward and guarantee that in that case the safe bid will lead to more reward. This trick guarantees that the reward of the action that corresponds to \(f_{}\) is at most \(\{_{t},_{t}\}\) worse than \(r_{t}(f)\). We emphasize that this step is necessary to guarantee bounded discretization error.

After using the above trick, the functions of \(_{t}\) are placed on a tree, with functions that are close in the tree implying that they are close in \(L_{}\) distance. The resulting algorithm is similar to the one in (Grover and Leskovec, 2017). There are two key differences. First, the leaves of the tree suggest bids that use the above modified bids to ensure no-negative reward. Second, each non-leaf node combines the bids of its children to create a new bid distribution by using Theorem 3.5 to adapt to the time-varying ranges and take advantage of the \(\)-good arm. We next present the regret bound we get and defer the algorithm's full description and its proof in Appendix D.3.

**Theorem 3.6**.: _For \([T]\) let \(U_{}=_{t}\{_{t},_{t}\}\) and \(\) be the set of \(L\)-Lipschitz bidding functions for \(L 1\). There exists an algorithm that generates bid distributions \(q_{t}\) that with probability \(1\),_

\[_{f}_{t=1}^{}r_{t}(f)-_{t=1}^{}_{b}q_{t} (b)r_{t}(b)U_{} T.\]

### Reduction from Regret to Interval Regret

We now show how to turn the regret bound of Theorem 3.6 to an interval regret bound. Recall that an interval regret bound is required to apply Theorem 2.1. We achieve the desired result by proving a general reduction from regret to interval regret, for any online learning problem with full information feedback.

Our reduction combines multiple online learning algorithms. Specifically, we consider \(T\) different algorithms. The \(t\)-th algorithm (\(t[T]\)) "starts" in round \(t\) and has low regret in the intervals \([t,t^{}]\), for \(t^{}>t\). For every interval, one of the \(T\) algorithm has low regret but it is unclear how to get a _single_ algorithm with low regret in _every_ interval. We combine the output of each algorithm into a single distribution of action, using another online learning algorithm. For this meta-algorithm, we consider Algorithm 1, with the \(K=T\) actions being the aforementioned \(T\) algorithms. This results in low _interval regret_ with a \(()\) additional error.

Before mentioning our full result, we note one additional technique we have to use. Under the above description, it is unclear how the meta-algorithm handles inactive algorithms, i.e. algorithms that have not been started yet. This is partially resolved by constraining the meta-algorithm to sample only active algorithms. However, this does not decide what is the reward assigned to inactive algorithms (recall the final step of Algorithm 1). We resolve this by assigning them reward equal to the expected reward of the meta-algorithm under only the active algorithms. This reward structure ends up being equivalent to re-sampling an action if a inactive algorithm is sampled. We state the full result next.

**Theorem 3.7**.: _Let \(A\) be a set of actions and \(r_{t}:A[0,U_{t}]\) be the reward function picked by an adaptive adversary. For every \(r_{1}[T]\) let \(_{r_{t}}\) be an algorithm that generates distributions \(q_{t}^{r_{1}}()_{t 1}\), over the actions \(A\) such that for all \(r_{2}_{1}\)_

\[_{a A}_{t[_{1},_{2}]}r_{t}(a)-_{t[_{1},_{ 2}]}*{}_{a_{t}^{a}}[r_{t}(a) ]*{}_{_{1}}(_{2})\]

_Then there is an algorithm that can generate action distributions \(q_{1}(),,q_{T}()\) such that for all \([_{1},_{2}][T]\):_

\[_{a A_{t}[_{1},_{2}]}r_{t}(a)-_{t[_{1},_{2}]} *{}_{a_{t}}[r_{t}(a)] *{}_{_{1}}(_{2})+4U_{_{2}}.\]

The full description of our algorithm is in Algorithm 3, which is in Appendix D.4, along with the proof of Theorem 3.7. Combining the above theorem with Theorem 3.6 gives the following result.

**Corollary 3.8**.: _In the same setting as Theorem 3.6, there is an algorithm that generates distributions over bids \(q_{1}(),,q_{T}()\) such that with probability \(1\), for every \(1_{1}<_{2} T\) it holds_

\[_{f}_{t=1}^{_{2}}r_{t}(f)-_{t=1}^{_{2}}_{b }q_{t}(b)r_{t}(b)U_{_{1}} T\]Corollary 3.8 immediately implies Theorem 3.2 using standard concentration inequalities. We include these calculations in Appendix D for completeness.

## 4. Exact satisfaction of the ROI constraint

In this section we show how we can turn every algorithm that has an approximate satisfaction of the ROI constraint into one with exact satisfaction. This reduction (Lemma 4.2), together with Theorem 3.3, lead to the following theorem.

**Theorem 4.1**.: _In the same setting as Theorem 3.3, there exists an algorithm that always satisfies the budget and ROI constraints and with probability \(1-\) has regret \(( T+)\)._

Theorem 4.1 follows from the following lemma, which combines two algorithms to get the desired reduction from approximate satisfaction of the ROI constraint to an exact one. One algorithm maximizes the objective and has low ROI violation with high probability. The other algorithm is much simpler: it maximizes value minus payment, i.e., the ROI constraint. By running the second algorithm for enough rounds, we can accumulate enough'slack' to mitigate the violation caused by the first algorithm.

**Lemma 4.2**.: _Assume that there is an algorithm \(_{1}\) such that for every \(>0\), with probability at least \(1-\), when running on any set of rounds \(_{1}[T]\) it generates bids that have_

* _regret at most_ \(_{}(|_{1}|)\)_._
* _total ROI violation at most_ \(V_{}(|_{1}|)\)_,_
* _and another algorithm_ \(_{2}\) _such that_
* _its bid_ \(b_{t}\) _in round_ \(t\) _satisfies_ \(1[b_{t} d_{t}](v_{t}-p(b_{t},d_{t})) 0\)_._
* _for every_ \(>0\)_, with probability at least_ \(1-\)_, when run in any set of rounds_ \(_{2}[T]\) _it generates bids_ \(\{b_{t}\}_{t_{2}}\) _such that_ \(_{t_{1}}[b_{t} d_{t}](v_{t}-p(b_{t},d_{t})) Q _{2}(_{2})\)_._

_Consider bidding \(\) on rounds where the remaining budget is less than \(1\), using \(_{1}\) on rounds \(t\) when \(_{t^{} t-1}[b_{t} d_{t}](v_{t}-p(b_{t},d_{t})) 1\), and \(_{2}\) on other rounds. This yields exact satisfaction of the ROI constraint. Moreover, for any \(>0\), with probability at least \(1-2\) it has regret at most \(_{}()+2Q_{}^{-1}(V_{}(T)+2)\) where \(Q_{}^{-1}()\) is the inverse function of \(Q_{}()\)._

The conditions of the two algorithms sketch the lemma's proof (the full proof is in Appendix E, along with the proof of Theorem 4.1). \(_{1}\) violates the ROI constraint by at most \(V(T)\) (for simplicity we drop the dependence on \(\)) and the second algorithm, in order to make up that violation, needs to be run for about \(Q^{-1}(V(T))\) rounds. This has two effects on the total regret. First, \(_{1}\) is run for \(Q^{-1}(V(T))\) fewer rounds, potentially missing out on any reward on those rounds. Second, \(_{2}\) can use at most \(Q^{-1}(V(T))\) of the budget, making the overall algorithm have to stop at most \(Q^{-1}(V(T))\) rounds earlier. This entails the desired bound.

We briefly explain how we get Theorem 4.1 from Lemma 4.2. Theorem 3.3 satisfies the constraints for \(_{1}\). \(_{2}\) is the algorithm of Theorem 3.2 by setting \(_{t}=_{t}=1\) in \(r_{t}()\) of (4). This makes \((T_{1})=V(T_{1})=}(T_{1})\) and \(Q(T_{2})=_{2}-}(})\); the last equality follows from the assumption on \(\), (see Equation (2)).

**Necessity of \(\).** In Appendix E we present an example that shows dependency on \(\) is necessary in our regret bounds. Specifically, as \(\) decreases, the regret of any algorithm that exactly satisfies the ROI constraint increases polynomially in \(1/\). We show this for second-price auctions, to simplify the problem of regret minimization.

**Theorem 4.3**.: _In second-price auctions any algorithm with strict satisfaction of the ROI constraint cannot guarantee regret less than \((1-2)}\) when compared to the optimum LP value (e.g., see (1)) for every constant \(<1/2\)._

## 5. Bandit information

In this section we study online learning in the bandit information setting. After bidding on round \(t\), the bidder does not observe the highest competing bid \(d_{t}\). Instead, she only gets to observe a Boolean value that indicates whether she wins this round, along with the price she paid if she wins, i.e., \(1[b_{t} d_{t}]\) and \(1[b_{t} d_{t}]p(b_{t},d_{t})\). Note that this setting is completely different between first-price and second-price auctions. In second-price, if the bidder wins a certain round, she gets to observe \(d_{t}\) and the same is true whenever \(p(b,d)=qb+(1-q)d\) for \(q>0\). In contrast, when the bidder participates in strictly first-price auctions, she can never observe \(d_{t}\). Next, we show that this is a crucial distinction that makes first-price auctions much harder: any algorithm has to suffer \((T^{2/3})\) regret,in contrast to the \(}()\) regret bounds for second-price auctions (Gardner and Boyd, 2014; Boyd, 2014). To complement the negative result, in Appendix F.2 we offer an algorithm with \(}(T^{3/4})\) regret.

**Regret Lower Bound.** An \((T^{2/3})\) bound is known for quasi-linear utility maximization and no constraints by (Barbieri et al., 2015), which follows from the matching bound of (Barbieri et al., 2015). Instead, we show a lower bound for value maximization with a budget constraint. In particular, our lower bound holds even if the value is fixed across rounds.

**Theorem 5.1**.: _There exists an instance in first-price auctions with no ROI constraint, \(=1/4\), and \(v_{t}=1\) such that, any value-maximizing algorithm with only bandit feedback has regret \((T^{2/3})\)._

The problem described in Theorem 5.1 seems straightforward at first glance: the buyer wants to maximize the number of wins while adhering to the budget constraint. For example, if the CDF of \(d_{t}\) is continuous and strictly increasing, one could consider that the bid \(b^{*}\) such that \(b^{*}\)\([b^{*}>d_{t}]=\), i.e., the bid that depletes the budget in expectation, is optimal. The problem of (approximately) finding such a bid \(b^{*}\) is not hard, since it can be reduced to the problem of noisy binary search (Gardner and Boyd, 2014). However, while this approach would work for second-price auctions, it does not work for first-price auctions. The reason is that our original hypothesis, that a single bid every round is optimal, is erroneous. There are strictly increasing and continuous CDFs for \(d_{t}\) where, to maximize the number of wins while staying within budget, the bidder needs to use two distinct bids. In particular, the value of using the optimal two bids and the single optimal bid can be as big as a factor of \(2\).

In our proof we study a CDF \(F\) for \(d_{t}\) where there are infinite pairs of bids that, if mixed properly, attain the optimal solution. Next, an adversary perturbs \(F\) by moving some mass to some bid \(b^{*}\) from bid \(b^{*}+\), making bid \(b^{*}\) more 'valuable'. This entails that any algorithm that does not use \(b^{*}\) enough faces considerable regret. The proof is completed by arguments similar to the ones in (Barbieri et al., 2015): the bandit information that the bidder receives requires any algorithm to waste multiple rounds using sub-optimal bids before finding \(b^{*}\). We include the full proof in Appendix F.1.

[MISSING_PAGE_FAIL:9]

wang23ao.html
*  Martin Zinkevich. 2003. Online Convex Programming and Generalized Infinitesimal Gradient Ascent. In _Machine Learning, Proceedings of the Twentieth International Conference (ICML 2003), August 21-24, 2003, Washington, DC, USA,_.
*  Tom Fawcett and Nina Mishra (Eds.). AAAI Press, Washington, DC, USA,_ 928-936. [http://www.aaai.org/Library/ICML/2003/exml03-120.php](http://www.aaai.org/Library/ICML/2003/exml03-120.php)
* 

## Appendix A Further Related Work

**Online bidding in truthful auctions**: Feng et al.  study online bidding in sequential truthful auctions under budget and ROI constraints in a stochastic environment. Their algorithm guarantees \(()\) regret with respect to the best bidding sequence and satisfies exactly both the ROI and budget constraints. Their results are an extension of the results of Balseiro and Gur , Balseiro et al.  where they study the same setting without ROI constraints. On the other hand, our paper studies a more general class of (possibly) non-truthful auctions and provides an algorithm that has the same regret guarantee. Balseiro and Gur , Balseiro et al.  also study the adversarial setting, where the value and the highest competing bid are not sampled by a stationary distribution but are picked by an adversary. In this setting, it is impossible to achieve regret that is sublinear in \(T\), so they bound the competitive ratio, i.e. the multiplicative error, instead. Aside from truthful auctions,  also extend these guarantees to settings where the learner gets to observe every parameter of a round before picking a decision, e.g., in auctions observe the highest competing bid before bidding.

**Online bidding without constraints**: Another line of work studies online bidding without constraints.  study quasi-linear utility maximization in first-price auctions, while  study the maximization of arbitrary Lipschitz continuous functions. Both use an algorithm with a tree structure similar to ours; we comment on the similarity/differences in Section 3.3. Balseiro et al.  study contextual online learning, which result into a \((T^{2/3})\) regret bound for quasi-linear utility maximizers in first-price auctions with bandit feedback. More recently, Kumar et al.  develop a \(()\) regret algorithm for first-price auctions with finite number of potential bids; they provide a \(( T)\) regret for stochastic inputs. Kleinberg and Leighton  study online pricing, where the learner wants to learn how to price an item to maximize revenue; one of their results implies that the above regret bound is tight.

**Online learning with budgets**: _Bandits with Knapsacks_ is a class of online learning problems where the learner has a general action space and multiple budget constraints . Bernasconi et al. , Kumar and Kleinberg , Slivkins et al.  study the same setting when the budget can also increase in some rounds.

**Online learning without constraints**: Finally, the problem of online learning without constraints has received extensive attention.  and  are excellent textbooks. The most commonly used algorithms for settings with finite number of actions are Hedge for full information feedback  and EXP3 for bandit feedback . For online convex optimization (where there are infinite number of actions) the most commonly used algorithm is Online Gradient Descent .

## Appendix B Limitation of Pacing multipliers

**Gap between Lipschitz bidding functions and Pacing multipliers.** Consider a value maximizer in first-price auctions with total budget \(T/2\). Assume that her values are either \(1/2\) or \(1\), each with probability \(1/2\). Also assume that the highest competing bid every round is \(1/2\). The best Lipschitz bidding function is to bid \(1/2\) every round: this stays within budget (in expectation) and leads to a total value of \(T\). In contrast, no fixed pacing multiplier can win all rounds and stay within budget. In particular the only pacing multipliers worth considering is bidding half or all of the value. In turns out that the best the bidder can do is bid her value with probability \(1/2\) and half her value with probability \(1/2\), leading to total value \(T\). This proves that the difference between the two benchmarks is \(T\), even in this very simple example.

**ROI Constraint for Pacing Multiplier in first-price auctions.** Another example that showcases the limitation of the best Pacing multiplier benchmark is value maximizing in first-price auctions. Here, an online learner who wants to compete against the best pacing multiplier does not have to consider the ROI constraint at all. This follows because the optimal such bidding would never use a pacing multiplier that would violate that constraint (i.e., bid above the value). In contrast, an optimal general bidding function for value maximization might bid above the bidder's value in some rounds, as long as it wins other rounds where the bid is sufficiently less than the value.

## Appendix C Deferred Text of Section 2

We now present the assumption that  make for their results. They assume that there must exist a distribution \(F()\) of functions such that if the player bids according to \(F\) then for some \( 0\) on expectation: (a) the payment of the player is no more than \(-\), and (b) the value earned by the player is at least the payment plus \(\). Formally, \( F()\):

(6)

The above conditions imply that substituting \(F\) in problem (1), both constraints are satisfied with a slack of \(\). In the absence of a ROI constraint, we would have \(=\). Given (6) and that \(>0\), the regret bounds of  depend on \(\) and become worse as \(\) becomes smaller.

We now prove how (2) implies (6) with \(=\). We require that the function \(f(v)=0\) is included in \(\) and bidding \(0\) guarantees \(0\) payment. Let \(f_{}\) be the bidding function that makes the guarantee in (2). We define \(F\) to be the following distribution of functions: \(f_{}\) with probability \(\) and the \(0\) bid with probability \(1-\). In this case we have

\[&}_{f F}}_{( ,d)}[(u-p(f(v),d))]1[f(v) d] \]

where the first inequality holds by \(1[0 d]p(0,d)=0\) and the second by (2). This proves the first inequality of (6). For the second constraint of (6) we have

\[}_{f F}}_{(a, d)}[pf((v),d)1[f(v) d]]^{}\\ =\ }_{(a,d)}[pf_{}(v),d)1[f_{ }(v) d]]\\ \ (1-)\]

where in the last inequality we used by (2) and \(v 1\). The above two inequalities prove (6) for \(=\), as claimed.

## Appendix D Deferred Proofs and Text of Section 3

In this section we present the deferred proofs and text of Section 3.

### Deferred Proof for safe bid in auctions

Here, we show that any mixture of first and second-price auction satisfies Assumption 3.1 and more specifically Condition (iii).

We start with first-price auctions, i.e., \(p(b,d)=b\). For simplicity, we start with a simpler safe bid than the one we claimed in the main body of the paper, \(b_{t}^{}=0\). Conditions (a) and (b) are obvious. For Condition (c) we notice that \(_{d_{t}}r_{t}(b)<0 b> t\). For such bids \(b\), \(r_{t}(b) 0\) for all \(d_{t}\) which makes Condition (c) follow from \(r_{t}(0) 0\).

We now move to second-price, i.e., \(p(b,d)=d\), where a safe bid is \(b_{t}^{}=\{}_{t}{b_{t}}\}\). Condition (b) follows from definition and Condition (a) follows by calculating \(r_{t}(b_{t}^{})=(_{t}t_{}-_{d}d_{t})^{}\). For condition (c), we notice that \(_{d_{t}}r_{t}(b)<0 b>}{b_{t}}n_{t}\); this is because \(r_{t}(b)<0 b d_{t}>}{b_{t}}n_{t}\) and the last inequality can be satisfied iff \(b>}{b_{t}}n_{t}\). For such, \(r_{t}(b_{t}^{})<r_{t}(b)\) would hold only if the bid \(b_{t}^{}\) could win an auction that \(b\) would not, which is impossible since \(b>b_{t}^{}\); this implies (c).

We note that the above safe bid \(b_{t}^{}\) satisfies something much stronger than Condition (c), as we mentioned in Section 1. For every \(d_{t}\), it holds that \(b_{t}^{}_{b}r_{t}(b)\).

We now focus on the most general case, a mix of first and second price, i.e., \(p(b,d)=qb+(1-q)d\) for some \(q\). Here the safe bid for second-price also works here: \(b_{t}^{}=\{}n_{t},1\}\). In this case \(r_{t}(b_{t}^{})=(1-q)(_{t}t_{}-_{d}t_{})^{}\), which proves Condition (b). Similar to before, \(_{d_{t}}r_{t}(b)<0 b>}{b_{t}}n_{t}\) in which case an analysis like the ones above proves Condition (c).

### Deferred Proofs from Section 3.2

We first prove Theorem 3.5, the main theorem of Section 3.2.

Proof of Theorem 3.5.: We first shift the rewards: since Hedge remains the same if a (possibly time-dependent) constant is added to the rewards, we set for all \(t\), \(a\)

\[r_{t}(a) r_{t}(a)+ U_{t}-_{a^{}[K]}r_{t}(a^{ })\]

This means that now the rewards are \(r_{t}:[K][-(1-)U_{t}, U_{t}]\) and specifically for the good action, \(r_{t}(g) 0\).

Now let \(W_{t}=_{a}(_{t}R_{t-1}(a))\) and \(=}\). Notice that the probability to play action \(a\) in round \(t\) is \(p_{t}(a)=R_{t-1}(a))}{W_{t}}\).

We have that

\[}_{a}(_{t}R_{t}(a)) \] \[= _{a}R_{t-1}(a))}{W_{t}}(_{t}r_{t}( a))\] \[= _{a}p_{t}(a)(_{t}r_{t}(a))\] \[ (_{t}(1-_{t}U_{t})_{a}p_{t}(a)r_{t}(a)+_ {t}^{2}U_{t} U_{t})\] \[= (_{t}(1-)_{a}p_{t}(a)r_{t}(a)+^{2} ) \]

where we get the last equality by substituting \(_{t}=}\) and in order to prove the last inequality we first prove the following proposition.

**Proposition D.1**.: _Let \(X\) be a random variable such that \([X]=x\), \(c_{1} X c_{2}\), with \(c_{2} 0\). Then, for any \(0<|,|c_{2}|\}|}\),_

\[[( X)]( x(1-(c_{2}-c_{1}))+ ^{2}c_{2}(c_{2}-c_{1})).\]

Proof.: Let \(^{2}=[(X-x)^{2}]\) and \(c=\{|c_{1}|,|c_{2}|\}\). We have that

\[[( X)] =\ [((X-x))]( x)\] \[\ (}{c^{2}}(^{ c}-1- c))( x)\] \[\ (}{c^{2}}^{2}c^{2})( x)\] \[=\ (^{2}^{2})( x)\]

where the first inequality follows from Bernstein's inequality9 and the last inequality follows by the fact that since \( c 1\) we get \(^{ c} 1+ c+( c)^{2}\). We now bound

\[^{2}=[(X-x)^{2}]\]

\[[(c_{2}-X)^{2}](c_{2}-c_{1})\,[c_{2}-X]=(c_{2}-c_{1})(c_{2}-x)\]

where the first inequality follows from the fact that \([(X-y)^{2}]\) is minimized when \(y=x=[X]\), i.e., when it is equal to the variance. The second inequality follows from \(c_{2}-X 0\) and \(X c_{1}\). Rearranging proves the proposition. 

Now the inequality in (7) follows from the proposition by setting \(X=r_{t}(a)\) with probability \(p_{t}(a)\), \(c_{1}=-(1-)U_{t}\), and \(c_{2}= U_{t}\) and noticing that \(_{t}=}}=}|,|c_{2}|\}}\).

Taking the logarithm of (7) we get

\[}((_{t}R_{t}(a))}{_{a}( _{t}R_{t-1}(a))})(1-)_{a}p_{t}(a)r_{t}(a)+U\]

or equivalently

\[}(_{t}R_{t}(a))}{K}-}(_{t}R_{t-1}(a))}{K} \] \[(1-)_{a}p_{t}(a)r_{t}(a)+U\]In the above equation we use the fact that

\[}(_{t}R_{t}(a))}{K}}(_{t+1}R_{t}(a))}{K}\]

which follows from the fact that \(_{t+1}_{t}\) and the fact that the function

\[(_{i=1}^{K}x_{i}^{})^{1/}\]

is increasing in \(\) for \(x_{1}>0\). This makes the previous inequality

\[}(_{t+1}R_{t}(a))}{K}-}(_{t}R_{t-1}(a))}{K}\]

\[(1-)_{a}p_{t}(a)r_{t}(a)+U\]

Fix \([T]\). We add the above for all \(t[-1]\) along with (9) for \(t=\) and simplify the telescopic sum to get

\[}(_{}R_{t}(a))} {K}-}(0)}{K} \] \[(1-)_{t[]}_{a}p_{t}(a)r_{t}(a)+U T\]

Using the fact that \(_{a}(_{}R_{t}(a))(_{}R_{t}^{*})\) (where \(R_{t}^{*}=_{a}R_{}(a)\)) and substituting \(_{}=}\) and \(=}\) we get

\[R_{}^{*}-U_{}(1-)_{t[]}_{a }p_{t}(a)r_{t}(a)+U_{} \]

Using the fact that \(R_{}^{*} 0\) (since the reward of the good arm is always non-negative) we can use (10) to prove

\[_{t[]}_{a}p_{t}(a)r_{t}(a)-U_{}}-4U_{}} \]

where we use the fact that \(=} 1/2\) since \(}\). We rearrange the terms in (10) to get

\[R_{}^{*}-_{t[]}_{a}p_{t}(a)r_{t}(a)  2U_{}}-_{t[]} _{a}p_{t}(a)r_{t}(a)\] \[ 2U_{}}+4U_{}}\] \[ 4U_{}}\]

where for the second inequality we used (11) and for the final inequality we used \(\). This proves the theorem. 

We now state and prove the high probability version of Theorem 3.5.

**Theorem D.2**.: _In the same setting as Theorem 3.5, Algorithm 1 guarantees the following high probability bound: for every \(>0\) probability at least \(1-\), it holds that_

\[[T]:_{a[K]}_{t[]}r_{t}(a)- _{t[]}r_{t}(a_{t})\] \[ 4U_{}}+\{ },(T/)\}\]

The high probability bound does not follow from a simple application of the Azuma-Heffoffding inequality, since the range of \(_{t}r_{t}(a_{t})\) can be \((UT)\) making the resulting error \(O(U})\) and not \(O(})\) like in the above. Instead, we use Freedman's inequality, which offers a bound based on \(_{t}r_{t}(a_{t})\) which we prove is \(O(U^{2}T)\). This allows us to get the improved dependence on \(\).

Proof of Theorem D.2.: For every \(t\), let \(X_{t}=_{a}p_{t}(a)r_{t}(a)\) and \(Y_{t}=_{t=1}^{T}(X_{t}-r_{t}(a_{}))\). The theorem follows by showing that for every \(>0\)

\[[[T]:Y_{} 4U_{}\{},(T/)\}] 1-. \]

We are going to use Freedman's inequality [15, Theorem 1.6] on the sequence \(Y_{0}\), \(Y_{1}\), \(\) which we first prove is a martingale with respect to the the history of the rounds (we denote with \([t]\): the expectation conditioned on the history of the rounds up to \(t\), i.e., the actions that the player and the adversary has take up to \(t\)): for every \(t 1\)

\[_{t-1}[Y_{t}-Y_{t-1}]=_{t-1}[X_{t}-r_{t}(a_ {t})]=0\]

where the last inequality holds because \(a_{t}=a\) with probability \(p_{t}(a)\). This proves the martingale property. We now notice that \(|Y_{t}-Y_{t-1}| U_{t}\) and that

\[_{t-1}[(Y_{t}-Y_{t-1})^{2}]\] \[= _{t-1}[(X_{t}-r_{t}(a_{t}))^{2}]\] \[ _{t-1}[( U_{t}-r_{t}(a_{t}))^{2}]\] \[ U_{t}\,_{t-1}[ U_{t}-r_{t}(a_{t})]=  U_{t}^{2}-U_{t}X_{t} \]

where we notice that in the first inequality we use the fact that \(_{t-1}[r_{t}(a_{t})]=X_{t}\) and that \(_{t-1}[(X_{t}-r_{t}(a_{t}))^{2}]\) is the conditional variance of \(r_{t}(a_{t})\), which means that \(_{t-1}[(c-r_{t}(a_{t}))^{2}]\) is minimized when \(c=_{t-1}[r_{t}(a_{t})]=X_{t}\). For the second inequality we used that \(-(1-)U_{t} r_{t}(a) U_{t}\). We now have that

\[_{t[]}_{t-1}[(Y_{t}-Y_{t-1})^{2}]\] \[ _{t[]}( U_{t}^{2}-U_{t}X_{t})=U_{ }^{2}T-U_{}_{t[]}X_{t}\] \[ U_{}^{2}T-U_{}(_{a[K]}_{t=1}^{T}r_ {t}(a)-4U_{}})\] (Theorem 3.5) \[ U_{}^{2}T+4U_{}^{2}} r_{t}(g) 0\] \[ 3U_{}^{2}T\] \[ 3U_{}^{2}T. \]

Now using Freedman's inequality gives us that for all \(>0\)

\[[Y_{}<] 1-(- /2}{3U_{}^{2}T+U_{}/3}) \]

Let \(>0\) such that

\[=U_{}\{},(1/)\}. \]

This and a union bound over all \(\) proves the lemma as long as we prove that

\[/2}{3U_{}^{2}T+U_{}/3}( 1/) \]

or equivalently

\[^{2} 6U_{}^{2}T(1/)+U_{} (1/) \]The above inequality is true because, by the definition of \(\) in (12),

\[^{2} 12U_{t}^{2}T(1/) U_{t}(1/)\]

Multiplying the second inequality with \(\) and adding them gives us the desired bound on \(^{2}\). 

### Deferred Proof and Algorithm of Section 3.3

As we mention at the beginning of Section 3, we focus on subsets of \(\) that approximate \(\) and have finite cardinality. We use a slightly different notation and define the sets \(_{0},_{1},,_{M}\) (for some \(M\)) such that for every \(i=0,,M\) it holds that

\[ f, f^{}_{1}: f( ) f^{}() f()+2^{-i},\; . \]

As we mentioned in Section 3.3, instead of directly using a function \(f_{M}_{M}\) and its bid \(f_{M}(v_{t})\) in round \(t\), we modify that bid, since for certain values of \(d_{t}\), \(r_{t}(f_{M})\) might be negative. This is where we use Assumption 3.1 and define

\[b_{t}^{f_{M}}=f_{M}(v_{t}),&_{d_{t}}r_{t} (f_{M}(v_{t})) 0\\ b_{t}^{},&_{d_{t}}r_{t}(f_{M}(v_{t}))<0 \]

where \(b_{t}^{}\) is as defined in Assumption 3.1, depending on the payment function. \(b_{t}^{f_{M}}\) is never worse than \(f_{M}(v_{t})\), since \(r_{t}(b_{t}^{f_{M}}) r_{t}(f_{M})\) and \(r_{t}(b_{t}^{f_{M}}) 0\), which follow from Assumption 3.1. This guarantees that our reward is always non-negative, which as we discussed in Section 3.1 is the key to bidding according to \(_{M}\) and have \(O(2^{-M})\) error compared to using the full \(\).

We use the functions in \(_{0},_{1},,_{M}\) to create a tree. For \(i=0,,M\) the \(i\)-th level of the tree has \(|_{1}|\) nodes, each representing an \(f_{i}_{i}\). We connect these nodes by defining each node's parent: for every \(i=1,,M\) and \(f_{i}_{1}\), \(P(f_{i})\) is the parent of \(f_{i}\) such that \(P(f_{i})_{1-1}\) and \(\|f_{i}-P(f_{i-1})\|_{} 2^{i+1}\). We note that such a \(P(f_{i})\) always exists because of (13). The parent function \(P()\) subsequently defines the children \(C(f_{i})\) and leaves \((f_{i})\) of a node \(f_{i}\), \(i=0,,M-1\). Finally, we assume that \(|_{0}|=1\) (which does not violate (13)), making the tree have a unique root.

Using this tree, in every round \(t\) the algorithm creates a distribution of bids in the following way. First, each leaf \(f_{M}_{M}\) calculates the bid \(b_{t}^{f_{M}}\) as defined in Eq. (14); this bid defines a trivial bid distribution \(q_{t}^{f_{M}}()\) which suggests the bid \(b_{t}^{f_{M}}\) with probability \(1\). On the \(i\)-th level where \(i<M\), each non-leaf node \(f_{i}_{i}\) uses a distribution \(p_{t}^{f_{i}}()\) over its children \(C(f_{i})\) and the bid distribution \(q_{t}^{f_{M}}()\) of each child \(f_{i+1} C(f_{i})\) to define its own bid distribution \(q_{t}^{f_{i}}()\). This node \(f_{i}\) runs an instance of Algorithm 1 to calculate the distribution over its children \(p_{t}^{f_{i}}()\). This recursive calculation defines the bid distribution of the root, \(q_{t}^{f_{i}}()=q_{t}()\), which is the output of the algorithm.

We show the full algorithm in Algorithm 2, which has one slight modification compared to the process described above: each non-leaf node considers one more bid in addition to the bids suggested by its children. This bid is the maximum bid suggested by any of its leaves. This additional bid is key to the guarantee of our algorithm since it is the action that is \(\)-good (recall Definition 3.4). This allows us to use the improved regret bound of Theorem 3.5 which is crucial: a node on the \(i\)-th level can have as many as \(((2^{i}))\) children, which, if \(\) was a constant, would lead to terrible regret bounds. Instead, we show that the additional action makes \( 2^{-i}\) which makes the regret of every non-leaf node \(()\).

```
0: Number of rounds \(T\), Lipschitz parameter \(L\), number of levels of tree \(M\)
0: For every \(i=0,1,,M\), let \(_{i}\) be as described in (13)
1: // Description of tree structure
1: Let \(P(f_{i})_{i-1}\) such that \(\|P(f_{i})-f_{i}\|_{} 2^{i+1}\), \( i=1,,M\),
2:\(f_{i}_{i}\) // Parent of \(f_{i}\)
3:for\(i=M-1,M-2,,0\) and \(f_{i}_{i}\)do
4: Let \(C(f_{i})=\{f_{i+1}_{i+1}:f_{i}=P(f_{i+1})\}\) // Children of \(f_{i}\)
5: Let \((f_{i})=_{f_{i+1}(f_{i})}(f_{i+1})\) // Leaves of \(f_{i}\)
6: Let \((f_{i})\) be an instance of Algorithm 1, \(=2^{i+1}\) and
7:\(K=\|C(f_{i})\|+1\) // Algorithm for \(f_{i}\), with actions its children and the good action
8:endfor
9:for\(t[T]\)do
10: Receive \(v_{t}\), \(v_{t}\), \(_{t}\) and calculate \(U_{t}=_{r}\{_{r},_{r}\}\) // \(U_{t}\) is the upper bound of rewards
11: For every \(f_{M}_{M}\) calculate \(b_{t}^{f_{M}}\) as in Eq. (14) // Improve bid \(f_{M}(v_{t})\) using safe bid
12: and let \(q_{t}^{f_{M}}()\) be the bid distribution that bids \(b_{t}^{f_{M}}\) with probability \(1\) // \(f_{M}\)'s suggestion
13:// Recursive construction of bid distributions
14:for\(i=M-1,M-2,,0\) and \(f_{i}_{i}\)do
15: Calculate \(g_{t}^{f_{M}}=_{f_{M}(f_{i})}p_{t}^{f_{M}}\) // \(f_{i}\)'s good bid
16: Get \(p_{t}^{f_{i}}()\) from \((f_{i})\) after passing \(U_{t}\) // Distribution over \(C(f_{i})\{g_{t}^{f_{i}}\}\)
17: Calculate bid distribution \(q_{t}^{f_{i}}()\) over bids \(\{p_{t}^{f_{M}}\}_{f_{M}(f_{i})}\)
18: where \(q_{t}^{f_{i}}(p_{t}^{f_{M}})\) equals
19:endfor
20: Sample and use bid \(b_{t} q_{t}^{f_{i}}()=q_{t}()\) // Sample bid
21: according to the root
22: Receive \(d_{t}\) // Get \(d_{t}\), making \(r_{t}()\) calculable
23:// Update algorithms of non-leaf nodes
24:for\(i=0,1,,M\) and \(f_{i}\)do
25: Let \(r_{t}(g_{t}^{f_{i}})=r_{t}(g_{t}^{f_{i}})\) // Reward of good bid
26: Let \(_{t}(f_{i+1})=_{b}q_{t}^{f_{i}}(b)r_{t}(b)\), for every \(f_{i+1}_{t+1}\)
27:// Reward of each child of \(f_{i}\)
28: Pass above \(()\) to \((f_{i})\) // Update \(f_{i}\)'s algorithm
29:endfor
30:endfor
31:
```

**ALGORITHM 2** Tree algorithm

We now use Algorithm 2 to prove Theorem 3.6.

Proof of Theorem 3.6.: Fix \(M=_{2}\). We first make the observation that Algorithm 2 is well defined: in order to calculate \(b_{t}^{f_{M}}\) for every \(f_{M}_{M}\) we only need knowledge of \(q_{t}\), \(_{t}\), \(_{t}\) and not \(d_{t}\), as explained in Assumption 3.1. We also note that the rewards\(_{t}()\) that are fed into each \((f_{i})\) are in the range \([0,U_{t}]\) and \(U_{1} U_{2}\), as needed for the guarantee of Theorem 3.5. The lower bound for the rewards comes from the fact that every bid used in round \(t\) is \(b_{t}^{fat}\) for some \(f_{M}_{M}\), which because of (14) and Assumption 3.1 guarantees non-negative reward. The upper bound follows from the definition of \(r_{t}()\) and the fact that values and bids are in \(\).

We now show that for every \(i<M\) and \(f_{i}_{i}\), the good bid, \(g_{t}^{f}\), is \(2^{+i+3}\)-good with respect to the bids used by \(f_{i}\), i.e. \(\{b_{t}^{fat}\}_{fit(f_{i})}\). We prove that

\[r_{t}(g_{t}^{f_{i}}) r_{t}(b_{t}^{fat})-2^{-i+3}U_{ t}, f_{M}(f_{i}) \]

which implies \(_{t}(g_{t}^{f_{i}})_{t}(f_{i+1})-2^{-i+3}U_{t}\) for all \(f_{i+1}(f_{i})\), since \(_{t}(g_{t}^{f_{i}})=r_{t}(g_{t}^{f_{i}})\) and \(_{t}(f_{i+1})\) is a convex combination of \(\{r_{t}(b_{t}^{fat})\}_{fit(f_{i})}\).

Fix \(f_{M}(f_{i})\). We distinguish two cases to prove (15) for this \(f_{M}\):

* If \(b_{t}^{fat}\) loses the auction in round \(t\) (\(b_{t}^{fat}<d_{t}\)), then \(r_{t}(b_{t}^{fat})=0\) and (15) follows from \(r_{t}(g_{t}^{f_{i}}) 0\).
* If \(b_{t}^{fat}\) wins the auction in round \(t\), then \(g_{t}^{f_{i}} b_{t}^{fat}\) and therefore \(g_{t}^{f_{i}}\) also wins the auction in round \(t\). This means that \(1[b_{t}^{fat} d_{t}]=1[g_{t}^{f_{i}} d_{t}]=1\) and so, in order to prove (15) we have to prove that the payment of \(g_{t}^{f_{i}}\) is not more than the payment of \(b_{t}^{fat}\) plus \(2^{-i+3}\). The last statement follows from the Lesheltziness of \(p(,d_{t})\) (Assumption 3.1) and the fact that \(|g_{t}^{f_{i}}-b_{t}^{fat}| 2^{-i+3}\) which follows by the following: For every \(f_{M},f_{M}^{}(f_{i})\) it holds that \(\|f_{M}-f_{M}^{}\|_{} 2^{-i+3}\) since \[\|f_{M}-f_{M}^{}\|_{}_{j=0}^{M-i-1}\|p^{ j+1}(f_{M})-p^{j}(f_{M})\|_{}\] \[_{j=0}^{M-i-1}2^{-M+j+2} 2^{-i+2}\]

where \(P^{j}()\) is the application of the parent function \(P\)\(j\) times, the first inequality uses the triangle inequality, and the second uses the definition of the the parent function \(P\). The fact that \(\|f_{M}-f_{M}^{}\|_{} 2^{-i+3}\) follows from the above using the triangle inequality and the fact that \(|g_{t}^{f_{i}}-b_{t}^{fat}| 2^{-i+3}\) follows from the fact that \(g_{t}^{f_{i}}=b_{t}^{fat}\) for some \(f_{M}^{}(f_{i})\).

Now we summarize the setting of each algorithm \((f_{i})\), for \(f_{i}_{i}\), \(i<M\):

* The reward range is \([0,U_{t}]\) in round \(t\), where \(U_{t}=\{_{f_{i}},_{t}\}\).
* In every round there is an action that is \(_{t}\)-good, where \(_{t}:=2^{t+3}\).
* There are at most \(K_{t}\) actions, where \(K_{t}:=(C_{}D2^{t+1})+1(C_{}D2^{t+2})\) where last inequality holds because \(C_{}L 1\).

Let \(_{T}(f_{i})=_{t=1}^{T}_{t}(f_{i})\) denote the total reward of algorithm \((f_{i})\) and similarly define \(_{T}(g_{t}^{f_{i}})\) the total reward of the good bids of \((f_{i})\). Because of the guarantee of each algorithm (Theorem 3.5) we have that with probability \(1\):

\[_{f_{M}(f_{i})\{g^{f_{i}}\}}_{T}(f_{i+1})-_ {f_{M+1}(f_{i})\{g^{f_{i}}\}}p_{t}^{f_{i}}(f_{i+1}) _{T}(f_{i+1}) \] \[ 4U K_{t}} 23U}LT}\]

We now bound the error because we bid according to the bidding functions \(_{M}\) and not \(\). For any \(f\) let \(f_{M}_{M}\) be such that \(f_{M} f\) and \(\|f-f_{M}\|_{} 2^{-M}\). For every round \(t\) we have

\[r_{t}(f)\] \[= 1[f(_{t}) d_{t}](_{t}_{t}- _{t}p(f(_{t}),d_{t}))\] \[ 1[f_{M}(_{t}) d_{t}](_{t}_{t}- _{t}p(f(_{t}),d_{t}))^{+}\] \[ 1[f_{M}(_{t}) d_{t}](_{t}_{t}- _{t}p(f_{M}(_{t}),d_{t}))^{+}+U_{t}Z^{-M}\] \[= (r_{t}(f_{M}))^{+}+U_{t}Z^{-M}_{t}(f_{M})+U _{t}Z^{-M}\]

where in the first inequality we used that \(f_{M} f\), in the second inequality that \(p(,d_{t})\) is \(1\)-Lipschitz, in the third that \(\|f-f_{M}\|_{} 2^{-M}\), and in the final one that \(_{t}(f_{M})(r_{t}(f_{M}))^{+}\); recall that \(_{t}(f_{M})=r_{t}(b_{t}^{fat})\).

The above implies

\[_{f}_{t=1}^{T}r_{t}(f)_{f_{M}_{M}} _{t}(f_{M})+UT2^{-M} \]

Let \(f_{M}^{}\) be a maximizer of the r.h.s. in the inequality above and for every \(i<M\), let \(f_{i}^{}\) be the ancestor of \(f_{M}^{}\) in the \(i\)-th level. Using this notation we prove

\[_{f}_{t=1}^{T}r_{t}(f)-_{T}(f_{0})  UT2^{-M}+_{T}(f_{M}^{})-_{t}(f_{0})\] \[= UT2^{-M}+_{j=0}^{M-1}(_{T}(f_{j+1}^{ })-_{T}(f_{j}^{}))\] \[ UT2^{-M}+_{j=0}^{M-1}23U}LT}\] \[= UT2^{-M}+23UM}LT}\]

where the first inequality holds by (17) and the second one by (16). Picking \(M=_{2}\) the above becomes

\[_{f}_{t=1}^{T}r_{t}(f)-_{T}(f_{0}) 2U+U}LT} T\]

The above is the claimed regret bound since \(_{t}(f_{0})=_{b}q_{t}^{f_{0}}r_{t}(b)\).

### Deferred Proofs and Algorithm of Section 3.4

We first present in Algorithm 3 the reduction from standard no-regret to no interval regret. We use that to prove Theorem 3.7.

**Input:** Number of rounds \(T\), action space \(A\), algorithms \(\{_{}\}_{r[T]}\) over action space \(A\)

Initialize an instance \(\) of Algorithm 1 with \(=1\) and \(K=T\)

for\(t[T]\)do

Receive reward range \([0,U_{t}]\)

for\(r_{1} t\)do

Pass \(U_{t}\) to \(_{_{t}}\) and receive \(q_{t}^{_{1}}()\) // Distribution over actions of \(_{_{t}}\)

end

Pass \(U_{t}\) to \(\) and receive \(p_{t}()\) // Distribution over algorithms of \(\)

Calculate for every action \(a\): \(q_{t}(a)=p_{t}(r_{1})q_{t}^{_{1}}(a)}{_{T t}p_ {t}(r_{1})}\)

// Distribution of actions by sampling an algorithm

\(_{_{t}},t_{1}\) and then an action from \(q_{t}^{_{1}}()\)

Sample and output \(a_{t} q_{t}()\)

Receive function \(r_{t} A[0,U_{t}]\)

Pass \(r_{t}()\) to \(_{}\) for \(r_{1} t\) // \(_{_{t}}\) internal update

Calculate \(_{t}(_{1})=_{a} q_{t}^{_{1}}[r_{t}(a) ]\) for \(r_{1} t\) // Expected reward of \(q_{t}^{_{1}}()\)

Calculate \(_{t}^{}=_{a q_{t}}[r_{t}(a)]\) // Expected reward of \(q_{t}()\)

Update \(\) with reward \(_{t}(_{1})\) for \(r_{1} t\) // \(\) update for active algorithms

and with reward \(_{t}^{}\) for \(r_{1} t\) // \(\) update for inactive algorithms

end

end

**Algorithm 3** Reduction from regret to interval regret

Proof of Theorem 3.7.: We first extend the definition of \(_{t}()\) for inactive algorithms. This makes

\[_{t}(_{1})=_{a q_{t}^{_{1}}}[ r(t)],&r_{1} t\\ _{t}^{},&r_{1}>t.\]

Now we re-write \(_{t}^{}\), the expected reward if the action is sampled according to \(q_{t}()\):

\[_{t}^{}=_{a q_{t}}[r_{t}(a)]= t}p_{t}(_{1})_{_{1} t}(p_{t}(_{1}) _{t}(_{1}))}. \]

We notice that \(_{t}()\) has the same reward range as \(r_{t}()\). Theorem 3.5 for algorithm \(\) gives us a regret guarantee by every round \(_{2}\):

\[_{_{1}[_{1}]}_{t[_{2}]}_{t}(_{1})-_{ t[_{2}]}_{_{1}[T]}p_{t}(_{1})_{t}(_{1}) 4U_{ _{2}}\]

which implies that for all intervals \([_{1},_{2}][T]\)

\[_{t[_{2}]}_{t}(_{1})-_{t[_{1}]}_{_{1 }[T]}p_{t}(_{1})_{t}(_{1}) 4U_{_{2}} \]

We are going to show that (19) implies our theorem. First we prove that for every round \(t\),

\[_{_{1}[T]}p_{t}(_{1})_{t}(_{1}) =_{_{1} t}p_{t}(_{1})_{t}(_{1})+ _{_{1}>t}p_{t}(_{1})_{t}^{}\] \[=_{t}^{}_{_{1} t}p_{t}(_{1})+ _{t}^{}_{_{1}>t}p_{t}(_{1})\] \[=_{t}^{}\]

where the second equality holds by (18).

The above and the fact that \(_{t}(_{1})=_{t}^{}\) for \(r_{1}>t\) makes (19) imply that for all intervals \([_{1},_{2}][T]\)

\[_{t_{1}}_{t}()+_{t[_{1},_{2}]} {r}_{t}(_{1})-_{t[_{2}]}_{t}() 4U_{_{2}}\]

or equivalently that for all such intervals \([_{1},_{2}][T]\)

\[_{t[_{1},_{2}]}_{t}(_{1})-_{t[_{1}, _{2}]}_{t}() 4U_{_{2}}.\]

Given the regret bound of each algorithm \(_{}\) by round \(_{2}\) the above implies that for all intervals \([_{1},_{2}][T]\)

\[_{a}_{t[_{1},_{2}]}r_{t}(a)-_{t[_{1},_{2}]} _{t}() 4U_{_{2}}+_{_{1}}(_{2}).\]

which is the desired regret bound. 

Using a simple concentration inequality and the union bound, we prove Theorem 3.2 from Corollary 3.8.

Proof of Theorem 3.2.: Fix \(1_{1}<_{2} T\). For \(t[_{1},_{2}]\) define \(X_{t}=r_{t}(b_{t})-_{b}q_{t}(b)r_{t}(b)\) and \(M_{t}=_{r[_{1},t]}X_{t}\). We notice that the sequence \(M_{t}\) is a martingale with respect to the history of the previous rounds \(_{t-1}\) (player's and adversary's decisions): for every \(t\)

\[[M_{t}-Mt-1]|_{t-1}|= [X_{t}|_{t-1}]=0\]

In addition we notice that \(|X_{t}| U_{t}\) since \(r_{t}(b)[0,U_{t}]\). This allows us to use Azuma's inequality, proving that for every \(\), with probability at least \(1-\) it holds

\[M_{t_{2}}-U_{_{2}}-_{1})(1/)}\]

which implies that with probability at least \(1-\)

\[_{t=_{1}}^{_{2}}r_{t}(b_{t})-_{t=_{1}}^{_{2}}_{b}q_ {t}(b)r_{t}(b)-U_{_{2}}\]

Using the union bound over all \(1_{1}<_{2} T\) we get that for every \(\) with probability at least \(1-\) it holds that for all \(1_{1}<_{2} T\)

\[_{t=_{1}}^{_{2}}r_{t}(b_{t})-_{t=_{1}}^{_{2}}_{b}q_ {t}(b)r_{t}(b)-U_{_{2}}}{}}\]

Using Corollary 3.8 we get the theorem. 

## Appendix E Deferred Proofs of Section 4

We first prove the reduction of how to turn approximate ROI satisfaction to an exact one.

Proof of Lemma 4.2.: We first notice that since \(_{2}\) never has value less than payment and \(_{1}\) is run only when the accumulated value is at least \(1\) higher than the payment, the ROI constraint is never going to be violated. Now we need to prove the total value guarantee.

Assume that the high probability bounds of the two algorithms are true (which happens with probability at least \(1-2\) for any \(\) due to the union bound). Let \(r\) be the last round when algorithm \(_{2}\) is run. Let \(_{1}\) be the rounds up to \(r\) where algorithm \(_{1}\) is run and \(_{2}\) be the rounds up to \(r\) where algorithm \(_{2}\) is run; note that \(|_{2}|\) is the total number of rounds \(_{2}\) is run in total. We now have

\[_{t[_{1}]}1\{b_{t} d_{t}\}(o_{t}-p(b_{t},d_{t}) ) Q_{}(|_{2}|)-V_{}(|_{1}|)\]where the inequality follows from the ROI guarantees of the two algorithms. Using the fact that on round \(\) we run \(_{2}\) which upper bounds the above quantity by \(2\) we get

\[Q_{}(|_{2}|) 2+V_{}(|_{1}|) 2+V_{ }(T)\]

Using the definition that \(Q_{}^{-1}(V_{}(T,))\) is the solution to \(Q_{}()=V_{}(T)\) we get

\[|_{2}| Q_{}^{-1}(V_{}(T))\]

This proves that the total number of rounds \(_{1}\) was run is at least \(T-Q_{}^{-1}(1+V(T))\). This means that the total regret is at most

\[_{}T-Q_{}^{-1}(1+V(T))+2Q_{}^{-1}( 1+V_{}(T))\]

where the second term represents the rounds \(_{2}\) was run instead of \(_{1}\) and the loss because of the budget consumption of \(_{2}\), which is at most \(Q_{}^{-1}(V_{}(T))\) making the overall algorithm run out of budget \(Q_{}^{-1}(V_{}(T))\) rounds earlier, missing out on that much utility. 

We now prove Theorem 4.1.

Proof of Theorem 4.1.: As we explained in Section 4 there are algorithms that satisfy the assumptions of Lemma 4.2 with

\[_{}(T)=V_{}(T)=\, \, T+\]

and

\[Q_{}(T_{2})= T_{2}-} T_{2}+(T_{2}/)}=( T_{2})\]

where the last inequality follows from

\[=}\, T+} \]

since otherwise the regret statement is vacuous.

The theorem follows by algebraic calculations. 

We now prove Theorem 4.3.

Proof of Theorem 4.3.: Each round the value/highest-competing-bid distribution is the following

\[(v_{t},d_{t})=(1,0),&\\ (,1),&1-\]

It is not hard to see that the LP optimum holds \(1\) every round and on expectation has reward \(=1-\) and ROI violation of \(0\).

Let \(R_{t}\) be the cumulative "ROI amount" that any algorithm has collected in round \(t\), i.e. the total value minus the total price paid. Because the algorithm needs to satisfy the ROI constraint with probability \(1\), for all rounds it must hold that \(R_{t} 0\). This means that the best any algorithm can do is to bid \(1\) in round \(t\) if \(R_{t-1}\) (thus guaranteeing to always win) or bid less than \(1\) if \(R_{t-1}<\) (thus guaranteeing to win only if \(d_{t}=0\)).

Let \(N_{t}\) be the number of rounds the algorithm bid less than \(1\) and missed an item with \(d_{t}=1\). The regret of the algorithm is \(N_{T}\) so we need to calculate \([N_{T}]\).

Let \(R_{t}^{}\) be the cumulative "ROI amount" if the algorithm won every item. We notice that

\[R_{t}-R_{t}^{}=N_{t}\]

since the difference does not change from round to round if the algorithm wins the item but increases if the the algorithms misses a high value item. Since \(R_{t} 0\) and it holds that

\[N_{T}=_{t}N_{t}-_{t}R_{t}^{}\]

We now note that \(R_{t}^{}\) is an unbiased random walk, since

\[R_{t}^{}-R_{t-1}^{}=1,&\\ ,&1-\]

and \([R_{t}^{}-R_{t-1}^{}]=0\) and \(R_{t}^{}-R_{t-1}^{}=\). We will show that

\[[_{t}R_{t}^{}]-},\]

which implies that the expected regret of the algorithm wrt to the LP optimum is

\[(1-2)}\]

which proves the theorem.

We now prove that \([_{t}R_{t}^{}]-}s\). We have that

\[[_{t=0,,T}R_{t}^{}] =[_{t=0,,T}\{0,R_{t}^{} \}]\] \[[\{0,R_{T}^{}\}]\] \[=-[|R_{T}^{}|]\] \[-[|(Q})||]\] \[=-}}\]

where the second equality holds because \([R_{T}^{}]=[\{0,R_{T}^{} \}]+[\{0,R_{T}^{}\}]=0\) and \([R_{T}^{}]=[\{0,R_{T}^{} \}]-[\{0,R_{T}^{}\}]\), the next one by the Central Limit Theorem for large \(T\), and the last equality uses standard facts of \(G(0,})\), the \(0\)-mean Gaussian with standard deviation \(}\).

We note that instead of using the central limit theorem one could explicitly use the fact that \(R_{T}^{}=M1+-T\) where \(M\) is a binomial random variable with \(T\) tries and probability of success \(b\) to calculate

\[[R_{T}^{}] =\] \[= }- {1}{}\]

which leads to a similar bound.

Deferred Proofs and Algorithm of Section 5

We first present in Appendix F.1 the deferred proof of Theorem F.4, the lower bound on the regret for budgeted first-price auctions with only bandit information. Next we present an upper bound on the regret, in Appendix F.2.

### Deferred Proof for Regret Lower Bound

In this section we prove Theorem 5.1.

Proof of Theorem 5.1.: Fix the player's budget, \(=1/4\). Define the following

* \(K=2T^{1/3}\) for some positive \(c\) that will be defined later.
* \(=\).
* For \(i=0,1,2,,K\) let \(d_{i}=1/3+i\). Note \(d_{0}=1/3\) and \(d_{K}=2/3\).

We will consider that \(d_{i}\) can only be \(0\), \(1\), or one of the values in \(\{d_{i}\}_{i=0,,K-1}\); note that even though we defined the value \(d_{K}\), \(d_{i}\) cannot take that value. We will consider different distributions that can generate \(d_{i}\), each specified by a CDF. We first define the base CDF-like distribution:

\[F_{}(x)= \]

We use \(_{}\) and \(_{}\) to denote the probability and expectation when \(d_{i}\) is generated by \(F_{}\), which means that for any \(x=0,d_{0},d_{1},,d_{K-1}\), \(_{}[d_{i} x]=F_{}(x)\) and \(_{}[d_{i} 1]=1\). More precisely, even though we do not use the description of the probability density function, we have

\[_{}[d_{i}=x]=F_{}(0)=3/4&, { if }x=0\\ F_{}(d_{0})-F_{}(0)=3/44&,x=d_{0}=1/3\\ F_{}(d_{i})-F_{}(d_{i-1})=()&,x=d_{i},i[K-1]\\ 1-F_{}(d_{K-1})&,x=1\\ 0&,.\]

We note that \(F_{}(d_{K-1})<F_{}(d_{K})=1/10\); this guarantees that our distribution is well defined.

Given the base CDF \(F_{}\) we now define a different CDF-like function, \(F_{j}\) for every \(j=0,1,,K-1\). The distribution of \(d_{i}\) associated with \(F_{j}\) is going to be identical to the one associated with \(F_{}\), except for the probability of

\[F_{j}(x)=F_{}(x),&x[d_{j},d_{j+1})\\ F_{}(d_{j+1}),&x[d_{j},d_{j+1}) \]

We use \(P_{j}\) and \(E_{j}\) to denote the probability and expectation when \(d_{i}\) is generated by \(F_{j}\). This means that \(_{j}[d_{i} x]=_{}[d_{i} x]\) for all \(x=0,d_{0},d_{1},,d_{j-1},d_{j+1},,d_{K-1},1\) and \(_{j}[d_{i} d_{j}]=_{}[d_{i} d _{j+1}]\).

We are going to assume that before round \(t\), \(j\) is picked adversarially. We now prove what a lower bound on the value of the optimal solution under \(F_{j}\).

**Lemma F.1**.: _When \(d_{i}\) is generated according to \(F_{j}\) for any \(j=0,,K-1\) as explained above, then the value of the optimal solution is_

\[_{j}T+T\]

Proof.: We consider the strategy that bids \(d_{j}\) with probability \(\) and bids \(0\) otherwise. The expected per round payment of this strategy is

\[d_{j}F_{j}(d_{j})= {11-3(i+1)}{12(1+3i)}(+i)+(i+1))}\] \[=\]

which means it satisfies the budget constraint in expectation, since \(=1/4\). The expected per-round value of this solution is

\[F_{j}(d_{j})+1- F_{j}(0)\] \[=+(i+1))}+1- \] \[=++\]

where the last inequality follows from \(i K\). Substituting \(=\) we get the lemma. 

In round \(t\) the player bids \(b_{t}\) and observes \(x_{t}\{0,1\}\) (if she won or not). We are going to assume that the player runs some deterministic algorithm, which comes w.l.o.g. since the environment is randomized. This means that the player's bid \(b_{t}\) in round \(t\) is a deterministic function of \(x_{1},,x_{t-1}\). We denote \(x=x_{1:T}\). We also assume w.l.o.g. that the player's bids are always one of \(0,d_{0},,d_{K-1},1\), since any other bid is suboptimal. Let \(N_{i}\) be the total number of times the player bids \(d_{i}\) and \(M_{i}\) be the total number of times the player bids \(d_{i}\) and wins. We prove the following.

**Lemma F.2**.: _Let \(g:\{0,1\}^{T}\) be a function defined on \(x\). Then, for every \(j\), it holds that_

\[|_{j}\,g(x)-_{}\,g(x)|(_{x }|g(x)|)_{}[M_{j}]}\]

Proof.: For any \(j\) and \(i\) we have

\[|_{j}\,f(x)-_{}\,f(x)| =|_{x\{0,1\}^{T}}f(x)(_{j}[x ]-_{}[x])|\] \[(_{x}|f(x)|)_{x\{0,1\}^{T}}| _{j}[x]-_{}[x]|\] \[=(_{x}|f(x)|)|_{j}-_{0}|_{1} \]

where \(_{j}\) and \(_{0}\) are the probability distributions on \(x\) given that \(d_{t}\) is sampled from \(F_{j}\) and \(F_{}\), respectively. Using the properties of the KL divergence, we have that

\[|_{j}-_{0}|_{1}^{2} 2D_{}(_{} |_{j}).\]

Now fix some sequence \(x\). Conditioned on \(x_{1:t-1}\), \(b_{t}\) is deterministic. In addition, if \(b_{t} d_{j}\) it holds

\[_{}(x_{t}=1|x_{1:t-1})=_{j}(x_{t}=1|x_{1:t-1})\]

which, if \(b_{t} d_{j}\), implies

\[D_{}(_{}(x_{t}=|x_{1:t-1}))|_{j}(x_{t}= |x_{1:t-1}).=0\]When \(b_{t}=d_{j}\) it holds that

\[_{0}(x_{t}=1|x_{1:t-1})=F_{ b}(d_{j})=+j)}\]

\[_{j}(x_{t}=1|x_{1:t-1})=F_{j}(d_{j})=F_{ b}(d_{j+1})=+( j+1))}\]

which implies that if \(b_{t}=d_{j}\)

\[D_{ KL}(_{0}(x_{t}=|x_{1:t-1})\|_{j}(x_{ t}=|x_{1:t-1})).\] \[= D_{ KL}((F_{ b}(d_{j}))\|(F _{ b}(d_{j+1}))).\] \[= F_{ b}(d_{j})(d_{j})}{F_{ b}(d_{j+1})} +(1-F_{ b}(d_{j}))(d_{j})}{1-F_{ b}(d_{j+1})}\] \[ F_{ b}(d_{j})((d_{j})}{F_{ b}(d_{j+1} )}-1)+(1-F_{ b}(d_{j}))((d_{j})}{1-F_{ b}(d _{j+1})}-1)\] \[= 3,}{(1-d_{j})(4-d_{j+1})^{2}}\ \ ^{2}\]

where \((p)\) is a Bernoulli random variable with mean \(p\), the first inequality follows by using \( x x-1\) for all \(x>0\) and the last inequality follows by using \(d_{j}\), \(d_{j+1} 2/3\). This implies that for any \(b_{t}\),

\[D_{ KL}(_{0}(x_{t}=|x_{1:t-1})\|_{j}(x_{ t}=|x_{1:t-1}).)^{2}[b_{t}(x)=d_{j}]\]

Taking expectations over \(x_{0}\) and adding over \(t\) we get

\[D_{ KL}(_{0}\|_{j}\|_{j}) ^{2}_{ b}[N_{j}] ^{2}_{ b}[M_{j}]^{2} _{ b}[M_{j}]\]

where the last inequality follows from the fact that \(_{ b}[M_{j}]=F_{ b}(d_{j})_{  b}[N_{j}]\) and \(F_{ b}(d_{j}) 9/11\). Combining this with what we proved before we get

\[\|_{ b}f(x)-_{ b}f(x)\| (_{x}|f(x)|)_{ b} [M_{j}]}\]

which proves the claim. 

Now note that since the agent's payment is at least \(d_{j}M_{j}\), it must always hold \(d_{j}M_{j} T\), implying \(M_{j} T/d_{j}T\). Applying Lemma F.2 for \(f(x)=M_{j}\) we get that for any \(j\)

\[_{j}M_{j}_{ b}M_{j}+}{4 }T_{ b}M_{j}}\]

We notice that because it must hold \(_{j}d_{j}M_{j} T\), there must be some \(j\) such that \(_{ b}M_{j}T\). Fix that \(j\) and use that in the above inequality, making the above inequality for that \(j\)

\[_{j}M_{j}+}{4}T}=+\ T^{3/2}}{8}\]

Recalling that \(K=2T^{1/3}\) and \(=1/(3K)\) we get

\[_{j}M_{j}T^{2/3}+}{16}T\]

Since \(_{j}M_{j}=F_{j}(d_{j})_{j}N_{j}\) and \(F_{j}(d_{j})=} 9/11\) (which follows from \(d_{j+1} 1/3\)) the above becomes

\[_{j}N_{j}T^{2/3}+}{144}T:=UT \]

Using the above upper bound on \(_{j}N_{j}\), we are going to upper bound the value the player can earn in the \(j\)-th instance. To do that, we are going to consider that the player knows that she is playing against the distribution of \(_{j}[]\) but is restricted by (22). In addition, we consider that the player can satisfy the budget constraint in expectation, since this only increases the budget she can earn. Given this setting, the player's value is upper bounded by the following LP, where \(n_{i}\) represents the expected number of times the player bids \(d_{i}\), \(t_{0}\) is the expected number of times the player bids \(0\), and \(_{1}\) is the expected number of times the player bids \(1\):

\[_{d_{i},_{1},n_{0},,n_{K-1}} F_{j}(0)_{0}+_{1}+_{i=0}^{K-1}F_{j}(d_{i})n_{i}\] \[_{1}+_{i=0}^{K-1}d_{i}F_{j}(d_{i})n_{i}\] \[_{0}+_{1}+_{i=0}^{K-1}n_{i} T\] \[n_{j} UT\]

We upper bound the value of the above using its dual:

\[_{,,} + T+ UT\] \[ F_{j}(0)\] \[+ 1\] \[ d_{i}F_{j}(d_{i})+ F_{j}(d_{i}), i j\] \[ d_{j}F_{j}(d_{j})++ F_{j}(d_{j})\]

We notice that the solution \(=1/4\), \(=3/4\), \(=/4\) is feasible (which follows from some simple but lengthy algebra). This means that the player's expected reward is at most

\[T(+)\]

Using Lemma F.1 we get that the expected regret it at least

\[  T-(T^{2/3 }+}{144}T)\] \[= T^{2/3}-T^{1/3}-}{3456}T^{ 2/3}(=}{6})\] \[ 0.01T^{2/3}-0.02T^{1/3}\]

which is \((T^{2/3})\), as promised. 

### Regret Upper bound for Bandit information

In this section we present and prove an upper bound on the regret when learning with bandit feedback. Our regret bound is in the order of \((T^{3/4})\), which is slightly higher than the lower bound in Theorem 5.1. It remains to be interesting open question to achieve the optimal \((T^{2/3})\) regret bound.

**Theorem F.3**.: _There is a polynomial time algorithm for value or quasi-linear utility maximization under Assumption 3.1 such that, for every \(>0\) with probability at least \(1-\) its regret and ROI violation is at most_

\[((L^{1/4}T^{3/4}+T^{1/2}L^{1/2}) ).\]

_In addition, if \(=(T^{-1/4}(L^{1/4}+L^{1/2}T^{-1/4}))\) for some constant \(>0\), the above can be turned in an algorithm with exact ROI satisfaction regret that is \(1/\) times worse._

Theorem F.3 is based on the following theorem, Theorem F.4, which offers an algorithm with \(()\) high probability interval regret bound for the Lagrangian.

```
**ALGORITHM 4**No interval regret algorithm for bandit information with for time-varying ranges

To use this algorithm, we discretize the interval \(\) into \(N\) values \(\{i/N\}_{i[N]}\) and \(K\) bids \(\{j/K\}_{j[K]}\). Then for each \(i[N]\) we run an instance of the algorithm of Theorem F.4, which is used for round \(t\) when \(v_{t} i/N<v_{t}+1/N\) and outputs a bid \(j_{t}/K\). As in Section 3, we do not directly use the bid suggested by the algorithms, since it might lead to a negative reward in \(r_{t}()\); we use the safe bid of Assumption 3.1 when it is possible to get a negative reward. This roughly leads to a total regret bound of \(}()\), along with a discretization error of \((T(L/N+1/K))\). Appropriately picking \(N,K\) gives the regret bound.

Theorem F.4 provides the \(U_{_{t}}}()\) interval regret bound for every interval \([_{1},_{2}]\). In algorithm used is similar to the EXP-SIX algorithm that (Hardt et al., 2017) uses to bound the regret of the best sequence of actions and that (Ghosh and Hosh, 2017) use to bound the interval regret. However, our algorithm is not the same as EXP-SIX: we modify the algorithm to get a regret bound that scales linearly with \(U_{_{2}}\) instead of \(U_{_{2}}^{2}\) as shown in (Ghosh and Hosh, 2017).

**Theorem F.4**.: _Suppose there are \(K\) actions and the reward of round \(t\), \(r_{t}:[K][0,U_{t}]\), is picked by an adaptive adversary. There exists an algorithm that generates actions \(a_{1},,a_{T}\) such that for every \(>0\), with probability at least \(1-\), we have that for all \(1_{1}<_{2} T\)_

\[_{a[K]}_{t=_{1}}^{_{2}}r_{t}(a)-_{t=_{1}}^{_{2} }r_{t}(a)U_{_{2}}(+K) \]

We first present the algorithm for Theorem F.4 which low interval regret. The algorithm is a modification of the algorithm of (Hardt et al., 2017) so that it works for time-varying ranges. We present the algorithm in Algorithm 4.
```
1:Input: Number of rounds \(T\), number of actions \(K\)
2:Set \(=\), \(=}\), \(=}\)
3:Initialize for each action \(w_{1}(a)=1 a[K]\)
4:for\(t[T]\)do
5: Calculate \(p_{t}(a)=(a)}{_{a^{}}w(a)}\)
6: Sample and play \(a_{t} p_{t}()\)
7: Receive \(_{t}(a)\)
8: Calculate \(_{t}(a)=(a)}{p_{t}(a)+}1[a=a_{t}]\) for all \(a[K]\)
9: Receive loss range \([0,U_{t+1}]\) and calculate \(_{t+1}=}\)
10: Calculate \(w_{t+1}(a)=\)
11:\((1-)w_{t}(a)(-_{t+1}_{t}(a))+_{a^{}}w_ {t}(a^{})(-_{t+1}_{t}(a^{}))\)
12:end ```
**ALGORITHM 4**No interval regret algorithm for bandit information with for time-varying ranges

We now prove Theorem F.4.
```
1:Input: Number of rounds \(T\), number of actions \(K\)
2:Set \(=\), \(=}\), \(=}\)
3:Initialize for each action \(w_{1}(a)=1 a[K]\)
4:for\(t[T]\)do
5: Calculate \(p_{t}(a)=(a)}{_{a^{}}w(a)}\)
6: Sample and play \(a_{t} p_{t}()\)
7: Receive \(_{t}(a)\)
8: Calculate \(_{t}(a)=(a)}{p_{t}(a)+}1[a=a_{t}]\) for all \(a[K]\)
9: Receive loss range \([0,U_{t+1}]\) and calculate \(_{t+1}=}\)
10: Calculate \(w_{t+1}(a)=\)
11:\((1-)w_{t}(a)(-_{t+1}_{t}(a))+_{a^{}}w_ {t}(a^{})(-_{t+1}_{t}(a^{}))\)

[MISSING_PAGE_POST]

[MISSING_PAGE_EMPTY:20]

probability at least \(1-\), for every \(_{1}_{2}\)

\[_{j[K]}_{t_{1}(_{1},_{2}]}_ {t}^{i}(j)-_{t_{1}(_{1},_{2}]}_{t}^{i}(j_{t})\] \[ U_{_{2}}O((K}+K)K}{})\]

We do the above process for every \(i\) and use it as an algorithm10. Doing this for every \(i\) and using the union bound we get that for every \(>0\) with probability at least \(1-\), for every \(i[N]\) and \(_{1}_{2}\)

\[_{j[K]}_{t_{1}(_{1},_{2}]}_{t}^{i_{t}}(j)-_{t_{1}(_{1},_{2}]}_{t}^{i_{t}}(j_{t})\] \[ U_{_{2}}O((K}+K)KN}{})\] \[= U_{_{2}}O((}{L^{1/4}}}+ }{L^{1/4}})) 56.905512pt N= L^{1/4}/L^{1/4}\\ K= L^{1/4}/L^{1/4} \]

Fix \(_{1}\), \(_{2}\). We want to use (30) to upper bound

\[_{f}_{f=_{1}}^{_{2}}r_{t}(f)-_{f=_{1}}^{ _{2}}r_{t}(_{t,j_{t}})\]

Fix \(f\) and define \(j_{1},,j_{N}[K]\) such that \(_{j_{t}}\) (recall \(_{j}=j/K\)) is the bid that is above \(f()\) for every \(v(v_{i-1},v_{i}]\) and as small as possible, i.e., for all \(i[N]\) it holds

\[_{j_{t}}=[K_{v(_{i-1},_{i})}f(v)]/K\]

We notice that for all \(i[N]\) and \(v(v_{i-1},v_{i}]\): \(_{j_{t}}[f(v),f(v)++)\) (by \(L\)-Lipschitz of \(f\)). We now have

\[r_{t}(f) = [f(v_{t}) d_{t}](_{t}v_{t}- _{t}pf(v_{t}),d_{t})\] \[ [_{j_{t}} d_{t}](_{t} _{i_{t}}-_{t}p_{j_{t}}--.d _{t})^{+}\] \[ [_{j_{t}} d_{t}](_{t} _{i_{t}}-_{t}p_{j_{t}},d_{t})^{+} _{t}+\] \[ _{t}^{i_{t}}(j_{t})+_{t}+ {K}\]

where the last inequality follows because the bid that corresponds to \(j_{t}\) in round \(t\) is the safe bid of that round whose reward is non-negative and at least as good as \(_{j_{t}}\)'s. Summing the above over \(t[_{1},_{2}]\) and taking a sup over \(f\) and a max over \(j_{1},,j_{N}\) we get

\[_{f}_{t=_{1}}^{_{2}}r_{t}(f)\] \[ _{j_{t}_{j_{t}}}_{t_{1}}_ {t}^{i_{t}}(j_{t})+U_{_{2}}+ \] \[= _{i[N]}_{j[K]}_{t_{1}(_{1}, _{2}]}_{t}^{i_{t}}(j)+U_{_{2}}T^{}L^{} N L^{1/4}_{1}^{}\\ K T^{1/4}\] \[ _{t[_{1},_{2}]}_{t}^{i_{t}}(j_{t})\] \[+_{i[N]}U_{_{2}}O((}{L ^{}}}+}}{L^{}}))\] \[+U_{_{2}}T^{}L^{}\] (by (30)) \[= _{t[_{1},_{2}]}_{t}^{i_{t}}(j_{t})\] \[+U_{_{2}}O((^{L}}}{L ^{}}+(TL)^{})+T^{}L ^{})\] \[= _{t[_{1},_{2}]}_{t}^{i_{t}}(j_{t})\] \[+U_{_{2}}O((^{L}}}{L ^{}}+(TL)^{})) \]

where the second to last inequality we substituted \(N= L^{3/4}T^{1/4}\) and we used the fact that \(_{i}}\) since \(_{i}T_{i}=T\). By noticing that \(_{t}^{i}(j_{t}) r_{t}(_{j_{t},t})+U_{t}(1/K+1/N)\) and bounding \(T(1/N+1/K)=O(L^{1/4}T^{3/4})\) we get the high probability interval regret bound on the rewards \(r_{t}()\). Using Theorem 2.1 we get the desired result.

The tight satisfaction of the ROI constraint follows by using Lemma 4.2. The first algorithm is the one we describe above. The second algorithm is the primal algorithm described above with \(_{t}=_{t}=1\), which achieves \(Q(,)=-O((L^{1/4}x^{3/4}+^{1/2}L^{1/2}))\).

## Appendix G Polynomial time algorithm for full information feedback

In this section, we present a polynomial time algorithm that has regret guarantees matching the ones in Theorems 3.3 and 4.1, when the value and the highest competing bid are independent.

**Theorem G.1**.: _Assume that the value \(v_{t}\) and highest competing bid \(d_{t}\) are sampled independently. There is a polynomial time algorithm with full information feedback for value or quasi-linear utility maximization when the payment function satisfies Assumption 3.1 such that, for every \(>0\) with probability at least \(1-\), the algorithm has regret against the class of \(L\)-Lipschitz continuous functions and ROI violation at most \(O()\). In addition, if \(=T^{-1/2+}(TL/)\) for some constant \(>0\), it can be turned into an algorithm with exact ROI satisfaction and with regret that is \(1/\) times worse._

The algorithm for the above theorem is similar to the one in Theorem F.3: discretize the values and run a separate online learning algorithm for each of those values. The important difference is that, instead of running/updating each algorithm only when we observe its corresponding value, we run every algorithm in every round \(t\), even if the value \(o_{t}\) is completely different than the one the algorithm represents. This allows each algorithm to run for many more rounds, making it 'learn' faster. The assumption that \(o_{t}\) and \(d_{t}\) are independent is crucial since the \(d_{t}\) of every round can be used for every algorithm.

We note that the above technique cannot directly be used for bandit information, since we do not observe \(d_{t}\) and therefore the observed reward is dependent on the bidding.  use this technique, along with the assumption that the value distribution is known, to get regret bounds that would imply to \(T^{2/3}\) regret for bandit information. However, their bounds are only in expectation and for the entire horizon, not every interval. We leave as future work extending these techniques to get regret matching the one in Theorem 5.1.

Proof.: We use a scheme similar to the one used in the proof of Theorem F.3. Fix \(N= LT\) and \(K=T\). Let \(_{i}=i/N\) for \(i[N]\) and \(_{j}=j/N\) for \(j[N]\). For every \(i[N]\) define \(r^{i}_{t}:[K]_{ 0}\) such that

\[r^{i}_{t}(j)=1[_{i,j} d_{t}](_{t}_{ i}-_{t}p(_{t,j},d_{t}))\]

where \(_{t,j}\) is either \(_{j}\) or the safe bid (Assumption 3.1) of \(^{i}_{t}()\).

Define for every round \(i_{t}=_{t}N\) (i.e. the \(i\) that corresponds to the value that is closest and above to \(v_{t}\)). For every \(i[N]\) let \(_{t}\) be an instance of the algorithm in Theorem 3.5 for \(=1\) with \(K\) actions that has also been modified to have low interval regret, using Theorem 3.7. Unlike the bandit setting, here we run \(_{t}\) even in rounds where \(i_{t} i\). In every round \(_{t}\) outputs an action \(j^{i}_{t}[K]\) which corresponds to the bid \(_{t,j^{i}_{t}}\). The reward of the \(j\)-th action in round \(t\) is \(^{i}_{t}(j)\). The bid we use in round \(t\) is the one suggested by algorithm \(_{t_{t}}\), \(_{t,j^{i}_{t^{i}_{t}}}\).

Using Theorems 3.5 and 3.7, an application of Azuma's inequality and a union bound over \(i\), we have that with probability at least \(1-\) for every \(1_{1}<_{2} T\) and every \(i[N]\):

\[_{j[K]}_{l=_{1}}^{_{2}}r^{i}_{t}(j)-_{l=_{1}}^{_ {2}}r^{i}_{t}(j^{i}_{t}) U_{2}O  \]

Fix \(_{1}\), \(_{2}\). To get our regret bound we have to upper bound

\[_{f}_{l=_{1}}^{_{2}}r_{t}(f)-_{l=_{1}}^{ _{2}}_{i}1[t_{l}=i]r_{t}(_{t,j^{i}_{t}})\]

Fix \(f\) and define \(j_{1},,j_{N}[K]\) such that for all \(i[N]\)

\[_{j_{i}}= K_{(_{i-1},_ {i})}f()/K\]

We notice that for all \(i[N]\) and \(o(v_{i-1},v_{i}]\): \(_{j_{i}}[f(o),f(o)++)\) (by \(L\)-Lipschitz of \(f\)). We now have

\[r_{t}(f) = 1[f(v_{t}) d_{t}](_{t}_{i}- _{t}p(f(v_{t}),d_{t}))\] \[ 1[_{j_{i_{t}}} d_{t}](_{t} _{i_{t}}-_{t}p(_{j_{t}}--,d_ {t}))^{+}\] \[ 1[_{j_{i_{t}}} d_{t}](_{t} _{i_{t}}-_{t}p(_{j_{t}},d_{t}))^{+}+ _{t}(+)\] \[ ^{i_{t}}_{t}(i_{t})+_{t}(+)\]

where in the second to last inequality we used the fact that \(p(,d)\) is 1-Lipschitz and in the last inequality follows because the bid that corresponds to \(j_{i_{t}}\) in round \(t\) is the safe bid of that round whose reward is non-negative and at least as good as \(_{j_{t}}\)'s. Summing the above over \(t[_{1},_{2}]\) and taking a sup over \(f\) and a max over \(j_{1},,j_{N}\) we get

\[_{f}_{l=_{1}}^{_{2}}r(f)  _{j_{1},,j_{N}}_{l=_{1}}^{_{2}}r^{i_{t}}_{t }(j_{t})+U_{_{2}}T(+) \] \[ _{i[N]}_{j^{}[K]}_{l=_{1}}^{_{2} }1[t_{l}=i]^{i}_{t}(j^{})+2U_{_{2}}\]

where in the last inequality we used that \(N TL\) and \(K=T\).

We now prove a high probability bound on \(_{l=_{1}}^{_{2}}1[t_{l}=i]^{i}_{t}(j^{})\), for a fixed \(j^{*}\). Using Azuma's inequality we can prove that for all \(>0\) with probability at least \(1-\),

\[_{l=_{1}}^{_{2}}1[t_{l}=i]^{i}_{t}(j^{*}) _{l=_{2}}^{_{2}}[t_{l}=i]^{i}_{t}(j^ {*})+OU_{_{2}}\]

Note that to use Azuma's inequality we have to rely on the fact that \(i_{t}\) and \(^{i}_{t}(j^{*})\) are independent conditioned on the history of rounds up to \(t-1\), since \(^{i}_{t}(j^{*})\) does not depend on \(o_{t}\) but only \(d_{t}\).

By letting \([t_{l}=i]=q_{i}\) and taking a union bound over all \(j_{e}[K]\) and all \(i[N]\), we have that with probability at least \(1-\) for all \(j^{*}_{t}\), \(t_{l}\), and \(_{1}\), we have

\[_{l=_{1}}^{_{2}}1[t_{l}=i]^{i}_{t}(j^{*}) _{l=_{1}}^{_{2}}q_{i}^{i}_{t}(j^{*})+OU_{_{ 2}}\]

Substituting \(N\) and \(K\), the above makes (33)

\[_{f}_{l=_{1}}^{_{2}}r_{t}(f)\  _{i[N]}q_{i}_{j^{}[K]}_{l=_{1}}^{_{2}}r^{i}_{t }(j^{*})+OU_{_{2}} \]

We now bound \(q_{i}_{j^{}}_{l=_{1}}^{_{2}}r^{i}_{t}(j^{*})\) for some \(i\). By (32) we have

\[q_{i}_{j^{}}_{l=_{1}}^{_{2}}r^{i}_{t}(j^{*})  q_{i}_{l=_{1}}^{_{2}}r^{i}_{t}(j^{*}_{t})+q_{i}U_{_{ 2}}O\] \[ _{l=_{1}}^{_{2}}1[t_{l}=i]^{i}_{t} (j^{*}_{t})+q_{i}U_{_{2}}O\] \[+q_{i}U_{_{2}}O\] \[= _{l=_{1}}^{_{2}}1[t_{l}=i]^{i}_{t}(j^{ }_{t})+q_{i}U_{_{2}}O\]

where in the last inequality we used Azuma's inequality, which heavily depends on the fact that \(i_{t}\) and \(^{i}_{t}(j^{}_{t})\) are independent 

[MISSING_PAGE_FAIL:23]