# ESANS: Effective and Semantic-Aware Negative Sampling for Large-Scale Retrieval Systems

Anonymous Author(s)

###### Abstract.

Industrial recommendation systems typically involve a two-stage process: retrieval and ranking, which aims to match users with millions of items. In the retrieval stage, classic embedding-based retrieval (EBR) methods depend on effective negative sampling techniques to enhance both performance and efficiency. However, existing techniques often suffer from false negatives, high cost for sampling quality and semantic information deficiency. To address these limitations, we propose Effective and Semantic-Aware Negative Sampling (ESANS), which integrates two key components: Effective Dense Interpolation Strategy (EDIS) and Multimodal Semantic-Aware Clustering (M3AC). EDIS generates virtual samples within the low-dimensional embedding space to improve the diversity and density of the sampling distribution while minimizing computational costs. MSAC refines the negative sampling distribution by hierarchically clustering item representations based on multimodal information (visual, textual, behavioral), ensuring semantic consistency and reducing false negatives. Extensive offline and online experiments demonstrate the superior efficiency and performance of ESANS.

## CCS Concepts

* **Information systems Retrieval models and ranking.**

## 1. Introduction

Recommendation systems have been widely adopted across diverse domains, including online e-commerce, advertising, short video platforms and delivery services , due to their effectiveness in mitigating information overload by providing tailored recommendations from large-scale item collections . Industrial recommendation systems typically involve two stages: retrieval and ranking. The retrieval stage is responsible for retrieving thousands of candidate items, whereas the ranking stage predicts the likelihood of user interaction with these candidates. Considering that retrieval tasks can be formulated as identifying the nearest neighbors in a vector space, substantial research has been devoted to developing high-quality representations for both users and items. Collaborative Filtering (CF) methods  address this issue by encoding user preference and item representation into low-dimensional embedding space, based on historical interacted information. With the rapid development of deep learning, neural networks have been widely adopted in personalized recommendation systems . Recently, Embedding-Based Retrieval (EBR) methods  have demonstrated significantly better performance compared to traditional CF methods, establishing themselves as the dominant approach in recommendation systems. EBR methods encode user and item information into separate embeddings using parallel neural networks, and these embeddings are trained through the strategy of contrastive learning .

EBR methods rely heavily on the contrast between positive and negative samples to produce distinguishable representations. The careful selection of negatives is crucial to enhancing the model's ability to differentiate between relevant and irrelevant items, significantly impacting overall retrieval performance. The classic Uniform

Figure 1. Visual diagram of our ESANS compared with other methods. Each method has sampled ten negatives equally.

Negative Sampling (UNS) method [23; 44] randomly selects negatives from the item candidate set, providing efficiency but yielding **low-quality samples**. Following this, additive margin  and temperature coefficient [33; 51] adjust the contrastive loss function to mine high-quality negatives from naive negatives sampled by UNS. FairNeg  reweights negatives in accordence with item group fairness to provide high-quality samples. Adap-\(\) dynamically adjusts the temperature coefficient for reweighting uniform negatives in accordence with their relevance to user interests. However, these methods fail to introduce more challenging negatives and further expand the scale of sampling which limit the performance of EBR methods. To address this issue, In-batch sampling  introduces relatively harder negatives by the in-batch sharing strategy. Airbnb  heuristically introduces orders from the same city as harder negatives. MixGCF  employs a hop-mixing interpolation technique in Graph Neural Networks (GNNs) to generate virtual hard negatives. However, these methods fail to effectively adjust the difficulty of negatives and distinguish users' potential interests from hard negatives, which may exacerbate the issue of **false negatives** (i.e. items relevant to users' potential interests but incorrectly regarded as negatives). Moreover, existing methods require substantial computational resources to further **improve the sampling quality** (i.e. sufficient hard negatives) . Furthermore, from a contrastive learning perspective, these methods are unable to regulate sampling strategies based on **semantic information** in the real world, rendering the sampling process a black box.

Inspired by recent works in multi-modal learning [35; 40] and vector quantization techniques , we propose the Effective and **Semantical-Aware Negative Sampling** (ESANS) to address these challenges in the sampling process. Our method consists of two main components: the first part is Effective Dense Interpolation Strategy (EDIS), and the second part is Multimodal Semantic-Aware Clustering (M3AC). EDIS is devised to generate a sufficient number of virtual samples within the low-dimensional embedding space. More specifically, generating virtual samples among existing negatives creates a more uniform, dense, and diverse sampling distribution. Virtual samples positioned between the positive sample and surrounding negative samples contribute to gradually enhance the discriminatory ability of the neural network. By adjusting the interpolation parameters and strategies, we can control the difficulty of generated negatives and generate sufficient hard negatives. Meanwhile, in contrast to memory banks , interpolation within the low-dimensional embedding space leads to minimal computational cost and eliminates the need for extra memory storage.

Nevertheless, EDIS strongly relies on the judicious selection of negative sample anchors. In practice, virtual negative samples generated via interpolation may lack clear semantic information, occasionally producing meaningless samples. For example, interpolating between "iPhone" and "Cola" produces meaningless results, potentially introducing noise. Moreover, interpolating among randomly sampled negative anchors may introduce false negatives, further complicating the training process.

To address these deficiencies, we propose the MSAC method to optimize the sampling space by integrating the real-world semantic information. Firstly, we propose a multimodal-aligned technique to fuse multi-perspective item information from visual, textual and behavioral perspectives. Subsequently, a two-level vector quantized clustering approach is employed to assign semantic representations into multiple secondary clusters. Consequently, we can mitigate the issue of false negatives by selecting hard negatives from the same primary cluster as the positive sample, while ensuring they belong to a different secondary cluster. Additionally, we dynamically calibrate the sampling probabilities for each negative cluster to control the difficulty of negatives and refine the sampling quality. It is worth noting that this calibration is precisely guided by the semantic distance between the cluster centers of positives and negatives. This allows us to adjust the difficulty of the sampling process by increasing the sampling probabilities of clusters that are semantically similar to the positive cluster. Once the MSAC is introduced, EDIS based on semantics can be performed within the well-established semantic clusters. More specifically, we can ensure that the interpolated outcomes remain confined within the convex hull of that cluster. This intrinsic constraint preserves a measurable degree of semantic consistency and "**real-world applicability"** in the interpolated samples. Furthermore, interpolation between positives and hard negatives is also employed to generate additional high-quality hard negatives. Figure 1 shows the comparison between our ESANS and other methods. Our contributions can be summarized as follows:

* We propose a novel and effective sampling approach called ESANS, which provides explicit semantics guidance for interpolation negative sampling. Moreover, ESANS effectively enhances the diversity and richness of negative samples and allows for controllable negative sample difficulty, thereby boosting performance.
* We propose a general multimodal-aligned clustering approach that captures the multi-perspective similarities among candidate items on e-commerce platforms, thereby enabling a more refined semantic description in the interpolation space and eliminating false negative instances in the hard negative sampling process.
* We provide both extensive offline and online experiments to demonstrate the effectiveness and the efficiency of ESANS.

## 2. Related Work

This section presents a brief review of the relevant literature, specifically addressing techniques for negatives re-weighting, heuristic negative sampling, and model-based negative sampling.

**Negatives Re-weighting**. UNS [23; 44] represents the foundational negative sampling method, where negative samples are uniformly drawn from the entire dataset. The simplicity of UNS's algorithmic design provides substantial efficiency gains. Nevertheless, it exhibits notable deficiencies in the quality of negative samples. UMA2  computes the sampling probabilities of random negative samples according to the current model and subsequently employs the Inverse Probability Weighting (IPW) technique to assign loss weights to these negative samples. The method proposed by  implements position-weighted approach for negative samples, where the weight is determined by the sample's ranking position. These approaches mimic high-quality negatives from naive negatives sampled by UNS, which fails to introduce more challenging negatives.

**Heuristic Negative Sampling**. Heuristic negative sampling algorithms primarily define the sampling distribution by predefinedheuristic rules. Popularity-biased Negative Sampling (PNS) (Brocker et al., 2017) utilizes item popularity as the sampling probability. Airbnb (Shi et al., 2018) applies personalized negative sampling within the same city, assuming bookings in the same location exhibit similar patterns. While this approach enhances the sampling process, it solely focuses on similarity-based sampling, neglecting sampling bias. CBNS (Shi et al., 2019) employs in-batch negative sampling and expands the negative sample set by incorporating previously trained items. The method (Shi et al., 2019) incorporates estimated item frequency into the batch softmax cross entropy loss to reduce sampling bias within the batch. MNS (Shi et al., 2019) integrates UNS with in-batch negative sampling, adopting a hybrid strategy. While these methods enhance sampling quality, they introduce popularity bias, aggravating the Sample Selection Bias (SSB) issue. Our method enhances sampling quality via a multimodal-aligned clustering algorithm and dense interpolation negative sampling, while also mitigating sampling bias.

**Model-based Negative Sampling.** Model-based negative sampling algorithms are highly effective at selecting high-quality negative samples. Model-based scoring methods are demonstrated by Dynamically Negative Sampling (DNS) (Shi et al., 2019) and ESAM (Eisaman et al., 2019), where the current model scores samples and selects the highest-scoring ones as negative samples. Adversarial learning methods also contribute to sampling improvements. MixGCF (Shi et al., 2019) employs a hop-mixing technique to synthesize hard negative samples by leveraging the user-item graph structure and the aggregation mechanism of Graph Neural Networks (GNNs). IRGAN (Zhou et al., 2019) utilizes two recommendation models, a discriminator and a generator, trained adversarially. AdvIR (Shi et al., 2019) and RNS (Rasmussen, 2006) further optimize IRGAN's structure, improving both efficiency and performance. The Adap-r(Shi et al., 2019) adaptively adjusts the temperature coefficient of the loss function by calculating the loss for each user and the corresponding random negative samples. This method leverages personalized user preferences to effectively identify hard negative samples. FairNeg (Chen et al., 2019) enhances the sampling distribution by fairly sampling from groups and then reweighting based on their relevance to the user. Our method precisely controls the difficulty of negatives, improving sampling quality and eliminating false negatives without increasing the complexity of the retrieval model.

## 3. Methodology

In this section, we formulate the problem and describe our proposed framework specifically, as well as introducing the detailed process of our negative sampling method.

### Problem Formulation

The primary objective of the retrieval stage in industrial recommendation systems is to efficiently retrieve a potentially relevant subset of items from a large item pool \(\) for each user \(u\). In pursuit of this objective, each instance can be represented by a tuple \((_{u},_{u},_{i})\) where \(_{u}\) denotes the sequence of user historical behaviors, \(_{u}\) denotes the basic profile of user \(u\), \(_{i}\) denotes the information of target item such as item id and category id. In the classical two-tower architecture (Shi et al., 2019) of the EBR models, users and items are separated into two individual encoders to reduce online computational complexity. We can define the user encoder as \(f_{user}\) and the item encoder as \(g_{item}\), so we have:

\[_{u} =f_{user}(_{u},_{u})\] \[_{i} =g_{item}(_{i}) \]

where \(_{u}^{d_{u} 1}\) is the output vector of the user encoder called user embedding, and \(_{i}^{d_{u} 1}\) is the output vector of the item encoder called item embedding. \(K\) denotes the dimension of output embeddings. Finally, the relevance of a user-item pair can be estimated by a scoring function:

\[s(,)=^{} \]

### Overall Framework

As previously discussed, existing methods fail to balance sampling quality, bias, and efficiency simultaneously. To address these limitations, we designed ESANS, as illustrated in Figure 2. ESANS consists of two main components:

* **Multimodal Semantic-Aware Clustering(MSC)**, which performs hierarchical clustering based on visual, textual, and behavior based representations to optimize the sampling process by integrating semantic information. Our proposed method addresses the limitations of unclear anchor semantics, improves sampling quality, and reduces the risk of introducing false negatives.
* **Effective Dense Interpolation Strategy(EDIS)**, which employs linear interpolation among existing samples within the same semantic cluster to make sure the semantic consistency. Our proposed method works with minimal computational cost, enhances the diversity and richness of negative samples, and facilitates the controllable difficulty of hard negative samples.

### Multimodal Semantic-Aware Clustering

Most existing negative sampling methods tend to ignore the semantic correlations among samples. Against this deficiency, our MSAC is proposed to capture the multi-perspective similarities among items and incorporate explicit semantics into the negative sampling process.

#### 3.3.1. Multimodal-aligned Technique

When users browse items on the e-commerce platform, they primarily perceive items through three views: visual images, descriptive text, and collaborative filtering recommendations. To generate a comprehensive description of items, it is necessary to consider these views concurrently. The visual representations \(_{}\) and textual representations \(_{}\) can be pre-trained by specific encoders (Shi et al., 2018; Zhai et al., 2019) in advance. The behavior-based representations \(_{}\) can be pretrained using graph representation learned based on a substantial number of user behaviors. Given a mini-batch of N items, we design multimodal-aligned encoders for each view.

\[_{} =H_{}(_{})^{N d_{m}}\] \[_{} =H_{}(_{})^{N d _{m}}\] \[_{} =H_{}(_{})^{N d _{m}} \]

where \(H_{}\) denotes the encoder of each view, \(_{}\) denotes the output embedding of each view, \(d_{m}\) denotes the output dimension of each multimodal-aligned encoder.

Inspired by the Contrastive Language-Image Pre-training (CLIF) (Shen et al., 2017), We propose a multimodal alignment method to fuse item representations from three perspectives. Given a dataset of \(_{}\) that consists of a collection of output embeddings \((_{I}^{i},_{}^{i},_{}^{i} )_{I=1}^{N}\), we contrast congruent and incongruent pairs across any two modalities. For instance, we sample from the joint distribution of image-text models \(_{I-}(_{I},_{})\) or \(_{I-}=\{_{I}^{i},_{}^{i}\}\), which we call positive samples. We sample from the product of marginals, \(_{I-}(_{I})(_{})\) or \(_{I-}=\{_{I}^{i},_{}^{i}\}\), which we call negative samples. Multimodal-aligned encoders are optimized to correctly select a single positive sample \(_{I-}\) out of the set \(=\{_{I-},_{I-}^{1},..., _{I-}^{N-1}\}\) which contains \(N-1\) negative samples:

\[_{align}^{I-}=-}{S}[log_{I-})}{h(_{I-})+_{i=1}^{N-1}h( _{I-}^{i})}]\] \[_{align}^{I-}=-}{S}[log_{I-})}{h(_{I-})+_{i=1}^{N-1}h( _{I-}^{i})}]\] \[_{align}^{-}=-}{S}[ log_{-})}{h(_{-})+ _{i=1}^{N-1}h(_{-}^{i})}] \]

where \(h()\) is the cosine similarity operation after exponentiation, \(_{align}^{I-}\) is the alignment loss between visual and textual modals, \(_{align}^{I-}\) is the alignment loss between visual and behavior-based models, \(_{align}^{-}\) is the alignment loss between behavior-based and textual modals.

#### Vector Quantized Clustering with Cascaded Codebooks

While aligning \(_{I},_{},_{}\) into the same embedding space, we simultaneously quantize these representations into several clusters with cascaded codebooks, as illustrated in Figure 2. Specifically, the primary codebook is designed to effectively differentiate coarse-level item representations, while the secondary codebook enhances this distinction by refining the differentiation of fine-grained item representations, especially when significant disparities persist among aligned representations across partial modalities.

The _primary codebook_\(C_{p}=\{z_{p}^{k}\}_{k=1}^{K_{p}}\) consists of \(K_{p}\) codewords (Zhu et al., 2017) and the dimension of each codeword is \(d_{m}\). The clustering stage is conducted by calculating the mean of the aligned embeddings:

\[_{p}^{i}=(_{I}^{i}+_{}^ {i}+_{}^{i}) \]

Subsequently \(_{p}=\{_{p}^{i}\}_{i=1}^{N}\) is quantized by assigning it to the nearest codeword within the primary codebook. We denote that the nearest codeword to \(_{p}^{i}\) is \(c_{p}^{i}=_{k}\|_{p}^{i}-p\|\).

In the _secondary codebook_, we compute the residual between \(\{_{I},_{},_{}\}\) and the primary corresponding codeword \(z_{p}^{C_{p}}\). These residuals are concatenated to a vector \(_{p}^{i}\), which is used to

Figure 2. Our proposed ESANS framework. a) Multimodal-aligned Technique. b) Vector Quantized Clustering with Cascaded Codebooks. c) Semantic-Aware Negative Sampling & Effective Dense Interpolation Strategy (EDIS).

describe the modal-specific information between different items.

\[_{s}^{i}=[_{T}^{i}-z_{p}^{C_{p}^{i}};_{T}^{i}-z_{ p}^{C_{p}^{i}};_{G}^{i}-z_{p}^{C_{p}^{i}}] \]

The advantages of using information from three modalities for secondary clustering are illustrated in Figure 3. Similar to the primary clustering, we select the codeword closest to \(_{s}=\{_{s}^{i}\}_{i=1}^{N}\) from another codebook \(C_{s}=\{z_{s}^{k}\}_{k=1}^{K_{s}}\), where \(K_{s}\) denotes the number of codewords in the secondary codebook. The nearest secondary codeword to \(_{s}^{i}\) is recorded as \(C_{s}^{i}=_{k}\|_{s}^{i}-z_{s}^{k}\|\).

Once we have all cluster indicate for an item, the clustering loss can be defined as:

\[_{}=_{i=1}^{N}\|_{p}^{i}-z_{p}^{C_{p}^{i}} \|^{2}+_{i=1}^{N}\|_{s}^{i}-z_{s}^{C_{p}^{i}}\|^{2} \]

Finally, the loss function for multimodal-aligned clustering is given by Equation 8:

\[=_{1}_{align}^{T-}+_{2} _{align}^{T-}+_{3}_{align}^{-}+ _{} \]

#### 3.3.3. Semantic-Aware Negative Sampling

Based on the above framework, we divide the whole set of candidate items into multiple semantic clusters. Then we introduce the semantic-aware negative sampling which includes simple negative sampling and hard negative sampling. In simple negative sampling, we select primary clusters for each positive sample based on the following probability formula, ensuring that none of these selected clusters are the same as the primary cluster of the positive sample.

\[ Q(C_{p}=i)&=^{i},z_{p}^ {i})^{p}},i+\\ P(C_{p}=i)&==i)}{_{j+}Q(C_{p}=j)}  \]

where \(d(,)\) measures the distance between primary codewords using an inner-product operation, which is subsequently normalized to a range from 0 to 1. \(z_{p}^{n}\) is the primary cluster of the positive sample, \(Q(C_{p}=i)\) is the unnormalized sampling probability of similar primary clusters with \(\), \(P(c_{p}=i)\) is the normalized sampling probability of primary cluster \(z_{p}^{i}\). Then, we randomly select samples from each cluster which enhances the diversity of negative samples. After being encoded by the item tower (Zhou et al., 2017), the embedding set of simple negative samples can be represented as \(V_{s}\):

\[V_{s}=\{V_{s}^{1},...,V_{s}^{k},...,V_{s}^{m_{c}}\} \]

where \(V_{s}^{k}\) is the embedding set of the simple negative samples in k-th cluster, \(m_{c}\) is the number of selected clusters and \(m_{o}\) is the number of selected samples in each cluster. In this way, we dynamically adjust the difficulty of the simple negatives as well as mitigate group-level sampling biases.

In hard negative sampling strategy, we randomly select partially similar samples within the positive primary cluster. Then, we consider samples in the same secondary cluster as false negatives and remove these samples from the hard negative samples set. The output embedding set of hard negative samples can be represented as \(V_{h}=\{v_{h}^{1},v_{h}^{2},...,v_{h}^{m_{h}}\}\), where \(m_{h}\) is the number of selected samples in the positive primary cluster.

### Effective Dense Interpolation Strategy

By employing our negative sampling process, we obtain negative sample clusters and randomly selected negative sample anchors for each cluster. It's a well-established principle (Bang et al., 2017) that increasing the negative sampling size can enhance the performance of the EBR models. However, the process mentioned above does not guarantee a sufficient sampling size for each cluster. To solve this problem, we propose a parameter-adaptive negative sampling augmentation technique based on the linear interpolation to increase the number of negative samples. The detailed interpolation process is applied to both simple negative samples and hard negative samples, which is illustrated in Figure 2.

#### 3.4.1. Interpolation on Simple Negative Samples

Suppose we select \(n_{o}\) negative anchors (\(2 n_{o} m_{o}\)) from the \(k\)-th cluster. The output item embeddings are reordered as \(V_{}=\{v_{}^{1},...,v_{}^{n_{o}}\}\). Each vector in the embedding set is selected once as the anchor vector \(v_{}^{a}\), and generate the virtual negative samples similar to the embedding set by linear interpolation:

\[_{}^{a}&=_{i=1} ^{n_{o}}a_{i}v_{}^{i}\\ & a_{j}=}^{a},v_{}^{i})^{}}{_ {j=1}^{n_{o}}d(v_{}^{a},v_{}^{i})^{}} \]

Figure 3. The visualization of items in the representation space during secondary clustering. Although item 1-3 and item 4-5 have similar mean embeddings, but in each view their embeddings differ significantly, resulting in their assignment to different secondary clusters. By leveraging three modalities, clustering accuracy is significantly enhanced.

[MISSING_PAGE_FAIL:6]

features of these contents have already been extracted using PixelNet, a network proposed concurrently with Pixel-Rec.
* **Industrial Dataset.** We establish the offline dataset by collecting the users' sequential behaviors and feedback logs from Alibaba's international e-commerce platform, Lazada. The dataset comprises four categories, each representing a distinct Southeast Asian country, labeled from #A1 to #A4.

#### Graph Construction

Due to space limitation, the introduction of the behavior-based graph construction is provided in Appendix Section A.

**Baselines.** We compared our ESANS with five representative negative sampling methods based on the classical two-tower architecture. The methods are as follows:

* **UNS**(Zhou et al., 2017; Wang et al., 2018): A widely used negative sampling approach involves randomly selecting instances from a uniform distribution.
* **PNS**(Bianchi et al., 2017): A negative sampling method that adjusts the sampling distribution based on item popularity.
* **Debiased MNS**(Zhu et al., 2017; Wang et al., 2018): A method that integrates UNS with in-batch negative sampling, and introduces a technique to address the oversampling issue of popular items.
* **MixGCF**(Zhu et al., 2017): A method synthesizes hard negatives between negatives and positives in a graph-based model. We adapt this to a two-tower structure to generate virtual hard negatives in the item representation space.
* **FairNeg**(Bianchi et al., 2017): A method that improves item group fairness by adaptively adjusting the distribution of negative samples at the group level.
* **Adap-\(\)(Chen et al., 2018): A method that adjusts the temperature coefficient of the loss function by the embedding similarity between users and corresponding negatives.

**Evaluation Metrics.** For the evaluation metrics in recommendation tasks, we follow (Chen et al., 2018; Chen et al., 2018) and use Recall@K for each group based on the Top-K recommendation results. Finally, the Recall@K is averaged over all users.

**Parameter settings.** We divide users in each public dataset into three subsets: training, validation, and testing, with a ratio of 8:1:1. As for the Industrial Dataset, we set aside the instances from the final day for testing, while using the preceding instances for training. For each user in the training set, we employ their first \(k\) actions to predict the (\(k+1\))-th action. To ensure computational manageability, we limit the length of user behavior sequences to 10 for the Amazon Review dataset, 32 for the Pixel-Rec dataset and 64 for the Industrial dataset. Due to space limitation, additional implementation details are provided in Appendix Section B.

### Performance Comparison (RQ1)

Table 3 summarizes the overall performance of our ESANS as well as the baselines on both industrial and public datasets, with the best results emphasized in bold and the second-best results underlined. It is noteworthy that ESANS consistently outperforms all baseline methods across the aforementioned datasets, achieving an average improvement of up to **15.32%** in recall@50 and **10.73%** in recall@200 compared to its base method UNS. PNS generally outperforms UNS across most datasets, indicating that boosting the sampling possibility for popular items improves sampling quality. However, it is worth noting that PNS does not exceed UNS performance in the #A3 dataset, which might be attributed to the introduced popularity bias. Once the challenge of popularity bias is addressed, the debiased MNS Sampling method outperforms UNS and PNS across all datasets and outperforms other baselines on #A4. MixGC introduces virtual hard negatives by hop-mixing interpolation which achieves similar performance with the debiased MNS and proves the feasibility of hard negatives augmentation. However, the interpolation process fails to consider semantics and yields noisy negatives, so it is outperformed by our method. FairNeg is another work conducted to reduce the sampling bias via adjusting the group-level negative sampling distribution which provides the best recommendation utility on Pixel-Rec and #A2 in all baselines. However, this work determines the groups by the only item attribute view which is not comprehensive and thus is surpassed by our method. Ada-\(\) is proposed to design a learnable \(\), which enables the adaptive adjustment of the difficulty level for negatives. This work outperforms other baseline models on Amazon Elecs. However, Ada-\(\) fails to provide incremental information by deriving more challenging negatives during the training process so that it is beats by our method.

In summary, our method effectively addresses the inherent limitations of these methods and achieves SOTA performance across all datasets in terms of retrieval efficiency. It is worth noting that the MSAC module is actually detached from the training process of DSSM and the EDIS module is only applied to the output of the deep neural network. Therefore, our method does not introduce the additional computational complexity for either offline training and online serving.

### Ablation Study (RQ2)

To investigate the effectiveness of each component in the proposed model, in this subsection, we conduct a series of ablation studies on the \( A2\) industrial datasets as follows:

* **w/o MSAC**, removes the Multimodal Semantic-Aware Clustering before the Interpolation-based negative sampling.
* **w/o EDIS**, removes the Effective Dense Interpolation Strategy employed in both simple negative sampling and hard negative sampling strategies.
* **w/o Multimodal Aligning**, removes the textual and visual modalities and reserves the behavior-based modality for further clustering.
* **w/o Secondary Codebook**, removes the secondary codebook in the Vector Quantized Clustering, thereby invalidating the interpolation-based hard negative sampling.

Table 4 presents the performance of these ablation experiments. Firstly, we can observe that adopting Multimodal-aligned Clustering Algorithm improves recall@50 by **6.41%** and recall@200 by

   Scenarios & \#A1 & \#A2 & \#A3 & \#A4 \\  \#User & 4,936,611 & 24,931,581 & 17,037,221 & 15,914,765 \\ \#Item & 2,163,338 & 4,268,324 & 2,905,716 & 3,067,253 \\ \#click & 61,579,472 & 336,746,141 & 194,341,806 & 163,611,291 \\ \#Impression & 2,353M & 9,648M & 4,274M & 6,134M \\   

Table 2. Statistics of the Industrial Dataset.

[MISSING_PAGE_FAIL:8]