# Using Model Calibration to Evaluate Link Prediction in Knowledge Graphs

Anonymous Author(s)

###### Abstract.

Link prediction models assign scores to predict new, plausible edges to complete knowledge graphs. In link prediction evaluation, the score of an existing edge (positive) is ranked w.r.t. the scores of its synthetically corrupted counterparts (negatives). An accurate model ranks positives higher than negatives, assuming ascending order. Since the number of negatives are typically large for a single positive, link prediction evaluation is computationally expensive. As far as we know, only one approach has proposed to replace rank aggregations by a distance between sample positives and negatives. Unfortunately, the distance does not consider individual ranks, so edges in isolation cannot be assessed. In this paper, we propose an alternative protocol based on posterior probabilities of positives rather than ranks. A calibration function assigns posterior probabilities to edges that measure their plausibility. We propose to assess our alternative protocol in various ways, including whether expected semantics are captured when using different strategies to synthetically generate negatives. Our experiments show that posterior probabilities and ranks are highly correlated. Also, the time reduction of our alternative protocol is quite significant: more than 77% compared to rank-based evaluation. We conclude that link prediction evaluation based on posterior probabilities is viable and significantly reduces computational costs.

Knowledge Graph Embedding, Link Prediction, Model Calibration +
Footnote †: ccs: Computing methodologies Semantic networks

+
Footnote †: ccs: Computing methodologies Semantic networks

+
Footnote †: ccs: Computing methodologies Semantic networks

+
Footnote †: ccs: Computing methodologies Semantic networks

+
Footnote †: ccs: Computing methodologies Semantic networks

+
Footnote †: ccs: Computing methodologies Semantic networks

+
Footnote †: ccs: Computing methodologies Semantic networks

+
Footnote †: ccs: Computing methodologies Semantic networks

+
Footnote †: ccs: Computing methodologies Semantic networks

+
Footnote †: ccs: Computing methodologies Semantic networks

+
Footnote †: ccs: Computing methodologies Semantic networks

+
Footnote †: ccs: Computing methodologies Semantic networks

+
Footnote †: ccs: Computing methodologies Semantic networks

+
Footnote †: ccs: Computing methodologies Semantic networks

+
Footnote †: ccs: Computing methodologies Semantic networks

## 1. Introduction

Knowledge graphs contain entities of interest (vertices) and relationships between them (directed, labeled edges) (Henderson, 2002). These graphs enable a focus on concepts rather than strings, so they are at the core of search engines, social networks, product catalogs, health and life-science services, and more (Henderson, 2002; Henderson, 2002; Henderson, 2002; Henderson, 2002). Knowledge graphs are typically incomplete due to knowledge acquisition problems that happen during their creation, such as extraction errors, unreliable information and information disparity (Henderson, 2002; Henderson, 2002). Knowledge graphs comprise (subject, predicate, object) triples, where subject and object are entities, and predicate is the relationship's label.

Link prediction consists of training a machine learning model to predict missing triples (Chen et al., 2016). Link prediction models typically output a score measuring the plausibility of a prediction that needs to be assessed against other scores (Chen et al., 2016; Chen et al., 2016; Chen et al., 2016; Chen et al., 2016; Chen et al., 2016; Chen et al., 2016). Assuming certain training, validation and test splits, the protocol to evaluate accuracy is as follows (Chen et al., 2016): for each triple in the validation/test split, rank its score in ascending order with respect to the scores obtained when the triple's subject is replaced by all available entities. All these new triples are considered negatives. Then, obtain a similar rank but replacing the triple's object. A link prediction model is considered accurate if it outputs low ranks, i.e., if the positive triples are ranked higher than their negative counterparts. Note that this protocol corresponds to the transductive case in which all entities are known during training (Chen et al., 2016), which is our focus in this paper.

The evaluation protocol has two shortcomings: 1) It is computationally expensive since it requires to generate many negatives per positive, compute their scores, and compare them to determine ranks; and 2) Assessing the plausibility of a single triple is not possible, since it is mandatory to compare its score w.r.t. the scores of other triples derived from it. To address the first shortcoming, Bastos et al. (2016) proposed to compute a distance between samples of positive and negative predictions made by a link prediction model. This distance is correlated to rank aggregations like mean rank. However, the distance provides a single value for the whole link prediction evaluation, and it does not take individual ranks into account; therefore, Bastos et al. (2016) do not address the second shortcoming. To address the second one, two studies explored model calibration for knowledge graphs, which outputs the posterior probability (the probability to be plausible) for an input triple (Santos et al., 2016; Chen et al., 2016). Unfortunately, these two studies did not analyze link prediction evaluation, but triple classification and predicate prediction evaluation. Hence, these studies do not address the first shortcoming. As a result, to the best of our knowledge, there is no approach in the literature that has jointly addressed these two shortcomings. Such an approach is appealing since link prediction evaluation requires significant computational resources (Chen et al., 2016; Chen et al., 2016), assessing individual triples is a must in production, and there is uncertainty about negatives due to the open-world assumption, i.e., it is unknown whether missing knowledge is correct or incorrect (Henderson, 2002).

In this paper, we propose an alternative protocol for link prediction evaluation based on the posterior probabilities output by a calibration function. This function is learned during the validation step when training the link prediction model. We use the triple scores output by the link prediction model at hand, and align thosescores with expected positives and negatives. Instead of ranks, our alternative protocol considers only the posterior probabilities of the triples present in the test split (positives). Thus, generating negatives is no longer necessary, significantly reducing computation time (first shortcoming). The posterior probability of a triple \(t_{i}\) determines its plausibility: \(t_{i}\) is negative or positive if \(f(x(t_{i}))[0,0.5)\) or \(f(x(t_{i}))[0.5,1]\), respectively, where \(f\) is the calibration function, and \(x\) the function that assigns a plausibility score to \(t_{i}\). Hence, we can determine the plausibility of \(t_{i}\) without comparing it to other triples (second shortcoming).

How accurate and reliable are calibration functions for link prediction evaluation? Are the posterior probabilities output by these functions statistically correlated to ranks? What is the time reduction of the alternative protocol based on posterior probabilities? Can we rely on posterior probabilities to compare link prediction models side by side? We experimentally answer these questions using nine different link prediction methods that are diverse (Bishop, 2006; Lee et al., 2016; Lee et al., 2016; Lee et al., 2016; Lee et al., 2016; Lee et al., 2016; Lee et al., 2016), i.e., they exploit a variety of mathematical constructs to compute scores, such as complex numbers, quaternions, and more. Calibration functions are learned using both Platt scaling and isotonic regression (Lee et al., 2016; Lee et al., 2016), i.e., two different approaches to compute posterior probabilities. Our experimental results show that calibration functions achieve high accuracy and reliability. The posterior probabilities output by these functions exhibit high correlation with ranks used to evaluate link prediction. The time reduction of the original vs. our alternative protocols is significant, between 77% and 99%. When comparing models side by side, more than 78% of the individual comparisons are preserved between the original and our alternative protocols.

The summary of our contributions is as follows:

* We discuss how to learn a calibration function for link prediction evaluation using Platt scaling and isotonic regression. As far as we know, it is the first time this has been studied.
* We propose an alternative protocol for link prediction evaluation based on the output of the calibration function learned. This new protocol only works with positives.
* We propose several ways of assessing the accuracy and reliability of the calibration function learned, and of our alternative protocol to evaluate link prediction.
* We conduct experiments involving popular methods, such as BoxE, HAKE, QuatE and TransE, and datasets, such as FB15K-237, NELL-995, WN18RR and YAGO3-10.

The rest of the paper is organized as follows: Section 2 introduces link prediction evaluation and model calibration. Section 3 discusses our alternative protocol. Sections 4 and 5 respectively present how to learn and assess calibration functions. Section 6 describes our experiments and results. Section 7 presents the related work. Finally, Section 8 presents conclusions and future work.

## 2. Background

A knowledge graph \(G\) is a set of \((s,p,o)\) triples, where \(s\) and \(o\) are entities and \(p\) is a predicate. We use \(E\) to denote the set of entities in \(G\), which is partitioned into training, validation and test, i.e., \(G_{}\), \(G_{}\) and \(G_{}\). A link prediction model comprises a number of embeddings (numerical vectors) that are associated to entities and predicates (Luo et al., 2016). Each model exploits a scoring function \(x(s,p,o)\) that assigns scores to input triples. These models are trained to minimize a loss function such that the model assigns low scores for positive triples (triples that belong to the graph), and high scores for negative triples (they do not belong to the graph).

Below, we introduce link prediction evaluation (Section 2.1) and model calibration (Section 2.2).

### Link prediction evaluation

Currently, link prediction is evaluated using ranking-based metrics (Chen et al., 2016). Since the goal is to predict low scores for positives, it follows that the metrics should measure a model's ability to do so. The idea is to register the position, rank, of a positive w.r.t. its negative counterparts when sorted by score in ascending order.

_Problem statement_. Link prediction evaluation consists of, for each triple \(t_{i}=(s,p,o) G_{}\), computing ranks \(r_{i}^{s}\) and \(r_{i}^{o}\) as follows:

\[r_{i}^{e}=1+|\{t_{i}^{} t_{i}^{} N_{LW}^{e}(t_{i},G) x (t_{i}) x(t_{i}^{})\}| \]

where \(e\) can be either \(s\) or \(o\), and \(x(t_{i})\) is the scoring function of the model under evaluation applied over \(t_{i}\):1

Above, \(N_{LW}^{e}(t_{i},G)\) computes the negative counterparts of \(t_{i}\) considering \(G\). Generating negative counterparts is challenging: knowledge graphs only contain positive triples (Kang et al., 2016), and they usually operate under the open-world assumption, i.e., triples that are not present in the graph at hand may be either missing or negatives (Lee et al., 2016). Different strategies generate the negative counterparts of a positive triple \((s,p,o)\). The local-closed world assumption (LCWA) is the most popular one (Chen et al., 2016), which is as follows:

\[& N_{LW}^{s}((s,p,o),G)=\{(s^{},p,o)(s^{ },p,o) G s^{} E\}\\ & N_{LW}^{o}((s,p,o),G)=\{(s,p,o^{})(s,p,o^{})  G s^{} E\} \]

Intuitively, LCWA uses every entity present in \(G\) as long as the corrupted triple is not in \(G\). This is known as the filtered setting since the whole \(G\) (no splits) is used to filter corrupted triples out (Chen et al., 2016). The raw setting uses \(G_{}\) only to filter corrupted triples out. Since the number of training and validation triples is typically larger than those in the test split, i.e., \(|G_{} G_{}||G_{}|\), the raw setting may consider many negatives that are present in \(G_{} G_{}\) and, therefore, are indeed positives. As a result, the filtered setting is preferred for link prediction evaluation (Chen et al., 2016).

_Metrics_. We focus on _MR_, the mean of the ranks of the positive triples. _MR_ is recommended to evaluate link prediction since others like the mean reciprocal rank are problematic (Kang et al., 2016; Lee et al., 2016; Lee et al., 2016), e.g., when using reciprocal ranks, the difference between 1 and 2 is the same as between 2 and \(\).2_MR_ is computed as follows:

\[MR=}|}_{t_{i} G_{}}r_{i}^{s }+r_{i}^{o} \]

The _MR_ value of an accurate link prediction model is expected to be closer to one (\(MR[1,)\)).

[MISSING_PAGE_FAIL:3]

sampling can be biased depending on the negative counterparts selected, we propose to use the whole set of negatives, and the following weights for positives and negatives, respectively:

\[w_{+}=}|}; w_{-}=(G_{},G_{TV })|} \]

where \(G_{TV}=G_{TR} G_{}\) and \(N_{LW}(G_{},G_{TV})\) are the negatives generated by LCWA as follows:

\[N_{LW}(G_{},G_{TV})=_{t G_{}}N_{LW}^{s}(t,G_{TV})  N_{LW}^{o}(t,G_{TV}) \]

Eq. 12 applies the filtered setting over the training and validation splits, i.e., the test split remains unseen since it will be used for evaluation purposes only.

Expected behaviorEven though LCWA is the de facto standard strategy, we can use other strategies to identify negatives within LCWA. The main benefit is that these negatives have certain expected semantics and, therefore, they are useful to shed light on the behavior of the calibration function. Bansal et al. (2016) defined the following strategies: global naive (GB) and type-constrained LCWA (TC). These are extra conditions applied to LCWA. For a given triple \(t=(s,p,o)\), the GB strategy is as follows (Bansal et al., 2016):

\[N_{}^{s}(t,G)=\{t^{} t^{}=(s^{},p,o) N_{LW} ^{s}(t,G) s^{} S(G)\} \]

where \(S(G)=\{s(s,p,o) G\}\) and \(O(G)=\{o(s,p,o) G\}\). Intuitively, \(s^{}\) and \(o^{}\) are entities that are never subjects and objects in \(G\), respectively. For instance, assume a graph such that locations only appear as objects; negatives generated using GB contain locations as subjects, which is never the case in this graph. GB is expected to generate nonsensical negatives: entities that are never subjects (objects) are forced to be subjects (objects).

Similarly, the TC strategy is as follows (Bansal et al., 2016):

\[N_{TC}^{s}(t,G)=\{t^{} t^{}=(s^{},p,o) N_{LW}^{s}(t, G) s^{} S(G,p)\} \]

where \(S(G,p)=\{s(s,p,o) G\}\) and \(O(G,p)=\{o(s,p,o) G\}\), i.e., \(s^{}\) and \(o^{}\) are entities that are subjects and objects in \(G\) for the predicate \(p\), respectively. Following the same example presented above, negatives generated using TC contain locations as objects, which is always the case in the graph at hand. TC is expected to generate negatives that are semantically plausible, i.e., they are more prone to be actual missing triples than other negatives.

We propose a third strategy, local naive (LC), as follows:

\[N_{LC}^{s}(t,G)=\{t^{} t^{}=(s^{},p,o) N _{LW}^{s}(t,G) s^{} C_{}(G,p)\} \] \[N_{LC}^{o}(t,G)=\{t^{} t^{}=(s,p,o^{}) N _{LW}^{o}(t,G) o^{} C_{}(G,p)\}\]

where \(C_{}(G,p)=O(G,p) S(G,p)\) and \(C_{}(G,p)=S(G,p) O(G,p)\). The expected semantics of these negatives is also nonsensical. However, different from GB, \(s^{}\) is an entity that is object but never subject of predicate \(p\) (similarly for \(o^{}\)). LC is more restrictive than GB: using the same example above, if a predicate does not have any locations as objects, these locations are never used to corrupt subjects, while these locations appear when applying GB.

## 5. Assessing \(f\)

Once \(f\) has been learned, we aim to evaluate its reliability and accuracy. The weighted Brier score (\(BS_{}\)) and the weighted coefficient of determination (\(R_{}^{2}\)) presented above are proper assessment measurements. We use the test split, \(G_{}\), for evaluation purposes, which has the same drawback as the validation split: it is highly imbalanced. We propose to use the same weighting scheme as define in Equation 11, where we consider the total number of negatives for a given strategy that synthetically generates negatives.

We also compute true and false positives and negatives as described above (Equation 9), and we aggregate them to compute different measurements like precision and recall. However, certain aggregations are more appealing than others. The reason why we rely on negative generation strategies is because knowledge graphs typically operate under the open-world assumption. This entails that positive triples in the test split are generally true.3 However, we are uncertain whether generated negative triples are indeed negatives. We thus expect _TP_ and _TN_ to be large and _FN_ to be small. However, we cannot make any assumptions about _FP_: some negatives may be considered positives because they are indeed positives. To mitigate this issue, we propose to use aggregations that consider positives and negatives independently. We focus on true positive and negative rates as follows:

\[=}{+};= }{+} \]

Note that other aggregations like precision will yield poor results because of the imbalanced nature of the datasets. This is the case for every aggregation that combines _TP_ and _FP_, or _TN_ and _FN_, which have different upper bounds. Both _TPR_ and _TNR_ can be combined into a single balanced accuracy measurement as follows:

\[=+}{2} \]

Expected behaviorEven though LCWA is the most common strategy, other negative generation strategies are also appealing to assess calibration functions. Specifically, using the expected semantics of these strategies is useful to gain additional insights on these functions. Since GB and LC are expected to generate nonsensical negatives, calibration functions should easily discern between positives and negatives. \(BS_{}\) and \(BA\) values when using GB and LC should thus remain low and high, respectively. Similarly, calibration functions should not be as accurate when using the TC strategy, since negatives are expected to be semantically plausible. \(BS_{}\) and \(BA\) values should thus increase and decrease, respectively.

Comparison with ranksWe wish to assess our alternative and the original protocols for link prediction evaluation. To do so, we propose to compare the relationship between posterior probabilities output by \(f\), and the ranks computed during link prediction evaluation, i.e., \(r^{s}\) and \(r^{o}\) for each triple in the test split. The ideal scenario is that, for a given pair of link prediction model and calibration function, ranks are linearly correlated with posterior probabilities. We thus focus on the Pearson correlation coefficient as follows:

\[r_{xy}=(x_{i}-)(_{i}-)}{(x_{i}- )^{2}(_{i}-)^{2}}}} \]

where \(x_{i}\) is either \(r^{s}\) or \(r^{o}\) for a given triple, and \(\) is the mean rank, i.e., _MR_. Correlation values range between -1 and 1, such that high correlation implies that \(r_{xy}-1\) or \(r_{xy} 1\), while \(r_{xy}=0\) means no correlation. There are two considerations. On one hand, ranks have different upper bounds (Steiner, 1995): for a triple \(t\) with ranks \(r^{s}\) and \(r^{o}\), \(r^{s}\) and \(r^{o}\) are respectively upper-bounded by \(|N_{LW}^{s}(t,G)|\) and \(|N_{LW}^{o}(t,G)|\). Therefore, we adjust ranks as follows:

\[^{s}=1--1}{|N_{LW}^{s}(t,G)|};^{o}=1--1}{|N_{LW}^{o}(t,G)|} \]

On the other hand, a single triple has two relative ranks associated to it, \(^{s}\) or \(^{o}\), but a single posterior probability, \(\). We duplicate posterior probabilities and produce the following two pairs for each triple: \((^{s},)\) and \((^{o},)\), which we use to compute \(r_{xy}\).

## 6. Experiments

In this section, we discuss the experiments we conducted. We present the datasets we used, and how we trained and learned link prediction models and calibration functions (Section 6.1). We discuss accuracy and reliability results (Section 6.2), as well as time results and comparisons between protocols (Section 6.3).

### Datasets and models

The datasets we used in our experiments and their total number of entities (\(|E|\)), predicates (\(|R|\)) and triples (\(|T|\)) are as follows:

These are commonly used to evaluate link prediction (Bao et al., 2017; Chen et al., 2018; Chen et al., 2019; Chen et al., 2019; Chen et al., 2019), are publicly available, and already partitioned into training, validation and test splits. BioKG integrates a number of biological databases into a single knowledge graph. FB15K and FB15K-237 were extracted from Freebase (Chen et al., 2018), Heitomet from the Heitomet integrative network constructed using millions of biomedical studies (He et al., 2017), NELL-995 from the Never-Ending Language Learning project (Nell et al., 2018), WN18 and WN18RR from WordNet (Wang et al., 2019), and YAGO-10 from YAGO (Wang et al., 2019). Note that FB15K-237 and WN18RR are subsets of FB15K and WN18, respectively, in which redundancy has been reduced. Furthermore, the validation and test splits publicly available for NELL-995 contain 0.4% and 2.6% of the triples, respectively. However, we found that the size of the validation split and the variety of the triples contained in it were detrimental to learn calibration functions. Therefore, we decided to reconfigure these validation and test splits to contain 1.6% of the triples each.

We trained a number of link prediction models 4 using the following link prediction methods: BoxE (Chen et al., 2018), CompLex (Yang et al., 2018), HAKE (Yang et al., 2018), HoLE (Yang et al., 2018), QuatE (Yang et al., 2018), RotatE (Wang et al., 2019), RotPro (Wang et al., 2019), TorusE (Choi et al., 2018), and TransE (Choi et al., 2018). These methods are diverse and use a variety of approaches: rectangles in the Euclidean space, complex numbers, points in a polar coordinate system, circular correlations in the Euclidean space, quaternion and rotations in the quaternion space, rotations and projection functions in the complex space, toruses in the Euclidean space, and Euclidean or Manhattan distances over real numbers. (See Appendix A for training details.)

For each link prediction model, we learned a single calibration function using isotonic regression, and six additional functions using Platt scaling, where we varied parameter initialization as follows: \(\{a,b\}\{-1,0,1\}\). We used the validation split to learn \(f\), and LCWA to generate negatives as described above.

### Accuracy and reliability results

What is the reliability and accuracy of the best calibration functions? Which calibration approach is superior, Platt scaling or isotonic regression? Are posterior probabilities correlated to relative ranks? Is the behavior of the calibration functions as expected when using different negative generation strategies?

We used \(R_{w}^{2}\) over the validation split to determine the best calibration function for each link prediction model. Figure 1 presents the five-number summary (min, max, mean, and first and third quartiles) of the accuracy of these models over the test split. We aggregate all of the \(R_{w}^{2}\) values for the datasets and group by link prediction method. We observe that all mean values are between 0.7 and 0.9 (see Figure 0(a)). This indicates that the posterior probabilities output by the calibration functions are good approximations of the ground truth of positive and synthetically-generated negatives. Similarly, the \(BS_{w}\) mean values are less or equal than 0.1 (see Figure 0(b)), which entails that posterior probabilities are properly calibrated. These best calibration functions also exhibit high \(BA\) values with mean values greater or equal than 0.9 (see Figure 0(c)). They are thus able to accurately discern between positives and negatives using the 0.5 threshold over the posterior probabilities. We also studied the correlation between relative ranks and posterior probabilities (see Figure 0(d)). Even though the \(r_{xy}\) mean values are greater than 0.55, we observe differences among calibration functions. The functions for the CompLex, HoLE and QuatE models are the ones achieving the best and most consistent correlation results, with mean \(r_{xy}\) values close to 0.8. In the next group, HAKE and TorusE achieve mean values closer to 0.6. RotPro and TransE are the next ones, with RotPro exhibiting a larger min value. Finally, BoxE and RotatE exhibit different behavior: the former achieves consistent correlation values around 0.6, while the range of values for RotatE is uneven between 0.4 and 0.8 with a mean of 0.7 (approximately). We note that the BoxE and RotatE link prediction models achieve uneven accuracy using ranks (described below), which is the reason why these correlation results deviate.

To study expected behavior, we combine the GB and LC strategies presented above, which are expected to output nonsensical negatives only. We also study TC that is expected to output semantically plausible negatives only. Figure 2 compares the results for both settings side by side. All the calibration functions behave as expected: the functions are able to better discern between nonsensical negatives and positives than between semantically-plausible negatives and positives. BoxE and TransE achieve best and second-best consistent behavior. Using these results, we can better understand some of previous results. For instance, we observe that RotatE and RotPro achieve a wider range of \(BS_{w}\) and \(BA\) values. Specifically, RotPro exhibits better values discerning negatives generated using TC than GB and LC, which is unappealing.

_Takeaways._ Calibration functions generally achieve competitive accuracy measured using \(R_{w}^{2}\), \(BS_{w}\) and \(BA\). Without exception, all of the calibration functions that achieve the best \(R_{w}^{2}\) results over the validation split were learned using isotonic regression. In other words, none of the functions learned using Platt scaling achieved better results than those using isotonic regression. Furthermore, many calibration functions exhibit high correlation (\(r_{xy}\)) between posterior probabilities and relative ranks. The exceptions are BoxE, RotatE and RotPro, which achieve the worse correlation results and, at the same time, exhibit uneven accuracy results measured using ranks. Calibration functions generally behave as expected when different strategies to generate negatives are exploited.

### Time and comparison results

Does the time taken to compute link prediction using our alternative protocol improve w.r.t. the original protocol? For the best-performing calibration functions, we study the time reduction between both protocols. Specifically, \(t_{A}\) is the time taken to learn \(f\) over \(G_{VA}\) plus the time taken to compute posterior probabilities over \(G_{TE}\). The time difference is \(\%At=100\) (\(t_{A}+t_{LP}\))/\(t_{LP}\), where \(t_{LP}\) is the time taken to evaluate link prediction using the original protocol. In Table 1, the time reduction is significant, more than 90%, in many cases. We also observe that RotPro and TorusE are generally the most and the least benefited from the alternative protocol, respectively. However, there is still a 77% time reduction for the TorusE model over NELL-995, which is the overall minimum.

The next question we aim to answer is: can we compare models side by side based on posterior probabilities instead of ranks? Even though there may not be a strong linear correlation (\(r_{xy}\)) between ranks and posterior probabilities, this correlation considers both positives and negatives, while our alternative protocol focuses on positives only. To answer the question, we sort link prediction models and calibration functions by accuracy. For presentation purposes, we scale \(MR\) as follows: \(MR^{}=1-(MR-min(MR))/(max(MR)-min(MR))\), where \(min(MR)\) and \(max(MR)\) are the minimum and maximum \(MR\) values obtained for all the models over the dataset

Figure 1. Five-number summary assessing the best calibration functions using LCWA to generate negatives. We aggregate the results obtained over the test split of each dataset under evaluation and group by link prediction method.

Figure 2. Five-number summary assessing the best calibration functions using the GB and LC (nonsensical) and the TC (semantically plausible) strategies, respectively. Results are aggregated and grouped by link prediction method.

[MISSING_PAGE_FAIL:7]

single, global value for each link prediction model (the distance between the sampled positive and negative subgraphs). In our case, we deal with individual predictions made by link prediction models, which entails that our approach has a finer level of granularity. Additionally, calibration functions can be used in production to assign posterior probabilities to new, unseen triples, and decide whether they are positives or negatives without comparing to other triples. This is not possible with the approach by Bastos et al. (Bastos et al., 2016).

Pezeshkpour et al. (Pezeshkpour et al., 2017) conducted an experiment in which triple scores from link prediction models were directly transformed into posterior probabilities applying the sigmoid function, i.e., \((x(t_{i}))\) for a given triple \(t_{i}\). The experiment compared positive triples with a sample of negative triples that were selected using three different strategies: LCWA, TC and another strategy similar to LC that takes types into account. The authors found that link prediction models are generally overconfident. The experiment did not consider Platt scaling or isotonic regression to compute posterior probabilities. The experiment relied on negative sampling, which is unappealing since different results may be observed for different samples. Applying the sigmoid function directly to model scores can be detrimental. The sigmoid function assumes that input values lie in the range of \((-,)\). However, distance-based methods like RotatE and TransE produce scores in the \((0,)\) range, which implies that all these posterior probabilities will lie in the \((0.5,1]\) range, i.e., they are all considered positives. Chen et al. (Chen et al., 2017) evaluated two types of functions to compute posterior probabilities: \((a\,x(t_{i})+b)\), equivalent to Platt scaling, and \(min(max(a\,x(t_{i})+b,0),1)\). Their proposal aims to enable link prediction for uncertain knowledge graphs rather than link prediction evaluation.

There have been many studies in the context of link prediction evaluation (Bastos et al., 2016; Chen et al., 2017; Chen et al., 2018; Chen et al., 2019; Chen et al., 2019). All of them focus on different aspects of the link prediction evaluation protocol, e.g., how hyperparameter optimization, graph splits and/or loss functions affect reported accuracy results. These studies are complementary to our work: model calibration can be applied to link prediction models learned in various ways.

### Model calibration for knowledge graphs

Tabacof and Costabello (Tabacof and Costabello, 2018) focused on triple classification evaluation. Different from link prediction, triple classification is a binary classification task that places every triple into one of two classes: positives that belong to the graph or negatives that do not. Tabacof and Costabello (Tabacof and Costabello, 2018) applied model calibration to several triple classification models in which the calibration function was learned by sampling synthetically-generated negatives instead of those available in the ground truth. These learned calibration functions achieved comparable results to those learned using negatives in the ground truth. We focus on link prediction evaluation in which there is no ground truth of negatives available. Furthermore, sampling synthetically-generated negatives may significantly alter the output of the calibration function and, therefore, the learning is not deterministic. In our case, we use the whole set of synthetically-generated negatives, so the learning process of our calibration functions is deterministic.

Safavi et al. (Safavi et al., 2019) studied the difference in the effectiveness of model calibration between open-world and closed-world assumptions for predicate prediction. Different from link prediction, predicate prediction aims to determine the missing predicate between two entities, subject and object. They used multiclass model calibration, in which each class is a given predicate in the knowledge graph at hand. The authors showed that model calibration for the closed-world assumption results in low expected calibration errors and high accuracy. We focus on link prediction rather than predicate prediction, which is more challenging since the number of predicates in knowledge graphs is generally orders of magnitude smaller than the number of entities. For the same reason, multiclass model calibration for link prediction is unfeasible.

## 8. Conclusions

There are a large number of link prediction methods that aim to predict missing triples. These methods are evaluated following a well-defined protocol: for every positive triple, generate a number of negative counterparts by corrupting its subject and object, but not both at the same time. Positives and negatives are sorted by score and the ranks of the positives are recorded. Accuracy is measured based on rank aggregations and it is expected that these ranks are close to one. Evaluating accuracy is thus a challenging task. Reducing the computational cost of this task is appealing; however, only a single approach has focused on such a reduction, as far as we know. The reduction consists of computing a single measurement for the whole evaluation that is correlated to rank aggregations. Unfortunately, this does not solve other shortcomings like determining the plausibility of an individual triple in isolation.

We explore the use of model calibration for efficient link prediction evaluation. A calibration function transforms link prediction scores into posterior probabilities such that \(0.5\) is the threshold to distinguish between positives and negatives. In our approach, first, each posterior probability indicates the plausibility of an individual triple. Second, learning calibration functions is accomplished at the validation step during the training of a link prediction model. Evaluation consists of computing the mean of the posterior probabilities of the positive triples, so negative counterparts can be avoided. Third, different strategies to generate negatives can be exploited when training and evaluating calibration functions. Our experiments show that the computational cost of link prediction evaluation is significantly reduced, that there is a high correlation between ranks and posterior probabilities, and that model comparisons using the mean of the posterior probabilities is similar to comparisons using mean ranks.

As future work, we aim to train a variety of link prediction models by altering the strategies to generate negatives, loss functions and graph splits. We will learn calibration functions for these link prediction models, and we will study their posterior probabilities. Besides being more efficient, we hypothesize that posterior probabilities will help shed more light on model behavior than ranks. We aim to apply posterior probabilities to interpret link prediction evaluation.

[MISSING_PAGE_FAIL:9]

Figure 3 presents the accuracy results for these calibration functions. The \(R_{w}^{2}\) and \(BS_{w}\) values are significantly lower and higher, respectively, than those achieved by isotonic regression. It is notable that the mean of the \(R_{w}^{2}\) values is negative in the cases of HolE and TransE. These results indicate that the learned functions are not properly calibrated. Similarly, all \(BA\) values are also lower than those achieved by the functions learned using isotonic regression. However, some functions achieve reasonable accuracy with a mean greater than 0.75: BoxE, HAKE, RotatE, RotPro and TorusE.

Correlation results for these functions are uneven. Many calibration functions using Platt scaling achieve lower \(r_{xy}\) values than their isotonic regression counterparts, e.g., RotPro. We also observe that the calibration functions for the TransE models have a wide range of values, which was not the case for isotonic regression. Finally, the functions for the ComplEx models also have a wider range of values than using isotonic regression, and some of these values slightly outperform the previous results. This is more evident in the QuatE models, which achieve better correlation results than the isotonic regression functions.

We conclude that functions learned using Platt scaling are generally not well calibrated. Even though, a few of them achieve better correlation results than isotonic regression, it is unappealing to rely on poor calibrated functions. Isotonic regression is thus preferred in the context of link prediction evaluation. Also, learning \(f\) using Platt scaling is more efficient than using isotonic regression. However, the performance of isotonic regression is generally quite superior.

Figure 3. Five-number summary of the accuracy of the calibration functions learned using Platt scaling and LCWA to generate negatives. Results are aggregated and grouped by link prediction method.