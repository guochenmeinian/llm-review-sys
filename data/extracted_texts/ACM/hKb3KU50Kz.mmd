# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

Our main contributions are as follows:

1. We demonstrate that by leveraging LLMs to enhance and maintain multiples indexes, we can get improvements in first stage retrieval performance. Our results are consistent across multiple bi-encoder models and various prompting strategies.

2. We detail the implementation of such a system using AWS services and thus provide a template for a multi index system that improves retrieval in an industrial setting.

## 2. Methodology

### Embedding Model and Dataset Details

For the purpose of demonstrating the effectiveness of our approach, we fine-tune bi-encoder models(Kang et al., 2018) for the customer service domain and use that across all our approaches as the first stage retrieval. All experiments are performed on publicly available self-help documents on the customer service tab on Amazon.com (example: Track Your Package). Because these articles are far fewer than in a web-scale setting, we use only a single stage of retrieval and some lightweight re-ranking as detailed in section 2.3.

In order to fine-tune the model, we generate around 30K data points in a self-supervised fashion by using the title of the document and the body content of the document as a positive pair. To this, we also include 10k queries mimicking the real customer input (and the corresponding ground truth document) that were manually labeled. In order to gather ground truth relevance information, we presented the annotators with the query and the top-6 options as picked from an LLM from an initial candidate pool of 15 candidates that was a generated by a previously optimized BM25-style system. It is worth noting that some of our annotators have previously worked as Customer Service Representatives, and so are domain experts in identifying the right document for a given customer query.

We train the biencoder embedding model using the Multiple-Negative ranking loss (Bahdanau et al., 2014). Our test set consists of 5K "difficult" queries (and a single relevant document as a ground truth answer) as rated by human annotators, that are representative of real customer queries to Amazon. Throughout the remainder of the paper, we present results with the MP-net model (Chen et al., 2019) that was pretrained on 1B sentences and is optimized for semantic search/relevance (Huggig Face model ID: sentence-transformers/all-mpnet-base-v2), though our results are consistent across a variety of bi-encoder models.

### Multi-Index enhancement

Inspired by previous work (Chen et al., 2019), we hypothesize that for dense retrieval, the presence of varied, but highly relevant terms that represent the main themes in the document can generate a more representative embedding than feeding in the entire document since the document may also include individual sentences that are unhelpful or not representative of the theme of the overall document which may perturb the embeddings generated by Bi-encoder models. Accordingly, we prompt an LLM model to produce separate 5-6 sentence and 2-3 sentence summaries for every document in the index. Also, inspired by past work (Chen et al., 2019), we recognize the importance of having additional expansion terms in the document to help BM25/biencoder models match the right document. We also prompt the LLM to generate 5 queries in a doc2query style (Chen et al., 2019) and 5 high-level labels or "tags" for the document.

After experimenting with various index-forms, we empirically zeroed-in on the following 5-index setup:

1) _Content Index_: This is the original HTML content of the document.

2) _LLM Summary Index_: In this index, the LLM generates a concise summary of the document in 6 sentences and that summary gets indexed.

3) _LLM Short Summary Index_: In this index, the LLM generates a highly concise summary of the document in 3 sentences and that summary gets indexed.

4) _LLM Questions and Tags Index_: In this index, the LLM generates 4 key questions that can be answered by the document in the doc2query style, and also 4 keywords or phrases that can describe the document and these get indexed.

5) _Metadata Index_: In this index, the document category information get indexed along with the title of the article.

Each entry in every index gets prefixed with the title of the document. We've found the title of the document to be the single most powerful signal in generating a representative embedding for the document. This also follows from our self-supervised training approach highlighted in the previous section.

### Combining Results from Indexes

Reciprocal rank fusion (Chen et al., 2019) gives us a way to combine ranks from different indexes based on a document's rank in individual indices. We use a variant of reciprocal rank fusion that also takes into account the presence of a document in the top-5 positions for that index - we use the following empirical formulate to score the documents. The final score for a document \(j\) can be calculated as:

\[score_{j}=(_{i 1}}{rank_{ij}})*frac_{j}{}\]

where \(frac_{j}\) is the fraction of indexes in which the document \(j\) appears in the top-5 results, \(sim_{ij}\) is the cosine similarity score between the embedding for the query and the document \(j\)'s representation in index \(i\), \(I\) represents the set of all indices and \(rank_{ij}\) represents the rank of document \(j\) in index \(i\).

## 3. Results

Recall performance is listed in Table 1 and the relative lifts in recall@k for various values of k and the different methods are presented in fig 2. As with re-ranking using LLMs, when relevance identification is offloaded to the LLM, as expected, an LLM is highly capable of identifying the top-1 and the top-2 results from the list of top-5 results, however we observe diminishing gains for k>2. This suggests that LLMs, while good at identifying the best of top-2 best documents from a list, don't particularly do well at exhaustively ranking a list of results.

We find that our best performing single-index for dense retrieval is indeed the short summary index that is produced by the LLM, introducing highly relevant terms and brief sentences that are highly representative of the high level semantic concepts in the document. As an example, consider the query '_i am currently in school and would like to try the amazon membership._' Dense retrieval over the default content index (baseline) ranks the 'Prime student' document and 'Join Prime student' document in 1st and 3rd place respectively. It ranks 'Sign up for Amazon Prime' at the 2nd place. When matched with the short and long summary, the 'Join Prime student' correctly ranks in 1st place with a much higher score. This document also ranks 1st in the _Questions and tags index_ and consequently is ranked 1st in the multi index setting. The text encoded as part of the Join Prime Student document across all indices is shown below. The embedding based on the summary of the document, and its tags and questions is much more representative of the concept "joining prime student", while in contrast, the embedding obtained from the raw document content is closer to the concept "verify student status" because of the nature of the exact text in the document. The text in the document actually contains quite a few sentences dedicated to verifying student status even though the high-level goal is to list steps to join Amazon Prime Student. Here is the example of the above self-help content represented across multiple indexes:

_Content Index:_ Content at Join Prime Student

_LLM Summary:_ Students can sign up for a free 6-month trial of Amazon Prime Student membership. To join, students go to the Prime Student website, fill out a form, and verify their email. The trial includes free 2-day shipping and other benefits like streaming media, music, photo storage, and discounts. Students can use a non-edu email but may need to resend a verification email. Current Prime members get a refund for remaining time and Prime Student benefits cannot be shared.

_LLM Short Summary:_ The article explains how to sign up for a free trial of Amazon Prime Student, which is Amazon's prime program tailored for students enrolled in college. It includes free 2-day shipping and other benefits like free streaming and gaming perks. To sign up, students need to verify their status by submitting proof of enrollment, a.edu email address, or documents

   \\ 
**Method** & **k-1** & **k-2** & **k-3** & **k-4** & **k-5** \\  HyDe & 5.07 & -2.97 & -5.13 & -7.37 & -8.74 \\  Query2Doc-CoT & 5.67 & -0.17 & -2.74 & -3.87 & -4.85 \\  Reranking-CoT & **50.14** & **24.27** & **12.22** & 4.66 & 0.00 \\  Multi-Index RRF & 13.78 & 8.21 & 9.78 & **9.55** & **7.31** \\ (ours) & & & & & \\  

Table 1. Relative Recall@K improvements of various methods

Figure 1. Software architecture of the indexing component (1) AWS SQS queue collects newly modified documents (2) AWS Lambda processs the queued documents and orchestrates indexing steps 3-5 (3) LLM in AWS Bedrock rewrites each input document in multiple forms (4) LLM model in AWS SageMaker produces an embedding for each document rewrite (5) AWS OpenSearch builds an index for each style of rewriting

Figure 2. Recall@K performance comparison

showing their age.

_LLM questions and tags_Queries: 'How do I sign up for Amazon Prime Student?', 'I'm a student, can I join Amazon Prime?', 'Is there a student discount for Amazon Prime?', 'I am in college and I would like to trial out the Amazon Student Prime membership program', 'tags': 'Prime Student Membership', 'Student Prime', 'Amazon Prime for Students', 'Sign up for Amazon prime student'

**Simulated Online Results** - In an experiment simulating a week-long traffic on amazon.com, we observed a 0.25% decrease in average agent Contact Per Customer (CPC) in response to improved ranking results for customer queries. Contact Per Customer (CPC) is an important business metric that stands as one of the proxies for the effectiveness of our automated solutions, ranked self help content being one of them.

### Comparison to other approaches

We compare our approach of index-enhancement to:

1. Using an LLM to augment the query at run-time by introducing expansion terms, _Query2Doc_(Golov et al., 2013; He et al., 2016).

2. Using an LLM to transform the query to document space, _HyDE_(Hardt et al., 2016).

3. Using an LLM to re-rank an initial set of results, _Reranker_(He et al., 2016).

For Query2Doc and Re-ranking, we use 5-shot CoT prompting with an LLM to generate query expansion terms and re-ranked results. Multi-Index RRF (using 5-shot prompting) outperforms all the compared approaches for Recall@5 performance while being highly amenable to a production setting. Crucially, by re-using the query embedding across the indices and parallelizing AWS Open Search calls to multiple indices, we achieve these gains at no extra latency (the latency overhead introduced by RRF/any heuristic re-ranking or filtering is negligible in this case). While re-ranking using an LLM shows superior recall@k performance for K\(<\)3, its performance is ultimately bounded by the quality of first stage retrieval. As such, each of the presented methods can be used in conjunction with the multi-index approach. In an industrial settings, there are strict constraints on latency for ranking/retrieval systems, rendering methods like HyDE or re-ranking with LLMs infeasible (refer the qualitative plot in fig 4). With 50B+ LLMs, it can consistently take over 0.8 sec to produce query transformations/ re-ranking with CoT which may not satisfy the latency constraints. The multi-index approach makes documents available to query in near-real time and once indexed, adds nearly no additional latency cost during query-time inference.

## 4. System Details

System details during indexing time and query time are shown in figure 1 and figure 3 respectively. To keep the documents up-to-date with Amazon's latest policies and offerings, a team of specialized authors in Amazon Customer Service compose new or update existing documents. With the help of AWS SQS and AWS Lambda, we fetch these document changes (including creation, update, and deletion) in a near-real-time fashion with a configurable delay in minutes (Step 1). Upon receiving a change, we enrich the index by sending the document to an LLM hosted in AWS Bedrock to produce the various forms of rewrite. With parallel invocation, it takes about 3 seconds for the LLM to rewrite a single document (Step 3). We maintain a separate index for different forms of rewrite. The updated document, in multiple rewritten forms, are then converted into embeddings by the same model hosted in AWS SageMaker

Figure 3. Software architecture of the inference component (4) LLM model in AWS SageMaker produces an embedding for the input customer query (5) AWS OpenSearch retrieves candidate documents from multiple document indices (6) AWS Lambda re-ranks all the top candidates using algorithms such as modified RRF (7) LLM model in AWS SageMaker paraphrases the best matching document into a coherent answer

Inference (Step 4). Note that this is not the exact system we use in production, and the diagrams here are for illustration purposes only to build out a system that is functionally capable of the components described.

The embedding model, which is shared across indices, creates one vector per document, per rewrite form, and the vectors are then collected into their respective indices with AWS Open Search (Step 5). Overall, the indexing micro-services make the new documents searchable in about 2 minutes. During inference, we compute the embedding of the customer query only once (Step 4), broadcast the query embedding across multiple Open Search indices, and retrieve the results in parallel (Step 5). Each of the parallel call returns its own ranking of candidate documents, and we truncate the ranking with a preset relevance score threshold to reduce noise from irrelevant documents during re-ranking.