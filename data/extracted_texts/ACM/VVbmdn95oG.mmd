# Linear-Time Algorithms for Representative

Subset Selection From Data Streams

Anonymous Author(s)

###### Abstract.

Representative subset selection from data streams is a critical problem with wide-ranging applications in web data mining and machine learning, such as social media marketing, big data summarization, and recommendation systems. This problem is often framed as maximizing a monotone submodular function subject to a knapsack constraint, where each data element in the stream has an associated cost, and the goal is to select elements within a budget \(B\) to maximize revenue. However, existing algorithms typically rely on restrictive assumptions about the costs of data elements, and their performance bounds heavily depend on the budget \(B\). As a result, these algorithms are only effective in limited scenarios and have super-linear time complexity, making them unsuitable for large-scale data streams. In this paper, we introduce the first linear-time streaming algorithms for this problem, without any assumptions on the data stream, while also minimizing memory usage. Specifically, our single-pass streaming algorithm achieves an approximation ratio of \(1/8-\) under \(O(n)\) time complexity and \(O(k)\) space complexity, where \(k\) is the largest cardinality of any feasible solution. Our multi-pass streaming algorithm improves this to a \((1/2-)\)-approximation using only three passes over the stream, with \(O()\) time complexity and \(O()\) space complexity. Extensive experiments across various applications related to web data mining and social media marketing demonstrate the superiority of our algorithms in terms of both effectiveness and efficiency.

Web data mining, streaming algorithm, data summarization, submodular maximization +
Footnote †: copyright}\)Corresponding held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06

+
Footnote †: copyright}\)Corresponding held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06

+
Footnote †: copyright}\)Corresponding held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06

+
Footnote †: copyright}\)Corresponding held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06

+
Footnote †: copyright}\)Corresponding held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06

+
Footnote †: copyright}\)Corresponding held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06

## 1. Introduction

Representative subset selection from large datasets is a fundamental problem with various data-driven applications related to web data mining and machine learning, including but not limited to social media marketing , recommendation systems , collective summarization  and feature selection . Many studies (e.g., ) have formulated this problem as the task of selecting a subset that maximizes a submodular function. This approach leverages the "diminishing returns" property of submodular functions to quantify the "representativeness" or "utility" of the selected subset. Moreover, constraints such as cardinality or knapsack constraints are typically imposed on the objective submodular function to model real-world limitations .

Since the seminal work of Fisher et al. , constrained submodular maximization problems have been extensively studied, with a variety of algorithms proposed that achieve good approximation ratios . However, the advent of big data has introduced new challenges, rendering many of these algorithms less practical due to their computational demands. Over the past decades, the exponential growth in data size has placed increasing demands on algorithmic efficiency, leading to substantial research efforts to develop faster submodular maximization algorithms. Early work in this line has achieved nearly linear time complexity of \(O_{}(n k)\), where \(O_{}\) hides \(\) factors, \(n\) denotes the size of the ground set and \(k\) denotes the maximum cardinality of any feasible solution1. More recent work (e.g., ) has focused on further reducing runtime, surpassing the nearly linear complexities of previous approaches and proposing clean linear-time algorithms for submodular maximization problems. Since linear time complexity is the minimum required to read all elements of the ground set, it is unlikely that any algorithm can be more efficient without employing parallelization, while still maintaining a reasonable approximation ratio . Moreover, in many domains, data volumes are expanding at a rate exceeding the capacity of computers to store them in main memory . Therefore, many studies such as  have focused on memory-efficient algorithms and proposed streaming submodular maximization algorithms, which takes a constant number of passes through the ground set while accessing only a small fraction of the data stored in main memory at any given time.

In this paper, we formulate the representative subset selection problem as the problem of monotone submodular maximization subject to a knapsack constraint (abbreviated as the **SMKC** problem). The knapsack constraint is a fundamental constraint that can capture real-world limitations such as budget, time, or size, and thus the SMKC problem has been extensively studied since 1982 .

Currently, for this problem, existing work (Shi et al., 2018; Shi et al., 2019) has only successfully proposed linear-time algorithms with provable approximation ratios in the offline setting, where all data must be stored in main memory--an impractical requirement for many real-world applications. In the streaming setting, the current best single-pass (Shi et al., 2019) (resp. multi-pass (Shi et al., 2019)) streaming algorithm can only achieve the super-linear time complexity of \(( B)\) and the space complexity of \(( B)\) (resp. \(( B)\)) with an approximation ratio of \(1/3-\) (resp. \(1/2-\)), where \(B\) is the budget for the knapsack constraint. Note that the value of \(B\) also influences the complexities of other existing streaming algorithms for the SMKC problem to the same extent, if not more (refer to Table 1). In the worst case, the budget \(B\) can be arbitrarily large and grow exponentially with the input size \(n\), resulting in a quadratic or worse complexity for these algorithms. More critically, the approximation ratios of existing algorithms are derived under the assumption that the cost of each element is no less than 1. These algorithms suggest using normalization to ensure the assumption holds, thereby supporting their approximation ratios. However, such normalization is impractical for single-pass streaming algorithms as the costs are not known in advance, rendering their performance guarantees perhaps invalid. Meanwhile, this normalization implies that \(B\) cannot be normalized to reduce time and space complexity in these algorithms, further compounding the efficiency issues. Therefore, we aim to answer the following questions in this paper:

* Given that the assumptions and performance guarantees of existing streaming algorithms for the SMKC problem may not always hold in practical scenarios, is it possible to design more practical streaming algorithms for the SMKC problem that maintain provable performance guarantees without relying on restrictive assumptions?
* Furthermore, if such algorithms exist, can they achieve linear time complexity while using minimal memory?

### Our Contributions

In this paper, we provide confirmative answers to the above questions, by presenting two novel streaming algorithms for the SMKC problem without any assumptions on the data stream. The contributions of our paper can be summarized as follows:

* We propose a single-pass streaming algorithm dubbed OneStream that achieves an approximation ratio of \(1/8-\) for the SMKC problem. The time and space complexities of the OneStream algorithm are \((n)\) and \((k)\), respectively. To our knowledge, OneStream is the _first_ streaming algorithm with a provable approximation ratio and linear time complexity for the SMKC problem.
* Based on the OneStream algorithm, we further propose a multi-pass streaming algorithm, dubbed MultiStream, which achieves an approximation ratio of \(1/2-\) within three passes over the data stream. This matches the best ratio achieved by existing streaming algorithms for the SMKC problem. However, while existing streaming algorithms require super-linear time complexity, our MultiStream algorithm only has a linear time complexity of \(()\) under \(()\) space complexity.
* We conduct extensive experiments using several real-world applications related to the web, including maximum coverage on networks and revenue maximization on networks. The experimental results strongly demonstrate the effectiveness and efficiency of our algorithms.

### Challenges and Techniques

To our knowledge, existing streaming submodular maximization algorithms with linear time complexity are limited to handling cardinality (Shi et al., 2019; Shi et al., 2019) or matroid constraints (Bahdanau et al., 2016; Krizhevsky et al., 2012; Krizhevsky et al., 2012), and fail to offer performance guarantees for the knapsack constraint. Additionally, many techniques used in these algorithms are specific to cardinality or matroid constraints and do not easily extend to knapsack constraints. For example, the linear-time streaming algorithms for cardinality constraints rely heavily on the fact that a solution with \(k\) elements satisfies the constraint. This property is used (1) to control the number of elements maintained by the algorithm, thereby ensuring that memory consumption stays within an acceptable bound of \((k)\), and (2) to select the last \(k\) elements from the tail of the solution set to form the final feasible solution. However, in the SMKC problem, we lack prior knowledge of the value of \(k\), and a solution with \(k\) elements may not necessarily satisfy the knapsack constraint. Similarly, the performance guarantees of linear-time streaming algorithms for matroid constraints rely on the exchange property of matroids, a characteristic absent in knapsack constraints.

Moreover, existing streaming algorithms for the SMKC problem rely on guessing an "ideal threshold" to achieve their approximation ratios, and they find this threshold through a canonical geometric search process under the assumption that each element's cost is at least 1. However, their threshold guessing approach needs extra time and memory complexity of \(( B)\), resulting in unsatisfactory super-linear time complexity, especially since the budget \(B\) can be arbitrarily large and even grow exponentially with the input size \(n\) in the worst-case scenario. To address cases where elements' costs are less than 1, existing streaming algorithms suggest using normalization to ensure that each element's cost is at least 1. However, such normalization is impractical for single-pass streaming algorithms as the costs are not known in advance, which invalidates their performance guarantees. Furthermore, this normalization implies that \(B\) cannot be normalized to reduce time and space complexity in these algorithms, further compounding the efficiency issues.

To address the above challenges, our OneStream algorithm maintains a "cumulative set" \(_{i=j}^{i}S_{t}\), which consists of a small number of candidate solutions \(S_{t} t[j,i]\). Each candidate solution \(S_{t}\) is initialized as an empty set and grows by adding elements from the data stream until it becomes a "nearly feasible solution", i.e., a set that satisfies the knapsack constraint by removing at most one element. OneStream also computes a threshold based on the utility of the cumulative set to control the cost-effectiveness of elements added to the candidate solutions. This approach eliminates the need for the time-consuming geometric search process used in previous streaming algorithms to find an ideal threshold. By constructing nearly feasible solutions, OneStream offers two key benefits: (1) it limits the size of each candidate solution to at most \(k+1=(k)\);and (2) it ensures the final solution (a subset of the cumulative set satisfying budget \(B\)) is of high quality, as each element in the cumulative set has a cost-effectiveness ratio above the computed threshold. Besides, OneStream employs a sliding window mechanism to control the total number of nearly feasible solutions stored in memory. Thanks to these techniques, OneStream achieves linear time complexity with minimal memory usage.

Our MultiStream algorithm leverages the OneStream algorithm to efficiently guess the "ideal threshold" (associated with the utility of the optimal solution), which guides a better selection of elements to obtain an improved approximation ratio. Different from existing threshold guessing approaches, we dynamically choose an easier-to-find ideal threshold based on the cost distribution of elements in the optimal solution in our proof, which is combined with On-eStream's ability to provide accurate upper and lower bounds for the utility of the optimal solution in linear time, enabling an efficient threshold guess process without relying on the assumption that each element's cost is at least 1. This guess process incurs only a small amount of extra \((1/)\) (rather than \(( B)\)) time and memory overhead, ensuring that MultiStream achieves a better approximation ratio while maintaining linear complexities. More details about our algorithms can be found in Section 4-5.

Due to the space limit, we defer the detailed proofs of most lemmas and theorems to Appendix A, while only providing some intuitions and key ideas for them in the main text.

## 2. Related Work

### Algorithms for Monotone Submodular Maximization Under a Knapsack Constraint

Monotone submodular maximization under a knapsack constraint (i.e., the SMKC problem) has been extensively studied (Zhou et al., 2017; Li et al., 2018; Li et al., 2019) in the offline setting. Among these works, Li et al. (2019) achieved the optimal approximation ratio of \(1-1/\), but its \((n^{3})\) time complexity renders it impractical for real-world applications. Subsequent studies (Zhou et al., 2017; Li et al., 2019; Li et al., 2019; Li et al., 2019; Li et al., 2019) put effort into more efficient algorithms, and recent work (Li et al., 2019; Li et al., 2019) have proposed linear-time algorithms for the SMKC problem, where (Li et al., 2019) achieves \((1/2-)\)-approximation using \(()\) time complexity. However, these algorithms are still limited to the offline setting, requiring all elements to be stored in memory, which is impractical in many real-world scenarios.

Recently, great efforts have been devoted to designing streaming algorithms for the SMKC problem, as shown by Table 1. Among single-pass streaming algorithms, (Zhou et al., 2017) achieves the best approximation ratio of \(2/5-\) with time complexity of \((^{B})\), while (Li et al., 2019) offers the best time complexity of \(( B)\) with a worse approximation ratio of \(1/3-\). Among multi-pass streaming algorithms, (Li et al., 2019) first achieved an approximation ratio of \(1/2-\) using \(()\) passes over the data stream, with time complexity of \((}^{2}B)\). Subsequently, (Li et al., 2019) developed a new streaming algorithm aimed at avoiding the large polynomial factors of \(1/\) in (Li et al., 2019)'s complexity, reducing the time complexity to \((n(+ B))\) while maintaining the same approximation ratio and number of passes over the data stream. However, (Li et al., 2019) later pointed out errors in the theoretical analysis of (Li et al., 2019), invalidating its approximation guarantee. (Li et al., 2019) proposed a new algorithm that restores the \(1/2-\) approximation ratio using two passes over the data stream with time complexity of \(( B)\). However, as shown in Table 1, the complexities of all existing streaming algorithms depend on \(B\), which, in the worst case, can grow exponentially with the input size \(n\), leading to quadratic or worse time complexity for these algorithms. More critically, the approximation ratios of existing algorithms are derived under the assumption that the cost of each element is no less than 1. These

   Passes & Reference & Ratio & Space Complexity & Time / Query Complexity \\  \(=1\) & (Kumar et al., 2017) & \(1/6-\) & \(( B)\) & \(( B)\) \\  & (Li et al., 2019) & \(4/11-\) & \((^{4}B)\) & \((^{4}B)\) \\  & (Li et al., 2019) & \(2/5-\) & \((^{4}B)\) & \((^{4}B)\) \\  & (Li et al., 2019) & \(1/3-\) & \(( B)\) & \(( B)\) \\  & Algorithm 1 & \(1/8-\) & \((k)\) & \((n)\) \\   & (Li et al., 2019) & \(1/2-^{*}\) & \((B)\) & \((n(+ B))\) \\  & (Li et al., 2019) & \(1/2-\) & \((^{2}B)\) & \((^{2}B)\) \\  & (Li et al., 2019) & \(1/2-\) & \(( B)\) & \(( B)\) \\  & Algorithm 2 & \(1/2-\) & \(()\) & \(()\) \\   & (Li et al., 2019) & \(1/2-\) & \((B)\) & \((n(+ B))\) \\  & (Li et al., 2019) & \(1/2-\) & \((^{2}B)\) & \((^{2}B)\) \\   & (Li et al., 2019) & \(1/2-\) & \(( B)\) & \(( B)\) \\   & Algorithm 2 & \(1/2-\) & \(()\) & \(()\) \\   

* \(k\) denotes the largest cardinality of any feasible solution, \(B\) is the budget. Bold font and magenta color indicate the best results in each setting.
* \(()\) & \(()\) \\   
* \(k\) denotes the largest cardinality of any feasible solution, \(B\) is the budget. Bold font and magenta color indicate the best results in each setting.
* \(()\) & \(()\) \\   
* \(k\) denotes the largest cardinality of any feasible solution, \(B\) is the budget. Bold font and magenta color indicate the best results in each setting.
* \(()\) & \(()\) \\   
* \(k\) denotes the largest cardinality of any feasible solution, \(B\) is the budget.

algorithms suggest using normalization to ensure the assumption holds, thereby supporting their approximation ratios. However, such normalization is impractical for single-pass streaming algorithms, rendering their performance guarantees perhaps invalid. Meanwhile, this normalization implies that \(B\) cannot be normalized to reduce time and space complexity in these algorithms, further compounding the efficiency issues.

### Linear-Time Algorithms for Streaming Submodular Maximization

Chakrabarti and Kale (Chakrabarti and Kale, 2011) pioneered the achievement of linear complexity for streaming submodular maximization. Their algorithm is tailored for matroid constraints, requires one pass over the data stream, and uses \((n)\) time complexity and \((k)\) space complexity to achieve a \(1/4\) approximation ratio. Subsequently, (Kumar et al., 2017) reduces the number of queries in (Chakrabarti and Kale, 2011)'s algorithm from \(2n\) to \(n\), while maintaining the same approximation ratio and complexities. (Kumar et al., 2017) further generalize (Kumar et al., 2017)'s algorithm to handle not necessarily monotone submodular functions, achieving a \(1/11.66\) approximation ratio with the same complexities.

For simpler cardinality constraints, (Kumar et al., 2017) proposed a linear-time single-pass (resp. multi-pass) streaming submodular maximization algorithm, achieving \(1/4\) (resp. \(1-1/e-\)) approximation ratio with \((n)\) (resp. \((n/e)\)) time complexity and \((k k(1/e))\) (resp. \((k k)\) space complexity. (Kumar et al., 2017) extends (Kumar et al., 2017)'s algorithms to handle not necessarily monotone submodular functions, achieving \(1/23.313-\) (resp. \(0.25-\)) approximation ratio using one (resp. \((1/)\)) pass(es) over the data stream with unchanged complexities.

However, as explained in Section 1.2, none of these algorithms offer any approximation guarantee for our SMKC problem, and many techniques used in these algorithms are tailored to cardinality or matroid constraints, which do not readily generalize to knapsack constraints. Thus, whether there exists a linear-time streaming algorithm for the SMKC problem remains an open question.

## 3. Problem Statement

We consider the problem of selecting a representative subset of elements from a streaming dataset \(\) of size \(n\), aiming to maximize a non-negative set function \(f:2^{}_{ 0}\). For any subset \(S\), \(f(S)\) quantifies the utility of \(S\), i.e., how well \(S\) represents \(\) according to some objective. In many data summarization problems (e.g., (Kumar et al., 2017; Kale and Kale, 2011; Kale and Kale, 2011; Kale and Kale, 2011; Kale and Kale, 2011)), the utility function \(f()\) exhibits an intuitive property known as submodularity characterized by diminishing returns. The function with submodularity can be defined as follows:

**Definition 3.1** (Submodular Function).: A set function \(f:2^{}_{ 0}\) is submodular if for all \(X Y\) and \(u Y\), it holds that \(f(u Y) f(u X)\), where \(f(u S)=f(S\{u\})-f(S)\) represents the marginal gain of \(u\) with respect to \(S\) for any \(S\{X,Y\}\).

Intuitively, submodularity implies that adding an element \(u\) to a set \(Y\) yields no more utility gain than adding \(u\) to a subset \(X\) of \(Y\). Besides, \(f()\) is monotone if \(f(X) f(Y)\) for all \(X Y\), indicating that adding a new element never decreases the utility. In this paper, we assume that the utility function \(f()\) is monotone and submodular. Furthermore, we consider a fundamental constraint that the feasible solution follows a knapsack constraint, which can model real-world constraints such as budget, time, and size.

Assume that each element \(u\) has an associated cost \(c(u)\), and the total cost of a set \(S\) is defined as a modular function \(c(S)=_{u S}c(u)\). Our subset selection problem can then be formulated as the problem of submodular maximization subject to a knapsack constraint (abbreviated as the **SMKC** problem):

\[\{f(S):S c(S) B\},\]

where \(f()\) is submodular and monotone; \(B 0\) is the given budget. Following common practice in submodular optimization, we assume that there exists an oracle that can return the value of \(f(S)\) for any \(S\). Oracle queries typically have a significantly higher time complexity than other basic operations, so the efficiency of submodular optimization problems is commonly measured by the number of oracle queries (Bauer and Kale, 2011; Kale and Kale, 2011; Kale and Kale, 2011). We study the SMKC problem in the streaming setting, where elements in \(\) arrive sequentially in an arbitrary order. The streaming algorithm is allowed to make a few passes over the elements, using a small memory.

Without loss of generality, we assume that \(c(u) B\) for every \(u\), as any element with a cost exceeding the budget can be immediately discarded upon arrival. Throughout this paper, we denote an optimal solution to the SMKC problem as \(O\), the element with the highest cost in \(O\) as \(o_{m}\) (i.e., \(o_{m}=_{u O}c(u)\)), and the maximum cardinality of any feasible solution as \(k\). For notational convenience, let \([i]=\{1,,i\}\) for any natural number \(i\).

## 4. The Single-Pass Streaming Algorithm

In this section, we propose our single-pass streaming algorithm dubbed OneStream, which is the _first_ streaming algorithm with a provable approximation ratio and linear time/query complexity for the SMKC problem. Moreover, OneStream does not rely on the assumption that each element's cost is at least \(1\), which may not hold in single-pass scenarios where elements cannot be normalized in advance. This makes OneStream more practical than existing single-pass streaming algorithms.

### Algorithm Design

As shown by Algorithm 1, the OneStream algorithm maintains a "cumulative set" \(_{i=j}^{t}S_{i}\) composed of a small number of candidate solutions, where each candidate solution is initialized as an empty set when first added to the cumulative set and then gradually grows by incorporating valuable elements from the data stream until it becomes a "nearly feasible solution" (a set that satisfies the knapsack constraint after removing no more than one element). More specifically, the algorithm uses a threshold based on the utility (i.e., objective function value) of the cumulative set to check the marginal cost-effectiveness (i.e., marginal gain/cost) of each incoming element from the data stream (Line 4). If the element satisfies the threshold requirement, it is added to the most recent candidate set \(S_{i}\) in the cumulative set (Line 5). Once this candidate set grows as a nearly feasible solution, a new candidate set \(S_{i+1}\) is initialized to receive elements that meet the threshold requirement, and the process repeats (Line 6 and Line 9). By using the threshold based on the utility of the cumulative set itself, the OneStream algorithm avoids the geometric search for a suitable threshold, thus achievinglinear complexity. Moreover, constructing candidate solutions as nearly feasible solutions offers a two-fold benefit: (1) limiting the cardinality of each solution to no more than \(k+1=(k)\), ensuring less memory consumption; (2) ensuring the cost of each solution is no less than \(B\), coupled with the fact that the cost-effectiveness of each element in the solution surpasses the threshold, guarantees a satisfactory overall utility.

OneStream algorithm also employs a sliding window mechanism to control the number of the nearly feasible solution, to ensure the total memory consumption is small. Specifically, if the total number of these solutions reaches the predefined limit of \(2h\), OneStream deletes the oldest \(\) sets from the cumulative set \(_{t=j}^{i}S_{t}\) (Line 7-8). Recall that the threshold used to add elements in Line 4 depends on \(f(_{t=j}^{i}S_{t})\), which increases as the algorithm runs. Thus, elements added earlier are tested by lower thresholds and are likely to have lower utility. Consequently, deleting these older elements results in only a small loss in utility.

When the data stream ends, the cumulative set \(_{t=j}^{i}S_{t}\) may be an unfeasible solution. To extract a good feasible solution from it as the final output, OneStream searches for a feasible solution \(S(z)\) from the tail of \(_{t=j}^{i}S_{t}\) (Line 14). The intuition behind this is that elements added later have passed the test by higher thresholds and are more likely to possess good utility. The one with better utility between \(S(z)\) and the best singleton element set (generated by Line 10) is then returned as the final solution \(Q^{*}\) (Line 15).

```
Input: integer \(h 1\)
1 initialize \(i 1\), \(j 1\), \(S_{i}\) and \(e^{*}\) null;
2 take a new pass over the data stream;
3while there is an incoming element \(e\)do
4if\(^{i}S_{t})}{e(e)}^{i}S_{t})}{B}\)then
5\(S_{i} S_{i}\{e\}\);
6if\(c(S_{i})\)then
7if\(i-j+1=2h\)then
8\(_{t=j}^{i}S_{t}\); \(j j+\);
9\(i i+1\); \(S_{i}\);
10
11\(e^{*}_{e\{e^{*},e\}}f(\{u\})\);
12\(i_{n} i\); \(j_{n} j\);
13if\(c(_{t=j_{n}}^{i_{n}}S_{t}) B\)then\(Q^{*}_{t=j_{n}}^{i_{n}}S_{t}\) ;
14
15else
16 let \(S(x)\) denote the set of the last \(x\) elements added to \(_{t=j_{n}}^{i_{n}}S_{t}\); find \(z[_{t=j_{n}}^{i_{n}}S_{t}]\) such that \(c(S(z)) B c(S(z+1))>B\);
17\(Q^{*}_{Q\{S(z),e^{*}\}}f(\{Q\})\); return\(Q^{*},_{t=j_{n}}^{i_{n}}S_{t}\)
```

**Algorithm 1**OneStream (\(\))

### Theoretical Analysis

Our overall analysis approach is as follows. We first demonstrate that the complete "cumulative set" \(_{t=1}^{i}S_{t}\) without any deletions can provide an upper bound for the utility of the optimal solution upon the termination of the algorithm (Lemma 4.1); then show that the utility loss caused by deleting old nearly feasible solutions from the cumulative set is small and can be bounded (Lemma 4.2-4.4). Based on these, we can prove that the final solution obtained after solution deletion and element extraction can also upper bound the utility of the optimal solution, resulting in the approximation ratio of the algorithm (Lemma 4.5).

Lemma 4.1 ().: _Upon termination of Algorithm 1, the following inequality holds: \(f(_{t=1}^{i_{n}}S_{t}) f()/2\)._

To demonstrate that the utility loss caused by deleting nearly feasible solutions in Line 8 is small, we first demonstrate that incorporating a new nearly feasible solution \(S_{t}\) into the cumulative set \(_{t=j}^{-1}S_{t}\) doubles the utility of it (Lemma 4.2), resulting in a continuous increase in the utility of the cumulative set as OneStream runs, even when the deletion occurs (Lemma 4.3).

Lemma 4.2 ().: _At the end of the each iteration of the **while** loop in Algorithm 1, we must have \(f(_{t=j}^{}S_{t}) 2 f(_{t=j}^{-1}S_{t})\) for any \(q[j+1,i]:c(S_{q}) B\)._

Lemma 4.3 ().: _Let \(T_{i}\) (\(i>1\)) denote the state of \(_{t=j}^{i}S_{t}\) right before the execution of Line 9 in Algorithm 1. Consider the iteration of the **while** loop in Algorithm 1 when \(T_{i}\) is generated, then:_

* _If the deletion in Line 8 is not executed in the current iteration, we have_ \(2 f(T_{i-1}) f(T_{i})\)_._
* _Otherwise, we have_ \(f(T_{i-1}) f(T_{i})\)_._

Building upon the previous two lemmas, we can demonstrate that solution deletions do not result in a significant utility loss and the cumulative set finally stored in memory retains a substantial utility, as shown by Lemma 4.4.

Lemma 4.4 ().: \(f(_{t=1}^{i_{n}}S_{t})(1+-1})f(_{t=j_{n}}^{i_{n}} S_{t})\)

Before deriving the approximation ratio of Alg. 1, we only need to prove the solution \(Q^{*}\) returned by the algorithm can upper bound the final cumulative set \(_{t=j_{n}}^{i_{n}}S_{t}\), based on the observation that elements added later to \(_{t=j_{n}}^{i_{n}}S_{t}\) have passed higher threshold tests than those added earlier, thus possessing high utility of \(_{t=j_{n}}^{i_{n}}S_{t}\).

Lemma 4.5 ().: \(f(_{t=j_{n}}^{i_{n}}S_{t}) 4 f(Q^{*})\)

Combining Lemma 4.1, 4.4 and 4.5, we can immediately get the performance bounds of OneStream, as shown by Theorem 4.6.

Theorem 4.6 ().: _By setting \(=_{2}()+1\) where \((0,1)\), OneStream can return a solution \(Q^{*}\) satisfying \(c(Q^{*}) B\) and \(f(Q^{*})(1/8-)f(Q)\) for the SMCK problem in a single pass over the data stream. The time/query and space complexities of the algorithm are \((n)\) and \((k)\), respectively._

Proof.: The approximation ratio can be directly derived by combining Lemmas 4.1, 4.4, and 4.5. For each incoming element, the algorithm incurs one oracle query at Line 4 and another at Line 10, resulting in a total of \(2n\) oracle queries and a time complexity of \((n)\). As shown in Lines 6-9, the algorithm maintains at most \(\) candidate solutions, each with a size no more than \(k+1\), leading to a space complexity of \((k)\).

```
Input: integer \(h 1\) and number \(e(0,1)\)
1\(M_{1},M_{2}(h);P\{(1-e)^{-2}:z\)
2\(_{})}{2B}(1-e)^{-z} f(M_{2})}{c}\};\)
3 initialize \(A_{}\) for each \( P\) and \(L^{*} M_{1};\)
4 take a new pass over the data stream;
5while there is an incoming element \(e\)do
6foreach\( P f(e)/c(e)\)do
7if\(c(A_{})+c(e) B f(e A_{}) c(e)\)then
8\(A_{} A_{}\{e\};\)
9if\(f(M_{})>f(L^{*})\)then
10\(L^{*} A_{};\)
11
12 take a new pass over the data stream;
13while there is an incoming element \(e\)do
14foreach\( P\)do
15if\(e A_{} c(A_{})+c(e) B f(A_{}\{e\}) f (L^{*})\)then
16\(L^{*} A_{}\{e\};\)
17
18foreach\( P\)do
19foreach\(e A_{}\)do
20if\(e L^{*} c(L^{*})+c(e) B\)then\(L^{*} L^{*}\{e\}\) ;
21
22return\(L^{*}\)
```

**Algorithm 2**MultiStream \((h,e)\)

## 5. The Multi-Pass Streaming Algorithm

In this section, we propose our multi-pass streaming algorithm dubbed MultiStream, which improves the approximation ratio to \(1/2-\), matching the best ratio achieved by existing streaming algorithms for the SMKC problem while maintaining linear time complexity.

### Algorithm Design

As shown by Algorithm 2, MultiStream algorithm first efficiently guess the "ideal threshold" related to the utility (i.e., objective function value) of the optimal solution, based on OneStream algorithm (Line 1). It then performs element selection based on each guessed threshold \( P\) (i.e., potential ideal threshold) within two passes over the data stream (Line 2-14). The specific element selection process is as follows. For each incoming element in the data stream, the algorithm first selects thresholds from \(P\) that are smaller than the current element's cost-effectiveness (Line 5), as the candidate solution with a threshold larger than the element's cost-effectiveness would not accept the element, rendering further checking unnecessary. Then, for each selected threshold \(\), the algorithm adds the element to \(A_{}\) if the element satisfies both the knapsack constraint and the threshold requirement (Line 6-7). The candidate solution with the best utility is stored in \(L^{*}\) (Line 8-9). Subsequently, the algorithm re-reads the data stream and attempts to insert each element into each existing candidate solution without violating the knapsack constraint, thereby enhancing the candidate solution's utility (Line 10-14). Finally, the algorithm attempts to insert elements stored in memory into the current optimal solution \(L^{*}\), further improving the utility of the returned solution in practice (Line 15-17).

### Theoretical Analysis

As shown by Lemma 5.1, if the element with the highest cost in the optimal solution (i.e., \(o_{m}\)) has a large utility, we can directly conclude the algorithm exhibits a favorable approximation ratio.

**Lemma 5.1**.: _If \(f(\{o_{m}\}) f(0)/2\), the solution returned by \(L^{*}\) Algorithm 2 satisfying \(c(L^{*}) B\) and \(f(L^{*})(1/2-)f(O)\)._

Proof.: The lemma follows as \(f(M_{1}) f(L^{*})\) and OneStream ensures that \(f(M_{1})_{u}f(\{u\})\). 

Now focus on the case where the utility of \(o_{m}\) is relatively small. We divide our following analysis into two cases based on whether \(o_{m}\) consumes the majority of the budget of the optimal solution \(O\), and then demonstrate that in both cases, the algorithm can find a candidate solution with a corresponding ideal threshold that can upper bound the utility of the optimal solution (Lemma 5.2-5.3). A key purpose of the case-by-case discussion is to quickly find the ideal threshold without relying on the assumption that the cost of any element is no less than 1, which can be better understood through the following example. The \(B-c(o_{m})\) in the ideal threshold \(^{*}\) of Lemma 5.3 might be very small or even zero. Therefore, we consider the case where it is greater than or equal to \(eB\), ensuring that \(^{*}\) can be quickly found through geometric search.

The proof ideas for Lemma 5.2 can be explained as follows. We first establish the existence of a candidate solution with the ideal threshold \(^{*}\). Then we show that if the cost of this candidate solution is sufficiently large, its utility is also sufficiently large due to the threshold filtering process. Otherwise, the candidate solution retains enough budget to include all elements in the optimal solution except for \(o_{m}\). Thus, elements in \(O\{o_{m}\}\) that are excluded from this candidate solution must have low marginal cost-effectiveness, which implies excluding these elements causes little utility loss. The proof of Lemma 5.3 follows a similar line of reasoning.

**Lemma 5.2**.: \(fB-c(o_{m})<eB\) _and \(f(\{o_{m}\})<f(O)/2\), then Algorithm 2 can generate a candidate solution \(A_{^{*}}\) satisfying \(c(A_{^{*}}) B\) and \(f(A_{^{*}})(1/2-)f(O)\), where \(^{*}[))},))}]\)._

**Lemma 5.3**.: \(fB-c(o_{m}) B\)_, then Algorithm 2 can generate a candidate solution \(A_{^{*}}\) satisfying one of the following conditions:_

1. \(c(A_{^{*}}) B\) _and_ \(f(A_{^{*}})(1/2-)f(O)\)__
2. \(c(A_{^{*}}\{o_{m}\}) B\) _and_ \(f(A_{^{*}}\{o_{m}\})(1/2-)f(O)\)__

_where \(^{*}[))},))}]\)._

Based on the two lemmas above, we can readily derive the performance bounds of MultiStream, as shown in Theorem 5.4.

**Theorem 5.4**.: _By setting \(h=O(1)\), MultiStream can return a solution \(L^{*}\) satisfying \(c(L^{*}) B\) and \(f(L^{*})(1/2-)f(O)\) for the SMKC problem within three passes over the data stream. The time/query and space complexities of the algorithm are \(O()\) and \(O()\), respectively._

Proof.: Since Line 11-14 of Algorithm 2 ensure that \(f(L^{*}) f(A_{^{*}}\{o_{m}\})\) when the second condition of Lemma 5.3, we also have \(f(L^{*}) f(O)/2\) when the case described by Lemma 5.3 happens. Combining this result with Lemmas 5.1-5.2 yields the approximation ratio.

Note that Algorithm 2 maintains at most \((|P|)\) candidate solutions. Moreover, for each candidate solution \(A_{}\) (\( P\)), the algorithm incurs at most \(n\) oracle queries at Lines 5, 6 and 13, respectively. Thus, the query/time and space complexities of Algorithm 2 are \((n|P|)\) and \((k|P|)\), respectively. Based on the definition of \(P\) and the fact that \(f(M_{2}) 4 f(M_{1})\) due to Lemma 4.5, we have \(|P|=()\). Combining these results completes the proof.

## 6. Performance Evaluation

In this section, we empirically evaluate the performance of our algorithms against the state-of-the-art streaming algorithms for two real-world applications of the SMKC problem, including maximum coverage on networks and revenue maximization on networks. The metrics compared include the utility (i.e., the objective function value), the number of oracle queries to the objective function, and the maximum number of elements in memory. The following four algorithms are implemented in the experiments:

* OneStream: our single-pass streaming algorithm (i.e, Algorithm 1).
* MultiStream: our multi-pass streaming algorithm (i.e, Algorithm 2).
* DynamicMRT : the state-of-the-art single-pass streaming algorithm for the SMKC problem.
* SmkStream : the state-of-the-art multi-pass streaming algorithm for the SMKC problem.

All our experiments are conducted on a Windows workstation with Intel(R) Core(TM) i7-14700 @ 2.10 GHz CPU and 64GB memory2. For each of the implemented algorithms, the parameter \(\) for accuracy (if any) is set to 0.1.

### Maximum Coverage on Networks

Maximum coverage has various real-world applications such as web monitoring , influence maximization , community detection  and sensor placement . This application has also been considered in previous studies, such as . Given a network \(G=(,E)\), our goal is to identify a subset of seed nodes \(S\) that can influence a large number of users within a budget \(B\). This goal is formulated as maximizing a monotone submodular function:

\[\{f(S)=|_{u S}N(u)|:c(S) B\},\]

where \(N(u)=\{v:(u,v) E\}\) denotes the neighbors of \(u\). Following , each node \(u\) is associated with a non-negative cost \(c\{(u)\}=1+\), where \(d(u)\) represents the out-degree of \(u\), and the costs of all nodes are normalized so that the average cost is 2 and the cost of each element is at least 1, ensuring that the approximation ratios of baselines are valid. In our experiments, we use two network datasets sourced from SNAP : (1) the epin

Figure 1. Experimental results of maximum coverage on networks

network with 131,828 nodes and 841,372 edges; and (2) the email network with 265,214 nodes and 420,045 edges.

Figure 1 shows the experimental results of maximum coverage on networks. It can be observed that our OneStream algorithm achieves almost 90% of the best utility results using only \(2n\) oracle queries, which is 3.43% (resp. 3.09%) of the query count of the super-linear time complexity algorithm DynamicMRT (resp. SmkStream) algorithm on average. This demonstrates that our algorithm significantly improves the efficiency of solving the SMKC problem while sacrificing little utility. Moreover, our OneStream algorithm exhibits the lowest memory consumption, occupying only 1.72% (resp. 5.45%) of the memory used by DynamicMRT (resp. SmkStream) algorithm on average, highlighting its exceptional memory efficiency. Regarding our MultiStream algorithm, it consistently achieves the best utility while using significantly fewer queries and lower memory consumption compared to the baseline algorithms with super-linear time complexity (i.e., DynamicMRT and SmkStream). While OneStream and MultiStream both have linear time complexity and small space complexity, we observe that MultiStream performs worse than OneStream on query count and memory consumption in experiments due to the additional \(\) constant term in complexity.

### Revenue Maximization on Networks

This application is based on the social network marketing model proposed by (Song et al., 2018), and is considered by many previous studies (e.g., (Beng et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018)). In this application, we are given a network \(G=(N,E)\) where each node \(u\) represents a user with an associated cost \(c(u)\), and each edge \((u,v) E\) has a weight \(w_{u,v}\) denoting the influence of \(u\) on \(v\). Our goal is to select a subset \(S\) of seed users within a budget \(B\) (i.e., \(_{u S}c(u) B\)), and pay \(c(u)\) to each seed user \(u S\) for advertising products to maximize the total revenue. The revenue function is defined as

\[f(S)=_{u}w_{u,u}},\]

which is monotone and submodular as indicated by (Li et al., 2018; Li et al., 2018). Following (Li et al., 2018; Li et al., 2018; Li et al., 2018), the network \(G\) is constructed by randomly selecting 25 communities from the top \(5,000\) communities in the YouTube social network(Li et al., 2018); the edge weights are randomly sampled from the continuous uniform distribution \((0,1)\); the cost of any user \(u\) is determined by \(c(u)=w_{u,v}}\), and the costs of all nodes are normalized so that the average cost is 2 and the cost of each element is at least 1, ensuring that the approximation ratios of baselines are valid.

Figure 2 shows the experimental results for revenue maximization on networks, which further demonstrates the effectiveness of our proposed algorithms. More specifically, our OneStream algorithm achieves approximately 94% of the best utility results, while using only 3.47% (resp. 2.80%) of the query count of the DynamicMRT (resp. SmkStream) algorithm and occupying only 3.00% (resp. 6.17%) of the memory used by DynamicMRT (resp. SmkStream) algorithm. Our MultiStream algorithm consistently achieves the best utility while using significantly fewer oracle queries and lower memory compared to the baseline algorithms with super-linear time complexity (i.e., DynamicMRT and SmkStream). Again, these results demonstrate the superiority of our algorithms in terms of both time and memory usage.

## 7. Conclusion

In this paper, we study the problem of extracting a representative subset from data streams, formulated as maximizing monotone submodular functions subject to a knapsack constraint. Existing streaming algorithms for this problem only achieve super-linear time complexity depending on the budget, potentially reaching quadratic or even higher complexities in the worst case. Moreover, these algorithms rely on a restrictive assumption which may render their performance guarantees invalid in practical scenarios. To address these limitations, we propose a more practical single-pass streaming algorithm that does not depend on such an assumption, achieving an approximation ratio of \(1/8-\) with linear complexity of \((n)\) and space complexity of \(O(k)\). Furthermore, we propose a multi-pass algorithm achieving an approximation ratio of \(1/2-\), matching the best achievable approximation ratio in streaming settings while maintaining linear time complexity and minimum memory usage. The experiments on real-world applications related to web data mining and machine learning demonstrate the superiority of our algorithms in terms of both effectiveness and efficiency.

Figure 2. Experimental results of revenue maximization on networks

Linear-Time Algorithm for Representative

Subset Selection From Data Streams