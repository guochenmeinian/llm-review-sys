# FG-CIBGC: A Unified Framework for Fine-Grained and Class-Incremental Behavior Graph Classification

Anonymous Author(s)

###### Abstract.

Learning-based Behavior Graph Classification (BGC) has been widely adopted in Internet infrastructure for partitioning and identifying similar behavior graphs. However, the research communities realize significant limitations when deploying existing proposals in real-world scenarios. The challenges are mainly concerned with (i) fine-grained emerging behavior graphs, and (ii) incremental model adaptations. To tackle these problems, we propose to (i) mine semantics in multi-source logs using Large Language Models (LLMs) under In-Context Learning (ICL), and (ii) bridge the gap between Out-Of-Distribution (OOD) detection and class-incremental graph learning. Based on the above core ideas, we develop the first unified framework termed as Fine-Grained and Class-Incremental Behavior Graph Classification (**FG-CIBGC**). It consists of two novel modules, i.e., gPartition and gAdapt, that are used for partitioning fine-grained graphs and performing unknown class detection and adaptation, respectively. To validate the efficacy of FG-CIBGC, we introduce a new benchmark, comprising a new 4,992-graph, 32-class dataset generated from 8 attack scenarios, as well as a novel Edge Intersection over Union (EIoU) metric for evaluation. Extensive experiments demonstrate FG-CIBGC's superior performance on fine-grained and class-incremental BGC tasks, as well as its ability to generate fine-grained behavior graphs that facilitate downstream tasks. The code and dataset are available at: [https://anonymous.4open.science/r/FG-CIBGC-70BC/README.md](https://anonymous.4open.science/r/FG-CIBGC-70BC/README.md).

**CCS Concepts - Mathematics of computing \(\) Graph algorithms; - Networks \(\) Network security.**

## 1. Introduction

Sophisticated attacks increasingly threaten the global internet infrastructure. As graph offers an ideal representation for security investigation (Hernandez et al., 2017), analysts often transform audit logs into a large, unified graph containing numerous operations. However, navigating and investigating the large-scale graph presents a non-trivial challenge of heavy analysis workload (Kirchhoff et al., 2017). Behavior Graph Classification (BGC) addresses this challenge by partitioning the large graph into a set of smaller behavior graphs and subsequently classifying them, enabling analysts to focus on a few representative behaviors. BGC has emerged as an indispensable technique for various security investigation domains (Kirchhoff et al., 2017), including Host Intrusion Detection Systems (HIDSs), vulnerability detection, _etc_.

Existing solutions on BGC task can be categorized into three types: pattern-based (Zhu et al., 2017; Wang et al., 2017), rule-based (Zhu et al., 2017; Wang et al., 2017; Wang et al., 2017; Wang et al., 2017), and learning-based (Zhu et al., 2017; Wang et al., 2017). The former two rely on static patterns and expert knowledge, demanding heavy manual effort. Learning-based methods have addressed this limitation by leveraging machine learning models. While it sounds promising, the research communities have uncovered a series of limitations when implementing the learning-based approaches in real-world scenarios. By summarizing those issues in Fig. 1, we recognize the following two main challenges.

**(i) Fine-Grained Emerging Behavior Graphs.** Prior learning-based proposals rely solely on coarse-grained behavior graphs. That is to say, while these methods can ascertain whether a behavior graph is related to a specific service (e.g., "apache"), they lack the granularity to differentiate between distinct operations that service, such as distinguishing "apache processing request 1" from "apache processing request 2". Partitioning discrete operations from large graphs generated by audit logs remains a formidable challenge, as existing approaches consolidate all operations on a given object into a single graph, limiting their ability to differentiate distinct service operations. Yet fine-grained labels are pivotal for analysts to comprehend service activities and deploy effective countermeasures. Existing coarse-grained BGC approaches present a significant _semantic gap_ between graph identification and actionable intelligence. A more granular scheme capable of automatically distinguishing distinct service operations would substantially enhance the understanding of attack vectors and facilitate targeted

Figure 1. An illustration of fine-grained and class-incremental behavior graph classification task. This task faces both _fine-grained emerging behavior graphs_ and _incremental model adaptations_ challenges, leading to performance degradation of state-of-the-art baselines.

defenses. _Consequently, the primary challenge lies in developing a methodology for fine-grained behavior graph partition._

**(ii) Incremental Model Adaptations.** In real-world scenarios, behavior graphs evolve in an incremental manner, presenting the requirement of _class increments. Class increments_ refer to emerging novel classes that should be incrementally updated into the model to become known classes subsequently. In production environments, continuous service updates introduce novel behavior graph classes unknown to analysts (also known as the _open-world_ issue). Actively detecting and attaching new classes to a model's knowledge base without _catastrophic forgetting_ is a significant challenge. Despite the importance, graph-level class-incremental learning with novel class detection remains a largely unexplored area.

In this paper, we propose the first unified framework called Fine-Grained and Class-Incremental Behavior Graph Classification (**FG-CIBGC**), aiming to enable two vital abilities, i.e., the fine-grained behavior graph partitioning and incremental model updates with novel class detection. At the high level, FG-CIBGC is designed with two novel components named _gPartition_ and _gAdapt_. **First**, gPartition processes multi-source logs by leveraging Large Language Models (LLM) under In-Context Learning (ICL) paradigm to correlate semantically similar logs, forming _behavior units_. Each _behavior unit_ corresponds to a single operation performed by a service application and is subsequently converted into a compact behavior graph. **Second**, gAdapt is responsible for utilizing Out-Of-Distribution (OOD) detection to identify unknown classes, assigns fine-grained labels to both known and unknown classes of behavior graphs, and incrementally updates the model accordingly. **Finally**, existing BGC benchmarks suffer from insufficiencies in both completeness (lacking multi-source logs) and diversity, as well as the absence of metrics tailored to the fine-grained requirements of BGC tasks. To address these issues, we introduce a new benchmark comprising 8 attack scenarios and 3 log types, amounting to 4,992 graphs across 32 classes. Additionally, we propose a novel metric, Edge Intersection over Union (EIoU), to fully evaluate FG-CIBGC.

In summary, this paper makes three key contributions:

* Through analysis of current learning-based behavior graph classification proposals, we identify two critical challenges that impede their deployment. To tackle these challenges, we propose FG-CIBGC, the first unified framework for fine-grained and class-incremental behavior graph classification.
* We design two novel components (i.e. gPartition and gAdapt) for FG-CIBGC, thereby realizing the fine-grained behavior graph identification and incremental model update with novel class detection simultaneously.
* We construct a new benchmark that satisfies both _completeness_ and _diversity_, featuring 3 log types, 4,992 graphs across 32 classes, as well as a new EIoU metric for fine-grained evaluation. Extensive experiments demonstrate the superiority of FG-CIBGC.

## 2. Related Work

**Behavior Graph Classification.** Existing BGC methods can be divided into three categories: **(i) pattern-based methods**(Kumar et al., 2017; Wang et al., 2018) mine graph patterns from behaviors of interest and use them as templates to identify similar behaviors; **(ii) rule-based methods**(Kumar et al., 2017; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) match audit events against a knowledge store of rules that describe behaviors; **(iii) learning-based methods**(Kumar et al., 2017; Wang et al., 2018) utilize machine learning models to represent behavior graphs as vectors, enabling identifying of semantically similar behaviors. Compared with prior work, we pioneer the exploration of class-incremental BGC task, thereby demonstrating greater competence in real-world scenarios. Furthermore, we are the first to produce fine-grained behavior graphs matching operations in services.

**LLMs-based Log Processing.** In recent years, with the increase in model sizes and richer training corpora, LLMs have notably grown in power. Given the vast pretraining datasets that encompass logging-related data, LLMs possess immense potential for log processing tasks. Existing research has explored the application of large language models across a wide range of log-related tasks, including **log parsing**(Kumar et al., 2017; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018), **vulnerability detection**(Wang et al., 2018; Wang et al., 2018) and **anomaly detection**(Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). Compared with prior works, we uniquely explore the capabilities of large language models in correlating multi-source log data. Furthermore, we have designed a novel type-position-aware prompt format to enable more effective in-context inference of log correlation using the LLMs.

**Class-Incremental Graph Learning.** Recently, class-incremental graph learning has garnered growing attention owing to its broad applications (Chen et al., 2018; Wang et al., 2018), with existing works in this domain generally falling into three primary categories: **regularization-based**(Kumar et al., 2017), **architecture-based**(Kumar et al., 2017), and **replay-based**(Kumar et al., 2017; Wang et al., 2018) methods. Crucially, existing methods have largely assumed that all data comes from a predefined set of classes known to humans. However, this assumption does not hold for behavior graph classification task in real-world scenarios, where previously unknown classes may emerge during the learning process. Compared with prior works, we are the first to bridge the gap between Out-of-Distribution (OOD) detection methods and class-incremental graph learning, thereby handling _class increments_ in real-world scenarios.

## 3. Problem Definition

Our goal is to incrementally identify and classify semantically similar behavior graphs within a stream of multi-source logs. Given that fine-grained and class-incremental behavior graph classification involves two critical challenges, we provide precise definitions for each of these respective aspects.

**Fine-Grained Emerging Behavior Graphs.** Given a prior dataset \(_{tra}\) comprising multi-source logs (e.g., audit, application, and network logs), we first extract multiple fine-grained behavior graphs \(_{tra}\) from the dataset. The term _fine-grained behavior graph_ denotes a representation where each behavior graph precisely specifies an operation of a service. Each behavior graph \(G_{i}_{tra}\) corresponds a category label \(y_{i}_{tra}\), where \(_{tra}=\{y_{tra}^{1},y_{tra}^{2},,y_{tra}^{n}\}\) where \(n\) refers to the number of known classes. And we use \(_{tra}\) as the training set to fit the model \(\). When deploying the model \(\) in practice, it will encounter the open-world test set \(_{t}\) at stage \(t\), which includes: (i) samples whose ground-truth labels are present in the training set \(_{tra}\); and (ii) instances of emerging unknown classes \(\{y_{1}^{1},y_{t}^{2},,y_{tra}^{m}\}\), where \(m\) denotes the number of unknown classes, which is unknown to us a priori.

**Incremental Model Adaptations.** The _class increments_ represent an inherent challenge in class-incremental learning, and it distinguishes our approach from existing techniques. We assume there is no prior knowledge when suffering unprecedented behavior graphs in practice. When a novel class of behavior graphs emerges, we aim to proactively detect it and seamlessly incorporate it into the knowledge base, thereby strengthening the model's classification performance. Crucially, the updated model should be able to accurately identify previously unseen classes of behavior graphs in the future. This is a formidable challenge that eludes the capabilities of existing approaches.

## 4. Proposed Method

In this section, we present the proposed Fine-Grained and Class-Incremental Behavior Graph Classification (**FG-CIBGC**).

### Overview

As shown in Fig. 2, the proposed FGCIBGC consists of two novel modules: gPartition and gAdapt. **First**, gPartition is proposed to learn the correlation of multi-source logs in a prompt and finish the log correlation process. gPartition builds on the in-context learning paradigm with the large language models (LIMs). The derived correlation results, referred to as _behavior units_, are then used to partition the audit logs into fine-grained behavior graphs, which are subsequently reduced to achieve greater compactness. **Second**, gAdapt is proposed to proactively detect both known and unknown classes, classify them accurately, and correspondingly update the model in an incremental fashion. This is especially achieved through the synergistic combination of Out-Of-Distribution (OOD) detection and class-incremental graph learning.

### Partition: Fine-Grained Graph Partition

**Challenges Analysis.** Audit logs alone contain only low-level information about system activities, lacking the necessary knowledge to partition them into fine-grained operations. To accurately capture operations, the research communities observe that introducing logs with high-level semantics offers a more promising solution (Shen et al., 2017; Wang et al., 2018). Specifically, application logs of services are designed to record each

Figure 2. The main framework of the proposed FG-CIBGC.

operation and its attributes, while network logs explicitly track the corresponding network sessions incurred by each operation.

A vanilla method would be to leverage existing log correlation approaches to correlate these diverse log sources, allowing audit logs belonging to the same operations to be used in generating fine-grained behavior graphs. However, prior log correlation techniques rely on static correlation rules and overlook the potential for different logs to describe the same operation. This renders the use of logs with high-level semantics to partition audit logs into fine-grained behavior graphs a _fundamentally_ new task.

**Rationale Behind gPartition.** To address this challenge of multi-source log correlation, we resort to the In-Context Learning (ICL) paradigm of large language models (LLMs), as LLMs have demonstrated superiority in log processing tasks. ICL with LLMs not only enables mining of the latent semantic correlations across multi-source logs, but also bolsters the advantages of rapid deployment and easy interactivity without large tuning costs. Additionally, we propose a novel type-position-aware prompt template specifically designed for log correlation tasks, coupled with a warmup mechanism that enhances the ICL capabilities of LLMs.

**Model Backbone.** The performance of the large language model is a key factor in the success of ICL. Considering that log messages are semi-structured sentences that are mainly composed of natural language descriptions (i.e., log template) [(53)], we chose GPT-3.5 [(5)], an LLM that is pre-trained on an extremely large amount of semantic information from the open-source corpus, as the backbone for gPartition. Recent large language models, including GPT-3.5, have demonstrated in-context learning (ICL) capabilities, motivating its use as the backbone for gPartition. As gPartition utilizes the LLM in a black-box manner, the backbone model can be replaced as long as the relevant API is accessible.

**Prompt Strategy.** Prompt strategy is the most significant part of ICL. To preprocess the influx of multi-source logs before prompting, we first parse the logs into standardized forms. After that, we merge these parsed logs in chronological order and segment them into batches of 400 entries. This approach considers that logs representing the same operation are typically temporally close, and a service's related operations do not correlate with an excessive number of logs.

Before designing the prompt template, an essential question arises: _How should we model log correlation task to facilitate understanding by LLMs?_ To enable LLMs to perform the _"which-correlates-with-which"_ task while adhering to the _"one query, one inference"_ principle, we explicitly represent log types and position information, transforming the problem into a compact format. Specifically, we have observed that position closeness is a crucial factor in log correlation, and different types of logs contain various fields that serve distinct roles in the correlating process. To address this, we insert two special tokens, namely <type> and <lineF>, at each newline to indicate the log type and line ID, respectively, and subsequently flatten the log sequences into a linear format. Additionally, the instruction necessitates that the model provide the <type>|<lineF> pairs to form a _behavior unit_, where each _behavior unit_ represents the complete set of multi-source logs generated by a single service operation. This type-position-aware prompt format ensures both efficiency and accuracy in inference, as illustrated in Fig. 3.

It is important to note that the selection of examples in the prompt significantly impacts the downstream task performance of LLMs under the ICL paradigm. In this study, we utilize KATE [(24)] for in-context example augmented selection. Due to page limit, the details of the selection algorithm are provided in the Appendix.

**Warmup Strategy.** Given that the model's ICL abilities can be enhanced through a warmup phase prior to ICL inference, we employ the following warmup process: gPartition first randomly selects 800 samples from the validation set to serve as prompt queries for the warmup. For each query, gPartition employs the aforementioned selection algorithm to identify the eight most similar samples in the training set as prompt examples, appending their ground-truth labels. These are then combined with fixed instructions to form a complete prompt. Subsequently, all prompts will be submitted to GPT-3.5 for parameter tuning in batches.

**Behavior Graph Generation.** Prior works showed the conversion of unstructured logs into a unified graph. Having obtained the behavior units, we implement an audit log parser to transform the audit logs within the same behavior units into a behavior graph, where each edge represents an audit log. This allows the audit logs to be partitioned into multiple small-scale behavior graphs. _However, not all audit logs belonging to the same operation can be incorporated into a behavior unit simply through log correlation._ For instance, system configuration activities of an operation may leave no traces in high-level semantics. To tackle this issue, we design a novel heuristic search algorithm to capture operations comprehensively. The overall workflow is shown in Alg 2.

**Graph Reduction.** Background noisy events resulting from the inherent low-level nature of auditing mechanism are massive, and they do not impact BGC task according to prior works [(10; 17)]. To reduce noisy events, we employ a few existing graph reduction algorithms including LogGC [(20)], CPR [(43)] and NodeMerge [(38)].

Figure 3. The prompt templates omit insignificant log details for simplicity. Each prompt contains several labeled examples and one query. The last example in the prompt is the most similar to the query, whereas the first example is the least similar.

### gAdapt: Incremental Model Adaptation

**Challenges Analysis.** A vanilla design would be to directly apply an existing class-incremental graph learning method to implement gAdapt. However, when deploying existing proposals in real-world scenarios, the research community faces _class increments_ challenge that renders a straightforward design impractical. This is because the unknown class labels are inherently unpredictable, as the set of system operations is continuously expanding. Yet, the majority of existing methods tend to overlook this _open-world_ problem.

In order to detect unknown classes, Out-Of-Distribution (OOD) Detection is showing promising potential recently. Therefore, a straightforward method is to use existing graph OOD detection method in an unsupervised way. However, they only finish _partial_ of task requirements, that is, they only tell if a given behavior graph is OOD, but can not assign label to them so making it incapable of attaching new classes to the model's knowledge base. Besides, they do not consider the challenge of determining a threshold to decide if a datapoint is OOD.

**Rationale Behind gAdapt.** To address the aforementioned challenges, we implement gAdapt based on the idea that bridging the gap between OOD detection and class-incremental learning. We adopt a disentangled graph encoder as the model backbone, as graph formation typically follows a complex relational process, and such disentangled representation learning has shown promising results. Additionally, we employ replay-based methods for model updates. Upon receiving new samples, an OOD Detector generates OOD scores to identify _class increments_. The OOD samples are then clustered to obtain new-class labels, and used to update the model, alongside the structure incremental samples.

**Features Extraction.** We propose a novel strategy to extract node features for behavior graphs. In our analysis, we identified three distinct types of nodes within each behavior graph: processes, files, and sockets. Given the heterogeneous nature of these nodes, their respective feature vectors comprise different elements. For process nodes, we utilize [process_name, p_id, exe_name] as the feature set. File nodes are characterized by [file_name, inode, file_type], while socket nodes are represented by [ip, port, socket_type]. To encode all textual components within these feature sets, we employ FastText (Chen et al., 2017), a library for efficient learning of text representations.

**Outlier Detection.** We choose GOOD-D (Zhu et al., 2018) as basic OOD Detector for its ability to detecting OOD graphs without using ground-truth labels. It is worth noting that OOD samples are often noisy. In real-world deployment, manual labeling is often required to enable custom configuration and adapt the model to new OOD samples (Hardt et al., 2016). However, this incurs nontrivial annotation cost.

With Weakly-supervised Relevance Feedback, we propose a method to overcome the challenge of custom configuration and human labeling. We introduce the hyperparameter \(q\), which is a domain-interpretable value of the expected ratio of new OOD behavior graphs in the next time step. Specifically, we first apply the OOD detector to all inputs in the dataset \(_{l}\) and compute their OOD scores \(S(G_{1}),,S(G_{n})\). These scores are sorted in ascending order, resulting in a permutation \(\), \(S(G_{(1)}),,S(G_{(n)})\). Subsequently, we allow analysts to assign pseudo labels to the datapoints by selecting a domain-specific value for the hyperparameter \(q\), e.g. 0.05. The analysts label the top \(q\) percent of the datapoints as OOD and the lower \(1-q\) percent as ID, receiving a labeled dataset for OOD detection feedback, i.e. \(G_{(1)} G_{(n)}\) is labeled with \(0_{(1)},,0_{((1-q))},1_{((1-q) )},-1_{(n)}\). With this labeled dataset, we fine-tune the OOD detector. The introduced hyperparameter \(q\) represents the ratio of OOD scores in the domain, which is not only an interpretable number but can also be determined with the help of prior knowledge without extensive tuning.

**Class Annotation.** For new-class samples, we utilize a K-Means (Zhu et al., 2018) based clustering algorithm for class annotation due to its efficiency. Given the uncertainty in the number of unknown classes, naive K-Means, which requires a predefined cluster count, is not applicable. This prompts us to consider whether reference values exist for determining \(K\). Fortunately, we observe that the lower bound of \(K\) is defined by the distinct types of application logs, as limitations in logging tools and attack complexity prevent the capture of all execution details, leading similar logs to potentially reflect different behaviors. Thus, the number of different application log type can serve as the reference value \(R_{k}\). Our goal is to find the optimal \(R_{k}\) close to the reference value, evaluating the effectiveness using the Silhouette Score. The process is demonstrated in Alg 1.

```
Input: Behavior graphs \(G_{i}\) and behavior units \(U_{i}\). Output: Behavior Clusters \(C_{1},C_{2},C_{3}...C_{n_{c}}\).
1\(T,s\);
2for\(l_{app,i} U_{i}\)do
3\(_{l_{app}}\) Log templates of application logs in \(U_{i}\);
4\(n_{ref}\) Get_Reference_Value(\(T\));
5for\(i n_{ref}\) to \(3n_{ref}\)do
6\(_{n_{ref}}\) Get_Silhouette_Score(\(G_{i},n_{ref}\))
7\(n_{c}\) choose_Optimal(\(s\))
8\(C_{1},C_{2},C_{3}...C_{n_{c}}\) Minibatch_Kmeans(\(G_{i},n_{c}\));
9 Return \(C_{1},C_{2},C_{3}...C_{n_{c}}\).
```

**Algorithm 1**Kmeans with parameter selection

**Model Update.** We choose DisenGCN (Miyato et al., 2017) as the backbone graph encoder considering its effectiveness and efficiency. Besides, to enhance model adaptability to structural and class increments, we adopt a replay-based incremental learning strategy utilizing class prototypes. Specifically, after stage \(t-1\), the old model has learned the optimal parameters \(_{old}\). By feeding \(_{t-1}\) into \(^{old}\), we obtain old class embeddings. To overcome catastrophic forgetting, we construct class prototypes \((_{i},_{i})\) to approximate \(_{t-1}\), where \(_{i}\) is the diagonal covariance. This reduces memory cost while preserving key information, as disentangled embeddings have most variance along the diagonal. For robustness, we use only correctly predicted samples to estimate \(_{i},_{i}\).

In addition, since we only save the prototypes of the old data rather than the raw data, the old saved prototypes may not be available when the backbone is training on new data, i.e., the saved prototypes cannot represent the current positions of the old data in the embedding space. Therefore, when training the backbone \(\) at stage \(t\), we need to limit the shift of the old prototypes in the embedding space to ensure their availability. Thus, we add a loss to distill the knowledge of the old backbone \(_{old}\).

\[_{kd}:=_{(,)_{t}}[ |_{ std}()-_{}() |]]. \]

Besides, we use Prototype Augmentation (PA)  strategy to enhance the incremental learning backbone. Let \(P_{t}\) denote the class prototypes before stage \(t\) and \(f_{PA}\) be the classifier after adding the classification heads of virtual classes. The loss function over old data is calculated by:

\[_{old}:=_{(,)_{t}} [(f_{PA}(,))]. \]

In addition, we use the following equation to calculate the classification loss on the new data:

\[_{cls,PA}:=_{(,)_{t}} [(f_{PA}([_{}()]_{PA}, ))], \]

where \(|_{}()|\)\(p_{A}\) represents the embeddings obtained by using \(_{}()\) after the Prototype Augmentation step. Finally, we have the total loss function as follows:

\[=_{cls,PA}+*_{old}+*_{ kd}, \]

where \(\), \(\) are used to balance \(_{cls,PA}\), \(_{old}\) and \(_{kd}\).

## 5. Experiment Setup

In this section, we introduce the experimental setup, including the datasets, baselines, evaluation metrics and implementation details.

### Datasets

In order to support the thorough evaluation of FG-CIBGC, the dataset should have the following properties:

* **Completeness of Log Sources.** The dataset should offer complete log sources including application logs and network logs. Without them, behavior graph identification relies either on a static knowledge base or search within a pre-obtained graph, both of which are impossible in a class-incremental setting.
* **Diversity of Behavior Types.** The dataset should contain a diverse range of system behaviors. Limited types of behavior graphs restrict thorough evaluation.

Open-source datasets like DARPA Trace (Chen et al., 2017) and StreamSpot (Kumar et al., 2018) lack application and network logs _completely_. Moreover, they also fail to cover diverse operations in Internet Infrastructure. Given the above limitations, we construct a new behavior dataset that satisfies both properties, featuring 3 log types and 4,406 graphs across 31 classes. The statistics of the dataset is shown in Table 1. Due to page limit, more details are presented in the Appendix.

### Baselines

To facilitate a comprehensive evaluation, we compare the proposed FG-CIBGC framework with existing methods from two key perspectives: (**i) Behavior Graph Classification Performance. (**ii) Efficacy in Attack Investigation.**

**(i) Behavior Graph Classification Performance.** In terms of behavior graph classification task, we use 12 existing methods as baselines, covering state-of-the-art methods in the behavior graph classification landscape. (**i) Three behavior graph classification methods: **Tgeminer**(Wang et al., 2017), **Watson**(Wang et al., 2017) and **DepComm**(Wang et al., 2017)(ii) Six class-incremental incremental learning methods: **EWC**(Liu et al., 2019), **LwF**(Wang et al., 2019), **GEM**(Wang et al., 2019), **TWP**(Kumar et al., 2018), **CPCA**(Wang et al., 2017) and **Fine-Tuning**(Chen et al., 2017). (**iii) Three graph-level dynamic graph learning methods: **tdGraphEmbed**(Chen et al., 2017), **GraphERT**(Chen et al., 2017) and **TP-GNN**(Kumar et al., 2018).

**(ii) Efficacy in Attack Investigation.** The fine-grained behavior graph classification task is inherently designed to facilitate downstream applications. Thus, we analyze whether the generated behavior graphs can benefit downstream tasks. Specifically, we select attack investigation as the representative downstream task, given its practical significance. In brief, the attack investigation task aims to identify attack-related edges within a given behavior graph. We use **Watson**(Wang et al., 2017) and **DepComm**(Wang et al., 2017) which generate coarse-grained behavior graphs as baselines. Besides, we employ **DepImpact**(Chen et al., 2017) as the baseline for attack investigation.

### Evaluation Metrics

Metrics such as F1-Score (F1) have been adopted in prior studies to evaluate the behavior graph classification task. Following the convention, we use F1 as evaluation metric to conduct the experiments on behavior graph classification. However, none of these metrics take into account the fine-grained requirements of behavior graph classification. Considering such fine-grained characteristics could enable a fair comparison between coarse-grained and fine-grained behavior graph classification methods. In this regard, borrowing the idea from the MIoU (Mean Intersection over Union) metric in semantic segmentation, we propose a new metric called EIoU (Edge Intersection over Union). The EIoU metric enables a comprehensive evaluation of different methods by capturing fine-grained requirements. Specifically, EIoU reframes the graph classification problem as an edge-level classification task, where the classification of a graph into a specific category corresponds to the assignment of its edges to that category. By applying matching criteria to the

  
**Scenarios** & **Attack Cases** & **\#Graph** & **\#Class** & **Avg of Node** & **Avg of Edge** & **Avg of Node** & **Avg of Edge** & **\#Audit** & **\#App** & **\#Net** \\  Apache & Data Leakage & 502 & 3 & 10.01 & 46.02 & 5.10 & 16.99 & 11.7MB & 43KB & 216KB \\ IM-1 & Data Leakage & 1,040 & 4 & 88.12 & 401.69 & 28.12 & 102.54 & 1.25GB & 52.1MB & 6.93GB \\  Vim & Unsafe Action & 125 & 3 & 28.6 & 1,256.00 & 14.6 & 52.32 & 432MB & 275KB & - \\ Redis & Unsafe Action & 201 & 2 & 14.00 & 451.19 & 8.23 & 66.54 & 63.78MB & 17KB & 987KB \\ Pgsql & Unsafe Action & 512 & 9 & 30.27 & 145.20 & 17.75 & 54.30 & 48.5MB & 360KB & 65.2MB \\ ProFTPd & Unsafe Action & 1,001 & 3 & 11.13 & 179.90 & 8.34 & 29.01 & 112.2MB & 95KB & 2.5MB \\ IM-2 & Unsafe Action & 1,040 & 4 & 65.68 & 1,360.25 & 35.68 & 114.96 & 796.8MB & 8.9MB & 3.94GB \\  Nginx & Misconfiguration & 1,001 & 4 & 5.14 & 17.93 & 3.00 & 10.51 & 9.8MB & 105KB & 133KB \\   

Table 1. Overview of dataset for FG-CIBGC evaluation. We implement 8 attack scenarios based on their detailed reports of real-world APT campaigns (Wang et al., 2017). This dataset consists of 4,992 graphs which can be categorized into 32 classes.

ground truth at each fine-grained category, we construct a confusion matrix delineating True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). With these definitions in place, Intersection over Union (IoU) can be formulated by \(=|}{|FP_{e}|+|FP_{e}|+|FN_{e}|}\), where \(|TP_{e}|\), \(|FP_{e}|\), and \(|FP_{e}|\) respectively stand for the number of edge-level TP, FP, and FN. For the EIoU metric used in behavior graph classification, the matching criterion is determined by the accurate edge-level prediction corresponding to the ground truth fine-grained labels. The EIoU is calculated as:

\[=_{i=0}^{k}_{i} \]

where IoU represents the IoU of fine-grained class \(i\) and \(k+1\) is the total number of fine-grained classes in the evaluated dataset.

To quantify the attack investigation performance, we treat the task as an edge-level binary classification problem, as attack investigation inherently aims to identify attack-related edges. Consequently, we compute the Accuracy (Acc) and F1-Score (F1) to evaluate the attack investigation tasks.

### Implementation Details

We prototype FG-CIBGC in 42K lines of Python code. The proposed model is implemented by PyTorch 2.1 framework on Ubuntu 22.04, and all the evaluations are conducted on NVIDIA GeForce RTX 3090 card. For a fair comparison, we tune the hyper-parameters of the base Class-Incremental learning model using grid-search: learning rate \(lr\{0.005,0.001,0.01\}\), batch size \(b\{512,1024,2048\}\), embedding dimension \(d\{32,64,128,256\}\). We set \(C_{k}:C_{u}\) (representing the number of known and unknown classes) \(=9:1\).

## 6. Results and Analysis

In this section, we conduct experiments regarding behavior graph classification performance, ablation study, efficacy in attack investigation and hyper-parameter sensitivity to validate the proposed FG-CIBGC. Due to the page limit, we have to move additional results, including but not limited to more ablation study results and the associated time analysis to the Appendix.

### Behavior Graph Classification Performance

In this section, we compare the behavior graph classification performance of FG-CIBGC with the constructed baselines. The results are shown in Table 2. By observing the experimental results, we can have the following observations:

(1) Coarse-grained behavior graph-based baselines generally exhibit relatively low EIoU performance. Watson performs an adapted DFS on every single data object found in the KG, except for libraries that do not reflect the roots of user-intended goals. DepComm identifies process-centric communities. They all fail to find operations centered around data/process objects, leading to undesirable performance. Among all coarse-grained behavior graph-based methods, Tgminer emerges as the top performer in terms of the EIoU metric. This can be attributed to its strategic focus on finding frequent patterns, rather than centering around data or processes, which sets it apart from other methods. In contrast, FG-CIBGC excels by leveraging more comprehensive information from diverse sources, resulting in accuracy in identifying behavior boundaries.

(2) FG-CIBGC demonstrates significant superiority over class-incremental graph learning baselines. These baselines struggle to accommodate scenarios with unknown new classes, and thus fail to adapt effectively. Furthermore, graph-level dynamic graph learning baselines lag behind class-incremental learning methods, as they lack the ability to adapt to known new classes.

(3) FG-CIBGC outperforms baselines across all datasets, achieving an average improvement of 4.89% in EIoU and 6.82% in F1-Score

   &  &  &  &  &  &  &  &  \\  & EIoU & F1 & EIoU & F1 & EIoU & F1 & EIoU & F1 & EIoU & F1 & EIoU & F1 & EIoU & F1 & EIoU & F1 & EIoU & F1 & EIoU & F1 \\  Tgminer & 48.82 & 65.15 & 59.64 & 66.33 & 52.63 & 67.67 & 58.72 & 60.13 & 64.21 & 79.06 & 51.72 & 66.43 & 59.44 & 59.75 & 53.23 & 65.91 & 78.1 \\ Watson & 40.24 & 69.23 & 42.62 & 56.34 & 39.67 & 66.30 & 47.09 & 56.79 & 62.35 & 82.65 & 48.27 & 61.16 & 41.22 & 52.25 & 44.74 & 54.86 & 76.22 \\ DepComm & 41.09 & 70.54 & 51.39 & 75.99 & 43.61 & 72.51 & 54.80 & 72.84 & 60.71 & 80.80 & 52.75 & 70.45 & 57.70 & 67.91 & 57.56 & 76.46 \\  Fine-Tuning(+) & 50.84 & 71.08 & 52.63 & 76.35 & 55.93 & 74.94 & 56.05 & 73.75 & 64.23 & 78.97 & 56.24 & 73.27 & 54.38 & 68.21 & 57.41 & 78.92 & 74.38 \\ EWC(+) & 55.81 & 74.95 & 54.60 & 78.02 & 61.90 & 80.74 & 64.10 & 82.42 & 69.20 & 85.15 & 68.49 & 87.32 & 67.43 & 84.34 & 63.62 & 80.84 \\ LwF(+) & 62.32 & 83.22 & 58.46 & 77.83 & 59.75 & 76.85 & 62.80 & 76.10 & 64.21 & 85.84 & 57.29 & 81.61 & 66.46 & 87.26 & 63.54 & 82.23 & 76.48 \\ GEM(+) & 58.74 & 76.83 & 58.98 & 74.98 & 59.86 & 76.93 & 68.20 & 82.15 & 64.53 & 84.43 & 58.67 & 81.26 & 57.72 & 81.03 & 64.27 & 83.53 & 76.19 \\ TWP(+) & 69.42 & 86.88 & 65.64 & 85.55 & 67.04 & 66.57 & 72.08 & 84.43 & 70.12 & 85.43 & 50.74 & 76.95 & 70.16 & 84.36 & 61.64 & 85.89 \\ CPCA(+) & 67.08 & 86.42 & 59.94 & 81.91 & 67.63 & 78.84 & 71.27 & 83.26 & 70.29 & 78.24 & 67.93 & 85.54 & 71.09 & 87.82 & 71.60 & 87.60 \\  tddGraphEmbed(+) & 56.43 & 75.12 & 46.43 & 60.78 & 48.25 & 67.37 & 56.26 & 73.81 & 59.52 & 79.03 & 53.68 & 72.62 & 51.48 & 67.63 & 46.48 & 61.54 & 77.71 \\ GraphERT(+) & 67.62 & 77.71 & 52.36 & 68.44 & 58.69 & 78.23 & 65.71 & 75.32 & 64.52 & 78.39 & 64.15 & 79.42 & 62.08 & 77.89 & 55.37 & 75.32 & 77.11 \\ TP-GNN(+) & 62.32 & 78.56 & 55.32 & 71.84 & 57.36 & 77.93 & 64.08 & 74.26 & 63.12 & 76.21 & 59.34 & 73.45 & 53.48 & 72.09 & 60.02 & 79.84 & 77.22 \\ 
**Ours** & **74.62** & **91.62** & **70.35** & **90.26** & **73.93** & **93.92** & **76.08** & **96.07** & **78.24** & **98.32** & **74.56** & **93.08** & **74.28** & **92.73** & **73.31** & **93.27** & 77.33 \\  

Table 2. Comparison results (EIoU & F and F1 %) of fine-grained class-incremental behavior graph classification task across all datasets. ”(+)” indicates that the input to this baseline is the fine-grained behavior graphs generated by gPartition. The best results are shown in bold type and the runner-ups are underlined.

   &  &  &  \\  & EIoU & F1 & EIoU & F1 & EIoU & F1 & EIoU & F1 \\  Baseline & 43.95 & 71.93 & 54.36 & 73.21 & 48.56 & 77.21 \\ Baseline-T & 50.95 & 77.93 & 55.36 & 76.21 & 65.56 & 82.21 \\ Baseline-C & 67.08 & 86.42 & 59.44 & 81.91 & 67.63 & 87.84 \\ 
**FG-CIBGC** & **74.62** & **91.62** & **70.35** & **90.26** & **73.93** & **93.92** \\  

Table 3. Ablation study results. The best results are shown in bold type and the runner-ups are underlined.

compared to the baselines. The superiority is largely attributed to the combination of the innovative gPartition and gAdapt components. Therefore, FG-CIBGC excels in fine-grained and class-incremental behavior graph classification task.

### Ablation Study

The proposed FG-CIBGC framework contains two major components. We conduct an ablation study on 3 representative datasets to further verify their effectiveness. Specifically, 4 combinations of key modules are compared in the ablation study as follows:

* **Baseline:** For this variant, we employ Tginner to identify behavior graphs, and leverage CPCA to perform incremental graph classification.
* **Baseline-T:** For this variant, we substitute the gPartition component of FG-CIBGC with the behavior graph generation algorithm of Tginner.
* **Baseline-C:** For this variant, we replace the gAdapt component of FG-CIBGC with the class-incremental learning baseline CPCA.
* **FG-CIBGC:** This variant is the proposed FG-CIBGC model. As shown in Table 3, we can draw the following conclusions:

(1) The Baseline' performs the worst due to its inability to generate behavior graphs matching service operations. Furthermore, it lacks an efficient strategy to detect known classes.

(2) Incorporating gPartition or gAdapt into the 'Baseline' markedly enhances its performance, highlighting the necessity of generating fine-grained behavior graphs and combining OOD detection with class-incremental learning.

(3) The two proposed modules achieve stable and effective performance on different datasets. FG-CIBGC leverages the advantages of its modules to achieve significant performance gains.

### Efficacy In Attack Investigation

In this section, we seek to ascertain whether the proposed FG-CIBGC can indeed generate fine-grained behavior graph classification results that are effective in facilitating the downstream attack investigation task. We utilize **Depl Impact** to identify critical components in a unified graph derived from raw audit logs. FG-CIBGC and baselines partition the unified graph and classify the resulting behavior subgraphs, which guide forward and backward causality analysis. As illustrated in Fig. 4, it is evident that FG-CIBGC demonstrates the best performance in fine-grained behavior graph generation, thereby optimally facilitating attack investigation.

### Hyper-Parameter Sensitivity

In this section, we perform hyper-parameter sensitivity analysis on 3 representative datasets to investigate the impact of \(\) and \(\) on FG-CIBGC by conducting a grid search for their optimal values. Initially, we set \(\) = 0.3 and vary \(\), followed by fixing \(\) = 1 while varying \(\). The experimental results are illustrated in the Fig. 5. Overall, FG-CIBGC maintains solid performance with different parameter settings. The optimal performance is observed when \(\) = 3 and \(\) = 0.1, indicating its strongest capability under this setting.

## 7. Conclusion

This paper presents FG-CIBGC, the first unified framework for fine-grained and class- incremental behavior graph classification. FG-CIBGC comprises two novel modules: gPartition for fine-grained graph partitioning, and gAdapt for unknown class detection and adaptation. To validate its efficacy, we introduce a novel benchmark. This benchmark includes a new dataset of 4,992 graphs across 32 classes, derived from 8 attack scenarios. It also features a novel Edge Intersection over Union (EIoU) evaluation metric. Extensive experiments demonstrate FG-CIBGC's superior performance on fine-grained and class-incremental behavior graph classification tasks. Furthermore, FG-CIBGC has the ability to generate fine-grained behavior graphs that facilitate downstream applications.

Figure 4. Experimental results regarding effect on attack investigation task.

Figure 5. Hyper-parameter sensitivity analysis results for different values of hyper-parameters \(\) and \(\) on Apache, IM-1 and IM-2 datasets.