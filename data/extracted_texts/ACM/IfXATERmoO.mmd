# Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions

Anonymous Author(s)

###### Abstract.

The remarkable ability of diffusion models to generate high-fidelity images has led to their widespread adoption. However, concerns have also arisen regarding their potential to produce Not Safe for Work (NSFW) content and exhibit social biases, impeding their practical use and progress in real-world applications. In response to this challenge, prior work has primarily focused on employing security filters to identify and subsequently exclude toxic text, or alternatively, fine-tuning pre-trained diffusion models to erase sensitive concepts. Unfortunately, existing methods struggle to achieve satisfactory performance in the sense that they can have a significant impact on the normal model output while still failing to prevent the generation of harmful content in some cases. In this paper, we propose a novel self-discovery approach to identifying a semantic direction vector in the embedding space to restrict text embedding within a safe region. Our method circumvents the need for correcting individual words within the input text and steers the entire text prompt towards a safe region in the embedding space, thereby enhancing model robustness against all possibly unsafe prompts. In addition, we employ a Low-Rank Adaptation (LoRA) for semantic direction vector initialization to reduce the impact on the model performance for other semantics. Furthermore, our method can also be integrated with existing methods to improve their socially responsible performance. Extensive experiments on benchmark datasets demonstrate that our method can effectively reduce NSFW content and mitigate social bias generated by diffusion models compared to several state-of-the-art baselines.

WARNING: This paper contains model-generated images that may be offensive in nature.

Anonymous Author(s). 2024. Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions. In. ACM, New York, NY, USA, 15 pages. [https://doi.org/10.1145/mnmnmn.nnnnnnnn](https://doi.org/10.1145/mnmnmn.nnnnnnnn)

## 1. Introduction

Recently, large-scale text-to-image diffusion models (Zhu et al., 2017; Wang et al., 2018) have attracted much attention due to their ability to generate photo-realistic images based on textual descriptions. However, considerable concerns about these models also arise because their generated content has been found to be possibly unsafe and biased, containing pornographic and violent content, gender discrimination, or racial prejudice (Bengio et al., 2018; Wang et al., 2018).

There have been two common types of approaches employed to address such concerns. One class of methods involves integrating some external safety validation mechanisms (Zhu et al., 2017; Wang et al., 2018; Wang et al., 2018), which harness classifiers to detect toxic input from users and reject them, with diffusion models. However, these mechanisms might be unreliable, as some prompt texts that do not explicitly contain Not Safe for Work (NSFW) content can still result in images with such content. Taking the Stable Diffusion (SD) model as an example, the prompt "a beautiful woman" may lead to the generation of an image of a nude woman (Wang et al., 2018).

The other class of approaches seeks to construct more responsible diffusion models by training data cleaning, parameter fine-tuning, model editing, or intervention in the inference process. A naive method (Wang et al., 2018) is to filter out inappropriate content from the training data of diffusion models to prevent them from internalizing such content. Although effective, retraining models on new datasets can be computationally intensive and often leads to performance degradation (Zhu et al., 2017). Therefore, more efforts have been made to fine-tune parameters so that models can "forget" undesirable concepts (Bengio et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). However, the catastrophic forgetting problem can potentially arise when fine-tuning parameters. Meanwhile, another line of studies (Wang et al., 2018; Wang et al., 2018) seeks to selectively edit certain parameters of pre-trained models to construct a responsible image generation model. But these methods typically tailor the projection matrix within the cross-attention layers to specific target words, thus yielding suboptimal outcomes for other related but non-targeted words. Finally, a few methods (Wang et al., 2018; Wang et al., 2018) leverage the principle of classifier-free guidance. They directly modify the denoising process of the original model to steer away from inappropriate content. Although these methods refrain from updating the model parameters, they may still impact the semantics of the original image and introduce additional overhead during the inference process. In summary, despite the fact that the above methods are effective to some extent, there are still considerable gaps in ensuring the responsibility of diffusion models.

In this paper, we endeavor to address the problem of responsible text-to-image generation using diffusion models from a different perspective. Generally, our approach focuses on manipulating the input text embedding to avoid generating inappropriate content. As the encoded text prompt is fed into the U-Net as a condition and plays a critical role in the image generation process, it can be used to identify the global semantic direction related to a certain conceptin the embedding space. Accordingly, this direction can restrict the text embedding to a specific "safe region", reducing the generation of harmful content in various contexts beyond the token level.

We note that previous prompt-tuning methods (Chen et al., 2018; Wang et al., 2019; Wang et al., 2019) have also attempted to train one or more pseudo-tokens in the CLIP embedding space in a supervised manner. Nevertheless, these pseudo-tokens are designed to symbolize specific concepts and are discontinuous in nature. The pseudo-tokens, along with the other words in the text prompt, are jointly encoded by the CLIP model before input into the U-Net for image generation. These approaches still operate at the token level to instruct the model in generating corresponding images. However, each token in the text prompt will contain information from other tokens. Therefore, attempting to encapsulate a concept such as "safety" within a single token typically fails to produce desirable outcomes. In the computational linguistics domain, some methods such as prefix-tuning (Zhu et al., 2019) consider generating continuous pseudo-tokens for specific tasks. Although these tokens are continuous, they are only used as a prefix added to the beginning of the input sentence to guide the language model in the autoregressive process. This differs from our goal of directing the embedding of the entire input text to a specific region. At the same time, the performance of the trained pseudo-tokens largely depends on the quality of the training data or the accuracy of the classifier. In general, existing prompt-tuning methods cannot be easily applied to prevent the generation of unsafe content and mitigate social biases within diffusion models.

Toward this end, as shown in Figure 1, we propose a novel self-discovery approach to identify the semantic direction in the embedding space, thus constraining the text prompt embedding within the safe region. Specifically, we utilize the classifier-free guidance technique that leverages internal knowledge of the diffusion model to learn a semantic direction vector. Then, we use a low-rank direction vector to strengthen the semantic representation. In this way, the semantic direction vector can guide the original text prompt to move to a specific region in the embedding space. This movement is confined solely to the specific semantic dimension, ensuring that semantics in other dimensions remain unaffected. In simple terms, we leverage the diffusion model as an implicit classifier to get the noise estimate that is close to or far away from a concept during the denoising process. The direction vector learns the corresponding semantic information by minimizing the \(l_{2}\)-loss of the predicted noise and the noise estimate of this implicit classifier. To achieve responsible generation, we learn semantic direction vectors related to unsafe concepts and social bias. For safe generation, we learn a safe vector that can guide the text prompt away from inappropriate content to eliminate the generation of unsafe images. For fair generation, we learn a concept-related direction vector that can guide the input text prompt to a certain concept (e.g., gender and race). Extensive experiments on the widely used benchmark datasets demonstrate that our approach substantially reduces NSFW content generation and mitigates the social bias inherent in the stable diffusion model. Our contributions are summarized as follows:

* We propose a novel self-discovery approach to identify the specific semantic direction vector in the embedding space. Our approach effectively guides unsafe text prompts to a safe region within the embedding space, whether or not these texts contain explicit toxic content. In addition, our approach is effective in reducing multiple types of inappropriate concepts simultaneously, including pornography, violence, societal bias, etc.
* We employ a low-rank direction vector to learn a more precise semantic direction while reducing the impact on model performance regarding other semantics. Furthermore, we show that multiple semantic vectors can be linearly combined to exert influence, and our approach can seamlessly integrate with existing methods to enhance their responsibility in image generation.
* We conduct extensive experiments on benchmark datasets to demonstrate that our approach is capable of effectively suppressing the generation of inappropriate content and mitigating potential societal biases in diffusion models compared to several state-of-the-art baselines.

## 2. Background and Related Work

In this section, we introduce the background of diffusion models and discuss existing methods to improve the responsibility of diffusion models for image generation.

**Diffusion Models:** Currently, most text-to-image generative models are Latent Diffusion Models (LDMs) (Wang et al., 2019). They utilize pre-trained variational autoencoders (He et al., 2016) to encode images into a latent space, where noise addition and removal processes are conducted. Specifically, the forward process takes each clean image \(\) as input, encodes it as a latent image \(_{0}\), and then adds Gaussian noise of varying intensities to \(_{0}\). At each time step \(t[0,T]\), the latent noisy image \(_{t}\) is indicated by \(}_{0}+}\), where \(_{t}\) signifies the strength of Gaussian noise \(\), gradually decreasing with time steps. The final latent noisy image is denoted as \(_{T}(0,I)\). Then, the reverse process trains the model to predict and remove the noise from the latent image, thereby restoring the original image. At each time step \(t\), the LDM predicts the noise added to the noisy latent image \(_{t}\) under the text condition \(c\), represented as \(_{}(_{t},c,t)\). The loss function is expressed as:

\[=_{_{t}(_{t}),,(0,I)}[\|e-_{}(_{t},c,t )\|_{2}^{2}], \]

where \(()\) is an image encoder.

In the inference stage, an LDM typically employs the classifier-free guidance technique (He et al., 2016), which utilizes an implicit classifier to guide the process, thereby avoiding the explicit use of classifier

Figure 1. Intuitive illustration of our method that utilizes the disparities in diffusion noise distribution to identify semantic directions in the CLIP embedding space to guide the generation process and avoid inappropriate content.

gradients. To obtain the final noise for inference, an LDM adjusts towards conditional scores while moving away from unconditional scores by utilizing a guidance scale \(\) as follows:

\[_{}(_{t},,t)=_{ }(_{t},t)+(_{}(_{t},,t)-_{}(_{t},t )). \]

**Responsible Diffusion Models:** Different methods have been proposed to address social biases within diffusion models and mitigate the generation of unsafe content. A straightforward approach is to construct a fair and clean dataset by filtering out unsafe content and retrain a diffusion model using the new dataset (Shen et al., 2017). However, the training datasets and the parameters of diffusion models can be very large. As such, data filtering and model retraining often incur high overheads. Moreover, some studies (Shen et al., 2017) also indicated that this may lead to significant performance degradation. To avoid retraining from scratch, fine-tuning approaches (Shen et al., 2017; Chen et al., 2017; Wang et al., 2017; Wang et al., 2017) were proposed to address safety and fairness issues in diffusion models. Shen et al. (Shen et al., 2017) treated fairness enhancement as a distribution alignment problem and proposed a biased direct approach to fine-tuning the diffusion model. Fan et al. (Fan et al., 2017) identified key model parameters using a gradient-based weight saliency method and fine-tuned them to make the model forget sensitive concepts. Gandikota et al. (Gandikota et al., 2017) used a distillation method to fine-tune the parameters of the cross-attention layer in the diffusion model to remove a certain concept. Lyu et al. (Lyu et al., 2017) used a one-dimensional adapter to learn the erasure of a specific concept rather than fine-tuning all the model parameters. In addition, they used the similarity between the input prompt and the erased concept as a coefficient to determine the extent of erasure. As a result, the effectiveness of the method is reduced when the input prompt does not include the concept intended for erasure. Although fine-tuning methods can make the model safer and fairer with small training costs, they may cause catastrophic forgetting problems, leading to unpredictable consequences (Li et al., 2017; Li et al., 2017).

To further overcome the problems caused by fine-tuning, several recent studies aimed to achieve responsible generation using model editing. As non-training methods, they attempt to edit specific knowledge embeddings in the model according to user needs to adapt to new rules or produce new visual effects. Arad et al. (Artal et al., 2017) and Orgad et al. (Orgad et al., 2017) changed the internal knowledge of a diffusion model by editing the cross-attention layer or the weight matrix in the text encoder. Gandikota et al. (Gandikota et al., 2017) mapped sensitive concept words onto appropriate concept features by modifying the projection matrix of the cross-attention layer. Other methods (Chen et al., 2017; Chen et al., 2017; Chen et al., 2017; Wang et al., 2017) focused mainly on modifying the input text to avoid generating inappropriate images. They generally suppress certain unsafe words in the input prompt or modify the embedding after prompt encoding. However, text-based model editing approaches are very limited because secure prompts may still generate unsafe images (Shen et al., 2017). Moreover, listing all possible unsafe and biased words is infeasible. Note that our method in this paper also operates in the prompt embedding space. However, our method does not target specific words, thus circumventing such limitations.

Finally, another line of methods suppresses the generation of inappropriate content by intervening in the diffusion denoising process. Schramowski et al. (Shen et al., 2017) used classifier-free guided techniques to modify the noise space during the denoising process to remove harmful content. This kind of methods does not require training and is based merely on the model, but directly interfering with the diffusion process is not controllable. In this paper, we use a conditional reflex strategy similar to that of (Shen et al., 2017) to find semantic vectors. We find the semantic vector in the CLIP embedding space through the noise difference in the diffusion process and directly apply it to the prompt embedding during inference. A recent study (Shen et al., 2017) used similar ideas as ours, which adopt a negative prompting method to generate images that are far away from unsafe content and use these images to train a safe semantic vector in the U-Net bottleneck layer. This method changes the image output by perturbing the semantic space found in diffusion (Orgad et al., 2017; Wang et al., 2017). However, it requires a large number of images for training and, additionally, uses the pixel reconstruction loss that may introduce background noise in the image, resulting in lower generation quality.

## 3. Method

In this section, we describe our proposed method in detail. We first introduce how to utilize optimization techniques within the denoising process of the diffusion model to identify the required semantic vectors in the prompt embedding space. Then, we show the low-rank adaptation (LoRA) based semantic vector initialization method we employ. Finally, we explain how the identified semantic vectors are used for safe and fair generation tasks.

### Latent Region Anchoring

We observe that the feature representations in the CLIP embedding space can often be regarded as linear. Intuitively, there are many semantic directions in the embedding space, where moving a sample's features in one direction can yield another feature representation with the same class label but different semantics. Our method aims to identify a semantic direction that translates the original text embedding into a feature with safe semantics. However, finding such a semantic direction is not trivial and may require collecting a significant amount of labeled data. We first propose an intuitive

Figure 2. Examples of using two contrasting prompts to identify specific semantic directions. The images in each column are generated with the same prompt and seed.

approach whereby the semantic direction of relevant attributes is discerned through the disparity in the embeddings derived from two contrasting prompts (such as "a person wearing clothes" vs. "a person" for the "pornographic" attribute). Therefore, the direction vector \(d\) can be obtained as follows:

\[d=_{}(_{+})-_{}(_{-}), \]

where \(_{}\) is the CLIP text encoder and \(_{+}\) denotes a text prompt containing relevant attributes, whereas \(_{-}\) does not. Once the direction vector \(d\) is acquired, we can constrain the input text embedding within a region by either adding or subtracting this direction vector. This process serves to guide images towards or away from the respective attribute. Figure 2 illustrates that the direction vector discovered through this approach indeed affects the relevant attributes such as nudity and gender but also affects other attributes, leading to significant disparities from the original images. This naive approach makes it difficult to obtain highly precise semantic directions. Next, we will introduce an optimization-based approach to learn a more precise direction vector.

Inspired by recent studies (Wang et al., 2019; Wang et al., 2019), we employ a reflexive strategy similar to moving away from or towards certain concepts to find the direction vector. Specifically, we focus on the stable diffusion model with parameters \(\). In our goal of identifying a particular direction vector \(d\), we first need the base prompt used for training. Then, we use a target concept \(c_{0}\) that we aim to move toward or move away from. For example, if we want to find a direction vector toward "male", we can set \(=\) "a person" and \(_{0}=\) "a male person". Our goal is to generate an image related to the target concept by adding the direction vector to the base prompt. Consider the following implicit classifier:

\[p_{}(_{0}|_{t})=(_{t}| _{0})p_{}(_{0})}{p_{}(_{t})}, \]

where \(p_{}(_{0})\) is a categorical distribution, with the assumption that this is a uniform distribution by default. Therefore, we can derive the following equation:

\[p_{}(_{0}|_{t})(_{t }|_{0})}{p_{}(_{t})}, \]

where \(p_{}\) represents the data distribution generated by the diffusion model and \(_{t}\) is the latent noise image at time step \(t\). According to classifier-free guidance (Kumar et al., 2018), the gradient of this classifier can be written as:

\[ p_{}(_{0}|_{t}) &= p_{}(_{t}|_{0})- p _{}(_{t})\\ &=-}}(_{}(_{t},_{0},t)-_{}(_{t},t)).  \]

Using the gradient of this implicit classifier for guidance (Beng et al., 2019), we obtain the noise estimate \(_{}^{+}(_{t},_{0},t)=_{ }(_{t})+w(_{}(_{t},_{0},t)- _{}(_{t},t))\), where \(w\) represents the coefficient of guiding strength. Similarly, we also get the noise estimate \(_{}^{-}(_{t},_{0},t)=_{ }(_{t})-w(_{}(_{t},_{0},t)- _{}(_{t},t))\) if we want to steer the image away from the target concept. Note that during training, the text condition used for iterative denoising is \(+d\), so the predicted noise is \(_{}(_{t},c+d,t)\). By minimizing the distance between \(_{}(_{t},c+d,t)\) and \(_{}(_{t},_{0},t)\), we can find a direction vector that steers the image towards or away from the target concept. Formally, the optimal direction vector \(d^{*}\) for the given concepts is:

\[d^{*}=_{d}_{c}_{t\{0,T\}}\|_{ }(_{t},c+d,t)-(_{t},_{0},t)\|^{2}, \]

where \(\) is a set of base prompts that includes \(m\) same concepts such as "a person", \(\) depends on whether to move towards or away from the target concepts:

\[(_{t},_{0},t)=_{}^{+}(_{t},_{0},t)&\\ _{}^{-}(_{t},_{0},t)&. \]

Figure 3 illustrates the optimization process of our method. Unlike common methods that sample discrete time steps for training, the optimization process occurs during the iterative denoising process, where the optimization at time step \(t\) will affect the result at time step \(t-1\). We optimize the same direction vector for each time step. This choice is motivated by the fact that the text condition is applied at every diffusion step during image generation. Furthermore, this method of dynamically adjusting while simultaneously inspecting during generation aligns with intuition.

Figure 3. Illustration of the optimization process to find a direction vector associated with the target concept in the CLIP embedding space. The noise distribution close to or far away from the target concept is obtained through the frozen pre-trained diffusion model. The \(l_{2}\)-loss between \(_{}(_{t},c+d,t)\) and \((_{t},_{0},t)\) in Eq. 8 at each step \(t\) makes the noise predicted by the base prompt with the direction vector added close to the noise distribution. The updated direction vector \(d^{}\) are used in the next step of denoising, and the precise direction is learned in the iterative denoising process.

### LoRA-based Direction Vector Initialization

The text prompt is encoded into a prompt embedding \(P_{}^{L D}\) through a CLIP text encoder, where \(L\) is the number of tokens and \(D\) is the dimension of the model for token embedding. In the stable diffusion model, we always have \(L=77\) and \(D=768\). Typically, it is recommended to initialize the direction vector \(d^{L D}\) with the same shape as the prompt embedding. However, we find that this approach often results in distorted and warped images. We speculate that this is due to the sensitivity of the embedding space, where even subtle perturbations can have significant impacts on the final results. To address this issue, we adopt Low-Rank Adaptation (LoRA) (Kumar et al., 2017), which uses low-rank decomposition to represent parameter updates, for direction vector initialization. During training, the LoRA matrix can selectively amplify features relevant to downstream tasks, rather than the primary features present in the pre-trained model. The search for a direction vector in the embedding space can also be viewed as fine-tuning for downstream tasks. As such, using a low-rank direction vector can help us identify a more precise direction. Specifically, to amplify the features of the target direction, we initialize the direction vector as \(d=BA\), where \(B^{L 1}\) with all 0's and \(A^{1 D}\) drawn randomly from a normal distribution. We illustrate the two methods for initializing direction vectors in Figure 4. More results for the comparison of the two methods can be found in Appendix C.1.

### Responsible Generation

**Safe Generation.** We perform safe generation by guiding text prompts that contain explicit or implicit unsafe content to prevent inappropriate content. Specifically, we learn the opposite direction of an unsafe concept, using our training method to move away from a specific concept. Our method does not pay attention to the intermediate images generated during training, only to the noise distribution in the intermediate process, so when we set the base prompt, there is no need to guarantee that this prompt will generate reasonable images. For example, in safe generation, the base prompt should contain unsafe content. The base prompt \(c\) can be "an image of nudity", and the target concept \(c_{o}\) "nude", so we can learn a direction vector that guides the input text prompt away from the concept of "nude", regardless of whether the input text prompt contains the word "nude". The reason for adopting this strategy is that it is difficult to list all the opposites of "nude", such as "dressed", "wearing clothes", and others.

After the training process, we keep the inference process unchanged and only add the direction vector we learned to the embedding \(P_{}\) after the text prompt encoding, i.e., \(P_{} P_{}+ d\), where \(d\) refers to the direction away from unsafe concepts, such as the opposite of "nude", and \(\) is a guidance strength coefficient.

**Fair Generation.** We perform fair generation by learning a direction vector close to a specific attribute to debias the text prompt. A text prompt contains specific words that may unintentionally create biased associations. We should generate images with uniformly distributed attributes for a given text prompt. For example, for the prompt "doctor", we want to generate a male doctor or a female doctor with equal probability. Therefore, we learn a direction vector close to each sensitive attribute in order to generate an image that contains a person with the attribute.

At the beginning of the inference phase, we select a vector with the same probability and add it to the prompt embedding. For example, the "male" and "female" direction vectors are chosen with equal probability, denoted as \(P_{} P_{}+d\), where \(D=\{d_{1},,d_{k}\}\) is the set of direction vectors, each for a distinct attribute, and \(d\) is drawn from a uniform distribution on \(D\). In this way, the number of generated images with different attributes should be equal by expectation, e.g., an equal number of male and female doctors.

## 4. Experiments

In this section, we conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our method for safe and fair generation tasks using the stable diffusion model. We also perform transferability, ablation, and case studies for our method. Our code is published anonymously at [https://anonymous.4open.science/r/Responsible-Diffusion-21C2/](https://anonymous.4open.science/r/Responsible-Diffusion-21C2/).

### Safe Generation

**Setup.** For safe generation, our aim is to learn a direction vector in the embedding space and add it to the text prompt embedding to suppress the generation of inappropriate content. We used Stable Diffusion (SD) v1.4 as the base model and set the denoise steps to 50. Following a series of unsafe concepts defined in (Zhu et al., 2017), we set the target concept \(c_{}\) as "hate, harassment, violence, suffering, humiliation, harm, suicide, sexual, nudity, bodily fluids, blood, obscene gestures, illegal activity", drug use, theft, vandalism, weapons, child abuse, brutality, cruelty" and the base prompt \(c\) as "an image of hate, harassment, violence,...". Some prior work such as (Kumar et al., 2017) needs to collect related images for training but abstract concepts like "hate" pose difficulties in finding their opposite images. In contrast, our method directly obtains a safe direction vector away from multiple dangerous concepts.

**Baselines.** We use ESD (Zhu et al., 2017), SLD (Zhu et al., 2017), SPM (Zhu et al., 2017) and IntDiff (Kumar et al., 2017) as baselines in our experiments. We also compare with the original SD and the Negative Prompt technique in the SD. The same target concept \(c_{o}\) is used for these methods. More details about the implementation of our method and these baselines are included in Appendix A.1.

Figure 4. Illustration of two methods for direction vector initialization. The images on the top and bottom are generated using two different vectors; the images in the middle are generated without direction vectors.

**Datasets and Evaluation Metrics.** We use the I2P benchmark (Liu et al., 2019) for evaluation. I2P has been widely used to evaluate the safety of text-to-image generative models. It contains 4,703 inappropriate prompts from real-world user input. We also used the red teaming tool, Ring-A-Bell (Spiegel et al., 2017), to generate two sets of adversarial prompts related to 'hudity' and 'violence', consisting of 95 and 250 prompts, respectively. This method utilizes a pre-trained text encoder to generate adversarial prompts by leveraging relative text semantics and a genetic algorithm. We use NudeNet (Brock et al., 2018) and Q16 (Wang et al., 2019) as inappropriate image classifiers. Following previous studies (Zhu et al., 2019; Liu et al., 2019), an image is considered inappropriate if either of the two classifiers reports a positive prediction. We generate one image per prompt, and all the methods use the same seed for each prompt.

**Results.** Figure 5 illustrates that the "safe" direction vector by our method effectively guides the generation of safe images across various prompt categories, including sexual, horror, hate, and others. Meanwhile, the overall harmony of the image is still maintained, indicating that our method effectively constrains the image within a safe region in the latent space rather than forcefully altering its semantics. Table 1 shows that the safe direction vectors our method learns effectively suppress the generation of inappropriate content. Compared with baselines, our method achieves the best results on all categories of unsafe prompts except the category 'hate', where our method is second-best. Table 2 shows the results of various methods on adversarial prompts. In contrast to other methods, our approach demonstrates superior performance against adversarial

  
**Method** & **Harassment** & **Hate** & **Illegal** & **Self-harm** & **Sexual** & **Shocking** & **Violence** & **Overall** \\  Original SD & 0.32 & 0.45 & 0.35 & 0.42 & 0.37 & 0.50 & 0.42 & 0.40 \\ Negative Prompt & 0.17 & 0.17 & 0.15 & 0.17 & 0.14 & 0.31 & 0.23 & 0.19 \\ SD (Liu et al., 2019) & 0.21 & 0.19 & 0.16 & 0.16 & 0.17 & 0.28 & 0.21 & 0.19 \\ ESD (Spiegel et al., 2017) & **0.14** & **0.13** & 0.16 & 0.19 & 0.14 & 0.25 & 0.24 & 0.18 \\ IntDiff (Zhu et al., 2019) & 0.25 & 0.38 & 0.27 & 0.30 & 0.19 & 0.42 & 0.33 & 0.29 \\ SPM (Liu et al., 2019) & 0.25 & 0.31 & 0.29 & 0.38 & 0.32 & 0.41 & 0.37 & 0.34 \\  Ours (†) & **0.14 (+0.00)** & 0.17 (+0.04) & **0.11 (-0.04)** & **0.09 (-0.07)** & **0.08 (-0.06)** & **0.18 (-0.07)** & **0.15 (-0.06)** & **0.12 (-0.06)** \\   

Table 1. Results on the I2P benchmark dataset. Here, each number represents the ratio of images that are detected as containing inappropriate content by NudeNet and Q16. The lower the ratio, the better the performance of the method. Each column indicates the performance of each method in suppressing inappropriate content generation on a certain category of unsafe prompts. The best and second-best results in each category are highlighted in bold and underline fonts, respectively.

Figure 5. Illustration of our method and baselines for reducing inappropriate content in image generation. Each column contains the images generated by different methods with the same prompt (from the I2P benchmark) and random seed.

attacks. This is because other methods typically isolate a specific concept or simply filter out certain words, but adversarial prompts often contain uncommon characters while embedding implicit associations with unsafe concepts, leading to poor performance in such methods. Our method, however, constrains the adversarial prompt within a relatively safe region through a safe semantic direction. Additional examples can be found in Appendix C.4.

### Fair Generation

**Setup**. For fair generation, we learned a direction vector for each sensitive attribute:'male' and 'female' in gender and 'black', 'white', and 'Asian' in race. We set the base prompt \(c\) as "a person" and the target concept \(c_{o}\) as "a [mask] person", where [mask] is filled with the corresponding attribute, such as'male' and 'black'. In the inference stage, we used the process in Section 3.3 to sample a direction vector.

**Baselines.** We used UCE (Kohn et al., 2017) and IntDiff (Zhu et al., 2017) as baselines in the experiments. Since DebiasVL (Chen et al., 2018) and Concept Algebra (Zhu et al., 2017) can only debias binary attributes, we exclude them from the evaluation. More details about the implementation of our method and these baselines are included in Appendix A.2.

**Datasets and Evaluation Metrics.** We used the Winobias (Zhu et al., 2017) benchmark for fairness evaluation. Winobias contains 36 professions known to have social biases. Following IntDiff, we set up two sets of templates to generate images, such as "a photo of a [profession]" and "a photo of a successful [profession]". The latter set of templates is more challenging for debiasing methods, as "successful" is known to incur greater biases. For each set of templates, 150 images are generated for each profession. Then, we used the pre-trained CLIP classifier to predict the attributes of an image. Finally, we measured the balance of different attributes in the generated images using the deviation ratio \(=_{c C}|-1/C|}{1-1/C}\), where \(C\) is the number of attributes of a social group, \(N\) is the number of generated images, and \(N_{c}\) represents the number of images predicted to have an attribute \(c\).

**Results.** Figure 6 compares the original SD with our method for gender fairness. We find that our direction vector can guide the model to generate images with a balanced gender distribution, but the original SD model cannot. As shown in Table 3, our method greatly alleviates the social bias manifested by the original SD and shows better performance than baselines. We randomly select 6 professions from 36 professions in the Winobias benchmark in Table 3, where our method performs the best or the second best in most cases. The average deviation ratio of our method is always the lowest, indicating that its generated images are closer to a uniform distribution in sensitive attributes. Since our direction vector acts directly on the CLIP embedding space without targeting specific words, it can achieve good results on most prompt templates without performance degradation. In contrast, as UCE debiases different profession nouns individually, it lacks generalizability in terms of templates and shows significant performance decreases.

### Transferability

Next, we verify whether our direction vectors are transferable. The I2P benchmark is still used to evaluate whether the direction vector we learned in the original SD can be used directly in other approaches to improve their effectiveness. Table 4 shows that our direction vectors can significantly enhance the performance of existing methods. We transfer the direction vectors to ESD and SLD in a training-free manner. For other diffusion models, such as SDXL (Zhu et al., 2017), which utilizes two text encoders and concatenates their outputs to form the final result, directly applying the direction vector obtained from the original SD is difficult. Therefore, we adjusted the shape of the low-rank direction vector and retrained it to fit the SDXL setup. The results still indicate that our method is effective in models like SDXL that use multiple encoders, successfully identifying specific safe regions within a more complex latent space. In summary, our method demonstrates strong transferability for both existing fine-tuning approaches based on the original SD and for models with different architectures.

### Combination of Direction Vectors

Experiments on safe generation have shown that our method can learn multiple concepts simultaneously. We further show that it can also combine multiple single-concept vectors. After learning

  
**Concept** & SD & Neg. Prompt & SLD & ESD & IntDiff & SPM & Ours (\({}^{}\)) \\  nudity & 0.947 & 0.947 & 0.968 & 0.537 & 0.968 & 0.653 & **0.316** \\ violence & 0.976 & 0.812 & 0.828 & 0.740 & 0.924 & 0.720 & **0.116** \\   

Table 2. Results on the adversarial prompts produced by Ring-A-Bell. Here, each value represents the ratio of images classified as inappropriate out of all generated images.

Figure 6. Comparison of SD and our method for gender fairness when generating eight “photos of a doctor”.

Figure 7. Illustration of the linear combination of multiple direction vectors.

the direction vectors for different single concepts, such as "male" and "black", these vectors can be linearly combined and added to the text prompt embedding as \(P_{e} P_{e}+_{i=1}^{k}_{i}d\). The results of linear combinations are shown in Figure 7. Using the linear combination method, the model can be guided to generate images with multiple attributes simultaneously. As seen from the last line, the superposition of multiple direction vectors may weaken the effect of each vector. For example, the impact of the "safe" vector on the original text prompt becomes weaker as the number of superimposed vectors increases.

### Image Fidelity and Text Alignment

Finally, we evaluate the impact of our method on the quality of the generated images and the fidelity to the original text prompt. The FID score (Golov et al., 2013) is used to evaluate the fidelity of the generated images by comparing them to real images. The CLIP score (Golov et al., 2013) measures the semantic alignment between images and input text. The COCO-30K dataset is used for evaluation, with one image generated per prompt. As shown in Table 5, the quality and text alignment of the images generated using our method on COCO-30k are at the same level as the original SD. This also shows from another aspect that the semantic direction we learn is accurate and less relevant to other semantics.

## 5. Conclusion

In this paper, we approach the problem of responsible generation using diffusion models from a new perspective and propose a novel self-discovery approach to find the specific semantic direction vector in the embedding space. Unlike previous methods that train pseudo-tokens to represent a certain concept, our method learns a direction vector in the embedding space for concept representation. This direction vector can perform fine-grained continuous control of text embedding on specific semantics, thereby constraining the text embeddings within a safe region. Since our method only guides the text embedding along specific directions, it will not affect other semantics, thus hardly impacting the quality of the generated images. Our method can be applied to the responsible generation of diffusion models, including safe and fair generation tasks. Extensive experiments have demonstrated the effectiveness and superiority of our method, which greatly reduces the generation of harmful content and mitigates social bias in diffusion models.

  
**Category** & **SLD** & **SLD+** & **ESD** & **IntDiff** & **SPM** & **Ours (\(\))** \\   FID (↓) & 14.30 & 18.22 & 17.34 & 15.87 & 14.77 & 15.13 \\ CLIP (↑) & 0.2626 & 0.2543 & 0.2381 & 0.2632 & 0.2581 & 0.2588 \\   

Table 4. Transferability results on the I2P benchmark dataset, where "SLD+/ESD+/SDXL+" represents the integration of our method with SLD/ESD/SDXL, respectively.

  
**Dataset** & **Method** & **Designer** & **Farmer** & **Cook** & **Hairdresser** & **Librarian** & **Writer** & **Winobias (Vinobias, 2018)** \\   & SD & 0.46 & 0.97 & 0.13 & 0.77 & 0.84 & 0.60 & 0.68 \\  & UCE & 0.40 & 0.32 & 0.24 & 0.49 & 0.50 & **0.13** & 0.26 \\  & IntDiff & **0.05** & 0.28 & 0.25 & 0.65 & 0.21 & 0.14 & 0.22 \\  & Ours (↑) & 0.18 (+0.13) & **0.12 (-0.16)** & **0.11 (-0.02)** & **0.32 (-0.17)** & **0.12 (-0.09)** & 0.18 (+0.05) & **0.19 (+0.03)** \\   & SD & 0.40 & 0.96 & 0.28 & 0.79 & 0.84 & 0.32 & 0.71 \\  & UCE & 0.56 & 0.52 & 0.19 & 0.57 & 0.54 & 0.11 & 0.46 \\  & IntDiff & 0.13 & **0.02** & **0.04** & 0.84 & **0.08** & 0.08 & 0.23 \\  & Ours (↑) & **0.08 (-0.05)** & 0.07 (+0.05) & 0.11 (+0.07) & **0.37 (-0.20)** & 0.09 (+0.01) & **0 (-0.08)** & **0.16 (+0.07)** \\   & SD & 0.42 & 0.58 & 0.35 & 0.63 & 0.78 & 0.82 & 0.55 \\  & UCE & **0.11** & 0.49 & 0.19 & 0.49 & 0.60 & 0.64 & 0.30 \\  & IntDiff & 0.36 & 0.27 & 0.41 & 0.43 & 0.33 & 0.41 & 0.31 \\  & Ours (↑) & 0.23 (-0.12) & **0.05 (-0.22)** & **0.03 (-0.16)** & **0.22 (-0.21)** & **0.07 (-0.26)** & **0.05 (-0.36)** & **0.13 (-0.18)** \\   & SD & 0.38 & 0.34 & 0.29 & 0.49 & 0.86 & 0.74 & 0.54 \\  & UCE & 0.16 & 0.46 & 0.30 & 0.68 & 0.67 & 0.83 & 0.38 \\  & IntDiff & 0.36 & 0.26 & **0.08** & 0.37 & 0.32 & 0.24 & 0.29 \\  & Ours (↑) & **0.10 (-0.06)** & **0.18 (-0.08)** & 0.09 (+0.01) & **0.14 (-0.23)** & **0.13 (-0.19)** & **0.04 (-0.20)** & **0.14 (-0.15)** \\   

Table 3. Results for fair generation measured by the deviation ratio \(\), where a lower value indicates fairer results. Here, “Gender/Race” uses a normal template to generate images, and “Gender+/Race+” uses an extended template to generate images. Each column represents the deviation ratios of different methods for each profession and “Winobias” presents the average results for all professions. The best and second-best results in each setting are also highlighted in bold and underline fonts.

  
**Method** & **SD** & **SLD** & **ESD** & **IntDiff** & **SPM** & **Ours (\(\))** \\  FID (↓) & 14.30 & 18.22 & 17.34 & 15.87 & 14.77 & 15.13 \\ CLIP (↑) & 0.2626 & 0.2543 & 0.2381 & 0.2632 & 0.2581 & 0.2588 \\   

Table 5. Results of FID and CLIP scores on the COCO-30K dataset.