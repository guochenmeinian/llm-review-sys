# Query in Your Tongue: Reinforce Large Language Models with Retrievers for Cross-lingual Search Generative Experience

Query in Your Tongue: Reinforce Large Language Models with Retrievers for Cross-lingual Search Generative Experience

Anonymous Author(s)

###### Abstract.

In the contemporary digital landscape, search engines play an invaluable role in information access, yet they often face challenges in Cross-Lingual Information Retrieval (CLIR). Though attempts are made to improve CLIR, current methods still leave users graphing with issues such as misplaced named entities and lost cultural context when querying in non-native languages. While some advances have been made using Neural Machine Translation models and cross-lingual representation, these are not without limitations. Enter the paradigm shift brought about by Large Language Models (Lims), which have transformed search engines from simple retrievers to generators of culturally relevant information. This paper introduces the Multilingual Information Model for Intelligent Retrieval (Mimr). Built on the power of Llims, Mimr directly responds in the language of the user's query, reducing the need for post-search translations. Our model's architecture encompasses a dual-module system: a retriever for searching multilingual documents and a responder for crafting answers in the user's desired language. Through a unique unified training framework, with the retriever serving as a reward model supervising the responder, and in turn, the responder producing synthetic data to refine the retriever's proficiency, Mimr's retriever and responder iteratively enhance each other. Performance evaluations via CLEF and MKQA benchmarks reveal Mimr's superiority over existing models, effectively addressing traditional CLIR challenges.

Large Language Models, Search Generative Experience, Cross-lingual Information Retrieval +
Footnote †: ccs: Information systems Web search engines

+
Footnote †: ccs: Information systems Web search engines

+
Footnote †: ccs: Information systems Web search engines

+
Footnote †: ccs: Information systems Web search engines

+
Footnote †: ccs: Information systems Web search engines

+
Footnote †: ccs: Information systems Web search engines

+
Footnote †: ccs: Information systems Web search engines

+
Footnote †: ccs: Information systems Web search engines

## 1. Introduction

In an age characterized by the relentless pursuit of information, the role of search engines in our daily lives cannot be overstated. Search engines have become indispensable tools for accessing a vast repository of knowledge, connecting individuals with an ever-expanding digital universe. Yet, despite their ubiquity and utility, a significant limitation persists within the current landscape of search engines: their predominant focus on information retrieval within the confines of a single language. While this approach proves effective for users conducting searches in their native tongue, it often falls short in accommodating the diverse linguistic preferences and globalized communication patterns of today's internet users. It is common for individuals to find themselves unable to locate desired information when expressing their queries in their native language, only to discover that altering their search language opens a doorway to a wealth of relevant content. This conspicuous disparity highlights a critical deficiency in the realm of contemporary search engines--their inherent incapacity for Cross-Lingual Information Retrieval (CLIR).

Given the conspicuous underperformance of contemporary search engines in the realm of CLIR, researchers have made efforts to enhance the CLIR abilities with Neural Machine Translation (NMT) models or cross-lingual representation models. While these research endeavors have undoubtedly contributed to bolstering cross-lingual transferability in information retrievers, the practical application of CLIR remains severely constrained. A pivotal challenge arises when users are presented with retrieved results in languages they do not comprehend. To address this issue, existing methods have incorporated a translation model in the post-retrieval phase, aiming to translate the results into the user's native language, thereby facilitating comprehension. However, CLIR still introduces the following challenges: (1) **Named Entity Recognition (NER) Issues**: Proper nouns, especially names of places or people, might not translate directly or can get misrepresented. (2) **Cultural Topic Context Loss**: Some terms or concepts are deeply rooted in cultural context, and a straightforward translation can lose this context.

In recent years, the landscape of search engines has witnessed a transformative evolution with the advent of Large Language Models (Llims). These models have ushered in a paradigm shift, propelling search engines beyond mere information retrieval into the realm of Search Generative Experiences (SGE). Unlike traditional search engines, which primarily return lists of matching documents, Llims are capable of directly providing accurate and contextually relevant answers to user queries. This advancement has significantly improved the user experience, enabling more precise and efficient access to information. In this research, we try to harness the capabilities of the search generative experience to augment the practicality and utility of CLIR and emphasize the importance of ensuring that the language of the generated answer remains consistent with the language used in the user's query. By combining with Llims, we believe SGE can show the following advantages: (1) With extensive training data, Llims have seen numerous named entities across different contexts and languages and can recognize and correctly handle entities. (2) The accommodation of the input context length is large, which gives Llims a broader understanding of the topic and cultural contexts.

In this paper, we introduce the _Multilingual Information Model for Intelligent Retrieval_ (Mimir). It enables the direct generation of responses in the user's query language, capturing and aligning with their intent, thus eliminating the need for post-retrieval translation models. Our method consists of two main modules: a retriever, which searches multilingual documents aligning with the user's query, and a responder, crafting responses matching the user's language based on the retrieved documents. To improve performance, we devised an **unsupervised unified training framework**. In responder fine-tuning, we use the retriever as a reward model, enhancing the language model's cross-lingual transferability. Conversely, for retriever refinement, we use synthetic data from the responder, boosting its performance through augmented supervision signals. These training tasks are iterative, each improving the other.

We designed experiments to assess the accuracy of our retriever and how our Llim's generated results match the user's query, using CLEF (Chen et al., 2019) and MKQA (Zhou et al., 2019). Results from these benchmarks show Mimir surpasses state-of-the-art performance against strong baselines. Further tests on entity recognition and topic translation consistency show Mimir's advantage over traditional post-retrieval translation methods.

## 2. Related Work

### Cross-lingual Information Retrieval

One line of works has tried CLIR with the help of Translation models (Zhou et al., 2019; Chen et al., 2019; Chen et al., 2019; Chen et al., 2019). They apply translation models to translate the multilingual queries into English or translate the retrieval document back to user's language. Researchers (Chen et al., 2019; Chen et al., 2019; Chen et al., 2019; Chen et al., 2019) on CLIR have been long researched through a long time, given the perspective of XLM-R and m-BERT (Devlin et al., 2019; Chen et al., 2019) with different kinds of improvement on Cross-lingual representations (Chen et al., 2019; Chen et al., 2019; Chen et al., 2019; Chen et al., 2019). Many researches (Chen et al., 2019; Chen et al., 2019; Chen et al., 2019; Chen et al., 2019; Chen et al., 2019; Chen et al., 2019) have tried to distill the knowledge of information retrieval from monolingual model to a multilingual architecture. Methods such as code-switching (Chen et al., 2019; Chen et al., 2019; Chen et al., 2019), query generation (Chen et al., 2019; Chen et al., 2019; Chen et al., 2019) or sequential sentence relation (Zhou et al., 2019; Chen et al., 2019); Chen et al. (2019) are also applied in the context of CLIR. VMSST (Chen et al., 2019; Chen et al., 2019) has tried the disentanglement method, a variational generative model to separate semantic information. Another popular technique for CLIR is contrastive learning (Chen et al., 2019; Chen et al., 2019; Chen et al., 2019; Chen et al., 2019), researchers (Chen et al., 2019; Chen et al., 2019; Chen et al., 2019) have undertaken extensive efforts to enhance this crucial capability. The pursuit of improved CLIR abilities has largely converged on two primary technique routines. The first approach involves the utilization of knowledge distillation (Chen et al., 2019; Chen et al., 2019; Chen et al., 2019), a method that commonly employs a monolingual retrieval teacher model to impart its expertise to a multilingual student architecture. Concurrently, contrastive learning (Chen et al., 2019; Chen et al., 2019; Chen et al., 2019; Chen et al., 2019) has gained widespread adoption within the CLIR field due to its remarkable proficiency in aligning sentence embeddings that share similar semantics. Some (Chen et al., 2019) have designed cross-lingual soft prompts to improve cross-lingual information retrieval. Now, CLIR has also been taken as a tool to further improve the performance of other kinds of work, such as fact-checking. (Chen et al., 2019).

### Large Language Models for Search

The emergence of Llims, typified by ChatGPT 1, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Recent research has sought to leverage Llims to improve IR systems. Given the rapid evolution of this research trajectory, the confluence of Llims and IR systems has emerged in different aspects, including crucial aspects such as query rewriters (Chen et al., 2019; Chen et al., 2019; Chen et al., 2019; Chen et al., 2019; Chen et al., 2019; Chen et al., 2019), retrievers (Chen et al., 2019; Chen et al., 2019), retrankers (Chen et al., 2019; Chen et al., 2019), and readers (Chen et al., 2019; Chen et al., 2019). In this paper, we focus on leveraging Llims to alleviate CLIR problems.

## 3. Methodology

We introduce the _Multilingual Information Model for Intelligent Retrieval_ (Mimir). At the heart of Mimir are two pivotal models: the _Retriever_ (\(R_{t}\)) and the _Responder_ (\(R_{p}\)), as illustrated in Figure 1. To enhance Mimir's precision and robustness, unsupervised query augmentation is employed during its training phase. When provided with a document \(D_{y}\) in language \(y\), the _Responder_ (\(R_{p}\)) generates two sets: a positive query set \(Q^{+}\), consisting of diverse queries that align with the content of \(D_{y}\), and a negative query set \(Q^{-}\), containing queries closely related, yet not answerable solely using \(D_{y}\). Leveraging contrastive learning, the _Retriever_ (\(R_{t}\)) is then fine-tuned using both query sets. In parallel, the _Responder_ (\(R_{p}\)) undergoes refinement via a reward signal \(_{<Q,D_{y}>}\), sourced from the _Retriever_ (\(R_{t}\)), and harnessing reinforcement learning mechanisms. A comprehensive breakdown of these modules follows in this section.

### Synthetic Query Generation Using Responder

The quality of the synthetic queries plays a central role in Mimir's training paradigm. Presented with the document \(D_{y}\), the _Responder_ (\(R_{p}\)) generates two kinds of synthetic queries: positive queries, which resonate with the document's content, and negative queries, which, while closely related (often touching upon the same topic or entities) cannot be satisfactorily answered using only the document in question. To direct the _Responder_'s query generation process, we employ the following prompts:

* **Positive:**_Given the content of the document:_ [_document_]. Based on the content and essence of the provided document, generate a user-like query in _[_target_language_].
* **Negative:**_Given the content of the document:_ [_document_]. Devise a query in _[_target_language_] that, while related, cannot be fully addressed by the provided document's content._"

Substituting appropriate values for _[target_language_], the _Responder_ creates \(N^{+}\) distinct positive queries and \(N^{-}\) related yet unanswerable negative queries in different target languages. This nuanced approach to multilingual query generation captures the diverse nature of real-world search inquiries and deepens the training experience. With this rich set of queries, the _Retriever_ is primed to navigate a variety of linguistic challenges, ensuring it delivers peak performance in real-world search scenarios.

### Retriever Training with Synthetic Queries

To ensure excellence in cross-lingual document retrieval, the _Retriever_ (\(R_{t}\)) undergoes intensive training with our crafted synthetic query sets. This rigorous training process amplifies the _Retriever_'s proficiency in discerning relevant documents across a myriad of languages. Specifically, the _Retriever_ learns to gravitate towards positive queries when linked with a pertinent document and, conversely, distances itself from negative queries that are not in alignment with the document's content. This behavior is captured using a contrastive loss, designed such that embeddings of positive queries are drawn nearer to their associated documents in the embedding space, while the embeddings of negative queries are repelled. Mathematically, the contrastive loss \(_{R_{t}}\) can be expressed as:

\[_{R_{t}}=}[R_{t}(D_{y}),R_{t}( q)]}{_{q Q^{+}}[R_{t}(D_{y}),R_{t}(q)]+_{k  Q^{-}}[R_{t}(D_{y}),R_{t}(k)]}\] \[[R_{t}(D_{y}),R_{t}(q)]=[ (R_{t}(D_{y})||R_{t}(q))+] \]

Here, \([,]\) represents the semantic relative score, for which we employ a dense linear layer parameterized by \(,\). \(R_{t}(D_{y})\) retrieves the embedding representations of the document \(D_{y}\) through the _Retriever_. \([\|\|]\) means the concatenation operation. Note that \(R_{t}(D_{y})\) can be more than a single embedding, it can represent all kinds of information we use to represent the document \(D_{y}\). We take \(D_{y}\) as the document representation for simplicity. Through this training approach, the _Retriever_ is finely calibrated to deliver unparalleled performance in cross-lingual document retrieval tasks.

After the fine-tuning process of the _Retriever_, the semantic relevance score between a document \(D_{y}\) and a query \(q\) is determined with the score \([R_{t}(D_{y}),R_{t}(q)]\). To provide a consistent and interpretable reinforcement signal for the _Responder_ (\(R_{p}\)), we rescale the relative score \([R_{t}(D_{y}),R_{t}(q)]\) to lie within the interval \([-,]\). The reward score, \(\), is then calculated as:

\[(q,D_{y})=2[R_{t}(D_{y}),R_{t}(q) ]-}{-}- \]

where max_rel and min_rel represent the maximum and minimum relevance values of the document \(D_{y}\), respectively. This transformation ensures that the reward score spans the spectrum of relevancy between the query and document, assisting the _Responder_ in crafting superior queries by heeding the feedback encapsulated in the reward.

Figure 1. The overall training framework of Mimic, which is an iterative framework. In one iteration, the _Retriever_ is trained on the synthetic multilingual queries generated by the responder through contrastive learning. In reverse, the _Responder_ is trained under the reward signal scored by the _Retriever_.

### Reinforcing the Responder with Cross-lingual Proximal Policy Optimization

Once the reward signals are derived from the _Retriever_, they become instrumental in steering the training of the _Responder_ (\(R_{p}\)). In particular, we employ reinforcement learning (RL) techniques to optimize the query generation process of the _Responder_ based on these reward signals. Given a document \(D_{y}\), the _Responder_ crafts a query \(q\), guided by the previously mentioned prompts, and subsequently evaluates its quality by consulting the reward signal. The objective is to maximize the expected reward:

\[()=\] \[_{q q(1:D_{y})}r_{t}()-J_{KL}D_{KL}(_{ PPO}(y=q|x=D_{y})||_{base}(y=q|x=D_{y})) \]

where \(\) are the parameters of the _Responder_, \(D_{KL}\) represents the KL-divergence, and \(_{}\) represents the policy of generating a query when provided with a document. \(r_{t}()\) is the ratio of the current policy to the old policy. We introduce the _Cross-lingual Proximal Policy Optimization_ (_X-PPO_), a tailored adaptation of the traditional PPO suited for multilingual contexts:

\[_{X-PPOJ}=[r_{t}()_{t}, {clip}(r_{t}(),1-e_{l},1+e_{l})_{t}] \]

where \(_{t}\) denotes the advantage estimate, which is calculated based on \((d,D_{y})\). Details about the PPO variables can be found in the following papers (Spiegel and Stone, 2011; Sutton and Barto, 2011; Sutton and Barto, 2011). To elucidate the components of this approach:

**Dynamic Clipping Range \(e_{l}\)**: Capturing the unique training trajectories languages might exhibit:

\[_{l}=_{base}+(_{X-PPOJ}) \]

\[(_{X-PPOJ})=_{t=1}^{n}(_{X-PPOJ} ^{(t)}-}_{X-PPOJ})^{2}\]

where the \(_{base}\) represents a base clipping value, and \(\) serves as a scaling factor to control the impact of the variance on the clipping range. \(_{X-PPOJ}^{(t)}\) is the loss at the \(i\)-th epoch for language \(l\) and \(}_{X-PPOJ}\) is the mean loss for the last \(n\) epochs. The culmination of these elements in X-PPO ensures a nuanced reinforcement learning regime. It uniquely positions Mimir to adeptly navigate the multifaceted landscape of multilingual information retrieval.

### Overall Procedure for Mimir

To make the overall training procedure much easier to understand, we summarize the training procedure of Mimir in Algorithm 1.

```
0: Unsupervised Multilingual Document set \(\), Pre-trained _Responder_ (\(R_{p}\)), Pre-trained _Retriever_ (\(R_{t}\))
0: Fine-tuned _Responder_ (\(R_{p}\)), Fine-tuned _Retriever_ (\(R_{t}\))
1:Training:
2:while not converged do
3: Step 1: Synthetic Query Generation Using _Responder_
4: Construct \(Q^{+}\) with Positive Prompt
5: Construct \(Q^{-}\) with Negative Prompt
6: Step 2:Retriever Training with Synthetic Queries
7: Fine-tune \(R_{t}\) with Eq. 1
8: Calculate reward signal \((q,D_{y})\) with Eq. 2
9: Step 3: Fine-tuning the _Responder_ with \((q,D_{y})\)
10: Fine-tune \(R_{p}\) with Eq. 3
11:endwhile
12:return Responder (\(R_{p}\)), Retriever (\(R_{t}\))
```

**Algorithm 1** Training Procedure for Mimir

In the Mimir framework, the search process is straightforward. When a user provides a query \(q\), the _Retriever_ scans our multilingual document set \(\) to find the most \(K\) relevant documents. Once these are identified, the _Responder_ uses them, along with the user's query, to generate a clear answer with the following prompts:

_"Given the user's query, [User_Query], and the relevant document information, [Document_Content], please formulate a clear and concise answer in the same language as the user's query that effectively addresses the user's question."_

When plugged into the model, the placeholders [_User_Query]_ and [_Document_Content_] would be replaced with the actual content of the user's query and the selected relevant document, respectively. This approach improved through our training methods, ensures accurate and context-aware responses.

## 4. Experimental Setup

In our assessment of Mimir, we focus on two core tasks in the cross-lingual domain: Cross-lingual Information Retrieval (CLIR) and Multilingual Knowledge-Based Question Answering (MKBQA). This dual evaluation is key to understanding the full capacity of Mimir. With CLIR, we assess the _Retriever_'s skill in finding relevant documents based on synthetic queries. The MKBQA task, in contrast, evaluates the _Responder_'s ability to provide precise answers to user queries. Together, these tasks allow us to comprehensively evaluate both retrieval and response capabilities of Mimir.

### CLIR and MKBQA Settings

**CLIR settings.** Our primary objective in this setting is to tackle a prevalent scenario arising from the abundance of online English data: processing non-English queries against an English document collection. To ensure a meticulous evaluation of cross-lingual retrieval performance in Mimir, we utilize human translations of a standard query set. This enables us to secure queries in diverse languages. Nevertheless, despite this variation in query translations, the content and language of the retrieval corpus remain consistent. For a balanced comparison with previous methods (Spiegel and Stone, 2011), we've chosen four low-resource languages from distinct linguistic families: Niger-Congo (Swahili), Afro-Asiatic (Somali), Austronesian (Tagalog), and Indo-European (Marathi). We also incorporate three medium to high-resource languages--Finnish, German, and French--to provide a more comprehensive insight into Mmir's performance.

**MKBQA settings.** Our objective here is to evaluate how effectively Mimir can generate cross-lingual answers, rather than retrieving documents from a collection. Traditional evaluation strategies, reliant on exact match for retrieval models, are ill-suited for this task which involves comparing the model's output to a reference answer to gauge accuracy. While LLMs typically produce textparagraphs embedding answers, these might not always mirror precise answers. Often, they present a reformulation of the reference answer. For results that mirror the exact match evaluations with LLMs, we adopt the token overlap recall score for the initial 2000 tokens (R@2kt). In our assessment of Minir, it is tested across 10 languages, including German, Spanish, French, Italian, Norwegian, Portuguese, Thai, Turkish, Vietnamese, and Chinese, in line with Sorokin et al. (2018).

### Dataset

#### Evaluation data.

Our focus encompasses two distinct tasks: retrieval from English collections using multilingual queries and generating accurate answers in multiple languages with English collections. Accordingly, we formulate three test sets, varying in collection size, relevance distribution, and language configurations.

* C200 topic, omitting queries without relevant judgments. The English document collection is sourced from the Los Angeles Times corpus, which boasts 113k news articles. For Finnish, German, and French, the queries are provided by the CLEF campaign. For low-resource languages, Bonab et al. (2018) supplies Somali and Swahili translations of English queries. Additionally, we enlist bilingual human experts from the Gengo service to translate English queries into Tagalog and Marathi.
* **MKQA.** The MKQA dataset (Sorokin et al., 2019) is an exhaustive benchmark tailored to evaluate open-domain question answering (QA) within a multilingual context. Featuring over 10,000 examples, it provides questions in 26 unique languages, ensuring each English question is complemented by 26 high-quality translations. For our evaluation of Minir, we select 20 out of the available 26 languages, aligning with Sorokin et al. (2018). The dataset's answers, sourced from open-domain passages, can vary in form--from numbers and dates to concise phrases. With its vast linguistic range, the MKQA dataset serves as a crucial tool for assessing the adaptability and precision of QA systems across different languages.

#### Supervised warm-up data.

To ensure the stable and consistent performance of Minir during the iterative process, we utilize multilingual triples from the MS MARCO dataset to warm up both the _Retriever_ and _Responder_ modules. From this dataset, we randomly select a subset comprising 7 million cross-lingual triples per language, thereby constructing a multilingual training set. As our warm-up strategy for the _Retriever_, we adhere to the fine-tuning methods described in Huang et al. (2017). For the _Responder_, we employ cross-lingual Question-Answer pairs extracted from MS MARCO triples to perform supervised fine-tuning (SFT).

#### Unsupervised training data.

The iterative fine-tuning framework within Mimir operates in an unsupervised manner. For this purpose, we source multilingual data from Wikipedia2. Notably, our collection only requires English document data, since Mimir is designed to autonomously generate multilingual queries based on the English content. For data extraction, we utilize WikiExtractor3 on the Wikipedia database backup dump4. Following the data pre-processing, we randomly select 50 million English sentences to facilitate the iterative training of Mimir.

### Implementation Details

Mimir's architecture is underpinned by two pivotal components: the _Retriever_ (\(R_{i}\)), initialized using the multilingual pre-trained LaBSE model, and the _Responder_ (\(R_{}\)), based on the multilingual instruction-tuned BLOOMZ-7B1 model. In the synthetic query generation phase, the _Responder_ is tasked with generating queries in 20 different languages, as detailed in Section 4.1. During the retrieval training phase, our focus was on enhancing the _Retriever_5 efficiency using both synthetic positive and negative query sets. We maintained a consistent sampling of positive and negative queries at \(N^{+}=5\) and \(N^{-}=25\), respectively, to strike a balance between the queries and the document. During the contrastive learning fine-tuning stage for the _Retriever_, we utilized a learning rate of \(3 10^{-6}\), processing in batches comprising 4 documents each, yielding an effective batch size of 120. For the _Responder_'s reinforcement learning fine-tuning, we adhered to hyperparameters in line with the PPO framework. For determining the dynamic clipping range, \(\) was set at 0.95, and the base clipping range parameter was \(_{}=0.2\). This phase processed data in batches of 256, employed gradient accumulation, and used a learning rate of \(5 10^{-5}\). Convergence was achieved in three epochs. All experiments were conducted using PyTorch, supported by the Huggingface5 and DeepSpeed-Chat (Paszke et al., 2019) toolkits.

#### Evaluation.

To gauge retrieval effectiveness, we draw from established methodologies on the CLEF dataset (Bahdanau et al., 2015; Huang et al., 2017; Huang et al., 2017; Huang et al., 2017; Huang et al., 2017) reporting both the mean average precision (MAP) for the top 100 and the precision for the top 10 (P@10) ranked documents. Statistical significance was ascertained using a two-tailed paired \(t\)-test with a p-value threshold of 0.05. For assessing generation quality, we followed the methodology from prior work (Zhu et al., 2018) and presented the recall scores for the initial 2000 tokens (R@2kt).

### Compared Methods

We compare Mimir with the methods in the following:

* **SMT+BM25**: This approach leverages the Statistical Machine Translation (SMT) method to translate queries. Using the GIZA++ toolkit, a translation table is built for each language pair. The top 10 translations from this table are selected for each query term, which is then used with Galago's weighted \(\)combine operator to generate a translated query. BM25 is then employed to retrieve documents using the translated queries.
* **NMT+BM25**: Utilizing the superiority of Neural Machine Translation (NMT) models over SMT in translation quality,this method first translates the query into English using an NMT model. The translated query is then subjected to the BM25 algorithm for document retrieval.
* **Code-Switch:** This method focuses on data augmentation techniques that enhance training for cross-lingual tasks. Qin et al. (Qin et al., 2019) introduced a code-switching framework that turns monolingual training data into mixed-language data. Taking this further, Bonab et al. (Bona et al., 2019) suggested a shuffling algorithm to intersperse and mix the translated terms into the query. The code-switch method is applied to queries in the MS MARCO triples, which is then used to train the ColBERT retrieval model.
* **LaBSE:** The _Retriever_ in Mimic draws its initialization from LaBSE (Lahbaee et al., 2019). Once trained on the MS MARCO triples, this _Retriever_ can be directly run on the CLIR evaluation data in a zero-shot setting.
* **OPTICAL:** The OPTICAL approach by Huang et al. (Huang et al., 2019) treats the cross-lingual token alignment task as an optimal transport problem. It learns by distilling knowledge from a proficient monolingual retrieval model. Notably, it requires bitext data for the distillation training phase.
* **Translate-Test:** Mirroring the NMT-BM25 method, this approach uses an NMT model to translate the evaluation query into English. Once translated, an English-to-English query-document matching is executed using a trained monolingual neural retrieval model like ColBERT.
* **BLOOMZ-7B1:** Muennighoff et al. (Muennighoff et al., 2019) offers the BLOOMZ, a publicly accessible multitask model instruction fine-tuned on the BLOOM basis, which is renowned as one of the highly multilingual Llims, having training across 46 languages. The 7.1B model variant of BLOOMZ is used post-warm-up on the MS MARCO dataset for experiments.
* **GPT-3.5-TURBO:** Among the most prominent Llims, GPT-3.5-TURBO is proprietary, harnessing the power of instruction tuning, Reinforcement Learning with Human Feedback, and instruction fine-tuning. For the studies, GPT-3.5-TURBO-0301 is accessed via its official Python API.
* **Sentri:** Sentri, as presented by Sorokin et al. (Sorokin et al., 2019), employs a singular encoder for both query and passage retrieval from a multilingual collection. Combined with a cross-lingual generative reader, it sets new standards in retrieval. Remarkably, it can be extended to over 20 languages using a zero-shot approach.

## 5. Experimental Results

The experimental phase of our study evaluated the retrieval performance of Mimic compared to multiple baseline methodologies. Here, we detail the comparative insights and distinct advantages of Mimic.

### Retrieval Performance Improvement in Mimic

The comparative results between the baseline methods and Mimic are documented in Table 1. An overarching observation is the dominance of Mimic across all 7 tested languages. The model, on average, outperforms the strongest of the previously established baselines by an impressive 8.8%. Such robust results can be attributed to the high-quality synthetic queries produced by the _Responder_. Unlike traditional methods, Mimic empowers the _Retriever_ with richer supervisory signals across various languages. A pivotal component contributing to this supremacy is the strategic design of negative queries, which discernibly distinguishable between answerable and unanswerable questions within a given document. Such intricate supervision is evidently beneficial for retrieval tasks, as corroborated by the improvements in Mimic:

**Mimic performs better on low resource languages.** From Table 1, we find that the overall improvements of Mimic over previous baselines is 2.8% on medium and high resource languages, however, the performance gap further rises to 15.4% on low resource languages. Since the low-resource languages often lack a great amount of labeled data to train a retrieval model, the synthetic queries from Mimic can provide more supervision signals than previous methods. From the results in Table 1, we believe the synthetic queries are of great importance in improving the retrieval performance on low-resource languages.

**Cross-lingual encoder performs better than translation models.** Instead of building a CLIR dataset for model training, the Translate-Test method translates the query to English using an NMT model and then retrieves the document based on a monolingual neural retrieval model. With the help of the NMT model at test time,

  &  &  \\  Retrieval Methods &  &  &  &  &  &  &  \\   & MAP & P@10 & MAP & P@10 & MAP & P@10 & MAP & P@10 & MAP & P@10 & MAP & P@10 & MAP & P@10 \\
347 &  & & & & & & & & & & & & & & \\ SMT+BM25 & 0.2271 & 0.2139 & 0.1978 & 0.1832 & 0.1655 & 0.0951 & 0.1047 & 0.0965 & 0.3089 & 0.2810 & 0.3921 & 0.3419 & 0.4052 & 0.3754 \\ NMT+BM25 & 0.2187 & 0.2088 & 0.1448 & 0.1356 & 0.3527 & 0.3202 & 0.1820 & 0.1781 & 0.3742 & 0.3603 & 0.4092 & 0.3955 & 0.4299 & 0.3862 \\ Code-Switch & 0.2420 & 0.2258 & 0.1845 & 0.1682 & 0.3542 & 0.2934 & 0.1573 & 0.1662 & 0.3831 & 0.3403 & 0.4553 & 0.3827 & 0.4589 & 0.3993 \\ Translate-Test & 0.2632 & 0.2537 & 0.2132 & 0.2098 & 0.3816 & 0.3355 & 0.2155 & 0.2246 & 0.4401 & 0.3889 & 0.4795 & **0.4091** & 0.4988 & 0.4234 \\ OPTICAL & 0.3129 & 0.2901 & 0.2477 & 0.2365 & 0.4188 & 0.3623 & 0.2414 & 0.2384 & 0.4228 & 0.3874 & 0.4832 & 0.4067 & 0.4764 & 0.4119 \\  LaBSE & 0.3185 & 0.2998 & 0.2581 & 0.2605 & 0.4207 & 0.3773 & 0.2762 & 0.2505 & **0.4405** & **0.4038** & 0.4874 & 0.4030 & 0.4896 & 0.4090 \\ Muxc & **0.3482** & **0.3269** & **0.3214** & **0.2956** & **0.4498** & **0.3815** & **0.3029** &this pipeline approach can be the strongest baseline in our experiment. From the results in Table 1, we observe that the performance of Mimic on low resource languages is 29.2% percent better than the Translate-Test method, while the Translate-Test method can outperform Mimic by 0.8% on medium and high resource languages. This implies the Translate-Test method is severely influenced by the performance of the NMT model. On low resource languages, it is hard to find an NMT model of good quality, hence the poor performance of NMT model drags down the performance of the overall retrieval performance. As for medium and high-resource languages, obtaining a high-quality NMT model is easy, and with accurate English translation results, retrieving relevant documents is a lot easier, even with traditional statistical methods that can achieve comparable performance. Mimic can train the _Retriever_ in an unsupervised manner, hence, it achieves consistent improvements no matter the scale of the datasets in different languages.

### More accurate answer generation in Mimic

In Table 2, we have compared Mimic with previous post-hoc translation methods. These methods first retrieved the passages from English Wikipedia, extracted the answer from the top-ranked passage, and translated it with a machine translation model. Compared with post-hoc translation methods, we find that directly applying Lims on MKQA benchmark cannot match the performance of previous post-hoc translation methods. Even after fine-tuning on the MS MARCO triples, we still observe 9.9 performance gap between BLOOMZ-7B1 and Sentri. The GPT-3.5-TURBO achieves comparable performance when compared with post-hoc translation model, with the performance gap shrinking to 2.4. However, after Mimic fine-tuning, the performance of Lims can exceed the post-hoc translation methods by 1 score in accuracy. This further reveals the effectiveness of the unsupervised fine-tuning paradigm in Mimic. Since both GPT-3.5-TURBO and Mimic have utilized reinforcement learning with human feedback during fine-tuning, this may further prove that reinforcement learning is efficient and useful at achieving cross-lingual consistency on Lims. For the detailed analysis of why Lims can gain better performance than post-hoc translation methods, we leave them in Section 5.5.

### Ablation Study

We have reported the performance using only the warm-up model in Table 1 and Table 2. Compared with the warm-up LaBSE model in Table 1, we can find that after using unsupervised iteration fine-tuning in Mimic, the overall performance increases by 5.9% in MAP and 4.9% in P@10. The improvement mainly comes from the low-resource languages. On low resource languages, we find that Mimic gains about 11.7% in MAP and 8.4% in P@10. This proves that Mimic can provide high-quality synthetic queries to help better align the low-resource representations in the _Retriever_ space to the high-resource languages. And eventually improves the cross-lingual performance of low-resource languages.

As for the _Responder_, the results are shown in Table 2. Compared with warm-up BLOOMZ-7B1, we find that Mimic can further improve the performance by 10.9 compared with only using MS MARCO triples to fine-tune. This illustrates the effectiveness of Mimic on both _Retriever_ and _Responder_. What's more, we conjecture that great improvement may come from reinforcement learning. During reinforcement learning, the _Responder_ can learn from a sequence-level feedback signal rather than a previous token-level signal in auto-regressive decoding. This can help improve the quality and relevance of the output. We will dig deeper and explore more in this area in future work.

   Models & De & Es & Fr & It & Nl & Pt & Th & Tr & Vi & Zh & Avg. & 78.5 \\   \\  CORA & 44.6 & 45.3 & 44.8 & 44.2 & 47.3 & 40.8 & 45.0 & 34.8 & 33.9 & 33.5 & 41.4 \\ BM25+MT & 43.9 & 45.3 & 41.7 & 41.1 & 45.2 & 46.4 & 45.9 & 42.7 & 44.3 & 38.2 & 43.5 \\ Bi-Encoder & 50.5 & 48.0 & 48.9 & 41.2 & 48.4 & 48.6 & 46.1 & 45.0 & 48.1 & 46.8 & 47.2 \\ Sentri & 56.5 & 55.9 & 55.1 & 54.3 & 56.3 & 54.8 & 55.3 & 53.0 & 54.4 & 50.2 & 54.6 \\   \\  BLOOMZ-7B1 & 49.3 & 46.9 & 46.7 & 48.8 & 50.1 & 37.0 & 38.8 & 39.5 & 37.9 & 52.1 & 44.7 \\ GPT-3.5-TURBO & 53.8 & 56.5 & **56.0** & 53.2 & 53.9 & 44.5 & 50.2 & 52.0 & 50.5 & 51.0 & 52.2 \\ Mimic & **57.5** & **57.9** & 54.6 & **54.8** & **59.2** & **56.3** & **55.4** & **55.8** & **56.8** & **53.0** & **55.6** \\   

Table 2. The performance of post-retrieval translation methods and large language models on MKQA dataset. We report the R@2kt scores and the best performance is marked with bold font.

Figure 2. Analysis about the influence of cross-lingual proximal policy optimization in Mimic. We have set the parameters to four different values and report the R@2kt score on ten languages in MKQA. From the result, we set \(=0.3\) for the best performance.

### Analysis of the Cross-lingual Proximal Policy Optimization

To better adapt reinforcement learning in the context of cross-lingual sentence learning, we propose cross-lingual proximal policy optimization, which designs different clipping ranges for different languages. To better exploit the impact of the X-PPO, we conduct experiments on different values of the hyper-parameters \(\) in the calculation of the dynamic clipping range. The results are shown in Figure 2. \(\) determines how sensitive the variation of the loss function can affect the clipping range for each language. Ideally, if the variation in the loss of one language is large, this implies the performance on this language is not fully convergence, hence we tend to broaden the clipping range for this language to make it faster convergence. We have tried different values of \(\). When \(=0\), X-PPO will deteriorate to normal PPO, and we find that the performance in different languages varies a lot. We believe that during the pre-training, different languages consume different amounts of data, hence the competence for different languages varies and this cannot be resolved with a constant clipping range. After increasing \(\) to 0.3, the performance for those edge languages increases significantly. However, further increasing \(\) to 0.5 will not lead to further improvement in the overall performance. This indicates the model may focus on an edge gradient direction, and lead to general degradation on most languages. Empirically, we set \(\) to 0.3 to achieve the best performance in all languages.

### Advantages of Mimic than previous translation models

Since the previous translation models suffer from NER issues and cultural context loss issues owing to the restriction of the context length, we conduct some case studies to clearly show that Mimic can help alleviate these issues. We list two cases in Table 3. For case 1, The Japanese query searched for a special dish "Nikujaga", but baseline translation models mistranslated the words into "meat and potatoes", which led to recipes for various meat and potato dishes, missing out on the specific Japanese dishes the user is interested in. While Mimic correctly captures the entity meaning and keeps the special words untranslated for searching. This indicates that after the extensive training data, Mimic has seen numerous entities across different contexts and languages. It can recognize and correctly handle proper nouns, ensuring that they are not inappropriately translated or misrepresented. For case 2, the query is in Spanish asking about the "Latest genetic editing techniques". After searching the corresponding documents, baseline translation models incorrectly translate "guide RNA" to "guia turistica de ARN", which even alters the topic of the document. Due to the training on vast and diverse datasets, Mimic has a broader understanding of cultural contexts and a longer accommodation of context length, hence, it correctly translates the word to "ARN guia'. These two cases prove that Mimic can alleviate the problem in baseline translation models. However, Mimic still shows limitations in some query ambiguity and recent culture loss cases, we will focus on building a comprehensive searching framework in future works.

## 6. Conclusion

The challenges of CLIR in today's digital age are undeniable. With Mimic, we introduce an innovative solution that leverages Llms to address these challenges head-on. By seamlessly responding in the user's native language and employing a synergistic dual-module architecture, Mimic has demonstrated its edge over existing systems in our evaluations. As the digital landscape evolves, Mimic represents a significant step towards a more inclusive and efficient multilingual information retrieval.

 p{142.3pt}}  & **Case 1** \\
**Query:** & **Nikujaga** no tsukurikata \\
**Baseline Translation:** & How to make **meat and potatoes**. \\
**Mimic Translation:** & How to make **Nikujaga**. \\
**Mistakes:** & The user might get recipes for various meat and potato dishes, missing out on the specific Japanese dish they were interested in. \\
**Query:** & **Tcnicas de edición genetica mis recientes.** \\
**Results in English:** & CRISPR-Cas9 has revolutionized genetic editing. It allows for precise DNA modifications. \\
**Using guide RNA**, it targets specific DNA sequences and introduces the desired change. \\
**Baseline Translation:** & CRISPR-Cas9 ha revolucionado la edición genetica. Permite modifications precisas de ADN. \\
**Usando guia turistica de ARN**, se dirige a secuencias especifcas de ADN e introduce cl cambio desado. \\
**Mimic Translation:** & CRISPR-Cas9 ha revolucionado la edición genetica. Permite modificaciones precisas del ADN. \\
**Utilizando ARN guia**, se dirige a secuencias especifcas de ADN e introduce cl cambio deseado. \\
**Mistakes:** & **guia turistica de ARN” translates to "tourist guide of RNA," a severe error.** \\ The translation "guia turistica” changes the entire context from a genetic editing scenario to a travel scenario. \\ 

Table 3. Cases in real cross-lingual search scenarios. Baseline search methods can not accurately capture cross-lingual named entities and fail to translate under the same topic due to the limitations of the context length. On the contrary, with the large scale of training data, Mimic can alleviate such issues.