# Robust Deep Signed Graph Clustering via Weak Balance Theory

Anonymous Author(s)

###### Abstract.

Signed graph clustering is a critical technique for discovering community structures in graphs that exhibit both positive and negative relationships. We have identified two significant challenges in this domain: i) existing signed spectral methods are highly vulnerable to noise, which is prevalent in real-world scenarios; ii) the guiding principle "an enemy of my enemy is my friend", rooted in _Social Balance Theory_, often narrows or disrupts cluster boundaries in mainstream signed graph neural networks. Addressing these challenges, we propose the Deep Signed Graph Clustering framework (DSGC), which leverages _Weak Balance Theory_ to enhance preprocessing and encoding for robust representation learning. First, DSGC introduces Violation Sign-Refine to denoise the signed network by correcting noisy edges with high-order neighbor information. Subsequently, Density-based Augmentation enhances semantic structures by adding positive edges within clusters and negative edges across clusters, following _Weak Balance_ principles. The framework then utilizes _Weak Balance_ principles to develop clustering-oriented signed neural networks to broaden cluster boundaries by emphasizing distinctions between negatively linked nodes. Finally, DSGC optimizes clustering assignments by minimizing a regularized clustering loss. Comprehensive experiments on synthetic and real-world datasets demonstrate DSGC consistently outperforms all baselines, establishing a new benchmark in signed graph clustering. The code is provided in [https://anonymous.4open.science/r/DSGC-C05C/](https://anonymous.4open.science/r/DSGC-C05C/).

Representation learning; Balance theory; Signed graph clustering +
Footnote †: journalyear: 2018

**ACM Reference Format:**

Anonymous Author(s). 2018. Robust Deep Signed Graph Clustering via Weak Balance Theory. In _Proceedings of conference title (Conference acronym XX)_. ACM, New York, NY, USA, 13 pages. [https://doi.org/XX](https://doi.org/XX)

## 1. Introduction

Deep graph clustering has emerged as a pivotal technique for uncovering underlying communities within complex networks. However, existing methods (Bishop, 2006; Kipf and Welling, 2016; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017) predominantly target unsigned graphs, which represent relationships solely with "non-negative" edges and inherently fail to capture conflicting node interactions, such as friendship versus enmity, trust versus distrust, and approval versus denouncement. Such dynamics are commonplace in social networks and can be effectively modeled by signed graphs that incorporate both positive and negative edges. Although significant work has studied link prediction tasks in deep signed graphs (Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017), deep signed graph clustering remains substantially unexplored. In this paper, we aim to develop a deep signed graph clustering method that enhances the robustness of graph representations, facilitating more distinctive clusters and better reflecting the intricate relationships within signed graphs.

Signed graph clustering is broadly applied in the analysis of social psychology (Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017), biologic gene expressions (Song et al., 2018; Hamilton et al., 2017), etc. Recent studies have predominantly focused on spectral methods (Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017), which design various Laplacian matrices specific to a given network to derive node embeddings, aiming to find a partition of nodes that maximizes positive edges within clusters and negative edges between clusters. However, these methods are vulnerable to random noise, a common challenge in real-world scenarios. For instance, on shopping websites, the signed graph encoding user-product preferences often includes noisy edges, typically when customers unwillingly give positive ratings to items in exchange for meager rewards or coupons. Fig. 1 illustrates the significant impact of noise on signed spectral methods like BNC and SPONCE (Hamilton et al., 2017; Hamilton et al., 2017). As perturbation ratios increase, which indicates a higher percentage of randomly flipped edge signs or inserted negative edges in a synthetic signed graph with five clusters, these methods suffer a sharp decline in clustering accuracy. Therefore, denoising the graph structure is essential to enhance robust representation learning in deep signed clustering.

Furthermore, the investigation on deep signed graph neural networks (SGNNs) reveals that existing SGNNs -- which are mostly developed for link prediction (Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017) -- do not adapt well to signed clustering. Specifically, mainstream SGNNs models typically leverage principles from the well-established _Social Balance Theory_(Hamilton et al., 2017) (or Balance Theory) to design their messaging-passing aggregation mechanisms, including the classical principle "_an Enemy of my enemy is my Friend (EEF)_", "_a Friend of my Friend is my Friend (FFF)_", "_an Enemy of my Friend is my Enemy (EEF)_". However, "_EEF_" implies an assumption that a given signed network has only \(2\) clusters, which is not directly applied to signed graphs with \(K\) (\(K>2\)) clusters. Specifically, as illustrated in Fig. 2, "_EEF_"

Figure 1. Effects of different perturbations, including flipping signs and randomly adding negative edges, on the clustering performance of popular spectral methods in signed graphs.

can narrow cluster boundaries, leading to more nodes being located at the margins of clusters, which makes it difficult to assign them to the correct clusters and thus results in poor performance. For example, node \(v_{i}\) aggregates its positive neighbor \(v_{2}\) (recognized by "_EEF_" but inconsistent to the real semantic relationship in clusters), which causes its positive representation \(Z_{i}^{+}\) mapped closer to the cluster of node \(v_{2}\), thus leading to narrowed or even overlapped cluster boundaries. In contrast, _Weak Balance Theory_(Cheng et al., 2017) (or Weak Balance), introducing a new principle, "_an enemy of my enemy migh be my friend or enemy_", can generalize Balance Theory to \(K\)-way (\(K>2\)) clustering situation but remains underexplored.

To address these challenges, we propose eep Signed Graph Clustering (DSGC) for \(K\)-way clustering, designed to enhance representations' robustness against noisy edges and reduce the impact of the ill-suited principle on cluster boundaries. DSGC first introduces the Signed Graph Rewiring module (SGR) in the preprocessing stage for denoising and graph structure augmentation. SGR provides two rewiring strategies, including _Violation Sign-Refine_, which can identify and correct noisy edges with long-range neighbor relationships, and _Density-based Augmentation_, which follows Weak Balance principles to insert new positive edges to increase positive density within clusters and negative edges to increase negative density across clusters. Such refined graph topology can promote signed encoders to enhance the robustness of node representations. DSGC then constructs a clustering-oriented signed neural network that utilizes Weak Balance. This helps design clustering-specific neighbor aggregation mechanism for enhancing the discrimination among node representations, specifically for nodes with negative edges to widen cluster boundaries. Finally, DSGC designs a \(K\)-way clustering predictor that optimizes a non-linear transformation function to learn clustering assignments. This framework is designed to refine the clustering process by correcting noisy edges and enhancing the discriminative capability of node representations, ultimately leading to more accurate clustering outcomes.

Overall, our major contributions are as follows:

* We develop DSGC, the first Deep Signed Graph Clustering framework, by leveraging Weak Balance Theory.
* We design two graph rewiring strategies to denoise and augment the overall network topology.
* We propose a task-oriented signed graph encoder to learn more discriminative representations, particularly for nodes connected by negative edges.
* Extensive experiments on synthetic and real-world datasets demonstrate the superiority and robustness of DSGC.

## 2. Related Work

In this section, we succinctly review existing studies for signed graph neural networks and signed graph clustering.

**Signed Graph Neural Networks (SGNNs)**, which maps nodes within a signed graph to a low-dimensional latent space, has increasingly facilitated a variety of signed graph analytical tasks, including node classification (Shen et al., 2017), signed link prediction (Shen et al., 2017; Wang et al., 2018; Wang et al., 2019), node ranking (Shen et al., 2017; Wang et al., 2019), and signed clustering (Bahdanau et al., 2014; Chen et al., 2015; Wang et al., 2019; Wang et al., 2019). Most works of signed graph center around integrating _Social Balance Theory_ to signed convolutions into Graph Neural Networks (GNNs). As the pioneering work, SGCN (Gershtein et al., 2017) adapts unsigned GNNs for signed graphs by aggregating and propagating neighbor information with Balance Theory. Thereafter, other work has integrated additional social-psychological theories. (Bahdanau et al., 2014) appends the status theory, which is applicable to directed signed networks, interpreting positive or negative signs as indicators of relative status between nodes. SiGATs (Shen et al., 2017), which extends Graph Attention Networks (GATs) to signed networks, also utilizes these two signed graph theories to derive graph motifs for more effective message passing. SiNets (Shen et al., 2017) proposes a signed network embedding framework guided by the extended structural balance theory. SGDNET (Shen et al., 2017) leverages a random walk technique specifically tailored for signed graphs, effectively diffusing hidden node features in line with Social Balance Theory. GS-GNN (Shen et al., 2017) applies a dual GNN architecture that combines a prototype-based GNN to process positive and negative edges to learn node representations. SLGNN (Shen et al., 2017) especially design low-pass and high-pass graph convolution filters to capture both low-frequency and high-frequency information from positive and negative links.

**Signed Graph Clustering.** The study of signed graph clustering has its roots in Social Balance Theory (Bahdanau et al., 2014), which is equivalent to the 2-way partition problem in signed graphs (Shen et al., 2017). Building upon this foundational concept, (Shen et al., 2017) propose a signed spectral clustering method that utilizes the signed graph Laplacian and graph kernels to address the 2-way partition problem. However, (Wang et al., 2019) argues that community detection in signed graphs is equivalent to identifying \(K\)-way clusters using an agent-based heuristic. The _Weak Balance Theory_(Cheng et al., 2017) relaxes balance theory to enable \(K\)-way clustering. Following (Shen et al., 2017), (Bahdanau et al., 2014) proposed the "Balanced Normalized Cut (BNC)" for \(K\)-way clustering, aiming to find an optimal clustering assignment that minimizes positive edges between different clusters and negative edges within clusters with equal priority. SPONGE (Chen et al., 2015) transforms this discrete NP-hard problem into a continuous generalized eigenproblem and employs LOBPCG (Shen et al., 2017), a preconditioned eigensolver, to solve large positive definite generalized eigenproblems. In contrast to the above \(K\)-way complete partitioning, (Shen et al., 2017) targets detecting \(K\) conflicting groups in a signed network, allowing other nodes to be neutral regarding the conflict structure in search. This conflicting-group detection problem can be characterized as

Figure 2. Illustration of “_an Enemy of my Enemy is my Friend (EEF)_” narrowing cluster boundaries. Aggregating positive (/ negative) neighbor 2 (/ 3) causes \(Z_{i}^{+}\) (/ \(Z_{i}^{+}\)) mapped far from its clusters or even cross the boundary, where positive (/ negative) neighbors 2 (/ 3) are defined by “EEF”.

the maximum discrete Rayleigh's quotient problem and solved by two spectral methods.

While GNNs have been extensively applied to unsigned graph clustering (Gan et al., 2017; Li et al., 2018; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019), their adoption in signed graph clustering remains overlooked. A notable exception is the Semi-Supervised Signed NETwork Clustering (SSSNET) (Gan et al., 2017), which simultaneously learns node embeddings and cluster assignments by minimizing the clustering loss and a Cross-Entropy classification loss. In contrast, our work develops an unsupervised method for signed graph clustering, eliminating the reliance on ground truth labels.

## 3. Preliminaries

### Notations

We denote an undirected signed graph as \(=\{,,\}\), where \(=\{v_{1},v_{2},,v_{n}\}\) is the set of nodes, \(\) is the set of edges, and \(^{||_{0}}\) is the \(d_{0}\)-dimensional node attributes. Each edge \(e_{ij}\) between \(v_{i}\) and \(v_{j}\) can be either positive or negative, but not both. A is the adjacency matrix of \(\), where \(_{ij}=1\) if \(v_{i}\) has a positive link to \(v_{j}\); \(_{ij}=-1\) if \(v_{i}\) has a negative link to \(v_{j}\); \(_{ij}=0\) otherwise. The signed graph is conceptually divided into two subgraphs sharing the common vertex set \(\): \(=\{^{+},^{-}\}\), where \(^{+}=\{,^{+}\}\) and \(^{-}=\{,^{-}\}\) contain all positive and negative edges, respectively. Let \(^{+}\) and \(^{-}\) be the adjacency matrices of \(^{+}\) and \(^{-}\) with \(=^{+}-^{-}\), where \(^{+}_{ij}=max(_{ij},0)\) and \(^{-}_{ij}=min(_{ij},0)\).

### Relaxation of Social Balance

Balance and Weak Balance Theories, essential for signed graph clustering, are briefly explained here; more details are in Appx. A.

**Balance Theory**(Gan et al., 2017) consists of four fundamental principles: "_the friend of my friend is my friend_", "_the enemy of my friend is my enemy_", "_the friend of my enemy is my enemy_", and "_the enemy of my enemy is my friend (EEF)_". A signed network is balanced if it does not violate these principles. Theoretically, the Balance Theory is equivalent to 2-way clustering on graphs (Gan et al., 2017).

**Weak Balance Theory**(Gan et al., 2017) relaxes Balance Theory to accommodate \(K\)-way clustering, by replacing the "_EEF_" principle with "_the enemy of my enemy might be my enemy (EEE)_". This principle allows nodes in a triangle to belong to three different clusters, e.g., the blue triangle in Fig. 10 (b), thus relaxing Social Balance Theory. The partition \(\{_{1},,_{K}\}\) of a signed graph \(\) satisfying either theory can be uniformly defined as the following conditions:

\[_{ij}>0&(e_{ij})(v_{i} _{k})(v_{j}_{k})\\ _{ij}<0&(e_{ij})(v_{i}_{k})(v_{j }_{l})(k l), \]

where \(_{ij}\) is the weight of edge \(e_{ij}\) and \(0<k,l<K\).

### Problem Definition

This paper aims to leverage the capabilities of deep representation learning to enhance robust graph signed clustering. Unsupervised **Deep Signed Graph Clustering** is formally defined below.

**Problem 1**.: _Given a signed graph \(=\{,,\}\), deep signed graph clustering is to train a function \(f(,)\) that transforms each node \(v\) into a low-dimensional vectors \(_{v}^{d}\). It aims to optimize a partition to divide all nodes \(\{_{i}\}_{i=1}^{||}\) into \(K\) disjoint clusters \(=_{1}_{K}\), by minimizing a signed clustering loss objection that makes as many as positive edges exist within clusters and as many as negative edges exist across clusters._

## 4. Methodology

As illustrated in Fig. 3, DSGC consists of 4 major components, including Violation Sign-Refine and Density-based Augmentation for graph rewiring, signed clustering encoder, and cluster assignment.

### Signed Graph Rewiring

In real-world signed graphs, noisy edges(violations)--negative edges within clusters and positive edges across clusters--can disrupt ideal clustering structures. To address this, we propose two graph rewiring methods to enhance clustering integrity: Violation Sign-Refine (VS-R), which corrects the signs of violated edges to align negative and positive edges with the expected inter-cluster and intra-cluster relationships; and Density-based Augmentation (DA), which adds new edges based on long-range interaction patterns to reinforce message passing. Both methods leverage Weak Balance and are used as preprocessing steps to denoise and augment the initial graph topology--specifically, the message-passing matrix.

#### 4.1.1. Violation Sign-Refine

To address noisy edges, we utilize high-order neighbor interactions to correct their signs. Based on Weak Balance, we first adapt the definitions of positive and negative walks for \(K\)-way clustering. Following Social Balance Theory, (Gan et al., 2017) defines a positive walk as one containing an even number of negative edges and a negative walk as one containing an odd number of negative edges. However, they are not suitable for \(K\)-way clustering due to the uncertainty brought by the "_the enemy of my enemy might be my enemy or my friend_" principle of Weak Balance. We formally redefine positive and negative walks as follows.

**Definition 1**.: _A walk of length \(l^{+}\) connecting nodes \(v_{l}\) and \(v_{j}\) is positive if all its edges are positive; it is negative when it contains exactly one negative edge and all other edges are positive._

Since violations are sparse in graphs, we assume that leveraging higher-order information from longer-range neighbors helps revise the signs of violated edges. Lemma 1 specifies the non-noise score between \(v_{i}\) and \(v_{j}\) w.r.t. the \(l\)-length positive and negative walks.

**Lemma 1**.: _For \(v_{i}\), \(v_{j}\) in a signed graph \(=(,,)\), let \(^{+}_{l}(i,j)\) and \(^{-}_{l}(i,j)\) be the number of positive and negative walks with length \(l\) connecting \(v_{i}\) and \(v_{j}\), respectively. Then, \( a\),_

\[^{+}_{l}(i,j)-^{-}_{l}(i,j)=(^{+})^{l}_{ij}-_{a=0}^{l-1}(( ^{+})^{a}^{-}(^{+})^{l-1-a})_{ij}. \]

If we consider all walks up to length \(L^{}\), the non-noise score of the connection between \(v_{i}\) and \(v_{j}\) can be defined:

\[_{ij}(L^{})=_{l=1}^{L^{}}a_{l}(^{+}_{l}(i,j)-^{-}_ {l}(i,j)), \]

where \(_{l}=1,&l=1\\ 1/l(l),&1<l<L^{}\\ 1-_{l^{}=1}^{L^{}-1}1/(l^{}),&l=L^{}.\)Here, \(a_{l}\) decreases with \(l\), indicating that shorter walks have more influence. \(\) is utilized to correct violations as \(_{lj}\) extracts high-order information from neighbors of \(v_{l}\) and \(v_{j}\) within \(L^{}\)-hop. With \(_{lj}\), we obtain a refined adjacency matrix \(\) via the following rules:

\[}_{lj}=1,&_{lj}>^{+}\\ _{ij},&^{-}_{lj}^{+}\\ -1,&_{lj}<^{-}, \]

where \(^{+}>0\) and \(^{-}<0\) are two thresholds. \(v_{l}\) and \(v_{j}\) are considered _effective friends_ when \(_{lj}>^{+}\), indicating a positive edge (+); \(v_{l}\) and \(v_{j}\) are considered _effective enemies_ when \(_{lj}<^{-}\), indicating a negative edge (-); otherwise, the original adjacency entries in \(\) retains. The magnitude \(|_{lj}|\) represents the confidence level of two nodes being _effective friends_ or _enemies_. A larger (resp. smaller) \(|_{lj}|\) represents a stronger (resp. weaker) positive or negative relationship between \(v_{l}\) and \(v_{j}\). This method refines the adjacency matrix by reinforcing accurate relational signals and reducing the impact of noisy edges, thereby facilitating more effective clustering.

#### 4.1.2. Density-based Augmentation

Following the noise corrections made by VS-R, the revised graph, denoted as \(}=\{,},\}\), is processed through Density-based Augmentation to increase the density of positive edges within clusters and negative edges between clusters. The revised adjacency matrices for positive and negative edges, \(}^{+}\) and \(}^{-}\), are augmented as below:

\[^{+}=(}^{+})^{m^{+}};^{-}= _{a=0}^{m^{-}}(}^{+})^{a}}^{-}(}^{+})^{m^{-}-a}, \]

where \(m^{+}\) and \(m^{-}\) are scalar hyper-parameters indicating the extent of augmentation. The augmented adjacency matrices are:

\[^{+}=1,&^{+}_{ij}>0,\;i j \\ 0,&^{+}_{ij}=0,\;i j;^{ -}=1,&^{-}_{ij}>0,\;i j\\ 0,&^{-}_{ij}=0,\;i j\\ 0,&^{+}_{ij},\;i=j. \]

If \(m^{+}=1\) (resp. \(m^{-}=0\)), no augmentation is performed on \(}^{+}\) (resp. \(}^{-}\)). For \(m^{+}>1\), it adds a positive edge between any two nodes connected by a \(m^{+}\)-length positive walk (Dfn. 1). For \(m^{-}>0\), it adds a negative edge between any two nodes connected by a \((m^{-}+1)\)-length negative walk. This strategy effectively enhances the clustering potential by reinforcing intra-cluster connectivity with positive edges and inter-cluster separations with negative edges. It is particularly effective for a signed graph with few violations.

### Signed Clustering Encoder

Signed Graph Convolution Network, our signed graph encoder in DSGC, is tailored for \(K\)-way clustering. It leverages Weak Balance Theory principles to learn discriminative node representations that signify greater separation between nodes connected by negative edges and closer proximity between those by positive edges 1.

Based on the rewired graph topology defined by \(^{+}\) and \(^{-}\), we first introduce self-loops to each node using \(}^{+}=^{+}+e^{+}\), \(}^{-}=^{-}+e^{-}\), where \(\) is the identity matrix and \(e^{+}\) and \(e^{-}\) are the balance hyperparameters. The adjacency matrices are then normalized as follow: \(}^{+}=(}^{+})^{-1}}^{+}\) and \(}^{-}=(}^{-})^{-1}}^{-}\), where \(}^{+}\) and \(}^{-}\) are diagonal degree matrices with \(}^{+}_{ii}=_{j}}^{+}_{ij}\) and \(}^{-}_{ii}=_{j}}^{-}_{ij}\). We learn \(d\)-dimensional positive and negative embeddings, \(^{+}_{i}\) and \(^{-}_{i}\), for each node \(v_{l}\), and concatenate them as the final node representation: \(_{i}=(^{+}_{i},^{-}_{i})\), where \(^{+}_{i}^{1 d}\) and \(^{-}_{i}^{1 d}\) are computed through layers of our signed graph convolution network:

\[^{+}_{i}=_{l=0}^{L}^{+(I)}^{+(I)}_{i},\;^{-}_{i}=_{l=0}^{L}^{-(I)}^{-(I)}_{i}. \]

\(^{+(I)}\) and \(^{-(I)}\), shared by all nodes, are layer-specific trainable weights that modulate the contribution of different convolution layers to the final node representation. \(L\) is the number of layers

Figure 3. The overall framework of DSGC. The Violation Sign-Refine first computes non-noise scores to correct the signs of noisy edges. Then, the Density-based Augmentation adds positive edges within clusters and negative edges across clusters. These two rewiring methods generate a new adjacency matrix with reduced noise and enhanced semantic structures. Thereafter, clustering-specific signed convolutional networks can be trained by minimizing the differential clustering loss for learning and strengthening the discrimination among node representations linked negatively.

in the neural network. This design allows the encoder to leverage information from different neighborhood ranges. The intermediate representations of all nodes, \(^{(I)}^{|| d}\) and \(^{-(I)}^{|| d}\), can be obtained as

(9) \[^{(I)} =(^{})^{I}^{(0)},\] \[^{-(I)} =_{b=0}^{I-1}(}^{})^{b}(-} ^{})^{I-1-b}^{-(0)},\] (10)

where the superscript \((I)\) and \(I\) denote the layer index and power number, respectively. The initial node embeddings, \(^{+(0)}^{|| d}\) and \(^{-(0)}^{|| d}\), are derived from the input feature matrix \(^{|| d_{b}}\) by two graph-agnostic non-linear networks:

\[^{+(0)} =_{1}^{}((_{0}^{+})), \] \[^{-(0)} =_{1}^{}((_{0}^{-})), \]

where \(\) is the \(ReLU\) activation function. \(_{0}^{}^{d_{b} d}\) and \(_{1}^{}^{d d}\) are the trainable parameters of the positive network; \(_{0}^{}^{d_{b} d}\) and \(_{1}^{}^{d d}\) are that of the negative network. We claim that the positive aggregation function, Eq. (9), can pull the nodes linked by positive walks, thus reducing the intra-cluster variances. Meanwhile, the negative aggregation function, Eq. 10, can push nodes linked by negative walks, thus increasing the inter-cluster variances. We also investigate Weak Balance principles implied in Eq. (9) and Eq. (10), as well as the effect of the minus sign "-" in the term (\(-}^{}\)) to nodes representations linked negatively and the clustering boundary in App. B.

### \(K\)-way Signed Graph Clustering

With node representations \(^{|| 2d}\) learned in our encoder, we propose a non-linear transformation to predict clusters.

**Clustering Assignment.** Considering a \(K\)-way clustering problem, where \(K\) is the number of clusters, node \(o_{i}\) is assigned a probabilities vector \(_{i}=[_{i}(1),,_{i}(K)]\), representing the likelihood of belonging to each cluster. \(k\{1,,K\}\) denotes the index of a cluster and \(_{k=1}^{K}_{i}(k)=1\). This probability is computed using a learnable transformation followed by a softmax operation:

\[_{i}(k)=q_{}(k|_{i})=_{i} _{k})}{_{k^{}=1}^{K}(_{k^{}}_{i^{}})}, \]

where \(_{k}^{d 1}\) is a parameter for cluster \(k\) to be trained by minimizing the signed clustering loss. The assignment vectors of all nodes \(\{_{i}\}_{i=1}^{||}\) form an assignment matrix \(^{|| K}\).

**Differential Signed Clustering Loss.** Signed graph clustering aims to minimize violations, which was historically considered as an NP-Hard optimization problem (Kang and Yang, 2017) with designed discrete (non-differential) objectives in spectral methods. We transform it into a differentiable format by utilizing a soft assignment matrix \(\) in place of a hard assignment matrix \(\). Specifically, given that the cluster number \(K\) is known, let \(\{0,1\}^{|| K}\) be a hard cluster assignment matrix where \(_{( k)}(i)=1\) if node \(o_{i}\) belong to the cluster \(k\); otherwise \(_{( k)}(i)=0\). The number of positive edges between cluster \(k\) and other clusters can be captured by \(_{( k)}^{T}^{+}_{( k)}\) with the positive graph Laplacian \(^{+}=^{+}-^{+}\). The number of negative edges within cluster \(k\) can be measured by \(_{( k)}^{T}^{-}_{(,k)}\). So the violations w.r.t. cluster \(k\) can be measured by \(_{( k)}^{T}(^{+}+^{-})_{(,k)}\). By replacing the hard assignment \(_{(,k)}\) with the soft assignment probability \(_{(,k)}\), the differential clustering loss is constructed as:

\[=|}_{k=1}^{K}_{(,k)}^{T} (^{+}+^{-})_{(,k)}+_{r}, \]

where \(\) is a hyperparameter, and \(_{r}\) is a regularization term computing the degree volume in cluster to prevent model collapse:

\[_{r}=-|}_{k=1}^{K}_{( ,k)}^{T}}_{(,k)}, \]

where \(}\) is the degree matrix of \(\). Minimizing \(\) equals finding a partition with minimal violations. We iteratively optimize the signed encoder and non-linear transformation by minimizing \(\).

**Inference stage.** Each node \(v_{i}\) is assigned to the cluster with the highest probability in its vector \(_{i}\):

\[s_{i}=argmax_{_{i}}_{i}, \]

where \(s_{i}\{1,,K\}\) is the cluster index for \(v_{i}\). The set of all node cluster assignments, \(\{s_{i}\}_{i=1}^{||}\), is used to evaluate the performance of the clustering approach.

## 5. Experiments

This section evaluates our DSGC model with both synthetic and real-world graphs to address the following research questions. **RQ1:** Can DSGC achieve state-of-the-art clustering performance on signed graphs without any labels? **RQ2:** How does each component contribute to the effectiveness of DSGC? **RQ3:** How does the Violation Sign-Refine (VS-R) impact signed topology structures by correcting noisy edges? **RQ4:** How do the strategies in our signed encoder, specifically abandoning the "\(EEF\)" principle and the minus sign in term (\(-}^{-}\)), contribute to forming wider clustering boundaries?

### Experimental Settings

#### 5.1.1. Datasets.

Follow SPONGE (Gueron et al., 2017), we evaluate DSGC with a variety of synthetic and real-world graphs: (i) **Synthetic SBM graphs.** The Signed Stochastic Block Model (SSBM) is commonly used to generate labeled signed graphs (Gueron et al., 2017; Wang et al., 2017), parameterized by \(N\) (number of nodes), \(K\) (number of clusters), \(p\) (edge probability or sparsity), and \(\) (sign flip probability). This model first sets edges within the same cluster as positive, and edges between clusters as negative. It then models noises by randomly flipping the sign of each edge with probability \([0,1/2)\). Each generated graph can be represented as SSBM (\(N\),\(K\),\(p\),\(\)). (ii) **Real-world graphs.** S&P is a stock correlation network from market excess returns during \(2003-2005\), consisting of \(1,193\) nodes, \(1,069,319\) positive edges, and \(353,930\) negative edges. Rainfall is a historical rainfall dataset from Australia, where edge weights are computed by the pairwise Pearson correlation. Rainfall is a complete signed graph with \(306\) nodes, \(64,408\) positive edges, and \(29,228\) negative edges.

#### 5.1.2. Baselines

DSGC is compared against 9 representative signed spectral clustering methods. Five are basic signed spectral methods utilizing various forms of median matrix: (1) symmetric adjacency matrix \(^{*}=(+^{T})\); (2) simple normalized signed Laplacian \(}_{}=}^{-1}(^{*} -^{*})\); (3) balanced normalized signed Laplacian \(}_{}=}^{-1}(^{*} -^{*})\); (4) signed Laplacian graph \(}=}^{-1}\) - \(\) with a diagonal matrix \(}\); and (5) its symmetrically normalized version \(_{}\)(((21)\)). Two are \(K\)-way spectral clustering methods: (6) Balanced Normalized Cut (BNC) and (7) Balanced Ratio Cut (BRC) ((5)). The last two ((6)) are two generalized eigenproblem formulations: (8) SPONGE and (9) SPONGE\({}_{}\). Moreover, we compare with 6 state-of-the-art deep unsigned graph clustering methods: (10) DAEGC ((37)), (11) DPCN ((35)), (12) DCRN ((28)), (13) Dink-net ((27)), (14) DGCLUSTER ((1)) (15) MAGI ((26)). Please refer to App. E for hyperparameters settings and experiment details.

#### 5.1.3. Evaluation Metrics

For _Labeled graphs (SSBM)_, Accuracy (ACC), Adjusted Rand Index (ARI) ((12)), Normalized mutual information (NMI), and F1 score are used as the ground truths of nodes are available. For _Unlabeled graphs (S&P and Rainfall)_, due to the lack of ground truths, clustering quality is visualized by plotting network adjacency matrices sorted by cluster membership. See more detailed settings in App. E.

### Overall Performance

To address **RQ1**, we evaluated our DSGC and baselines on a variety of labeled signed graphs generated from four SSBM configurations, including SSBM (\(N=1000\), \(K=5\), \(p=0.01\), \(\)) with \(\{0,0.02,0.04,0.06,0.08\}\), SSBM (\(N=1000\), \(K=10\), \(p\), \(=0.02\)) with \(p\{0.01,0.02,0.03,0.04,0.05\}\), SSBM (\(N\), \(K=5\), \(p=0.01\), \(=0\)) with \(N\{300,500,800,100,1200\}\), and SSBM (\(N=1000\), \(K\), \(p=0.01\), \(=0.02\)) with \(K\{4,5,6,7,8\}\). The performance of each experiment was measured by taking the average of 5 repeated executions. Table 1 reports the results in ACC and NMI. Appendix F provides the results in ARI and F1 score.

Table 1 shows: (i) _Superior performance:_ Our DSGC significantly outperforms all baseline models across all metrics, even though SPONGE and SPONGE\({}_{}\) are known for their effectiveness on such datasets. (ii) _Robustness_: Regardless of whether the graph is dense or sparse (\(p\)), large or small (\(N\)), noisy or clean (\(\)), and the number of clusters is few or many (\(K\)), DSGC maintains notably superior performance on all 20 labeled signed graphs. (iii) _Comparative analysis:_ While deep unsigned clustering methods (DAEGC, DPCN, DCRN, Dink-net, DGCLUSTER MAGI) consistently underperform our DSGC due to the limitation of their capabilities to only handle non-negative edges. DSGC still has a clear advantage, highlighting the effectiveness of its design specifically tailored for signed graph clustering.

### Ablation Study

To address **RQ2** and evaluate the contributions of key components of DSGC, we performed an ablation study using labeled signed graphs, including SSBM (1000, \(b\), 0.01), SSBM (1000, 5, 0.01, \(\)), SSBM (N, 5, 0.01, 0.01), and SSBM (1000, \(K\), 0.01, 0.01). The variants of DSGC tested are: **w/o VS-R** is DSGC without Violation Sign-Refine; **w/o DA** is DSGC without Density-based Augmentation; **w/o Regu** is DSGC without Regularization term; **w/o VS-R & DA** is DSGC without VS-R and DA; **w/o DA & Regu** is DSGC without VS-R and Regu; and **w/o All** is DSGC without VS-R, DA, and Regu.

From the results depicted in Fig. 4, it is evident that: (i) _Performance trends:_ As the edge probability (\(p\)) and the number of nodes (\(N\)) increase, the accuracy (ACC) of DSGC and its variants consistently improves. Conversely, increases in the sign flip probability (\(\)) and the number of clusters (\(K\)) lead to a decline in ACC across all models. (ii) _Component impact:_ DSGC outperforms all variants on all labeled signed graphs, demonstrating the significant role each component plays in enhancing clustering performance. Specifically, DA emerges as the most influential component, affirming its effectiveness in reinforcing the graph structure and improving node representations by adding strategically placed new edges.

### Analysis of Violation Sign-Refine

To investigate **RQ3**, we analyzed the impact of applying Violation Sign-Refine (VS-R) on the performance of spectral clustering methods. VS-R was first used to pre-process and denoise signed graphs to generate new graphs. Then we compared the performance of all spectral methods before and after applying VS-R. Signed graphs were generated by fixing \(N=1000\) and varying \((K,,p)\), including SSBM (1000, \(K,0.01,0.02\)) with \(K\{5,6\}\), SSBM (1000, 5, 0.01, 0.04), and SSBM (1000, \(10,p,0.02\)) with \(p\{0.01,0.02\}\).

Table 2 shows that VS-R significantly improves the clustering performance across all tested spectral methods w.r.t. ACC and NMI by generating cleaner graphs with better clustering structure. Specifically, the performance increments vary inversely with the strength of the baseline methods--stronger baselines show smaller gains, whereas weaker baselines benefit more substantially from the VS-R preprocessing. VS-R also consistently reduces the _violation ratio_, defined as the ratio of the number of violated edges to the number of non-violated edges, across various graph configurations.

In addition to numerical analysis, Fig. 14 in App. D provides visual evidence of the impact of VS-R. the embeddings of new

Figure 4. Ablation study. (a)–(d) The ACC(%) performance vs. edge probability (\(p\)), flip probability (\(\)), node number \(N\) and cluster number \(K\).

graphs, displayed in the bottom row, exhibit clearer clustering boundaries than those of the original graphs in the top row.

### Impact of Signed Encoder to Clustering

To address **RQ4**, we developed two variants of DSGC encoder, including DSGC w/o (\(-\)A\({}^{-}\)) that replaces (\(-\)A\({}^{-}\)) with (A\({}^{-}\)), and DSGC w / _EEF_ that incorporates the '_the enemy of my enemy is my friend_ (_EEF_' principle from Balance Theory to DSGC. Both variants and DSGC used the layer number \(L=2\).

The positive and negative representations of DSGC w/ _EEF_ are \(_{}^{+}=^{+}+(}^{-})^{2}^{+(0)}\); \(_{}^{-}=^{-}\) where \(^{+}\) and \(^{-}\) are the positive and negative representations computed by Eq. 8 in DSGC. Similarly, the positive and negative embeddings of DSGC w/o (\(-\)A\({}^{-}\)) are \(_{}^{+}=^{+}\); \(_{}=^{-}\). We define a metric, _SoEN_, to measure the distance between nodes linked by negative edges:

\[=^{+}|}{|^{-}|}_{}^{-}}s(_{},_{})}{_{_{}^{+}}s(_{ },_{})},\]

    &  &  \\ 
7881 & =0\)} & =0.62\)} & =0.64\)} & =0.66\)} & =0.68\)} & =0.61\)} & =0.62\)} & =0.63\)} & =0.64\)} & =0.65\)} & =0.67\)} & =0.68\)} & =0.69\)} & =0.61\)} & =0.61\)} & =0.65\)} & =0.69\)} & =0.61\)} & =0.65\)} & =0.69\)} & =0.61\)} & =0.65\)} & =0.68\)} & =0.67\)} & =0.69\)} & =0.68\)} & =0.67\)} & =0.69\)} & =0.68\)} & =0.67\)} & =0.69\)} & =0.68\)} & =0.

where \(s(,)\) is the inner product, indicating the similarity between two nodes. Ideally, _SoEN_ is a negative value and a lower _SoEN_ indicates a greater distance between nodes connected by negative edges and a clearer clustering boundary. Fig. 5 illustrates the ACC and _SoEN_ of DSGC and its variants. The results show that: (i) DSGC consistently outperforms its variants. Incorporating the "_EEF_" principle or altering the sign of \((-}^{-})\) significantly impacts clustering performance because DSGC achieves lower _SoEN_ along with epochs than its variants. This demonstrates its advantage in separating nodes linked by negative edges, leading to clearer clustering boundaries and larger inter-cluster variances. (ii) The term \((-}^{-})\) has higher impact than the inclusion of _EEF_, suggesting the original negative edge handling in DSGC is critical for maintaining clear cluster separations.

### Visualization

We utilized t-SNE to visualize the embeddings produced by DSGC and several strong baselines, including BNC (Wang et al., 2017), BRC (Wang et al., 2017), SPONGE (Wang et al., 2017), and SPONGE\({}_{sym}\)(Wang et al., 2017), on SSBM (\(N=1000\), \(K=5\), \(p=0.01\), \(=0.02\)) in Fig. 6. Both BNC and BRC exhibit mode collapse, where most nodes are grouped into one or a few clusters. SPONGE and SPONGE\({}_{sym}\) show improved clustering structures. However, SPONGE lacks a clear boundary between clusters while SPONGE\({}_{sym}\) appears to form 6 clusters with a central cluster where nodes from different true clusters are mixed. This indicates its potential issue with handling nodes connected by negative edges, which are typically located at cluster boundaries. In contrast, DSGC successfully pushes nodes linked by negative edges apart, effectively eliminating the central cluster phenomenon in SPONGE\({}_{sym}\). This result is attributed to the exclusion of the "_EEF_" principle and the incorporation of the term \((-^{-})\) in the graph encoder.

### Unlabeled Graphs

We also evaluated DSGC on unlabeled real-world signed graphs, S&P1500 (Wang et al., 2017) and Rainfall (Wang et al., 2017), comparing it against three baselines, BRC (Wang et al., 2017), BNC (Wang et al., 2017), and SPONGE\({}_{}\)ym (Wang et al., 2017). The adjacency matrices of these graphs were sorted by predicted cluster membership to visually assess clustering outcomes.

**Rainfall.** Following (Wang et al., 2017), we analyzed the clustering structures for \(K=\{5,10\}\) in Fig. 7, where blue and red denote positive and negative edges, respectively2. Both BRC and BNC fail to identify the expected number of clusters, resulting in model collapse. In contrast, DSGC successfully identifies the specified clusters (5 or 10) and exhibits higher ratios of positive internal edges and stronger negative inter-cluster edges compared to SPONGE, indicating more cohesive and well-defined clusters.

**S&P1500.** Fig. 8 shows the clustering structures for \(K=\{5,10\}\). BRC and BNC suffer model collapse, placing most nodes into a single large, sparse cluster. In contrast, DSGC produces clear, compact clusters with significantly higher ratios of positive to negative internal edges than the entire graph, indicating more effective clustering that even surpasses SPONGE in identifying relevant groupings.

## 6. Conclusion

In this paper, we introduce DSGC, a novel deep signed graph clustering method, to enhance the clarity of cluster boundaries by effectively utilizing positive and negative edge connections for node partitioning. Existing approaches generally rely on the Social Balance Theory, which is primarily suitable for 2-way clustering. In contrast, DSGC leverages the Weak Balance Theory to address more general \(K\)-way clustering without the need for explicit labels. DSGC first introduces two pre-processing techniques, VS-R and DA, to denoise and structurally enhance signed graphs before clustering. Then, DSGC constructs a clustering-oriented signed neural network that produces more discriminative representations, specifically for nodes linked negatively. By optimizing a non-linear transformation for node clustering assignments, DSGC significantly outperforms existing methods, establishing clearer and more meaningful cluster distinctions in complex multi-cluster scenarios.

Figure 6. Visualization of clustering results from different algorithms. The ground truth class number is \(5\).

Figure 7. Sorted adjacency matrix for the Rainfall dataset with \(K=5\) (top row) and \(K=10\) (bottom row).