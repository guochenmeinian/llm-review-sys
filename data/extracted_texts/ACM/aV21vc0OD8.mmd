# AN-Net: An Anti-Noise Network For Anonymous Traffic Classification

Anonymous Author(s)

###### Abstract.

Anonymous networks employ a triple proxy to transmit packets to enhance user privacy, causing traffic packets from all applications and web services to form a unified flow. The traditional approach of applying flow-level encrypted traffic classification methods to anonymous traffic (i.e., treating consecutive packets as a single flow) is hindered by irrelevant packet noise. Moreover, fluctuations in the network environment can introduce per-packet attribute noise and discrepancies between training and test data. How to extract robust patterns from consecutive packets replete with noise remains a key challenge. In this paper, we propose the **Anti-Noise Network** (**AN-Net**) to construct robust short-term representations for a single modality, effectively countering irrelevant packet noise. We also incorporate an enhanced multi-modal fusion approach to combat per-packet attribute noise. AN-Net achieves state-of-the-art performance across two anonymous traffic classification tasks and one VPN traffic classification task, notably elevating the F1 score of SJTU-AN21 to 94.39% (6.24%). In particular, attackers cannot easily disrupt all short-term features of all modalities and thus AN-Net is robust against injected noise packet attacks. Our codes will be available on GitHub after the double-blind review process.

2018

## CCS CONCEPTS

* **Information systems \(\) Traffic analysis; \(\) Security and privacy \(\) Network security; \(\) Computing methodologies \(\) Artificial intelligence.**

## 1. Introduction

Network traffic classification, aiming at classifying network traffic from various applications or web services, plays a critical role in quality of service (QoS) enhancement, resource usage planning, and even malware detection (Han et al., 2017; Li et al., 2018; Li et al., 2019). Recently, various traffic encryption techniques have been employed (Li et al., 2019; Li et al., 2019), such as SSL, for protecting user privacy. However, these encryption mechanisms also help malicious traffic evade the surveillance system, thus bringing great challenges to traffic classification (Li et al., 2019). Traditional deep packet inspection (DIP) based methods (Li et al., 2019; Li et al., 2019), which explore regular expression for matching the payload data, fail to identify encrypted traffic since the payload data is changed relying on the encryption algorithm. Therefore, encrypted traffic classification has become a research hotspot in recent years.

Over the past decade, many different methods have been proposed to classify encrypted traffic, which can be divided into three categories according to the types of input: statistical feature-based methods, sequential attribute-based methods, and raw traffic-based methods. Early works (Li et al., 2019; Li et al., 2019; Li et al., 2019; Li et al., 2019; Li et al., 2019) extract the statistical features at the flow level (e.g., mean, minimum, maximum, and standard deviation of packet sizes in a flow) to train the machine learning-based classifier. These methods rely on expert-designed features heavily and have limited generalization ability. Recently, some deep learning-based methods (Li et al., 2019; Li et al., 2019; Li et al., 2019) automatically learn complicated patterns from the raw flow attribute sequences (e.g., packet sizes in a flow), and achieve significant performance improvement. However, these methods require a large number of labeled data to train the deep learning-based models for more robustness. As a comparison, raw traffic-based methods (Li et al., 2019; Li et al., 2019) directly capture the implicit and robust patterns in the encrypted payload at flow level using complicated models. In addition to large amounts of labeled

Figure 1. Threat models of encrypted network traffic classification and anonymous network traffic classification.

data, these methods are also limited by long training time and high requirements on computing resources.

Most importantly, the majority of encrypted traffic classification methods described above is based on flow-level features. Flow level aggregation is beneficial for extracting robust patterns, but also limits the effectiveness in special cases, e.g., the anonymous network. As illustrated in Figure 1, for network, the most mainstream anonymous network, uses triple proxy to transmit traffic packets for protecting user privacy. As a result, traffic packets from all applications or web services form a single flow. The conventional approach of applying flow-level encrypted traffic classification methods to anonymous traffic is to take consecutive packets as a flow (Wang et al., 2017). Obviously, this approach cannot guarantee that all packets originate from the same web service. Therefore, the key difference between anonymous traffic and encrypted traffic is noise, i.e., irrelevant packets from other flows, denoted as **irrelevant packet noise**.

Extracting robust patterns from consecutive packets full of noisy packets is crucial for anonymous traffic classification. Fu _et.al_(Fu et al., 2017) found that most flows completed in less that 2 seconds, which indicates that consecutive packets in short-term are likely to originate from the same flow. Inspired by their observation, we visualize the number of consecutive packets and their probability of belonging to the same flow on ISCX-nonTor (Zhou et al., 2018), Cross-Platform (Yang et al., 2018), and Browser datasets (see Figure 2). For simplicity, we use the first \(1 10^{4}\) packets of each pcap file to plot the figure. Results show that consecutive packets in long-term have a high probability of not belonging to the same flow, but packets in short-term are likely to originate from the same. Therefore, anonymous traffic classification methods should learn to model short-term features with low noise and then aggregate them for robust representation.

In addition to irrelevant packet noise, fluctuations in the network environment can introduce noise in the per-packet attributes, denoted as **per-packet attribute noise**. For example, internal arrival time (IAT) is likely to be affected by network congestion and time-to-live (TTL) may also change due to network routing. Therefore, extracting robust patterns from noisy per-packet attributes is also important for anonymous traffic classification. Considering that traffic packets usually have more than one attribute, e.g., packet size, IAT, TTL, etc., a good idea to combat the interference of per-packet attribute noise is to combine attributes from different modalities.

In this paper, we propose an **Anti-Noise Network (AN-Net)** for classifying anonymous traffic via short-term representation building and enhanced multi-modal fusion. It aims to learning robust patterns from anonymous traffic full of irrelevant packet noise and per-packet attribute noise. We first propose a Uni-modal Short-term Representation Learning Module. It divides consecutive packets in a "flow" into multiple short-term packet sequences. Short-term features are then extracted from them by using the Short-term Feature Extraction Module (SFEM). Once short-term features are extracted, they are fed into the Short-term Representation Aggregation Module (SRAM), which aggregates the short-term features into flow-level representation. The SRAM is specifically designed to identify which short-term features come from irrelevant flows and which ones originate from the target flow by adopting a novel high temperature self-attention mechanism, thus helps resist the irrelevant packet noise. Finally, flow-level representations from different modalities are fused in the Enhanced Multi-modal Representation Fusion Module to combat the per-packet attribute noise.

The main contributions of this paper are summarized as follows:

* We present a Uni-modal Short-term Representation Learning Module to construct robust short-term representations for a single modality to resist irrelevant packet noise. We design a novel high temperature self-attention mechanism, which pays less attention to noise packets.
* We propose to fuse representations from different modalities to combat per-packet attribute noise. A novel representation enhancement strategy is employed to further improve fusion performance.
* AN-Net achieves state-of-the-art performance over two anonymous traffic classification tasks and one VPN traffic classification task. Moreover, it exhibits strong robustness against injected noise packet attacks.

## 2. Related Work

### Conventional Traffic Classification

Port-based methods (Yang et al., 2018) identify the application type based on the portset. Their efficiency declined with the increase use of dynamic ports (Yang et al., 2018) and default ports (Yang et al., 2018). Payload-based methods (Yang et al., 2018; Wang et al., 2017; Wang et al., 2017; Wang et al., 2017; Wang et al., 2017), also called deep packet inspection (DPI), explore the specific signature strings for matching the payload data. These methods are unable to classify encrypted traffic because the signature strings cannot be obtained from payloads after encryption.

### Encrypted Traffic classification

Encrypted traffic classification methods can be divided into three categories according the the types of input: statistical feature-based methods, sequential attribute-based methods, and raw traffic-based methods.

#### 2.2.1. Machine Learning Based Encrypted Traffic Classification

Most statistical feature-based methods use ML-based models for classification. These methods propose to leverage statistical features at flow-level (e.g., mean, minimum, maximum, and standard deviation of packet sizes in a flow) to solve encrypted traffic classification problem combined with machine learning algorithms (Bishop, 1996; Abbeel et al., 2006; Chen et al., 2010; Wang et al., 2017; Wang et al., 2017). AppScanner (Yang et al., 2018) trains random forest classifiers by

Figure 2. The relationship between the number of consecutive packets and the probability that they belong to the same flow. Consecutive packets in short-term are likely to originate from the same flow.

exploiting statistical features of packet sizes, while Gerard _et.al._(Gerard et al., 2018) trains C4.5 decision tree and KNN classifiers using time-related features. As a supplement of statistical features, Whisper (Whisper, 1998) extracts the frequency domain features of flows and uses clustering algorithms for classification. These methods rely heavily on professional knowledge and it is difficult to design generic statistical features to handle different applications.

#### 2.2.2. Deep Learning Based Encrypted Traffic Classification

Some statistical feature-based methods (Zhu et al., 2017) also apply DL-based models for better representation extraction capabilities, and they also rely on human-designed features and have limited generalization ability. As an alternative, sequential attribute-based methods (Zhu et al., 2017; Li et al., 2018; Li et al., 2019; Li et al., 2019; Li et al., 2019) extracts discriminative representations from raw sequential attributes (e.g., packet sizes in a flow). Flowlets (Bai et al., 2017) computes for each flow a memory-efficient representation of packet sizes named "flow marker". FlowPic (Zhu et al., 2017) transforms raw packet size sequences and arrival interval sequences in a flow into an intuitive picture. FS-Net (Li et al., 2019) uses recurrent neural networks (RNN) to automatically extract representations from raw packet size sequences. Another alternative approach is to learning implicit representations from raw traffic. Raw traffic-based methods (Li et al., 2018; Li et al., 2019; Li et al., 2019; Li et al., 2019) directly capture the implicit and robust patterns in the encrypted payload at flow level using complicated DL-based models.

However, the majority of encrypted traffic classification methods described above is based on flow-level features, which limits their effectiveness in anonymous networks, where the traffic from all applications or web services form a single flow. Moreover, None of these methods paid attention to the unreliability of per-packet attributes and attempted to solve it by combining information from different modalities. In this paper, we propose to build strong short-term representations to resist irrelevant packet noise, and adopt an enhanced multi-modal fusion module to combat per-packet attribute noise.

## 3. An-Net

In this paper, we aim to accurately classify anonymous network traffic under the interference of irrelevant packet noise and per-packet attribute noise. To this end, we propose an Anti-Noise network (AN-Net) (see Figure 3) to build strong short-term representations for a single modality to resist irrelevant packet noise (Section 3.1) and achieve enhanced multi-modal fusion to combat per-packet attribute noise (Section 3.2).

### Short-term Representation Learning

In this section, we propose a Uni-modal Short-term Representation Learning Module to build short-term representations for resisting irrelevant packet noise.

#### 3.1.1. Flow Division and Packet Parsing

Given a "flow" that consists of consecutive packets \(P\) of length \(L\), we first divide \(P\) into \(N\) parts and obtain multiple short-term consecutive packet sequences: \(P=[P_{1},P_{2},,P_{N}]\). As illustrated in Figure 2, packets in short-term are likely to originate from the same flow. Therefore, features extracted from short-term packet sequences are more likely to be

Figure 3. Overview of AN-Net Framework.

immune to the interference of irrelevant packet noise. Then we parse out the short-term per-packet attribute/payload sequences for classification, denoted as \(A=[A_{1},A_{2},,A_{N}]\), where \(A_{i}\) is a short-term per-packet attribute/payload sequence.

#### 3.1.2. Short-term Feature Extraction

We design two Short-term Feature Extraction Modules (SFEM) to extract short-term features from raw data and statistical data respectively, denoted as Raw-SFEM and Stat-SFEM.

Suppose the input is a short-term per-packet payload sequence \(A_{i}^{N d}\), where \(l\) is the length of the short-term sequence and \(d\) is the length of payload, Raw-Stat employs a bidirectional GRU to extract the short-term feature: \(F_{i}=GRU(A_{i})^{C}\). If the input is a short-term per-packet attribute sequence \(A_{i}^{N 1}\) (e.g., packet size), Raw-Stat first embeds each attribute to a vector via an embedding layer, and then also uses a bidirectional GRU to extract the short-term feature \(F_{i}^{C}\). The extracted short-term features are denoted as \(F=[F_{1},F_{2},,F_{N}]^{N C}\).

Stat-SFEM can only deal with attribute sequences. It first extracts 7 general statistical features (i.e., mean, max, min, median, standard deviation, skewness, and kurtosis) and the frequency domain features from these short-term attribute sequences as the short-term statistical features, denoted as \(T=[T_{1},T_{2},,T_{N}]\). Then it employs a MLP to extract the short-term feature \(F_{i}\) for each short-term statistic feature: \(F_{i}=MLP(T_{i})^{C}\). The MLP consists of two fully-connected layers and one ReLU layer between them. Despite its simplicity, the MLP is able to extract discriminative features thanks to the high-level statistical features and the nonlinear transformation of the ReLU layer. Thanks to the high-level statistical features, Stat-SFEM exhibits higher stability than Raw-SFEM when training data collection environment is inconsistent with the actual test environment, as detailed in Section 4.5.

#### 3.1.3. Short-term Representation Aggregation

Once short-term features \(F^{N C}\) are extracted, we use the Short-term Representation Aggression Module (SRAM) to aggregate the short-term features \(F^{N C}\) into flow-level representation \(Z\). Since short-term features may also come from irrelevant flows, it is critical to distinguish among \(N\) short-term features which ones originate from the irrelevant flow and which ones come from the target flow. We design a novel high temperature self-attention mechanism to achieve this. Specifically, the SRAM is composed of a Transform Layer (Shen et al., 2017) and a Pooling layer. A normal Transformer layer consists of two key sub-layers: self-attention layer and feed forward layer. Each sub-layer uses the residual structure (Shen et al., 2017) to avoid the degradation problem that occurs as the depth of the network increases.

Given the short-term features \(F=[F_{1},F_{2},,F_{N}]^{N C}\), the self-attention layer first calculate Query Matrix \(Q\) by using linear transformation:

\[Q=FW^{Q}=[q_{1},q_{2}, q_{N}]^{T}, \]

where \(W^{Q}^{C D}\) is learnable parameter and \(q_{i}^{D}\) denotes the query vector of i-th short-term feature \(F_{i}\). Similarly, we calculate Key Matrix and Value Matrix by another two linear transformations:

\[K=FW^{K}=[k_{1},k_{2}, k_{N}]^{T}, \]

\[V=FW^{V}=[v_{1},v_{2}, v_{N}]^{T}, \]

where \(k_{i}\) denotes the key vector and \(v_{i}\) denotes the value vector of i-th short-term feature \(F_{i}\). Then, considering the query vector of i-th short-term feature \(q_{i}\), we compute the dot products of \(q_{i}\) and key vectors of all short-term features:

\[S_{i}=[q_{i}:k_{1}^{T},q_{i}:k_{2}^{T},,q_{i}:k_{N}^{T}]=[s_{1i},s_{2}, ,s_{iN}], \]

where \(s_{ij}\) is the similarity between \(q_{i}\) and \(k_{j}\), and reflects the importance of j-th short-term feature to i-th short-term feature. Normal self-attention layer scales dot products \(S_{i}\) by \(}\) before applying a softmax function to make the sum of the elements be 1:

\[_{i}=softmax(}{})=[w_{i1},w_{i2},,w_{iN}],_{j=1}^{N}w_{i|j}=1, \]

The output at i-th position is then calculated using weighted summation over value vectors of all short-term features \(V\):

\[z_{i}=_{i}V=_{j=1}^{N}w_{ij}v_{j}. \]

Finally, the output of self-attention layer on all short-term features \(F\) is represented as: \(SelfAttn(F)=[z_{1},z_{2},,z_{N}]\). The above process can also be expressed in matrix form:

\[SelfAttn(F)=softmax(}{})V. \]

As stated above, the original self-attention layer scales the dot products \(S_{i}\) by \(}\) before applying a softmax function. Wei _et.al._(Wei et al., 2017) demonstrated that increasing the magnitude \(||S_{i}||\) will cause a sharp distribution for softmax weight score \(_{i}\). Original self-attention layer reduces the magnitude \(||S_{i}||\) by scaling by \(}\) to avoid the softmax function from producing extremely small weights in \(_{i}\). However, in the anonymous traffic classification scenario, since some short-term features may come from irrelevant flows, a sharp distribution for softmax weight score \(_{i}\) needs to be generated to resist irrelevant packet noise. To this end, we design a novel high temperature self-attention mechanism by increasing the magnitude of dot products:

\[HTSelfAttn(F)=softmax((Q)(K)^{T}}{})V, \]

where \(\) is the normalize function that makes the vector norm equal to \(1\) and \(\) is the temperature hyper-parameter. Note that the real temperature is the reciprocal of \(\). After employing a high temperature, the softmax function produces extremely small weights for short-term features from irrelevant flows.

The feed forward layer can enhance the expression ability of the output features by mapping them to high-latitude space and then back to low-latitude space through two linear transformations. In the middle of them, a GeLU layer (Shen et al., 2017) is adopted to alleviate the vanishing gradient problem.

Finally, the short-term features are aggregated into flow-level uni-modal representation \(Z\) by using an Average Pooling layer.

### Multi-modal Representation Fusion

In this section, we propose the Enhanced Multi-modal Representation Fusion Module to fuse flow-level representations from different modalities for combating per-packet attribute noise.

#### 3.2.1. Modal Selection

Before fusing representations from different modalities, we resort to information leakage (Zhou et al., 2017) to remove useless modalities. We utilize the mutual information between statistical features \(T\) of each modality and the ground truth labels \(C\) to measure the importance of this modality:

\[I(T;C)=H(C)-H(C|T). \]

Representations from modalities with high information leakage are then fused to obtain the final robust representation.

#### 3.2.2. Representation Enhancement

As mentioned above, the unreliability of per-packet attributes may make the representation of certain modalities full of noise. A conventional approach to combat input noise is to employ data augmentation strategy, which has been widely used in CV (Zhou et al., 2017; Wang et al., 2018; Wang et al., 2018) and NLP (Zhou et al., 2017; Zhou et al., 2017). However, due to the limitation of input data type (i.e., numeric values), few data augmentation methods have been proposed to cope with encrypted traffic classification. To this end, we propose a novel representation enhancement strategy to perform data augmentation in representation-level.

Given representations from \(M\) modalities \([Z_{1},Z_{2},,Z_{M}]\), we perform data augmentation on representation from each modality, respectively:

\[Z_{i}=\{0,&p\\ B Z_{i},&1-p. \]

where \(p\) is a random probability and \(B\) is a scaling factor sampled from Beta distribution. Note that the representation-level data augmentation will only be adopted during the training phase. For a uni-modal representation, we randomly drop it to force the model to learn from other modalities, or scale it to make the model learn more robust patterns.

#### 3.2.3. Representation Fusion

We then aggregate the enhanced representations from different modalities into the final multi-modal representation by using an Average Pooling layer:

\[=([},},,}]). \]

Finally, we make the prediction on it with a fully-connected layer \(=FC()\), and train the whole model through cross-entropy loss: \(=CE(,Y)\), where \(Y\) is the ground-truth label.

## 4. Experiments

In this section, we first present the datasets, baselines, evaluation metrics and implementation details (Section 4.1). We then compare AN-Net with seven methods (Section 4.2), and demonstrate that AN-Net is robust against injected noise packet attacks (Section 4.3). We further perform an ablation analysis of two key structures: Short-term Representation Learning (Section 4.4.1) and Multi-modal Representation Fusion (Section 4.4.2). Finally, we show that high-level statistical features are more stable when there are discrepancies between training and test data (Section 4.5).

### Experiment Setup

#### 4.1.1. Datasets

To evaluate the effectiveness and generalization of AN-Net, we conduct experiments on two anonymous traffic datasets (Zhou et al., 2017; Wang et al., 2018) and one VPN traffic dataset (Zhou et al., 2017). The statistical information of three datasets is shown in Table 1. Note that we take 100 consecutive packets as a flow. SJTU-AN21 provides a test set separately, which is collected in a different network environment than the training set. For other two datasets, we divide them into the training set and the test set according to the proportion of 80% and 20% for each class.

#### 4.1.2. Baselines

We use seven state-of-the-art flow-level encrypted traffic classification methods covering three basic categories as baselines. For a fair comparison, all methods use the same partitioned flows for training and test.

* **AppScanner (_Statistical features and ML-based model_). AppScanner (Zhou et al., 2017) trains random forest classifiers by exploiting statistical features of packet sizes at flow-level. We retrained the ML-based model using the default hyper-parameter settings in their paper.
* **Decision Tree (_Statistical features and ML-based model_).
* Gerard _et.al._(Zhou et al., 2017) trains C4.5 decision tree using statistical features of internal arrivals at flow-level. Likewise, we use the default settings.
* **Whisper (_Statistical features and ML-based model_). Whisper (Whisper, 2018) extracts the frequency domain features of packet sizes at flow-level as a supplementation of conventional statistical features, and uses clustering algorithms for classification. We reproduce Whisper on three datasets without modifications and then retrain the ML-based model.
* **Flowlens (_Sequential attributes and ML-based model_). Flowlens (Florot and Bengio, 2010) computes for each flow a memory-efficient representation of packet sizes named "flow marker", and uses a Multinomial Naive-Bayes classifier for classification. We retrained the ML-based model using the hyper-parameter settings that produce the most accurate results.
* **FS-Net (_Sequential attributes and DL-based model_). FS-Net (Zhou et al., 2017) uses recurrent neural networks (RNN) to automatically extract representations from raw packet size sequences. A multi-layer encoder-decoder structure and the reconstruction mechanism are adopted to enhance the effectiveness of features. We use the default hyper-parameter setting in their paper.
* **AttLSTM (_raw traffic and DL-based model_). AttnLSTM (Wang et al., 2018) is an end-to-end network based on the LSTM model to directly perform classification on raw traffic. It introduces an attention mechanism to score the importance of each flow. Similarly, we use the default setting in their paper.
* **ET-Bert (_raw traffic and DL-based model_). ET-Bert (Zhou et al., 2017) pre-trains deep traffic representations from large-scale unlabeled raw traffic, then fine-tunes on a small

    &  &  &  \\    & & train & & & \\  SJTU-AN21 (Wang et al., 2018) & Anonymous & 37529 & 9133 & 10 \\ ISCX-Tor (Zhou et al., 2017) & Anonymous & 92458 & 23101 & 8 \\ ISCX-VPN (Zhou et al., 2017) & VPN & 20009 & 4957 & 7 \\   

Table 1. The Statistical Information of three Datasets.

amount of labeled data. We re-pretrain the model and fine-tune it on three datasets, respectively.

#### 4.1.3. Evaluation Metrics and Implementation Details

We evaluate our AN-Net and compare it with other state-of-the-art methods by four typical metrics, including Accuracy (AC), Precision (PR), Recall (RC), and F1 (Zhu et al., 2017; Wang et al., 2018; Wang et al., 2018). In the training phase, we train the AN-Net with a stochastic gradient descent (SGD) optimizer, and the learning rate is set to 0.001. The batch size is 64 and the total steps is 50,000. The temperature hyper-parameter \(r\) is set to 0.1. The probability of randomly drop uni-modal representation \(p\) is set to 0.2 and the scaling factor \(B\) is sampled from beta distribution \(B Be(4,4)\). All the experiments are implemented using Pytorch 1.9.0 and trained on PC with Intel(r)xo (8col) 5218K CPU@2.10GHz, 256 GB RAM, and an NVIDIA GeForce RTX3090 GPU. The modal selection criteria is illustrated in Appendix B. For ISCX-VPN and ISCX-Tor datasets, we use Raw-SFEM for short-term feature extraction. For SJTU-AN21 dataset, we employ Stat-SFEM and drop the payload modality (see Section 4.5 for more details).

### Comparison with State-of-the-Art Method

We compare AN-Net with seven state-of-the-art (SOTA) methods on three datasets. The experimental results are shown in Table 2. The seven methods can be devided into three categories: statistical feature-based methods (i.e., AppScanner, Decision Tree, and Whisper), sequential attribute-based methods (i.e., Flowlens and FS-Net), and raw traffic-based methods (i.e., AttnLSTM and ET-Bert).

**SJTU-AN21.** SJTU-AN21 (Wang et al., 2018) is a new anonymity network traffic dataset collected in the open network and the test set is collected separately in different network environment. Due to the interference of irrelevant packets and discrepancies between training and test data, previous methods never achieved more than 90% accuracy. According to Table 2, AN-Net significantly outperforms all existing methods. Specifically, AN-Net improves Accuracy and F1 by 8.15% and 6.24% respectively over the existing state of the art (i.e., ET-Bert). Previous methods designed for encrypted traffic classification ignored the noise of irrelevant packets and the unreliability of uni-modal attributes. As a comparison, we build strong short-term features and aggregate them with a carefully designed high temperature self-attention mechanism to resist irrelevant packet noise, and then propose to fuse representations from different modalities to combat per-packet attribute noise. Moreover, Stat-SFEM exhibits strong transfer capabilities thanks to the high-level statistical features when the network environments of the training data and test data are inconsistent (see Section 4.5 for more details).

**ISCX-Tor.** ISCX-Tor (Zhu et al., 2017) is a frequently used Tor network traffic dataset. Compared with SJTU-AN21, this dataset is less noisy and significantly larger in size. Because of the purity and large amount of data, DL-based methods that directly learn from raw sequential attributes or raw traffic payload (i.e., FS-Net, AttnLSTM, and ET-Bert) perform very well. AN-Net has an accuracy of 99.50% and a F1 of 99.51%, slightly better than these three DL-based methods, and significantly outperforms other four ML-based methods (i.e., AppScanner, Decision Tree, Whisper, and Flowlens). For example, AN-Net achieves 2.42% and 5.05% improvement on F1 over AttnLSTM and ET-Bert, respectively. Although the amount of data is large enough to support raw traffic-based methods to extract implicit and robust features from payload, attributes from other modalities can still help improve model performance.

**ISCX-VPN.** ISCX-VPN (Zhu et al., 2017) is a commonly used VPN traffic dataset. Similar to anonymous networks, VPN networks use proxies to transmit information for hiding IP information. Therefore, VPN traffic classification also suffers from irrelevant packet noise. AN-Net pushes F1 on ISCX-VPN to 99.96%. Our model achieves more than 15.98% improvement on F1 over statistical-based methods and sequential attribute-based methods, and performs slightly better than two raw traffic-based methods (2.18% and 1.08% improvement on F1 over AttnLSTM and ET-Bert). Moreover, AN-Net exhibits greater robustness against injected noise packet attacks (see Section 4.3).

### Robustness Analysis

To evaluate the robustness of AN-Net, we assume that attackers construct injected noise packet attacks, i.e., injecting irrelevant packets into original traffic to evade supervision. In the experiments, for simplicity, we assume attackers randomly select packet sequences from the entire dataset as noise traffic. We then mix original traffic with noise traffic in different ratios, i.e., the proportion of noise traffic ranges from 0 to 75%. We do not inject a higher proportion of noise traffic because the effectiveness of other methods is already low. Figure 4 shows F1 scores of AN-Net and seven SOTA methods

   Dataset &  &  &  \\  Method & AC & PR & RC & F1 & AC & PR & RC & F1 & AC & PR & RC & F1 \\  AppScanner (Sutter et al., 2017) & 0.7181 & 0.7535 & 0.7181 & 0.7038 & 0.8203 & 0.8117 & 0.8203 & 0.8022 & 0.7293 & 0.7378 & 0.7293 & 0.7193 \\ Decision Tree (Zhu et al., 2017) & 0.5702 & 0.6630 & 0.5702 & 0.5621 & 0.8059 & 0.7926 & 0.8059 & 0.7942 & 0.8259 & 0.8204 & 0.8259 & 0.8211 \\ Whisper (Wang et al., 2018) & 0.4820 & 0.5629 & 0.4820 & 0.5066 & 0.6723 & 0.7886 & 0.6723 & 0.6975 & 0.5848 & 0.6027 & 0.5848 & 0.5486 \\  Flowlens (Chen et al., 2018) & 0.6943 & 0.7576 & 0.6943 & 0.7128 & 0.8003 & 0.8703 & 0.8003 & 0.8256 & 0.6336 & 0.6674 & 0.6336 & 0.5820 \\ FS-Net (Zhu et al., 2017) & 0.8083 & 0.8233 & 0.8083 & 0.7949 & 0.9322 & 0.9342 & 0.9322 & 0.9315 & 0.8457 & 0.8502 & 0.8457 & 0.8398 \\  AttnLSTM (Wang et al., 2018) & 0.8120 & 0.8176 & 0.8120 & 0.8030 & 0.9725 & 0.9718 & 0.9725 & 0.9708 & 0.9778 & 0.9781 & 0.9778 & 0.9778 \\ ET-Bert (Zhu et al., 2017) & 0.8661 & 0.9163 & 0.8661 & 0.8815 & 0.9525 & 0.9514 & 0.9525 & 0.9445 & 0.9885 & 0.9895 & 0.9885 & 0.9888 \\  AN-Net (ours) & **0.9476** & **0.9490** & **0.9476** & **0.9439** & **0.9951** & **0.9951** & **0.9951** & **0.9950** & **0.9996** & **0.9996** & **0.9996** & **0.9996** & **0.9996** \\   

Table 2. Comparison Results on SJTU-AN21, ISCX-Tor, and ISCX-VPN datasets.

over three datasets under injected noise packet attacks. According to the results, we conclude that attackers cannot confuse AN-Net via injected noise packet attacks. However, attackers can fool other encrypted traffic classification models.

Raw traffic-based methods (i.e., AttnLSTM and ET-Bert) are very vulnerable to injected noise packet attacks. For instance, the F1 scores of ET-Bert and AttnLSTM On three datasets are reduced by at least 64.6% and 69.6% respectively. Payloads from irrelevant packets can easily lead to incorrect recognition results. Similarly, long-term statistical features (e.g., Maximum value, Mean value) can also be severely corrupted by inserted noise packets. Statistical feature-based methods (i.e., AppScanner, Decision Tree, and Whisper) have at most 43.0%, 36.5%, and 36.7% F1 score decrease over three datasets, respectively. As a comparison, sequential attribute-based methods (i.e., Flowlens and FS-Net) is more robust against irrelevant packet noise, since a part of clean original attribute sequences is retained. However, the F1 scores of FS-Net on the SJTU-AN21 and ISCX-VPN datasets still drop by 1.79% and 16.9%, respectively. In contrast, AN-Net maintains similar classification performance, where the F1 score fluctuations are less than 2.39% on SJTU-AN21 dataset and 0.32% on ISCX-VPN dataset. On ISCX-Tor dataset, due to the large amount of data, the F1 score of AN-Net drops by 8.8% to 90.71%, which is still higher than all other SOTA methods.

In summary, AN-Net can achieve robust classification because of the short-term representation learning and the multi-modal representation fusion. In particular, attackers cannot easily disrupt all short-term features of all modalities and thus AN-Net is robust against injected noise packet attacks.

### Ablation Analysis

We provide an ablation analysis to verify the contribution of each component on SJTU-AN21 dataset. In addition to normal traffic, we also perform ablation experiments under injected noise packet attacks to prove the effectiveness of these components against noise.

#### 4.4.1. Short-term Representation Learning

In this section, we ablate short-term features and high temperature (HT) self-attention mechanism, as shown in Table 3. We do not perform flow division and directly extract long-term statistical features when ablating short-term features. Then we use vanilla self-attention mechanism to substitute high temperature self-attention mechanism. As discussed above (Section 4.3), long-term statistical features are severely corrupted by inserted noise packets. Results show that the F1 score of using long-term features is 19.05% lower than using short-term features when the noise ratio of irrelevant packets is set to 75%. Therefore, it is crucial to model short-term features to combat irrelevant packet noise. Besides, high temperature self-attention mechanism improves model performance through paying less attention to noise packets. This improvement increases as the noise ratio increases, from 0.87% to 1.32%. The visualization of high temperature self-attention mechanism is shown in Appendix 5.

We further investigate the impact of the temperature hyper-parameter \(\), as shown in Figure 5. Results show that AN-Net

Figure 4. F1 score of AN-Net and seven SOTA methods under injected noise packet attacks.

Figure 5. The impact of the temperature hyper-parameter \(\).

    &  &  \\   & & 0 & 1/4 & 1/2 & 3/4 \\  Long-Term & / & 0.9427 & 0.9276 & 0.9093 & 0.7295 \\   &  & 0.9352 & 0.9329 & 0.9327 & 0.9068 \\  & & **0.9439** & **0.9430** & **0.9423** & **0.9200** \\   

Table 3. Ablation study on short-term features and high temperature (HT) self-attention mechanism.

[MISSING_PAGE_FAIL:8]