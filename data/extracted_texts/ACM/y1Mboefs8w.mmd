# EXGC: Bridging Efficiency and Explainability in Graph Condensation

Anonymity

###### Abstract.

Graph representation learning on vast datasets, like web data, has made significant strides. However, the associated computational and storage overheads raise concerns. In sight of this, Graph condensation (Gcond) has been introduced to distill these large real datasets into a more concise yet information-rich synthetic graph. Despite acceleration efforts, existing Gcond methods mainly grapple with efficiency, especially on expansive web data graphs. Hence, in this work, we pinpoint two major inefficiencies of current paradigms: (1) the concurrent updating of a vast parameter set, and (2) pronounced parameter redundancy. To counteract these two limitations correspondingly, we first (1) employ the Mean-Field variational approximation for convergence acceleration, and then (2) propose the objective of Gradient Information Bottleneck (GDIB) to prune redundancy. By incorporating the leading explanation techniques (_e.g._, GNNExplainer and GSAT) to instantiate the GDIB, our EXGC, the Efficient and **eX**plainable **G**raph **C**ondensation method is proposed, which can markedly boost efficiency and inject explainability. Our extensive evaluations across eight datasets underscore EXGC's superiority and relevance. Code is available at [https://anonymous.4open.science/r/EXGC](https://anonymous.4open.science/r/EXGC).

Graph Neural Networks, Graph Condensation, Model Explainability +
Footnote †: copyright: none

## 1. Introduction

Web data, such as social networks (Hamilton et al., 2017), transportation systems (Zhu et al., 2017; Li et al., 2017), and recommendation platforms (Xu et al., 2018; Li et al., 2018), are often represented as graphs. These graph structures are ubiquitous in everyday activities, including streaming on Netflix, interacting on Facebook, shopping on Amazon, or searching on Google (Li et al., 2018; Li et al., 2018). Given their tailor-made designs, Graph Neural Networks (GNNs) (Li et al., 2018; Li et al., 2018; Li et al., 2018) have emerged as a prevalent solution for various tasks on graph-structured data and showcased outstanding achievements across a broad spectrum of graph-related web applications (Gil et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018).

However, real-world scenarios often entail the handling of large-scale graphs encompassing millions of nodes and edges (Li et al., 2018; Li et al., 2018), posing substantial computational burdens during the training of GNN applications (Li et al., 2018; Li et al., 2018; Li et al., 2018). Worse still, the challenges are exacerbated when fine-tuning hyperparameters and discerning optimal training paradigms for over-parametrized GNN models. Against this backdrop, a crucial inquiry arises: _can we effectively simplify or reduce the graph size to accelerate graph algorithm operations, including GNNs, while also streamlining storage, visualization, and retrieval essential for graph data analysis (Li et al., 2018; Li et al., 2018; Li et al., 2018)_

As a primary solution, graph sampling emphasizes selecting pivotal edges/nodes and omitting the less relevant ones (Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018). However, this can lead to considerable information loss, potentially harming model performance (Zhu et al., 2017; Li et al., 2018). Conversely, graph distillation aims to compress the extensive real graph \(\) into a concise yet information-rich synthetic graph \(\), enhancing the efficiency of the graph learning training process. Within this domain, the **graph condensation** (Gcond) stands out due to its exceptional compression capabilities (Li et al., 2018; Li et al., 2018). For instance, as depicted in Figure 1 (a), the graph learning model trained on the synthetic graph \(\) (containing just 154 nodes generated by Gcond) yields a 91.2% test accuracy on Reddit, nearly matching the performance of the model trained on the original dataset with 153,932 nodes (_i.e._, 93.5% accuracy).

Despite their successes, we argue that even with various acceleration strategies, current Gcond methods remain facing efficiency challenges in the training process, particularly on large graph datasets such as web data. This inefficiency arises from two main factors:

* Firstly, as depicted in Figure 1 (b), the **prolonged convergence** stems from the concurrent updating of an overwhelming number of parameters (_i.e._, elements in node features of \(\)). Specifically, unlike conventional graph learning where parameter dimensionality is dataset-agnostic, in Gcond, the number of parameters grows with the nodes and node feature dimensions, imposing substantial computational and storage demands.
* Secondly, as illustrated in Figure 1 (c), the current Gcond approaches mainly exhibit **node redundancy**. Concretely, when compressing new datasets, to counteract the risk of insufficient information capacity from too few nodes, a higher node count

Figure 1. The compression capability and limitations of current Gcond. (a) Gcond adeptly compresses the dataset to just 0.1% of its initial size without compromising the accuracy benchmarks. (b) Contrary to traditional graph learning, Gcond’s parameters scale with node count. (c) To avoid insufficient information capacity, Gcond typically introduces node redundancy.

is typically employed by \(\), leading to parameter redundancy in the training process. Depending on the dataset attributes, this redundancy can vary, with some instances exhibiting as much as 92.7% redundancy. We put further discussion in Section 3.3.

In sight of this, in this work, we aim to refine the paradigm of GCond to mitigate the above limitations. Specifically, **for the first limitation**, we scrutinize and unify the paradigms of the current methods from the perspective of Expectation Maximization (EM) framework (Brockman, 1988; Goyal et al., 2017), and further formulate it as the theoretical basis for our forthcoming optimization schema. From this foundation, we pinpoint the efficiency bottleneck in the training process, _i.e._, the computation of intricate posterior probabilities during the Expectation step (E-step). This insight led us to employ the Mean-Field (MF) variational approximation (Brockman, 1988) - a renowned technique for improving the efficiency of E-step with intricate variables - to revise the paradigm of GCond. The streamlined method is termed Mean-Field Graph Condensation (MGCond).

Then, **for the second limitation**, our solution seeks to 'explain' the training process of the synthetic graph \(\): we prioritize the most informative nodes in \(\) (_i.e._, nodes encapsulating essential information for model training) and exclude the remaining redundant nodes from the training process. To formulate this objective, inspired by the principle of graph information bottleneck, we introduce the Gradient Information Bottleneck (GDIB). Building upon GDIB, our EXGC, the Efficient and eXplainable Graph Condensation method, is proposed by integrating the leading explanation strategies (e.g., GNNExplainer (Sandes et al., 2017) and GSAT (Sandes et al., 2017)) into the paradigm of MGCond.

Our contribution can be summarized as follow:

* For the limitation of inefficiency, we unify the paradigms of current approaches to pinpoint the cause and leverage Mean-Field variational approximation to propose the MGCond for boosting efficiency (Section 3.1 & 3.2).
* For the caveat posed by node redundancy, we introduce the objective of Gradient Information Bottleneck, and utilize the leading explanation methods to develop an explainable and efficient method, EXGC (Section 3.3 & 3.4).
* Extensive experiments demonstrate that our EXGC outperforms the baselines by a large margin. For instance, EXGC is 11.3 times faster than the baselines on Citeseer (Section 4).

Furthermore, it is worth mentioning that beyond the tasks of graph condensation, the superior performance of EXGC across various backbones (_i.e._, explainers) also verifies the effectiveness of the graph explanation methods in enhancing downstream graph tasks. To our knowledge, this stands as one of the vanguard efforts in the application of graph explainability, addressing a crucial yet rarely explored niche.

## 2. Problem Formulation

In this part, we retrospect the objective of graph condensation. Specifically, graph condensation endeavors to transmute a large, original graph into a compact, synthetic, and highly informative counterpart. The crux of this process is to ensure that the GNNs trained on the condensed graph manifest a performance comparable to those trained on the original graph.

**Notations.** Initially, we delineate the common variables utilized in this study. We start from the original graph \(=(,,)\), where \(^{N N}\) is the adjacency matrix, \(N\) is the number of nodes and \(^{N d}\) is the \(d\)-dimensional node feature attributes. Further, we note the label of nodes as \(=\{0,1,,C-1\}^{N}\) denotes the node labels over \(C\) classes. Our target is to train a synthetic graph \(=(^{},^{},^{})\) with adjacency matrix \(^{}^{N^{} N^{}}\) and feature attributes \(^{}^{N^{} D}\) (\(N^{} N\)), which can achieve comparable performance with \(\) under GNNs inference process.

**Graph condensation via gradient matching.** The above objective of graph condensation can be formulated as follows:

\[&_{}(f_{_{ }}(,),),\\ &\ \ _{}=*{arg\,min}_{ }(f_{}(^{},^{}), ^{}), \]

where \(\) represents the loss function and \(f_{}\) denotes the graph learning model \(f\) with parameters \(\). In pursuit of this objective, the previous works typically employ the gradient matching scheme following (Sandes et al., 2017; Wang et al., 2018; Wang et al., 2018). Concretely, given a graph learning model \(f_{}\), these methods endeavor to reduce the difference of model gradients _w.r.t._ real data \(\) and synthetic data \(\) for model parameters (Sandes et al., 2017). Hence, the graph learning models trained on synthetic data will converge to similar states and share similar test performance with those trained on real data.

## 3. Methodology

In this section, we first unify the paradigms of current GCond methods in Section 3.1. Building upon this, we propose the MGCond, which employs MF approximation to boost efficiency in Section 3.2. Furthermore, to eliminate the redundancy in the training process, we introduce the principle of GDIB in Section 3.3 and instantiate it to develop our EXGC in Section 3.4.

### The Unified Paradigm of GCond

As depicted in Section 2, graph condensation aims to match the model gradients _w.r.t_ large-real graph \(\) and small-synthetic graph \(\) for model parameters. This process enables GNNs trained on \(\) and \(\) to share a similar training trajectory and ultimately converge to similar states (parameters). We formulate this gradient matching process as follows:

\[&_{}_{-_{ }}P(^{}_{}=_{}),\\ &\ ^{}_{}=(f_{ }(),^{})}{_{}},_{}= (f_{}(),)}{_{ }}, \]

where \(_{}\) denotes the distribution of \(\)'s potential states during the training process. For example, (Sandes et al., 2017) defines \(_{}\) as the set of parameter states that can appear throughout a complete network training process, while (Sandes et al., 2017) simply defines it as the potential initial states of the parameters.

Considering the computational complexity of jointly optimizing \(^{}\), \(^{}\), and \(^{}\), and the interdependency between these three variables, current methods typically fix the labels \(^{}\) and design a MLP-based model \(g_{}\) with parameters \(\) to calculate \(^{}\) following \(^{}=g_{}\) (\(^{}\)) (Zhou et al., 2017). In this case, Equation 2 can be rewrite as:

\[_{^{},}E_{_{ }}P(^{}_{}=_{}),\] \[\ ^{}_{}= (f_{}(^{},g_{ }(^{})),^{})}{_{}},_{ }=(f_{}(,),) }{_{}}. \]

Without loss of generality, \(^{}_{}\) and \(_{}\) are consistently defined as provided here in the following text, even though not all previous methods have employed the MLP-based simplification strategy 1.

After random initialization, Equation 3 can be achieved by alternately optimizing the variable \(^{}\) and the model parameters \(\), which naturally adheres to the Expectation-Maximization schema, as shown in Figure 2 (a). Specifically, the EM algorithm alternates between the expectation step (E-step) and the maximization step (M-step):

* \(\)**-step:** Estimate the variable \(^{}\) while freezing the model \(g_{}\), then utilize it to calculate the Evidence Lower Bound (ELBO) of the objective of the gradient matching in Equation 3.
* \(\)**-step:** Fine the parameters \(\) which maximizes the above ELBO.

After instantiating the above schema, graph condensation can be formulated as follows, where \(t\) represents the training epoch and \(_{}\) is a simplified notation for \(^{}_{}=_{}\):

* **Initialization:** Select the initial value of the parameter \(^{(0)}\) and the node feature \(^{(0)}\), then start the iteration;
* \(\)**-step:** Use the model \(g(^{(t)})\) to estimate the node features \(^{(t)}\) according to \(P(^{(t)}|_{},^{(t)})\) and calculate the ELBO:

\[ E_{^{(t)}|_{},^{(t)}} ^{(t)},_{})}{P( ^{(t)}_{},^{(t)})}; \]
* \(\)**-step:** Find the corresponding parameters \(^{(t+1)}\) when the above ELBO is maximized:

\[^{(t+1)}_{}E_{^{(t)}|_{ },^{(t)}}^{(t)},_{} )}{p(^{(t)}_{},^{(t)})}; \]
* **Output:** Repeat the E-step and M-step until convergence, then output the synthetic graph \(\) according to the final \(^{}\) and \(\).

The detailed derivation of the above Equations is shown in Appendix B.

**Revealing the Limitation of Inefficiency.** However, we have noticed that even with various acceleration strategies (Zhou et al., 2017), the above paradigm remains facing efficiency challenges in the training process. We attribute this limitation to the estimation process of \(^{}\) in E-step. Specifically, in contrast to traditional graph learning tasks where the number of network parameters is **dataset-agnostic**, for graph condensation task, the number of to-be-updated parameters in E-step (_i.e._, elements in \(^{}\)) linearly **increases** with the number of nodes \(\) and feature dimensions \(d\), posing substantial burden of gradient computation and storage.

This flaw is particularly evident on large graph datasets such as web data with millions of nodes (\(N\)) and thousands of feature dimensions (\(d\)). Therefore, it is crucial to find a shortcut for expediting the current paradigm.

### Boost Efficiency: MGCond

To address the limitation of inefficiency, we aim to inject the Mean-Field variational approximation (Brockman and Komodakis, 2018) into the current GCond paradigm. In practice, MF approximation has been extensively verified to enhance the efficiency of the EM framework containing variables with complex distributions. Hence, it precisely matches the challenge encountered in our E-step, where the to-be-updated variable \(^{}\) possesses large dimensions. Next, we elucidate the process of leveraging MF estimation to enhance the GCond paradigm.

Firstly, MF approximation assumes that the to-be-updated variable can be decomposed into multiple independent variables, aligning naturally with the property of node features \(^{}=\{x^{}_{1},x^{}_{2},...,x^{}_{N^{}}\}\) of \(\) in our E-step (_i.e._, Equation 4):

\[P(^{})=_{i=1}^{N^{}}P(x^{}_{i}), \]

where \(x^{}_{i}\) is the feature of the \(i\)-th node in graph \(\). By substituting Equation 6 into the ELBO in Equation 4 we obtain:

\[&=_{i=1}^{N^{ }}P(x^{}_{i}) P(_{},^{})d ^{}\\ &-_{i=1}^{N^{}}P(x^{}_{i}) _{i=1}^{N^{}}P(x^{}_{i})d^{}.  \]

Figure 2. The paradigm of current GCond methods from the perspective of the EM schema, and the E-step of our proposed MGCond and EXGC.

In this case, while we focus on the node feature \(x^{}_{j}\) and fix its complementary set \(}_{ j}=\{x^{}_{1},...x^{}_{j-1},x^{} _{j+1},...,x^{}_{N^{}}\}\), the ELBO in Equation 7 can be rewritten as:

\[&= P(x^{}_{j} )_{i=1,i j}^{N^{}}P(x^{}_{i}) P( _{},})d_{i j}x^{}_{i}dx^{}_{j}\\ &- P(x^{}_{j}) P(x^{}_{j} )dx^{}_{j}+_{i=1,i j}^{N^{}} P(x^{}_{i }) P(x^{}_{i})dx^{}_{i}, \]

where the third term can be considered as the constant \(C\) because \(}_{ j}\) is fixed. Then, to simplify the description, we define:

\[_{j}(},_{ })&=E_{_{i=1,i j}^{N^{}}P(x^{}_{ i})}[ P(},_{})]\\ &=_{i=1,i j}^{N^{}}P(x^{}_{i} ) P(},_{})d_{i j}x^{}_{i},  \]

and combine it with Equation 8 to obtain the final form of the ELBO which is streamlined by the MF variational approximation:

\[&= P(x^{}_{j}) _{j}(},_{})}{P(x^{}_{j} )}dx^{}_{j}+C\\ &=-KL(P(x^{}_{j})[(},_{})]+C, \]

where \(KL\) denotes the Kullback-Leibler (KL) Divergence (Kirkpatrick et al., 2017). Due to the non-negativity of the KL divergence, maximizing this ELBO is equivalent to equating the two terms in the above KL divergence. Based on this, we have:

\[P(})_{j=1}^{N^{}}_{j}(},_{}), \]

which can be regarded as the theoretical guidance for the \(}\) estimation process in the E-step. The detailed derivation is exhibited in Appendix C.

**The Paradigm of MGCond.** Equation 11 indicates that the estimation of node feature \(x^{}_{j}\) in E-step can be performed while keeping its complementary features \(}_{ j}\) fixed. Without loss of generality, we generalize this conclusion from individual nodes to subsets of nodes, and distribute the optimization process of each set evenly over multiple iterations. This optimized E-step is the key distinction between our MGCond and the prevailing paradigm, as illustrated in Figure 2 (b). To be more specific, the paradigm of MGCond can be formulated as follows:

* **Initialization:** Select the initial value of the parameter \(^{(0)}\) and features \(}^{(0)}\), divide the nodes in graph \(\) into \(K\) parts equally _i.e._, \(}=\{}_{1},}_{2},..., }_{K}\}\), and start the iteration;
* **E-step:** Use the model \(g(^{(t)})\) to estimate the features in subsets \(}_{k}^{(t)}\) for \(k=\{1,2,...,K\}\) according to: (12) \[}_{k}^{(t+1)}=_{_{k}}P(}_{k}|}_{k}^{(t)},_{},^{(t)}),&k=r+1,\\ }_{k}^{(t)},&\]

where \(r\) is the remainder when \(t\) is divided by \(K\).

* **M-step:** Find the corresponding parameters \(^{(t+1)}\) when the following ELBO is maximized: \[^{(t+1)}:=_{}E_{}^{(t+1)}|_{}, ^{(t)}}[}^{(t+1)},_{} )}{p(}^{(t+1)}_{},^{(t)})}];\]
* **Output:** Repeat the E-step and M-step until convergence, then output the synthetic graph \(\) according to the final \(}\) and \(\).

### Node Redundancy and GDIB

After executing MGCond we summarize two empirical insights that primarily motivated the development of our XEGC as follows:

1. The training process of \(}\) in E-step exhibits a _long-tail problem_. That is, when **20%** of the node features \(}\) are covered in training (_i.e._, \(t 0.2K\)), the improvement in test accuracy has already achieved **93.7%** of the total improvement on average. In other words, the remaining 80% of the node features only contribute to 6.3% of the accuracy improvement.
2. This long-tail problem has a larger variance. Specifically, even for the same task with the same setting and initialization, when 20% of the \(}\) are covered in training, the maximum difference between test accuracy exceeds 25% (_i.e._, difference between 72.4% and 98.8%), since those 20% trained nodes are randomly selected from \(\).

These two observations indicate that there is a considerable **redundancy** in the number of to-be-trained nodes. That is, the synthetic graph \(\) comprises a subset of key nodes that possess most of the necessary information for gradient matching. If the initial random selections pinpoint these key nodes, the algorithm can yield remarkably high test accuracy in the early iterations. On the other side, entirely training all node features \(}\) in \(\) would not only be computationally wasteful but also entail the potential risk of overfitting the given graph learning model.

Therefore, it naturally motivates us to identify and train these key nodes in E-step (instead of randomly selecting nodes to participate in training like MGCond). To guide this process, inspired by the Graph Information Bottleneck (GIB) for capturing key subgraphs (Stein et al., 2017; Zhang et al., 2017) and guiding GNNs explainability (Stein et al., 2018; Zhang et al., 2018), we propose the GraDient Information Bottleneck (GDIB) for the compact graph condensation with the capability of redundancy removal:

**Definition 1** (Gib).: _Given the the synthetic graph \(\) with label \(}\) and the GNN model \(f_{0}\), GDIB seeks for a maximally informative yet compact subgraph \(_{sub}\) by optimizing the following objective:_

\[_{_{sub}}I(_{sub}; ^{}_{})- I(_{sub}; ),\,\,\,_{sub}_{sub}(),  \]

_where \(^{}_{}\) denotes the gradients \((f_{}(),})/\); \(_{sub}()\) indicates the set of all subgraphs of \(\); \(I\) represents the mutual information (MI) and \(\) is the Lagrangian multiplier._

### Prune Redundancy: EXGC

**A Tractable Objective of GDIB.** To pinpoint the crucial node features to participate in the training process in E-step, we first derive a tractable variational lower bound of the GDIB. Detailed derivation can be found in Appendix D, which is partly adapted from (Zhang et al., 2017; Zhang et al., 2017).

Specifically, for the first term \(I(_{sub};^{}_{})\), a parameterized variational approximation \(Q(^{}_{}|_{sub})\) for \(P(^{}_{}|_{sub})\) is introduced to derive its lower bound:

\[I(_{sub};^{}_{})_{_{sub} ;^{}_{}}[ Q(^{}_{}|_{ sub})]. \]

For the second term \(I(_{sub};)\), we introduce the variational approximation \(R(_{sub})\) for the marginal distribution \(P(_{sub})=_{}R(_{sub}| )P()\) to obtain its upper bound:

\[I(_{sub};)_{}[ (P(_{sub}|)\|R( _{sub}))]. \]

By incorporating the above two inequalities, we derive a variational upper bound for Equation 14, serving as the objective for

\[*{arg\,max}_{_{sub}}[ Q(^{ }_{}|_{sub})]-[ (P(_{sub}|)\|R(_{sub})) ]. \]

**Instantiation of the GDIB**. To achieve the above upper bound, we simply adopt \((f_{}(_{sub}),^{})/\) to instantiate the distribution \(Q\). Then, we specify the distribution \(R\) in Equation 16 as a Bernoulli distribution with parameter \(r\) (_i.e._, each node is selected with probability \(r\)). As for \(P(_{sub}|)\), we suppose it assigns the importance score \(p_{i}\) (_i.e._, the probability of being selected into \(_{sub}\)) to the \(i\)-th node in \(\). After that, GDIB can be instantiated by the post-hoc explanation methods such as:

* **Gradient-based** methods like SA  and GradCAM . For the \(i\)-th node, these methods first calculate the absolute values of the elements in the derivative of \((f_{}(),^{})\)_w.r.t_\(x_{i}\) (_i.e._, the features of the \(i\)-th node). After that, the importance score \(p_{i}\) is defined as the normalized sum of these values. More formally: \[p_{i}=*{softmax}_{i[1,2,,N]}\{|(f_{}(),^{})}{ x_{i}} |^{T}\}.\] (18)
* **Local Mask-based** methods like GNNExplainer  and GraphMASK . Concretely, for the first term of Equation 17, these methods firstly multiply the node's features \(x^{}_{i}\) with the initialized node importance score \(p_{i}\) to get \(^{}=\{p_{1}x^{}_{1},p_{2}x^{}_{2}, ,p_{N} x^{}_{N^{}}\}\), and feed \(^{}\) into model \(f_{}\) to obtain the output \(y_{p}\). Then they attempt to find the optimal score \(p_{i}\) by minimizing the difference between this processed output \(y\) and the original prediction. Concurrently, for the second term of Equation 17, these methods set \(r\) to approach \(0\), making the value of the KL divergence proportional to the score \(p_{i}\). As a result, they treat this KL divergence as the \(l_{1}\)-norm regularization term acting on \(p_{i}\) to optimize the training process of \(p_{i}\). After establishing these configurations, the optimal score can be approximated through several gradient descents following: \[=_{}D(y;y_{p})+ ^{T},\] (19) where \(\) is defined as \(\{p_{1},p_{2},...,p_{N}\}\); \(D\) denotes the distance function; \(\) is the trade-off parameter; \(y\) and \(y_{p}\) represents: \[\{y=f_{}(\{^{},g_{}(^{})\}),\\ y_{p}=f_{}(\{^{},g_{}(^{ })\}),.\] (20)
* **Global Mask-based** methods like GSAT2 and PGExplainer . Here, during the instantiation process of the first term of Equation 17, the trainable \(p_{i}\) in Local Mask-based methods is replaced with a trainable \(_{}\) (_i.e._, \(p_{i}=_{}(x^{}_{i})\)) and \(y_{p}\) is correspondingly replaced with \(y_{}\). Meanwhile, for the second term in Equation 17, these methods set \(r(0,1)\) to instantiate the KL divergence as the _information constraint_ (\(_{l}\)) proposed by , where \(_{l}\) is defined as:

\[_{l}=_{i 1,2,,N}p_{i}}{r}+(1-p_{i})}{1-r}. \]

Treating \(_{l}\) as a regularization term acting on \(\), the explainers can obtain the approximate optimal score \(\) through several gradient optimizations of \(\) following:

\[=_{}D(y;y_{})+_{l}. \]

After obtaining the importance score \(p_{i}\), the crucial subgraph \(_{sub}\) in GDIB can be composed of nodes with larger scores \(p_{i}\).

**The Paradigm of EXGC**. As illustrated in Figure 2 (c), after leveraging the above leading post-hoc graph explanation methods to achieve the objective of GDIB, we summarize the paradigm of our EXGC as follows:

* **Initialization:** Select the initial value of the parameter \(^{(0)}\), the node features \(^{}({}^{0})\), the set of the node index \(\) and the ratio of nodes optimized in each E-step as \(\), then start the iteration;
* **E-step:** Leverage the above explainers to assign an importance score \(p_{i}\) to the \(i\)-th node in \(\) for the index \(i\) in set \(\): \[\{p_{i}\}=(\{x_{i}\},f_{}),\ i.\] (23)

Subsequently, remove the indices corresponding to the nodes with the top \( N^{}\) scores from set \(\). Then use the model \(g(^{(t)})\) to estimate the features \(^{}(t+1)\) according to:

\[\{^{}_{}(t+1)=_{ _{}}P(^{}_{}|^{}_{ ^{}}_{},^{(t)}),\\ ^{}_{}(t+1)=^{}_{^{}} . \]

where \(^{}_{}=\{x_{i}\}\) for \(i\), and \(^{}_{}=^{}^{}_{}\).
* **M-step:** Find the corresponding parameters \(^{(t+1)}\) when the following ELBO is maximized: \[^{(t+1)}:=*{arg\,max}_{}E_{^{}(t+1)}|_{_{},^{(t)}} [^{}(t+1),_{})}{p(^{}(t+1) _{},^{(t)})}];\] (25)
* **Output:** Repeat the E-step and M-step until convergence, then output the synthetic graph \(\) according to the final \(^{}\) and \(\).

The comparison between E-steps in the paradigms of GCM, MGCond and EXGC is exhibited in Figure 2 (d). By leveraging graph explanation methods to instantiate the objective of GDIB and seamlessly integrating it within the MGCond's training paradigm, our proposed EXGC adeptly identifies pivotal nodes in the synthetic graph \(\) during early training stages. Experimental results in the ensuing section underline that EXGC frequently converges early - specifically when a mere **20%** of the nodes in \(\) participate in training - attributed to the successful identification of these key nodes. EXGC's computational focus on these essential nodes ensures resource optimization, precluding superfluous expenditure on extraneous nodes. As a result, it can not only boost the efficiency but also enhance the test accuracy.

## 4. Experiments

In this section, we conduct experiments on six node classification graphs and three graph classification benchmarks to answer the following research questions:

* **RQ1.** How effective is our EXGC _w.r.t_ efficiency and accuracy?
* **RQ2.** Can the design of EXGC be transferred to the state-of-the-art graph condensation frameworks (_e.g._, DosGcond)?
* **RQ3.** What is the impact of the designs (_e.g._, the backbone explainers) on the results? Is there a guideline for node selection?
* **RQ4.** Does the condensed graph exhibit strong cross-architecture capabilities?

### Experimental Settings

**Datasets.** To evaluate the effectiveness of EXGC, we utilize six node classification benchmark graphs, including four transductive graphs, Cora (Zhou et al., 2017), Citeseer (Citeseer, 2018), Ogbn-Arxiv and Ogbn-Product (Zhou et al., 2017) and two inductive graphs, _i.e._, Flickr (Citeseer, 2018) and Reddit (Kawaguchi et al., 2018). For a fair comparison, we adopt the setup outlined in (Zhou et al., 2017) and document the performance of various frameworks on the aforementioned datasets. Without loss of generality, we also select three graph classification datasets for evaluation: the Ogbg-molhiv molecular dataset (Zhou et al., 2017), the TUDatasets (DD) (Zhu et al., 2017) and one superpixel dataset CIFAR10 (Krizhevsky et al., 2009).

**Backbones.** In this paper, we employ a wide range of backbones to systematically validate the capabilities of EXGC. We choose one representative model, GCN (Zhou et al., 2017), as our training model for the gradient matching process.

* To answer **RQ1**, we follow GCM to employ three coreset methods (_Random_, _Herding_(Zhu et al., 2017) and _K-Center_(Citeseer, 2018)) and two data condensation models (DC-Graph) and GCond provided in (Zhou et al., 2017). Here we showcase the detailed settings in Table 2.
* To answer **RQ2**, we choose the current SOTA graph condensation method, DosGcond as backbone (Citeseer, 2018). DosGcond eliminates the parameter optimization process within the inner loop of GCM, allowing for one-step optimization. This substantially reduces the time required for gradient matching. We employ DosGcond to further assess the generalizability of our algorithm.
* To answer **RQ3**, we select the explanation methods for node in \(\) based on gradient magnitude (SA) (Bog et al., 2018), global mask (GSAT) (Zhou et al., 2017), local mask (GNNExplainer) (Zhou et al., 2017) as well as random selection, to evaluate the extensibility of backbone explainers.
* To answer **RQ4**, we choose currently popular backbones, such as APPNP (Zhou et al., 2017), SGC (Kawaguchi et al., 2018) and GraphSAGE (Kawaguchi et al., 2018) to verify the transferability of our condensed graph (GCN as backbone). We also include MLP for validation.

**Measurement metric.** To ensure a fair comparison, we train our proposed method alongside the state-of-the-art approaches under identical settings, encompassing learning rate, optimizer, and so forth. Initially, we generate three condensed graphs, each developed using training methodologies with distinct random seeds. Subsequently, a GNN is trained on each of these graphs, with this training cycle repeated thrice (**Record the mean of the run time**). To gauge the information retention of the condensed graphs, we

    &  &  &  &  &  &  \\   & & **Random** & **Hering** & **K-Center** & **Gcond-X** & **Gcond** & **EXGC-X** & **EXGC** & **Full graph** & \\  Citeseer (47.1M) & 0.3\% & 33.87\({}_{40.82}\) & 31.31\({}_{1.20}\) & 34.03\({}_{42.52}\) & 64.13\({}_{4.18}\) & 63.98\({}_{44.31}\) & 67.82\({}_{41.31}\) & 69.16\({}_{26.00}\) & 71.12\({}_{0.06}\) & 0.142M & 333.3\(\) \\ Citeseer (47.1M) & 1.8\% & 42.66\({}_{41.30}\) & 40.61\({}_{2.13}\) & 51.79\({}_{3.20}\) & 67.24\({}_{1.85}\) & 66.82\({}_{22.70}\) & 69.60\({}_{41.88}\) & 70.09\({}_{40.72}\) & 71.12\({}_{0.06}\) & 0.848M & 55.6\(\) \\ Citeseer (47.1M) & 3.6\% & 59.74\({}_{2.88}\) & 63.85\({}_{1.77}\) & 67.25\({}_{1.80}\) & 69.86\({}_{0.97}\) & 69.74\({}_{2.36}\) & 70.18\({}_{1.17}\) & 70.55\({}_{0.98}\) & 71.12\({}_{0.06}\) & 1.696M & 27.8\(\) \\ Cora (14.9M) & 0.4\% & 37.04\({}_{7.41}\) & 43.47\({}_{0.55}\) & 46.33\({}_{3.32}\) & 69.10\({}_{4.01}\) & 72.76\({}_{0.45}\) & 80.91\({}_{3.09}\) & 82.02\({}_{0.42}\) & 80.91\({}_{0.10}\) & 0.060M & 250.0\(\) \\ Cora (14.9M) & 1.3\% & 59.62\({}_{2.48}\) & 62.18\({}_{1.91}\) & 69.12\({}_{2.25}\) & 75.38\({}_{1.89}\) & 79.29\({}_{2.06}\) & 80.74\({}_{0.41}\) & 81.94\({}_{0.81}\) & 80.91\({}_{0.10}\) & 0.194M & 76.9\(\) \\ Cora (14.9M) & 2.6\% & 73.29\({}_{1.00}\) & 70.91\({}_{2.12}\) & 73.66\({}_{1.86}\) & 75.93\({}_{0.80}\) & 80.02\({}_{0.61}\) & 65.07\({}_{2.82}\) & 82.26\({}_{0.80}\) & 80.91\({}_{0.10}\) & 0.388M & 38.5\(\) \\ Ogbn-arxiv (100.4M) & 0.5\% & 46.83\({}_{2.49}\) & 47.23\({}_{40.77}\) & 47.28\({}_{1.15}\) & 56.49\({}_{1.69}\) & 57.39\({}_{0.58}\) & 65.46\({}_{0.85}\) & 57.62\({}_{0.64}\) & 70.76\({}_{0.04}\) & 0.050M & 2000.0\(\) \\ Ogbn-arxiv (100.4M) & 0.25\% & 57.32\({}_{1.19}\) & 58.64\({}_{2.18}\) & 54.36\({}_{2.67}\) & 62.38\({}_{1.62}\) & 62.49\({}_{2.56}\) & 64.82\({}_{0.51}\) & 62.34\({}_{0.26}\) & 70.76\({}_{0.04}\) & 0.251M & 400.0\(\) \\ Ogbn-arxiv (100.4M) & 0.5\% & 60.09\({}_{0.49}\) & 61.25\({}_{0.88}\) & 60.84\({}_{0.59}\) & 63.77\({}_{0.95}\) & 64.85\({}_{0.74}\) & 65.79\({}_{0.32}\) & 64.99\({}_{0.70}\) & 70.76\({}_{0.04}\) & 0.502M & 200\(\) \\ Ogbn-Product (1412.5M) & 0.5\% & 57.49\({}_{2.53}\) & 60.10\({}_{0.36}\) & 59.46\({}_{1.22}\) & 61.59\({}_{0.61}\) & 62.15\({}_{0.36}\) & 62.71\({}_{0.91}\) & 62.09\({}_{0.74}\) & 70.76\({}_{0.04}\) & 70.63M & 200.0\(\) \\ Ogbn-Product (1412.5M) & 1.5\% & 58.84\({}_{1.87}\) & 63.17\({}_{0.93}\) & 60.71\({}_{0.85}\) & 62.98\({}_{1.30}\) & 63.89\({}_{0.51}\) & 65.85\({}_{0.95}\) & 64.69\({}_{1.43}\) & 70.76\({}_{0.04}\) & 21.189M & 66.7\(\) \\ Ogbn-Product (1412.5M) & 3\% & 60.10\({}_{0.94}\) & 63.87\({}_{0.94}\) & 62.60\({}_{1.08}\) & 65.82\({}_{0.95}\) & 65.30\({}_{0.92}\) & 67.50\({}_{1.05}\) & 63.76\({}_{0.72}\) & 70.76\({}_{0.04}\) & 42.378M & 33.3\(\) \\ Flickr (86.8M) & 0.1\% & 44.81\({}_{1.87}\) & 43.09\({}_{0.56}\) & 43.30\({}_{0.40}\) & 46.93\({}_{0.10}\) & 46.10\({}_{0.10}\) & 46.95\({}_{0.50}\) & 43.09\({}_{0.10}\) & 46.30\({}_{0.10}\) & 46.95\({}_{0.03}\) & 46.20 to train GNN classifiers, which are then tested on the real graph's nodes or entire graphs. By juxtaposing the performance metrics of models on these real graphs, we discern the informativeness and efficacy of the condensed graphs. All experiments are conducted in three runs, and we report the mean performance along with its variance.

### Main Results (RQ1)

In this subsection, we evaluate the efficacy of a 2-layer GCN on the condensed graphs, juxtaposing the proposed methods, EXGC-X and EXGC, with established baselines. It's imperative to note that while most methods yield both structure and node features, note as \(^{}\) and \(^{}\), there are exceptions such as DC-Graph, GCond-X, and EXGC-X. Owing to the absence of structural output from DC-Graph, GCond-X, and EXGC-X, we employ an identity matrix as the adjacency matrix when training GNNs predicated solely on condensed features. Nevertheless, during the inference, we resort to the complete graph in a transductive setting or the test graph in an inductive setting to facilitate information propagation based on the pre-trained GNNs. Table 1 delineates performance across six benchmarks spanning various backbones, from which we can make the following observations:

**Obs 1. EXGC and EXGC-X consistently outperform other baselines** under extremely large condensation rates, thereby validating their exceptional performance. To illustrate, on smaller datasets such as Citeseer and Cora, our models achieve compression rates ranging from 0.3% \(\)0.4%, representing an improvement of approximately 3.84% to 8.15% over the current state-of-the-art model, GCond and GCond-X. On larger graphs like Ogbn-product and Reddit, EXGC surpasses GCond by 0.56% to 0.78% when aiming for a 0.5% compression rate. These findings underscore the substantial contributions of iterative optimization strategy of the subset of the nodes in \(\) solely to the field of graph condensation (see Table 1). Additionally, our visualization results in Table 1 reveal that the graphs we condensed exhibit high density and compactness, with edges serving as efficient carriers of dense information, thereby facilitating effective information storage.

**Obs 2. Both EXGC and EXGC-X can achieve an extreme compression rate compared with the original graph** without significant performance degradation. On all six datasets, when compressing the original graph to a range of 0.05% to 5% of its original size, the compressed graph consistently maintains the performance of the original data while significantly accelerating the inference speed. This enhancement proves to be highly advantageous for information extraction and reasoning. Furthermore, as the storage requirement is reduced to a fraction of the original data's size (even better in some cases), this greatly facilitates the transmission of graph-type information in the web space, underscoring the exceptional contribution of our model to web systems.

### Generalizability on DosGCond (RQ2)

To answer RQ2, we choose a gradient-based explainer as the backbone explainer. We transfer the SA into the current SOTA graph condensation method, DosGCond, and named as EXDos. We record the **performance** and **training time** of each backbone. As shown in Table 3 and Figure 3, we can make the observations as following:

**Obs 3. Upon incorporating the backbone explainers, significant reductions in the training time of GCond are achieved.** Furthermore, when our approach is applied to DosGCond, EXDos still exhibits substantial efficiency gains. Specifically, we observe efficiency improvements ranging from \(1.78 5.05\) times on five node classification datasets and speed enhancements of \(1.58 2.51\) times on graph classification datasets. These findings validate the effectiveness of our algorithm, offering a viable solution for efficient data compression.

**Obs 4. When employing the backbone explainers, the algorithm accelerates without noticeable performance decline.** As shown in Table 3, we find that on the eight datasets, the model consistently achieves acceleration without evident performance deterioration. Particularly, it gains performance improvements ranging from \(2.08\% 9.26\%\) on the Cora and Citeseer datasets. These

Figure 3. The training process of EXGC and GCond across Cora, Citeseer, Ogbn-Arxiv and Ogbn-Product four benchmarks. We can observe that EXGC achieves optimal performance ahead by 507, 1097, 832, and 366 epochs respectively, at which points training can be terminated.

  
**Dataset** & **Ratio** & **GCond** & **EXGC** & **DosGCond** & **EXDos** \\  Cora & 0.4\% & 29.78\% (22.72\%) & 6.68\% (12.15\%) & 3.22\% (0.45\%) & 1.13\% (0.14\%) \\ Citeseer & 0.3\% & 30.12\% (38.98\%) & 2.67\% (14.04\%) & 2.83\% (27.70\%) & 0.56\% (0.04\%) \\ Ogbn-arxiv & 0.05\% & 184.90\% (29.37\%) & 96.31\% (37.22\%) & 20.49\% (0.28\%) & 5.60\% (0.03\%) \\ Flicker & 0.1\% & 8.77\% (48.81\%) & 4.54\% (42.27\%) & 1.16\% (0.48\%) & 0.56\% (0.03\%) \\ Reddit & 0.1\% & 53.04\% (98.56\%) & 19.83\% (98.04\%) & 5.75\% (0.47\%) & 1.78\% (0.11\%) \\ DD & 0.2\% & & & & 1.48\% (72.40\%) & 0.59\% (0.23\%) \\ CIFAR10 & 0.1\% & – & – & 3.57\% (0.41\%) & 1.85\% (0.38\%) \\ Ogbg\(\)moliv & 0.01\% & – & – & 0.49\% (72.2\%) & 0.31\% (73.44\%) \\   

Table 3. Comparing the time consumption and performance across different backbones. All results in seconds should be multiplied by 100. We activate 5% of the nodes every 50 epochs and stop training if the loss does not decrease for 4 consecutive epochs, subsequently reporting the results should be multiplied by 100).

findings demonstrate that while reducing training and inference time, our approach does not lead to performance degradation and can even enhance the performance of the condensed graph.

### Selection guidelines of EXGC (RQ3)

In this section, we choose a backbone explainer for nodes in \(\) based on gradient magnitude (SA), random selection, and explainable algorithms (GNNExplainer and GSAT). Our target is to determine whether different backbone explainers influence the performance of the EXGC. We employ a 2-layer GCN and introduce an early stopping mechanism, halting the network training if the loss fails to decrease over 4 consecutive epochs. Subsequently, we monitor and record the model's performance. By leveraging various backbone explainers, every 50 epochs, we select an additional 5% of the elements in \(^{}\). Here we can make the observations:

**Obs 5.** As shown in Table 4, EXDO denotes our backbone explainers transitioned into the overall framework of DosGcond. We discovered that the convergence time is similar for both random and gradient-based explainers. However, explainable algorithms necessitate considerable time due to the training required for the corresponding explainer. Intriguingly, despite the GNNExplainer-based selection method incurring the most substantial time cost, it still remains lower than the conventional DosGcond algorithm (see Table 3). Going beyond our explanation strategies, we need to further observe the performance of models under different algorithms to better assist users in making informed trade-offs.

**Obs 6.** After examining the efficiency, as illustrated in Figure 4, we observed that while balancing efficiency, GSAT can achieve the best results. In contrast, GNNExplainer has the lowest efficiency. Interestingly, while SA and random have similar efficiencies, SA manages to yield superior results in comparison.

### Transferability of EXGC (RQ4)

Finally, we illustrate the transferability of condensed graphs from the different architectures. Concretely, we show test performance across different GNN backbones using a 2-layer GCN as the training setting. We employ popular backbones, APPNP, SGC and GraphSAGE, as test architectures. Table 5 exhibits that:

**Obs 7.** Across five datasets, our algorithm consistently outperforms GCond and demonstrates relatively lower variance, validating the effectiveness of our approach. Notably, on the Cora dataset, our model achieves a performance boost of nearly 6.0%\(\)7.0%. On the Citesser, we can observe that our framework achieves a performance improvement of approximately 5% to 6% over GCond. These results all underscore the transferability of our algorithm.

## 5. Limitation & Conclusion

**Limitation.** Our EXGC mitigates redundancy in the synthetic graph during training process without benefiting inference speed in downstream tasks. Moving forward, we aim to refine our algorithm to directly prune redundant nodes from the initialization of the synthetic graph, enabling simultaneous acceleration of both training and application phases. Additionally, we hope to adopt more advanced explainers in the future to better probe the performance boundaries.

**Conclusion.** In this work, we pinpoint two major reasons for the inefficiency of current graph condensation methods, _i.e._, the concurrent updating of a vast parameter set and the pronounced parameter redundancy. To address these limitations, we first employ the MeanField variational approximation for convergence acceleration and then incorporate the leading explanation techniques (_e.g._, GNNExplainer and GSAT) to select the important nodes in the training process. Based on these, we propose our EXGC, the efficient and explainable graph condensation method, which can markedly boost efficiency and inject explainability.

    &  &  \\   & **Cora** & **Citeseer** & **Ogbn-Araiv** & **DD** & **CIFAR10** & **Ogbg-molhiv** \\  Random & 7.24s & 2.84s & 7.32s & 0.87s & 2.07s & 0.32s \\ SA & 6.89s & 2.67s & 6.31s & 0.59s & 1.85s & 0.31s \\ GSAT & 10.37s & 4.50s & 9.45s & 0.76s & 2.61s & 0.34s \\ GNNExplainer & 11.62s & 5.97s & 11.23s & 0.99s & 2.80s & 0.42s \\   

Table 4. Time comusing of different backbone explainers. We set the compress ratio of Cora, Citesser and Ogbn-Araiv as 0.4%, 0.3%, 0.05%, respectively. As for graph classification, we set DD, CIFAR10 and Ogbg-molhiv as 0.2%, 0.1% and 0.01%. All displayed results should be multiplied by 100.

Figure 4. Performance comparison across six benchmarks under various explanation methods.

    &  &  \\   & **APPNP** & **SGC** & **SAGE** & **APPNP** & **SGC** & **SAGE** \\  Cora & 69.32s4.26 & 67.95s46.10 & 60.34s4.83 & 75.17s3.74 & 0.42s4.88 & 66.49s4.25 \\ Citeseer & 61.27s5.80 & 62.43s4.52 & 61.74s5.01 & 67.34s3.83 & 68.58s4.42 & 66.62s4.17 \\ Ogbn-Araiv & 58.50\({}_{1.66}\) & 59.11\({}_{1.13}\) & 59.04\({}_{1.13}\) & 59.37s.089 & 60.07s1.82 & 58.72s0.99 \\ Flicker & 45.94s2.37 & 45.82s3.73 & 43.62s4.65 & 44.06s1.72 & 46.51s2.18 & 45.10e.23 \\ Reddit & 85.42s1.76 & 87.33s2.97 & 84.80\({}_{1.34}\) & 87.46s2.73 & 36.10\({}_{1.15}\) & 87.59\({}_{2.92}\) \\   

Table 5. Transferability of condensed graphs from the different architectures. Test performance across three popular GNN backbones, _i.e._, APPNP, SGC and GraphSAGE using 2-layer GCN as training setting is exhibited.