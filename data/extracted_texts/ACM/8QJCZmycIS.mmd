# Policy-Guided Causal State Representation for Offline Reinforcement Learning Recommendation

Anonymous Author(s)

###### Abstract.

In offline reinforcement learning-based recommender systems (RLRS), learning effective state representations is crucial for capturing user preferences that directly impact long-term rewards. However, raw state representations often contain high-dimensional, noisy information and components that are not causally relevant to the reward. Additionally, missing transitions in offline data make it challenging to accurately identify features that are most relevant to user satisfaction. To address these challenges, we propose Policy-Guided Causal Representation (PGCR), a novel two-stage framework for causal feature selection and state representation learning in offline RLRS. In the first stage, we learn a causal feature selection policy that generates modified states by isolating and retaining only the causally relevant components (CRCs) while altering irrelevant components. This policy is guided by a reward function based on the Wasserstein distance, which measures the causal effect of state components on the reward and encourages the preservation of CRCs that directly influence user interests. In the second stage, we train an encoder to learn compact state representations by minimizing the mean squared error (MSE) loss between the latent representations of the original and modified states, ensuring that the representations focus on CRCs and filter out irrelevant variations. We provide a theoretical analysis proving the identifiability of causal effects from interventions, validating the ability of PGCR to isolate critical state components for decision-making. Extensive experiments demonstrate that PGCR significantly improves recommendation performance, confirming its effectiveness for offline RL-based recommender systems.

Offline Reinforcement Learning, Recommendation, Causal State Representation +
Footnote †: ccs: Information and Control

+
Footnote †: ccs: Information and Control

+
Footnote †: ccs: Information and Control

+
Footnote †: ccs: Information and Control

+
Footnote †: ccs: Information and Control

+
Footnote †: ccs: Information and Control

## 1. Introduction

Reinforcement Learning (RL) has emerged as a powerful approach for developing recommender systems (RS), where the objective is to sequentially learn a policy that maximizes long-term rewards, typically measured by user satisfaction or engagement. Unlike traditional recommendation methods that primarily aim to optimize immediate rewards, RLRS focuses on learning a recommendation strategy that adapts to user preferences over time (Brockman et al., 2016). This allows RLRS to dynamically update recommendations based on user feedback, aiming to improve long-term outcomes and enhance user experiences.

However, deploying RL in recommender systems poses significant challenges. Traditional RLRS rely on continuous user interaction to learn and adapt their policies, which may be impractical in many real-world applications due to concerns such as exploration risks, privacy issues, and computational costs (Brockman et al., 2016). To address these challenges, offline RL-based recommender systems have been proposed, where the goal is to learn optimal recommendation policies from a fixed dataset of historical user interactions without further online data collection. This offline setting leverages existing data to refine and optimize recommendations, but it also introduces some challenges.

A critical aspect in offline RLRS is learning efficient state representations (Brockman et al., 2016; Bockman et al., 2016). In the offline setting, the agent must learn solely from historical data without additional interactions, making the challenges of high-dimensional and noisy state representations more pronounced. The state space, which includes information about user interactions, context, and preferences, is fundamental for deciding actions (i.e., recommendations). However, raw state representations are often complex and may contain components that are not causally relevant to the reward.

Recent advances in representation learning in RL have focused on extracting abstract features from high-dimensional data to enhance the efficiency and performance of RL algorithms (Kirkpatrick et al., 2017; Sutton and Barto, 2018). However, these challenges are compounded in the context of offline RLRS due to the static nature of the data and the inability to interact with the environment. Techniques such as those developed by Zhang et al. (2018), which use the bisimulation metric to learn representations that ignore task-irrelevant information, may encounter challenges when applied directly to offline settings. In particular, missing transitions in the offline dataset can particularly impair the effectiveness of the bisimulation principle, resulting in inaccurate state representation and poor estimation (Zhu et al., 2018). Moreover, the complexity and high-dimensional nature of user data in offline RLRS require isolating the components that are causally relevant to the reward, rather than merely compressing the state space. Thus, there is a need for targeted techniques that emphasize causally critical state components within the constraints of offline learning.

To address these challenges, we propose a policy-guided approach for causal feature selection and state representation learning. Our approach is designed to use a policy to generate intervened states that isolate and retain only the causally relevant components (CRCs). By focusing on the features that directly impact user satisfaction, this method enables the state representation to concentrate on the most informative components, reducing noise and irrelevant variations. Additionally, by creating targeted interventions, this approach augments offline datasets, enhancing the learning of state representations even with finite datasets.

We introduce a method called PGCR (Policy-Guided Causal Representation), which operates in two stages. In the first stage, we learn a causal feature selection policy that generates modified states, retaining the CRCs and modifying the causally irrelevant components (CIRCs). We quantify the causal effect of the state components on the reward, which reflects user feedback, by using the Wasserstein distance between the original and modified reward distributions. This metric effectively measures the distributionalchange caused by the interventions, and we use it to design a reward function that encourages the retention of CRCs while altering CIRCs. Furthermore, we provide theoretical analysis on the identifiability of the causal effects resulting from these interventions. In the second stage, we leverage the learned causal feature selection policy to guide the training of a state representation encoder. Given a pair consisting of an original state and its modified counterpart generated by the causal feature selection policy, the encoder is trained to produce latent representations that preserve only the CRCs. Specifically, we minimize the mean squared error (MSE) loss between the latent representations of the original and modified states, encouraging the encoder to ignore irrelevant variations and focus on causally meaningful features. This process allows the encoder to map states into a latent space where only the information necessary for optimal decision-making is preserved.

Our contributions are as follows:

* We propose PGCR, a novel two-stage framework for offline RL-based recommender systems. In the first stage, we learn a causal feature selection policy to generate modified states that retain causally relevant components. In the second stage, we train an encoder to learn state representations concentrating on these components.
* We design a reward function based on the Wasserstein distance to guide the causal feature selection policy in identifying and retaining the state components that directly influence user interests.
* We provide a theoretical analysis proving the identifiability of causal effects from interventions, ensuring that our method isolates the components of the state critical for decision-making.
* Extensive experiments demonstrate the effectiveness of PGCR in improving recommendation performance in offline RL-based recommender systems.

## 2. Preliminaries

### Offline RL-Based Recommender Systems

Offline Reinforcement Learning (RL) in Recommender Systems (RS) aims to optimize decision-making by learning solely from historical user interaction data within the framework of a Markov Decision Process (MDP). The MDP is represented by the tuple \(,,,,\), where:

* \(\) represents the state space, encompassing user data, historical interactions, item characteristics, and contextual factors.
* \(\) denotes the action space, which includes all candidate items available for recommendation.
* \(:\) defines the reward function, based on user feedback such as clicks, ratings, or engagement metrics.
* \(\) describes the transition probabilities, governing the dynamics of state transitions.
* \(\) is the discount factor, used to balance immediate and future rewards.

Unlike online RL, the agent does not interact with the environment in real-time but must infer the optimal policy solely from historical data. In this MDP setup, the agent (RS) learns from a fixed dataset \(\) of interactions collected by a behavior policy. Each entry in this dataset consists of a state \(s_{t}\), an action \(a_{t}\) taken by the behavior policy, the resulting reward \(r_{t}\), and the next state \(s_{t+1}\). The primary goal of the RS agent is to learn a policy \(:\) that maximizes the cumulative discounted return, thereby ensuring the long-term effectiveness of the recommendations provided to the user.

### Causal Models

Causal models provide a structured way to represent and analyze the causal relationships among a set of variables. Consider a finite set of random variables denoted by \(=\{X_{1},X_{2},,X_{n}\}\), each associated with an index in \(=\{1,2,,n\}\). These variables have a joint distribution \(P_{}\) and a joint density function \(p()\). A causal graphical model is represented by a Directed Acyclic Graph (DAG) \(=(,)\), where \(\) is the set of nodes, each corresponding to one of the variables in \(\) and \(\) is the set of directed edges between the nodes, indicating direct causal influences.

**Definition 2.1** (Structural Causal Model).: A Structural Causal Model (SCM) \(M=(,P_{})\) associated with a DAG \(\) consists of a set \(\) of structural equations:

\[X_{i}=f_{i}(_{i},U_{i}), i=1,2,,n,\]

where \(_{i}\{X_{i}\}\) denotes the set of parent variables (direct causes) of \(X_{i}\) in the graph \(\). \(U_{i}\) represents the exogenous (noise) variables, accounting for unobserved factors, and \(=\{U_{1},,U_{n}\}\) is the set of all such variables. A joint distribution \(P_{}\) over the noise variables \(\), assumed to be jointly independent.

Each structural function \(f_{i}\) specifies how \(X_{i}\) is generated from its parents \(_{i}\) and the noise term \(U_{i}\). The combination of the structural equations \(\) and the distribution \(P_{}\) induces a joint distribution \(P_{}\) over the endogenous variables \(\).

**Definition 2.2** (Intervention).: An intervention in an SCM \(M\) is an operation that modifies one or more of the structural equations in \(\). Specifically, suppose we replace the structural equation for variable \(X_{j}\) with a new equation:

\[X_{j}=f_{j}(}_{j},U_{j}).\]

This results in a new SCM \(}\), reflecting the intervention on \(X_{j}\). The corresponding distribution changes from the observational distribution \(P_{}^{M}\) to the interventional distribution \(P_{}^{M}\), expressed as:

\[P_{}^{M}=P_{}^{M do(X_{j}=f_{j}(}_{j},U_{j}))},\]

where the \(do\)-operator \(do(X_{j}=_{j}(}_{j},U_{j}))\) denotes the intervention that replaces the structural equation for \(X_{j}\).

## 3. Methodology

### Problem Formulation

To learn a policy that identifies the causally relevant components in the state, we first represent the MDP from a causal modeling perspective. Assuming there are no unobserved confounders, the SCMs for the MDP can be formulated using deterministic equations augmented with exogenous noise variables to capture stochasticity, as shown in Figure 1 (a):

[MISSING_PAGE_FAIL:3]

After the causal feature selection policy intervenes on the action \(a_{t}\), setting it to a specific value \(a_{t}^{T}\), the environment transitions to a new state \(s^{T}\). This intervened state \(s^{T}\) is expected to preserve only the causally relevant components of the original state \(s_{t}\), while any causally irrelevant CIRCs are modified or filtered out.

Since the CRCs are the parts of \(s_{t}\) that have a significant causal impact on rewards, we regard the new state \(s^{T}\), induced by the intervention on \(a_{t}\), as an effective intervention on \(s_{t}\) in the original tuple \(\{s_{t},a_{t},s_{t+1},r_{t}\}\). By comparing the rewards obtained before and after the intervention, we can evaluate the causal effect of the original state \(s_{t}\) on the reward \(r_{t}\), isolating the impact of the causally relevant components.

Formally, following Pearl's rules of \(do\)-calculus (Pearl, 2009), as outlined in Appendix A, the causal effect of \(s_{t}\) on \(r_{t}\) is given by the formula:

\[p^{_{t},do(s_{t}:=s^{T})}(r_{t})\] \[=_{a_{t}}_{_{t}}P(r_{t} do(s_{t}:=s^{T}),a_{t}, _{t})\,P(_{t})\,P(a_{t} do(s_{t}:=s^{T}))\,d_{t}\] \[=_{a_{t}}_{_{t}}P(r_{t} s_{t}:=s^{T},a_{t},_{ t})\,P(_{t})\,P(a_{t} s_{t}=s^{T})\,d_{t}\] \[=_{a_{t},_{t}}[P(r_{t} s_{t}:=s^{T},a_{t}, _{t})]. \]

If the intervened probability distribution of the reward is similar to the original distribution, substituting \(s_{t}\) with \(s^{T}\) has a minor causal effect on the reward. This indicates that the causally CRCs of \(s_{t}\) that significantly influence learning the user's interest have been retained. To quantify this effect, we measure the distance between the two probability distributions of the reward before and after the intervention. Inspired by bisimulation for state abstraction (Bhattacharya et al., 2017), we adopt the first-order Wasserstein distance to measure how the intervened reward probability distribution \(p^{_{t},do(s_{t}:=s^{T})}(r_{t})\) differs from the original distribution \(p^{_{t}}(r_{t})\):

\[W_{1}(p^{_{t},do(s_{t}:=s^{T})}(r_{t}),\,p^{ }(r_{t}))=_{Y\{Y^{T},p^{}\}}_{}|r-r^{}|\,dy(r,r^{}), \]

where \((p^{T},p^{})\) is the set of all joint distributions \((r,r^{})\) with marginals \(p^{_{t},do(s_{t}:=s^{T})}(r_{t})\) and \(p^{}(r_{t})\). A small Wasserstein distance indicates that the intervention on the state \(s_{t}\) has a negligible effect on the reward distribution, suggesting that the components altered by the intervention are causally irrelevant to the reward. Conversely, a large Wasserstein distance implies that the intervention significantly changes the reward distribution, highlighting the causal relevance of the components modified in the state.

By evaluating the Wasserstein distance between the original and intervened reward distributions, we can quantify the causal effect of the state components on the reward. This measurement not only guides the causal feature selection policy in identifying and retaining the causally relevant components in the state but also serves as a crucial guide for the agent's learning process. To operationalize this measurement within the agent's learning, we introduce an effective reward function defined as:

\[r_{t}=(-\,W_{1}(p^{_{t},do(s_{t}:=s^{T})}(r_{ t}),\,p^{}(r_{t}))), \]

where \((0,1]\) is a scaling parameter that controls the sensitivity of the reward to changes in the Wasserstein distance.

By maximizing this reward, the agent is incentivized to select actions that minimize the Wasserstein distance between the intervened and original reward distributions. This encourages the agent to choose actions that retain the causally relevant components of the state, effectively filtering out causally irrelevant features. Consequently, the agent adjusts its policy to focus on the essential causal elements.

### Policy-Guided State Representation

Having identified the CRCs of the state through our causal feature selection policy, we proceed to learn a state representation that effectively captures these essential components. The objective is to encode the current state \(s_{t}\) and its intervened counterpart \(s_{t}^{T}\) into a latent space where only the CRCs are preserved, and the CIRCs are minimized or disregarded. To achieve this, we employ an encoder trained using mean squared error (MSE) loss, which focuses on aligning the representations of \(s_{t}\) and \(s_{t}^{T}\) by minimizing the differences in their latent representations.

By using the causal feature selection policy to generate modified states \(s_{t}^{T}\), which share the same CRCs but differ in CIRCs compared to the original state \(s_{t}\), we provide the encoder with pairs of states that should be mapped to similar latent representations. The MSE loss between the latent representations of \(s_{t}\) and \(s_{t}^{T}\) encourages the encoder to focus on the CRCs and ignore the CIRCs. Moreover, generating modified states \(s_{t}^{T}\) through interventions allows us to augment the dataset, addressing the issue of missing transitions commonly encountered in offline recommender systems.

Practically, we design an encoder network \(\) that processes the input states and outputs their latent representations:

\[z_{t}=(s_{t}), z_{t}^{T}=(s_{t}^{T}). \]

We train the encoder by minimizing the mean squared error (MSE) loss between the latent representations of \(s_{t}\) and \(s_{t}^{T}\):

\[J=\|(s_{t})-(s_{t}^{T})\|_{2}^{2}. \]

This loss function encourages the encoder to focus on the CRCs by reducing the differences in the latent representations of \(s_{t}\) and \(s_{t}^{T}\), which differ only in their CIRCs.

**Proposition 2** (Optimal Policy Based on Latent State Representation).: Let \(s_{t}\) be the full state at time \(t\), and let \(G=_{k=0}^{}^{k}r_{t+k}\) be the expected discounted return. Let \(:\) be an encoder that maps \(s_{t}\) to a latent state representation \(z_{t}=(s_{t})\), capturing the causally relevant components. Suppose for \(z_{t}\), we have:

* \(r_{t} s_{t} z_{t},a_{t}\).
* For all \(s_{t-1},s_{t-1}^{o}\) with \((s_{t-1})=(s_{t-1}^{o})\), \(p((s_{t}) s_{t-1}^{o})=p((s_{t}) s_{t-1}^{o})\).

Then the optimal policy \(_{}\) depends only on the latent state representation \(z_{t}\), and not on the full state \(s_{t}\). That is, there exists

\[_{}_{}[G],\]such that

\[_{}(a_{t} s_{t-1})=_{}(a_{t} s_{t-1}^{o})  s_{t-1},s_{t-1}^{o}:(s_{t-1})=(s_{t-1}^{o}).\]

The proof of Proposition 2 is given in Appendix C. This proposition shows that using the encoder \((s_{t})\) as a means of simplifying the state is theoretically justified. The encoder learns to isolate the CRCs from the full state, ensuring that the resulting latent representation \(z_{t}\) contains all information needed for decision-making. This supports the approach of training an encoder to map states into a latent space that focuses on the essential causal features.

```
Input: Initial parameters \(_{t^{}}\), \(_{^{c}}\); replay buffer \(D_{}\); reward buffers \(R\), \(\)forepisode = 1 to \(E\)dofor\(t=1\)to\(T\)do  Expert observes state \(s_{t}\), executes action \(a_{t}\), and stores reward \(r_{t}\) in \(R\);  Causal agent intervenes with action \(a_{t}^{T}\) and obtains modified state \(s_{t}^{T}\);  Expert observes \(s_{t}^{T}\), executes action \(a_{t}\), and stores reward \(_{t}\) in \(\).  Calculate reward \(r\) based on the reward function ; // See Eq. (5)  Store transition \((s_{t},a_{t}^{T},s_{t}^{T},r)\) in replay buffer \(D_{}\);  Sample minibatch from \(D_{}\) and update parameters \(_{t^{}}\), \(_{^{c}}\);

#### 3.3.1. Learning of Causal Feature Selection Policy

The causal feature selection policy is trained by leveraging the reward function in Equation (5). The objective is to design interventions that retain the CRCs while minimizing changes to the reward distribution, thereby preserving the essential components influencing user satisfaction. The algorithm for learning the causal feature selection policy is provided in Algorithm 1.

A one-step illustration of the training process is depicted in Figure 1 (c). The causal feature selection policy is trained with the assistance of a pre-trained expert policy, which uses external knowledge to obtain both the observational and intervened reward distributions. The expert policy can be learned using any RL-based algorithm, and the causal feature selection policy can follow a similar approach.

During training, the expert policy interacts with the environment to collect tuples of the form \((s_{t},a_{t},r_{t},s_{t+1})\), where \(r_{t}\) contributes to the observational reward distribution. Simultaneously, the causal feature selection policy observes the state \(s_{t}\) and intervenes on the action to generate a modified state \(s_{t}^{T}\). This modified state \(s_{t}^{T}\) is treated as an intervention on the original tuple's state. The expert policy then observes \(s_{t}^{T}\) and executes the original action \(a_{t}\), thereby obtaining an intervened reward, which is used to construct the intervened reward distribution.

By maximizing the reward in Equation (5), the causal feature selection policy is incentivized to produce modified states \(s_{t}^{T}\) that yield reward distributions similar to the original. This similarity indicates that the CRCs are effectively retained while the CIRCs are altered, ensuring that the modified states preserve the key causal components.

#### 3.3.2. Integrated Learning Process

In the offline RL setting, we integrate the causal feature selection policy with the training of both the state representation encoder and the recommendation policy, as depicted in Figure 1 (d). Given a current state \(s_{t}\) from the offline dataset, the causal feature selection policy generates a modified state \(s_{t}^{T}\) that retains only the CRCs. The state pair \((s_{t},s_{t}^{T})\) is then used to train the encoder network \(\), which processes the input states and outputs their latent representations.

The encoder is trained by minimizing the loss defined in Equation (6), which encourages it to focus on the CRCs by reducing the differences in the latent representations of the state pairs, which differ only in their CIRCs. Consequently, the encoder learns to map states into a latent space where only the causally relevant features are preserved, effectively filtering out irrelevant variations.

The recommendation policy \(_{}\) is subsequently trained using the latent representations \(z_{t}\) as inputs. Because the encoder prioritizes the CRCs, the recommendation policy is equipped to make decisions based on the most pertinent information influencing user satisfaction. The full algorithm for the integrated learning process is presented in Algorithm 2.
``` Input: Offline dataset \(\); causal policy \(_{}\); encoder \(\) with parameters initial \(\); recommendation policy \(_{}\) with initial parameters \(\); learning rate \(\)foreachtraining epoch \(\) from \(\)do // Generate Modified State Using Causal Feature Selection foreach\((s_{t},a_{t},r_{t},s_{t+1})\)do  Generate modified state \(s_{t}^{T}=_{}(s_{t})\); // Train Encoder Using MSE Loss Encode states: \(z_{t}=(s_{t})\), \(z_{t}^{T}=(s_{t}^{T})\);  Compute MSE loss: \(_{}=\|z_{t}-z_{t}^{T}\|_{2}^{2}\);  Update encoder parameters: \(=-_{}_{}\); // Train Recommendation Policy Using Latent Representations Update policy parameters \(\) with offline RL algorithm; ```

**Algorithm 2**Integrated Learning Process

## 4. Experiments

In this section, we begin by performing experiments on an online simulator and recommendation datasets to highlight the remarkable performance of our methods. We then conduct an ablation study to demonstrate the effectiveness of the causal-indispensable state representation.

### Experimental Setup

We introduce the experimental settings with regard to environments and state-of-the-art RL methods.

#### 4.1.1. Recommendation Environments

For offline evaluation, we use the following benchmark datasets:

* **MovieLens-1M1**: These datasets, derived from the Movielens website, feature user ratings of movies. The ratings are on a 5-star scale, with each user providing at least 20 ratings. Movies and users are characterized by 23 and 5 features, respectively.

* **Coat**(Zhou et al., 2018); is a widely used dataset that is proposed for product recommendation.
* **YahooR3**: a music recommendation dataset that proposed by (Zhou et al., 2018).
* **KuaiRec**: a video recommendation dataset that proposed by (Li et al., 2018) which is fully-observable.
* **KuaiRand**: a video recommendation dataset similar to KuaiRec but with a randomly exposed mechanism (Li et al., 2018).

When converting them into the RL environments, we use the GRU as the state encoder for those offline datasets. In addition to those offline datasets, we also conduct experiments on an online simulation platform - VirtualTB (Zhou et al., 2018).

#### 4.1.2. Baseline

Since limited work focuses on causal state representation learning for offline RLRS, we selected the traditional RL algorithm as the baseline. In concurrent work, CIDS (Vaswani et al., 2017) proposes using conditional mutual information to isolate crucial state variables. The key difference between our work and CIDs is that CIDs is tailored for online RLRS, focusing primarily on the causal relationship between action and state. In contrast, our work addresses offline RLRS, incorporating the reward into the framework to train a policy that guides the learning of state representations. In our experiments, we employ the following algorithms as the baseline:

* **Deep Deterministic Policy Gradient (DDPG) (Krishna et al., 2017)**: An off-policy method suitable for environments with continuous action spaces, employing a target policy network for action computation.
* **Soft Actor-Critic (SAC) (Goodfellow et al., 2014)**: An off-policy maximum entropy Deep RL approach, optimizing a stochastic policy with clipped double-Q method and entropy regularization.
* **Twin Delayed DDPG (TD3) (Dwork et al., 2016)**: An enhancement over DDPG, incorporating dual Q-functions, less frequent policy updates, and noise addition to target actions.

To evaluate the performance of the proposed PGCR, we have plugged the PGCR into those mentioned baselines to evaluate the performance.

#### 4.1.3. Evaluation Measures

Following the previous work (Zhou et al., 2018), we will use the cumulative reward, average reward and interaction length as the main evaluation metric for those mentioned offline datasets. For VirtualTB, we use the embedded CTR as the main evaluation metric.

### Implementation Details

In our experiments, we first need to train the causal agent to conduct the intervention and thus generate the intervened state. The offline demonstration is required to train the causal agent. We use a DDPG algorithm to conduct the process to obtain the offline demonstrations for various datasets. The algorithm is trained for 100,000 timesteps, and we save the policy with the best performance during the evaluation stage. The saved policy will be used to generate the offline demonstrations. For the training of our proposed method, we set the learning rate to \(10^{-4}\) for the actor-network and \(10^{-3}\) for the critic network. The discount factor \(\) is set to 0.95, and we use a soft target update rate \(\) of 0.001. The hidden size of the network is set to 128, and the replay buffer size is set to \(10^{6}\).

For those baselines, we are using the standard hyper-parameters settings from the Tianshou2.

### Overall Results

The results in Table 1 show that PGCR, a causal state representation learning method, significantly enhances state representation in reinforcement learning algorithms. Across different datasets, the PGCR-enhanced versions of standard algorithms (DDPG, SAC, TD3) demonstrate consistent improvements in cumulative and average rewards. This suggests that PGCR effectively strengthens the algorithms' ability to learn better state representations, leading to more informed decision-making and improved policy performance.

Moreover, the enhanced state representation provided by PGCR does not adversely affect the interaction length, which remains stable or slightly increases, indicating efficient learning processes. Additionally, the relatively low variance in the results for PGCR-enhanced methods further emphasizes their stability and reliability across different environments. These findings highlight the effectiveness of PGCR in boosting the overall learning and performance of reinforcement learning models by focusing on improved causal state representations.

### Ablation Study

In this section, we aim to investigate the impact of the proposed causal agent on the final performance. To do this, we replaced the

Figure 2. The 1-step CTR performance in the VirtualTaobao simulation is presented as the mean with error bars.

causal agent with a randomly sampled state. We denote the model without the causal agent as "-C."

Table 2 presents a comparison between the performance of PGCR, the proposed causal state representation learning method, and its variant, PGCR-C, which excludes the causal agent. Across all datasets and reinforcement learning algorithms (DDPG, SAC, TD3), PGCR consistently outperforms PGCR-C in terms of cumulative and average rewards. This highlights the importance and effectiveness of incorporating the causal agent within the PGCR framework, suggesting that the causal state representation significantly enhances the learning process, leading to better policy decisions and improved overall performance.

Regarding interaction length, the differences between PGCR and PGCR-C are generally minor, indicating that the causal agent does not significantly change the duration of interactions but rather improves the quality of decisions during those interactions. The consistent improvements in both cumulative and average rewards across various settings demonstrate that the causal aspect of PGCR is crucial for achieving optimal performance in reinforcement learning tasks. These results underscore the value of the causal state representation in capturing the underlying structure of the environment, enhancing the algorithm's ability to learn and adapt effectively.

### Hyper-parameter Study

In this section, we investigate how the reward balance parameter \(\) in Equation (5) influences the final performance. To account for computational costs, this study is conducted using an online simulation platform, with the results presented in Figure 4. We observe that all three models--PGCR-DDPG, PGCR-SAC, and PGCR-TD3--are highly sensitive to the value of \(\). Each model achieves peak performance in terms of CTR around a \(\) range of 0.1 to 0.2, suggesting that this range is optimal for maximizing the CTR across the models. However, as \(\) increases beyond 0.2, there is a noticeable decline in performance for all models, with PGCR-DDPG experiencing the most significant drop.

  & & MovieLens-1M & & & Coat & & 758 \\  & Cumulative Reward & Average Reward & Interaction Length & Cumulative Reward & Average Reward & Interaction Length & 768 \\  DDPG & \(9.3706 4.49\) & \(3.0329 1.44\) & \(3.11 0.02\) & \(16.3348 7.23\) & \(2.3277 1.03\) & \(7.02 0.03\) & 768 \\ PGCR-DDPG & \(\) & \(\) & \(\) & \(\) & \(2.7675 0.57\) & \(7.02 0.05\) & 768 \\ SAC & \(10.2424 3.66\) & \(2.8852 1.03\) & \(3.55 0.03\) & \(17.5432 7.22\) & \(2.4231 1.00\) & \(7.24 0.02\) & 762 \\ PGCR-SAC & \(\) & \(4.4544 1.25\) & \(\) & \(20.4272 4.70\) & \(2.7164 0.63\) & \(7.52 0.10\) & 763 \\ TD3 & \(10.1620 4.90\) & \(2.9410 1.42\) & \(3.45 0.02\) & \(16.3232 7.02\) & \(2.3542 1.01\) & \(6.93 0.03\) & 764 \\ PGCR-TD3 & \(\) & \(3.4375 1.27\) & \(\) & \(\) & \(\) & \(7.55 0.11\) & 765 \\    &  &  &  \\  & Cumulative Reward & Average Reward & Interaction Length & Cumulative Reward & Average Reward & Interaction Length & 767 \\  DDPG & \(9.2155 4.05\) & \(1.0192 0.45\) & \(9.04 0.04\) & \(1.4232 0.51\) & \(0.3287 0.12\) & \(4.33 0.03\) & 768 \\ PGCR-DDPG & \(\) & \(1.5948 0.55\) & \(8.92 0.04\) & \(2.0334 0.65\) & \(0.3657 0.10\) & \(5.56 0.03\) & 769 \\ SAC & \(10.5235 3.92\) & \(1.1693 0.44\) & \(9.00 0.10\) & \(1.8272 0.55\) & \(0.3500 0.11\) & \(5.22 0.04\) & 770 \\ PGCR-SAC & \(\) & \(\) & \(\) & \(\) & \(\) & \(5.39 0.04\) & 771 \\ TD3 & \(\) & \(0.8610 0.36\) & \(9.09 0.04\) & \(1.5083 0.40\) & \(0.3010 0.08\) & \(5.01 0.05\) & 772 \\ PGCR-TD3 & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & 773 \\    &  &  &  \\  & Cumulative Reward & Average Reward & Interaction Length & Cumulative Reward & Average Reward & Interaction Length & 767 \\  DDPG & \(9.2155 4.05\) & \(1.0192 0.45\) & \(9.04 0.04\) & \(1.4232 0.51\) & \(0.3287 0.12\) & \(4.33 0.03\) & 768 \\ PGCR-DDPG & \(\) & \(1.5948 0.55\) & \(8.92 0.04\) & \(2.0334 0.65\) & \(0.3657 0.10\) & \(5.56 0.03\) & 769 \\ SAC & \(10.5235 3.92\) & \(1.1693 0.44\) & \(9.00 0.10\) & \(1.8272 0.55\

## 5. Related Work

**RL-based Recommender Systems** model the recommendation process as a Markov Decision Process (MDP), leveraging deep learning to estimate value functions and handle the high dimensionality of MDPs (Hoffman, 2015). Chen et al. (2016) proposed InvRec, which uses inverse reinforcement learning to infer rewards directly from user behavior, enhancing policy learning accuracy. Recent efforts have focused on offline RLRS. Wang et al. (2018) introduced CDT4Rec, which incorporates a causal mechanism for reward estimation and uses transformer architectures to improve offline RL-based recommendations. Additionally, Chen et al. (2018) enhanced this line of research by developing a max-entropy exploration strategy to improve the decision transformer's ability to "stitch" together diverse sequences of user actions, addressing a key limitation in offline RLRS. Gao et al. (2018) developed a counterfactual exploration strategy designed to mitigate the Matthew effect, which refers to the disparity in learning from uneven distributions of user data.

**Causal Recommendation**. The recommendation domain has recently seen significant advancements through the integration of causal inference techniques, which help address biases in training data. For example, Zhang et al. (2019) tackled the prevalent issue of popularity bias by introducing a causal inference paradigm that adjusts recommendation scores through targeted interventions. Similarly, Li et al. (2019) proposed a unified multi-task learning approach to eliminate hidden confounding effects, incorporating a small number of unbiased ratings from a causal perspective. Counterfactual reasoning has also gained traction in recommender systems. Chen et al. (2018) developed a causal augmentation technique to enhance exploration in RL-based recommender systems (RLRS) by focusing on causally relevant aspects of user interactions. Wang et al. (2018) introduced a method to generate counterfactual user interactions based on a causal view of MDP for data augmentation. In a related vein, Li et al. (2019) explored personalized incentive policy learning through an individualized counterfactual perspective. Further studies have focused on the use of causal interventions. Wang et al. (2018) proposed CausalInt, a method inspired by causal interventions to address challenges in multi-scenario recommendation. Additionally, He et al. (2018) tackled the confounding feature issue in recommendation by leveraging causal intervention techniques. These efforts collectively demonstrate the growing importance of causal inference and intervention in improving recommendation performance and addressing biases.

## 6. Conclusion

In this work, we introduced Policy-Guided Causal Representation (PGCR), a framework designed to enhance state representation learning in offline RL-based recommender systems. By using a causal feature selection policy to isolate the causally relevant components (CRCs) and training an encoder to focus on these components, PGCR effectively improves recommendation performance while mitigating noise and irrelevant features in the state space. Extensive experiments demonstrate the benefits of our approach, confirming its effectiveness in offline RL settings.

For future work, we plan to explore the extension of PGCR to more complex, multi-agent environments where user preferences may dynamically change over time.

  &  &  &  \(_{373}\) \\ Interaction Length \\  }} \\  & Cumulative Reward & Average Reward & Interaction Length & Cumulative Reward & Average Reward & Interaction Length \\  PGCR-DDPG & 13.0722 ± 3.55 & 4.0587 ± 1.10 & 3.22 ± 0.03 & 19.4281 ± 4.01 & 2.7675 ± 0.57 & 7.02 ± 0.05 & 873 \\ PGCR-C-DDPG & 9.9271 ± 4.02 & 3.1022 ± 1.26 & 3.20 ± 0.03 & 17.0237 ± 6.55 & 2.3611 ± 0.91 & 7.21 ± 0.04 & 873 \\  PGGR-SAC & 13.4522 ± 3.77 & 4.4544 ± 1.25 & 3.02 ± 0.05 & 20.4272 ± 4.70 & 2.7164 ± 0.63 & 7.52 ± 0.10 & 873 \\ PGCR-C-SAC & 11.0238 ± 3.44 & 2.7491 ± 0.86 & 4.01 ± 0.05 & 18.1253 ± 7.02 & 2.5209 ± 0.98 & 7.19 ± 0.03 & 873 \\  PGGR-TD3 & 14.1281 ± 5.21 & 3.4375 ± 1.27 & 4.11 ± 0.02 & 19.1192 ± 3.81 & 2.5323 ± 0.50 & 7.55 ± 0.11 & 879 \\ PGCR-C-TD3 & 11.0261 ± 4.45 & 3.4349 ± 1.39 & 3.21 ± 0.03 & 17.0221 ± 6.42 & 2.3907 ± 0.91 & 7.12 ± 0.04 & 88 \\   \(_{373}\) \\ Interaction Length \\  } &  &  \(_{373}\) \\ Interaction Length \\  }} \\  & Cumulative Reward & Average Reward & Interaction Length & Cumulative Reward & Average Reward & Interaction Length \\  PGGR-DDPG & 14.2254 ± 4.87 & 1.5948 ± 0.55 & 8.92 ± 0.04 & 2.0334 ± 0.65 & 0.3657 ± 0.10 & 5.56 ± 0.03 & 833 \\ PGCR-C-DDPG & 10.4222 ± 4.19 & 1.1087 ± 0.45 & 9.40 ± 0.07 & 1.6410 ± 0.62 & 0.3447 ± 0.13 & 4.76 ± 0.04 & 884 \\  PGGR-SAC & 15.3726 ± 4.02 & 1.8588 ± 0.49 & 8.27 ± 0.04 & 2.4421 ± 0.23 & 0.4531 ± 0.05 & 5.39 ± 0.04 & 835 \\ PGGR-C-SAC & 11.2890 ± 4.11 & 1.2858 ± 0.47 & 8.78 ± 0.11 & 2.0316 ± 0.41 & 0.3999 ± 0.08 & 5.08 ± 0.05 & 886 \\  PGGR-TD3 & 14.0021 ± 4.90 & 1.5203 ± 0.53 & 9.21 ± 0.03 & 2.0001 ± 0.34 & 0.3992 ± 0.07 & 5.01 ± 0.02 & 887 \\ PGGR-C-TD3 & 8.1247 ± 3.01 & 0.8793 ± 0.33 & 9.24 ± 0.08 & 1.6218 ± 0.36 & 0.2998 ± 0.07 & 5.41 ± 0.03 & 888 \\  

Table 2. Ablation Study

Figure 4. Hyper Parameter Study in VirtualTB