# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

model, we propose the Framework of dfffusion approxi**M**ation (FIM) to estimate diffusion parameters by reconstructing the propagations from cascade data, thereby inferring the underlying network structures. Subsequently, to achieve the desired efficiency for influence estimation, we propose the sampling technique _shortest diffusion time of set_ (SDTS). We analyze the network inference error of FIM and quantify the effect of such errors on the influence estimation theoretically. Our experiments on synthetic and real-world datasets demonstrate that (i) the FIM approach significantly improves the state-of-the-art learning ability on network inference, and (ii) it significantly boosts the efficiency of influence estimation, scaling to realistic datasets. In particular, our method is the only one able to tackle real-world networks with \(12,677\) nodes with thousands of cascades, while offering superior performance.

In a nutshell, our contributions can be summarized as follows.

* We develop a model for continuous-time diffusion and design a scalable and effective framework (FIM) to address network inference and influence estimation.
* We systematically analyze the approximation errors of FIM for both network inference and influence estimation.
* We enhance the scalability of influence estimation, by a sampling technique to estimate the shortest diffusion time of sets.
* We run experiments on synthetic and real-world data, which confirms the superior scalability and effectiveness of FIM.

## 2. Preliminary

### Diffusion Networks

**Notations.** We use calligraphic fonts, bold uppercase letters, and bold lowercase letters to represent sets (e.g., \(\)), matrices (e.g., \(\)), and vectors (e.g., e) respectively. The \(i\)-th row (resp. column) of matrix \(\) is denoted by \([i,]\) (resp. \([,i]\)). For ease of exposition, node \(u\) can indicate the row (resp. column) associated with \(u\) in the matrix, e.g., \([u,]\) (resp. \([,u]\)). Frequently used notations are summarized in Table 1.

Let \(=(,)\) be a directed graph with nodes \(\) and edges \(\), and let \(n=||\) and \(m=||\). We assume that the diffusion process along edge \((u,v)\) from \(u\) to \(v\) is determined by a diffusion probability density function (PDF) \(p(_{uw},t)=_{uw}^{-_{uw}t}&t 0\\ 0&t<0\)

where \(_{uw}[0,)\) is the _diffusion rate_ determining the strength of node \(u\) influencing \(v\) and \(t\) is the diffusion delay. For example, Figure 1 illustrates the expanding tendency of a diffusion PDF with \(_{uw}=2\) with time. Accordingly, the cumulative distribution function (CDF) \(F(_{uw},t)=_{0}^{t}p(_{uw},t^{})t^{}=1- ^{-_{uw}t}\) (\(t 0\)) is the probability that node \(u\) succeeds in influencing \(v\) along edge \((u,v)\) by time \(t\). Let \(\) be the _adjacency parameter_ matrix of \(\) with \([u,v]=_{uw}\) if \((u,v)\) and \([u,v]=0\) otherwise, and let \(N_{u}\) be the direct _incoming_ neighbor set of \(u\) with _in-degree_\(d_{u}=|N_{u}|\).

### Continuous-time Diffusion Model

The independent cascade (IC) and linear threshold (LT) models (Zhou et al., 2017) are well-adopted discrete-time models for vanilla IM. Yet discrete-time models have a synchronization property by nature and have been shown empirically to have limitations for modeling diffusion processes in reality (Zhou et al., 2017; Wang et al., 2018; Wang et al., 2019; Wang et al., 2020). Therefore, we adopt here the _continuous-time independent cascade_ (CIC) model for diffusion between users (Zhou et al., 2017; Wang et al., 2018; Wang et al., 2020; Wang et al., 2020). Given a time window \(T>0\) and a source set \(\), the influence from \(\) under the CIC model stochastically spreads as follows.

At time \(t=0\), all nodes \(u\) are activated. When a node \(u\) is activated at time \(t_{u}\), it attempts to influence each of its outgoing and inactive neighbors \(v\) by following the diffusion function \(p(_{uw},t-u_{u})\). Each node \(v\) is activated by its incoming neighbors _independently_. Once activated, \(v\) remains active and tries to influence its outgoing neighbors. A diffusion process reaches a fixed point ultimately and ends at time \(T\). In what follows, To avoid clutter in notations, we do not specify the source set \(\) nor the time window \(T\) (if applicable) when these are clear from the context.

A diffusion process originating at the source (seed) set \(\) and ending by time \(T\), which underwent as just described, is denoted as a _cascade_\(:=\{t_{u}: u\}_{u_{0}^{2}}^{n}\) where \(t_{u}[0,T]\{\}\) is the activation time of node \(u\). In particular, \(t_{u}:=\) if node \(u\) is not activated within the time window \(T\). In applications, for previously recorded cascades, the source set \(\) may not be given explicitly but can be easily inferred from any cascade \(\) as \(=\{u: t_{u},u_{u}=0\}\); \(\) may be a singleton. For diffusions initialized by \(\), let \(\) be the set of all the known cascades from \(\) within the time window \(T\). Given a cascade \(\) and time stamp \(t[0,T]\), let \(_{t,u}_{u_{0}^{2}}^{n}\) denote the _observation_ of \(\) at time \(t\), i.e., \(_{t,u}=t_{u}\) for \(t_{u}\) if \(t_{u} t\); otherwise \(_{t,u}:=\). Therefore, \(_{t}\) is the snapshot of cascade \(\) that \(t\). In general, let the variable \(_{t}\) be the observation (random vector) of cascades randomly sampled from \(\). To quantify the probability of each node being activated by time \(t\), the states of nodes during diffusion at time \(t\) are recorded. Specifically, given an observation \(_{t}\), let \(_{t}\{0,1\}^{n}\) be the corresponding _state_ vector of nodes in \(\); i.e., \(_{t,o}=1\) if \(_{t,o} t\) and \(_{t,o}=0\) otherwise. Accordingly, let \(_{t}\) be the corresponding random state vector of \(_{t}\). Therefore, \(E[_{t}]=E_{t,o}-C[_{t}]\) indicates the probability vector of nodes in \(\) activated by time \(t\).

 
**Notation** & **Description** \\  \(=(,)\) & a network with node set \(\) and edge set \(\) \\  \(n,m\) & the number of nodes and the number edges in \(\) \\ \(_{u},d_{u}\) & the incoming neighbor set of node \(u\) and its indegree \\  \(\) & the adjacency parameter matrix of \(\) \\  \(,C\) & a cascade recording the activation times for nodes in \(\) and the set of all known cascades \\  \(_{t},_{t}\) & the specific observation of cascade \(\) and the observation of a random cascade from \(C\) at time \(t\) \\  \(_{t},_{t}\) & the specific node state in observation \(_{t}\) and the random node state in random observation \(_{t}\) \\  \(_{t},_{t}\) & the diffusion rates of nodes in cascade \(\) and the expected diffusion rates of nodes at time \(t\) \\  \(\) & the Hadamard product \\   

Table 1. Frequently used notationsFor example, Figure 2 illustrates a cascade within time \(T\), which is \(=\{t_{0},t_{1},,t_{7},t_{8}\}\), with \(0=t_{0}<t_{1}<t_{3}<t_{4}<t_{5}<T\) and \(t_{\{2,6,7,8\}}=\). By taking an observation at some time moment \(t^{*}(t_{4},t_{5})\), we have \(0=_{t^{*},0}<_{t^{*},1}<_{t^{*},3}<_{t^ {*},4}<t^{*}\) and \(_{t^{*},\{2,5,6,7,8\}}=\). Correspondingly, we have \(_{t^{*},\{0,1,3\}}=1\) and \(_{t^{*},\{2,5,6,7,8\}}=0\).

**Definition 1** (Network Inference).: _Let \(=(,)\) be a diffusion network with a known node set \(\), \([]=n\), an unknown edge set \(\), and an unknown associated adjacency parameter matrix \(^{n n}\). Given a set of cascades \(\), network inference seeks to estimate the adjacency parameter matrix \(\) based on \(\), thereby inferring the underlying edge set \(\) of \(\)._

Definition 2 (Influence Estimation).: _Consider a diffusion network \(=(,)\) with node set \(\), edge set \(\), and the associated adjacency parameter matrix \(\). Given a set of seed nodes \(\) and a time window \(T\), influence estimation aims to estimate the expected number of nodes influenced by \(\) within the time window \(T\) under the continuous-time independent cascade (CIC) diffusion model._

## 3. Diffusion Framework

In this section, we develop the framework to model _continuous-time diffusions_ in a diffusion medium.

### Conditional Diffusion Rate

During one diffusion process, an inactive node can have multiple active incoming neighbors, thus possibly being influenced by them simultaneously. The parameterization of the cumulative impact conditioned on a given observation is determined by the joint diffusion rates of its active neighbors. In this context, \(_{t}^{n}\) denotes the _conditional diffusion rate_, for which the following holds.

**Lemma 1**.: _Given an observation \(_{t}\) with \(_{t,n}=0\) for node \(v\), we have_

\[_{t,v}=_{u_{v}}(_{uv}_{t,u}). \]

Let \(_{t}\) be the _expected_ conditional diffusion rate over the randomness of observations at time \(t\), i.e., \(_{t}=_{u}_{t} p(_{t})\) where \(p(_{t})\) is the probability of observation \(_{t}\). By Equation (1), we have

\[_{t}=[(1-_{t})^{}_{t}_{t}], \]

where \(=\{1\}^{n}\) is an \(n\)-dimension all-ones vector and \(\) is the Hadamard product.

### Continuous-time Diffusion Propagation

As described in Section 2.2, influence from source sets spreads along edges to other nodes as a continuous-time cascade. Seeing such influence propagations as temporal evolving dynamics, a continuous-time diffusion among nodes of a network is essentially a _continuous-time dynamical system_ (CDS), succinctly defined as follows.

**Definition 3** (Continuous-time Dynamical System (Durham, 2005; 2015)).: _A continuous-time dynamical system consists of a phase space \(\) and a transformation map \(:(t,)\) where \(t^{n}_{0}\) is the time._

Accordingly, we apply CDS to diffusion networks and formalize the concept of _continuous-time diffusion propagation_ (CDP).

**Definition 4** (Continuous-time Diffusion Propagation).: _Consider a diffusion network \(=(,)\) associated with adjacency parameter matrix \(\), source set \(\), and time window \(T\). The continuous-time diffusion propagation starting from \(\) by time \(T\) is defined as a tuple \((,_{T},)\), where \(_{T}\) is the state space of \(\) and \(\) is the state transition function, which advances a state \(_{t}_{T}\) to \(_{t+t}\) for an infinitesimal interval \(t 0^{+}\) with respect to graph \(\)._

The foundation of CDP is the state transition function \(\), which quantifies the evolution of node states over time. In particular, function \(\) computes node states in the future time \(t+\) solely upon states at the current time \(t\). To capture the transition within \(\), we first establish the _ordinary differential equation_ (ODE) of \(\). Consequently, CDP can be formulated as follows.

**Theorem 1**.: _Let \(\) be a given source set and \(_{t}\) be the random observation variable at time \(t\). Consider the infinitesimal interval \( 0^{+}\). The continuous-time diffusion propagation for all \(t 0\) is defined as_

\[[\{_{t+}-_{t}_{t}\}= _{t}, \]

_where \(\) denotes the Hadamard product and \(_{}\{0,1\}^{n}\) is the indicator vector, with \(_{0}=1\) for \(v\) and \(_{u}=0\) otherwise._

### Framework of Continuous-time Diffusion

Note that \(_{t}\) and \([_{t}]\) at time \(t=0\) can be simply known from Equation (4) and Equation (5) respectively, once the source set \(\) is given. Furthermore, Equation (3) shows that the exact value of \([_{t+t}]\) within interval \( 0^{+}\) is derived from \([_{t}]\). Instead, by relaxing the constraint from \( 0^{+}\) to \(z>0\) for a small interval \(\), we are able to acquire an approximation of the future state \([_{t+}]\).

Figure 1. Diffusion function \(p(2,t)\).

Figure 2. Illustration of a diffusion cascade.

For this, we resort to approximate \([_{t}]\) based upon \([_{t}]\) and \(_{t}\), after which we then estimate \(_{t}\), i.e.,

(6) \[[_{t}_{t}] [_{t}_{t}]+_{t},\] (7) \[_{t} [(1-_{t})^{ }_{t}_{t}].\] (8)

Based on this analysis, we propose our framework FIM (Framework of diffusion approximation) to approximate \(\{[_{}],[_{2}],, [_{T}]\}\) progressively.

Approximation by \(\) instead of intuitively incurs an approximation error at each iteration. Let \((t,)^{n}\) be the corresponding approximation error of \([_{t}]\) given observation \(_{t}\), i.e., \((t,)=[_{t}-_{t}_{ t}]+_{t}\). To quantify \((t,)\), we derive the following theorem.

**Theorem 2**.: _Given time \(t[0,T)\) and time interval \(t(0,T-t]\), the approximation error \((t,)\) at \(t\) is_

\[(t,)=_{t}+(-_{t})-1. \]

As shown, \((t,)\) is therefore monotonically increasing when \( 0\) and \((t,)=0\) if \(=0\).

**Complexity.** The time complexity is dominated by the calculation of \(\) in Equation (4), with a cost of \(O(m)\), where \(m\) is the number of non-zero elements in \(\). Thus the total time complexity of the approximation is \(O((m+n))\).

## 4. Network inference and influence estimation

FIM assumes that the adjacency parameter matrix \(\) is known. However, this assumption rarely holds, as \(\) is typically unknown in reality. In this section, we aim to learn the parameter matrix \(\) by leveraging FIM to model the cascade data and then infer the underlying network structure. Afterwards, we develop a sampling technique for continuous-time influence estimation.

### Network Inference

As introduced in Section 2.2, a cascade \(\) within time \(T\) records the activation time \(t_{}\) of each node \(v\). Given cascade \(\), we can restore its diffusion process by generating a series of diffusion observations \(_{t}\) and node states \(_{t}\). In particular, given a small interval \(\), we construct \(_{k}\) and \(_{k}\) for \(k\{0,1,, T/\}\) by comparing \(t_{}\) with \(k\) for each node \(v\). Specifically, we set \(_{k,v}=t_{}\) and \(_{k,v}=1\) if \(t_{} k\), else \(_{k,v}=\) and \(_{k,v}=0\) for \( v\).

With observation \(_{(k-1)}\) and state \(_{(k-1)}\) at \(t=(k-1)\), the expected state \([_{k}_{(k-1)}]\) conditioned on \(_{(k-1)}\) can be estimated by FIM, after which the corresponding _binary cross-entropy_ (BCE) loss \(_{k}\) over all nodes is calculated as

\[_{k}&=_{v }_{k,v}[_{k,v} _{(k-1)}]\\ &+(1-_{k,v})(1-[_{k,v }_{(k-1)}]). \]

By simulating the complete cascade \(\), the total loss \(()\) of \(\) is \(()=_{k=1}^{ T/}_{k} +_{T}\). Formally, the procedure of FIM simulating cascade \(\) is presented in Algorithm 1.

```
Input: Cascade batch \(_{}\), time interval \(\), parameter matrix \(\) Output: Loss \((_{})\)
1for cascade \(_{}\)do
2 Initialize observation \(_{0}\) and state \(_{0}\) according to \(\);
3for\(k 1\) to \( T/\)do
4\(Y_{(k-1)}=(1-_{(k-1)})^{}_{ (k-1)}\) ;
5\([_{k}_{(k-1)}]_{ (k-1)}+ Y_{(k-1)}\);
6
7 Obtain \(_{k}\) and state \(_{k}\) from \(\);
8 Calculate \(_{k}\) as Equation (9);
9\( T-k\);
10 Calculate \(_{T}\) conditioned on \(\{_{T/}\}\) by following the procedure from Line 4 to Line 7;
11\(()=_{k=1}^{ T/}_{k}+_ {T}\);
12
13\((_{})_{} }_{_{}}()\);
14return\((_{})\);
```

**Algorithm 1**Diffusion approximation by FIM

Once the parameter matrix \(\) is estimated, the underlying network structure can be inferred accordingly. By following the literature (Ashburn and Boyd, 1995; Boyd, 1995; Boyd, 1995; Boyd, 1995), an empirical threshold \(^{}\) can be set such that an edge \((u,v)\) exists if \([u,v]^{}\). (More details in Appendix A.2).

### Influence Estimation

Given a graph \(=(,)\) with the adjacency parameter matrix \(\), a source set \(\), and a time window \(T\), let \(I(T,)\) be the number of activated nodes in a cascade \(c\) sampled from \((T,)\). The influence estimation problem is to compute the expected number of nodes \([I(T,)]\) influenced by \(\) within the time window \(T\).

It is intuitive that a node \(u\) is influenced by the source node \(s\) that has the shortest time (path) to \(u\) under the CIC model, where the edges are weighted by their transmission times, which is known as the _shortest path property_ in the literature (Brockman, 1995; Boyd, 1995; Boyd, 1995). By consequence, one instance of \(I(T,)\) can be calculated as follows. By sampling the diffusion time \(t_{u,v}\) of each edge \((u,v)\) according to the diffusion function \(p(u_{uv},t),I(T,)\) is the number of nodes reachable by \(\) within time \(T\). Existing methods (Brockman, 1995; Boyd, 1995) first identify one activation set for each source node \(s\) and then take the union over all the source nodes to calculate \(I(T,)\).

[MISSING_PAGE_FAIL:5]

**Limitations of the baseline methods.** We start by reporting that, on our reasonably powerful server1, both NMF and NetRate are _out of memory_ (OOM) on the larger datasets (Weibo and Twitter) and thus their performance is not available on them. Furthermore, NetRate could not finish in less than 12 hours on MemeTracker and the synthetic datasets, except the smallest one (HR). We believe this is due to the joint effect of the node count, cascade count, and cascade sizes. We can therefore conclude from these outcomes that NetRate- especially with the official Matlab implementation - is not efficient for large networks2.

**Parameter learning on HR and MemeTracker.** In a first experiment, we compare FIM's parameter learning performance on (i) the only dataset common to all three methods (HR), and (ii) the only real-world dataset common to both NMF and FIM (MemeTracker). (We defer a similar comparison between FIM and NMF, on the other six synthetic datasets, to the next experiment, where they are complemented by effectiveness scores on network inference based on the inferred parameters.) Table 3 presents the BCE loss results - as defined in Equation (9) - for parameter learning from cascades with a varying time window \(T\) and time interval \(\). We can notice that FIM outperforms NMF by achieving slightly smaller BCE loss on both datasets. Furthermore, FIM consistently achieves notably smaller BCE loss compared to NetRate on HR, across all settings. These findings provide good evidence of FIM's superior ability to learn a high-quality parameter matrix \(\).

Furthermore, Figures 3 and 4 plot the training time of FIM and NMF, with varying time window values \(T\) and time intervals \(\), on HR and MemeTracker respectively. As displayed, FIM runs notably faster than NMF, with speedups up to \(3 4\). Moreover, the efficiency advantage becomes more obvious as the data grows.

**CIM based evaluation on the learned parameter matrix.** Following the previous experiment, we further assess of quality of each inferred matrix \(\) from the perspective of _continuous influence maximization (CIM)_, based on _test cascades_ (unseen during training), as follows.

Recall that CIM algorithms are able to identify the seed set \(\) with the largest expected spread \([]\) in a given diffusion network (as per \(\)). In our setting, following the convention (Brockman et al., 2016; Chen et al., 2016), we can obtain the _ground-truth_ spread (i.e., quality) of a source set \(\) as follows: randomly sample from the test cascades one for each node in \(\), and take the union of the nodes from the sampled cascades, leading to a single estimate of \([]\); repeat this process 1000 times to obtain the average over estimates as \(\)'s ground-truth spread. Therefore, generically, when a state-of-the-art CIM algorithm selects a seed set \(\) in the network defined by an estimated parameter matrix \(\), we can assess the quality of \(\) and, by design, also the accuracy of \(\) itself. _A larger ground-truth spread means a higher estimation quality for the parameter matrix \(\)_. (A more detailed justification for this CIM experiment can be found in Appendix A.2.2.)

As the HR dataset is unsuitable for this CIM experiment, we used Core2048 instead. This is because HR's cascades contain multiple source nodes (see the dataset's description in Appendix A.2.1), hence ground-truth spread cannot be obtained as described. Nevertheless, we describe in Appendix A.3 an influence estimation (IE) experiment on HR, designed for multi-source cascades, to assess the quality of the estimated \(\) by FIM, NMF, and NetRate.

We applied the state-of-the-art CIM algorithm (Shen et al., 2016) on the estimated \(\) of both MemeTracker and \(Core2048\), selecting seed sets \(\) with sizes from \(||=4\) to \(||=10\). Table 4 shows the spread

    &  & |\)} \\  & & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 68 \\   & FIM & 65.85 & 75.18 & 88.33 & 106.88 & 113.90 & 127.11 & 136.67 \\  & NMF & 53.14 & 64.78 & 70.66 & 76.10 & 83.39 & 88.23 & 94.95 \\   & FIM & 517.12 & 576.19 & 559.19 & 634.83 & 636.26 & 639.38 & 723.88 \\  & NMF & 442.25 & 506.17 & 476.44 & 570.90 & 591.80 & 637.40 & 567.28 \\   

Table 4. Influence maximization on Meme.

    &  & _{1024}\) / Hier\({}_{2048}\)**} & _{1024}\) / Core\({}_{2048}\)**} & _{1024}\) / Rand\({}_{2048}\)**} &  &  &  \\
**\#Nodes** & 128 & 1024 / 2048 & 1024 / 2048 & 1024 / 2048 & 1024 / 2048 & 498 & 8,190 & 12,677 & 642 \\   & 10,000 & 20,000 / 10,000 & 20,000 / 10,000 & 20,000 / 10,000 & 8,304 & 43,365 & 3,461 & 642 \\   

Table 2. Dataset details.

Figure 3. Running time on HR.

results for seed sets selected based on the estimated \(\) of these two datasets, by FIM and NMF respectively. We can observe that the spread values obtained based on FIM's estimated \(\) are significantly larger than those based on NMF's output. This observation further supports the high learning ability of FIM over NMF.

**Network inference on synthetic networks.** Recall that, once the parameter matrix is learned, we can infer a most likely diffusion topology by applying a predefined threshold \(^{*}\) to the estimated \(\), i.e., \([u,v]^{*}\) indicating the existence of the edge \((u,v)\). In this experiment, we assess the network inference performance of FIM and NMF, on the \(6\) synthetic datasets, for which the ground-truth networks are available. Here, we fixed the time window \(T=10\) and the time interval \(=1.0\). When an estimated parameter matrix \(\) is obtained, we set the threshold \(^{*}=0.01\) to determine the existence of an edge.

We use the _F1-score_ metric to measure the performance on predicting the existence of edges, and the results are presented in Table 5. We can observe that the F1-scores of FIM are significantly higher compared to those of NMF, especially on Hier1024 and Core1024. For a complete perspective, besides the F1-score, we also provide the BCE loss of parameter learning on those synthetic datasets. Once again, FIM achieves consistently a smaller BCE loss than NMF, for all the synthetic datasets.

Finally, in this same setting (\(T=10\) and \(=1.0\)), Figure 5 compares the training time of FIM and NMF on the synthetic datasets. Similar to the results of Figures 3 and 4, FIM outperforms NMF significantly (e.g., speedup of up to \(8.24\) on Hier2048).

### Influence estimation on inferred networks

**Setting and baselines.** By fixing the underlying inferred networks, we can evaluate the performance of FIM (Section 4.2) for influence estimation (IE), against the baselines ConTinEst (Chen et al., 2018) and NMF (Kang et al., 2018).

We randomly generate a series of source sets \(\) with size \(||\{4,5,6,7,8,9,10\}\) from the test cascades. We then compute the _mean absolute error_ (MAE) between the spread estimated by each method and the ground-truth, denoted by _IE MAE_, as well as the influence estimation time. For a fair comparison, we use the same sample size for FIM and ConTinEst (no sampling in NMF).

**Results.** Due to space limitations, we chose to present the results of IE on the inferred networks from MemeTracker and Core2048, since we observed similar results across the other synthetic datasets. From the results on IE MA (Figure 6) we can conclude FIM achieves the smallest MAE on both datasets, regardless of the source set size. The IE MAEs of NMF are notably larger than those of the competitors on MemeTracker, while ConTinEst performs clearly the worst on Core2048. This observation also indicates the robustness of FIM across diverse datasets.

Figure 7 compares the influence estimation times for the three tested methods. We can observe that FIM is _orders of magnitude_ faster than NMF and ConTinEst on both datasets, which confirms the gains by our proposed sampling technique SDTS (Section 4.2). In particular for NMF, its running time consists of the model loading time, i.e., loading the pre-trained model for IE, and the estimation time, while the former obviously dominates the latter, as observed in our experiment.

### Scalability evaluation on large networks

**Larger real-world datasets.** To further explore the scalability of FIM, we employ two large real-world datasets, i.e., Weibo and Twitter, containing \(8,190\) and \(12,677\) nodes respectively, for continuous-time network inference. To the best of our knowledge, Twitter is the largest real-world dataset tested in the literature for this problem.

Table 6 presents the BCE loss under various time window values \(T\) and time intervals \(\), while Figure 8 plots the corresponding running time of FIM. It is worth pointing out that the BCE loss of FIM on Twitter is surprisingly low (at most \(0.01\)). Regarding the running time on the two datasets, FIM completes the training phase within \(30\) seconds for the largest time window \(T=15\) and around \(35\) seconds for the smallest \(=0.5\). These results not only validate the practical interest of FIM for network inference from cascades,

  & **Method** &  &  \\  & 5 & 8 & 10 & 12 & 15 & 0.5 & 1.0 & 1.5 & 2.0 \\  Weibo & FIM & 0.69 & 1.07 & 1.79 & 1.93 & 2.25 & 2.65 & 1.79 & 0.82 & 0.77 & 758 \\  Twitter & FIM & 0.003 & 0.006 & 0.008 & 0.01 & 0.014 & 0.015 & 0.008 & 0.005 & 0.005 \\  

Table 6. BCE loss on Weibo and Twitter.

Figure 5. Running time on synthetic datasets.

Figure 6. Influence estimation MAE.

but are also promising indicators that our method could scale to even larger datasets.

**Larger synthetic datasets.** To further investigate the efficiency gains of FIM over the baseline ConTinEst, for influence estimation, we also generated a synthetic dataset, \(_{4096}\), with \(4096\) nodes and \(10000\) cascades. The corresponding MAE and running time of the two tested methods are in Table 7. (Recall that both NetRate and NMF are OOM in our computing environment for networks of this scale). First, we observe the IE MAEs of FIM are consistently smaller than those of ConTinEst, with a difference ranging from \(11.70\%\) to \(33.98\%\), which is in line with the results of Figure 6.

Moreover, FIM clearly outperforms ConTinEst in terms of running time. In particular, the speedup of FIM over the state-of-the-art ConTinEst is up to \(100-120\), so two orders of magnitude faster. These observations, along with the results in Figure 7, further support the scalability of our approach.

## 6. Related Work

**Network Inference.** Gomez-Rodriguez et al. (2018) establish a generative probabilistic model to calculate the likelihood of cascade data. They devise the NetIng approach to infer the network connectivity, by a submodular maximization, aiming to maximize the cascades' likelihood. Similarly, Gomez-Rodriguez et al. (2018) set a conditional likelihood of transmission with parameter \(a_{i,j}\) for each pair of nodes \(i,j\) and derive the corresponding survival and hazard functions to express the likelihood of a cascade. They propose the algorithm NetRate to maximize the likelihood of cascades, by optimizing the pairwise parameter \(_{i,j}\), which is the indicator of existence for edge (\(i,j\)). To capture heterogeneous influence among nodes (instead of following a fixed, parametric form), Du et al. (2019) adopt a linear combination of multiple parameterized kernel methods to approximate the hazard function for the cascade likelihood maximization. This approach is shown to be more expressive than previous models. Later, Gomez-Rodriguez et al. (2018) develop a more general additive and multiplicative risk model by adopting survival theory. As a result, the network inference problem is solved via convex optimization. We did not include the methods of Du et al. (2019), Gomez-Rodriguez et al. (2018) in our experimental comparison, as we were not able to obtain their implementation.

**Influence Estimation.** Du et al. (2019) explores the influence estimation problem when the underlying network and transmission parameters are accessible. They point out that the set of influenced nodes is tractable through the shortest-path property. Based on this finding, they devise a novel size estimation method, by using a randomized sampling method from (D'Angelo et al., 2018) and develop the ConTinEst algorithm for influence estimation. Later, Du et al. (2019) further study the estimation of transmission parameters when only the network and cascade data are available. They propose InfuLearner to learn the diffusion function - using a convex combination of random basis functions - directly from cascade data, by maximizing the likelihood. However, InfuLearner requires knowledge of the cause (source node) of each activation in the cascades. Instead of focusing on the global influence of a seed set, Qiu et al. (2019) aims to predict the local social influence of each individual user. To this end, they design an end-to-end framework called DeepIng to learn latent representations, by incorporating both the network topology and user-specific features. Recently, He et al. (2018) adopt neural mean-field dynamics to design NMF to approximate continuous-time diffusion processes. As shown by our experiments, our model FIM outperforms NMF in terms of both efficiency and accuracy, for the problems of network inference and influence estimation.

## 7. Conclusion

In this paper, we revisit the problems of network inference and influence estimation from continuous-time diffusion cascades. We propose the framework FIM, with the objective of improving upon both the learning ability and the scalability of existing methods. To this end, we first propose a continuous-time dynamical system to model diffusion processes, and build our framework FIM for network inference based on it. Furthermore, we improve upon the influence estimation by proposing a new sampling technique. We analyze the approximation error of FIM for network inference and the effect thereof on influence estimation. Comprehensive experimental results demonstrate the state-of-the-art performance of FIM for both network inference and influence estimation, as well as its superior scalability and applicability to real-world datasets. As a future work, we intend to extend and adapt FIM to different diffusion functions, besides the exponential one.

    &  &  \\  & & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\   & FIM & 34.682 & 571.38 & 355.36 & 253.58 & 299.41 & 290.72 & 300.26 \\  & ConTinEst & 401.92 & 433.66 & 402.42 & 384.46 & 406.90 & 362.82 & 353.24 \\   & FIM & 19.22 & 20.87 & 21.42 & 19.13 & 20.19 & 20.98 & 19.93 \\  & ConTinEst & 2172.89 & 2219.13 & 2189.17 & 2271.28 & 2291.80 & 2156.48 & 2210.29 \\   

Table 7. IE MAE and running time (RT) on \(_{4096}\).

Figure 8. Running time of FIM on Weibo and Twitter.

Figure 7. Influence estimation times.