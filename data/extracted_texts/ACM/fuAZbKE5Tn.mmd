# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

that might hinder previous research. Fortunately, with the advent of LLMs, their impressive proficiencies have been showcased across numerous NLP and IR tasks (Zhu et al., 2019; Wang et al., 2019; Wang et al., 2019). They provide deep contextual insights, precise semantic understanding, and hold great promise in modeling content across various modalities (Zhu et al., 2019; Wang et al., 2019). Thus, we are optimistic about leveraging LLMs to interpret graph structures linguistically, enabling a comprehensive exploration of both semantic and structural information within a search session. Concretely, we first create a heterogeneous graph, distinguishing between _query_ and _document_ as the primary node types. To capture the diversity of user interactions, we also integrate three distinct edge types _click on_, _query transition_, and _document transition_. Then, to represent the rich heterogeneous information from a search session, we transform the explicit structural details of the graph into text conforming to a specific symbolic textual format. We integrate the graph and task instructions into our prompt design, which serves as input to the LLM. The prediction distributions of the answer tokens are used as relevance probabilities for ranking. In this way, we formulate the session search task as the link prediction between the document node and the query node of a session graph, all in text format. Note that while LLMs are pretrained on pure text, we come up with new symbolics to represent graphs. Hence, it is necessary to augment the LLM's comprehension of these symbols. Correspondingly, we propose a set of pre-training tasks including link prediction, generation of node text attributes, and generative contrastive learning with graph augmentation. These tasks reflect the topological information of a session graph from coarse-grained to fine-grained, heuristically guiding the LLMs to understand the heterogeneous session graph structure. By pre-training LLMs on in-domain datasets, we equip them with domain-specific knowledge. This becomes particularly useful when a query or a document recurs across multiple sessions, as this global graph information is stored within the LLM's parameters. Consequently, these LLMs not only capture an inter-session perspective but also enhance their comprehension of the intra-session context.

Experiment results on two public search log datasets, AOL and Tiangong-ST, show that our proposed method outperforms the existing methods with relatively little training data. We also conduct extensive experiments to verify the effectiveness of our symbolic graph representation, and showcase how our symbolic learning tasks enhance LLMs' comprehension of graphs.

Our contributions in this paper can be summarized as follows:

* We aim to integrate structural information in session search with textual data, ensuring that both semantic meaning and topological knowledge from the search session are fully explored and utilized to yield better search results.
* To accomplish this, we harness the capabilities of LLMs, converting graph data into text using a series of symbolic rules. Recognizing the disparity between LLMs and graph-based symbols, we devise a range of self-supervised pretraining tasks to better adapt the LLM for our purpose.
* Our experimental findings, derived from two widely recognized search log datasets (AOL and Tiangong-ST), indicate that our proposed technique surpasses existing methods, especially when training data is limited.

## 2. Related Work

### Session Search

Contextual information in sessions is considered conducive to infer users' search intent, providing retrieval results that better align with users' information needs. Early studies extract statistical and rule-based features from users' search history so as to better characterize their search intent (Zhu et al., 2019; Wang et al., 2019; Wang et al., 2019). With the development of deep learning approaches, a series of works have emerged that model user behavior sequences to obtain semantically dense representations for session search tasks. For instance, Ahmad et al. ( Ahmad et al., 2019) utilize a hierarchical neural structure with RNNs to model the session sequence and achieve competitive performance in both document ranking and query suggestion. Taking one step further, the attention mechanism is introduced to existing RNN-based architecture and yields better results (Bahdanau et al., 2015). As Pre-trained Language Models have demonstrated their capabilities in various NLP and IR tasks, employing a PLM as backbone has become a new paradigm that treats each search session as a natural language sequence (Bahdanau et al., 2015; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019).

Recent works suggest that modeling search sessions as sequences may ignore the topological interactions between queries and documents, while the session history can be regarded as a graph for interaction modeling. For example, Ma et al. (Ma et al., 2019) regard session search as a graph classification task on a heterogeneous graph that represents the search history in each session, and Wang et al. (Wang et al., 2019) propose a heterogeneous graph-based model with a session graph and a query graph. However, previous Graph Neural Network (GNN)-based studies tend to solely focus on the session structure while neglecting the importance of node semantic modeling, where they only use one vector to represent the whole document in the interaction process. In our work, we explore the potential of LLMs to integrate the benefits of the two approaches, i.e., modeling the nuance of semantic meaning as well as the user behavior structure. Concretely, we flatten the session graph in structural language into prompts and design symbolic pre-training tasks to help the LLM understand and reason over the graph.

### Pre-training on Graphs

To enable more effective learning on graphs, researchers have explored how to pre-train GNNs for node-level representations on unlabeled graph data. Inspired by pre-training techniques in natural language processing (Han et al., 2019) and computer vision (Shen et al., 2019), recent studies have been proposed to pre-train GNNs with self-supervised information (Wang et al., 2019). This approach aims to tackle the issue of limited labels by employing self-supervised learning on the same graph (Han et al., 2019), or to bridge the disparity in optimization objectives and training data between self-supervised pre-training activities and subsequent tasks (Wang et al., 2019; Wang et al., 2019). The representative tasks include link prediction, node classification, and contrastive learning. etc. For instance, Hu et al. (Hu et al., 2019) introduce a method based on graph generation factorization to guide the foundational GNN model in reconstructing both the attributes and structure of the input graph, and Qiu et al. (Qiu et al., 2019) put forward a contrastive pre-training model devised to capture universal and transferable structural patterns from multiple input graphs. Different from previous works that achieve these tasks in vector space, we transform the task into text format by a set of symbolic grammars.

### Large Language Model For Graph Learning

Existing works have demonstrated the outstanding performance of LLM in natural language processing tasks. Recent studies suggest the use of graph data to create heuristic natural language prompts, aiming to augment the proficiency of large language models. For example, in the field of job recommendations, Wu et al. (Wu et al., 2019) propose using a meta-path prompt constructor to fine-tune a large language model recommender to understand user behavior graphs. In the domain of molecular property prediction, Zhao et al. (Zhao et al., 2019) introduce a unified language model for both graph and text data, eliminating the need for additional graph encoders to learn graph structures. Through instruction pre-training on molecular tasks, the model effectively transfers to a wide range of tasks. Different from previous works, our approach structures historical conversation graphs into natural language prompts to fully exploit the advantages of LLM.

## 3. Methodology

### Task Formulation

Before introducing our proposed methodology, we first state some notations and briefly formulate the task of session search. We denote the historical queries of a user's search session as \(Q=\{q_{1},q_{2},,q_{M}\}\), where each query is the text that the user submitted to the search engine and has been ordered by their issued timestamp. \(M\) is the history length Given the query \(q_{i}\), its candidate documents list is denoted as \(D_{i}=\{d_{i,1},d_{i,2},,d_{i,N}\}\), and each document has a binary click label \(y_{i,j}\), indicating if user clicks the document. The session search task aims to re-rank the candidate document set \(D_{i}\) considering the in-session contextual information and the issued query \(q_{i}\). In our paper, we denote the session context as the sequence of the historical queries, the clicked documents and the current query. Formally, the session context \(S_{i}\) of \(q_{i}\) is formulated as \(\{(q_{1},D_{j}^{+}),(q_{2},D_{j}^{+}),,(q_{i-1},D_{j}^{+}),q_{i}\}\), where \(D_{i}^{+}=\{d_{i,j}\,|\,y_{i,j}=1\}\) is the clicked documents of query \(q_{i}\).

### Overview

Our model is illustrated in Figure 2. We first establish a heterogeneous session graph that stores the user query and interaction information, which is then translated into symbolic text following our specific graph-to-text symbolic grammars. This symbolic depiction, along with the task description, is directly fed into the LLM to generate the ultimate search result. To enhance the LLM's understanding of the symbolic, we come up with three sub-tasks centered around the symbolic text. These sub-tasks require comprehension of the existing graph structure, thus eliminating the need for additional annotations. Finally, we formulate the document ranking task in the search session as a link prediction task, which is also based on the symbolic prediction token.

### Session Graph Construction

#### 3.3.1. Graph Schema

A search session includes user queries, candidate documents, and user click activities. Intuitively, the queries and documents can be regarded as nodes in a graph, with an edge forming between a query-document pair when a user selects the document for a given query. Beyond this, transitions also exist within queries and documents themselves. As such, a search session naturally fits a heterogeneous graph scenario, where multiple sessions sharing common documents and queries weave into a comprehensive global graph.

Formally, the definition of a graph is \(G(V,E)\), where \(V,E\) denote the sets of the nodes and edges respectively. Our graph is heterogeneous since it contains multiple types of nodes and edges. In the following, we detail the process of constructing our behavior graph for each session. We consider both queries and documents as nodes, i.e., \(V=\{q_{1},,q_{i},d_{1,1},,d_{i,J}\}\). For these nodes, we define three types of edges:

**Query-query.** Users typically engage in multiple interactions with a search engine to meet their evolving and ambiguous information needs. Consequently, understanding the transition between queries can be beneficial for discerning user search intent. Previous work (Zhou et al., 2019) propose to connect all query pairs in the same session, where a previous query will also be connected to all future queries. Herein, we assume that linking all query pairs could potentially dilute the distinct progression of user intents and introduce unnecessary noise into the session. Therefore, we opt for a more streamlined approach, connecting only adjacent queries denoted as _query transition_, e.g., \(e_{i}=(q_{j},q_{j+1})\).

**Query-document.** In large-scale search logs, click-through data is often leveraged as an indicator of the relevance between queries and documents (Zhou et al., 2019). As a result, we consider the click relation _click on_ between queries and their clicked documents, e.g. \(e_{i}=(q_{j},d_{j,k})\), where \(y_{j,k}=1\). Liu et al. (Liu et al., 2019) link queries to their top returned results to enrich relevance signals, addressing the sparsity of click-through in comprehensive search logs. However, we assume this approach might undermine the significance of the click relationship and inadvertently incorporate unrelated documents. To provide more nuanced signals to the model, we resort to the pre-training tasks which will be introduced in SS3.4.

**Document-document.** Until now, the constructed edges are centered around the queries. Nevertheless, within the context of session search, documents also hold significant importance. While queries are typically brief and ambiguous outlines, documents provide detailed and specific information related to the query. We believe that transitions between clicked documents can also shed light on the evolution of a user's intent, complementing the insights gained from query transitions. Correspondingly, we construct a fully connected graph, named _document transition_, for the clicked document pairs of the given query, exemplified by edges such as \(e_{i}=(d_{j,k},d_{j,l})\), where \(y_{j,k}=1\) and \(y_{j,l}=1\). Note that the query-query and query-document edges are asymmetrical, each bearing distinct implications. Conversely, the document-document edges between documents are symmetrical, owing to their shared attribute.

#### 3.3.2. Symbolic Graph Construction

Traditional works use neural networks for graph modeling (Zhou et al., 2019; Zhang et al., 2019). However, the dense vector format is not easily understood by language models. Therefore, our objective is to transform the graph structure into a task-specific symbolic language that's understandable by LLMs. The benefits are twofold. First, LLMs are renowned for their profound contextual insights and accurate semantic understanding, attributes invaluable for text-based session search. Second, symbolic inference ensures transparency and trustworthiness; the reasoning is grounded insymbolically represented knowledge and adheres to well-defined inference rules consistent with logical principles (Selvin et al., 2017).

Formally, we transform the session graph \(G\) into symbolic language following setting a few symbolic grammars:

\(\)**Nodes:** the node \(v V\) is either a query \(q_{i}\) or a document \(d_{i,j}\). Each node has its node type, index, and text content. Thus, the symbolic representation of a node can be formulated as:

\[SGC_{v}(v)=(. \]

For example, a third query asking about the MacBook price is denoted as (\(q_{3}\), MacBook Price?), and the corresponding fifth document candidate is represented as (\(d_{5}\), \(\$1,999\)).

\(\)**Edges:** in our symbolic grammar, we define two kinds of edges. Firstly, we retain the original _click on_ relationship to represent the direct interaction between a query and its selected documents. Secondly, we include both the query transition and document transition under the umbrella term _transfer to_. This is because our preliminary experiments indicate that differentiating between query-to-query and document-to-document transitions didn't significantly benefit our model's performance. By unifying these transitions under the term _transfer to_, we not only simplify the graph representation but also ensure a more streamlined interpretation for the LLM.

Formally, for the edge \(e\) that links the node \(v_{1}\) with \(v_{2}\), the symbolic representation of an edge can be formulated as:

\[SGC_{e}()=SGC_{v}(v_{1})<>\ SGC_{v}(v_{2}). \]

For example, an edge between the above query node and the document node is denoted as:

\[SGC_{v}(q_{3},d_{5})=(q_{3},)<>(d_{5}, \$1,999). \]

\(\)**Session Graph:** As we have contained the node information in the symbolic representation of the edge, the translation of the session graph \(G(V,E)\) into a symbolic language prompt can be represented as the concatenation of the edges in chronological order.

\[SGC(G)=concat(SGC_{e}(e_{1}),SGC_{e}(e_{2}),,SGC_{e}(e_{|E|})). \]

An example of the symbolic session graph is in Figure 2.

### Symbolic Learning

We've developed a unique set of grammars to convert graph structures into symbolic text. This allows LLMs to interpret and analyze them. To further ensure that LLMs are adept at understanding this symbolic representation, we introduce a series of pre-training tasks tailored to familiarize the LLM with the nuances and intricacies of the transformed text.

#### 3.4.1. Link Prediction

Link prediction has traditionally been a cornerstone task in self-supervised graph pretraining (Bengio et al., 2017; Wang et al., 2018). It is based on predicting connections between nodes, leveraging the inherent structure and attributes within a graph. This method not only harnesses the topological patterns of the graph but also serves as an effective approach to capture and represent the latent relationships between nodes in a graph-based model. While previous methods computed the similarity based on the embeddings of each node, we have re-envisioned the task into a textual question format, relying on the capabilities of the LLM to discern the nuances and relationships.

Specifically, edges connecting nodes serve as positive samples, while non-connected edges are treated as negative samples. Given two nodes \(v_{1}\) and \(v_{2}\) that we aim to predict a link for, and the graph

Figure 2. Overall architecture of our model, which consists of three parts: (1) Session Graph Construction: this part organizes the interaction process as a heterogeneous global graph. (2) Symbolic Learning: we design three self-supervised subtasks to enhance the understanding of LLM on the symbolic representation of the session graph. (3) Symbolic Link Prediction: the LLM is fine-tuned for the document ranking task reframed as symbolic link prediction.

without the target link information \(G_{link}\), the input presented to the LLM is structured as follows:

\[X=[]SGC(G_{link}) \] \[SGC_{v}(v_{1}). \]

The task instruction example is in Figure 2.

We use \(p(X)\) to denote the logits of the answer token predicted by the model, which is considered as the link probability. Thus, the optimization goal is:

\[_{link}=-z p(X)+(1-z) p(X),\]

where \(z\) is the link label. If \(z\) is 1 (corresponding to the 'yes' class), the loss function aims to minimize \((- p(X))\), pushing the predicted probability for the positive class closer to 1. If \(z\) is 0 (corresponding to the 'no' class), the loss function aims to minimize the negative log likelihood of the prediction being the negative class, pushing the predicted probability for the positive class closer to 0.

#### 3.4.2. Node Content Generation

Traditional graph modeling typically offers just one overall representation for each node (Zhou et al., 2017; Wang et al., 2018; Wang et al., 2018). As a result of this simplification, earlier pre-training tasks on graphs have been largely confined to predicting node attributes, often restricted to a handful of class labels. In contrast, our approach emphasizes preserving the concrete semantic meaning of each node. We achieve this by retaining word-level information for both query and document nodes. Building on this foundation, in this study, we elevate the challenge by requiring the LLM to predict the context within each node, be it the query or the document content.

Specifically, as shown in Figure 3(b), we randomly mask a node \(v_{2}\) in the session graph, denoting the resulting graph as \(G_{node}\). The input to the LLM is then given by:

\[X=[]SGC(G_{node}) \] \[SGC_{v}(v_{1}). \]

The training objective of the content generation task is to reconstruct the target node content:

\[_{node}=-_{i=1}^{|SGC(v_{i})|} p(SGC(v_{2})_{i} |X,SGC(v_{2})_{<i}),\]

where \(SGC(v_{2})_{<i}\) represents the words in \(SGC(v_{2})\) preceding the \(i\)-th word. Note that we let the LLM predict both the content and the index of the target node, drawing inspiration from existing recommendation works that are based on IDs (Zhou et al., 2017).

#### 3.4.3. Generative Contrastive Learning

Contrastive learning is a traditional pre-training task for graphs, as highlighted in studies such as (Han et al., 2017; Li et al., 2017; Li et al., 2017). The core objective of this approach is to ensure that the representations of adjacent nodes are similar while distancing those of non-adjacent nodes. Drawing inspiration from this, we present a new paradigm, a generative contrastive learning task tailored for symbolic graph representation. The basic concept is to emphasize the LLM's awareness of the session history. As a result, this method could furnish models with a deeper understanding of context, allowing them to adapt more efficiently to evolving graph structures over time.

Specifically, as in Figure 3(c), we consider two distinct scenarios for inputs: In the first scenario, the LLM predicts the content of the target node with access to the search history, represented as \(X=SGC(G)SGC_{v}(v_{1})\). In the second scenario, the LLM lacks this access, denoted by \(X_{s}=SGC_{v}(v_{1})\). Our objective is for the performance with history to surpass that of the model without it.

(6) \[_{contrast}=_{i=1}^{|SGC_{v}(v_{2})|} p(SGC_{v} (v_{2})_{i}|X,SGC_{v}(v_{2})_{<i})\] \[-_{i=1}^{|SGC_{v}(v_{2})|} p(SGC_{v}(v_{2})_{i}|X_{s},SGC _{v}(v_{2})_{<i}).\] (7)

### Symbolic Document Ranking

Our ultimate objective for session search is to provide a sequence of related documents. This approach shares similarities with the link prediction task discussed in Section SS 3.4.1, but it differs in that it involves returning either a single result or multiple results. Consequently, the optimization function also varies. For a given query node \(q\) and a candidate document \(d_{j}\) from the candidate sets within a user session graph \(G\), the input presented to the LLM is formulated as:

\[X_{j}=[]SGC(G) \] \[SGC_{o}(q)SGC_{o}(d_{j}). \]

The logits of the 'yes' answer token \(p(X_{j})\) is regarded as the ranking score of document \(d_{j}\). To optimize the model, we employ the negative log-likelihood loss for learn-to-rank as follows:

\[_{rank}=-)}}{_{X_{j}}e^{p(X_{j})}},\]

where \(X_{*}\) denotes the related positive documents. This loss tries to push the positive document's score higher than those of other documents.

Figure 3. Three self-supervised symbolic learning tasks to bridge the gap between LLMs and symbolic representations.

## 4. Experiment Setup

### Research Questions

We list four research questions that guide the experiments:

\(\)**RQ1** (See SS 5.1): What is the overall performance of SGR compared with different kinds of baselines?

\(\)**RQ2** (See SS 5.2): What is the effect of each module in SGR? Is the performance improvement attributed to the symbolic graph representation we propose?

\(\)**RQ3** (See SS 5.3): Is our method robust with session lengths?

\(\)**RQ4** (See SS 5.4): How does our model scale with data?

\(\)**RQ5** (See SS 5.5): How does SGR perform in the pre-training stage?

### Dataset and Evaluation Metrics

#### 4.2.1. Dataset

Following previous studies (Zhu et al., 2017), we conduct experiments on two large-scale search log datasets, i.e., AOL (Zhu et al., 2017) and Tiangong-ST-click (Zhu et al., 2017).

We use the AOL dataset provided by Ahmad et al. (Ahmad et al., 2018). It contains numerous search logs grouped as sessions. Specifically, five candidate documents are provided for each query in both training and validation sets. In the test set, for each session query, we utilize 50 documents retrieved by BM25 (Zhu et al., 2017) as candidates. Every query in this dataset has a minimum of one corresponding click. When multiple clicked documents exist for a query, we construct the user behavior sequence using the first document from the list.

For the Tiangong-ST dataset, the session data are extracted from an 18-day search log provided by a Chinese search engine, and each query has ten candidate documents. Our setting follows (Zhu et al., 2018). In training and validation sets, we use the click-through labels as relevant signals. In this test set, only prior queries--excluding the last one--and their associated candidate documents are used. As with the AOL dataset, documents in this test scenario are labeled either 'click' or 'unclick'. The model's objective is to rank clicked documents as highly as possible. Note that queries without any clicked documents are excluded from testing. The statistics of both datasets are shown in Table 1.

#### 4.2.2. Evaluation Metrics

Following the earlier studies, we employ the Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain at position \(k\) (NDCG@\(k\), \(k\) = 1, 3, 5, 10) as metrics. All evaluation results are computed by the TREC's official evaluation tool (trec_eval) (Zhu et al., 2017).

### Baseline

In our experiment, we compare our methods with two kinds of baselines including (1) _ad-hoc_ ranking methods, and (2) _context-aware_ ranking methods.

(1) Ad-hoc ranking. These methods focus on the matching between the issued query and candidate documents, neglecting the information from the search context.

\(\)**BM25** (Zhu et al., 2017) is a traditional probabilistic model, which models the relevance of a document to a query as a probabilistic function. We use the pyserini (Zhu et al., 2017) tool in our work to calculate the BM25 scores.

\(\)**MonoT5** (Zhu et al., 2017) is a sequence to-sequence re-ranker that uses T5 to calculate the relevance score. In our paper, we use the trained checkpoint on Ms Marco (Baccaria et al., 2018) on HuggingFace.

(2) Context-aware ranking methods These methods employ either sequential modeling to process historical queries or graph-based modeling to represent user behavior.

\(\)**RICR** (Baccaria et al., 2018) is an RNN-based method that uses the history representation to enhance the representation of queries and documents on word-level.

\(\)**COCA** (Zhu et al., 2018) pre-trains a BERT encoder with data augmentation and contrastive learning for better session representation.

\(\)**ASE** (Zhu et al., 2017) designs three generative tasks to help the encoding of the session sequence. In contrast to other multi-task approaches that consider only the subsequent query generation as the auxiliary task, it further takes the succeeding clicked document and a similar query as generation targets.

\(\)**HEXA** (Zhu et al., 2017) proposes graph modeling user behavior in session search. It constructs two heterogeneous graphs, a session graph, and a query graph, to capture user intent from global and local aspects respectively.

### Implementation Details

We use PyTorch (Paszke et al., 2017) to implement our model. Specifically, LLaMa-7B (LeCun et al., 2015) and BaiChuan-7B (LeCun et al., 2015) are used as the backbone LLMs for AOL and Tiangong-ST, respectively. To facilitate lightweight fine-tuning, we employ LoRA (LeCun et al., 2015) to train our model, which freezes the pre-trained model parameters and introduces trainable rank decomposition matrices into each layer of the Transformer architecture. We adopt AdamW optimizer (Kingma and Ba, 2015) to train our model. The learning rate is set as 2e-5 with a cosine decay. We train our model by 2 epochs and the batch size is set as 8. Due to computational constraints, we randomly selected 1,000 sessions from the AOL test set for evaluation. All hyperparameters are tuned based on the performance of the validation set. For further implementation details, please refer to our code2.

**AOL** & **Training** & **Validation** & **Test** \\  \# Sessions & 219,748 & 34,090 & 29,369 \\ \# Queries & 566,967 & 88,021 & 76,159 \\ Avg.\# Query per Session & 2.58 & 2.58 & 2.59 \\ \# Candidate per Query & 5 & 5 & 50 \\ Avg. Query Len & 2.86 & 2.85 & 2.9 \\ Avg. Document Len & 7.27 & 7.29 & 7.08 \\ Avg. \# Clicks per Query & 1.08 & 1.08 & 1.11 \\ 
**Tiangong-ST** & **Training** & **Validation** & **Test** \\  \# Sessions & 143,155 & 2,000 & 2,000 \\ \# Queries & 344,806 & 5,026 & 6,420 \\ Avg. \# Query per Session & 2.41 & 2.51 & 3.21 \\ \# Candidate per Query & 10 & 10 & 10 \\ Avg. Query Len & 2.89 & 1.83 & 3.46 \\ Avg. Document Len & 8.25 & 6.99 & 9.18 \\ Avg. \# Clicks per Query & 0.94 & 0.53 & (3.65) \\  

Table 1. The statistics of the two datasets used in our paper. The number in parentheses is the average number of relevant documents.

## 5. Results and Analysis

### Overall Results

Addressing **RQ1**, in Table 2, SGR consistently surpasses other techniques, underscoring our approach's effectiveness. Based on the results, we can make the following observations.

(1) _Context-aware ranking methods consistently outperform ad-hoc models_. While ad-hoc models like BM25 and MonoT5 primarily focus on immediate query-document matching, they overlook the wealth of information embedded in the user's session history. On the other hand, context-aware methods, such as RICK, COCA, ASE, and HEXA, effectively harness sequential or graph-based representations to model user behavior over time. This not only provides a deeper understanding of user intent but also captures evolving search nuances. The superiority of context-aware methods in the results suggests that in a dynamic and interactive search session, understanding the broader context is crucial for achieving higher relevance and accuracy in rankings.

(2) _Our LLM-based SGR model significantly outperforms the state-of-the-art method HEXA (in paired t-test at \(p\)-value < 0.01)_. While both models integrate session data into their respective graphs, ours does so with better performance. On the other hand, HEXA's heterogeneous graph constructs more edges compared to our model, as discussed in SS3.3. Additionally, HEXA introduces both a query and session graph, whereas our model is founded on a singular graph structure. The positive results from our model suggest that LLMs can be effective in graph information modeling when paired with a well-designed pretraining task. Additionally, the potential of our approach is substantial and can grow alongside the evolution of LLM technologies.

### Ablation Study

(1) _The effects of various symbolic learning pre-training tasks_. For **RQ2**, we initiated ablation studies to delve into the impacts of different symbolic learning pre-training tasks. The findings of these studies are presented in Table 3. The term 'None' indicates the use of the baseline LLM for document ranking without incorporating our suggested symbolic learning phase.

Notably, the full deployment of SGR strategies yields the best results across all metrics, underscoring the comprehensive strength of a holistic approach. When breaking down combinations, the 'Link + Node' performs the best. However, as strategies are decoupled or 'What's used singly, there's a noticeable dip in performance, with the 'None' configuration highlighting the least effectiveness. This gradient in results underlines the criticality of integrative symbolic learning in refining sequence representation and optimizing for superior outcomes.

(2) _The effects of our symbolic graph representation method_. While our SGR demonstrates impressive results, it's crucial to determine whether the improvements come solely from the text in the search history or from the symbolic graph structure. Hence, we design an experiment presented in Table 4.

Our experiment comprises three distinct scenarios: (1) SGR w/o SG (symbolic graph): In this configuration, we omit graph information, which encompasses nodes and edges. Instead, the text is represented by sequences strung together with delimiters. (2) SGR

    & MAP & MRR & NDCG@1 & NDCG@3 & NDCG@10 \\  SGR (Full) & **0.5859** & **0.5972** & **0.4349** & **0.6225** & **0.6509** & 776 \\ None & 0.5580 & 0.5701 & 0.4050 & 0.5925 & 0.6242 & 77 \\ Link & 0.5783 & 0.5907 & 0.4282 & 0.6146 & 0.6443 & 778 \\ Node & 0.5747 & 0.5876 & 0.4247 & 0.6097 & 0.6410 & 779 \\ Contrast & 0.5740 & 0.5863 & 0.4177 & 0.6150 & 0.6398 & 780 \\ Link + Node & 0.5846 & 0.5966 & 0.4349 & 0.6206 & 0.6490 & 781 \\ Link + Contras & 0.5809 & 0.5925 & 0.4259 & 0.6197 & 0.6469 & 782 \\ Node + Contras & 0.5782 & 0.5916 & 0.4294 & 0.6151 & 0.6433 & 782 \\   

Table 4. Performance of SGR on the AOL dataset with different symbolic strategies. SG denotes ‘symbolic graph’ and SL denotes ‘symbolic learning task’.

    &  &  \\  Model & MAP & MRR & NDCG@1 & NDCG@3 & NDCG@5 & NDCG@10 & MAP & MRR & NDCG@1 & NDCG@3 & NDCG@5 & NDCG@10 \\  _ad-hoc ranking_: & & & & & & & & & & & & & \\ BM25 & 0.2703 & 0.2799 & 0.1608 & 0.2410 & 0.2693 & 0.3063 & 0.2845 & 0.2997 & 0.1475 & 0.1983 & 0.2447 & 0.4527 & 762 \\ MonoT5 & 0.3741 & 0.3856 & 0.2415 & 0.3496 & 0.3816 & 0.4308 & 0.3306 & 0.3447 & 0.1494 & 0.2465 & 0.3315 & 0.4939 & 763 \\    & & & & & & & & & & & & & & \\ RICK & 0.5506 & 0.5628 & 0.4084 & 0.5442 & 0.5823 & 0.6154 & 0.7472 & 0.7697 & 0.6401 & 0.7450 & 0.7822 & 0.8174 & 764 \\ ASF & 0.5667 & 0.5788 & 0.4081 & 0.5732 & 0.6073 & 0.6319 & 0.7410 & 0.7637 & 0.6277 & 0.7381 & 0.7790 & 0.8130 & 763 \\ COCA & 0.5469 & 0.5743 & 0.4149 & 0.5655 & 0.6010 & 0.6301 & 0.7481 & 0.7606 & 0.6386 & 0.7445 & 0.7858 & 0.8180 & 764 \\ REA & 0.5700 & 0.5819 & 0.4165 & 0.5744 & 0.6097 & 0.6372 & 0.7427 & 0.7660 & 0.6352 & 0.7378 & 0w/o SL (symbolic learning): While we incorporate the symbolic graph text in this setup, it is directly fine-tuned, bypassing the symbolic learning pre-training phase. (3) Full SGR model: Here, we seamlessly integrate both the symbolic graph input and symbolic learning tasks.

The outcomes indicate that excluding either SG or SL severely impairs the model's performance. Specifically, when we only keep the text while dismissing the graph information (SGR w/o SG), we observe a noticeable decline in MAP and MARR scores. This underscores that the mere inclusion of session history text isn't enough to improve performance. Moreover, pretraining tasks are also important for LLM to understand the graph structure (SGR w/o SLT). This highlights the efficacy of our symbolic graph representations in aiding large language models to grasp and leverage this concept.

### Impact of Session Lengths

The session length plays a pivotal role in determining the richness of contextual information, thereby influencing the efficacy of context-aware ranking models. For **RQ3**, to analyze this effect, we categorize the test sessions into three groups: short sessions with a length of 2 or fewer, medium sessions with a length of 3 or 4, and long sessions where the length exceeds 4.

Figure 4 illustrates the superior performance of SGR when compared to multiple baselines, with the \(y\)-axis representing the MAP score. SGR consistently surpasses all baseline models, highlighting its robustness regardless of session length variations. There is a distinct trend, i.e., the longer the session length, the more pronounced SGR's advantage becomes. This demonstrates the proficiency of the LLM in handling long contexts and intricate behavioral relations. These findings not only validate the efficacy of symbolic graphs in capturing session behaviors but also emphasize the critical role of session search logs in the ranking mechanism.

### Impact of Amount of Training Data

Recent research (Kumar et al., 2017; Li et al., 2018) has highlighted the significant influence of the data volume on downstream tasks, such as document ranking in our scenario. To delve deeper into this aspect and address **RQ4**, we trained our model using varying data proportions. Note that due to computational limitations, we only sampled a portion of the corresponding dataset for training and testing.

In the graph presented on the left in Figure 5, we examine the performance metrics of two models: SGR and COCA, across varying data scales ranging from 2% to 25%. Two key performance indicators, MAP and NDCG@10, were considered. It's evident that our SGR consistently outperforms COCA across all data amounts in both MAP and NDCG@10 metrics. Specifically, SGR outperforms fully-trained COCA with only 10% of training data, which demonstrates the impressive efficiency of the SGR model. The consistent performance elevation of SGR reiterates its robustness and scalability in handling larger datasets, making it a more reliable choice for tasks requiring adaptive search results to data scale variations.

### Performance of SGR in Pre-training Stage

For **RQ5**, while our earlier experiments were primarily centered around our core session search task, it's important to note that our model is initially pre-trained on symbolic learning tasks. Hence, apart from the previous experiments that implicitly demonstrate the effectiveness of the pre-training stages, we here examine directly the performance of SGR in the pre-training phase. On the right of Figure 5, we present the perplexity (PPL) scores during the symbolic learning task. Notably, the PPL exhibits a downward trend, indicating substantial improvements in model training. This observation serves as evidence that SGR excels in comprehending symbolic graph grammar during the pre-training phase, successfully capturing the underlying graph structures.

## 6. Conclusion

In this paper, we introduced the Symbolic Graph Ranker (SGR), a novel approach that combines the strengths of sequential and structural modeling for session searches, using the prowess of Large Language Models (LLMs). By transforming graph data into text through symbolic grammar rules, and implementing self-supervised pre-training tasks, we effectively bridged the gap between traditional session search methods and LLMs. Our results on AOL and Tiangong-ST datasets validated the superiority of SGR, marking a promising step forward in the realm of session search. Moving forward, we aim to further explore the integration of multi-modal data and refine the self-supervised tasks to enhance SGR's adaptability across diverse search environments.

Figure 4. Comparison of MAP and NDCG@3 performance across varying session lengths for COCA, HEXA, and SGR.

Figure 5. (Left) The performance of SGR with different training data amounts. (Right) Perplexity of SGR along the training process.

* Ahmad et al. (2018) Wasi Uddin Ahmad, Kai-Wei Chang, and Hongming Wang. 2018. Multi-Task Learning for Document Ranking and Query Suggestion. In _Proc of ACL_.
* Ahmad et al. (2019) Wasi Uddin Ahmad, Kai-Wei Chang, and Hongming Wang. 2019. Context Attentive Document Ranking and Query Suggestion. In _Proc of SIGIR_.
* Bai et al. (2018) Payal Baiqi, Daniel Campos, Nick Carswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Ti Nguyen, Mir Rosenberg, Xia Song, Alan Slotas, Saurabh Tiwary, and Tong Wang. 2018. MS MARCO: A Human Generated Machine Reading Comprehension Dataset. arXiv:1611.09288 [cs.CL]
* Cai and Shao (2020) Lei Cai and Shiquang S. Zhou. A multi-scale approach for graph link prediction. In _Proc of AAAI_.
* Chen et al. (2023) Haonan Chen, Zheleheng Dou, Qiannan Zhu, Xiaochen Zou, and J-Rong Wen. 2023. Integrating Representation and Interaction for Context-Aware Document Ranking. _ACM Trans. Inf. Syst._ (2023).
* Chen et al. (2022) Haonan Chen, Zheleheng Dou, Yutao Zhu, Zhao Cao, Xiaobua Cheng, and Ji-Rong Wen. 2022. Enhancing User Behavior Sequence Modeling by Generative Tasks for Session Search. In _Proceedings of the 31st ACM International Conference on Information and Knowledge Management_.
* Chen et al. (2002) Haonan Chen, Zheleheng Dou, Yutao Zhu, Zhao Cao, Xiaobua Cheng, and Ji-Rong Wen. 2002. Enhancing User Behavior Sequence Modeling by Generative Tasks for Session Search. In _Proc of CIKM_.
* Chen et al. (2021) Jia Chen, Jiaxia Mao, Yiqin Liu, Han Zhang, Ming Zhang, and Shaoping Ma. 2021. Towards a better understanding of query reformulation behavior in web search. In _Proceedings of the web conference_. 2743-2758.
* Chen et al. (2019) Jia Chen, Jiaxia Mao, Yiqin Liu, Min Zhang, and Shaoping Ma. 2019. TianGong: ST: A new dataset with large-scale refined real-world web search sessions. In _Proc of CIKM_.
* Chen et al. (2020) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In _Proc of ICML_.
* Chen et al. (2022) Xinyang Chen, Hiai Alamro, Minghez Li, Shen Gao, Rui Yan, Xin Gao, and Xiangliang Zhang. 2022. Target-aware Abstractive Related Work Generation with Contrastive Learning. In _Proc of SIGIR_.
* Chen et al. (2022) Xinyang Chen, Minghez Li, Shen Gao, Rui Yan, Gao, and Xiangliang Zhang. 2022. Scientific Paper Extractive Summarization Enhanced by Citation Graphs. In _Proc of EMNLP_.
* Cohen et al. (2020) Arman Cohen, Sergey Feldman, I. Behagy, Doug Downey, and Daniel S Weld. 2020. SPECTER: Document-level Representation Learning using Citation-informed Transformers. In _Proc of ACL_.
* Deehn et al. (2020) Joel Deehn, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2020. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In _Proc of NAACL_.
* Gao et al. (2021) Tianyu Gao, Xingxheng Yao, and Danqi Chen. 2021. SincSE: Simple Contrastive Learning of Sentence Embeddings. In _Proc of EMNLP_.
* Gao et al. (2022) Shiliu Gao, Shuchang Liu, Zubui Yu, Tingjiao Ge, and Yongfeng Zhang. 2022. Recommendation as language processing (tplp): A unified pretrain, personalized prompt 6 k predict paradigm (p5). In _Proceedings of the 16th ACM Conference on Recommender Systems_.
* Han et al. (2021) Xueting Han, Zhenzhun Huang, Hang An, and Jing Bai. 2021. Adaptive transfer learning on graph neural networks. In _Proc of KDD_.
* Hu et al. (2022) Edward Ji, who, Stephen Phillip Wallis, Zeyuan Allen-Zhu, Yuanhui Li, Shen Wang, Lu Wang, and Weihan Chen. 2022. LoRa: Low-Rank Adaptation of Large Language Models. In _Proc of ECLR_.
* Liu et al. (2020) Zimin Hu, Yuxiao Dong, Kunan Wang, Kai-Wei Chang, and Yuhao Sun. 2020. Gpt-gm: Generative tree-training of graph neural networks. In _Proc of KDD_.
* Jiang et al. (2021) Xunqiang Jiang, Yuanli Lu, Yuan Pang, and Chun Shu. 2021. Contrastive pre-training of CNNs on heterogeneous graphs. In _Proc of CIKM_.
* Lee et al. (2020) Jinghyuk Lee, Woonin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chen Ho So, and Jaewoo Kang. 2020. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics (2020).
* Liu et al. (2020) Jimmy Lin, Xingqiang Ma, Sheng-Chieh Lin, Jiheng-Hong Yang, Ronak Pradeep, and Rodrigo Negueria. 2020. Tgym: A Python Toolkit for Reproducible Information Retrieval Research with Sparse and Dense Representations. In _Proc of SIGIR_.
* Liu et al. (2022) Jiduan Liu, Jiahao Liu, Yang Yang, Jingyang Wang, Wei Wu, Dongyan Zhao, and Rui Yan. 2022. GSN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval. arXiv:2204.08214 [cs.NI]
* Loshi and Hutter (2017) Ilya Loshi and Frank Hutter. 2017. Fixing Weight Decay Regularization in Adam. arXiv (2017).
* Liu et al. (2021) Yuanfu Liu, Xingang Jiang, Yuan Fang, and Chuan Shi. 2021. Learning to pre-train graph neural networks. In _Proc of AAAI_.
* Ma et al. (2023) Shengjie Ma, Chong Chen, Jiuxin Liu, Mao Qi, Tian, and Xului Jiang. 2023. Session Search with Pre-Trained Graph Classification Model.
* Niu et al. (2022) Guanghui Niu, Bo Li, Yongfei Zhang, and Shiliang Pu. 2022. CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion. In _Proc of ACL_. ACM.
* Nogueira et al. (2020) Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020. Document Ranking with a Pretrained Sequence-to-Sequence Model. In _Proc of EMNLP_.
* Pan et al. (2023) Liangming Pan, Alon Albak, Xinyi Wang, and William Yang Wang. 2023. Logic-Inform: Improving large language models with symbolic solvers for faithful logical reasoning. _arXiv preprint arXiv:2303.12295_ (2023).
* Pust et al. (2006) Garg Pust, Abdu Chowdhury, and Cayley Tergonen. 2006. A picture of search. In _Proceedings of the 17 international conference on Scalable information systems_.
* Paszke et al. (2017) Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Tang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in pytorch. (2017).
* Qin et al. (2023) Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhang, Junru Wu, Jianing Shen, Tianqi Liu, Joul Liu, Donald Metzler, Xunhui Wang, and Michael Isenhericky. 2023. Large Language Models as Effective Text Runkers with Pairwise Ranking. Prompting. arXiv:2306.17558 [cs.NI]
* Qiu et al. (2020) Jiaom Chun, Ericheng Dou, Yong Zhu, Jing Zhang, Hongxing Yang, Ming Ding, Kaunan Wang, and Jie Tang. 2020. Gcc: Graph contrastive coding for graph neural network pretraining. In _Proc of KDD_.
* Qi et al. (2020) Chen Qu, Chenyan Xiong, Tufie Zhang, Conby Rosset, W Bruce Croft, and Paul Bennett. 2020. Contextual Re-Ranking with Behavior Aware Transformers. In _Proc of SIGIR_.
* Robertson et al. (2009) Stephen Robertson, Hugo Tang Zangouei, et al. 2009. The probabilistic relevance framework RIM25 and beyond. _Foundations and Trends in Information Retrieval_ (2009).
* Rosset et al. (2009) Corbin Rosset, Chenyan Xiong, Xia Song, Daniel Campos, Nick Carswell, Saurabh Tiwary, and Paul Bennett. 2020. Leading conversational search by suggesting useful questions. In _Proceedings of the workshop on conference_. 2020. 1106-1170.
* Shen et al. (2007) Xuebin Shen, Bin Tan, and Chengxiang Zhai. 2005. Context-Sensitive Information Retrieval Using Implicit Feedback. In _Proc of SIGIR_.
* Ni et al. (2008) Weiwei Ni, Lingyong Han, Xinyu Ma, Pengfei Ben, Dawei Yin, and Zhaochun Ren. 2023. Is CHATCOT Gord: Search? Investigating Large Language Models as Re-Ranking Agent. arXiv:2009.04592 [cs.CL]
* Tian et al. (2023) Shuo Tian, Qiao Jin, Liang Yeguo, Yong Lu, Qingping Zhu, Xiuying Chen, Yifan Yang, Qingya Chen, Wen Kim, Donald C Comeau, et al. 2023. Opportunities and Challenges for Chattap and Large Language Models in Biomedicine and Health. _arXiv preprint arXiv:2306.10070_ (2023).
* Tian et al. (2021) Zhiliang Tian, Wei, Zi Han Zhang, Dongjolu Lee, Yiping Song, and Nevin I. Zhang. 2021. Learning from friends: few-shot personalized conversation systems via social networks. In _Proc of AAAI_.
* Tu et al. (2020) Hsige Tuoru Tuoru, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yangmine Rabei, Nikolay Rashidov, Soumya Batra, Pajaylal Bhargava, Shruti Bhosek, Daan Binkel, Lukas Belcher, Christian Canton Ferrer, Moya Chen, Guillem Cucurull, David Estoo, Judm Fernandez, Jeremy Peru W. Hu, Brian Fuller, Cynthia Gao, Vedunjour, Naiman Goyal, Anthony Hartmann, Saghur Hassett, Rui Hou, Hulan Iman, Marcia Karbas, Videh Kerre, Matiu Khosha, Isabel Khosjunan, Artem Korenev, Puri Singh Koura, Marie-Amee Lachura, Tishant Levi, Jervea Dime, Ilalovin Li, Yinguo Mao, Xavier Martinet, Todd Mithaylov, Pushar Mishra, Mollo Pidov, Yilin Nie, Andrew Poulton, Jeremy Retenstein, Raish Rungta, Kaban Saladi, Alan Schelten, Ruan Silva, Eric Michel Smith, Ruan Shouham, Ruan Shouyuan, Xiaoqing Elen Tan, Hui, Jingx Ros Taylor, Adrian Williams, Jian Xiang, Runia Yu, Zheng Zang, Tifan Xiong, Yuchang Zhang, Angela Han, Meiname Kambarburu, Sharan Narang, Anreelin Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialon. 2023. 12023 [cs.CL]
* Yang and de Rijke (2018) Christophe Yang and Marcine de Rijke. 2018. Dytree_eval: An extremely fast python interface to eee, english for _SIGIR_.
* Wang et al. (2023) Shuang Wang, Zixheng Duo, and Yutao Zhu. 2023. Heterogeneous Graph-Based Context-Aware Document Ranking. In _Proc of WSDM_.
* White et al. (2010) Ryen W. White, Paul N. Bennett, and Susan T. Domain. 2010. Predicting Short-Term Interests Using Activity-Based Search Context. In _Proc of CIKM_.
* Wu et al. (2023) Likan Wu, Zhaopei Qiu, Zhi Zheng, Henghu Zhu, and Enhong Chen. 2023. Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations. arXiv:2307.057225 [cs.AI]
* Xiang et al. (2010) Miao Xiang, Davin Jiang, Jun Pei, Xiaohui Sun, Enhong Chen, and Hang Li. 2010. Context-aware Ranking in Web Search. In _Proc of SIGIR_.
* Zheng et al. (2023) Han Xie, Da Zheng, Jin Ma, Hongyang, Yassili N Ioannidis, Xiang Song, Qing Ping, Sheng Wang, Carl Yang, Yiq, Yin Xu, et al. 2023. Graph-Aware Language Model for Pre-Training on a Large Graph Corpus. In _Proceedings of WSDM_.
* Xing et al. (2023) Aryun Yang, Bin Xiao, Bingqing Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Hongzhu Zhang, Huilin, Jiang, N. Liu, Yao Zhang, Rui Li, Liu,Open Large-scale Language Models. arXiv:2309.10305 [cs.CL].
* Zhang and Chen (2018) Muhan Zhang and Yixin Chen. 2018. Link prediction based on graph neural networks. _Proc of NeurIPS_ (2018).
* Zhang et al. (2022) Tong Zhang, Congregt Qiu, Wei Ke, Sabine Sinstrunk, and Mathein Salzmann. 2022. Leverage your local and global representations: A new self-supervised learning strategy. In _Proc of CVPR_.
* Zhang et al. (2019) Yuan Zhang, Dong Wang, and Yan Zhang. 2019. Neural ID meets graph embedding: A ranking model for product search. In _The World Wide Web Conference_. 2390-2400.
* Zhang et al. (2023) Haiting Zhang, Shenghao Liu, Chang Ma, Hannan Xu, Jie Fu, Zhi-Hong Deng, Lingpeng Kong, and Qi Liu. 2023. GMLT: A Unified Graph-Text Model for Instruction-Based Molecular Zero-Shot Learning. arXiv:2306.13089 [cs.LG].
* Zhou et al. (2023) Jewiez Zhou, Xiaoxan He, Liwen Sun, Jiannan Xu, Xiuying Chen, Yuen Lu, Longei Zhou, Xingyu Liao, Bin Zhang, and Xin Gao. 2023. SkinGFT+: An Interactive Dermatology Diagnostic System with Visual Large-Language Model.
* Zhu et al. (2023) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minggrid-E: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2301.10592_ (2023).
* Zhu et al. (2021) Jason Zhu, Yanling Cui, Yuanjing Lin, Hao Sun, Xue Li, Markus Poleyer, Tianqiang, Lingzie Zhang, Roriei Zhang, and Huang Zhao. 2021. TextGNN: Improving Text Encoder via Graph Neural Network in Sponsored Search. In _Proc of WWW_.
* Zhu et al. (2018) Yutao Zhu, Jian-Yun Nie, Zhicheng Dou, Zhenqiyu Ma, Xiuying Zhang, Pan Du, Xiaochen Zhu, and Hao Jiang. 2021. Contrastive Learning of User Behavior Sequence for Context-Aware Document Ranking. In _Proceedings of the 30th ACM International Conference on Information and Knowledge Management_.
* Zhu et al. (2018) Yutao Zhu, Jian-Yun Nie, Zhicheng Dou, Zhenqiyu Ma, Xiuying Zhang, Pan Du, Xiaochen Zhuo, and Hao Jiang. 2021. Contrastive learning of user behavior sequence for context-aware document ranking. In _Proc of CIKM_.
* Zuo et al. (2022) Xiaochen Zuo, Zhicheng Dou, and J-Rong Wen. 2022. Improving Session Search by Modeling Multi-Granularity Historical Query Change. In _Proc of WSDM_.