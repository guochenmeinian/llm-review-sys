# Fast Inference of Removal-Based Node Influence

Anonymous Author(s)

Submission Id: 434

###### Abstract.

Graph neural networks (GNNs) have been widely utilized to capture the underlying information propagation patterns in graph-structured data. While remarkable performance has been achieved in extensive classification tasks, there comes a new trending topic of identifying influential nodes on graphs. This paper investigates a new yet practical problem of evaluating the influence of node existence itself, which aims to efficiently measure the overall changes in the outputs of a trained GNN model caused by removing a node. A realistic example is, "Under a task of predicting Twitter accounts' polarity, had a particular account not appeared, how might others' polarity be changed?". A straightforward way to obtain the node influence is to alternately calculate the influence of removing each node, which is reliable but time-consuming. The related lines of work, such as graph adversarial attack and counterfactual explanation, cannot directly satisfy our needs since they typically suffer from low efficiency on large graphs. Besides, they cannot individually evaluate the removal influence of each node. To upgrade the efficiency, we design an efficient algorithm, **NOde**-**R**emoval-based fA**st GNN inference (**NORA**), which uses the gradient of the neural networks to approximate the node-removal results. It only costs one forward propagation and one backpropagation to approximate the influence score for all nodes. Extensive experiments are conducted on six benchmark datasets, where NORA exceeds the compared methods. Our code is available at [https://anonymous.4open.science/r/NORA](https://anonymous.4open.science/r/NORA).

## 1. Introduction

In recent years, the booming development of big data has brought about many relational data, that can be naturally represented as graphs. Evaluating node influence and identifying influential nodes on a graph has become a trending and beneficial topic (Golov et al., 2013). It can help with viral advertising (Golov et al., 2013; He et al., 2016; Li et al., 2017), online news dissemination (Golov et al., 2013; Li et al., 2017), police breaking down a criminal network (Golov et al., 2013), pandemic control (Golov et al., 2013; Li et al., 2017), etc. A lot of research on the "influence maximization" problem (Golov et al., 2013; He et al., 2016; Li et al., 2017; Li et al., 2017; Li et al., 2017; Li et al., 2017; Li et al., 2017; Li et al., 2017) focus on identifying influential nodes whose triggered influence spreading range can be maximized. These works can answer the question: "Which Twitter accounts post information that can spread to the greatest amount of audiences?"

Yet, another question is under-explored: "If a Twitter account had never appeared, how could other Twitter users' opinions/interactions (e.g. following, retweeting, and replying) have been?", such as the example we illustrate in Figure 1. Actually, studying the influence of node removal can benefit many real-world applications including finding the bottlenecks and improving the infrastructure network robustness (Li et al., 2017; Li et al., 2017), modeling how vaccination can decrease virus spreading (Golov et al., 2013; Li et al., 2017; Li et al., 2017) and figuring out the top scientists contribute to knowledge spreading based on a science co-authorship network (Golov et al., 2013; Li et al., 2017; Li et al., 2017). A lot of research on the "network dismantling" problem(Golov et al., 2013; Li et al., 2017; Li et al., 2017; Li et al., 2017; Li et al., 2017) have studied the structural influence of node removal. However, the task-specific influence of node removal considering both attributes and structures has been under-explored. Therefore, we focus on measuring the influence of node existence itself by evaluating the task-specific influence of node removal.

Graph neural networks (GNNs) are among the most powerful graph representation learning tools. Different from research on the "influence maximization" problem that uses a propagation model to simulate node influence spreading range, we use GNNs as a surrogate to capture the information propagation patterns. Propagation models cannot evaluate the influence of node removal, but it is not the case for GNNs. Based on the message-passing nature of GNN (Golov et al., 2013), we assume that a trained GNN model can capture the propagation patterns of a graph. After removing a node, we can use a pre-trained GNN's new outputs to simulate the scenario if that removed node had not existed based on the learned propagation patterns. For node classification task, it simulates what other node labels could have been; for link prediction task, what the connections could have been; for graph classification task, what the graph label could have been. We calculate the influence of node removal as the total variation distance between the original outputs and new outputs of the trained GNN model, which is illustrated in Figure 2.

We aim to calculate the influence score for each node. However, brutal-force direct calculation is very time-consuming, so we demand an efficient method. Our method that changes GNN predictions by changing its input graph structure is similar to some common practice in graph adversarial attack and graph counterfactual explanation, though we aim at a different problem setting. Graph adversarial attack aims to maximally undermine GNN model performance or change GNN predictions by inducing unnoticeable perturbations. The perturbations mainly include modifying node

Figure 1. An example of a possible node-removal scenario on a social network. Red versus Blue represents two different opinions. Color shade represents how firm a userâ€™s stance is.

features (Shi et al., 2017; Wang et al., 2017; Wang et al., 2018), injecting new nodes (Brock et al., 2018; Chen et al., 2018; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018), or modifying edges (Brock et al., 2018; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018). To the best of our knowledge, none of the adversarial attack methods utilizes node removal, since it is impractical in real-world attacking scenarios.

Graph counterfactual explanation aims to explain a GNN's prediction of a target node/edge/graph by finding the minimum perturbation on the input graph that can change the prediction of the target (Shi et al., 2017). They do utilize node removal (Shi et al., 2017; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018), However, directly using these methods to evaluate node influence confronts two difficulties. First, we evaluate the influence of removing a particular node on all other nodes/edges, while graph counterfactual explanation evaluates the influence of removing many nodes and edges on a single target. Second, graph counterfactual explanation strategies are not typically good at scaling up to handle large graphs. Most of the existing graph-level classification datasets contain a lot of small graphs, such as molecules. For node classification and link prediction tasks, some models only need to consider the computation graph of a targeted node/edge, which is also small. Most of the existing works mentioned above only conduct experiments on graphs with less than 4,000 nodes. LARA (Lara et al., 2018) designs a scalable model to predict the influence of surrounding nodes on the target node, but it requires the time consuming labeling of the ground truth, thus it is efficient in space but not in time. Our method is much faster.

The node influence measurement problem we proposed has not been studied yet, and related lines of work cannot directly satisfy our demands. To efficiently calculate the node influence score, we use the gradient to approximate the influence based on the first-order derivatives and heuristics. We propose the algorithm, **N**ode-**R**emoual-based \(\)**Ast GNN Inference (**NORA**), that only needs one forward propagation and one backpropagation to approximate the removal influence for all nodes. Since we are studying a new problem without mature baselines, we adapt two approaches in graph counterfactual explanation as supplementary baselines to this problem. We conduct extensive experiments on six datasets. The experimental results demonstrate the effectiveness and efficiency of NORA. To sum up, this paper makes the following contributions:

* We propose a novel perspective of evaluating node influence based on node removal and a pre-trained GNN.
* We propose an efficient and effective algorithm, NORA, to approximate the removal influence for all nodes.
* Experimental results on six datasets demonstrate that NORA outperforms the baselines on performance and efficiency.

## 2. Related Work

### Graph Adversarial Attack

Graph adversarial attack aims to undermine GNN performance or change GNN predictions by imposing a small perturbation to the graph within a limited budget. Zugner et al. (Zugner et al., 2017; Wang et al., 2018) started the race of graph adversarial attacks. Pioneering works are mainly based on modifying node features (Shi et al., 2017; Wang et al., 2018; Wang et al., 2018) and perturbing edges (Brock et al., 2018; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018), including adding, removing, and rewiring edges. Some recent works (Brock et al., 2018; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) study the node injection attack, which inspects some nodes into the graph and connects them with some existing nodes. Among them, Chen et al. (Chen et al., 2018) prove that the node injection attack can theoretically cause more damage than the graph modification attack with less or equal modification budget. G-NIA model (Wang et al., 2018) sets a strong limitation that the attacker can only inject one node with one edge, and it achieves more than 90% successful rate in the single-target attack on Reddit and ogbn-products datasets. They demonstrate the strong potential of altering nodes' existence. To the best of our knowledge, none of the adversarial attack methods considers node removal, since it is impractical in real-world applications. Nonetheless, as our target is to analyze node influence instead of attacking, node removal is worth exploring.

### Graph Counterfactual Explanation

Graph counterfactual explanation explains why a GNN model gives a particular result. Such as, to explain the GNN prediction of a target node in the node classification task, a target edge in the link prediction task, or a target graph in the graph classification task. The explanation is provided by finding the minimum perturbation on the input graph that can change the prediction of the target. There are some methods (Brock et al., 2018; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018) based purely on edge removal. Some methods utilize both node removal and edge removal by optimizing mask matrices (Wang et al., 2018; Wang et al., 2018), predicting node influence with neural network (Lara et al., 2018), applying graph generation models (Wang et al., 2018; Wang et al., 2018), or searching for an optimal neighbor graph (Wang et al., 2018; Wang et al., 2018). As analyzed in Section 1, these methods are not directly applicable to evaluating the proposed node influence, so we adapt two famous methods as supplementary baselines to this novel problem. CF-GNNExplainer (Lara et al., 2018) optimizes a real-value mask matrix that multiplies the adjacency matrix during training, and elements in the mask matrix must be within range \(\). During inference, elements below 0.5 indicate edge removal. We adapt it to also consider node removal with a node mask matrix. Optimizing the mask matrix is a very common practice (Brock et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018), so we use CF-GNNExplainer as a baseline. As discussed in Section 1, most graph explanation methods are

Figure 2. Our schema of evaluating node influence based on node removal. The GNN model is trained on the original graph, then we remove a node and apply the trained GNN to the new graph structure. We measure the total variation distance between the originally predicted and newly predicted distributions of node/edge/graph classes.

not scalable. To solve the problem, a recent work, LARA (Yang et al., 2019), uses a GNN to predict node influence, whose parameter size does not grow with the input graph size. We adapt it as our second baseline.

### Network Dismantling

Network dismantling studies the structural influence of node removal on unattributed graphs. It aims to maximally decrease network connectivity by analyzing by removing influential nodes. The influence is usually evaluated by the network connectivity, such as the size of the largest connected component, efficiency (i.e. the average of the reciprocals of shortest path lengths of all node pairs), etc (Zhou et al., 2017). Betweenness centrality is one of the most widely-used methods in the network dismantling problem setting to measure node influence (Zhou et al., 2017; Li et al., 2018; Wang et al., 2019; Wang et al., 2019). It is the ratio of shortest paths that pass through a node among all shortest paths between all node pairs. We use it as a simple baseline in our experiments.

## 3. Problem Definition

### Notations

A graph \(G=(V,E)\) consists of \(N\) nodes \(V=\{v_{1},v_{2},...,v_{N}\}\) and edges \(E=\{e_{ij}|j(i)\}\), where \((i)\) denotes the neighbor nodes of \(v_{i}\) (without \(v_{i}\)), and \(e_{ij}\) denotes the edge from \(v_{i}\) to \(v_{j}\). \((i)\) denotes neighbor nodes of \(v_{i}\) plus \(v_{i}\) itself. \(A\) denotes the adjacency matrix. Each node \(v_{i}\) is associated with a feature vector \(x_{i}^{d}\), and a label \(y_{i}\) if node classification task is applicable. We denote the degree of \(y_{i}\) as \(d_{i}=|(i)|\). When we remove node \(v_{r}\) (\(r\{1,2,,N\}\)) and all edges connected with \(v_{r}\), from graph \(G\), we denote the new graph as \(G_{-v_{r}}\), \(g_{}\) denotes a trained GNN model. We denote as \(v_{r}\) the target node to analyze removal influence, and \(_{g_{}}(v_{r})\) denotes the proposed influence.

Graph neural networks (GNNs) generally follow the message-passing framework (Gil et al., 2017). A GNN model consists of multiple graph convolutional layers. In a typical graph convolutional layer, a node updates its representation by aggregating its neighbor nodes' representations:

\[h_{i}^{(I)}=U_{I}(h_{i}^{(I-1)},(_{j(i)} _{l}(h_{j}^{(I-1)},h_{i}^{(I-1)}))), \]

where \(h_{i}^{(I)}\) denotes the node \(v_{i}\)'s representation after passing the \(l\)-th layer (\(l 1,2,\)), and \(h_{i}^{(0)}\) denotes the input features. \(_{l}\) is the message function, AGG is the aggregation function, and \(U_{l}\) is the update function.

### Problem Definition

In order to evaluate the change of node removal, we use GNN models as a surrogate to predict the scenario where the removed node does not exist based on the existing propagation patterns. GNN models typically make prediction on a node label, edge existence, or graph class, via transforming its output to a vector, which denotes the probability of different classes or options. In general, we measure the change of prediction by the \(t_{1}\)-norm of the difference between the original and updated probability vectors. The difference can equally capture the prediction change for every class. For graph classification, we directly use the prediction change. For node classification and link prediction, we evaluate the sum of the prediction change of all remaining nodes/edges.

**Definition 1.** (**Node Influence in Node Classification Task**): Given a node classification model \(g_{}\) trained on graph \(G\), we denote its prediction of node \(v_{i}\) as \(g_{}(G)_{i}\). The influence of removing node \(v_{r} V\) is calculated as:

\[_{g_{}}(v_{r})=_{i=1,l r}^{N}\|f(g_{}(G)_{i})-f(g_ {}(G_{-v_{r}})_{i})\|_{1}, \]

where \(f()\) is the optional MLP layers and softmax layer that transform a GNN's output to the probabilistic vector.

**Definition 2.** (**Node Influence in Link Prediction Task**): Given a link prediction model \(g_{}\) trained on graph \(G\), we denote its prediction of edge \(e_{ij}\) as \(g_{}(G)_{e_{ij}}\). We use \(D_{r}\) to denote the whole link prediction set, and \(D_{r}\) to denote edges that link \(v_{r}\). The influence of removing node \(v_{r} V\) is calculated as:

\[_{g_{}}(v_{r})=_{e_{ij} D_{r}-D_{r}}\|f(g_{}(G)_{e_ {ij}})-f(g_{}(G_{-v_{r}})_{e_{ij}})\|_{1}, \]

The definition can be similarly generalized to the graph classification task, where we simply take \(\|f(g_{}(G))-f(g_{}(G_{-v_{r}}))\|_{1}\).

The ground truth is generated by the brute-force algorithm, where we alternatively remove each node from the original graph one at a time, and calculate the node influence. Iterating through all nodes causes short efficiency. One intuitive way for acceleration is neighborhood sampling. If the GNN has \(l\) layers, removing a node will only affect the outputs of its \(l\)-hop neighborhood, and computing their new outputs will only require a \(2l\)-hop neighborhood. However, it is still time-consuming, especially on dense-connected graphs, (e.g., ogbn-arxiv and two Twitter datasets in our experiments) where \(2l\)-hop neighborhoods might already contain most of the nodes. Therefore, we need to look for an efficient and effective method to calculate the influence score.

## 4. Methods

To upgrade the efficiency, we propose **Node-Removal-**S**ast **G**NN **I**nference **(**NORA**) algorithm. In general, we approximate the influence of the single-node-removal process by decomposing the calculation process into three parts, which correspond to three parts of changes caused by the node removal. Figure 3 illustrates the three parts. We will describe our approximation algorithm in detail in the following subsections.

### Influence Score Calculation Decomposition

The general idea of NORA is that we approximate the influence of node removal via first-order derivatives. We only need the gradient information from one backpropagation to approximately calculate

Figure 3. The influence calculation decomposition of our method.

the influence scores for all nodes.Our method could be applied to node classification and link prediction tasks, and can also be extended to the graph classification task. In general, our method could be adapted to various downstream tasks of a GNN model.

Equation 1 illustrates a message-passing GNN layer. A typical parameterization of it is:

\[h_{i}^{(l)}=(W_{u}^{(l)}(W_{s}^{(l)}h_{i}^{(l-1)}+_{j N(i)}_ {ji}W_{m}^{(l)}h_{j}^{(l-1)})), \]

where \(\) denotes the activation function, \(W_{u}^{(l)}\), \(W_{s}^{(l)}\), and \(W_{m}^{(l)}\) are model parameters. \(_{ji}\) is the edge normalization of messages coming from \(v_{i}\)'s neighbors and is usually related to node degree or attention mechanism, e.g., \(_{ji}=1/\) in GCN (Kipf and Welling, 2017). Suppose the GNN model has \(L\) layers, the last layer output \(g_{}(G)_{i}=h_{i}^{(L)} R^{e}\) is the predicted class probability, where \(e\) is the number of classes.

We cannot directly calculate the first-order derivatives based on Equation 2, since there is a 1-norm. However, intuitively, removing a node usually causes consistent change to the class of other nodes, e.g., raising the probability of a particular class for all nodes. Therefore, we can rewrite the formula. We denote as \(f_{r}=_{i=1,i r}^{N}h_{i}^{(L)}\) the sum of all node predictions except for node \(v_{r}\), and we denote as \( f_{r}\) the change of \(f_{r}\) when removing node \(v_{r}\).

Lemma 1 ().: _If removing \(v_{r}\) consistently changes the class distribution of other nodes, the influence defined in Equation 2 equals:_

\[||_{i=1,i r}^{N}g_{}(G)_{i}-_{i=1,i r}^{N}g _{}(G_{-v_{i}})_{i}||_{1}=|| f_{r}||_{1}\] \[=||_{f r} h_{i}^{( L)}||_{1}=||_{f r}}{ h_{i}^{(L)}} h_{i}^{(L)}||_{1}. \]

Though the second line contains the derivative symbol, it is strictly equal because \(}{ h_{i}^{(L)}}=1\). We write it in this form because we want to keep a uniform form with later formulas. We can extend this form from the last layer's formula to the frontmost layer. Here we analyze how to extend it from the \(L\)-th layer to the \((L-1)\)-th layer. Since the 1-norm is difficult to compute, we first ignore it and just approximate \( f_{r}\).

In a typical GNN layer in Equation 4, the model parameters are fixed during inference, but \(_{ji}\) and \(h_{j}^{(L-1)}\) might change due to removing \(v_{r}\). Therefore, we can approximate \( h_{i}^{(L)}\) with the first-order derivatives:

\[ h_{i}^{(L)}-I(v_{r} N(i))^{( L)}}{ h_{i}^{(L-1)}}h_{r}^{(L-1)}\] \[+_{j(i),j r}(^{(L)}}{  x_{ji}} x_{ji}+^{(L)}}{ h_{i}^{(L- 1)}} h_{j}^{(L-1)}), \]

where \(I(.)\) is the indicator function. Then by combining the above formula with the definition of \( f_{r}\), we can derive the following formula.

Lemma 2 ().: _We can approximate \( f_{r}\) for the GNN model described in Equation 4 with a second-order error term as:_

\[ f_{r}-_{i N(r)}}{ h_{i}^{(L)} }^{(L)}}{ h_{r}^{(L-1)}}h_{r}^{(L-1)}+_{i r }_{j(i),j r}\]

\[(}{ h_{i}^{(L)}}^{(L)}}{  x_{ji}} x_{ji})+_{i r}_{j(i),j r} _{j(i),j r}(}{ h_{i}^{(L)}} ^{(L)}}{ h_{i}^{(L-1)}} h_{j}^{(L-1)}). \]

The error term is in the second order because we are using the first-order derivatives to approximate. We now decompose the calculation into three terms, divided by "+" in the above formula. The first term measures the direct influence of the disappearance of \(v_{r}\)'s latent representations, which decreases an input to its neighbor node; The second term measures the change of its neighbor's edge normalization term \(_{ji}\); and the third term measures the change of other nodes' latent representations, which will influence further neighbors. The three terms correspond to the three kinds of influence in Figure 3.

### Approximation of Each Decomposed Term Term 1: Direct impact to the neighbors

For clarity, the first term refers to the portion between the first minus sign and the first plus sign in Equation 7. To begin with, by applying the chain rule, the first term equals to:

\[}{ h_{r}^{(L-1)}}h_{r}^{(L-1)}- }{ h_{r}^{(L)}}^{(L)}}{ h_{r}^{(L-1)}}h_{r }^{(L-1)}. \]

The derived equation consists of two parts. The form of the first part is simpler and more convenient to handle, so we want to eliminate the second part and only keep the first part. We do this by approximating the ratio of the second part to the first part. Here we make a rough assumption that every node is equal, which means they have the same number of neighbors, the same node representation, and the same gradient. We denote the change of node representation, \( h_{r}^{(L-1)}\), \( j V\), as \( h\). We denote the gradient coming from a neighbor node as \(g\), and the gradient coming from the higher-layer representation of a node itself as \( g\). \(\) is typically higher than 1, because self-loop and residual connection make the gradient coming from the higher-layer representation of a node itself larger than the gradient from the higher-layer representation of neighbor nodes. Therefore, the first part of Equation 8 is \((d_{r}+)g h\), and the second part is \( g h\). Based on their ratio, and by rewriting the enumeration variable \(j\) as \(i\), we derive the following equation.

Lemma 3 ().: _If every node in the graph has equal structures and attributes, the first term of Equation 7 equals:_

\[}{d_{r}+}}{ h_{r}^{(L-1)}}h_{r}^{(L -1)}. \]

In our experiments, we find that the most effective way of calculating \(}{ h_{r}^{(L-1)}}h_{r}^{(L-1)}\) is to change it to \(||}{ h_{r}^{(L-1)}} h_{r}^{(L-1)}||_{2}\). \(\) means elementwise product between the two same-dimensional vectors, and \(||.||_{2}\) means the 2-norm.

**Term 2: Aggregation term change..** In the second term of Equation 7, \(}{ h_{i}^{(L-1)}}^{(L-1)}}{ a _{ji}}=}{ a_{ji}}\). We have tried using \(}{ a_{ji}}\) but it didn't perform well, so we only consider approximating \( a_{ji}\). Then we analyze \( a_{ji}\). Unlike the first term, \( a_{ji}\) greatly depends on the design of the specific GNN model. Some GNN models, e.g., GCN (Kipf and Welling, 2017) and GraphSAGE (Hamilton et al., 2017), only use structural information like node degree, while some models, e.g., GAT (Velickovic et al., 2017) and DrGCN (Wang et al., 2018), uses the attention mechanism. To reach a flexible and universally adaptable approximation, we use structural measurement. We consider two widely-used GNNs: GCN (Kipf and Welling, 2017) and GraphSAGE (Hamilton et al., 2017). The edge normalization of GCN is \(_{ji}=1/\), and that of GraphSAGE is \(_{ji}=1/|N(i)|\). If neither \(v_{i}\) nor \(v_{j}\) is \(v_{r}\)'s neighbor, \(_{ji}\) of GCN and GraphSAGE does not change.

If \(v_{i}\) or \(v_{j}\) is a neighbor of \(v_{r}\), we combine the fashion of GCN and GraphSAGE to approximate \( a_{ji}\). We denote the degree of node \(v_{i}\) as \(d_{i}=|N(i)|\). Suppose \(v_{i}\) is \(v_{r}\)'s neighbor, and \(v_{j}\) is \(v_{i}\)'s neighbor, we approximate \( a_{ji}\) by \( a_{ji}\):

\[ a_{ji}=[k_{1}(-1}}-}})+(1-k_{1})(-1}-})]\] \[[k_{2}}}+(1-k_{2})}], \]

where \(k_{1}\) and \(k_{2}\) are hyper-parameters ranging in . An interesting intuition is that there exist hyper-parameters \(k_{1}\) and \(k_{2}\) that make \( a_{ji}\) equal to \( a_{ji}\) for GCN. Based on \( a_{ji}\), we approximate the second term as:

\[ Topo_{r}=_{i N(r)}_{j N(i)} a_{ji}. \]

**Term 3: Hidden representation change.** Using the chain rule to analyze \(}{ h_{i}^{(L-1)}}\), we can simplify the third term. The third term in Equation 7 equals Equation 12, which can be further equally transformed into Equation 13.

\[_{j r}(}{ h_{j}^{(L-1)}}-}{ h_{r}^{(L)}}^{(L)}}{ h_{ j}^{(L-1)}}) h_{j}^{(L-1)} \] \[=_{j r}}{ h_{j}^{(L-1)}} h _{j}^{(L-1)}-_{j N(r)}}{ h_{r}^{(L)}} { h_{r}^{(L)}}{ h_{j}^{(L-1)}} h_{j}^{(L-1)}. \]

Similar to the simplification process of the first term, here we also arrive at a formula with two parts. The form of the first part is more convenient to handle, and it takes the same form as Equation 5, so we want to eliminate the second part and only keep the first part. We make the same rough assumption that every node is equal. Equation 8 as below. We denote the average node degree as \(d\). Using the notations from the simplification process of the first term, we can approximate the first part of the third term (Equation 12) as \((N=1)(d+)g h\), and the second part as \(dg h\). Based on their ratio, and by rewriting the enumeration variable \(j\) as \(i\), we derive the following equation.

Lemma 4 ().: _If every node in the graph has equal structures and attributes, the third term of Equation 7 equals:_

\[(_{j r}}{ h_{j}^{(L-1)}} h_{i}^{(L-1 )})(1-). \]

We use this equation to approximate the third term. Its algebraic form is similar to Equation 5, so the third term can successfully extend the formula to previous layers.

### Combined Derivation

By combining the approximations of three terms together, we get:

\[ f_{r} (_{i r}}{ h_{i}^{(L-1) }} h_{i}^{(L-1)})(1-)+ topo_{r}\] \[-}{d_{r}+}||}{ h_{r}^{( L-1)}} h_{r}^{(L-1)}||_{2}. \]

Now we successfully extend the original formula to a fronter layer. By repeating this process, we can approximate \( f_{r}\) by the gradient from every layer. Our original goal in Equation 5 is the 1-norm of \( f_{r}\). However, it is difficult to approximate via gradient. Instead, we calculate the sum of the square of each element in \( f_{r}\), which is highly positively correlated with its 1-norm. Based on the first-order derivative, we approximate it as:

\[(|| f_{r}||_{2})^{2} f f_{r}, \]

where \(\) is the dot product. Based on it and by extending Equation 15 to all previous layers, we derive:

\[_{ g}(v_{r}) f\{_{i=0}^{L-1}[(s(1-))^{(L-1-i)}}{d_{r}+}\] \[||}{ h_{r}^{(i)}} h_{r}^{(i)}||_{ 2})+k_{3} L Topo_{r}\}. \]

In the formula, \(h_{i}^{(0)}\) is the input feature of \(v_{i}\). Since our derivation is from the back layer to the front layer, approximation error might accumulate. To eliminate this issue, we add an additional decay term \(s\) to reduce the weight of frontier layers. \(s\) usually falls in \([0,9,1.0]\). Since each layer generates a \( Topo_{r}\) term, we multiply it by the number of layers \(L\).

However, Equation 17 is still not efficient. It needs to backpropagate \(f_{r}\) to acquire the approximation for node \(v_{r}\), but we want to simultaneously generate the approximation results for all nodes. In the standard way, when we are backpropagating \(f_{r}\), we set the loss of every node \(v_{i} V\) as \(f_{r}\), so that we can accurately get \(}{ h_{r}^{(i)}}\). To upgrade the efficiency, We relax this restriction and set the loss of node \(v_{i} V\) as \(f_{r}\), allowing for each node to backpropagate a different loss. In this way, we can backpropagate them simultaneously. When we are approximating the influence of removing node \(v_{r}\), we not only base on \(f_{r}\) but also on \(f_{r}\), \(i r\), so it downgrades the performance. However, \(f_{r}\) still has a dominant influence on the gradient of \(v_{r}\)'s hidden representations, because self-loop and residual connections are stronger than normal edges. The experimental results show a satisfactory performance, so the accuracy drop is tolerable, with a huge gain in time efficiency. In this way, we can generate the approximation for all nodes simultaneously. It only takes a few seconds to complete the computation.

The approximation of the link prediction task is similar. We just replace \(f_{r}\) with the sum of edge predictions which are not connected with \(_{r}\). The other processes during derivation are the same.

### Complexity Analysis

Here we analyze the time and space complexity of the ground truth and the proposed method. We use \(N\) to denote the number of nodes, \(M\) to denote the number of edges, \(L\) to represent the number of the GNN's layers, \(h\) to represent the hidden size of the GNN model, and \(d\) to represent the average node degree. In most cases, the adjacency matrix is sparsely stored, and in this situation, according to Paper (Brockman et al., 2017), the time complexity of the forward propagation or backpropagation of a common message-passing GNN model is \(O(LNh^{2}+LMh)\), and the space complexity is \(O(M+LH^{2}+LNH)\). Based on it, we list the time and space complexities in Table 1.

We list the detailed computation of these time and space complexity in the appendix A.2. As shown in Table 1, NORA cost significantly less time than the brute-force method, and basically the same space complexity as the brute-force method. Therefore, it is generalizable to very large real-world graphs when considering time. For example, it takes about 41 hours to generate the ground truth influence scores for DrGAT model on the ogbn-arxiv dataset, but it only takes a few seconds by NORA. When considering space, since they have the same space complexity as the GNN model, the bottleneck is the GNN's space consumption.

## 5. Experiments

### Baseline Adaption

Since there is no mature baseline for this new problem we propose, we adapt two methods from graph counterfactual explanation as baselines.

**CF-GNNExplainer.** CF-GNNExplainer (Zhu et al., 2017) is a famous graph counterfactual explanation method. Its basic idea is to multiply the adjacency matrix with a mask matrix. It optimizes the mask matrix to drive the GNN prediction away from its original prediction. After training, a smaller element in the mask matrix indicates a more influential edge. We adapt it to evaluate node influence. We optimize a node mask \(M R^{[V]}\), and its elements are limited in the range \(\). In every GNN layer, we multiply node embeddings by \(M\) before the message passing. After training, we evaluate influence as the distance between node mask and 1. Following CF-GNNExplainer, our loss function consists of a prediction loss term that drives the new prediction away from the original prediction and a regularization term that drives the value in the mask to be close to 1 (otherwise removing all nodes might be the best solution). The loss function is:

\[Loss=-_{i=1}^{N}||g_{}(V,E)_{i}-g_{}(V;E M)_{i}||_{1} +||M||_{1}, \]

**Lara.** Lara (Lara, 2017) is a recent work that greatly improves scalability by applying a GCN model to predict the edge influence. The GCN model generates a source embedding, \(p_{i}\), and a target embedding, \(t_{i}\) for every node \(o_{i} V\). It predicts the influence of \(o_{i}\) on \(o_{j}\) by \(p_{i} t_{j}\), where \(\) is the dot product. We approximate the influence of node removal as the sum of its influence on its neighbors:

\[_{g_{}}(v_{r})_{i N(r)}p_{r} t_{i}. \]

Besides, we also try to directly predict the node influence score with the GCN model, but it is not as effective as first generating node embeddings and calculating link influence.

### Experiment Settings

_Datasets._ To comprehensively evaluate NORA in different scenarios, we conduct experiments on six datasets and two tasks. The datasets include four widely-applied benchmark citation networks (Cora, CiteSeer, and PubMed (Yang et al., 2017), and ogbn-arxiv (CiteSeer et al., 2017)) and two social networks. Nodes on the four citation networks are papers, and undirected edges represent citations. The original task is to predict the research field of each paper. We also add a link prediction task to verify NORA's capacity in different settings. We follow the same data split ratio as the original link prediction task on the two social networks. The two social networks are heterogeneous Twitter datasets constructed by a previous study (Zhu et al., 2017). Nodes are users, and directed edges represent one of five Twitter actions or their counterparts (e.g., be followed): follow, retweet, like, reply, and mention. It originally contains two tasks. The first task is to predict the political leaning of each user. The second task is to predict whether there is a specific type of link from one user to another. Table 2 lists the dataset statistics.

An issue is that the trained GNN model is biased to the training-set nodes/edges. To fairly evaluate the influence of every node, we run each experiment 5 times and cycle the data split of nodes and edges by 20% per time, giving every node an equal chance to show up in training, validation, or test sets. For the link prediction task, we also cycle the link data split. After evaluation, we take the mean of the 5 results as the node influence score.

_GNN Models._ We select representative GNN models. On the citation datasets, we use three commonly used GNNs, GCN (Kipf and Welling, 2016), GraphSAGE (Hamilton et al., 2017), and GAT (Velickovic et al., 2017). As the ogbn-arxiv dataset is a heated benchmark, we use the SOTA model on its leaderboard at the time we started this project, DrGAT (Zhu et al., 2017), to replace the vanilla GAT. DrGAT is an improved variant of GAT, which is further equipped with a dimensional reweighting mechanism. Since the Twitter datasets

   Type & Time & Space \\  brute-force & \(O(LN^{2}h^{2}+LNHh)\) & \(O(M+LH^{2}+LNH)\) \\ NORA & \(O(LNH^{2}+LNH)\) & \(O(M+LH^{2}+LNH)\) \\   

Table 1. Complexity Comparision.

   Dataset & \#Nodes & \#Edges & \#Features & \#Classes & Homo/Hetero \\  Cora & 2,708 & 5,429 & 1,433 & 7 & homogeneous \\ CiteSeer & 3,327 & 4,752 & 3,703 & 6 & homogeneous \\ PubMed & 19,717 & 44,338 & 500 & 3 & homogeneous \\ ogbn-arxiv & 16,343 & 1,166,243 & 128 & 40 & homogeneous \\ P50 & 5,435 & 1,593,721 & - & 2 & heterogeneous \\ P20 50 & 12,103 & 1,976,985 & - & 2 & heterogeneous \\   

Table 2. Dataset statistics.

are heterogeneous, GCN, GraphSAGE and GAT are no longer useful, we use TIMME model, the GNN proposed in the same paper as the datasets (Shi et al., 2019). It tackles the challenges on the Twitter datasets, e.g., sparse features, sparse labels, and heterogeneity.

_Evaluation Metrics._ We compare NORA against the baseline methods introduced above. In the following tables, "Betweenness" denotes the betweenness centrality; "CF-GNNExplainer" is our adaption of CF-GNNExplainer. Among the adaptions of LARA, "N" and "E" represent the node-version and edge-version adaptions. We use two metrics to evaluate the similarity between approximation results and the ground truth. The first one is the top-k score, which is the sum of the influence score of the top k nodes ranked by the approximation method divided by that ranked by the ground truth. We evaluate top 1, top 5%, and top 10% nodes. The second metric is the Pearson correlation coefficient between the ground truth influence score and the approximated one.

_Hyper-parameters._ We keep the hyper-parameters for DrGAT and TIMME models the same as their original settings since they are already carefully tuned. We search for the best hyper-parameters for GCN, GraphSAGE, and GAT models. We also tune the hyper-parameters of each approximation method for each dataset and model. We list the hyper-parameter details in the appendix.

### Performance Comparison

The main results of the compared methods are recorded in Table 3. We evaluate the approximation performance of the GCN model on each citation dataset, since GCN is one of the most commonly used GNN models. Since GCN is not applicable to the heterogeneous graph, we use TIMME model on the Twitter datasets. Table 4 shows the results of more GNN models on the node classification task on the four citation networks. In the two tables, NORA outperforms the baseline methods. The betweenness centrality can not take node attributes into consideration. The CF-GNNExplaner' method

    & &  &  &  \\ Dataset & Method & top-1 & top-5\% & top-10\% & Corr & Time & top-1 & top-5\% & top-10\% & Corr & Time & 799 \\   & Betweenness & 100.0\% & 74.6\% & 72.9\% & 0.763 & 26S & 100.0\% & 79.0\% & 78.4\% & 0.864 & 26S & 761 \\  & CF-GNNExplainerâ€™ & 65.5\% & 58.6\% & 63.0\% & 0.567 & 10s & 5.8\% & 21.5\% & 29.7\% & 0.052 & 5.58 & 762 \\  & LARA-N & 100.0\% & 90.2\% & 89.4\% & 0.815 & 4.7\% & 100.0\% & 92.6\% & 89.9\% & 0.770 & 4.68 & 763 \\  & LARA-E & 100.0\% & 90.5\% & 89.7\% & 0.831 & 7.5s & 100.0\% & 93.9\% & 90.4\% & 0.878 & 6.28 & 764 \\  & NORA & 88.8\% & 92.6\% & 91.9\% & **0.884** & 11s & 100.0\% & 91.0\% & 89.0\% & **0.907** & 13s & 765 \\   & Betweenness & 28.1\% & 76.1\% & 76.4\% & 0.630 & 26s & 17.7\% & 76.5\% & 80.4\% & 0.591 & 26s & 766 \\  & CF-GNNExplainerâ€™ & 78.0\% & 37.4\% & 38.6\% & 0.478 & 9.3s & 1.8\% & 24.4\% & 31.4\% & 0.018 & 6.58 & 767 \\  & LARA-N & 100.0\% & 91.2\% & 88.6\% & 0.797 & 6.6s & 100.0\% & 94.5\% & 90.4\% & 0.718 & 7.08 & 768 \\  & LARA-E & 100.0\% & 89.8\% & 85.7\% & 0.812 & 6.2s & 100.0\% & 96.0\% & 94.7\% & **0.917** & 7.4s & 769 \\  & NORA & 100.0\% & 83.9\% & 86.6\% & **0.833** & 14s & 100.0\% & 95.3\% & 94.1\% & 0.822 & 14s & 779 \\   & Betweenness & 63.3\% & 76.8\% & 85.4\% & 0.528 & 42min & 66.4\% & 80.1\% & 86.3\% & 0.569 & 42min & 771 \\  & CF-GNNExplainerâ€™ & 31.3\% & 71.4\% & 70.8\% & 0.509 & 9.1s & 75.1\% & 20.3\% & 23.3\% & 0.230 & 6.7s & 772 \\  & LARA-N & 79.6\% & 90.2\% & 91.0\% & 0.799 & 3.8s & 39.1\% & 88.7\% & 93.1\% & 0.837 & 4.6s & 773 \\  & LARA-E & 79.6\% & 91.5\% & 92.8\% & **0.836** & 7.5s & 76.4\% & 96.7\% & 97.4\% & **0.923** & 5.5s & 774 \\  & NORA & 51.1\% & 83.2\% & 88.6\% & 0.745 & 19s & 100.0\% & 89.1\% & 91.4\% & 0.873 & 228 & 778 \\   & Betweenness & 100.0\% & 74.4\% & 77.9\% & 0.782 & \(\)140h & 100.0\% & 75.8\% & 78.9\% & 0.786 & \(\)140h & 777 \\  & CF-GNNExplainerâ€™ & 66.4\% & 24.8\% & 32.1\% & 0.666 & 19s & 0.1\% & 14.7\% & 21.6\% & 0.213 & 15s & 778 \\  & LARA-N & 100.0\% & 86.0\% & 83.5\% & 0.595 & 9.1s & 100.0\% & 91.5\% & 89.4\% & 0.559 & 21s & 779 \\  & LARA-E & 77.4\% & 53.0\% & 55.1\% & 0.506 & 21s & 100.0\% & 64.4\% & 65.3\% & 0.758 & 39s & 760 \\  & NORA & 77.4\% & 86.5\% & 86.1\% & **0.900** & 35s & 100.0\% & 95.6\% & 94.2\% & **0.997** & 31s & 780 \\   & Betweenness & 100.0\% & 83.6\% & 91.8\% & 0.643 & \(\)6h & 100.0\% & 72.3\% & 86.2\% & 0.644 & \(\)6h & 782 \\  & CF-GNNExplainerâ€™ & 100.0\% & 17.1\% & 16.0\% & 0.811 & 34s & 73.1\% & 95.8\% & 74.5\% & 0.666 & 10min & 783 \\  & LARA-N & 100.0\% & 89.4\% & 92.1\% & 0.435 & 10s & 100.0\% & 81.1\% & 88.2\% & 0.540 & 59s & 784 \\  & LARA-E & 100.0\% & 90.2\% & 86.3\% & 0.877 & 23s & 100.0\% & 88.2\% & 90.2\% & 0.862 & 68s & 785 \\  & NORA & 100.0\% & 98.7\% & 98.5\% & **0.956** & 19s & 100.0\% & 92.3\% & 91.3\% & **0.943** & 24s & 786 \\   & Betweenness & 98.3\% & 88.5\% & 93.5\% & 0.707 & \(\)14h & 100.0\% & 89.5\% & 92.4\% & 0.838 & \(\)14h & 788 \\  & CF-GNNExplainerâ€™ & 66.9\% & 62.5\% & 57.4\% & 0.612 & 76s & 100.0\% & 21.6\% & 21.9\% & 0.789 & 15min & 789 \\  & LARA-N & 98.3\% & 83.6\% & 91.9\% & 0.556 & 13s & 100.0\% & 88.4\% & 92.4\% & 0.549 & 71s & 790 \\  & LARA-E & 98.3\% & 93.7\% & 93.1\% & 0.968 & 25s & 100.0\% & 94.2\% & 93.3\% & 0.968 & 84s & 791 \\  & NORA & 100.0\% & 98.7\% & 96.7\% & **0.979** & 37s & 100.0\% & 95.5\% & 95.4\% & **0.984** & 42s & 791 \\   

Table 3. Approximation method performance and efficiency. We use GCN for Cora, CiteSeer, PubMed, and ogbn-arx datasets, and TIMME model for P50 and P_20_50.

is useful in its original design, which is to analyze the influence on a single target node. However, when it considers all nodes or all edges, different nodes/edges might pick different influential nodes w.r.t. them, and thus the large mask is difficult to optimize. The LARA adaptions work best among the baselines, but they require a lot of labels, which must be generated by the time-consuming ground truth method. The original paper proposes a neighborhood sampling strategy to improve efficiency since it only targets one node, but it is not applicable in our scenario.

When comparing the efficiency, CF-GNNExplainer', LARA, and NORA are similar on small graphs. However, when we increase the graph size, NORA remains the most stable efficiency. Besides, LARA requires labeling of the ground truth to train the model. The time in the table does not include the labeling time, but it actually takes a lot of time. For example, it takes about 41 hours to generate the ground truth influence scores for the DrGAT model on the ogbn-arxiv dataset. If LARA requires 20% labeled data to train, it still needs about 8 hours. Calculating the betweenness centrality takes the longest time, since it traverses the shortest paths on the graph. For the ogbn-arxiv dataset, we only sample 10000 nodes to run the algorithm, and we approximate that it takes about 140 hours according to its time complexity.

### Stability of The Proposed Influence Score

As the novel node-removal approach provides a new perspective of evaluating node influence, we want to examine whether the real influence of node removal generated by the brute-force method is stable across different GNNs and different hyper-parameters. We conduct experiments on the four citation datasets: Cora, CiteSeer, PubMed, and ogbn-arxiv. We use the same models as above. We change a sensitive hyper-parameter, hidden size, to evaluate the results' stability. For each model, we use three different hidden sizes: 128, 256, and 512, except for DrGAT on ogbn-arxiv, which only uses 128 and 256 due to memory limitation. For each model and each dataset, we traverse each two-hidden-size pair and calculate the Pearson correlation coefficient of each pair's results, and we report the mean of them. For each hidden size and each dataset, we also traverse each two-GNN pair and calculate the Pearson correlation coefficient of each pair's results, and we calculate the mean of them. Then, we further calculate the mean of the different hidden sizes' results ("Inter-model"). We list the results in Table 5.

From the results, we can observe that the performance generated by different hidden sizes are very similar. It indicates that the node-removal approach is stable across different hyper-parameters. Results generated by different GNN models are also quite similar. Nevertheless, it is not as similar as that of different hidden sizes. It indicates that the influence of node removal is still dependent on the specific GNN model.

## 6. Conclusion

It is important to study node influence and identify influential nodes on a graph. Existing approaches that capture node influence typically focus on how a node functions given its existence, but they ignore the node-removal perspective. We step into this important yet neglected perspective, which could provide a new perspective on node influence and benefit real-world applications. We use graph neural network (GNN) models as a surrogate to learning the underlying propagation patterns on a graph. We formalize the problem by removing a node, re-applying a trained GNN model, and using the output change to measure the influence.

For detecting the influence of node removal for each node, the ground-truth method is the brute-force algorithm, which is reliable but low in efficiency. To overcome this defect, we analyze how GNN's prediction changes when a node is removed and approximate it with gradient information. We propose **N**O**de-**R**emoval-based **f**a**st **GNN inference (**NORA**). It can efficiently approximate such change in GNN's prediction for all nodes by one forward propagation and one backpropagation. As we are studying a new problem without mature baselines, we also adapt two methods from graph counterfactual explanation as baseline methods for comparison. We conduct extensive experiments on six networks and demonstrate NORA's effectiveness. We also verify the transferability of the node influence score across different models, which indicates that it is a stable indicator of node influence. This paper mainly focuses on the approximation and the influence of node removal. We hope this work can opens up an inspirational new perspective. In the future work, we would extend our proposed NORA to a broader line of research fields such as graph-level analysis, molecular property prediction and link prediction.

  Dataset & GNN & Method & top-5\% & top-10\% & Corr \\   &  & LARA-E & 84.3\% & 77.9\% & 0.819 \\  & & NORA & 85.8\% & 82.0\% & **0.839** \\  & & LARA-E & 84.8\% & 83.6\% & **0.792** \\  & & NORA & 76.7\% & 78.3\% & 0.774 \\   &  & LARA-E & 73.4\% & 72.6\% & 0.714 \\  & & NORA & 84.1\% & 86.0\% & **0.799** \\  & & LARA-E & 80.7\% & 76.9\% & **0.782** \\  & & NORA & 79.5\% & 78.3\% & 0.746 \\   &  & LARA-E & 85.1\% & 90.3\% & 0.785 \\  & & NORA & 84.9\% & 91.0\% & **0.792** \\  & & LARA-E & 84.3\% & 87.6\% & 0.794 \\  & & NORA & 92.1\% & 93.7\% & **0.915** \\   &  & LARA-E & 39.6\% & 43.0\% & -0.007 \\  & & NORA & 92.6\% & 90.8\% & **0.961** \\   & & LARA-E & 97.3\% & 97.7\% & 0.895 \\   & & NORA & 98.1\% & 98.4\% & **0.924** \\  

Table 4. Further Verification on More GNNs on the node classification task.

  Dataset & GCN & GraphSAGE & GAT/DrGAT & Inter-model \\  Cora & 0.9956 & 0.9857 & 0.9393 & 0.8765 \\ CiteSeer & 0.9968 & 0.9931 & 0.9585 & 0.8167 \\ PubMed & 0.9970 & 0.9963 & 0.9451 & 0.8372 \\ ogbn-arxiv & 0.9984 & 0.9979 & 0.9914 & 0.9557 \\  

Table 5. Stability results. The three column named by a GNN model shows the correlation coefficient of different results generated by the same GNN with different hidden sizes. The rightmost column means the correlation coefficient of different results generated by different GNN models.