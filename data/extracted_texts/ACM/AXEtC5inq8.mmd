# NCTM: A Novel Coded Transmission Mechanism for Short Video Deliveries

###### Abstract.

With the rapid popularity of short video applications, a large number of short video transmissions occupy the bandwidth, placing a heavy load on the Internet. Due to the extensive number of short videos and the predominant service for mobile users, traditional approaches (e.g., CDN delivery, edge caching) struggle to achieve the expected performance, leading to a significant number of redundant transmissions. In order to reduce the amount of traffic, we design a Novel Coded Transmission Mechanism (NCTM), which transmits XOR-coded data instead of the original video content. NCTM caches the short videos that users have already watched in user devices, and encodes, broadcasts, and decodes XOR-coded files separately at the server, edge nodes, and clients, with the assistance of cached content. This approach enables NCTM to deliver more short video data given the limited bandwidth. Our extensive trace-driven simulations show how NCTM reduces network load by 3.02%-14.75%, cuts peak traffic by 23.01%, and decreases rebuffering events by 43%-85% in comparison to a CDN-supported scheme and a naive edge caching scheme. Additionally, NCTM also increases the user's buffered video duration by 1.21x-13.53x, ensuring improved playback smoothness.

short video delivery, coded transmission, client-side cache +
Footnote †: ccs: Networks Mobile networks

+
Footnote †: ccs: Networks Mobile networks

+
Footnote †: ccs: Networks Mobile networks

+
Footnote †: ccs: Networks Mobile networks

+
Footnote †: ccs: Networks Mobile networks

## 1. Introduction

Short video applications such as Tiktok (Tiktok, 2016) and YouTube Shorts (Brockman et al., 2017) are rapidly rising in popularity, attracting billions of active users per month (Brockman et al., 2017; Bockman et al., 2017), and consistently topping the best-selling apps lists (Brockman et al., 2017). Taking Tiktok as an example, it had 1.4 billion monthly active users in 2022, and it is predicted to reach 1.8 billion by the end of 2023 (Brockman et al., 2017). Billions of users imply a huge number of video streams. According to TikTok's report (Brockman et al., 2017), globally, 167 million hours of short videos are consumed every minute, putting an enormous pressure on the current Internet infrastructure.

Traditional video transmission solutions maintain videos on the cloud and stream them to users via content delivery networks (CDNs) (Brockman et al., 2017; Bockman et al., 2017). A major weakness of CDN is the huge redundant traffic, causing network pressure and affecting the user viewing experience. Edge caching approaches (Brockman et al., 2017; Bockman et al., 2017; Bockman et al., 2017) can reduce the amount of redundant traffic by caching user-desired contents at edge nodes (Brockman et al., 2017; Bockman et al., 2017; Bockman et al., 2017). However, in short video services, users have different preferences determined by their hobbies, culture, educational backgrounds, lifestyles, etc. Even though videos are rarely watched in groups, they might be popular with other users (Bockman et al., 2017). Therefore, at the edge, it is difficult to identify which are the most popular videos among various ones. As a result, it is difficult for edge nodes to cache adequate user-desired contents due to limitations in cache capability, leading to low caching efficiency (Kumar et al., 2018). According to Nokia's report (Nokia, 2018), the hit rate of edge caching solutions is only 29% for short video delivery. Therefore, edge caching is not always as effective as expected.

Recent studies (Zhu et al., 2018; Wang et al., 2018) have revealed that using client-side caches within a peer-to-peer (P2P) network can significantly reduce bandwidth pressure caused by redundant video transmissions (Zhu et al., 2018). P2P is a promising solution to increase the cache hierarchy and can compensate for the limited edge node capability. However, the majority of short video deliveries are from mobile devices, e.g., 97% of TikTok video deliveries (Brockman et al., 2017). The expensive uploading data bills or the data cap make P2P not a feasible solution. Therefore, we aim to make better use of the client-side cache to compensate for the lack of edge cache capacity, while avoiding the upload traffic costs.

Coded cache (Zhu et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) could provide a solution. It avoids transmission redundancy by transmitting coded files without incurring additional traffic charges. It is based on merging two separate video files into one coded file at the server and forward it to the destination. The most commonly used encoding approach is XOR-merging (Exclusive OR). After receiving the coded file, the destination device separates the original files by locally cached content prefetched during network idle time. So the coded cache technique requires fewer transmission files to transmit all data. But it also means some contents need to be cached in the destination device before use, and devices need to be active simultaneously to receive the coded files in time. However, in practice, short video applications generally do not allow network usage when they are closed, and the server lacks mechanisms to confirm the apps running status or interact with the users' cache. The above-mentioned problems hinder the practical deployment of the coded cache solution.

In this paper, we propose an innovative **Novel Coded Transmission Mechanism (NCTM)** for efficient short video delivery. With deep insight on the characteristics of short video delivery, NCTM avoids content prefetching or uploading through an effective cooperation between the cloud, edge, and mobile clients. Particularly, NCTM caches videos that the user **has already watched** for subsequent decoding rather than prefetching ones.

As shown in figure 1, the cloud server in NCTM applies XOR operations to combine video files into a single coded file, and sends it to the edge node (coded transmission). Then the edge node broadcasts the coded file to specific mobile clients. The mobile clients will decode the coded file by applying XOR operations with the assistance of previously watched and cached videos. To make sure that the coded file can be successfully decoded, we should divide the clients into multiple groups, ensuring that any two clients

Figure 1. File flows example for NCTMwithin a group have previously watched and cached the files being requested currently by each other (group divided problem). So we introduce a User Cache Table (UCT) and a Transmission Matching Graph (TMG) that record the cache status and explore coded transmission opportunities. Recognizing the similarity between the above-mentioned requirement and the mathematical concepts of "cliques", we model the group divided problem as the clique cover problem (Zhou et al., 2017; Zhang et al., 2018) and propose the novel _Minimum Clique Coverage algorithm_. This algorithm involves multiple linear-time searches to find a relatively optimal solution. Furthermore, we introduce the new _Recommendation Reorder algorithm_, which modifies the playback order by bringing forward videos scheduled for later to create more opportunities for coded transmission. Finally, we propose a _Client-side Cache Update method_ based on the video recommended queue, which is informed by the recommendation system in short video services.

To evaluate the performance of NCTM, we utilized the kuaiRec dataset (Zhou et al., 2017) and collected real user interactions with the Kuaishou app on August 12, 2020, totaling 117,977 records. For each client, edge node, and cloud server, we created separate docker containers (Zhou et al., 2017) to simulate video requests and playback behaviors based on their app usage time. The results show that under sufficient bandwidth, NCTM reduces the average network load by 3.02%-14.75%, and cuts the peak traffic by 23.01%. With limited bandwidth, NCTM reduces 43%-85% of rebuffering events and increases video occupancy in the user's buffer by 1.21x-13.53x. Furthermore, we use the NCTM to assist the edge caching, proving that they are compatible. In addition, we demonstrate that NCTM can achieve real-time performance on the server and provide reference values for the hyperparameters in the proposed approach.

In summary, our contributions are as follows:

* We propose the innovative **NCTM** that sends XOR-coded files and utilizes the client-side cache to store watched videos for file decoding. This approach reduces the network load without data prefetching or uploading.
* We design the _Minimum Clique Coverage algorithm_ and the _Recommendation Reorder algorithm_ to find a relatively optimal solution for the client dividing problem with linear time complexity, which ensures successful decoding for the coded files in coded transmissions. The _Client-side Cache Update method_ based on the video recommended queue is also proposed.
* We devise trace-driven experiments to emulate the NCTM and verify its effectiveness in the reduction of bandwidth consumption, buffer variation, and rebuffering frequency.

The remaining of the paper is structured as follows. In section 2, we summarize the NCTM related work, in section 3, we illustrate the overall structure of NCTM and in section 4, NCTM is described in details. In section 5, the experimental results are presented. Finally, we give a brief conclusion of our work in section 6.

## 2. Related Works

The popularity of short video applications leads to massive video traffic and introduces a significant load on CDNs.

**Edge caching**(Zhou et al., 2017; Zhang et al., 2018) is a key approach to alleviate the CDN load. It utilizes cache-enabled edge servers, such as base stations and smart gateways, to store popular contents, so that these contents can be transmitted directly from the caches instead of from the remote cloud (Zhou et al., 2017). As these cache edges are closer to the users, there is a reduction in the traffic load on the core network. The cache hit rate is an essential metric to evaluate the performance of edge caching. For short video deliveries, (Zhou et al., 2017) proposes to consider the number of views and likes as the popularity basis to choose the edge cache content. Furthermore, (Zhou et al., 2017) takes user preferences into consideration. To achieve a higher hit rate, (Zhou et al., 2017) introduces a multi-agent deep reinforcement learning algorithm where each edge learns its own best policy.

**P2P-CDNs**(Zhou et al., 2017; Zhang et al., 2018; Zhang et al., 2018; Zhang et al., 2018) enables content caching on mobile phones by utilizing wireless channels for cache sharing, also known as P2P sharing. With a P2P approach, users can send their stored content to other users, effectively balancing the upstream and downstream transfers, and increasing the cache hierarchy (Zhou et al., 2017). (Zhou et al., 2017) considers different network environments. High-quality Internet access shares its resources (e.g., bandwidth) with slower or unreliable ones. (Zhou et al., 2018) takes advantage of user social information (friends/ followers), using the friend list to identify additional P2P sharing.

**Coded cache**(Zhou et al., 2017; Zhang et al., 2018; Zhang et al., 2018) utilizes network coding techniques, which aggregate (encode), broadcast, and separate (decode) data messages in the cloud, edge node, and client devices, respectively. This technique results in lower traffic load for data transmission. For example, if user A caches video file \(c_{1}\) and user B caches video file \(c_{2}\), when user A requests video file \(c_{2}\) and user B requests video file \(c_{1}\), the server can transmit a single coded file \(c_{1}\) @ \(c_{2}\) (the symbol @ represents bitwise XOR) instead of two separate files \(c_{1}\) and \(c_{2}\). It can deliver the coded file simultaneously to both users by broadcasting at the edge node, and users can decode it based on their local files (e.g., for user A: \(c_{1}\) @ \(c_{2}\) @ \(c_{1}=c_{2}\)).

The previous works made important contributions in terms of edge caching, P2P-CDNs and coded cache solutions individually, but they have not combined them in an approach that reduces the load for short video deliveries as proposed by NCTM.

## 3. NCTM Architecture and Principle

The overall NCTM architecture is shown in figure 2. The mobile clients cache their watched video files based on their cache capabilities, and the cloud server sends the coded files desired by multiple clients to the edge node. Then the edge node broadcasts the coded file to the mobile clients. These clients decode the coded file based on the cached files to obtain the desired one. To make sure the coded file can be successfully decoded, we need to ensure that all the files involved in the coded file have been cached at the clients, except the desired one. So the challenge lies in finding **how to efficiently and accurately identify the clients with the above

Figure 2. Overall structure of NCTM

cache situation among multiple ones**. We employ the following designs to address this challenge.

The cloud server includes three key modules: **Client Cache Management, Coded Chances Exploration**, and **XOR-encoder**. The Client Cache Management informs the cache status of the client's devices. We design the User Cache Table (UCT) (section 4.1) to record the cached files in clients. Due to the limited cache capability, we specify the corresponding cache update policy (section 4.4). The Coded Chances Exploration module finds the coded transmission opportunities, thus minimizing bandwidth consumption. Since the condition of multivariate-coded transmission is similar to the concept of clique in mathematics, we design the Transmission Matching Graph (TMG) and reduce the chances exploration problem to the clique cover problem (Zhu et al., 2017; Wang et al., 2018) (section 4.1). To solve this NP-Hard problem, we designed the _Minimum Clique Coverage algorithm_ to find a relative optimal solution within linear time complexity (section 4.2). Moreover, based on our observation, switching the playback order of short video can create more coded transmission opportunities, so we further proposed _Recommendation Reorder algorithm_ (section 4.3). File encoder merges several origin video files into one coded file by XOR before transmission.

The edge nodes copy the coded file and broadcast it to clients. Clients decode the coded file and extract the original files, as desired.

## 4. Coded Transmission Mechanism

### Definitions

We assume that there are currently \(N\) short video files, denoted as \(=\{C_{1},C_{2},...,C_{N}\}\). Short videos are commonly delivered with an adaptive bitrate paradigm (Zhu et al., 2017; Wang et al., 2018). Now we assume that each short video consists of only one video chunk; this will be generalized to the common case in the next section. Suppose there are \(K\) mobile clients, denoted as \(=\{U_{1},U_{2},...,U_{K}\}\). Each client is assigned an independent recommendation queue, storing short videos recommended for this user. We denote \(v_{lj}\) as the \(j\)-th video pushed to the \(i\)-th user (\(v_{lj}\) determined by the recommendation system). Therefore, for the user \(U_{i}\), the recommendation queue can be denoted as \(V_{i}=\{v_{11},v_{i2},...\}\). At this point, we define the UCT as \(=\{t_{ij}\}\), where \(t_{ij}=1\) indicates that user \(U_{i}\) has **watched and cached** video \(C_{j}\), and vice versa (\(i[1..K]\), \(j[1..N]\)). Thus, for the user \(U_{i}\), all the videos in their client-side cache can be recorded as \(T_{i}=\{C_{j}|t_{ij}=1,j[1..N]\}\).

For example, here we assume that user \(U_{a}\) has cached video \(C_{j}\), and user \(U_{b}\) has cached video \(C_{i}\). It can be denoted as \(t_{aj}=1\) and \(t_{bi}=1\), respectively. When the user \(U_{a}\) needs the video \(C_{i}\), and user \(U_{b}\) needs video \(C_{j}\), the coded file \(C_{i} C_{j}\) can be transmitted. So the condition for coded transmission is that **they have cached the files needed by each other** (e.g., \(t_{aj}=1\), \(t_{bi}=1\)), which is to ensure successful decoding in the user device (e.g., for user \(U_{a}\), \((C_{i} C_{j}) C_{j}=C_{i}\), where \(C_{j}\) is watched and cached video file of user \(U_{a}\)). This is defined as a binary-coded transmission.

Similarly, here we assume that there are three users \(U_{a},U_{b}\), and \(U_{c}\) who need videos \(C_{i},C_{j}\), and \(C_{k}\), respectively. When \(t_{aj}=t_{ak}=1\), \(t_{bi}=t_{bk}=1\) and \(t_{ci}=t_{ej}=1\), indicating that each user has cached the files needed by the other two users, the coded file \(C_{i} C_{j} C_{k}\) can be transmitted. The client devices can also successfully decode the coded file with their cached video files. (e.g., for user \(U_{a}\), decoding can be achieved through \((C_{i} C_{j} C_{k}) C_{j} C_{k}=C_{i}\)). This is defined as a ternary-coded transmission. Similar operations can be generalized to multivariate-coded transmission involving x video files (\(x 2\)).

Clearly, a larger value of \(x\) indicates merging more video files into one coded file, resulting in fewer number of transmissions and less bandwidth consumption. Therefore, we prefer the coded transmission that covers more users. To make sure the client devices can successfully decode, multivariate-coded transmission requires that any two users satisfy the condition for binary-coded transmission. This requirement can be associated with the mathematical concept of "cliques" (a clique is a complete graph where any two nodes are connected by an edge (Zhu et al., 2017)). So we design the TMG as \(=(E,D)\) to model the multivariate-coded transmission exploration problem as a graph theory problem. \(D\) is the set of vertices representing the active users currently. If \(d_{i} D\), it means the user \(U_{i}\) is **watching videos**, and vice-versa. \(E\) is the set of edges. If \(e_{ij} E\), it means there is an edge between nodes \(i\) and \(j\), indicating that users \(U_{i}\) and \(U_{j}\) currently satisfy the **condition for binary-coded transmission**, and vice-versa.

Let us denote the watching video of user \(U_{i}\) as \(v_{i}^{*}\) (\(v_{i}^{*} V_{i}\)). In this case, for users \(U_{i}\) and \(U_{j}\), if \(t_{it^{}_{j}}=t_{jt^{}_{i}}=1\), it indicates that they have cached the files desired by each other, binary-coded transmission can be achieved, i.e., \(e_{ij} E\). Following this rule, we can construct the \(\) with \(K^{}\) nodes, where \(K^{}\) is the number of active users (\(K^{} K\)). In \(\), if we can find a clique with \(x\) nodes, it means that among these \(x\) users, any two users satisfy the condition for binary-coded transmission, thus satisfying the condition for multivariate coded transmission involving \(x\) users. On the other hand, all user requests need to be fulfilled, so any node in current TMG needs to be covered by a clique (a single node is also considered a clique, we define it as the separate clique). Therefore, the problem can be transformed into finding the **minimum number of cliques** that **cover all nodes** in the TMG. We call this the group divided problem.

Formally, we denote \(=\{R_{1},R_{2},...\}\) as all cliques in TMG, where the i-th clique is represented as \(R_{i}=\{d_{x},d_{y},...\}\). We must ensure \(R_{1} R_{2}...=D\) and minimize \(||\). This problem is a classical clique cover problem and has been proven to be NP-hard when the degree of vertices is greater than 6 (Zhu et al., 2017; Wang et al., 2018). Finding an optimal solution requires high complexity. In this paper, we propose the _Minimum Clique Coverage algorithm_, which utilizes the previous result and a single traversal to solve this problem with linear time complexity. The details of this algorithm are presented in section 4.2.

Figure 3. Example of NCM with 6 users and 6 videos

Figure 3 illustrates an example containing \(6\) users and \(6\) videos, and gives intuitive cases of binary-coded transmission and ternary-coded transmission. In the example, two cliques are used to cover the TMG, with \(R_{1}=\{d_{1},d_{2},d_{3},d_{4}\}\) and \(R_{2}=\{d_{5},d_{6}\}\). Therefore, in the coded transmission, two coded files need to be transmitted. \(F_{1}\) merging \(4\) original video files \(C_{6},C_{4},C_{5},C_{3}\) and \(F_{2}\) merging \(2\) video files \(C_{2},C_{1}\). The decoding process will be completed in the client devices with cached video files. In practical deployment, we group users with close geographic proximity and similar playback queues together to perform coded transmission. So the scale of the UCT and the TMG will not be very large. In our experiments, there are 1398 users and 10230 videos, and the storage capacity for the UCT is 202MB, which does not impose a significant load on the server.

### Minimum Clique Coverage algorithm

To ensure smooth video playback, short video transmission uses the adaptive bitrate paradigm. We define a video \(C_{i}\) as composed of multiple video chunks, denoted as \(C_{i}=\{c_{i1},c_{i2},...\}\). Note that the UCT and TMG are dynamically maintained as the videos play.

As shown in figure 4, users \(U_{1}-U_{4}\) have already watched and cached parts of the videos. For example, the user \(U_{1}\) cached the videos \(C_{3},C_{4},C_{5}\), i.e., \(T_{1}=\{C_{3},C_{4},C_{5}\}\). Now let us consider that user \(U_{1}\) is watching video \(C_{6}\), user \(U_{2}\) is watching video \(C_{4}\), i.e., \(v_{1}^{*}=C_{6}\), \(v_{2}^{*}=C_{4}\), and so on. In the previous example, we assumed that all users request videos strictly at the same time. However, in this one, the user request time is different, and there are variations in the duration of the videos. For example, user \(U_{1}\) watches video \(C_{6}\) within the time range of 4-14 seconds, user \(U_{2}\) watches video \(C_{4}\) within the time range of 0-22 seconds, and so on. Therefore, the TMG also changes within different time intervals. The figure depicts the TMGs corresponding to the current 5 intervals. At this point, encoding operations are performed between different video chunks rather than the entire video (for example, at the 8th second, \(c_{63} c_{45} c_{51}\) is encoded for transmission). It is evident that the TMG changes only when a user either completes a video playback (e.g., 14th second) or starts watching a new video (e.g., 0th second, 4th second, 8th second, and 18th second). During each time interval, the TMG remains unchanged.

After completing a video playback, the user will leave the current TMG. Since the subgraph of a clique remains a clique, the removal of a node will not break the current clique. In the given example, at the 14th second, user \(U_{1}\) (node \(d_{1}\)) leaves the TMG, but users \(U_{2}\) (node \(d_{2}\)) and \(U_{3}\) (node \(d_{3}\)) can still form a clique. When a user starts playing a new video, a new node is added to TMG, but the cliques formed by all the existing nodes remain unchanged. So we can decide whether the new node can be added to an existing clique (maintaining the same total number of cliques) or the new node forms a separate clique (increasing the total number of cliques by 1). A similar situation can be seen in the example at the 8th second when user \(U_{3}\) joins the TMG as a new node \(d_{3}\). According to the rules, node \(d_{3}\) has edges with nodes \(d_{1}\) and \(d_{2}\), so it can be added to the clique formed by these two nodes, forming a clique with three nodes and achieving ternary-coded transmission.

Therefore, whenever a new request arrives, we need to add a new node to the TMG and try to incorporate this node into existing cliques. We can judge whether the incorporation condition is met by iterating through all existing cliques and checking whether all nodes can connect to the new node. If a suitable clique is found, the new node will be added to this clique, maintaining the existing number of cliques and the coded files to be transmitted unchanged. Otherwise, the new node forms a separate new clique, leading to an increase in both the number of cliques and the coded files to be transmitted.

However, the aforementioned approach often yields poor performance, as shown in figure 5. As new requests from user \(U_{5}\) and \(U_{6}\) arrive at time \(t_{2}\) and \(t_{3}\), new nodes \(d_{5}\) and \(d_{6}\) are added to current TMG. Since they do not have edges connecting to all nodes in the existing clique, they will form new cliques. Clearly, the optimal solution involves only two cliques, but the result at time \(t_{3}\) contains three. To address this issue, we propose a search algorithm with linear time complexity, the _Minimum Cliques Coverage algorithm_, to find a relatively optimal solution based on the previous results. The algorithm includes \(T\) iterations, where \(T\) is a hyperparameter that controls the computational cost. In each iteration, the following steps are executed:

1. Randomly select a node \(d_{i}\) among all separate cliques. We assume that \(d_{i}\) comes from the clique \(R_{x}\).
2. Randomly select an edge \(e_{ij}\) from all the edges connected the node \(d_{i}\). Obviously \(e_{ij}\) connects to node \(d_{j}\). We assume that \(d_{j}\) comes from the clique \(R_{y}\).
3. Let \(d_{j}\) exits the original clique \(R_{y}\) and joins the clique \(R_{x}\) where \(d_{i}\) belongs to. The processed cliques are denoted as \(R_{y}^{{}^{}}\) and \(R_{x}^{{}^{}}\), respectively.
4. Iterate through all the nodes (\(d_{k}\)) from \(R_{y}^{{}^{}}\). Check if there exist nodes that have edge connected to \(d_{i}\) (\(e_{ik} E\)).
5. Make these nodes (\(d_{k}\)) exit the original clique \(R_{y}^{{}^{}}\) and join the clique \(R_{x}^{{}^{}}\). The processed cliques are denoted as \(R_{y}^{{}^{}}\) and \(R_{x}^{{}^{}}\), respectively.
6. Iterate through all the remaining separate cliques (\(d_{k}\)). Check if one can be added to the clique \(R_{y}^{{}^{}}\) after excluding some nodes.
7. If such a separate clique exists, node \(d_{k}\) joins the clique \(R_{y}^{{}^{}}\). The processed clique is denoted as \(R_{y}^{{}^{}}\). Now a solution is found and the algorithm terminates. Otherwise, all adjustments are retained and the next iteration starts.

The pseudocode is shown in appendix A, and we prove this algorithm in appendix B. During the iteration, if such a separate

Figure 4. An example of asynchronous user requests

clique is found in Step 7, it means that we successfully merge two separate cliques (\(R_{x},R_{z}\)) and one non-separate clique (\(R_{y}\)) into two non-separate cliques (\(R_{x}^{},R_{y}^{}\)). The total number of cliques and the coded files to be transmitted reduces by one, and the algorithm terminates. Otherwise, we still need to preserve the above operations so that the next iteration can discover more matching opportunities. Figure 6 illustrates an example of the _Minimum Cliques Coverage algorithm_. Here the separate cliques \(R_{3}(d_{4})\) and \(R_{1}(d_{7})\) are merged with the clique \(R_{2}\) containing three nodes \(\{d_{1},d_{2},d_{3}\}\) to form the new clique \(R_{i}^{{}^{}}\) with three nodes \(\{d_{1},d_{2},d_{7}\}\) and the new clique \(R_{2}^{{}^{}}\) with two nodes \(\{d_{3},d_{4}\}\). At this point, the clique \(R_{3}\) disappears.

This algorithm is executed whenever a new request arrives and the newly added node in the TMG cannot join an existing clique (i.e., forming a separate clique). In each round, all nodes are traversed at most three times, resulting in a computational complexity of \((KT)\), where \(K\) is the number of users in the current TMG, and \(T\) is a hyperparameter that controls the number of iterations. We will further explore the time complexity of this algorithm and the impact of \(T\) on the success rate of the search in our experiments.

The major benefit of the above algorithm is that we do not rely on any prior knowledge (such as video popularity or predictions of user viewing behavior). Instead, we dynamically adjust the cliques through the UCT and TMG. This allows us to determine which users currently satisfy the coded transmission conditions. Furthermore, with the assistance of the _Minimum Clique Coverage algorithm_, we utilize the historical results to explore a better solution for the clique cover problem in linear time complexity. This allows us to use fewer cliques, which means fewer coded files to be transmitted, thereby reducing bandwidth consumption.

### Recommendation Reorder algorithm

Based on our observations, the opportunities for coded transmission are related to users currently watching videos. In some cases, playing a video located further in the recommended queue may create more chances for coded transmission than playing the very next video. A typical example is presented in figure 7.

The example contains two users, \(U_{1}\) and \(U_{2}\). User \(U_{1}\) has a recommendation queue \(V_{1}=\{C_{3},C_{4},C_{5},C_{6},C_{1},\}\), and videos \(C_{3}\), \(C_{4}\) and \(C_{5}\) have been watched and cached in the client device, i.e., \(T_{1}=\{C_{3},C_{4},C_{5}\}\). User \(U_{1}\) is currently watching the video \(C_{6}\), i.e., \(v_{1}^{s}=C_{6}\). Similarly, \(V_{2}=\{C_{6},C_{3},C_{1},C_{4},C_{5},\}\), and \(T_{2}=\{C_{6},C_{3}\}\). Since the previous video has just ended, no video is being played by user \(U_{2}\). Now we find that if video \(C_{1}\) is played next based on the recommendation queue, since \(t_{11}=0\), NCTM cannot be used between users \(U_{1}\) and \(U_{2}\). However, if video \(C_{4}\) is chosen to be played next (also in the recommendation queue, but not the very next one), we have \(t_{26}=t_{14}=1\), and NCTM can be used. Specifically, if the user \(U_{2}\) chooses video \(C_{1}\) as the next one, there will be no edges connecting to node \(d_{2}\) in the TMG, and the node \(d_{2}\) will inevitably form a separate clique. However, if \(U_{2}\) chooses video \(C_{4}\), in the TMG, the node \(d_{2}\) will have at least one edge. Even if it forms a separate clique, with the _Minimum Clique Coverage algorithm_, it also offers the potential for coded transmission.

For push-playback short videos, there are no content correlations between two consecutive videos. Therefore, changing the playback order of videos without altering their content will not significantly affect the user experience. The above example illustrates that in certain situations, reordering the recommendation queue can create more opportunities for coded transmission. Given that the users might end, skip, or re-watch videos at any time, this makes it difficult to predict users' viewing behaviors. Therefore, we propose the _Recommendation Reorder algorithm_, which focuses on short-term benefits. Whenever a user switches videos, this algorithm will filter the videos that can trigger **NCTM** based on the current UCT and select one based on the recommendation queue. This method does not change the recommendation queue results, but changes the video playback order by prioritizing the ones that enable coded transmission. Thus, it creates more chances for coded transmission. The pseudocode is shown in appendix D. The algorithm considers the next \(G\) videos in the recommendation queue of user \(U_{i}\). For each video, it iterates through all users to determine if the binary-coded transmission can be used. The algorithm can be described as the following process:

1. Traverse the next \(G\) videos in \(V_{i}\), denote as \(C_{j}\).
2. For each \(C_{j}\), traverse all other active users, denote as \(U_{k}\). Check if one satisfies the requirement of binary-coded transmission, i.e., \(t_{kj}=t_{t_{t_{k}^{j}}}=1\).
3. If such a user exists, let \(v_{i}^{s}=C_{j}\). That is, the next video to be played is \(C_{j}\). Otherwise, still play the first video in \(V_{i}\).

Here \(G\) is a hyperparameter that decides the maximum number of look-forward videos as well as limits the computational cost.

Figure 5. Naive solutions often fail to achieve optimal solutions

Figure 6. An example of the Minimum Cliques Coverage algorithm

This algorithm involves two iterations. So the time complexity is referred to \((GK)\), where \(G\) represents the number of next videos considered in the recommendation queue, and \(K\) represents the number of users watching videos. To further prune the search path, we can avoid the case where \(t_{tv_{i}^{*}}=0\) during the first traversal, i.e., excluding the users who are watching the video not cached by user \(U_{i}\). That is because the user \(U_{i}\) can not meet the coded transmission condition whatever the next video is, and it is impossible to change others playing videos.

Currently, most short video applications pre-cache the first video chunk of several subsequent videos in the recommendation queue, to alleviate the stalling and rebuffering issues caused by rapid video switching. For example, TikTok pre-caches the first video chunk of the next five videos (TikTok, 2018). Assuming the application pre-caches the first video chunk of \(P\) subsequent videos, it should ensure \(G P\) to mitigate the risk of increased stalling and rebuffering events.

### Client-side cache update method

As discussed in Section 1, both edge caching and client-side cache have limited storage capabilities. Compared to edge caching, user devices (such as smartphones) may be more restricted. In this section, we will discuss how to perform cache placement/replacement to better cooperate with the _Minimum Clique Coverage algorithm_ and _Recommendation Reorder algorithm_.

We define the maximum storage capacity for user \(U_{i}\) as \(M_{i}\), thus we need to ensure that \(_{C_{j} T_{i}}|C_{j}| M_{i}\), where \(T_{i}\) is the cached videos of user \(U_{i}\) and \(|C_{j}|\) represents the file size of video \(C_{j}\). Suppose that after watching video \(C_{k}\), \((_{C_{j} T_{i}}|C_{j}|)+|C_{k}|>M_{i}\), indicating that the user \(U_{i}\) cannot store all watched videos on the user device. Therefore, it is necessary to determine which content should be replaced.

For video \(C_{k}\), we cache video files so we can decode the coded files with the assistance of them in the future. Hence, cached videos \(C_{k}\) are only valuable if they are watched by other users later. Using the "push playback" style, a recommendation system can keep track of the video playback list. Therefore, we can estimate the earliest possible time when the video \(C_{k}\) will be used for file decoding. As NCTM mainly focuses on short-term benefits, we prefer the files that can create coded transmission opportunities in the short term and replace those that would take longer to be used.

Formally, we define \(F(C_{k})\) as the earliest occurrence time of video file \(C_{k}\) in the recommendation queue. That is, \(F(C_{k})=(j)\)_s.t._\(v_{ij}=C_{k}\)_for all_\(i[1..K]\), where we find the smallest \(j\) among all \(K\) users' recommendation queues such that \(v_{ij}=C_{k}\). This is the earliest possible time when the current video could be used for decoding. For the user \(U_{i}\), we can sort the videos in \(T_{i}\) based on \(F(C_{k})\) and replace the videos that would take longer to be used.

Note that from the perspective of the _Recommendation Reorder algorithm_, it can be regarded as \(F^{*}(C_{k})=\{1,F(C_{k})-G\}\), where \(G\) is the hyperparameter defined in the algorithm description. This is because the _Recommendation Reorder algorithm_ can look ahead \(G\) videos based on the current recommendation queue, thus this video could potentially be played ahead by up to \(G\) videos. Additionally, when we consider edge cache, if video \(C_{k}\) is cached in the edge cache, it means that requests involving video \(c_{k}\) will not use NCTM and can be regarded as \(F(C_{k})=+\).

### Further considerations

According to our further observations, due to the spatial and temporal densification of of short videos, NCTM exhibits significant potential in practical applications. For the uncompleted watching events, we will consider weighted _TMG_ in our subsequent work to explore more stable coded transmission opportunities. Furthermore, short video playback on mobile devices also exhibits a significant degree of randomness. For instance, users may slide the progress bar to skip some uninteresting scenes, network conditions can affect the video bitrate, and user movements can impact the connection status of edge nodes. We have taken these issues into consideration and discussed them in detail in appendix C.

## 5. Evaluation

### Methodology

We used the **traditional CDN delivery** and **edge cache** (size of 500MB with a LRU update method) approaches for comparison in order to evaluate the performance of NCTM. To replicate the users video-watching behavior, we utilized the kuailee dataset (Zhu et al., 2017) and collected 117,977 real request records from 1,398 users on the Kuaishou app on August 6, 2020, involving 10,230 short videos. To simulate the network conditions, we used the MAWI (Zhu et al., 2017) and FCC18 (Zhu et al., 2018) network traces from August 12, 2020. In the actual implementation, we created separate Docker (Docker, 2020) containers for the server, the edge node, and each user device. Users sent requests to servers via edge nodes according to the real request records mentioned above, in order to replicate the video transmission and playback behaviors. Additionally, we used the Mahimahi network tool (Mahimahi, 2017) to reply the network trace, aiming to closely reproduce the network conditions at that time. In the experiments, we considered two fundamental scenarios: the sufficient bandwidth network and the limited bandwidth network. For the former one, we mainly considered the network bandwidth utilization. For the latter one, we mainly focused on buffer variation and rebuffer events. More detailed settings are shown in appendix F.

### Performance

**Under sufficient bandwidth:** Figure 8 presents the network load variations under high-speed connections (200Mbps) across CDN and edge caching approaches with and without NCTM, as well as a histogram of short video requests distribution. From the results, it can be observed that network load variations are closely related to the request frequency. The Pearson correlation coefficient between network load and request frequency is 0.77. The peak value of requests occurs at around the 29700th second, approximately at 8:15 AM. During this time, there are 118 requests within one minute (with a total of 1398 users), and the network load also reached its peak value. In CDNs delivery mode, the real-time throughput was 69.94 Mb/s without NCTM, while it is 53.84 Mb/s with the assistance of NCTM, cutting peak traffic by 23.01%. Similar results are observed in the edge caching mode, indicating that NCTM can effectively alleviate peak throughput pressure. Figure 9 presents the cumulative distribution function (CDF) of the normalized bandwidth usage. From the results, it can be observed that in CDNs delivery mode, the average network load decreased by 14.06% with NCTM versus no NCTM. In the edge caching mode (with an edge cache size of 500MB), NCTM decreased network load by 13.30% compared to no NCTM. As a result of the change in the order of video playback, there may be a short period of time when NCTM's throughput exceeds that of baselines. From a global perspective, the NCTM is clearly superior to the baseline.

**Limited bandwidth scenario:** Figure 10 shows the distribution of rebuffer events using NCTM in both CDNs delivery mode and the edge caching mode in a limited bandwidth scenario (60Mbps). Figure 11 and 12 show the statistics of rebuffer events and the proportion of rebuffered requests respectively. From the figures, it can be observed that due to the limited bandwidth, in the CDN delivery mode, 73.93% of user requests suffer from rebuffer events. During peak request times, almost all users experience rebuffer events, resulting in a total of 35,726 rebuffer instances. NCTM relieves some of the bandwidth pressure, resulting in a 40.23% reduction in rebuffer events for user requests. This represents a decrease of 33.7% compared to the baseline. Total rebuffer instances decreased by 45.6% to 19,431. Similarly, in the edge caching mode, although rebuffer events have also decreased compared to CDNs delivery, the improvement is limited to users who request popular files. However, with the efficient cooperation of NCTM and edge caching, the number of user requests experiencing rebuffer events has been reduced to only 9.21%, and the total count of rebuffer events has decreased by 80.8% compared to the baseline.

Figure 13 shows the average proportion of rebuffer time across CDN and edge caching approaches with and without NCTM. Figure 14 shows the average buffered video duration during video playback in client devices. It can be concluded that, due to frequent rebuffer events in the CDN delivery mode, the average buffered video duration is only 1.97 seconds. About 45.82% time of video watching is waiting for rebuffering. With NCTM, the average buffered video duration increases by 13.53x to 26.66 seconds, mitigating the effects of fluctuations in network performance. Similarly, in the edge caching mode, the average buffered video duration increases by 1.21x, and the proportion of buffer time decreases to only 5.79%. This optimization significantly improves user experience in limited network bandwidth conditions.

**Dynamic bandwidth scenario:** Figure 15 illustrates the average bandwidth usage in different delivery modes in a scenario where the network bandwidth ranges from 60Mbps to 100Mbps. Results reveal that when the bandwidth is relatively abundant (100Mbps 80Mbps), NCTM reduces the bandwidth usage rate by 3.02%-14.75%. Even in CDNs delivery mode, using NCTM outperforms edge caching mode. Under the constrained bandwidth (70Mbps), NCTM significantly alleviates the bandwidth pressure, reducing the time of full bandwidth occupancy by 15.28%, thereby substantially reducing rebuffering events.

**NCTM assisting edge caching:** To further explore the cooperation between NCTM and edge caching, we analyzed the number of requests in different transmission stages, including the number of hit-edge-cache requests and the number of NCTM requests, as shown in Figure 18. Result indicates that in the early stages, when the response workload of the edge cache is low, LRU can satisfy over 20% of the requests. However, with the video files increasing in the later stages, the edge cache becomes overwhelmed. Due to limited cache accumulation on the client side, NCTM does not occur frequently in the early stages. But in the later stages, as user caches accumulate, a large number of requests (over 25%) can use NCTM to transfer coded files. Therefore, edge caching and NCTM are not only compatible, but they are complementary and further reduce the network load by cooperation.

### Time complexity and hyperparameters

**Time complexity:** We design an experiment to compute the time complexity of NCTM in order to assess NCTM's capability to process the received requests in real-time. Figure 16 illustrates the time complexity of NCTM. Figure 17 illustrates the relationship between the average execution time of NCTM and short video playback latency. The execution time of NCTM primarily includes four distinct components, i.e., cliques traversal, the _Minimum Clique Coverage algorithm_, the _Recommendation Reorder algorithm_, and the XOR encoding. Figure 16 shows four fitting curves, illustrating the actual execution time of the algorithms with different entity counts (\(N\) for time complexity), such as the number of current nodes or cliques. From the results, it can be concluded that both the _Minimum Clique Coverage algorithm_ and the XOR-encoding process show nearly linear growth, consistent with our analysis of time complexity in \(O(N)\) (when \(T=G=5\)). Although the _Recommendation Reorder algorithm_ exhibits a slightly super-linear growth trend, its execution time remains below \(10ms\) even when \(N>10000\). Besides, the average execution time from opening the APP to the start of the first video playback is \(706ms\) (tested with the Chrome browser for Tiktok). Considering that NCTM's execution time is much shorter than the video playback delay, we can conclude that NCTM can process requests in real time for short videos.

**Hyperparameter analysis:** In the _Minimum Clique Coverage algorithm_ and the _Recommendation Reorder algorithm_, we set hyperparameters \(T\) and \(G\) to control the computational cost, respectively. To determine the values of \(T\) and \(G\), we design experiments with values \(\{1,3,5,7,9\}\) and calculated the success rate for finding solutions and execution time under each parameter setting. Results in Figure 19 and figure 20 reveal that both \(T\) and \(G\) exhibit a nearly linear increase in average execution time as their values increase. When \(T=5\), the probability of reducing the number of cliques in the _Minimum Clique Coverage algorithm_ is \(8.11\%\), with an average execution time of \(12.92ms\); when \(G=5\), the probability of finding videos in the _Recommendation Reorder algorithm_ is \(68.25\%\), with an average execution time of \(2.54ms\). As \(T\) and \(G\) further increase, the success rate of finding solutions approaches a plateau, indicating that \(T=5\) and \(G=5\) are relatively optimal values. In practical applications, the parameters can be adjusted according to the server's computing resources.

## 6. Conclusions

As short videos become increasingly common, they put a significant strain on network resources due to the massive amount of data they transmit. Due to the time- and space-intensive nature of short videos, CDNs are faced with considerable bandwidth challenges. Moreover, edge caching and other solutions rely too heavily on predicting popular files. In this paper, we propose NCTM, which employs XOR-encoded files to make effective use of user-side caches, reducing bandwidth consumption and improving transmission efficiency. Trace-driven experiments show that compared to CDNs and edge caching, NCTM reduces bandwidth consumption, rebuffering events, and the reliance on caching popular files, ultimately improving the user experience when watching short videos.

Figure 18. Cooperation between NCTM Figure 19. Average execution time and Figure 20. Average execution time and and edge caching success rate in different \(T\) success rate in different \(G\)

Figure 15. Average bandwidth usage Figure 16. Time complexity of NCTM-Figure 17. Average execution time of NCTM-related algorithm and video request delay