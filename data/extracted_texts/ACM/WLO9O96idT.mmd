# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

according to the current optimization context; and 2) optimizing the model based on this identified distribution. However, transferring this appealing technique to GNN-based recommendation poses two primary challenges:

**Challenge 1**: Current DRO methods are mainly applied in data with Euclidean structures (_e.g._, images(Wang et al., 2017; Wang et al., 2017; Wang et al., 2017; Wang et al., 2017), sentences(Wang et al., 2017)). Adapting them to manage graph-structured data remains an an open problem. More critically, the effect of graph structure is tangled with the complex graph neural networks, complicating the derivation of the hardest distribution within DRO.

**Challenge 2**: Another challenge arises from the typically sparse nature of recommendation data(Wang et al., 2017). With nodes often having limited neighbors, the feasible distribution family for DRO is inherently constrained, which might dampen its robustness and overall performance(Wang et al., 2017).

To bridge these gaps, we propose DR-GNN, a novel method that seamlessly integrates DRO into GNN-based recommendation. To address the first challenge, we harness insights from graph filtering theories(Golovin et al., 2011), recasting the GNN into an equivalent graph smoothing regularizer that penalizes the distance between adjacent nodes' embeddings. Through this perspective, we incorporate DRO into this regularizer, enhancing the GNN's robustness against shifts existing in neighbor distributions. For the second challenge, we propose a strategy to augment the observed neighbor distribution with slight perturbations. Notably, while DR-GNN does introduce intricate nested optimization, meticulous simplification ensures its implementation remains easy and efficient -- primarily drawing from similar nodes as new neighbors and adjusting edge weights according to embedding distances. Our rigorous theoretical analysis proves that if the divergence between training and testing distributions is bounded, the robustness of DR-GNN can be guaranteed.

Our contributions are summarized as follows:

* Exploring the less-explored task of GNN-based recommendation with distribution shift, and revealing the limitations and inadequacies of existing methods.
* Proposing a new GNN-based method DR-GNN for OOD recommendation, which seamlessly integrate DRO in GNN-based methods.
* Conducting extensive experiments to validate the superiority of the proposed method against three types of distribution shift (_i.e._, popularity shift, temporal shift and exposure shift).

## 2. Preliminary

In this section, we present the background of Graph-based Recommender System and Distributionally Robust Optimization.

### GNN-based Recommender Systems

Given a data of user-item interactions \(=\{u,i,r_{u,i} u,i I\}\), where \(\) denotes the set of users, \(\) denotes the set of items, and \(r_{u,i}=\{0,1\}\) indicates whether user \(u\) has interacted with item \(i\). We can represent user-item interaction data in the form of a bipartite graph \(=(,)\) where \(=\) denotes the user/item nodes and \(\) denotes the edge set representing the interactions between users and items. Let \(A[|+|I][ |+|I]\) denote the adjacency matrix of graph \(\), where \(_{u,i}=1\) if \(r_{ui}=1\), and \(_{u,i}=0\) otherwise. Let \(N(u)=\{i I|_{u,i}=1\}\) represents the item set that the user \(u\) has interacted with. We also define \(P_{u}\) as the distribution of the neighbors of \(u\), a uniform distribution over neighbors. Let \(d_{u}\) represent the degree of user \(u\). The goal of GNN-based RS is to learn high-quality embeddings from the graph \(\) and accordingly make accurate recommendations.

**LightGCN.** As a representative GNN-based recommendation methods, LightGCN learns user/item representations following the general message passing of GNNs. Nevertheless, it removes feature transformation and non-linear activations, as they tend to increase overfitting risks without enhancing performance. Let denote the embeddings of users and items as \(^{(||+|I|) c}\) ( \(c\) is the dimension of representations). In LightGCN, the final embeddings were obtained via \(K\)-th propagation layers, which can be abstracted as:

\[^{(k)}=}^{(k-1)}, \]

where \(^{(k)}\) denotes the embeddings after \(k\)-th propagation layers and \(=(D^{-}AD^{-})\) denotes as the normalized adjacency matrix. \(D\) is the diagonal node degree matrix. The notation \(\) denote the Laplacian matrix of graph \(\), _i.e._, \(=I-\).

To facilitate understanding, considering the prominence and widespread application of LightGCN, we simply take it as the backbone for analysis.

### Distributionally Robust Optimization

The machine learning model's success is based on the IID assumption, _i.e._, training data and testing data are drawn from the same distribution. However, the assumption fails to hold in many real-world applications, leading a sharp drop in performance. Distributionally Robust Optimization (DRO) addresses the issue by considering the uncertainty of the distribution. Specifically, instead of focusing on performance under a single observed data distribution, DRO aims to ensure that the model performs well across a range of potential data distributions. It first identifies the worst distribution(_i.e._,

Figure 1. The performance comparison of MF (Golovin et al., 2011) and LightGCN (Golovin et al., 2011) under both in-distribution (IID) and out-of-distribution (OOD) testing scenarios. For the OOD testing, here we introduce popularity shift in Yelp2018 and temporal shift in Movielens-1M. More details about experimental setting refer to section 4.

the loss function is maximized under the expectation of the worst-case distribution) in the family of distributions within a predefined range around the empirical data distribution, and then optimizes the objective function under that distribution, thereby ensuring the robustness of the model under unfavorable conditions. Formally, DRO is solved in a bi-level optimization via playing the min-max game on the objective function \((x;)\), in which \(x\) represents the data variable and \(\) represents the model parameter to be optimized. The optimization objective of DRO is as follows:

\[&=*{argmin}_{}\{ }{}_{x P}(x; )\}\\ &=\{P:D(P,P_{})\}  \]

where \(\) denotes the set of all distributions, \(\) denotes robust radius. The _uncertainty set_\(\) is composed of all distributions within the robust radius distance from \(P_{}\), and DRO tried to identify _the worst-case distribution_\(P^{*}\) from a family of eligible distributions \(\) by maximization. The function \(D(.,.)\) measures the distance between two distributions _e.g._, KL-divergence.

## 3. Methods

In this section, we detail the proposed DR-GNN (subsection 3.1.83.2), followed by the theoretical analyses to demonstrate its robustness (subsection 3.3). Finally, we discuss the connections our DR-GNN with existing methods (subsection 3.4). The schematic diagram of DR-GNN is depicted in Figure 4.

### Distributionally Robust GNN

It is highly challenging to directly apply DRO in GNN-based recommendation methods, as the graph structure is tangled with the complex GNN procedure. To tackle this problem, we draw inspiration from graph filtering theories(Golov et al., 2013) and recast the GNN into an equivalent graph smoothing regularizer.

**LightGCN as a Graph Smoothness Regularizer.** By analyzing LightGCN from the graph signal filtering perspective, we have the following important lemma:

**Lemma 1**.: _Performing graph aggregation in LightGCN is equivalent to optimizing the following graph smoothness regularizer using gradient descent with appropriate learning rate:_

\[_{smooth} =_{u}_{ P_{u}}[d_{u}g(u, v;)] \]

_Here, \(g(u,v;)\) signifies the Fibonacci norm between the normalized embeddings, with \(\) representing the model parameters. \(P_{u}\) denotes the distribution of the neighbor nodes of \(u\)._

The proof can be found in Appendix A.1. This lemma clearly elucidates the effect of the graph neural network - graph aggregation tend to draw the embeddings of neighbors closer. Moreover, the impact of distribution shifts on GNN are highlighted. Taking the popularity shift (_a.k.a._ popularity bias) as an example, user representations may become excessively aligned with popular items, thereby exacerbating the Matthew effect, as demonstrated in(Golov et al., 2013). For the convenience of subsequent analysis, we denote \(_{smooth}(u)=_{ P_{u}}[d_{u}g(u,v;)]\) as the smoothness regularizer on the specific node \(u\).

**Leveraging DRO in the Regularizer.** Holding the view of GNN as a regularizer, we further introduce DR-GNN, a model that incorporates DRO to enhance its robustness against distribution shifts. Following the definition of DRO Eq.(2), the objective function of the proposed DR-GNN can be formulated as:

\[_{}_{DRO\_smooth}(u)& =_{}_{P}_{ P}[d_{u}g(u,v; )]\\ &D_{KL}(P,P_{u}) \]

DR-GNN engages in a min-max optimization: 1) it identifies the most difficult distribution over a set of potential distributions. It is defined in the vicinity of the observed neighbor distribution subject to the constraint \(D_{KL}(P,P_{u})\); 2) Subsequently, the model's optimization is performed on this identified hardest distribution instead of original observed distribution. This strategy intrinsically incorporates potential distribution shifts during training, thereby naturally exhibits better robustness against distribution shifts. We will further validate this point in both theoretical analyses (Section 3.3) and empirical experiments (Section 4).

**Efficient Implementation.** Despite the promise, the objective of DR-GNN involves the complex nested optimization, which may incur heavily computational overhead. Fortunately, it can be largely simplified with the following lemma:

**Lemma 2**.: _The bi-level optimization problem of Eq.(5) can be transformed into optimizing:_

\[_{}_{DRO\_smooth}(u)=_{}_{ P _{u}}[d_{u}g(u,v;)] \]

_where \(\) represents the optimal Lagrange coefficient of the constraint \(D_{KL}(Q,P_{u})\), which can be regarded as a surrogate parameter of \(\). The worst-case distribution \(P_{u}^{*}\) can be calculated as following_

\[P_{u}^{*}(o)=P_{u}(o)_ { P_{u}}[(g(u,v;)/)]} \]

The proof is placed in the appendix A.2. This lemma provides a close-formed expression of the most difficult distribution, greatly simplifying the implementation of DR-GNN. Specifically, modifications to the distribution of neighboring nodes can be simply realized

Figure 2. Illustration of how DR-GNN augments LightGCN: it gives edge weights during graph aggregation and introduces new nodes as neighbors.

through the alteration of edge weights within the graph. Formally, the normalized adjacency matrix of the graph \(\) is transformed into \(^{}\) and

\[^{}_{ij}=}{_{k  N(i)}}_{ij} \]

The adjusted normalized adjacency matrix \(^{}\), characterized by modified weights, is subsequently utilized to execute the aggregation operation pertaining to the graph. Given the equivalence between the GNN and the regularizer, this can be integrated during graph aggregation, necessitating only minor adjustments to the edge weights. In practice, we may suffer numerical instability due to the introduce of \((.)\) in weights. To counteract this, we suggest to introduce the embedding normalization via \(L2\) norm in calculating the weights, empirically yielding more stable results.

### Graph Edge-Addition Strategy

Applying DRO in GNN-based recommendation incurs another challenge: Particularly, in DRO, all potential distributions \(P\) should share the same support as the original distribution \(P_{u}\), otherwise the KL-divergence would become infinite. This inherently implies that the support of \(P\) would be restricted to the neighboring nodes, excluding the vast pool of non-neighboring nodes. The situation exacerbates given the typically sparse nature of recommendation data - most users may only have interactions with a handful of items. This significantly limits the flexibility and scope of potential distributions, increasing the risk of missing testing distribution and thereby undermining model robustness.

To address aforementioned issue, we propose a strategy called Graph Edge-Addition(GEA) that introduces slight perturbations \(p_{u}^{add}\) in \(P_{u}\) to expand its support. \(p_{u}^{add}\) is defined over the support of all item set such that those non-neighboring nodes can be utilized for training better and robust embeddings. Formally, The objective of DR-GNN is improved as:

\[_{DRO\_smooth}(u)=_{P}_{P_{u}^{ new}}_{u P}[d_{u}g(u,v;)]\\  D_{KL}(P,p_{u}^{new}) \]

where the new distribution \(p_{u}^{new}\) is defined as \(p_{u}^{new}=yP_{u}+(1-)p_{u}^{add}\), with \(\) serving as a hyperparameter that controls the magnitude of the perturbations. Remarkably, we adjust \(P_{u}\) towards minimization of regularizer rather than maximization. This is premised on the belief that a non-neighboring item, which exhibits greater similarity to user \(u\), is more likely to be favored by the user. Such items are potentially useful in refining user embeddings.

The introduce of \(p_{u}^{add}\) mitigates the inherent shortcomings of DRO in sparse recommendation datasets. It extends the range of potential distributions and harnesses the abundant information from non-neighboring nodes. Practically, the minimization optimization on \(p_{u}^{new}\) can be formulated into finding the items minimizing \(g(u,v;)\). However, it can be computationally expensive, as it requires traversing over all nodes. To mitigate computational complexity, we employ a strategy of random sampling. Specifically, we select a subset of nodes randomly to form a candidate set, and then confine the traversal operation solely to this subset.

### Theoretical Analyses

In this subsection, we provide a theoretical analysis to demonstrate the robustness of DR-GNN to distribution shift. For any user \(u\), let \(p_{u}^{add}\) denotes \(u\)'s ideal neighbor distribution used for model testing and the corresponding smoothness regularizer for node \(u\) can be written as \(_{ideal}(u;)=_{u p_{u}^{add}}[d_{u}g(u,v; )]\).

**Theorem 3.1**.: _Let \(}_{DRO\_smooth}(u;)\) serve as the estimation for \(_{DRO\_smooth}(u;)\). If \(D_{KL}(P_{u}^{ideal},P_{u})\), then we have that with probability at least \(1-\):_

\[_{ideal}(u;)}_{DRO\_smooth}(u; )+(q,d_{u},) \]

_where \((q,d_{u},)=g_{u}(})+8}{d_{u}}}\) and \(q\) is the Vapnik Chern-vonenkis dimension of the hypothesis space of parameter \(\)._

The proof is presented in appendix A.3. Theorem 3.1 exhibits that the ideal loss is upper bound by empirical DRO loss if a sufficiently large data size is employed. At this point, we provide the theoretical guarantee for the resistance to distribution shift capability of DR-GNN.

### Discussions

**The connection with APDA(Sen et al., 2017).** Interestingly, the edge weights introduced in our DR-GNN is highly similar with APDA. The key distinction lies in our choice to use initial embeddings for weight calculation, while APDA uses embeddings with multi-layer aggregation. This seemingly minor difference leads to a pronounced performance disparity in favor of DR-GNN over APDA in our experiments. The reason behind this is the theoretical grounding of our approach. Specifically, DR-GNN is derived from the theoretical-sound DRO framework, while APDA's design is predominantly heuristic and lacks a solid theoretical base.

Furthermore, our framework offers theoretical insights into several heuristic settings found in APDA:

Our framework also gives a theoretical explanations of many heretical settings used in APDA: 1) Operations such as the exponential function and symmetric normalization, which are adopted heuristically in APDA, can be indeed derived to the DRO objective. 2) APDA also heuristically employs a hyperparamter \(\) to modulate the magnitude of the value in \(()\)1. In the context of DR-GNN, this \(\) finds its theoretical counterpart \(-\) serves as a Lagrange multiplier, acting as a surrogate hyperparameter to control the robust radius. We will discuss in depth the role of \(\) and its relationship with the degree of distribution shift in section4.3.

**The connection with Attention Mechanism(Sen et al., 2017).** The attention mechanism has been adopted by recent work like Graph Attention Network (GAT) (Sen et al., 2017) to determine the edge weights. These weights in GAT, similar to ours, are predicated on node embeddings. However, there's a stark difference: while GAT employs a learnable non-linear layer to ascertain the weights, the weights in our DR-GNN are derived directly from DRO.

Despite the success of the attention mechanism across various domains, it underperforms in GNN-based recommendation. The primary reason lies in the sparsity and the lack of rich features of recommendation data. It heavily hinders the effective training of the attention function. This limitation is evident in our experimental results: GAT demonstrates suboptimal performance, while our DR-GNN exhibits effectiveness.

**The connection with GraphDA(Chen et al., 2019)** GraphDA is a method that enhances the adjacency matrix of a graph. This method initially pretrains a graph encoder to obtain the user/item embeddings following several iterations of graph convolution. Subsequently, the graph is reconstructed based on the similarity between these embeddings. This process facilitates denoising for active users and augmentating for inactive users. With the enhanced graph adjacency matrix, GraphDA retrains a randomly initialized graph encoder. Contrasting with GraphDA, the edge-adding operation in DR-GNN also reconstructs a new adjacency matrix, but with different motivations. The primary objective of GraphDA's graph enhancement is to equalize the number of neighbors for each node. This is achieved by denoising users with an excess of neighbors in the original data and augmenting users with a paucity of neighbors. Furthermore, GraphDA preemptively introduces edges between user-to-user and item-to-item to enable long-distance message passing. However, the integration of the GAE module aims to broaden the support of neighbor distribution. This expansion subsequently expand the uncertainty set of DRO, thereby endowing the model with enhanced generalization capabilities.

## 4. Experiments

We aim to answer the following research questions:

* **RQ1:** How does DR-GNN perform compared with existing methods under various distribution shifts?
* **RQ2:** What are the impacts of the components (e.g., DRO on neighbour nodes, GEA) on DR-GNN?
* **RQ3:** How does the parameter \(\) impacts DR-GNN?

**Datasets.** The experiments are conducted under three prevalent distribution shift scenarios: popularity shift, temporal shift, and exposure shift. Thus, eight datasets are employed for testing, namely Gowalla, Douban, AmazonBook, Yelp2018, Movielens-1M, Food, Coat, and Yahoo. For the popularity shift setting, we re-divide the train and test set of the dataset based on item popularity. The test set was designed in such a way that the popularity of all items approximated a uniform distribution, while a long-tail distribution was preserved within the training set. For the datasets under the temporal shift setting, we took the most recent 20% of interaction data from each user as the test set, and the earliest 60% of interaction data as the training set. Table 5 in appendix A.3 shows the statistics of each processed dataset.

**Baselines.** We use the conventional LightGCN as backbone and BPR loss for all the baselines. The methods compared in the study fall into several categories:

* **Methods against distribution shifts in Recommendation System(InvCF(Zhou et al., 2019), BOD(Wang et al., 2019))** InvCF is the SOTA method on addressing the popularity shift through invariant learning. BOD achieves data denoising through bi-level optimization. Meanwhile, we acknowledge that there are some other methods to address the COD problem, including CausPref(Zhou et al., 2019), COR(Yang et al., 2019), HIRL(Wang et al., 2019), and InvPref(Wang et al., 2019). However, these methods require additional information that is not available in our dataset and hence cannot be tested. Certain methods among these necessitate pre-partitioned environmental datasets(Wang et al., 2019), others demand prior semantic information about users and items(Zhou et al., 2019; Wang et al., 2019), and some necessitate the assignment of environmental variables for each interaction, thereby rendering the strategy of random negative sampling inapplicable(Wang et al., 2019).
* **Graph contrastive learning methods(LightGCCL(Chen et al., 2019), SGL(Zhou et al., 2019))**. The methods use the contrastive learning on the graph and has been proven to alleviate the prevalent popularity bias.
* **Reconstructing the adjacency matrix methods(APDA(Wang et al., 2019), GAT(Wang et al., 2019), GraphDA(Chen et al., 2019))** Such methods reconstruct the adjacency matrix of the graph by adjusting edge weights or reconstructing the edges between nodes according to certain rules. Moreover, APDA and GAT can experience memory overflow issues on datasets with a large number of interactions. This is because GAT needs to calculate weights for each edge and perform backpropagation, while APDA needs to calculate weights for each layer of aggregation operations.

We used the source code provided in the original papers and searched for optimal hyperparameters for all comparison methods according to the instructions in the original papers.

**Evaluation Metrics.** Three commonly used metrics--Precision@_K_, Recall@_K_, and Normalized Discounted Cumulative Gain(NDCG@_K_) -- are used to assess the quality of the recommendations, with \(K\) being set by default at 20.

Further experimental details are presented in the appendixA.5.

### Performance Comparison (RQ1 and RQ2)

In this section, we analyze the superior of DR-GNN under different distribution shift setting as compared with other baselines.

#### 4.1.1. Evaluations on Popularity Shift Setting

Table 1 reports the comparison of performance on all the baselines under popularity shift. The majority of comparative methodologies fail to consistently yield satisfactory results across a variety of datasets, including algorithms specifically designed for popularity shift, such as APDA and InvCF. The improvement of APDA compared to LightGCN is quite limited, indicating that its heuristic dynamic edge weight adjustment algorithm cannot handle popularity shift well.

Figure 3. t-SNE Visualization on Douban. DR-GNN ensures that the representations of hot items and cold items are almost distributed in the same space.

Meanwhile, GAT, which also dynamically adjusts edge weights, performs much worse than LightGCN, suggesting that the attention mechanism may inadvertently intensify the impact of distribution shift. Our proposed method, DR-GNN, consistently and significantly surpasses the state-of-the-art benchmarks across all popularity shift datasets. The robustness of DR-GNN is ascribed to the incorporation of uncertainty inherent in observed data distributions by DRO. This feature enables the model to perform optimally under various environments, rather than solely relying on training data.

**Visualization Results.** As shown in Figure 3, to better understand how DR-GNN handles distribution shift, we perform t-SNE visualization(Maaten and Hinton, 2008) of the item embeddings on the Douban dataset. According to the ranking results of item popularity, the top 5% most popular items are selected as hot items and the bottom 5% as cold items. It can be observed that in the representations learned by LightGCN, there is a clear gap between hot items and cold items, while both are distributed in the same space in the representations learned by DR-GNN. This suggests that DR-GNN eliminate the impact of distribution shift caused by popularity shift.

#### 4.1.2. Evaluations on Temporal Shift Setting

Temporal bias takes into account changes in user interests over time. It is more complex than the distribution shift caused by popularity, as it encompasses factors beyond popularity alone. We simulate temporal bias by dividing training and test sets for each user according to the timestamp of the interaction. Dataset Food and Movielens-1M are used in this setting.

In the context of the temporal shift setting, we noticed that on the Movielens-1M dataset, although APDA performed better than DR-GNN, the improvement is not significant, and its performance was relatively unstable, evidenced by its inferior performance compared to LightGCN on the Food dataset. Similar observations were made for GraphDA. Compared to most other benchmark methods, DR-GNN continues to consistently deliver good performance.

#### 4.1.3. Evaluations on Exposure Shift Setting

In practical contexts, users are typically exposed to a limited subset of items, thereby neglecting a significant majority. This phenomenon named "exposure bias" suggests that the absence of user interaction with specific items does not unequivocally signify their disinterest. Consequently, in real-world datasets, the patterns of missing user interaction records are not randomly distributed, but rather, they are "missing-not-at-random". Here we conduct experiments on widely used missing-complete-at-random datasets: Yahoo and Coat.

   & &  &  &  &  \\   & NDCG & Precision & Recall & NDCG & Precision & Recall & NDCG & Precision & Recall & NDCG & Precision & Recall \\  LightGCN(SIGIR20) & 0.0369 & 0.0170 & 0.0563 & 0.0792 & 0.0510 & 0.0723 & 0.0227 & 0.0129 & 0.0278 & 0.0136 & 0.0060 & 0.0221 & 685 \\ LightGCL(CLR23) & 0.0380 & 0.0177 & 0.0593 & 0.0746 & 0.0464 & 0.0721 & 0.0243 & 0.0137 & 0.0305 & 0.0136 & 0.0061 & 0.0227 & 686 \\ SoftS(SIGIR21) & 0.0381 & 0.0173 & 0.0595 & 0.0773 & 0.0493 & 0.0716 & 0.0232 & 0.0131 & 0.0296 & 0.0143 & 0.0063 & 0.0242 & 647 \\ InvCF(WWW23) & 0.0399 & 0.0181 & 0.0636 & 0.0740 & 0.0459 & 0.0725 & 0.0210 & 0.0118 & 0.0268 & 0.0144 & 0.0062 & 0.0243 & 649 \\ BOD(KDD23) & 0.0361 & 0.0169 & 0.0556 & 0.0815 & 0.0559 & 0.0686 & 0.0253 & 0.0141 & 0.0315 & 0.0145 & 0.0061 & 0.0253 & 690 \\ APDA(SIGIR23) & 0.0367 & 0.0168 & 0.0579 & OOM & OOM & OOM & OOM & OOM & OOM & OOM & 600 & 61 \\ GAT(ICLR18) & 0.0227 & 0.0104 & 0.0334 & OOM & OOM & OOM & OOM & OOM & OOM & OOM & OOM & 622 \\ GraphDA(SIGIR23) & 0.0341 & 0.0166 & 0.0570 & 0.0892 & 0.0596 & 0.0808 & 0.0199 & 0.0113 & 0.0262 & 0.0140 & 0.0059 & 0.0243 & 633 \\ DR-GNN & **0.0517** & **0.0238** & **0.0774** & **0.1044** & **0.0704** & **0.0905** & **0.0327** & **0.0183** & **0.0102** & **0.0193** & **0.0086** & **0.0322** & 648 \\  

Table 1. The performance comparison on popularity shift datasets using LightGCN backbone. The best result is bolded and the runner-up is underlined. OOM stands for out of memory.

   &  &  \\   & NDCG & Precision & Recall & NDCG & Precision & Recall \\  LightGCN & 0.0802 & 0.0243 & 0.1576 & 0.0736 & 0.0118 & 0.1504 \\ LightGCL & 0.0839 & 0.0243 & 0.1577 & 0.0734 & 0.0118 & 0.1476 & 0.0547 \\ SGL & 0.0839 & 0.0245 & 0.1631 & 0.0740 & 0.0122 & 0.1548 \\ InvCF & 0.0842 & 0.0241 & 0.1668 & 0.0742 & 0.0122 & 0.1539 & 667 \\ BOD & 0.0817 & 0.0219 & 0.1542 & **0.0816** & 0.0115 & 0.1481 & 647 \\ APDA & 0.0821 & 0.0243 & 0.1706 & 0.0756 & 0.0124 & 0.1588 & 648 \\ GAT & 0.0836 & 0.0243 & 0.1633 & 0.0722 & 0.0119 & 0.1500 & 649 \\ GraphDA & 0.0847 & 0.0245 & 0.1641 & 0.0748 & 0.0117 & 0.1528 & 670 \\ DR-GNN & **0.0874** & **0.0255** & **0.1744** & 0.0774 & **0.0126** & **0.1624** & 6121 \\  

Table 2. The performance comparison on temporal shift datasets using LightGCN backbone. The best result is bolded and the runner-up is underlined.

   &  &  \\   & NDCG & Precision & Recall & NDCG & Precision & Recall \\  LightGCN & 0.0802 & 0.0243 & 0.1576 & 0.0736 & 0.0118 & 0.1504 \\ LightGCL & 0.0839 & 0.0243 & 0.1577 & 0.0734 & 0.0118 & 0.1476 \\ SGL & 0.0839 & 0.0245 & 0.1631 & 0.0740 & 0.0122 & 0.1548 \\ InvCF & 0.0842 & 0.0241 & 0.1668 & 0.0742 & 0.0122 & 0.1539 \\ BOD & 0.0817 & 0.0219 & 0.1542 & **0.0816** & 0.0115 & 0.1481 & 647 \\ APDA & 0.0821 & 0.0243 & 0.1706 & 0.0756 & 0.0124 & 0.1588 & 648 \\ GAT & 0.0836 & 0.0243 & 0.1633 & 0.0722 & 0.0119 & 0.1500 & 649 \\ GraphDA & 0.0847 & 0.0245 & 0.1641 & 0.0748 & 0.0117 & 0.1528 & 670 \\ DR-GNN & **0.0874** & **0.0255** & **0.1744** & 0.0774 & **0.0126** & **0.1624** & 6121 \\  

Table 3. The performance comparison on exposure shift datasets using LightGCN backbone. The best result is bolded and the runner-up is underlined.

According to table 3, DR-GNN can also handle distribution shift caused by exposure bias well. Besides, we noticed that on the Yahoo dataset, BOD's NDCG metric surpasses other comparison methods, but it is weaker than LightGCN in terms of Precision and Recall metrics. We believe this might be due to BOD's weight adjustment strategy overfitting to certain interactions, causing them to rank higher, which leads the recommendation model to concentrate too much on a few highly relevant results. Furthermore, BOD's performance on the coat dataset is only slightly better than LightGCN, indicating that its performance is not stable.

The experiments under the aforementioned three settings demonstrate that our method can handle different types of distribution shifts.

### Ablation Study (RQ3)

We conducted an ablation study to investigate the effects of different modules in DR-GNN, including the DRO and GEA modules. We compared the DR-GNN with its two variants "DR-GNN w/o DRO" and "DR-GNN w/o GEA" based on whether the DRO and GEA modules were enabled. The results in Table 4 demonstrate that the use of DRO for graph aggregation operations can significantly enhance the model's performance, and GEA further improves the model's effectiveness. Surprisingly, we also found that the simple use of GEA could also yield some gains. This may be attributed to the additional edges introduced by GEA, which have accelerated the convergence of smoothness regularization. The ablation study highlights the fact that all modules in DR-GNN can boost the model's learning.

### Role of the parameter \(\) (RQ4)

The parameter \(\) in Eq.(7) plays an important role in the DRO. Based on the previous derivation, the role of \(\) is to control the size of uncertainty set. A smaller \(\) indicates a larger uncertainty set for optimizing. However, an excessively large search space can easily lead to model overfitting. As \(\) tends towards zero, DRO will excessively amplify the weight of the node with the maximum smooth regularization loss, which can easily lead to overfitting to the meaningless distribution. When \(\) is infinitely large, the worst-case distribution tends towards a uniform distribution. At this point, DR-GNN without GEA is equivalent to LightGCN. Therefore, \(\) is an important hyperparameter that needs to be carefully tuned. Different values need to be set depending on the degree of distribution shift in the dataset. Empirically, we search for \(\) within the range of \(\).

Besides, we empirically verified the relationship between the optimal \(\) and the degree of distribution shift on the popularity shift dataset. We utilize the KL-divergence between item frequencies as a metric to measure the degree of distribution shift between the training and testing sets, and then tried to train the model by tuning different \(\) parameters. Figure 4(a) shows the NDCG performance of DR-GNN with different \(\) under varying degrees of popularity shift in different datasets. It can be observed that either excessively large or small \(\) can impact the performance, and the optimal value varies across different datasets. In Figure 4(b), we further quantified the distribution shift using KL divergence. It was found that datasets with a larger degree of shift require a smaller optimal \(\), as they necessitate a larger uncertainty set. This corresponds to our previous explanation.

## 5. Related Work

### GNN-based Recommender System

In recent years, graph-based recommendation systems have attracted extensive attention and research. Such systems leverage the structure of graphs to discover and infer user preferences and interests, thereby providing more personalized and accurate recommendations. Compared to the traditional collaborative filtering method that only use first order information of node, the GNN can capture higher order signal in the interactions by aggregating the information from neighboring nodes. LightGCN(Li et al., 2019) simplify the original stucture of GCN(Li et al., 2019) by dropping the feature transformation nonlinear activation and self-connection. NIA-GCN(Zhou et al., 2019) improves the aggregation way in the GCN. They use PNA aggregator to model more complex interactions in the graph data.

Contrastive learning has also been extensively applied in graph-based recommendation models. SGL(Wang et al., 2019) applies data augmentation to graph-structured data through node dropout, edge dropout and random walk, and constructs the positive and negative pairs using the different views of the nodes' embedding. Other CL-based recommenders mainly improve the way of augmenting the graph.

   Dataset & Method & NDCG & Precision & Recall \\   & LightGCN & 0.0369 & 0.0170 & 0.0563 \\  & DR-GNN w/o DRO & 0.0391 & 0.0174 & 0.0583 \\  & DR-GNN w/o GEA & 0.0494 & 0.0231 & 0.0761 \\  & DR-GNN & **0.0517** & **0.0238** & **0.0774** \\   & LightGCN & 0.0136 & 0.0060 & 0.0221 \\  & DR-GNN w/o DRO & 0.0161 & 0.0070 & 0.0265 \\   & DR-GNN w/o GEA & 0.0182 & 0.0081 & 0.0305 \\   & DR-GNN & **0.0193** & **0.0086** & **0.0322** \\   

Table 4. Ablation Study on Gowalla and Yelp2018

Figure 4. Analysis of the role of \(\). (a) Left: The performance of DR-GNN in terms of NDCG across different \(\) on three datasets with varying degrees of distribution shift. (b) Right: The relationship between the degree of distribution shift and the optimal \(\).

LightGCL(Chen et al., 2019) uses SVD to generate new graph structures, which emphasizes the important signal in the user/item interactions. SimGCL(Shi et al., 2019) and XSimGCL(Xu et al., 2019) uses the random noise-based data augmentation on embeddings avoiding the popularity bias by making features more uniformly distributed.

Some methods are designed to target the OOD problem on GNN-based recommendation models. APDA(Zhou et al., 2019) dynamically adjusts the edge weights of the graph, reducing the impact of popular items while amplifying the influence of unpopular items. GraphDA(Chen et al., 2019) reconstructs the adjacency matrix of the graph through a pre-trained encoder, thereby achieving signal augmentation and denoising within the graph. RGCF(Chen et al., 2019) improves graph structure learning by identifying more reliable message-passing interactions, while simultaneously maintaining the diversity of the enhanced data.

Despite these efforts, the aforementioned methods can only address specific types of OOD problems, that is, they only focus on the distribution shift resulting from certain specific factors, such as noise and popularity bias, hence are not generic. Moreover, these methods are mostly based on heuristic rule design and lack theoretical guarantees.

### OOD in Recommender System

When training models, it is a common assumption that test data originates from the same distribution as the training data. However, in real-world scenarios, models may encounter test data that deviates from the distribution of the training data - a phenomenon known as Out-of-Distribution. The presence of OOD data can potentially precipitate a degradation in the model's performance. There are many methods used to solve the OOD problem in recommender systems.

One category of methodologies endeavors to alleviate the impact of distribution shift through the identification of invariant components within embeddings. InvCF(Shi et al., 2019) effectively mitigates the impact of popularity shifts by incorporating an auxiliary classifier that formulates recommendations predicated on popularity. InvPref(Shi et al., 2019) and HIRL(Luo et al., 2019) seperate the dataset into multiple environments by attributing an environment variable to each interaction and then leverage invariant learning to automatically identify elements that remain constant irrespective of the environment. Some of the works employ causal learning in addressing the OOD problem. COR(Zhou et al., 2019) uses causal graph modeling and counterfactual reasoning to address the effect of out-of-date interactions. CausPref(Shi et al., 2019) utilizes a differentiable causal graph learning approach to obtain the invariant user preferences. Other works, such as BOD(Wang et al., 2019) applies bi-level optimization to ascertain the weight for each interaction to negate the effects of noisy interactions.

However, the kind of works are not specifically designed for GNN-based recommender system and thus fail to address the impact of distribution shifts on the structure of the graph models themselves.

### Distributionally Robust Optimization

Distributionally Robust Optimization is a method for addressing OOD problems. The primary goal of DRO is to find a solution that is not only optimal for the given data distribution but also robust against variations in the data distribution, i.e. the family of distributions consisting of all distributions within a certain distance from the current data distribution. Many measures of distribution distance are used in DRO, including KL-divergence(Krizhevsky et al., 2009), Wasserstein-distance(Srivastava et al., 2014), MMD distance(Munroul et al., 2016). It has been found that DRO tends to induce model overfitting to the noisy samples(Shi et al., 2019) and GroupDRO(Shi et al., 2019) was introduced to address this issue.

DRO has also been applied in the field of RS. S-DRO(Shi et al., 2019) categorizes users into different groups based on the popularity of the items they interact with, and then employs group DRO to improve long-term fairness for disadvantaged subgroups. DROS(Shi et al., 2019) applies DRO to sequential recommendation tasks to address the OOD problem in the streaming of recommendation data. These methods are not used on GNN-based RS.

Some research has explored the application of DRO on GNN(Chen et al., 2019; Chen et al., 2019; Chen et al., 2019). On one hand, these methods fundamentally aim to address distributional shifts caused by noise in node embeddings, which is distinct from our approach that considers distributional shifts within the graph's topological structure. On the other hand, these methods are not applied in recommendation systems and do not take into account the challenges posed by the unique characteristics of recommendation datasets when applying DRO.

## 6. Conclusion

This paper proposes an method DR-GNN, which introduces DRO into the aggregation operation of GNN, to solve the problem of existing graph-based recommendation systems being easily affected by distribution shift. Based on comparative experiments under multiple datasets and settings, as well as visualization studies on real datasets, DR-GNN has been proven to effectively solve the distribution shift problem, enhancing the robustness of graph models.

A direction worthy of exploration in future work is the application of DRO based on other distance metrics, such as the Wasserstein distance, MMD distance, etc., to address some of the shortcomings of the KL divergence-based DRO discussed in this paper. We believe that DR-GNN could provide a new perspective for future work aimed at enhancing graph-based RS robustness.