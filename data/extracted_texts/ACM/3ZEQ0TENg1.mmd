# Taxonomy Completion via Implicit Concept Insertion

Anonymous Author(s)

###### Abstract.

High quality taxonomies play a critical role in various domains such as e-commerce, web search and ontology engineering. While there has been extensive work on expanding taxonomies from externally mined data, there has been less attention paid to enriching taxonomies by exploiting existing concepts and structure within the taxonomy. In this work, we show the usefulness of this kind of enrichment, and explore its viability with a new taxonomy completion system ICON (Implicit **CON**cept Insertion). ICON generates new concepts by identifying implicit concepts based the existing concept structure, generating names for such concepts and inserting them in appropriate positions within the taxonomy. ICON integrates techniques from entity retrieval, text summary, and subsumption prediction; this modular architecture offers high flexibility while achieving state-of-the-art performance. We have evaluated ICON on two e-commerce taxonomies, and the results show that it offers significant advantages over strong baselines including recent taxonomy completion models and the large language model, ChatGPT.

2018 acmcopyright ACM ISSI 978-1-4503-XXXX-X/18/06$15.00 [http://doi.org/10.11450/1878-1-4503-XXXXXX](http://doi.org/10.11450/1878-1-4503-XXXXXX)

2

## 1. Introduction

A taxonomy is a hierarchical knowledge graph where edges in the graph represent _is-a_ relationships between concepts. It takes the form of a directed acyclic graph (DAG), and is sometimes simplified to a tree. For a wide range of domains such as natural sciences, medicine, web search, and e-commerce, taxonomies form the backbone of domain knowledge, thus serving many downstream applications such as query answering, natural language understanding, recommendation, and information retrieval.

In order to meet the needs of these applications, taxonomies are expected to be complete and accurate. Completeness refers to the nodes in a taxonomy (often called concepts) covering as many concepts relevant to the underlying domain as possible, and accuracy refers to the edges in a taxonomy (often called subsumption relations) correctly capturing the _is-a_ relationships in the domain, possibly through transitive closure. There is a large body of research oriented at completing taxonomies (see related works in Section 2), but most of these studies focus on deriving new concepts from external resources. However, taxonomies can also be enriched from information within themselves. This is often observed as a concept whose existence is implied by the structure of the taxonomy, but is currently missing from it (Kang et al., 2018). We call these concepts **implicit concepts**. Consider a typical segment of an e-commerce taxonomy with \(\) concepts, as shown in Figure 1. The top concept, "Clothing, Shoes, & Accessories (CSA)", is divided along gender at level 2, and both subconcepts are further divided along product type at level 3. The concepts at level 3 imply the existence of standalone "clothing" and "shoes" concepts, as illustrated in Figure 1, but these concepts are in fact missing.1 What is _not_ a remedy to this problem is to replace the level 2 concepts with the two inferred concepts, as the current Men's and Women's CSA concepts will then be lost. Instead, one must break the tree structure to add these new concepts. This is clear when we consider more profound examples such as the one illustrated in Figure 2, where the implicit concept reveals semantic connections between concepts in distant branches of the taxonomy. In this case the implicit concept "Sporting Shoes" reveals connections between sub-concepts of CSA and Sporting Goods; it is a sub-concept of both these level-1 concepts, and its sub-concepts form part of the intersection between these level-1 concepts (we show only a fraction of these sub-concepts due to space limits). Similar to these examples, most implicit concepts are intermediatethe nodes that reflect alternative ways to organise the hierarchy. However, the pool for possible intermediate nodes is exponential in terms of taxonomy size. Therefore, careful identification of intermediate nodes is required in order to find useful implicit concepts that can improve the taxonomy's quality and benefit downstream applications.

In this work we aim to address the issue of finding such implicit concepts and inserting them in the taxonomy; we call this problem

Figure 1. Implicit concept examples (“Clothing” and “shoes”) in an e-commerce taxonomyimplicit taxonomy completion_ (see Section 3.2 for the formal definition). To the best of our knowledge, there is no existing work that directly tackles implicit taxonomy completion. GenTaxo (Sen et al., 2019) is closely related in that it generates new concept names and predicts whether the new concepts can be inserted into given positions in the taxonomy. However, it doesn't identify where a new concept might be useful or where to insert it in the taxonomy; this information must be provided to the system in the form of a complete set of parent and child concepts for the proposed new concept.

Implicit taxonomy completion can be decomposed into three sub-tasks: identifying potentially useful implicit concepts, naming implicit concepts, and inserting them in the taxonomy. We propose a novel taxonomy completion system, Implicit **CO**Ncept Insertion (ICON), that integrates solutions for each of the three sub-tasks. First, we use ideas from _entity retrieval_ to address the identification sub-task; specifically, we use a KNN algorithm with BERT embeddings to identify clusters of existing concepts that might correspond to an implicit concept. Second, we use ideas from _text summarization_ to address the naming sub-task; specifically, we use TS (Li et al., 2019) to create a label for the union of the concepts selected in the first sub-task. Finally, we use ideas from _taxonomy completion_ to address the insertion sub-task; specifically, we use a modified enhanced traversal algorithm that employs BERTSubs (Devlin et al., 2019) to perform subsumption tests. Note that the traversal might identify an existing equivalent concept, in which case ICON simply adds any missing _i_-_a_ links.

We have evaluated ICON on two large real-world taxonomies, eBay and the concept hierarchy extracted from AliOpenKG. Our experiments indicate a dramatic improvement over existing techniques and baselines. We also examine in ablation studies the effects of varying hyperparameters and search options.

Our contributions are summarised as follows:

1. Proposal of the implicit taxonomy completion task based on the observation of real-world taxonomies.
2. Design of a flexible framework for implicit taxonomy completion.
3. Implementation of the framework in the ICON system with novel optimisation and language model fine-tuning methods for higher scalability.
4. Extensive evaluation demonstrating ICON's strength at implicit taxonomy completion.

## 2. Related Work

### Taxonomy Construction

Taxonomy construction is the building of a taxonomy, often from scratch, using a corpus. Traditionally, taxonomy construction can be divided into concept taxonomy construction, which applies semantic inductions on candidate concepts to gradually build hierarchies, and topic taxonomy construction, which features some clustering on keywords (Shen et al., 2019). Early taxonomy construction methods are usually graph-based (Shen et al., 2019; Li et al., 2019; Li et al., 2019), or distribution-based (Shen et al., 2019). These methods use either graph models or probabilistic models to propagate taxonomic knowledge, and the atomic semantic induction is usually based on simple lexicosynatic features. Later methods incorporate word embeddings for better semantic understanding (Shen et al., 2019; Li et al., 2019). Reinforcement learning has also been applied for taxonomy construction, where the reward is set to a similarity metric between the constructed taxonomy and a gold-standard taxonomy (Li et al., 2019).

### Taxonomy Expansion

Taxonomy expansion inserts new concepts into existing taxonomies. Large open source taxonomies such as WordNet (Sen et al., 2019) and MeSH (Sen et al., 2019) have led to increased attention on taxonomy expansion, since improving upon a well curated taxonomy is easier than building one from the ground up. In contrast to taxonomy construction, taxonomy expansion emphasises the interaction between new concepts and existing concepts. An early method known as APOLLO (Shen et al., 2019) uses graph knowledge propagation algorithms to predict category memberships for text mentions. Most later methods are embedding-based, with the central idea of first encoding both the new concept and the known concepts with embeddings, then decoding to obtain a concept prediction, as practiced in ETF (Shen et al., 2019). Further improvements include Arborist (Arborist, 2019) which consideres implicit edge semantics, STEAM (Shen et al., 2019) which builds the mini-path corpus to improve encoding quality. Other recent models explore different ways to utilise the tree structure or multimodal information as features to improve ranking. For instance, TaxoExpan (TaxoExpan, 2019) injects neighbourhood information of the anchors, forming a mini-graph known as Egonet; HyperExpan (TaxoExpan, 2019) is an improvement that shares a similar philosophy but uses hyperbolic embeddings which is better at encoding hierarchical structures (Bahdanau et al., 2015; Li et al., 2019): HEF (Shen et al., 2019) builds the ego-tree which consists of all the known ancestors and children of a node, in an effort to maximise hierarchical information visible to the model. TaxoExpan and HyperExpan use Graph Neural Networks for decoding, while HEF uses pre-trained language models (PLMs).

### Taxonomy Completion

The main drawback of taxonomy expansion is that the task only considers superconcepts of the candidate. It does not express the full ground truth when the candidate is not leaf. Therefore, TMN (TaxoExpan, 2019) spearheads a more realistic task known as taxonomy completion, which considers both superconcepts and subconcepts of the candidate. The primary-auxiliary scorer structure of TMN is used by many later models. TaxoEnrich (Li et al., 2019) adopts the TMN structure,

Figure 2. Implicit concept “Sporting Shoes” that connects concepts spreading across different level 1 branches

injects sibling information, and replaces the vector dot product metric with PLMs. QEN (Yang et al., 2017) adds even more information and evaluates (candidate, parent, child, sibling) quadruplets instead of triplets.

Together with the taxonomy expansion methods, these methods share a common pattern in utilising the contextual data: feed a large set of features into the model, obtain a representation and decode this represention for a score. However, both the model and the corpus quickly grow in size, increasing the computational overhead that is required to process marginally relevant features. Therefore, we try to handle contextual data differently in our work: we use minimal information for a single prediction, and perform multiple predictions to _search_ for the optimal subconcepts and superconcepts, where the search is naturally guided by taxonomic hierarchy.

### Implicit Concepts

Despite the common presence of implicit concepts, very little research has addressed this issue. Existing work has focussed on determining whether a _mention_ (in some external source) corresponds to a concept missing from a knowledge base (Bowman et al., 2016; Chen et al., 2017; Chen et al., 2017), but this approach can only spot implicit concepts one at a time and only when a candidate is provided from an external source.

## 3. Preliminaries

### Taxonomy

A _taxonomy_\(=(,)\) is a transitively reduced DAG where the nodes \(n\) are concepts, and the edges \(e\) are _is-a_ relations. We say \(n_{1}\) is a _parent_ of \(n_{2}\), or equivalently \(n_{2}\) is a _child_ of \(n_{1}\), if \((n_{2},n_{1})\). The collection of all parents and children of a concept are denoted \(p(n)\) and \(c(n)\) respectively. The _ancestors_ of \(n\), denoted \((n)\), is the set of all nodes reachable from \(n\) in the DAG. Conversely the _descendants_ of \(n\), \(D(n)\), is the set of all nodes reachable from \(n\) when all the edges in \(\) are reversed. \(n_{1}(A(n_{2})\{n_{2}\})\) is also denoted as \(n_{2} n_{1}\), which reads "\(n_{1}\)_subsumes_\(n_{2}\)". In particular, this subsumption is _direct_ if \(n_{1} p(n_{2})\). Each concept should have a _label_, consisting of the concept's name (a text string) and optionally some additional natural language description. Where there is no ambiguity we will use \(n\) to refer to both a concept and its label. The _top_ concepts \(_{}\) and _bottom_ concepts \(_{}\) of \(\) are defined as the concepts without parents and children respectively. An _intermediate concept_ in \(=(,)\) is defined as a non-singleton subset of \(\). We define a taxonomy \(=(_{},_{})\) to be a _subtaxonomy_ or _subgraph_ of a taxonomy \(=(_{},_{})\) if \(_{}_{}\) and \(_{}_{}\).

### Problem Statement

Given a taxonomy \(=(,)\) and a candidate concept label \(q\) (candidate for shorthand), _taxonomy completion_ is the task of inserting \(q\) into \(\). It consists of finding \(q\)'s parents \(p(q)\) and children \(c(q)\) from \(\), and replacing \(\) with the transitive reduction of \(^{}=(\{q\},\{(q,p)|p p(q)\} \{(c,q)|c c(q)\})\), such that \(^{}\) is a DAG. Notice that the inserted concept \(q\) could coincide with an existing concept in \(\), in which case we add extra edges to \(\) but do not add extra nodes.

Implicit taxonomy completion is a special case of taxonomy completion where the candidates are generated from the existing taxonomy. An implicit concept within our scope of interest is simply an intermediate concept \(I\). We define implicit concept insertion as follows: given a taxonomy \(=(,)\) and an intermediate concept \(I\), generate a label \(q\) for \(I\)'s most specific super-concept and insert \(q\) into \(\). Implicit taxonomy completion consists of (repeatedly) identifying "relevant" intermediate concepts and inserting them via implicit concept insertion. An intermediate concept is relevant if the resulting implicit concept insertion usefully enriches the structure of the taxonomy; this must be empirically determined and may be application dependant.

## 4. Methodology

We will start by outlining the overall architecture of ICON, which includes its three main components, stepwise workflow, relations between its outer loop and inner loop, and three modes of operation. Next, we present the self-supervised training scheme for each submodel including its data and training objectives. These models can be freely updated or replaced for later improvements.

### Overall Architecture

ICON adopts an iterative workflow that consists of two nested loops. The outer loop ranges over the taxonomy and creates clusters from which intermediate concepts are built. The inner loop ranges over subsets of each cluster, generates intermediate concepts and performs implicit concept insertion on them. Figure 3 illustrates the basic workflow of ICON, and Figure 4 provides a running example of how ICON processes a single seed concept.

#### 4.1.1. The outer loop

The outer loop (steps 1 and 2 in Figure 3) covers the entity retrieval part of the task. It starts with a seed \(s\) that is either given as input or randomly selected. \(s\) can be either a concept in the taxonomy or a textual phrase, and all operations in this iteration happen in the semantic vicinity of \(s\). A sub-model, which we call the \(\) model because our implementation of it uses k-nearest neighbours on embeddings, retrieves the \(k\) concepts most similar to \(s\), denoted as its cluster \((s)\). A practical setting of \(k\) is between \(5\)-\(10\). The reason we select similar concepts is to increase the chance a subset built from these concepts identifies an implicit concept. In Figure 4a, we illustrate as an example the case where \(s=\)'s Vintage T-Shirts. Setting \(k=5\), the \(\) model retrieves the following relevant concepts (shown as light green nodes): Men's T-Shirts, Men's Equestrian Shirts, Men's Western Show Shirts, Vintage Sports Shirts. These four concepts together with \(s\) form the cluster.

#### 4.1.2. The inner loop

The inner loop (steps 3 to 6 in Figure 3) receives \((s)\) and covers the other two sub-tasks: label generation and concept insertion. Given a cluster \((s)\), it first enumerates all the subsets of \((s)\) up to size \(m\). There is the option to limit the subsets to those that include \(s\), which leads to \(O(|k^{m-1}|)\) subsets as opposed to \(O(|k^{m}|)\) when we do not impose that limit. For tractable computation \(m\) is usually set to \(2\) or \(3\). For each subset \(\{c_{1},c_{2},,c_{m}\}\), the GEN model is called to generate a new label \(q\) that summarises \(\{c_{1},c_{2},,c_{m}\}\). In our example, we set \(m=2\) and one of the subsets thus built is \(\{\)'s Vintage T-Shirts, Men's Western Show Shirts\}. Our GEN model then produces "Shirt" as its label, indicated by the blue arrows in Figure 4a. The new labelledcandidate_ concept will then be inserted into the taxonomy using enhanced traversal.

Enhanced traversalEnhanced traversal [(1; 2; 9)] can be understood as a two-stage Breadth First Search (BFS) that locates where a candidate concept \(q\) should be inserted in the taxonomy. The first stage is top-down, searching for the lowest / most specific parents of \(q\). The second stage is bottom-up and searches for the highest / most general children of \(q\). Both searches use the hierarchy to prune branches; e.g., if a node has already been determined to not subsume \(q\), then neither can any of its children subsume \(q\). If the two searches intersect at a concept \(D\), then \(D\) is both a parent and a child of \(q\) and we have \(q D\). Algorithm 1 gives pseudocode for the basic variant of enhanced traversal. In practice, we make several modifications to the search algorithm to tackle the uncertainty of the neural-based SUB model and improve search speed:

* _Tolerance._ The standard enhanced traversal uses logical reasoning for the atomic subsumption test, which provides a semantically deterministic subsumption test. Here we use the PLM-based SUB model as a substitute. Using a neural model could exploit some likely subsumptions but also introduce inevitable errors. To compensate for errors we introduce the option to add tolerance to pruning, which allows the algorithm to explore up to depth \(r\) below / above a node that has failed the subsumption test. High tolerance runs the risk of making search slow as more candidates are pushed into the queue to be visited.
* _Foreeful inclusion of results._ Since \(q\) is intended to represent a union of several concepts, it makes sense to automatically include these _base_ concepts into \(c(q)\). In addition, it makes sense to include the lowest common ancestor (LCA) of base concepts in the original taxonomy into \(p(q)\). This is because the LCA must be at least as general as the union, assuming that GEN model provides a faithful representation. Turning this option off allows the SUB model's decisions to override the GEN model's assumptions.
* _Search space constraints._ A significant optimisation on the search is to limit the search space to a highly relevant subgraph of the whole taxonomy. Since we already have a good estimate of what could be relevant to \(q\) (the base concepts, or the cluster from which the base concepts are selected), a natural choice of this subgraph is the subgraph _spanned_ by the bases (cluster) _up_ to the LCA. More formally, this is the subgraph induced by all nodes that transitively subsume at least one base (cluster member), and are transitively subsumed by the LCA of the bases (cluster). Restricting the search space has another safety benefit when forceful inclusion is enabled, since the SUB model might erroneously predict a descendant of a base concept to be in \(p(q)\), or an ancestor of the LCA in \(c(q)\), causing cyclic subsumption.

Update taxonomy.We consider three cases when processing the search results (\(p(q)\), \(c(q)\)):

Figure 4. Example for the seed “Men’s Vintage T-Shirts”

Figure 3. Workflow of ICON. In steps 1 - 2, it retrieves a cluster of nodes for a given seed. In steps 3 - 6, it traverses some subsets of the cluster, generates a labelled concept for each subset and inserts it into the taxonomy. After looping steps 3 - 6 over all the intended subsets, it goes back to step 1 and selects a new seed. Steps 1 - 2 form the outer loop, and steps 3 - 6 form the inner loop.

* _Reject_. When \((q)=\), we say that \(q\) has nowhere to go in the taxonomy and is therefore rejected. The system would then process the next intermediate concept generated in step 3 of Figure 3.
* _Insert_. When \((q)\) and \((q) c(q)=\), we do not find any existing node that equals \(q\). Therefore, we declare \(q\) to be a new concept, and insert this concept with the predicted edges accordingly into the taxonomy.
* _Merge_. When \((q) c(q)\), we find that at least one existing node equals \(q\). However, if we declare equality across all nodes in \((q) c(q)\), we will be effectively gluing several existing nodes together, which is usually wrong and can cause logical contradictions. Therefore, we keep only the one node in \((q) c(q)\) with the highest confidence score, denoted as \(e\), merge it with \(q\), and set other nodes to either a parent or a child of \(e\), depending on which prediction has a higher confidence2. Notice that, in this case, we effectively perform _missing link prediction_ on the taxonomy.

For our running example, we adopt a search space spanned by the cluster, obtaining a subgraph of 21 concepts. Our search on this subgraph returns the results illustrated in Figure 4b. The new concept Shirt will be inserted to the taxonomy under CSA (Clothing, Shoes and Accessories), with the five concepts in the cluster being its children. Its predicted subsumptions are marked with violet arrows. Notice that the five predicted child concepts are scattered across different branches in the taxonomy, but with our approach we can establish links across these branches via suitable intermediate concepts.

#### 4.1.3. Modes of operation

ICON can function in three modes, depending on the task. _Auto_ mode is fully automated and exploits the full workflow in Figure 3. The system keeps selecting new seeds from nodes that haven't been involved in any clusters yet, until having all of the original nodes of the taxonomy involved in at least one cluster. _Seminauto_ mode is frequently used in evaluation settings, in which seeds are given by manual inputs, replacing step 1 in Figure 3. _Manual_ mode is used for conventional taxonomy completion where candidate nodes are directly given by manual input, and only steps 4b and 5 (optionally 6) will be called.

### Required Models

As mentioned above, the system depends on three models: KNN, GEN, and SUB. Their basic properties are summarised in Table 1. We now describe our implementation for each model.

#### 4.2.1. _Knn model_

We use the cosine metric on contrastive learning and BERT-based embeddings to compute similarity scores. The model is trained with the supervised SimCSE (Devlin et al., 2018) framework. For a taxonomy, we collect nodes that have more than one child to obtain all sibling instances. With careful arrangement, we organise the sibling pairs into mini-batches \((x_{i},x_{i}^{}),i=1,,N\) such that no \(x_{i}\) and \(x_{i}^{}\) are siblings if \(i j\). The training objective is to minimise the following contrastive loss, summed over all mini-batches:

\[=-_{i=1}^{N}(h_{i}h_{i}^{})/r}}{ _{j=1}^{N}^{(h_{i}h_{j}^{})/}} \]

where \(h_{i}\) denotes the hidden representation of \(x_{i}\), \(r\) is a temperature hyperparameter and \((,)=}{\|\|\|\|}\) is the cosine similarity.

#### 4.2.2. _Gen model_

T5 (Kumar et al., 2018) is a transformer-based sequence to sequence language model. We leverage the capability of T5 to summarise concepts into an upper level of abstraction. Our training data follows of \(\{n_{1},n_{2},,n_{m},\ \!:\!n_{LCA}\}\), where \(n_{1},n_{2},,n_{m}\) are \(m\) common delimited node labels with \(m\) ranging from 2 to 5, and \(n_{LCA}\) is the label of the LCA of the nodes. Following standard T5 prompt templates, we prepend the task

 Model & Input & Output & Function \\   KNN & \(\), query concept or phrase & set of concepts & retrieve concepts similar to query \\ GEN & set of concepts & concept label & generate union label \\ SUB & \((n_{1},n_{2})\) & confidence score & predict whether score & \(n_{1} n_{2}\) \\   

Table 1. Basic properties of the adopted models in ICONprompt "summarize:" to all inputs (e.g., "summarize: Men's clothing, Women's clothing, Kid's clothing").

For 70% of the data, the nodes to be summarised are different children or grandchildren of the same LCA. However, for the other 30% we randomly corrupt one or several of the input nodes, so that the overall LCA becomes the global root node of the taxonomy. In these corrupted rows the reference summary is a special placeholder token which indicates that the input node combination is of poor quality. When ICON receives the placeholder token from step 4a in Figure 3, it rejects the intermediate concept immediately and proceeds to the next inner loop. From this type of training, the model learns the ability to discern implicit concepts from combinations of loosely relevant concepts (e.g., {smartphone models, smartphone cases, retail display cases}). The training objective is to minimise the cross entropy loss for language modelling (Golovneaux et al., 2013) between the prediction and the reference.

#### 4.2.3. SUB model

Subsumption prediction can be formalised as an instance of binary classification (whether the child-parent pair holds between the two concepts). We fine-tune BERT for training and inference. Child-parent pairs (\(n_{c},n_{p}\)) are used as positive samples, while negative samples are obtained by replacing the \(n_{p}\) with a node that does not subsume \(n_{c}\). Such a node comes from two sources: random sampling, which creates easily discernible negatives; and graph random walk from the true subsumer, which creates harder negatives. The model is fine-tuned on this dataset with a binary cross entropy loss. The SUB model is the most frequently called of all three sub-models, thus it demands a lightweight build in order to ensure inference speed. We adopt the BERTsubs model with its "isolated class" setting (Devlin et al., 2018) which features fast inference while maintaining satisfying accuracy.

## 5. Evaluation

Following our formulation and decomposition of implicit taxonomy completion, we design our evaluation based on the following research questions:

* **RQ1.** How well do the methods identify the implicit concepts in a given taxonomy?
* **RQ2.** Given an implicit concept, what are the qualities of the labels generated by the methods?
* **RQ3.** Given a concept label, can the methods accurately predict its relevant subsumptions for insertion?

These research questions correspond to the sub-tasks of implicit concept identification, concept name generation, and concept insertion, respectively.

As mentioned in Section 2, we are not aware of any existing system that can perform implicit taxonomy completion. The GenTaxo system can perform some of the relevant tasks, and we extended it to GenTaxo++ by integrating conventional taxonomy completion models as suggested in (Wang et al., 2018). We tried three such models: TaxoExpan (Wang et al., 2018), TMN (Wang et al., 2018), and QEN (Wang et al., 2018). We also used a general purpose large language model, ChatGPT3, a GPT-like model (Brockman et al., 2018) further trained with instruction tuning (Wang et al., 2018). We use ChatGPT with appropriate prompts for each of the three sub-tasks.

A fundamental assumption of taxonomy completion is that the given taxonomy does not represent the full truth (i.e., it is incomplete). Measuring precision is therefore very challenging because many concepts and subsumptions that are absent in the reference taxonomy could still be factually valid (Song et al., 2018). To address this problem, we use **human labelling** in the relevant experiments.

### Experiment Setup

#### 5.1.1. Datasets

We conduct the experiments on two large real-world taxonomies: the concept hierarchy extracted from the e-commerce ontology AliOpenKG4, and eBay's product taxonomy5. Table 2 lists their basic properties. Since both taxonomies are trees, they possess one fewer edges than nodes.

For evaluation, we artifically create implicit concepts by "masking" some of the existing taxonomy concepts. For each taxonomy, we mark its _second-to-bottom_ level nodes as either non-testing or testing in a 4:1 ratio. Nodes marked for testing will be removed from the taxonomy and their children will be directly connected to their grandparents. The original ontology can then be used as a gold standard for implicit concept insertion.

Both ICON and GenTaxo++ require training. While ChatGPT cannot be trained, its performance can be improved with examples of the task as demonstrations in the prompt (Wang et al., 2018). Therefore, we split the datasets into training and validation branches. The validation branch is generated as follows: start from a random leaf node and traverse the taxonomy randomly until 10% of the nodes have been visited, then use the subgraph induced by the visited nodes as the validation branch. The training branch is then constructed from the remaining nodes by adding any edges needed to reconnect the taxonomy after removing the validation branch. We use the training branch to fine-tune each of the methods, or in the case of ChatGPT to provide it with examples.

#### 5.1.2. Task

Our experiment consists of two stages. First, we attempt to recover the masked nodes and generate their labels. This corresponds to RQ1 and RQ2. For ICON, we use the semantic node where we input one seed for each masked node, selected randomly from the children of the masked node; for GenTaxo++, we provide _candidate positions_ for each masked node, which are the sets of parent and child concepts derived from the masked nodes' original positions; for ChatGPT, we give one input for each masked node consisting of all its children, and prompt the model to either create intermediate concepts based on the input, or output a negative response if it cannot find any intermediate concept. We will provide the details of our prompts in Appendix C. Note that this gives a significant advantage to GenTaxo++ and ChatGPT as we provide

   Taxonomy & \(\)Nodes & Max depth & Avg. depth & Language \\  eBay & 20,322 & 6 & 4,235 & English \\ AliOpenKG & 7,100 & 4 & 3,896 & Chinese \\   

Table 2.: Metadata of the taxonomies used in evaluation as input the complete set of child concepts (and in the case of GenTaxo++ also the parent concept) for the masked concept; in effect we have already identified the intermediate concept in the input.

Secondly, we predict the parents and children of the candidate concept labels generated in the first task. This corresponds to RQ3. For GenTaxo++, this task is performed by the conventional taxonomy completion model.

Following the spirit of RQ2 and RQ3, we only compute the relevant metrics on label quality and subsumption prediction with the subset of masked nodes that each method has successfully identified. This subset could be different for different methods.

#### 5.1.3. Metrics

For RQ1, we follow the metrics used in GenTaxo, i.e., **Precision** (P), **Recall** (R) and **F1**. Precision is defined as the number of correctly recovered nodes divided by the total number of generated nodes, and recall is defined as the number of correctly recovered nodes divided by the number of masked nodes.

For RQ2, we adopt **BERtscore**(Rajpurkar et al., 2017), a representation-based metric that captures semantic correlations, instead of the exact match accuracy used in GenTaxo. This is because exact match with the reference (ground truth) is not necessary for regarding a generated label as good. For instance, a prediction of "footwear" where the reference is "shoes" would satisfy most practical demands and should be accepted. Such a prediction would still achieve a high BERTScore, but zero exact match accuracy. The model used to evaluate AliOpenKG results is bert-base-chinese, and the model used for eBay results is roberta-large. The metric contains three scores: **Precision** (denoted Bs-P), **Recall** (Bs-R), and **F1** (Bs-F1). Notice that while BERtscore claims contextual understanding and generally aligns better with human natural language understanding, it is computationally more intensive and lacks interpretability. However, it is the most appropriate metric we find for this evaluation given the lexical variations in labels.

For RQ3, we measure **Precision** and **Recall** of the predicted edges. Recall is defined as the proportion of the masked node's direct parent and children that are successfully recovered:

\[R=}(q) D_{}(q))|}}{| \{(p(q) c(q))|\}} \]

Precision is defined as the proportion of predicted edges that are factually true. However, our previous argument has established that the taxonomy itself is insufficient at expressing all true sub-sumptions. Therefore, we verify all edge predictions with qualified human judge contributors. Since the nodes in both taxonomies are **e**-commerce categories, we give the contributors auxiliary information on these categories (e.g., browse pages6 for items in that category) to assist their judgment. Details of the human evaluation will be presented in Appendix D. With precision and recall, we also calculate and report **F1**. We do not report ranking based metrics which are commonly used in other taxonomy completion works, such as Mean Reciprocal Rank (MRR) and Hit@_k_, because ICON and ChatGPT are not ranking-based.

### Main Results

We now present our evaluation outcomes in Table 3. As previously mentioned, the three GenTaxo-based baselines only differ in the edge prediction task, and the two previous tasks are completed with GenTaxo alone.

For RQ1, we observe that despite having less information on the masked implicit concepts, ICON claims a solid 14% advantage over ChatGPT. This is partially explained by the fact that ChatGPT does not have access to the entirety of taxonomy structure (see Appendix C). Because of the way GenTaxo is used, it essentially faces a binary classification problem for each candidate position (either accept or reject the candidate position). GenTaxo's recall on this problem depends solely on one of its hyperparameters, negative sampling rate \(r_{}\). In fact its recall approaches 1 when \(r_{}=0\). However, we choose \(r_{}\) differently so that it optimises RQ2 metrics, as maximising recall for this binary classification problem is trivial. Precision for GenTaxo is always 1 because GenTaxo always generates one node for each accepted candidate position, and we do not have a sound mechanism to _reject_ nodes: verifying whether a generated node is indeed an implicit concept is a difficult problem, and any human labelling attempt at it would be impeded by ambiguity and the vast amount of undecidable cases.

For RQ2, both ChatGPT and ICON are capable of generating very high quality labels. ICON outperforms ChatGPT slightly due to more comprehensive fine-tuning. ChatGPT uses verbal prompts with examples, which is less effective at adapting its style of generated labels to the style of reference. The GRU model used by GenTaxo lags in the competition, and in particular scores lower precision compared to recall. This is because the GRU in our experiment tends to generate excessively long labels.

For RQ3, our enhanced traversal-based search achieves the highest metrics while using the most lightweight model (other than TaxoExpan), thus displaying the strength of replacing a single prediction (as in previous works (Goyal et al., 2017; Wang et al., 2018)) with multiple graph-guided predictions. ChatGPT achieves similarly high precision due to its grasp of common sense knowledge which makes illogical subsumption predictions unlikely. However, ChatGPT suffers from the problem of searching through a large corpus, lowering its recall.

Performance is roughly similar across the two datasets, with ICON and GenTaxo++'s performance being slightly better on eBay due to its larger size and English labels: its larger size gives the models more fine-tuning data while its English text allows us to use PLMs pre-trained on larger corpora. ChatGPT on the other hand, is barely affected by either difference.

### Ablation Studies

In order to attain high flexibility, ICON has many hyperparameters or settings during every phase of its workflow, some of which we leave to Appendix A due to their relatively minor influence. Here we study the effects of the following hyperparameters, which we find to be the most significant: search tolerance, forceful inclusion, and search space restriction.

The effects of varying each hyperparameter are shown in Figure 5. We conduct these experiments on the eBay dataset, with all other hyperparameters specified in Appendix B.

_Tolerance._ As tolerance increases, the algorithm traverses through a larger portion of the search space, reaching a nearly exhaustive search at \(=4\). Due to imperfections of the SUB model, increased tolerance will monotonically enlarge the sets of predicted parentsand children, shifting towards recall in the P-R tradeoff. The optimal F1 is achieved near \(=1\). The average time spent on each candidate is displayed by the gray curve.

#### Forceful inclusion

Enabling this option will reduce the time cost slightly since the base concepts and their LCAs no longer require model inference. We notice in Figure 4(b) that both precision and recall are improved by forceful inclusion, since without it the SUB model has a chance to reject these correct subsumptions. However, this is partially an artefact of the evaluation setup. In real applications, there is no guarantee that the candidate, only represented by its label which is generated by a language model, faithfully represents its expected semantics of being a union of the base concepts.

#### Search space restriction

Restricting search space in the taxonomy brings tremendous speed improvement: cluster-level restriction makes the search about 300x faster, and base-level restriction is over 700x faster. The average search space is 20,322 concepts without restriction, 38 concepts with cluster-level restriction and 10 concepts with base-level restriction. Eliminating 99.5% of the nodes from the search space proves to not only gain speed massively but also improve the overall F1, demonstrating the strength of our restriction mechanism.

## 6. Conclusion

In this paper, we study the phenomenon of implicit concepts within taxonomies, and propose the task of implicit taxonomy completion. Three sub-tasks are required to perform implicit taxonomy completion. By mapping each sub-task to a well studied research task, we develop ICON, a system to automatically find and insert implicit concepts via enumerating intermediate concepts and enhanced traversal. ICON organically integrates three component models and features high malleability, i.e., it allows flexible control over its search behaviour and supports various levels of human involvement. Extensive evaluation has verified the strength of ICON when applied to real world taxonomies.

In future work we plan to extend this work to a larger domain, in particular general Knowledge Graphs (KGs) and ontologies. The core idea of enhanced traversal, which is using hierarchies to prune search branches, could be applied to any semantic relation that induces a hierarchy. Searching on multi-relational KGs would, however, require a different approach to constructing intermediate concepts and new ways of restricting the search space, suggesting promising directions for future research.

Figure 5. Effects of varying key search parameters based on the eBay dataset

    &  \\   &  &  &  & \)}} \\   & P & R & F1 & Bs-P & Bs-R & Bs-F1 & P & R & F1 \\   &  \\ GenTaxo++ & TMN & **1.000** & 0.680 & **0.809** & 0.711 & 0.822 & 0.763 & 0.516 & 0.553 & 0.534 & 0.577 \\  & GEN & & & & & & & 0.557 & 0.601 & 0.578 & 0.578 \\   & ChatGPT & 0.401 & 0.730 & 0.519 & 0.924 & 0.915 & 0.919 & 0.762 & 0.625 & 0.686 & 0.879 \\  & ICON & 0.544 & **0.859** & 0.665 & **0.938** & **0.942** & **0.940** & **0.805** & **0.737** & **0.769** & 0.80 \\    &  \\   &  &  &  & \)}} \\   & P & R & F1 & Bs-P & Bs-R & Bs-F1 & P & R & F1 \\   & TaxoExpan &  &  &  &  &  &  & 0.378 & 0.368 & 0.373 & 0.846 \\  & TMN & & & & & & & 0.530 & 0.571 & 0.549 & 0.555 \\  & GEN & & & & & & 0.562 & 0.609 & 0.584 & 0.585 \\   & ChatGPT & 0.406 & 0.734 & 0.523 & 0.927 & 0.921 & 0.924 & 0.777 & 0.616 & 0.687 & 0.877 \\   ICON & 0.563 & **0.887** & 0.689 & **0.946** & **0.955** & **0.950** & **0.831** & **0.751** & **0.789** & 0.885 \\   

Table 3. Main evaluation results. Boldface marks the best performers in each metric.