# Subgraph-Aware Training of Language Models for Knowledge Graph Completion Using Structure-Aware Contrastive Learning

Subgraph-Aware Training of Language Models for Knowledge Graph Completion Using Structure-Aware Contrastive Learning

Anonymous Author(s)

###### Abstract.

Fine-tuning pre-trained language models (PLMs) has recently shown a potential to improve knowledge graph completion (KGC). However, most PLM-based methods focus solely on encoding textual information, neglecting the long-tailed nature of knowledge graphs and their various topological structures, e.g., subgraphs, shortest paths, and degrees. We claim that this is a major obstacle to achieving higher accuracy of PLMs for KGC. To this end, we propose a Subgraph-Aware Training framework for KGC (SATKGC) with two ideas: (i) subgraph-aware mini-batching to encourage hard negative sampling and to mitigate an imbalance in the frequency of entity occurrences during training, and (ii) new contrastive learning to focus more on harder in-batch negative triples and harder positive triples in terms of the structural properties of the knowledge graph. To the best of our knowledge, this is the first study to comprehensively incorporate the structural inductive bias of the knowledge graph into fine-tuning PLMs. Extensive experiments on three KGC benchmarks demonstrate the superiority of SATKGC. Our code is available.1

2

## 1. Introduction

Factual sentences, e.g., Leonardo da Vinci painted Mona Lisa, can be represented as _entities_, and _relations_ between the entities. Knowledge graphs treat the entities (e.g., Leonardo da Vinci, Mona Lisa) as nodes, and the relations (e.g., painted) as edges. Each edge and its endpoints are denoted as a triple \((h,r,t)\), where \(h,r\), and \(t\) are a head entity, a relation, and a tail entity respectively. Since KGs can represent complex relations between entities, they serve as key components for knowledge-intensive applications (Han et al., 2017; Li et al., 2017; Li et al., 2018; Li et al., 2018; Li et al., 2018).

Despite their applicability, factual relations may be missing in incomplete real-world KGs, and these relations can be inferred from existing facts in the KGs. Hence, the task of knowledge graph completion (KGC) has become an active research topic (Li et al., 2018). Given an incomplete triple \((h,r,?)\), this task is to predict the correct tail \(t\). A true triple \((h,r,t)\) in a KG and a false triple \((h,r,)\) which does not exist in the KG are called positive and negative, respectively. A negative triple difficult for a KGC method to distinguish from its corresponding positive triple is regarded as a hard negative triple.

Existing KGC methods are categorized into two approaches. An embedding-based approach learns embeddings of entities in continuous vector spaces, but ignores contextualized text information in KGs, thus being inapplicable to entities and relations unseen in training (Li et al., 2018; Li et al., 2018). A text-based approach, based on pretrained language models (PLMs), learns textual representations of KGs, but suffers from a lack of structural knowledge inherent in KGs (Li et al., 2018). Moreover, most methods in this approach fail to account for the long-tailed distribution of entities in KGs, though they can perform KGC even in the _inductive setting_2.

Figure 1. False positive (FP) ratio against the distance (i.e., length of the shortest path) between head and tail of a FP triple in KG across different text-based methods.

Figure 2. False positive (FP) ratio against the degree of tail for a FP triple across different text-based methods.

Meanwhile, contrastive learning has become a key component of representation learning (He et al., 2017; He et al., 2017; He et al., 2018; He et al., 2019), but an important aspect of contrastive learning, i.e., the effect of hard negatives, has so far been underexplored in KGC.

In this paper, we empirically validate significant relationships between the structural inductive bias of KGs and the performance of the PLM-based KGC methods. To demonstrate the limitations of language models exhibiting competitive performance such as SimKGC (He et al., 2017) and StAR (StAR, 2017), we investigate the characteristics of false positive (FP) triples, i.e., false triples ranked higher than a true triple by the models, on two well-known datasets WN18RR and FB15k-237. Our analysis draws two observations.

First, the closer the tail and head of a false triple are to each other in the KG, the more likely the false triple is to be ranked higher than the corresponding true triple. Figure 1 illustrates the distribution of distance, i.e., the length of the shortest path, between the head and tail of a FP triple, where \(g\)-axis represents the FP ratio3 for each distance. For StAR and SimKGC, the FP ratio dramatically grows as the distance decreases (see green and red bars in Figure 1). These findings highlight the importance of considering the proximity between two entities in a triple to distinguish between true and false triples.

Second, the higher the degree of the tail in a false triple is, the more likely that triple is to be ranked higher than the corresponding true triple. Figure 2 illustrates the distribution for the degree of tails of FP triples. They are sorted in ascending order of their degrees, and then are divided into five groups such that each group contains an equal number of distinct degrees. The \(g\)-axis represents the FP ratio4 in each degree group. The FP ratio for StAR and SimKGC increases as the degree of the tail grows (see green and red bars in Figure 2). This indicates that the existing text-based methods have difficulty in predicting correct tails for missing triples with the high-degree tails. Hence, the degree can be taken into account to enhance the performance of language models.

In this paper, we tackle the above two phenomena5, thereby significantly reducing the FPs compared to the existing methods (see blue bars in Figures 1 and 2). For this, we hypothesize that incorporating the structural inductive bias of KGs into hard negative sampling and fine-tuning PLMs leads to a major breakthrough in learning comprehensive representations of KGs.

To this end, we propose a **subgraph-aware PLM training framework for KGC (SATKGC)** with three novel techniques: (i) we sample subgraphs of the KG, and treat triples of each subgraph as a mini-batch to encourage hard negative sampling and to alleviate the negative effects of long-tailed distribution of the entity frequencies during training; (ii) we fine-tune PLMs via a novel contrastive learning method that focuses more on harder negative triples, i.e., negative triples whose heads and tails are close to each other in the KG, induced by topological bias in the KG; and (iii) we propose balanced mini-batch loss that mitigates the gap between the long-tailed distribution of the training set and a nearly uniform distribution of each mini-batch. To sum up, we make three contributions.

* We provide key insights that the topological structure of KGs is closely related to the performance of PLM-based KGC methods.
* We propose a new training strategy for PLMs, which not only effectively samples hard negatives from a subgraph but also visits all entities in the KG nearly equally in training, as opposed to the existing PLM-based KGC methods. Based on the structural properties of KGs, our contrastive learning enables PLMs to pay attention to difficult negative triples in KGs, and our mini-batch training eliminates the discrepancy between the distributions of a training set and a mini-batch.
* We conduct extensive experiments on three KGC benchmarks to demonstrate the superiority of SATKGC over existing KGC methods.

## 2. Related Work

An **embedding-based approach** maps complex and structured knowledge into low-dimensional spaces. This approach computes the plausibility of a triple using translational scoring functions on the embeddings of the triple's head, relation, and tail (He et al., 2017; He et al., 2017; He et al., 2019; He et al., 2019), e.g., \(h+r t\), or semantic matching functions which match latent semantics of entities and relations (He et al., 2017; He et al., 2019; He et al., 2019; He et al., 2019). KG-Mixup (He et al., 2019) proposes to create synthetic triples for a triple with a low-degree tail. UniGE (Li et al., 2019) utilizes both hierarchical and non-hierarchical structures in KGs. The embedding-based approach exploits the spatial relations of the embeddings, but cannot make use of texts in KGs, i.e., the source of semantic relations.

In contrast, a **text-based approach** learns contextualized representations of the textual contents of entities and relations by leveraging PLMs (He et al., 2017; He et al., 2019; He et al., 2019; He et al., 2019; He et al., 2019; He et al., 2019). Recently, a sequence-to-sequence model for KGC (Zhu et al., 2019) highlights the growing trend of leveraging natural language generation models. With the rise of large language models (LLMs), their application in KGs has significantly increased (He et al., 2019). Despite these advancements, PLMs often lack awareness of the structural inductive bias inherent in KGs.

A few attempts have been made to utilize the above two approaches at once. StAR (StAR, 2017) proposes an ensemble model incorporating an output of a Transformer encoder (Vaswani et al., 2017) with a triple score produced by RotatE (He et al., 2019). CSProm-KG (He et al., 2019) trains KG embeddings through the soft prompt for a PLM. Nevertheless, the integration of structural and textual information in a KG in training has not yet been fully realized.

**Contrastive learning**, shown to be effective in various fields (He et al., 2019; He et al., 2019; He et al., 2019; He et al., 2019; He et al., 2019), has recently emerged as a promising approach in the context of KGC (He et al., 2019; He et al., 2019; He et al., 2019). KRACL (Zhu et al., 2019) introduces contrastive learning into an embedding-based method, particularly through the use of a graph neural network (GNN), while HaSa (Sa, 2019) applies contrastive learning to a PLM. HaSa aims to sample hard negatives which are unlikely to be false negatives by selecting tails of a negative triple based on the number of 1- and 2-hop neighbors of its head entity, i.e., the greater the number, the lower the probability that the tail is included in the false negative. In contrast, our contrastive learning method estimates the difficulty of negative triples by utilizing various length of the shortest path between their head and tail, while simultaneously mitigating the adverse effects caused by the long-tailed distribution in KGs. The long-tailed distribution problem, which reflects class imbalance in data, has been extensively studied in other domains, particularly computer vision, where numerous efforts have been made to address this challenge (Kang et al., 2017; Li et al., 2018; Li et al., 2019; Li et al., 2020; Zhang et al., 2021).

Random walk with restart (RWR) (Kang et al., 2017) and its extension, biased random walk with restart (BRWR), have been employed in various domains such as node representation learning (Kang et al., 2017) and graph traversals (Bahdanau et al., 2014). In BRWR, a random walker performs random walks in a graph from the source node. For each iteration, the walker moves from the current node \(u\) to either (i) source with a probability of \(p_{r}\), or (ii) one of the neighbors of \(u\) with a probability of \(1-p_{r}\), where \(p_{r}\) is a hyperparameter. In case (ii), the probability of selecting one of the neighbors is decided by a domain-specific probability distribution, whereas one is selected uniformly at random in RWR. To our knowledge, we are the first to extract a subgraph of KG via BRWR to utilize the subgraph as a mini-batch during training.

## 3. Preliminary

Let \(=(,,)\) denote a KG in which \(\) and \(\) represent a set of entities and relations, respectively, and \(=\{(h,r,t)|h,t,r\}\) is a set of triples where \(h\), \(r\), and \(t\) are a head entity, a relation, and a tail entity, respectively.

**Problem Definition.**Given a head \(h\) and a relation \(r\), the task of knowledge graph completion (KGC) aims to find the most accurate tail \(t\) such that a new triple \((h,r,t)\) is plausible in \(\).

**InfoNCE Loss.** InfoNCE loss (Zhou et al., 2017) has been widely employed to learn the representations for audio, images, natural language, and even for KGs (Zhu et al., 2019; Li et al., 2020). Given a set \(X=\{t_{1},t_{2},...,t_{n}\}\) of \(n\) tails containing one positive sample from \(p(t_{j}|h,r)\) and \(n-1\) negative samples from the proposal distribution \(p(t_{j})\), InfoNCE loss (Zhu et al., 2019) for KGC is defined as:

\[_{X}=_{X}[- X}((h,r,t_{j}))}] \]

\[(h,r,t)=(_{hr},_{t}) \]

where a scoring function \((h,r,t)\) is defined as the cosine similarity between two representations \(_{hr}\) for \(h\) and \(t\), and \(_{t}\) for \(t\).

## 4. Method

In this section, we describe a novel PLM fine-tuning framework for KGC. Figure 3 illustrates the overview of our framework for a given KG. First, for every triple, a subgraph is extracted around that triple from the KG before training (Section 4.1). During training, we keep track of the number of visits for every triple. For each iteration, a subgraph is selected based on that number, and then all forward and inverse triples in the subgraph are fetched as a mini-batch \(\) to the language model (Section 4.2). We adopt the bi-encoder architecture (Zhu et al., 2019) with two pre-trained MPNets (He et al., 2016) as a backbone. Specifically, \(_{hr}\) and \(_{t}\) take the text, i.e., name and description, of \((h,r)\) and \(t\) as input, and produce their embeddings \(_{hr}\) and \(_{t}\) respectively. We then calculate the cosine similarity between \(_{hr}\) and \(_{t}\) for every \((h,r)\) and \(t\) in the mini-batch, and perform new contrastive learning equipped with two topology-aware factors (Section 4.3 and 4.4). The details of the input format are provided in Appendix A, and the model inference is described in Appendix B.

Figure 3. Overview of the proposed training framework, which consists of: (i) Random-walk Based Subgraph Sampling (before training); (ii) Subgraph as a Mini-batch; (iii) Proximity-aware Contrastive Learning; (iv) Frequency-aware Mini-batch Training.

### Random-walk Based Subgraph Sampling

We aim to extract subgraphs from the KG to treat all the triples in the extracted subgraph as a mini-batch for training. For each triple in the KG, therefore, we perform BRWR starting from that triple called a center triple, and the triples visited by BRWR compose the subgraph before training as follows: (i) either head \(h\) or tail \(t\) of the center triple is selected as the start entity \(s\) based on an inverse degree distribution of \(h\) and \(t\), i.e., \(}{|N(h)|^{-1}+|N(t)|^{-1}}\), where \(v\{h,t\}\) and \(N(v)\) denotes a set of \(v\)'s neighbors; (ii) next, we perform BRWR from \(s\) until we sample \(M\) triples where \(M\) is a predefined maximum number (e.g., \(10,000\)). For each iteration in BRWR, a random walker moves from the current node to either \(s\) with a probability of \(p_{T}\) or one of the neighbors of the current node with a probability of \(1-p_{r}\). We define the probability of selecting one of \(u\)'s neighbors \(v N(u)\) as \(p_{v}=}{_{v N(u)}|N(v_{j})|^{-1}}\), which is a normalized inverse degree distribution of the neighbors. Figures 3(a) and 3(b) show the running example of step (i) and an iteration of step (ii).

In this manner, giving more chances for an entity with a lower degree to engage in training alleviates the skewed frequency of entity occurrences throughout training, which in turn enhances the KGC performance. This claim will be validated in Section 5.5.

### Subgraph as a Mini-batch

We now present a way to select one of the sampled subgraphs, and utilize that subgraph as a mini-batch, dubbed **S**ubgraph as **a** **M**ini-batch (**SaaM**). For every iteration, we select the subgraph whose center triple has been the least frequently visited, to prioritize unpopular and peripheral triples. For this, we count the number of visits for all triples in the training set \(\) throughout the training process. The rationale behind this selection will be elaborated in Section 5.5. Next, we randomly select \(||/2\) triples from the subgraph, and feed to the bi-encoders a mini-batch \(\) of the selected triples \((h,r,t)\) and their inverse triples \((t,r^{-1},h)\). For every positive triple \((h,r,t)\), we obtain negative triples \((h,r,)\) with \(t\) replaced by \(||-1\) tails \(\) of the other triples in \(\). As per our observation in Figure 1, these negative triples are likely to be hard negatives, which will facilitate contrastive learning. As a result, we end up with iterating the above process, i.e., selecting the subgraph and feeding the triples in that subgraph, \(||/||\) times for each epoch.

### Proximity-aware Contrastive Learning

Most PLMs for KGC overlook capturing the proximity between two entities in a negative triple in the KG, though they capture semantic relations within the text of triples, as described in the first observation of Section 1. For effective contrastive learning for these methods, we incorporate a topological characteristic, i.e., the proximity between two entities, of the KG into InfoNCE loss with additive margin (Hardt et al., 2016; Liu et al., 2017) by measuring how hard a negative triple is in the KG. For each positive triple \((h,r,t)\) in a mini-batch \(\), we propose loss \(_{(h,r,t)}\) below based on our aforementioned observation, i.e., entities close to each other are more likely to be related than entities far away from each other:

\[_{(h,r,t)}=-}(h,r,t)-}{ })}{(}(h,r,t)-}{})+_{i=1}^{| |-1}(}(h,r,t_{i})}{})} \]

\[_{}(h,r,t_{i})=(_{hr},_{t_{i}})+ _{hti}\]

where \(\) is an additive margin, a temperature parameter \(\) adjusts the importance of negatives, a structural hardness factor \(_{hti}\) stands for how hard a negative triple \((h,r,t_{i})\) is in terms of the structural relation between \(h\) and \(t_{i}\) in \(\), and \(\) is a trainable parameter that adjusts the relative importance of \(_{hti}\). We define \(_{hti}\) as the reciprocal of the distance (i.e., length of the shortest path) between \(h\) and \(t_{i}\) to impose a larger \(_{hti}\) to the negative triple with a shorter distance between \(h\) and \(t_{i}\) in \(\), serving as a hard negative triple.

Since computing the exact distance between every head and every tail in \(\) may spend considerable time, we calculate the approximate distance between \(h\) and \(t_{i}\), i.e., the multiplication of two distances: (d1) the distance between \(h\) and head \(h_{c}\) of the center triple of \(\), and (d2) the distance between \(t\) and \(h_{c}\). Thus, the distance from \(h_{c}\) to every entity in \(\) is pre-computed before training, the multiplication between the two distances is performed in parallel during training, requiring a minimal computational overhead.6

### Frequency-aware Mini-batch Training

For many triples with the same head in the KG, varying only relation \(r\) in \((h,r,?)\) may lead to different correct tails with various semantic contexts. The text-based KGC methods may find it difficult to predict the correct tail for these triples, as their head-relation encoders may generate less diverse embeddings for \((h,r)\) due to much shorter text of relation \(r\) than entity \(h\).

Furthermore, the frequency of every entity that occurs in a set \(\) of triples varies; this frequency distribution follows the power law. However, the text-based methods have shown difficulties in addressing the discrepancy between this long-tailed distribution of the KG and the distribution of a mini-batch. Figure 5 shows the frequency distributions of the original KG, 100 randomly-sampled mini-batches from \(\), and those from SaaM, where \(y\)-axis denotes the frequency ratio of entity occurrences, and entities in \(x\)-axis are

Figure 4. Example of BRWR-based subgraph sampling; (a) probability of selecting start entity \(s\) between \(h\) and \(t\) of a center triple, where \(t\) with a lower degree is more likely to be \(s\) than \(h\); (b) probability of selecting a neighbor of current entity \(u\). A random walker is more likely to move to \(v_{1}\) than to \(v_{2}\) with its degree larger than \(v_{1}\).

sorted in the ascending order of their degrees in KG. Given \(h\) and \(t\), in fact, KGC can be regarded as the multi-class classification task to predict a correct tail \(t\). From this perspective, severe class imbalance originating from the long-tail distribution (red lines) in the KG may result in the less-skewed distribution (green lines) for 100 randomly-sampled mini-batches from \(\), due to a limited number of entities in the mini-batches. In addition, we observe the nearly uniform distribution for 100 mini-batches selected by SaaM (blue lines), since SaaM is likely to give preference to low-degree entities. Therefore, we apply the importance sampling scheme to associate the expected error of \(_{t}(h,r,t)\) with the long-tailed original domain of the KG:

\[_{p_{}(h,r,t)}[_{t}(h,r,t)] =_{p_{}(h,r,t)}[(h,r,t)}{p_{ }(h,r,t)}_{t}(h,r,t)]\] \[=_{p_{}(h,r,t)}[(t)p(h,r|t)}{ p_{}(t)p(h,r|t)}_{t}(h,r,t)]\] \[:=_{p_{}(h,r,t)}[_{}(t) _{(h,r,t)}]\]

where \(p_{}(h,r,t)\) and \(p_{}(h,r,t)\) are probabilities of sampling \((h,r,t)\) by SaaM and randomly in \(\) respectively. A weighting factor \(_{}(t)\) of tail \(t\) denotes \(p_{}(t)/p_{}(t)\). Assume the degree-proportional \(p_{}(t)=|N_{t}|/2||\) and the nearly-uniform distribution \(p_{}(t)=1/||\) where \(||=d_{}||/2\) with \(d_{}\) being the average number of neighbors for every entity. Consequently, \(p_{}(t)/p_{}(t)=|N_{t}|/d_{}|N_{t}|\).

To encourage PLOs to more sensitively adapt to varying relations for many triples with the identical head and to eliminate the discrepancy of the two distributions, we propose to reweight the importance of each sample loss via \(_{}(t)\) above, so the frequency distribution of \(\) becomes close to that of \(\). For each triple in a mini-batch \(\) in SaaM, mini-batch loss \(_{}\) is defined as:

\[_{}=_{(h,r,t)}_{}(t) _{(h,r,t)} \]

where \(_{}(t)\) is defined as \(log(|N_{t}|+1)\) where \(N_{t}\) is a set of \(t\)'s neighbors in the KG7. To sum up, \(_{}(t)\) ensures that the triples with larger-degree tails contribute more significantly to \(_{}\).

## 5. Experiments

### Experimental Setup

For evaluation we adopt widely-used KG datasets WN18RR, FB15k-237 and Wikidata5M. Table 1 shows their statistics, and more details are provided in Appendix D.

For every incomplete triple in the test set, we compute mean reciprocal rank (MRR) and Hits@\(k\) where \(k\{1,3,10\}\) as evaluation metrics based on the rank of the correct entity among all the entities in the KG. We use the mean of the forward and backward prediction results as the final performance measure. The hyperparameters of SATKGC are set based on the experimental results in Appendix F. Further implementation details are described in Appendix E.

### Main Results

We compare SATKGC with existing embedding-based and text-based approaches. Table 2 shows the results on WN18RR, and FB15k-237, and Table 3 shows the results on Wikidata. Due to the page limit, we present comparison with recent models in these two tables. Additional comparison results can be found in Appendices K and L.

SATKGC denotes the bi-encoder architectures trained by our learning framework in Figure 3. SATKGC consistently outperforms all the existing methods on all the datasets. SATKGC demonstrates significantly higher MRR and Hits@1 than other baselines, with Hits@1 improving by 5.03% on WN18RR and 5.28% on FB15k-237 compared to the existing state-of-the-art models.

As shown in Table 3, SATKGC demonstrates its applicability to large-scale KGs, and achieves strong performance in both inductive and transductive settings.8 SATKGC shows an improvement of 7.42% in MRR and 10.84% in Hits@1 compared to the existing state-of-the-art model on Wikidata5M-Ind. On Wikidata5M-Trans, SATKGC achieves an improvement of 13.97% in MRR and 16.93% in Hits@1 over the previous best-performing model.9 In the transductive setting, performance degrades in the order of WN18RR, Wikidata5M-Trans, and FB15k-237, showing that a higher average degree of entities tends to negatively affect performance. A more detailed analysis of performance differences across the dataset is described in Appendix J.

  
**dataset** & **\#entity** & **\#relation** & **\#train** & **\#valid** & **\#test** \\  WN18RR & 40,943 & 11 & 86,835 & 3,034 & 3,134 \\ FB15k-237 & 14,541 & 237 & 272,115 & 17,535 & 20,466 \\ Wikidata5M-Trans & 4,594,485 & 822 & 20,614,279 & 5,163 & 5,163 \\ Wikidata5M-Ind & 4,579,609 & 822 & 20,496,514 & 6,699 & 6,894 \\   

Table 1. Statistics of datasets.

Figure 5. Frequency distributions of entities for original KG, 100 mini-batches randomly sampled from \(\), and those randomly sampled by SaaM on WN18RR and FB15k-237. The entities are sorted in the ascending order of their degrees.

To compare our framework employing MPNets with a LLM-based model, we evaluate the KGC performance of KoPA , which adopts Alpaca  fine-tuned with LoRA  as its backbone, on the FB15k-237N dataset .10 Since KoPA cannot perform inference for all queries within a reasonable time, i.e., 111 hours expected, we rank the correct tail among 1,000 randomly selected entities for both KoPA and SATKGC. As shown in Table 4, SATKGC outperforms KoPA on all metrics, demonstrating that LLMs do not necessarily produce superior results on KGC.

    &  \\   & MRR & Hits@1 & Hits@3 & Hits@10 & MRR & Hits@1 & Hits@3 & Hits@10 \\   \\  TuckER  & 0.466 & 0.432 & 0.478 & 0.518 & 0.361 & 0.265 & 0.391 & 0.538 \\ RotatTuner  & 0.471 & 0.421 & 0.490 & 0.568 & 0.335 & 0.243 & 0.374 & 0.529 \\ KGTuner  & 0.481 & 0.438 & 0.499 & 0.556 & 0.345 & 0.252 & 0.381 & 0.534 \\ KG-Mixup  & 0.488 & 0.443 & 0.505 & 0.541 & 0.359 & 0.265 & 0.395 & 0.547 \\ UniEG2 & 0.491 & 0.447 & 0.512 & 0.563 & 0.343 & 0.257 & 0.375 & 0.523 \\ CompoundE  & 0.492 & 0.452 & 0.510 & 0.570 & 0.350 & 0.262 & 0.390 & 0.547 \\ KRACL  & 0.529 & 0.480 & 0.539 & 0.621 & 0.360 & 0.261 & 0.393 & 0.548 \\ CSPFrom-KG  & 0.569 & 0.520 & 0.590 & 0.675 & 0.355 & 0.261 & 0.389 & 0.531 \\   \\  STAR  & 0.398 & 0.238 & 0.487 & 0.698 & 0.288 & 0.195 & 0.313 & 0.480 \\ HaSa  & 0.535 & 0.446 & 0.587 & 0.711 & 0.301 & 0.218 & 0.325 & 0.482 \\ KG-SZS  & 0.572 & 0.529 & 0.595 & 0.663 & 0.337 & 0.255 & 0.374 & 0.496 \\ SimKGC  & 0.671 & 0.580 & 0.729 & 0.811 & 0.340 & 0.252 & 0.365 & 0.515 \\ GHN \(\) & 0.678 & 0.596 & 0.719 & 0.821 & 0.339 & 0.251 & 0.364 & 0.518 \\   SATKGC (w/o SaaM) & 0.673 & 0.595 & 0.728 & 0.813 & 0.349 & 0.256 & 0.367 & 0.520 \\ SATKGC (w/o PCL, FMT) & 0.676 & 0.608 & 0.722 & 0.820 & 0.355 & 0.261 & 0.386 & 0.537 \\ SATKGC (w/o FMT) & 0.680 & 0.611 & 0.729 & 0.823 & 0.351 & 0.265 & 0.389 & 0.541 \\ SATKGC (w/o PCL) & 0.686 & 0.623 & 0.740 & 0.827 & 0.366 & 0.272 & 0.396 & 0.545 \\ SATKGC & **0.694** & **0.626** & **0.743** & **0.833** & **0.370** & **0.279** & **0.403** & **0.550** \\   

Table 2: KGC results for the WN18RR, and FB15k-237 datasets. “PCL” and “FMT” refer to Proximity-aware Contrastive Learning and Frequency-aware Mini-batch training respectively. The best and second-best performances are denoted in bold and underlined respectively. \(\): numbers are from Liu et al. \(\): numbers are from Qiao et al. .

    &  &  \\   & MRR & Hits@1 & Hits@3 & Hits@10 & MRR & Hits@1 & Hits@3 & Hits@10 \\   \\  TransE  & 0.253 & 0.170 & 0.311 & 0.392 & - & - & - & - \\ TuckER  & 0.285 & 0.241 & 0.314 & 0.377 & - & - & - & - \\ RotatE  & 0.290 & 0.234 & 0.322 & 0.390 & - & - & - & - \\ KGTuner  & 0.305 & 0.243 & 0.331 & 0.397 & - & - & - & - \\   \\  DKRL  & 0.160 & 0.120 & 0.181 & 0.229 & 0.231 & 0.059 & 0.320 & 0.546 \\ KEPLER  & 0.212 & 0.175 & 0.221 & 0.276 & 0.403 & 0.225 & 0.517 & 0.725 \\ BLP-CompLEx  & - & - & - & - & 0.491 & 0.261 & 0.670 & 0.881 \\ BLP-Simple  & - & - & - & - & 0.490 & 0.283 & 0.641 & 0.868 \\ SimKGC  & 0.358 & 0.313 & 0.376 & 0.441 & 0.714 & 0.609 & 0.785 & 0.917 \\  SATKGC & **0.408** & **0.366** & **0.425** & **0.479** & **0.767** & **0.675** & **0.815** & **0.931** \\   

Table 3: KGC results for Wikidata5M-Trans (transductive setting) and Wikidata5M-Ind (inductive setting). The results for embedding-based approach on Wikidata5M-ind are missing as they cannot be used in the inductive setting. Additionally, BLP-CompLEx  and BLP-SimplE  results on Wikidata5M-Trans are missing because they are inherently targeted for inductive KGC.

### Ablation Study

To demonstrate the contribution of each component of our method, we compare the results across five different settings, including SATKGC, as shown in Table 2. In SATKGC (w/o SaaM), instead of using SaaM, a mini-batch of triples is randomly selected without replacement, while both Proximity-aware Contrastive Learning and Frequency-aware Mini-batch training are applied. SATKGC (w/o PCL, FMT) refers to applying only SaaM and using the original InfoNCE loss. SATKGC (w/o FMT) applies both SaaM and Proximity-aware Contrastive Learning, and SATKGC (w/o PCL) applies both SaaM and Frequency-aware Mini-batch Training. The results show that SATKGC (w/o SaaM) performs worse than SATKGC, indicating that substituting SaaM with random sampling significantly hurts the performance. SATKGC (w/o PCL, FMT) already achieves higher Hits@1 than other baselines on WN18RR, highlighting that SaaM alone leads to performance improvement11. Between SATKGC (w/o PCL) and SATKGC (w/o FMT), SATKGC (w/o PCL) achieves higher performance, indicating that Frequency-aware Mini-batch Training contributes more than Proximity-aware Contrastive Learning.

### Performance Across Encoders

To investigate the impact of the encoder architecture and the number of model parameters, we conduct experiments replacing MPNet in SATKGC with BERT-base, BERT-large, DeBERTa-base, and DeBERTa-large (He et al., 2016). Table 5 presents the results. SATKGC is highly compatible with different encoders, showing the competitive performance. DeBERTa-large fine-tuned by SATKGC achieves the best performance on WN18RR. In addition, an increase in the number of model parameters may not necessarily result in enhanced performance on KGC, e.g., BERT-large on WN18RR, and BERT-large and DeBERTa-large on FB15k-237 underperform the smaller encoders.

### Comparing Subgraph Sampling Methods

We investigate how model performance varies depending on the probability distribution \(p_{u}\) used for neighbor selection in Section 4.1.12 We compare the performance of SATKGC using \(p_{u}\) in subgraph sampling with two variants, one with \(p_{u}\) replaced by the uniform distribution (dubbed RWR) and the other with \(p_{u}\) replaced by the degree proportional distribution (dubbed BRWR_P). Table 6 shows the results. The three methods mostly outperform existing KGC methods in Hits@1, with BRWR performing best and BRWR_P performing worst. We also employ a Markov chain Monte Carlo (MCMC) based subgraph sampling method (Zhu et al., 2017), referred to as MCMC (see details of MCMC in Appendix C). Note that in Table 2, MCMC outperforms other methods for Hits@1 on WN18RR.

To verify why variations in \(p_{u}\) lead these differences, we conduct further analysis. Figures 5(a) and 5(b) show the number of visits of every triple and every entity, respectively, in the training process for RANDOM and the SaaM variants, i.e., RWR, BRWR_P, and BRWR. RANDOM, adopted in all the baselines denotes selecting a mini-batch of triples at random without replacement. Triples and entities are sorted by descending frequency. Figure 5(a) demonstrates that RANDOM exhibits a uniform distribution, while RWR, BRWR_P, and BRWR display varying degrees of skewness, with BRWR being the most skewed and BRWR_P the least. Figure 5(b) illustrates that BRWR is the least skewed whereas RANDOM shows the most skewed distribution. BRWR_P, which employs the degree-proportional distribution, extracts many duplicate entities in the subgraphs, leading to the most skewed distribution among the SaaM variants. As a result, a larger skewness in the distribution of number

    \\  Methods & MRR & Hits@1 & Hits@3 & Hits@10 \\  RWR & 0.690 & 0.622 & 0.730 & 0.825 \\ BRWR & **0.694** & **0.626** & **0.743** & **0.833** \\ BRWR P & 0.676 & 0.615 & 0.727 & 0.823 \\ MCMC & 0.687 & 0.609 & 0.736 & 0.825 \\    \\  Methods & MRR & Hits@1 & Hits@3 & Hits@10 \\  RWR & 0.358 & 0.269 & 0.385 & 0.534 \\ BRWR & **0.370** & **0.279** & **0.403** & **0.550** \\ BRWR\_P & 0.351 & 0.262 & 0.383 & 0.530 \\ MCMC & 0.332 & 0.245 & 0.362 & 0.507 \\   

Table 6. Experiments comparing subgraph sampling methods on WN18RR and FB15k-237.

Figure 6. (a) Number of occurrences of triples; (b) number of occurrences of entities. Both are counted throughout the entire training process for RANDOM and the SaaM variants, i.e., RWR, BRWR_P, and BRWR.

    \\  Encoders & MRR & Hits@1 & Hits@3 & Hits@10 & Parameters \\  MPNet & 0.693 & 0.626 & 0.747 & 0.833 & 220M \\ BERT-base & 0.659 & 0.621 & 0.731 & 0.820 & 220M \\ DeBERTa-base & 0.689 & 0.631 & 0.736 & 0.832 & 280M \\ BERT-large & 0.685 & 0.619 & 0.723 & 0.817 & 680M \\ DeBERTa-large & **0.706** & **0.638** & **0.759** & **0.854** & 800M \\   \\  Encoders & MRR & Hits@1 & Hits@3 & Hits@10 & Parameters \\  MPNet & **0.370** & **0.279** & **0.403** & **0.550** & 220M \\ BERT-base & 0.366 & 0.273 & 0.400 & 0.546 & 220M \\ DeBERTa-base & 0.359 & 0.274 & 0.399 & 0.545 & 280M \\ DeBERTa-large & 0.339 & 0.268 & 0.378 & 0.532 & 680M \\ DeBERTa-large & 0.345 & 0.272 & 0.389 & 0.540 & 800M \\   

Table 5. Performance comparison for different encoders.

of visits for triples in turn leads to more equally visiting entities, thus improving the performance.13

Further analysis reinforces this finding. In FB15k-237, the average degree of FP triples' tails is 75 for SATKGC and 63 for SimKGC. A smaller portion of low-degree tails for SATKGC than for SimKGC indicates that exposure to more low-degree entities \(}\) in training helps the model position their embeddings farther from the \((h,r)\) embeddings for negative triples \((h,r,})\), as SATKGC visits low-degree entities more often during training than RANDOM for SimKGC.14

We examine the structural characteristics on sets \(S_{m}\) and \(S_{l}\) of entities in \(1,000\) most and least frequent triples, respectively, visited by BRWR. The entities in \(S_{m}\) have an average degree of \(11.1\), compared to 297.3 for those in \(S_{l}\). The betweenness centrality15 averages around \(5.2 10^{-5}\) for \(S_{m}\) and \(8.2 10^{-4}\) for \(S_{l}\). These observations implies that SaaM prioritizes visiting unpopular and peripheral triples in KG over focusing on information-rich triples.

## 6. Analysis

### Analysis on Negative Triples

Figure 7 shows how the cosine similarity distribution of in-batch negative triples varies depending on the epoch of SimKGC and SATKGC for FB15k-237. SATKGC encounters consistently more hard negatives with scores from 0.2 to 1.0 than SimKGC, though the majority of the scores range from -0.2 to 0.2 by the end of training for both methods.16 We speculate that SATKGC ends up with distinguishing positives from the hard negatives sampled from the subgraphs of the KG, as opposed to SimKGC which employs randomly-sampled easy negatives.

Based on our analysis, only 2.83% and 4.42% of the true triples ranked within the top 10 by SimKGC drop out of the top 10 for SATKGC on WN18RR and FB15k-237, respectively. In contrast, 34.87% and 13.03% of the true triples dropping out of the top 10 for SimKGC are ranked within the top 10 by SATKGC. This indicates that SATKGC effectively samples hard negatives while reducing false negatives.

An additional experiment in Appendix I shows how proximity-aware contrastive learning and frequency-aware mini-batch training selectively penalize hard negative triples. We also compare the effectiveness of using negatives fully sampled from SaaM and that partially sampled from SaaM in Appendix H.

### Runtime Analysis

Running SATKGC incurs a marginal computational overhead, because (i) sampling subgraphs and (ii) computing distances and degrees are performed in advance before training. As shown in Table 7, the computational cost for (i) and (ii) is acceptable, and depends on the mini-batch size, which can be adjusted. Moreover, the time complexity for (ii) is acceptable. For each mini-batch \(B\) of triples, we run Dijkstra's single source shortest path algorithm, and thus the runtime to compute the distance is \((|V|log|V|+|B|log|V|)\), where \(V\) is a set of entities in \(B\).17 Table 7 shows the training time per epoch for SATKGC, SimKGC (Liu et al., 2019), and STAR (Yang et al., 2020). SATKGC remains competitive, though it takes slightly more time than SimKGC due to the computational overhead for (a) computing its loss using the shortest path length and the degree, (b) counting the occurrences of visited triples in SaaM, and (c) fetching subgraphs in SaaM.18

## 7. Conclusion

We propose a generic training scheme and new contrastive learning (CL) for KGC. Harmonizing SaaM using a subgraph of KG as a mini-batch, and both CL and mini-batch training incorporating the structural inductive bias of KG in fine-tuning PLMs helps learning the contextual text embeddings aware of the difficulty in the structural context of KG. Our findings imply that unequally feeding triples in training and leveraging the unique characteristics of KG lead to the effective text-based KGC method, achieving state-of-the-art performance on the three KG benchmarks.