# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

## 2. Related Work

### Knowledge Tracing

Researchers have explored different modes of KT conduction. Existing KT methods can be divided into two types: probabilistic or logistic model-based traditional methods and deep learning-based methods. Probabilistic model-based methods generally define a student's knowledge states as a binary variable and use _Hidden Markov Model_ to estimate the student's conceptual mastery level, and the representatives include BKT (Brocker, 1977) and its variants (Kumar et al., 2017). Logistic model-based methods mainly estimate student performance by usually learning a logistic function, based on different factors in some students who solve the same set of problems, and the representatives include _Performance Factor Analysis_(Kumar et al., 2017) and _Learning Factor Analysis_(Brocker, 1977). Differently, deep learning-based KT methods leverage various neural network techniques to solve the sequential prediction task of the student answering exercises for tracing the student's knowledge states, which are commonly implicit in the hidden states of models. The representatives include the RNN-based method DKT (DKT, 1971), memory-augmented methods (e.g., DKVMN (Kumar et al., 2017)), attention mechanism-based methods (Kumar et al., 2017; Kumar et al., 2017), transformer-based methods (Kumar et al., 2017; Kumar et al., 2017) and graph neural network-based methods (Kumar et al., 2017; Kumar et al., 2017; Kumar et al., 2017).

Among them, some KT methods not only focus on designing novel network architectures but also try to solve some intrinsic difficulties in KT. For example, CL4KT (Kumar et al., 2017) and CMKT (Kumar et al., 2017) aim to address the student-exercise interaction sparseness problem; ATKT (Kumar et al., 2017) and DLKT (Kumar et al., 2017) pursue to improve model generalization performance; LPKT (Kumar et al., 2017), HawkesKT (Kumar et al., 2017) and CT-NCM (Kumar et al., 2017) attempt to model the forgetting behaviors of students during the learning process; DTransformer (Kumar et al., 2017) was proposed to obtain stable knowledge state estimation and tracing, instead of only improving the prediction performance, by inventing a new training paradigm. It can be observed that many intrinsic difficulties (including sparseness, forgetting, stable tracing, and so on) in KT have been well solved, but how to overcome the influence caused by the abnormal conditions that occur among students during the online learning process has been less explored. The abnormal conditions may arise from low data quality and abnormal student behaviors, which is ubiquitous in online learning system and will affect the accuracy and interpretability of KT tasks, and thus it is urgent for us to develop corresponding KT methods to solve this difficulty.

### Anomaly Detection

Anomaly detection is an important research topic with broad application prospects. For example, in the recommendation system, there are certain abnormal behaviors in the user's click sequence (such as clicking on a product that he does not like), which will affect the recommendation of the next item for the user (Kumar et al., 2017; Kumar et al., 2017). In industry, researchers detect whether abnormalities occur in the sensors to improve production efficiency (Kumar et al., 2017; Kumar et al., 2017; Kumar et al., 2017). Anomaly detection has been applied for various types of data. Here we focus on anomaly detection for time series data. These existing researches can be divided into prediction-based methods (Kumar et al., 2017; Kumar et al., 2017) and reconstruction-based methods (Kumar et al., 2017; Kumar et al., 2017). Prediction-based models utilize advanced

Figure 1. The illustrative examples of a sequence of interactions for a student learning online and the corresponding diagnosed knowledge states. The record comprises 9 learning interactions, spanning 6 exercises and encompassing 5 knowledge concepts.

Figure 2. The impact of different proportions of noise data on the DKT method (DKT, 1971) in the ASSISTment12 dataset. The range of change in knowledge proficiency denotes the maximum rate of change in each studentâ€™s mastery level for different knowledge concepts within a learning process.

machine learning components to predict the future variable performance based on the historical time series through modeling the spatiotemporal correlation between variables in time series data.
* The abnormality is detected through prediction probabilities. In order to improve the accuracy of abnormality detection, a variety of discriminant models attempt to better learn the complex relationship between variables to enhance the prediction performance. For example, _Deng and Hooi_(Deng and Hooi, 2017) proposed a graph neural network based prediction model to capture complex inter-sensor relationships to detect and explain anomalies that deviate from these relationships. _Zhao et al._(Zhao et al., 2019) combined feature-oriented graph attention network (GAT) and time-oriented GAT to handle spatial dependence and temporal dependence in predicting. Reconstruction-based methods pursue precise representations of the entire time series data for data reconstruction, and detect anomalies according to the difficulty of reconstruction. To be specific, it is more difficult to reconstruct abnormal data and less difficult to reconstruct normal data. Therefore, this category pursues to learn robust and accurate representations of input data for reconstructing input data. For example, _Li et al._(Li et al., 2019) used the generative adversarial network (GAN) framework with long short-term memory (LSTM) as the basic unit to accurately reconstruct input data by considering the entire set of variables concurrently. In the literature (Zhu et al., 2019), the proposed OmniAnomaly uses stochastic recurrent neural networks (RNN) to find robust representations for multivariate time series. _Aubieter et al._(Aubieter et al., 2019) proposed an AutoEncoder architecture with adversarial learning inspired by GANs. Recent work (Abadi et al., 2016) exploits spectral analysis of latent representations and produces simultaneous representations of multivariate data. However, to the best of our knowledge, no researchers have head-on addressed the anomaly issue in knowledge tracing tasks.

## 3. Problem Definition

In this section, we formally define the problem of knowledge tracing (KT). Suppose there is a set of \(N\) students, \(=\{s_{1},s_{2},,s_{N}\}\), a set of \(M\) exercises, \(=\{e_{1},e_{2},,e_{M}\}\), and a set of \(\) knowledge concepts, \(=\{k_{1},k_{2},,k_{C}\}\). Each exercise is associated with specific knowledge concepts and the \(Q\)-matrix \(Q=\{q_{ij}\{0,1\}\}^{M C}\) is utilized to indicate the relationship between exercises and knowledge concepts, where \(q_{ij}=1\) if exercise \(e_{i}\) involves concept \(k_{j}\) and \(q_{ij}=0\) otherwise. For the exercise-solving sequence for each student during the learning process, we denote it with \(=\{(e_{k_{1}},k_{1}),(e_{2},k_{2},r_{2}),,(e_{T},k_{T},r_{T})\}\), where the triplet \((e_{t},k_{T})\) is the \(t\)-th learning iteration behavior, and \(e_{t}\), \(k_{t},r_{t}\{0,1\}\) represent the answered question, the related knowledge concept and the response result, respectively.

**Problem Definition.**_Given students' learning sequence \(=\{(e_{1},k_{1},r_{1}),(e_{2},k_{2},r_{2}),,(e_{T},k_{T},r_{T})\}\), the KT task aims to monitor students' evolving knowledge state during the learning process and predict their future performance at the next time step \(T+1\), which can be further applied to individualized students' learning scheme and maximize their learning efficiency._

## 4. Methodology

In this section, we initially provide an overall overview of our proposed framework **HD-KT** (short for **Hy**brid learning interactions **D**enoising **K**nowledge **T**racing). Subsequently, we explore each component of the model with a detailed explanation.

**Overview.** Our HD-KT model innovatively introduces the measurement of anomalous factors during the students' learning processes, effectively achieving robust knowledge tracing through the implementation of the hybrid learning interaction denoising strategy. As shown in Figure 3, the overall architecture of HD-KT consists of four main components, including the embedding layer, the knowledge state-guided anomaly detector, the student profile-guided anomaly detector, and the KTM adaptor. Specifically, by taking learning sequence, the embedding layer first outputs the vectorized representation of students, exercises and concepts. In the first detector, a knowledge concept-aware sequential variational autoencoder is designed to reconstruct the proficiency distribution of students with the dimension of knowledge concepts. Meanwhile, we leverage an effective attention mechanism with modeling students' long-term characteristics to explore anomalous interactions in the student profile-guided anomaly detector. In particular, both of these signals modeling different aspects of anomaly perception are jointly exploited to denoise and refine the sequential exercising behaviors of learners. Finally, the KTM adaptor that allows the integration of different KT models is conducted to realize the prediction of the future response performance of students.

### Embedding Layer

As is well known, the learning process of students is inherently intricate, characterized by students progressively engaging with exercises and continually enhancing their cognitive abilities (Dong et al., 2019; Zhai et al., 2019). In HD-KT, to effectively model the response interaction behaviors during the learning process of students, we consider the following elements: students, exercises, concepts, answers, and knowledge status. We define the basic unit of the learning process as the triplet _exercise-concept-response_ and construct an embedding layer to encode them with trainable parameter matrices. Specifically, for the \(t\)-th exercising behavior \((e_{t},k_{t},r_{t})\) of student \(s\), we transform them into the corresponding embedded representations by multiplying their one-hot vectors with the parameter matrices:

\[_{e_{t}}=_{i}^{E},\ _{a_{t}}=_{t} ^{A}, \]

where \(_{t}^{M}\) and \(_{t}^{2C}\) denote the one-hot vector of the exercise and the response interaction, respectively; \(_{e_{t}}^{d_{e}}\) and \(_{a_{t}}^{d_{a}}\) stand for their embeddings representations; \(^{E}^{M d_{e}}\) and \(^{A}^{2C d_{a}}\) denote the trainable weight matrices; \(d_{e}\) and \(d_{a}\) are corresponding dimensions. In particular, \(_{t}\) here is the response interaction vector representing the knowledge performance, which is obtained by combining the knowledge concept \(c_{t}\) and the answer \(r_{t}\):

\[_{t,i}=1,i=k_{t}+C r_{t}\\ 0,otherwise \]

Furthermore, we introduce an adaptable embedding representation \(_{a}=^{S}\) for student \(s\) to delineate its profile, which supports the consistency of knowledge evolution, thus facilitating the exploration of the learning trajectory, where \(^{N}\) denotes the one-hot vector of student \(s\), \(_{s}^{d_{s}}\) is the global student profile, \(W^{S}^{N d_{a}}\) denotes the trainable weight matrix, and \(d_{s}\) is the corresponding embedding size. Finally, to effectively model each of the student's learning behaviors, with reference to (Zhu et al., 2019), we acquire the learning embedding by fusing the exercise representation and the knowledge performance representation together and employinga multi-layer perceptron (MLP) as follows:

\[_{t}=[_{_{t}}|_{a_{t}}|_{1}+ _{1}, \]

where \(\|\) denotes the operation of concatenating, \(_{1}^{(d_{e}+d_{a}) d}\) is the weight matrix, \(_{1}^{d}\) is the bias term, \(d\) is the dimension. As a result, we get the representation of the learning sequence of student \(s\): \(_{s}=[_{1},_{2},,_{T}]^{T d}\).

### Knowledge State-Guided Anomaly Detector

The consistency and gradual progression of competence growth are recognized as inherent characteristics of the student's learning process . Nonetheless, in real and intricate learning environments, anomalous signals can manifest due to external influences, e.g., a student correctly answers multiple questions that he has not genuinely mastered, potentially due to cheating, or a highly proficient student may inaccurately respond to straightforward exercises due to carelessness, among other possibilities. Therefore, in this part, we develop a knowledge state-guided anomaly detector to explore the anomalous signals thus enabling more effective modeling and diagnosing of student learning behaviors.

Firstly, to proficiently exploit the sequential learning behaviors of students and capture dependencies in the contextual knowledge states, the encoded bidirectional long short-term memory network (Bi-LSTM)  is utilized to model and process the embedded learning sequence representation \(^{s}\) as follows:

\[}_{s}^{L},}_{s}^{R}=Bi\_LSTM(_{s}, _{1}), \]

where \(}_{s}^{L},}_{s}^{R}^{T d_{e}}\) represent the bidirectional intermediate hidden states, respectively, \(_{s}=[_{1},_{2},,_{T}]^{T d_{e}}\) denotes the knowledge state matrix, \(Bi\_LSTM()\) refers to the Bi-LSTM network architecture, \(_{1}\) is the corresponding trainable parameterset, and \(\) stands for the element-wise addition operator. After obtaining the student knowledge states, inspired by , we contemplate utilizing a _Variational Autoencoder_ (VAE) to reconstruct the temporal evolving competencies for capturing anomalous signals during the learning process. Specifically, we model the latent variable \(}_{s}\) to adhere to a Gaussian distribution for deriving more robust embedding as follows:

\[}_{s}(,^{2}), \ =MLP_{}(_{s}),=MLP_{}( _{s}), \]

where \(}_{s}=[}_{1},}_{2},,}_{T}]^{T d_{e}}\) represents the reconstructed sequential competency level consisting of the knowledge state at each time step, and both \(MLP_{}()\) and \(MLP_{}()\) are two trainable MLP networks for learning the distribution parameters. After getting the reconstructed knowledge state sequence, we can calculate the completed reconstruction loss as follows:

\[^{Rec}=_{t=1}^{T}(}_{t}- _{t})^{2}+^{kl},\] \[^{kl}=_{1 t T}_{t}^{2}+ _{t}^{2}-log(_{t}). \]

After acquiring the reconstructed student knowledge state, intuitively, we can capture the inconsistency by integrating it with the student's initial knowledge level and inputting this combined information into the fully connected layers. Nevertheless, the minimization of the reconstruction loss can make it challenging to discern the distinctions between the aforementioned representations. Inspired by previous works , we endeavor to leverage a convolutional neural network (CNN) to enhance the detection capacity for capturing disparities among distinct representations of the same dimension. Specifically, we concatenate the original embedding of the knowledge state with the decoded representation at each moment and utilize a convolution operator to preserve dimensional information by:

\[_{t}=(_{t}_{2}),\] \[_{t}=Conv([}_{t}||_{t}],_{2}), \]

Figure 3. The overall framework of the proposed HD-KT.

where \(_{t}^{d_{s}}\) is the output of the convolution layer, \(()\) is a two-dimensional convolution operation with a filter size of 2\(\)1 and a stride of 1, \(_{2}\) is the trainable parameter of each channel, \(_{2}^{d_{s} 2}\) is the trainable parameter matrix. Notably, \(_{t}^{2}\) denotes the relation vector, where the first dimension represents the consistency between \(}_{t}\) and \(_{t}\), while the second dimension refers to the inconsistency. Therefore, the scores can be treated as a binary distribution (i.e., consistency vs. inconsistency). To generate binary values (i.e., 0 vs. 1) and facilitate gradient back-propagation, we utilize a Gumbel-Softmax function (Gumbel and Softmax, 2017; Gumbel and Softmax, 2017; Gumbel and Softmax, 2017) to support the learning of model via:

\[}_{t}=(_{t},), \]

where \(}_{t}^{2}\) denotes whether the changes in student's knowledge status is stable, \(g_{t}\) is i.i.d sampled from the Gumbel distribution as noise disturber, _Gumbel-Softmax_(\(\)) denotes the Gumbel-Softmax function and \(>0\) is the temperature parameter that controls the selection distribution. When \( 0\), \(}_{t}\) approximates a one-hot vector (i.e. hard selection). When \(\), \(}_{t}\) approximates a uniform distribution. When \( 1\), the Gumbel-Softmax function is the same as the general Softmax function.

### Student Profile-Guided Anomaly Detector

Due to the unique attributes of each student within the learning process, even when subjected to the same learning experience, differential learning outcomes and memory retention may be observed. We contend that this phenomenon imparts crucial insights into sequence denoising, specifically, the fact that abnormal learning behaviors frequently exhibit substantial deviations from the individual characteristics of students throughout the learning process. Therefore, we design a student profile-guided anomaly detection module to explore the asymptotic smoothness of the student's evolving competency. Specifically, we develop an attention module as the discriminator to detect inconsistency between the learning status and the student profile, which utilizes the student representation as a query vector and assign different attention weight to each learning encoding within the learning sequence:

\[_{t}=(([_{t}|}_{t}]_{3}+_{s}_{4})_{5}), \]

where \(_{t}^{2}\) is the \(t\)-th attention vector, \(_{3}^{2d_{s} d}\), \(_{4}^{d_{s} d}\) and \(_{5}^{d_{s} 2}\) are the trainable parameter matrix, and \(()\) and \(()\) denote the sigmoid and tanh activation function, respectively. Notably, the first dimension of \(_{t}\) represents the consistency between student response performance and student learning profile, as well as the second dimension denotes the inconsistency. Therefore, the scores can be viewed as binary distributions (i.e., consistency vs. inconsistency), and then we leverage a similar process to generate binary value for \(_{t}\) via:

\[}_{t}=(_{t},),\] \[=((_{t,i})+g_{i})/}{_{j=0 }^{1}((_{t,i})+g_{j})/}, \]

where \(}_{t}^{2}\) denotes the predicted anomalous vector about the knowledge state of student \(s\), and \(\) is the same temperature parameter used in formula Eq. (8) to tune the learned distribution from the Gumbel-Softmax function.

### KTM Adaptor

With the previously mentioned anomaly detectors, the proposed HD-KT model enables to detect noise components within the sequence based on signals derived from the knowledge state and student profile levels, which involves labeling a response as noise when it exhibits inconsistency with the respective student attributes or the amalgamated knowledge state. Nevertheless, in practical applications, the false positives may be introduced, leading to the inadvertent exclusion of valuable information essential for predicting student performance. Hence, we advocate the development of a more stringent criterion for the elimination of anomalous learning interaction, aimed at retaining solely dependable noise-free data while preserving valuable information. An instance is categorized as noise only when incongruities are concurrently identified in both signals, typically adhering to the principle of consensus. Formally, we generate noise-free sequences from the input sequential learning behaviors of individual KTM via the following steps:

\[_{s}^{+}=[p_{1}_{1},p_{2}_{2},,p_{T} _{T}], \]

where \(p_{t}\{0,1\}\) indicates whether an learning interaction is noisy (i.e., \(p_{t}=0\)) or not, \(a_{t}\) and \(b_{t}\) denote the second dimension scalar of above mentioned \(_{t}\) and \(_{t}\), respectively. Note that we apply the denoised signal to the embedding representation of learning sequence \(_{s}\) to support the gradient backpropagation. Particularly, we design a KTM adaptor to adapt our proposed HD-KT framework for the integration into various mainstream knowledge tracing model for predicting the feature response performance of students, and we formalize as follows:

\[=(_{s}^{+}), \]

where KTM is a basic knowledge tracing model (e.g., DKT, LPKT, etc.), which takes the denoised learning sequence representation \(_{s}^{+}\) as input, and outputs the predicted future performance \(\).

### Model Optimization

In the training phase, we mainly evaluate the performance of the predicted student's responses in the interaction sequences. Similar to (Zhu et al., 2017; Zhu et al., 2017), the binary cross entropy loss function between the predicted value \(_{t}\) of student \(s\) at time step \(t\) and the ground truth \(r_{t}\) is utilized, as follows:

\[^{Pre}=-_{t=1}^{T}(r_{t}(_{t})+(1-r_{t})(1- _{t})). \]

where \(^{Pre}\) represents the prediction loss. Meanwhile, we also introduce the reconstruct loss to enhance the stability of parameter training of the anomaly detector according to Eq. (6), and build the final training loss as follows:

\[=|}_{s}(_{s}^{Pre} +_{s}^{Rec})+\|\|_{2}^{2}, \]

where \(\) represents the hyperparameter of \(L_{2}\) regularization strength, and \(\) is the set of all model parameters. The objective function was minimized using Adam optimizer (Kingma and Ba, 2015) on mini-batches. More details of settings are specified in the part of experiments.

## 5. Experiment

In this section, we conduct a series of experiments using four real-world benchmark datasets to validate the efficacy of our proposed model. We aim to address the subsequent research questions:

* **RQ1**: Can our proposed HD-KT framework effectively enhance the performance and robustness of the existing KT models?
* **RQ2**: What benefit does each component of the proposed HD-KT model offer?
* **RQ3**: Does our approach facilitate the analysis of question quality and enable student clustering based on learning behaviors?

### Experimental Setting

#### 5.1.1. Datasets

In this paper, we conducted our experiments on four public benchmark datasets, i.e., ASSISTment12, ASSISTment17, Slepemapy.cz, and Junyi. The ASSISTment12 dataset, referenced in (Brockman et al., 2016), was collected from the ASSISTments online tutoring system and encompasses student activity data for the academic year 2012-2013. ASSISTment17 (Shen et al., 2016) was released during the ASSISTments Longitudinal Data Mining Competition in 2017. The dataset Slepemapy.cz (Shen et al., 2016) originates from an online adaptive system, i.e., slepemapy.cz, for practicing geography. The Junyi dataset (Brockman et al., 2016) was collected from the Junyi Academy, an E-learning platform established in 2012. To optimize calculation efficiency, we followed (Shen et al., 2016) to set the maximum sequence length to 50 and truncate the learning sequences exceeding this length into multiple sub-sequences. To ensure reasonableness, we screened out the sequences with lengths less than 5. The statistics of four datasets are shown in Table 1.

#### 5.1.2. Evaluation Metrics

We employed both accuracy (ACC) and the area under the receiver operating characteristics curve (AUC) as metrics to assess the efficacy of various methods in predicting the binary outcomes of future student responses to exercises.

#### 5.1.3. Baseline Methods

To validate that our proposed HD-KT framework can significantly enhance the performance of different KT models, we selected three representative KT models as the backbone, including DKT, HawkesKT, and LPKT. The details are displayed as follows:

* **DKT**(Shen et al., 2016) pioneered the use of Recurrent Neural Networks (RNNs) to model students' knowledge states, inferring current exercise performance from past learning records. In our implementation, we employed the LSTM architecture.
* **HawkesKT**(Shen et al., 2016) posits that students' proficiency in each knowledge concept is influenced not only by prior interactions with that concept but also by other relevant concepts, termed as cross-effects among knowledge concepts. HawkesKT employs collaborative filtering and matrix factorization to discover the temporal cross-effects between different concepts.
* **LPKT**(Shen et al., 2016) distinguishes students' absorption of knowledge and forgetting of knowledge during the learning process through specially designed learning gates and forgetting gates respectively. The state undergoes intermittent updates via a straightforward weighted blend of both learning and forgetting factors.

We applied our framework to these models, resulting in three variants named HD-DKT, HD-HawkesKT, and HD-LPKT. Additionally, we selected two representative anomaly information section methods in the field of time series and sequential recommendation to serve as the baselines, including:

* **DSAN**(Shen et al., 2016), known as the dual sparse attention network, is designed to pinpoint items in a recommendation system that diverge from the user's anticipated preferences by assigning unique weights to each item in the sequence. In our experiment, we treated students and exercises as target item and interactive items, respectively. By integrating DSAN with various KT models, we leveraged the unique weights within DSAN to detect anomalous data.
* **GDN**(Shen et al., 2016), known as the graph deviation network, is a prediction-based multivariate temporal anomaly detection method leveraging graph attention (GAT) to capture the relationships within each feature of the time series data. In our experiments, we treat the sequential knowledge states on different concepts as the multivariate time series.

Moreover, we compared our HD-KT with the state-of-the-art robust KT model, that is,

* **DTransformer**(Shen et al., 2016), which introduces a unique transformer-based architecture combined with a novel training paradigm to achieve consistent and reliable knowledge state tracing.

#### 5.1.4. Implementation Details

In our experiment, we performed 5-fold cross-validation. Specifically, the 80% of the learning sequences are split as the training set (70%) and the validation set (10%), while the rest 20% are used as the test set. We faithfully implemented DKT, HawkesKT and LPKT based on their original papers. To be specific, if parameters were consistent across various datasets in the original paper, we retained them as described (e.g., all parameters for HawkesKT). However, if the sensitivity to datasets was indicated, we performed parameter tuning on the validation set, adhering to the value ranges specified in the original works (e.g., parameters for DKT). We performed hyperparameter tuning for each KTM combined with HD based on the validation set. We searched the embedding size in (Krizhevsky et al., 2012; He et al., 2012; He et al., 2012; Zhang et al., 2013), hidden size in (Krizhevsky et al., 2012; He et al., 2012; He et al., 2012; He et al., 2012), and dropout rate in \([0,0.1,0.2,0.25]\). We used the Adam algorithm (Kingma and Ba, 2014) as the optimizer. All experiments were implemented with PyTorch by Python and conducted with GeForce RTX4090 GPU.

### Overall Performance (RQ1)

To verify the effectiveness of our HD-KT framework, we conducted future students' performance prediction experiments in the above four datasets. Table 2 shows the experimental results of the proposed HD-KT implemented in three KTMs compared with the baselines. First, it is clear that integrating our HD-KT framework to filter out the anomalous learning interaction has resulted in marked improvements in the performance of various KT models on all the

   Dataset & \#Students & \#Concepts & \#Exercises & \#Interactions \\  ASSISTment17 & 25.3k & 245 & 50.9k & 2,621.3k \\ ASSISTment17 & 1.7k & 102 & 3.2k & 942.8k \\ Slepemapy.cz & 81.7k & 1,458 & 2.9k & 9,786.5k \\ Junyi & 175.4k & 40 & 0.7k & 25,670.2k \\   

Table 1. Statistics of all datasets.

[MISSING_PAGE_FAIL:7]

original dataset. Additionally, the right side of Figure 5 presents the proportion of noise data correctly identified by our HD-LPKT after introducing varying percentages of noise data into each student's learning sequence. It can be observed that even added 2% noise data, our framework can still accurately capture approximately 70% of them. Moreover, as the amount of noise data increases, the effective detection rate of our model gradually rises. This is because the more noise introduced, the more volatile the student's knowledge state becomes, making it easier for our model to detect.

#### 5.4.2. One Case Study of KT on Junyi Dataset

In this case study, we showcased the results of knowledge tracing for student #14's learning sequence in the Junyi dataset using both HD-LPKT and LPKT. The results are presented in Figure 6. We can observe that our HD-LPKT model identified the second interaction with exercise #26 as anomalous, leading to the HD-LPKT and LPKT models diagnosing the student's mastery level of the knowledge concept "\(_{1}\): Isoseles triangle" as 0.86 and 0.63, respectively. Subsequently, we found that for future answer predictions related to exercise #18, which is associated with the knowledge concept \(c_{1}\), our HD-LPKT model could predict accurately, while LPKT could not. Moreover, for the knowledge concept "\(_{6}\): Square", which potentially relates to \(c_{1}\), our model also predicts the student's future performance more effectively. This validates that our HD-KT framework can robustly diagnose students' knowledge states by removing anomalous data from learning interactions.

#### 5.4.3. Exercise Quality Analysis

In online learning systems, an important task is to evaluate the quality of exercises, since high-quality exercises can more precisely track the students' knowledge states. Our method enables the detection of anomalous learning interactions within the data, facilitating an analysis of the proportion of anomalies across various exercises during data interaction. Figure 7 illustrates the distribution of exercise across different anomaly proportions in ASSISTment17. This result can serve as a basis for exercise quality analysis, whereby exercises detected with a higher anomaly rate can be revisited and reviewed by domain experts.

#### 5.4.4. Learning Behavior-based Student Clustering

As previously mentioned, some anomalous interactions in a student's learning sequence result from their learning behavior, such as carelessness. Here, we identify student groups with similar learning behaviors by analyzing the detected anomalous interactions from our HD-KT. Specifically, we first computed the proportion of detected anomalous interactions per student, for each knowledge concept, relative to all interactions associated with that concept. Subsequently, we utilized these proportions as feature vectors, representing potential anomalous behaviors of students across various knowledge concepts. These vectors were visualized after dimensionality reduction using t-distributed stochastic neighbor embedding (t-SNE). As shown in Figure 8, students with similar anomalous behavior are grouped into distinct clusters. These separated student groups assist educators in identifying representative student behavior patterns, enabling the creation of more tailored online learning experiences.

## 6. Conclusion

In this paper, we proposed a novel framework, termed **HD-KT**, to enhance the robustness of existing knowledge tracing (**KT**) methodologies with Hybrid learning interactions **D**enoising approach. In HD-KT, two detectors for anomalous learning interactions (namely knowledge state-guided anomaly detector and student profile-guided anomaly detector) were specially designed. More specifically, in the first detection module, a sequential autoencoder was designed to identify anomalous learning interactions by detecting atypical student knowledge states. In the second module, an attention mechanism was incorporated by modeling a student's long-term profile to capture irregular interactions. Extensive experiments validate the significant advantages of our HD-KT from multiple aspects. On the one hand, HD-KT markedly boosts both the robustness and accuracy of prevailing KT models. On the other hand, the HD-KT can facilitate exercise quality analysis and learning behavior-based student clustering.

Figure 8. Student clustering based on the proportion of detected anomalous interactions, wherein we sampled 1000 students in ASSISTment12. We used K-means to cluster the students and marked them with different colors accordingly.

Figure 6. A case study of HD-LPKT. This experiment shows the knowledge proficiency radar chart of the student with ID #14 in the Junyi data set using LPKT or HD-LPKT.

Figure 7. Distribution of anomalous proportions for exercises in ASSISTment17â€™s learning interactions.