# Towards Expansive and Adaptive Hard Negative Mining:

Graph Contrastive Learning via Subspace Preserving

Anonymous Author(s)

###### Abstract.

Graph Neural Networks (GNNs) have emerged as the predominant tool for analyzing graph data on the web and beyond. Contrastive learning (CL), a self-supervised paradigm, not only mitigates the reliance on annotations but also leads to breakthroughs in performance. The hard negative sampling strategy that benefits CL in other domains proves ineffective in the context of Graph Contrastive Learning (GCL) due to the message passing mechanism. Embracing the subspace hypothesis in clustering, we propose a method towards expansive and adaptive hard negative mining, referred to as Graph ContRastive LeArning via subsPace prEServing (GRAPE). Beyond homophily, we argue that false negatives are prevalent over an expansive range and exploring them confers benefits upon GCL. Diverging from existing neighbor-based methods, our method seeks to mine long-range hard negatives throughout subspace, where message passing is conceived as interactions between subspaces. Additionally, our method adaptively scales the hard negatives set through subspace preservation during training. In practice, we develop two schemes to enhance GCL that are pluggable into existing GCL frameworks. The underlying mechanisms are analyzed and the connections to related methods are investigated. Comprehensive experiments demonstrate that our method achieves state-of-the-art performance on multiple graph datasets and maintains competitiveness in various application settings. Our work contributes to the improvement of representation learning on web graphs, aligning with the scope of The Web Conference. Our code is available at [https://anonymous.4open.science/r/Grape-code](https://anonymous.4open.science/r/Grape-code).

Graph neural networks, Graph contrastive learning, Hard negative mining, Subspace preserving, Web data mining 2020 acmcopyright

## 1. Introduction

Graph data is ubiquitous in both real-world and virtual realms, encompassing a broad spectrum of areas such as social networks, molecular structures, trade circulation. Recently, GNNs have witnessed significant strides in the domain of analyzing graph data, exhibiting exceptional performance in tasks such as graph classification , node clustering , link prediction  and graph generation . Following the pioneering contributions of GCN , GraphSAGE , GAT , etc., numerous GNN architectures have been developed and enhanced. Almost all GNNs are built upon the message passing mechanism between neighbors, where each node acquires feature information from its neighbors and contributes its own feature information. Analogous to most neural networks, GNNs are typically trained in a supervised manner and require an abundance of annotations.

Contrastive Learning (CL), as a category of self-supervised methods, has recently demonstrated a series of state-of-the-art performances in various domains . These studies emphasize that the representations learned by CL perform comparably to supervised learning in downstream tasks. The essence of CL lies in learning representations that retain invariance under a variety of distortions, referred to as "data augmentations" . To achieve this, researchers develop InfoNCE objective , which maximizes a lower bound of mutual information between augmented views . The core conception is to draw positive pairs closer while repelling negative pairs apart .

The breakthroughs of CL in computer vision have motivated studies to extend the analogous concepts from visual representation learning to graph data, referred to as Graph Contrastive Learning (GCL). These GCL methods achieve sofa in both graph-level and node-level tasks . GCL adheres to the typical CL paradigm, albeit with specific variations . In general, the application paradigms of CL in visual, textual, and graph data domains can be illustrated as Figure 1. As demonstrated, existing research in GCL can be summarized in the following two main threads: (1) augmentation for graph , which aims to adapt semantic-preserving augmentation techniques from visual data to irregular graph data. (2) contrastive loss for graph , which explores the loss functions suitable for GNN training within CL framework. Our work falls into the latter category. Unlike other mainstream instance-discriminating backbones  where instances do not exhibit explicit interactions, GNNs rely on message passing among neighbors. A notable issue arises where hard negative sampling techniques, proven to contribute in CL , does not confer benefits in GCL and may even impair performance, which has been discussed in . The main concept behind is that hard negatives in GCL are prone to being false negatives, consequently, pushing away the semantically similar representations leads to a degradation in performance.

In this paper, we report that mining **expansive** and **adaptive** hard negatives enhances node-level tasks. To achieve both objectives, we introduce a negative hardness estimation scheme for GCL, aligning with the subspace preservation hypothesis in clustering. The core strength of our method lies in its ability to capture hard negatives beyond the scope of message passing and adjust the hard negatives set in a self-scaled manner. In node-level tasks, the concept of subspace preservation is intuitive. For instance, in a citation network, it can be elucidated as follows: from the semantic perspective, articles with the similar theme tends to share keywords (features); from the structural perspective, mutual citations within the same subfield are frequent whereas cross-domain article citations are limited. Prominent recommendation mechanisms within social or e-commerce networks, which curate personalized content for individual entities, have catalyzed the emergence of subspaces (Song et al., 2016; Wang et al., 2017; Wang et al., 2018). We provide theoretical and experimental analyses to illuminate why and how our method works. To the best of our knowledge, our work is the first to address the GCL through subspace techniques.

In summary, the main contributions of this paper can be encapsulate in threefold:

* We show that more expansive and adaptive hard negative mining is promising for enhancing node-level GCL. In line with this idea, we propose GRAPE, a novel negative hardness estimation method for GCL based on subspace theory.
* In GRAPE, the hard negatives beyond the scope of message passing can be captured and the hard negatives set can be adaptively scaled. Two schemes are devised to alleviate the influence of false negative samples on GCL. Besides, we provide a theoretical exposition of GRAPE's properties and its connection with related methods.
* In comparison to several advanced GCL methods, GRAPE exhibits superior performance on eight widely-used public graph datasets. We conduct comprehensive experiments under various settings to thoroughly analyze the results and behaviors of GRAPE.

The proofs of involved theorems, experimental settings and supplementary experiments are relegated to the appendix.

## 2. Related Work

In line with the focus of our work, we provide an overview of related works on graph contrastive learning and subspace preserving.

### Graph Contrastive Learning

Amidst the increasing recognition of contrastive learning's expressive capability, DGI (Zhu et al., 2017) and InfoGraph (Zhu et al., 2018) first leverage the maximization of mutual information (Zhu et al., 2019) at the node- and graph-level, respectively, to attain effective representations. In subsequent works, MVGRL (Zhu et al., 2019) utilizes graph diffusion (Zhu et al., 2019) to obtain augmented views and applies contrastive learning at both the node and graph levels. GMI (Zhu et al., 2019) extends mutual information computations from vector spaces to the graph domain and assesses the correlation between input graphs and high-level hidden representations. GRACE (Zhu et al., 2019), GCA (Zhu et al., 2019) employ the InfoNCF-style objective and obtain node representations by treating others as negative samples, which serves as a baseline in follow-up research. To mitigate the sampling bias issue, BGRL (Zhu et al., 2019) extends the BYOL (Zhu et al., 2019) framework to graph. In this strand, CCA-SSG (Zhu et al., 2019) optimizes a feature-level objective inspired by classical canonical correlation analysis. SpCo (Zhu et al., 2019) is introduced as a spectral GCL module based on the general graph augmentation rule to enhance existing GCL methods. In another thread, ProGL (Zhu et al., 2019) estimates the probability of a true negative using a two-component beta mixture model. Empirical studies (Zhu et al., 2019) verify that assigning higher weights to hard negatives or generating hard negatives fails to improve GCL. GDCL (Zhu et al., 2019) jointly performs GCL and DEC (Zhu et al., 2019); nevertheless, this unsupervised process may lead to training collapse. COSTA (Zhu et al., 2019) advocates generating covariance-preserving augmented features inspired by matrix sketching. HomoGCL (Zhu et al., 2019) proposes utilizing the homophily in graph to filter positive pairs. PHASES (Zhu et al., 2019) employs a progressive negative masking strategy to enhance tolerance between sample pairs. We recommend readers to refer to (Zhu et al., 2019; Wang et al., 2018; Wang et al., 2018) for a comprehensive overview.

### Subspace Preserving

One underlying tenet in machine learning is that the data contains certain type of structure for intelligent representation. From this, the subspace assumption, which runs through the research journey of machine learning, can be described as follows (Zhu et al., 2019): high-dimensional data is drawn from a union of multiple affine or linear subspaces. In a simplified perspective, affine subspace is more closely related to manifold learning (Bishop, 1996; Bishop, 1996; Bishop, 1996; Bishop, 1996; Bishop, 1997), whereas linear subspace aligns more closely with dictionary learning (Song et al., 2016; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). Over the past decade, subspace learning based on the self-expression model, which enjoys the benefits of the both, has made significant strides (Song et al., 2016; Wang et al., 2018; Wang et al., 2018). The main divergence among these methods is the constraints imposed on the self-expression coefficients, such as sparse constraint (Song et al., 2016; Wang et al., 2018; Wang et al., 2018), low-rank constraint (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018), connectivity constraint (Wang et al., 2018; Wang et al., 2018).

Figure 1. A comparison of CL for visual, textual, and graph data. The irregularity of graph data and the message passing mechanism of GNNs distinguish GCL from CL in other domains. Graph convolutional operator introduces smoothing property among neighbors, while necessitating some technical changes to GCL.

and smooth constraint (Brockman, 2011; Chen et al., 2013; Chen et al., 2014). We adopt the fundamental principles of such methods to tackle hard negative mining in GCL. Both empirical investigations and theoretical analyses confirm the suitability in the context of GCL. Recent studies in graph dictionary learning (Brockman, 2011; Chen et al., 2014; Chen et al., 2014) focus on sparse encoding for molecules, which are not directly related to our work.

## 3. Methodology

### Notations and Preliminaries

Let \(G=(,)\) denotes a graph with \(n\) nodes, where \(\{0,1\}^{n n}\) denotes the adjacency matrix and \(^{n d}\) denotes the feature matrix. Let \(}=+_{n}\) be the adjacency matrix with self-loops. The normalized adjacency matrix is then given by \(}=D^{-}}D^{-}\), where \(\) is the degree matrix. Vectors and matrices in this paper are denoted by bold lowercase and bold uppercase letters, respectively. Set \(\{1,2,,n\}\) is abbreviated by \([n]\) for simplicity.

Our objective is to train a GNN encoder \(f_{}(,)\) in a label-scarce scenario, where \(\) represents the network parameters. The output node embeddings are supposed to be directly applicable for downstream tasks, such as node classification and node clustering in this paper. Take GCN for example, the layer-wise forward-propagation operation at the \(l\)-th layer is formulated as:

\[^{(l)}=(}^{(l-1)}^{(l)}), \]

where \(^{(l)}\) is the trainable weights for feature transformation and \(^{(l)}\) denotes the node embeddings at the \(l\)-th layer. Clearly, there is \(^{(l)}=\) at the initial layer. \(()\) denotes an activation function such as ReLU. In context of GCL, two views \(G_{1}=(_{1},_{1})\), \(G_{2}=(_{2},_{2})\) are generated by augmentation strategies (Glorot and Bengio, 2011) each epoch. \(G_{1}\) and \(G_{2}\) are fed into a Siamese GNN encoder (Chen et al., 2014) to produce node embeddings \(\{_{i}\}_{i=1}^{n}\) and \(\{_{i}\}_{i=1}^{n}\), respectively. The contrastive loss can then be computed, followed by backpropagation. The common baseline for graph contrastive loss is the InfoNCE-style loss in GRACE (Glorot and Bengio, 2011). Specifically, the contrastive loss for \(_{i}\) is defined as:

\[(_{i})=\] \[-_{i},_{i})/ }}{}+e^{(_{i },_{j})/}}_{}+e^{(_{i},_{j})/}}_{}, \]

where \((,)\) denotes cosine similarity and \(\) is the temperature parameter. The objective \((_{i})\) is defined symmetrically. Then the overall loss is given as:

\[=_{i=1}^{n}((_{i})+ (_{i})). \]

It can be observed in Eq. (2) that by minimizing loss \(\), embeddings of the same sample under two augmentations are pulled closer (**positives**), while embeddings of different samples are repelled away (**negatives**). For simplicity, in this paper, we represent \((_{i})\) in the following form

\[(_{i})=-(_{i})}{(_{i})+ (_{i})}. \]

### Graph Contrastive Learning via Subspace Preserving

Recent studies (Wang et al., 2017; Wang et al., 2017) report that considering all samples other than the anchor itself as negatives (Eq. (2)) unduly distances **false negatives** (i.e., samples of the same class as the anchor). This so-called "class collisions" phenomenon makes the marriage of CL and GNNs seem subtle and, as a result, leads to a performance decline. Hard negative mining provides a remedy to rectify this deficiency, where **hard negatives** refer to samples exhibiting a high degree of similarity to the anchor or having a greater likelihood of being false negatives. Let \(_{i}\) be the hard negatives set of the \(i\)-th sample. With \(\{_{i}\}_{i=1}^{n}\) identified, hard negative mining mainly employs two forms of loss: one explicitly treats \(_{i}\) as positives (Glorot and Bengio, 2011; Wang et al., 2017)(referred to as "Positive" strategy), i.e., modifying the "pos" term in Eq. (4) to:

\[(_{i})=e^{(_{i},_{j})/}+_{ j_{i}}e^{(_{i},_{j})/}+_{j_{i}}e^{(_{i},_{j})/}; \]

another strategy masks \(_{i}\) within the negatives (Wang et al., 2017; Wang et al., 2017) (referred to as "Mask" strategy), i.e., modifying the "neg" term in Eq. (4) to:

\[(_{i})=e^{(_{i},_{j})/}+_{ j_{i}}e^{(_{i},_{j})/}+_{j_{i}}e^{(_{i},_{j})/}. \]

While both strategies are intuitive, their effectiveness on graphs remains to be thoroughly explored. We employ GRACE with two-layer GCN as a baseline and probe the quality of hard negative mining on three popular datasets. The results are shown in Table 1, with each value representing the average of 10 repeated runs. The different settings are explained as follows: "w/o MP" denotes GRACE without message passing, "x-hop" denotes selecting neighbors within \(x\)-hop as hard negatives, "x-hop" denotes selecting false negatives within \(x\)-hop as hard negatives, and "all labels" denotes selecting all false negatives as hard negatives. Here, 1-hop includes own neighbors for each node, while 2-hop encompasses the neighbors of neighbors and so forth. The "all labels" setting is an extreme scenario with all labels available in which \(f(_{i})\) is akin to the tuplet loss in metric learning (Wang et al., 2017; Wang et al., 2017).

Through empirical study, we make the following observations: (1) the performance of GCL significantly deteriorates in the absence of message passing; (2) "x-hop" settings provide limited benefits and may even be detrimental to GCL; (3) training under "x-hop" setting improves GCL performance; (4) "x-hop" setting with larger \(x\) leads to more noticeable performance improvements. A follow-up query

   Datasets & Cora & CiteSeer & PubMed \\  Settings & Positive & Mask & Positive & Mask \\  GRACE & 81.05 & 71.27 & 79.57 & \\ w/o MP & 44.34 & 60.52 & 72.94 & \\
1-hop & 81.71 & 82.20 & 71.09 & 71.57 & 79.76 & 79.98 \\
2-hop & 80.68 & 81.95 & 69.43 & 71.52 & 77.41 & 78.80 \\
3-hop & 79.06 & 81.52 & 68.47 & 70.14 & 74.89 & 76.86 \\
1-hop & 82.49 & 81.83 & 71.42 & 71.06 & 80.54 & 80.31 \\
2-hop\({}^{*}\) & 84.12 & 83.49 & 72.11 & 72.19 & 81.42 & 80.96 \\
3-hop\({}^{*}\) & 86.61 & 86.15 & 74.86 & 73.52 & 83.26 & 83.20 \\ all labels & 97.57 & 94.63 & 95.93 & 93.44 & 97.01 & 95.52 \\   

Table 1. Empirical study (node classification accuracy in percentage) on hard negative mining in GCL.

arises as to whether it is feasible to narrow the gap between "x-hop" and "x-hop"? Drawing insights from above observations: (2) and (3) inspire us to capture more _"precise"_ hard negatives, while (4) encourages capturing "_expansive_" hard negatives. In self-supervised scenarios, the notion of "_precise_" may appear impractical, and thus, we pivot towards the pursuit of an _"adaptive"_ solution.

We plot the average distributions of false negatives on eight most commonly used network datasets in Figure 2. Cora and Citeseer correspond to the left coordinate axis, while the rest correspond to the right coordinate axis. It can be observed that false negatives are prevalent over an expansive range. This gives rise to the following concern: on the one hand, capturing more expansive false negatives approximates the performance under "all labels" setting: on the other hand, it is essential to prevent the capture of true negatives and thus avert the occurrence of "x-hop" scenario. In other words, this is promising intuitively and entails practical risks.

For a specific anchor, its neighbors in close proximity frequently engage in message passing with it. Hence, its close neighbors are inherently hard to distance, whereas points with no message exchange with the anchor are susceptible to being pushed farther away, as shown in Figure 2(a). This also explains why the "1-hop" in Table 1 provides limited boost to the baseline. Beyond well-known graph homophily (Sundar, 2016), we employ subspace preserving techniques to address this issue. The essence behind is to mine hard negatives across the entire subspace, rather than limiting it to graph-structured neighbors. Next, we provide the brief definition of subspace preserving.

**Definition 1**.: _(Subspace Preserving) The given data \(\{x_{i}\}_{i=1}^{n}\) is drawn from a union of an unknown number \(k\) of subspaces \(\{S_{j}\}_{j=1}^{k}\) with unknown dimensions \(\{d_{i}\}_{i=1}^{k}\). \(_{j}\) is subspace preserving if \(_{i}_{j}\) can be expressed as a linear combination of other points in \(_{j}\)._

Based on the so-called self-expressiveness property (Gardner, 2016), the coefficients representing the contribution to the anchor can be obtained by solving the optimization problem:

\[_{}\|z-\|_{2}^{2}+(), \]

where \(^{d}\) is the representation of the anchor, and matrix \(^{d m}\) is formed by concatenating the representations of \(m\) hard negatives of the anchor. \(\) corresponds to a specific constraint on \(\). Note that the anchor in problem (7) represents any sample and we omit subscript \(i\) for simplicity. Upon comparative analysis, we opt for elastic net (Gardner, 2016) as \(\) in this paper, which is a combination of the \(_{1}\) and \(_{2}\)-norms widely used in machine learning (Gardner, 2016; Gardner, 2016; Gardner, 2016). \(_{1}\)-penalty encourages sparsity, while \(_{2}\)-penalty promotes the connectivity. Furthermore, we expect to capture the consistent contribute from each hard negative throughout the entire process. The hard negatives selection for anchor \(z\) turns out to be:

\[_{}_{l=1}^{L}}\|^{(l)}- ^{(l)}\|_{2}^{2}+(\|\|_{1}+ \|\|_{2}^{2}) \]

where \(L\) is the number of network layers and \(d^{(l)}\) is the dimension of the \(l\)-th layer. There is \(^{(l)}^{d^{(l)}}\) and \(^{(l)}^{d^{(l)} m}\). \(>0\) is the regularization parameter and \(\) controls the trade-off between two terms in the elastic net regularizer. As GNN performs message passing between neighbors at each layer, the subspaces at each layer may shift. Therefore, each forward propagation can be regarded as interactions between subspaces: some nodes are drawn into certain subspaces, while some are pushed out of their original subspaces. Scalar \(}\) is for scale equilibrium. The interpretation of the first term in Eq. (8) is to seek consistent coefficients \(\) across training layers. In other words, if a node consistently resides in the same subspace as the anchor, it is highly likely to be a false negative of the anchor. The magnitude of this possibility depends on the magnitude of self-expression coefficients.

Due to the sparse constraints, problem (8) can not be computed in closed form by SVD. Multiple solutions are provided below.

**Full parameterization**: If each position of \(\) is considered as a parameter, then problem (8) can be solved in fully parametric way, such as Iterative Shrinkage Thresholding Algorithm (ISTA) (Bauer, 2016). Moreover, \(\) can also be solved by gradient-based training. Since each sample serves as an anchor, the number of parameters in this strategy is \(_{i=1}^{n}m_{i}\). Updating these parameters during training may bring computational burdens on large-scale data.

**MLP parameterization**: In this scheme, self-expression coefficients can be computed on the lower-dimensional representations output from MLP. For example, SENet proposed in (Sundar, 2016) employs a lightweight query and key network to parameterize the self-expression coefficients. Since MLP parameters does not depend on \(n\), such methods alleviate computational overhead.

**Atentive parameterization**: Attentive models, such as GAT (Vaswani et al., 2017), presuppose varying contributions of distinct features. These models also utilize dimension-related memory to parameterize \(\).

The number of parameters in the above three ways decreases in order. Correspondingly, the expressive power decreases and the efficiency increases. Details are given in Appendix B. Since problem (8) is strongly convex, such accelerated proximal gradient method or linearized alternating direction method can be applied for seeking unique solution. Selecting non-zero indices in solution

Figure 3. Qualitative schematic of our method.

Figure 2. The number of average false negatives at each hops on eight graph datasets.

\(\) and applying strategies like Eq. (5) and (6) may help attract false negatives within the subspace. Ideally, as shown in Figure 3b, true negatives are pushed farther away while false negatives are masked or explicitly drawn closer. It encourages the emergence of clear class boundaries, i.e. the golden hand is stretched in Figure 3b. While the solution process is straightforward, problem (8) itself may not be static; in other words, the pre-selected matrix \(H\) may not be optimal. This naturally prompts the question: how is the hard negatives set \(\) selected? Moreover, when dealing with large-scale data, it is extravagant to employ the self-representation of all samples on one single sample, we use a subset instead. Instead, we necessitate the adaptive selection of a subset.

Therefore, we aim to seek an adaptive matrix \(\) which can be self-scaled during the training process. The selected indices are expected to effectively preserve hard negative samples without becoming excessively large and causing training difficulties. Remark that problem (8) is independent for each sample. Next we introduce the definition of Adaptive Hard Negative Set for individual anchor.

**Definition 2**: _(Adaptive Hard Negative Set) Assume \(}()\) is the optimal solution of problem (8) with the \(i\)-th sample as the anchor. \(\) is the adaptive hard negatives set of the \(i\)-th sample if the following conditions are satisfied:_

\[(a)& j,\;}^ {T}([,_{j}])=[}^{T}( ),0],\\ (b)& j,\;}^{T} ([,_{j}])=[^{T}(),_{j}], \]

_where \([^{T}(),_{j}]\) denotes the solution vector with scalar \(_{j} 0\)._

The interpretation of this definition is intuitive: \(j\) within \(\) make a contribution to the self-expression of the anchor (i.e., the optimal corresponding coefficient \(_{j}\) are not zero), while \(j\) outside \(\) will not (i.e., the corresponding optimal coefficient equals to zero). In training, \(\) can be ascertained via the following theorem.

**Theorem 1**: _Assume \(}()\) is the optimal solution of problem (8) with the \(i\)-th sample as the anchor. The auxiliary function is defined as_

\[g(j)=_{l=1}^{L}_{j}^{(l)T}(_{i}^{(l)}-^{(l)}}()). \]

_Then hard negatives set can be computed by \(=\{j|g(j)|>\}.\)_

We can now give an understanding of what kind of samples are "hard" for a given anchor in the subspace framework. Theorem 1 implies that a sample is indispensable for subspace preserving if its representation sufficiently resembles the residual of existing self-expression. This diverges from homophily and similarity-based methods. Hence, our method exhibits "adaptive" in two aspects: On the one hand, as evident from the proof, it is clear that \(_{j}=0\) is equivalent to \(j\). Therefore, it can be removed from the adaptive hard negative set by updating \(\) once. On the other hand, throughout the training process, updating \(\) continuously expands the hard negatives set for the \(i\)-th sample. The distributional shifts from contrastive loss make it possible for \(\) to capture long-range hard negatives.

Inspired by the OMP in dictionary coding , we aim for the gradual expansion of \(\) with the training process. Beginning with an initial set, \(\) can be periodically updated every few epochs to reduce additional time overhead while capturing expansive hard negatives. In addition, to avoid large-scale computations, the size of \(\) can be controlled by confining hard negatives within a specified \(K\)-hop radius. The hyperparameter \(K\) dictates the range of selectable hard negatives.

Combining the solutions of all subproblems, the self-expression matrix can be defined as \(=[_{1},,_{n}]\), where \(C_{ji}\) reflects the hardness of \(j\) with respect to \(i\). To incorporate the subspace information into the contrastive loss, the self-expression coefficients \(C_{ij}\) is supposed to be mapped to the probability that \(j\) serves as a false negative for \(i\). This can be done through either a softmax operation or a linear mapping as follows:

\[(a)\;S_{ij}=|/)}{_{k _{j}}(|C_{kj}|/)},(b)\;S_{ij}= \{_{ij}|}{},\}. \]

\(S_{ij}\) in \((a)\) satisfies probabilistic properties and \(\) is tunable. \(S_{ij}\) in \((b)\) is proportionally scaled from \(C_{ij}\), where \(\) is the maximum value within a sampled subset \(\{C_{ij}\}_{(i,j)}\). The truncated parameter \(\) controls the ceiling of \(S_{ij}\) and is set to \(1\) by default. Thus, numerous values of \(S_{ij}\) in equation \((b)\) can be equivalent to \(\). In turn, \(S\) can be

Figure 4: The model architecture of GRAPE. The two views are generated through data augmentation of the initial graph. These three are fed into the parameter-sharing GNN encoder, where the projection header is alternative. The hard negatives set and the corresponding subspace coefficients \(\) are computed within the middle pathway. The green line in contrastive loss indicates hard negatives while the red line indicates true negatives, which are vary across epochs.

symmetrized by \((S^{T}+S)/2\). Upon obtaining \(S\), two schemes can be straightforwardly developed to enhance the performance of GCL. \(_{mask}\): GRACE in Eq. (2) treats all samples except itself as negatives, whose negatives set for anchor \(i\) can be denoted as \(}_{i}=[n]\{i\}\). While GRAPE estimates negatives' hardness and obtains the probability \(S\) for false negatives in turn, it can subsequently excluded the highly probable false negatives from \(}_{i}\). Specifically, in each epoch, \(j\) is included in the false negatives set \(_{i}\) for anchor \(i\) with a probability of \(S_{ji}\). The negatives set in this case turns out to be \(_{i}=}_{i}_{i}\). Therefore, the objective for \(_{i}\) in GRAPE\({}_{mask}\) is defined as:

\[_{mask}(_{i})=\] \[-_{i},_{i})/}}{e^ {(_{i},_{i})/}+_{j_{i}} (e^{(_{i},_{j})/}+e^{( _{i},_{j})/})}, \]

\(_{pos}\): GRACE in Eq. (2) exclusively treats itself as positives, whose positives set for anchor \(i\) is \(}_{i}=\{i\}\). For anchor \(i\), GRAPE\({}_{pos}\) incorporates \(j\) into the positives set with a probability of \(S_{ji}\) each epoch. The expanded positives set is denoted as \(_{i}\). Therefore, the objective for \(_{i}\) in GRAPE\({}_{pos}\) is defined as:

\[_{pos}(_{i})=\] \[-_{i},_{i})/}+ _{k_{i}}(e^{(_{i},_{k})/}+ e^{(_{i},_{k})/})}{e^{(_{i}, _{k})/}+_{j i}(e^{(_{i},_ {j})/}+e^{(_{i},_{j})/})}. \]

It is noteworthy that loss (13) is a variant of MIL-NCE (Miyato et al., 2018). Optimizing loss (13) enhances the overall similarity of positive pairs relative to negative pairs, rather than focusing on instance-specific distances.

Similar to GRACE, the overall contrastive loss is given as:

\[_{mask/pos}=_{i=1}^{n}(_{mask/pos}( _{i})+_{mask/pos}(_{i})). \]

Reviewing the results in Table 1, we can empirically summarize that \(_{pos}\) are suitable for high-confidence false negatives, while \(_{mask}\) tolerates low-confidence false negatives. Therefore, \(_{mask}\) is deemed as a more robust scheme. The model architecture is presented in Figure 4 and the procedure for GRAPE is detailed in Appendix A.

### Theoretical Analysis

**Why GRAPE works?** Reflecting on our motivation: we aim to identifies expansive and adaptive hard negatives as false negatives, which appears to be empirically derived. The essence behind this is the message-passing in GNNs: neighbors that encompass a substantial proportion of shared connections are not unduly distanced from each other. Therefore, local hard negative mining yield limited benefits. Recall the results in Table 1 that \(1\)-hop\({}^{*}\) (even \(2\)-hop\({}^{*}\)) does not significantly boost the baseline, while \(3\)-hop\({}^{*}\) shows a leap, which interprets the pursuit of expansive hard negatives. Besides, the self-expression loss can be expanded as follows

\[_{}\|-\|_{2}^{2}_{}-2 _{i}^{T}_{i}_{i}+_{i,j}_{i}^{T}_{j}_{i}_{j}. \]

The first term endows larger self-expression coefficients for negatives similar to the anchor, while the second term endows smaller coefficients for those highly similar to the other negatives. In GCL, the second term implies that the contributions of those involved in message passing with other hard negatives are diminished in self-expression, which is consistent with the intent in Figure 3a. This is rooted in its capacity to capture global long-range interactions, as discussed in (Zhu et al., 2018). Moreover, the regularizer in Eq. (8) exhibits sparsity as \(\) approaches \(1\) and group effect as \(\) approaches \(0\). It is worth noting that \(\) and \(\) directly impact the tightness of hard negative selection. GRAPE with large values of \(\) and \(\) results in a small hard negatives set.

By iteratively updating self-expression coefficients during training, the efficacy of GRAPE loss is qualitatively described as follows:

**Proposition 1**.: _In cases where GRAPE captures hard negatives \(\{_{i}\}_{i=1}^{n}\) within each individual subspace, both \(_{mask}\) and \(_{pos}\) contribute to the inter-subspace separation and intra-subspace cohesion._

Furthermore, GRAPE is associated with various methods, such as graph attention (Zhu et al., 2018), nonlinear latent subspace clustering (Zhu et al., 2018), and uniformity-tolerance dilemma (Zhu et al., 2018), as revealed in Appendix D.

**Maximizing mutual information** The improvement of GRAPE over the baseline can also be elucidated from the perspective of maximizing Mutual Information (MI):

**Theorem 2**.: _The contrastive loss in Eq. (14) gives a stricter lower bound of MI between input features \(\) and embeddings in two views \(\) and \(\), compared with the contrastive loss \(\) in Eq. (3) proposed by GRACE. This can be written formally as_

\[-<-_{mask/pos}(;, ) \]

Therefore, maximizing GRAPE loss corresponds to optimizing a more rigorous lower bound for the mutual information between node features and the acquired node representations, thereby furnishing a theoretical justification for the performance enhancement.

**Complexity Analysis** The procedure for GRAPE is detailed in Appendix A. Compared to our baseline, GRACE, extra complexity arises from the periodic updating of hard negatives set \(\{_{i}\}_{i=1}^{n}\) and the computation of self-expression coefficients \(\{_{i}\}_{i=1}^{n}\) every _intl_ epochs. Each of these \(n\) independent subproblems can be solved concurrently in parallel. The additional time overhead is \((Md)\), where \(M\) represents the largest cardinality within \(\{_{i}\}_{i=1}^{n}\). Since the hard negatives sets are restricted within the \(K\)-hop, there is \(M n\). Therefore, the additional time overhead is manageable.

## 4. Experiments

### Experimental Protocol

We conducted comparisons between GRAPE and ten advanced methods on eight node prediction datasets. The benchmark graph datasets include: **Cora**, **CiteSeer**, **PubMed**, **Wiki** CS, **Amazon Photo**, **Amazon Computers**, **Coauthor CS**, **Coauthor Physics**. They are all hosted by DGI. package 1. The dataset information is detailed in Appendix E.1. The comparative methods include: two supervised baselines (**GCN**(32), **GAT**(71)), two autoencoder-based baselines (**GAE**(33), **VGAE**(33)), eight state-of-the-art GCL methods (**DGI**(72), **GMI**(57), **MVGRL**(21), **GRACE**(109), **CCA-SSG**(97), **BGRL**(67), **ProGCLW**(84), **COSTA\({}_{MV}\)(102)).

For all augmentation-based methods, we adopt the most commonly used strategies for the graph augmentation: "edge removing" and "feature masking" (Kumar et al., 2018). At each epoch, "edge removal" randomly removes a certain proportion of edges from the original graph, while "feature masking" randomly masks a certain proportion of features. To be consistent with the comparison method, we configure the GNN encoder as a two-layer GCN. Self-supervised training is conducted on the entire graph and on the features of all samples. The embeddings obtained are fed into a semi-supervised linear classifier to get the final result. For Cora, CiteSeer, and PubMed datasets, we employ the standard split settings: 20 nodes per class are available for training, 500 nodes for validation and 1000 for testing. For the other datasets, we randomly assign 10% of the nodes for training, another 10% for validation, and allocate the remaining 80% for testing. The linear classifier is uniformly set to be a simple regularized logistic regression. The overall model is trained using the Adam optimizer.

We implement our GRAPE based on GRACE. The max training epoch is set to 100. The dimensions in the two-layer GNN encoder are set to 512 and 256, respectively. The learning rate for GRAPE is set to \(1 10^{-3}\), while that for linear classifiers is set to \(1 10^{-2}\). The interest for updating \(C\)_intol_ is fixed to 5 and the truncated parameter \(\) is fixed to 1. Our graph augmentation is achieved through a combination of 40% edge removal and 10% feature masking. The trade-off parameter \(\) is selected within \(\{10^{-1},10^{0},10^{1},10^{2}\}\) and \(\) is selected within \(\{0,0.1,,0.9,1.0\}\). The temperature parameter \(\) is selected within \(\{0,1,0.2,,1.0\}\) and the range of hard negatives \(K\) is selected within \(\{1,,5\}\). For all comparative methods, we adhere to the authors' default parameter settings and, where necessary, conduct parameter grid searches to achieve fair comparisons. Their implementations are all open-sourced. All experiments are conducted on NVIDIA RTX A6000 GPU with 48GB memory.

### Main Results

The node classification results are presented in Table 2. The reported results are averaged over 10 runs with random seeds. The "Input" refers to data for training, where \(\), \(\) and \(\) denotes feature matrix, adjacency matrix and label matrix respectively. OOM denotes out of memory. It can be observed that GRAPE achieves the state-of-the-art self-supervised performance on the first six datasets and surpasses the performance of supervised baselines (GCN, GAT) on the first seven datasets. Compared to its baseline GRACE, GRAPE achieves a comprehensive improvement. The hyperparameters involved in the experiment are listed in Appendix E.2. We perform node clustering performance evaluations in the completely unsupervised case in Appendix F.1. In addition, we execute comparative experiments on heterophily graphs (where connections primarily occur between dissimilar nodes) and the results are presented in Appendix F.2. These results corroborate GRAPE's capacity for precise identification of false negatives.

### How GRAPE Affects Training?

Subsequently, we conduct empirical investigations to explore what properties GRAPE learns and what hard negatives it captures. The experiments below are based on GRAPE\({}_{mask}\). Figure 4(a) illustrates the evolution of the percentage of false negatives within the hard negatives set \(\{_{i}\}_{i=1}^{n}\) throughout the training process. Figure 4(b) depicts the distribution of hard negatives set over different hops after training. Despite the expansion of the hard negatives set, the proportion of false negatives within it scarcely declines. Besides, a substantial portion of the hard negatives set consists of large-hop neighbors. Both observations imply that we achieve expansive yet dependable sets of hard negatives through subspace preservation.

Wang et al. (2019) introduced the concept of uniformity-tolerance dilemma in contrastive representation. We employ the two metrics to showcase the difference between GRAPE and its baseline GRACE. Specifically, the cohesion (CO) and uniformity (UN) of the learned

  
**Methods** & **Input** & **Cora** & **CiteSeer** & **PubMed** & **Wiki CS** & **Am-Photo** & **Am-Computer** & **Co-CS** & **Co-Physics** \\  GCN & \(,,\) & \(81.32 0.5\) & \(70.84 0.7\) & \(77.69 0.3\) & \(76.85 0.1\) & \(92.16 0.2\) & \(87.06 0.5\) & \(92.54 0.3\) & \(95.65 0.2\) \\ GAT & \(,,\) & \(82.57 1.0\) & \(71.96 1.0\) & \(77.51 0.3\) & \(78.35 0.1\) & \(91.45 can be defined as follows:

\[=_{g_{i}=g_{j}}(f^{T}(_{i})f(_{j})),=_{i,j}(-f^{T}(_{i})f(_{j})) \]

\(f\) denotes our GNN encoder \(f_{}(,)\). A higher CO implies higher intra-class cohesion, while a higher UN implies a more uniform embedding distribution. The comparison of the two metrics for GRAPE and GRACE during training is depicted in Figure 6.

At the beginning of training, CO and UN for both GRAPE and GRACE are nearly identical due to the similar initialization. As discussed ahead, GRAPE explicitly or implicitly brings the representations inside the same subspace closer, which strengthens the intra-class cohesion and reduces the global uniformity. With the expansion of the hard negatives set, the margin of cohesion between GRAPE and GRACE is enlarged, which is in line with our original intention. Since the mask mechanism of GRAPE\({}_{mask}\) is presented in a probabilistic form, uniformity doesn't exhibit significant decreases compared to GRACE. Additionally, Figure 7 shows how the test accuracy steadily improves as the GRAPE loss is optimized.

Furthermore, the efficiency of GRAPE can be enhanced through aforementioned the scalable parameterization or by integration into negatives-independent methods. The corresponding results are provided in Appendix F.3 and F.4.

### Visualization and Hyperparameter Study

In this subsection, we present intuitive results to illustrate the effectiveness of GRAPE. Figure 7 shows the distributions of the true/false negatives of the same anchor in different phases on Cora. The horizontal axis denotes the cosine similarity between negatives and anchor, which is non-negative due to the ReLU before output. The variation with training is discernible, especially from (b) to (c), validating the efficacy of adaptive hard negative selection.

We present t-SNE visualization of GRAPE's running results without labels (i.e., before classification). As depicted in Figure 9, nodes are partitioned into multiple distinct clusters.

The influence of the hyperparameters in GRAPE is examined to validate the feasibility. The sensitivity analysis of the two trade-off parameters in Eq. (8) is depicted in Figure 10. The test accuracy of GRAPE remains stable across a wide range of \(\) and \(\), indicating its independence from meticulous parameter settings. Simultaneously, both parameters indeed exert an influence on the model. Besides, the parameter analyses of GRAPE under different interval \(intl\) and range \(K\) are shown in Appendix F.5.

## 5. Conclusion

In this paper we propose a novel method for estimating negatives' hardness in GCL. Our method emphasizes the potential in exploring expansive and adaptive negatives. These two goals are coupled in our subspace preserving scheme. We elucidate the motivation, provide empirical and theoretical underpinnings and conduct comprehensive experiments to dissect the effectiveness of GRAPE. Drawing from the contributions of this paper, we hopefully point out two interesting and promising avenues for further research. First, since subspace theory is not directly reliant on existing connections, it shows potential in addressing the impact of noisy, incomplete, or vulnerable graph structures on GNNs (a branch called graph structure learning). Second, self-expression contribute to preserving local structures and may serve as a form of constraint to slow down message passing for deeper GNNs.

Figure 8. Negatives distributions in different phases.

Figure 6. Variation of cohesion and uniformity.

Figure 7. Variation of loss and test accuracy with training.

Figure 9. Visualization of node embedding without labels.

Towards Espanuive and Adaptive Hard Negative Mining:

Graph Contrastive Learning via Subspace Preserving