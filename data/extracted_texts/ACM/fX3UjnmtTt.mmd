# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

GSSL methods (Zhou et al., 2017; Wang et al., 2018; Zhang et al., 2019; Zhang et al., 2019) often fail to fully leverage the complementary nature of structural and positional information, which hinders their ability to differentiate non-isomorphic graphs with similar local attributes but different global topologies.

Building on these insights, we develop a framework that fundamentally reimagings graph representation learning by innovating both GNN architecture and the self-supervised learning process. Our goal is to significantly enhance the expressiveness and representational capacity of GSSL. In distinguishing non-isomorphic graphs with similar local structures but different global topologies.

To this end, we focus on two main components:

_GenHopNet_ GNN: A novel GNN architecture designed to capture complex structural information beyond immediate neighborhoods. It implements a k-hop message-passing scheme that expands the receptive field of each node, allowing the model to capture long-range dependencies and global structural information.

Structural and Positional Aware Self-Supervised Learning: A new self-supervised learning framework that preserves and uses crucial topological information by incorporating both structural and positional information into learning. It overcomes limitations of methods that focus solely on final graph representations.

**Contributions.** Below we summarize our main contributions:

1. We introduce _GenHopNet_, a GNN framework that implements a k-hop message-passing aggregation scheme and surpasses the expressiveness of the WL test.
2. We propose a structural- and positional-aware GSSL framework, namely _StructPosGSSL_, for GNN pre-training, enabling the learning of representations invariant to specific structural and feature augmentations while preserving topological and positional information.
3. With extensive experiments on both real-world and synthetic datasets we demonstrate that our _StructPosGSSL_ achieves superior performance on most graph classification benchmarks.

## 2. Related Work

GNNs are a class of neural networks designed to effectively process and represent graph-structured data. Since the development of the previous GNN models, various adaptations have emerged, including GCN (Kipf and Welling, 2016), GAT (Vaswani et al., 2017), GraphSAGE (Hamilton et al., 2017), and GIN (Vaswani et al., 2017), among others. These models aim to learn distinguishing representations of graphs based on their data labels. However, annotating graph data, such as identifying categories of biochemical molecules, often requires specialized expertise, making it challenging to obtain large-scale labeled graph datasets (Wang et al., 2018). This challenge highlights a key limitation of supervised graph representation learning.

Contrastive Learning (CL) stands out as a highly effective self-supervised technique embedding unlabeled data (Zhou et al., 2017). By bringing similar examples closer together and pushing dissimilar ones apart, CL methods--including SimCLR (He et al., 2016), MoCo (Kipf and Welling, 2016), BYOL (Kipf and Welling, 2016), MetAug (Wang et al., 2018), and Barlow Twins (2018)--have demonstrated remarkable success in the realm of computer vision (Zhou et al., 2017; Wang et al., 2018).

**Graph Self-Supervised Learning (GSSL)** is a promising technique for learning representations of graph-structured data without requiring labeled examples, making it especially effective for graph classification tasks. To date, many GSSLs with unique strategies have been proposed to enhance graph classification. These methods build on the strengths of GNNs and CL techniques (Zhou et al., 2017; Wang et al., 2018; Zhang et al., 2019).

A key focus of GSSL is the development of effective graph augmentation strategies. For instance, GraphCL (Wang et al., 2018) introduces perturbation invariance and proposes various graph augmentations, such as node dropping, edge perturbation, attribute masking, and subgraph extraction. Recognizing the limitations of using complete graphs, Sub-Con (Shi et al., 2019) advocates for subgraph sampling as a more effective method for capturing structural information. To improve the semantic depth of sampled subgraphs, MICRO-Graph (Kipf and Welling, 2016) proposes generating informative subgraphs by learning graph motifs. Furthermore, the process of selecting suitable graph augmentations can be time-consuming and labor-intensive; JoAO (Wang et al., 2018) addresses this by introducing a bi-level optimization framework that automates the selection of data augmentations tailored to specific graph data. RGCL (Wang et al., 2018) argues that random destruction of graph properties during augmentation can lead to a loss of critical semantic information and proposes a rationale-aware approach for graph augmentation. Additionally, SPAN (Zhou et al., 2017) introduces a spectral perspective for guiding topology augmentation, noting that previous work has largely concentrated on spatial domain augmentation. To address the neglect of hierarchical structures in existing GSSL methods, HGCL (Wang et al., 2018) proposes Hierarchical GSSL, which integrates node-level CL, graph-level CL, and mutual CL components. Another important aspect of GSSL is the process of negative sampling; BGRL (Wang et al., 2018) simplifies this process by eliminating the need for constructing negative samples, allowing it to scale efficiently to large graphs. To mitigate sampling bias, PGCL (Wang et al., 2018) introduces a negative sampling strategy based on semantic clustering. In contrast to existing GSSL methods, our approach enhances both global and local structural understanding. It ensures that global graph representations effectively capture complex topological similarities and differences, while local node embeddings are refined to preserve detailed structural and positional nuances. By incorporating structural and positional awareness through invariance, variance, and covariance across node features, our method improves the ability to distinguish between isomorphic and non-isomorphic graphs. This ensures that both global graph structure and local node characteristics are robustly represented and aligned.

**Enhancing GNN Expressiveness.** A substantial amount of effort has been devoted to enhancing the expressive power of GNNs beyond the 1-WL1. This pursuit arises from the need to capture more intricate graph structures and relationships to address complex real-world problems effectively. Broadly, there are four primary directions which GNNs can extend beyond the 1-WL level: (1) A number of studies have introduced higher-order variants of GNNs, demonstrating comparable expressiveness to k-WL with \(k 3\)(Kipf and Welling, 2016). As an example, k-order graph networks, introduced by (Wang et al., 2018), offer expressiveness that is similar to a set-based variation of k-WL. (Wang et al., 2018) introduced a 2-order graph network that maintains expressive power similar to 3-WL. Furthermore, (Wang et al., 2018) introduced a localized variant of k-WL, focusing solely on a subset of vertices within a neighborhood. Nevertheless, using these expressive GNNs presents challenges due to their intrinsic computational demands and intricate architecture. In addition, some studies aimed to integrate inductive biases on isomorphism counting w.r.t predefined topological attributes such as triangles, cliques, and cycles . These efforts similar to the traditional graph kernels, as outlined by . However, the task of predefining topological characteristics needs specialised knowledge in the respective domain, a resource that is frequently not easily accessible. (3) In a different vein, there has been a recent surge in studies exploring into the notion of enhancing GNNs through the augmenting of node identifiers or stochastic features. For example,  introduced an approach that preserves a node's local context through the manipulation of node identifiers in a permutation-equivariant fashion.  developed ID-GNNs, incorporating vertex identity information in their design.  and  assigned one-hot identifiers to nodes, drawing inspiration from the principles of relational pooling. In a similar vein,  enriched the representational capability of GNNs by incorporating a random feature for each node. There are some other approaches modify the MPNN framework or incorporate additional heuristics to enhance their expressiveness . (4) Some works inject positional encoding (PE) as initial node features because nodes in a graph lack inherent positional information. Canonical index PE can be assigned to the nodes in a graph. However, the model must be trained on all possible index permutations, or sampling must be employed . Another direction for PE in graphs is using Laplacian Eigenvectors , as they establish a meaningful local coordinate system while maintaining the global structure of the graph.  proposed a PE scheme (RWPE) based on random-walk diffusion to initialize the positional representations of nodes. These positional encoding methods such as Laplacian positional encoding  or RWPE  have a significant limitation in that they usually fail to quantify the structural similarity between nodes and their surrounding neighborhoods. Nonetheless, while these techniques have demonstrated their expressivity to go beyond 1-WL. However, it remains uncertain what further attributes they can encompass beyond the scope of 1-WL.

Despite these limitations, our method offers notable advantages. _GenHopNet_ enjoys greater expressive power than the 1-WL test, providing improved node and graph-level distinction by accounting for both local and global graph structures through closed walk counts and positional information. Additionally, by incorporating edge centrality measures to enrich message-passing, _StructPosGSSL_ enhances the model's ability to differentiate various types of connections, making it strictly more expressive than Subgraph MPNNs  in distinguishing certain non-isomorphic graphs.

## 3. An Expressive and Generalizable k-hop Message Passing Framework

In this section, we introduce the Expressive and Generalizable Message-Passing (EGMP) framework, designed to incorporate learnable local structural information through an aggregation method that leverages the k-hop neighborhood without the need for explicit extraction of local substructure patterns. We provide a theoretical analysis demonstrating how k-hop GNNs within this framework can achieve greater expressiveness than 1-WL.

Let \(G=(V,E,)\) be an undirected graph with a set of nodes \(V\) and a set of edge \(E\), where \(|V|=m\), \(|E|=e\), and \(^{m m}\) is the adjacency matrix. Nodes are associated with a feature matrix \(^{m x}\) with \(z\) features for each node. Let \(=-\) be a Laplacian matrix, where a diagonal matrix \(^{m m}\), and \(_{ii}=_{j}_{j}\). Let be a real symmetric matrix and diagonalizable as \(=^{H}\). Here, \(=\{u_{i}\}_{j=1}^{m}^{m}\) are orthogonal eigenvectors, \(=diag(\{1,,,m\})^{m m}\) are real eigenvalues, and \(^{H}\) is a hermitian transpose of \(\).

Let \(\{\}\) represent a multiset. Let \(}^{k}=(}^{k}_{})_{  V}\) where \(}^{k}_{}=}^{k}_{}} {_{i N(v)^{k}_{}}^{k}_{}}\) refers to a normalized value of \(^{k}_{}\), and \(^{m x}\) be the matrix of input feature vectors with each \(_{v}^{x}\) corresponding to each vertex \(v V\). We indicate the feature vector of vertex \(v\) at the \(^{}\) layer as \(^{(t)}_{v}\) and set \(^{(0)}_{v}=_{v}\). Then, the definition of the

Figure 1. (a) A high-level overview of the GSSL model architecture of _StructPosGSSL_ (\(G\) is an input graph and \(G^{},G^{}\) are two augmented views). Our design comprises three main components: (i) a structural encoder (SE) that generates structural embeddings (\(h_{SE}\) and \(h^{}_{SE}\)) for nodes based on their local structural properties; (ii) a positional encoder (PE) that generates positional embeddings (\(h_{PE}\) and \(h^{}_{PE}\)) for nodes; and (iii) a pooling layer that aggregates the node representations to generate the final graph representation. Moreover, \(_{}\) and \(_{}\) are two shared projection heads for node representations and graph representations, respectively. (b) The real-world graph structures of two molecules, Decalin and Bicyclopentyl. While standard Graph SSL frameworks cannot distinguish between these molecular structures, our model successfully differentiates them.

(+1)th layer in an aggregation scheme is given as:

\[_{e}^{(t)} =^{E}\{(_{}, _{u}^{(t)},e_{}^{b},e_{}^{c})|u(v) \}, \] \[_{u}^{(t)} =^{N}\{(}_{}^{k},_{u}^{(t)})|u^{k}(v)\};k 2,\] (3) \[_{v}^{(t)} =^{G}\{}_{}^{k},_{v}^{(t)}\};k 2,\] (4) \[_{v}^{(t+1)} =_{v}^{(t)},_{e}^{(t)},_{u}^{(t)},_{v}^{(t)}. \]

The above equations define a process for aggregating messages in our GNN, where:

1. \(^{E}()\) in Eq. 1 computes the edge-level aggregated message \(M_{e}^{(t)}\) for vertex \(v\) based on the node features and edge attributes of its neighbors \(u(v)\) within the 1-hop neighborhood;
2. \(^{N}()\) in Eq. 2 aggregates a normalized message \(M_{u}^{(t)}\) from the \(k\)-hop neighbors (\(k 2\)) of vertex \(v\) weighting their contributions by the normalized adjacency matrix \(}_{}^{k}\);
3. \(^{I}\) in Eq. 3 calculates the self-message \(_{v}^{(t)}\) for vertex \(v\) using its own adjacency information and features, focusing on capturing closed-walks of length up to \(k\) (\(k 2\)) that return to node \(v\) itself;
4. Eq. 4 combines the current feature vector \(_{v}^{(t)}\) of vertex \(v\) with the aggregated messages \(_{e}^{(t)},_{u}^{(t)}\), and \(_{v}^{(t)}\) to update final node representation of \(v\) for the next layer.

We use the above set of equations to compute the graph's topological information. For the positional information, we only use Eq. 1 by replacing node features \(_{u}^{(t)}\) with positional features \(_{u,pos}^{(t)}\).

In more detail, \(_{u}^{(t)}\) is a message aggregated from neighbors of node \(v\), using their normalized coefficients \(}_{}^{k}\), while \(_{v}^{(t)}\) is the adjusted message from node \(v\) to itself, considering walk lengths up to k-hop. The diagonal elements of \(^{k}\), namely \(_{}^{k}\), count the number of closed walks of length \(k\) that start and end at the same node \(v\). Such a mechanism highlights the importance of node \(v\) within its local topology and its role in the connectivity of the graph over multiple hops. This should exhibit the following properties:

1. **Closed Walks and Connectivity:** The power \(^{k}\) enumerates all possible walks of length \(k\) in the graph. By examining \(_{}^{k}\), one can infer how _connected_ or _central_ a node is with respect to walks of length \(k\). \((^{k})\) counts the total number of closed walks of length \(k\) starting and ending at the same vertex. This measure gives a quantitative sense of the graph's connectivity:
2. **Local Connectivity:** High numbers of shorter closed walks (smaller \(k\)) indicate strong local connectivity. This is useful for understanding how tightly knit individual neighborhoods are within the graph.
3. **Global Connectivity:** As \(k\) increases, the nature of the closed walks provides insights into the global connectivity and the presence of cycles within the graph. Any cycle in a graph is a closed walk; however, not all closed walks are cycles as cycles have the additional constraint of not repeating vertices or edges except the starting/ending vertex.
4. **Isomorphic Invariant:** This simple trick facilitates the creation of unique node representations by ensuring that each node possesses a distinct k-hop topological neighborhood, provided that k is sufficiently large. The sum \(_{k}_{}^{k}\) across different powers \(k\) reflects the number of closed walks of varying lengths starting and ending at node \(v\). For instance, the equality \(_{k}_{}^{k}=_{k}_{^{k}}^{k}\), holds if nodes \(v\) and \(v^{}\) are k-hop isomorphic, implying that they share identical local connectivity patterns up to k-hops. This characteristic serves as a powerful tool for identifying and distinguishing nodes based on their structural roles within the network.

Understanding cycle and closed-walk isomorphisms is essential for elucidating the structural roles of nodes within a graph, revealing both local and global connectivity patterns. Cycle isomorphism highlights nodes that engage in similar closed-loop interactions, while closed-walk isomorphism provides insights into broader connectivity by capturing indirect relationships that contribute to the overall network topology. The definitions of cycle and closed-walk isomorphisms formalize these concepts, emphasizing their significance in analyzing graph structures.

**Definition 1**.: _Cycle Isomorphism (\(_{Cycle}\)): Two nodes \(v\) and \(v^{}\) are cycle isomorphic if the sum of the powers of \(\) over \(k\) (i.e., \(_{k}_{}^{k}\) and \(_{k}_{^{k}}^{k}\)) considers only closed walks that are also cycles (i.e., walks that do not repeat any vertices or edges except the starting/ending vertex)._

**Definition 2**.: _Closed Walk Isomorphism (\(_{ClosedWalk}\)): Two nodes \(v\) and \(v^{}\) are closed walk isomorphic if the sum of the powers of \(\) over \(k\) (i.e., \(_{k}_{}^{k}\) and \(_{k}_{^{k}}^{k}\)) considers all possible closed walks (i.e., walks that start and end at the same vertex, but may repeat vertices and edges)._

**Theorem 1**.: _The following statement is true: (a) If \(_{k}_{}^{k}_{Cycle}\)\(_{k}_{^{k}^{}}\), then \(_{k}_{}^{k}_{ClosedWalk}\)\(_{k}_{^{k}^{}}\); but not vice versa._

Going back ot Eq. 1, \(_{v}^{(t)}\) is a message aggregation function that considers the 1-hop neighborhood, including their edge connections and edge attributes. We enrich the 1-hop neighborhood aggregation by injecting three different centrality based edge attributes \(_{}^{c}\), between given pair of nodes: (1) Edge Betweenness (EB), (2) Edge Closeness (EC), and (3) Edge Clustering Coefficients (ECC) as additional features (Shen et al., 2016). These attributes are designed to augment the distinguishing capabilities of the model, enabling it to better differentiate between various types of connections and facilitating nuanced understanding and processing of edge-related information in graphs. This should exhibit the following properties:

Figure 2. A high-level overview of different closed-walks \(k=2,,4\), where the blue node represents the source node. For \(k=2\), the walk can traverse between nodes multiple times, forming a non-cyclic path. For \(k=3\) and \(k=4\), the walks can capture closed cycles of length 3 and 4, respectively, effectively identifying cyclic structures in the graph.

[MISSING_PAGE_FAIL:5]

and space complexities of _GenHopNet_ are \(O(tezd)\) and \(O(e)\), respectively, where \(e\) denotes the number of edges in the graph, \(t\) represents the number of layers, and \(z\) and \(d\) correspond to the dimensions of the input and output feature vectors.

## 5. Graph Self-Supervised Learning Framework

In this section, we introduce Structural and Positional GSSL (_StructPosGSSL_), a new class of graph self-supervised learning framework based on structural and positional information within graphs.

### Data Augmentation for Graph

The goal of data augmentation is to produce consistent, identity-preserving positive samples of a specific graph. In this work, we use two main types of augmentation strategies: structural augmentation and feature augmentation (Wang et al., 2017). In structural augmentation, three distinct strategies are considered: (1) Subgraph Induction by Random Walks (RWS), (2) Node Dropping (ND), and (3) Edge Dropping (ED). For feature augmentation, we employ three different approaches: (1) Feature Dropout (FD), (2) Feature Masking (FM), and (3) Edge Attribute Masking (EAM). In our work, we generate different augmented graphs from a single input graph \((,)\), resulting in two correlated views, namely \((}^{},}^{})\) and \((}^{^{}},}^{^{}})\).

### Expressive/Generalizable Graph Encoders

For each augmented view, we process it through two distinct GNN encoders. One encoder is dedicated to structural encoding via the proposed _GenHopNet_. Alongside structural features in the _GenHopNet_, we employ another encoder for capturing positional information. We initiate the positional feature vectors using Laplacian eigenvectors, as outlined in (Shen et al., 2017). This second encoder, also a GNN, applies Eq.1 with the Combine function, i.e., \(_{,}^{(t+1)}= _{,}^{(t)},_{^{}}^{(t)}\), and leverages the spectral properties of the graph Laplacian. This approach targets the smallest non-trivial eigenvalues to derive meaningful positional encodings that accurately reflect the structural roles of nodes within the graph. By focusing on the spectral characteristics of the Laplacian, this encoding strategy effectively captures both the local connectivity of nodes and the broader topology of the graph, significantly enhancing the model's capability to understand and manage complex graph structures. Each encoder outputs node representations and a final graph representation for augmented views. We then pass them through another shared projection head (an MLP) to obtain the final structural and positional representations for both nodes and graphs. Next, we concatenate the structural features with positional features for node representations and graph representations separately, ensuring a comprehensive integration of both structural and positional data. To facilitate the end-to-end training of encoders and generate comprehensive node and graph representations that are independent of specific downstream tasks, we implement a loss function that merges NT-Xent (Kumar et al., 2017) and refined VICReg (Chen et al., 2019).

### Training Pipeline

In the pursuit of effective self-supervised learning frameworks, the quality of learned representations hinges critically on the choice of loss functions. To enable end-to-end training of the encoders and to develop robust graph and node representations that are independent of downstream tasks, we employ the NT-Xent (Kumar et al., 2017) loss to learn the graph representations and use the refined VICReg (Chen et al., 2019) loss as a regularization term to learn the node representations. The NT-Xent loss, which maximizes discriminative power between positive and negative samples of graph representations, while VICReg's regularization terms that ensure a balanced and non-redundant spread of node features across dimensions to reduce the representational collapse. This integration stabilizes training in self-supervised setup and enriches graph representations for downstream tasks.

The NT-Xent loss, is defined by the formula:

\[L_{N}(^{},^{})=-log^{ },^{})/)}{_{k=1}^{2m}_{\{k+r^{} \}}exp(sim(^{},^{k})/)}, \]

where \(^{},^{}^{d}\) are graph representations for two augmented views, with \(\) denoting the embedding size of each graph augmented view in the dataset. The parameter \(\) is the temperature scaling parameter, and \(_{\{k+i\}}\) is an indicator function that is \(\) if \(k i\) and \(0\) otherwise. Here \(sim(^{},^{})\) is a similarity function, typically the cosine similarity, between two vectors \(^{}\) and \(^{}\).

We combine the refined VICReg loss with the contrastive NT-Xent loss to create a unified loss function that maximizes mutual information between graph embeddings while preserving structural and positional node alignment. This unified approach not only aligns embeddings for isomorphic node pairs but also maintains diversity and enhances the model's topological discriminative power. By enabling the model to differentiate between nodes with subtle structural or positional differences, this method is crucial for accurately identifying both isomorphic and non-isomorphic node pairs. In this work, the refined VICReg loss is specifically adapted for correspondence node alignment to improve isomorphic graph representation learning in a self-supervised setting.

The refined VICReg loss is given by:

\[L_{V}(^{^{}},^{^{}})= _{inv} L_{inv}(^{^{}},^{^{ }})+_{outer} L_{outer}(^{^{}},^{^{}})+\] \[_{coo} L_{coo}(^{^{}},^{ ^{}}) \]

where \(_{inv},_{coo}\) and \(_{coo}\) are weighting factors for the invariance, variance, and covariance components, respectively. The Invariance Loss \(L_{inv}(^{^{}},^{^{}})= _{i}||^{^{}}_{i}-^{^{}}_{i}||_{2}^ {2}\), where \(^{^{}},^{^{}}^{m d}\) represent the node representations of the two augmented views. The Variance Loss \(L_{coo}(^{^{}},^{^{}})= _{j=1}^{d}|max(0,-std(^{^{}}_{j},e))-max(0, -std(^{^{}}_{j},e))|\). The Covariance Loss \(L_{coo}(^{^{}},^{^{}})= _{i j}|[(^{^{}})]_{2}^ {2}-[(^{^{}})]_{2}^{2}|\) where \((^{^{}})=_{i=1}^{m}( ^{^{}}_{i}-^{^{}})(^{^{}}_{i} -^{^{}})^{T}\), with \(^{^{}}=_{i=1}^{m}^{^{}}_{i}\) denoting the mean over feature vectors. The Invariance Loss captures the inherent structure of the graph, maintaining node representations'

   Method & CSL & SR25 \\  GCN & \(10.0 0.0\) & \(6.6 0.0\) \\ GIN & \(10.0 0.0\) & \(6.6 0.0\) \\
3WLGNN & \(97.8 10.9\) & - \\
3-GCN & \(95.7 14.8\) & \(6.6 0.0\) \\ GCN-RNI & \(16.0 0.0\) & \(6.6 0.0\) \\  StructPosGSSL-SA & \(\) & \(\) \\ StructPosGSSL-FA & \(\) & \(\) \\   

Table 1. Classification accuracy (%) on the test set for CSL and SR25 datasets.

structural and positional properties despite augmentation. The Variance Loss helps distinguish non-isomorphic nodes, enhancing the model's ability to capture structural differences. Lastly, the Covariance Loss ensures that node embeddings reflect diverse structural and positional aspects, improving the expressiveness and discriminative power of the representations.

The total combined loss for integrating NT-Xent and refined VICReg is simply the sum of these two losses:

\[L_{T}(^{},^{},^{},^{ })=L_{N}(^{},^{})+ L_{V}(^{},^{}), \]

where \(\) is the weighting factor for VICReg loss. This unified loss function effectively integrates isomorphism preservation with representation expressiveness and diversity.

Theorem 3 ().: StructPosGSSL2 _is more expressive than subgraph MPNNs in distinguishing certain non-isomorphic graphs._

## 6. Numerical Experiments

In this section, we evaluate our self-supervised learning framework on graph classification benchmark tasks. The results from our models are statistically significant with a 95% confidence level. We evaluate _StructPosGSSL_ on graph classification benchmark tasks and compare their performance with leading baselines to address the following questions:

1. How effective are _StructPosGSSL_ in small graph classification task based on empirical performance?
2. How effective are _StructPosGSSL_ in large graph classification tasks based on empirical performance?
3. How effective is _StructPosGSSL_ for isomorphism testing in synthetic graph classification tasks?
4. How do structural and positional encodings impact overall performance?

In this following sections, we analyze the experimental results to address the four previously mentioned questions.

### Experiments on Small Graphs

We use eight datasets from two categories: (1) bioinformatics datasets: MUTAG, PTC-MR, NCI1, and PROTEINS (Gil et al., 2018; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019); (2) social network datasets: IMDB-B, IMDB-M, COLLAB and RDT-MSK (Gil et al., 2018).

We compare our method against fourteen baseline approaches: (1) Graph kernel methods: WI. subtree kernel (WL) (Walner et al., 2018), WL-OA (Wang et al., 2019), RetGK (Wang et al., 2019), P-WL (Wang et al., 2019), and WL-PM (Wang et al., 2019); (2) GNN-based methods: PATCHY-SAN (Wang et al., 2019), DGCNN (Wang et al., 2019), CAPSGNN (Wang et al., 2019), and GIN (Wang et al., 2019); (3) Unsupervised methods: InfoGraph (Wang et al., 2019), GraphCL (Wang et al., 2019), MVGRL (Wang et al., 2019), AutoGCL (Wang et al., 2019), and JOAO (Wang et al., 2019).

Specifically, the _GenHopNet_ encoder models are initially trained in an unsupervised manner, and the resulting embeddings are then input into a linear classifier to accommodate the labeled data. Then, for fair comparison, we execute our method using ten random splits (Wang et al., 2019) and utilize the 10-fold cross-validation method and present the best mean accuracy (%) along with the standard deviation. The results are presented in table 2 and 3. We have two settings: (1) StructPosGSSL-SA, which considers structure augmentation, and (2) StructPosGSSL-FA, which considers feature augmentation. In both settings, we employ the Adam optimizer (Kingma and Ba, 2014), with hidden dimension of 128, weight decay is 0.0003, a 2-layer MLP with batch normalization, 100 epochs, positional encoding dimension of 6, a dropout rate of 0.5, and a temperature scaling parameter \(\) of 0.10.

We choose a batch size from \((32,64,128,256)\) and a number of hops \(k\{2,3,4,5,6\}\). We use \(_{inn}=1,_{var}=3,_{coa}=2\), and \(=0.5\) for MUTAG and \(_{inn}=1,_{var}=23,_{coa}=23\), and \(=0.0003\) for PTC-MR, and \(_{inn}=1,_{var}=24,_{coa}=24\), and \(=0.005\) for the remaining datasets. The readout function, as described in (Wang et al., 2019), is utilized, which involves concatenating representations from all layers to derive a final graph representation.

To address **Q1**, in Tables 2 & 3, StructPosGSSL outperforms the best baseline by 0.4% (PATCHY-SAN), 1.1% (CapsGNN), 1.1% (WL-OA, MVGRL), 0.5% (WL-PM), and 0.2% (GIN) on the datasets MUTAG, PTC-MR, IMDB-B, IMDB-M, and RDTMSK, respectively.

These gains are a reflection of the inherent characteristics of the datasets. Graphs with smaller diameters, _i.e._, IMDB-B, IMDB-M, PTC-MR, and MUTAG, feature nodes that are closer together, promoting localized interactions that enable GNNs to capture both local and global information effectively, even with few message-passing steps. In such cases, structural encoding with closed walks is particularly beneficial, as it differentiates local structures by capturing repeated node interactions and identifying cycles. Conversely, datasets such as NCI1 and COLLAB, with larger diameters, present increased structural complexity, making it challenging to capture patterns using closed walks alone due to the difficulty of accounting for distant node interactions with limited local information.

### Experiments on Large Graphs

We utilize five large graph datasets from the Open Graph Benchmark (OGB) (Kumar et al., 2018), comprising one molecular graph dataset (ogbg-moltoxcast, ogbg-moltox21, ogbg-moltbox) and one protein-protein association network (ogbgbg-ppa). We compare our approach with the following methods that have reported results on the aforementioned OGB datasets: GIN and GIN+VN (Kumar et al., 2018), GSN (Wang et al., 2019), ID-GNNs (Wang et al., 2019), Deep LRP (Wang et al., 2019), GraphSNN (Wang et al., 2019), DS-GNN (EGO+), (Wang et al., 2019), DSS-GNN (EGO+) (Wang et al., 2019), and POLICY-LEARN (Chen et al., 2019).

For large graph datasets, we adopt the same experimental framework as outlined in (Kumar et al., 2018). Our evaluation process is divided into two distinct learning phases. In the initial phase, the models are trained in a self-supervised fashion using only node features and graph structure without any label data. Subsequently, in the second phase, the representations generated by the GNN encoders during the first phase are fixed in place and employed to train, validate, and test the models using a straightforward linear classifier.

We utilize the Adam optimizer with a learning rate of 0.001, a batch size of 32, dropout of 0.5, positional encoding dimension of 6, and run training for 100 epochs across all datasets. We use a 2-layer MLP with a hidden dimension of 200 and a temperature scaling parameter \(\) of 0.10 for both settings. We choose \(_{inn}=1,_{var}=24,_{coa}=24\), and \(=0.005\) for all datasets. The classification accuracy results are presented in Table 4.

To address **Q2**, in Table 4, StructPosGSSL consistently outperforms all the baseline methods across all OGB graphs listed. StructPosGSSL surpasses best results of existing GNNs by 0.5% (POLICY-LEARN), 0.55% (DSS-GNN (EGO+)), 0.32% (GIN+VN), 0.24% (GraphSNN), and 0.42% (GIN+VN) on the datasets ogbg-moltoxcast, ogbg-moltox21, ogbg-moltiv, ogbg-ppa, and ogbg-moltox, respectively.

[MISSING_PAGE_FAIL:8]