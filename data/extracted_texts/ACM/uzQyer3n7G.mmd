# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

To overcome these limitations, designing a backdoor attack that implants triggers without altering the graph's topology presents a promising solution to circumvent the creation of anomalous edges. The challenges of developing such an attack lie in two aspects. First, the budget for node attribute manipulation is limited. Although routine in many attacks, performing all-dimensional feature perturbation or generalization is costly and often infeasible in real-world scenarios where security and privacy are major concerns. For example, in financial systems, where nodes represent participants and edges represent transactions, modifying sensitive attributes such as credit records is impractical and likely to trigger alarms within regulatory systems. Thus, practical constraints demand a smaller attack budget, allowing only perturbations in limited feature dimensions. Second, the manipulated node attributes should be reasonable. With a limited attack budget, minimizing modifications is imperative to maintain the trigger's stealthiness and restraining significant semantic changes.

To tackle these issues, we propose an approach named **SPEAR** (Structure-PresErving grAb backdooR attack). First, to maximize the efficiency of the attack budget, we focus on effectively selecting poisoned nodes by examining the variety and prediction uncertainty of candidate nodes. Second, to ensure that the triggers exert significant influence over the downstream classifier and generalize well to unseen nodes, we implement a global importance-driven feature selection strategy to identify the most impactful feature for trigger implantation. The structure-preserving manipulation is achieved via a trigger generator that leverages neighborhood-aware semantic enrichment. In addition, we integrate a self-similarity regularization term into the loss function to promote minimal modifications. In summary, our main contributions can be summarized as follows:

* We empirically demonstrate the vulnerability of existing backdoor attacks to robust test models and anomalous edge detection, leading to our proposal of a structure-preserving attack named SPEAR.
* In SPEAR, we introduce a novel node selection method to mitigate antagonistic effects, a global importance-driven feature selection method to enhance the effectiveness of trigger implanting, along with a refined trigger generator to leverage neighborhood information.
* Extensive experiments on real-world datasets with various test and defense models demonstrate that SPEAR outperforms state-of-the-art backdoor attacks.

## 2. Related Works

### Adversarial Attacks against GNNs

Graph Neural Networks (GNNs) have emerged as indispensable tools for complex graph-structured data analysis (Glorot and Bengio, 2010; Goodfellow et al., 2014; Goodfellow et al., 2014; Goodfellow et al., 2014), showing exceptional capability in tasks like node classification and link prediction (Sutskever et al., 2016; Goodfellow et al., 2014). Foundational models such as Graph Convolutional Networks (GCN) (Glorot and Bengio, 2010) and GraphSAGE (Hamilton et al., 2017) have provided scalable solutions, driving the adoption of GNNs across various fields, including fraud detection, biological systems, and recommendation systems (Bahdanau et al., 2015; Goodfellow et al., 2014; Goodfellow et al., 2014; Goodfellow et al., 2014). However, GNNs remain vulnerable to adversarial attacks (Goodfellow et al., 2014; Goodfellow et al., 2014; Goodfellow et al., 2014; Goodfellow et al., 2014).

Adversarial attacks on GNNs exploit the model's inherent vulnerabilities, leading to either reduced accuracy or manipulated predictions (Goodfellow et al., 2014; Goodfellow et al., 2014; Goodfellow et al., 2014). Broadly classified into evasion and poisoning attacks, these methods undermine GNNs at different stages. Evasion attacks occur during the testing phase, where adversaries perturb the graph structure or node attributes of a trained model (Goodfellow et al., 2014; Goodfellow et al., 2014; Goodfellow et al., 2014). Poisoning attacks, on the other hand, target the training phase, where attackers tamper with training data to mislead the model (Goodfellow et al., 2014; Goodfellow et al., 2014; Goodfellow et al., 2014).

As a specific type of poisoning attacks, backdoor attacks have emerged as a more insidious threat (Goodfellow et al., 2014; Goodfellow et al., 2014). These attacks implant triggers that activate malicious behavior only under predefined conditions, making them exceptionally difficult to detect and defend against. This increasing threat elevates the urgency of addressing the growing security concerns surrounding GNNs.

### Backdoor Attacks and Defenses on GNNs

Backdoor attacks implant malicious triggers within training data, causing the model to function normally under standard conditions but to misbehave when encountering trigger-implanted samples. Early approaches, such as in (Goodfellow et al., 2014), introduced subgraph-based backdoors, which implant universal triggers into training samples. More recent advancements like GTA (Goodfellow et al., 2014) focus on generating adaptive triggers, tailoring them to individual samples to enhance attack effectiveness. To further improve stealthiness, methods like UGBA (Goodfellow et al., 2014) and DPGBA (Goodfellow et al., 2014) incorporate regularization terms in the loss functions, enabling these triggers to evade certain anomalous edge detection mechanisms. NFTA (Goodfellow et al., 2014), on the other hand, manipulates both node features and graph structure as triggers. However, its reliance on binary feature data limits its adaptability, especially in real-world scenarios where feature types tend to be more complex. Our proposed method is inherently different from these approaches in the following ways: (i) We aim to manipulate the attribute space to generate triggers that avoid producing anomalous edges; (ii) We adopt a novel candidate selection method to adapt the attack to real-world constraints.

In response to the growing threat of backdoor attacks, several defense mechanisms have been proposed. Among these, anomalous edge detection stands out as a promising approach. Prune (Prune, 2015) aims to mitigate the effect of triggers by identifying and removing edges that deviate from the homophily assumption. Another approach, OD (Goodfellow et al., 2014), leverages an autoencoder-based outlier detection method to identify out-of-distribution edges by filtering those with the

  
**Defense** & **SBA-Gen** & **GTA** & **UGBA** & **DPGBA** \\  None & 47.52 & 74.99 & 97.39 & 94.65 \\ OD & 12.91 & 0.00 & 10.29 & 92.40 \\ RIGBD & 0.00 & 53.56 & 0.00 & 0.00 \\ GNNGuard & 40.03 & 0.94 & 97.29 & 91.22 \\   

Table 1. Attack success rate (%) of backdoor attacks under different defenses on the dataset OGB-arxiv. The underlined results indicate unsuccessful attacks compared with performance without defense.

highest reconstruction errors. By analyzing prediction variance after removing anomalous edges, RIGBD (Shi et al., 2017) was developed to identify compromised target nodes and mitigate their impact on downstream tasks. Furthermore, robust GNN models such as GN-NGuard (Shi et al., 2017) and RobustGCN (Shi et al., 2018) have demonstrated resistance to backdoor attacks. Our proposed method is capable of bypassing such defenses as we avoid introducing anomalous edges through structure-preserving manipulation and constrain the magnitude of manipulation to prevent substantial semantic shifts.

## 3. Preliminaries

### Notions

Let \(=(,,)\) represent an attributed graph with \(N\) nodes, where \(\) is the node set, \(\) is the edge set, and \(=\{x_{1},,x_{N}\}\) is the original feature matrix of nodes, where \(x_{i}^{d}\) is the node feature of \(o_{i}\). The adjacency matrix of the graph \(\) is denoted by \(\{0,1\}^{N N}\), with \(_{ij}=1\) indicating an edge between nodes \(o_{i}\) and \(o_{j}\). The neighborhood of node \(o_{i}\), including the node itself, is denoted by \(_{l}\). In this paper, we focus on an inductive semi-supervised node classification task, where a small set of nodes \(_{l}\) in the training graph \(\) are provided with labels from \(=\{1,,C\}\), and the test graph \(_{T}=(_{T},_{T},_{T})\) is not available during the training stage. Let \(=\{y_{1},,y_{N}\}\) denote the ground-truth labels of nodes in the training graph, with \(_{l}\) and \(_{l}\) denoting the ground-truth labels of labeled and unlabeled nodes, respectively.

### Threat Model

In this paper, following prior studies (Shi et al., 2017; Shi et al., 2017; Shi et al., 2017), we focus on gray-box backdoor attacks on node classification tasks. In a gray-box scenario, attackers have access to the training data, including node attributes, graph structure, and label information, but lack knowledge of the specific architecture or parameters of the target model. The objective of the backdoor attack is to manipulate the downstream GNN classifier into producing malicious outputs on poisoned samples, while behaving normally on clean ones. To achieve this, attackers poison the training set by implanting triggers into a set of poisoned nodes, then labeling them with the target class. Ideally, this approach ensures that the backdoored GNN associates the trigger with the target label, causing any node containing the trigger to be misclassified as the target class. However, the effectiveness of typical graph backdoor attack lies on introducing malicious edges which can be detected then eliminated by defenses, making it significantly harder to achieve a high level of stealthiness for the attack.

### Problem Formulation

We consider a standard semi-supervised inductive node classification task in which the goal is to learn a mapping function \(f:\), where \(\) denote nodes in the training graph comprising of labelled nodes \(_{L}\) and unlabelled ones \(_{U}\), \(\) denotes the ground-truth labels, and \(f\) is a GNN classifier. Following the standard training scheme of graph backdoor attack, we first poison the training set by implanting triggers in target nodes before classifier is trained. We denote \(_{P}_{U}\) as the poisoned samples selected from unlabelled nodes in the training graph. For each node \(v_{i}^{P}_{P}\), we transform it into a poisoned sample by implanting trigger in attribute space and alter its ground-truth label as shown in Eq. (1):

\[x_{i}^{P}=(_{}(v_{i}^{P},x_{i}), y _{i}^{P}=y_{i}, \]

where \(x_{i}\) is the original feature, which is transformed into \(x_{i}^{P}\) after trigger implanting, \(_{}()\) denotes the trigger generator that takes \(o_{i}\) as input, and \(y_{i}\) is the target label assigned by the attacker. Taking the attack budget into account, the transformation also need to satisfy the following inequality:

\[_{k=1}^{d}(x_{i}[k] x_{i}^{P}[k])_{D}, \]

where \(_{D}\) is the budget for attribute manipulation, \(()\) is an indicator function, which equals 1 when \(x_{i}[k] x_{i}^{P}[k]\). We denote the modified features as \(\).

Given a GNN classifier \(f\) trained on the poisoned training set, ideally, its behavior is manipulated so that:

\[f(x_{j},_{j})=y_{j}, f(x_{j}^{P},_{j})=y_{i}, \]

for node \(v_{j}\) from an unseen test graph \(_{T}\) that is sampled from the same data distribution as the training graph. Following the setting of gray-box attack, the architecture and parameters of \(f\) are unknown to the attacker. Therefore, we adopt surrogate model \(f_{o}\) to simulate the downstream classifier. In the empirical risk minimization setting, the objective is to minimize the loss function in Eq. (4) on the training graph:

\[_{sur}(,)=_{n_{i}_{L}}l(f_{o}(x_{i}, _{i}),y_{i})+_{n_{i}_{P}}l(f_{o}(x_{i}^{P}, _{i}),y_{i}), \]

where \(\) denotes the learnable parameter of \(f_{o}\), and \(l()\) is the cross entropy loss. Our goal is to learn the selection of poisoned nodes \(_{P}\), the selection of feature dimensions \(\) for trigger implanting, and the trigger generator \(_{}\) by solving a bi-level optimization problem:

\[&_{_{P},_{}} _{n_{i}_{P}}l(f_{o}(x_{i}^{P},_{i}),y_{i}),\\ &(i)^{*}=_{i}_{sur}( ,),\\ &(ii)|_{P}|_{P},\ _{} (o_{i}), \]

where \(_{P}\) is the budget of poisoned nodes in training set, and \(\) denotes all triggers that meet the stealthiness requirement. Considering that jointly optimizing \(_{P}\), \(\), and \(_{}\) is computationally prohibitive, we break the optimization into two steps. First, we heuristically select poisoned nodes and trigger implanting dimensions as a preprocessing step to approximate the optimal \(_{P}\) and \(\). Then, we fix these selections to optimize \(_{}\).

## 4. Methodology

### Overall Architecture of SPEAR

The overall architecture of our framework revolves around three key components, namely poisoned node selection, feature selection, and trigger generator. Effective poisoned node selection is achieved by assessing both the class variety and prediction uncertainty among unlabelled nodes, focusing on those that meet criteria for both effectiveness and stealthiness. To ensure that the implanted triggers exert an impactful influence on the downstream classifier while maintaining generalizability to unseen data, we employ a global importance-driven feature selection strategy. This method identifies the most critical feature dimensions for trigger implantation by assessing their overall contribution to the model's prediction. Lastly, the trigger generator is designed to perform structure-preserving manipulations, utilizing semantic information from the neighborhood of poisoned nodes. The generator also incorporates a self-similarity normalization term into the loss function, promoting minimal perturbations to enhance stealthiness. The overall framework is illustrated in Fig. 1.

### Effective Poisoned Nodes Selection

Given the extensive and diverse nature of graph data, the selection of poisoned nodes is critical for ensuring efficient use of the attack budget. Accordingly, the selection of poisoned nodes should adhere to two principles: (0) the chosen nodes should effectively deceive the downstream classifier into associating the trigger with the poisoned label; and (ii) manipulations of these nodes should not have adverse effects on the classifier's performance on clean data.

Therefore, we propose selecting samples with high classification uncertainty within each category as poisoned nodes, which provides several notable advantages. First, sampling from different categories provides class variety which is crucial for successfully implanting targeted backdoor triggers across different class distributions. Second, this approach helps to avoid the creation of strong outliers, thereby mitigating the negative impact on the classifier's generalization, ensuring that the clean accuracy is intact. Besides, robust samples with high classification confidence demonstrate a strong semantic correlation between their attributes and true labels, leading to antagonistic effects between the original attributes and implanted triggers (Bishop, 2006; Lee et al., 2016), making it more complex for the classifier to learn the mapping between triggers and the target label.

Recall that poisoned nodes are selected from the unlabeled training set \(_{U}\), in order to obtain the uncertainty and pseudo labels, we train a GNN classifier \(_{p}\) on clean data based on Eq. (\(\)):

\[_{pre}=_{n_{i}_{L}}l(_{p}(x_{i},_{i}),y_{i}). \]

For each node \(v_{i}\) in the training set, its classification uncertainty and pseudo label are calculated as follows:

\[H(v_{i})=-_{c}P_{v_{i}}(c|_{p}) P_{v_{i}}(c|_{p}),(v_{i})=_{p}(x_{i},N_{i}), \]

where \(\) denotes label set, and \(P_{v_{i}}(c|_{p})\) indicates the confidence of \(_{p}\) that node \(v_{i}\) belongs to class \(c\). For nodes assigned with pseudo-label \(c\), we rank them in descending order by their classification uncertainty, as defined in Eq. (7), and denote the resulting sequence as \(T_{c}\). Given the size of poisoned nodes \(_{P}\), the final selected node set \(_{P}\) can be written as:

\[_{P}=_{c}T_{c}[:}{C}]. \]

### Global Importance-Driven Feature Selection

As mentioned in Sec. 1, performing all-dimension manipulation is impractical in real-world scenarios, so we set a budget \(_{D}\) to restrict the number of poisoned features, and introduce \(\) to denote sensitive features that are not allowed to be modified. Considering that the dimension of node features is typically large for graph data, as shown in Table 2, randomly selecting poisoned features exhibits

Figure 1. Overall framework of SPEAR. We use ‘+’ and ‘-’ to denote the annotated labels, with shades of orange and green indicating the corresponding ground-truth labels. During the training phase (blue background), SPEAR selects target nodes \(_{P}\), identifies high global-importance features \(\), and optimizes the trigger generator \(_{t}\) to produce the poisoned graph \(_{P}\). The downstream classifier is trained on \(_{P}\) and is embedded with backdoor consequently. In the inference phase (green background), SPEAR implants triggers into the test graph to manipulate the classifier’s predictions.

[MISSING_PAGE_FAIL:5]

## 5. Experiments

In this section, we empirically analyze the effectiveness and stealthiness of SPEAR on various datasets. Specifically, we aim to answer the following research questions:

* **RQ1:** Does SPEAR outperform the state-of-the-art backdoor models under various defenses?
* **RQ2:** What is the impact of different attack budgets on SPEAR's performance?
* **RQ3:** How do the key components contribute to the attack performance?
* **RQ4:** How does SPEAR balance training time and performance?

### Experimental Settings

#### 5.1.1. Datasets

To evaluate the effectiveness of our proposed method, we conduct experiments on three widely used datasets, i.e., Cora, Pubmed (Zhu et al., 2018), and OGB-arxiv (Zhu et al., 2018), which correspond to small, medium, and large graphs, respectively. The detailed statistics of these datasets are presented in Table 2.

#### 5.1.2. Baselines

We compare SPEAR against five representative and state-of-the-art graph backdoor attack methods, namely **SBA-Samp**, **SBA-Gen**(Zhu et al., 2018), **GTA**(Zhu et al., 2018), **UGBA**(Zhu et al., 2018), and **DPGBA**(Zhu et al., 2018). To assess the stealthiness of SPEAR we implement three graph backdoor defenses: **Prune**(Zhu et al., 2018), **OD**(Zhu et al., 2018), and **RGBD**(Zhu et al., 2018). Additionally, to validate its stealthiness and transferability, we conduct tests across various models on OGB-arxiv, including prominent GNN architectures such as **GCN**(Zhu et al., 2018), **GraphSAGE**(Hamilton et al., 2017), and **GAT**(Zhu et al., 2018), as well as robust GNNs like **GNNGuard**(Zhu et al., 2018) and **RobustGCN**(Zhu et al., 2018). Comprehensive details regarding these methods can be found in Appendix B.

#### 5.1.3. Evaluation

Following (Zhu et al., 2018; Zhu et al., 2018), we perform experiments on the inductive node classification task, where the test graph is unseen by both the attacker and the victim model before inference. To evaluate effectiveness and evasiveness, we use two metrics: (i) attack success rate (ASR), which measures the likelihood that the backdoored GNN classifies trigger-implanted nodes into the target class \(y_{t}\); and (ii) clean accuracy (CA), which measures the classification accuracy of the backdoored GNN on clean nodes. We randomly mask out 20% of the nodes in each dataset, with half designated as target nodes for assessing attack performance and the other half as clean test nodes to evaluate performance on clean data. The remaining 80% of nodes form the training graph, where 10% of the nodes are used for labeled training and another 10% for validation.

#### 5.1.4. Implementation

For SPEAR, the rate of poisoned nodes in training data is set to less than 1% for each dataset, and the feature budget \(_{D}=(0.02d,5)\), where \(d\) is the dimension of node attribute. For baseline attack methods, we adopt the same rate of poisoned nodes, and set the trigger size to 3 with all-dimensional manipulable generated nodes. A two-layer MLP is deployed as trigger generator. As for the surrogate model, a two-layer GCN is used for all attack methods. Each experiment is run five times per architecture, and the average ASR and CA are reported. More details can be found in Appendix A, and the implementation of SPEAR is available at [https://anonymous.4open.science/r/SPEAR-48EC](https://anonymous.4open.science/r/SPEAR-48EC).

### Attack Performance

To answer RQ1, we evaluate SPEAR against baseline backdoor attacks across three datasets with or without defenses. To further assess the transferability and stealthiness, we vary the test models from prominent GNN architectures to robust GNNs.

#### 5.2.1. Comparison with Baseline Attacks

Table 3 shows the performance on three datasets against baseline attacks. The top two performances are highlighted in bold and underline. From this experiment, we have the following observations:

* All baseline attacks fail to bypass at least one defense across all datasets, revealing their vulnerabilities. Specifically, SBA-Samp and its variant consistently exhibit lower ASRs, while GTA fails under Prune and OD defenses in most cases. UGBA and DPGBA benefit from specific countermeasures, showing resistance to Prune and OD, respectively, but remain ineffective against other defenses. In contrast, SPEAR shows significantly smaller ASR drops across all defenses, highlighting its superior stealthiness.
* When no defense is applied, SPEAR consistently achieves comparable or better ASR compared with baselines across all datasets, demonstrating its effectiveness. This consistent performance highlights SPEAR's ability to implant effective and impactful triggers without producing anomalous edges.
* SPEAR maintains CA levels similar to the baselines, and often higher than those on the clean graph, both with and without defense methods. This shows that the SPEAR-backed downstream

  
**Datasets** & **Nodes** & **Edges** & **Features** & **Classes** \\  Cora & 2,708 & 5,429 & 1,443 & 7 \\ Pubmed & 19,717 & 44,338 & 500 & 3 \\ OGB-arxiv & 169,343 & 1,166,243 & 128 & 40 \\   

Table 2. Dataset statistics classifier preserves its classification ability in clean samples, further highlighting the stealthiness of SPEAR.
* We note that SPEAR's ASR on OGB-arxiv against OD was slightly lower than that of DPGBA by about 4%. This discrepancy can be attributed to DPGBA's distribution-preserving module, which is tailored for out-of-distribution detectors, enhancing its ability to bypass OD. However, this advantage is less effective against other defense mechanisms. In contrast, SPEAR focuses on overall stealthiness, enabling it to demonstrate resilience against a broader range of defenses.

#### 5.2.2. Performance with Different Test Models

Table 4 shows the performance of SPEAR and baseline models with different test models on OGB-arxiv, including three mainstream GNN models (GCN, GAT and GraphSAGE) and two robust GNN models (GNNGuard and RobustGCN), to demonstrate its transferability and stealthiness. From the table, we make the following observations:

* For the two robust GNN models, GNNGuard calculates edge pruning probabilities through a non-linear transformation, while RobustGCN uses Gaussian distributions to represent nodes and employs a variance-based attention mechanism. Results demonstrate that SPEAR is resistant to both GNNGuard and RobustGCN, showcasing its ability to bypass robust GNNs that focus on either graph structure or node attributes.
* SPEAR maintains leading ASR across all test models compared to baseline attacks. Given that we fix the surrogate model to be a 2-layer GCN while varying the test models, such performance demonstrates SPEAR's ability to generate triggers that effectively adapt to different GNN architectures.

### Impact of Attack Budget

To answer RQ2, we conduct experiments to evaluate the sensitivity of SPEAR under different attack budgets. Specifically, for \(_{P}\), we vary the ratio of poisoned nodes in the training graph across \(\{0.05,0.1,0.2,0.5,1,2,5\}\%\), and for \(\), we adjust the ratio of manipulated features to \(\{0.2,0.5,1,2,5,10,15\}\%\), with at least one feature manipulated. Fig. 2 shows the results on Pubmed and OGB-arxiv. We only report attack success rate, as no notable changes in clean accuracy were detected across all baselines and SPEAR. From Fig. 2 we can observe that:

* As the ratio of poisoned nodes grows, SPEAR's ASR shows a consistent increase on both datasets, highlighting that it can effectively leverage a larger attack budget to achieve better results. Meanwhile, SPEAR maintains an ASR above 89% even with a poisoned node ratio as low as 0.05%, showcasing its efficient use of the attack budget.
* When varying the ratio of features budget \(\), similar trends are observed. The ASR increases steadily as the proportion of

  
**Test Models** & **Clean** & **SBA-Samp** & **SBA-Gen** & **GTA** & **UGBA** & **DPGBA** & **SPEAR** & & & & & & & \\  GCN & 65.60 & 17.70 & 65.19 & 47.52 & 65.03 & 74.99 & 63.22 & **97.39** & **65.65** & 94.65 & 64.56 & 96.95 & 66.91 & 780 \\ GAT & 66.25 & 49.75 & 65.06 & 94.86 & 65.01 & 1.72 & 63.00 & 96.53 & 65.08 & 95.88 & 64.79 & **96.86** & **64.56** & 782 \\ GraphSAGE & 65.86 & 21.64 & 65.47 & 40.49 & 65.44 & 96.67 & 65.20 & 96.61 & 65.20 & 78.30 & 65.33 & **97.93** & **65.50** & 782 \\ GNNGuard & 66.03 & 22.59 & 64.66 & 39.76 & 64.60 & 0.94 & 65.14 & 97.29 & 65.64 & 91.22 & 63.94 & **97.64** & **65.65** & 783 \\ RobustGCN & 61.36 & 56.50 & 62.03 & 55.03 & 64.02 & 86.86 & 61.01 & 94.93 & 61.19 & 89.90 & 61.08 & **95.99** & **61.37** & 784 \\   

Table 4. Backdoor attack results (ASR (%) | CA (%)) on different test models using the OGB-arxiv dataset. The top two performances in terms of ASR are highlighted in bold and underline.

  
**Datasets** & **Defense** & **Clean** & **SBA-Samp** & **SBA-Gen** & **GTA** & **UGBA** & **DPGBA** & **SPEAR** & & & & & & \\   & None & 83.49 & 29.52 & 82.49 & 44.65 & 82.96 & 91.79 & 84.07 & 96.05 & 83.70 & 95.67 & 82.96 & **98.85** & **83.96** \\  & Prune & 81.48 & 16.70 & 82.98 & 19.56 & 83.19 & 0.06 & 84.04 & 97.41 & 82.82 & 16.97 & 78.52 & **97perturbed features grows, indicating that SPEAR benefits from manipulating more features. Even with a feature budget of just 1%, meaning that only 1 out of 128 features is manipulable in OGB-arxiv, the attack remains highly effective. This confirms that SPEAR can still be applied in real-world environments where feature sensitivity is a critical concern.

### Ablation Study

To answer RQ3, we conduct an ablation study to examine the contributions of the key components in SPEAR: the node selection module, the feature selection module, and the neighborhood-aware mechanism in trigger generation. To evaluate the effectiveness of node selection, we design a variant of SPEAR named SPEAR-RN, which randomly selects poisoned nodes in the training graph. To assess the contribution of the global importance-driven feature selection strategy, we replace the original feature selection module with random selection, naming it SPEAR-RF. We also introduce a variant called SPEAR-NE, where an MLP is used to substitute the aggregation function in Eq. (13) to evaluate the role of the neighborhood-aware mechanism in trigger generation. To guarantee fair comparisons, hyperparameters for each variant are tuned according to the performance on validation set. Fig. 3 illustrates the results, showing that:

* We observe a notable drop in ASR for SPEAR-NE compared to SPEAR, with reductions of 22.47% on Cora, 4.48% on Pubmed, and 1.14% on OGB-arxiv. This result highlights the crucial role of the neighborhood-aware mechanism in SPEAR's effectiveness. We infer that the larger benefit observed on Cora arises from its simple local structure and high homophily, which allow the aggregation function to extract more relevant information and thus enhancing trigger generation.
* SPEAR achieves higher ASR than its variants, SPEAR-RN and SPEAR-RF, affirming the contributions of both the node selection and feature selection modules. We notice that the advantage of the global importance-driven feature selection method becomes more pronounced as the feature size increases. This suggests that the method is highly effective at identifying influential features from large candidate sets, a critical capability for datasets with a large number of features.

### Trade-off between Time and Performance

To answer RQ4, we conduct experiments examining the trade-off between the training time of the trigger generator and the average ASR on Pubmed and OGB-arxiv. We compute the average ASR both with and without defenses, following the setup in Sec. 5.2.1. For the learnable trigger generators in SPEAR, DPGBA, UGBA, and GTA, the same number of training epochs is used. On OGB-arxiv, we adopt the simplified version of DPGBA, which is designed to handle large-scale graphs. The experiments are performed on an NVIDIA GeForce RTX 4090 with 24GB of memory, and the results are shown in Fig. 4. Compared to other attacks with similar or shorter training time, SPEAR significantly outperforms SBA-Gen and GTA in terms of average ASR. In contrast, DPGBA and UGBA require considerably more time for training the trigger generator, e.g., DPGBA takes 391.98 seconds on Pubmed, making them \(50.51\) and \(4.84\) slower than SPEAR, respectively. Overall, SPEAR strikes a superior balance between attack success rate and training efficiency, making it a more practical choice for real-world applications.

## 6. Conclusion

In this work, we propose SPEAR, a novel structure-preserving backdoor attack designed to exploit the vulnerabilities of GNNs while minimizing the risk to be detected. Unlike traditional backdoor attacks that rely on structural perturbations, SPEAR preserves the graph's topology, focusing instead on perturbing node attributes. By leveraging novel selection methods and neighborhood-aware trigger generation, SPEAR effectively balances attack success rate and stealthiness, making it particularly suited for sensitive graph-based applications. Through extensive experiments on real-world datasets, we demonstrated that SPEAR consistently outperforms existing methods in both effectiveness and stealthiness, even under rigorous defense mechanisms. Our work highlights the significant threat posed by stealthy, structure-preserving backdoor attacks, particularly as GNNs are increasingly deployed in security-sensitive domains. Future research may explore further optimizations in trigger generation and investigate defenses specifically tailored to detect such attribute-focused backdoor attacks.

Figure 4. Training time of trigger generator vs. performance

Figure 3. Comparisons between SPEAR and its variants.

* Allen-Zhu and Li (2022) Zeyuan Allen-Zhu and Yuamshi Li. 2022. Feature partitioning: How adversarial training performs robust deep learning. In _2022 IEEE 40th Annual Symposium on Foundations of Computer Science (FOCS)_. IEEE, 977-988.
* Battaglia et al. (2018) Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Langhold, Mateusz Malinowski, Andrea Zacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. 2018. Relational inductive biases, deep learning, and graph networks. _arXiv preprint arXiv:1806.01201_ (2018).
* Rongini et al. (2021) Pietro Rongini, Monica Ranchini, and Franco Scoerselli. 2021. Molecular generative graph neural networks for drug discovery. _Neurocomputing_ 450 (2021), 242-252.
* Yinpan et al. (2017) Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. 2017. Targeted backdoor attacks on deep learning systems using data poisoning. _arXiv preprint arXiv:1712.05526_ (2017).
* Chen et al. (2023) Yang Chen, Zhonglin Yu, Haiqing Zhao, and Ying Wang. 2023. Feature-based Graph Backdoor Attack in the Node Classification Task. _International Journal of Intelligent Systems_ 2023, 1 (2023), 541-8398.
* Covert et al. (2020) Ian Covert, Scott M Lindberg, and Jeff So In Lee. 2020. Understanding global feature contributions with additive importance measures. _Advances in Neural Information Processing Systems_ 32 (2020), 1721-17225.
* Dai et al. (2023) Daigu Dai, Minhua Lin, Xiang Zhang, and Suhang Wang. 2023. Unnoticeable backdoor attacks on graph neural networks. In _Proceedings of the ACM Web Conference 2023 (2023)_. 2265-2273.
* Dai et al. (2014) Hanjun Dai, Hui Li, Tian Tan, Xin Huang, Lin Wang, Jun Zhu, and Le Song. 2014. Adversarial attack on graph structured data. In _International conference on machine learning_. PMLR, 1119-1124.
* Fan et al. (2019) Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. 2019. Graph neural networks for social collaboration. In _The world wide web conference_. 417-426.
* Fan et al. (2020) Wenqi Fan, Yao Ma, Qing Li, Jianping Wang, Guoyang Cai, Jiliang Tang, and Dawei Yin. 2020. A graph neural network framework for social recommendations. _IEEE Transactions on Knowledge and Data Engineering_ 34, 5 (2020), 2033-2047.
* Gao et al. (2013) Yinghua Gao, Yiming Li, Linghui Zhu, Dongfan Wu, Yong Jiang, and Shu-Tao Xia. 2013. Not all samples are born equal: Towards effective clean-label backdoor attacks. _Pattern Recognition_ 139 (2013), 19512.
* Guo et al. (2019) Tianyu Guo, Kang Liu, Brendan Dani-Gavatt, and Salidharth Garg. 2019. Badnets: Evaluating backdooring attacks on deep neural networks. _IEEE Access_ 7 (2019), 4292-42744.
* Hamilton et al. (2017) Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. _Advances in neural information processing systems_ 30 (2017).
* Martinis et al. (2002) Weihua M. Martinis Key, Marina Zitnik, Yuxiong Deng, Hongyu Ren, Bowen Liu, Michela Catalya, and Jure Leskovec. 2002. Open graph benchmark: Datasets for machine learning on graphs. _Advances in neural information processing systems_ 33 (2020), 22118-22133.
* Huang et al. (2017) Mengda Huang, Yang Liu, Xiang Ao, Kuan Li, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing Li. 2017. An oriented graph neural network for fraud detection. In _Proceedings of the ACM web conference_. 2021 1131-1221.
* Kipf and Welling (2017) Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In _30th International Conference on Learning Representations_. 2017. 76017. Toulon, France, April 24-26, 2017. _Conference Track Proceedings_.
* Lee et al. (2018) John Boa Lee, Ryan Rossi, and Xiangnan Kong. 2018. Graph classification using structural attention. In _Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_. 1666-1674.
* Li et al. (2016) Yuqi Li, Daniel Tarlow, Marc Hochschmidt, and Richard S. Zemel. 2016. Gated Graph Sequence Neural Networks. _In the International Conference on Learning Representations_. 2016. 2016. May 2-4, 2016. _Conference Track Proceedings_.
* Lin et al. (2023) Xixun Lin, Chuan Zhou, Jia Wu, Hong Yang, Haho Wang, Yann Cao, and Bin Wang. 2023. Exploratory adversarial attacks on graph neural networks for semi-supervised node classification. _Pattern Recognition_ 133 (2023), 109042.
* Liu et al. (2012) Zihan Liu, Yun Luo, Lihong Wu, Zicheng Liu, and Shan Z. Li. 2012. Towards reasonable budget allocation in targeted graph structure attacks via gradient debias (ANDS 22) Red Hook, NY, USA. Article 2028, 12 pages.
* Yang et al. (2020) Fanchao Q. Yang, Qi Yangci Chen, Nikkai Li, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2020. Onnet: A simple and effective defense against textual backdoor attacks. _arXiv preprint arXiv:2011.10699_ (2020).
* Qi et al. (2021) Yanchao Qi, Muskai Li, Yunqi Chen, Zhenguang Zhang, Zhiyuan Liu, Yusheng Wang, and Maosong Sun. 2021. Hidden labelmix: Intrinsic textual backdoor attacks with syntactic trigger. _arXiv preprint arXiv:2105.12400_ (2021).
* Sen et al. (2002) Prithviraj Sen, Galileo Namata, Mustafa Rigli, Lise Getoor, Brian Galligher, and Tim Eliasu-Rad. 2008. Collective classification in network data. _AI magazine_ 29, 3 (2002), 93-93.
* Shuman et al. (2013) David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. 2013. The emerging field of signal processing on graphs. Extending high-dimensional data analysis to networks and other irregular domains. _IEEE signal processing magazine_ 30, 3 (2013), 83-98.
* Sun et al. (2022) Lichao Sun, Yinglong Dou, Carl Yang, Kai Zhang, J Wang, S Yu Philip, Lifang He, and Bo Li. 2022. Adversarial attack and defense on graph data: A survey. _IEEE Transactions on Knowledge and Data Engineering_ 35, 8 (2022), 793-711.
* Tao et al. (2021) Shuchang Tao, Qi Cao, Huawei Shen, Junjie Huang, Yunfan Wu, and Xueqi Cheng. 2021. Single node injection attack against graph neural networks. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_. 1794-1803.
* Velickovic et al. (2017) Peta Velickovic, Guiflem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, et al. 2017. Graph attention networks. _stat_ 1050, 20 (2017), 10-4850.
* Wang et al. (2021) Binghui Wang, Minhua Lin, Tianxiang Zhou, Pan Zhou, Ang Li, Meng Pang, Hai Li, and Yixin Chen. 2021. Efficient, direct, and restricted black-box graph evaluation tasks to analyze graph neural networks via influence function. In _Proceedings of the 17th ACM International Conference on Web Search and Data Mining_. 693-701.
* Wang and Zhang (2022) Xiyuan Wang and Muhan Zhang. 2022. How Powerful are Spectral Graph Neural Networks. In _Proceedings of the 39th International Conference on Machine Learning_. Vol. 162, 2341-23262.
* Wu et al. (2019) Felix Wu, Amami Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. 2019. Simplify graph convolutional networks. In _International conference on machine learning_. PMLR, 6861-6871.
* Zhou et al. (2020) Zonghui Wu, Shirui Pan, Pengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. 2020. A comprehensive survey on graph neural networks. _IEEE transactions on neural networks and learning systems_ 32, 1 (2020), 4-24.
* Xu et al. (2021) Zhaohan Xu, Ren Pang, Shouling Ji, and Ting Wang. 2021. Graph backdoor. In _30th USENIX Security Symposium (USENIX Security 21)_. 1523-1540.
* Zhang et al. (2021) He Zhang, Bing Wu, Xiangyu Yang, Chun Zhu, Shou Wang, Xingxiang Yuan, and Silvirajani. 2021. Projective ranking: A framework that enables method on graph neural networks. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_. 3407-3621.
* Zhang and Chen (2018) Mahan Zhang and Yixin Chen. 2018. Link Prediction Based on Graph Neural Networks. In _Advances in Neural Information Processing Systems_, Vol. 31.
* Zhang et al. (2018) Maun Zhang, Zhicheng Cui, Maun Neumann, and Yixin Chen. 2018. An end-to-end deep learning architecture for graph classification. In _Proceedings of the AAAI conference on artificial intelligence_, Vol. 32.
* Zhang and Zitnik (2020) Xiang Zhang and Martinis Zitnik. 2020. Gnuguraf: Defending graph neural networks against adversarial attacks. _Advances in neural information processing systems_ 33 (2020), 9293-9275.
* Zhang et al. (2021) Zasii Zhang, Jiayuan Jin, Binghui Wang, and Neil Zhenqiang Gong. 2021. Backdoor attacks to graph neural networks. In _Proceedings of the 30th ACM symposium on Access Control Models and Technologies_. 15-26.
* Zhang et al. (2024) Zhiwei Zhang, Minhua Lin, Enyan Dai, and Suhang Wang. 2024. Rethinking graph backdoor attacks: A distribution-preserving perspective. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_. 4386-4397.
* Zhang et al. (2024) Zhiwei Zhang, Minhua Lin, Junjie Xu, Zongyu Wu, Enyan Dai, and Suhang Wang. 2024. Robustness-Inspired Defense Against Backdoor Attacks on Graph Neural Networks. _arXiv preprint arXiv:2402.00868_ (2024).
* Zhou et al. (2021) He Zhou, Garcin Cui, Shengling Hu, Zheng Zhang, Jingyuan Liu, Lifeng Wang, Changxiang Li, and Xiaoduan Sun. 2021. Graph neural networks: A review of methods and applications. _AI open_ (2020), 57-81.
* Zhu et al. (2019) Dingyuan Zhu, Ziwei Zhang, Peng Cui, and Wenwu Zhu. 2019. Robust graph convolutional networks against adversarial attacks. In _Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining_. 1399-1407.
* Zhu et al. (2021) Xiaoqian Zhu, Xiang Ao, Zidi Qin, Yanpeng Chang, Yang Liu, Qing He, and Jianping Li. 2021. Intelligent financial fraud detection practices in post-pandemic era. _The Innovation_ 2, 4 (2021).
* Zogner et al. (2018) Daniel Zogner, Amir Abbarapedi, and Stephan Gunnemann. 2018. Adversarial attacks on neural networks for graph data. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_. 2867-2856.
* Zogner et al. (2020) Daniel Zogner, Oliver Borchert, Anir Abbarapedi, and Stephan Gunnemann. 2020. Adversarial attacks on graph neural networks: Perturbations and their patterns. _ACM Transactions on Knowledge Discovery from Data (TKDD)_ 14, 5 (2020), 1-31.

## Appendix A Detailed Implementation

### Hyperparameters

All hyperparameters are tuned based on the loss and accuracy of the validation set. For SPEAR, the parameter \(\), which controls the contribution of the self-similarity loss, is tuned from {0.1, 1, 3, 5,..., 15}, and \(E\), the number of repeated iterations for inter-optimization, is tuned from {1, 3, 5, 7, 9, 11}. A 2-layer GCN is deployed as \(_{p}\), and a 2-layer MLP is used as the trigger generator, both with hidden dimensions set to 32. Another 2-layer GCN is deployed as the surrogate model, which also acts as the aggregator providing input for the trigger generator. Its hidden dimension \(N_{h}\), which determines the level of information compression in the input to the generator, is tuned from {16, 32, 64, 80, 128}. The learning rate \(\) for the surrogate and trigger generator is tuned from {0.0001, 0.001, 0.005, 0.01}. The ratio of poisoned nodes \(_{P}\) in training set is tuned from {0.5, 1}% for experiments in Sec. 5.2.1, with all baseline attack methods using the same poisoning rate as SPEAR to guarantee a fair comparison. The specific choice of hyperparameters is listed in Table 5 and 6 as follows:

In Table 5, the values separated by \(^{}\) correspond to the parameter settings for Cora, Pubmed, and OGB-arxiv, respectively.

### Computing infrastructures

We implement the proposed methods with Numpy 1.26.0, PyTorch 2.2.1 and PyTorch Geometric 2.5.1. We conduct the experiments on a Linux server with an Intel Xeon E5-2680 v4 CPU and a NVIDIA GeForce RTX 4090 GPU with 24GB memory.

## Appendix B Baselines Information

In this section, we provide a brief overview of the baseline methods used in our experiments.

* **SBA-Samp & SBA-Gen**: SBA-Samp injects fixed subgraph triggers, while SBA-Gen enhances this by generating adaptive triggers. These methods face challenges in high ASR and are susceptible to defense mechanisms such as pruning.
* **GTA**: GTA introduces a trigger generator that tailors triggers to specific samples, improving attack effectiveness but vulnerable to several defense mechanisms.
* **UGBA**: UGBA selects poisoned nodes based on representativeness and generates triggers that obey the homophily assumption, maintaining high stealth under a limited budget.
* **DPGBA**: DPGBA improves by ensuring that triggers remain within distribution through adversarial learning, further reducing detectability.
* **Prune**: Prune removes dissimilar edges to disrupt the effectiveness of backdoor triggers by filtering out suspicious connections.
* **OD**: OD leverages autoencoders for outlier detection, identifying and removing out-of-distribution edges to mitigate backdoor attacks.
* **RIGBD**: RIGBD detects poisoned nodes by leveraging random edge dropping and the prediction variance, offering strong defense while maintaining high clean accuracy.
* **GCN**: A standard GNN widely used for node classification, serving as a benchmark for graph-based tasks.
* **GAT**: GAT assigns different attention weights to neighboring nodes, enabling more expressive and flexibility without requiring prior knowledge of the global graph structure.
* **GraphSAGE**: GraphSAGE generates inductive node embeddings by sampling and aggregating information from local node neighborhoods.
* **RobustGCN**: RobustGCN models nodes with Gaussian distributions and employs a variance-based attention mechanism to reduce the spread of adversarial attacks through the network.
* **GNNGuard**: GNNGuard dynamically reweights edges based on cosine similarity, pruning adversarial connections and improving the model's robustness against attacks.

  
**Dataset** & \(N_{h}\) & \(\) \\  Cora & 80 & 0.001 \\ Pubmed & 64 & 0.01 \\ OGB-arxiv & 32 & 0.01 \\   

Table 6. Hyperparameters settings for different datasets.

  
**Model** & \(\) & \(\) & \(_{P}\) \\  GCN & 0.1 & 1 & 0.5 \\ GAT & 0.1 & 1 & 0.5 \\ GraphSAGE & 0.1 & 1 & 0.5 \\ GNNGuard & 1 & 2 & 1 \\ RobustGCN & 5 & 2 & 1 \\ Prune & 1/1/5 & 2 & 0.5 \\ OD & 5/1/15 & 3 & 1 \\ RIGBA & 1/1/5 & 3 & 1 \\   

Table 5. Hyperparameter settings for different models.