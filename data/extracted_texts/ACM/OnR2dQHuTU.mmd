# ODNS Clustering: Unveiling Client-side Dependency in Open DNS Infrastructure

Anonymous Author(s)

###### Abstract.

There are over a million open DNS servers in the wild. However, not all servers perform recursive queries directly. Instead, many DNS forward queries to upstream recursive servers or other DNS forwarders for name resolving on their behalf. The groups of open servers that have such dependencies on each other form **ODNS Clusters**. The dependencies can result in vulnerabilities; yet we have little knowledge of the ODNS cluster structure. In this work, we measure the inter-dependence of open DNS resolvers and find that 1.9 million open DNS servers form _only_ 81,636 ODNS clusters. We further analyze the characteristics of the clustered ODNS structure. The key observations include biased cluster size distribution, discrepancy of ODNS infrastructures among countries, concentration in major public DNS server providers, and potential security and resilience risks due to the dependence.

## 1. Introduction

The Domain Name System (DNS) serves as a foundational infrastructure for the web (Hernandez et al., 2017; Wang et al., 2017; Wang et al., 2017), facilitating the translation of human-readable domain names into machine-readable IP addresses. Open DNS infrastructure (ODNS) provides a free entrance for billions of web users to access DNS services. There are more than one million IP addresses hosting open DNS servers in the wild (Shen et al., 2017; Wang et al., 2017; Wang et al., 2017). However, not all of these open DNS servers issue queries to authoritative name servers. Specifically, DNS forwarders (Wang et al., 2017) do not resolve domain names by themselves. Instead, they forward queries from clients to another upstream server, such as a public DNS server or a dedicated gateway. On the client side, the multi-layer forwarding dependency between forwarder and upstream servers results in confusing hierarchies or unexpected dependencies. For instance, multi-level forwarding chains or loops may pose potential security risks (Wang et al., 2017).

Forwarding dependencies in ODNS infrastructure means that a large number of forwarders _may_ in fact rely on the responses of a small set of upstream servers (Shen et al., 2017). We refer to the collection of upstream servers and forwarders that have direct (or indirect) dependencies as an **ODNS cluster**. As a result, open DNS servers form multiple ODNS clusters with dependencies and naturally reveal the status of DNS infrastructure. Yet we have little knowledge of the ODNS cluster structure, such unclear clustered dependencies can amplify vulnerabilities and the impact of malicious attacks (Wang et al., 2017; Wang et al., 2017). It therefore becomes difficult to identify critical points of failure (Bauer et al., 2017; Wang et al., 2017), as seemingly distinct servers may actually be in the same ODNS cluster. Alternative DNS servers set by users without prior knowledge may be universally affected by the same upstream server and lead to a failure of the redundancy mechanism.

Consequently, we argue that understanding the client-side dependencies of open DNS servers is vital for improving DNS configurations and facilitating a better understanding of DNS infrastructure. In this work, we therefore measure the dependencies of open DNS servers and divide open DNS servers into multiple ODNS clusters. To this end, we propose an ODNS clustering method that leverages the footprints left in caches during the forwarding process to identify ODNS clusters. We further propose cluster aggregation to improve clustering accuracy for large public DNS servers that have complex infrastructure (Wang et al., 2017; Wang et al., 2017). Besides, we propose a forwarder classification method based on forwarding behaviors inside clusters, revealing the composition of large ODNS clusters.

Overall, we identify 81,636 distinct ODNS clusters for 1.9 million open DNS servers in the wild. We further analyze the characteristics of the cluster and make the following key observations1:

* A significant portion (95%) of open DNS servers rely on other servers for name resolution. Moreover, we see a heavily biased distribution of cluster size, where 0.25% top clusters cover 44.1% of open DNS servers.
* Cluster size distribution varies significantly across countries. Some countries are dominated by large clusters, indicating higher dependency (_e.g._, China), while others exhibit a more balanced distribution (_e.g._, US and FR). This implies differences in DNS infrastructure across countries.
* ODNS clusters that are led by major public DNS servers cover 47% of the open DNS servers. The use of anycast results in many clusters for one public DNS with unbalanced cluster size distribution. This serves as another evidence of infrastructure concentration (Shen et al., 2017; Wang et al., 2017).
* Over 9% of the ODNS servers exhibit a range of misconfigurations or malicious behavior as they direct web requests to potentially harmful destinations. The dependence captured by clusters amplifies the impact of such behavior.
* ODNS cluster consists of forwarders with diverse behaviors. Notably, about 61.7% forwarders are non-caching proxies. These proxies may be leveraged by attackers to attack the upstream resolvers within clusters.

Figure 1. DNS infrastructure and client-side structure.

## 2. Background and Related Work

### Client-side DNS Structure

The client-side DNS infrastructure consists of multiple layers of servers. Previous works have investigated the hierarchical structure of DNS (Zhao et al., 2016; Wang et al., 2017; Wang et al., 2018; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019). The key components of the DNS infrastructure are illustrated in Fig. 1. We refer to servers that accept requests directly from _any_ client as the _open DNS servers_ (**ODNS**), while servers that cannot be directly accessed by clients are _hidden DNS servers_ (**HDNS**). Functionally, servers within the ODNS can be categorized into _forwards_ and _resolvers_. Forwarders forward the original DNS query either to an upstream forwarder or a designated egress resolver, while the egress resolver ultimately communicates with the _authoritative DNS servers_ (**ADNS**). The forwarding behavior in the client-side causes the dependency between open DNS servers, including 1 the dependency between the forwarder and resolver, and 2 the dependency between different forwarders.

Prior studies have focused on identifying forwarders and resolvers by correlating initial queries with logs from ADNS. Luo _et al._(Lu et al., 2017) and Xu _et al._(Xu et al., 2018) focus on matching forwarders and resolvers by encoding details of the forwarder in the request domain name in order to identify the corresponding resolver for the requested forwarder. Nawrocki _et al._(Nawrocki et al., 2018) and Censys (Censys et al., 2018, 2019) establish the linkage between forwarders and resolvers by encoding resolver IP within ADNS responses. While these approaches can discover the dependency between the upstream backend resolver and open forwarders, they fail to illustrate the complete client-side structure of DNS infrastructure. Firstly, ADNS can only record DNS queries that do not hit the client-side cache (Wang et al., 2019), meaning it cannot unveil the hidden dependencies on the client side (_e.g._, the dependencies between forwarders). Secondly, existing works primarily measure and analyze well-known public DNS servers, relying on prior knowledge of backend server addresses (Xu et al., 2018), and lack a universal methodology for measuring all ODNS servers comprehensively.

### ODNS Clusters

An important observation about the above setup is that multiple DNS forwarders may rely on the same upstream DNS server. We refer to this collection of an upstream DNS server and the forwarders that have direct (or indirect) dependencies on it as an _ODNS Cluster_. Forwarders in an ODNS cluster may forward queries directly to a resolver (direct), or forward through other forwarders with multiple hops (indirect). All open DNS servers can be divided into multiple ODNS clusters due to the limited number of egress resolvers. It is worth noting that a server may belong to multiple clusters due to anycast or forwarding strategies (discussed in Section 3.2).

Because the forwarding strategy of open DNS servers is typically configured by an unknown third party (_e.g._, Router administrator (Brocki et al., 2018)), the dependencies in client-side ODNS infrastructure are invisible to clients and the ADNS administrators. Such unknown dependencies may cause potential security risks:

_Invalid redundancy configuration:_ The dependence of DNS infrastructure makes the redundancy configuration of DNS resolvers invalid. Specifically, without prior knowledge of the inter-dependencies of open DNS servers, configuring primary DNS and secondary DNS which belong to the same ODNS cluster leads to a higher risk of a single point failure.

_Amplify the impact of malicious responses:_ The impact of malicious behaviors (_e.g._, hijacking, cache poisoning) may be amplified by ODNS cluster. Because malicious behavior against the upstream resolver can further affect the clients of forwarders. Although the query processing of forwarders is not directly compromised, they obtain malicious responses from the cache of upstream resolvers.

_Exposing vulnerable entrances for attackers:_ Queries from forwarders in ODNS cluster are forwarded to upstream servers or ADNS, which means that some poorly configured forwarders may be used by attackers as entrances to attack upstream forwarders. Larger clusters suffer from more risks as they contain more uncontrollable attack vectors (exploited forwarders).

We argue that unveiling the client-side dependencies with ODNS clustering can help users achieve better DNS configuration and help administrators improve management. We widely measure the dependence of ODNS servers and take a further step toward understanding the client-side structure of ODNS infrastructure.

## 3. Methodology

To measure the dependencies among all open DNS servers, our goal is to let servers with dependencies demonstrate consistent response behavior, while others reply with different responses.

### ODNS Clustering

The underlying idea for measuring the dependencies is straightforward: Servers with dependencies should retrieve the same cached record directly from the _same_ upstream server if this domain has been previously accessed. Thus, we leverage the cache of upstream DNS servers to label different clusters by responding with unique record content for each query from a controlled ADNS. Fig. 2 illustrates our measurement method and an example scenario. In our example scenario, we see that servers \(S\)1-_S_3 have dependencies, and 54-56 have dependencies. Our method tries to detect the dependencies in \(S\)1-_S_6 and divides these servers into two ODNS clusters (_S_1-_S_3 and \(S\)4-_S_6). The whole process involves ODNS discovery, ODNS labeling, and ODNS clustering, which are detailed below.

_Step 1: ODNS discovery._ We first acquire a list of all open DNS servers. To this end, we use Zmap-based (Zhao et al., 2016) script to send DNS A queries to all routable IPv4 addresses with an unused domain newly

Figure 2. ODNS dependency detection and clustering.

registered by us. An IP is considered to be an open DNS server if it responds with a NOERROR reply code and has an A record.

Through this step, we obtain a complete list of open DNS servers. Meanwhile, we check the transparent forwarders (Steiner, 2017) which can be utilized as additional vantage points.

_Step 2: ODNS labeling._ We next leverage the footprint in the cache for labeling all ODNS servers. For this, we built a controlled ADNS for the domain (denoted as _sub.example.com_ here for illustration propose) we registered. Note, this domain must not have been used before this step. Specifically, our vantage point sends A queries for _sub.example.com_ to each server in the open DNS server list compiled in the previous step. For example, the vantage point will first send the query to \(S1\). Ultimately, the query will be responded by our controlled ADNS. For each A query received, ADNS generates a unique A record (_e.g._, 0.0.0.1). This unique A record will be returned to \(S1\) through the forwarding chain and be cached due to the ubiquitous cache structure in resolvers (Steiner, 2017). The vantage point will then send the query to the next server on the list. For instance, the next query will be sent to \(S2\). In this example, it will directly hit the cache in \(S2\), and return the same (unique) A record. The cached responses act as the **labels** for ODNS servers.

_Step 3: ODNS clustering._ Finally, we use the shared A records (the label we get in previous steps) to group servers with the same label into a single cluster. This is again illustrated in Fig. 2. Here, the DNS queries to \(S1\)-\(S3\) traverse through \(S2\) to reach the ADNS. Consequently, they will receive the same A record, which is cached (0.0.0.1). Similarly, queries to \(S4\)-\(S6\) will reach ADNS via \(S6\). Given that our ADNS server delivers unique responses for each query, \(S6\) obtains the A record 0.0.0.2. Thus, we can easily divide the ODNS address space into two ODNS clusters based on the A record responses and identify the client-side structure of ODNS infrastructure.

### Public DNS Clusters

As public DNS resolvers become a common default configuration, our measurement methods encounter challenges when addressing clusters formed by public DNS resolvers, primarily due to their use of multiple Points-of-Presence (PoPs) with anycast addresses and fragmented backend caches (Ghosh et al., 2017; Stille et al., 2017). Fig.3 illustrates how public DNS resolver behaves during the clustering process. For forwarders specifying a public resolver (with anycast address) as the upstream server, queries are routed to the nearest PoP and then handled by one fragmented backend cache component. Consequently, even if a domain was recently resolved by a public DNS resolver, subsequent queries for the same domain may miss the previous cache. This results in forwarders being divided into more clusters unexpectedly.

We now describe our approach for handling multiple ODNS clusters formed by public resolvers. We aggregate clusters created by volatile dependencies (_e.g._, randomly assigned fragmented caches) while preserving clusters formed by stable dependencies (_e.g._, multiple PoPs with anycast). To this end, we perform multiple rounds of ODNS clustering using different controlled subdomains and obtain multi-round clustering results. The core of our approach is:

**1**Cluster aggregation for fragmented caches._ In multi-round measurements, servers in such clusters may be reallocated to one another in different rounds (_e.g._, changes in cache allocation of the public resolver). We leveraged the shifts across multiple rounds to aggregate clusters. We provide a quick example of the aggregation process in Fig. 4. Among them, servers \(S1\)-\(S9\) represent ODNS servers in the ODNS space, which are divided into multiple clusters during the clustering process. Initially, we have a clustering result (donated as Current Clusters) consisting of three clusters: clusters1 1, clusters1 2, and clusters1 3. In the clustering result in another detection round, we observe that the fragmented cache's random allocation has generated a different clustering result (donated as New Round) of clusters1.1 and clusters1 2 to clusters2.1, clusters2.2. We found that across multiple rounds of clustering, servers \(S1\)-\(S8\) will generate variable clustering results, but never mixed with unrelated clusters (donated as \(S9\)). By aggregating the clusters1.1 and clusters1.2, we can achieve a more stable clustering result. The specific aggregation method and principles are detailed in Algorithm 1.

Algorithm 1 outlines the workflow of cluster aggregation. The current clustering result is denoted as \(C_{}\), while the clustering result from the new round is denoted as \(C_{}\). The goal is to use \(C_{}\) to merge clusters in \(C_{}\) that may be related. For each cluster in \(C_{}\) (lines 1-2), we calculate the overlap ratio with each cluster in \(C_{}\) (lines 4-6). If the overlap ratio exceeds a predetermined threshold \(\) (lines 7-9), the clusters are considered related, and we merge them (lines 10-13). The threshold\({}^{2}\) represents the

Figure 4. Example for cluster aggregation.

Figure 3. Clusters with public resolvers.

probability that servers in the current cluster will remain in the same fragmented cache in the next round of detection.

#### 3.2. Clustering for multiple PoPs

Because of the use of anycast address (Shan et al., 2017), public resolvers will also form multiple clusters due to multiple PoPs. Unlike the formation of fragmented caches above, we do not need to further aggregate this stable cluster structure. This is because the queries from forwards will be forwarded to the nearest PoP and will not be changed frequently in future measurements. As such, a public DNS server can correspond to multiple clusters. For DNS servers with multiple PoPs sharing an anycast address (_e.g.,_ public DNS), our method can naturally divide forwarders that depend on PoPs into different clusters.

### Vantage Points and Transparent Forwarder

Revealing that an anycast IP is in different clusters requires vantage points corresponding to the PoPs service area. We deployed 5 controlled vantage points in Singapore, the United States, Ireland, Ukraine, and Brazil for ODNS discovery and labeling. However, it is still insufficient to measure all PoPs behind public resolvers. To this end, we turned our attention to transparent forwarders (Vantage et al., 2017). Transparent forwarders respond to clients using the address of the upstream server instead of itself. Such behavior exposes its upstream server to clients, and this upstream server is usually a public DNS resolver. We leverage about 370k transparent forwarders across 186 regions as additional vantage points for measurement.

## 4. Measurements & Analysis

We conducted measurements with 5 controlled subdomains and repeated queries 5 times in each run. We show the cluster-level measurement results and findings in this section.

### ODNS Clusters In-the-Wild

We find over 1.9 million addresses of open DNS servers in the wild. Among them, 972,383 servers complete the measurement process, the churning of IP is caused by some open DNS devices that change addresses in a short period (_e.g.,_ servers with DHCP (Shan et al., 2017)). These volatile addresses (Shan et al., 2017; Wang et al., 2017) are not within the scope of our research. Our method groups open DNS servers with successful responses into 81,636 clusters, distinguished by A record responses. We illustrate the distribution of ODNS cluster size in Fig. 5(a) and the cumulative distribution function of cluster size and open DNS servers covered by different cluster sizes in Fig. 5(b).

The size of ODNS clusters ranges from only a few DNS servers to thousands. The size of the ODNS cluster demonstrates features of the DNS infrastructure. Our results reveal that 48,894 clusters contain only one IP address (covering 5% of servers after eliminating duplicate IPs). Such clusters with only one DNS server signify an individual resolver. Notably, the top 207 clusters (0.25%) with a size of over 1,000 contain over 429,499 open DNS servers in total, accounting for 44.1% of the open DNS servers. These ODNS clusters typically come from public DNS or gateways with great popularity.

_Observation 1:_ 95% open resolvers exhibit dependencies on others for name resolution as they fall into clusters with more than 1 server. Notably, the distribution of cluster size is heavily biased with 0.25% top clusters containing 44.1% open DNS servers.

### ODNS Clusters with Countries

Forwarders typically opt for a nearby upstream resolver to improve service quality and reduce latency (Shan et al., 2017). Consequently, we hypothesize that servers in the same ODNS clusters are in the same region. We therefore explore the geographical patterns of the ODNS clusters. For this, we map all ODNS IP addresses to their country codes using IDINFO (Brockman et al., 2016). In cases where an ODNS cluster is composed of servers from multiple countries, we use the country with the highest proportion to represent cluster geographical identity. We present the proportion of servers from the country with the highest proportion within each cluster. Fig. 7 illustrates the cumulative distribution function of the proportion of servers, revealing that over 95.5% of clusters have servers 99% from the same country. This result confirms the concentrated geographical distribution among servers in the same ODNS cluster.

We analyze the cluster sizes of each open DNS server belonging in each country. Fig. 6 illustrates countries with the most open DNS servers. Results show significant differences in the distribution of clustering size, which implies regional disparities of DNS infrastructures. In countries like China and South Korea, most DNS servers

Figure 5. Distribution of cluster size and open DNS servers.

(86% in China and 88% in South Korea) are part of clusters with sizes exceeding 100, highlighting the widespread use of forwarders depending on prominent public DNS servers such as 114DNS in China. We find even more extreme cases (_e.g._, Bangladesh), nearly all open DNS servers fall into ODNS clusters larger than 100. These countries tend to have more servers with concerns caused by clusters. In contrast, open DNS servers in countries like the United States tend to operate within numerous medium-sized clusters. Countries like Germany and France exhibit many clusters consisting of only one server, suggesting a higher prevalence of individual resolvers. This means less cross-dependency between servers, thereby mitigating the risk of single points of failure.

**Observation 2:** There is a significant difference in the distribution of cluster size across countries, where some (_e.g._, CN) are dominated by large-size clusters (_i.e._, \( 100\)), while others (_e.g._, DE and FR) have many clusters containing only one server. This implies different DNS infrastructures.

### ODNS Clusters with Public DNS

The above shows that often public DNS acts as an upstream DNS provider within an ODNS cluster. We next try to understand the impact of public DNS within their ODNS clusters.

_Client-side centralization of Public DNS._ Concern about DNS centralization has been mounting over the past few years . We use the list in  to obtain 28 popular public DNS service providers. To discover the popularity of forwarders on public DNS, we identify ODNS clusters containing these well-known public DNS service IP addresses. We summarize the cluster characteristics of the top public DNS servers, with the most open DNS servers depending on it in Table 1. We show the number of detected clusters and the aggregate volume of servers in all clusters as a percentage of the total (refer to covered rate). The clusters of the top 4 public DNS cover over 47% of the open DNS servers. Among the 28 public DNS providers we investigated, Google is in the absolute lead, providing DNS service for 27.99% of the open DNS servers with 268 different ODNS clusters worldwide. The second place is Cloudflare, with 9.76% open DNS servers having dependencies on it.

_Unbalanced Cluster behind public DNS anycast._ We further analyze the cluster size behind public DNS. Public DNS servers usually belong to multiple clusters distributed in different regions. This is because public DNS servers typically consist of numerous Points-of-Presence (PoPs)  with any

  
**Provider** & **\# of clusters** & **\% covered rate** \\  Google & 268 & 27.99\% \\ Cloudflare & 228 & 9.76\% \\ OpenDNS & 46 & 5.33\% \\ Yandex.DNS & 118 & 4.24\% \\ Others & — & 4.63\% \\   

Table 1. Clusters for top public DNS.

Figure 6. Top 50 countries with the most open DNS servers.

Figure 7. CDF of servers’ proportion from the most country in ODNS cluster. Note that the \(y\)-axis is in log scale.

of our methodology is that this structure can be discovered, _i.e._, the same public DNS address can belong to different clusters when using vantage points from different regions.

However, the population of forwarders in clusters of different PoPs is unbalanced, which may violate the original design intention for facilitating load balancing and attack defense (Zhou et al., 2018). We show the size of clusters of popular public DNS in Fig. 8 in descending order (including Google Public DNS, Cloudflare, OpenDNS, and Yandex DNS). The maximum cluster size exceeds 23k, indicating that a small subset of PoPs with large clusters in public DNS infrastructures may experience more concentrated traffic from a higher number of forwarders. Since DNS forwarders can serve as potential vectors for DDoS attacks (Berg et al., 2017), PoPs with larger clusters are exposed to more forwarders and thus face a higher risk of DDoS attacks compared to those with smaller clusters (Zhou et al., 2018).

Specifically, we detail the top 10 clusters by size for Google Public DNS in Table 2. Our results reveal that large clusters are concentrated in certain regions (e.g., CN, UA, ID), indicating a high number of forwarders in these areas, but potentially limited PoPs were deployed. Such deployment may result in some PoPs facing greater pressure than those in more established areas (_e.g._, We found 23 clusters located in the US, with an average size of only 637).

**Observation 3**: Popular public resolvers lead the clusters that consist of a large portion of open resolvers, showing another evidence of concentration. The use of anycast results in many clusters for one public DNS, where clusters are constituted by unbalanced server populations.

### Problematic Clusters: Misconfiguration and Maliciousness

In our measurements, not all ODNS servers direct us to the correct destination. We found a total of 91,403 (9.3 %) open DNS servers in 7,346 ODNS clusters that responded to an unexpected A record to us (which does not come from our ADNS). Using these ODNS servers for DNS services during web access could lead to unpredictable and potentially harmful outcomes.

_What type of records are in these responses?_ Table 3 presents the types of IP addresses included in these problematic response records. The problematic A records comprise loopback addresses (_e.g._, 127.0.0.1) and private addresses (_e.g._, 10.0.0.1). A more dominant case (from 87,753 servers) involves the ODNS servers returning a third-party public IP, which redirects out web request to a specific, potentially unintended destination. These unexpected responses may be due to special configurations set by administrators or the result of malicious resolution behavior.

_How do clusters amplify the problematic responses?_ Such misconfigurations will lead to all forwarders within the ODNS cluster returning the same incorrect answer and causing a cluster-level failure. We then investigate the impact scope of problematic clusters by analyzing the distribution of cluster size and geolocations. As shown in Fig. 9(a), the clusters responding with loopback address and private address mainly consist of a single server. In contrast, the impact of unexpected public addresses is more likely to be amplified by ODNS clusters since the vast majority of clusters have a size of 2-100. This means that the same error responses from the upstream node will have an amplified effect (impact on more than one server) at the cluster level. Fig. 9(b) illustrates the top 5 countries with the most problematic clusters (response public addresses), note that nearly four thousand problematic clusters are located in the US, with over 80,000 ODNS servers responding with incorrect answers.

_Where do these responses lead us to?_ To explore the origins and purpose of such clusters, we performed reverse DNS lookup to identify the domains associated with public addresses in problematic responses, as shown in Table 4. The majority direct us to hosts of cloud hosting providers like HostGator and Bluehost, where tenants may inadvertently set up misconfigured DNS servers.

We send HTTP requests to these IPs to evaluate the types of pages that these IP addresses lead us to. We show the Responses in Table 5. Among the successful responses, most pointed to parked domains (218 instances), suggesting that these domains may not be actively used or are reserved for advertising purposes. In redirection responses, 166 IPs returned a redirect response code and we found IPs from 65 clusters (63 in VG, 1 in DE, and 1 in US, affecting 1581 ODNS servers) led to pages highly related to malicious, phishing, or abused parked domains (_e.g._, secures

  
**Source** & **cluster size** & **Country** & **cluster size** \\  CN & 23482 & ZA & 8223 \\ UA & 13810 & BD & 7346 \\ ID & 13806 & ID & 7229 \\ RU & 10432 & BD & 6008 \\ BD & 8356 & BD & 5879 \\   

Table 2. Details of Top10 Clusters for Google public DNS and the countries that most servers are mapped to.

Figure 8. Size of clusters for public DNS.

  
**Response type** & **\# of servers** & **\# of clusters** \\  Public address & 87,753 & 7,057 \\ Private address & 1,473 & 15 \\ Loopback address & 2,177 & 274 \\   

Table 3. Types of IP in problematic responses.

## 5. Analysis inside clusters

In the ODNS infrastructure, the cache behavior of forwarders speeds up the response procedure and reduces the overhead of upstream resolvers. Given the multi-layer forwarding dependency inside ODNS clusters, it is critical to give a deeper insight into how requests reach upstream servers and how forwarders cache the responses. In this section, we analyze the diverse caching behaviors of forwarders inside clusters as well as the potential security risks.

### In-cluster Server Classification

**Challenges:** Dissecting the forwarding and caching behavior of servers within ODNS clusters is non-trivial, as the forwarding process between the forwarder and the upstream server is unknown to clients (_i.e._, we don't know whether the responses obtained from the forwarder's own cache or is forwarded directly upstream).

**Methodology:** The forwarder's caching behavior can be measured by leveraging the forwarding chain inside the cluster: when a downstream forwarder receives a new response, the record of the response is cached by all upstream servers along the path. As shown in Fig. 10, we leverage caching behaviors in the forwarding chain to analyze server caching types. First, our vantage point sends A queries for a controlled domain to a randomly selected server in the ODNS cluster (). The query propagates through the forwarding chain, while the response caches along the way. Next, we do cache snooping 3 for all other servers in the cluster ().

**Results analysis:** With the cache-snooping results, we classify all servers into the following types based on their behavior:

* **Caching forwarders:** Servers _have not cached_ current record is a _caching forwarder_ (_S2_), indicating that this server is not an upstream resolver. Instead, it is a forwarder and has independent cache processing itself.
* **Proxies & resolver:** Servers _have cached_ current record is a _upstream resolver_ (_S4_) or _non-caching proxy_ (_S3_). This signifies that the cache record passed through such servers when the DNS query was initially performed. While non-caching proxies (Bartos et al., 2016) simply follow the status of the upstream resolver, such proxies are classified into the same category with upstream resolver.
* **New-trigger:** In addition, if our cache detection triggers a new DNS response from ADNS, we mark such server as a _new-trigger_.

### Results and Analysis

We conduct measurements of all clusters we got in ODNS clustering. The measurement results of each cluster are presented in Fig. 11.

  
**SLD** & **\# of servers** & **\# of clusters** \\  hostgator & 33,488 & 982 \\ bluehost & 29,994 & 1,411 \\ hostmonsmeter & 4,616 & 115 \\ justhost & 3,148 & 78 \\ seabost-mail & 1,308 & 1 \\ flashstart & 1,069 & 1 \\ accountservergroup & 913 & 119 \\ fortinet & 734 & 1 \\ wehostbox & 534 & 43 \\   

Table 4. Top SLDs associated with unexpected records.

Figure 10. Server classification inside ODNS cluster.

  
**Response Type** & **Subcategory** & **\# of clusters** \\   & Parked Domain & 218 \\  & Filtered/Blocked & 60 \\  & Error Page & 72 \\  & Others & 29 \\   & Malicious & 65 \\  & Normal & 101 \\   & \(-\) & 3648 \\   & \(-\) & 2864 \\   

Table 5. Response type statistics.

Figure 9.: Problematic cluster size and their geolocation.

For illustration purposes, we randomly selected 150 clusters across three different size ranges without losing generality.

_Classification of servers:_ We found that a large portion of the ODNS address space is composed of proxies or upstream resolvers, which account for more than 61.7% of all detected ODNS servers. Since there are only a few upstream resolvers in a single DNS cluster due to our clustering methodology, the servers in this category are mainly proxies. From the perspective of clusters, we found a more extreme case that the proportion of proxies in 131 clusters exceeded 95%. This means that although the servers in these clusters show a two-layer structure of forwarder-resolver, they do not form a multi-level cache, and the DNS query traffic from the client will still directly reach the upstream resolver.

_Potential risks with proxies:_ Although assigning non-caching proxies to an upstream server is allowed by previous standard (Kumar et al., 2017) and public DNS providers (Bahdanau et al., 2016), it poses a security risk. Each DNS query sent to such forwarders directly reaches the upstream server, making these open forwarders vulnerable to being exploited for DDoS attacks. We recommend implementing appropriate access controls when configuring DNS forwarding devices, rather than processing all network-wide requests indiscriminately.

_In-cluster server composition pattern:_ From Fig. 11, we can intuitively observe a clear trend in the proportion of server types within clusters of different sizes. Notably, larger clusters tend to have a higher proportion of non-caching proxies since the upstream DNS server of proxies is typically a public DNS server or one assigned automatically by the ISP, which results in a larger cluster size. As a result, large clusters often face a greater risk of traffic concentration from proxies. In addition, the proportion of new-trigger servers is higher in smaller-size clusters, indicating such servers may no longer be part of the current cluster due to configuration changes.

_Observation 5:_ ODNS cluster consists of forwarders with diverse behaviors. Notably, the majority of forwarders (61.7%) in large clusters are non-caching proxies, which may lead to increased DNS traffic pressure on upstream resolvers and in turn serve as potential entry points for attackers.

## 6. Discussion

### Implications of ODNS Clusters

Based on our results and observations, client-side dependencies may give rise to potential risks.

_Single points failure caused by cluster._ DNS queries from different forwarders will be concentrated on a small set of resolvers. Such concentration caused by ODNS cluster raises the risk of single-point failure (Kumar et al., 2017; Wang et al., 2017; Wang et al., 2017). Configuring primary and secondary DNS servers that have no dependencies (_i.e._, exist in different ODNS clusters) can help mitigate the risk of single-point failure.

_Cluster-level malicious behavior._ The size of the ODNS cluster reveals the potential attack scope of DNS infrastructure under malicious behavior, and this risk has regional differences because of the disparity in terms of cluster size distribution across countries. Clusters containing a large volume of servers may amplify the impact of malicious behaviors (Zhao et al., 2017). For example, hijacking (Kumar et al., 2017) or cache-poisoning (Zhao et al., 2017) against key nodes (_e.g._, the egress resolver) in a large ODNS cluster can affect a wide range of (unseen) forwarders. Deploying DNS infrastructure as multiple small-sized ODNS clusters without dependencies helps reduce the effect of malicious behavior.

_Exploited forwarders behind resolvers._ As DNS forwarders can potentially function as attack vectors for DDoS attacks (Bahdanau et al., 2016), resolvers with more forwarders facing higher DDoS risks (Wang et al., 2017). The ODNS clustering result can effectively assistant estimating the number of forwarders behind each resolver and their types, which helps assess potential risks to DNS infrastructure. Meanwhile, we strongly recommend stricter access control when deploying DNS forwarders.

### Limitation and Ethics

We leverage transparent forwarders (Zhao et al., 2017) as additional vantage points in our measurements in order to improve the coverage for popular public DNS (_e.g._, Google DNS). However, the transparent forwarders can only measure the public DNS specified in their forwarding strategy, upstream resolvers that are not widely used by transparent forwarders may not be fully measured (_e.g._, 114DNS). We will further increase the number of our vantage points and achieve finer-grained regional coverage in the future.

For ethical considerations, all measurements were conducted with our own controlled domain names, thereby preventing any query traffic from reaching third-party authoritative servers. Our methodology only needs to send 5 A-type queries per open DNS server in each controlled vantage point in a round of clustering, thereby avoiding disruption to normal services.

## 7. Conclusion

In this paper, we introduce the concept of ODNS clusters and propose methodologies for measuring the dependence between open DNS servers. We conducted measurements for 1.9 million open DNS servers in the wild and formed 81,636 ODNS clusters. Measurement results show the ODNS infrastructure differences crossing regions and the network centralization led by major public DNS providers. We further discuss the potential risks of problematic clusters and non-caching proxies with a cluster-level perspective. Our findings shed light on the current status of the ODNS ecosystem, and possible enhancements to the ODNS infrastructure.

Figure 11. Cluster composition analysis results.