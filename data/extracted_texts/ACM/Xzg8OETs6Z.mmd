# Leveraging Invariant Principle for Heterophilic Graph Structure Distribution Shifts

Anonymous Author(s)

###### Abstract.

Heterophilic Graph Neural Networks (HGNNs) have shown promising results for semi-supervised learning tasks on graphs. Notably, most real-world heterophilic graphs are composed of a mixture of nodes with different neighbor patterns, exhibiting local node-level homophilic and heterophilic structures. However, existing works are only devoted to designing better unified HGNN backbones for node classification tasks on heterophilic and homophilic graph benchmarks simultaneously, and their analyses of HGNN performance concerning nodes are only based on the determined data distribution without exploring the effect caused by the difference of structural pattern between training and testing nodes. How to learn invariant node representations on heterophilic graphs to handle this structure difference or distribution shifts remains unexplored. In this paper, we first discuss the limitations of previous graph-based invariant learning methods in addressing the heterophilic graph structure distribution shifts from the perspective of data augmentation. Then, we propose **HEI**, a framework capable of generating invariant node representations through incorporating **H**eterophily information, node's estimated neighbor pattern, to infer latent Environments without augmentation, which are then used for Invariant prediction. We provide detailed theoretical guarantees to clarify the reasonability of HEL Extensive experiments on various benchmarks and backbones can also demonstrate the effectiveness and robustness of our method compared with existing state-of-the-art baselines. Our codes can be accessed through HEL.

Graph Representation Learning, Node Classification, Invariant Learning, Distribution Shifts, Heterophily and Homophily +
Footnote â€ : journal: Pattern Recognition

**ACM Reference Format:**

Anonymous Author(s). 2018. Leveraging Invariant Principle for Heterophilic Graph Structure Distribution Shifts. In _Proceedings of Make sure to enter the correct online trip from your rights uniformian email (Conference acronym_ 'XO). ACM, New York, NY, USA, 14 pages. [https://doi.org/XXXXXXXXXXXX](https://doi.org/XXXXXXXXXXXX)

## 1. Introduction

Graph Neural Networks (GNNs) have emerged as prominent approaches for learning graph-structured representations through the aggregation mechanism that effectively combines feature information from neighboring nodes . Previous GNNs primarily dealt with _homophilic graphs_, where connected nodes tend to share similar features and labels . However, growing empirical evidence suggests that these GNNs' performance significantly deteriorates when dealing with _heterophilic graphs_, where most nodes connect with others from different classes, even worse than the traditional neural networks . An appealing way to address this issue is to tailor the heterophily property to GNNs, extending the range of neighborhood aggregation and reorganizing architecture , known as the heterophilic GNNs (HGNNs).

_Heterophilic Graph Structure distribution Shift (HGSS): A novel data distribution shift perspective to reconsider existing HGNNs works._ Despite promising, most previous HGNNs assume the nodes share the determined data distribution , we argue that there is data distribution disparity among nodes with different neighbor patterns. As illustrated in Figure 1(a1), heterophilic graphs are composed of a mixture of nodes that exhibit local homophilic and heterophilic structures, _i.e._, the nodes have different neighbor patterns . The node's neighbor pattern can be measured by node homophily, representing homophily level by comparing the label between the node and its neighbors. Here, we identify their varying neighbor patterns between train and test nodes as the Heterophilic Graph Structure distribution Shift (Figure 1(a2)). This kind of shift was neglected by previous works but truly affected GNN's performance. As shown in Figure 1(a3), we visualize the HGSS between training and testing nodes on the Squirrel dataset. Compared with test nodes, the train nodes are more prone to be categorized into groups with high homophily, which may yield a test performance degradation. More statistical results on other heterophilic graph datasets can be shown in Figure 5. Notably, though some recent work  also discusses homophilic and heterophilic structural patterns, until now they haven't provided a clear technique solution for this problem. Compared with traditional HGNN works that focus on backbone designs, it's extremely urgent to seek solutions from a data distribution perspective to address the HGSS issue.

_Existing graph-based invariant learning methods perform badly for HGSS due to the augmentation-based environment construction strategy._ In the context of general distribution shifts, the technique of invariant learning  is increasingly recognized for its efficacy in mitigating these shifts. The foundational approach involves learning node representations to facilitate invariant predictor learning across various constructed environments (Figure 1(b1)), adhering to the Risk Extrapolation (REx) principle . Unfortunately, previous graph-based invariant learning methods may not effectively address the HGSS issue, primarily due to explicit environments that may be ineffective for invariant learning. As illustrated in Figure 1(c1), within HGSS settings, altering the original structure does not consistently affect the node's neighbor patterns. In essence, obtaining optimal and varied environments pertinent to neighbor patterns is challenging. Our observation (Figure 1(c2)) reveals that EERM , a pioneering invariant learning approach utilizing environment augmentation to tackle graph distribution shifts in node-leveltasks, does not perform well under HGSS settings. At times, its enhancements are less effective than simply employing the original V-Rex (Hess et al., 2018), which involves randomly distributing the train nodes across various environmental groups. We attribute this phenomenon to the irrational environment construction. According to our analysis, EERM is essentially a node environment-augmented version of V-Rex, _i.e._, the disparity in their performance is solely influenced by the differing strategies in environmental construction. Besides, from the perspective of theory assumption, V-Rex is initially employed to aid model training by calculating the variance of risks introduced by different environments as a form of regularization. The significant improvements by V-Rex also reveal that the nodes of a single input heterophilic graph may reside in distinct environments, considering the variation in neighbor patterns, thus contradicting EERM's prior assumption that all nodes in a graph share the same environment (Song et al., 2019). Based on this insight, our goal is to break away from previous explicit environment augmentation to learn the latent environment partition, which empowers the invariant learning to address the HGSS better.

_HEI: Heterophily-Guided Environment Inference for Invariant Learning._ Recent studies explore the effect of prior knowledge on the environment partition (Zhu et al., 2019; Wang et al., 2020) and subsequently strengthen the importance of the environment inference and extrapolation for model generalization (Wang et al., 2020; Wang et al., 2020). Therefore, our initial step should be to quantify the nodes' neighbor pattern properties related to the HGSS, which are central to the issue at hand. Consequently, a critical question emerges: During the training phase, how can we identify an appropriate metric to estimate the node's neighbor pattern and leverage it to deduce latent environments to manage this HGSS issue? As previously mentioned, node homophily can assess the node's neighbor patterns (Hess et al., 2018). Unfortunately, this requires the actual labels of the node and its neighbors, rendering it inapplicable during the training stage due to the potential unlabeled status of neighbor nodes. To cope with this problem, several evaluation metrics pertinent to nodes' neighbor patterns, including local similarity (Song et al., 2019), post-aggregation similarity (Song et al., 2019), and SimRank (Zhu et al., 2019), have been introduced. These metrics aim to facilitate node representation learning on heterophilic graphs during the training phase. But these studies primarily concentrate on employing these metrics to help select proper neighbors for improved HGNN architectures, while we aim to introduce a novel invariant learning framework-agnostic backbones to separate the spurious features from selected neighbors, tackling the structure distribution shifts. Therefore, we propose HEI, a framework capable of generating invariant node representations through incorporating heterophily information to infer latent environments, as shown in Figure 1 (b2), which are then used for downstream invariant prediction, under heterophilic graph structure distribution shifts. Extensive experiments on various backbones and benchmarks can verify the effectiveness of our proposed method in addressing this neglected HGSS issue.

**Our Contributions:** (i) We highlight an important yet often neglected form of heterophilic graph structure distribution shift, which is orthogonal to most HGNN works that focus on backbone designs; (ii) We propose HEI, a novel graph-based invariant learning framework to tackle the HGSS issue. Unlike previous efforts, our method emphasizes leveraging a node's inherent heterophily information to deduce latent environments without augmentation, thereby significantly improving the generalization and performance of HGNNs; (iii) We demonstrate the effectiveness of HEI on several benchmarks and backbones compared with existing methods.

Figure 1. (a) illustrates the heterophilic graph structure distribution shifts (HGSS), where the figure and histogram show the HGSS and neighbor pattern (measured by node homophily) difference between train and test nodes on the Squirrel dataset; (b) displays the comparison of different environment construction strategies between previous invariant learning works and ours from augmentation; (c) shows that the environment construction of previous methods may be ineffective in addressing the HGSS due to the unchanged neighbor pattern distribution. The experimental results between traditional and graph-based invariant learning methods can support our analysis and verify the superiority of our proposed HEL

## 2. Preliminaries

**Notations.** Given an input graph \(G=(V,X,A)\), we denote \(V\{v_{1},...,v_{N}\}\) as the nodes set, \(X R^{N D}\) as node features and \(A\{0,1\}^{N N}\) as an adjacency matrix representing whether the nodes connect, where the \(N\) and \(D\) denote the number of nodes and features, respectively. The node labels can be defined as \(Y\{0,1\}^{N C}\), where C represents the number of classes. For each node \(v\), we use \(A_{}\) and \(X_{}\) to represent its adjacency matrix and node feature.

**Problem Formulation.** We first provide the formulation for the general optimized object of node-level OOD (Out of Distribution) problem on graphs, then recalir the formulation of previous works to help distinguish our work in the next section. From the perspective of data generation, we can get train data (\(G_{train},Y_{train}\)) from train distribution \(p(,)|=e\)), the model should handle the test data \((G_{test},Y_{test})\) from a different distribution \(p(,)|=e^{})\), varying in different environments \(\). Thus, the optimized object of node-level OOD problem on graphs can be formulated as follows:

\[_{,0}_{}_{G}_{  V}_{:p(|_{},_{}, _{},_{},_{},_{ },_{})}I(f_{}(f_{}(A_{},X_{} )),y_{}) \]

where the \(\) represents the support of environments \(\), the \(f_{}\) and \(f_{}\) refer to GNN's classifier and feature extractor respectively and \(I()\) is a loss function (e.g. cross entropy). The Eq 1 aims to learn a robust model that minimizes loss across environments as much as possible. Only in this way, can the trained model be likely to adapt to unknown target test distribution well. However, environmental labels for nodes are usually unavailable during the training stage, which inspires many works to seek methods to make use of environmental information to help model training.

**Previous Graph-based Invariant Learning.** To approximate the optimized object of Eq. 1, previous works (Golovin et al., 2013; He et al., 2016; Wang et al., 2017) mainly construct diverse environments by adopting the masking strategy as Figure 1(b). Thus, we conclude previous works from the masking strategy (\(Mask_{}()\) parameterized with \(\)). Given an input single graph, we can obtain K augmented graphs as Eq. 2, where each graph corresponds to an environment. The \(\) is a pre-defined number of training environments and \(X^{m},A^{m}\), and \(V^{m}\) are corresponding mask versions of feature, adjacency matrix, and node sets.

\[G^{e k}=Mask_{}^{e k}(G)=(X^{m},A^{m},V^{m})_{e  k},k=1,2,...,K \]

Then, assisted by these augmented graphs with environment label, the GNN \(f()\) parameterized by \((,)\) can be trained considering environmental information. We can define the ERM (Empirical Risk Minimization) loss in the \(k\)-th environment as the Eq.3, which only calculates the loss on the corresponding augmented \(G^{e k}\).

\[R_{e k}(,)=_{ V^{m}}I (f_{}(f_{}(A_{},X_{}),y_{}). \]

Following the principle of Variance Risk Extrapolation (V-Rex) to reduce the risks from different environments, the final training framework can be defined as Eq. 4. Where the \(\) controls the effect between reducing the average risk and promoting equality of risks.

\[_{,}_{}\ \ L(,,)=\] \[_{k=1}^{K}R_{e k}(,)+ V r (R_{e k}(,),,R_{e k}(,)) \]

The maximization means that we should optimize the masking strategy (parameter \(\)) to construct sufficient and diverse environments, while the minimization aims to reduce the training loss for the model (parameter \(\) and \(\)).

**Discussions.** Exactly, previous graph-based invariant learning methods introduce extra augmented graphs to construct nodes' environments while our work only infers nodes' environments on a single input graph. Specifically, there exists a latent assumption for previous works that nodes on a single graph belong to the same environment so we need to construct diverse environments by data augmentation. This assumption arises from the insight that nodes on an input graph come from the same _outer domain-related environments_ (e.g. Financial graphs or Molecular graphs) (Wang et al., 2017). But considering the message-passing mechanism on heterophilic graphs (the ideal aggregation target should be nodes with the same label), the nodes should exist exactly in _inner structure-related environments_. To cope with this issue, as shown in Figure 1(c1), directly utilizing _data augmentation may be ineffective in changing the node's neighbor pattern distribution_ to construct diverse environments for invariant prediction. At the same time, the neighbor pattern difference between train and test has verified that even on a single graph, the nodes may belong to different structure-related environments. These simultaneously inspire us to directly infer node environments on a single graph assisted by the node's neighbor pattern, rather than constructing environments from different augmented graphs, for addressing heterophilic graph structure distribution shift.

## 3. Methodology

In this section, we present the details of the proposed HEL Firstly, on heterophilic graphs, we verify that the similarity can serve as a neighbor pattern indicator and then review existing similarity-based metrics to estimate the neighbor patterns during training stages. Then, we elaborate the framework to jointly learn environment partition and invariant node representation on heterophilic graphs without augmentation, assisted by the estimated neighbor patterns. Finally, we clarify the overall training process of the algorithm and discuss its complexity. Moreover, we provide a detailed theoretical analysis in Appendix A.2 to clarify the details of HEL.

### Neighbor Patterns Estimation

The node homophily is commonly used to evaluate the node's neighbor patterns, representing the node's structural pattern distribution (Golovin et al., 2013). Unfortunately, it needs the true labels of the node and its neighbors, which means they can not be used in the training stage because the neighbor nodes may be just the test nodes without labels for node classification tasks when given an input graph. To cope with it, we aim to utilize the similarity between node features to estimate the node's neighbor pattern.

**Similarity: An Indicator of Neighbor Patterns.** Previous works have shown there exists some relationship between similarity and homophily from the experimental analysis (Chen et al., 2018), it can not be guaranteed to work well without a theory foundation. Thus, we further investigate its effectiveness from the node cluster view and verifythe similarity between nodes can be exploited to approximate the neighbor pattern without the involvement of label information.

For simplicity, we take K-Means as the cluster algorithm. For two nodes \(v\) and \(u\), let \(v\) belong to the cluster centroid \(c_{1}\) and denote the square of the distance between \(v\) and \(u\) as \(=\|u-\|^{2}\), we can get \(c_{1}=\|v-c_{i}\|^{2}\), where \(c_{i}\) represent the \(i\)-th cluster centroid. Then the distance between \(u\) and cluster centroid \(c_{1}\) can be acquired as the Eq. 5. Exactly, the neighbor pattern describes the label relationship between the node and its neighbors. From the Eq 5, we can find the smaller \(\), the more likely the \(v\) and \(u\) belong to the same cluster and own the same label. Therefore, the similarity between nodes can be exploited to serve as a neighbor pattern indicator without using label information.

\[&\|u-c_{1}\|^{2}=\|(u-v)+(v-c_{1})\|^{2}\\ &=\|(u-v)\|^{2}+2\|u-v\|\| c_{1}+\|b-c_{1}\|^{2}\\ &=+2\|v-c_{1}\|+\|b-c_{1}\|^{2}\\ &=(\|v-c_{1}\|+)^{2} \]

**Existing Similarity-based Metrics.** Existing similarity-based metrics on heterophilic graphs can be shown as Eq.6.

\[(u,v)=(X_{u},X_{u})&\\ (_{}X_{u},_{u}X_{u})&\\ (u)||(u)|}_{v N(u) \\ d^{} N(u)}(X_{u^{}},X_{u^{}})&\\  \]

where the \(c(0,1)\) is a decay factor empirically set to 0.6, the \(NS(v)\) denotes \(v\)'s neighbor set including the nodes connected to \(v\),the \(_{u}\) denotes the aggregation operation on the node \(e\) and the \(Sim\) denote the similarity calculation between two objects. We can observe that the local similarity (Local Sim [(7)] and post-aggregation similarity (Agg-Sim [(26)] respectively calculate the similarity of the original and post-aggregation embedding between two nodes. In contrast, the SimRank [(22)] calculates the similarity between their respective neighbor nodes.

**Estimated Node's Neighbor Pattern.** Thus, as Eq.7, we can obtain the estimated neighbor patterns \(z_{}\) for the node \(v\) during the training stage by averaging the node's similarity with neighbors.

\[z_{}=_{u(v)}Similarity(u,v) \]

Notably, we further strengthen our object of using similarity metrics is indeed different from previous HGN works that utilize the similarity metrics((7; 22; 26)) to design backbones. From the perspective of causal analysis shown in Figure 4, when given the neighbors, we aim to separate and weaken the effect of spurious features from full neighbor features by utilizing the estimated neighbor pattern to infer the node's environment for invariant prediction. However, previous HGN works mainly aim to help the node select proper neighbors and then directly utilize full neighbor features as aggregation targets for better HGNN backbone designs. Our work is exactly _orthogonal_ to previous HGNN works.

Figure 2. Illustrations of our framework HEL (a) The neighbor pattern for each train node can be estimated by similarity first and then used for inferring environments without augmentation; (b) Based on the train nodes belonging to different inferred environments, we can train a set of environment-independent GNN classifiers with the shared encoder compared with the base GNN. The shared encoder outputs the representations of nodes in each environment and then forwards them to the base GNN classifier and the environment-independent classifier respectively. By calculating the loss gap between these two different classifiers, an invariance penalty is introduced to improve model generalization.

### HEI: Heterophily-Guided Environment Inference for Invariant Learning

We aim to utilize the estimated neighbor patterns \(Z R^{G_{}}\), which represent the node's heterophily information, as an auxiliary instrument to jointly learn nodes' environment partition and invariant node representation without augmentation. Similar techniques can be also shown in (Brock et al., 2018; Chen et al., 2019) for image classification tasks. Specifically, assisted by the estimated neighbor patterns for nodes, we can train an environment classifier \(():R^{G_{}} R^{K}\) that softly assigns the train nodes to \(K\) environments. The \(K\) is a pre-defined number, \(\) is a two-layer MLP and the \(^{(k)}()\) is denoted as the \(k\)-th entry of \(()\), with \((Z)^{K}\) and \(_{k}^{(k)}(Z)=1\). Denote the ERM loss calculated on all train nodes as \(R(,)\). Then, as shown in Eq. 8, the ERM loss in the \(k\)-th inferred environment can be defined as \(R_{(k)}(,)\), which only calculates the loss on the nodes belonging to the \(k\)-th environment.

**Overal Framework:** Based on the above analysis, the training framework of HEI can be defined as follows:

\[R_{^{(k)}}(,)=_{}^{(k)}(z_{ })l(f_{}(f_{}(A_{},X_{}),y_{}). \]

\[._{,}_{(_{1},,,k)}L( ,_{},_{1},,_{K},)=.\] \[.R(,)+^{K}[R_{ (k)}(,)-R_{(k)}(_{},)]}_{}. \]

Compared with previous graph-based invariant learning methods shown in Eq. 3 and Eq. 4, our framework mainly differs in the maximization process. Thus, we clarify the effectiveness and reasonability of our framework from two aspects: (i) The invariance penalty learning that introduces a set of environment-dependent GNN classifiers \(\{f_{_{k}}\}_{k=1}^{K}\), which are only trained on the data belonging to the inferred environments; (ii) The adaptive environment construction through optimizing the environmental classifier \(()\).

**Invariance Penalty Learning.** As shown by Eq.1, the ideal GNN classifier \(f_{}\) is expected to be optimal across all environments. After the environment classifier \(^{(k)}()\) assigns the train nodes into k inferred environments, we can adopt the following criterion to check if \(f_{}\) is already optimal in all inferred environments: Take the \(k\)-th environment as an example, we can additionally train an environment-dependent classifier \(f_{_{k}}\) on the train nodes belonging to the \(k\)-th environment. If \(f_{_{k}}\) achieves a smaller loss, it indicates that \(f_{}\) is not optimal in this environment. Moreover, we can further train a set of classifiers \(\{f_{_{k}}\}_{k=1}^{K}\), each one with a respective individual environment, to assess whether \(f_{_{k}}\) is simultaneously optimal in all environments. Notably, all these classifiers share the same encoder \(f_{}\), if \(f_{}\) extracts spurious features that are unstable across the inferred environments, \(R_{^{(k)}}(,)\) will be larger than \(R_{^{(k)}}(_{k},)\), resulting in a non-zero invariance penalty, influencing model optimization towards achieving optimality across all environments. In other words, as long as the encoder extracts the invariant feature, the GNN classifier \(f_{}\) and its related environment-dependent classifier \(\{f_{_{k}}\}_{k=1}^{K}\) will have the same prediction across different environments.

**Adaptive Environment Construction.** As shown in Figure 1(c), the effectiveness of previous methods is only influenced by environmental construction strategy. A natural question arises: What is the ideal environment partition for invariant learning to deal with the HOSS? We investigate it from the optimization of environment classifier \(()\). Specifically, a good environment partition should construct environments where the spurious features exhibit instability, incurring a large penalty if \(f_{}\) extracts spurious features. In this case, we should maximize the invariance penalty to optimize the partition function \(()\) to generate better environments, which is also consistent with the proposed strategy. Though previous works (Chen et al., 2019; Chen et al., 2019; Chen et al., 2019) also adopt the maximization process to construct diverse environments, they just focus on directly optimizing the masking strategy to get augmentation graphs. During the optimization process, these methods lack guidance brought by auxiliary information \(Z\) related to environments, ideal or effective environments are often unavailable in this case. That's why we propose to introduce the environment classifier to infer environments without augmentation, assisted by the \(Z\). Exactly, to make sure the guidance of \(Z\) has a positive impact on constructing diverse and effective environments for the invariant node representation learning, there are also two conditions for \(Z\) from the causal perspective. We will further clarify it in Appendix A.2.

### Training Process and Complexity Analysis

**Training Process:** As shown by Algorithm 1: Given a heterophilic graph input, we first estimate the neighbor patterns for each train node by Eq. 7. Then, based on Eq. 8 and Eq. 9, we aim to learn environment partition and invariant node representation, assisted by the estimated neighbor patterns through a min-max alternativeoptimization. Specifically, maximizing the invariance penalty is devoted to optimizing the environmental classifier to construct as diverse environments as possible to enlarge the loss gap between the base GNN feature encoder and additionally introduced GNN feature encoders. In contrast, the minimization of total loss aims to promote the base GNN to learn invariant representation agnostic neighbor patterns to address the HGSS issues.

**Complexity Analysis:** Given a graph with \(N\) nodes, the average degree is \(d\). GNN with \(l\) layers calculate embeddings in time and space \((Nld^{2})\). HEI assigns N nodes into \(k\) inferred environments \(N_{k=1}++N_{e=k}=N\) and executes \(k+1\) classifier computations, where the \(k\) corresponds to the \(k\) environment-independent classifiers, and \(1\) refers to the basic GNN classifier. Denote \(N^{}\) as the average number of nodes belonging to an inferred environment, the overall time complexity is \((Nld^{2}+kN^{}ld^{2})\), which is linear to the scale of the graph. More detailed efficiency studies compared with previous methods can be shown in Experiments.

## 4. Experiments

In this section, we investigate the effectiveness of HEI to answer the following questions.

* **RQ1:** Does HEI outperform state-of-art methods to address the HGSS issue?
* **RQ2:** How robust is the proposed method? Can HEI solve the problem that exists in severe distribution shifts?
* **RQ3:** How do different similarity-based metrics influence the neighbor pattern estimation, so as to further influence the effect of HEI?
* **RQ4:** What is the sensitivity of HEI concerning the pre-defined number of training environments?
* **RQ5:** How efficient is the proposed HEI compared with previous methods?

### Experimental Setup

**Datasets.** We adopt six commonly used heterophilic graph datasets (chameleon, Squirrel, Actor, Penn94, arxiv-year, and twitch-gamer) and three homophilic graph datasets (Cora, CiteSeer and PubMed) to verify the effectiveness of HEI (Gran et al., 2017; Zhang et al., 2018). To make sure the evaluation is stable and reasonable, we utilize the filtered versions of existing datasets to avoid data leakage (Zhu et al., 2018). Notably, considering that we should further split the test datasets to construct different evaluation settings. Those excessive small-scale heterophilic graph datasets, such as Texa, Cornell, and Wisconsin (Yoon et al., 2018), are not fit and chosen for evaluation due to their unstable outcomes. Moreover, considering the nodes on homophilic graphs means there exists a mere structure difference between train and test nodes. We just provide experiments and discussions in the Appendix A.3.

**Settings.** Based on previous dataset splits, we construct two different settings to evaluate the effectiveness and robustness of HEI: **(i) Standard Settings:** We sort the test nodes based on their nodes' homophily values and acquire the median. The part that is higher than the median is defined as the High Hom Test, while the rest is defined as the Low Hom Test. The model is trained on the previous train dataset and evaluated on more fine-grained test groups; **(ii) Simulation Settings where exists severe distribution shifts.**: We sort and split the train and test nodes simultaneously adopting the same strategy of (i). The model is trained on the Low/High Hom Train and evaluated on the High/Low Hom Test.

**Backbones.** To further verify our framework is orthogonal to previous HGNN works that focus on backbone designs, we adapt HEI to two existing SOTA and scalable backbones with different foundations, LINKX (MLP-based) (Gran et al., 2017) and GloGNN++ (GNN-based) (Gran et al., 2017). In this way, our improvements can be attributed to the design that deals with the neglected heterophilic structure distribution shifts.

**Baselines.** Denote the results of the backbone itself as ERM. Our comparable baselines can be categorized into: (i) Reweight-based methods considering structure information: Rende (Chen et al., 2016) and StruRW-Mixup (Zhu et al., 2018); (ii) Invariant Learning methods involving environment inference for node-level distribution shift: SRGNN (Zhu et al., 2018), EEM (Zhu et al., 2018), BAGNN (Gran et al., 2017), FLOOD (Zhu et al., 2018), CaNet (Zhu et al., 2018) and IENE (Zhu et al., 2018) ; (iii) Prototype-based methods for structural distribution shift on the special domain(e.g. graph anomaly detection): GDN (Gran et al., 2017). Notably, though we can utilize estimated neighbor patterns as auxiliary information to infer environments related to HGSS, the true environment label is still unavailable. So we don't compare with those traditional invariant learning methods that rely on the explicit environment labels, e.g. IRM (Chen et al., 2016), V-Rex (Gran et al., 2017) and GroupDRO (Zhu et al., 2018).

### Experimental Results and Analysis

**Handling Distribution Shifts under Standard Settings (RQ1).** We first evaluate the effectiveness of HEI under standard settings, where we follow the previous dataset splits and further evaluate the model on more fine-grained test groups with low and high homophily, respectively. The results can be shown in Table 1 and Table 2. We have the following observations.

On the one hand, _the impact brought by the HGSS is still apparent though we adopted the existing SOTA HGNN backbones._ As shown by the results of ERM in Table 1 and Table 2, for most datasets, there are significant performance gaps between the High Hom Test and Low Hom Test, ranging from 5 to 30 scores. These results further verify the necessity to seek methods from the perspective of data distribution rather than backbone designs to deal with this problem.

On the other hand, _HEI can outperform previous methods in most circumstances._ Specifically, compared with invariant learning methods, though HEI does not augment the training environments, utilizing the estimated neighbor patterns to directly infer latent environments still benefits invariant prediction and improves model generalization on different test distributions related to homophily. In contrast, directly adopting a reweight strategy (Renode and StruRW) or evaluating the difference between the training domain and target domain (SRGNN) without environment augmentation can't acquire superior results than invariant learning methods. This is because these methods need accurate domain knowledge or structure information in advance to help model training. However, for the HGSS issue, the nodes' environments on heterophily graphs are unknown and difficult to split into the invariant and spurious domains, like the GOOD dataset (Gran et al., 2017) which has clear domain and distribution splits. Simultaneously, the neighbor pattern distribution represents more fine-grained label relationships between 

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]