# Motivation-Aware Session Planning over Heterogeneous Social Platforms

Anonymous Author(s)

Submission Id: 919

###### Abstract.

With the explosive growth of online service platforms, an increasing number of people and enterprises are undertaking personal and professional tasks online. In real applications such as trip planning and online marketing, planning sessions for a sequence of activities or services will enable social users to receive the optimal services, improving their experience and reducing the cost of their activities. These online platforms are heterogeneous, including different types of services with different attributes. However, the problem of session planning over heterogeneous platforms has not been studied so far. In this paper, we propose a Motivation-Aware Session Planning (MASP) framework for session planning over heterogeneous social platforms. Specifically, we first propose a novel HeterBERT model to handle the heterogeneity of items at both type and attribute levels. Then, we propose to predict user preference using the motivations behind user activities. Finally, we propose an algorithm together with its optimisations for efficient session generation. The extensive tests prove the high effectiveness and efficiency of MASP.

2018

Session planning, Heterogeneous social platform 2018

## 1. Introduction

The popularity of online service platforms has provided a vital channel for people and enterprises to undertake personal and professional activities online. Recent statistics show there are now 2.8 million active Australians on TripAdvisor and 1.5 million users on Yelp 1. Users get access to these online service platforms for various purposes such as trip planning and online purchase. These have given rise to a demand for assisting users in planning sessions of activities they wish to engage in. Popular platforms like Meituan and Google Maps provide services to numerous travellers for information on points of interest, as they offer item details and recommendations. According to EnterpriseAppsToday 2, Google Maps locates hundreds of millions of places and businesses. More than a billion people use Google Maps every month to search for destinations and check the best routes. However, these platforms only recommend a list of items based on item type or keywords. In practice, users could set up a set of activities and require a detailed travel plan. Take travel planning as an example as shown in Figure 1. The user gives a set of interested activities and the system provides a plan that contains exact items and corresponding time. Thus, designing advanced session planning solutions becomes a new research problem and is promising for improving the service quality of these platforms, and improving their user experience.

Session planning has contexts and objectives that are different from those for Session-Based Recommender Systems (SBRS) and Personalized Route Planning (PRP). Traditionally, SBRS [(10; 14; 16; 29; 41)] predict the next item or item session based on historical sessions. PRP methods [(5; 21; 31)] typically generate user-specific routes in response to users' queries, considering user preferences and other factors like checkpoints or distance constraints. However, in session planning, users provide multiple activity categories, resembling a set of item categories, such that the system predicts the optimal session plans for their future actions based on historical item sessions. A well-generated session should align with the user preferences and certain related constraints. Figure 1 shows examples of SBRS, PRP, and session planning. Suppose a user \(u_{i}\) named Jack recently visits 7-Eleven from home, and his profile keeps his historical activity sessions as shown in Figure 1 (a). SBRS would suggest McDonald's and ANZ Bank since his profile keeps a historical session of activities, _Home, 7-Eleven, McDonald's, ANZ Bank_. As shown in Figure 1 (b), Jack wants to travel from home to _The Flyfisher_, with two checkpoints, _IGA Hawthorn_ and _QV Melbourne_. PRP would provide several routes that pass through all the POIs, based on his historical routes, considering different modes of transport, travel time, and transportation costs. While Jack would like to go to QV Melbourne from home and wants to conduct four activities: shopping, refuelling, having lunch, and finding a parking lot as shown in Figure 1 (c), he needs the system to plan his activities. The session planning could generate a series of specific POIs, _7-Eleven, Wilson Parking, Qv Melbourne, Grill 4 QV_, for his planned activities.

This paper proposes a _Session Planning_ over _Heterogeneous social Platform_ (_SPHP_) problem, where users provide sets of activity categories and request ordered item sequences. The platforms could provide heterogeneous services (items) to users, and users can request multiple services. However, a big challenge is that wecannot predict an optimal session if only the contexts (source and destination) and objectives (service/activity categories) are provided. This is because a user may have dynamic preference for each activity category and the activity categories in a session are usually unrelated with each other. As shown in Figure 0(c), the activity categories, _refueling_ and _having lunch_, have no connections for current user session prediction. Thus, a system generated plan may not reflect user's dynamic preferences on activities and overall interests with respect to the time and distance constraints of the session. **How can we predict the optimal activity sessions without knowing the dynamic preferences of users?** Another challenge is that the item variety on platforms causes data heterogeneity at both attribute and type levels. Unlike SBRS where a session includes highly related items or user behaviours of the same type, the items within a session can vary significantly from each other in SPHP. As shown in Figure 0(c), the gas station 7-Eleven and the restaurant Grill'd QV are two different types of establishments, each with distinct services. Existing solutions for heterogeneity problem take item attributes as auxiliary information (Sendry et al., 2017) or describe each item as an attribute set (Kumar et al., 2017), which generates item embeddings with extremely high dimensionality and ignores the correlation between attributes and types. Table 1 shows three items with their attributes from Yelp. Clinic 1 and Restaurant 1 belong to different categories and have different attributes. Restaurant 1 and Restaurant 2 belong to the same category, but still have different attributes such as Alcohol and GoodForKids. **How can we model the heterogeneous items with variable and large number of attributes and types?** SPHP is a problem of global optimisation over all the services on heterogeneous platforms. Since the volume of services is huge, we have a further challenge on real-time response of the system. **How can we quickly identify the activity sessions over a huge number of heterogeneous online services?**

This paper proposes Motivation-Aware Session Planning (MASP) framework that fully exploits the driving force behind activities to predict the user preferences for SPHP. We first propose a novel HeterBERT model to capture the attribute-level and type-level heterogeneity. Then, we design a motivation-aware solution to generate motivation-aware item/user embeddings. Final plans are obtained by multi-constraints session generation.

* We propose a novel session planning over heterogeneous social platform (SPHP) problem, which generates optimal sessions for users requesting services.
* We propose a novel HeterBERT model to address the attribute-level heterogeneity. HeterBERT well handles the problem of uncertain attribute number of items and captures the type-attribute correlation in heterogeneous items.
* We design a motivation-aware prediction solution that fully exploits the motivations behind activities to capture the dynamic preferences of user for session planning.
* We propose a multi-constraints session generation algorithm together with optimisation strategies that enables effective and efficient multi-constraints session generation. The test results prove the performance of MASP.

## 2. Related Work

This research is relevant to session-based recommendation, personalized route planning and heterogeneous social media processing.

SBRS learn users' preferences from the sessions associated and generated during the consumption process. Each session includes multiple user-item interactions occurring together over a period, typically lasting for up to several hours. Conventional SBRSs (Kumar et al., 2017; Kumar et al., 2017) employ data mining or machine learning techniques to capture the dependencies embedded in sessions for recommendations. Latent representation-based SBRS (Kumar et al., 2017; Kumar et al., 2017) construct a low-dimensional latent representation for each interaction within sessions using shallow models for recommendation. Recently, DNN-based SBRSs (Kumar et al., 2017; Kumar et al., 2017; Kumar et al., 2017; Kumar et al., 2017; Kumar et al., 2017) has been popular due to their powerful capabilities to model the complex intra-session and inter-session dependencies. In (Kumar et al., 2017), a session graph and its mirror graph are constructed, and the information propagation between them is conducted with an iterative dual refinement for representation learning. GRec (Shao et al., 2017) adopts CNN with sparse kernels for item and session embeddings.SEOL (Song et al., 2017) enhances recommendation using session tokens, session segment embeddings, and temporal self-attention. KMVG (Chen et al., 2018) learns three item representations from the knowledge graph, contextual transitions in sessions, and local item-item relationships, which are merged as the final item representation. In SPHP, a session is an activity plan that includes items to interact with a user shortly. However, in SBRS, a session refers to a series of history or current user behaviours in a period. Thus, SPHP is a new research problem. The SBRS methods cannot be applied or extended for SPHP.

Route planning aims to generate the top \(k\) probable routes that satisfy a query containing the origin, destination, a set of checkpoints, a maximum time cost, etc. Conventional methods (Brockman et al., 2016; Chen et al., 2018; Chen et al., 2018) aim to find the optimal routes according to a specified objective, such as minimizing distance, time, or cost. Recent deep learning (DL)-based methods (Chen et al., 2018; Chen et al., 2018; Chen et al., 2018; Chen et al., 2018; Kumar et al., 2017) prevail in route planning since they can discover complex relationships among data. NASR+ (Kumar et al., 2017) models the observable trajectory by attention-based RNNs and estimates the future cost using position-aware graph attention

 
**Items** & **Attitudes** \\  Clinic 1 & Accepts Insurance; By Appointment Only; Business Accepts Bitcoin. \\  Restaurants 1 & Good For Mae;WH; Conf For Kids; Has TV; Restaurants Beversations; Business Pricing. \\  Restaurants 2 & Restaurants Attire; Business Accepts CreditCards; Alcohol; Good For Kids; Restaurants Restrictions; Business Pricing; Risk Parting; Restaurants Delivery. \\  

Table 1. Variety of items.

networks. SpeakNav (Abadi et al., 2016) exploits BERT to extract clues from user speeches for generating routes. MaORL (Ma et al., 2017) adopts a multi-agent multi-objective reinforcement learning framework. VIAL (Zhou et al., 2018) enhances A' algorithm with a variational inference-based estimator to model the distribution of travel time between two nodes. MAC (Mikolov et al., 2013) learns the knowledge from geographical and semantic neighbours to be combined for predicting the next item. In SPHP, a user keeps implicit objectives and only gives a set of activities, while PRP methods require explicit objectives like checkpoints and transport modes. In addition, existing PRP (Zhou et al., 2018; Ma et al., 2017; Ma et al., 2017) only handle heterogeneous data from item level and interaction level, while cannot handle attribute-level heterogeneity in SPHP.

Heterogeneous social media has been handled using cross-domain and transfer learning (TL)-based methods. Cross-domain methods (Zhou et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018) address the data heterogeneity using the information from multiple domains or sources. For example, GCBAN (Zhou et al., 2018) embeds items/users and their auxiliary information into two latent spaces for each data domain. Two types of latent features are concatenated and applied to a Gaussian-based probabilistic model for recommendation. EquiTensors (Zhou et al., 2018) aligns heterogeneous datasets to a consistent spatio-temporal domain, and learns shared representations using convolutional denoising autoencoders. HetSANN (Li et al., 2018) constructs a heterogeneous user-item graph. An attention mechanism learns the node embeddings for node classification. However, HetSANN requires uniform attributes, thus inapplicable to handling the attribute-level heterogeneity in SPHP. TL-based models (Zhou et al., 2018; Li et al., 2018) exploit the knowledge gained from a previous task to generalize the model for other tasks. DJTCDR (Djumdar et al., 2018) transfers information between two types of items by dual learning. BALANCE (Li et al., 2018) downside from dynamic and heterogeneous workloads by transfer reinforcement learning. In SPHP, the number of attributes is variable and large, causing attribute-level heterogeneity. However, none of existing PRP can handle the attribute-level heterogeneity.

## 3. Problem Formulation

This section defines the concepts of item, activity, and session, and formally formulates the SPHP problem.

Definition 1 ().: _In heterogeneous social platforms, an **item** refers to a real-world entity, such as a restaurant or store, providing a type of service at a specific location and time period. An **activity** refers to a particular type of user behaviour or action such as shopping, park visits, or medical appointments. Each activity is taken by users through the platform's service. A user may request multiple services from different items within a time period._

Each item has several content attributes that describe its properties, and two contextual attributes, geographic location and opening hours. An item can have up to 33 attributes. The heterogeneous items present special characteristics in contrast to general items.

* Attribute-level heterogeneity: The number of attributes in each heterogeneous item can be large and different items may have different numbers of attributes.
* Type-level heterogeneity: A social platform includes various item types such as "restaurant" and "hotel". Different types normally share few common attributes.
* Type-attribute correlation: Attributes are associated with types. Even if some attributes from different item types share common names (e.g. "price" in "hotel" and "restaurant"), they reflect different semantic meanings.

In practice, a user may take a series of activities within a time period. A number of items, each of which is associated with an activity, are arranged in order to form a session.

Definition 2 ().: _A **session** is a service plan for a user including a series of items to be interacted by the user. Formally, a session is a series \(<\)\({}_{1}\),\({}^{}\), \({}_{q}\)\(>\), where \({}_{q}\) is the \(i^{th}\) item and \(q\) is the session length._

Given a user and a set of his/her planned activities, an ideal session should contain the items that provide these service activities, satisfying the user's preference, under the context constraints. The problem of _SPHP_ is formally defined as follows:

Definition 3 ().: _Given a user, a set of activities \(\{e_{1},,e_{q}\}\), and a session score function score, SPHP aims to detect a list of sessions with the highest probability scores, satisfying the constraints below:_

* _Distance constraint: a user can only visit the items that are within the radius of this user._
* _Time constraint: each item has available time and a user can only visit the available items._

We address the problem of effective and efficient SPHP, and propose a motivation-aware session planning framework (MASP) for SPHP. Here, _motivation_ refers to the internal or external driving force, such as seeing doctor or sales promotion, that drive individuals to undertake specific actions. Motivation may stem from users' needs, desires, goals, or external stimuli, which guide user behaviours to fulfil their objectives. When planning activities, individuals typically align their behaviours with their underlying motivations to achieve their goals. Figure 2 shows MASP that mainly contains three parts: heterogeneous data model, motivation-aware preference prediction, and multi-constraints session generation.

## 4. Motivation-Aware Session Planning

Intuitively, a set of activities could imply users' motivations which lead to different strategies when selecting individual activities. Figure 3 shows two activity sets \(<\)parking, go to hospitality- and \(<\)parking, go hiking\(>\). When users are seeking medical care, they may prioritize hospitals with closer parking facilities. However,

Figure 3. Motivations behind activities.

Figure 2. Overview of MASP framework.

when heading out for outdoor activities, they might value parking lots that are more affordable. This indicates that motivations behind activities could influence users' decisions.

### Heterogeneous Data Model

We build a model to discover the heterogeneous item types with an uncertain number of correlated attributes.

#### 4.1.1. HeterBERT

Each heterogeneous item on social platforms can be described as a set of attributes, and different items can have different numbers of attributes. To represent a heterogeneous item, one-hot encoding takes the item attributes as auxiliary information (Hardt et al., 2016). Other approaches represent each item as a set of attributes (Hardt et al., 2016). With these methods, the dimensionality of an item embedding equals the number of attributes. The total number of attributes could be huge, which leads to a large size of one-hot encoding and incurs the curse of dimensionality, further incurring high memory costs for data storage and high time costs for data updates introduced by new items or attributes. In addition, all existing methods ignore the correlation between attributes and types, which fails to capture the item attributes under different types. BERT-based model is suitable for text embedding learning, which addresses the uncertain attribute number problem by the BERT padding operations. By dividing attributes into words, BERT can learn word embedding and indirectly learn attribute embedding as item representation. However, turning each attribute sentence into separate words, the BERT-based model cannot capture the word correlation in the same attribute, and thus cannot address the attribute heterogeneity of items. In addition, BERT cannot handle type-level heterogeneity or capture the type-attribute correlation since it ignores the type information. For example, the word "insurance" carries completely different meanings when it comes to hotels and clinics. We need to build a model that is robust to the heterogeneity of items at attribute and type levels and captures the type-attribute correlation.

We propose a novel HeterBERT model for heterogeneous items, as shown in Figure 4. HeterBERT advances BERT (He et al., 2016) in fourfold: (1) HeterBERT adopts our new proposed recurrence positional encoding and type encoding to keep the correlation among inner-attribute words and capture the type information respectively; (2) a new type attention layer is designed to inject type into word embedding learning, with a double threshold mechanism to enhance the correlation among inner-attribute words. (3) a C-Merge layer is proposed to capture the type-attribute correlation; (4) considering the type-level heterogeneity, a contrastive learning task is designed to learn discriminative embeddings.

**Input Embedding.** Given an item, the input embedding layer divides the item attributes into words and generates an initial embedding for each word. To address the heterogeneity, we propose a recurrence positional encoding which considers the relative position of the words within each attribute while ignoring the relative position of attributes. Figure 5 shows an example of this recurrence positional encoding. When the special token _isep_ emerges, we reset the index of tokens. The positional embedding is generated by the sine and cosine functions as in (Wang et al., 2017). By this encoding, the generated word embeddings can capture correlation among words in the same attribute, while allowing the attribute orders to be swapped for fitting the characteristics of the item attributes semantically.

To capture type information in embedding learning, intuitively, type vectors should be distinct from each other to make word embeddings discriminative from the type aspect. We propose a type encoding model as Figure 6 shows. Each type is first divided into words and the corresponding token embeddings \(E\) are integrated by linear transformation \(e=W_{c}E\) to generate intermediate type representations \(E_{C}=[e_{1},,e_{C}]\), where \(W_{c}\) is the transformation matrix. Though each type has a parameter matrix \(W_{c}\), the complexity of this type encoding model is limited, since the number of types is not large and the model architecture is shallow. Another linear transformation \(_{C}=WE_{C}\) is applied on \(E_{C}\) to generate the final type representation. We aim to maximise the difference among type representations and formulate the loss function as below:

\[Loss=1/Var(_{C}), \]

where \(Var(_{C})\) is the variance of matrix \(_{C}\). This loss function is supported by Theorem 1 proved in Appendix A.1:

**Theorem 1**.: _Given a set of type representations with mean value \(\), the distinct difference among these representations is achieved when \(Var(_{C})\) is maximised._

The loss function has no regularization term, as we train this model over all types and directly use \(_{C}\) as the type representations, which avoids the overfitting issue. Given an item, we generate the initial embeddings by summing the corresponding token embedding (Wang et al., 2017), recurrence positional encoding, and type encoding.

**Type Attention.** Given type representations and initial word embeddings, we feed them into the transformer layer. As shown in Figure 4b (we omit softmax, scaling layers, residual connection, and normalization layers for convenience), input embeddings are projected into word-level query, key, and value matrices \(Q,K,\) and \(V\) by learned linear transformation matrices. Unlike the vanilla attention

Figure 4. HeterBERT.

Figure 5. Recurrence positional encoding.

Figure 6. Type encoding model.

layer that only feeds \(Q\) and \(K\) into the matrix multiplication layer to compute the word-level attention weights, we design C-Merge to capture the correlation among inner-attribute words, as Figure 4c shows. We merge word- and attribute-level attention weights, capturing the correlation among inner-attribute words. Specifically, given a word-level query matrix \(Q\), a key matrix \(K\), and type representation \(_{C}\), we first divide \(Q\) and \(K\) into subsets and feed them into the C-Merge layer, each of which contains word embeddings to the same attribute. Given a subset, the C-Merge layer computes inner-attribute word weights by \(W_{att}=HE\), where \(E\) is the concatenation of word embeddings and the item type representation, \(H\) is the transformation matrix. Then, we can get the attribute embedding by merging word embeddings \(e_{att}=W_{att}^{T}\), where \(\) contains word embeddings. Especially, the C-Merge layers of each type attention layer share parameters and type representation to reduce the computation time and maintain type information in the whole embedding learning. The attribute-level query and key matrices \(Q^{}\) and \(K^{}\) are generated by combining all attribute embeddings from \(Q\) and \(K\) respectively. Given attribute-level \(Q^{}\) and \(K^{}\), we compute attribute-level attention weights by the dot product of these matrices. We select the word- and attribute-level weights larger than their thresholds to enhance the correlation among words within the same attribute. Then the selected weights are merged by weighted sum to generate the new word-level weights:

\[W^{*}=w_{T}W_{T1}+(1-w_{T})W_{T2}, \]

where \(W_{T1}\) and \(W_{T2}\) are filtered weights and \(w_{T}\) is a trade-off parameter. The attention layer feeds the value matrix \(V\) and the new word-level weights into the matrix multiplication layer to derive the updated word embeddings. The output word embeddings are fed into a position-wise fully connected feed-forward network with a residual connection normalization (Shi et al., 2017). The feed-forward network produces updated word embeddings for the next attention layer.

Given the output word embeddings of the last attention layer, we feed them into a C-Merge layer in HeterBERT (Figure 4a) to get attribute embeddings, injecting the type into attribute embeddings and capturing the type-attribute correlation.

**Pre-training HeterBERT.** To save the training cost, we pre-train HeterBERT by two tasks: Masked Language Model (MLM) and Contrastive Learning (CL). For MLM, we randomly select 15% of the words from all attributes, adopt a masking procedure, and predict the masked tokens, as in (Chen et al., 2018). Specifically, we use the BERT masking which replaces 80% of the selected words with _mask_ token, replaces 10% of those with a random word, and keeps the rest 10% of those unchanged. For example, given an input "Restaurant Take Out" where "Out" is the selected word, there are three masked inputs: "Restaurant Take [mask]", "Restaurant Take WEB", and "Restaurant Take Out". By applying softmax to the output embeddings at the positions of the masked tokens, the prediction results can be obtained. As in (Chen et al., 2018), we use the categorical cross-entropy loss \(L_{MLM}\).

For the second learning task, the ideal attribute embeddings should be distinct when these attributes belong to different types, due to the type-level heterogeneity. These embeddings should keep the diverse information of attributes belonging to the same type, as these attributes may describe different items from different aspects. Unlike BERT that outputs two sentences from two segments of input tokens, HeterBERT outputs \(l\) sentences to attribute embeddings, where \(l\) is the number of attributes in an item. Thus, Next Sentence Prediction (NSP) used by BERT, is improper for HeterBERT training. Other supervised learning tasks like type classification are also unsuitable since they could make attributes in an item excessively similar in feature space. We propose a C L task for training HeterBERT by using "positive" and "negative" data. Specifically, we first randomly select \(n_{p}\) pairs of attribute embeddings from the same items as "positive" pairs. Then, we derive \(n_{n}\) "negative" pairs by selecting two attribute embeddings from two items. We assume the distance of "positive" pairs is smaller than "negative" pairs. We formulate the loss function as follows:

\[L_{CL}=-ln(_{p_{i}^{+}}^{n_{p}}(e_{i}^{+},e_{k}^{-})-_{p_{i}^{-}}^{ n_{p}}(e_{j}^{+},e_{k}^{-})), \]

where \(()\) is cosine similarity, \(p_{i}^{+}\) is the \(i\)-th "positive" pair \((e_{j}^{+},e_{k}^{+})\), and \(p_{i}^{-}\) is the \(i\)-th "negative" pair \((e_{j}^{+},e_{k}^{-})\). With HeterBERT contrastive learning, we learn the attribute embeddings that are distinct at the type level, while keeping the diverse information of attributes.

#### 4.1.2. User Profile Construction

Each social user contains many types of data, like friendship and interaction records. We construct the profile based on her/his interacted items and friends on social platforms. Given a user \(u\), s/he should be interested in some attributes of the interacted items, and s/he should share some common interests with her/his friends. Thus, we first construct a historical attribute set \(A_{u}\) by collecting attributes from interacted items and form a neighbour attribute set \(A_{n}\) by combining historical attribute sets \(A_{u}\) of the user's friends. Then, we combine these sets as \(_{u}\) to reflect which attributes the user is interested in. The attribute embeddings of \(_{u}\) form the user profile as \(E_{u}\). Formally, a user profile is described as \(\)-tuple \(U=c\)_uid_, \(l,N,A_{u},A_{n},E_{u}>\), where _uid_ is the user id; \(l\) is the history set, containing _uid_ of interacted items; \(N\) is the neighbour set, containing _uid_ of \(u\)'s friends; \(A_{u}\) is the attribute set of interacted items; \(A_{n}\) is the attribute set, combining \(A_{u}\) sets of \(u\)'s friends; \(E_{u}\) is the attribute embedding set, combining the corresponding attribute embeddings of \(A_{u} A_{n}\).

### Motivation-aware Preference Prediction

We build a model that considers motivation and dynamically merges attribute embeddings of users and items.

#### 4.2.1. Activity Category Arrangement Algorithm

Given a activity set, we need to generate all the possible arrangements. However, in practice, some arrangements are not reasonable. For example, given an activity set \(\{Parking,Restaurant,Shop\}\), an arrangement \(<Restaurant,Parking,Shop>\) is unreasonable since people often park before doing other activities. Thus, we propose an Activity Category Arrangement (ACA) algorithm to select candidate category arrangements. The algorithm is detailed in Appendix A.2.

#### 4.2.2. M-Merge User Preference Prediction Model

To predict user preference, we need to represent users and items from attribute embeddings into uniform features. Existing methods (Yang et al., 2019) represent an item by merging its attributes with concatenation or weighted summing, which incurs redundancy from similar attributes and weaken the influence of key attributes. As Figure 3 shows, under different motivations, user preference could vary for the same type of items. We propose a motivation-based merge (M-Merge) layer,as shown in Figure 7, to dynamically generate user and item representations under the motivation behind an arrangement. Then, the user-item relevance is computed for candidate item list generation.

**Motivation-aware Item Representation.** Given an item, M-Merge generates an item feature by merging its attribute embeddings under the motivation behind a category arrangement. M-Merge needs to capture the influence of previous activities in item representation, due to the contextual correlation from activity series. Given a set of historical attribute embeddings \(H_{t-1}\), the current category \(c_{u_{t}}\), and attribute embeddings of the current item \(E_{u_{t}}\), M-Merge integrates attribute embeddings based on the current motivation and previous items. Specifically, given a \(c_{u_{t}}\), we sample \(N_{M}\) attribute embeddings from the corresponding attribute pool, as the motivation set \(M_{t} R^{N_{M} d}\), based on their occurrence frequency (\(N_{M}\) is set to 20 empirically3). We construct an attribute pool for each type by gathering attribute embeddings from items to that type. Given the historical attribute embedding set \(H_{t-1}\), the current motivation set \(M_{t}\), and the current item attribute embedding set \(E_{u_{t}}\), the M-Merge layer first measures how much an attribute meets another attribute from the current motivation/previous interaction by the dot product of two attribute embeddings. Extending to the whole input \(E_{u_{t}}\), we formulate two similarity matrices \(E_{u_{t}}M_{t}^{T}\) and \(E_{u_{t}}H_{t-1}^{T}\). Then, we apply \(RowSum\) on these matrices, followed by \(SoftMax\) and \(Transposition\), to generate weights \(W_{M}\)gR1\( N_{E}\) and \(W_{H}\)gR1\( N_{E}\). The integration is formulated by:

\[e_{u_{t}}=Softmax(_{0}W_{M}+(1-_{0})W_{H})E_{u_{t}}, \]

where \(_{0}\) is a trade-off parameter. We regard the integrated weights as corresponding probabilities for item attribute embeddings. Then, we repeatedly sample an attribute from \(E_{u_{t}}\) at the probability distribution and add it to \(_{t}\) until \(|_{t}|>p_{top}|E_{u_{t}}|\), where \(p_{top}\) is a parameter which controls the number of sampling. \(H_{t} R^{N_{M} d}\) is generated by combining the previous attributes \(H_{t-1} R^{m_{t} d}\) and current attributes \(_{t} R^{m_{t} d}\), where \(N_{H}=m_{1}+m_{2}\).

**Motivation-aware User Representation.** Similar to item representation, given a user, the model needs to capture the influence of previous activities in user representation and the current motivation. We use another M-Merge model to dynamically generate the user feature. Given a type \(c_{u_{t}}\), we first randomly select \(N_{M}\) attribute embeddings from the corresponding pool as the motivation set \(M_{t} R^{N_{M} d}\). Given a set of user attribute embeddings for previous activities \(G_{t-1}\), the current motivation \(M_{t}\), and a user profile \(E_{u_{t}}\), we then formulate two similarity matrices \(E_{u_{t}}M_{t}^{T}\) and \(E_{u_{t}}G_{t-1}^{T}\) and generate historical/current weights \(W_{M} R^{1 N_{E}}\) and \(W_{G} R^{1 N_{E}}\) by \(RowSum,SoftMax,\) and \(Transposition\) operations. The user feature is formulated by:

\[e_{u_{t}}=Softmax(_{u}W_{M}+(1-_{u})W_{G})E_{u_{t}}, \]

where \(_{u}\) is a trade-off parameter. Like constructing current historical item attributes, we first sample attributes at the corresponding probabilities to form \(_{t} R^{n_{1} d}\), then combine it with previous historical user attributes \(G_{t-1} R^{n_{2} d}\) to construct current historical user attributes \(G_{t} R^{N_{G} d}\), where \(N_{G}=n_{1}+n_{2}\). The item and user representations are fed into the relevance prediction layer. Given a user feature \(e_{u_{t}}\) and an item feature \(e_{u_{t}}\), we define a user-item relevance \(r_{u_{t},p_{t}}=e_{u_{t}}e_{u_{t}^{T}}\). A larger \(r\) means a user is more likely attracted by an item. Following (Wang et al., 2019; Wang et al., 2019), we utilise AdamW as the optimiser and cross entropy as the loss function.

**Candidate Item List Generation.** Given a user \(u\) and her candidate category arrangement \(<c_{1}, c_{q}>\), we compute user-item relevance scores by \(q\) steps. When \(t=1\), we first feed the user, items belonging to \(c_{1}\) and current type \(c_{1}\) into two M-Merge models. Especially, the historical attributes for an item and a user \(H_{0}\) and \(G_{0}\) are initialized as empty sets. After we achieve user and item features, we select the top-\(k_{2}\) items with high relevance scores into the candidate list \(R_{c_{1}}\) for the first activity. In addition, for each selected item, we obtain a pair of current historical attributes set \(_{1}\) and \(_{1}\). Accordingly, we derive \(H_{1}\) and \(G_{1}\) by combining the corresponding sets as the input at step \(t=2\). Extending the process to step \(t\), we generate a candidate item list \(R_{t}\). For the given arrangement, we generate a set of item lists \(R=\)\(R_{1},,R_{q}>\). The detailed algorithm ACA is shown in Appendix A.2. In practice, new items and session records could appear over platform. We dynamically and incrementally maintain the models of MASP to reflect the most recent social updates. We adopt fine-tune training strategy (Wang et al., 2019) to jointly update the HeterBERT and M-merge on the new data.

### Multi-constraints Session Generation

We propose a naive multi-constraint session generation (MCSG), and optimizations for fast candidate item generation and MCSG.

#### 4.3.1. Naive MCSG

With HeterBERT and motivation-aware preference prediction, we achieve candidate item lists for different categories under given arrangements. As Figure 2 (c) shows, for each candidate arrangement, we first regard its item lists as a \(q\)-partite graph to \(q\) independent sets. The task of generating a session can be converted into finding a path through each set sequentially. Then, our goal is to generate sessions that align with users' interests, meet their requirements for activity arrangement, and avoid breaching constraints. We evaluate each path by the user preference, correlation of adjacent items, and constraints. Given a user \(u\) and a path \(p_{u}=c_{1},,p_{q}>\), the user preference is described by the relevance score \(r_{u,u}\), the correlation is computed by \(e_{u}^{T}e_{u_{t}}\), and constraints filters improper paths. The session score is computed as follows:

\[f_{score}(p_{u})=_{i=1}^{q}(r_{u,u_{t}})+_{j=1}^{q-1}(e_{u_{t}}^{T}e_{u _{j+1}}). \]

The MCSG includes path generation and evaluation. For each item list \(^{(i)}\), we generate possible paths based on its distance and time constraints. We compute the spherical distance of adjacent items, check the distance constraint (\(rad=1\) km) and the available time of these items, as in (Chen et al., 2019; Wang et al., 2019). We evaluate each path by Eq. 6 and select top-\(K\) paths for the \(i^{th}\) candidate arrangement.

#### 4.3.2. Optimisation

We propose attribute-based candidate items generation to accelerate the item list generation and a greedy algorithm to accelerate the session generation.

Figure 7. M-Merge model.

[MISSING_PAGE_FAIL:7]

### Efficiency Evaluation

#### 5.3.1. Effect of Optimisation

We first test the effect of the proposed optimisation and report the response time in Table 3. Clearly, the combination of two optimisations achieves the best efficiency, followed by ILG optimisation. This is because ILG could quickly filter out the items that do not share attributes with a target user. On the other hand, MCSG optimisation improves efficiency slightly. This is because Greedy-MCSG balances the information loss and efficiency, leading to a slight improvement in efficiency. MASP incurs the highest time cost, which proves the importance of optimisation.

#### 5.3.2. Efficiency Comparison

We compare our MASP and MASP-OPT with SOTA methods, GNAAutoScale and ToCoSeRec, in terms of overall time cost. The the experimental results over three datasets are reported in Table 4. Clearly, our MASP-OPT is much faster than MASP, GNAAutoScale and ToCoSeRec. This is because the proposed ILG algorithm significantly reduces the candidate items for computing the relevance scores and Greedy-MCSG optimisation further accelerates the session generation. GNAAutoScale is slightly faster than MASP and ToCoSeRec because it generates sessions based only on item types, whereas MASP and ToCoSeRec consider both item types and correlations between items.

#### 5.3.3. Time Cost of Model Update

We take the model update in two ways: fully re-training the learned model over new and old data and fine-tuning the trained model over new data. We split datasets into training, validation, new data, and test sets, as in Sec 5.2.2. We use 100% of new data to fine-tune the models of MASP for Yelp, Yelp-L, and Douban. For fine-tuning-based method, the time costs for the model update are 12.2 minutes, 50.7 minutes, and 31.4 minutes, respectively. For the re-training, it requires 16 hours, 65 hours, and 39 hours. Compared to re-training, the incremental strategy improves model update efficiency by up to about 75 times. Thus the time cost of model updates in MASP is well controlled.

### Ablation Study

We conduct the ablation study to evaluate the impact of three main components of our MASP model: input embedding layer, C-Merge, double threshold mechanism, and M-Merge. Figures 9 -10 depict our ablation analysis's experimental results over three datasets.

#### 5.4.1. Input Embedding Layer

We compare the HR@K results of MASP for two settings: the full version of the proposed input embedding (Full) and the naive input embedding of the original BERT (Naive embedding). Figure 9 displays the effectiveness comparison results. With the proposed input embedding, MASP outperforms the model with naive input embedding across all K values, which indicates that our input embedding can handle the heterogeneity at attribute and type levels, thus enhancing the effectiveness of MASP.

#### 5.4.2. C-Merge Layer

We compare the HR@K results of MASP obtained under two settings: full version (Full) and None C-Merge layers (None-Merge). As shown in Figure 9, the model's performance significantly declines without the C-Merge layer. This proves that C-Merge has strong capability to effectively capture the correlation between types and items.

#### 5.4.3. Double threshold mechanism

We evaluate the effect of the double threshold mechanism by conducting tests in two settings: full version (Full) and None-Threshold. As shown in Figure 9, the model's performance slightly declines without the double threshold mechanism, indicating that the noise in the word-level and attribute-level weights are limited and can be filtered out by our mechanism.

#### 5.4.4. M-Merge

We evaluate the effect of M-Merge in motivation-aware preference prediction by conducting tests in three settings: full version of MASP (Full), only merging items' attributes (V-Merge), and only merging users' attributes (U-Merge). As shown in Figure 10, the full version of MASP outperforms other variants across all K values for all datasets. This is because M-Merge combines current and historical attributes for both users and items, enhancing the representativeness of the user and item embeddings. In addition, U-Merge consistently outperforms V-Merge, which indicates that M-Merge has a greater impact on user representations.

## 6. Conclusion

This paper proposes a new SPHP problem and a novel framework MASP for SPHP. We first propose a novel HeterBERT for heterogeneous item presentation. Then we propose a motivation-aware model for user preference prediction. Finally, we propose an MCSG algorithm together with two optimisation strategies for efficient session generation. Extensive tests over three real datasets prove that MASP outperforms the SOTA methods in terms of efficacy.

  Time (s) & MASP & MASP-ILG & MASP-MCSG & MASP-OPT \\  Yelp & 21.2 & 4.5 & 19.5 & **3.9** \\  Yelp-L & 141.8 & 45.2 & 120.5 & **33.4** \\  Douban & 37.5 & 9.2 & 35.2 & **5.8** \\  

Table 3. Optimisation comparison.

Figure 10. Ablation study on M-Merge.

Figure 9. Ablation study on input embedding, C-Merge, thresholds.