# Local Centrality Minimization with Quality Guarantees

Anonymous Author(s)

###### Abstract.

Centrality measures, quantifying the importance of vertices or edges, play a fundamental role in network analysis. To date, triggered by some positive approximability results, a large body of work has been devoted to studying centrality maximization, where the goal is to maximize the centrality score of a target vertex by manipulating the structure of a given network. On the other hand, due to the lack of such results, only very little attention has been paid to centrality minimization, despite its practical usefulness.

In this study, we introduce a novel optimization model for local centrality minimization, where the manipulation is allowed only around the target vertex. We prove the NP-hardness of our model and that the most intuitive greedy algorithm has a quite limited performance in terms of approximation ratio. Then we design two effective approximation algorithms: The first algorithm is a highly-scalable algorithm that has an approximation ratio unachievable by the greedy algorithm, while the second algorithm is a bicriteria approximation algorithm that solves a continuous relaxation based on the Lovasz extension, using a projected subgradient method. To the best of our knowledge, ours are the first polynomial-time algorithms with provable approximation guarantees for centrality minimization. Experiments using a variety of real-world networks demonstrate the effectiveness of our proposed algorithms: Our first algorithm is applicable to million-scale graphs and obtains much better solutions than those of scalable baselines, while our second algorithm is rather strong against adversarial instances.

**ACM Reference Format:**

Anonymous Author(s). 2018. Local Centrality Minimization with Quality Guarantees. In _Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym 'XX')_. ACM, New York, NY, USA, 13 pages. [https://doi.org/XXXXXXXXXXXX](https://doi.org/XXXXXXXXXXXX)

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

## 1. Introduction

Among the many analytical tools that social network analysis (Shen et al., 2015) borrowed from graph theory, centrality measures play a fundamental role in a wide variety of analyses (Gardner et al., 2016). Centrality, which quantifies the _importance_ of vertices or edges only based on the graph structure, has found many applications, including, e.g., identification of important users or connections in social networks, community detection (Shen et al., 2015), anomaly detection (Zhu et al., 2016), to name a few.

_Local centrality minimization_ is the problem of removing a few existing edges around a target vertex, so as to minimize its centrality score. A direct application can be found in the context of reducing the visibility, or influence, of a targeted harmful user in a social network, without explicitly blocking the user account. The edges to be removed, identified by local centrality minimization, are the most important edges for the centrality (i.e., visibility or influence) of the target vertex. In this regard, another direct application is to keep satisfying influential users so that they are engaged in the platform. The degree of satisfaction of such influencers often depends on how actually influential they are in the platform, i.e., how much their content is consumed by the network. Local centrality minimization can be used for revealing the most important connections between the influencers and their followers, which are key in contributing to their visibility.

While a large body of work has been devoted to studying _centrality maximization_(Gardner et al., 2016), much less attention has been paid to centrality minimization (see Section 2 for a brief literature survey). Generally speaking, the goal of centrality maximization is to maximize the centrality score of a target vertex by adding a limited number of edges to the network. In many reasonable optimization models, the objective function becomes monotone and submodular (Gardner et al., 2016), and thus a simple greedy algorithm admits a \((1-1/e)\)-approximation (Zhou et al., 2016). This positive result makes the basis of various studies on centrality maximization (Bouvier et al., 2015; Goyal et al., 2016; Goyal et al., 2016; Goyal et al., 2016; Goyal et al., 2016).

The lack of positive approximation results has instead limited the attention on the centrality minimization problem, especially in its local variant. Waniek et al. (Waniek et al., 2016) introduced several optimization models for local centrality minimization under some specific objectives and constraints, investigated the computational complexity of their models, and devised some algorithms. Later, Waniek et al. (Waniek et al., 2016) investigated local centrality minimization from a game-theoretic point of view. However, the work by Waniek et al. (Waniek et al., 2016; Waniek et al., 2016) proposed only (exponential-time) exact algorithms and heuristics.

In this paper, we study the local centrality minimization problem, adopting the most well-established centrality measures called the _harmonic centrality_(Gardner et al., 2016), which quantifies the importance of vertices based on the level of reachability from the other vertices. The harmonic centrality is known as an effective alternative to the closeness centrality (Bouvier et al., 2015), which was employed in Waniek et al. (Waniek et al., 2016), in the sense that unlike the closeness centrality, it is well-defined even in the case where a graph is not strongly connected.

Boldi and Vigna (Bouvier et al., 2015) showed that among all the known centrality measures, only the harmonic centrality satisfies all the desirable axioms, namely the size axiom, density axiom, and score monotonicity axiom. Recently, Murai and Yoshida (Murai and Yoshida, 2016) theoretically and empirically demonstrated that among well-known centrality measures, the harmonic centrality is most stable (thus reliable) against the uncertainty of a given graph.

### Paper contributions and roadmap

In this paper, we introduce a novel optimization model for local centrality minimization, where the harmonic centrality is employed as an objective function. Specifically, in our model, given a directed graph \(G=(V,A)\), a target vertex \(v V\), and a budget \(b_{>0}\)we aim to find a set of incoming edges of \(v\) with size (no greater than) \(b\) whose removal minimizes the harmonic centrality score of \(v\), denoted by \(h_{G}(v)\) (to be defined in Section 3).

For our optimization model, we first analyze the computational complexity. Specifically, we show that our model is NP-hard even on a very limited graph class (i.e., acyclic graphs), by constructing a polynomial-time reduction from the minimum \(k\)-union problem. Furthermore, we prove that the most intuitive greedy algorithm, which iteratively removes an incoming edge of \(v\) that maximally decreases the objective value, cannot achieve an approximation ratio of \(o(|V|)\), while any reasonable algorithm has an approximation ratio of \(O(|V|)\). This negative result motivates the design of algorithms that exploit the characteristics of our model.

We design two polynomial-time approximation algorithms. The first algorithm is a highly-scalable algorithm that has an approximation ratio of \((v)}\). We stress that as \((v)}=O()=o(|V|)\), this approximation ratio is unachievable by the above greedy algorithm. Our algorithm first sorts the incoming neighbors of the target vertex \(v\) in the decreasing order of their harmonic centrality scores on a slightly modified graph, and then removes \(b\) incoming edges from the top-\(b\) vertices in the sorted list. To prove the approximation ratio, we scrutinize the relationship between the harmonic centrality scores of the target vertex and its incoming neighbors. In the end, we also prove the tightness of our analysis of the approximation ratio.

The second algorithm is a polynomial-time algorithm that has a bicriteria approximation ratio of \((,(,e))\) for any \((0,1)\) and \(e>0\). That is, the algorithm finds a subset of incoming edges of the target vertex \(v\) with size at most \(b/\) but attains the objective value at most the original optimal value times \(\) plus \(e\). Therefore, the algorithm approximates the original optimal value while violating the budget constraint to some bounded extent. To design the algorithm, we first introduce a continuous relaxation of our model. To this end, we use the well-known extension of set functions, called the Lovasz extension (Lovasz, 1965). An important fact is that the objective function of our model is submodular, which guarantees that its Lovasz extension is (not necessarily differentiable but) convex. Therefore, we can solve the relaxation (with an arbitrarily small error) using a projected subgradient method (Baudoin, 1994). Once we get a fractional solution, we apply a simple probabilistic procedure and obtain a subset of incoming edges of the target vertex \(v\).

Finally, our experiments on a variety of real-world networks show that our first algorithm is applicable to million-scale graphs and obtains much better solutions than those of scalable baselines, while our second algorithm is strong against adversarial instances.

In summary, our contributions are as follows:

* We study the _local harmonic centrality minimization problem_: We prove that it is NP-hard even on acyclic graphs, its objective function is submodular, and the most intuitive greedy algorithm cannot achieve \(o(|V|)\)-approximation (Section 3).
* We devise a highly-scalable algorithm with an approximation ratio of \((v)}\), unachievable by the greedy algorithm.
* We then devise a bicriteria approximation algorithm that solves a continuous relaxation based on the Lovasz extension, using a projected subgradient method (Sections 5 and 6).

To the best of our knowledge, ours are the first polynomial-time algorithms with provable approximation guarantees for centrality minimization.

## 2. Related Work

In this section, we review related literature about centrality minimization and maximization, submodular minimization, and other relevant applications in social networks analysis.

**Centrality minimization**. The most related to the present paper is the work on centrality minimization by Waniek et al. (Waniek et al., 2016; Waniek et al., 2016) whose goal is to provide a methodology that contributes to hiding individuals in social networks from centrality-based network analysis algorithms.

More specifically, Waniek et al. (Waniek et al., 2016) introduced the following optimization model: Given a directed graph \(G=(V,A)\), a target vertex \(v V\), a budget \(b_{>0}\), and a set \(M\) of possible single-edge modifications, we are asked to select at most \(b\) actions in \(M\) so as to minimize the centrality score of \(v\) while satisfying some constraint on the influence of some vertices in the graph. As a centrality measure, they considered the degree centrality, closeness centrality, and betweenness centrality. They showed that except for the degree centrality case, the above model is NP-hard even when the constraint on the influence of vertices is ignored, and devised simple heuristics.

Waniek et al. (Waniek et al., 2016) investigated local centrality minimization from a game-theoretic point of view. As a tool to analyze their game, they studied the following optimization model: Given a directed graph \(G=(V,A)\), a target vertex \(v V\), a hiding parameter \(\), and a set \(M\) as above, we are asked to find a minimal subset of \(M\) guaranteeing that there are at least \(\) vertices having a centrality score greater than that of \(\). As a centrality measure, they again considered the above three. They showed that the model is \(2\)-approximable for the degree centrality but is inapproximable within any logarithmic factor for the other two. Note that the above approximation is just for the size of the output rather than the ranking of the centrality score of \(v\) (or the centrality score of \(v\)).

Veremyev et al. (Veremyev et al., 2016) studied a global centrality minimization problem: Given an undirected graph \(G=(V,E)\) with cost \(c_{e}_{ 0}\) for each \(e E\), a target vertex subset \(S V\), and a budget \(b_{>0}\), we are asked to find \(F E\) whose removal minimizes the centrality score of the target vertex subset \(S\) subject to the budget constraint \(_{e F}c_{e} b\). The centrality score of a vertex subset is defined as a generalization of the centrality score of a vertex. As a centrality measure, they considered a quite general one, based on distance between vertices, which includes the harmonic centrality as a special case. They proved that the above model is NP-hard for any centrality measure included in the above, and as a by-product of the analysis, they also mentioned the NP-hardness of its local variant, which coincides with (the undirected-graph counterpart of) our proposed model. In the present paper, by focusing on the harmonic centrality, we prove that our model is NP-hard even on a very limited graph class (i.e., acyclic graphs). On a positive side, they presented an exact algorithm based on mathematical programming and greedy heuristics. Very recently, Liu et al. (Liu et al., 2019) addressed another global centrality minimization problem, where the objective function is a centrality measure called the information centrality and the connectivity of the resulting graph is guaranteed.

**Centrality maximization.** Centrality maximization has more actively been studied in the literature (e.g., (Bahmalkar et al., 2017; Goyal et al., 2017; Goyal et al., 2018; Goyal et al., 2019; Goyal et al., 2019)), where the most related to ours is due to Crescenzi et al. (Goyal et al., 2017). They introduced the harmonic centrality maximization problem, where given a directed graph \(G=(V,A)\), a target vertex \(v V\), and a budget \(b_{>0}\), we are asked to insert at most \(b\) incoming edges of \(v\) so as to maximize the harmonic centrality score of \(v\). Our proposed optimization model can be seen as a minimization counterpart of their problem. They proved that the problem is APX-hard, but devised a polynomial-time (\(1-1/\))-approximation algorithm based on the submodularity of the objective function. Finally, we note that there is another class of problems also called centrality maximization, where the goal is to find \(S V\) that has the maximum group centrality score (Bahmalkar et al., 2017; Goyal et al., 2018; Goyal et al., 2019; Goyal et al., 2019; Goyal et al., 2019; Goyal et al., 2019; Goyal et al., 2019; Goyal et al., 2019; Goyal et al., 2019), which is less relevant to the present paper.

**Submodular minimization.** Submodular minimization is one of the most well-studied problem classes in combinatorial optimization. Among the literature, the most related work is due to Svitkina and Fleischer (Svitkina and Fleischer, 2017). They stated that a polynomial-time (\(}\), \(}\))-bicriteria approximation algorithm for submodular minimization with a cardinality upper bound (and thus for our proposed model) is possible for any \((0,1)\), using techniques in Hayrapetyan et al. (Hayapetyan et al., 2017). However, Hayapetyan et al. (Hayapetyan et al., 2017) did not consider the problem: They addressed the problem called the minimum-size bounded-capacity cut, where the function in the constraint instead of the objective function is submodular. Therefore, the above statement is not trivial even our proposed model should be handled in a formal way. Lovasz extension has actively been used for developing novel network analysis algorithms (e.g., (Zhou et al., 2019; Goyal et al., 2019; Goyal et al., 2019)).

**Applications.** Reducing the visibility or influence of target users in social networks has been studied in the context of influence minimization (Goyal et al., 2019; Goyal et al., 2019; Goyal et al., 2019). All of existing studies are based on some influence diffusion models such as the independent cascade model (Goyal et al., 2019; Goyal et al., 2019) and the linear threshold model (Goyal et al., 2019). Unlike those, our model does not assume any influence diffusion model, but is just based on the network structure. Very recently, Fabbri et al. (Fabbri et al., 2019) and Couette et al. (Couette et al., 2019) addressed the problem of reducing the exposure to harmful contents in social media networks.

On the other hand, identifying the users and/or connections that play a key role for user engagement in social networks has also attracted much attention. Bhawalkar et al. (Bhawalkar et al., 2019) initiated this kind of study from an optimization perspective. They invented a model that aims to find a group of users whose permanent use of the service guarantees user engagement as much as possible, and designed polynomial-time algorithms for some cases. Later, Zhang et al. (Zhang et al., 2019) and Zhu et al. (Zhu et al., 2020) introduced variants of the above model, and devised intuitive heuristics.

## 3. Problem Formulation and Characterization

In this section, we mathematically formulate our problem (Problem 1), and prove its NP-hardness (Theorem 1) and the submodularity of the objective function (Theorem 2). Finally, we show the quite limited performance of the greedy algorithm (Theorem 3).

Let \(G=(V,A)\) be a directed graph (or _digraph_ for short). Throughout the paper, we assume that digraphs are simple, that is, there exist neither self-loops nor multiple edges. For \(F A\), we define \(G F\) as the subgraph of \(G\) that is constructed by removing all edges in \(F\) from \(G\), i.e., \(G F=(V,A F)\). For \(v V\), we denote by \((v)\) the set of incoming edges of \(v\), i.e., \((v)=\{(u,v) A u V\}\). For \(v V\), let \(h_{G}(v)\) be the harmonic centrality score of \(v\) on a digraph \(G\), i.e.,

\[h_{G}(v)=_{u V\{v\}}(u,v)},\]

where \(d_{G}(u,v)\) is the (shortest-path) distance from \(u V\) to \(v V\) on \(G\), and by convention, \(d_{G}(u,v)=\) when \(v\) is not reachable from \(u\). Note that, contrarily to other centrality measures, even in the case where a digraph is not strongly connected, the harmonic centrality is still well-defined (assuming by convention that \(1/=0\)). Intuitively, the harmonic centrality quantifies the importance of a given vertex \(v\) based on the level of reachability from the other vertices. The problem we tackle in this paper is formalized as follows:

**Problem 1** (Local harmonic centrality minimization).: _Given a digraph \(G=(V,A)\), a target vertex \(v V\), and a budget \(b_{>0}\), we are asked to find \(F(v)\) with \(|F| b\) whose removal minimizes the harmonic centrality of \(v V\), i.e., \(f_{(G,b)}(F)=h_{G F}(v)\)._

By constructing a polynomial-time reduction from the NP-hard optimization problem called the _minimum \(k\)-union_, we can prove the following. The proof can be found in Appendix A.1.

**Theorem 1**.: _Problem 1 is NP-hard even on acyclic graphs._

We next show that the objective function \(f_{(G,)}\) of Problem 1 is submodular, which helps us design our bicriteria approximation algorithm in Section 5. Let \(S\) be a finite set. A set function \(f 2^{S}\) is said to be _submodular_ if for any \(X,Y S\), it holds that

\[f(X)+f(Y) f(X Y)+f(X Y).\]

We prove the following in Appendix A.2:

**Theorem 2**.: _For any \(G=(V,A)\) and \(v V\), the objective function \(f_{(G,)}\) of Problem 1 is submodular._

Finally, we prove that the most intuitive greedy algorithm has a quite limited performance in terms of the approximation ratio. Specifically, we consider the algorithm that iteratively removes an incoming edge of the target vertex \(v\) that maximally decreases the harmonic centrality score of \(v\), until it exhausts the budget. For reference, the pseudo-code is given in Algorithm 4 in Appendix A.3. This algorithm runs in \(O(b|(v)|(|V|+|A|))\) time. Note that, unlike many submodular maximization algorithms, the _lazy evaluation_ technique (Goyal et al., 2019) cannot be used to obtain a practically efficient implementation. The proof of the following is available in Appendix A.4.

**Theorem 3**.: _The greedy algorithm has no approximation ratio of \(o(|V|)\) for Problem 1, while any algorithm that outputs \(F(v)\) with \(|F|=b\) has an approximation ratio of \(O(|V|)\)._

## 4. Scalable Approximation Algorithm

In this section, we present a highly-scalable \((v)}\)-approximation algorithm for Problem 1.

### Algorithm

Let \(N_{}(o)\) be the set of incoming neighbors of \(o\), i.e., \(N_{}(o)=\{w V(w,o)(o)\}\). The intuition behind our algorithm is quite simple: As long as there exists a vertex \(w N_{}(o)\) that has a large harmonic centrality score, so does the target vertex \(o\). This means that it is urgent to remove incoming edges of \(o\) that come from vertices having large harmonic centrality scores. Note that our algorithm and analysis consider the harmonic centrality scores on \(G(o)\) rather than \(G\); this is essential to obtain our approximation ratio. Specifically, our algorithm first sorts the elements of \(N_{}(o)\) as \((w_{1},,w_{|(o)|})\) so that \(h_{G(o)}(w_{1}) h_{G(o)}(w_{|(o)|})\) and just returns \(\{(w_{1},o),,(w_{|(o)|})\}\). For reference, the entire procedure is described in Algorithm 1.

The algorithm is highly scalable. Indeed, the time complexity of Algorithm 1 is dominated by the part of computing the harmonic centrality scores of vertices in \(N_{}(o)\), which just takes \(O(|(o)|(|V|+|A|))\) time. Therefore, the algorithm is asymptotically \(b\) times faster than the greedy algorithm (Algorithm 4).

### Analysis

From now on, we analyze the approximation ratio of Algorithm 1. The following lemma demonstrates that the optimal value can be lower bounded using the maximum harmonic centrality score over the remaining incoming neighbors of \(o\) in the resulting graph:

**Lemma 1**.: _Let \(F^{*}\) be an optimal solution to Problem 1 and \(N^{*}\) the vertex subset corresponding to \(F^{*}\), i.e., \(N^{*}=\{w V(w,o) F^{*}\}\). Then it holds that_

\[f_{(G,o)}(F^{*})(_{w N_{}(o)  N^{*}}h_{G(o)}(w)+1).\]

Proof.: For any \(w N_{}(o) N^{*}\), we have

\[f_{(G,o)}(F^{*})=h_{G F^{*}}(o)=_{w V \{o\}}}(u,w)}\] \[_{w V\{o\}}}(u,w )+1}=1+_{w V\{w,o\}}}(u,w)+1}\] \[ 1+_{w V\{w,o\}}(u,w)} 2+_{w V\{w\}}}(u,w)}\] \[=(h_{G F^{*}}(w)+1)(h_{G(o)}(w)+1),\]

where the first inequality follows from the triangle inequality of distance \(d_{G F^{*}}\), the third equality follows from \(d_{G F^{*}}(w,w)=0\), and the second inequality follows from the fact that the addition of \(1\) to the denominator makes it at most twice the original. The arbitrariness of the choice of \(w N_{}(o) N^{*}\) derives the statement. 

On the other hand, the next lemma upper bounds the objective value of the output of Algorithm 1 using the harmonic centrality scores of the remaining incoming neighbors of \(v\) in the resulting graph:

**Lemma 2**.: _Let \(F_{}\) be the output of Algorithm 1 and \(N_{}\) the vertex subset corresponding to \(F_{}\), i.e., \(N_{}=\{w V(w,o) F_{}\}\). Then we have_

\[f_{(G,o)}(F_{})(|(o)|-b)+_{w N_{}(o) N _{}}h_{G(o)}(w).\]

Proof.: On digraph \(G F_{}\), any \(u V\{v\}\) satisfies either (i) there exists no (shortest) path from \(u\) to \(v\) or (ii) there exists \(w(u) N_{}(o) N_{}\) that is contained in a shortest path from \(u\) to \(v\), i.e., \(d_{G F_{}}(u,w)=d_{G F_{}}(u,w(u))+1\). Let \(V^{} V\{v\}\) be the subset of vertices that satisfy the condition (ii). Then we have

\[f_{(G,o)}(F_{}) =h_{G F_{}}(o)=_{u V\{v\}} }}(u,v)}\] \[=_{w V\{v\}}}} (u,w(u))+1}. \]

We see that the shortest path corresponding to \(d_{G F_{}}(u,w(u))\) does not contain \(o\) (and thus any edge in \((o) F_{}\)). Otherwise there would exist \(w^{}(u) N_{}(o) N_{}\) satisfying that \(d_{G F_{}}(u,w^{}(u))<d_{G F_{}}(u, w(u))\), which contradicts the fact that \(w(u)\) is contained in a shortest path from \(u\) to \(v\) on \(G F_{}\). Hence, we have

\[d_{G F_{}}(u,w(u))=d_{G(o)}(u,w(u)).\]

Combining this with the equality (1), we have

\[f_{(G,o)}(F_{})=_{u V^{}\{v\}} (u,w(u))+1}\] \[=(|(o)|-b)+_{u V^{}\{v\}}_{(N_{ }(o) N_{})}(u,w(u))+1}\] \[<(|(o)|-b)+_{u V^{}\{v\}}_{(N_{ }(o) N_{})}(u,w(u))}\] \[(|(o)|-b)+_{w N_{}(o) N_{} }h_{G(o)}(w),\]

where the last inequality holds by the fact that any term \((u,w(u))}\) in the summation of the left-hand-side appears as a term in \(h_{G(o)}(w)\) for appropriate \(w=w(u)\) in the right-hand-side. Therefore, we have the lemma. 

We are now ready to prove our main theorem:

**Theorem 4**.: _Algorithm 1 is a \(2(|(o)|-b)\)-approximation algorithm for Problem 1._

Proof.: Here we use the notation that appeared in Lemmas 1 and 2. By the behavior of Algorithm 1, we have

\[_{w N_{}(o) N_{}}h_{G (o)}(w)_{w N_{}(o) N^{*}}h_{G(o )}(w).\]Using Lemmas 1 and 2 together with this inequality, we have

\[f_{G,}(F_{})(|(v)|-b)+_{w N_{}() N_{}}h_{G()}(w)\] \[(|(v)|-b)(1+_{w N_{}() N _{}}h_{G()}(w))\] \[(|(v)|-b)(1+_{w N_{}() N _{}}h_{G()}(w))\] \[(|(v)|-b)(1+2f_{(G,)}(F^{*})-1)=2(|(v )|-b)f_{(G,)}(F^{*}),\]

which completes the proof. 

Based on the theorem, we obtain the desired approximation ratio:

**Corollary 1**.: _Algorithm 1 is a \((o)}\)-approximation algorithm for Problem 1._

For any instance that satisfies \(b=|(v)|\), Algorithm 1 outputs the trivial optimal solution (i.e., \((v)\)). Therefore, in what follows, we focus only on the instances with \(b<|(v)|\). Obviously the output of any algorithm for Problem 1 has an objective value at most \(h_{G}(v)\). On the other hand, the optimal value is at least \(|(v)|-b\) because in the resulting digraph, there are still \(|(v)|-b\) incoming neighbors of \(v\), each of which contributes exactly 1 to the objective value. Therefore, any algorithm (including Algorithm 1) for Problem 1 has an approximation ratio of \((v)}{|(v)|-b}\). By combining this with Theorem 4, the approximation ratio of Algorithm 1 can be improved to

\[\{2(|(v)|-b),\ (v)}{|(v)|-b}\}(v)},\]

which presents the statement. 

It should be remarked that as \((v)}=o(|V|)\), the approximation ratio is unachievable by the greedy algorithm. Finally, we conclude this section by showing that the analysis of the approximation ratio is tight up to a constant factor. The proof is available in Appendix B.1.

**Theorem 5**.: _Algorithm 1 has no approximation ratio of \(o((v)})\)._

## 5. Bicriteria Approximation Algorithm

In this section, we present a polynomial-time \((,(,))\)-bicriteria approximation algorithm (\((0,1)\) and \(>0\)) for Problem 1. Our algorithm first solves a continuous relaxation of the problem and then applies a simple probabilistic procedure to the fractional solution to obtain the output.

### Continuous relaxation

To obtain a continuous relaxation of Problem 1, we consider the well-known extension of set functions, called the Lovasz extension (Lovasz, 1977). For our objective function \(f_{(G,)}\), the Lovasz extension \(_{(G,)}\): \(^{(v)}\) is defined in the following way: Let \((v)=\{_{1},,_{|(v)|}\}\). For \(x^{(v)}\), we relabel the elements of \((v)\) so that \(x_{x_{1}} x_{v_{2}} x_{v_{|(v)|}}\), and construct a sequence of subsets \(=X_{0} X_{1} X_{|(v)|}=(v)\), where \(X_{i}=\{_{1},,_{i}\}\) for \(i=1,,|(v)|\). Based on these, we define the value of \(_{(G,)}(x)\) as follows:

\[_{(G,)}(x)=(1-x_{_{1}})f_{(G,)}( )\] \[+_{i=1}^{|(v)|-1}(x_{_{i+1}}-x_{_{i+1}})f _{(G,)}(X_{i})+x_{v_{|(v)|}}f_{(G,)}((v)).\]

Observe that for any \(F(v)\), it holds that \(_{(G,)}(1_{F})=f_{(G,)}(F)\), where \(1_{F}\) is an indicator vector of \(F\), taking \(1\) if \(e F\) and \(0\) otherwise. Therefore, \(_{(G,)}\) is indeed an extension of \(f_{(G,)}\).

The Lovasz extension can be defined on any (not necessarily submodular) set function. The Lovasz extension is always continuous but not necessarily differentiable. An important fact is that the Lovasz extension is convex if and only if the original set function is submodular (Lovasz, 1977). Therefore, by Theorem 2, the Lovasz extension \(_{(G,)}\) of \(f_{(G,)}\) is convex.

Using \(_{(G,)}\), we introduce our continuous relaxation as follows:

\[_{(G,)}(x)\] \[\|x\|_{1} b,\ x^{(v)}.\]

For convenience, we denote by \(C\) the feasible region of the problem, i.e.,

\[C\{x^{(v)}:\|x\|_{1} b\ \ x^{(v)}\}.\]

From the above, we see that Relaxation is a non-smooth convex programming problem. We will present an algorithm for Relaxation and its convergence result in Section 6. In the remainder of this section, we assume that for \(^{}=(1-)\), we can compute, in polynomial time, an \(^{}\)-additive approximate solution for Relaxation, i.e., a feasible solution for Relaxation that has an objective value at most the optimal value plus \(^{}\).

### Algorithm

Let \(^{*}^{(v)}\) be an \(^{}\)-additive approximate solution for Relaxation, where \(^{}=(1-)\). Then our algorithm picks \(p[,1]\) uniformly at random and just returns \(\{e(v) x_{e}^{*} p\}\). For reference, the entire procedure is summarized in Algorithm 2.

```
Input :\(G=(V,A)\), \(v V\), and \(b_{>0}\) Output :\(F(v)\)
1\(^{}(1-)\);
2 Solve Relaxation (using Algorithm 3 in Section 6) and obtain its \(^{}\)-additive approximate solution \(^{*}^{(v)}\);
3 Pick \(p[,1]\) uniformly at random;
4return\(\{e(v) x_{e}^{*} p\}\);
```

**Algorithm 2**\((,(,))\)-bicriteria approximation algorithm for Problem 1

### Analysis

The following theorem gives the bicriteria approximation ratio of Algorithm 2:```
Input :\(_{0} C\) and some stopping condition Output :\( C\)
1\(t 0\);
2whilethe stopping condition is not satisfieddo
3 Pick a stepsize \(_{t}>0\) and a subgradient \(_{(G,)}(_{t})\) of \(_{(G,)}\) at \(_{t}\);
4\(x_{t+1}*{proj}_{C}(_{t}-_{t}^{}(_{t}))\) and \(t t+1\);
5return\(_{t}\);
```

**Algorithm 3**Projected subgradient method for Relaxation

**Theorem 6**.: _For any \((0,1)\) and \(>0\), Algorithm 2 is a polynomial-time \((,(,))\)-bicriteria approximation algorithm for Problem 1._

Proof.: Let \(F(v)\) be the output of Algorithm 2. The approximation ratio with respect to the size of \(F\) can be evaluated as follows:

\[[|F|]=[_{e F} ][_{e F}x_{e }^{*}]_{e(v)}x_{e}^{*},\]

where the first inequality follows from \( p x_{e}^{*}\) for any \( F\), the second inequality follows from the nonnegativity of \(^{*}\), and the third inequality follows from the first constraint in Relaxation.

Next we analyse the approximation ratio with respect to the quality of \(F\). Let \(F^{*}\) be an optimal solution to Problem 1. As Relaxation is indeed a relaxation of Problem 1 and \(x^{*}\) is its \(^{*}\)-additive approximate solution, we have \(_{(G,)}(^{*}) f_{(G,)}(F^{*})+^{}\). For convenience, we define \(x_{0}^{*}=1\) for an imaginary element \(_{0}\). Let \(\) be the maximum number that satisfies \(x_{e}^{*}\). Then we have

\[[f_{(G,)}(F)]=^{-1}(x _{e_{i}}^{*}-x_{e_{i+1}}^{*})f_{(G,)}(X_{i})+(x_{e_{i}}^{*}-)f_ {(G,)}(X_{})}{1-}\] \[^{(v)-1}(x_{e_{i}}^{*}-x_{e _{i+1}}^{*})f_{(G,)}(X_{i})+x_{e_{i+(v)}}^{*}f_{(G,)}( (v))}{1-}\] \[=_{(G,)}(^{*})}{1-} )}(F^{*})+^{}}{1-}=)}(F^{*})}{1-}+,\]

where the first equality follows from the random choice of \(p\) and the first inequality follows from the monotonicity of elements in \(^{*}\) and the nonnegativity of \(f_{(G,)}\). Therefore, we have the theorem. 

## 6. Solving Relaxation

In this section, we present our algorithm for solving Relaxation.

### Algorithm

Specifically, we design a projected subgradient method for Relaxation. The algorithm is an iterative method, where each iteration consists of two parts, i.e., the subgradient computation part and the projection computation part. The pseudo-code is given in Algorithm 3. All the details will be given later. The sequence generated by the algorithm is \(\{_{t}\}_{t 0}\), while the sequence of function values generated by the algorithm is \(\{_{(G,)}(^{t})\}_{t 0}\). As the sequence of function values is not necessarily monotone, we are also interested in the sequence of best-achieved function values at or before \(t\)-th iteration, which is defined as

\[_{}^{()}=_{t=0,1,,}_{(G, {})}(_{t}).\]

**Subgradient computation.** From the definition of \(_{(G,)}\), a subgradient \(_{(G,)}\) at \(_{t} C\) is given by

\[_{(G,)}^{}(_{t})=_{i=1}^{(v) }(f_{(G,)}(X_{i})-f_{(G,)}(X_{i-1})) _{e_{i}}, \]

where \(_{e_{i}}\) is the \((v)\)-dimensional vector that takes \(1\) in the element corresponding to \(e_{i}\) and \(0\) elsewhere. To compute the subgradient \(_{(G,)}^{}(_{t})\), we need to sort the entries of \(_{t}\) and compute \(f_{(G,)}(X_{t})\) for all \(i=0,1,,(v)\), which takes \(O((v)(|V|+|A|))\) time.

**Projection computation.** For a given \(^{(v)}\), it is not trivial how to compute the projection of \(\) onto \(C\) because \(C\) is the intersection of the two sets \(\{^{(v)}\|\|_{1}\}\) and \(\{^{(v)}^{(v)}\}\). For simplicity, define \([,]=\{^{(v)}[ 0,1]^{(v)}\}\). Let \(_{}[]()\) be the projection of \(\) onto \([,]\). Then by Lemma 6.26 in Beck (Beck, 2015), we have

\[_{}[,]()=(\{\{_{ }}}}}}}}}}}}}} }\}},1\})_{()}.\]

Using this projection, we can give the projection of \(\) onto \(C\) as follows:

**Fact 1** (A special case of Example 6.32 in Beck (Beck, 2015)).: _Let \(_{C}()\) be the projection of \(^{(v)}\) onto \(C\). Then we have_

\[_{}()=_{ }[,]()&\|_{}[,]()\|_{1} b,\\ _{}[,](-^{*})& ,\]

_where \(^{*}\) is any positive root of the nonincreasing function \(()=\|_{}[,](- 1)\|_{1}-b\)._

In practice, we can compute the value of \(^{*}\) for \(:=_{t}-_{t}_{(G,)}(_{t})\) using binary search. Assume that the stepsize \(_{t}>0\) is no greater than \(1\) for any iteration \(t=0,1,\), which is indeed the case of ours (specified later). As initial lower and upper bounds on \(^{*}\), we can use \(0\) and \(_{e(v)}x_{e}\), respectively. From the fact that \(_{t}\) is always contained in \(C\) and the definition of the subgradient (2), we see that

\[_{e(v)}x_{e} _{e(v)}x_{e}()+_{t}_{i=1,, (v)-1}f_{(G,)}(X_{i-1})\] \[ 1+f_{(G,)}()|V|.\]

where \(_{t}()\) is the element of \(_{t}\) corresponding to \(\). Therefore, the binary search finds \(^{*}\) in \(O((v)(|V|/))\) time with an additive error of \(>0\). Note that any polynomial-time algorithm cannot recognize an additive error of \(o(2^{-|V|^{*}})\) for constant \(c\), due to its bit complexity. Hence, if we set \(=O(2^{-|V|^{*}})\), we can assume that the projection is exact, and the time complexity is still polynomial.

### Convergence result

Let \(L_{_{(G,)}}=_{(G,)}()\) (\(=f_{(G,)}()\)). Based on the convergence result of the projected subgradient method in Beck (Beck, 2015), which is reviewed in Appendix C.1, we present the convergence result of Algorithm 3 as follows:

**Theorem 7**.: _Let \(\) be an upper bound on the half-squared diameter of \(C\), i.e., \(_{,}\|-\|^{2}\). Determine the stepsize \(_{t}\) (\(t=0,1,\)) as \(_{t}=}{L_{_{(G,)}}}\). Let \(^{}\) be the optimal value of Relaxation. Then for all \(t 2\), it holds that_

\[^{(t)}_{ best}-^{}_{(G,)}}}{}.\]

The proof can be found in Appendix C.2. By this theorem and the above discussion of the time complexity, the following is almost straightforward:

**Corollary 2**.: _Let \(^{}>0\). Set the stopping condition of Algorithm 3 as follows:_

\[t(_{(G,)}}}{ ^{}})^{2}-2.\]

_Then, Algorithm 3 outputs, in polynomial time, an \(^{}\)-additive approximate solution for Relaxation._

## 7. Experimental Evaluation

In this section, we thoroughly evaluate the performance of our proposed algorithms (i.e., Algorithms 1 and 2) using various real-world networks.

### Setup

**Instances.** Table 1 lists real-world digraphs on which our experiments were conducted. All graphs were collected from the webpage of the KONECT Project1. Note that self-loops and multiple edges were removed so that the graphs are made simple. For each graph, we randomly chose 20 vertices as target vertices among those having the in-degree at least 100. The last column of Table 1 gives the statistics of the in-degrees of the target vertices, i.e., the maximum, average, and minimum in-degrees. For each graph and each target vertex \(v\), we vary the budget \(b\) in \(\{|(v)|,|(v)|, |(v)|\}\).

**Baselines.** We employ the following baseline methods:

* Empty: This algorithm just outputs the empty set, thus presenting an upper bound on the objective function value (i.e., \(h_{G}(v)\)) of any feasible solution.
* Random: This algorithm randomly chooses \(b\) edges from \((v)\). For each instance, this algorithm is run 100 times and the average objective value is reported.
* Degree: This algorithm sorts the elements of \(N_{ in}(v)\) as \((w_{1}, w_{|(v)|})\) so that \(|(w_{1})||(w_{|(v)|})|\) and just returns \(\{(w_{1},v),,(w_{b},v)\}\).
* Greedy: Execute the greedy algorithm (Algorithm 4).

**Machine specs and code.** All experiments were conducted on Mac mini with Apple M1 Chip and 16 GB. All codes were written in Python 3.9.

### Performance of algorithms

Here we evaluate the performance of our algorithms. To this end, we run the algorithms together with the baselines for all graphs and budgets. For each graph and each budget, if the algorithm tested does not terminate within 1,200 seconds for the target vertex having the largest in-degree, we do no longer run the algorithm for the graph and the other budgets. Note that Algorithm 3 (in Algorithm 2) is run with stopping condition \(t 1000\) for scalability and initial solution \(_{0}=\).

The quality of solutions of the algorithms except for Algorithm 2 is illustrated in Figure 1. Due to space limitations, only the results for the budget \(b=|(v)|\) are presented here. Although the trend is similar, the results for the other budget settings \(b=|(v)|,|(v)|\) are given in Appendix D.1. As the solutions of Algorithm 2 may violate the budget constraint, it is unfair to compare those with the others in the same plots; thus, the solutions are evaluated later. In the plots in Figure 1, once we fix a value in the vertical axis, we can observe the cumulative number of solutions (i.e., targets) that attain the harmonic centrality score at most the fixed value. Therefore, we can say that an algorithm drawing a lower line has a better performance.

  Name & \(|V|\) & \(|A|\) & Stat. of in-degrees of targets \\    } & 1,224 & 19,022 & (337, 158, 30, 101) \\  & 1,224 & 3,430 & (274, 143, 454, 104) \\  & 1,224 & 3,430 & (361, 152, 50, 100) \\  & 46,591 & 84,379 & (174, 123, 56, 100) \\  & 384,654 & 1,744, & (495, 180, 70, 106) \\  & 1,138,494 & 94,229 & (311, 273, 40, 106) \\  & 1,056,266 & 1,485,859 & (104, 920, 201, 11) \\  & soc-pbcc-relationships & 1,632,803 & 30,622,564 & (316, 157, 35, 101) \\  

Table 1. Real-world graphs used in our experiments.

  Name & \(b\) & Greedy & Algorithm 1 & Algorithm 2 & 737 \\    } & \(|(v)|\) & 4.53 & 0.10 & 323,55 & 738 \\  & \(|(v)|\) & 7.79 & 0.10 & 333,48 & 779 \\  & \(|(v)|\) & 9.75 & 0.10 & 332,48 & 760 \\    } & \(|(v)|\) & 4.40 & 0.12 & 393,72 & 761 \\  & \(|(v)|\) & 7.59 & 0.13 & 462,1 & 762 \\  & \(|(v)|\) & 9.55 & 0.13 & 404,52 & 763 \\    } & \(|(v)|\) & 6.17 & 0.15 & 482,83 & 763 \\  & \(|(v)|\) & 10.39 & 0.15 & 502,63 & 765 \\  & \(|(v)|\) & 13.33 & 0.15 & 489,92 & 765 \\    } & \(|(v)|\) & 4.05 & 0.14 & – & 766 \\  & \(|(v)|\) & 7.03 & 0.14 & – & 767 \\  & \(|(v)|\) & 8.90 & 0.14 & – & 768 \\    } & \(|(v)|\) & – & 4.58 & – & 769 \\  & \(|(v)|\) & – & 4.57 & – & 770 \\  & \(|(v)|\) & – & 4.57 & – & 770 \\    } & \(|(v)|\) & – & 130,56 & – & 771 \\  & \(|(v)|\) & – & 130,54 & – & 772 \\  & \(|(v)|\) & – & 132,19 & – & 773 \\    } & \(|(v)|\) & – & 188,84 & – & 774 \\  & \(|(v)|\) & – & 187,92 & – & 773 \\  & \(|(v)|\) & – & 187,68 & – & 776 \\    } & \(|(v)|\) & – & 402,24 & – & 777 \\  & \(|(v)|\) & – & 392,63 & – & 778 \\    } & \(|(v)|\) & – & 512,05 & – & 778 \\  & \(|(v)|\) & – & 512,05 & – & 778 \\  

Table 2. Computation time (seconds) of the algorithms tested.

As can be seen, Algorithm 1 outperforms the baselines. Indeed, Algorithm 1 is applicable to all graphs tested, thanks to its high scalability, and the quality of solutions is much better than that of the scalable baselines, i.e., Random and Degree. Most interestingly, for the graph citeseer, Algorithm 1 succeeds in reducing the harmonic centrality scores of all target vertices to the values relatively close to 0, although Degree fails to have centrality scores less than 9,000 for five target vertices. Note that this result does not contradict the fact that even an optimal solution has the harmonic centrality score no less than \(|()|-b\): The minimum objective value attained by Algorithm 1 is 54, rather than 0. For small graphs, Greedy performs slightly better than Algorithm 1; however, due to its heavy computation burden, Greedy is not applicable to citeseer and the larger graphs.

The detailed report of the computation time is found in Table 2, where the average computation time over all target vertices is reported. Note that the results for Random and Degree are omitted because Random is obviously quite fast and Degree records 0.00 seconds for all graphs and budgets. We remark that the computation time of Greedy grows roughly proportionally to the budget \(b\), but that of Algorithm 1 remains almost the same for all settings of \(b\).

Finally, the quality of solutions of Algorithm 2 (with \(=,\)), averaged over the target vertices, is reported in Table 3, where each of the objective value and the size of the solutions is a relative value compared with that of the solutions of Algorithm 1. As the rounding procedure of the algorithm contains randomness, we run the procedure 100 times and report the average value. As can be seen, Algorithm 2 returns very small solutions that even do not exhaust the budget. However, this result does not contradict the guarantee given in Theorem 6; the objective value is not much worse than that of Algorithm 1 and the (expected) size of solutions is just upper bounded in Theorem 6.

To further examine the performance of Algorithm 2, we run it on the instance that appeared in the proof of Theorem 5, where we set \(k=50\) (and \(b=50\)). Note that it is theoretically guaranteed that Algorithm 1 outputs a poor solution to the instance. On the other hand, Algorithm 2 with \(=\) always obtains the optimal solution among 100 routing trials. Hence, we see that the algorithm is rather strong against adversarial instances, verifying the effectiveness of the theoretical approximation guarantee of Algorithm 2.

## 8. Conclusion

In this study, we have introduced a novel optimization model for local centrality minimization and designed two effective approximation algorithms. The first algorithm (Algorithm 1) is a highly-scalable \(()}\)-approximation algorithm. We stress that this approximation ratio is unachievable by the most intuitive greedy algorithm (Algorithm 4). The second algorithm (Algorithm 2) is a polynomial-time \((,e)\)-biteriria approximation algorithm. To the best of our knowledge, ours are the first polynomial-time algorithms with provable approximation guarantees for centrality minimization. Experiments using a variety of real-world networks demonstrate the effectiveness of our proposed algorithms.

Our work opens up several interesting problems. Can we design a polynomial-time algorithm that has an approximation ratio better than that of Algorithm 1 or a bicriteria approximation ratio better than that of Algorithm 2? Another interesting direction is to study Problem 1 with a more capable setting. For example, it would be valuable to have a target vertex subset (rather than a single target vertex) and aim to minimize its group harmonic centrality score, as in the literature on global centrality minimization (Shenzhen et al., 2016).

Figure 1. Quality of solutions of the algorithms (except for Algorithm 2) with \(b=|()|\).