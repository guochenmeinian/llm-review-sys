# Globally Interpretable Graph Learning via Distribution Matching

Anonymous Author(s)

###### Abstract.

Graphs neural networks (GNNs) have emerged as a powerful graph learning model due to their superior capacity in capturing critical graph patterns. Instead of treating GNNs as black boxes in an end-to-end fashion of training and deployment, people start to turn their attention to understand and explain the model behavior. Existing works mainly focus on local interpretation, which aims to reveal the discriminative pattern for individual instances. However, the retrieved pattern cannot be directly generalized to reflect the high-level model behavior, i.e., patterns captured by the model for a certain class. To gain global insights about graph learning mechanism, we aim to answer an important question that is not yet well studied: _how to provide a global interpretation for the graph learning procedure?_ We formulate this problem as _globally interpretable graph learning_, which targets on distilling high-level and human-intelligible patterns that dominate the learning procedure, such that training on this pattern can recover a similar model.

To address this problem, we first propose a new interpretation metric _model fidelity_, which is tailored for evaluating the fidelity of the resulting model trained on interpretations. Our preliminary analysis shows that interpretative patterns generated by existing global methods fail to recover the model training procedure. Thus, we further propose our solution, _Graph Distribution Matching_ (GDM), which synthesizes interpretive graphs by matching the distribution of the original and interpretive graphs in the feature space of the GNN as its training proceeds. These few interpretive graphs demonstrate the most informative patterns the model captures during training. Extensive experiments on graph classification datasets demonstrate multiple advantages of the proposed method, including high model fidelity, predictive accuracy and time efficiency, as well as the ability to reveal class-relevant structure.

Graph Neural Networks, Model Interpretability +
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

## 1. Introduction

Graph neural networks (GNNs)(Ganin and Lempitsky, 2015; Goodfellow et al., 2014; Graves et al., 2014; Graves et al., 2014; Graves et al., 2014; Graves et al., 2014) have attracted enormous attention and prominently advanced the state of the art on graph learning tasks. Despite their great success, GNNs are usually treated as black boxes in an end-to-end fashion of training and deployment, which may raise trustworthiness concerns in decision making, if humans cannot really understand what pattern are really captured by the model during graph learning procedure. Lack of such understanding could be particularly risky when using a GNN model for high-stakes domains, e.g., finance (Golovin et al., 2013) and medicine (Bishop, 2006). For instance, in the context of predicting the effect of medicines, if a GNN model mistakenly learns false patterns that violate chemical principles, it may provide incorrect assessments. This highlights the importance of ensuring a comprehensive interpretation of the working mechanism for graph learning.

To improve transparency of GNNs, a large body of existing interpretation techniques focuses on providing _instance-level local interpretation_, which explains specific predictions a GNN model makes on each individual graph instance (Ganin and Lempitsky, 2015; Goodfellow et al., 2014; Graves et al., 2014; Graves et al., 2014; Graves et al., 2014; Graves et al., 2014; Graves et al., 2014; Graves et al., 2014; Graves et al., 2014). Despite different strategies adopted in these works, in general, local interpretation aims to identify critical substructure for a particular graph instance, which would require manual inspections on many local interpretations to mitigate the variance across instances and conclude a high-level pattern of the model behavior. As a sharp comparison to such instance-specific interpretations, relatively few recent works study _model-level global interpretations_(Bishop, 2006; Graves et al., 2014; Graves et al., 2014) to understand the general behavior of the model with respect to a certain class instead of any particular instance.

The goal of global interpretation is to generate a few compact interpretive graphs, which summarize class discriminative patterns the GNN model learns for decision making. Existing works generate such interpretive graphs via different strategies, including reinforcement learning (Sutton et al., 2015), concept combination (Bishop, 2006) and probabilistic generation (Sutton et al., 2015). These solutions can extract meaningful interpretive graphs with a high _predictive accuracy_, evaluated from the perspective of _model consumers_: given a pre-trained GNN model, the end user can use these interpretation methods to understand what patterns this model is leveraging for _inference_.

In this paper, we aim to interpret at the side of _model developers/providers_, who usually care about what patterns really dominate the model training, which could help improve training transparency. This demands specialized evaluation, which are long ignored: if the interpretation indeed contains essential patterns the model captures during training, then when we use these interpretive graphs to train a model from scratch, this surrogate model should present similar behavior as the original model. We are the first to realize this principle and define a new metric, _model fidelity_, which evaluates the predictive similarity between the surrogate model (trained via interpretative graphs) and the original model (normally trained via the training set). We evaluate model fidelity of existing global interpretation method, XGNN (Sutton et al., 2015) and GNNInterpreter (Sutton et al., 2015), by comparing the surrogate model and the original model for eachtraining iteration on MUTAG data. As shown in Figure 1, they have relatively low model fidelity, which suggests that their generated interpretive graphs are less discriminative for recovering the original model's training procedure. Thus these interpretation methods may not suit for explaining model training behavior.

To this end, we attempt to provide a novel globally interpretable graph learning framework, which is designed for the model developers to distill high-level and human-intelligible patterns the model learns in its training procedure. To be more specific, we propose Graph Distribution Matching (GDM) to synthesize a few compact interpretive graphs for each class following the _distribution matching principle_: as the model training progresses, the interpretive graphs maintain a similar distribution as the original graphs. We optimize interpretive graphs by minimizing the distance between the interpretive and original data distributions, measured as the maximum mean discrepancy (MMD) (Bordes et al., 2014) in a family of embedding spaces obtained by a series of model snapshots. Presumably, GDM simulates the model training trajectory, thus the generated interpretation can provide a general understanding of what patterns dominate and result in the model training behavior.

Note that as model developer, we can access the model training trajectory, and our proposed framework is an efficient plug-and-play interpretation tool that can be easily integrated to usual model development pipeline, without interfering the normal training procedure. The success of our framework enables the model develops to provide an interpretation byproduct when publishing their models, which can benefits multiple parties: for the developers, models are published with better transparency without leaking training data; for the consumers, the interpretation can help screen whether the models' discriminative patterns fit their needs.

Extensive quantitative evaluations on three synthetic and three real-world datasets for graph classification task verify the effectiveness of GDM: it can simultaneously achieve high model fidelity and predictive accuracy. Our ablation study also shows the advantage of generating interpretation guided by the model training trajectory. Qualitative study further intuitively demonstrates the human-intelligible patterns captured by GDM.

## 2. Related Work

Due to the prevalence of graph neural networks (GNNs), extensive efforts have been conducted to improve their transparency and interpretability. Existing techniques can be categorized as _local instance-level_ interpretation and _global model-level_ interpretation depending on the interpretation form.

### Local Instance-Level Interpretation

Instance-level methods provide input-dependent explanations for each individual graph (Golov et al., 2013; Velickovic et al., 2014). Given an input graph, these methods explain GNNs by extracting a small interpretive subgraph. Existing solutions can be categorized as gradient-based (Golov et al., 2013; Velickovic et al., 2014), attention-based (Golov et al., 2013), perturbation-based (Golov et al., 2013; Velickovic et al., 2014), decomposition-based (Golov et al., 2013), and surrogate-based methods (Sutskever et al., 2014). Gradient-based method directly uses the gradients as the approximations of feature importance. Attention-based methods use the attention mechanism to identify important subgraph as interpretation. Perturbation-based methods optimize a subgraph mask to captures the important nodes and edges. Surrogate-based explanation methods use data sampling to filter out unimportant features and an explainable small model - such as a probabilistic graphical model - is fitted on the filtered data as a topological explanation. Decomposition-based methods decompose predictive scores to represent how importance the input contributes to the predicted results. Again, instance-level methods are based on each input instance. Although they are helpful for getting an explanation for every single graph, they can hardly capture the commonly important features that are shared by graph instances for each class. Therefore, it is necessary to have both instance-level and model-level interpretations for GNNs.

### Global Model-Level Interpretations

Model-level interpretation aims at capturing the global behaviour of the model as a whole, such that a robust overview of the model can be summarized from individual noisy local explanations. This type of interpretation on graph learning is less studied. KQNN (Krishna et al., 2017) frames this problem as a form of input optimization, leveraging a reinforcement learning technique to sequentially generate edges based on the prediction reward. However, this approach requires domain expert knowledge to design valid reward function for different inputs, which is not always available. GNNInterpreter (Sutskever et al., 2014) learns a probabilistic generative graph distribution and identifies the key graph pattern when GNN tries to make a certain prediction. GLExplainer (Golov et al., 2013) generates explanations as Boolean combinations of learned graphical concepts, represented as clusters of local explanations. While these methods identify intuitive class-related patterns that can be recognized by the model (with high predictive accuracy), they usually ignores the training utility of these explanations. Ideally, high-quality interpretations capturing class discriminative patterns from the training data should be able to train a similar model. From this perspective, in this work, we define model fidelity as a new metric, and propose a novel globally interpretable graph learning framework that explains by matching the distribution along the model training trajectory.

## 3. Methods

We first discuss existing global training methods and provide a general form of the targeted problem. To improve the utility of class discriminative explanations in training a similar model, we propose a novel globally interpretable graph learning framework.

Figure 1. _Model Fidelity_ (i.e., cosine similarity between the predictive logits of original model and that of surrogate model) and _Predictive Accuracy_ (i.e., the original model’s accuracy on interpretive graphs) as model training proceeds.

This framework aims to align the model's behavior on original training data and synthesized interpretive data along the model training trajectory. We realize this goal via the distribution matching principle, which can be formulated as an optimization problem. We further discuss several practical constraints for optimizing interpretive graphs. Finally, we provide the designed algorithm for the proposed interpretation method.

### Graph Learning Background

We focus on explaining GNNs' global behavior for the graph classification task. A graph classification dataset with \(N\) graphs can be denoted as \(=\{G^{(1)},G^{(2)},,G^{(N)}\}\) with a corresponding ground-truth label set \(=\{y^{(1)},y^{(2)},,y^{(N)}\}\). Each graph consists of two components, \(G^{(1)}=(^{(1)},^{(i)})\), where \(^{(1)}\{0,1\}^{n n}\) denotes the adjacency matrix and \(^{(i)}^{n d}\) is the node feature matrix. The label for each graph is chosen from a set of \(C\) classes \(y^{(i)}\{1,,C\}\), and \(y^{(i)}_{c}\) denotes that the label of graph \(G_{i}\) is \(c\), that is \(y^{(i)}=c\). A set of graphs that belong to class \(c\) could be further represented as \(g_{}=\{G^{(i)}|y^{(i)}=c\}\).

A GNN model \(()\) is a concatenation of a feature extractor \(f_{}()\) parameterized by \(\) and a classifier \(h_{}()\) parameterized by \(\), where \(()=h_{}(f_{}())\). The feature extractor \(f_{}:^{d^{}}\) takes in a graph and embeds it to a low-dimentional space with \(d^{} d\). The classifier \(h_{}:^{d^{}}\{1,,C\}\) further outputs the predicted class given the graph embedding.

### Revisit Global Interpretation Problem

We now provide a general form for the global interpretation problem. The idea is to generate a small set of compact graphs that can explain the high-level behavior of the GNN model, e.g., what patterns lead the model to discriminate different classes. Specifically, given a GNN model \(^{*}\), existing global interpretation method aims to generate interpretive graphs that have the maximal predicted probability for a certain class \(y_{c}\). Formally, this problem can be defined as:

\[_{}}(^{*}(}),y_{c}), \]

where \(}\) is one or multiple compact interpretive graphs capturing key graph structures and node characteristics for interpretation, and \((,)\) is the loss (e.g., cross-entropy loss) of predicting \(}\) as label \(y_{c}\). Existing global interpretation techniques can fit in this form but differ in the generation procedure of \(}\). For instance, in XGNN (Xu et al., 2018), \(}\) is defined as a set of completely synthesized graphs with each edge generated by a reinforcement learning strategy. The goal of the reward function is to maximize the probability of predicting \(}\) as a certain class \(y_{c}\). In GNNInterpreter (Shen et al., 2018), \(}\) is generated by sampling from an estimated graph distribution. In GLGExplainer (Chen et al., 2019), \(}\) is generated by a Boolean logic function. Despite their difference in generation techniques, they stand on a common ground as a model consumer: they can only access and inspect the final pre-trained model \(^{*}\) to explain its behavior.

If standing from the perspective of model provider, such a problem formulation may not fully leverage all accessible information, such as the whole training trajectory, leading to limited interpretation capability. Specifically, we consider interpretation quality from the following two aspects:

* _Predictive Accuracy_ reflects whether the extracted interpretative patterns are really class-relevant. It is calculated as the model accuracy on generated interpretive graphs. Existing works mainly focus on this aspect (Chen et al., 2019; Chen et al., 2019; Wang et al., 2020).
* _Model Fidelity_ measures whether the interpretive graphs are class discriminative enough to train a similar model. It is calculated as the cosine similarity between the predictive probabilities of the target model and that of the surrogate model (trained by interpretative graphs) on a same set of instances. This aspect however has never been inspected in prior studies.

As shown in Figure 1, existing works following this formulation provide a limited model fidelity. This observation motivates us to rethink the global interpretation problem from the model provider's perspective and design a globally interpretable learning framework.

### Globally Interpretable Graph Learning

Our goal is to generate global explanations that can not only be accurately predicted as the corresponding class, but also lead to a high-fidelity model. In order to achieve this goal, we propose to optimize the explanations in the model developing stage, such that the training trajectory information can be leveraged. We thus propose a novel research problem: _how to provide global interpretation for a model training procedure, such that training on such interpretation can recover a similar model?_ We frame this problem as _globally interpretable graph learning_, which can be defined as the following optimization problem:

\[&_{}}_{t}[(_{t}(),y_{c})],\\ &\,_{t}=-_{}(_{}(_{t-1}),), \]

where \(=[0,,T-1]\) is the normal training iterations for the target GNN model, and \(-_{}\) is a specific model update algorithm (e.g., gradient descent) with a fixed number of steps \(\). \(_{}()=_{G,}-,[( (G),y)]\) is the cross-entropy loss used for normal GNN model training.

This formulation of globally interpretable graph learning states that the interpretable patterns \(}\) should be optimized based on the whole training trajectory \(_{0}_{1}_{T-1}\) of the model. This stands in sharp contrast to other global interpretation where only the final model \(^{*}=_{T-1}\) is considered. The training trajectory reveals more information about model's training behavior to form a constrained model space, such as essential graph patterns that dominate the training of this model.

### Interpretation via Distribution Matching

To realize globalinterpretation as demonstrated in Eq. (2), we now introduce the exact form of the objective function for optimizing interpretive graphs that encapsulate the model's learning behavior from the data. Recall that a GNN model is a combination of feature extractor and a classifier. The feature extractor \(f_{}\) usually carries the most essential information about the model, while the classifier is a rather simple multi-perceptron layer. Since the feature extractor plays the majority role, a natural idea for generating interpretation is to match its distribution with training graphs in the model's feature space. We name this interpretation principle as _Graph Distribution Matching_ (GDM).

#### Graph Distribution Matching (GDM)

To realize this principle, we first measure the distance between two graph distributions via their maximum mean discrepancy (MMD), which is the difference between means of distributions in a Hilbert kernel space \(\)(Golov et al., 2013):

\[_{\|f\|_{} 1}(}{}[f_{}(G)]- _{c}}{}[f_{}(S)]). \]

Empirically, MMD can be estimated as the difference between the encoded training graphs and interpretive graphs in the embedding space. Based on this idea, we instantiate the outer objective in Eq. (2) as a distribution matching loss \(_{}()\):

\[(_{t}(),y_{c}) :=_{}(f_{_{t}}(_{c}))\] \[=\|_{c}|}_{G_{c}}f_{ _{t}}(G)-_{c}|}_{S_{c}}f_{ _{t}}(S)\|^{2}, \]

where \(_{c}\) is the interpretive graph(s) for explaining class \(c\), and \(_{c}\) is the training graphs belonging to class \(c\). By optimizing Eq. (4), we can obtain interpretive graphs that produce similar embeddings to training graphs for the current GNN feature extractor \(_{t}\) in the training trajectory. Thus, the interpretive graphs provide a plausible explanation for the model learning process. Note that there can be multiple interpretive graphs for each class, i.e., \(|_{c}| 1\). With this approach, we are able to generate an arbitrary number of interpretive graphs that capture different patterns.

#### Globally Interpretable Learning via Distribution Matching

By plugging the distribution matching objective Eq. (4) into Eq. (2), and simultaneously optimizing interpretive graphs for multiple classes \(=\{_{c}\}_{c=1}^{C}\), we can rewrite our learning goal as follows:

\[}{}}{ }[_{c=1}^{C}_{}(f_{_{t}}(S_{c}))]\] \[_{t},_{t}=-_{, }(_{}(h_{_{t-1}},f_{_{t-1}}),), \]

where the cross entropy loss is w.r.t. the feature extractor and predictive head, \(_{}()=_{}(h_{},f_{})= _{G,y}-,y[(h_{}(f_{}(G)),y) ]\), and for each class \(c\), we optimize its corresponding interpretive graph(s) \(_{c}\). The interpretation procedure is based on the model training trajectory, while the model is normally trained on the original classification task. Thus this interpretation method can serve as a plug-and-play tool without interfering normal model training.

The proposed framework is illustrated in Figure 2, for each training step \(t\), we update interpretive graphs by aligning with the training graphs in the GNN model's feature space via distribution matching. Along the whole training trajectory, we keep updating interpretive graphs in a curriculum learning manner to capture the model's training behavior. It is worth noting that such a distribution matching scheme has shown success in distilling rich knowledge from training data to synthetic data (Zhu et al., 2019), which preserve sufficient discriminative information for training the underlying model. This justifies our design of distribution matching for interpretation.

### Practical Constraints in Graph Optimization

Optimizing each interpretive graph is essentially optimizing its adjacency matrix and node feature. Denote a interpretive graph as \(S=(_{s},_{s})\), with \(_{s}\{0,1\}^{m m}\) and \(_{s}^{m d}\). To generate solid graph explanations using Eq. (5), we introduce several practical constraints on \(_{s}\) and \(_{s}\). The constraints are applied on each interpretive graph, concerning discrete graph structure, matching edge sparsity, and feature distribution with the training data.

#### Discrete Graph Structure

Optimizing the adjacency matrix is challenging as it has discrete values. To address this issue, we assume that each entry in matrix \(_{s}\) follows a Bernoulli distribution \(():p(_{s})=_{s}()+(1- _{s})(-)\), where \(^{m m}\) is the Bernoulli parameters, \(()\) is element-wise sigmoid function and \(\) is the element-wise product, following (Golov et al., 2013; Golov et al., 2013; Golov et al., 2013). Therefore, the optimization on \(_{s}\) involves optimizing \(\) and then sampling from the Bernoulli distribution. However, the sampling operation is non-differentiable, thus we employ the reparameterization method (Golov et al., 2013) to refactor the discrete random variable into a function of a new variable \((0,1)\). The adjacency matrix can then be defined as a function of Bernoulli parameters as follows, which is differentiable w.r.t. \(\):

\[_{s}()=((-(1-)+)/ ), \]

where \((0,)\) is the temperature parameter that controls the strength of continuous relaxation: as \( 0\), the variable \(_{s}\) approaches the Bernoulli distribution. Now Eq. (6) turns the problem of optimizing the discrete adjacency matrix \(_{s}\) into optimizing the Bernoulli parameter matrix \(\).

#### Matching Edge Sparsity

Our interpretive graphs are initialized by randomly sampling subgraphs from training graphs, and their adjacency matrices will be freely optimized, which might result in too sparse or too dense graphs. To prevent such scenarios, we exert a sparsity matching loss by penalizing the distance of sparsity between the interpretive and the training graphs, following (Golov et al., 2013):

\[_{}()=_{(_{s}(), _{s})}(-,0), \]

Figure 2. Overview of the proposed globally interpretable learning framework via graph distribution matching GDM.

where \(=_{ij}(_{ij})/||\) calculates the expected sparsity of a interpretive graph, and \(\) is the average sparsity of initialized \(()\) for all interpretive graphs, which are sampled from original training graphs thus resembles the sparsity of training dataset.

```
1:Input: Training data \(=\{_{c}\}_{c=1}^{C}\)
2:Initialize explanation graphs \(=\{_{c}\}_{c=1}^{C}\) for each class \(c\)
3:for\(t=0,,T-1\)do
4: Sample mini-batch interpretive graphs \(^{}=\{^{}_{c}-_{c}\}_{c =1}^{C}\)
5: Sample mini-batch training graphs \(^{}=\{^{}_{c}-_{c}\}_{c =1}^{C}\)
6:\(\) Optimize global interpretive graphs
7:for class \(c=1,,C\)do
8: Compute the interpretation loss following Eq. (9): \(_{c}=_{}(_{_{t}}(^{ }_{c}))+_{}(^{}_{c})+_{}(^{}_{c})\)
9:endfor
10: Update explanation graphs \(-_{}_{c=1}^{C} _{c}\)
11:\(\) Optimize GNN model as normal
12: Compute normal training loss for graph classification task \(_{}(h_{_{t-1}},_{_{t-1}})=_{ =^{}}(h_{_{t-1}}(_{_ {t-1}}(G),y)\)
13: Update feature extractor \(_{t+1}=_{t}-_{1}_{}_{ }(h_{_{t-1}},_{_{t-1}})\)
14: Update predictive heat \(_{t+1}=}_{t-2}_{}_{}(h_{_{t-1}},_{_{t-1}})\)
15:endfor
16:Output: Explanation graphs \(^{*}=\{^{*}_{c}\}_{c=1}^{C}\) for each class \(c\)
```

**Algorithm 1** Globally Interpretable Learning via Graph Distribution Matching (GDM)

**Matching Feature Distribution** Real graphs in practice may have skewed feature distribution; without constraining the feature distribution on interpretive graphs, rare features might be overshadowed by the dominating ones. For example, in the molecule dataset MUTAG, node feature is the atom type, and certain node types such as Carbons dominate the whole graphs. Therefore, when optimizing the feature matrix of interpretive graphs for such unbalanced data, it is possible that only dominating node types are maintained. To alleviate this issue, we propose to match the feature distribution between the training graphs and the interpretive ones.

Specifically, for each graph \(G=(,)\) with \(n\) nodes, we estimate the graph-level feature distribution as \(}=_{i=1}^{n}_{i}/n^{d}\), which is essentially a mean pool of the node features. For each class \(c\), we then define the following feature matching loss:

\[_{}(_{c})=\|_{c}|}_{( ,)_{c}}}-_{c}|}_{(_{c},_{c})_{c}}}_{c} \|^{2}, \]

where we empirically measure the class-level feature distribution by calculating the average of graph-level features. By minimizing the feature distribution distance in Eq. (8), even rare features can have a chance to be distilled in the interpretive graphs.

### Final Objective and Algorithm

Integrating the practical constraints discussed in Section 3.5 with the distribution matching based interpretation framework in Eq. (5), we now obtain the final objective for interpretation optimization, which essentially is determined by the Bernoulli parameters for sampling discrete adjacency matrices and the node feature matrices. Formally, we aims to solve the following optimization problem:

\[_{}}_{t}_{c= 1}^{C}_{}(_{_{t}}(_{c}))+ _{}(_{c})+_{}()\]

\[\;_{t},_{t}=-_{, }(_{}(h_{_{t-1}},_{_{t-1 }}),) \]

where we use \(\) and \(\) to control the strength of regularizations on feature distribution matching and edge sparsity respectively. Algorithm 1 details the steps for solving this optimization problem.

**Complexity Analysis** We now analyze the time complexity of the proposed method. Suppose for each iteration, we sample \(B_{1}\) interprettive graphs and \(B_{2}\) training graphs. Denote their average edge number as \(m\). The inner loop for interpretive graph update takes \(m(B_{1}+B_{2})\) computations on node, while the update of GNN model uses \(mB_{2}\) computations. Therefore the overall complexity is \((mT(B_{1}+2B_{2}))\), which is of the same magnitude of complexity for normal GNN training. This demonstrates the efficiency of our interpretation method: it can simultaneously generate interpretations as the training of GNN model proceeds, without introducing extra complexity.

## 4. Experimental Studies

This section aims to verify the necessity of our proposed method for globally interpretable graph learning. Specifically, we conduct extensive experiments to answer the following questions:

* **Q1**: Does the proposed global interpretation result in similar GNN models as trained in original data (i.e., with high fidelity)?
* **Q2**: Is the training trajectory necessary for accurate global interpretation (compared with ensemble model snapshots)?
* **Q3**: Are the generated interpretations human-intelligible?

We provide both quantitative and qualitative study to evaluate the global interpretations generated by GDM, comparing with existing global interpretation baselines and ablation variants.

### Experimental Setup

**Dataset** The interpretation performance is evaluated on the following synthetic and real-world datasets for graph classification, whose statistics can be found in Table 1.

* _Real-world_ data includes: **MUTAG**(Czhang et al., 2017) consists of chemical compounds with atoms as nodes and chemical bonds as edges, labeled by whether it has a mutagenic effect on a bacterium. **GraphTwitter**(Zhou et al., 2017) includes Twitter comments for sentiment classification with three classes. Each comment sequence is presented as a graph, with word embedding as node feature. **Graph-SST5**(Zhu et al., 2017) is a similar dataset with reviews, where each review is converted to a graph labeled by one of five rating classes.
* _Synthetic_ data includes: **Shape** contains four classes, i.e., Lollipop, Wheel, Grid, and Star. Each class has the same number of synthesized graphs with a random number of nodes. **BA-Motif**(Lollipop et al., 2017) uses Barabasi-Albert (BA) graph as base graphs, among which half graphs are attached with a "house" motif and the rest with "non-house" motifs. **BA-LRP**(Shi et al., 2017) based on Barabasi-Albert (BA) graph includes one class being node-degree concentrated graphs, and the other degree-evenly graphs. These datasets do not have node features, thus we use node index as the surrogate feature.

**Baseline** We mainly compare GDM with global interpretation baselines, and ablative variants of our method.
* _Global interpretation baselines_: **XGNN**(Wang et al., 2019) generate global interpretation via reinforcement learning. Since it heavily relies on domain knowledge (e.g. chemical rules) in the reward function, thus we only evaluate it on MUTAG. **GNNInterpret**(Zhu et al., 2019) generates interpretations based on label and embedding similarity but it is only based on a pre-trained GNN model1. We also include a simple **Random** strategy as a reference, which randomly selects graphs from the training set as interpretations. * _Ablation variants of GDM:_ We also consider the variants of GDM which generate interpretation based on selective model snapshots. **GDM-First** and **GDM-Last** uses only the first or the last model snapshot respectively for the outer optimization in Eq. (5). **GDM-Ensemble** uses the same set of model snapshots as in GDM for conducting the outer optimization of Eq. (5), but ignores the sequence of model trajectory (i.e., disabling the inner optimization).

Meanwhile, a comparison of GDM with several local interpretation methods (which extract interpretive graphs for each training instance) can be found in Appendix A.1. A simple inherently global-interpretable method is also compared in Appendix A.2.

**Evaluation Protocol** We comprehend global interpretability from two perspectives, i.e., the interpretation should lead to high-fidelity model that is similar to the original target model (i.e., the model to be explained), and should have high chance to be predicted as the right classes. Based on this intuition, we establish the following evaluation protocols accordingly:

* _Model Fidelity_ aims to verify whether the generated interpretation indeed captures essential class-discriminative patterns, such that the interpretation can be utilized to train a similar model as if it is trained on the original training set. Desired interpretation should capture patterns that dominate the model training procedure. To calculate this metric, we first use the generated interpretive graphs to train a surrogate model (with the same architecture as the original model) from scratch. Then we calculate model fidelity as the ratio of cases when the surrogate model makes the same decision as the original model on test data.
* _Model Utility_ is to investigate whether the interpretation can lead to a high-utility model. Similarly, we train a surrogate model on the interpretation graphs. Then model utility is calculated as the surrogate model's predictive accuracy on test data.
* _Predictive Accuracy_ is to validate whether the interpretation can be correctly perceived by the target model as its corresponding class. Ideal interpretive graphs should be correctly classified to their classes by the target model being explained. We report the target model's predictive accuracy on the interpretive graphs as predictive Accuracy.

**Configurations** We choose the graph convolution network (GCN) as the target GNN model for interpretation, as it is widely used for graph learning. It contain 3 layers with 256 hidden dimension, concatenated by a mean pooling layer and a dense layers in the end. Adam optimizer (Kingma and Ba, 2015) is adopted for model training. In both evaluation protocols, we split the dataset as 85% training and 15% test data, and only use the training set to generate interpretative graphs. To learn interpretive graphs that generalize to a distribution of model initializations, we empirically adopt regular model restarts to sample multiple trajectories. Given the interpretative graphs, each evaluation experiments are run 5 times, with the mean and variance reported.

### Quantitative Results

This evaluation aims to answer the first question **Q1**. Meanwhile, we also report the commonly adopted predictive accuracy.

**Model Fidelity and Model Utility Performance** In Table 2, we compare GDM with baselines in terms of model fidelity and utility. XGNN performed on MUTAG achieves 89.47 fidelity and 68.40 utility with 10 graphs per class. We observe that GDM achieves remarkably better performance almost on all datasets, which indicates that GDM indeed captures discriminative patterns the model learns during training, such that our generated interpretation can also train a similarly useful model (with high model fidelity and utility). Meanwhile, different from XGNN, we do not include any dataset specific rules, thus is a more general interpretation solution.

**Predictive Accuracy** In Table 3, we compare the predictive accuracy of GDM, XGNN and GNNInterpret respectively. Note that the predictive accuracy for GDM on all datasets except MUTAG is larger than 90%, implying that the generated graphs could preserve those essential information of the data, which plays a crucial role in guiding the desicion-making. Comparatively, GNNInterpretzel has worse performance on most datasets, including Graph-Twitter, Graph-SST5, BA-LRP, and MUTAG, which indicates that several significant patterns of the data during training trajectory are lost and GNNInterpretzel could not recover those undisclosed information along the training trajectory.

**Efficiency** Another advantage of GDM is that it generates interpretations in an efficient manner. As shown in Appendix A.3, GDM is almost 4 times faster than the global interpretation method XGNN. Our methods takes almost no extra cost to generate multiple interpretative graphs, as there are only few interpretive graphs compared with the training dataset. XGNN, however, select each edge in each graph by a reinforcement learning policy which makes the interpretation process rather expensive.

### Model Analysis

**Ablation Study** In Table 4, we generate 10 interpretive graphs per class based on model snapshots. Intuitively, only using the first

   Dataset & \#Graph & \#Node & \#Edge & \#Class & GCN Accuracy \\  BA-Moff & 1000 & 25 & 50.93 & 2 & 100.00 \\ BA-LRP & 20000 & 20 & 42.04 & 2 & 97.95 \\ Shape & 100 & 53.39 & 813.93 & 4 & 100.00 \\ MUTAG & 188 & 17.93 & 19.79 & 2 & 88.63 \\ Graph-Twitter & 4998 & 21.10 & 40.28 & 3 & 61.40 \\ Graph-SST5 & 8544 & 19.85 & 37.78 & 5 & 44.39 \\   

Table 1. Basic Graph Statisticsmodel snapshot would capture less feature and structure information, thus the model fidelity score would be smaller than GDM as shown in Table 4. In the ablation study, there are also notable discrepancies between the GDM-Ensemble fidelity and GDM fidelity on a few datasets, including Graph-Twitter, BA-Motif, and BA-LRP. Those ensemble snapshots would possibly preserve misleading patterns which could be filtered out during model training but been captured while distribution matching, leading to the large deviates of the fidelity score for the GDM-Ensemble model. Generally, we can observe that the distribution matching design is effective: disabling this design will greatly deteriorate the performance.

**Parameter Sensitivity** In our final objective Eq. (9), we defined two hyper-parameters \(\) and \(\) to control the strength for feature matching and sparsity matching regularization, respectively.

In this subsection, we explore the sensitivity of hyper-parameters \(\) and \(\). Since MUTAG is the only dataset that contains node features, we only apply the feature matching regularization on this dataset. we vary the sparsity coefficient \(\), and report the utility and predictive accuracy for all of our datasets in Figure 3. For most datasets excluding Shape, the utility performance start to degrade when the \(\) becomes larger than 0.5. This means that when the interpretive graph becomes more sparse, it will lose some information during training time. Given the small values of \(\), the graphs are relatively dense and the model predictive accuracy for all datasets except Graph-SSTS and Graph-Twitter converges to be stationary, denoting that the sparsity of those graphs would not heavily influence generating interpretations.

Moreover, we report the model utility and model fidelity with different feature-matching coefficients \(\) in Table 5. A larger \(\) means we have a stronger restriction on the node feature distribution. We found that when we have more strict restrictions, the utility increases slightly. This is an expected behavior since the node features from the original MUTAG graphs contain rich information for classifications, and matching the feature distribution enables the interpretation to capture rare node types. By having such restrictions, we successfully keep the important feature information in our interpretive graphs. However, as the coefficient \(\) increase, the model fidelity would slightly decrease, which means the restrictions about

   Dataset & Graph-Twitter & Graph-SST5 & BA-Motif & BA-LRP & Shape & MUTAG & XGN on MUTAG & 779 \\  GDM-First & 25.84\(\)4.06 & 21.28\(\)0.21 & 51.20\(\)2.23 & GDM-Last & 28.61\(\)3.41 & 27.19\(\)0.27 & 46.40\(\)2.53 & 760 \\ GDM-Ensemble & 30.68\(\)6.00 & 25.70\(\)0.25 & 51.40\(\)5.56 & **GBM** & **58.13\(\)2.74** & **36.62\(\)0.76** & **91.60\(\)3.72** & 782 \\  Dataset & BA-LRP & Shape & MUTAG & & & & & \\  GDM-First & 51.03\(\)0.75 & 60.00\(\)0.00 & 73.68\(\)2.19 & & & & & \\ GDM-Last & 49.95\(\)0.28 & 60.00\(\)1.00 & 56.84\(\)5.78 & & & & & \\ GDM-Ensemble & 56.39\(\)0.54 & 58.00\(\)0.00 & 87.37\(\)0.73 & **GBM** & **95.50\(\)0.50** & **64.00\(\)8.00** & **94.73\(\)0.00** & **94.73\(\)0.00** & **94.73\(\)0.00** \\   

Table 4. Ablation study showing _Model Fidelity_ when generating 10 interpretive graphs per class.

    &  &  &  &  &  \\    & & GDM & & & & & GDM & & & \\   & 1 & **81.05 \(\) 9.76** & 79.53 \(\) 2.58 & 49.47 \(\) 10.84 & **71.92 \(\) 2.48** & 70.17 \(\) 2.48 & 50.87\(\) 15.0 & 799 \\  & 5 & **92.63 \(\) 2.58** & 84.21 \(\) 0.00 & 65.26 \(\) 6.31 & 77.19 \(

[MISSING_PAGE_FAIL:8]