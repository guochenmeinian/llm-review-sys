# Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models

Anonymous Author(s)

###### Abstract.

Temporal reasoning is a crucial natural language processing (NLP) task, providing a nuanced understanding of time-sensitive contexts within textual data. Although recent advancements in Large Language Models (LLMs) have demonstrated their potential in temporal reasoning, the predominant focus has been on tasks such as temporal expression detection, normalization, and temporal relation extraction. These tasks are primarily designed for the extraction of direct and past temporal cues from given contexts and to engage in simple reasoning processes. A significant gap remains when considering complex reasoning tasks such as event forecasting, which requires multi-step temporal reasoning on events and prediction on the future timestamp. Another notable limitation of existing methods is their incapability to illustrate their reasoning process for explaining their prediction, hindering explainability. In this paper, we introduce the first task of explainable temporal reasoning, to predict an event's occurrence at a future timestamp based on context which requires multiple reasoning over multiple events, and subsequently provide a clear explanation for their prediction. Our task offers a comprehensive evaluation of both the LLMs' complex temporal reasoning ability, the future event prediction ability, and explainability--a critical attribute for AI applications. To support this task, we present the first instruction-tuning dataset of explainable temporal reasoning (ExpTime) with 26k derived from the temporal knowledge graph datasets, using a novel knowledge-graph-instructed-generation strategy. Based on the dataset, we propose the first open-source LLM series TimeLaMA based on the foundation LLM LiMaMA, with the ability of instruction following for explainable temporal reasoning. We compare the performance of our method and a variety of LLMs, where our method achieves the state-of-the-art performance of temporal prediction and explanation generation. We also explore the impact of instruction tuning and different training sizes of instruction-tuning data, highlighting LLM's capabilities and limitations in complex temporal prediction and explanation generation.

## CCS CONCEPTS

* **Computing methodologies Temporal reasoning.**research questions (RQ) to guide our study: 1) **RQ 1**: Can LLMs be effective in predicting future events by considering the context's complex relations among events, and how do they compare with traditional methods? 2) **RQ 2**: What impact does instruction tuning have, particularly when using our new dataset derived from temporal knowledge graphs, on the temporal prediction capabilities of LLMs? 3) **RQ 3**: How effectively can LLMs clarify their prediction and reasoning process, thereby enhancing their transparency in temporal reasoning tasks?

To address these challenges, our study aims to explore LLMs' capabilities in complex temporal reasoning, future event prediction, and, importantly, explainability--an essential aspect of AI applications. We propose the pioneering task of explainable temporal reasoning, aiming to predict the occurrence of future events based on context, demanding reasoning across multiple events, and subsequently, providing a coherent explanation for the prediction. To support this task, we propose the first-of-its-kind multi-source instruction tuning dataset ExpTime, fostering improvement and assessment of LLMs. ExpTime comprises 26k entries, built from a variety of event forecasting datasets and their derived temporal reasoning paths.

Our methodology begins with aggregating data from various recognized datasets, encompassing diverse sources. For each data point, explanations are generated, drawing inspiration from the proven self-instruct approach (Zhu et al., 2017). However, we observed that merely prompting LLMs, such as ChatGPT (Chen et al., 2018), yielded suboptimal results in terms of coherence and accuracy. Recognizing this limitation, we pivoted to a novel Temporal Knowledge Graph-Instructed Generation (GIG) approach. We extract explainable reasoning paths and context from the temporal knowledge graph for each dataset's future event prediction query. We then design prompts to guide LLMs to convert these paths and contexts into coherent explanations. This results in triples of <query, context, answer>, with each answer containing the original prediction and LLM-generated explanation. To ensure the reliability of the dataset, the human evaluation is performed on a subset of the collected data with a carefully designed annotation scheme, evaluating their correctness, completeness, and fluency. We then build a golden-standard testing dataset with human annotation.

Using ExpTime, we propose the TimeLaMA series, an innovative open-source LLM ensemble based on the LiMaA2 (Zhu et al., 2017), using instruction fine-tuning. Specifically, we fine-tune four TimeLaMA models: TimeLaMA-7B, ChatTimeLaMA-7B, TimeLaMA-13B, and ChatTimeLaMA-13B. Our empirical results compare the TemporalLaMA with other LLMs, highlighting its superior performance in terms of temporal prediction and explanation generation. Our experiments demonstrate that with proper instruction tuning using even a small volume of high-quality data, the temporal reasoning capabilities of LLMs can be substantially improved. Model size does not necessarily correlate with performance gains in temporal reasoning when employing instruction tuning under 13 billion parameters.

To encapsulate, our contributions are manifold: 1) We pioneer the first task of explainable temporal reasoning, setting the stage for subsequent research, 2) We introduce ExpTime, the first instruction-tuning dataset to improve and evaluate LLMs' ability of explainable temporal reasoning. 3) We propose a novel knowledge graph-instructed generation (GG) method, for generating explainable temporal reasoning data with LLMs from temporal knowledge graphs, 4) We propose TimeLlaMA, an open-source LLM series tailored for this specific task, achieves SOTA performance, 5) We conduct a holistic evaluation of our method and various LLMs in the realm of temporal reasoning, critically analyze the strengths and limitations of LLMs, providing directions for future research 1.

## 2. Related Work

### Temporal Reasoning in NLP

Based on the level of difficulty, temporal reasoning in NLP can be categorized into three tasks: temporal expression detection and normalization, temporal relation extraction, and event forecasting. The temporal expression detection task aims to detect the phrases in the text that describe the temporal information, such as "yesterday" and "last year" (Zhu et al., 2017). After the detection, the model is required to normalize the temporal expression into a TimeML standard format, such as "2013-01-06". The temporal expression detection and normalization task was first introduced in TempEval-2 (Zhu et al., 2017), where the most successful models are rule-based, such as SUTime and NavyTime (Beng et al., 2018; Chen et al., 2018). The normalization task was further improved by incorporating pre-trained embeddings later (Zhu et al., 2017; Zhu et al., 2017).

When time expressions can be detected, the next level of temporal reasoning is to determine the chronological order of events described in the text, namely temporal relation extraction. The temporal relation extraction task was first introduced in TempEval (Zhu et al., 2017). Initially, this task was tackled by leveraging the sequential neural networks, such as LSTM and RNN, to detect temporal order (Song et al., 2018; Chen et al., 2018; Chen et al., 2018). Later, GNN was introduced to better capture

    & Explanation & Event Forecasting & Model & Multi-hop Reasoning & Instruction Finetuning & Context Infer \\  TEMPLAMA (Song et al., 2018) & ✗ & ✗ & T5 & ✗ & ✗ & ✗ \\ TEMPREASON (Zhu et al., 2017) & ✗ & ✗ & T5 & ✗ & ✗ & ✓ \\ AutoCast (Zhu et al., 2017) & ✗ & ✓ & T5 & ✓ & ✗ & ✗ \\ ExpTime & ✓ & ✓ & LLama2-7b/13b & ✓ & ✓ & ✓ \\   

Table 1. The comparison between temporal reasoning datasets and corresponding finetuned models. “Context Infer” denotes if inference based on context is required and “multi-hop reasoning” means engaging in multi-step reasoning is required to arrive at the correct answer.

the dependency explicitly between the events and time expressions (Brocker et al., 2015; Chen et al., 2016; Chen et al., 2017). As LLMs become popular, some work also investigated the zero-shot ability of LLM in temporal relation extraction and reported that the zero-shot performance is worse than supervised models (Chen et al., 2016; Chen et al., 2016).

With the acquisition of a chronological order of events, the final level of temporal reasoning is event forecasting. The goal of this task is to determine if a specific event will happen in the future given the context events described in the text (Chen et al., 2016). Some work has designed a dataset to train the model (Chen et al., 2016), in which the model can access the context information through links. However, the exploration of this task is still limited despite the importance of this task.

### Temporal Knowledge Graph Event Forecast

There are two settings in the temporal knowledge graph reasoning (TKGR) task: extrapolation and interpolation. Extrapolation focuses on predicting whether events will occur in future timestamps, while interpolation aims to complete the temporal knowledge graph within a given timespan (Brocker et al., 2015; Chen et al., 2016). Some works also refer to the extrapolation setting as event forecasting in temporal knowledge graph (Chen et al., 2016; Chen et al., 2016). A key difference between event forecasting in NLP and TKG is the input format - NLP uses textual context, whereas TKG relies on graph structure. To enhance explainability, some methods for TKGR produce predictions along with validated reasons. The explainable methods can be roughly summarized into three categories: logic rule-based approach, reinforcement learning-based approach, and attention network-based approach. For example, TLogic mines logic rules from temporal knowledge graphs for forecasting (Chen et al., 2016). Lin et al. proposed graph and logic encoders to incorporate graph information into rules (Chen et al., 2016). In reinforcement learning-based (RL) approaches, Sun et al. used an RL agent to travel on the graph to predict events, explaining the prediction (Sun et al., 2017). Similarly, Li et al. found event clusters and then searched them with an RL agent (Li et al., 2017). Some models expand an initial query graph via attention until the query entity is reached, using the subgraphs as explanations (Li et al., 2017). Jung et al. also used an attention GNN to iteratively propagate towards the target entity (Li et al., 2017). As explainable TGKR models provide structural reasoning steps on the graph, we leveraged the RL-based and logic-based models to instruct the LLM explanation generation to construct the ExpTime dataset.

### Temporal Reasoning in LLM

As growth took place in pre-trained LLMs, a natural question is if LLM is capable of serving as a temporal knowledge base (Li et al., 2017; Chen et al., 2016). The pivotal concept of this task is to understand the context under temporal expression and perform temporal-sensitive reasoning to predict missing entities (Chen et al., 2016). Temporal datasets have been developed to evaluate LLM on temporal understanding, like Custom-News which evaluates if LLMs can predict masked entities given timestamps (Li et al., 2017). Dhingra et al. then proposed TEMPLAMA which emphasizes temporal questions as \((h,r,?,t_{1})\) and \((h,r,?,t_{2})\) where the answers differ due to different timestamps (Li et al., 2017). TemporalWiki addresses temporal misalignment in LLMs similarly (Tang et al., 2017). Tan et al. expanded TEMPLAMA's time range and added more time-unrelated questions (Li et al., 2017).

Some work also further investigated the capability of LLM in the event forecasting task, which is more challenging than temporal-sensitive learning as it requires a full understanding of time and logic. Zhou et al. constructed an Autocast dataset that consists of question-and-answer pairs about future events (Chen et al., 2016). Lee et al. tested the zero-shot event forecasting ability of LLM on a temporal knowledge graph and demonstrated that only through in-context learning, LLMs can achieve comparable performance wrt current supervised TKG methods (Li et al., 2017). Similarly, Xu et al. designed various prompts to query LLM for temporal knowledge graph completion task (Li et al., 2017). This ability was further improved by few-shot abductive reasoning over LLM and temporal knowledge graph (Chen et al., 2016). However, these studies did not evaluate or improve the textual temporal reasoning skills of LLMs. Additionally, the lack of explainability in these LLMs is concerning given their importance in temporal reasoning tasks. To the best of our knowledge, our proposed ExpTime is the first dataset that evaluates and improves the explainability and textual temporal reasoning ability of LLMs.

## 3. Method

The objective of this work is to assess and enhance the complex temporal reasoning capabilities of large language models (LLMs). To accomplish this goal, we propose the explainable event forecasting task for complex temporal reasoning and construct the first dataset of its kind: the Explainable Temporal Event Forecasting (ExpTime) dataset. We benchmark the performance of popular LLMs using this new dataset. We then propose the novel LLM series: TimetLaMA, by instruction finetuning a series of LLma2 models, with the aim of improving the temporal reasoning abilities of LLMs.

### Task Definition

We define the explainable temporal reasoning task as follows: given an input document \(D\) describing events \(_{t_{1}-t_{2}}=\{e_{1},e_{2},,e_{n}\}\) occurring during time interval \(t_{1} t_{2}\), the task is to predict the probability \(P=P(e_{k}|_{t_{1}-t_{2}})\) that event \(e_{k}\) will occur at future time \(t_{3}\), where \(t_{3}>t_{2} t_{1}\). Additionally, the LLM must also generate an explanation \(F\) that demonstrates its reasoning for the prediction. Each training instance \(_{r}\) for fine-tuning the language model consists of the input document \(D_{i}\), question \(Q_{i}\), prediction answer \(P_{i}\), and explanation \(F_{i}\): \(_{r1}=\{D_{i},Q_{i},P_{i},F_{i}\}\).

### Graph-Instruct-Generation: Construct ExpTime Dataset

Recent work has explored using LLMs like ChatGPT to generate datasets by prompting the model to produce answers (Xu et al., 2017). However, directly prompting LLMs to generate temporal reasoning data results in low-quality explanations, as we demonstrate in Section 4.2.2. To address this issue, we propose a novel framework called Temporal Knowledge Graph-instructed Generation (GIG) to produce more coherent and accurate reasoning explanations.

The key insight behind our approach is to leverage temporal knowledge graphs (TKGs), which have been effectively utilized for explainable event forecasting. As illustrated in Figure 1, we first apply explainable TKG reasoning models to generate reasoning paths for a given query about a future event. We then convert these paths into natural language explanations \(F_{i}\) using a two-level prompting technique we developed. Next, we identify relevant context quadruples from the TKG and reasoning paths to construct a context quadruple set, which is transformed into a coherent natural language document \(D_{i}\). Finally, we convert the original query into a question \(Q_{i}\) to produce a complete training instance \(r_{i}=\{D_{i},F_{i},P_{i},Q_{i}\}\). In this way, our GIG framework overcomes the limitations of directly prompting LLMs by leveraging structured knowledge in TKGs to generate high-quality explanations. The technical details of each step are provided in the following sections.

**Reasoning Paths Generation.** As discussed in Section 2.2, temporal knowledge graph reasoning models can be categorized into three main types. In this work, we select two popular methods representing the most common approaches: TimeTraveler (Tran et al., 2017), which uses a reinforcement learning-based approach, and TLogic (Song et al., 2017), which employs logic rules. We chose these models because they provide high quality and human-readable reasoning chains, as shown in the following equation:

\[(E_{1},R_{c},E_{m+1},T_{m+1})_{i=1}^{m}(E_{i},R_{i},E_{i+1},T _{i}) \]

where \(E_{i}\), \(T_{i}\), and \(R_{i}\) are the i-th entity, timestamp, and relation, respectively. For example, Fig. 1 shows that given the query quadruple, the explainable TKGR model generates the following reasoning path:

\[\ }{}\ \\ \  \]

To leverage the reasoning chains from these models, we take the average confidence scores (or probability values) of the predictions from the two models and select the reasoning paths \(Pa\) with the highest confidence.

**Context Document Generation.** Given a query quadruple \(q_{u}=(e_{1},r,e_{2},t_{i})\), we first extract relevant quadruples from the TKG to form the context quadruple set, and then transform them into natural language sentences. Specifically, to extract relevant information, we obtain quadruples \(q\) that meet two criteria: 1) either entity \(e_{1}\) or \(e_{2}\) from the original query is present in \(q\), and 2) the occurrence time \(t_{q}\) of \(q\) falls within a defined time span from the query time \(t_{i}\) to time \(t_{j}\). Formally, we extract quadruples \(q\) where \((e_{1} q e_{2} q)(t_{q}>t_{i} t_{q}<t_{j})\). We also add the quadruples along the reasoning path \(Pa_{i}\) to the context set.

Once we have the context quadruple set \(\), the next step is to convert \(Q\) into natural language sentences. Prior work such as KELM (Kelm, 2014) and GAP (Garay et al., 2017) have proposed rule-based or pipeline methods, but these cannot generate sufficiently diverse documents from knowledge graphs. Therefore, we designed a prompt to leverage the generative capabilities of ChatGPT to produce more diverse and coherent context documents from \(\). The prompt is defined as follows:

_Please generate a coherent paragraph to describe the following quadruples and the time should be precise to dates: [\(\)]_

In this way, we use the response from ChatGPT as the input document \(D_{i}\) for each training instance \(r_{i}\).

**Explanation Generation.** Recall that for each query quadruple \(q_{u}=(e_{1},r,e_{2},t_{i})\), we have obtained the reasoning path \(Pa_{i}\). First, we automatically generate a template-based explanation \(F^{}_{i}\) for each query quadruple \(q_{u}=(e_{1},r,e_{2},t_{i})\) using the corresponding reasoning path \(Pa_{i}\) obtained from the above steps. This explanation template aims to concisely describe the prediction and the reasoning steps in natural language:

_Based on the information provided by the document, it is plausible that \(e_{1}\) will \(r\)\(e_{2}\) in \(t_{i}\). Here are my reasons: \(Pa_{1}\), and \(Pa_{2},\), therefore, it is plausible that \(e_{1}\) will \(r\)\(e_{2}\) in \(t_{i}\)._

We refer to this as the template synthesized explanation \(F^{}_{i}\).

However, these template-generated explanations \(F^{}_{i}\) may lack coherence or omit critical reasoning details. To improve the quality of explanations, we implement a two-step chain-of-thought (CoT)

Figure 1. The pipeline of generating ExpTime dataset. The pos, neg, and neu denote the positive sample, negative sample, and neutral sample, respectively.

prompting approach using LLMs like ChatGPT. First, we prompt ChatGPT to evaluate the correctness of the template explanation \(F^{}_{i}\) and provide a brief justification, e.g.:

_Given the text, \(F^{}_{i}\), please evaluate the correctness of the prediction..._

We provide detailed prompt in Appendix B.1 for this and all subsequent prompts. If ChatGPT concludes that the explanation \(F^{}_{i}\) is correct, we propose a "polish prompt" to ChatGPT:

_Can you make the text more coherent and readable by expanding the explanation of each reasoning step?_

However, if ChatGPT determines that the template explanation \(F^{}_{i}\) contains flawed reasoning leading to an incorrect prediction, we provide a "revision prompt" asking ChatGPT to correct the flaws by considering additional context quadruple information from \(Q\):

_Please revise... You can add information from the following quadruples... [\(Q\)]_

In this way, the ChatGPT response represents the final, improved explanation \(F_{i}\) for each training instance. This CoT prompting approach allows us to leverage the reasoning and language capabilities of LLMs to enhance the quality of automatically generated explanations.

**Negative and Neutral Samples.** Note that by following the previously introduced steps, we can easily acquire positive training instances, i.e., the prediction is that the event will happen. However, using only positive examples to fine-tune language models can lead to highly skewed and imbalanced training. Therefore, we also propose two methods to generate negative and neutral samples individually.

The negative samples represent counterfactual events that did not occur. For each positive training instance \(_{I}=\{D_{i},F_{i},P_{i}\}\), we generate a negative example by replacing the relation \(r_{i}\) in the query quadruple \(q_{u}\) with an opposite relation \(r^{}_{i}\) such that the meaning of \(r^{}_{i}\) should be as opposite as possible to the original one. For example, we replaced (_Africa, Host a visit, Rx Tillicrap, 2018-03-10_) with (_Africa, withdraw visiting horizons, Rx Tillicrap, 2018-03-10_). The resulting negative example quadruple is \(q^{}_{u}=(_{1},r^{},e_{2},t_{1})\). In this way, as the original event did actually happen, the newly synthesized event should be highly unlikely to happen. We manually designed 546 opposite relations for all 265 relations in the temporal knowledge graph. Details are illustrated in Appendix C.

Then, similar to the explanation generation, we first generate a simple template synthesized explanation and then query ChatGPT if the synthesized explanation is correct or not. The prompt is designed as follows:

_Given the text, "Based on..., we predict that \(_{1}\)\(r^{}\)\(_{2}\) will not happen in \(_{i}\). We could find the following patterns from the text: \(P_{}\), and \(P_{}\),\(\), therefore, it is plausible that \(_{1}\) will \(_{2}\) in \(_{i}\),\(\), please evaluate the correctness..._

Note that the reasoning path \(Pa\) is still the same as the positive sample. Then we can obtain the explanation result based on the ChatGPT decision by following the exact same "Polish Prompt" or "Revision Prompt".

In neutral training samples, we expect the LLMs to predict "unsure" for the query quadruple because there is no context information in the given document related to the query. Additionally, for explanation, we also expect the LLMs to summarize the document and then demonstrate that there is no related context in the given document. To achieve this goal, we first replace the query quadruple \(q_{u}=(_{1},r,_{2},t_{i})\) with \(q^{}_{u}=(^{}_{1},r,^{}_{2},t_{i})\) in the positive training instances, where \(e^{}_{1}\) and \(e^{}_{2}\) are entities that never appear in the context quadruple set \(\). In other words, we ensure the entities in the neutral sample's query do not exist anywhere in the context set \(\). Formally, we have \(e^{}_{1} e^{}_{2}\). Then we designed the following prompt to query ChatGPT to generate an explanation:

_Given the document "[\(D_{i}\)]", how likely the event that [\(e^{}_{1}\)\(r^{}\)\(e^{}_{2}\)] in [\(_{i}\)] would happen? \(\) if the context is unrelated, summarize the context..._

### Data Statistics and Annotation

We utilize ICEWS14 (Lewis et al., 2015), ICEWS18 (Lewis et al., 2015), and ICEWS0515(Lewis et al., 2015) datasets to generate the proposed dataset, as they are the most popular temporal knowledge graph reasoning datasets. From the three datasets, we extracted 12,229 reasoning paths and therefore generated 12,229 positive samples in the dataset. The detailed statistics of ExpTime are shown in Table. 3.

To further evaluate the quality of our dataset and construct a standardized testing dataset, two experienced annotators independently

    & Positive & Negative & Neutral & Overall \\  Correct & 0.73 & 0.64 & 0.81 & 0.74 \\ Complete & 0.65 & 0.59 & 0.70 & 0.66 \\ Fluency & 0.98 & 0.97 & 0.98 & 0.98 \\   

Table 2. Cohen’s Kappa score of human annotation for each criterion under three labels.

Figure 2. The box plots of human annotation for each criterion under positive, negative, neutral, and overall dataset. The dashed line denotes the mean value and the bold line indicates the median value.

evaluated a random sample of 1,200 explanations. The annotators rated each explanation on three criteria: 1) correctness, which assessed whether the prediction and explanation were accurate; 2) completeness, which evaluated if the explanation provided the necessary context to understand the prediction; and 3) fluency, which measured if the explanation was clear and understandable. The annotation guidelines and annotator qualifications are detailed in Appendix A. Cohen's kappa coefficient was calculated to determine inter-rater agreement for each criterion. As shown in Table 2, a high level of agreement was achieved for all criteria. In particular, the annotators demonstrated strong agreement on fluency ratings and agreement was higher overall for samples receiving neutral labels. As illustrated in Fig. 2, most samples received high scores across all three criteria. The strong inter-rater agreement and generally high scores indicate the testing dataset represents a high-quality, standardized sample for evaluation. Low-scoring samples on any of the criteria were excluded.

### TimeLlama

As illustrated in Fig. 3, we present the TimeLlama model series, representing the first LLMs fine-tuned specifically for complex temporal reasoning tasks, namely explainable event forecasting. By instruction tuning the models on datasets requiring the comprehension and synthesis of temporal information, TimeLlama gains an enhanced ability to make logical inferences about the timing, duration, and relations between events. This supports a more accurate prediction of what events may occur next given a historical context. We construct TimeLlama-7b and TimeLlama-13b by finetuning the base Llama-7b and Llama-13b models, respectively. The finetuning process utilizes Flash Attention and DeepSpeed to accelerate training (Han et al., 2017; Wang et al., 2018). Full hyperparameters can be found in Appendix D.2. Additionally, by finetuning the Llama-7b/13b conversational models, we construct ChatTimeLlama-7b and ChatTimeLlama-13b, based on Llama-2-Chat-7b/13b optimized using reinforcement learning from human feedback (RLHF) (Wang et al., 2018).

## 4. Experiments

### Experimental Settings

**Baselines.** We evaluate and compare the following LLMs as the baselines: **Flan T5**(Chen et al., 2017). An instruction-finetuned T5 model based on chain-of-thought data that increased the number of tasks. **BART**(Wang et al., 2018): An encoder-decoder architecture model that is proficient in abstractive dialogue, question answering, and summarization tasks. **MPT-7b**(Wang et al., 2018): A LLM that is optimized for extremely long inputs. The MPT model with 7b parameters fine-tuned for dialogue generation is used in our experiment. **Falcon-7b**(Wang et al., 2018): A LLM that is optimized for faster inference with decoder-only architecture. The 7B dialogue-fine-tuned version is used. **Vicuna-7b**(Wang et al., 2018): A chatbot trained by fine-tuning L1AMA on a dataset collected from ShareGPT. **ChatGPT**(Chen et al., 2017): A chatbot based on GPT-3.5 LLM that is capable of having natural conversations. **Llama2-7b/13b-chat**(Wang et al., 2018): Llama-2 is a collection of open-sourced LLMs that outperform other models in most tasks. The chat-fine-tuned Llama2-7b/13b is used.

**Metrics.** Our evaluation can be roughly divided into automatic and human evaluation. In automatic evaluation, we first report precision, recall, and F1 scores of event predictions. For explanation evaluation, we choose BLEU (Krishna et al., 2015) (unigram, bigram, 3-gram, 4-gram) and ROUGE (Ross et al., 2016) (rouge1, rouge2, rougeL) to compare the explanation generated by the LLMs with the golden explanations in the testing set. Besides the metric-based methods, we also report the BertScore (Krishna et al., 2015) that computes the similarity based on PLMs. We use the same evaluation criteria introduced in Sec. 3.3 for human evaluation, namely correctness, completeness, and fluency.

### Automatic Evaluation Results

#### 4.2.1. Prediction Evaluation.

In Table 4, we present compelling evidence of the substantial enhancements achieved through the fine-tuning of the ChatTimeLlama-7b model. Notably, our finetuned Llama2-7b model surpasses its baseline counterpart across multiple performance metrics. Specifically, we observe impressive F1 gains

Figure 4. The automatic evaluation scores of finetuned Llama2 with various percentages of dataset usage. From left to right: F1 scores of each category, BLEU and ROUGE scores, BERTScore.

Figure 3. The pipeline of finetuning and evaluating TimeLlama series models. GIG Strategy denotes our proposed dataset construction approach.

    & Pos. & Neg. & Neu. & ICEWS14 & ICEWS18 & 0515 \\  Train & 11703 & 8705 & 5360 & 10327 & 7651 & 7790 \\ Test & 435 & 300 & 266 & 387 & 351 & 263 \\   

Table 3. The statistics of constructed dataset. Pos., Neg., Neu. denote the number of positive, negative, and neutral samples, respectively.

improvements of 44.0, 32.5, 56.3, and 49.2 across four categories: positive, negative, neutral, and overall. These figures underscore the efficacy of our fine-tuning approach, even in the presence of noise within the training dataset. Notably, this underscores the capacity of LLMs to leverage high-quality generated datasets by instruction-tuning for substantial performance enhancements.

**Bigger LLM is not always better.** Interestingly, increasing the model scale does not necessarily improve performance. Doubling the parameters from Llama2-7b-chat to Llama2-13b-chat yielded

    &  &  &  &  &  \\    & Prec & Recl & F1 & Prec & Recl & F1 & Prec & Recl & F1 & Prec & Recl & F1 \\  Flan T5 & 62.9 & 29.2 & 39.9 & 31.4 & 57.0 & 40.5 & 32.2 & 30.8 & 31.5 & 45.3 & 38.0 & 38.0 & 737 \\ BART & 45.7 & 28.2 & 34.9 & 26.3 & 11.7 & 16.2 & 21.5 & 18.3 & 19.8 & 33.6 & 27.0 & 25.3 & 738 \\  MPT-7b & 48.5 & 64.6 & 55.4 & 39.5 & 35.7 & 37.5 & 25.8 & 14.7 & 18.7 & 39.8 & 42.7 & 40.3 & 798 \\ Falcon-7b & 47.7 & 56.6 & 51.7 & 37.9 & 22.0 & 27.8 & 19.9 & 23.3 & 21.5 & 37.4 & 37.4 & 36.5 & 760 \\ Vicuna-7b & 48.4 & 80.5 & 60.4 & 41.3 & 21.3 & 28.1 & 35.8 & 16.5 & 22.6 & 42.7 & 45.6 & 40.4 & 761 \\ ChatGPT & 90.9 & 39.1 & 54.7 & 29.5 & 31.7 & 30.5 & 30.7 & 56.8 & 39.8 & 56.5 & 41.6 & 43.5 & 762 \\ Llama2-7b-chat & 50.1 & 83.9 & 62.7 & 41.9 & 13.0 & 19.8 & 27.4 & 18.4 & 22.0 & 41.6 & 45.3 & 39.1 & 763 \\ Llama2-13b-chat & 51.3 & 53.8 & 52.5 & 40.0 & 26.0 & 31.5 & 28.0 & 36.8 & 31.8 & 41.7 & 41.0 & 40.7 & 764 \\  TimelLlama-7b & 90.1 & 97.6 & 93.7 & 67.6 & 84.9 & 75.3 & 97.8 & 55.1 & 70.5 & 84.6 & 82.7 & 81.5 & 765 \\ ChatTimelLlama-7b & 91.3 & 99.3 & 95.2 & 68.3 & 86.0 & 76.1 & 98.7 & 55.6 & 71.2 & 86.4 & 83.7 & 83.1 & 766 \\ TimeLlama-13b & 94.6 & **100** & 97.2 & 73.9 & 91.3 & 81.7 & **99.4** & 63.5 & 77.5 & 89.6 & 87.7 & 87.3 & 767 \\ ChatTimelLlama-13b & **96.2** & 99.5 & **97.9** & **75.0** & **94.0** & **83.4** & 98.9 & **65.0** & **78.5** & **90.6** & **88.7** & **88.4** & 768 \\   

Table 4. The prediction performance of each model on gold temporal reasoning testing set. The overall denotes the weighted average precision, recall, and F1 score.

Figure 5. The box plots of human evaluation for each LLM. The dashed line denotes the mean value and the bold line indicates the median value.

only marginal gains, with Llama2-7b-chat actually outperforming Llama2-13b-chat on the positive class. For instance, Llama2-7b-chat has a 10.2 F1 gain compared with Llama2-13b-chat in the positive category. Another example is the comparison between MPT-B and Flan T5. For instance, when we examined the 'unsure' category, we observed that Flan T5 demonstrated an impressive F1 score of 31.5. It outperforms both MPT-7b and Falcon-7b, which achieved F1 scores of 18.7 and 21.5, respectively.

**ChatGPT performs mediocre in the zero-shot setting.** Notably, even though our dataset is generated by prompting ChatGPT, it is evident that ChatGPT exhibits suboptimal performance when presented with direct prompts, in contrast to our dataset construction approach. To provide a comprehensive view of ChatGPT's performance, we compare it with Vicuna-7b, a model that was not involved in the dataset construction process. The results reveal that ChatGPT achieves an overall F1 score of 43.5, while Vicuna-7b demonstrates a comparable F1 score of 40.4. Furthermore, our fine-tuned model, Llama2-7b-chat, exhibits a substantial 39.6 F1 point improvement over ChatGPT's performance.

#### Explanation Evaluation

In Table 5, we present the automatic evaluation results for the explanation generation. Notably, our fine-tuned variant, ChatTimeLlama-7b, demonstrates remarkable improvements across all key evaluation metrics. For instance, when compared to the baseline Llama2-7b-chat, ChatTimeLlama-7b exhibits substantial enhancements in BLEU, ROUGE, and BertCore scores, with gains of 35.1, 19.3, and 6.4 points, respectively. These results underscore the significant potential for enhancing the explainable temporal reasoning capabilities of LLMs through instruction tuning based on high-quality datasets.

Parallel to our prediction evaluation, our examination of explanation quality yields insightful observations. First, our explanation evaluation results also demonstrate that ChatGPT with direct prompting exhibits limitations in generating coherent reasoning explanations. For example, the BLEU and ROUGE scores of ChatGPT are 31.1 and 37.1 while Llama2-7b-chat can also achieve comparable performance, i.e., 26.8 BLEU score and 38.4 ROUGE score. We include a failure example of ChatGPT in Appendix B.3. Second, the explanation quality of TimeLlama-13b is not better than that of TimeLlama-7b. For example, ChatTimeLlama-7b achieves a 61.9 BLEU score while ChatTimeLlama-13b has 46.3 BLEU. This may be due to overfitting, and lack of grounding where maximizing prediction harms explainability.

Another interesting finding is that even Flan T5 and BART can achieve comparable performance on prediction evaluation, these two LLMs along with MPT-7b produce subpar explanations compared to other LLMs. One possible reason could be the different coverage of their training dataset and the difference between "encoder-decoder" and "decoder" only architecture.

### Human Evaluation Results

To provide an objective assessment of the quality of the generated explanations, two experienced annotators evaluated explanations from four language models: Llama2-7b, TimeLlama2-7b, ChatGPT, and Vicuna-7b. The annotation guidelines and annotator qualifications are detailed in Appendix A. 50 explanations from each model were randomly selected, paired with the corresponding question, and evaluated by the annotators. As shown in Figure 5, the results demonstrate that overall the TimeLlama2-7b model achieved the highest scores across the three assessment criteria. Specifically, all models generated fluent explanations, as indicated by the high fluency scores. Llama2-7b and ChatGPT performed similarly on correctness and completeness. Compared to the baseline Llama2-7b, the TimeLlama2-7b showed significantly improved correctness and completeness, suggesting that finetuning on the high-quality dataset enhanced its ability to provide coherent temporal reasoning explanations. Cohen's kappa coefficients in Appendix A also show a high level of inter-annotator agreement for most model evaluations. In summary, the finetuned Llama2 model generated the highest quality explanations according to the human evaluation, demonstrating the efficacy of finetuning on a curated dataset to improve the explanatory capabilities of language models for temporal reasoning.

### Fractional Data Trains LLM Reasoning Skills

Previous experiments have demonstrated that fine-tuning LLMs on high-quality datasets can significantly improve their ability to provide explainable temporal reasoning. This leads to an investigation of the minimum amount of high-quality data required to improve the explainable temporal reasoning capabilities of LLMs. To test this, 10%, 50%, and 75% of the training samples were randomly selected from the dataset to fine-tune Llama2-7b using the same fine-tuning methodology. Interestingly, Llama2 fine-tuned on reduced amounts of data achieved comparable or better performance on automatic prediction and explanation evaluation metrics in some cases (Fig. 4). For instance, Llama2 fine-tuned on 75% of the dataset attained a higher F1 score for prediction accuracy compared to the full dataset. Moreover, the Llama2 fine-tuned on just 10% of the data obtained similar performance on explanation metrics such as ROUGE score and BERT score versus Llama2 fine-tuned on 75% and the full dataset. These results demonstrate that with guidance from even a small volume of high-quality data, the temporal reasoning and explanation generation skills of LLMs can be substantially enhanced.

## 5. Conclusion

In this work, we propose the first task of explainable temporal reasoning, to predict an event's occurrence at a future timestamp and generate the explanation for their prediction. To support this task, we introduce a novel dataset ExpTime, containing 26k examples derived from temporal knowledge graphs, developed by a novel knowledge-graph-instructed-generation strategy. Based on this dataset, we develop TimeLlama, an open-source LLM series tuned with instructions for temporal reasoning and explanation generation. Experiments demonstrate the SOTA performance of TimeLlama on future event prediction and explanation generation compared to other LLMs. We find the instruction-tuning using high-quality data is critical for improving LLM's temporal reasoning and explainability. We discuss associated ethical considerations and limitations in Appendix F. In the future, we plan to expand the breadth and diversity of our benchmark dataset by incorporating more temporal reasoning tasks.