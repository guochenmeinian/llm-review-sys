# Decoupled Variational Graph Autoencoder for Link Prediction

Anonymous Author(s)

###### Abstract.

Link prediction is an important learning task for graph-structured data, and has become increasingly popular due to its wide application areas. Graph Neural Network (GNN)-based approaches including Variational Graph Autoencoder (VGAE) have achieved promising performance on link prediction outperforming conventional models which use hand-crafted features. VGAE learns latent node representations and predicts links based on the similarities between nodes. While the inner product based decoder effectively utilizes the node representations for link prediction, it exhibits sub-optimal performance due to the intrinsic limitation of the inner product. We found that the the cosine similarity and norm simultaneously try to explain the link probability, which hinders the gradient flow during training. We also point out the message passing scheme is unexpectedly dominated by the nodes with large norm values. In this paper, we propose a stochastic VGAE-based method that can effectively decouple the norm and angle in the embeddings. Specifically, we relate the cosine similarity and norm to two fundamental principles in graph: _homophily_ and _node popularity_ respectively. Following the principles in graph, we define a generative process in the VGAE framework. Our learning scheme is based on a hard expectation maximization learning method; we infer which of the two has been exerted for link formation, and subsequently optimize based on this guess. We comprehensively evaluate our proposed method on link prediction task. Through extensive experiments on real-world datasets, we demonstrate our model outperforms the existing state-of-the-art methods on link prediction and achieves comparable performances on other downstream tasks such as node classification and clustering. Our code is at [https://anonymous.4open.science/r/dygae-A0B4](https://anonymous.4open.science/r/dygae-A0B4).

2018 acmcopyright CCS CONCEPTS 2018

2018

## 1. Introduction

Graph-structured data is omnipresent in various fields, such as citation networks, social networks, recommender systems, and knowledge graphs. In graph-structured data, links reflect the relation between the nodes, where nodes in such applications can be documents, web users, items, or concepts. One of the main challenges with graph-structured data is link incompleteness, where many edges are missing or unobserved. Due to this nature, link prediction is one of the critical tasks in network analysis and has attracted increasing attention. Under the homophily assumption (Gan et al., 2016; Wang et al., 2017; Wang et al., 2018), link prediction approaches attempt to estimate the link through evaluating the similarity within a pair of nodes based on observed links and the associated attributes of nodes.

The link prediction problem has been a long-standing challenge and has been extensively studied within academia and industry. The recent success of Graph Neural Networks (GNNs) has boosted research on various graph learning tasks, including capturing the relations between nodes in graph-structured data. Several recent studies present promising results on link prediction (Wang et al., 2017; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). Methods adopting GNNs, including Graph Convolutional Network (GCN) (Shen et al., 2016) automatically learn latent representations from node attributes and their local neighborhoods. The core of GNN lies in the message-passing scheme (Wang et al., 2018; Wang et al., 2018), where the the node embeddings are passed along the edges of the graph. GNN-based approaches replace hand-crafted algorithms to optimize graph and permits more flexible modeling. Variational Graph Autoencoder (VGAE) (Wang et al., 2018) is a variational probabilistic generative framework with GCN. With its flexibility and proven performance, various extensions have been proposed within this framework. However, they exhibit underperformance in link prediction on low-degree and high-degree nodes. We hypothesize that this is due to the intrinsic limitation of inner-product decoder in VGAE.

The inner-product can be decomposed into the cosine similarity (or normalized inner product) and norm, where these two components compete each other simultaneously trying to explain the link probability. This becomes problematic when one of the components becomes easier to perform gradient descent during backpropagation, which motivates our study. Figure 1 shows the norm of node embeddings from Cora and PubMed datasets obtained via VGAE with respect to the node degrees. For the nodes with high degree or low degree, the loss with inner-product can be easily compensated through the norm values. In Figure 1a, we observe low-degree nodes tend to have small norms in their node embeddings, and vice versa. For high-degree nodes, the norm of node embedding gets high to account for the high number of edges; thus cosine similarity has minimal effect on the link probability. The same logic applies to the opposite case with low-degree nodes. For high-degree and low-degree nodes, optimizing the node embeddings in terms of the direction in vector space can be challenging. However, the high norm (magnitude) value causes a negative impact on message passing scheme, where messages are aggregated from the neighboring nodes. Specifically, we illustrate how high norm of node embeddingharms the message passing in Figure 0(b). When the messages are propagated to the target node in gray, the node with high norm unexpectedly dominates the messages. As such, when the high-degree node happen to behave differently with embedding vector heading toward different direction, the message passing scheme can hinder the learning of node embeddings.

To solve the aforementioned problems, we propose a novel generative algorithm to decouple the two components in the inner product within VGAE framework. Specifically, we incorporate two different embedding spaces, namely the embedding space for _homophily_ with the cosine similarity based decoder and the embedding space for _node popularity_ with the norm based decoder. Through this approach, we can effectively learn each embedding and account for the links independently. Moreover, when decoupled, we can also restrict the message passing only on angular embeddings to avoid the _domination_ effect in Figure 0(b). However, even with the separate-space approach, decoupling the two effects is not trivial as the two effects are not observable. A simple remedy is to focus only on one (homophily) by ignoring the other (node popularity). While it is effective than vanilla-VGAE, it is still sub-optimal. Our model considers both components individually through decoupling the two properties through the proposed stochastic generative process. We also propose a hard Expectation-Maximization (EM) algorithm to perform end-to-end learning. Our model achieves state-of-the-art (SOTA) results in link prediction on attributed networks.

We summarize the main contributions of our work as follows:

* We discuss the intrinsic limitation of VGAE decoder, and propose an end-to-end approach within the framework of VGAE, which decouples norm and angular node embeddings through the proposed generative process.
* We comprehensively evaluate our method on numerical experiments and show that it consistently outperforms the existing state-of-the-art link prediction models.
* We additionally show how the latent embeddings learned only from the normalized form excluding the norm component performs for link prediction, which already achieves better results than the current SOTA model.

## 2. Related Work

Link prediction.While the work in link prediction spans many fields over a long period of time, we review three major streams of research in link prediction: _heuristic methods_, _network embedding methods_, and _graph neural networks_. Heuristic methods compute the likelihood of an edge to appear based on different heuristic metrics between nodes within a pair. Common neighbor (Zhu et al., 2017), Katz index (Katz, 1999), and PageRank (PageRank, 2001) rely on hand-crafted rules. Network embedding methods map nodes in a network to lower dimensional spaces while effectively preserving the network structure. Random-Walk based models such as DeepWalk (Perozzi et al., 2014) and Node2Vec (Perozzi et al., 2015) define notations of the node's neighborhood and learn latent space representations of social interactions. Methods adopting GCN automatically learn node embeddings for link prediction and other downstream tasks, such as node classification and community detection. Our model and the baseline approaches in this study perform link predictions using graph neural networks. We later review the link predictions with GNNs as baseline models in more detail (see Section 5.2).

Node Popularity.Node popularity has been frequently discussed in the community detection literature prior to the introduction of GNNs. In (Katz et al., 2017), the authors proposed the degree-corrected block-model for community detection. In (Song et al., 2018), heterogeneity of actor degrees has been taken into account for latent cluster random effects models. The authors in (Katz et al., 2017) integrated node popularity into the mixed membership stochastic blockmodels (Brockman et al., 2018), which is perhaps the closest to ours in spirit. In their approach, the probability of a link is defined by the membership assignment and the node popularity through linear summation. Our approach however decouples the two effects and lets one of each effect attend to the probability of a link, which avoids the smoothing of the two. The decoupling can also prevent message passing the high-magnitude embeddings. Node popularity has been more frequently accounted for in the Recommender System (RS) literature. Previous studies (Katz et al., 2017; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) in this literature incorporated item popularity to RS. To the best of our knowledge, this is the first in VGAE to incorporate node popularity. We emphasize that our proposed model is not just an extension of previous studies in homophily and node popularity, but rather a study finding the problem of the competition between norm and angular embeddings. We associate them with two popular phenomena in network science simply for better explanation.

Figure 1. (a) Degree of nodes and their \(\|z\|\)s learned from VGAE. The norm of node embedding increases respect to its degree. (b) Node embedding with large norm unexpectedly dominates the message passing to the node in gray.

## 3. Preliminaries

### Problem Formulation

We are interested in the problem setting where we are given an undirected, unweighted graph \(=(,)\) with \(N=||\) nodes and a \(N D\) feature matrix \(\). Let \(\) be an adjacency matrix of \(\), and \(\) be its degree matrix. With these settings, the goal is to predict whether an edge exists within an unobserved pair of nodes based on \(\) and observed links with \(\). This can be achieved by learning the \(F\)-dimensional node embeddings (or stochastic latent variables) \(_{i}\) that best reconstruct the network \(}\), where we can summarize the \(\{_{i}\}\) in an \(N F\) matrix \(\).

### Background: VGAE For Link Prediction

_Variational Graph Autoencoder._ VGAE (VGAE, 2017) tries to solve the problem of observed link predictions by learning the node embeddings (or latent variables) \(_{i}\) that best reconstruct the network \(}\). VGAE learns a distribution over the latent space for each input, where \(_{i}\) is sampled from the distribution. Given \(=\{_{1},,_{N}\}\), a simple inner-product decoder in VGAE reconstructs the adjacency matrix as \(}=(^{})\) with a Gaussian prior. Thus, the encoder \(q_{}}(,)\) becomes the inference model which learns the variational posterior; the decoder \(p_{}}()\) becomes the generative model.

_Inference model._ Due to the intractability of the marginal likelihood, true posterior is approximated by a Gaussian distribution in variational inference (VGAE, 2017; 2017).

\[q_{}}(_{i},)=( _{i},(_{i}^{2})), \]

where the mean vector and variance vector for Gaussian distribution are parameterized by a two-layer graph convolutional network (GCN) (Vaswani et al., 2017): \(_{i}=_{}(,)\) and \(_{i}=_{}(,)\). Here, the node features are naturally incorporated through input \(\).

_Generative Model._ In the generative model, the similarity are computed across all pairs of nodes, where the inner product is used along with sigmodal function.

\[p(A_{ij}=1_{i},_{j})=(_{i}^{} _{j}), \]

where the probability of a link between node \(i\) and \(j\) is determined by the similarity between the node embeddings. Here, the inner product is used for similarity measure, which is fed into sigmoid.

_Observation._ The inner-product in the decoder is a simple yet effective method, where the VGAE and its extensions have been achieving SOTA performances. However, we found that the inner-product decoder likely results in sub-optimal performance when performed on low-degree nodes or high-degree nodes. We hypothesize that the two factors, namely norm and cosine similarity, provide different and complementary effects to the inner product while the two effects are tightly coupled and trained jointly. This observation and the property motivate us to decouple the two effects for better training. In this study, we correspond the norm to _node popularity_; the cosine similarity to _homophily_. Thus, the main research question we consider is **RQ1)** how to decouple the node popularity and homophily in graph structured data. Decoupling the two property is not trivial as the two simultaneously try to account for the probability of link generation. In backpropagation, the gradient can still flow to any direction unless specified. This becomes problematic when one of the factor is favored dominantly in backpropagation over the other factor. Similar observation also has been discussed in (Brockman, 2017), where only the norms of node embeddings gets close to zero for nodes with small degree. Their remedy to this problem was using the normalized embeddings (or the cosine similarity). However, the approach in (Brockman, 2017) can only account for the _homophily_. Their experimental results also reveal how the normalized embeddings become effective especially when the network is sparse. While it effectively addresses the problems with near-zero-degree nodes, due to the additional constraints, it becomes less flexible than the vanilla-VGAE. In fact, the model in (Brockman, 2017) exhibits performance degradation on link predictions associated with high-degree nodes (see Appendix B.1).

When the node popularity and homophily can be decoupled, it brings us to the second research question. We are interested to see **RQ2)** whether the embedding for homophily itself becomes more accurate when the node popularity effect gets removed during training. When the two effects coexist and are trained jointly, we suspect that the two effects interfere each other, which may distort the node embeddings. As such, we expect to have more accurate embedding when the node popularity gets removed during training. However, this is not trivial as the property (node popularity vs homophily) that contributes to generation of link is not observable. To this end, we propose a novel approach that can decouple the node popularity and the homophily for link prediction. While there have been some studies in recommendation system literature using GNN for addressing item popularity (Goyal et al., 2017; Wang et al., 2017; Wang et al., 2017), to the best of our knowledge, this is the first study to incorporate node popularity in VGAE for link prediction.

## 4. Proposed Method

Here, we address the aforementioned questions by introducing a model within VGAE framework. Our main idea is threefold; (a) we consider two properties in graph namely _node popularity_ and _homophily_ for link predictions, (b) we propose an EM-like learning algorithm that alternates between estimating the associated effects from the two and learning their embeddings, (c) we stochastically estimate the respective effect and achieve more attended representations.

The term _node popularity_ also appears in other literature, and we introduce our definition of node popularity for this study.

**Definition 1** (Node Popularity in Graph).: For a graph \(=(,)\), given two nodes \(i,j\), a link can be generated even when there is **no similarity** between node \(i\) and \(j\). An undirected link is defined by a node popularity function of node \(i\) or \(j\).

In our generative process, at each interaction between a given pair, one of the _promising_ scenario under homophily or node popularity is stochastically sampled. This is particularly a natural idea, where we observe frequently in real-world. In social network, users also make friends with or follow _popular_ users without sharing common interest. In this section, we elaborate our generative model in VGAE framework, and provide the learning algorithm. Our model decouples the node embeddings into two components: norm and angular, which corresponds to the decoder only using the norms and the decoder only using the cosine similarities separately. We nameour model "Decoupled VGAE (D-VGAE)". The overall framework and its learning process is illustrated in Figure 2.

### Generative Process

_D-VGAE generative process_. We assume each node can establish a link with others under one of the two phenomena: homophily or node popularity. For each pair, instead of stochastically selecting one of the two phenomena directly, we first sample the value (in binary) of interaction through Bernoulli with respect to the similarity measure between the two nodes. We take this approach for three reasons: (1) two phenomena are not observable and cannot be compared directly unless one of the function is pre-given; (2) homophily can be directly inferred from the node features, while the node popularity can be inferred indirectly by removing homophily; and (3) _decoder collapse_ (two decoders behave similarly) can be better prevented. We start by extending the embedding vector \(\) from VGAE to \(^{p}\) and \(^{h}\) for node popularity and homophily respectively, and further define the generative process. The overall generative process can be summarized as below:

* For each node \(i\), sample node latent variables for homophily: \(_{i}^{h}(^{h},^{h})\).
* For each node \(i\), sample node latent variables for node popularity: \(_{i}^{p}(^{p},^{p})\).
* For each pair of node \(i\) and \(j\), draw a binary undirected link from a Bernoulli distribution through two-stage process: 1. **Homophily**: Draw a binary undirected link from a Bernoulli \[A_{ij}(((_{i}^{h},_{j}^ {h}))),\] (3) where \(()\) is a sigmoid function, and \(()\) is a similarity measure function. 2. **Node Popularity**: If no link were sampled in stage 1, draw a binary undirected link \[A_{ij}(((_{i}^{p},_{j}^ {p}))),\] (4) where \(()\) is a function that measures the strenght of node popularity.

In vanilla-VGAE, we identify that the engagement of norm and cosine similarity within the inner product hinders the learning of node embedding. Through our generative process, we expect the homophily (sim\(()\)) only account for the quasi-cosine similarity; and expect the node popularity (\(()\)) only account for the quasi-norm. Thereby, we use normalized inner product for \(()\) and summation of each of the strength of given pair for \(()\). More details on \(()\) is provided in the Appendix B.2. Through the two-stage generative process we propose, homophily and node popularity can be decoupled and considered separately. In the following, we elaborate the above through inference model and generative model within the VGAE framework. To fully verify our hypothesis, we add only necessary changes incorporating our scheme. Finding better posterior distribution can be found in (Golovin et al., 2013), and also in (Golovin et al., 2013; Golovin et al., 2013; Golovin et al., 2014) from more general VAE literature. We also refer the readers to (Golovin et al., 2013; Golovin et al., 2013; Golovin et al., 2014; Golovin et al., 2015; Golovin et al., 2016) for designing better decoder than a simple inner product.

_Inference model._ We use a probabilistic encoder to perform variational posterior inference. The posterior distribution (Eq. 1) is parameterized by encoder which generates the mean \(_{i}\) and the \(_{i}\). As our proposed generative process requires \(_{i}^{h}\) and \(_{i}^{p}\), we let our probabilistic encoder generate two sets of variational parameters in the same sense. For learning the embedding of \(^{p}\), we remove the message passing scheme, as node popularity is the characteristic of a node itself and to prevent the _domination_ effect. We use the encoder from VGNAE (Benson et al., 2009) as one of the building blocks in our implementation. The \(l^{2}\)-normalization in VGNAE can naturally account for the cosine similarity (or normalized inner-product) assumption for homophily. To accomplish this \(l^{2}\)-normalization, a GCN called _graph normalized convolutional network_ (GNN) was proposed in (Benson et al., 2009), which is defined as follows:

\[(,,s)=s^{-} ^{-}g(), \]

where \(g(\{_{1},_{2},...,_{n}\}^{})=[_{1}}{|_{1}|},_{2}}{|_{2}|},...,_{n}}{|_{n}|}]^{}\), \(\) is a learnable weight matrix, and \(s\) is a scaling factor which is set to 1.8 in the following experiments. In a nutshell, normalization in the encoder of GNAE brings same effect as having cosine similarity based decoder. Using GNCN as our encoder can also verify the effectiveness of our proposed scheme over the current SOTA model: VGNAE.

_Generative model._ Through reparameterization trick (Golovin et al., 2013), \(^{h}\) and \(^{h}\) are sampled from the variational distribution. Given the \(^{h}\) and \(^{h}\) for all the nodes, every possible pair are compared through two-stage process. We first draw a binary undirected link using the Gumbel trick (Gumbel, 1955), where we have \(p(A_{ij}=1)=(_{i}^{h}_{j}^{h})\). When a positive link is sampled at this stage, it becomes the final link for \(}\); otherwise a binary undirected link can be sampled with \(p(A_{ij}=1)=((_{i}^{p}+_{j}^{p})^{})\), where \(\{_{i}^{p}\}\) in an \(N 2\) matrix \(^{p}\). We elaborate why we use two-component vector \(^{p}\) and take one scalar for computing strength of node popularity treating the other as dummy. The encoder generates normalized vectors \(^{h}\) through GNCN. When performing inner product with normalized vectors, the inner product in sigmoid function becomes bounded within acceptable limits through scaling factor. The value \(^{p}\) should be bounded as such, otherwise model loses its stability. Therefore, we use the two-component vector for \(^{p}\) for normalization, and only use one of the component: \(^{p}\). We can naturally have a bounded values of \(^{p}\) by separately normalizing the given vector through GNCN. The links of interest in the literature is the undirected link, and we can sum the two node properties to account for the link due to node popularity. We also only perform message passing on \(^{h}\) but not on \(^{p}\). This is because node polarity is not a feature that propagates through its neighbor, but rather an unique value of a given node itself. Most of all, this avoids the domination effect from Figure 1b. Our additional experimental results in Appendix B.3 justifies our idea. The most closest to ours is (Golovin et al., 2013), which integrates node popularity with community detection. However, their approach tries to incorporate node popularity in a mixed manner, while we are more interested in decoupling the two effects. We also incorporate their idea into VGAE framework for further analysis and discuss on the differences in Section 5.4.

### The Variational Bound

_Evidence Lower-Bound._ For the inference and learning, we optimize the evidence lower bound (ELBO) with respect to the variational parameters \(\).

\[ p() _{q_{}(Z^{h,h}|,)}[ p _{}( Z^{(p,h)})]\] \[-(q_{}(Z^{p},)||p(Z^{p}))- (q_{}(Z^{h},)||p(Z^{h}))\] \[}}{{=}}(, ;). \]

The first term of RHS in Equation 6 can be reformulated into following expression:

\[_{q_{}(Z^{h,h}|,)} p_{^{h}}( Z^{h})p^{h}+_{q_{}(Z^{h,h}|,)} p_{ ^{h}}( Z^{p})q^{h}, \]

where \(p_{^{h}}( Z^{h})\) and \(p_{^{h}}( Z^{p})\) follow Equation 3 and 4 respectively, which accounts for homophily and node popularity. The \(p^{h}\) and \(q^{h}=1-p^{h}\) are Bernoulli parameters which reflects the probability of a link generated under homophily. We rely on EM-like learning algorithm to estimate each probability, where we provide the details in the following section. The second and third term of the ELBO are the Kullback-Leibler (KL) divergences between the variational distribution and true prior for each latent embedding. Following previous work, we use the Gaussian prior with \(p(_{i})=N(_{i} 0,)\) and assume the posterior approximation \(q(_{i}_{i},)\) as Gaussian, which brings the closed form of KL-divergence. It is also worth noting that the inference model for node popularity takes \(\) instead of \(\) for its input (see second term in RHS in Equation 6). This is mainly because node popularity is one of the characteristics of node itself, and the dependence on \(\) has been intentionally dropped.

### Training and Inference

Our model follows a 'winner-take-gradient' training strategy (Zhu et al., 2017) by positing the problem to hard EM algorithm for end-to-end learning. We thus introduce additional parameter \(\) as an indicator, where \((_{ij}=1)=p^{h}_{ij}\). At each interaction, \(_{ij}\{0,1\}\) is sampled from Bernoulli. The overall learning process is provided in Algorithm 1.

```
0:\(^{N D}\), \(^{N N}\)
0:\(^{h},^{p}\) for a single GNCN encoder while normalizations are performed separately for homophily and node popularity.  Initialize \(^{h},^{p}\) while not converged do  Obtain batch of nodes Expectation step: for node \(i,j\) in a batch do  Sample \(_{ij}\) from the variational distribution with the latent setting of \(p^{h}_{ij}\). endfor  Maximization step: Take average of gradients from the batch to maximize \((,;)\) with \(_{ij}\) from Expectation step. endwhile
```

**Algorithm 1** Training procedure for D-VGAE

Specifically, when we sample \(_{ij}\), we borrow the latest setting of \(p^{h}\) which can be obtained from Equation 3 with temperature added. We anneal the temperature from high value to low value in a way the model can sufficiently explore the two scenarios and avoid _over-confidence_ issue. We also reveal how our 'winner-take-gradient' training strategy (hard EM) achieves better performance than soft counterpart through our experimental results in Appendix B.4.

Figure 2. The overview of D-VGAE’s learning scheme.

## 5. Experiments

### Experimental Setup

ConfigurationBefore presenting our experimental results, we first describe the experimental setup for the empirical evaluations of our proposed method. The model is implemented and trained based on PyTorch Geometric (Paszke et al., 2017) library. The experiments are conducted using an NVIDIA RTX A6000. Every experiment in this paper follows the experimental protocols in (Paszke et al., 2017) with 10% of links for testing, and 5% of links for validation. The models are trained on 85% of links, and the associated node features. In validation and testing, we compare the positive edges against the same number of negative edges which have been sampled randomly from pairs of unconnected nodes. Following a standard manner of learning-based link prediction, we also perform 10 runs of experiments and report _area under the ROC curve_ (AUC) and _average precision_ (AP) on the test set. The embedding of each node is learned in 256-dimensional latent space. Further details on configuration is provided in the Appendix.

DatasetsWe evaluate the proposed approach based on benchmark network datasets with node features for undirected link prediction. Table 1 summarizes each dataset used across our experiments. The datasets in top three rows in Table 1 have always been used in previous methodologies for performance evaluations. Cora (Corna, 2010), CiteSeer (Citeer, 2010), and PubMed (Citeer, 2010) are citation network datasets containing list of citation links between documents and bag-of-words (BOW) feature vectors for each document. The CS, Physics, Computers, Photo datasets have been used in (Bauer et al., 2017) along with Cora, CiteSeer, and PubMed. CS and Physics are co-authorship graphs based on the Microsoft Academic Graph from the KDD Cup 2016 challenge. In CS and Physics datasets, authors are connected if the authors co-published a paper, where paper keywords for each author's papers are aggregated for node features. Computers and Photo are segments of the Amazon co-purchase graph in (Paszke et al., 2017). In Computers and Photo datasets, nodes represent goods, edges are generated if the two goods are frequently bought together, and bag-of-words encoded product reviews are used as node features. While these four datasets have been originally used for node classifications when first appread in (Paszke et al., 2017), we use these datasets for link prediction as in (Bauer et al., 2017) removing node labels. As in the previous studies, if the original links are directed, they were treated as undirected links. Node labels are only being used in specific downstream tasks: classification and clustering.

### Baselines

We compare D-VGAE against the competitive baseline methods including the current SOTA model. GAE and VGAE are introduced in (Paszke et al., 2017), where both GAE and VGAE use the Graph Convolutional Network (GCN) (Goyal et al., 2016) encoder and a simple inner-product decoder. The GCN in GAE/VGAE has been replaced by a simple linear model in LGAE (Laszke et al., 2017). Experimental results in (Laszke et al., 2017) shows how the simple first-order linear encoders are effective as the popular GCNs. ARGA (Chen et al., 2016) is an adversarial graph embedding framework for graph data, which enforces latent representation to match a prior distribution. GIC (Citeer, 2010) leverages cluster-level node information through differentiable \(K\)-means. sGraphite (Chen et al., 2016) tries to enlarge the normal neighborhood through maximizing the mutual information. SIG-VAE (Krizhevsky et al., 2012) combines semi-implicit variational inference (SIVI) (Zhu et al., 2017) and normalizing flow (NF) (Zhu et al., 2017; He et al., 2017; He et al., 2017) into the VGAE framework, and also proposes a new Bernoulli-Poisson link decoder. MSVGA (Krizhevsky et al., 2012) tries to learn multiple sets of low-dimensional vectors of different dimensions, which is extended by SPN-MVGAE (Zhu et al., 2017) that incorporates conditional sum-product networks as constraints. GIC+WP is reported as the best performing model in (Zhu et al., 2017), which applies a new pooling scheme called WalkPool on embeddings learnt from GIC. Among these baseline models, GAE/VGAE (Bauer et al., 2017) achieves the best performance both on Cora and CiteSeer datasets; GIC+WP(Zhu et al., 2017) achieves the best performance on PubMed. GIC+WP cannot be compared directly to other end-to-end baselines. Models using class labels associated to nodes haven't been included as our baseline methods.

### Numerical Results

Main ResultsWe quantitatively evaluate D-VGAE through experiments on the benchmark datasets for link prediction. Our first set of experiments compares D-VGAE against existing baseline methods. The experiment has been standardized by (Paszke et al., 2017), where 10% of links were used for testing and other 5% of links for validation. We perform link predictions using the baseline approaches and D-VGAE using the rest of 85% of the links and the features of the associated nodes. With the observed link and its full feature data used as A and X for the input through D-VGAE, we obtain each probability of the component of \(}\). When 10% of links were sampled for testing, the same number of non-link were sampled. We use these links and non-links as the ground-truth and compare them to our predictions. Through binary classification, for each round of experiments, AUC (area under the ROC curve) and AP (average precision) can be obtained. We perform 10 rounds of link prediction for each dataset across all the models in this study, and report the overall results in Table 2. The best results in each metric are marked in bold. We set all the hyperparameters the same throughout three benchmark datasets, without further tuning on each dataset. From Tables 2 and 3, we observe D-VGAE achieves state-of-the-art results consistently across all the benchmark datasets.

[MISSING_PAGE_FAIL:7]

Figure 3 compares the performance of three models, where VGAE-AMP is our VGAE implementation of AMP in (Huang et al., 2019). In VGAE-AMP, we take into account both homophily and node popularity in a same manner as in AMP, where the logit of link probability is defined by linear sum of node popularity and homophily. In Cora and CiteSeer datasets, we observe VGAE-AMP underperforms VGAE (Amic et al., 2019), while in PubMed, it slightly performs better than VGAE.

The results reflects how decoupling the two effects contributes to the model performance answering **RQ1**. The other research question to be answered was **RQ2**) whether the embedding for homophily itself becomes more accurate when the node popularity effect gets removed during training. The results in Figure 4 provide the answer, where we observe the link prediction performed only using \(}\) without node popularity effect have already improve the performance from the previous SOTA. We believe the embedding for homophily itself becomes more accurate when the node popularity effect gets removed during training letting homophily better captured.

_WalkPool._ In Table 2, D-VGAE+WP on PubMed achieved higher performance than the results from (Wang et al., 2019). The results in Table 5 show that the results we achieved are not merely due to the GNCN (by comparing against VGAE). We also stress that VGAE variants cannot be directly compared with the results from (Wang et al., 2019). VGAE and D-VGAE are end-to-end models, while the model in (Wang et al., 2019) requires node embedding obtained from other models such as GIC, VGAE. The authors reported GIC+WP as their best performing model, which we had as one of our baselines in Table 2.

### Qualitative Evaluations

For qualitative study, we use IMDb network data, and learn node embeddings of \(}\) and \(}\). This dataset has been crawled in 2018, where the edge means the two actors have appeared in a movie. The dataset has 11,384 nodes and 68,264 undirected edges, which is summarized in Figure 5 with degree. The size and color of node represents degree. The dataset we use in this study is featureless network data, where we take featureless approach replacing input \(\) with the identity matrix. We want to verify whether the node popularity we learn is beyond observable degree. The top 10 actors in terms of node degree in the data happen to be all Indian actors with Prakash Raj the highest followed by Mohan Joshi. Interestingly, the popular nodes we found based on \(}\) were quite different from the actors based on degree. The node with highest node popularity was Pierce Brosnan. In top 10, we additionally found Nicolas Cage, Jonny Depp, Sylvester Stallone. We qualitatively show that D-VGAE can capture popular nodes of which the links are established beyond homophily, e.g., across various genres, countries. We also observe that the popularity is not a mere artifact of degree.

## 6. Conclusion

In this paper, we discuss the intrinsic limitation of inner product based decoder in VGAE, where the norm and the cosine similarity both try to explain the probability of link. We propose a novel framework of VGAE through decoupling the two effects and associate each component to its own phenomena: node popularity and homophily. The decoupling also avoids unexpected domination effect in message passing. To effectively decouple the two effects, we propose a two-stage generative process accounting for each individually. We perform end-to-end learning using the hard EM algorithm consistently achieving SOTA results on standard datasets. The link prediction which only uses the node embedding from homophily in our model already outperforms previous models. We hope our study can inspire other researchers to perform further studies in VGAE.

Figure 4. Link prediction with homophily from D-VGAE (center bar in orange for each dataset)

Figure 5. IMDb actor network

   Model & Method &  \\   & AUC & AP \\  GIC (Miller et al., 2019) & -aP & 93.00 & 92.32 \\  +b & 98.72 & 98.72 \\  VGAE (Mic et al., 2019) & -aP & 95.32 & 95.36 \\  +b & 98.89 & 98.88 \\  D-VGAE (ours) & -aP & 98.07 & 97.87 \\  +b & **98.94** & **98.93** \\   

Table 5. Applying WalkPool

Figure 3. Link prediction using VGAE-AMP (Huang et al., 2019) (center bar in orange for each dataset)

* Ahn and Kim (2021) Seong Jin Ahn and MyoungHG Kim. 2021. Variational Graph Normalized AutoEncoders. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_. 2827-2831.
* Aioldi et al. (2008) Edo Aioldi, David Hiei, Stephen Flenberg, and Eric Xing. 2008. Mixed memberships stochastic blockmodels. _Advances in neural information processing systems_ 21 (2008).
* Chen et al. (2020) Alexei Chen, Yihuo Zhou, Abdelahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations. In _Advances in Neural Information Processing Systems_. H. Larochelle, N. Marinato, R. Hadsell, M. H. Salcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 12409-12600. [https://proceedings.neurips.cc/paper/2020/file/d1e1e1d6d6b92232700b1670-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/d1e1e1d6d6b92232700b1670-Paper.pdf)
* Brin and Page (1998) Sergey Brin and Lawrence Page. 1998. The anatomy of a Large-Scale Hierarchical Web Search Engine. In _Proceedings of the Seventh International Conference on World Wide Web_. 7 (9thicum, Australia) (WWW) (Heviker Science Publishers B. V., NM), 107-117.
* Cai et al. (2021) Lei Cai, Andong Li, Jie Wang, and Shuiwang Ji. 2021. Line graph neural networks for link prediction. _IEEE Transactions on Pattern Analysis and Machine Intelligence_ (2021).
* Cremonesi et al. (2010) Paolo Cremonesi, Yehuda Koren, and Roberto Turrin. 2010. Performance of recommender algorithms on top-a recommendation tasks. In _Proceedings of the fourth ACM conference on Recommender systems_. 39-46.
* Di et al. (2020) Xinhan Di, Pengxin Yu, Rui Bu, and Mingchao Sun. 2020. Mutual information maximization in graph neural networks. In _2020 International Joint Conference on Neural Networks (IJCNN)_. IEEE, 1-7.
* Du et al. (2022) Yuntao Du, Xinjun Zhu, Lu Chen, Bahua Zheng, and Yunjun Gao. 2022. HARXG hierarchy-Aware Knowledge and Network for Recommendation. In _Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval_. 1309-1400.
* Feyn and Lemesen (2019) Matthias Feyn and Jan Lemesen. 2019. Fast Graph Representation Learning with PyTorch Geometric. In _ICLR Workshop on Representation Learning on Graphs and Manifolds_.
* Fortunato (2010) Santo Fortunato. 2010. Community detection in graphs. _Physics Reports_ 486, 3 (2010), 75-174. [https://doi.org/10.1016/j.physrep.2010.102](https://doi.org/10.1016/j.physrep.2010.102).
* Geiss et al. (1998) Lee Geiss, Kurt D. Ballacker, and Steve Lawrence. 1998. CnScher: An Automatic Citation Indexing System. In _Proceedings of the Third ACM Conference on Digital Libraries, Pittsburgh, PA, USA_. ACM, New York, NY, USA, 89-98. [https://doi.org/10.1145/276675.2766685](https://doi.org/10.1145/276675.2766685)
* Kopalan et al. (2013) Prem K Kopalan, Chong Wang, Ashuqi Bai, 2013. Modeling overlapping communities with node boundaries. _Advances in neural information processing systems_ 26 (2013).
* Cover and Leskovec (2016) Aditya Cover and Jure Leskovec. 2016. Node2vec: Scalable Feature Learning for Networks. In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_. ACM, 11(3), (KDD '16). Association for Computing Machinery, New York, NY, USA, 858-864. [https://doi.org/10.1145/293962739574](https://doi.org/10.1145/293962739574)
* Grover et al. (2019) Aditya Grover, Aaron Zweig, and Stefano Ermon. 2019. Graphite: Iterative generative modeling of graphs. In _International conference on machine learning_. PMLR, 2343-2446.
* Guo et al. (2022) Zhihao Guo, Feng Wang, Kaixuan Yao, Jye Liang, and Zhiqiang Wang. 2022. Multi-scale variational graph autoencoder for link prediction. In _Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining_. 334-342.
* Hassanzadeh et al. (2019) Arman Hassanzadeh, Ehsan Hajiramezanali, Krishna Narayanan, Nick Duffield, Mingyuan Zhou, and Xiaoming Qian. 2019. Semi-implicit graph variational auto-encoders. _Advances in neural information processing systems_ 32 (2019).
* Jiang et al. (2017) Eric Jiang, Shixiang Gu, and Ben Poole. 2017. Categorical Reparametrization with Gumbel-Softmax. In _Proceedings International Conference on Learning Representations 2017_. OpenReviews. [https://openreviews.com/epjt/jt4k-AE3956e](https://openreviews.com/epjt/jt4k-AE3956e)
* Karet and Jewman (2011) Brian Karet and Mark Jewman. 2011. Stochastic blockmodels and community structure in networks. _Physical review E_ 31 (2011), 041607.
* Kate (1953) Lee Kate. 1953. A new status index derived from sociometric analysis. _Psychometrika_ 18, 1 (1953), 39-43.
* Kingma et al. (2016) Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. 2016. Improved Variational inference with Inverse Autoregressive Flow. In _Advances in Neural Information Processing Systems_. I. Dec, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (Eds.), Vol. 29. Curran Associates, Inc., [https://proceedings.neurips.cc/paper/2016/file/d2e1d6b6b67e1d6b67e1d6b67e2d6b67e2d67e2d67e2d67e2d67e2d67e2d67e2d67e2d67e2d67e267e267e267e267e267e267e267e267e267e267e267267e267267e267267e267267e267267e267267e267267267e26](https://proceedings.neurips.cc/paper/2016/file/d2e1d6b6b67e1d6b67e1d6b67e2d6b67e2d67e2d67e2d67e2d67e2d67e2d67e2d67e2d67e2d67e267e267e267e267e267e267e267e267e267e267e267267e267267e267267e267267e267267e267267e267267267e26)* Shi et al. (2020) Han Shi, Haocheng Fan, and James T. Kwok. 2020. Effective Decoding in Graph Auto-Encoder using Triadic Closure. In _Proceedings of the Thirty-Fourth Conference on Association for the Advancement of Artificial Intelligence (AAAI)_, 906-913.
* Kleck (2011) Harald Kleck. 2011. Ingem popularity and recommendation accuracy. In _Proceedings of the fifth ACM conference on Recommender systems_. 125-132.
* Tang et al. (2022) Mingyue Tang, Pan Li, and Carl Yang. 2022. Graph Auto-Encoder via Neighborhood Wasserstein Reconstruction. In _International Conference on Learning Representations_. [https://openreview.net/forum?id=UTI2BiS2BuW](https://openreview.net/forum?id=UTI2BiS2BuW)
* Velickovic et al. (2014) Petar Velickovic, Guillem Cucurull, Arantxa Caanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2014. Graph Attention Networks. In _International Conference on Learning Representations_.
* Velickovic et al. (2018) Petar Velickovic, William Fedus, William H. Hamilton, Pietro Lio, Yoshua Bengio, and R Devon Hejna. 2018. Deep Graph Imax. In _International Conference on Learning Representations_.
* Welling and Kingma (2014) Max Welling and Diederik P Kingma. 2014. Auto-encoding variational bayes. ICLR (2014).
* Xia et al. (2023) Riting Xia, Yan Zhang, Chunxiu Zhang, Xueyan Liu, and Bo Yang. 2023. Multi-head Variational Graph Autoencoder Constrained by Sum-product Networks. In _Proceedings of the ACM Web Conference 2023_. 441-450.
* Yang and Leskovec (2014) Jaewou Yang and Dave Leskovec. 2014. Onifying Communities Explain Core-Prejudgment of Networks. _Proc. IEEE_ 102, 12 (2014), 1892-1902. [https://doi.org/10.1109/TDD.2014.29608](https://doi.org/10.1109/TDD.2014.29608)
* Yin and Zhou (2018) Mingzhong Yin and Mingzhong Zhou. 2018. Semi-implicit Variational Inference. In _Proceedings of the 35th International Conference on Machine Learning (Proceedings of Machine Learning Research Vol. 80)_. 2018, Jennifer Dy and Krause (Eds.). PMLR, 5660-5669. [https://proceedings.mlr.press/v80/v18b.html](https://proceedings.mlr.press/v80/v18b.html)
* Zhang and Chen (2018) Muhan Zhang and Yixin Chen. 2018. Link prediction based on graph neural networks. _Advances in neural information processing systems_ 31 (2018).

## Appendix A Experimental Details

### Implementation

We implement D-VGAE based on PyTorch Geometric [(9)] 2.0.4. In our experiments with D-VGAE, we use Graph Normalized Convolutional Network (GNN) as our encoder which is introduced in [(1)]. In our experiments with D-VGAE, we use instances of GNCNEncoder. We define a class, DvGAE that takes an encoder and a decoder of our own following the Equation 3 and 4. The decoder uses Gumbel-trick to predict edge under homophily or node popularity. To select one property, our generative model performs hard sampling. Following the previous approaches [(3; 17; 25)], the temperature \(\) is annealed from a high temperature to small temperature.

### Additional Experimental Details

The experiments are conducted with PyTorch 1.10.2 using an NVIDIA RTX A6000. The latent embedding dimension is fixed to 256 in our experiments. The model uses validation set for early stopping. For the baseline models, we also tested with different hyperparameters, but found the hyperparameters from the original papers perform the best. The Gumbel-Softmax temperature has been annealed from \(\{10.0,5.0,2.0,1.0\}\) to \(\{2.0,1.0,0.5,0.1,0.01\}\). We found having high temperature as 2.0 and low temperature as 0.5 generally achieves competitive performance. We anneal the temperature to 0.01 when reporting our results.

## Appendix B Extended Experiments

### VGNAE with High-Degree Nodes

When the similarity between nodes are measured using the normalized imader product, the link probability is bounded in a range for every nodes regardless of the node degree. While it brings performance improvement for low-degree nodes, it causes performance degradation for high-degree nodes. In this experiments, we perform link predictions with links associated with high-degree nodes. For each of dataset, we use node with degree above 5 as high-degree nodes.

As shown in Figure 5(a), VGAE achieves higher performance when the links are associated with high-degree nodes. This is expected as the magnitude of high-degree nodes can account for the link probability, which have been shown in Figure 0(a). However, we observe the opposite behavior when we perform the same testing using VGNAE with normalized inner product. As shown in Figure 5(b), the performance for high-degree nodes are always worse than overall performance. In fact, in certain cases, such as for the case with PubMed, performance on low-degree nodes (or isolated nodes) are better than the high-degree nodes.

### Node Popularity : snp\(()\)

In Equation 4, we have snp\(()\) which reflects the node popularity. We try three functions for snp\(()\). As the link of our interest is undirected, the probability of link under node popularity is affected by one of the two nodes. In this regard, we try adding the two \(a^{}\)s for snp\(()\). Another way of defining the node popularity function is to have sum of square values of \(a^{}\)s. In this approach, bias should be added in the sigmoid function to ensure the value can be between 0 and 1. The other function we try is obtaining \(a^{}\)s using vanilla-encoder without normalization. In this approach, we observe performance improvement on CiteSeer dataset (see Table 6). For CiteSeer dataset, the normalization constraint restrict the model to find best \(a^{}\) that reconstruct the links.

### Message Passing the Node Popularity

In this ablative study, we perform link prediction using a model based on our model, D-VGAE, where we apply propagation scheme both on homophily and node popularity. We denote this variant as

   Model &  &  &  \\   & AUC & AP & AUC & AP & AUC & AP \\   \(\) & 953.5 \(\) & 95.8\(\) & 96.0\(\) & 97.0\(\) & 97.1\(\) & 9.1\(\) & 9.1\(\) \\ \(\) & **96.34 \(\)** & **96.7\(\)** & 97.5\(\) & 97.2\(\) & 98.6\(\) & **95.7\(\)** & **95.7\(\)** \\  \(\) (on norm) & 95.5\(\) & 95.1\(-VGAE (PNP) in Table 7 and compare with D-VGAE which only performs propagation on embeddings for homophily.

As shown in Table 7, we observe that D-VGAE always outperforms other methods including our extension. We observe D-VGAE (PNP) performs better than VGNAE in two datasets: Cora and PubMed. The results support our hypothesis that the node popularity is the property of the node itself, which is not affected by their neighbors.

### Stochastic Sampling

Our model follows a 'winner-take-gradient' training strategy [(26)] by positing the problem to hard EM algorithm. In our E-like step, we perform sampling for \(_{ij}\) for a given pair of node \(i\) and \(j\), where we use Gumbel-rick for hard sampling. With the indicator vector \(_{ij}\), we let the gradient flow only through the selected property (homophily vs node popularity). Here, we verify how our approach is effective by comparing D-VGAE against the variant which uses probabilistic values instead of the \(_{ij}\).

D-VGAE (softmax)) performs worse than VGNAE on Cora dataset. We also observe D-VGAE (softmax) slightly underperforms than VGNAE on CiteSeer. However, on PubMed dataset, D-VGAE performs better than VGNAE. On every dataset, our proposed model D-VGAE with Gumbel-trick sampling always performs better than D-VGAE (softmax). We believe this is mainly due to the decoupling effect, which clearly devides the two property through sampling, and our generative process.

### Performing Normalization in GNCN

In this ablative study, we perform inference and obtain full node embeddings, and split the embeddings into homophily embedding and node popularity embedding. This is quite different from our proposed approach, where we perform GNCN for homophily and different GNCN for node popularity. We compare the performance from the two approaches.

The results in Table 9 provides the results when the embedding is normalized together first and split later. While this approach also takes into account the homophily and node popularity, and looks similar, major difference lies in the encoder. The embedding of homophily and node popularity should be learned separately and each should have its own normalization.

### Clustering Visualization

We perform 2D \(t\)-SNE projections of the node embeddings obtained through D-VGAE. For visualization, we only use the angular node embedding (\(^{H}\)), and no extra process has been applied. When compared with the ground-truth label, we qualitatively verify how D-VGAE performs on clustering.