# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

network, REACT decomposes its weights into the sum of two components: meta weights, which are meta-trained to form a solid foundation of general knowledge and shared globally, and adaptive weights, which are the residual component fine-tuned to specific distributions. We leverage a hypernetwork (Zhu et al., 2017) to generate adaptive weights based on data and contextual information. Intuitively, the hypernetwork maps its inputs onto a low-dimensional manifold within the parameter space (Selvin et al., 2016; Wang et al., 2017). This mapping positions adaptive weights for similar contexts and data patterns close to each other, enabling knowledge transfer across different distributions. During training, REACT optimizes the meta weights and the hypernetwork alternately through meta-learning on subsets sampled according to underlying shifts. At inference, the adaptive weights are fine-tuned from the prediction given by the trained hypernetwork, while the meta weights are fixed, preserving the generalizability of the model (Selvin et al., 2016; Wang et al., 2017; Wang et al., 2017; Wang et al., 2017).

We theoretically analyze the convergence of REACT on linear models, showing the parameters converge at a linear rate characterized by the eigenvalues of the sample matrices and other hyperparameters in REACT. Our framework is model-agnostic, broadly applicable to various neural networks and loss functions. We evaluate REACT on three datasets with different backbone models. Compared to models without adaptation, REACT improves the AUROC by 14.85% with few fine-tuning efforts (e.g., update 1 to 10 gradient steps on 10 to 100 samples). Ablation studies and sensitivity analyses show that REACT is robust to variations in the number of samples, the number of fine-tuning steps, and contamination in training data. We further showcase the capability of REACT for parameter-efficient fine-tuning, achieving 5.75% higher AUROC with 94.3% fewer parameters updated compared to full fine-tuning, highlighting its efficiency. Our contributions are as follows:

* We study the problem of fast model adaptation under distribution shift in threat detection, focusing on a practical yet challenging scenario where labels are unavailable and only limited data from new distributions are observed.
* We introduce REACT, a novel adaptation framework using a few unlabeled data and contextual insights. REACT decomposes model weights into meta and adaptive components and updates them through meta-learning alternately. It employs a hypernetwork to generate adaptive weights based on data and contexts, enabling knowledge transfer across distributions.
* We establish the convergence rate of REACT through theoretical analysis. Moreover, we conduct extensive evaluations on multiple model architectures and datasets, demonstrating that REACT consistently outperforms various state-of-the-art methods.

## 2. Related Work

**Threat Detection.** Threat detection (Chen et al., 2016; Wang et al., 2017; Wang et al., 2017; Wang et al., 2017) aims to identify security risks in systems and networks, such as insider threat (Wang et al., 2017; Wang et al., 2017), intrusion attacks (Wang et al., 2017), malware (Wang et al., 2017). Typically, the ground truths for benign and malicious activities are not available, as they require user reports or inspections by domain experts. As a result, threat detection follows unsupervised or semi-supervised approaches in anomaly detection based on different assumptions of data distribution (Chen et al., 2016; Wang et al., 2017). These methods assume the majority of data belong to a "normal" class, while anomalies deviate from this norm, e.g., lying in low-density regions (Deep Gaussian Mixture Model(Wang et al., 2017)), being far from normal data clusters (DeepSVDD (Wang et al., 2017)), or showing high reconstruction errors from the latent space of normal data (AutoEncoder (Chen et al., 2016)). These methods are designed for static environments and are not robust to distribution shifts. Changes in data distributions can significantly degrade model performance.

**Distribution Shifts in General Machine Learning.** Distribution shift means the distributions of the training and testing data are different, leading to poor model generalization to unseen data (Wang et al., 2017). To address the challenge, adaptation methods have been proposed (Wang et al., 2017; Wang et al., 2017; Wang et al., 2017). We focus on works designed for unsupervised or semi-supervised scenarios due to the data specificity in threat detection. Unsupervised domain adaptation (Chen et al., 2016) is a closely related topic, which adapts models to target domains that have no labeled data. Methods include invariant representation learning (Wang et al., 2017), prototype-oriented conditional transport (Wang et al., 2017), contrastive pre-training (Wang et al., 2017). However, these methods assume the availability of labels from source data to guide adaptation, falling short in threat detection where labels in source domains are also unavailable.

**Model Adaptation in Threat Detection.** Distribution shifts are observed in threat detection, such as malware detection (Wang et al., 2017), network intrusion detection (Wang et al., 2017; Wang et al., 2017), and log anomaly detection (Wang et al., 2017). Traditional supervised approaches (Chen et al., 2016; Wang et al., 2017; Wang et al., 2017) require extensive labeling, making them impractical for real-world deployment. Recent efforts have recognized this limitation and have been exploring adaptation approaches without relying on labels. Unsupervised domain adaptation methods, like learning domain-invariant representations (Chen et al., 2016) have been extended. However, they typically require simultaneous training on source and target domains, making them less suitable for emerging domains. Test-time adaptation, such as batch normalization updates (Wang et al., 2017), energy-based models (Wang et al., 2017), and trend estimation (Wang et al., 2017), updates models during inference without gradient descent. Though efficient, they are limited to minor shifts (Wang et al., 2017) or sequential shifts that display continuous patterns (Wang et al., 2017). To address more severe and random shift, meta-learning (Wang et al., 2017; Wang et al., 2017) is a promising approach that trains a meta model on a variety of learning tasks, enabling adaptation to new distributions with a small amount of data. Prior works have applied meta-learning to graph neural networks (Wang et al., 2017) and autoencoders (Wang et al., 2017) for few-shot detection, and introduced prototype-oriented optimal transport for adapting models to new multivariate time-series (Wang et al., 2017). However, these methods fine-tune models solely on limited data from new distributions, leading to variations in adaptation performance. In contrast, our method considers contextual information about shifts to understand correlations among distributions and transfer knowledge, improving adaptability.

Figure 1. Illustration of our problem setting.

## 3. Preliminaries

### Problem Definition

Distribution shifts in threat detection involve changes in the probability distribution of data over time or across domains (e.g., users, services). These shifts can affect the marginal feature distribution \((x)\), the conditional distribution \((y|x)\), or both. Consider a threat detection model \(f(;)\) trained on a dataset \(=\{x_{i}\}_{i=1}^{N}\) drawn from a distribution \(\). The dataset \(\) is _unlabeled_ and is dominated by samples from the benign class. The model parameters \(\) are optimized by minimizing a loss function \(\). Common choices for \(\) include reconstruction loss in autoencoders or contrastive loss in self-supervised learning. The objective is: \(^{*}=_{}_{}( f(x;))\).

Our goal is to develop an adaptation method that updates model parameters to \(^{*}\) using a few examples from the new distribution \(^{}\). The observed dataset \(^{}\) from distribution \(^{}\) is unlabeled, and its size is small \(|^{}|=k||\).

### Meta-Learning

Meta-learning trains models that can quickly adapt to new tasks using only a few examples. A task \(_{i}\) is defined as an independent learning problem with a dataset following a specific distribution \(_{i}\) and a learning objective which is to find the optimal parameters \(^{*}_{i}\) that minimize the expected loss \(^{*}_{i}=_{}_{_{i}} (f(x;))\). The dataset \(_{i}\) for task \(_{i}\) is divided into a support \(^{i}_{}\) and a query set \(^{i}_{}\). \(^{i}_{}\) is used to fine-tune the model to learn task-specific parameters for \(_{i}\), while \(^{i}_{}\) evaluates how well the model generalizes the learned task-specific knowledge.

One of the most representative algorithm is Model-Agnostic Meta-Learning (MAML) (Srivastava et al., 2014), which optimizes the initial model parameters \(\) so that after fine-tuning, the model performs well across various tasks, minimizing the average loss. For each task \(_{i}\), the parameters are fine-tuned using the support set \(^{i}_{}\): \(^{*}_{i}=_{^{i}_{}}(f(x;))\). The weight initialization \(\) is optimized using the query sets:

\[^{*}=_{}_{_{i}}_{x^{i}_{ }}(f(x;^{*}_{i})).\]

During inference, the model is fine-tuned on a few samples from the new distribution, and then applied to all testing samples.

### HyperNetwork

A hypernetwork (Srivastava et al., 2014) is a neural network that predicts the weights of another neural network (i.e., target network). By training a single hypernetwork to predict weights across multiple tasks rather than optimizing each one independently, hypernetworks offer a parameter-efficient solution for model adaptation. It has shown effective in improving learning efficiency through parameter sharing (Bahdanau et al., 2015; Gulrajani et al., 2016; Gulrajani et al., 2017; Chen et al., 2018; Li et al., 2019). Let \(h\) represent the hypernetwork with parameters \(\), and let \(f\) denote the target network. Given a representation \(_{}\) for describing task \(_{i}\), the hypernetwork generates the model weights \(_{i}=h(_{};)\), which are loaded into \(f\) for the downstream task. Given multiple tasks \(_{i}\) and the corresponding task representations \(_{i}\), the learning objective is to optimize the hypernetwork's parameters \(\) to minimize the loss \(\) across these tasks:

\[^{*}=_{}_{_{i}}_{x_{i}} [f(x;h(_{i};))].\]

```
Input: Task distribution \(()\), target network \(f\), hypernetwork \(h\) Output: Meta weights \(_{}\), hypernetwork weights \(\)
1 Initialize model weights \(_{}\) and \(\);
2whilenot convergeddo// Update meta weights.
3 Sample a set of tasks \(\{_{i}\}_{i=1}^{M}()\);
4foreach task\(_{i}\)do
5 Form support set \(^{i}_{}\) and query set \(^{i}_{}\) and extract contextual information \(c_{i}\);
6 Generate adaptive weights: \(^{i}_{}=h(^{i}_{};c_{i};)\);
7 Fine-tune \(^{i}_{}\) on \(^{i}_{}\) following Eq. 1;
8 Update \(_{}\) following Eq. 2;
9 // Update hypernetwork.
10 Sample a set of tasks \(\{_{i}\}_{f=1}^{M}()\);
11foreach task\(_{f}\)do
12 Form support set \(^{j}_{}\) and query set \(^{j}_{}\), and extract contextual information \(c_{j}\);
13 Generate adaptive weights: \(^{j}_{}=h(^{j}_{};c_{j};)\);
14 Update \(\) following Eq. 3;
15
16 end while
```

**Algorithm 1**Training Procedure of REACT

## 4. The REACT Framework

We approach the problem from both meta-training and fine-tuning perspectives. Through meta-training, the model establishes a strong, generalizable foundation that can be applied to most scenarios. Then, through fine-tuning, the model weights are slightly adjusted for specific shifts. We propose a framework that decomposes model weights into two components and alternately optimizes them to address both perspectives. Algorithm 1 provides the pseudo-code.

### Weight Decomposition

Given a neural network, we decompose its weights into two complementary components: meta weights \(_{}\) and adaptive weights \(_{}\). Meta weights capture global patterns that are common across different distributions, representing the core knowledge acquired during meta-learning. Adaptive weights, on the other hand, serve as a small "residual" component that allows the model to be fine-tuned to the unique characteristics of specific data distributions, while still leveraging the global patterns encoded in the meta weights. The full model weights are then formed by adding the two components together: \(=_{}+_{}\). By applying a small residual update to the pretrained meta weights, the model can adapt to new distributions without overwriting the essential pretrained knowledge.

### Residual-Adaptive Weight Generation with Hypernetwork

We incorporate a hypernetwork to generate adaptive weights based on data characteristics and contextual information. The architecture of the hypernetwork is presented in Figure 2.

**Data Encoding.** Our hypernetwork includes a data encoder that processes data from the support set to produce feature representations. These representations are averaged and passed through a series of linear layers, with each layer producing the weights for a corresponding layer in the target network.

**Context Encoding.** To enhance the hypernetwork's ability to handle varying distributions, we integrate the contextual information \(_{i}\) about distribution \(_{l}\) as an additional input. This context offers semantic insights into the shifts and helps capture similarities across distributions. We incorporate a context encoder in the hypernetwork to transform contexts into embeddings, which are then added to the data representations for weight generation. The choice of context depends on the type of shift. For example, we use time information for temporal shifts, with positional encoding (Zhu et al., 2017) generating embeddings. Details about context modeling for different tasks can be found in Section 6.1.

### Alternating Optimization

We design an alternating optimization scheme to iteratively update the meta weights and the hypernetwork. This approach balances the learning dynamics and prevents mutual interference between the two components. Figure 3 illustrates the process.

**Task Sampling for Meta Learning.** To let the model learn how to adapt to new distributions, we first create a diverse set of tasks that reflect the expected variations in the application. We sample tasks from training set by simulating the underlying data shifts. For instance, if the goal of adaptation is to address distribution shifts over time, the data can be grouped according to temporal factors such as day or month, with each time period forming a separate task. If the focus is on handling shifts across different users, the data can be grouped by users, with each user forming a task.

**Update of Meta Component.** Let \(^{i}_{}\) denote the adaptive weights of task \(_{l}\). In each iteration, we begin by updating the meta weights. We sample a set of tasks \(\{_{l}\}_{i=1}^{M}\) and contextual information \(\{c_{l}\}_{i=1}^{M}\). The hypernetwork \(h(;)\) is fixed and used to generate adaptive weights, \(^{i}_{}=h(^{i}_{},c_{l};)\). The generated adaptive weights are then fine-tuned to derive the optimal model weight \(^{i*}_{}\) for task \(_{l}\) by minimizing the empirical loss over the support set \(^{i}_{}\):

\[^{l*}_{}=*{argmin}_{_{}} _{x^{i}_{}}(f(x;_{},_{})). \]

We then fix these fine-tuned adaptive weights and update the meta model by minimizing the loss on the query set \(^{i}_{}\). Let \(_{}\) be the learning rate for updating meta weights. The update of meta weight after one gradient step is as follows:

\[_{}_{}-_{}_{ _{}}_{_{l}^{i}_{}}(f(x;_{},^{i*}_{})). \]

**Update of Hypernetwork.** Next, we sample another set of tasks \(\{_{j}\}_{i=1}^{M}\), fix the meta weights learned in the previous step, and update the hypernetwork using the query sets. Let \(_{h}\) be the learning rate for updating the hypernetwork. The weight update of hypernetwork after one gradient step is as follows:

\[-_{h}_{}_{_{j}} _{x^{i}_{}}(f(x;_{ },h(^{j}_{},c_{j};))). \]

**Regularization.** We apply L2 regularization to the adaptive weights generated by the hypernetwork, encouraging them to act as residuals to the globally shared meta weights. The query loss for optimizing the hypernetwork, denoted as \(L^{i}_{}\), is combined with the regularization as \(=^{i}_{}+\|^{i}_{}\| _{2}^{2}\), where \(\) is the hyperparameter to control the regularization strength.

### Adapting to New Distributions

When doing inference on a new distribution \(_{j}\), the meta weights and the hypernetwork are fixed. This ensures the pre-trained knowledge are not "forgotten" during fine-tuning (Golov et al., 2013; He et al., 2016; Li et al., 2017), preserving generalizability of the model. A small number of support data \(^{j}_{}\) from \(_{j}\) along with its contextual information \(c_{j}\) are fed into the hypernetwork to predict the adaptive weights \(^{j}_{}=h(^{j}_{},c_{j};)\). With this initialization, the adaptive weights are then fine-tuned on \(^{j}_{}\) following Equation 1. Finally, the two parts of the weights are merged by summing them as

Figure 3. Alternating optimization in REACT. In each training iteration, we sample a set of tasks to update the meta weights, then sample another set to train the hypernetwork.

Figure 2. Architecture of the proposed hypernetwork.

if there is only one target network. The merged weights are used for inference on data from the new distribution \(_{j}\).

## 5. Analysis

We provide convergence analysis of REACT on linear models. Let \(X^{l}\) be the matrix whose rows are the samples from the dataset of task \(i\{1,...,M\}\), i.e., \(_{l}\). The data matrix \(X^{l}\) can be split into support set \(X^{l}_{g}\) and query set \(X^{l}_{q}\). We assume that the relevant datasets are sampled at the beginning of the algorithm. Given linear model1

\[h(X;)=X, f(X;_{},_{})=X(_{ }+_{}), \]

Theorem 1 provides convergence guarantees for REACT.

Theorem 1.: _Consider REACT on the linear model in (4) with \(Eq.(1)\) being solved exactly. Let \(X^{l}_{g}\) and \(X^{l}_{q}\) satisfy \((X^{l}_{g})^{}X^{l}_{g}=(X^{l}_{q})^{}X^{l}_{q}=_{l}l\) for each task \(i\{1,..,M\}\), where \(_{l}\) are the variances and \(l\) is the identity matrix. Learning rates are chosen as \(_{}<1/_{l=1}^{M}_{l}/(_{l}+)\) and \(_{h}<1/(_{j=1}^{n_{0}}_{j}(_{j}+),\| _{g}\|)\) where \(_{g}=_{l=1}^{M}_{j}(X^{l}_{g})^{}\). Then, for any \(>0\), there exists_

\[K=(_{1/_{}}(1/)+_{1/_{h} }(1/))\]

_for \(_{}=1-_{}_{l=1}^{M}_{l}/(_{l}+)\) and \(_{h}=1-_{h}_{j=1}^{M}_{j}(_{j}+)\) such that the \(K\)-iteration of Algorithm 1 satisfies_

\[\|^{K}-^{*}\|,\|^{K}- ^{*}\|,\]

_where \(^{*}\) and \(^{*}\) are stationary points of the algorithm._

The proof is provided in Appendix A.2. Our results suggest that \(_{}\) and \(\) converge to stationary points at a linear rate which can be characterized based on the eigenvalues of the sample matrices in each task and hyperparameters considered in REACT.

## 6. Experiments

### Experiment Setup

**Datasets and Backbone Models.** Our evaluation focuses on two key applications in cybersecurity, network intrusion detection and malware detection, and targets both temporal and domain shifts. REACT is compatible with various neural network architectures. To assess its performance across different models, we employ three representative architectures, AutoEncoder (AE) (Abadi et al., 2016), DeepVDD (DSVDD) (Selvin et al., 2017), and GOAD (Chen et al., 2018), paired with the following datasets:

* **AnoShift**(Zhu et al., 2018) is a benchmark for network intrusion detection under distribution shifts. It collects traffic logs from a university network between 2006 and 2015. Data shifts occur over time due to reasons such as user behavior changes and software updates. Each sample has 15 features (9 numerical and 6 categorical) and a label of whether it is an attack. We use the train-test split provided by the dataset, including training subsets from 2006 to 2010 and test subsets from 2015. Each month is regarded as a separate task. AutoEncoder is used as the backbone model.
* **Malware**(Zhu et al., 2018) contains executables collected between 2010 and 2014 from VirusShare2, an online malware analysis platform. Data shifts happen along time. Each executable has 482 counting features and a risk score (ranging from 0 to 1) indicating the probability of it being malware. These risk scores are converted to binary labels using thresholding, with executables labeled as malicious if the score is greater than 0.6 and benign if the score is less than 0.4 (Zhu et al., 2018). Following previous work (Zhu et al., 2018), the dataset is split into training data from 2011 to 2013, validation data from 2010, and testing data from 2014. Each month is treated as a separate task. DeepSVDD is used as the backbone model.
* **NSL-KDD**(Zhu et al., 2018) is another dataset for evaluating network intrusion detection. Each sample has 40 attributes describing the network traffic, with 6 categorical and 34 numerical features. We simulate domain shifts by grouping data according to services (e.g., HTTP, Telnet) and randomly assigning half of the services as training tasks and the other half as test tasks. We use the official train-test split provided by the dataset and remove services not selected for the respective splits. Besides, services with fewer than 20 benign samples are excluded to ensure sufficient unseen data for testing. GOAD is used as the backbone model. We sample the datasets to form a 10% ratio of threat samples for both training and testing. In Section 6.4, we vary this ratio to test the robustness of REACT to the contamination of training data. For the NSL-KDD dataset, since GOAD is a semi-supervised method that trains only on benign data, we remove attack samples from the training set. Table 1 summarizes the statistics and configurations of the datasets and backbone models in the experiments. Further details on the backbone models are provided in Appendix A.1.

**Baselines.** We compare REACT with unsupervised methods from the anomaly detection benchmark (Zhu et al., 2018), including linear and statistical models: **ECOD**(Zhu et al., 2018), **COPOD**(Zhu et al., 2018), **OCSVM**(Zhu et al., 2018); distance- and proximity-based methods: **LOF**(Zhu et al., 2018), **KNN**(Zhu et al., 2018); ensemble methods: **LODA**(Zhu et al., 2018), **Floresf**(Zhu et al., 2018); and neural networks: **AE**(Abadi et al., 2016), **DSVDD**(Selvin et al., 2017), **LUNAR**(Zhu et al., 2018). These methods assume static environments and do not account for distribution shifts. In addition, we compare REACT with training from scratch, fine-tuning strategies, and state-of-the-art model adaptation methods. Brief descriptions are as follows:

* **w/o adaptation**: The model is trained on the training data and directly tested on each test task. This serves as the pretrained model, denoted as \(\).
* **Train-from-scratch**: For each task, a model is trained from scratch using \(k\) samples and is used for evaluation.
* **Fine-tuning**: For each task, the model is fine-tuned using \(k\) samples from the task based on the pretrained model \(\).
* **Continual Learning**: Starting from \(\), we sequentially fine-tune the latest updated model using \(k\) samples from each task.
* **Experience Replay (ER)**(Zhu et al., 2018) is a method to mitigate catastrophic forgetting in continual learning. We maintain a memory buffer to store historical data. In each fine-tuning iteration, we

   Dataset & \# Train/Test & \# Train/Test tasks & \(k\) & Shift by & Model \\  Anoshift & 1.3M / 1.8M & 50 / 110 & 100 & Time & AutoEncoder \\ NSL-KDD & 28K / 6K & 8 / 6 & 10 & Service & GOAD \\ Malware & 15K / 5K & 36 / 12 & 10 & Time & DeepSVDD \\   

Table 1. Experiment configurations and dataset statistics after preprocessing.

sample a batch from this buffer and compute its loss. This loss is then weighted and combined with the loss from the new batch.
* **ACR** is a zero-shot adaptation which adopts meta-learning to train the model and update the batch normalization layers with the batch statistics during inference. We add batch normalization layer after each linear or convolutional layer in the model.
* **OC-MAML** is a few-shot one-class classification method. It extends MAML by modifying the episodic data sampling strategy by forming one-class support sets to optimize the meta model.

**Training and Adaptation Configurations.** The size of support data \(k\) during training and adaptation is set based on the data quantity, with \(k=100\) for Anoshift and \(k=10\) for Malware and NSL-KDD. The size of query data varies proportionally, with 1000 for Anoshift and 100 for Malware and NSL-KDD. In each meta-training iteration, we sample \(M=5\) tasks for Malware and NSL-KDD, and \(M=1\) for Anoshift. The number of fine-tuning epochs \(E\) is determined by the convergence rate of the learning task, with \(E=10\) epochs for Anoshift and Malware, and \(E=1\) for NSL-KDD due to its faster convergence. Section 6.4 provide sensitivity analyses on \(k\) and \(E\) to assess the robustness of our model.

**Choices of Contexts.** For Anoshift and Malware whose shifts occur along time, we use time index as the context, which is modeled by positional encoding  to generate contextual embedding for each task. For NSL-KDD dataset whose shifts occur across services, we first feed these services names to a large language model with the prompt "please briefly describe each of these web services, including the normal and anomalous patterns". Then, we use Sentence Transformer3 to generate embeddings for the descriptions.

**Evaluation Metrics.** For each test task, we adapt the model and evaluate its performance using AUROC and AUPR scores. All experiments are repeated for five times with the same set of random seeds, and the results are averaged across all test tasks and runs.

### Main Results and Analysis

The results are presented in Table 2, where the left sub-table shows the performance of static methods and the right one focuses on fine-tuning and adaptation methods across three backbone models. The static methods (left table) generally show lower performance than models with adaptation (right table), highlighting the negative impact of distribution shifts on model performance. The right table also includes the performance of train-from-scratch using all data from each individual test task (in grey). When sufficient data is available from the new tasks, training a model from scratch yields better performance than using a pretrained model without adaptation. When comparing the models trained from scratch, fine-tuning, and continual learning, it is shown that the pretrained model \(\) can be a poor initialization for shifted distributions, e.g., Anoshift. We also observe that experience replay slightly improves performance compared to continual learning without any strategy to prevent catastrophic forgetting. However, this improvement is limited.

REACT consistently outperforms the baselines and even surpasses the model trained from scratch using all data from individual tasks on two datasets. This is because REACT adapts from a model meta-trained on a larger and more diverse training set than each individual task, providing a stronger foundation for adaptation. Furthermore, the training data in Anoshift and Malware contains noise (10% threat ratio). Training a model on all data from an individual task increases the likelihood of exposing the model to many threat samples within that distribution, which, due to the specific training objectives of AutoEncoder and DSVDD, may cause the models to mistakenly learn malicious patterns as benign ones. In contrast, REACT is less prone to such overfitting as it utilizes the meta model and only updates the adaptive weights on a small set of new data. We note that on NSL-KDD, GOAD achieves high scores when trained from scratch using all data since it is trained on benign data only, but such training is impractical in the real world. When compared to other baselines using the same \(k\) samples from new tasks, REACT achieves the highest scores. Among state-of-the-art methods, ACR, which performs test-time adaptation, shows relatively lower performance as it does not apply gradient updates during inference, limiting its adaptation ability. OC-MAML achieves the second-best performance, demonstrating the strength of meta-learning. However, REACT outperforms OC-MAML by weight decomposition and incorporating a hypernetwork for contextual tuning. These

   & _{i}^{}\)} &  &  \\   & & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR \\    & 0.6714 & 0.4062 & 0.2732 & 0.1040 & 0.5323 & 0.2956 \\  & LOF  & 0.6107 & 0.2873 & 0.2781 & 0.1101 & 0.4150 & 0.1827 \\  & OCSVM  & 0.6903 & 0.3157 & 0.3880 & 0.1190 & 0.6649 & 0.3492 \\  & Forest  & 0.6658 & 0.2830 & 0.2660 & 0.0722 & 0.7809 & 0.4798 \\  & LODA  & 0.5723 & 0.2111 & 0.5190 & 0.1368 & 0.5207 & 0.2532 \\  & AE  & 0.7110 & 0.3204 & 0.3789 & 0.1156 & 0.6057 & 0.2678 \\  & DSVDD  & 0.7716 & 0.3895 & 0.5165 & 0.1644 & 0.6006 & 0.2848 \\  & COPOD  & 0.7664 & 0.3831 & 0.4450 & 0.1102 & 0.7849 & 0.4471 \\  & ECOD  & 0.7461 & 0.3727 & 0.5403 & 0.1390 & 0.8100 & 0.4706 \\  & LluxAR  & 0.4449 & 0.2450 & 0.2719 & 0.0880 & 0.5243 & 0.2350 \\  

Table 2. Main experiment results (averaged over 5 runs). The left sub-table reports the performance of static methods, while the right focuses on fine-tuning and adaptation across three backbone models. REACT consistently outperforms all other methods. \(|_{i}^{}|\) denotes the number of samples observed from each test task for fine-tuning or training from scratch.

designs help maintain generalizability and enhance adaptability beyond meta-learning alone.

### Ablation Studies

We crafted five ablated versions of REACT by systematically removing each key component: (1) We remove the hypernetwork and perform meta-learning on the meta weights only, denoted as **w/o hypernetwork**. (2) We disable fine-tuning during inference and use the merged weights from meta weights and the hypernetwork's prediction to do the inference directly, denoted as **w/o fine-tuning**. (3) We remove the use of context and only provide the support data for hypernetwork, denoted as **w/o context**. (4) We replace the context with randomly-generated embeddings, denoted as **w/ random context**. (5) We remove the regularization term on the hypernetwork's prediction, denoted as **w/o regularization**.

The results in Table 3 show that every component in REACT contributes to performance improvement. Integrating the hypernetwork has a significant impact, as contextual information complements limited new data and facilitates knowledge transfer across distributions. Fine-tuning and regularization also have notable impacts. REACT w/o fine-tuning shows competitive performance on AnoShift and Malware compared to the baselines, indicating its potential for zero-shot adaptation. However, with just a few gradient updates, performance can be largely improved. Besides, adding regularization ensures the adaptive weights predicted by the hypernetwork do not overpower the full model, maintaining model generalizability. REACT w/o context learns distribution patterns solely from the support data, which is less effective than incorporating contexts since the support data is limited and might not provide sufficient insights. REACT w/ random context can recognize new tasks as the random context indicates whether the task has been seen during training, thus mitigating overfitting. Therefore, it performs slightly better than w/o context. However, these random contexts do not provide task-specific knowledge to capture meaningful patterns. With additional information about tasks, REACT can model similarity among distributions more effectively.

### Sensitivity Analyses

**Number of Support Samples.** We vary the number of support samples \(k\) for each task from 5 to 100 and compare REACT with the fine-tuning baseline. Figure 4 shows that REACT consistently outperforms the fine-tuning baseline across all datasets. This demonstrates REACT's robustness in data-scarce scenarios and highlights its ability to efficiently leverage available data for fast adaptation.

**Number of Fine-Tuning Epochs.** We vary the number of fine-tuning epochs for each new task from 1 to 10 and compare the performance of REACT with the fine-tuning baseline. Figure 5 shows that REACT outperforms the baseline in all settings. The improvement from additional fine-tuning epochs is less significant in the Malware and NSL-KDD datasets, as these two datasets are simpler and the model is able to adapt to them with fewer epochs.

    &  &  \\   & 1\% & 5\% & 10\% & 20\% & 1\% & 5\% & 10\% & 20\% \\  w/o adaptation (\(\)) & 0.506 & 0.513 & 0.517 & 0.567 & 0.764 & 0.753 & 0.711 & 0.634 \\ train-from-scratch & 0.366 & 0.366 & 0.356 & 0.377 & 0.818 & 0.791 & 0.740 & 0.740 \\ fine-tuning & 0.559 & 0.545 & 0.568 & 0.580 & 0.812 & 0.765 & 0.704 & 0.605 \\ continual learning & 0.562 & 0.559 & 0.588 & 0.590 & 0.683 & 0.572 & 0.609 & 0.453 \\ ER & 0.582 & 0.585 & 0.602 & 0.602 & 0.734 & 0.669 & 0.614 & 0.577 \\ ACR & 0.544 & 0.570 & 0.580 & 0.570 & 0.785 & 0.774 & 0.763 & 0.773 \\ OC-MAML & 0.683 & 0.688 & 0.678 & 0.687 & 0.827 & 0.803 & 0.777 & 0.755 & 796 \\  REACT (ours) & **0.725** & **0.738** & **0.725** & **0.719** & **0.832** & **0.813** & **0.823** & **0.775** \\   

Table 4. Sensitivity analysis: AUROC scores across different contamination levels.

Figure 4. Sensitivity analysis: number of support samples (\(k\)).

Figure 5. Sensitivity analysis: number of fine-tuning epochs.

**Contamination on Training Data.** We evaluate the robustness of our system against contamination in the training data when applying to AutoEncoder and DeepSVDD models on Anoshift and Malware respectively--both unsupervised methods. We note that GOAD is a semi-supervised method trained solely on benign data (as applied to the NSL-KDD dataset) thus the evaluation is trivial to it. We fix the number of benign samples while varying the ratio of threat samples from 1% to 20%. Table 4 shows the AUROC scores. REACT consistently achieves higher AUROC scores across different contamination rates than the fine-tuning baseline, showing that it is robust to noise in training data.

### Parameter-Efficient Fine-Tuning

Our framework supports parameter-efficient fine-tuning, which is especially useful when working with large models. By incorporating adaptive weights into only a subset of the model's parameters and having the hypernetwork predict this subset of weights, we can reduce the number of parameters to be fine-tuned. We conducted experiments using an AutoEncoder on the Anoshift dataset to showcase REACT's ability in parameter-efficient fine-tuning. Specifically, we predicted adaptive weights for either the two symmetric linear layers closest to the input (denoted as **REACT-Inner**) or those closest to the latent representations (denoted as **REACT-Outer**). Full fine-tuning of REACT is denoted as **REACT-Full**. The results are shown in Figure 6. Both methods achieve better performance compared to the baselines, although they slightly underperformed compared to REACT-Full which fine-tunes all layers. Notably, REACT-Inner achieved a 5.75% higher AUC while updating 94.3% fewer parameters compared to conventional full fine-tuning, highlighting its efficiency.

### Case Study

To understand how well REACT leverages contextual information, we analyze the weights generated by the trained hypernetworks. Specifically, we compare the adaptive biases of the last layer in the AutoEncoder for Anoshift across different months and calculate their cosine similarities. The results are presented in Figure 7 A, with warmer colors indicating higher similarity. The high similarities around the diagonal indicate the weights generated for each month are similar to those of nearby months. This suggests that REACT effectively captures the temporal dynamics and smoothly adapts model weights over time. As a reference for how data shifts, we follow the analyses in (Kumar et al., 2019) to calculate distances between data subsets of each year. Specifically, we measure the Jeffrey's Divergence (Kohn, 1999) averaged over categorical features and the Optimal Transport Dataset Distance (OTDD) (Bishop, 2006) across all features. As shown in Figure 7 B, data from adjacent years exhibit smaller distances (in red). Besides, it presents block patterns where data from 2006-2010, 2011-2013, and 2014-2015 are more similar within their respective groups than with other years. This temporal shift corresponds with trends in weight similarity over time. The observations also hint at the potential for detecting shifts, a research question actively discussed in the literature (Kumar et al., 2019; Wang et al., 2020; Wang et al., 2020)--by monitoring deviations in the hypernetwork's predictions compared to prior tasks, we may identify moments where shifts occur.

## 7. Conclusions

Our work sheds light on how to approach the distribution shift problem--from both meta-learning and fine-tuning perspectives. We propose a novel framework, REACT, that decomposes the weights of a neural network into the sum of meta and adaptive components, following a meta-learning paradigm to train the components. By integrating a hypernetwork to generate adaptive weights, REACT enables knowledge sharing and adjusts weights for new distributions with minimal fine-tuning effort. The framework is model-agnostic, generally applicable to arbitrary neural networks. It works effectively with unlabeled and imbalanced data, making it broadly applicable to various threat detection models and objectives.

While focused on cybersecurity, the principles and methods developed in our research can be adapted to other fields facing similar distribution shift challenges, such as finance (Kumar et al., 2019; Wang et al., 2020; Wang et al., 2020) and healthcare (Wang et al., 2020; Wang et al., 2020). Our study provides insights for studies in the general machine learning community, fostering a more comprehensive understanding of adaptation and fine-tuning by showcasing applications in cybersecurity. One of the future directions is to incorporate a lightweight mechanism for updating the meta model within our framework. A potential solution could involve applying aggregation of the predicted adaptive weights into the meta model. This approach could enhance the framework's ability to continuously adapt to evolving distributions, especially for scenarios with significant distribution shifts over long period of time.

Figure 6. Results of parameter-efficient fine-tuning. Both REACT-Inner and REACT-Outer outperform the baselines.

Figure 7. Case Study. The adaptive weights generated for each month are similar to those of nearby months, reflecting the data shift pattern.

REACT: Residual-Adaptive Contextual Tuning for Fast Model Adaptation in Threat Detection