# Coreness Maximization through Budget-Limited Edge Insertion

Anonymous Author(s)

###### Abstract.

The Budget Limited Coreness Maximization (BLCM) problem aims to enhance average user engagement by activating a limited number of connections, i.e., inserting up to \(b\) edges to maximize the coreness gain of all vertices in a graph. Due to the cascading feature, we prove the BLCM is NP-hard, APX-hard, and not submodular, meaning greedy sequential edge insertion fails to deliver satisfactory results. As a result, solving BLCM requires combinatorial edge insertion and must face the combinatorial exploration difficulty. This paper proposes the first effective and polynomial-time approach to BLCM. It embeds local combinatorial optimization into global greedy search to boost the benefits of combinatorial optimization while restricting its complexity. Specifically, we propose efficient methods to evaluate the cascaded coreness improvements of two local combinatorial strategies, i.e., when a leader or a group of nodes increase their coreness values via local edge insertion. Note that the key difficulty lies in evaluating the cascading effects. Based on these, we propose three efficient combinatorial edge insertion strategies: (1) Leader-Centric Greedy Insertion (LCGI), (2) Group-Centric Greedy Insertion (GCGI), and (3) a Leader-Group Balance (LGB) insertion. LCGI greedily finds the most influential leader that can produce the highest coreness gain together with its followers. GCGI finds the most influential group that can promote the most coreness gain. LGB combines the two strategies to select edge combinations adaptively. We prove the low complexity of LCGI, GCGI and LGB. Experiments conducted on 13 real-world datasets highlight their practical utility and superiority over existing approaches.

\(k\)-core, Coreness, Budget Limited Coreness Maximization, User Engagement +
Footnote â€ : copyright: none

## 1. Introduction

User engagement in social networks is a key indicator that reflects user involvement level in the community. How to increase user engagement in a social network using a limited amount of budget is a crucial problem. The \(k\)-core model extracts subgraphs in which vertices have degrees of at least \(k\). It can extract high engagement users(Sutton et al., 2012; Sutton et al., 2012). This paper considers the coreness value of a vertex as an indicator of the user's engagement in the social network (Sutton et al., 2012). Here the coreness of a vertex extends the \(k\)-core concept, which is the largest \(k\) value among the \(k\)-cores that cover the vertex. A case study on the Gowalla (Gowalla, 2016) dataset is illustrated in Figure 1, which shows the average user check-ins versus the coreness values of the user (vertex). It is evident that the coreness value exhibits a strong positive correlation with the user engagement.

Activating connections, i.e., inserting edges into the network is the most natural way to improve the vertex coreness. But edge insertion generally requires some costly edge activation operations, meaning it is necessary to consider the cost. Therefore, **Budget Limited Coreness Maximization problem** (BLCM) is important. It aims to insert at most \(b\) edges into the network to maximize the sum coreness improvements of all the vertices. Note that this BLCM problem is different from the traditional \(k\)-core Maximization (KM) problem (Bowalla et al., 2012; Sutton et al., 2012; Sutton et al., 2012), which only focuses on improving the size of \(k\)-core for a specific \(k\). The BLCM problem is also different from the Anchor Coreness (AC) problem proposed by Linghu et al. (Linghu et al., 2019). The "_anchor_" concept in AC raises the vertex's degree to positive infinity without actually inserting edges, which is theoretically interesting but is hard to realize in practice.

We first prove that the BLCM problem is NP-hard, APX-hard, and that the coreness gain function (the sum of coreness improvements across all vertices) is not submodular. This implies that greedy sequential edge insertion cannot achieve satisfactory results. To address this, we need to explore combinatorial edge insertion, but faces the difficulty of combinatorial explosion. Another difficulty is that a vertex coreness improvement may trigger cascading effect of neighbors' coreness improvements. Therefore, we must on one hand efficiently evaluate each combinatorial insertion, and on the other hand narrow down the search space.

This paper presents the first efficient and effective approach to the BLCM problem. It embeds the local combinatorial optimization into the global greedy search to explore the combinatorial insertion's benefits while restricting its complexity. In particular, we firstly focus on two practical local strategies: (1) increasing a leader's coreness by adding edges, and (2) enhancing a local group's coreness through edge insertion. Both strategies trigger a ripple effect of coreness improvements among the neighboring nodes. We present efficient methods for estimating the potential coreness gain from these cascading effects. For the leader augmenting case, we propose identifying "upstair paths" between the leader and followers to estimate the leader's impact score. For the group augmenting case, we characterize the upstair paths to efficiently estimate the group's impact score.

Based on efficient evaluation of the local combinatorial strategies, the optimized local operations are used as atoms in global greedy search. We firstly propose a Leader-Centric Greedy Insertion (LCGI) strategy, which greedily selects the most influential leader and inserts the corresponding set of edges in each round until the budget is used up. Then, we further propose a Group-Centric Greedy Insertion (GCGI) strategy, which greedily selects the most influential group in each round. We propose group reduction to filter out the poor-performing vertices in the group, followed by group expansion, where the external well-performing vertices are expanded. At last, we present a Leader-Group Balanced Algorithm

Figure 1. Vertex coreness vs. User checkins on Gowalla.

(LGB), which chooses LCGI and GCGI adaptively by comparing their scores during greedy selection.

We prove that LCGI, GCGI and LGB have low complexity, i.e., the complexity of LCGI is \(O(bmn)\), while the complexity of the latter two is upper bounded by \(O(bk_{max}mn)\), where \(k_{max}\) is the maximum coreness value of vertices. Since existing methods cannot directly solve the BLCM problem, we compare with other heuristic algorithms and extend the edge \(k\)-core algorithm (EKC) (Zhou et al., 2016) to solve BLCM. Experiments are conducted on 13 datasets and the results show that: 1) our proposed algorithms are more effective in improving the coreness than other heuristic algorithms; 2) the LGB algorithm has a significant improvement in efficiency while maintaining a good coreness gain compared with the Exact algorithm and the EKC algorithm; 3) the BLCM method has more coreness gain than the vertex-oriented algorithm (VEK) (Zhou et al., 2016) and fast core maximization algorithm (FASTCM+) (Zhou et al., 2016) which are traditional \(k\)-core maximization algorithms, and the improvement of coreness value is more diverse.

## 2. Problem Definition and Hardness Analysis

### Problem Definition

An undirected and unweighted graph is denoted as \(G=(V,E)\), where \(n=|V|\) represents the number of vertices, and \(m=|E|\) represents the number of edges. The neighbors of \(v\) in \(G\) are defined as \(nbr(v,G)\), and the degree of vertex \(v\) is denoted as \(deg(v,G)\). When the context is clear, we simply use \(nbr(v)\) and \(deg(v)\) for clarity. Given an integer \(k(k 0)\), the \(k\)-**core** of \(G\)(Goudi et al., 2016; Li et al., 2016), denoted by \(C^{k}(G)\), is the maximal subgraph of \(G\), such that \( v C^{k}(G),deg(v,C^{k}) k\). The **coreness** value of a vertex \(v V(G)\), denoted by \(core(v,G)\), is the largest \(k\) such that \(v\) is in the \(k\)-core, i.e., \(core(v,G)=max_{v C^{k}(G)}\{k\}\).

When the context is clear, we simply use \(core(v)\) as the coreness value of \(v\). Through core decomposition (Goudi et al., 2016), the initial coreness of each vertex can be determined, and this process can be accomplished in \(O(m)\)(3). Given a graph \(G=(V,E)\) and an edge set \(D=\{(u,v):u,v V,(u,v) E\}\), the **coreness gain** of \(G\) by inserting \(D\), denoted by \(g(D,G)\), is the total increment of coreness for vertices in \(V(G)\), i.e., \(g(D,G)=_{u V(G)}core(u,G^{*})-core(u,G)\), where \(G^{*}\) is the graph after adding the edge set \(D\).

**Problem Statement.** Given a graph \(G=(V,E)\) and a budget \(b 1\), the budget limited coreness maximization (BLCM) problem aims to find a set \(D=\{(u,v):u,v V,(u,v) E\}\) and \(|D| b\) such that the coreness gain \(g(D,G)\) is maximized, i.e.,

\[D^{*}=*{arg\,max}_{|D| b}_{u V(G)}(core(u,G^{*}) -core(u,G)) \]

### Hardness Analysis

Theorem 2.1 ().: _The BLCM problem is NP-hard._

We prove by employing a polynomial reduction from the Maximum Coverage (MC) problem (Kumar et al., 2016) and the rest of the proof can be found in Appendix A.

Theorem 2.2 ().: _The BLCM problem is APX-hard, i.e., for any \(>0\), the BLCM problem cannot be approximated in polynomial time within a ratio of \((1-1/e+)\), unless \(P\)-NP._

We also prove by a polynomial reduction from the MC (Kumar et al., 2016) and the proof is in Appendix A.

Theorem 2.3 ().: _The function \(g(.,G)\) of coreness gain is not submodular._

Proof.: If \(g(.,G)\) is submodular, for any graph \(G\) and two arbitrary edge sets \(A\) and \(B\) of \(G\), it must satisfy that \(g(A,G)+g(B,G) g(A B,G)+g(A B,G)\). But for a graph with \(G=(V,E)\), where \(V=\{1,2,3,4\}\) and \(E=\{(1,2),(2,3),(3,4),(4,1)\}\), if we set \(A=\{(1,3)\}\), \(B=\{(2,4)\}\), then \(g(A,G)+g(B,G)=0<g(A B,G)+g(A B,G)=4\). Therefore \(g(.,G)\) is not submodular. 

For being NP-hard, APX-hard and not sub-modular, sequential greedy edge insertion cannot provide satisfactory performance to the BLCM problem. It is necessary to explore the combinatorial edge insertion. Combinatorial insertion of \(b\) edges into \(n^{2}-m\) possible positions has obviously \(C^{b}_{n^{2}-m}\) complexity, which is prohibitive to explore. Another difficulty is that the coreness improvement has a cascading effect, i.e., a vertex's coreness improvement may trigger the coreness improvement of neighbors and further neighbors.

Seeing the difficulties, this paper pursues low complexity polynomial time while effective algorithms for the BLCM problem. The idea is to combine the advantages of local combinatorial search with the global greedy search. To efficiently evaluate the coreness gain of local combinatorial edge insertion operations despite the cascading effect, we focus on two kinds of local combinatorial edge insertion strategies, i.e., adding edges to improve the coreness of a leading vertex, and adding edges to improve the coreness of group of vertices. We first present methods to efficiently evaluate the coreness gains of these two local combinatorial edge insertion strategies to tailor the cascading effects.

## 3. Gain Evaluation of Local Strategies

We consider two local combinatorial edge insertion strategies. The key is how to efficiently evaluate the coreness gain in each strategy considering the cascading effect.

### Leader-Follower Cascading Effect Analysis

We firstly consider the cascading effect that can be triggered by one node, which is called a leader-follower strategy. We denote \(core(x) k\) as leading \(core(x)\) to \(k\), where \(k\) maybe \(\{core(x)+1,core(x)+2,...,k_{max}\}\) and \(k_{max}\) is the maximum coreness value of all vertices in \(G\). We call the process of \(core(x) k\) as **leading \(x\) to \(k\)-core**, and \(x\) is the _leader_. The vertices whose coreness increase accompanying with \(x\) are called _followers_, denoted as \(F(x,k)\). Then, the leader and followers form a leader-follower structure. **The edge insertion scheme for \(core(x) k\)** is to insert edges between \(x\) and disconnected vertices in \(k\)-core until \(x\) is promoted to the \(k\)-core. We evaluate benefit/cost ratio considering all cascadingly triggered followers by inserting edges to **lead \(x\)** to \(k\)-**core**.

#### 3.1.1. Leader Impact Score

Regarding cost, we denote \(cost(x,k)\) as the number of inserted edges for \(core(x) k\). As for the benefit, it comes from the coreness gain of the leader and the followers. The former is equal to \(k-core(x)\), while the latter is equal to the number of elements in \(F(x,k)\), i.e., \(|F(x,k)|\). By leading \(core(x) k\), any vertex \(u V(G) x\) can increase its coreness by at most \(1\), which is proved in Lemma A.1. So the gain is \(|F(x,k)|+k-core(x)\). The \(cost(x,k)\) can be calculated by:

\[cost(x,k)=k-|nbr(x,G) C^{k}|-|nbr(x,G) F_{k-1}(x,k)| \]

where \(F_{k-1}(x,k)\) indicates the vertices whose coreness is greater than or equal to \(k-1\) among the followers of \(core(x) k\). If we want to lead \(core(x) k\), at least \(k\) edges supported from \(k\)-core are required. \(|nbr(x,G) C^{k}|\) is the existing support, and \(|nbr(x,G) F_{k-1}(x,k)|\) is the followers to be increased to the \(k\)-core. We select vertices that are not adjacent to \(x\) from \(k\)-core and connect them to \(x\). This gets the detailed combinatorial edge insertion scheme to lead \(core(x) k\). Then, the impact score of leading \(x\) to \(k\) can be calculated as:

\[Iscore(x,k)= \]

#### 3.1.2. Leader Followers

The key to calculating \(Iscore(x,k)\) is to calculate \(|F(x,k)|\). We present the following theorem which can avoid repeated graph traversal for calculating \(F(x,k)\). We denote \(I(x,+)\) as the followers after setting \(deg(x)=+\) without adding edges, i.e., anchoring \(x\), and \(I_{k}(x,+)=\{v|v I(x,+) core(v,G) k\}\), i.e., the followers after anchoring \(x\) with original coreness not smaller than \(k\). We then prove:

**Theorem 3.1**.: _For a given graph \(G\), we have \(F(x,k)=I(x,+) I_{k}(x,+)\) for \( x V(G)\)._

Please see detailed proof in Appendix A. According to Theorem 3.1, our focus narrows to compute \(I(x,+)\). But in order to calculate the final \(F(x,k)\), we need to calculate the value of \(|I(x,+)|\) and the detail elements of \(I(x,+)\) to calculate \(I_{k}(x,+)\). To efficiently calculate \(I(x,+)\), we find that the vertices belonging to \(I(x,+)\) must form a "stair" structure with \(x\). Therefore, we define and calculate this "stair" structure by partitioning the graph into shells to calculate the layer structure and the upstair path.

**Definition 3.2**.: Given a graph \(G\), the \(k\)-**shell**, denoted by \(H_{k}(G)\), is the set of vertices whose coreness equal to \(k\), i.e., \(H_{k}(G)=\{v|core(v,G)=k\}\).

We divide the vertices in \(k\)-shell into different layers. We set the vertex set of the \(j\)-th layer in \(k\)-shell as \(H_{k}^{j}(G)\), where \(H_{k}^{1}(G)=\{v|deg(v,C^{k}(G))<k+1 v C^{k}(G)\}\) representing the set of vertices in the \(k\)-core whose degree is less than \(k+1\). The subsequent \(j\)-th layer is derived from the removal of the preceding \(j-1\) layer. Let \(G_{1}=C^{k}(G)\), and \(G_{j}\) is the subgrpah induced from \(C^{k}(G) H_{k}^{1}(G) H_{k}^{2}(G) H _{k}^{j-1}(G)\), then \(H_{k}^{j}(G)=\{v|deg(v,G_{j})<k+1 v G_{j}\}\), and the layer number \(layer(v)\) for \( v H_{k}^{j}(G)\) equals \(j\).

Based on the above analysis, we can traverse \(k\) from \(1\) to \(k_{max}\) to get the layer number \(layer(v)\) for \(v V(G)\). We call it the **Layer Decomposition** Algorithm. The detailed algorithm pseudocode in the rest of the paper can be found in Appendix B.

**Definition 3.3** (_Upstair Path_).: A path \(x v\) is called an upstair path in \(G\) for \(v V(G)\) if it satisfies the following two conditions: (1) for every vertex \(y\) on the path from \(x\) to \(v\) except \(x\), \(core(x) core(y)=core(v)\); and (2) for every two consecutive vertices \(u_{1}\) and \(u_{2}\) from \(x\) to \(v\), it must satisfy \(layer(u_{1})<layer(u_{2})(core(u_{1})=core(u_{2}))\) or \((core(u_{1})<core(u_{2}))\).

**Theorem 3.4**.: _If vertex \(v I(x,+)\), then there must be an upstair path respect to \(x\), that is \(x v\) in \(G\)._

According to Theorem 3.4, only the vertex that can form an upstair path with \(x\) can become an element of \(I(x,+)\). We use \(CF(x)=\{v|x v\}\) to represent the set of vertices that are candidate followers of \(I(x,+)\). Consequently, we only need to traverse the \(CF(x)\) vertices to calculate \(I(x,+)\). And then we determine the final followers by degree check, which we will describe in Section 4.1.

### Group Promotion Cascading Effect Analysis

We further investigate to promote coreness of a local group of vertices and design group score to select the most influential group. We hope to find the groups that can achieve a large increase in sum coreness by inserting a small number of edges in the group.

We find a one-hop structure has above desired property. A vertex may have enough neighbors which can support its coreness improvement, but some of its neighbors lack enough supports to achieve high coreness, so the coreness values of the whole group stay in low values. But only if we add edges among the lack-support neighbors, the overall group will improve their coreness values. More importantly, such one-hop structure is efficient to identify. We therefore propose to find such one-hop group-centric structure.

#### 3.2.1. Group Candidates Selection

To rank the potential groups, we need to identify the group and evaluate the potential coreness gain of the group. The initial step is to identify the central vertex of the group. We firstly define \(core^{}(u)\) as the set of \(u\)'s neighbors whose \(core(.) core(u)\). We designate \(GC(G)\) as the set of vertices that can become the center of the group, where \(GC(G)=\{u|\ |core^{}(u)| core(u)+1\}\). Given that a vertex \(u\) has a higher \(|core^{}(u)|\), its failure to achieve \((core(u)+1)\)-core is likely due to insufficient degree support among \(u\)'s neighbors. So, inserting edges among its one-hop neighbors may yield substantial follower returns with relatively low costs.

By selecting a center \(u\), the initial group \((u)=\{u\}\{v|v nbr(u)/ core(v)=core(u)\}\). We can find a determined scheme of combinatorial edge insertion in \(\) to promote the coreness of all vertices in \((u)\) to \(core(u)+1\). For each \(v\) in \((u)\), we define \(r(.)\) to represent the number of edges required for \(v\) to upgrade to \((core(u)+1)\)-core. **The scheme to promote \((u)\)** is as follows: (1) If \((u)\) has \(o_{1}\), \(o_{2}\), \((o_{1},o_{2}) E((u))\) and \(r(o_{1})>0\), \(r(o_{2})>0\), then connect \((o_{1},o_{2})\). (2) If case (1) doesn't appear, but \(v(u)\) has \(r(u)>0\), then connect \(v\) to the unconnected vertices in \((core(u)+1)\)-core with \(r(v)\) edges. It is clear that this process can promote \((u)\) and the number of inserted edges is defined as \(Gcost((u))\).

We denote \(Ggain()\) as the cascaded coreness gain by promoting \(\). The group score \(GS()=)}{Gcost()}\). To calculate \(Ggain((u))\), we define \(FG((u))\) as the set of vertices whose coreness increases after promoting \((u)\), i.e., \(v FG((u))\) satisfies \(core(v,G^{})>core(v,G)\) and \(G^{}\) represents the graph after promoting \((u)\). We define the improved coreness of \(v\) as \(i_{0}=core(v,G^{})-core(v,G)\). Note that \(FG((u))\) also includes the vertices in \((u)\). Then \(Ggain()\) is obtained by accumulating the coreness increment of each vertex in \(FG((u))\), i.e., \(Ggain((u))=_{v FG((u))}i_{0}\). The key is how to determine the followers.

**Theorem 3.5**.: _If a vertex \(v V(G)\) is a follower after promoting \((u)\), then \( x(u)\), such that \(x v\)._

Through theorem 3.5, the search range of the group's followers can be narrowed down to the vertices with upstair paths, which simplifies the calculation process of \(Ggain()\). Then, we use the improved degree check to determine the final group followers, which will be described in detail in the Section 4.2.

## 4. Greedy Selection of Local Stategies

Empowered by the efficient evaluation of the local combinatorial strategies, we embed the local combinatorial edge insertion into global greedy selection of local strategies to address BLCM both effectively and efficiently. We in particular propose LCGI, GCGI, and LGB three greedy algorithms for local strategy selection.

### Leader-Centric Greedy Insertion (LCGI)

This subsection introduces the Leader-Centric Greedy Insertion Algorithm (LCGI). We first focus on how to calculate \(I(x,+)\) to get \(Iscore(x,k)\). Then, based on the \(Iscore(x,k)\), we greedily select the most influential leaders until budget is used up.

#### 4.1.1. Compute \(I(x,+)\)

To calculate \(I(x,+)\), we first determine the candidate vertex set to reduce the search range. Then, we judge whether each candidate may have a coreness increase. To find candidate followers \(CF(x)\), we denote \(cel^{>}(u)\) as the set of \(u\)'s neighbors whose \(core(.)=core(u)\) and \(layer(.)>layer(u)\). Similarly, we denote \(cel^{}(u)\) as the set of \(u\)'s neighbors whose \(core(.)=core(u)\) and \(layer(.) layer(u)\). We utilize a minimum heap \(H\) to store the vertices in \(CF(x)\) that require traversal. The key for vertex \(v\) is defined as \((core(v),layer(v))\) for sorting, with \(core(.)\) as the first keyword and \(layer(.)\) as the second keyword. We start from the leader vertex \(x\) and continue to expand the vertices that can form an upsair path with \(x\) and put them into the queue \(H\).

To further determine whether the vertices in \(CF(x)\) belong to \(I(x,+)\), we define three statuses for the vertices: _survived_, _unexplored_, and _discarded_. The _unexplored_ status indicates that the vertex has not experienced the degree check. On the other hand, _survived_ signifies that the vertex has passed the degree check. Lastly, _discarded_ indicates that the vertex failed the degree check and will not be traversed in subsequent steps. We set \(d^{+}(v)\) as the degree bound of a vertex \(v CF(x)\), which represents its maximum support at \((core(v)+1)\)-core. _Survived_ vertices may potentially become _discarded_ later if their degree bound is insufficient.

**Theorem 4.1**.: _If a vertex \(u CF(x)\) satisfies \(d^{+}(u)<core(u)+1\), then \(u I(x,+)\)._

_Degree Check._ We use the degree check to check whether the vertices in \(CF(x)\) will be in the _discarded_ state. We calculate

\[d^{+}(u)=d^{+}_{g}(u)+d^{+}_{u}(u)+d_{>}(u) \]

where \(d^{+}_{g}(u)\) represents the number of _survived_ neighbors satisfying \(core(v)=core(u)\); \(d^{+}_{u}(u)\) represents the number of _unexplored_ neighbors belonging to \(\{cel^{}(u) H\} cel^{>}(u)\); and \(d_{>}(u)\) denotes the number of neighbors satisfying \(core(v)>core(u)\) or \(v=x\). According to Theorem 4.1, if \(d^{+}(u)<core(u,G)+1\), \(u\) cannot become a member of \(I(x,+)\) and is marked _discarded_, which leads to a decrease in the degree bound of \(nbr(u)\), which may cause them also to be _discarded_. This discarded state may have a cascading effect and propagate to neighbors again. We summarize the above process and propose the **FindFollowers** algorithm to calculate the set of followers when the coreness of leader \(x\) is raised to positive infinity.

#### 4.1.2. Leader-Centric Greedy Insertion Algorithm

Based on the calculation of \(Iscore(x,k)\), the Leader-Centric Greedy Insertion Algorithm (LCGI) is proposed as given in Algorithm 1. To reduce the search range of \(k\), we set \(Krange(x)\) as the set of coreness of all vertices in \(nbr(x)\) plus 1, representing the coreness values to which the \(nbr(x)\) can be increased. We first calculate the \(Iscore(v,k)\) for each vertex \(v V(G)\) and the corresponding \(k Krange(v)\). Based on the score, we select the leader with the best \(Iscore(v,k)\) in each round and insert the corresponding edges into \(G\) according to the local edge insertion strategy.

```
Input :\(A\) graph \(G=(V,E)\), a budget \(b\) Output :\(D\): the set of inserted edges
1cost = 0, \(D\);
2while\(cost b\)do
3Layer Decomposition(\(G,k_{max}\));
4\(epoch\_cost=0,epoch\_score=0,epoch\_D=\);
5foreach\(v V(G)\)do
6\(I(u,+)=FindFollowers(u,G)\);
7foreach\(k Krange(v)\)do
8\(F(u,k)=I(u,+)-I_{k}(u,+)\);
9 compute \(cost(u,k)\) according to Equation 2;
10\(Iscore(u,k)=\);
11\(D(u,k)=\) Randomly select \(cost(u,k)\) vertices (not adjacent to \(u\)) from \(k\)-core;
12if\(Iscore(u,k)>epoch\_score\)then
13if\(cost+cost(u,k)>b\)then continue;
14\(epoch\_score=Iscore(u,k)\);
15\(epoch\_cost=cost(u,k)\), \(epoch\_D=D(u,k)\);
16
17
18\(cost+=epoch\_cost,D=D epoch\_D\), update \(G\); return\(D\);
```

**Algorithm 1**LCGI (\(G,b\))

In Algorithm 1, in each round, we first calculates the layer decomposition (Line 3). Then, we traverse all vertices to examine all possible \(k\) values and calculate the \(Iscore(v,k)\) (Lines 5-11). We sets \(D(u,k)\) to represent the connecting edges required from leading \(core(x) k\). Simultaneously, if \(Iscore(v,k)\) is higher than the previous score (Line 12) and the \(cost(u,k)\) is no more than the remaining budget (Line 13), we record it as the current optimal solution (Lines 14-15). Finally, we updates the current cost, and inserts the corresponding edges combination (Line 16).

**Theorem 4.2**.: _The time complexity of Algorithm 1 is \(O(b n m)\)._

Proof.: First, the time complexity of \(FindFollowers(u)\) is \(O(m)\) as edge traversal occurs at most three times, we will explain this in detail in Appendix B.2. So the time complexity of Line 6 is \(O(b n m)\). Secondly, when calculating the \(F(v,k)\), we only need to traverse \(I(v,+)\) once to get all \(F(v,k)\) for each \(k Krange(v)\), so the time complexity of Line 8 is \(O(b n|I(v,+)|)<O(b n n)\). Thirdly, the time complexity of Line 9 is \(O(b k_{max}_{v V(G)}nbr(v))=O(b k_{max}_{v V(G)}nbr(v))\).

\(k_{max} m\)). Combined with the above analysis, the time complexity is \(O(b n m)\). 

### Group-Centric Greedy Insertion (GCGI)

This subsection introduces the greedy algorithm based on group cascading effect. We first perform GroupReduction and GroupExpansion on the original group to obtain a more efficient group. Then we calculate the group score \(GS()\) corresponding to each promoted group and select the most influential group in each round.

#### 4.2.1. Group Reduction

The purpose of GroupReduction is to eliminate the vertices with low contributions. For \(}(v)\), we define \(K_{R}=core(v)+1\). We denote \(gc(u)=|\{w|w nbr(u) w}(v)\}|\) as the contribution of vertex \(u\) to the \(}(v)\), and \(gr(u)=K_{R}-|N_{}(u)|\) as the additional cost brought by vertex \(u\), where \(X\) denotes the set of \(u\)'s neighbors and each neighbor \(w\) satisfies \(w}(v)\) or \(core(w) core(u)+1\). Then, we define the value of vertex \(u\) to \(}(v)\) as \(g(u)=gc(u)-gr(u)\).

\(gv(u)<0\) indicates that the contribution of vertex \(u\) to \(}(v)\) is less than the cost it brings, so vertex \(u\) should be removed from \(}(v)\). The removal of vertex \(u\) will alter the \(gc(.)\) and \(gr(.)\) values of \(nbr(u)\) in the group, and we subsequently need to determine whether these neighbors should also be removed from the \(}(v)\). We finally summarize the above process into the **GroupReduction** algorithm, which reduces \(}(v)\) until \( u}(v),gv(u) 0\).

Example 4.3 ().: In Figure 2, we calculate \(gr(11)=6-|\{10\}|=5\), \(gc(11)=|\{10\}|=1\), thus \(gv(11)=gc(11)-gr(11)=-4\). Therefore, vertex \(11\) will be removed from \(}(10)\). And \(gr(10)\) and \(gc(10)\) are also modified accordingly.

#### 4.2.2. Group Expansion

Group Expansion aims to identify vertices outside the \(}(v)\) but making significant contributions to promote \(}(v)\). We define \(K_{E}=core(v)+1\) and evaluate vertices within one-hop of \(}(v)\). We denote \(ec(u)=|\{w|w nbr(u) w}(v) gr(w)>0\}|\) as the contribution to the group, and \(er(u)=K_{E}-|N_{}(u)|\) (similar to \(gr(u)\)) as the additional required expenses. Then we define the value of vertex \(u\) to the group as \(ev(u)=ec(u)-er(u)\). \(ev(u) 0\) indicates that adding vertex \(u\) can contribute to the \(GS(}(v))\). Therefore, \(u\) can be added to \(}(v)\). If vertex \(u\) is added to \(}(v)\), then \(w nbr(u)\) outside the group will change the \(ev(w)\) and can be evaluated whether they should be added to \(}(v)\). We summarize the above process into the **GroupExpansion** algorithm, which searches for vertices around \(}(v)\) where \(ev(.)>0\) for expansion.

Example 4.4 ().: In Figure 2, we traverse the neighbors within the one-hop range of \(}(10)\) and obtain the set \(\{2,3,11\}\). Then, we calculate \(er(2)=6-|\{4,5,6\}|=3\) and \(ec(2)=|\{4,5,6\}|=3\), thus \(ev(2)=0\). Similarly, we get \(ev(3)=0,ev(11)=-5\). Therefore, \(2\) and \(3\) are added to \(}(10)\) during the GroupExpansion process.

#### 4.2.3. Group Score

According to the above two subsections, we have obtained all groups. For each vertex in the group, they must have one of \(er(.)\) or \(gr(.)\), which we will uniformly record as \(r(.)\) same as Section 3.2.1. The \(Gcost(})\) in \(GS(})\) can be easily derived from the \(r(.)\) value and the rules of promotion, so the key is in how to calculate \(Ggain(})\). Further, according to the analysis in Section 3.2.1, we need to calculate \(FG(}(u))\).

**Computing \(FG(}(u))\)**. To calculate \(FG(}(u))\), we define \(CF(}(u))\) as the set of candidate followers. According to Theorem 3.5, \(CF(}(u))\) is the set of vertices that can form an upstair path with \(}(u)\).

Then, we search for the final followers and its coreness increment in \(CF(}(u))\). We set the priority queue \(H\) and set the key to \((core(.)\), \(layer(.))\). When searching for \(CF(}(u))\), we introduce a hierarchical search, meaning that each hierarchy only explores the vertices with same coreness. The vertices within the initial \(}(u)\) are enquenet into the queue \(H\) and then expanded to find \(CF(}(u))\). To evaluate whether the vertices in \(CF(}(u))\) will be added to \(FG(.)\), we define multiple states for the vertices in \(CF(G(u))\). Different from leader's followers, there are cases where the vertex coreness increases to more than \(1\). Hence, vertices that pass the degree check need to be re-enqueued when the coreness of the hierarchy search increases. We add the _reexplored_ status, indicating that it has passed the degree check in the previous hierarchy but has yet to pass it for the current hierarchy. And we save the _survived_, _unexplored_, _discarded_ status same as the LCGI. We initialize the array **Survive()** to record the current increment in its coreness. Furthermore, note that for the _reexplored_ vertices, we default them to the 0-th layer of the high-level \(k\)-core. This enables us to utilize the concept of the upstair path to continue expanding the vertex in the high-level \(k\)-core.

_Degree Check._ To determine the vertex state in \(CF(}(u))\), we implement degree check. We denote \(core^{*}(v)=core(v)+Suroviev(v)\) as the coreness value \(v\) may achieve after promoting \(}(u)\). We calculate:

\[d^{*}(v)=d^{*}_{s}(v)+d^{*}_{u}(v)+d^{+}_{r}(v)+d_{>}(v) \]

where \(d^{*}_{s}(v)\) represents the number of _survived_ neighbors \(w\) satisfying \(core^{*}(w)>core^{*}(v)\), \(d^{*}_{u}(v)\) represents the number of _unexplored_ neighbors \(w\) satisfying \((core(w)=core^{*}(v) layer(w)>layer(v))\) or \(w H\), \(d^{*}_{r}(v)\) represents the number of _reexplored_ neighbors \(w\) satisfying \(core^{*}(w)=core^{*}(v)\) and \(d_{>}(v)\) denotes the number of neighbors \(w\) satisfying \(core(w) core^{*}(v)+1\) or \(w}(u)\).

Theorem 4.5 ().: _If a vertex \(v CF(}(u))\) satisfies \(d^{*}(v)<core^{*}(v)\) +1, then \(core(v,G_{}(u)}) core^{*}(v)\), where \(G_{}(u)}\) is the graph after promoting \(}(u)\)._

Figure 2. An Example of core decomposition. The number besides each vertex indicates its coreness value, while the dotted line indicates an edge connected to a vertex in \(k\)-core.

According to Theorem 4.5, we traverse the vertices in \(CF((u))\) and discard the vertices that can not pass the degree check, and update the \(d^{+}(.)\) of their neighbors, and then judge again whether the updated neighbors will be set as _discarded_. Otherwise, if vertex \(u\) passes the degree check, it is defined as the _survived_, and \(Survive(u)\) is increased, indicating that \(core(u)\) may be increased and \(u\) will continue to perform degree check as the hierarchy increases. Based on the above analysis, we propose the **GroupGain** algorithm, which calculates the total coreness gain of promoting \((u)\).

**Example 4.6**: _In Figure 2, initially, the vertices in \((10)\) are added to \(H\). During the hierarchical traversal of \(H\), the 4-core is first traversed. Vertex \(2\) is dequeued, and its neighbor vertex \(1\) (where vertex \(1^{>}(2)\)) is enqueued. We calculate \(d^{+}(1)=d_{>}(1)=6>core(1)+1=5\). Consequently, vertex \(1\) is marked as survived, and \(Survive(1)=1\). Later, during the hierarchical traversal of the 5-core, vertex \(1\) is enqueued again, and its status is set to _reexplored_. We recalculate \(d^{+}(1)=d_{>}(1)=6>core(1)+Survive(1)+1=6\). Therefore, \(Survive(1)=2\), and it is marked as survived once more. Finally, the coreness of vertex \(1\) is increased by \(2\)._

```
Input :\(A\) graph \(G=(V,E)\), a budget \(b\) Output :\(D\): the set of inserted edges
1\(=0\), \(D\);
2while\( b\)do
3\(epoch\_=0\), \(epoch\_score=0\), \(epoch\_D=\);
4 compute\(GC(G)\);
5foreach\(v GC(G)\)do
6\(D((v))\), \(=0\);
7 Initial\((v)\);
8 GroupReduction(\(G\), \((v)\));
9 GroupExpansion(\(G\), \((v)\));
10\(\)-GroupGain(\(G\), \((v)\));
11\(Q\{u|u r(u)>0\}\);
12while\( x,y Q(x,y) E(G)\)do
13 Add\((x,y)\) into \(D((v))\), \(\)++;
14\(r(x)=r(x)-1(y)=r(y)-1\);
15foreach\( x\)do
16\(\)++\(r(x)\), update\(D((v))\);
17\(GS((v))=}{}\);
18if\(GS((v))>\)then
19if\(cost>b\)then continue;
20\(epoch\_score=GS((v))\);
21\(epoch\_D=D((v))\), \(epoch\_cost=Geost\);
22\(cost\)++\(=epoch\_cost\), \(D=D\), update\(G\); return\(D\);
23
24
25return\(D\);

**Theorem 4.7**: _The time complexity of GCGI is \(O(b k_{max} n m)\)._

The time complexity of the Algorithm 2 is bounded by GroupReduction, GroupExpansion, and GroupGain. In the worst case, GroupReduction invokes GroupShrink for each vertex, leading to a time complexity is \(O(_{v}deg(v))=O(m)\). The worst time complexity of GroupExpansion is \(O(n k_{max}+m)\). It is bounded by the maximum number of vertex visits and updates to \(er(.)\) and \(ec(.)\), as detailed in Appendix B.5. Since the number of times a vertex \(u\) is put into \(H\) is bounded and does not exceed \(core(u)\), the worst time complexity of GroupGain is \(O(m k_{max})\). Since the above three algorithms are called at most \(b n\) times, the time complexity of Algorithm 2 is \(O(b n m k_{max})\). \(\)

### Leader-Group Balance Algorithm

We combine the LCGI Algorithm and the GCGI Algorithm to propose a Leader-Group Balance (LGB) algorithm. As shown in Algorithm 3, in each round LGB uses LCGI and GCGI to calculate the highest score \(LS\) and \(GS\) of the vertex and group respectively. If \(LS\) is higher, it uses Algorithm 1 to get the combinatorial edge insertion. On the contrary, it uses Algorithm 2 to get the combinatorial edge insertion. The time complexity of the LGB depends on Algorithm 1 and Algorithm 2.
``` Input :\(A\) graph \(G=(V,E)\), a budget \(b\) Output :\(D\): the set of inserted edges
1\(=0\), \(D\);
2while\( b\)do
3 compute \(LS\) according to Lines 3-15 in Algorithm 1;
4 compute \(GS\) according to Lines 3-22 in Algorithm 2;
5if\(LS>GS\)then
6 update \(D\), \(cost\), \(G\) according to Algorithm 1;
7
8else
9 update \(D\), \(cost\), \(G\) according to Algorithm 2 ;
10
11return\(D\); ```

**Algorithm 3**LGB (\(G\), \(b\))

The time complexity of the LGB depends on Algorithm 1 and Algorithm 2.

The time complexity of the LGB is \(O(b k_{max} n m)\). Since LGB algorithm calculates \(LS\) and \(GS\) at most \(b\) times in the worst case, the worst time complexity should be the time complexity of calculating \(LS\) and \(GS\), which are \(O(n m)\) and \(O(n m k_{max})\) respectively. Therefore, the final overall complexity will not exceed \(O(b n m k_{max})\). \(\)

Since LGB algorithm considers the optimal solution of two greedy algorithms, it can bring higher coreness gain. Therefore LGB can be used when higher effectiveness is required. Due to the change in the greedy strategy, the running time is not the sum of the two algorithms. In fact, LGB, which combines the two greedy algorithms, will reduce the number of iterations, so the actual running time is sometimes even better than LCGI and GCGI. We will conduct more coreness gain and running time analysis in Section 5.

## 5. Experiments

**Datasets.** Experiments are conducted on 13 real-world large graphs, which can be found in SNAP (Krishna et al., 2017) or Network Repository (Krishna et al., 2017). The statistics are shown in Table 1, sorted by increasing number of vertices, where abbreviations are used for each dataset, \(\) and \(deg_{max}\)represent the average \(deg(.)\) and the max \(deg(.)\) respectively, and \(k_{max}\) represents the max coreness among all \(u V(G)\).

**Settings.** All algorithms are implemented in C++ and compiled by g++ compiler at -O3 optimization level, and they are conducted on a machine with Inter(R) Xeon(R) CPU E5-2667 v4@3.20GHz processor and 256GB memory, with Ubuntu installed.

**Algorithms.** Since there is no algorithm that can directly solve the BLCM problem, to demonstrate the efficiency of our algorithm, we compare it with: (1) four heuristic algorithms; (2) EKC  algorithm revised to solve BLCM; and (3) an exact algorithm to explore edge insertion combinations. Additionally, we contrast it with the SOTA algorithms specialized in \(k\)-core maximization (KM) such as VEK  and FASTCM+ . Detailed descriptions of all algorithms are shown in Table 2. Due to lack of space, some experiments are supplemented in Appendix D.

### Effectiveness

We compare the effectiveness of the proposed algorithms with four heuristic algorithms, the modified EKC algorithm, and the exact algorithm in small graphs respectively.

**1. Comparison with heuristic algorithms.** We compare with the heuristic algorithms on 13 datasets and the results are depicted in Figure 3. It can be seen that the proposed LCGI, GCGI, and LGB algorithms all achieve remarkably higher coreness gain improvement than the four heuristic algorithms. LGB performs slightly better than LCGI and GCGI for it switches between the two policies adtively. Simple degree-based edge selections perform even worse than the random edge insertion in some datasets, showing the BLCM problem's non-intuitiveness.

**2. Comparison with Exact algorithm.** Due to the complexity of combinatorial search, Exact algorithm is only tested on the Gowalla and Facebook datasets after vertex sampling. Specifically, we randomly select a vertex and iteratively add its neighbors until the graph contains 50 vertices. For each dataset, we take 10 small graphs and calculate the average of the final results. We additionally select the deg-s algorithm, which performs better among the heuristic algorithms, for comparison. The experimental outcomes are shown in Figure 4. Comparing with the coreness gain of the Exact algorithm, LGB still provides a substantial coreness gain, and the efficiency comparison will be shown in Section 5.2. In addition, it can be seen that the coreness gain of the deg-s algorithm is far behind that of the Exact algorithm and is still much less than LGB, which further illustrates the effectiveness of our algorithm.

**3. Comparison with the extended EKC algorithm.** To adapt the EKC algorithm for the BLCM problem, we extend its edge traversal range to \(|E|\), so it can carry out the

   Dataset & Abbr. & \(n=|V|\) & \(m=|E|\) & \(\) & \(deg_{max}\) & \(k_{max}\) \\  twitter,open & TC & 8,590 & 473,614 & 110 & 1,516 & 582 \\ plankton2 & PK & 10,800 & 410,400 & 70 & 153 & 71 \\ Emall-Eaton & EE & 36,692 & 183,831 & 10,02 & 1,383 & 43 \\ Facebook & FB & 45,731 & 817,035 & 25,64 & 1,098 & 52 \\ Gowalla & GW & 196,591 & 590,327 & 9,647 & 14,730 & 51 \\ DBLP & DB & 377,080 & 1,080,866 & 6,62 & 93 & 112 \\ Amazon & AZ & 334,863 & 925,872 & 5.53 & 549 & 6 \\ youtube & TTT & 495,750 & 1,936,783 & 7.81 & 23,409 & 49 \\ Google & GCG & 575,713 & 432,051 & 9.87 & 6,332 & 44 \\ lastrin & LF & 1,918,805 & 451,930 & 7.58 & 5,150 & 70 \\ polec & FC & 1,829,803 & 300,624,564 & 7.57 & 14,854 & 47 \\ fluster & FS & 2,523,336 & 7.198,801 & 6.28 & 1,474 & 68 \\ LiveJournal & IJ & 4,847,571 & 64,99,773 & 28.46 & 20,333 & 372 \\   

Table 1. Datasets

  
**Algorithm** & **Description** \\ 
**Exact** & Enumerate all \(b\) edge combinations from \(|E|\) and select the optimal strategy. \\ 
**Rand** & **Randomly insert \(b\) edges from \(|E|\). \\ 
**deg** & Select the vertex with the highest \((u)\) and lead \(u\) to \(k\)-core, where \(k\) is determined in the same way as Section 4.1.2 and the same below. \\ 
**deg-c** & Select the vertex with the highest \(deg_{a}(u)=deg(u)-core(u)\) and lead \(u\) to \(k\)-core. \\ 
**deg-s** & Select the vertex with the highest \(deg_{a}(u)=\{|\{a\}_{0}vr(u)(core(u)>core(u)core(u)=core(u) layer(u)> layer(u)\})\) and lead \(u\) to \(k\)-core. \\ 
**EKC** & **A** greedy edge-enumeration algorithm for \(k\)-core maximization algorithm . We extend it to solve the BLCM problem. \\ 
**VEK** & **A** greedy vertex-enumeration algorithm for \(k\)-core maximization algorithm . \\ 
**FASTCM+** & The state-of-the-art algorithm for \(k\)-core maximization algorithm . \\ 
**LCGI** & **A** greedy edge insertion strategy based on leader-followers structure in Algorithm 1. \\ 
**GCGI** & **A** greedy edge insertion strategy based on group-centric structure in Algorithm 2. \\ 
**GCGI-R** & GCGI without GroupReduction. \\ 
**GCGI-E** & **G**CGI without GroupExpansion. \\ 
**GCGI-R-E** & **G**CGI without GroupReduction and Group Expansion. \\ 
**LGB** & **A** combined greedy edge insertion strategy to balance LCGI and GCGI in Algorithm 3. \\   

Table 2. Description of Algorithms

Figure 4. LGB vs. Exact and deg-s on Coreness Gain.

Figure 5. LGB vs. EKC on Coreness Gain.

task. However, from the comparison results shown in Figure 5, we can see that due to EKC's inability to explore the benefits of edge combinations, the coreness gain achieved by the EKC algorithm on twitter_copen and pkustk02 is less than LGB, even failing to yield any coreness gain on pkustk02.

### Efficiency

In efficiency comparison, we conduct experiments using four algorithms: LCGI, GCGI, LGB and extended EKC. We set \(b\) to 50.

**Overall Performance.** The experimental results are shown in Figure 6. The EKC algorithm cannot be fully displayed because it takes too long to run (\(>10^{5}\)s) on bigger datasets after FB or runs out of memory. As the graph size increases, the time-costs of the three proposed algorithms show a slow increasing trend. GCGI usually takes less time. Though LGB usually takes a longer time, it does not merely sum up the time costs of LCGI and GCGI. This is primarily because the fusion of the two greedy strategies results in alterations of the running rounds. In addition, the time of the extended EKC algorithm increases quickly as the data scale increases. Considering the corness gains achieved by our proposed algorithms, they achieve impressive corness gain improvements while using affordable time costs.

**Comparison with the Exact and the extended EKC algorithms.** The experimental settings for comparing with Exact and EKC Algorithms are the same as the previous comparison of coreness gain, and the final time cost is shown in Table 3. LGB has a remarkable improvement in time compared to Exact. The Exact algorithm requires huge time costs even on small-scale graphs and small \(b\). Therefore, although LGB loses a certain degree of coreness gain, it is highly efficient in larger-scale graphs, so the reduction in coreness gain is acceptable.

In comparison with extended EKC, the LGB algorithm has a significant improvement in time. As the graph size increases further, such as from pkustk02 to the Email-Enron dataset, the time complexity of EKC increases exponentially, which makes it unable to handle large-scale datasets.

### Comparison with other KM algorithms

VEK and FASTCM+ are efficient algorithms used to address the KM problem. We compare the sum corness gains achieved by these KM algorithms and the LGB algorithm. We focus on the Facebook and Gowalla datasets with \(b=50\), and \(k\) varying from 5 to 50. The results are illustrated in Figure 7. The experiments reveal that the effectiveness of VEK and FASTCM+ varies significantly when different values of \(k\) are set. However, the coreness gain they achieve is both less than the LGB algorithm. Additionally, compared to the KM problem, which needs to manually specify \(k\), the LGB algorithm eliminates this requirement, reducing the manual effort involved.

In addition, we conduct further experiments to explore the coreness distribution of followers in the KM and BLCM problems. We applied the VEK and FASTCM+ algorithms with different \(k\) values (VEK-\(k\) and FASTCM+\(k\) denote the set of the \(k\)) on Gowalla datasets and compared them with LGB. The results are shown in Figure 8. The followers of LGB are distributed among vertices with different coreness, whereas VEK-\(k\) and FASTCM+\(-k\) are concentrated in the vertex with coreness \(k-1\). The diversity of followers in the BLCM problem is more conducive to real-world scenarios and can better enhance average user engagement.

## 6. Conclusions

From the perspective of enhancing social network average user engagement through combinatorial edge insertions, this paper introduces the BLCM problem, proves the problem is NP-hard and APX-hard. We further prove the coreness gain function is not submodular. We propose efficient methods to evaluate the cascaded coreness improvements of two local combinatorial strategies and provide solutions to the key problem of evaluating the cascading effects. Based on this, we then propose three efficient combinatorial edge insertion strategies: LCGI, GCGI and LGB. We prove the polynomial time complexity of LCGI, GCGI and LGB. Experiments conducted on 13 real-world datasets highlight their practical utility, efficiency and effectiveness over existing approaches.

Figure 8. Distribution of followers on Coreness in Gowalla.

Figure 6. Time Cost from Different Algorithms, \(b\)=50.

  b & Gowalla & Facebook & b & pkust02 & Email-Enron \\   & Exact & LGB & Exact & LGB & EKC & LGB & EKC & LGB \\ 
1 & 0.001 & **0.001** & 0.002 & **0.001** & 10 & 1926.4 & **116.2** & 9487.4 & **56.2** \\
2 & 0.360 & **0.001** & 0.711 & **0.001** & 20 & 3513.1 & **156.7** & 17962.1 & **127.7** \\
3 & 104.74 & **0.001** & 2081.01 & **0.001** & 30 & 5397.4 & **156.9** & 26691.5 & **159.1** \\
2 & 2053.6 & **0.001** & 75627.7 & **0.001** & 40 & 7145.6 & **239.6** & 138435.5 & **230.6** \\
5 & -100000 & **0.001** & \(\)100000 & **0.001** & 50 & 8619.0 & **242.3** & 40521.3 & **258.9** \\  

Table 3. Comparison of Time Cost (s) between LGB, Exact, and EKC on Different Datasets

Figure 7. Coreness Gain on different \(k\).