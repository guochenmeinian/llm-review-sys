# Aggregate to Adapt: Node-Centric Aggregation for Multi-Source-Free Graph Domain Adaptation

Anonymous Author(s)

###### Abstract.

Unsupervised graph domain adaptation (UGDA) focuses on transferring knowledge from labeled source graph to unlabeled target graph under domain discrepancies. Most existing UGDA methods are designed to adapt information from a single source domain, which cannot effectively exploit the complementary knowledge from multiple source domains. Furthermore, their assumptions that the labeled source graphs are accessible throughout the training procedure might not be practical due to privacy, regulation, and storage concerns. In this paper, we investigate multi-source-free unsupervised graph domain adaptation, i.e., exploring knowledge adaptation from multiple source domains to the unlabeled target domain without utilizing labeled source graphs but relying solely on source pre-trained models. Unlike previous multi-source domain adaptation approaches that aggregate predictions at model level, we introduce a novel model named GraphATA which conducts adaptation at node granularity. Specifically, we parameterize each node with its own graph convolutional matrix by automatically aggregating weight matrices from multiple source models according to its local context, thus realizing dynamic adaptation over graph structured data. We also demonstrate the capability of GraphATA to generalize to both model-centric and layer-centric methods. Comprehensive experiments on various public datasets show that our GraphATA can consistently surpass recent state-of-the-art baselines with different gains. Our source codes and datasets are available at [https://anonymous.4open.science/r/GraphATA-CoDR](https://anonymous.4open.science/r/GraphATA-CoDR).

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

## 1. Introduction

Web data is inherently complex, characterized by diverse entities and intricate relationships, making it challenging to mine meaningful insights. Graph algorithms play a pivotal role in numerous web applications, enabling more efficient representation , analysis , and decision-making , etc. While graph neural networks (GNNs)  have achieved remarkable success across diverse tasks including node classification , traffic forecasting , molecular property prediction  and web-scale recommendation , these GNN models exhibit substantial performance deterioration when applied to graphs with domain discrepancies . To mitigate this gap and eliminate the need for label annotations, unsupervised graph domain adaptation  has been proposed to adapt the model by transferring knowledge from labeled source graph to unlabeled target graph. Existing graph domain adaptation approaches either learn domain invariant representations via adversarial training  or explicitly minimize the domain distribution discrepancy  to improve their generalization capability.

However, the above mentioned methods assume that the knowledge is specifically transferred from a single labeled source domain to an unlabeled target domain. Whereas, in the real world scenarios, data are often collected from multiple domains, which provides a range of complementary knowledge from different perspectives. This could significantly benefit target domains that do not strictly align with any single available source domain. For example, social networks might come from different countries and platforms with linguistic diversity. If the source networks are popular for a particular language like English or Spanish and the target network involves a mix of different languages, the adaptation can be tailored to target distribution by aggregating knowledge from multiple sources. To this end, Multi-Source Domain Adaptation (MSDA)  is introduced to learn from multiple source domains, allowing it to obtain complementary knowledge from various source domains and making it more resilient to domain shifts.

Unfortunately, recent MSDA approaches require labeled source data during the adaptation procedure, which might be impractical due to privacy as well as security concerns, especially when source data containing sensitive information, e.g., financial transactions  and medical diagnosis , etc. Therefore, it is imperative to investigate Multi-Source-Free Domain Adaptation (MSFDA) by relying solely on source pre-trained models without access to any labeled source data . A simple yet straightforward solution for addressing MSDFA is to employ existing single-source-free domain adaptation methods  to adapt each source model individually, then the predictions from different source models are averaged to generate the final prediction. Nonetheless, it ignores the transferability of different source domains, since they may contribute differently to the target domain.

There are some recent studies that automatically assign weights to source predictions , where a larger value indicates higher transferability. Nevertheless, these aforementioned methods are designed for independent and identically distributed (i.e., iid) data, while the existence of non-iid graph-structured data poses great challenges to MSFDA that remain unexplored. In graphs, nodes are interconnected with each other through edges, formingcomplicated graph structure. Existing model-centric adaptation approaches, which learn a weight for each model, might not be adequate to capture the complementary semantics encoded by each model, leading to inaccurate combination of predictions. The main reason is that different nodes are associated with distinct local neighborhoods, thus globally aggregating source model predictions ignores the fine-grained node level disparity. For instance, source models are trained on two different social networks, e.g., one's connections emphasizing shared interests and the other one's links indicating geographical proximity. Then, we want to adapt these source models to classify node in target network, where neighboring connections might arise from shared interests as well as geographical proximity. As shown in Figure 1, the combination of model level predictions fails to adapt to different local patterns in the target network and results in sub-optimal performance. No matter how the predictions are merged, the outcome remains inaccurate because the individual predictions themselves are flawed. Thus, more devotion is required to effectively handle the graph domain adaptation task with fine-grained information.

To address the aforementioned key challenges, we propose a novel framework named GraphATA (_Aggregate To Adapt_), _which performs node-centric adaptation through dynamically parameterizing each node with a unique graph convolutional matrix_. Instead of globally aggregating source model predictions, we conduct fine-grained adaptation by taking each node's local context information into consideration. At each layer, we generate a personalized graph convolutional matrix for each node by automatically aggregating source models' weight matrices based on its local neighborhood. Therefore, different nodes could have distinct optimal weight matrices, which is flexible to adapt to diverse patterns. Furthermore, sparse constraints are employed to filter out irrelevant information, since not all the source models are useful during the adaptation procedure. We have carried out extensive experiments including node as well as graph classification, and the experimental results demonstrate that our proposed GraphATA outperforms recent state-of-the-art baselines over widely used datasets.

In summary, the main contributions of this paper are as follows:

* To the best of our knowledge, we are the first to investigate the problem of multi-source-free unsupervised graph domain adaptation, which is a practical yet unexplored setting within the graph neural network community.
* We propose a node-centric adaptation framework that parameterizes each node with a personalized graph convolutional matrix according to its local context information, which enables a more generalizable model.
* Extensive experimental results show that GraphATA could achieve state-of-the-art performance across various public datasets with thorough ablation studies further validating the effectiveness of our node-centric adaptation.

## 2. Related Work

**Graph Neural Networks.** With the remarkable success in various graph related tasks, graph neural networks have drawn continuous attention in both academic and industrial communities. Different types of graph neural networks have been designed following the message passing paradigm, which can be categorized into spectral methods (Golov et al., 2013; He et al., 2016; He et al., 2016) and spatial methods (He et al., 2016; He et al., 2016; He et al., 2016). Among them, GCN (He et al., 2016) performs convolution by approximating the Chebyshev polynomial (He et al., 2016) using its truncated first-order graph filter. GAT (He et al., 2016) utilizes an attention mechanism to learn different weights for dynamically aggregating node's neighborhood representations. GraphSAGE (He et al., 2016) introduces an inductive framework that generates representations by sampling and aggregating local representations. For more details, please refer to comprehensive surveys on graph neural networks (He et al., 2016; He et al., 2016). Despite their success, the performance of GNNs depends on high-quality labeled data, which can be challenging for graph-structured data. To address this issue, adapting models trained on label-rich source domains to unlabeled target domains has emerged as a promising solution.

**Unsupervised Domain Adaptation.** The goal of domain adaptation is to transfer knowledge from labeled source domains to unlabeled target domains. One key challenge lies in how to mitigate the domain shifts between source and target domains. To reduce the distribution discrepancy, most methods focus on learning domain invariant representations, which involve either explicit or implicit constraints. For example, some works (Zhu et al., 2017; He et al., 2016) employ maximum mean discrepancy or central moment discrepancy to explicitly minimize the distance between source and target distributions. Other studies (He et al., 2016; He et al., 2016) utilize adversarial training to make the domain discriminator unable to differentiate source and target representations. Recently, there have been endeavors dedicated to unsupervised domain adaptation for non-iid graph-structured data. Particularly, UDAGCN (Wang et al., 2017) follows the adversarial training framework to learn domain invariant representations on graphs. GRADE (Zhu et al., 2017) introduces the metric of graph subtree discrepancy to minimize the distribution shift between source and target graphs. SpecReg (Zhu et al., 2017) designs spectral regularization for theory-grounded graph domain adaptation. Liu et al. (Liu et al., 2017) proposes an edge re-weighting strategy to reduce the conditional structure shift. Mao et al. (Mao et al., 2017) preserves target graph structural proximity and Zhang et al. (Zhang et al., 2017) conducts collaborative adaptation in the scenario of single source-free graph domain adaptation. However, these methods cannot address the multi-source-free graph domain adaptation problem since they require labeled data or are unable to adapt complementary knowledge from multiple source domains.

**Multi-Source-Free Domain Adaptation.** MSFDA extends domain adaptation by transferring knowledge from multiple source

Figure 1. A toy example, where GNN 1 excels in modeling shared interests, whereas GNN 2 is good at capturing geographical proximity. If node B has mixed connection types, simply combining the predictions from GNN 1 and GNN 2 is ineffective, as neither of the source pre-trained GNNs performs well in this scenario.

pre-trained models without accessing any source domain data. To capture the relationship among different source domains, various domain weighting strategies are utilized to estimate the contribution of each source domain to the target domain, including uniform weights, wasserstein distance-based weights and source domain accuracy-based weights (Zhu et al., 2017; Wang et al., 2018; Wang et al., 2019; Wang et al., 2019). Due to the absence of source data, the above strategies are not applicable in the MSFDA setting. Towards this end, DECISION (Abbott et al., 2018) and CAiDA (Cai et al., 2019) aggregate multiple source model predictions and construct pseudo labels for model adaptation. Shen et al. (Shen et al., 2019) propose to balance the bias-variance trade-off through domain aggregation, selective pseudo-labeling and joint feature alignment. Nonetheless, all these models are designed for independent and identically distributed data, which are not suitable for non-iid graph structured data. Moreover, aggregating model level predictions is insufficient to capture the highly diverse graph patterns, since the global weights cannot adequately reflect the importance of each node's local context. In contrast, our model performs adaptation at node granularity with aggregating weight matrices from multiple source models according to its local context.

## 3. Problem Statement

**Notations and Problem Definition.** In multi-source-free unsupervised graph domain adaptation, the goal is to jointly adapt multiple source pre-trained graph neural network models to a target graph without any labels. In this paper, we focus on adapting classification models with \(K\) categories. Formally, let \(=(,,)\) denote the unlabeled target graph, where \(\) and \(\) are the node set and edge set respectively. \(^{n d}\) indicates the node feature matrix, with \(n\) representing the number of nodes and \(d\) denoting the dimension of node features. Given a set of source pre-trained GNN models \(\{_{1},_{2},,_{m}\}\), where the \(i\)-th model is trained using the graph from \(i\)-th source domain, we decompose each source model \(_{i}\) into two basic components, i.e., the feature extractor \(_{i}:^{n d}\) encoding graph \(\) into node representation space and the classifier \(_{i}:^{n d}^{n d}\)\(^{K}\) projecting node or graph representations into corresponding class labels. Hence, the source model \(_{i}\) can be expressed as \(_{i}=_{i}_{i}\). Our ultimate problem can be reformulated as follows:

_Given an source trained graph neural network models \(\{_{1},,_{m}\}\) and an unlabeled graph \(\) (node level task) or a set of unlabeled graphs \(\{_{1},,_{n}\}\) (graph level task) from target domain, our goal is to build a target model \(_{t}\) that aggregates knowledge from multiple source models to achieve accurate predictions in target domain under distribution shifts._

**Message Passing GNN Revisiting.** Most GNNs adopt the message passing framework (Golov et al., 2013; Wang et al., 2018; Wang et al., 2019), where each node iteratively aggregates representations from its local neighborhood. Specifically, the node \(v\)'s representation at layer \(l\) can be calculated as follows:

\[_{v}^{l}=((\{_{v}^{l-1}\}\{ _{u}^{l-1}, u(v)\})^{l}), \]

where \(()\) is the activation function and \(()\) represents the permutation-invariant aggregation function that aggregates message from its neighbors \((v)\). \(^{l}\) denotes the convolutional matrix at layer \(l\). The aggregation process in mainstream GNNs can be generalized as a weighted summation. For example, GCN (Kipf and Welling, 2017) aggregates neighborhood representations using fixed weights inversely proportional to node degrees. GraphSAGE (Golov et al., 2013) utilizes a mean pooling aggregator, while GAT (Wang et al., 2019) employs an attention mechanism for learnable weighted aggregation. For graph classification task, we simply use global mean pooling and max pooling to assemble all the node representations in the graph. Advanced techniques like hierarchical graph pooling can also be utilized in this scenario (Wang et al., 2019).

## 4. The Proposed GraphATA Model

Figure 2 provides a comparison between existing model-centric methods and our proposed node-centric framework. _Specifically, model-centric adaptation approaches allocate a weight to each model, implying that all the nodes in the target graph share the same weight within each model._ Hence, it fails to reflect the unique characteristic of each individual target node, since the same model may exhibit varying capabilities when encoding different nodes. _In contrast, our node-centric adaptation framework GraphATA takes node disparity into consideration and parameterizes a unique convolutional matrix for each node to achieve fine-grained personalized adaptation._ Particularly, each node derives its own convolutional matrix by automatically aggregating matrices from multiple source GNN models based on its local neighborhood, which results in more generalizable model. Subsequently, we will elaborate the details of the proposed modules.

**Node Neighborhood Disparity.** We start by investigating the local context of each node within the graphs. Different nodes typically exhibit diverse structural patterns as they are not uniformly distributed across the graph. To characterize this property, we conduct a thorough examination of the node's homophilic and heterophilic patterns through the lens of node homophily ratio, which is a widely adopted metric that quantifies the proportion of a node's neighbors having the same class label (Zhu et al., 2017; Wang et al., 2019; Wang et al., 2019). It is formally defined as follows:

\[h_{o}=(v):y_{u}=y_{v}\}|}{|(v)|}, \]

where \((v)\) represents node \(v\)'s neighbors set and \(y_{v}\) indicates the class label for node \(v\). Figure 3 demonstrates the node homophily ratio distributions on three social graphs from Twitch datasets (Section 5.1). We can observe that _(1) all three graphs manifest a mixture of homophilic as well as heterophilic patterns; (2) the patterns' distributions vary significantly across different graphs_. Thus, existing model-centric methods (Abbott et al., 2018; Cai et al., 2019) overlook each node's neighborhood disparity and the allocated weights might be sub-optimal. The above observations motivate us to perform fine-grained node-centric adaptation.

**Node-Centric Adaptation.** In the above investigation, we recognize the necessity of adapting to the local context of each individual node. To achieve this goal, we propose to assign distinct matrices to different nodes by aggregating convolutional matrices from the source pre-trained models, rather than aggregating model predictions. Specifically, different pre-trained models in the source domains have encapsulated different semantic information, which demonstrate varying capabilities in encoding the local context of each target node. For each node \(o\), we utilize a straightforward yet effective way to represent its local contextual information at layer \(l\) as follows:

\[_{}^{l}=((_{u}^{l-1}, u(v) )), \]

where we adopt mean operation to pool its neighbor's representation \(_{u}^{l-1}\) from previous layer and \(_{}^{l}^{d_{l-1}}\). More alternative options are presented at Appendix E.

After having obtained the local context \(_{}^{l}\), we generate a personalized graph convolutional matrix for node \(v\) as follows:

\[_{v}^{l}=_{i=1}^{m}_{ui}^{l}(_{}^{l })_{i}^{l}+_{g}^{l}, \]

where \(_{i}^{l}^{d_{l-1} d_{l}}\) represents the convolutional matrix from the \(i\)-th source pre-trained GNN model at layer \(l\) and \((_{}^{l})\) is the \(d_{l-1} d_{l-1}\) diagonal matrix with its elements setting as \(_{}^{l}\). The attentive coefficient \(_{ui}\) characterizes the importance of each source domain model when adapting to node \(v\). We further incorporate a global parameter \(_{g}^{l}\) shared by all the nodes in the \(l\)-th layer to capture the global general patterns and \(\) is a trade-off parameter. Therefore, our derived personalized \(_{v}^{l}\) considers not only local but also global aspects of the graph, making it more adaptable to different types of distribution shifts.

**Sparse Attention Selection.** In Equation (4), although \(_{v}^{l}\) automatically aggregates the convolutional matrices from multiple source models according to its local context, we posit that not all the source domain models are useful, which is known as "negative transfer" (Chen et al., 2017; Wang et al., 2018). To combat this issue, we aim to filter out detrimental models and preserve a sparse mixture of effective models via attention coefficients. Particularly, we utilize a shared linear transformation parametrized by \(^{l}^{d_{l}}\) to quantify the trustworthy and reliability of each model when adapting to the target node's local contextual information \(_{}^{l}\). At each layer, the attention score can be calculated as follows:

\[_{ui}=(,_{},_{i})= ^{}(_{i}^{}_{}), \]

where the superscripts are omitted for simplicity. Additionally, we normalize the scores to ensure that \(_{ui}\) and \(_{i=1}^{m}_{ui}=1\). One commonly utilized approach is to employ the softmax function; however, it always produces non-zero values, which fails to yield the desired selective results.

Inspired by recent successes on sparse activation functions, we choose to adopt the sparsemax function (Wang et al., 2018), which preserves the crucial properties of the softmax function and generates sparse distributions. It projects the input onto the probability simplex as follows:

\[()=*{arg\,min}_{^{ m-1}}\|-\|^{2}, \]

where the simplex \(^{m-1}=\{^{m}|^{}=1, 0\}\). Its closed-form solution can be formulated as follows:

\[_{i}()=\{_{i}-()\}_{+}, \]

where \([x]_{+}=\{0,x\}\) and \(()\) is the threshold function that satisfies \(_{j}\{_{j}-()\}_{+}=1\). To compute \(()\), we first sort \(\) in descending order: \(_{1}_{2}_{m}\), and define \(=\{1 j m|_{j}>(_{i=1}^{j}_{i}-1)\}\). Then, we have \(()=^{m}_{i}-1}{}\). The sparsemax function truncates the values below the threshold to zero and shifts the remaining values by this threshold. Detailed proof is provided in Appendix A.

**Model Optimization.** To optimize the model's parameters, we leverage predictions from the nearest neighbors to generate pseudo labels. For the stability of the learning procedure, we maintain

Figure 3. Node homophily ratio distributions.

Figure 2. An illustrative comparison between existing model-centric methods and our proposed node-centric framework. (a) The target prediction is the weighted combination of source models’ predictions. (b) GraphATA performs fine-grained adaptation by considering each node’s unique characteristic. The grey box with dash lines shows the personalized convolutional matrix for each node at layer \(l\).

a target representation bank \(=[}_{1},,}_{n}]\) and a prediction bank \(=[}_{1},,}_{n}]\) through a momentum updating manner as follows:

\[}_{i}=(1-)}_{i}+_{i},\ }_{i}=(1-)}_{i}+_{i}, \]

where \(\) denotes the smoothing parameter setting as 0.9 by default. \(_{i}^{d}\) and \(_{i}^{K}\) are the outputs of feature extractor \(_{t}\) and classifier \(_{t}\), respectively. Then, for each target representation \(_{i}\), we extract \(r\) nearest neighbors from representation memory bank \(\) according to their cosine similarities. With the nearest neighborhood information, the pseudo label distribution of sample \(i\) can be obtained by aggregating the predicted class distributions of these nearest neighbors in memory bank \(\) as follows:

\[}_{i}=[_{k}((i)|}_{ j(i)}}_{j})], \]

where \([]\) represents the one-hot transformation function and \((i)\) is a set of \(r\) nearest neighbors' indices for sample \(i\). Thus, we could update the model's parameters by minimizing the cross entropy loss between the generated pseudo labels and the predicted class distributions as follows:

\[_{cls}=-_{i=1}^{n}_{k=1}^{K}}_{i, k}(_{i,k}). \]

Additionally, we further encourage the prediction to be individually certain and globally diverse (Hendricks et al., 2017) to avoid the degenerated prediction. Therefore, we minimize the entropy for each individual sample while maximizing the entropy for each class, which is expressed as follows:

\[_{reg}=(_{i=1}^{n}(_{i})]- (_{i=1}^{n}_{i}). \]

Among them, \((_{i})=-_{k=1}^{K}}_{i,k}( _{i,k})\) denotes the entropy function. Finally, we can obtain the overall objective function as follows and the training procedure is presented in Algorithm 1:

\[=_{cls}+_{reg}. \]

**Model Analysis.** We discuss how our GraphATA generalizes to existing state-of-the-art methods. _(1) Relation with layer-centric approaches._ In particular, when setting \(_{0}=1\) and \(_{d}=\), we suppress the awareness of each node's local contextual information, thus every node will have the same matrix expressed as \(_{1,,n}^{1}=_{k=1}^{n}_{k}_{i}^{k}\) within each layer. It essentially performs a weighted combination of the node representations propagated at each layer. Taking GCN (Kipf and Welling, 2017) as an example, we have \(^{I}=(}^{-1}_{i=1}^{n}_{i} _{i}^{I})=_{i=1}^{n}_{i}(} ^{-1}_{i}^{I})\), where \(()\) is the ReLU activation function and \(}\) indicates the normalized adjacent matrix with self-loops. _(2) Relation with model-centric methods._ If we further restrict the information aggregation to the last layer \(L\), i.e., the allocated weights are only employed for aggregating the predictions from each model and there is no information fusion in the intermediate layers, the simplified GraphATA degenerates to model-centric methods. _In summary, the design of GraphATA enjoys various benefits by taking each node's local context into consideration, and the current layer-centric as well as model-centric approaches are its special cases._

**Complexity Analysis.** Suppose that we have a graph with \(n\) nodes and \(e\) edges, the node representation dimension is set as \(d\) and the graph neural network has \(L\) layers. Calculating the local contextual information in each layer has the cost of \((ed)\). Then, if we have \(m\) source models from different domains, the time complexity of generating sparse attention weights for each node is \((md+m(m))\). Over \(L\) layers, the feature encoder has the time complexity of \((Lnd^{2}+Led)\). Computing pseudo labels involves obtaining nearest neighbors, which takes \((d(n))\) using k-d tree. Thus, the overall time complexity of our model falls within the same range as that of vanilla graph neural network. Detailed comparisons with baselines are presented at Appendix D.

## 5. Experiments

### Datasets

To fully validate the effectiveness of our proposed GraphATA, we perform node and graph classification tasks from various domains. The summary of dataset statistics is presented in Table 1 and the details are described as follows:

_CSBM_ is a synthetic dataset, which is composed of four graphs generated by a 4-class contextual stochastic block model (Chen et al., 2016). Each class contains 2,000 nodes in each graph. To synthesize different conditional structural shift, we fix the intra-class edge probability \(p\) and vary the inter-class probability \(q\) to generate different graphs. The node attributes are sampled from multivariate normal distributions with different mean vectors. The detailed process is described in Appendix B.

_Twitc1_(Wang et al., 2016) consists of six social networks from different regions, i.e., Germany (DE), England (EN), Spain (ES), France (F), Portugal (P) and Russia (R). The nodes represent users, while the edges denote their friendships. We construct node attributes from various factors such as users' gaming activities, preferences, geographic locations and streaming habits, etc. All the nodes are classified into two categories based on whether they use explicit language.

_Citation_(Zhu et al., 2016) contains three research paper citation networks from different platforms and time periods. Particularly, DBLPv7 (D) is extracted from the DBLP database spanning the years 2004 to 2008; ACMv9 (A) comprises papers from ACM database between years 2000 and 2010; Citation1 (C) is derived from MAG database prior to the year 2008. We categorize each paper into one of the five classes, i.e., DB, AI, CV, IS and Networking.

   Datasets & \#Nodes & \#Edges & \#Feat & \#Class \\  CSBM & 8,000\(-\)8,000 & 607,699\(-\)752,776 & 128 & 4 \\ Twitc1 & 1,912\(-\)9,498 & 31,299\(-\)153,138 & 3,170 & 2 \\ Citation & 5,484\(-\)9,360 & 8,117\(-\)15,556 & 6,775 & 5 \\  Proteins & -39.00 & -72.82 & 4 & 2 \\ Mutagenicity & -30.32 & -30.77 & 14 & 2 \\ Frankenstein & \(\)16.90 & \(\)17.88 & 780 & 2 \\   

Table 1. Dataset Statistics.

For graph classification task, we utilize three widely adopted datasets from TUMatasets2(TUMatasets, 2019), i.e., _Proteins_, _Mutagenicity_ and _Frankenstein_. To differentiate the distribution shifts in the datasets, we partition each dataset into four disjoint groups based on their density. Specifically, we first sort all the graphs in each dataset by their density in an ascending order, and then divide them into four equally disjoint groups.

### Baselines

We compare our proposed GraphATA with four groups of baselines including _source-needed_, _no-adaptation_, _single-source-free_ and _multi-source-free_ domain adaptation methods. The detailed introduction is elaborated as follows:

_Source-needed_. Approaches in this category leverage the labeled source domains' samples to explicitly address the distribution shifts. We consider two multi-source-needed models MDAN (Zhu et al., 2017), M\({}^{3}\)SDA (Zhu et al., 2017) and three single-source-needed models UDGCN (Zhu et al., 2017), GRADE (Zhu et al., 2017) and SpecReg (Zhu et al., 2017) as our baselines. For single-source-needed methods, we merge all the samples from different source domains into a single unified source domain.

_No-adaptation_. This group of baselines include widely used graph neural network models like GCN (Kipf and Welling, 2016), GraphSAGE (Hamilton et al., 2017), GAT (Vaswani et al., 2017) and GIN (Zhu et al., 2017). The model is trained on each labeled source domain and then directly evaluated on the target domain. We output final predictions by taking an average of soft predictions from all the source models.

_Single-source-free_. We also extend existing single-source-free models including SHOT (Zhu et al., 2017), BNM (Zhu et al., 2017), ATDOC (Zhu et al., 2017), NRC (Zhu et al., 2017), JMDS (Zhu et al., 2017), GTrans (Xu et al., 2017), SOGA (Zhu et al., 2017), GraphCTA (Zhu et al., 2017) and TPDS (Zhu et al., 2017) to work in the scenario of multi-source-free domain adaptation. To achieve this goal, we utilize ensemble averaging to integrate soft predictions from all adapted source models.

_Multi-source-free_. Methods in this classes are recent state-of-the-art multi-source-free domain adaptation baselines such as DECISION (Aguilar et al., 2017), CAIDA (Aguilar et al., 2017) and MSFDA (Zhu et al., 2017). They automatically assign suitable weights to each model's predictions for final predictions. These models are originally designed for i.i.d images and we replace its backbone to adapt them for graph structured data.

**Experimental Settings**. Following recent works (Zhu et al., 2017; Zhu et al., 2017), we randomly partition the samples in each source domain into training set (80%), validation set (10%) and test set (10%), respectively. The source model is first trained using the training set and its hyperparameters are fine-tuned on the validation set. Then, we conduct sanity check on the test set to ensure that it is well-trained on the labeled source domain. The final performance is evaluated on the entire target domain. For baselines, we employ the source codes released by the authors and utilize the same graph neural network backbone with identical layers. The node representation dimension is set as 128 for node classification and 64 for graph classification tasks. We implement our proposed GraphATA with Pytorch Geometric (He et al., 2016) and the parameters are optimized with Adam (Kingma and Ba, 2015). The optimal learning rate and weight decay are searched in the set of \(\{0.1,0.01,0.001,1e^{-4}\}\). We keep the smoothing parameter \(\) for memory banks as 0.9 by default and search the trader-off hyperparameter \(\) within the range of \(\). More implementation details are given at Appendix C.

### Results and Analyses

We show the results of node classification and graph classification in Table 2. Additional experiments on large-scale datasets are presented in Table 8 and Table 9 in Appendix E. All the experiments are repeated 5 times and we report the mean accuracy with standard deviation. In summary, we have the following key observations.

First, our proposed GraphATA surpasses all the baselines across various adaptation tasks with different margins. For instance, we achieve 12.20% average relative gains in the scenario of A,D\(\)C compared with the naive no-adaptation method GCN. This implies that simply taking an average of the source model predictions cannot obtain satisfied performance, which is because different domains might contribute differently to the target domain. Therefore, it is important to aggregate information from multiple source domains with suitable weights. We also notice that GAT exhibits relatively poorer performance within this group. The reason can be attributed to the distribution shifts between source and target domains, as the optimal attention weights in source domains become less suitable for the target domain.

Second, when further compared with source-need methods that utilize labeled source data during the training process, our model can still outperform them by significant margins. Meanwhile, single-source-needed baselines, which consolidate all the samples into one large source domain, occasionally beat approaches specifically designed for multi-source domain adaptation like MDAN and M\({}^{3}\)SDA in several settings. It justifies the necessity of aggregating rich information from multiple source domains, since they may contain complementary information for the target domain.

Third, all the multi-source extensions of single-source-free models demonstrate superior performance compared with non-adapted graph neural networks, even though they utilize the same ensemble strategy. These results suggest that domain adaptation is a promising way to mitigate the distribution shifts across different domains. However, there does not exist a clear winner that consistently outperforms the others within this category, because taking average of the predictions might be sub-optimal in some scenarios.

Finally, our GraphATA consistently exceeds the strongest baselines tailored for multi-source-free domain adaptation, such as DECISION, CAiDA and MSFDA. This stems from the fact that our model conducts fine-grained adaptation by comprehensively capturing each node's local context information, while existing model-centric approaches only aggregate information at the prediction level and overlook the fine-grained information. In challenging datasets with significant domain shifts, such as the Frankenstein and Protein datasets, traditional multi-source-free models struggle due to the likelihood of negative transfer. In contrast, our proposed GraphATA could filter out irrelevant models and reduce the impact of negative transfer, leading to more precise adaptation.

### Ablation Studies

**The Effect of Different Modules.** To fully investigate the contribution of each component in our proposed GraphATA model,we conduct a series of ablation studies on citation datasets. We first show the rationality and effectiveness of utilizing sparse attention to selectively aggregate convolutional matrices from multiple source pre-trained models. When replacing the sparsemax function with softmax function in Eq. (6) (termed as GraphATA\({}_{}\)), its performance degrades 4.46% and 2.04% in Table 3, respectively. This indicates that not all of the source models are useful, and filtering out irrelevant information would be beneficial. We further degenerate Eq. (4) to model-centric method (restricting the information aggregation to the last layer) and layer-centric method (setting \(_{0}=1,_{g}=0\)), which are denoted as GraphATA\({}_{}\) and GraphATA\({}_{}\). When compared with GraphATA, their performance decreases about 2.93% \(\) 4.26%. This justifies the advantages of conducting fine-grained node-centric adaptation. More ablation studies can be found at Appendix E.

**Hyperparameter Analysis and Attention Visualization.** We also show the impacts of several key hyper-parameters in Figure 4(a) and Figure 4(b). Particularly, when setting number of layers \(L=2\) and \(=0.2\), our model could always obtain the satisfied performance. To demonstrate the uniqueness of each node's graph convolutional matrices, we randomly sample a graph with 11 nodes and 11

    &  \\    &  &  &  \\   & C1,C2,C3 {

[MISSING_PAGE_FAIL:8]