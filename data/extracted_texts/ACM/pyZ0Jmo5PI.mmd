# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

distillation while introducing a knowledge contribution-aware aggregation rule via Shapley value to discern and aggregate high-quality model updates. To fulfill these objectives, FedMobile is guided by two primary goals: 1) Effectively reconstructing features of missing modalities without exacerbating modality heterogeneity or compromising main task performance. 2) Streamlining the process of identifying mobile nodes with substantial contributions while minimizing computational overhead. Next, we focus on responding to the following two challenges:

\(\)_(C1.) How to collaboratively interpolate missing sensing modal features for different nodes with cross-modal heterogeneity_

**(S1.)** - _The heterogeneous modality between different nodes has a common feature subspace._ In mobile sensing scenarios, malfunctioning sensor modalities at various nodes give rise to missing modalities, causing modality heterogeneity (Wang et al., 2017). In such circumstances, conventional solutions like zero-filling (Zhu et al., 2017) and parallel training of unimodal models (Zhu et al., 2017) often inadequately handle this inherent feature and modality diversity. Our goal, therefore, is to leverage knowledge distillation for constructing a shared feature subspace among node modalities to improve model performance. We implement a feature generator on both the server and node levels to tackle missing modality issues. This generator, trained in a coupled training manner, aims to align different modalities by capturing a common feature space.

\(\)_(C2.) How to find relevant metrics for measuring the contribution of a specific node in a computationally cost-friendly manner._

**(S2.)** - _Knowledge and model updates shared between nodes and server generalize the contributions of nodes._ In FedMobile, the contributor role of participating nodes is manifested across dual dimensions: knowledge shared by local generators and local model updates shared by local nodes. To incentivize local generators to yield high quality features for incomplete modalities, we devise a novel Clustered Shapley Value approach that quantifies the individual contributions of these generators. This subsequently allows for adaptive modulation of their respective weights, thus facilitating the aggregation of high-quality feature representations. Moreover, with the objective of discerning nodes that high-quality model updates, we introduce a contribution-aware aggregation mechanism designed to retain those elements that are conducive to the overall improvement of the global model. Conversely, it eliminates nodes that do not meet this criterion. By dynamically choosing nodes based on this principle, we effectively ensure the aggregation of high-quality model updates during the training.

Additionally, we evaluate FedMobile across five real-world multimodal sensing datasets: USC-HAD (Wang et al., 2017), MHAD (Zhu et al., 2017), Alzheimer's Disease Monitoring (ADM) (Zhu et al., 2017), C-MHAD (Wang et al., 2017), and FLASH (Zhu et al., 2017), which encompass tasks related to autonomous driving, mobile healthcare, and Alzheimer's disease detection. The results indicate that FedMobile effectively leverages various sensor types (such as GPS, gyroscopes, and radar) in scenarios with incomplete sensing modalities to accurately perform assigned tasks, even amid operational dynamics like sensor failures. Furthermore, we analyze the computational and communication overhead of FedMobile across different tasks. Extensive evaluations show that FedMobile outperforms existing multimodal FL systems (_e.g._, FedMM (Wang et al., 2017), AutoFed (Wang et al., 2017), and PmcmFL (Chen et al., 2018)), achieving higher model accuracy with reasonable additional computational and communication overheads, especially under dynamic modality missing conditions.

The contributions of our work can be summarized as follows:

_(1)_ We tailor FedMobile, a multimodal federated learning framework that is robust to incomplete modal data, for web-based mobile sensing systems in WoT.

_(2)_ We design a knowledge distillation-driven cross-node modality reconstruction network to efficiently reconstruct the missing modality data without introducing excessive overhead.

_(3)_ We design an efficient generator contribution evaluation module based on clustered Shapley value and contribution-aware aggregation mechanism to further improve system performance.

_(4)_ We implement our design and conduct extensive experiments on 5 datasets related to 3 mobile sensing downstream tasks to explore the performance, efficiency, generality, and parameter sensitivity of FedMobile. Compared to the baselines, our approach achieves state-of-the-art performance on all tasks while maintaining comparable computation and communication overhead.

## 2. Related Work

**Multimodal Learning for Mobile Sensing Systems.**Multimodal learning aims to extract complementary or independent knowledge from various modalities, enabling the representation of multimodal data (Zhu et al., 2017; Wang et al., 2017). This empowers machine learning models to comprehend and process diverse modal information (Zhu et al., 2017). As a result, multimodal learning techniques have become prevalent in mobile sensing, facilitating the development of systems that can understand and process diverse sensor data. For instance, multimodal learning can enhance model performance in areas such as traffic trajectory prediction (Zhu et al., 2017), disease diagnosis (Zhu et al., 2017), human activity recognition (Bengio et al., 2018), audio-visual speech recognition (Zhu et al., 2017), and visual question answering (Zhu et al., 2017). However, solving the problem of missing modalities in such systems remains an open challenge.

**Unimodal and Multimodal FL systems.** To address privacy concerns in mobile sensing systems, privacy-preserving distributed learning systems, notably FL (Zhu et al., 2017; Wang et al., 2017; Wang et al., 2017), are emerging as a solution. FL systems can be categorized into unimodal and multimodal FL based on the number of data modalities involved. Unimodal FL focuses on constructing a global model from unimodal data while preserving privacy (Zhu et al., 2017). Similarly, multimodal FL integrates data from multiple modalities to develop an effective global model (Wang et al., 2017). Multimodal FL systems are increasingly used in mobile sensing applications, particularly in tasks such as autonomous driving (Wang et al., 2017) and Alzheimer's disease detection (Zhu et al., 2017), due to their robust multimodal data processing capabilities.

**Multimodal FL Systems with Missing Modality.** Multimodal FL systems have emerged as a promising approach for training ML models across multiple modalities while preserving data privacy.

Figure 1. Unimodal FL vs multimodal FL.

However, in real-world scenarios, certain modalities may be missing from some nodes due to hardware limitations, data availability constraints, or privacy concerns (Han et al., 2017; Wang et al., 2018; Wang et al., 2019). To address this challenge, researchers have developed multimodal FL systems using various approaches, including modality filling (Wang et al., 2018), parallel training of unimodal models (Wang et al., 2018), and cross-model (Wang et al., 2019) techniques. For example, Xiong et al. (Xiong et al., 2019) introduced a modality-filling technique using reconstruction networks, while Ouyang et al. (Ouyang et al., 2019) proposed Harmony, a heterogeneous multimodal FL system based on disentangled model training. However, these methods often overlook the common feature space and the evaluation of node marginal contributions, leading to issues with model accuracy. This paper aims to address these challenges by developing a knowledge contribution-aware multimodal FL system for mobile sensing.

## 3. Preliminary

### Multimodal Federated Learning

Multimodal FL is a cutting-edge approach in machine learning (ML) that addresses the challenges of training models across multiple modalities while preserving data privacy. Formally, in mobile sensing scenarios, let us denote \(=\{m_{0},m_{1},,m_{M-1}\}\) as the set of modalities of the local multimodal dataset \(D_{k}\), \(K\) as the number of participating mobile nodes, \(n_{k}\) as the number of samples in the node \(k\), and \(d_{k}^{m}\) as the dimensionality of modality \(m\) in the node \(k\). The objective of Multimodal FL is to optimize a global model \(()\) parameterized by \(\) across all modalities while minimizing the following federated loss function:

\[_{}_{k=1}^{K}_{m}}{n}( ;_{k}^{m},_{k}), \]

where \((;_{k}^{m},_{k})\) is the loss function for modality \(m\) at node \(k\), \(_{k}^{m}\) represents the data samples for modality \(m\) at node \(k\), \(_{k}\) is the target label associated with the samples at node \(k\), and \(n=_{k=1}^{K}n_{k}\) represents the total number of samples across all nodes. In multimodal FL, the global model \(()\) is updated by aggregating local model updates from each node while respecting data privacy constraints. The update rule for the global model at iteration \(t\) can be formalized as:

\[^{t+1}=^{t}-_{k=1}^{K}}{n}( ^{t};_{k},_{k}), \]

where \(\) is the learning rate and \((^{t};_{k},_{k})\) is the gradient of the loss function with respect to the global model parameters \(^{t}\) at node \(k\). Clearly, when a modal sensor on a mobile node fails or ceases to function, resulting in a missing modality, the optimization of Eqs. (1) and (2) becomes challenging. This impediment implies that multimodal FL may struggle to fulfill the designated task effectively under such circumstances.

### Shapley Value in ML

Shapley Value (Shapley, 1952) is a concept from cooperative game theory used to fairly distribute the value generated by a coalition of players. In the context of ML, it is often applied to understand the contribution of each feature to a model's prediction (Bahdanau et al., 2015). Let us denote a predictive model as \(f\), and \(_{i}(f)\) represents the Shapley value of feature \(i\) in the model \(f\). The Shapley value of feature \(i\) can be computed as:

\[_{i}(f)=_{S N\{i\}}[ f(x_{S}\{i\})-f(x_{S})], \]

where \(N\) is the set of all features, \(x_{S}\) represents the instance with only features in set \(S\), \(f(x_{S}\{i\})\) is the prediction of the model when feature \(i\) is added to the set \(S\), \(f(x_{S})\) is the prediction of the model when only features in set \(S\) are considered, \(|S|\) denotes the cardinality of set \(S\), and \(|N|\) is the total number of features. The above formula computes the marginal contribution of feature \(i\) when added to different subsets \(S\) of features, weighted by the number of permutations of features in \(S\) to the total number of permutations of all features. In fact, calculating the Shapley value directly using the above formula might be computationally expensive (Bahdanau et al., 2015; Shapley, 1952), especially for models with a large number of features.

## 4. Our Approach

Overview.In this section, we present the proposed FedMobile framework, as illustrated in Fig. 2. First, to reconstruct missing sensing modalities, we design a knowledge distillation-driven cross-node modality reconstruction network. Secondly, to select generators with high-quality contributions, we design an efficient model contribution evaluation module based on clustered Shapley values. Finally, to further mitigate cross-node modal heterogeneity, we introduce a knowledge contribution-aware aggregation rule for robust aggregation. Next, we will introduce each functional component in detail.

### Knowledge Distillation-driven Cross-node Modalitiy Reconstruction Network

Impute Missing Modalities.Different from existing works such as AutoFed (Wang et al., 2019), which focus on reconstructing local missing modalities while ignoring cross-node feature information, FedMobile aims to collaboratively utilize the common feature subspace across nodes to iteratively reconstruct the feature information of missing modalities. Specifically, to gain insight into the data distribution of missing modalities across nodes and use this understanding to guide local model training with incomplete modalities, we employ conditional distributions to characterize the modal data distribution of each node. Let \(Q_{k}:_{k}_{k}\) denote the above conditional distribution, which is tailored for each node and aligns with the ground truth data distribution. This distribution encapsulates the necessary knowledge to guide multimodal FL training with incomplete modalities:

\[Q_{k}=*{arg\,max}_{Q_{k}:_{k}_{k}} _{y p(y_{k})}_{x Q_{k}(_{k}|y_{k})}[  p(y|x;_{k})], \]

where \(p(y_{k})\) and \(p(y x)\) denote the ground-truth prior and posterior distributions of the target labels, respectively. Given these conditions, we employ local models to infer \(p(y x)\). Consequently, a straightforward approach involves the direct optimization of Eq. (4) in the input space \(_{k}\) to approximate features for missing modalities. However, when \(_{k}\) is of high dimensionality, this approach may lead to computational overload and could potentially disclose information about user data configuration files. Therefore,a more feasible alternative is to reconstruct an induced distribution \(_{k}:_{k}_{k}\) over a latent space. This latent space, being more compact than the raw data space, can help mitigate certain privacy-related concerns:

\[_{k}=:_{k}_{k}}{ }\ _{y p(y_{k})}_{z_{k}(_{k}| y_{k})}[ p(y|z;_{k})]. \]

Hence, nodes engage in knowledge extraction from missing modality data by acquiring insights from a parameterized condition generator \(_{k}\) by \(_{G}^{k}\). The optimization process is as follows:

\[_{_{G}^{k}}J(_{G}^{k})=_{y p(y_{k})}_{z_{k}(z|y_{s})}[((g(z; _{k})),y)], \]

where \(y_{r}\) represents a set of random labels generated from the training dataset \(_{k}\), \(g()\) denotes the logits output of a predictor, and \(()\) signifies the non-linear activation applied to these logits.

Align Missing Modalities.On the other hand, it is necessary to refine features subspaces to more accurately encapsulate the local knowledge of nodes. For instance, considering a two-modality task, we can derive the generated latent space via the labels \(y_{k}\): \(^{m_{1}},^{m_{1}}=_{k}(y_{k};_{G}^{k})\), where \(^{m_{1}}\) and \(^{m_{1}}\) represent the respective latent features of each modality. Assuming \(m_{1}\) denotes the missing modality, our objective is to further empower \(_{k}\) to assimilate knowledge from various modalities, thereby enhancing the completeness and generalization of the feature space. For modality \(m_{0}\), the learning process can be expressed as follows:

\[_{KL}^{m_{1}}(_{G}^{k};_{k})=_{_{G}^{k}} _{l=1}^{B}_{x_{k}}[D_{KL}[(_{l} (x_{l}^{m_{1}};_{k}^{m_{1}})\|_{i}^{m_{0}} )\|]], \]

where \(B\) represents the number of samples in the local training batch. For modality \(m_{1}\), we only learn from the missing data of this modality, which is formally expressed as follows:

\[_{KL}^{m_{1}}(_{G}^{k};_{k})=_{_{G}^{k}} _{l=1}^{I}_{x_{k}}[D_{KL}[(f_{1}( x_{l}^{m_{1}};_{k}^{m_{1}})\|_{i}^{m_{1}}) \|]], \]

where \(I\) represents the remaining number of samples. Finally, we use \(^{m_{1}}\) instead of the feature \(f_{1}(x_{k}^{m_{1}};_{k}^{m_{1}})\) of modality \(m_{1}\) for multimodal feature fusion (_e.g._, concatenated fusion) to achieve feature alignment for missing modality. According to the above method, the overall optimization goal of every FL node is:

\[_{_{G}^{k};_{k}}_{Train}^{k}=J(_{G}^{k})+ _{KL}^{m_{1}}(_{G}^{k};_{k})+_{KL}^{m_{1}}( _{G}^{k};_{k})+_{CE}(_{k}), \]

where \(_{CE}(_{k})\) represents the cross entropy loss of model training.

Transfer Feature Space.In this context, we consider the global distribution generator, denoted as \(_{k}\), and the set of local distribution generators, represented by \(_{k}\) for each node \(k\), as the source and target domains, respectively, in a framework of domain adaptation. This particular form of adaptation is referred to as global-to-local knowledge transfer. Conversely, the local-to-global knowledge transfer takes place at the server side. During the knowledge exchange process, the node \(k\) transmits its locally generated distribution model, \(_{k}\), to the server. The server then orchestrates a guided adjustment of \(_{k}\) with the aim of systematic reduction in the discrepancy between the local and global knowledge domains through the mechanism of FL aggregation. The above process can be formalized as follows:

\[=_{k=1}^{K}_{k}. \]

### Clustering Shape Value-driven Generator Contribution Evaluation Module

Evaluate Generator Contribution.Considering the inherent heterogeneity of data across nodes and the varied modality missing scenarios that often arise on individual nodes, a naive aggregation of the local distribution models \(_{k}\) for knowledge transfer might inadvertently cause a general shift in the collective knowledge domain, leading to counterproductive outcomes. To mitigate this issue, we use the SV method to quantitatively evaluate the marginal contribution of each distinct \(_{k}\) to the overarching learning task. However, directly applying the SV to compute the marginal contributions of individual nodes is computationally burdensome, especially in FL scenarios involving hundreds of mobile devices. To address this challenge, we incorporate the K-means clustering algorithm to reduce the computational complexity of the SV computation. Specifically, we employ the K-means clustering algorithm to cluster the \(\) generated by \(_{k}\), resulting in multiple clusters containing \(_{k}\). We then perform average aggregation on the generator parameters in the cluster to obtain \(\) as the node representative

Figure 2. Workflow overview of FedMobile.

of the cluster. In this way, we can get \(\) node representatives and use these node representatives as a set \(N=\{_{1},,_{}\}\) in SV. Consequently, the final computation of SV can be expressed as:

\[_{i}()-_{S _{i}\{_{i}\}}(|N|-|S|-|1)}{| |}[p_{i}P_{i}(S_{i}^{})\{(_{i} \{_{i}^{}\})\}-p_{i}(_{i}(S_{ i}^{}))], \]

where \(\) is a performance metric function, such as accuracy, F1-score, or loss, \(\) represents the global model, and \(y_{i}^{}\) is a set of randomly generated labels from the proxy dataset \(_{proxy}\). Subsequently, the normalized \(_{1}(),_{2}(),,_{}()\) is used as the aggregation weight, therefore Eq. (10) can be rewritten as:

\[=}_{t=1}^{ }_{i}()_{i}. \]

### Contribution-aware Aggregation Rule

**Node Contribution.**  To generalize node contribution in a fine-grained manner, we divide the contribution of each node in each round into local and global contributions. The local contribution represents the node's performance in that round of local training, while the global contribution represents the node's impact on model aggregation. We can calculate the local contributions as follows:

\[ p_{,k}^{t}=(_{k}^{t}; _{proxy}), \]

where \(\) is a performance metric function, such as accuracy, F1-score, or loss, \(_{proxy}\) represents the proxy dataset, and \(_{k}\) represents the local model parameters of node \(k\). It is important to note that the proxy dataset does not compromise the privacy of the training set and can be collected by the server, as is consistent with previous work (Zhu et al., 2017; Wang et al., 2018). Furthermore, we need to traverse all nodes to calculate the above contribution. To assess how much a node's update would improve the global model, we can perform a hypothetical update by applying only node \(k\)'s update to the global model and measuring the global contribution:

\[_{,k}^{t+1}=^{t}+ _{k}^{t}, \]

\[ p_{,k}^{t}=(_{ ,k}^{t+1};_{proxy})-(^{t};_{ proxy})\\ =p_{,k}^{t+1}-p_{}^{t} \]

where \(\) is the learning rate. Due to computational constraints (since evaluating each node's update individually can be costly), we can approximate this by estimating the potential improvement based on surrogate metrics. Hence, we can approximate it using the node's local loss reduction metric as follows:

\[ E_{k}^{t}=(^{t}; _{proxy})-(_{k}^{t};_{proxy}). \]

We use \( E_{k}^{t}\) as a proxy for \( p_{,k}^{t}\). The reason we do this is that larger differences have a larger impact on the global model.

**Node Contribution-aware Aggregation.**  Upon determining the global and local contributions, we strive to incorporate them adaptively into the quality assessment process of nodes participating in model aggregation, thereby mitigating the impact of updating nodes with lower quality. To achieve this goal, we can define the aggregation weight \(_{k}^{t}\) for a node \(k\) as a function of \(p_{,k}^{t}\) and \( p_{,k}^{t}\):

\[ a_{k}^{t}=,k}^{t}; p _{,k}^{t})}{_{j=1}^{K}p(p_{,j}^{t}\, p_{ ,j}^{t})}, \]

where \(p\) is a function that combines the two performance metrics. Here, an intuitive choice for \(p\) is to multiply the normalized performance metrics. Thus, we normalize the Local Contribution Metrics:

\[_{,k}^{t}=,k}^{t}}{_{j=1}^{K}p_{,j}^{t}},\]

Improvements: \(p_{,k}^{t}=,k}^{t}}{ _{j=1}^{K} p_{,j}^{t}}\). Combining Eqs. (16) and (17), if we use local loss reduction, we have:

\[ a_{k}^{t}=_{,k}^{t}_{k}^{t}}{_{j=1}^{K}n_{j}_{j}^{t}} \]

where \(_{k=1}^{K}_{k}^{t}=1\). Therefore, we can use this weight to update the global model:

\[^{t+1}=^{t}+_{k=1}^{K}a_{k}^{t} _{k}^{t}. \]

The above process is summarized in Algo. 1 in the Appendix B.

## 5. Experiment Setup

To evaluate the performance of our FedMobile system, we conduct extensive experiments on four benchmarking datasets. All experiments are developed using Python 3.9 and PyTorch 1.12 and evaluated on a server with an NVIDIA A100 GPU.

**Datasets.**  We adopt five multimodal datasets for evaluations, _i.e._, USC-HAD (Wang et al., 2018), MHAD (Wang et al., 2018), ADM (Wang et al., 2018), C-MHAD (Wang et al., 2018), and FLASH (Wang et al., 2018) datasets. The datasets cover different modalities, objects, domains, attributes, dimensions, and number of classes, as shown in Table 7, allowing us to explore the learning effectiveness of FedMobile. To simulate an environment characterized by incomplete sensing modalities, we adopt a random selection methodology to identify a target mode from the local dataset, which will represent the state of incompleteness. We then proceed to randomly eliminate a predetermined proportion of the modal data, thereby simulating the phenomenon of missing information. Note that we distinguish between the small-scale node and large-scale node scenarios according to the scale of users (_i.e._, nodes) involved in the dataset. In the dataset used, FLASH will be evaluated in the large-scale node scenario. More details can be found in Appendix C.1.

**Models.**  When processing ADM dataset, we harness the TDNN for audio feature extraction and combine it with CNN layers for radar and depth image feature extraction. For USC, MHAD, and FLASH datasets, a 2D-CNN model is utilized to process accelerometer data, whereas a 3D-CNN architecture is employed to analyze skeleton data. Finally, when working with the CMHAD dataset, we exploit a 2D-CNN architecture to derive video features, while 3D-CNN layers are used for extracting features from inertial sensors.

**Parameters.**  For the ADM dataset, we set the learning rate at 1e-3, with a batch size of 64. Regarding the USC dataset, the learning rate is 1e-6, and the batch size is 16. For the MHAD and FLASHdatasets, the learning rate is 1e-3, with a batch size of 16. When working with the CMHAD datasets, we maintain a learning rate of 1e-4, alongside a batch size of 16. Throughout this experiment, we utilize the SGD optimizer with a momentum of 0.9 and a weight decay of 1e-4. We set the total number of nodes \(K=10\), local epoch \(E=5\), global epoch \(T=100\), and node participation rate \(q=100\%\). We set \(=5\) in the K-means algorithm. We use a multilayer perceptron as our generator (see Appendix C.3).

**Baselines.** To make a fair comparison, we employ FedProx (Krizhevsky, 2009), FedBN (LeCun et al., 2011), FedMM (LeCun et al., 2011), PromFL (Chen et al., 2011), Harmony (Zhou et al., 2014), and AutoFed (Zhou et al., 2014) as baseline methods. Among these, the first three techniques require adaptation to cope with scenarios characterized by incomplete modalities. This adaptation is achieved through the integration of interpolation techniques, namely zero padding (ZP) and random padding (RP), which are incorporated into the FedProx, FedBN, and FedMM baselines. This augmentation enables us to gauge the effectiveness of FedMobile in dealing with heterogeneous modalities. On the other hand, PmcmFL, Harmony, and AutoFed are multimodal FL solutions that naturally cater to situations involving incomplete modalities without needing further modifications to their methodologies. Thus, we can directly assess FedMobile's learning performance in similar contexts using these baseline methods. Note that all comparison results are the average of five repeated experiments to eliminate the effect of randomness.

**Metrics.** To assess the performance of our proposed method and benchmark it against the baseline approaches, we employed accuracy as the evaluation metric, a convention that has been widely utilized in prior research (Brockman et al., 2016). To quantify the computational overhead, we tracked the aggregate time consumed in uploading and downloading models for all participating nodes throughout the FL training process, as was previously done in (Zhou et al., 2014). And we computed the cumulative GPU usage across all nodes engaged in the FL training phase. For communication overhead, we perform a fair comparison by calculating the model updates that need to be transmitted for 100 rounds of global training.

### Numerical Results

**Research Questions.** In this section, we aim to answer the following research questions:

\(\) (RQ1) How effectively does FedMobile, along with its respective baseline methods, fare in handling diverse and complex scenarios characterized by incomplete modal environments?

\(\) (RQ2) How does FedMobile demonstrate computational and communicational efficiency in its running processes?

\(\) (RQ3) How does FedMobile perform in heterogeneous data scenarios, especially in dynamic modality missing scenarios?

\(\) (RQ4) How does FedMobile perform in scenarios with large-scale nodes and missing modalities?

\(\) (RQ5) What are the capabilities of FedMobile in terms of multimodal feature extraction, and how proficiently can it harness and integrate features from multiple modalities?

\(\) (RQ6) How do the individual components of the FedMobile framework contribute to its overall performance, and what specific impact do they have on its effectiveness?

**System Performance (RQ1).** To address RQ1, we perform an extensive evaluation of FedMobile, along with its comparative baseline algorithms, using four benchmark multimodal datasets. To assess performance under diverse levels of modal data loss, we introduce a set of modality missing rates designated as \(=\{20\%,40\%,60\%,70\%,80\%,90\%\}\). The experimental results demonstrate that FedMobile outperforms all other baseline algorithms consistently across all these varying degrees of missing modality data, as clearly depicted in Table 1. Notably, FedMobile showcases a 1.9% improvement relative to the current state-of-the-art baseline, AutoFed, specifically in the MiMAD dataset with \(=80\%\). These enhanced results stem from FedMobile's innovative strategy, which entails reconstructing modal features across nodes and tactically selecting nodes with high-quality contributions. By discovering a shared feature subspace among distinct missing modalities, FedMobile efficiently reconstructs features and simultaneously excludes nodes with inferior-quality data, thereby boosting the performance.

To further evaluate the performance of FedMobile, we tested it under a more challenging scenario involving the absence of two modalities (_i.e._, two-modal data missing). Specifically, we randomly omitted two modalities in a fixed ratio within the ADM dataset, which consists of three modal data, and maintained this missing configuration throughout the training process. The numerical results, recorded in Table 2, demonstrate that FedMobile continues to deliver excellent performance, outperforming other state-of-the-art baselines, including an average 4.3% improvement over AutoFed on the ADM dataset. Additionally, we observed that existing methods struggle with missing data across multiple modalities, as they heavily depend on sufficient modal information to reconstruct the missing data. FedMobile, on the other hand, does not require this, making it more robust in handling such scenarios. Additionally, we provide a privacy analysis in Appendix A.

**Computational & Communication Overhead (RQ2).** To address RQ2, we systematically document and analyze the communication cost, local running time, and GPU usage of all examined methods on the USC dataset with \(=60\%\). Note that since AutoFed and Harmony also include hardware equipment, they are not included in the comparison. For the convenience of comparison, we record the communication overhead of 100 global training rounds and ignore factors such as the network environment. First, while the introduction of the generator does cause additional communication overhead, this overhead is acceptable. Specifically, the additional overhead caused by the generator is 1.65 MB for each training round. Furthermore, compared to baselines such as FedProx, which do not introduce much additional overhead, our method only adds an additional 9.02% communication overhead, as shown in Fig. 3. In performance-critical multimodal services, a small amount of additional communication overhead is acceptable because it improves the quality of service (_i.e._, accuracy), which is a performance-overhead trade-off. The results depicted in Figs. 5-6 and Table 3 show that the GPU utilization and local running time of FedMobile consistently remains lower than or close to that of the comparative baseline methods. This indicates that our approach does not appreciably increase local computational overhead. Given that servers typically operate as resource-rich cloud infrastructure, computations related to the server-side SV calculation do not impose any significant extra computational load.

**Data Heterogeneity Scenarios (RQ3).** To address RQ3, we eval

[MISSING_PAGE_FAIL:7]

Specifically, we set modality missing rates at 40%, 60%, and 80%, and conduct five repeated experiments to record the average model accuracy. The results, presented in Table 5, show that FedMobile consistently outperforms the other advanced baselines even on large-scale nodes, demonstrating that its performance is not limited by the scale of the mobile node.

**Feature Visualization (RQ5).** In response to RQ5, we undertake a qualitative evaluation of the multimodal features generated by the competing methods by visualizing them. For this purpose, we employ the t-distributed Stochastic Neighbor Embedding (t-SNE) technique on dataset MHAD to project the high-dimensional multimodal features extracted by each method onto a lower-dimensional space. The resulting dimensionality reduction is presented in Fig. 4. Our visualization results indicate that FedMobile excels at extracting more precise and refined multimodal features, which in turn leads to enhanced classification accuracy. In comparison, alternative methods exhibit substantial deficiencies in feature extraction. This observation underscores the value of FedMobile's dual strategies of local-to-global knowledge transfer and modal feature reconstruction, which collectively facilitate the effective exploitation and extraction of information from incomplete modal sources.

**Ablation Studies (RQ6).** To investigate RQ6, we conduct a systematic dissection of FedMobile by analyzing the performance contributions of its constituent parts with \(=40\%\). To this end, we experimentally validate the performance of three ablated versions of FedMobile on four benchmark multi-modal datasets. Specifically, we successively deactivate the modal reconstruction network, the contribution sensing module, and the dynamic parameter aggregation module, forming three distinct variations of FedMobile. The experimental outcomes are summarized in Table 6, demonstrating that the modal reconstruction network and the contribution sensing module play pivotal roles in determining FedMobile's performance. On the other hand, the impact of the dynamic parameter aggregation module on FedMobile's performance appears to be less pronounced. For illustration, when the modal reconstruction network is removed from FedMobile, the performance degradation on the MHAD dataset reaches 3.2%, relative to the complete version of FedMobile. These findings highlight the critical importance of the modal reconstruction and contribution sensing mechanisms within the FedMobile framework.

## 6. Conclusion

The paper addresses the challenge of incomplete modalities in multimodal Federated Learning systems by proposing a new framework called FedMobile. Unlike existing methods that rely on unimodal subsystems or interpolation, FedMobile leverages cross-node multimodal feature information for reconstructing missing data and employs a knowledge contribution-aware mechanism to evaluate and prioritize node inputs, improving resilience to modality heterogeneity. The framework demonstrates superior performance in maintaining robust learning under significant modality loss compared to current standards, all while not increasing computational or communication costs. Overall, FedMobile represents a significant step forward in developing more efficient and resilient multimodal FL systems.

  
**Method** & **MHAD** & **USC** & **ADM** & **CMEAD** & **FLASH** \\  Ours (w/o \(Z_{}\)/tab) & 74.7 & 57.5 & 79.8 & 79.2 & 52.7 \\ Ours (w/o SV) & 77.3 & 61.6 & 81.8 & 82.7 & 56.9 \\ Ours (w/o ) & 74.5 & 58.6 & 84.1 & 77.4 & 53.8 \\ 
**Ours** & **77.9** & **62.8** & **84.3** & **75.8** & **57.6** \\   

Table 6. Numerical results of ablation experiments.

  
**Dataset** & **Method** & **Scenario 1** & **Scenario 2** \\   & FedMM+ZP & 69.3 & 71.4 \\  & FedMM+RP & 69.5 & 72.1 \\  & FedProx+ZP & 70.2 & 73.7 \\  & FedProx+RP & 69.5 & 73.2 \\
**ADM** & FedBN+ZP & 69.8 & 71.3 \\  & FedBN+RP & 70.3 & 71.7 \\  & PmcmFL & 71.5 & 74.5 \\  & Harmony & 70.7 & 76.8 \\  & AutoFed & 70.1 & 77.9 \\  & **Ours** & **76.5** & **80.2** \\   

Table 4. Performance results in the heterogeneity scenario.

Figure 4. Feature visualization results of different methods.

  
**Dataset** & **Method** & **40\%** & **60\%** & **80\%** \\   & FedMM+ZP & 52.7 & 51.1 & 50.4 \\  & FedMM+RP & 53.4 & 52.3 & 51.2 \\  & FedProx+ZP & 49.7 & 50.1 & 49.2 \\  & FedProx+RP & 49.4 & 48.7 & 49.4 \\
**FLASH** & FedBN+ZP & 49.9 & 49.1 & 47.8 \\  & FedBN+ZP & 50.4 & 49.7 & 48.2 \\  & PmcmFL & 52.4 & 51.6 & 50.4 \\  & Harmony & 55.8 & 54.7 & 54.1 \\  & AutoFed & 54.9 & 53.4 & 55.4 \\  & **Ours** & **57.6** & **57.1** & **56.8** \\   

Table 5. Performance results on FLASH Dataset.