# Online Bidding under RoS Constraints

without Knowing the Value

Anonymous Author(s)

###### Abstract.

We consider the problem of bidding in online advertising, where an advertiser aims to maximize value while adhering to budget and Return-on-Spend (RoS) constraints. Unlike prior work that assumes knowledge of the value generated by winning each impression (e.g., conversions), we address the more realistic setting where the advertiser must simultaneously learn the optimal bidding strategy and the value of each impression opportunity. This introduces a challenging exploration-exploitation dilemma: the advertiser must balance exploring different bids to estimate impression values with exploiting current knowledge to bid effectively. To address this, we propose a novel Upper Confidence Bound (UCB)-style algorithm that carefully manages this trade-off. Via a rigorous theoretical analysis, we prove that our algorithm achieves \((|T)})\) regret and constraint violation, where \(T\) is the number of bidding rounds and \(\) is the domain of possible bids. This establishes the first optimal regret and constraint violation bounds for bidding in the online setting with unknown impression values. Moreover, our algorithm is computationally efficient and simple to implement. We validate our theoretical findings through experiments on synthetic data, demonstrating that our algorithm exhibits strong empirical performance compared to existing approaches.

online bidding, Return-on-Spend, constrained bandits, UCB +
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

## 1. Introduction

Online advertising, a multi-billion dollar industry, relies on real-time auctions to connect advertisers with users. These auctions, triggered by user queries or website visits, allow advertisers to bid for advertising slots, such as prominent placements on search engine results pages or in social media feeds. Advertisers aim to maximize their returns, measured in conversions or other relevant metrics, by carefully determining their bids while adhering to budget constraints and desired return-on-spend (RoS) targets. To achieve this, a wide array of bidding strategies have been developed, leveraging techniques from optimization, online learning, and game theory to maximize advertiser utility (Brocker, 1983; Kahneman, 1984; Kahneman, 1984; Kahneman, 1985; Kahneman, 1986; Kahneman, 1987).

Despite the sophistication of these strategies, many rely on the assumption that perfect knowledge of the value an impression generates is available to the advertiser beforehand. In reality, however, advertisers frequently face uncertainty about the true value of an ad impression, especially when dealing with new ad campaigns or evolving user preferences (cf. Section 2 for more details). In this work, we consider the practical scenario in which the value of an impression is _unknown_ a priori and focus on developing bidding strategies that simultaneously learn the value of ad impressions as well as maximize the realized value of the advertiser.

Specifically, we study the problem of bidding for a single advertiser subject to total budget and RoS constraints. The budget constraint limits the total expenditure, while the RoS constraint ensures that the ratio of total value to total spend meets a predefined target, thus effectively capturing performance goals like target cost-per-acquisition (ICPA) and target return-on-ad-spend (tROAS), widely used in real-world advertising campaigns.1

We consider a stochastic setting where search queries and associated auctions arise dynamically. In this setting, the competing bids and the value of winning an auction are assumed to be sampled independently and identically distributed (i.i.d.) from an unknown distribution. In each round, the advertiser submits a bid without knowing the query's value beforehand. Upon bid submission, the auction mechanism determines the winner and price, with the value being revealed only if the advertiser wins. Our goal is to design an online bidding algorithm that maximizes the bidder's value over the entire horizon, while respecting the RoS and budget constraints.

### Our Main Result

We evaluate our algorithm's performance via the notion of _regret_ (Equation (5)), which quantifies the difference between its expected cumulative value and that achieved by an oracle, which possesses complete knowledge of the underlying competing bid and value distributions and employs a fixed strategy optimized for maximum cumulative value. Our main result now follows.

**Theorem** (Informal; see Theorem 3.3).: _We propose an algorithm (Algorithm 3) designed for value maximization in online advertising auctions with return-on-spend (RoS) and budget constraints, without any prior knowledge of the values associated with incoming user queries. In the stochastic setting described earlier, with an online horizon of length \(T\), our algorithm provably achieves \(O(|T)/V})\) regret for the objective of value maximization and \(O(|T)/V})\)violation of the RoS constraint and \(O(||T)})\) violation of the budget constraint. Here, \(\) is the domain of possible discrete bids and \(V\) is the maximum per-round value achieved by the above oracle._

Comparison to prior workTo the best of our knowledge, ours is the first algorithm to achieve near-optimal regret and constraint violation bounds in this setting _without_ knowledge of item values. This significantly extends recent work, which crucially assumes that the values are known to the bidder before bidding (Gardner et al., 2017; Gardner et al., 2018; Gardner et al., 2019). While (Gardner et al., 2019) also addresses the setting with unknown values, their regret and constraint violation bounds incur a dependence of \(O(|})\), which we improve to a _logarithmic_ dependence on \(||\). Additionally, (Gardner et al., 2019) requires assuming the existence of a Slater point (a strictly feasible solution (Gardner et al., 2019)), which limits the generality of their approach. Although (Gardner et al., 2019) removes this assumption, it still incurs a \(O(|})\) dependence in the regret bounds, which is exponentially weaker than the logarithmic dependence achieved by our bound. In contrast to prior works, our work replaces the assumption of a Slater point with a milder condition that \(V\) is bounded away from zero (see Section 1.2 for a detailed discussion). Furthermore, based on the lower bounds established by Achdou et al. (Achdou et al., 2018) for utility maximization (without RoS constraints) under second-price auctions, we believe that a dependence on \(V\) is unavoidable.

Our core strengthsA key strength of our algorithm and analysis is its simplicity. Unlike prior work that predominantly adopts a primal-dual approach -- requiring intricate analysis of dual variables and assumptions like Slater's condition -- we completely eliminate these restrictive (and often impractical) assumptions, making our approach both stronger and more general. Furthermore, by employing the upper confidence bound (UCB) framework, our method becomes easier to analyze (Sections 1.2 and 3) and simpler to implement, requiring very few hyper-parameters (cf. Section 4).

Computational aspectsA key challenge in our algorithm lies in efficiently estimating the arm recommended by the UCB. This estimation requires solving, in each round, a complex non-convex problem (Problem 3.5). We address this challenge by providing a computationally efficient technique that runs in \(O(||^{3})\) time. This technique leverages the key insight that both the allocation and payment functions in standard auctions are monotonically increasing, which holds for both truthful auctions (e.g., second-price) and non-truthful auctions (e.g., first-price, all-pay) (Steiner and Gardner, 2018).

### Key Technical Contributions

The problem of constrained reward-maximization may naturally be cast as one maximizing the "price-adjusted reward" (i.e., the reward minus a penalty on the constraint violation, with the penalty weighted by the dual variable associated with the constraint). This is the approach that has been widely adopted by much of the past work (Gardner et al., 2017; Gardner et al., 2018; Gardner et al., 2019; Gardner et al., 2019). The key idea is that as a constraint approaches violation, the corresponding dual variable grows large, signaling the need to bid conservatively in the next round; conversely, when previous bids create a buffer in the constraint, the dual variable shrinks, encouraging more aggressive bidding in the subsequent round.

However, in this primal-dual approach, one must assume the existence of a Slater point -- an action which _strictly_ satisfies the expected constraints. As dual variable magnitudes are bounded by \(O(1/)\) (cf. (Gardner et al., 2019, Theorem 8.42)), where \(\) is the minimum constraint slack of the Slater point, primal-dual methods can incur \(O(1/)\) regret and constraint violation. In online bidding with RoS and budget constraints, bids with \( 0\) are common (e.g., when competing bids are narrowly concentrated). Consequently, algorithms based on the primal-dual approach will fundamentally incur large regret and constraint violation. We circumvent this shortcoming by introducing a UCB-style algorithm.

In a typical UCB-style algorithm (see, for example, (Gardner et al., 2018, Chapter 7)), the idea is to create confidence sets for the unknown rewards and select the arm with the highest upper confidence bound. Our primary insight is to extend this principle to the constrained setting inherent in online bidding. In particular, we maintain appropriate confidence sets for both the constraints and rewards. Our algorithm then selects the bid that maximizes the reward while satisfying the constraints based on these confidence sets. This approach entirely eliminates the need for a Slater point (and, hence, avoids a regret dependence of \(1/\)) and addresses the problem even when the value is unknown. Finally, as noted earlier, finding this reward-maximizing bid is a highly nonconvex optimization problem. By utilizing the structure inherent to autobidding, we derive a provably efficient solution that is also easy to implement (Lemma 3.2).

### Related Work

Our problem falls under the broader umbrella of bandit optimization under long-term constraints and has witnessed a long line of work by various research communities e.g. Agrawal and Devanur (Agrawal and Devanur, 2018), Badanidiyuru et al. (Balseiro et al., 2018), Balseiro et al. (Castigoni et al., 2018), Castigoni et al. (Gao et al., 2018), Gao et al. (Zimmorien et al., 2018), Immorlica et al. (Mahdavi et al., 2018; Mannor et al., 2018), Mannor et al. (2018), Yu et al. (Yu and Neely, 2018).

Most of these works study the _budget/packing constraint_, e.g., Devanur et al. (Devanur et al., 2018) obtain the optimal \(O()\)-regret under linear objective and constraints, Agrawal and Devanur (Agrawal and Devanur, 2018) generalize it to nonlinear objectives, and Balseiro et al. (Balseiro et al., 2018) generalize it to nonlinear budget constraints. The RoS constraint we study differs fundamentally from the packing constraint studied in these works as well as in (Balseiro et al., 2018; Gardner et al., 2018). There also exist papers that study a variant of our problem with a constraint class more general than ours (e.g., Agrawal and Devanur (Agrawal and Devanur, 2018), Castiglioni et al. (Castiglioni et al., 2018)); however, their guarantees for our problem are not as strong as ours, as we elaborate next.

For example, Castiglioni et al. (Castiglioni et al., 2018) use a primal-dual framework for regret minimization with bandit feedback, which, when adapted to our bidding problem under the RoS constraint, achieves \((T^{3/4})\) regret with \((T^{3/4})\) constraint violation. Another crucial difference from our setting is that we do _not_ know the values of the bids, whereas (Gardner et al., 2019) (when adapted to this problem) does. Their bounds improve to our \(()\) bounds under a'strictly feasible' assumption; however, we require no such assumptions to get these bounds. In follow-up work, again with bandit feedback, Bernasconi et al. (Bernasconi et al., 2018) study "Best of Both Worlds"-type algorithms for constrained regret minimization without the Slater point assumption. This work is closest to ours, but with the regret and constraint violation bounds suffering from an \(O(|})\) dependence (just like (Gardner et al., 2019)), which can be substantial in practical settings; our work, in contrast, achieves an \(O((||))\) dependence. This difference in the bidding setting arises because their observational model does not account for the specifics of allocation and pricing functions. We empirically compare these algorithms against ours over synthetically generated bidding instances (cf. Section 4 for details).

Another example is the work of Agrawal and Devanur , which considers general online optimization with convex constraints. This work uses black-box low-regret methods with a strongly convex regularizer over the dual space. A sub-linear regret bound is attainable only when the dual space is well-bounded (e.g., a scaled simplex) or when the dual variable can be projected onto such a space without incurring too much additional regret. This canonical approach proves difficult for the RoS constraint, which can incur poor problem-specific parameters in generic guarantees. Hence, this technique cannot give sub-linear regret for the RoS constraint.

A recent line of work studies our bidding problem under both budget and RoS constraints; however, in each of these papers, the underlying assumption is that the bidder _knows_ the value before submitting its bid. For example, Feng et al.  provide \((T^{1/2})\) regret and almost-sure constraint satisfaction using a primal-dual algorithm. The work of Lucier et al. , also in this setting, additionally obtains vanishing regret in the adversarial setting and provides aggregate guarantees on the resulting expected liquid welfare when multiple autobidders all deploy their algorithm. Other closely related works include those of Golrezaei et al.  and Celli et al. , the latter also considering multiple different constraints. However, as noted earlier, all of these require knowing the value.

The following works study regret minimization with the bidder _not_ knowing the value. Weed et al.  study this for second-price auctions, and Achdotio et al.  and Feng et al.  study this for general auctions, proving \(O()\) regret bounds, with the former in a stochastic setting and the latter in an adversarial one. However, we note that all these works focus only on the unconstrained setting and maximize the utility, which is defined as the difference between the received value and the paid price.

A closely related line of work studies bandit optimization under long-term constraints [28; 37; 45; 53]. The works of Liu et al.  and Gangrade et al.  study this for linear bandits with long-term linear constraints. Liu et al.  use a primal-dual approach to provide \((d)\) rates (where \(d\) is the problem dimension), but require the existence of a Slater point. Gangrade et al.  avoid the need for Slater points, by maintaining doubly optimistic constraints and reward estimates, and obtain \((d)\) rates. Both these works, when specialized to autobidding, incur linear dependence on \(||\) in the regret. This problem was also studied for kernelized bandits in , but their algorithm requires knowledge of a lower bound on the Slater slack. A different, but related, problem of satisfying constraints in each round is studied in . This work shows that knowledge of a "safe" action is necessary for per-round constraint satisfaction and obtains \(O()\) regret under this assumption.

Finally, the related problem of learning to bid in repeated auctions has been explored in both academia and industry, e.g. Badanidiyuru et al. , Borgs et al. , Feng et al. , Han et al. , Nedelec et al. , Noti and Syrgkanis , Weed et al. . These works abstract the problem of learning to bid as one of contextual bandits, but do not incorporate constraints into them. Beyond these, there has been some work on bidding under budget constraints, e.g., Ai et al. , Balseiro and Gur . However, these papers focus on utility-maximizing agents with at most one constraint.

## 2. Preliminaries

We consider an auction with multiple bidders and study the online bidding problem from the perspective of a single learner (bidder). \(A\) each time step \(t\), nature generates an ad query associated with a value \(v_{t}\) and an auction mechanism \((x_{t},p_{t})\). The auction mechanism is determined by the allocation and payment functions:

* _Allocation function_, \(x_{t}:\), which specifies the probability of winning the auction for a given bid. We define \(x_{t}(\ \ ) x(\ \,B_{t}^{})\), where \(\) is a finite subset of \(_{ 0}\) with \(0\) (i.e., the bidder can submit a bid of zero), and \(B_{t}^{}\) denotes the vector of bids of the other bidders at time step \(t\). Observe that the allocation probability depends not only on the learner's bid but also on the bids of other participants.
* _Payment function_, \(p_{t}:\), which determines the payment required when the auction is won. Similar to the allocation function, we define \(p_{t}(\ \ ) p(\ \,B_{t}^{})\). We assume that the payment is zero when the allocation is zero and is always at most the submitted bid. This ensures that the bidder never pays more than their bid, a standard assumption in auctions.

We use the shorthand \(q_{t}(b) x_{t}(b) p_{t}(b)\) to denote the price paid for a bid \(b\). A key distinction of our model from those in prior works [1; 26] is that we do not assume the auctions to be truthful. Instead, all we require is that the functions \(x_{t}(\ \ )\) and \(p_{t}(\ \ )\) be _monotonic_, a property satisfied by many popular auctions, including first-price, second-price, and all-pay auctions .

Another important point of departure from previous work is that, at each time step \(t\), the value \(v_{t}\) is _unknown_ to the learner before submitting a bid. This model reflects the uncertainty inherent in many online advertising scenarios. The learner decides its bid \(b_{t}\) based on all the information obtained so far. After submitting its bid, the learner observes the outcome from the auction mechanism, i.e., \(x_{t}(\ \ )\) and \(p_{t}(\ \ )\). If the bidder wins, then the auction mechanism also reveals the value \(v_{t}\). For a bid \(b\) with value \(v\), allocation function \(x(\ \ )\), and payment function \(p(\ \ )\), the _realized_ value and _paid_ price are \( x(b)\) and \(\  p(b)\), respectively.

This setting of unknown value is common in several practical online bidding environments. Examples include advertisers who participate infrequently in auctions, new ad campaigns with uncertain performance, or scenarios where the value of an advertisement is influenced by multiple factors, such as clicks, conversions, brand awareness, and customer lifetime value. Even in autobidding systems , where machine learning models predict clicks and conversions to inform bidding algorithms, these predictions often capture only partial information about the true value and can be inaccurate, especially for new or infrequently shown advertisements.

Similar to prior works on online bidding [14; 18; 46], we assume a stochastic setting where the auction environment is governed by an underlying probability distribution. Specifically, for all \(t[T]\), the tuple \(y_{t}(v_{t},x_{t},p_{t})\) is drawn independently and identically (i.i.d.) from an unknown distribution \(\). This implies that the sequence of \(T\) samples, denoted by \(\{_{1},_{2},,_{T}\}\), follows the product distribution \(^{T}\). This induces the expectations \(=[v_{t}]\), \((b)[x_{t}(b)p_{t}(b)]\), and \((b)[x_{t}(b)]\) for any bid \(b\).

We design online bidding algorithms to maximize the learner's total realized value subject to RoS and budget constraints. Formally, this optimization problem is given by

\[:t=1,,T}{}&_{t=1}^{T}q_ {t} x_{t}(b_{t})\\ &_{t=1}^{T}q_{t}(b_{t})_{t=1}^{T}q_ {t} x_{t}(b_{t}),\\ &_{t=1}^{T}q_{t}(b_{t}) T, \]

where RoS \(>0\) is the target ratio of the RoS bidder and \( T\) the total budget, with \(>0\) (assumed a fixed constant) measuring the limit of the average expenditure over \(T\) rounds (ad queries). Throughout the paper we assume without loss of generality2 that \(=1\).

Analysis setup.We use the notions of regret and constraint violation to measure the performance of our algorithm. To define regret, we first define the reward collected by our algorithm (\(\)) for a sequence of requests \(\) over a time horizon \(T\) as

\[(,)_{t=1}^{T}_{t}  x_{t}(b_{t}). \]

To define the benchmark against which we measure the regret of Alg, we consider the following linear program (LP):

\[]}}{}& _{b}w(b)(b)\\ &_{b}w(b)(b)_{b }w(b)(b),\\ &_{b}w(b)(b). \]

and let us denote the value of this LP as \(V\) and its optimizer as \(w_{LP}^{*}\). Here \(_{||}\) is the set of all probability distributions over \(\). We define our benchmark to be:

\[()_{t=1}^{T}_{b}w_{LP}^{ *}(b)(b)=T V. \]

Thus, we are comparing against an algorithm that has knowledge of \(\), \(\), and \(\), and plays a bid sampled from \(w_{LP}^{*}\) for each of the \(T\) rounds. This is a commonly used benchmark in the stochastic setting (Gelman and Boyd, 2004; Boyd, 2004). These definitions lead to the following definition of regret of Alg in this setup:

\[(,^{T})()- _{^{T}}[(,)]. \]

We remark that Reward is defined for some specific input sequence, whereas Regret is defined with respect to a distribution. Additionally, we define budget and RoS constraint violations as \(_{t=1}^{T}q_{t}(b_{t})- T\) and \(_{t=1}^{T}(q_{t}(b_{t})-_{t} x_{t}(b_{t}))\), respectively.

## 3. UCB-RoS

In this section, we solve the online bidding problem formalized in Problem 2.1 by designing a novel UCB-style algorithm (presented in Algorithm 3). Our approach draws inspiration from the UCB technique widely used in the bandit literature (Bradley, 1998).

We rely on the principle of 'optimism in the face of uncertainty'. At each time step, our algorithm maintains confidence sets for the unknown parameters of the problem, namely the allocation function, the pricing function, and the value distribution. It then selects the bid that maximizes the expected reward within these confidence sets. These confidence intervals are carefully designed to satisfy two key properties: (1) they contain the true expected values with high probability, and (2) they shrink as more data is collected, reflecting increasing confidence in the estimates.

As mentioned earlier, finding the reward-maximizing bid within these confidence intervals is challenging. This optimization problem is inherently non-convex and can be computationally intractable in general. However, by exploiting the specific structure of typical auctions, we derive a simple and efficient solution to this problem, as detailed in Lemma 3.2. We now expand upon these ideas.

Recalling our setup, after submitting bid \(b_{t}\), the bidder obtains the allocation \(x_{t}(\ \ )\) and price function \(p_{t}(\ \ )\). Additionally, if the bid is won, then it also obtains its value \(q_{t}\). Let \(N_{t}\) denote the number of times the user wins the bid in the first \(t\) rounds. Then, the algorithm at time step \(t\) updates its sample estimators for the allocation, pricing functions, and value in the following way:

\[_{t}(\ \ )_{s=1}^{t}(\ \ )\ }{t}, _{t}(\ \ )_{s=1}^{t}(\ \ )\ }{t}, _{t}_{s=1}^{N_{t}}}{N_{t}}. \]

We remark that the first two estimators are functions defined on \(\) and taking values in \(\), while the value estimator takes real values built only from the subset of samples in which the algorithm wins the bid. Next, we describe our construction of confidence intervals around these estimators, in which we later show (Lemma 3.1) the true expected quantities lie with high probability.

Constructing confidence sets.For every \(t[T]\), the algorithm constructs confidence sets centered around the sample estimators defined in Equation (3.1). To introduce these constructions, we first let \(\) denote the set of all non-decreasing functions \(f\) on \(\), taking values in \(\). Then, these confidence sets are defined as:

\[ C^{_{t}}&\{ f\ |f(b)-_{t}(b)||T)}{2t}},\,  b\},\\ C^{_{t}}&\{f |f(b)-_{t}(b)||T)}{2t}},  b\},\\ C^{_{t}}&\{ |-_{t}|}}\}.  \]

Interestingly, the confidence set \(C^{_{t}}\) does not require its constituent functions to be of the form \(x p\), rather only that they are clustered around \(\). This generality proves crucial in Lemma 3.2. These confidence sets have been constructed to ensure that for all bids, the expectations of the true allocation functions \((\ \ )\), pricing functions \((\ \ )\), and values \(\) fall, with high probability, within their respective confidence sets. More precisely, we have the following.

**Lemma 3.1**.: _With probability at least \(1-\), for every \(t[T]\) it holds that \((\ \ ) C^{_{t}}\), \((\ \ ) C^{_{t}}\), and \( C^{_{t}}\), where \(C^{_{t}}\), \(C^{_{t}}\), and \(C^{_{t}}\) are as defined in Equation (3.2)._

Proof.As a result of the imposed ranges on \(x_{t}\) and \(p_{t}\), we infer that for each bid \(b\), the allocation function \(x_{t}(\ \ )\) and the pricing function \(q_{t}(\ \ )\) satisfy the bounded difference property, i.e., for any \((x_{t},p_{t})\) and \((x_{t}^{},p_{t}^{})\),

\[|x_{t}(b)-x_{t}^{}(b)| 1 \]

[MISSING_PAGE_FAIL:5]

This then implies that for any _fixed_ choice of \(q\) and \(v\), we have

\[_{b}w(b) v x^{1}(b) _{b}w(b) v x^{0}(b),\, b  \] \[S(x^{1},q,v)  S(x^{0},q,v).\]

Then, combining (3.6) with the definition of \(f\) implies that

\[f(x^{1},q,v) f(x^{0},q,v).\]

We can then infer that, for any fixed choice of \(v\) and \(q\), the maximizer of Problem 3.5 is the function that chooses the upper confidence bound of the current confidence set \(^{_{t}}\), i.e.,

\[x^{*}_{t}(\,\,)=\{1,\,_{t}(\,\,)+(2||T)}\}\]

An analogous argument can be applied to show that \(v^{*}_{t}\) is:

\[v^{*}_{t}=\{1,\,_{t}+}(2T)}\}\]

and that \(q^{*}_{t}\) is given by the lower confidence bound of the set \(^{_{t}}\):

\[q^{*}_{t}(\,\,)=\{0,\,_{t}(\,\,)-(2||T)}\}.\]

The form of \(w^{*}_{t}\) is obtained by plugging back into Problem 3.5 the explicit form of \(x^{*}_{t}\), \(^{*}_{t}\), and \(v^{*}_{t}\) obtained above. 

Since \(w^{*}_{t}\) is a solution to an LP, one can explicitly compute it with at most \(O(||^{3})\) computational effort .

_Regret and constraint violation bound._ Our main result below guarantees an \(()\) regret and constraint violation bound. We call the event when the concentration results in Lemma 3.1 and Lemma B.1 hold as _clean execution_ and note that it occurs with probability at least \(1-\).

**Theorem 3.3**.: _Consider the online bidding problem described in Section 2. Let \(V\) be the value of the LP defined in Equation (2.3). For any time horizon \(T\), Algorithm 3 suffers the following regret bound in expectation:_

\[[(,^{T})]=O( \{|T)}{V}},|T)}{ V^{2}}\}),\]

_where regret is as defined in Equation (2.5). Further, the violation of the RoS and budget constraint is, in expectation, at most \(O(|T)}{V}})\) and \(O(|T)})\), respectively._

Proof.: First, observe that under clean execution, we have

\[|x^{*}_{t}(t)-(b)|(2||T)}.\]

Combining this result with Lemma B.1, and utilizing the fact that \(w^{*}_{s}\) is a probability distribution over bids, gives us

\[|N_{t}-_{s=1}^{t}_{b}w^{*}_{s}(b)x^{*}_{s}(b)| ++_{s=1}^{t}|T)}{s}}. \]

Under clean execution, \((\,\,)^{_{t}}\), \((\,\,)^{_{t}}\), \(^{_{t}}\) for each \(t\) and hence \(((\,\,),(\,\,),,w^{*}_{})\) is a feasible point for Problem 3.5. Combining this observation with the optimality of \((x^{*}_{s},q^{*}_{s},q^{*}_{s},w^{*}_{s})\), we get that

\[V_{b}w^{*}_{s}(b)x^{*}_{s}(b)x^{*}_{s}(b)n^{*}_{s}\]

for each \(s t\). Noting that \(s^{*}_{s} 1\), and summing over \(s\), we have that \(t V_{s=1}^{t}_{b}w^{*}_{s}(b)x^{*}_{s}(b)\). Using this in (3.7), we get:

\[N_{t}--|T)}. \]

Hence,

\[N_{t}, t|T)}{V ^{2}}. \]

Consider the "per-round regret"

\[r_{t}=_{b}w^{*}_{}(b) (b)-_{b}w^{*}_{t}(b) (b).\]

We then have:

\[r_{t} _{b}w^{*}_{t}(b) v^{*}_{t} x^{*}_{ t}(b)-_{b}w^{*}_{t}(b)(b) \] \[=_{b}w^{*}_{t}(b)[v^{*}_{t}(x^ {*}_{t}(b)-(b))+(b)(^{*}_{t}-)]\] \[(2||T)}+ (2||T)}\] \[ O((||T)}),\]

where the first step is by clean execution, Lemma 3.1, and the optimality, for Problem 3.5, of \(v^{*}_{t}\), \(x^{*}_{t}\), and \(w^{*}_{t}\), all of which lie in the confidence intervals given by Lemma 3.1. In the third step we utilize the (3.9) and Lemma 3.1. Next, by definition of \(r_{t}\), we note that \(_{t=1}^{T}r_{t}\) may equivalently be expressed as below:

\[_{t=1}^{T}r_{t}=()-_{t=1}^{T}_{t-1}[ v_{t} x_{t}(b_{t})],\]

where \(_{t}[\,\,]\) is the conditional expectation. Computing the expectation over the randomness in the entire sequence of inputs gives:

\[_{^{T}}[_{t=1}^{T}r_{t}] =()-_{ ^{T}}[(,)]\] \[=(,^{T}).\]

Because we have a bound on \(_{t=1}^{T}r_{t}\) under clean execution, which holds with a probability at least \(1-\), we can bound the regret as:

\[(,^{T}) _{t=1}^{T}O((||T)} )(1-)+2T\] \[ O(|T)}{V}}).\]

This completes the proof of the regret bound. We now proceed to bound the violation of the budget constraint under clean execution. To this end, we consider the following expression:

\[_{b}w^{*}_{t}(b)(b)=_{b}w^{* }_{t}(b)((b)-q^{*}_{t}(b))+_{b}w^{*}_{t}(b)  q^{*}_{t}(b). \]By clean execution and Lemma 3.1, we have

\[_{t=1}^{T}_{b}w_{t}^{*}(b)((b) -q_{t}^{*}(b)) _{t=1}^{T}|T)}{2t}}\] \[ O(|T)}). \]

Next, since \(q_{t}^{*}\) satisfies the per round constraint, we have

\[_{t=1}^{T}_{b}w_{t}^{*}(b) q_{t}^{*}(b) T. \]

Plugging Inequalities (3.12) and (3.13) into Equation (3.11) yields

\[_{t=1}^{T}_{b}w_{t}^{*}(b)(b) O( |T)})+ T. \]

The expression on the left-hand side of Inequality (3.14) may be expressed as \(_{t=1}^{T}_{b}w_{t}^{*}(b)(b)=_{t= 1}^{T}_{t-1}[q_{t}(b_{t})]\). The expected budget violation may then be bounded as follows:

\[_{^{T}}[_{t=1}^{T}q _{t}(b_{t})- T]  O(|T)})(1-)+ \] \[ O(|T)}).\]

This concludes the proof of the bound on the total budget violation. To prove our bound on the RoS constraint violation, we apply a similar analysis, which we state here for completeness. Consider again Equation (3.11). Then, Inequality (3.12) holds again, due to clean execution. Continuing the analysis, we have

\[_{t=1}^{T}_{b}w_{t}^{*}(b)q_{t}^{*}(b)_{t=1}^{T} _{b}w_{t}^{*}(b)x_{t}^{*}(b)v_{t}^{*},\]

because of optimality of \(v_{t}^{*}\), \(w_{t}^{*}\), \(x_{t}^{*}\), and \(q_{t}^{*}\) for Problem 3.5 (from Lemma 3.2). Repeating, on the right-hand side above, the steps from Inequality (3.10), we get the following bound:

\[_{t=1}^{T}_{b}w_{t}^{*}(b) q_{t}^{*}(b)\] \[ O(|T)}{V}})+_{ t=1}^{T}_{b}w_{t}^{*}(b)(b). \]

From Inequality (3.15), we can obtain the following bound:

\[_{^{T}}[_{t=1}^{T} q_{t}(b_{t})-_{t=1}^{T}_{t} x_{t}(b_{t})]\] \[ O(|T)}{V}})(1 -)+\] \[ O(|T)}{V}}).\]

This concludes the proof of the RoS constraint violation bound in expectation and therefore finishes the proof of the lemma. 

Observe that we have exhibit a logarithmic dependence on \(||\). This is in contrast with existing algorithms for this problem, which suffer from a \(|}\) dependence. Moreover, ignoring the dependence on \(\), our algorithm achieves \(()\) regret and constraint violation bounds. In contrast, primal-dual approaches yield \(()\) bounds (where \(\) is the Slater slack) (Gardner, 2017). In many practical scenarios, \(\) can be very close to zero, while \(V\) remains bounded away from zero (see Appendix D). Consequently, our approach provides significantly stronger guarantees in cases where primal-dual methods may suffer from large regret and constraint violations. Furthermore, we believe that a dependence on \(V\) is unavoidable. This conjecture is supported by the lower bound established in (Barbieri et al., 2017) for online bidding with unknown value (albeit without RoS constraints), which depends on a quantity proportional to \(1/V\).

_Remark 3.1_ (Extension to linear bandits).: While our focus is on online bidding, our algorithm and analysis can be extended to the more general setting of stochastic linear bandits with linear long-term stochastic constraints (see Appendix E for results). The regret and constraint violation bounds in this case avoid the Slater slack \(\), thus improving on the existing primal-dual algorithms.

We now strengthen the in-expectation regret and constraint violation bounds of Theorem 3.3 by providing high-probability guarantees. These stronger bounds are obtained by leveraging Azuma's inequality (Fact A.3) to bound the deviations of the key quantities from their expectations

**Theorem 3.4** (High Probability bounds).: _Given i.i.d. inputs from a distribution \(\) over a time horizon \(T\) to Algorithm 3, with a probability at least \(1-\), we have that:_

\[()-(.\, ) O(|T)}{V}}),\] \[_{t=1}^{T}q_{t}(b_{t}) T+O(|T )}),\] \[_{t=1}^{T}q_{t}(b_{t})_{t=1}^{T}n_{t} x_{t}(b_{t} )+O(|T)}{V}}).\]

Theorem 3.4 demonstrates that, with high probability, the sample pathwise constraint violation of Algorithm 3 is bounded by \(O(|T)}{V}})\). This strengthens the in-expectation bounds from Theorem 3.3 (see Appendix C for the proof).

## 4. Experiments

We empirically study the performance of UCB-RoS and compare it with other existing approaches on synthetically generated datasets. We create synthetic problems where \(v_{t}\), \(x_{t}(\,\,)\), \(q_{t}(\,\,)\), and \(B_{t}^{}\) are sampled i.i.d. from specified distributions. Given a pre-specified mean value \(_{t}\), the values \(v_{t}\) are sampled i.i.d. from a corresponding beta distribution with shape parameters \((105,10(1-))\). The bidding set \(\) is assumed to be a uniformly spaced grid over \(\) with grid size of \(1/||\). The competing bid distribution of \(B_{t}^{}\) is a discrete distribution over \(\). Finally, the type of auction is also given as input. We allow for two types of auctions \(-\) first-price and second-price auctions. The distributions of \(x_{t}(\,\,)\) and \(q_{t}(\,\,)\) are fixed with these inputs of \(B_{t}^{}\), \(v_{t}\), and the auction type. We compare the performance of UCB-RoS against the approaches in (Gardner, 2017; Gardner, 2017).

The work of Castiglioni et al. (Castiglioni et al., 2018) suggests a meta algorithmic game between a primal regret minimizing algorithm and a dual algorithm that minimizes the constraint violation. In our implementation of their algorithm, we choose the primal regret minimizer to be Exp3.P.1, as given in Auer et al. (Auer et al., 2016, Section 6), and the full information dual minimizer to be the DS-OMD algorithm, introduced in Fang et al. (Eang et al., 2015, Section 6). The work of Bernasconi et al. (Bernasconi et al., 2018) weights the constraint violation in a time decaying fashion and uses the primal regret minimizer EXP-IX of Neu (Steiner, 2018).

We create a bidding instance with the parameters in Table 1 along with \(w_{}^{}\) and \(V\) of the benchmark (2.3).

Figure 1(a), Figure 1(b) shows the distribution of the competing bids (\(B_{}^{}\)). This distribution has a mode at the bid \(b=0.333\). Figure 1(c) plots expected pricing \(\) ( ) and realized value \(\)( ) \(\) as function of the bids. The bids with value and budget curves above the pricing curve are the feasible bids that satisfy the budget and RoS constraint in expectation. For the instance in Table 1, we see that all bids are feasible, and hence, the optimal \(w_{}^{}\) is \(_{1}(b)\). Thus, for this optimal allocation, the budget and RoS constraints are exactly satisfied. This suggests that both the constraints can be binding.

The experimental results are shown in Figure 3 for different horizons up to \(2 10^{5}\). Our algorithm, UCB-RoS (depicted in yellow), has a much smaller regret than those of Castiglioni et al. (2018) (in green) and (Bernasconi et al., 2018) (in blue). This primarily reflects our improved dependence on \(||\). The two baselines have much smaller constraint violations than UCB-RoS, which suggests that they each achieve a lower constraint violation at the cost of incurring near-linear regret. This near-linear regret for the chosen horizons is due to their worse dependence on \(||\). Hence, these baselines achieve sublinear regret only over much larger horizons. In contrast, UCB-RoS achieves a much better trade-off between regret and constraint violation.

We remark that the algorithms in the works of Castiglioni et al. (Castiglioni et al., 2018) and Bernasconi et al. (Bernasconi et al., 2018) were designed for _both_ adversarial and stochastic rewards. Often, such algorithms are outperformed by algorithms designed for specific stochastic setting.

## 5. Conclusion and Future Work

In this paper, we studied online bidding with RoS and budget constraints when the value of an impression is unknown a priori. We developed a novel UCB-style algorithm that achieves near-optimal regret and constraint violation bounds without relying on restrictive assumptions like the existence of a Slater point. Our algorithm is not only theoretically sound but also computationally efficient. This work opens up several exciting avenues for future research. One direction is to extend our approach to more complex settings, such as those with multiple advertisers. Another promising direction is to consider adversarial environments where the competing bids or impression values are chosen adversarially. Finally, it would be valuable to develop variants of our algorithm that can incorporate contextual information into the decision-making process. We believe that our work takes a significant step towards developing more robust and effective bidding algorithms for online advertising.