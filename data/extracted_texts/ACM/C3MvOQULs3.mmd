# Step 1: characterizing \(k_{E}\).

[MISSING_PAGE_EMPTY:1]

gets a higher ex-ante expected utility, i.e., the expected utility before agents know their types. In a Bayesian \(k\)-strong equilibrium, no group with at most \(k\) agents can deviate from the equilibrium strategy so that every group member gets a higher expected utility conditioned on every type.

The difference between the two solution concepts is how agents calculated their expected utilities when contemplating whether a deviation is beneficial. We interpret this difference as different attitudes of agents towards deviations. In the ex-ante Bayesian \(k\)-strong equilibrium, a group of agents deviates once the deviation is profitable in the ex-ante expectation. In the Bayesian \(k\)-strong equilibrium, an agent is assumed to be more risk-averse towards deviation and will deviate only when the deviation is profitable conditioned on every possible type. Proposition 1 shows that an ex-ante Bayesian \(k\)-strong equilibrium implies a Bayesian \(k\)-strong equilibrium.

Our technical contributions lie in the study of the collusion problem in peer prediction mechanisms, as a representative of group strategic behavior in Bayesian games, with our solution concepts. We exactly characterize the group sizes \(k\) where the truthful reporting in the peer prediction mechanism by (Brockman, 1983) is an ex-ante Bayesian \(k\)-strong equilibrium (Theorem 1) and a Bayesian \(k\)-strong equilibrium (Theorem 2), respectively. In each case, we show a threshold so that group sizes below this threshold cannot benefit by deviating while group sizes above this threshold can. In general, these thresholds are different for the two types of equilibria we consider. Our thresholds are characterized by the parameters of the game, including the number of agents, common prior, and the scoring adopted by the mechanism. Our result implies that our equilibria parameterized by \(k\) are natural criteria to evaluate the robustness against collusion for a peer prediction mechanism. If truth-telling is an equilibrium with a larger \(k\), the mechanism is more robust against collusion. In the application of the peer prediction mechanism, the scoring rule and the mechanism that maximizes the threshold could be chosen to prevent a wider range of collusion.

We also discuss two other possible scenarios where our solution concept may apply. In the voting scenario where voters only have partial information about the alternatives, it is known that strong equilibria with unlimited coalition sizes may fail to exist when there is a sufficiently large group of voters whose preferences are not aligned with the rest of the voters (Gil et al., 2017). However, the sizes of the deviating groups are typically large in those non-equilibria. Given that it is unlikely for large numbers of voters to collaborate in large elections, it is therefore appealing to study equilibria with bounded deviating groups and obtain more informative results. In the private Blotto game (Blotto, 2018), social media users with noisy information choose to annotate for/against one of the multiple posts. Agents aim to maximize the overall influence of their type on the posts. Our notion interpolates the centralized Colonel Blotto game and the decentralized private Blotto game. The parameter \(k\) becomes an evaluation to characterize scenarios where agents have different centralization levels, where a higher \(k\) represents a higher ability for agents to coordinate and for their type.

### Related Work

Previous work studies group strategic behavior in Bayesian games under different scenarios. Hahn and Yannelis (Hahn and Yannelis, 1992) and Safronov (Safronov, 1992) study coalitional implementation problems under an exchange economy with a strong equilibrium. Ichishi and Idzik (Ichiishi and Idzik, 1993) and Ichishi and Yamazaki (Ichiishi and Yamazaki, 1994) propose the Bayesian strong equilibrium and study its relationship with cooperative game theory. Schoenebeck and Tao (Schoenebeck and Tao, 1992), Han et al. (Han et al., 2007), and Deng et al. (Deng et al., 2017) adopt an approximated version of strong equilibrium to study information aggregation and voting with incomplete information. Nevertheless, none of these works characterizes group strategic behaviors with a bounded size of the group. Guo and Yannelis (Guo and Yannelis, 1995) proposed a coalitional interim equilibrium in which the set of admissible coalitions can be arbitrarily exogenously given. Their solution concept covers a wider range of admissible coalitions than our paper. However, truthful reporting is such an equilibrium only when agents are also coalitionally truthful when they know the report of all other agents, which does not hold for most peer prediction mechanisms. In our setting, truthful reporting fails to be such an equilibrium even with a constant coalition size under mild assumptions (Appendix E). Abraham et al. (Abraham et al., 2018) proposes a \(k\)-coalitional equilibrium where the deviators are allowed to arbitrarily share private information, which may not be applied to many real-world scenarios. For example, the organizer can randomly assign tasks or set limited response periods to prevent agents from arbitrary communication. Moreover, truthful reporting also fails to be such an equilibrium even with a constant coalition size, as signal sharing updates the deviators' beliefs and drives them to different strategies. Our ex-ante Bayesian \(k\)-strong equilibrium is related to the equilibrium in (Gil et al., 2017; Hohn and Yannelis, 1992; O'Connor and Ott, 2017), and our Bayesian \(k\)-strong equilibrium is an extension of the Bayesian strong equilibrium in (Ichiishi and Yamazaki, 1994). In the game with complete information, Aumann (Aumann, 1994) propose the strong Nash equilibrium in which no group of agents has an incentive to deviate. The strong Nash equilibrium (and its variants) has been applied to study group strategic behavior in many scenarios such as congesting game (Hahn and Yannelis, 1992; O'Connor and Ott, 2017; O'Connor and Ott, 2017), voting (Gil et al., 2017; O'Connor and Ott, 2017; O'Connor and Ott, 2017), and Markov game (Gil et al., 2017; O'Connor and Ott, 2017). Abraham et al. (Abraham et al., 2018) studies \(k\)-coalitional strategic behavior under games with complete information. However, a strong Nash equilibrium does not apply to Bayesian games where the information is incomplete.

Our paper is also related to studying the collusion problem in peer prediction mechanisms. Because the appropriate theoretical definitions have not been available, collusion has not been studied explicitly in theoretical peer prediction work. However, many works touch on related concepts. Intuitively, equilibrium selection is related to collusion because agents can coordinate to choose an equilibrium that is bad for the mechanism. Gao et al. (Gao et al., 2017) empirically showed this to be a problem, while Gao et al. (Gao et al., 2017) shows that agents may also coordinate on a low-effort signal. The problem of equilibrium selection is exacerbated by the inevitable existence of uninformative equilibria (Schoenebeck and Tao, 1992; O'Connor and Ott, 2017; Han et al., 2007). Many papers address the problem by developing mechanisms where truthful reporting is more profitable than uninformative collusions (Gil et al., 2017; O'Connor and Ott, 2017; O'Connor and Ott, 2017; O'Connor and Ott, 2017; O'Connor and Ott, 2017; O'Connor and Ott, 2017). More powerfully, works have shown that the truthful equilibrium has the highest possible payments either among all equilibrium (O'Connor and Ott, 2017) or even among all strategies profiles (O'Connor and Ott, 2017; O'Connor and Ott, 2017; O'Connor and Ott, 2017). However, all the latter results consider multi-task peer prediction, while no single-task peer prediction mechanisms have been discovered to have the same merit. Moreover, none of these works study the collusion problem from the perspective of strong equilibrium. Schoenebeck et al. (Schoenebeck et al., 2016) studied a more extreme case where the goal was to design peer prediction mechanisms that are robust against an adversary that controls a constant fraction of the nodes. The present work is different because the deviating groups are required to be strategic and not purely malicious.

Several works study collusion using simulations and measuring how many agents must deviate before truth-telling fails to be the best response for the remaining agents (Ghosh et al., 2016; Snoek et al., 2016) or so that certain dynamics fail to converge back to truth-telling (Schoenebeck et al., 2016). This shows that while the problem is interesting, the theoretical tools available for prior work were insufficient.

## 2. Preliminaries

For an integer \(n\), let \([n]\) denote the set \(\{1,2,,n\}\). For a finite set \(A\), let \(|A|\) be the number of elements in \(A\), and \(_{A}\) denote the set of all distributions on \(A\).

_Proper Scoring Rule._ Given a finite set \(\), a scoring rule \(PS:_{}\) maps an element \(s\) and a distribution \(\) on \(\) to a score. A scoring rule \(PS\) is _proper_ if for any distributions \(_{1}\) and \(_{2}\), \(_{_{1}}[PS(s,_{1})] _{_{1}}[PS(s,_{2})]\) and _strictly proper_ if the equality holds only at \(_{1}=_{2}\).

**Example 2**.: _Given a distribution \(\) on a finite set \(\), let \(q(s)\) be the probability of \(s\) in \(\). The log score rule \(PS_{1}(s,)=(q(s))\). The Brier/quadratic scoring rule \(PS_{B}(s,)=2 q(s)-\). Both the log scoring rule and the Brier scoring rule are strictly proper._

### Bayesian Game Model

A Bayesian game \(=([n],(_{l})_{l[n]},()_{l[n]},(v_{l})_{ l[n]},)\) is defined by the following components.

* The set of agents \([n]\).
* For each agent \(i\), \(_{l}\) is the set of available actions of \(i\). The action profile \(A=(a_{1},a_{2},,a_{n})\) is the vector of actions of all the agents.
* For each agent \(i\), \(_{i}\) is the set of possible types of agent \(i\). The type characterizes the private information agent \(i\) holds, and the agent can only observe his/her type in the game. The type vector \(S=(s_{1},s_{2},,s_{n})\) is the vector of types of all agents.
* For each agent \(i\), \(v_{l}:_{l}_{1}_{n} \) is \(i\)'s utility function that maps \(i\)'s type and the action of all the agents to \(i\)'s utility.
* A _common prior_ that the types of the agents follow is a joint distribution \(\). For a signal \(s_{i}\) of agent \(i\), we use \(q(s_{i})\) to denote the marginal prior probability that \(i\)'s signal is \(s_{i}\). We assume that \(q(s_{i})>0\) for any \(i\) and any \(s_{i}_{i}\).

For each agent \(i\), a (mixed) strategy \(_{i}:_{i}_{_{i}}\) maps \(i\) private signal to a distribution on his/her actions. A strategy profile \(=(_{l})_{l[n]}\) is a vector of the strategies of all the agents.

Given a strategy profile \(\), the _ex-ante_ expected utility of agent \(i\) is

\[u_{i}()=_{S}_{A}[v_{l}(s_{i},a_ {1},,a_{n})].\]

Similarly, given a strategy profile \(\) and a type \(s_{i}\), the _interim_ expected utility of agent \(i\) conditioned on his/her type being \(s_{i}\) is

\[u_{i}( s_{i})=_{S_{-i} _{-i}}_{A}[v_{l}(s_{i},a_{1},,a_{n})],\]

where \(S_{-i}\) is the type vector of all agents except for agent \(i\), and \(_{-i|s_{i}}\) is the joint distribution on \(S_{-i}\) conditioned on agent \(i\)'s signal being \(s_{i}\).

### (Ex-ante) Bayesian \(k\)-Strong Equilibrium

In this paper, we focus on agents that coordinate for strategic behaviors before they know their types. This assumption relates to various constraints in real-world scenarios that prevent agents from discussions after knowing their types.

**Example 3**.: _Consider the online crowdsourcing group in Example 1. The website requires workers to make an immediate report after seeing the task so that workers cannot communicate with each other after they know their types. (For example, workers have to submit the report in 30 seconds to reflect their intuition.) However, workers may collude on the same report before seeing the task._

Both equilibria share the same high-level form: there does not exist a group of \(k\) agents and a deviating strategy such that all the deviators' expected utility in the deviation is as good as the equilibrium strategy profile and at least one deviator's expected utility strictly increases. The difference lies in the expected utility. Ex-ante Bayesian \(k\)-strong equilibrium adopts ex-ante expected utility, while Bayesian \(k\)-strong equilibrium adopts interim expected utility on every type.

**Definition 1** (ex-ante Bayesian \(k\)-strong equilibrium).: Given an integer \(k 1\), a strategy profile \(\) is an ex-ante Bayesian \(k\)-strong equilibrium (\(k\)-ESSE) if there does not exist a group of agent \(D\) with \(|D| k\) and a different strategy profile \(^{}=(^{}_{1})\) such that

1. for all agent \(i D\), \(^{}_{i}=_{i}\);
2. for all \(i D\), \(u_{i}(^{}) u_{i}()\);
3. there exists an \(i D\) such that \(u_{i}(^{})>u_{i}()\).

**Definition 2** (Bayesian \(k\)-strong equilibrium).: Given an integer \(k 1\), a strategy profile \(\) is a Bayesian \(k\)-strong equilibrium (\(k\)-BSE) if there does not exist a group of agent \(D\) with \(|D| k\) and a different strategy profile \(^{}=(^{}_{i})\) such that

1. for all agent \(i D\), \(^{}_{i}=_{i}\);
2. for every \(i D\) and every \(s_{i}_{i}\), \(u_{i}(^{} s_{i}) u_{i}( s_{i})\);
3. there exist an \(i D\) and an \(s_{i}_{i}\) such that \(u_{i}(^{} s_{i})>u_{i}( s_{i})\).

In both solution concepts, if such a deviating group \(D\) and a strategy profile \(^{}\) exist, we say that the deviation succeeds.

When \(k=1\), both ex-ante Bayesian \(1\)-strong equilibrium and Bayesian \(1\)-strong equilibrium are equivalent to the Bayesian Nash equilibrium (Snoek, 2016). (See Appendix B.) However, the two solution concepts are not equivalent for larger \(k\). Example 5 illustrates a scenario in the peer prediction mechanism where the same deviation succeeds under the ex-ante Bayesian \(k\)-strong equilibrium but fails under the Bayesian \(k\)-strong equilibrium.

We interpret the difference between the two solution concepts as different attitudes of agents towards deviations. Agents are assumed to be more risk-averse towards deviations under Bayesian\(k\)-strong equilibrium, as they will deviate only when the deviation brings them higher interim expected utility conditioned on every type. On the other hand, agents under the ex-ante Bayesian \(k\)-strong equilibrium will deviate once their ex-ante expected utility increases. Proposition 1 supports our interpretation by revealing that an ex-ante Bayesian \(k\)-strong equilibrium implies a Bayesian \(k\)-strong equilibrium.

**Proposition 1**.: _For every strategy profile \(\) and every \(1 k n\), if \(\) is an ex-ante Bayesian \(k\)-strong equilibrium, then \(\) is a Bayesian \(k\)-strong equilibrium._

Proof.: Suppose \(^{}\) is an arbitrary deviating profile from \(\) with no more than \(k\) deviators, and \(i\) is an arbitrary deviator in \(^{}\). Since \(\) is an ex-ante Bayesian \(k\)-strong equilibrium, then \(u_{i}(^{}) u_{i}()\). By the law of total probability, \(u_{i}()=_{i_{i}}(s_{i}) u_{i}(  s_{i})\). Therefore, one of the following must hold: (1) for all \(s_{i}\), \(u_{i}(^{} s_{i})=u_{i}( s_{i})\), or (2) there exists \(s_{i}\), \(u_{i}(^{} s_{i})<u_{i}( s_{i})\). In either case, the deviation fails. Therefore, \(\) is a Bayesian \(k\)-strong equilibrium. 

### Peer Prediction Mechanism

In a peer prediction mechanism, each agent receives a private signal in \(=\{,h\}\) and reports it to the mechanism. All the agents share the same type set \(_{i}=\) and action set \(_{i}=\).

\(\) is the common prior joint distribution of the signals. Let \(_{i}\) denote the random variable of agent \(i\)'s private signal. We assume that the common prior \(\) is symmetric -- for any permutation \(\) on \([n]\), \((_{1}=s_{1},_{2}=s_{2},,_{n}=s_{n})=q(_{1}=s_{ (1)},_{2}=s_{(2)},,_{n}=s_{(n)})\).

\(q(s)\) is the prior marginal belief that an agent has signal \(s\), and \(q(s s^{})\) be the posterior belief of an agent with private signal \(s^{}\) on another agent having signal \(s\). We also define \(_{s}=q( s)\) be the marginal distribution on \(\) conditioned on \(s\). We assume that an agent with \(h\) signal has a higher estimation than an agent with \(\) signal on the probability that another agent has \(h\) signal, i.e., \(q(h h)>q(h)\). We also assume that any pair of signals is not fully correlated, which is \(q(h)>0\) and \(q( h)>0\).

We adopt a modified version of the peer prediction mechanism (Shen et al., 2017) characterized by a (strictly) proper scoring rule \(PS\). The mechanism compares the report of agent \(i\), denoted by \(a_{i}\), with the reports of all other agents. For each agent \(j\) with report \(a_{j}\), the reward \(i\) gains from comparison with \(j\)'s report is \(R_{i}(a_{j})=PS(a_{j},_{a_{i}})\). The utility of agent \(i\) is the average reward from each \(j\).

\[_{i}(s_{i},A)=_{j[n],j i}R_{i}(a_{j}).\]

**Remark 1**.: _In the original mechanism in (Shen et al., 2017), the reward of an agent \(i\) is \(R_{i}(a_{j})\), where \(j\) is chosen uniformly at random from all other agents. We derandomize the mechanism so that it fits better into the Bayesian game framework while the expected utility of an agent is unchanged._

**Example 4**.: _Suppose \(n=100\). For the common prior, the prior belief \(q(h)=2/3\), and \(q()=1/3\). The posterior belief \(q(h h)=0.8\) and \(q()=0.6\). Suppose the Brier scoring rule is applied to the peer prediction mechanism. Consider an agent \(i\) with report \(a_{i}=h\). Then, \(i\)'s reward from a peer \(j\) with report \(a_{j}=h\) is \(R_{i}(a_{j})=PS_{B}(h,_{h})=2 q(h h)-q(h h)^{2}-q(  h)^{2}=0.92\). Similarly, \(i\)' reward from another peer \(j^{}\) with report \(a_{j^{}}=\) is \(PS_{B}(,_{h})=-0.28\)._

A (mixed) strategy \(:_{i}_{i,_{i}}\) maps an agent's type to a distribution on his/her action. A strategy profile \(=(_{i})_{i[n]}\) is a vector of the strategies of all the agents. An agent is _truthful_ if he/she always truthfully reports his/her private signal. Let \(^{*}\) be the truthful strategy and \(^{*}\) be the strategy profile where all agents are truthful. We also represent a strategy in the form \(=(_{},_{h})^{2}\), where \(_{}\) and \(_{h}\) are the probability that an agent playing \(\) reports \(h\) conditioned on his/her signal begin \(\) and \(h\), respectively. The truthful strategy \(^{*}=(0,1)\).

Given the strategy profile \(\), the ex-ante expected utility of an agent \(i\) is

\[u_{i}()=_{j[n],j i}_{s_{i}_{i}_{i}(s_{i})}_{s_{j}_{i}_{j}(s_{j})}R_{i}(a_{j}).\]

Given a strategy profile \(\) and a type \(s_{i}\), the interim expected utility of an agent \(i\) conditioned on his/her type being \(s_{i}\) is

\[u_{i}( s_{i})=_{j[n],j i}_{a_{i} _{i}(s_{i})}_{s_{j}_{i}_{j}(s_{j})}R _{i}(a_{j}).\]

**Example 5**.: _We follow the setting in example 4. Let \(^{*}\) be the profile where all agents report truthfully. Let \(D\) be a group containing \(k=40\) agents and \(^{}\) be the profile where all deviators report \(h\)._

_For truthful reporting, consider an agent \(i\) and his/her peer \(j\). The probability that both \(i\) and \(j\) receive (and report) signal \(h\) is \(q(h) q(h h)=2/3*0.8=0.533\), and \(i\) will be reward \(PS(h,_{h})=0.92\). Other probabilities can be calculated similarly. Adding on the expectation of different pairs of signals, we can calculate the ex-ante expected utility of \(i\) in truthful reporting: \(u_{i}(^{*})=_{s_{i},s_{j}\{,h\}}q(s_{i}) q(s_{j} s_{i})  PS(s_{j},_{s_{i}})=0.627\)._

_Now we consider the expected utility of a deviator \(i\) deviating profile \(^{*}\). Since all the deviators always report \(h\), the expected reward \(i\) gets from a truthful reporter, \(i\)'s expected reward is \(q(h) PS(h,_{h})+q() PS(_{h})=0.52\). Among all the other agents, \(k-1=39\) agents are deviators, and \(n-k=60\) agents are truthful reporters. Therefore, \(i\)'s expected utility on \(^{*}\) is \(u_{i}(^{*})=0.682>u_{i}(^{*})\). Therefore, the deviation succeeds under the ex-ante Bayesian \(k\)-strong equilibrium._

_However, the deviating fails under the Bayesian \(k\)-strong equilibrium. The truthful expected utility conditioned on \(i\)'s signal is \(\) is \(u_{i}(^{*})=_{s_{j}\{,h\}}q(s_{j}) PS(s_{j}, _{})=0.52\). On the other hand, when agents deviate to \(^{}\), \(i\)'s reward from a truthful agents becomes \(_{s_{j}\{,h\}}q(s_{j}) PS(s_{j},_{h})=0.2\). Therefore, \(i\)'s interim expected utility \(u_{i}(^{*})=0.484<u_{i}(^{*})\)._

## 3. Dichotomies on Equilibria

Our theoretical results focus focus on the collusive behavior in the peer prediction mechanisms. While the mechanism is known to be prone to collusions, Shnayder and Parkes (Shnayder and Parkes, 2016) empirically shows that there is a lower bound for collusion to be profitable. With our new solution concepts, our theoretical results specify the exact threshold. For both equilibria, we find the largest group size \(k_{B}\) (\(k_{B}\), respectively) such that truthful reporting is an equilibrium. Moreover, for any \(k\) larger than \(k_{E}\) (\(k_{B}\), respectively), truthful reporting fails to be an equilibrium. We first present the result of the ex-ante Bayesian \(k\)-strong equilibrium.

**Theorem 1**.: _In the peer prediction mechanism, for any \(n 2\) and any strictly proper scoring rule PS, truthful reporting \(^{*}\) is an ex-ante Bayesian \(k_{E}\)-strong equilibrium, where_

\[k_{E}^{h}=}(PS(s_{_{k}}))- PS(s_{_{k}}))}{PS(h_{_{k}})-PS(s_{_{k}})}+1&PS(h_{_{k}})>PS(t,_{h})\\ \\ }{k_{E}^{}}=}( pS(s_{_{k}}))-PS(s_{_{k}}))}{PS(s_{_{k}})-PS(s_{_{k}})}]}{n}+1&PS(t,_{j})>PS(h,_{j})\\ \\ }{k_{E}}=(k_{E}^{h},k_{E}^{},n).\]

_For all \(n k>k_{E}\), truthful reporting is NOT an ex-ante Bayesian \(k\)-strong equilibrium._

While a proof sketch is presented below, here we give a brief explanation of the thresholds. \(k_{E}^{h}\) and \(k_{E}^{}\) are characterized by comparing the ex-ante expected utility of a deviator between truthful reporting and all the deviators always report \(h\) (\(\), respectively). Take \(k_{E}^{h}\) as an example. The numerator \(_{g-_{j}}[PS(s,_{j})-PS(s,_{j})]\) is proportional to the loss that the deviator suffers in his expected rewards from the truthful reporters in switching from truthful reporting to always reporting \(h\). The denominator \(PS(h,_{h})-PS(t,_{h})\) is proportional to the amount that, for a deviator, the expected reward gain from other deviators exceeds the expected reward loss from truthful reporting. If \(PS(h,_{h})-PS(t,_{h})<0\), the extra gain never compensates for the loss, so the deviation cannot succeed for any \(k n\). Otherwise, a group size of \(k>k_{E}^{h}\) is required for the deviation to succeed.

**Example 6**.: _We calculate the threshold for ex-ante Bayesian \(k\)-strong equilibrium for the instance in Example 4. For \(k_{E}^{h}\), the numerator equals to \(q(h)(0.28-0.92)+q()(0.68+0.28)=0.32\). The denominator, according to the Brier scoring rule, equals to \(PS(h,_{h})-PS(t,_{h})=2(q(h h)-q( h))=1.2\). Therefore, \(k_{E}^{h}=(n-1)+1\). Similarly, we calculate that \(k_{E}^{}=(n-1)+1\). Therefore, when \(n=100\), a deviation group needs at least \( 99+1=27\) deviators to succeed. This aligns with Example 5, where a 40-agent group succeeds._

Similarly, Theorem 2 characterizes the threshold under Bayesian \(k\)-strong equilibrium.

**Theorem 2**.: _In the peer prediction mechanism, there exists an no such that for every \(n n_{0}\) and any strictly proper scoring rule PS, truthful reporting \(^{*}\) is a Bayesian \(k\)-strong equilibrium in peer prediction, where_

\[k_{B}^{h}=[}(PS(s_{_{k}})- PS(s_{_{k}}))}{q()-PS(s_{_{k}})}]&PS(h,_{h})>PS(t,_{h})\\ \\ n&\]

\[k_{B}^{}=[}(PS(s_{_{k}}) -PS(s_{_{k}}))}{q(h h)-PS(s_{_{k}})}]&PS(t,_{j})>PS(h,_{j})\\ \\ n&\]

\[k_{B}=(k_{B}^{h},k_{B}^{},n).\]

_For all \(n k>k_{B}\), truthful reporting is NOT a Bayesian \(k\)-strong equilibrium._

The lower bound \(n_{0}\) on \(n\) is characterized by the common prior \(q\) and the scoring rule \(PS\) and is independent of \(n\). The explicit expression on \(n_{0}\) is in Appendix D.

The thresholds for the Bayesian \(k\)-strong equilibrium \(k_{B}^{h}\) and \(k_{B}^{}\) are larger than those for the ex-ante Bayesian \(k\)-strong equilibrium \(k_{B}^{h}\) and \(k_{E}^{}\) respectively. This is because, for example, \(k_{B}^{h}\) is characterized by comparing the interim utility of a deviator conditioned on signal \(\) between truthful reporting and all deviators reporting \(h\). In ex-ante, the deviator \(i\) has a probability of \(q()\) to report untruthfully and suffer a loss on expected reward from truthful reporters. When \(i\) has a private signal \(\), such probability becomes \(1\). Therefore, the deviator suffers more loss in the interim expected utility than in the ex-ante expected utility in the expected reward from truthful reporters. On the other hand, \(i\) gets the same extra gain in the reward from other deviators as in the ex-ante expected utility. Therefore, a larger group is needed to make the deviation succeed.

**Example 7**.: _We calculate the threshold for Bayesian \(k\)-strong equilibrium for the instance in Example 4. For \(k_{B}^{h}\), the numerator equals to \(0.32\). The denominator is multiplied by \(q()=0.6\) compared with \(k_{E}^{h}\) and equals to \(1.2 0.6=0.72\). Therefore, \(k_{E}^{h}=(n-1)\). Similarly, we calculate that \(k_{E}^{}= n-1\). Therefore, when \(n=100\), a deviation group needs at least \(45\) deviators to succeed. This also aligns with Example 5, where a 40-agent group fails in deviation._

Theorem 1 and 2 imply that the ex-ante Bayesian \(k\)-strong equilibrium and the Bayesian \(k\)-strong equilibrium are natural criteria to evaluate the robustness against collusion for a peer prediction mechanism. If truth-telling is an equilibrium with a larger \(k\), the mechanism is more robust against collusion. If an information collector aims to prevent collusion in a peer prediction task, he/she could carefully select the mechanism and the scoring rule to maximize the threshold \(k\) under which truth-telling becomes an equilibrium.

**Example 8**.: _If we change the scoring rule from the Brier scoring rule Example 4 to the lag scoring rule with base \(\) in and follow the calculation in Example 6, we have \(k_{E}= 0.275(n-1)+1\). When \(n=100\), a group of at least 28 agents is needed to perform a successful deviation. Therefore, the lag scoring rule is more robust than the Brier scoring rule in this instance._

### Proof Sketch of Theorem 1

The proof consists of two steps. In Step 1, \(k_{E}\) is characterized by comparing the ex-ante expected utility of a deviator when every agent reports truthfully and when all \(k\) deviators always report \(h\) (and always report \(\), respectively). The two deviations bring a deviator higher expected utility if and only if \(k>k_{E}\). In Step 2, we show that for any \(k k_{E}\) and any deviating strategy profile \(^{}\), the average expected utility among all the deviators when \(^{}\) is played will not exceed the expected utility when every agent reports truthfully. Therefore, either no deviators have strictly increasing expected utility or some deviators have strictly decreasing utility after deviation, and the deviation cannot succeed. The full proof is in Appendix C.

**Step 1: determine \(k_{E}\).** We show how \(k_{E}^{h}\) is determined by comparing truthful reporting strategy profile \(^{*}\) and the deviating strategy profile \(\) where all \(k\) deviators always report \(h\), i.e., \(=(1,1)\). The reasoning for \(k_{E}^{i}\) is similar. The condition that a deviator \(i\) is willing to deviate is \(u_{i}()>u_{i}(^{*})\). The inequality should be strict because all deviators have equal expected utility in \(\).

\(u_{i}()\) can be viewed as a linear combination of the expected utility \(i\) gets from the truthful agents, denoted by \(u_{i}()\), and the expected utility \(i\) gets from other deviators, denoted by \(u_{i}()\). In \(\), there are \(n-k\) truthful reporters and \(k-1\) deviators other than \(i\). Therefore, \(u_{i}()=\). \(u_{i}()+ u_{i}()\). Let \( u_{d}=u_{i}(^{*})-u_{i}()\), and \( u_{d}=u_{i}(^{*})-u_{i}()\). Then \(u_{i}()>u_{i}(^{*})\) is equivalent to \( u_{d}+ u_{d}<0\).

The ex-ante expected reward of deviator \(i\) from truthful reporters can be divided into two parts, one conditioned on \(i\)'s private signal being \(h\), the other on \(i\)'s signal being \(\). When \(i\)'s signal is \(h\), \(i\) reports \(h\) both in \(^{*}\) and in \(\), and the expected rewards from truthful reporters in this part are the same. When \(i\)'s signal is \(\), \(i\) reports \(\) in \(^{*}\) and \(h\) in \(\), and the expected rewards make a difference. Therefore, \( u_{}=q()_{q q}\{PS(s,_{})-PS(s, _{})\}\). According to the properness of \(PS\), \( u_{}>0\). Therefore, when \( u_{}> u_{d}\), \(u_{i}()>u_{i}(^{*})\) is equivalent to

\[k>}{ u_{}- u_{d}}(n-1)+1.\]

When \( u_{} u_{d}\), the condition does not hold for any \(k\), and the deviation will never succeed.

From the calculation, \( u_{}- u_{d}=q()(PS(h,_{h})-PS(, _{h}))\). Therefore, when \(PS(h,_{h})>PS(,_{h})\), \( u_{}> u_{d}\), and \(u_{i}()>u_{i}(^{*})\) is equivalent to

\[k>_{q q_{}}[PS(s,_{})-PS(s,_{h })]}{PS(h,_{h})-PS(,_{h})}(n-1)+1.\]

And when \(PS(h,_{h}) PS(t,_{h})\), \( u_{} u_{d}\), and \(u_{i}()>u_{i}(^{*})\) does not hold for any \(k\). This is how \(k_{E}^{i}\) is determined. \(k_{E}^{i}\) is determined in a similar reasoning.

**Step 2: Deviations cannot succeed for \(k k_{E}\)**. For \(k=1\), the statement holds from the truthfulness of the mechanism. Suppose \(2 k k_{E}\), and \(\) be an arbitrary deviating strategy. Let \(u(^{*})\) be the expected utility of truthful reporting, which is equal for all agents. For each deviator \(i\), let \(_{i}=(_{_{}}^{i}_{_{}}^{i})\) denote \(i\)'s strategy in \(\). We show that \(_{i D}u_{i}() u(^{*})\). Therefore, either there exists some deviator \(i\) such that \(u()<u(^{*})\), or for all the deviator \(i\) there is \(u_{i}()=u(^{*})\). In either case, the deviation fails.

Now let \(=(_{},_{h})=_{i D} _{i}\) be the average of the deviator's strategies, and \(\) be the strategy profile where all agents in \(D\) plays \(\) and all other agents report truthfully. \(u_{i}()\) is equal among all the deviators \(i\) due to symmetricity (and denoted by \(u()\)). We first show that \(_{i D}u_{i}( h) u()\) (the average expected utility of deviators playing \(\) will not exceed the expected utility when each deviator plays \(\)) and then that \(u() u(^{*})\) (the expected utility that each deviator play \(\) will not exceed the truthful expected utility).

To show \(_{i D}u_{i}() u()\), we compare the expected reward from truthful agents and deviators separately. For a deviator \(i\), \(u_{i}()\) is independent of the strategy of other deviators and is linear on \(_{}\) and \(_{h}\). Therefore, \(_{i D}u_{i}()=u()\).

For the deviator's part, \(u_{i}()\) is the average of \(i\)'s expected reward from comparing the report with all other deviators \(j D\). Such expected reward is linear on \(j\)'s strategy given a fixed \(i\)'s strategy and linear on \(i\)'s strategy given a fixed \(j\)'s strategy. Therefore, the average expected reward from agents with different strategies equals to the reward from a peer playing the average strategy, and \(u_{i}()\) equals to \(i\)'s expected reward from an agent playing the average strategy \(\) minus a share of \(i\)'s expected reward from an agent playing \(_{i}\). Given a strategy \(=(_{},_{h})\), let \(f(_{},_{h})\) be the expected reward of an agent playing \(\) from another agents also playing \(\). Then

\[_{i D}u_{i}()=f( _{},_{h})-_{i D}f(_{}^{i},_{h} ^{i}).\]

It turns out that \(f\) is a convex function. Therefore, \(_{i D}u_{i}() f(_{}, _{h})=u()\). Combining the truthful part and the deviator part, we show that \(_{i D}u_{i}() u()\).

Finally, we show that \(u() u(^{*})\). Note that \(u()\) can be viewed as a convex function on \(_{}\) and \(_{h}\). This is because \(u()\) is linear on \(\), and \(u()=f(_{},_{h})\) is convex on \(_{}\) and \(_{h}\). Therefore, it is sufficient to show that \(u() u(^{*})\) on the four corner cases of \(\): truthful reporting: \(=(0,1)\), always reporting \(\): \(=(1,0)\), and always tell a lie \(=(1,0)\). When \(=(0,1)\), all the deviator also report truthfully, and \(=^{*}\). For \(=(0,0)\) and \(=(1,1)\), \(k k_{E}\) guarantees that \(u() u(^{*})\). Finally, when \(=(1,0)\), similar reasoning to Step 1 shows that such deviation cannot succeed. 

### Proof Sketch of Theorem 2.

The steps of the proof resemble the steps of the proof of Theorem 1, yet the techniques are different. In Step 1, we determine \(k_{B}\) by comparing the interim expected utilities of a deviator when every agent reports truthfully and when all \(k\) deviators always report \(h\) (\(\), respectively). In Step 2, we show that for any \(k k_{E}\) and any deviating strategy profile \(\) where all the deviators play the same strategy \(\), the expected utility of a deviator on \(\) will not exceed the expected utility when every agent reports truthfully. In Step 3, we show that for sufficiently large \(n\), any \(k k_{B}\), and any deviating strategy profile \(\), there exists a deviator whose expected utility is strictly smaller than the expected utility when every agent reports truthfully. The full proof is in Appendix D.

The main technical difficulty lies in Step 2 and Step 3. Let \(u( h)\) and \(u()\) be the interim expected utility of a deviator conditioned on his/her signal being \(h\) and \(\), respectively, when \(\) is played. \(u( h)\) and \(u()\) can still be viewed as functions on \(_{}\) and \(_{h}\). However, unlike the ex-ante \(u()\), they are not convex. Therefore, we cannot get \(_{i D}u_{i}( h) u( h)\) or \(u( h) u(^{*})\) (or the \(\) side) directly from similar reasoning with those in Theorem 1.

**In Step 2**, we instead show that for any \(\), either \(u( h) u(^{*} h)\) or \(u() u(^{*} h)\) holds. Although \(u( h)\) is not convex, the convexity (or linearity) still holds in certain directions. Here we slightly abuse the notation to write \(u( h)\) as \(u(_{},_{h} h)\). The following properties hold. (1) When \(_{}\) is fixed, \(u(

[MISSING_PAGE_FAIL:7]

preferences, we mean that all voters' utilities for alternative \(\) are higher in world state \(X\) than in world state \(Y\) and their utilities for alternative \(\) are higher in world state \(Y\) than in world state \(X\). That is, all voters' preferences are aligned in that they agree \(X\) "corresponds to" \(\) and \(Y\) "corresponds to" \(\), although the extent the voters' preferences are aligned with this correspondence can be different and due to which voters can be classified into three types:

* the "left-wing voters" who always prefer \(\): \(o(,X)>o(,Y)>o(,Y)>o(,X)\)
* the "right-wing voters" who always prefer \(\): \(o(,Y)>o(,X)>o(,Y)\)
* "swing voters" who prefer the alternative corresponding to the world state: \(o(,X)>o(,X)\) and \(o(,Y)>o(,Y)\) where \(o(a,s)\) denotes the utility for alternative \(a\{,\}\) given the actual world state \(s\{X,Y\}\).

The story is much more complicated with general utilities \(o(,)\) that are not necessarily aligned. In Deng et al. (2017), it is proved that strong Bayes Nash equilibria may not exist even with only two types of voters with antagonistic preferences. In particular, "good" equilibria that identify the majority-favored alternative only exist when the voters from one type significantly outnumber the voters from the other type. When the population sizes of the two types of voters are close, Deng et al. (2017) show that no strong Bayes Nash equilibrium exists.

However, only strong equilibria with unrestrictive group sizes are considered in the above-mentioned work. A typical deviation group in a strategy profile consists of all voters of the same type, and the existence of this kind of large deviation group prevents many strategy profiles from being equilibria. When considering more practical scenarios with bounded coalition sizes, more "good" equilibria are attainable. Given the large size of deviation groups in the non-equilibria found in Deng et al. (2017), it is likely there is an interpolation between the deviation group size \(k\) and the distribution of voters from different types where "good" equilibria exist. This provides a fine-grained structure to the problem compared with the "all-or-nothing" result in Deng et al. (2017).

Strong equilibria are even less likely to exist for more general utilities. It is appealing to apply our new equilibrium concepts with bounded deviating group sizes to characterize voters' strategic behaviors and obtain more positive and fine-grained results. We believe this is a challenging yet exciting future research direction.

### Private Blotto Game

Private Blotto game (Deng et al., 2017) is a decentralized variation of the classic Colonel Blotto game (Deng et al., 2017). It is proposed in order to model the conflict in the crowdsourcing social media annotation. For example, the Community Notes on X.com (Zen et al., 2018) allows users to vote for/against posts to identify misinformation and toxic speech with the wisdom of the crowd.

**Example 9**.: _Suppose there are \(n\) platform users and \(m\) posts on a topic (for example, whether restrictions should be made for the COVID pandemic). Users obtain different private information from different sources, which can be generally categorized into two types, pros and cons. Each user simultaneously chooses and labels one post based on their type. The labels on each post will eventually determine the influence on the readers. A post with more supporters spreads more widely, and a post with more opponents will be announced as misinformation. Each user aims to maximize the influence of their type and plays the game strategically. What will be a stable status in such a scenario?_

The traditional Colonel Blotto game models this scenario as a centralized game, where two opposite "colonels" (for example, campaign groups) control all the users. In the Private Blotto game, on the other hand, users make their own decisions on where to deploy. This better simulates the modern social media environment where a central coordinator is generally lacking.

**Definition 3** (Private Blotto game.).: \(n\) agents are competing over \(m\) items. Each agent has a type (_pro_ or _con_). Every agent (simultaneously) chooses exactly one item to label. The outcome of each item is determined by some outcome function. The disutility of each agent is the distance from the agent's type to each item's outcome.

The results in the Private Blotto game (Deng et al., 2017) appear to heavily rely on the complete lack of coordination, which is also not entirely realistic. While a central coordinator is lacking, an agent can still locally coordinate with a few others. This allows (small) strategic groups and local campaigns to emerge in real-world scenarios. Moreover, these settings nearly always lack complete information and might be more faithfully modeled by agents receiving different information about various topics.

In this setting, our new solution concept of (ex-ante) Bayesian \(k\)-strong equilibrium seamlessly interpolates between these two extremes of complete centralization and complete decentralization. The bound \(k\) can characterize how well-organized the agents are. When \(k=1\), agents are fully decentralized. A larger \(k\) characterizes scenarios where agents coordinate with friends, neighborhoods, or campaigns on relevant issues. Finally, when \(k\) is large enough, agents can be viewed as commanded by two opposite centralized "colonels", and the game becomes closer to the traditional Colonel Blotto game. Moreover, our definition will also naturally extend to the setting where agents have more than two sides (for example, different political factions that are more or less aligned) and the scenario where the agent's utilities are related to an underlying ground truth rather than peer partisanship.