# Node Classification:

Inductive Graph Alignment Prompt: Bridging the Gap between Graph Pre-training and Inductive Fine-tuning From Spectral Perspective.

Anonymous Author(s)

###### Abstract.

The "Graph pre-training and fine-tuning" paradigm has significantly improved Graph Neural Networks(GNNs) by capturing general knowledge without manual annotations for downstream tasks. However, due to the immense gap of data and tasks between the pre-training and fine-tuning stages, the model performance on downstream task is still limited. Inspired by prompt fine-tuning in Natural Language Processing(NLP), many endeavors have been made to bridge the gap in graph domain. But existing methods simply reformulate the form of fine-tuning tasks to the pre-training ones, ignoring the inherent gap of graph data. With the premise that the pre-training graphs are compatible with the fine-tuning ones, these methods typically operate in transductive setting. In order to generalize graph pre-training to inductive scenario where the fine-tuning graphs might significantly differ from pre-training ones, we propose a novel graph prompt based method called Inductive Graph Alignment Prompt(IGAP). Firstly, we unify the mainstream graph pre-training frameworks and analyze the essence of graph pre-training from graph spectral theory. Then we identify the two sources of the data gap in inductive setting: (i) graph signal gap and (ii) graph structure gap. Based on the insight of graph pre-training, we propose to bridge the graph signal gap and the graph structure gap with learnable prompts in the spectral space. A theoretical analysis ensures the effectiveness of our method. At last, we conduct extensive experiments among nodes classification and graph classification tasks under the transductive, semi-inductive and inductive settings. The results demonstrate that our proposed method can successfully bridge the data gap under different settings.

Graph Pre-training, Graph Prompt, Graph Neural Networks +
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

## 1. Introduction

Graph Neural Networks(GNNs) taking advantage of message passing to fuse node features and topological structures, have been successfully applied in various applications such as Web search, personal recommendation and community discovery (Hinton et al., 2015; Kipf and Welling, 2016; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017). Traditional GNNs are trained under a supervised manner which not only necessitates laborious manual annotations but also is susceptible to over-fitting problem. Inspired by the success of the pre-training model in Natural Language Processing(NLP) (Hinton et al., 2015; Kipf and Welling, 2016; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017) and Computer Vision(CV) (Chen et al., 2016; Kipf and Welling, 2016; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017), many endeavors have been paid into transplanting the ethos of "pre-training and fine-tuning" into the domain of graph.

**Graph Pre-training and fine-tuning.** This paradigm involves two distinct stages: (i) graph pre-training stage and (ii) fine-tuning stage. During the graph pre-training stage, GNNs glean general patterns from unannotated data, encompassing many intrinsic graph properties such as local node feature distributions, topological patterns and the consistent fusion of local and global graph patterns. Subsequently, in the fine-tuning stage, the GNNs initialized with the pre-trained parameters can be adapted seamlessly to many downstream tasks even with scant labels and training epochs.

Although the paradigm of "graph pre-training and fine-tuning" emancipates GNNs from the burdensome need for extensive manual annotations and empowers them to perceive general graph patterns to improve downstream tasks, there is still an immense gap between the pre-training stage and fine-tuning stage limiting the performance of the pre-trained models. In the domain of NLP, many innovative prompt based methods are proposed to bridge this gap (Hinton et al., 2015; Kipf and Welling, 2016; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017), whose philosophy lies in reformulating fine-tuning tasks to mirror the format of pre-training objectives thus the pre-trained knowledge can be transferred seamlessly. Similar strategies have been applied in the realm of graphs to narrow down the gap (Hinton et al., 2015; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017). However, compared to the gap in NLP, it is far more challenging in graph scenario (Hinton et al., 2015), especially under the inductive setting. The inductive setting, where fine-tuning datasets significantly differ from their pre-training counterparts, is prevalent in the application of pre-training models. In NLP, different language datasets are naturally compatible with each other because the semantic information is consistent among them. But in the graph domain, not only the node/edge features might have disparate distributions but also the topological structures differ significantly. This diversity of graph data will result in incompatible patterns between the pre-training and fine-tuning graphs. We give an example to illustrate, considering the NLP domain with distinct corpus of pre-training and fine-tuning datasets: "I feel happy in passing the exam." and "It's happy to win that game.", the token "happy" retains the consistent semantic meaning among them thus can be transferred easily. While in the graph realm, the users from different communities might have different social patterns even though they are in the same social network, the knowledge specific to one graph is hard to be transferred to another. Additionally, the pre-training process usually operates as a black box where the pre-training dataset is unavailable. These distinctive traits inherent to inductive scenario raise new challenges for existing methods:

* **Transductive Limitation.** Although many graph prompt based methods have been proposed to bridge the gap caused by pre-training and fine-tuning task types as the language prompts do, these methods still ignore the diversity of graph data (Zhu et al., 2017; Wang et al., 2018). Existing graph prompt based methods operate under the assumption of compatibility between pre-training and fine-tuning graphs, meaning all these methods are all **transductive** where the **performance can only be ensured** when **GNNs are pre-trained and fine-tuned on the same graph.** Under the inductive setting, the pre-trained GNNs might have sub-optimal performance and even negative transfer on the fine-tuning graphs.
* **Inaccessibility of Pre-training Data.** Due to data privacy concerns, the GNNs' pre-training process often operates as a black box, meaning that we can only get the pre-trained model with the pre-training dataset unavailable. This lack of access complicates the fine-tuning process under the inductive setting. Traditional transfer learning based methods, which require additional information about the pre-training dataset to align the representation of the GNNs, are not suitable for inductive fine-tuning. Graph prompt based methods might also have the compromised performance in the absence of pre-training data as a prompt.

In order to generalize the paradigm of "graph pre-training and fine-tuning" to inductive setting where the fine-tuning graphs significantly differ from their pre-training counterparts, we propose a novel graph prompt based method named Inductive Graph Alignment Prompt(IGAP). To address the the data gap without direct access to the pre-training graph, we first delve into the process of graph pre-training and then design graph prompts according to the characteristics of pre-training for inductive fine-tuning. Specifically, in the graph pre-training stage, we analyze the essence of this process using spectral graph theory. Our key insight reveals that **graph pre-training predominantly aligns the graph signal with low-frequent components rather than high-frequent ones.** Then for the inductive fine-tuning stage, we identify two primary sources contributing to the data gap: (i) graph signal gap and (ii) topological structure gap. These kinds of gap manifest as node features perturbation and spectral space misalignment from graph spectral theory. Based on the understanding of graph pre-training, we propose an innovative solution for inductive fine-tuning. To counteract the influence of graph signal perturbation, we introduce a learnable graph signal prompt that offers adaptive compensations. Additionally, a spectral space alignment prompt is introduced to align the \(K\)-smallest frequent components, which rectifies spectral space misalignment and makes the transfer of essential knowledge possible. We provide a theoretical analysis to ensure the effectiveness of our method. Finally, we utilize a label prompt to reformulate the fine-tuning task to harmonize with the pre-training objective. We validate the effectiveness of IGAP through extensive experiments under transductive, semi-inductive, and inductive settings for both node and graph classification tasks. The experimental results demonstrate the better performance of IGAP in bridging the gap between graph pre-training and inductive fine-tuning.

## 2. Related Work

**Graph Pre-training.** Graph pre-training aims at leveraging vast amounts of label-free graph data to equip GNNs with universal graph knowledge. There are three mainstream graph pre-training frameworks: (i) subgraph contrastive based methods such as GraphCL (Zhu et al., 2017), GRACE (Zhu et al., 2017), and GCA (Zhu et al., 2017) train the GNNs by differentiating the negative subgraph patterns from positive ones; (ii) link prediction based methods such as GPT-GNN (Liu et al., 2018) train the GNNs through the masked link prediction task; (iii) local-global contrastive based methods such as DGI (Zhu et al., 2017), ST-DGI (Zhu et al., 2017) utilize the mutual information to encode global patterns into local representations.

**Graph Transfer Learning.** Graph transfer learning aims at facilitating the knowledge transfer learned from one task to another. It can narrow down the gap between the source and target tasks (Zhu et al., 2017; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). These kinds of methods achieve this goal by aligning the distribution of the two datasets with the regularization or generative constraints.

**Prompt and Graph Prompt.** To bridge the gap between pre-training and fine-tuning objectives, many prompt based methods are proposed. Most of the methods are from CV and NLP domains (Chen et al., 2016; Chen et al., 2016; Chen et al., 2016; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018), whose common idea lies at reformulating the fine-tuning tasks into the pre-training paradigms. Inspired by this idea, several graph prompt based methods are also proposed: GPPT (Wang et al., 2018) incorporates learnable graph label prompts, transforming node classification into a link-prediction task to narrow down the task type gap. GraphPrompt (Zhu et al., 2017) unifies the graph prompt templates and enhance their performance through learnable readout prompt functions. All-in-One (Wang et al., 2018) introduces a novel graph token structure accompanied by a token insertion technique.

However, all these methods are not fit for the inductive fine-tuning because the transfer learning based methods need addition information about the pre-training datasets which is unavailable; The graph pre-training and graph prompt based methods are transductive with the assumption that the pre-training and fine-tuning graphs have compatible patterns.

## 3. Preliminary

In this section we present the preliminary knowledge used in this paper.

**Graph and Graph Laplacian.** Graph \(\) is denoted as \(=(,,X)\), where \(\) is the set of nodes, \(\) is the set of edges and \(X\) is the graph signal matrix1. A set containing \(N\) graphs is denoted as \(_{}=\{_{1},...,_{N}\}\). The Graph \(\) can also be represented as \(=(A,X)\) where \(A\) represents the adjacent matrix. The graph Laplacian \(L\) of \(=(A,X)\) is calculated as \(L=D-A\) where \(D\) is the degree matrix. The graph Laplacian \(L\) is a real symmetric matrix thus can be diagonalized as (Chen et al., 2016):

\[L=U U^{T} \]where \(U=[v_{1},v_{2},..,v_{N}]\) is the eigenvectors and \(v_{i}\) is the eigenvector corresponding to the eigenvalue \(_{i}(_{1}<_{2}<..._{N})\). The graph signal Fourier transform is defined as:

\[=U^{T}x x X \]

and the inverse graph signal Fourier transform is:

\[x=U \]

**Graph Neural Networks(GNNs).** We use \(f_{0}\) to denote the GNN layer parameterized by \(\). In spectral domain it can be represented as filter kernel \(g_{}(.)\). The message passing in spectral domain is actually the convolution between the filter kernels and the graph signals, which can be presented as (Bishop, 2006):

\[Z=f_{0}(A,X)=Ug_{}()U^{T}X=_{i}v_{i}g_{}(_{i})v _{i}^{T}x \]

## 4. Method

In this section, we first analyze the essence of graph pre-training process from the graph spectral theory. Then based on the analysis, we propose a novel graph prompt method named Inductive Graph Alignment Prompt(IGAP) to deal with the challenges in the inductive fine-tuning, whose framework is shown in Figure 1. At last, we conduct a theoretical analysis to ensure the effectiveness of our proposed method.

### Exploring the Essence of Graph Pretraining.

In order to design effective graph prompts for the inductive fine-tuning, it's indispensable to understand the process of graph pre-training. Graph pre-training frameworks can be mainly categorized into three types: (i) link prediction, (ii) subgraph contrastive learning, and (iii) local-global contrastive learning. For the convenience of analysis, we first reformulate these diverse frameworks into an unified one which highlights their common essence.

#### 4.1.1. Unifying Pre-training Framework

We contend that all these pre-training frameworks fundamentally operate as a contrastive process, distinguishing positive samples from negative ones. This contrastive form is encapsulated in the InfoNCE loss (Ganin and Lempitsky, 2015) as follows:

\[InfoNCE=-_{i}log(_{i})),(f_{ }(_{i}^{*})))}{ sim((f_{}(_{i})), (f_{}(_{i}^{-/*})))} \]

where \(_{i}\) represents the \(i\)-th graph view, \(_{i}^{T}\) and \(_{i}^{-}\) are positive and negative samples respectively.2\(sim(.)\) is the similarity function and \(\) represents a readout head. Different graph pre-training methods are consistent with this formulation, differing only in how they define positive/negative samples and similarity functions.

**Subgraph Contrastive Learning.** This approaches ensure that similar graph views have similar representations, while disparate graph views are distinctly represented (Zhu et al., 2017; Wang et al., 2018; Wang et al., 2018). They naturally have the contrastive formation where positive samples are the small perturbated ego-subgraphs centered on the same node and negative samples are the ego-subgraphs centered on different nodes.

**Link Prediction.** The link prediction based methods equip GNNs with universal graph knowledge by predicting randomly masked edges (Ganin and Lempitsky, 2015). It increases the link probability between the nodes with masked edges while lowering the probability between the actually nonadjacent nodes. This process can also be represented as the form 5 where positive samples are the subgraphs centered on the nodes with masked edges while negative samples are the subgraphs centered on non-adjacent nodes. The link possibility calculation is denoted as the \(sim(.)\) function.

**Local-Global Contrastive Learning.** The local-global contrastive based methods enable GNNs to capture the consistent patterns among the local and global perspectives (Zhu et al., 2017; Wang et al., 2018). These methods assimilate representations between subgraphs and the entire graph

Figure 1. The framework of IGAP. We first align the graph signals and then we align the spectral space between the pre-train graph and fine-tune graph thus the pre-trained GNN model can be applied. A task-specific prompt is used to align the pre-train task and the fine-tune task.

while maintaining dissimilarity with feature-shuffled graphs. The local-global contrastive based methods also naturally have the InfoNCE formation where the positive samples are the subgraphs while the negative samples are the shuffled ones.

#### 4.1.2. The Spectral Character of Graph Pre-training

In order to dig out the essential characteristics of graph pre-training to facilitate the inductive graph fine-tuning, we delve into the pre-training process from a graph spectral perspective. We begin with the conclusion which is presented as the theorem 1, and then provide its proof.

**Theorem 1**.: _Graph pre-training aligns the graph signal \(x\) more with the low-frequent components than the high-frequent components, where \(sim(x,v_{1})>...>sim(x,v_{N})\) for \(_{1}<..<_{N}\)._

For analytical convenience, we use the contrastive pre-training paradigm as an example since we have demonstrated that all the mainstream graph pre-training frameworks can be reformulated as this. There are two major ways to generate positive/negative graph samples: (i) graph structure perturbation and (ii) graph signal perturbation. For the generation of positive samples, the structure perturbation is small and does not rotate the spectral basis, which can be denoted as \(v_{l^{*}}=q_{l^{*}}+v_{l^{*}}\) where \(q_{l^{*}}\) is small and parallel with \(v_{l^{*}}\) The graph signal perturbation actually is a small, angle-stable transformation denoted as a symmetric matrix \(F_{e}=l+F_{gp}\) where \(F_{gp}\) is sparse with small non-zero components. For the generation of negative samples, the structure perturbation introduces high-frequent noise to the spectral basis described as \(v_{l^{*}}=q_{l^{*}}+v_{l^{*}}\). The graph signal transformation is described as a matrix \(F_{-}=l+F_{dt}\) where \(F_{dt}\) is dense. For the graph signal \(x\), we have the proof:

The message passing process is described as:

\[& z=_{i}v_{i}g_{}(_{i})v_{i}^{T}x\\ & z_{+}=_{i}(q_{i^{*}}+v_{i})g_{}(_{i^{*}})(q_{i^ {*}}+v_{l^{*}})^{T}(l+F_{gp})x\\ & z_{-}=_{i}(q_{i^{*}}+v_{i})g_{}(_{i^{*}})(q_{i^ {*}}+v_{i})^{T}(l+F_{dt})x\\  \]

Where \(z\), \(z_{+}\) and \(z_{-}\) are normalized. The InfoNCE loss can be expressed as:

\[& InfoNCE(z,z_{+},z_{-})\\ &=-logx^{T}v_{i}g_{}(_{i})g_{ }(_{i}^{+})v_{i}^{T}(l+F_{gp})x+_{i}}{_{i,j}x^{T} v_{i}g_{}(_{i})v_{i}^{T}(q_{j^{*}}+v_{j})g_{}(_{j^{*}})(q_{j^ {*}}+v_{j})^{T}(l+F_{gp/dt})x}\\  \]

Where \(_{i}\) is the influence of \(q_{i^{*}}\) on the graph signal \(x\) and is very small. We assume that \(q_{j^{*}}=_{i}_{k,j}/v_{k}\) where \(_{i}_{k,j}=1\), thus the formulation 7 is transformed as:

\[& InfoNCE(z,z_{+},z_{-})\\ &=log(1+x^{T}v_{i}g_{}(_{i})(1+ _{i,i}^{2})det(I+F_{dt})ig_{}(_{i^{*}})v_{i}^{T}x}{ _{i}x^{T}v_{i}g_{}(_{i})det(I+F_{gp})g_{}(_{i^{*}})v_ {i}^{T}x+_{i}}\\ &+x^{T}v_{i}g_{}(_{i})( _{i,j})det(I+F_{gt})g_{}(_{i^{*}})(j^{-}_{i}_{i,i}v_{i}+v_{j})^{T}x}{_{i}x^{T}v_{i}g_{}(_{i})det(I+ F_{gp})g_{}(_{i^{*}})v_{i}^{T}x+_{i}})\\ & log(1+x^{T}v_{i}g_{}(_{i}) _{j}_{i,j}(1+_{id,j})g_{}(_{i^{*}})v_{j}^{T}x}{ _{i}x^{T}v_{i}g_{}(_{i})(1+_{gp,i})g_{}( _{i}^{+})v_{i}^{T}x}+\\ &x^{T}v_{i}g_{}(_{i})(1+_{j }_{i,j}^{2}(1+_{id,j})g_{}(_{i^{*}})v_{i}^{T}x}{ _{i}x^{T}v_{i}g_{}(_{i})(1+_{gp,j})g_{}( _{i}^{+})v_{i}^{T}x})\\  \]

Where \(_{gp,i}\) is the \(i\)-smallest the eigenvalue of \(F_{gp}\), \(_{id,i}\) is the \(i\)-smallest the singular value of \(F_{dt}\) and \(_{gp,i}\)\(_{id,j}\). Since the negative perturbation mainly contains the high-frequent noise corresponding to each spectral components, implying that \(_{i,j}<_{i+1,j}\) given \(_{i^{*}}<_{i 1^{*}}\). Besides, the graph patterns are also disturbed unevenly with \(_{1}<_{1^{*}}<...<_{N}<_{N^{*}}\). We deduce that \((_{i,j}+_{i,j}^{2})(1+_{id,j}g_{ }(_{j^{*}}))}{g_{}(_{i})}\) increases as \(_{i}\) decreases. Therefore, the minimization of loss 8 ensures \(v_{i}^{T}x>..>v_{N}^{T}x\) given \(_{i}v_{i}^{T}x=1\), thus validating the theorem 1. 

The theorem 1 provides an important clue for inductive fine-tuning: the knowledge of the pre-training process is mainly concentrated on the low-frequent components, which makes it possible to be transferred under the alignment of low-frequent space.

### Inductive Graph Alignment Prompt

During the inductive fine-tuning stage, data gap is the most significant challenge which can be attributed to two major sources: (i) graph signal gap and (ii) graph structure gap. The graph signal gap refers to differences in distribution between the fine-tuning graph features and the pre-training ones, while the graph structure gap indicates disparities in their structural properties. In this subsection, we focus on these challenges posed by inductive setting and propose a novel graph prompt based method named Inductive Graph Alignment Prompt(IGAP) to address these gaps.

#### 4.2.1. Graph Signal Gap

To address the graph signal gap, we propose a graph signal prompt. We view this gap as a signal perturbation and propose compensating for it by using a learnable graph signal prompt. Specifically, for graph signal \(x_{i}\) the compensation is expressed as follows:

\[_{i}=x_{i}+p_{i} \]

Where \(p_{i}\) represents the learnable prompt for \(x_{i}\). However, it's expensive if we employ a unique learnable prompt for each signal,which not only increases the prompt parameters but also raises the risk of over-fitting problem. To mitigate complexity and avoid over-fitting, we propose to utilize a set of graph signal prompts \(P_{S}=[p_{R_{1}},..,p_{R_{L}}]\). The graph signal compensation is then transformed as follows:

\[_{i}=x_{i}+_{j}_{j}^{i}p_{S_{j}} \]

Where \(_{j}^{i}\) is also a learnable parameter. By doing so, the complexity, which originally scaled with \(O(N F)\) for a graph signal matrix of size \(N F\), is reduced to \(O(N L+L F)\) with \(L N\).

#### 4.2.2. Graph Structure Gap

The graph structure gap is essentially the misalignment of spectral space between the pre-training graph and the fine-tuning graph. The pre-trained GNNs cannot be directly applied to the fine-tuning graph because the filters will be invalid in the new space. Fine-tuning GNNs directly might not only compromise the general knowledge but also lead to over-fitting problem. Besides, with the pre-training graph data unavailable, there is no reference for the direct alignment. To fully leverage the pre-trained knowledge, we propose a simple yet effective alignment strategy based on the characteristics of pre-training process. A theoretical analysis is also provided to ensure the effectiveness.

**Spectral Space Alignment: A Recessive Approach** According to the theorem 1, pre-trained GNNs align graph signals more with low-frequent components than high-frequent ones. Consequently, it's possible to align the basis of low-dimensional spectral space corresponding to low-frequent components while maintaining the main knowledge of the pre-trained GNNs. Supposing the pre-training graph and the fine-tuning graph are denoted as \(_{pt}\) and \(_{ft}\) respectively. \(U_{pt}=[v_{pt_{1}},v_{pt_{2}},..,v_{pt_{N}}] R^{N N}\) and \(U_{ft}=[v_{ft_{1}},v_{ft_{2}},..,v_{ft_{M}}] R^{M M}\) are the eigenvectors of \(_{pt}\) and \(_{ft}\) respectively. We can reduce the spectral space dimension of the pre-training graph into the fine-tuning one and only consider the alignment of \(K\)-dimensional spectral subspace based by \(U_{pt_{k}}=[v_{pt_{1}},..,v_{pt_{K}}] R^{M K}\) and \(U_{ft_{k}}=[v_{ft_{1}},..,v_{ft_{k}}] R^{M K}\). Since \(U_{pt_{k}}\) is inaccessible and we propose a recessive transformation matrix prompt \(P_{t}\) which is learnable:

\[U_{pt_{k}}=P_{t}U_{ft_{k}} \]

Subsequently, the fine-tuning graph signal can be projected into the aligned spectral space, and the aligned low-frequent signal is calculated as follows:

\[=P_{t}U_{ft_{k}}g_{}(_{ft})U_{ft_{k}}^{T}P_{t}^{T} {X} \]

Where the pre-trained knowledged can be transferred recessively to the fine-tuning stage.

**Why Align the \(K\)-Smallest Spectral Components?** Here we analyze the why the low-frequent spectral components alignment can guarantee the effectiveness under the inductive setting. In the spectral domain, the orthonormal spectral components describe distinct graph smooth patterns and each graph spectral component represents a specific graph smooth pattern. The graph signal encompasses a comprehensive description of all these patterns. Considering a normalized graph signal \(x\), we define the informative level and the noise level corresponding to the smooth pattern \(i\) as \(v_{i}^{T}x\) and \(1-v_{i}^{T}x\) respectively, since the more compatible between the graph signal and the pattern, the more smooth patterns are contained, and the less compatible between the graph signal and the pattern, the more noise is contained. The spectral component signal-to-noise ratio is defined as:

\[Sp\_SNR(v_{i})=^{T}x}{1-v_{i}^{T}x} \]

The graph signal-to-noise ratio is represented as the average of all the spectral component signal-to-noise ratios:

\[SNR(x)=_{i}^{T}x}{1-v_{i}^{T}x} \]

This ratio describes the purity of useful patterns in the graph signal. According to Theorem 1, we have \(Sp\_SNR(v_{1})>Sp\_SNR(v_{2})>...>Sp\_SNR(v_{N})\). If we align all the components, it will induce considerable noise. But if we choose less components, many useful patterns will be lost and thus the performance will be compromised. Therefore, we make a balance by aligning the \(K\)-smallest components.

**Why Does the Spectral Space Have Low Dimension?** According to the theorem 1, the projections of the graph signal on the high-frequent components are smaller, and these different components are mutually orthogonal. Consequently, the information contained in these high-frequent axes in the spectral space is relatively limited. Hence, the spectral space actually has the compacted dimension.

#### 4.2.3. Task Type Gap

During the fine-tuning stage, the task type might also differ from the pre-training one. To preserve the generalization ability of pre-trained GNNs, we propose aligning the task types. We demonstrate that the main kinds of downstream tasks like node classification, graph classification, and link prediction can also be reformulated into the contrastive form. As the link prediction task has been discussed above, here we focus on the node classification task and graph classification task. The Cross-Entropy loss of classification is:

\[CE=_{i}_{j}-y_{i,j}log(_{_{j}}(z_{i}))-(1-y_{i,j})log(1- _{_{j}}(z_{i})) \]

Where \(y_{i,j}\) is label \(j\) of the node \(i\), and if we view the parameters of \(_{_{j}}\) as the label representation \(l_{j}\), the loss in Equation 15 can be transformed into:

\[CE=_{i}-log,z_{i})}{sim(l_{j},z_{i})}-log\,sim(l_{j},z_{i}) \]

The optimization in Equation 16 is equivalent to the optimization of the InfoNCE loss 5. The graph classification task can be transformed in the same way. For classification downstream task with \(d\) labels, we formulate the classification InfoNCE loss function as follows using trainable label representations \(P_{l}=[p_{1},..,P_{d}]\):

\[InfoNCE=-_{i}log,_{i})}{_{j}sim(p_{j},_{i})} \]

The inductive fine-tuning optimization problem is expressed as:

\[&,p_{x},p_{y})}{argmin}-_{i}log ,_{i})}{_{j}sim(p_{j},_{i})}\, p_{ j} P_{1}_{i}\\ & S.T.=P_{t}U_{ftj}g_{}(_{ft})U_{ft}^{T }P_{t}^{T} \]Where \(g_{}\) is fixed, \(sim(,)\) is the similarity function and in this paper we use cosine similarity.

## 5. Experiments

In this section, we conduct experiments on both node classification and graph classification tasks under three distinct settings: (i) transductive setting where the pre-training graph is directly used for fine-tuning; (ii) semi-inductive setting where the pre-training graph and the fine-tuning graph are distinct but share some overlap; (iii) inductive setting where the pre-training graph and the fine-tuning graph have no overlap.

### Datasets and Metrics

The settings of datasets and metrics are described as follows, more details about the data process can be found in Appendix.1:

**Node Classification**: In the transductive setting, we use Citeseer (Brock et al., 2017) and Amazon-Photo (Shi et al., 2017) for both pre-training and fine-tuning. In the semi-inductive setting, we conduct experiments on two dataset pairs (Brock et al., 2017; Chen et al., 2018): (CoraFull, CoraFull-F) and (Arxiv-P, Arxiv-F) where CoraFull and Arxiv-P are used for pre-training while CoraFull-F and Arxiv-F are used for fine-tuning. CoraFull-F is sampled from CoraFull; Arxiv-P and Arxiv-F are sampled from Arxiv; Notably, in the semi-inductive setting, the nodes selected for fine-tuning are also part of the pre-training datasets. In the inductive setting, we conduct experiments on two dataset pairs (Brock et al., 2017): (Paper100M-P, Paper100M-F) and (Reddit-P, Reddit-F) (1), Paper100M-P and Reddit-P are used for pre-training while Paper100M-F and Reddit-F are used for fine-tuning. Paper100M-P and Paper100M-F are sampled from Paper100M; Reddit-P and Reddit-F are sampled from Reddit; In the inductive setting, there is no overlap between nodes and labels. The information on these datasets can be found in Table 1. We sample 100 nodes per class for the training set in Citeseer, Amazon-Photo, and CoraFull-F. For Arxiv-F, Paper100M-F, and Reddit-F, we sample 150, 250, and 250 nodes respectively. For all the fine-tuning datasets the remaining nodes are randomly split as 2:8 for evaluation and testing. We use classification accuracy as metrics. The statistics of these datasets can be found in Table 1.

**Graph Classification**: In the transductive setting, we conduct experiments on Molhiv and Moltox21 (Moltox21, 2018). In the semi-inductive setting, we use Molhiv for pre-training and Molbace, Molbbpp for fine-tuning. In the inductive setting, GNNs are pre-trained on Molmuv and fine-tuned on Molbace and Molbbb as there is no overlap between datasets. For all these datasets, we use RDKit3(Rossi et al., 2018) to pre-process them as their official settings and we randomly split these datasets as 4:2:4 for training, evaluation and testing. ROC-AUC is employed as the evaluation metric for all graph classification datasets. The statistics of these datasets can be found in Table 2.

### Baselines

To evaluate the effectiveness of our method, we compare it with several state-of-the-art baselines, which can be categorized as follows:

**Supervised Learning**: We train GCN (Kipf and Welling, 2017), GraphSAGE (Hamilton et al., 2017) and GAT (Vaswani et al., 2017) from scratch on the fine-tuning graphs in the supervised manner.

**Pre-training+Fine-tuning**: We pre-train GNNs in the pre-training graphs then fine-tune them. The pre-training methods we use are GraphCL (Zhu et al., 2017) and DGI (Zhu et al., 2017).

**Pre-training+Prompt Fine-tuning**: We pre-train the GNNs and fine-tune them using graph prompts to bridge the task gap. The graph prompt methods we use are GPPT (Zhu et al., 2017), GraphPrompt (Zhu et al., 2017), and All-in-One (Zhu et al., 2017).

### Experimental Settings.

We present the settings of all these methods, the detail information can be found in Appendix.2.

**Model Settings.** We use 2-layer GNN with 128 hidden neurons as the backbone. For the supervised baselines, we append a 2-layer Neural Network with Relu as the task head. For the graph pre-training baselines, we pre-train GNNs with a head and replace it with a new trainable head for fine-tuning, with the pre-trained GNNs frozen. For the graph prompt baselines, we use the official suggested templates to design prompts on the fine-tuning datasets. We apply grid search to find the optimal model hyperparameters for all these baselines. For IGAP, we set \(L\) as 16 and \(K\) as 32. We use GraphCL as the pre-training framework and use a new 2-layers Neural Network with ReLU as the task-specific head during fine-tuning. Only the task-specific head and the prompts are trainable with the GNNs frozen. For the graph classification tasks, we use mean pooling to calculate graph representation for all the baselines.

**Training Settings.** For the pre-training and supervised learning baselines, the learning rate is set as 0.0001. The maximum epoch number is set to 500, saving checkpoint every 50 epochs. For the

  Datasets & Graphs & Avg Nodes & Avg Edges & Tasks \\  Molhiv & 41,127 & 25.5 & 54.9 & 1 \\ Molmuv & 93,087 & 24.2 & 52.6 & 17 \\ Moltox21 & 7,831 & 18.6 & 38.6 & 12 \\ Molbace & 1,513 & 34.1 & 73.7 & 1 \\ Molbbbp & 2,039 & 24.1 & 51.9 & 1 \\  

Table 2. Statistics of the graph classification datasetsfine-tuning stage, the learning rate is set as 0.001. The maximum epoch number is 100 for all the baselines and we save checkpoint ever 10 epochs. We only report the best performance among all the checkpoints for each baseline. For both stages, we use the Adam (Kingma and Ba, 2014) without weight decay as the optimizer.

### Effectiveness

The node classification and graph classification results are presented in Table 4 and Table 5 respectively. More baselines are tested in Appendix 3. Here are the key observations:* **Transductive Setting**: Graph pre-training based methods outperform supervised learning methods because pre-training allows GNNs to grasp more universal knowledge and prevent over-fitting problem. Graph prompt based methods outperform graph pre-training by narrowing down the gap between pre-training and fine-tuning tasks. IGAP achieves competitive performance compared to graph prompt based methods as it also bridges the gap.
* **Semi-inductive Setting**: Graph pre-training and graph prompt based methods outperform supervised methods in some cases(e.g., CoraFull) but not in other(e.g., Arixiv). The possible reason is that CoraFull-F is more compatible with CoraFull but Arixiv-P and Arixiv-F have more gap. Graph prompt based methods bridge the task type gap and thus perform better than the pre-training based methods. Our method achieves the best performance as it succeed in narrowing down the data gap.
* **Inductive Setting**: Graph pre-training based methods perform worse than supervised learning, demonstrating that

   &  &  &  \\  & Molhiv & Moltox21 & Moltbox & Moltbox & Moltbox \\  GCN & 0.7606 & 0.7298 & 0.7812 & 0.6523 & 0.7812 & 0.6523 \\ GraphSAGE & 0.7532 & 0.7310 & 0.7895 & 0.6458 & 0.7895 & 0.6458 \\  GraphCL+GCN & 0.7678 & 0.7330 & 0.7991 & 0.6651 & 0.7525 & 0.6389 \\ GraphCL+GraphSAGE & 0.7775 & 0.7418 & 0.7948 & 0.6605 & 0.7677 & 0.6456 \\  GIPT+GCN & 0.7563 & 0.7361 & 0.7763 & 0.6572 & 0.7717 & 0.6434 \\ GIPT+GraphSAGE & 0.7619 & 0.7366 & 0.7956 & 0.6626 & 0.7639 & 0.6466 \\ All-in-One+GCN & **0.7936** & 0.7447 & 0.7950 & 0.6788 & 0.7671 & 0.6550 \\ All-in-One+GraphSAGE & 0.7865 & **0.7529** & 0.7963 & **0.6819** & 0.7740 & 0.6562 \\  IGAP+GCN & **0.7886** & 0.7435 & **0.8041** & 0.6796 & **0.7902** & **0.6644** \\ IGAP+GraphSAGE & 0.7752 & **0.7472** & **0.8022** & **0.6837** & **0.7933** & **0.6629** \\  

Table 5. Performance of node classification task. The best two results are bold.

   &  &  &  \\  & Citeseer & Amazon-Photo & CoraFull-F & Arixiv-F & Paper100M-F & Reddit-F \\  GCN & 70.76\% & 90.62\% & 75.76\% & 86.26\% & 71.13\% & 89.48\% & 737 \\ GraphSAGE & 71.12\% & 89.88\% & 76.12\% & 86.08\% & 71.89\% & 88.25\% & 788 \\ GAT & 69.75\% & 90.09\% & 75.05\% & 86.47\% & 72.28\% & 88.12\% & 788 \\  GraphCL+GCN & 73.81\% & 91.14\% & 77.26\% & 84.73\% & 67.34\% & 85.50\% & 761 \\ GraphCL+GraphSAGE & 73.56\% & 91.61\% & 77.56\% & 85.05\% & 68.92\% & 86.33\% & 762 \\ DGI+GCN & 72.94\% & 90.36\% & 75.94\% & 84.32\% & 66.24\% & 85.23\% & 763 \\ DGI+GraphSAGE & 73.69\% & 90.45\% & 76.69\% & 85.56\% & 65.32\% & 85.55\% & 764 \\  GIPT+GCN & 73.10\% & **92.54\%** & 76.10\% & 83.71\% & 68.88\% & 86.04\% & 765 \\ GPPT+GraphSAGE & 72.53\% & 91.89\% & 76.53\% & 84.80\% & 66.98\% & 85.61\% & 766 \\ GraphPrompt+GCN & 74.08\% & 92.22\% & 77.08\% & 84.58\% & 68.51\% & 85.67\% & 767 \\ GraphPrompt+GraphSAGE & 74.15\% & 92.18\% & 78.15\% & 83.16\% & 70.12\% & 84.29\% & 768 \\ All-in-One+GCN & 73.64\% & 92.24\% & 77.88\% & 85.63\% & 66.51\% & 85.10\% & 769 \\ All-in-One+GraphSAGE & **75.11\%** & 91.82\% & 77.81\% & 85.06\% & 69.72\% & 86.85\% & 771 \\  IGAP+GCN & 74.44\% & 91.84\% & **79.46\%** & 87.15\% & 72.16\% & **90.06\%** & 772 \\ IGAP+GraphSAGE & 74.23\% & **92.78\%** & **79.23\%** & **87.69\%** & **72.74\%** & **90.35\%** & 773 \\ IGAP+GAT & **74.55\%** & 91.35\% & 78.55\% & **87.55\%** & **73.68\%** & 89.56\% & 774 \\  

Table 4. Performance of graph classification task. The best two results are bold.

the data gap will result in negative transfer to the downstream tasks. Graph prompt based methods also suffer significantly because they fail to bridge the data gap. Our method outperforms all the baselines, demonstrating its success in bridging the data gap in the inductive setting.

**Graph Classification:**

* **Transductive and Semi-inductive Settings**: Graph pre-training based methods outperform supervised learning, indicating that the general pre-trained knowledge can mitigate the over-fitting problem. Graph prompt-based methods narrowing down the task type gap and thus have some improvements. Our proposed method achieves better performance compared with other graph prompt based methods.
* **Inductive Setting:** Graph pre-training and graph prompt-based methods do not perform well because the pre-trained knowledge cannot be directly applied to dissimilar fine-tuning graphs. It's worthy to note that there is less negative effect compared to the node classification task in the inductive setting, the main reason lies in: the molecular graphs have relative simple graph patterns thus the knowledge are easy to be transferred. Our proposed method achieves better performance, illustrating the effectiveness of narrowing down the data gap.

### Ablation Study

To demonstrate the effectiveness of different prompt modules, we conduct an ablation study in inductive node classification task and the results are shown in Table 5, the graph classification results can be found in Appendix 4. From the results, we find that the performance significantly decreases without spectral space alignment, indicating the crucial role of the alignment in inductive scenario. Besides, no signal alignment also results in a decrease in performance but is not as much as no space alignment, the possible reason might be that the space alignment can compensate the graph signal gap. At last, fine-tuning end-to-end has a minor impact on performance. This might be attributed to that the general knowledge can compensate for the task type gap.

### Hyperparameter Study

We conduct experiments to test the influence of hyperparameters on node classification task and more results can be found in Appendix 5. We set the \(L\) as 8, 16, 32, 64 and \(K\) as 16, 32, 64, 128 and the results can be found in Table 6 and Table 7 respectively. We find small \(L\) and \(K\) will make the alignment difficult because of less parameters but if we set \(L\) and \(K\) too large it will result in higher costs and make it hard for fine-tuning with redundant parameters.

### Visualization

We randomly sample 300 nodes from Reddit-F and visualize the representations of different baselines by t-SNE, the results are shown in Figure 2. GraphCL and GraphProm have difficult in discriminating some node classes because the pre-trained GNNs have compromised performance in inductive scenario. IGAP has better cluster property which corroborates the effectiveness of the alignment.

## 6. Conclusion

In this paper, we propose a novel graph prompt based method to deal with the data gap in inductive fine-tuning. We first analyze the essence of graph pre-training process under an unified framework. Then for the inductive fine-tuning stage, we identify the two main sources of the data gap: (i) graph signal gap and (ii) graph structure gap. Based on the insight of graph pre-training, we propose to align the graph signal and spectral space with the learnable prompts. Theoretical analysis is also given to justify our method. Extensive experiments shows the effectiveness in bridging the data gap under the inductive setting.

   \(L\) & Paper100M & Reddit & \(K\) & Paper100M & Reddit \\ 
8 & 71.96\% & 89.17\% & 16 & 71.24\% & 89.46\% \\
16 & 72.74\% & 90.35\% & 32 & 72.74\% & 90.35\% \\
32 & 72.93\% & 91.61\% & 64 & 73.22\% & 90.59\% \\
64 & 71.99\% & 90.88\% & 128 & 73.32\% & 90.25\% \\   

Table 6. Influence of \(L\).

Figure 2. Visualization of Different Baselines on Reddit-F.

   Method & Paper100M-F & Reddit-F \\  GraphSAGE & 71.13\% & 88.25\% \\ GraphCL & 68.92\% & 86.33\% \\ All-in-One & 69.72\% & 86.85\% \\  IGAP-GraphSAGE & 72.74\% & 90.35\% \\ IGAP(No \(P_{s}\)) & 70.58\% & 88.26\% \\ IGAP(No \(P_{t}\)) & 68.67\% & 87.44\% \\ IGAP(No \(P_{t}\), end2end) & 72.01\% & 89.28\% \\   

Table 5. Different Prompt Influence in Inductive Setting.