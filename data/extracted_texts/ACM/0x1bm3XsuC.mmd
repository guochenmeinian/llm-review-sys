# E2Usd: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series

Anonymous Author(s)

###### Abstract.

Cyber-physical system sensors emit multivariate time series (MTS) that monitor physical system processes. Such time series generally capture unknown numbers of states, each with a different duration, that correspond to specific conditions, e.g., "walking" or "running" in human-activity monitoring. Unsupervised identification of such states facilitates storage and processing in subsequent data analyses, as well as enhances result interpretability. Existing state-detection proposals face three challenges. First, they introduce substantial computational overhead, rendering them impractical in resource-constrained or streaming settings. Second, although state-of-the-art (SOTA) proposals employ contrastive learning for representation, insufficient attention to false negatives hampers model convergence and accuracy. Third, SOTA proposals predominantly only emphasize offline non-streaming deployment, we highlight an urgent need to optimize online streaming scenarios. We propose E2Usd that enables efficient-yet-accurate unsupervised MTS state detection. E2Usd exploits a Fast Fourier Transform-based Time Series Comressor (fftCompress) and a Decomposed Dual-view Embedding Module (ddEM) that together encode input MTS at low computational overhead. Additionally, we propose a False Negative Cancellation Contrastive Learning method (fncCleanXing) to counteract the effects of false negatives and to achieve more cluster-friendly embedding spaces. To reduce computational overhead further in streaming settings, we introduce Adaptive Threshold Detection (ddATD). Comprehensive experiments with six baselines and six datasets offer evidence that E2Usd is capable of SOTA accuracy at significantly reduced computational overhead. Our code is available at [http://bit.ly/3rMFJVv](http://bit.ly/3rMFJVv).

## 1. Introduction

In Cyber-Physical Systems (CPSs) (Stein et al., 2016; Krizhevsky et al., 2012; Krizhevsky et al., 2012), sensors monitor physical processes continuously, generating streams of Multivariate Time Series (MTS) data. This raw data, often complex and devoid of immediate interpretability, requires human labors to discern underlying "states" that correspond to specific conditions. For instance, consider an MTS corresponding to a dance routine as depicted in Fig. 1. The MTS is collected using four accelerometers situated in the dancer's arms and legs, capturing transitions between states that can be labeled "walk", "run", "jump", "kick", and "left hop". The aim of _state detection_ is to segment the MTS into a sequence of concise segments and assign each segment a state. Segments that share similar characteristics should be assigned the same state. In Fig. 1, the first and last segments exhibit similar fluctuations and are consequently assigned the same state, "walk".

Supervised state detection (Stein et al., 2016; Krizhevsky et al., 2012; Krizhevsky et al., 2012) requires known segments of an MTS and their labels, which are often not available. Thus, there has been a growing interest in _unsupervised state detection_ (USD) (Stein et al., 2016; Krizhevsky et al., 2012; Krizhevsky et al., 2012; Krizhevsky et al., 2012; Krizhevsky et al., 2012). USD is capable of identifying distinct states in an MTS directly, without relying on known segments and their labels. Once their USD process is completed, minimal human intervention is needed to assign a semantic label to each detected state. As depicted in Fig. 1, USD often uses clustering to associate each time step with a sufficiently similar already seen state or a new state if no similar state has been seen. Consecutive time steps with the same state are merged to form a segment. In the dance example, developers are not required to know the number and types of dance states beforehand. Instead, they can identify and label the dance states based on the USD results. This level of flexibility is particularly attractive in open-ended detection tasks, e.g., as found in cyber-attacks (Bartack et al., 2016), web application behavior (Krizhevsky et al., 2012), and beyond.

A commonly used USD approach (Krizhevsky et al., 2012; Krizhevsky et al., 2012; Krizhevsky et al., 2012), as depicted in Fig. 1, involves two stages. An initial encoding stage projects input data, acquired using a sliding window, into a latent embedding; then, a clustering stage identifies the state of this latent embedding. By moving the sliding window as data arrives, it is possible to support real-time detection of states. While recent advances in deep learning (DL) have improved the initial MTS data encoding (Krizhevsky et al., 2012; Krizhevsky et al., 2012), DL-based USD methods excel at capturing intricate MTS features, thus enhancing the subsequent clustering process. However, three main challenges remain.

**C1 (Resource-Intensive Architectures).** The intricate architectures, particularly those of DL-based MTS encoders (Krizhevsky et al., 2012; Krizhevsky et al., 2012), incur substantial computational and storage overheads. This precludes the deployment of such USD models on devices with limited resources, which occur frequently in practice.

**C2 (False Negative Sampling of Unsupervised Contrastive Learning).** State-of-the-art (SOTA) learning proposals for MTS encoders are rooted in unsupervised contrastive learning (Krizhevsky et al., 2012) and aim to maximize the similarity between similar samples (from consecutive windows) and to minimize the similarity between dissimilar ones (from distant windows). However, this approach can be prone to false negative sampling due to its idealized assumption that distant windows have distinct states. Ensuring the robustness of unsupervised contrastive learning at forming a clustering-friendly embedding space is an important concern.

Figure 1. An example of unsupervised state detection on MTS.

**C3 (Suboptimal Methodology on Streaming Scenarios)**. While current studies (Zhu et al., 2017; Wang et al., 2018; Wang et al., 2019) focus primarily on offline, non-streaming USD, there is a critical need for optimization for online deployments. Unconditional invocation of a USD model for all windowed MTS data can cause redundant clustering computations. Thus, more efficient strategies for streaming use are needed.

In this study, we present E2Usd, an efficient-yet-effective model addressing these challenges in unsupervised MTS state detection. To tackle **C1**, E2Usd includes a compact embedding method featuring two key strategies. First, it utilizes a Fast Fourier Transform-based Time Series Compressor (fftCompress) to selectively retain essential frequency components, discarding noisy ones. This reduces the computational overhead of subsequent operations. Second, to strike a balance between feature extraction capacity and model simplicity, E2Usd advocates a return to the original nature of a time series by decomposing compressed MTS into trend and seasonal components followed by a simple but sufficiently effective dual-view DL embedding module (named ddEM). This eliminates reliance on complex end-to-end DL architectures.

To tackle **C2**, we propose a False Negative Cancellation Contrastive Learning method (fnccLearning) tailored to mitigate false negative sampling in SOTA contrastive learning for USD. fnccLearning introduces a novel negative sampling scheme, where selecting genuinely false negatives is carried out by taking into account both the trend and seasonal similarities between paired samples. Moreover, instead of considering individual windowed samples, we take a holistic approach by harnessing groups of consecutive samples for similarity computation in the negative aspect, thereby ensuring consistent embedding within the same state. This unique treatment is reflected in the overall fnccLearning loss.

To tackle **C3**, we present Adaptive Threshold Detection (adaTD), which aims to reduce the number of clustering operations in online USD by first assessing the similarity between the currently windowed MTS data and the data in the preceding window and then deciding whether to perform clustering on the current windowed MTS data. A customized adaptive threshold, based on a simple and effective similarity metric is proposed to determine the similarity sufficiency. In experiments, E2Usd achieves the best accuracy while using only 4% of the total and 1% of the trainable parameters when compared to the SOTA method, while also achieving the lowest processing time among all competitors.

The primary contributions are as follows.

* We propose a compact MTS embedding method, comprising (i)fftCompress for retaining essential temporal information while mitigating noise for simplified time series representation and (ii) noEM, which enables dual-view embedding of trend and seasonal components in MTS, effectively integrating traditional and modern methodologies (Section 3.1).
* We propose fnccLearning, aimed at mitigating the likelihood of false negatives in unsupervised contrastive learning for MTS state detection. This is achieved by a unique treatment of potential negative pairs exhibiting the lowest similarities (Section 3.2).
* We devise the adATD scheme tailored for streaming USD. By comparing the current windowed MTS data to the preceding window, adATD reduces clustering operations based on an adaptive similarity threshold (Section 3.3).
* We study E2Usd on six datasets while considering six baselines, providing evidence of SOTA accuracy and substantial computational costs reduction. We also provide evidence of practical applicability by deploying E2Usd on an STM32 MCU (Section 4). Besides, Section 2 provides necessary background information, Section 5 reviews related work, and Section 6 concludes the paper.

## 2. Preliminaries

### Unsupervised State Detection for MTS

Definition 1 (Multivariate Time Series, MTS).: _A multivariate time series (MTS), denoted by \(\), is an ordered sequence of sensory observations:_

\[=\{_{i}\}_{i=1}^{},_{i}^{}, \]

_where \(_{i}\) is the observation at the \(i\)-th time step; the parameters \(\) and \(\) are the MTS dimensionality and the length of the MTS, respectively. A segment of \(^{}\) spanning time steps \(i\) to \(j\) is denoted as \(_{i:j}^{(j-1)}\)._

Definition 2 (State in MTS).: _A state acts as a concise representation of the underlying condition associated with an MTS segment._

States are discernible due to their unique internal features, such as recurring patterns or consistent statistical behaviors (Ghosh et al., 2017; Wang et al., 2018; Wang et al., 2019). Referring to the dance routine example in Fig. 1, states might correspond to different dance movements, each of which exhibits a unique pattern: "walk" with rhythmic variations, "jump" with intense spikes, "hop" with recurrent bursts, and "run" with higher frequency and intensity than "walk". Accurate identification of these specific patterns (states) is crucial to understanding the underlying process captured by an MTS and to enable downstream applications like urban monitoring (Brockman et al., 2018) and healthcare (Wang et al., 2019).

Definition 3 (Unsupervised State Detection, USD).: _Given an MTS \(\) of length \(\), the process of unsupervised state detection (USD) aims to assign each observation \(x_{i}\) at state index, \(s_{i}\), without any training data and a set of predefined states. This process ultimately yields a state sequence \(=\{s_{i}\}_{i=1}^{}\), where \(s_{i}^{+}\) identifies a specific state detected by the USD process._

Note that the number of distinct states found by the USD process, Distinct(\(\)), is unknown prior to the start of the process.

We proceed to introduce the **classical USD pipeline**(Wang et al., 2018; Wang et al., 2019; Wang et al., 2019). In general, USD utilizes a _sliding window_, i.e., MTS data is processed by the USD system per window along the time dimension. In Fig. 1, a sliding window of size \(\) and step size \(\) traverses the MTS. Let \(_{t}=_{t-:t}\) be the current window of the MTS. This window is processed by the **MTS encoder** to obtain an embedding \(_{t}\) in a latent space. This embedding is input to a **clustering model** that deduces its state index \(_{t}\{^{+}\}^{}\), which is then assigned to the \(\) time steps in window \(_{t}^{}\).

As the sliding window moves at step size \(\), each time step eventually has \(}{}\) state indexes determined by the USD process1. To reconcile these sets of state indexes and ensure smoother transitions, the final state index for each time step is determined through majority voting (Wang et al., 2018; Wang et al., 2019; Wang et al., 2019).

Conventional DL-based USD methods (Zhou et al., 2017; Zhang et al., 2018) employ intricate neural networks and take the raw MTS data as input directly. To enhance efficiency, we incorporate a Fast Fourier Transform (FFT)-based time series compressor (see Section 3.1.1). Below, we provide a brief overview of FFT.

### FFT in Time Series Analysis

FFT (Zhou et al., 2017) is fundamental to signal processing. Engineered for optimal computational efficiency, FFT calculates the Discrete Fourier Transform of numerical sequences. This capability is essential for identifying frequency components in a time series, enabling noise reduction and compression. Further, this capability supports our goal of reducing the computational overhead of the USD process, since this overhead is correlated with the length of the MTS. Integrating FFT offers a promising avenue for enhancing both efficiency and accuracy.

Specifically, we utilize real-valued FFT. Crafted as a variant of FFT, it transforms an MTS window \(\) with P time steps into \(K=( p/2+1)\) frequency components, represented as a matrix \(^{}\). Conversely, the inverse real-valued FFT converts \(\) back to a new time-domain representation, denoted by \(}\).

The computation of real-valued FFT and the inverse real-valued FFT are formulated in Equations 2 and 3, respectively.

\[q_{k}_{j=0}^{k-1}(x_{p}(-)+ (-)),k=0,,K-1 \]

\[_{p}_{j=0}^{k-1}(q_{k}( )-q_{k}()),p=0,,P-1 \]

Here, \(_{k}\) is the \(k\)-th (\(0 k<\)) frequency component of the real-valued FFT result \(\), and \(_{p}\) is the \(p\)-th (\(0 p<\)) time step's data of the inverse real-valued FFT result \(}\).

## 3. Key Techniques of E2USD

E2US follows the classical USD pipeline (Section 2.1), encompassing MTS embedding and clustering. In particular, E2USp utilizes the Dirichlet Process Gaussian Mixture Model (DPGMM) (Brockman, 1988) for clustering. Below, we present the key innovations in E2USp. E2USp first applies a compact embedding procedure to the input MTS to ease subsequent neural computations (Section 3.1). In the compact embedding procedure, E2USp also incorporates a novel False Negative Cancellation Contrastive Learning method (Section 3.2) to ensure or improve the effectiveness of the learned embeddings. Finally, E2USp employs an Adaptive Threshold Detection (adaTD) scheme for improved applicability in online streaming (Section 3.3).

### Compact Embedding of MTS

Contemporary DL-based embedding methods often utilize complex end-to-end networks (Zhou et al., 2017; Zhang et al., 2018; Zhang et al., 2018; Zhang et al., 2018), potentially overlooking the rich domain-specific knowledge in traditional feature engineering. To generate concise yet informative embeddings for input MTS, we advocate for revisiting fundamental principles in data analysis and signal processing, particularly in frequency domain analysis (Zhou et al., 2017) and time series decomposition (Zhou et al., 2017). Our studies indicate that these approaches can yield effective embedding outcomes while using simpler techniques. As shown in Fig. 2, our compact embedding comprises an FFT-based Time Series Compressor (fftCompress) and a Decomposed Dual-view Embedding Module (h0EM), detailed in Section 3.1.1 and Section 3.1.2, respectively.

#### 3.1.1. FftCompars

In practical applications, sensor designs often incorporate over-sampling rates to ensure comprehensive information capture. In this sense, maintaining a sparse representation of time series is reasonable and beneficial for retaining essential information and reducing noise, ultimately simplifying subsequent temporal feature extraction. Frequency domain analysis, primarily using FFT (Zhou et al., 2017; Zhang et al., 2018; Zhang et al., 2018; Zhang et al., 2018), is a well-established technique to achieve this goal. However, it remains challenging to effectively identify the set of distinct active frequency components for FFT analysis across different tasks. To address this aspect, we introduce fftCompars, which encompasses three steps.

**(1) Real-valued FFT.** Given a window \(_{t}=X_{t-P,t}^{,p}\), this step transforms \(_{t}\) into its frequency domain representation \(^{}\), using Equation 2. Expressing the input signal in the frequency domain is crucial for our goal of compressing time series data. In the frequency domain, we can identify and prioritize the most significant frequency components, thus achieving compression by emphasizing salient features and discarding less critical ones.

**(2) Energy-based Frequency Compressor.** As the core of fftCompars, this step dynamically selects and retains _active frequencies_ from the frequency domain based on the cumulative energy observed across all the MTS dimensions. Energy in signal processing quantifies the strength (magnitude) of a signal's frequency components. Typically, low energy implies reduced strength and activation. A possible solution is to select discrete frequencies, rather than forming a continuous band. However, two important factors need to be considered: first, research has shown that noise is often concentrated at the extreme frequencies (Zhang et al., 2018); second, active frequencies tend to cluster around a central range. Skipping frequencies could potentially lead to the omission of crucial information. Considering these factors, we choose to utilize a continuous frequency band. This decision is especially effective at removing both high- and low-frequency noise, which improves the overall data representation. Specifically, given a frequency-domain representation \(^{}\), we compute for its **cumulative energy \(^{}\)**(Zhou et al., 2017), where the \(k\)-th component \(_{k}\) corresponds to a viable starting frequency \(k\) (\(0 k<\)) and is computed as follows:

\[_{k}=_{i=k}^{k+0-1}_{n=0}^{N-1}(Q_{n,i})^{2}. \]

Here, Q (\(<\)) is a predefined frequency bandwidth; N is dimensionality of the MTS; and \(Q_{n,i}\), a scalar, is the amplitude of the \(i\)-th

Figure 2. Compact embedding of the input MTS.

frequency component of the \(n\)-th dimension. We then identify the starting position \(k^{}(0,)\) that yields the maximum cumulative energy over the given bandwidth \(Q\). Subsequently, we cut off \(Q\) consecutive frequency components starting from the \(k^{}\)-th frequency component of \(Q\):

\[Q^{}=Q[0:N:k^{}:k^{}+Q]. \]

As a result, \(Q^{}\) encompasses \(Q\) consecutive frequency components that capture the most pronounced energy contributions.

**(3) Inverse Real-valued FFT.** The compressed frequency domain representation \(Q^{}^{N Q}\) is then transformed back to its time domain using Equation 3. This yields a compressed MTS \(}^{N P^{}}\), where \(P^{}=2(0-1)\). As illustrated in the right part of Fig. 9, for an original time series of length \(P=480\), the energy-based frequency compressor retains a frequency bandwidth of \(Q=41\). This results in a compressed time series of length \(P^{}=80\). Through fftCompress, the MTS is compressed from \(P\) to \(P^{}\) in the temporal dimension. This leads to a significant reduction in computational overhead for the subsequent feature extraction while preserving essential signal attributes and reducing noise.

A detailed assessment of the energy-based frequency compressor's impact, along with a comprehensive parameter sensitivity analysis related to \(Q\), can be found in the Appendix [(1)].

#### 3.1.2. doEM

Many DL-based approaches employ computationally intensive modules for MTS feature extraction [(26; 34; 24; 36)]. While effective, these complex structures may capture redundantly features that can be obtained efficiently using lightweight, traditional tools, thus incurring unnecessary computational costs. Time series decomposition is a widely used technique for extracting essential components like trend and seasonality. Recognizing its significance, we introduce the Decomposed Dual-view Embedding Module (_d_e_DEM), which features an innovative and lightweight architecture that seamlessly combines time series decomposition with a subsequent lightweight dual-view neural embedding module. It facilitates the embedding of both trend and seasonality in MTS by leveraging the strengths of both traditional and modern approaches.

**(1) Decomposition of Compressed MTS**. Studies [(13; 42; 14)] show that time series data can be broken down into trend, seasonal, and residual (noise) components. In our approach, we exclude the residual component, as the prior fftCompress step has eliminated noise. Thus, with the compressed MTS \(}^{N P^{}}\), we employ a proven method, the _moving average_ scheme [(42)], for decomposing it into trend and seasonal components. This approach is well recognized for its effectiveness in this context.

* The **trend component \(^{N P^{}}\)** is calculated using a moving average kernel of size \(\), which is odd, as follows: (6) \[[n,t]=_{i:-(-(1)/2)}^{(-(-1 )/2)}}[n,t+\|.\]
* The **seasonal component \(^{N P^{}}\)** is obtained by subtracting the trend component \(\) from \(}\), formally, \(=}-\).

**(2) Dual-view Embedding**. Referring to Fig. 2, after decomposing the signals into two distinct views, they undergo embedding using lightweight networks. Both trend and seasonal views share identical embedding structures (1D convolution + max pooling + linear embedding). The embeddings resulting from both views are fused to create a compact MTS embedding.

Feature Projection Layer (no training). The trend and seasonal components are projected into high-dimensional latent spaces using 1D convolution:

\[^{}=()\ \ ^{}=(). \]

This step employs convolution feature maps to provide varying perspectives on the signals. Notably, this projection module does not require training. It has been proven effective in multiple MTS classification studies [(15; 14; 30)]. The subsequent layers are trained to extract features from each view. Additionally, a detailed parameter sensitivity analysis regarding the dimensionality of these two latent spaces can be found in the Appendix [(1)].

Linear Embedding Layer (trainable). A max pooling layer is used to reduce dimensionality, thus enhancing computational efficiency while highlighting the most informative features. Then, a linear layer is applied to effectuate the embedding:

\[^{}=(((^{ });^{})), \]

\[^{}=(((^{ });^{})), \]

where \(^{}^{0}\) and \(^{}^{0}\) are the trend and seasonal embeddings of size \(D\), respectively. Trainable parameters \(^{}\) and \(^{}\) for \(()\) encompass weights and biases.

Fusion Layer (trainable). The embeddings from both views are concatenated and further transformed to generate the final MTS embedding \(^{0}\), providing a comprehensive representation that captures inter-view relationships:

\[=((^{},^{});), \]

where \(\) is the corresponding trainable parameters. For a detailed sensitivity analysis regarding the final embedding size \(D\), refer to the Appendix [(1)].

By employing fftCompress and dDEM, we avoid intricate neural networks and their computational demands. In the following section, we introduce an innovative contrastive learning scheme to ensure that the compact embedding structure preserves crucial information effectively.

### False Negative Cancellation Contrastive Learning for Effective Embedding

As a technique used widely in the embedding stage of USD, contrastive learning [(11)] maximizes the similarity between similar samples (so-called positive pairs), while minimizing it for dissimilar ones (so-called negative pairs). However, **false negative sampling** is a common issue: current approaches [(34; 17; 36)] involve randomly sampling \(U\) distinct window groups from an MTS, each having \(\) consecutive windows (see Fig. 3) through a sliding window. With this setup, each group is assumed to represent a unique state, with positive sample pairs always being from the same group and negative sample pairs always being from different groups. However, this setup can lead to a problem, where different groups inadvertently share the same state, causing samples from these groups to be incorrectly regarded as negative pairs. In Fig. 3, the blue and green windowed samples, which come from different groups, are incorrectly classified as negative pairs. In reality, they belong to the same state "walk" and should be considered as positive pairs.

We thus propose fnrcLearning (False Negative Cancellation Contrastive Learning), a novel approach that addresses this issue with a unique similarity-based negative sampling scheme. By considering seasonal and trend embeddings, it can sample those genuinely dissimilar _negative pairs_ from the groups with low similarity, enhancing MTS embedding effectiveness for USD. We proceed to present the new sampling scheme, followed by the overall fnccLearning loss.

**(1) Similarity-based Negative Sampling.** This scheme ensures the selection of genuinely dissimilar negative pairs via a similarity-based approach, evaluating seasonal and trend similarities for each pair and retaining the least similar pairs. The process involves three main steps:

Listing Possible Negative Pairs. Let U be the number of randomly sampled window groups, each of which contains V consecutive windows. A comprehensive set, \(\), is compiled encompassing all conceivable pair combinations from a total of U groups, resulting in \((-1)/2\) possible negative pairs.

Computing Similarities for Each Pair. For each pair \((i,j)\), we compute seasonal \((^{}_{i,j})\) and trend \((^{}_{i,j})\) similarities using dot products, capturing both seasonal patterns and evolving trends:

\[^{}_{i,j}=(_{i}^{})^{}_{ j}^{},^{}_{i,j}=(_{i}^{})^{ }_{j}^{}, \]

where \(_{i}^{}\) (_resp._\(_{i}^{}\)) represent the centroid (i.e., average embedding) of the seasonal embeddings \(^{}\) (_resp._ trend embeddings \(^{}\)) of all V consecutive windowed samples in the \(i\)-th window group.

The comprehensive similarity, \(^{}_{i,j}\), for each pair \((i,j)\) is finally computed as the product of the trend and seasonal similarities:

\[^{}_{i,j}=^{}_{i,j}^{}_{i,j} \]

Filtering True False Negative Pairs. The \([||]\) least similar pairs based on \(^{}_{i,j}\), with \(\) as a fraction parameter in \((0,1]\) (set to 0.5 by default), are selected to form the set \(^{}\) of negative pairs.

**(2) fnrcLearning Loss.** The following negative loss \(_{}\) aims to minimize the similarity between negative pairs from \(^{}\):

\[_{}=^{}|}_{(i,j) ^{}}-((-_{i}^{} _{j})), \]

where \(_{i}\) refers to the centroid of the final embedding \(\) of all V consecutive windowed samples in the \(i\)-th window group.

In Fig. 3, it is assumed that consecutive windowed samples from the same group (indicated by the same color) share a uniform state. Accordingly, the positive loss \(_{}\) aims to maximize the similarity between their embeddings:

\[_{}=1/_{k=0}^{-1}_{j=0}^{ -1}_{j=0,j<l}^{-1}-(((_{k,l})^ {}(_{k,j}))), \]

where \(_{k,l}\) represents the embedding of the \(i\)-th (\(0 i<\)) window from the \(k\)-th (\(0 k<\)) group, and \(=((-1)}{2}\) is used for normalization.

Ultimately, the fnrcLearning aims to enhance the similarity among positive pairs while diminishing the similarity among negative pairs. Thus, the fnrcLearning loss is defined as the sum of its positive and negative loss components:

\[_{}=_{}+_{} \]

The two hyperparameters, U and V, directly impact the loss computation, and the evaluation of their impact has been provided in the Appendix (Becker et al., 2017).

### Streaming USD with Adaptive Threshold

Upon obtaining the embedding of each windowed sample, a clustering algorithm, typically the Dirichlet Process Gaussian Mixture Model (DPGMM) (Becker et al., 2017), is traditionally employed. However, current methodologies often do not take into account the challenges of real-world online streaming. In such scenarios, states tend to persist, making clustering unnecessary for successive samples. To avoid redundant clustering, we propose the Adaptive Threshold Detection (adATD) mechanism that defers clustering until a new sample shows low similarity (controlled by an adaptive threshold) to the previous one. This ensures that clustering is invoked only when needed, enhancing USD efficiency. Fig. 4 illustrates adATD's computational savings compared to the classical 'Always Clustering Detection" (acD) mechanism.

The adATD process is detailed in Algorithm 1. Specifically, to gauge the temporal consistency between the current sample and its predecessor, we employ the dot product to quantify the similarity between their respective embeddings, \(z_{}\) and \(_{t}\):

\[(_{},_{t})=_{}^{}_{t}. \]

Then, this similarity value is compared to an adaptive threshold \(\). If the similarity is below this threshold, this indicates a likely state transition, triggering clustering to identify the new state \(s_{t}\).

\[s_{t}s_{}&\ (_{},_{t})\\ (_{t})& \]

The overall adATD process is outlined in Algorithm 1, where the threshold \(\) adapts to the context of streaming MTS. Its tuning is controlled by the computed similarity with two _scaling factors_\(_{i}\) and \(_{}\). When the similarity exceeds \(\), it implies that the state remains unchanged, prompting an increase in the threshold by the scaling factor \(_{i}\) (line 10 in Algorithm 1), as the growing probability of a state transition. Conversely, if the similarity falls below the threshold, we hypothesize a state transition, prompting a clustering

Figure 4. Saved overhead.

Figure 3. Overview of \(_{}\). Same-color windows belong to the same group. The blue and green groups denote a pair of _false negatives_; these are random selections labeled as negative pairs, although they share the same state.

operation for confirmation (line 12). Upon verification (line 13), \(\) is raised to act conservatively against the state transition (line 15). If the transition is deemed false, the threshold is decreased to counter overestimation from an excessively high threshold (line 16), and a higher value of the scaling factor \(_{r}\) is selected to ensure a rapid response to incorrect state transition hypotheses. Concurrently, a reduced value for \(_{l}\) is preferred, particularly considering the probability of an impending state transition rises. Parameter sensitivity study of \(_{r}\) and \(_{l}\) is reported in the Appendix (A).

In general, and CamTD seamlessly incorporates a cost-effective similarity metric tailored for streaming USD, significantly reducing redundant clustering operations. Empirical evaluation in Section 4.4 confirms the efficacy of adaTD.

```
1:MTS stream \(X\), threshold \(\), and scaling factors \(_{r}\) and \(_{l}\)
2:State \(s_{}\) for continuous time step \(t\) on \(X\)
3:\(W_{0} X_{0:P-1}\)\(\) sliding window sampling
4:\(z_{0}\) CompactEmbedding(\(W_{0}\))\(\) see Section 3.1
5:\(s_{0}\) Clustering(\(z_{0}\))\(\) DPGMM
6:\(z_{},z_{} z_{0},s_{0}\)\(\) initialize state
7:while obtaining updated \(_{t}\) from \(X\)do
8:\(W_{t} X_{t p+1:t}\)\(\) sliding window sampling
9:\(z_{t}\) CompactEmbedding(\(W_{t}\))
10:if\((z_{},_{t})\)then
11:\(s_{t} s_{}\)\(\) keep current state
12:\((1+_{l})\)\(\) increase \(\)
13:else
14:\(s_{t}\) Clustering(\(_{t}\))\(\) acquire new state
15:if\(s_{t} s_{}\)then\(\) verify state transition
16:\(z_{},z_{} z_{t},s_{t}\)\(\) update state
17:\((1+_{l})\)\(\) increase \(\)
18:else\((1-_{r})\)\(\) decrease \(\)
19:
20:endwhile
```

**Algorithm 1** Adaptive Threshold Detection (adaTD)

## 4. Experiments

### Experimental Settings

The entire codebase, datasets, hyperparameter settings, and instructions are available at [http://bit.ly/3rMFJVv](http://bit.ly/3rMFJVv). We trained the DL models on a server with an NVIDIA Quadro RTX 8000 GPU. For model inference, we employed an Intel Xeon Gold 5215 CPU (2.50GHz). Additionally, we carried out a case study of MCU deployment using an STM32H747 device (Ball et al., 2017). Further implementation details can be found in the Appendix (A).

**Baselines**. The following baselines are introduced. Baselines 1-3 employ the USD pipeline outlined in Section 2.1, while the remaining ones do not. (1) HVGH (Hoffmann et al., 2016) employs a variational autoencoder for encoding MTS windows and utilizes the Hierarchical Dirichlet Process (HDP) for clustering. (2) TICC (Hoffmann et al., 2016) uses a correlation network for encoding and adopts Toeplitz inverse covariance-based clustering. (3) Time2State(Tieleman et al., 2016) employs a Temporal Convolutional Network to encode MTS windows and utilizes the DPGMM for clustering (as does E2Usb). (4) Autofalt(Auer et al., 2017) applies the Minimum Description Length principle to segment the MTS and recursively models each segment with the Hidden Markov Model. (5) ClaSPTS_KMeans(Hoffmann et al., 2016) identifies change points in an MTS using multiple binary classifiers and employs KMeans(Krishnan et al., 2017) for segment clustering. (6) HDP_HSMM(Krishnan et al., 2017) is a Bayesian non-parametric extension of the Hidden Semi-Markov Model that uses HDP to estimate the number of states.

**Datasets**. For evaluations, we employ six datasets used in previous studies (Auer et al., 2017; Tieleman et al., 2016). These include one synthetic dataset, Synthetic(Tieleman et al., 2016), and five real-world datasets from diverse fields: MoCap, ActRecTut, PAMAP2, and UscHad track various human activities (Ball et al., 2017; Auer et al., 2017; Auer et al., 2017; Auer et al., 2017), and UcrSeg covers MTS from applications such as insect research, robotics, and energy(Auer et al., 2017). Among these, PAMAP2 exhibits the largest MTS lengths (ranging from 253k to 408k), while UcrSeg has the shortest (varying between 2k and 40k). UscHad features the largest number of states (12 in total), whereas UcrSeg has the fewest, with up to 3 states. Notably, UcrSeg stands out as a univariate times series dataset. A detailed description of the datasets is available in the Appendix (A).

**Metrics**. We use the Adjusted Rand Index (ARI) and Normalized Mutual Information (NMI) to assess the detection accuracy, as does prior research(Tieleman et al., 2016). ARI quantifies the instance-wise consistency between predicted and ground truth clusters by emphasizing clustering granularity, while NMI measures the shared information between ground truth and model clusterings. Furthermore, for evaluating detection efficiency, we present the Processing Time (PT), which records the time in seconds required by each method to process USD over a specific length of an MTS.

### Overall Comparison

An overall comparison between E2Usb and the baselines across the six datasets is listed in Table 1. The comparison is conducted on an Intel CPU over the STM device due to higher computational demands for the baselines that surpass the STM device's capacity. E2Usb consistently exhibits top-tier performance, achieving the best or near-best ARI and NMI scores. Notably, E2Usb also exhibits superior efficiency, as evidenced by consistently having the shortest processing time (PT)2. Among the baselines, Time2State manifests high performance at ARI, NMI, and PT for most datasets, positioning itself as the strongest competitor to E2Usb. Later in this section, a more detailed comparison between E2Usb and Time2State is provided. Autofalt excels in the MoCap dataset--notable for its shortest average MTS length--achieving the highest ARI and NMI scores, albeit with longer PT. However, it struggles on other datasets, particularly as it is unable to process the PAMAP2 dataset (marked as 'N/A' in Table 1), which has the longest average MTS length. ClaSPTS_KMeans achieves the top ARI and NMI scores on the univariate UcrSeg dataset. However, its performance is not competitive on other datasets of MTS.

Efficiency is crucial in real-world applications, especially with large sequences or variable data volumes. Thus, we conduct experiments over sequence lengths ranging from 40k to 400k, using the Synthetic dataset. As illustrated in Fig. 5 (a), E2Usb exhibits by far the lowest overall processing times throughout the tested range of sequence lengths, from 0.52s for 40k to 1.7s for 400k. This renders E2Usb ideally suited for real-world scenarios demanding swift responses and the capacity to adapt to varying data volumes.

**E2Ust****vs.****Time2State****.** We compare with the current SOTA model Time2State on the Synthetic dataset. Both methods utilize DL-based encoders and share the same clustering model, directing our focus primarily on the encoder component. As depicted in Table 2, E2Ust outperforms Time2State significantly in terms of computational and storage efficiency. To be precise, E2Ust requires roughly 30 times fewer total parameters, 120 times fewer trainable parameters, and reduces Multiply-ACCumulate (MACC) counts3 by an impressive 83.5 times. Moreover, its peak memory usage is about a factor of 229 times smaller. All in all, these statistics provide evidence of E2Ust's strength in resource-constrained scenarios.

### Component Study of E2Ust

This section evaluates the effectiveness and efficiency of E2Ust's proposed components. As shown in Section 4.2, E2Ust exhibits relatively high consistency between API and NMI scores. Due to space limit, we thus focus on reporting the ARI scores. The corresponding NMI results are available in the Appendix [(1)].

#### 4.3.1. Encoder

We compare our encoder (denoted as E2Usd) with two variants: one without fftCompress (E2Ust w/o FFT) and the other without Trend-Seasonal Decomposition of dEdEM (E2Usd w/o TSD). Besides, we include widely used MTS encoders LSTM [(17)] and TCN [(36)] for comparison. To maintain fairness, we only substitute the encoder component of E2Ust with these alternatives, while keeping all the other settings unchanged.

We first examine the efficiency of all encoders using the setting described in Section 4.2. Fig. 5 (b) reveals that E2Ust consistently offers top-tier temporal efficiency, starting at a 0.52s PT for a 40k sequence and only slightly increasing to 1.71s for a 400k sequence. When comparing E2Ust to its variants, we observe that introducing TSD marginally increases PT but significantly boosts accuracy (explained later). Conversely, integrating fftCompress leads to a notable reduction in PT. Further, LSTM and TCN increase the processing time substantially, with LSTM at 39.46s and TCN at 10.51s for processing a 400k sequence.

Referring to the accuracy results reported in Fig. 6 (a), E2Usd consistently outperforms its competitors across all datasets. Notably, the inclusion of fftCompress does not compromise accuracy, owing to its noise reduction capability, while TSD significantly enhances accuracy (see E2Ustb vs. E2Ustb w/o TSD). When juxtaposed with LSTM and TCN, E2Usd also demonstrates better performance. One of the distinct advantages of E2Usd is its ability to clearly extract valid frequency and period trend information, which is crucial for accurate clustering. While LSTM and TCN have their merits, their black-box nature makes it uncertain whether they can effectively capture this information as reliably as the trend-seasonal decomposition feature of E2Ust.

#### 4.3.2. FneckLearning Loss

We compare our proposed \(_{}\) with SOTA loss functions, including Temporal Neighborhood Coding (TNC) [(34)], Contrastive Predictive Coding (CPC) [(24)], and Latent State Encoding (LSE) [(36)]. Besides, we include a variant of \(_{}\), \(_{,}\), by constructing negative pairs using Samples' Embeddings (SE), rather than employing the centroids (i.e., average embeddings) of these groups. As emphasized in existing studies, these losses are encoder-agnostic [(24; 36; 34)]. To ensure fairness, we only replace the loss function of E2Ustb, following established research conventions [(36)]. This setup ensures that any performance differences stem solely from the inherent qualities of the loss functions themselves, rather than variations in encoder architectures.

Fig. 6 (b) shows that \(_{}\) consistently outperforms baselines on all datasets.A key factor contributing to this robust performance

    &  &  &  &  &  &  &  \\   & PT & ARI & NMI & PT & AH & NMI & PT & ARI & NMI & PT & AH & NMI & PT & AH & NMI & PT & AH & NMI & 737 \\  TIVGR [(27)] & 27.53 & 0.8809 & 0.1406 & 25.98 & 0.0500 & 0.1523 & 26.17 & 0.0881 & 0.2088 & 24.06 & 0.0023 & 0.074 & 25.42 & 0.0778 & 0.183 & 26.74 & 0.0638 & 0.1451 & 738 \\ HDP JESM [(31)] & 51.24 & 0.6491 & 0.7795 & 35.48 & 0.5590 & 0.7230 & 56.57 & 0.6644 & 0.6473 & 52.10 & 0.2882 & 0.5378 & 0.43 & 0.6789 & 0.438 & 0.1625 & 0.2574 & 779 \\ TTC [(20)] & 20.55 & 0.8422 & 0.7489 & 2.169 & 0.7218 & 0.7218 & 0.7322 & 22.41 & 0.7320 & 0.7246 & 29.44 & 0.3040 & 0.5955 & 21.51 & 0.3927 & 0.7043 & 19.29 & 0.2325 & 0.2158 & 798 \\ ATWPTA [(25)] & 73.80 & 0.8713 & 0.1077 & 7.569 & **0.8057** & **0.8289** & 22.960 & 0.6148 & **N.8** & **N.

is \(_{}\)'s effectiveness in minimizing false negatives, which leads to a cluster-friendly latent embedding space and enhances accuracy. Note that incorporating \(_{}\) does not compromise the efficiency, as trained DL models remain loss-agnostic.

### Efficacy of adaTD

We have empirically evaluated the performance of the Adaptive Threshold Detection (adaTD) algorithm when applied to the processing of streaming MTS data using the Synthetic dataset. We simulate streaming scenarios by continuously feeding MTS data to the model. Our assessment involved a comparison with two baseline detection schemes: "Always Clustering Detection" (acD) and "Static Threshold Detection" (STD), with sTD(\(\)) representing detection based on a fixed threshold value \(\).

As depicted in Fig. 7 (a) and (b), adaTD demonstrates a well-balanced trade-off between accuracy and efficiency. While acD achieves slightly higher accuracy, adaTD excels significantly in terms of efficiency. Furthermore, adaTD surpasses sTD across a range of static thresholds in terms of accuracy while requiring fewer clustering operations and maintaining competitive processing time. Notably, when increasing the static thresholds in sTD (i.e., \(\) ranging from 0.2 to 0.8), the detection accuracy improves and approaches that of adaTD at \(=0.8\), However, this increase in \(\) is accompanied by a decrease in efficiency, and even at \(=0.4\), it still falls short of matching the efficiency of adaTD. This observation highlights the superior adaptability of adaTD to variations in the similarity across different states, thus ensuring efficient and accurate detection.

### Case Study on Resource-limited MCU

To assess the viability of deploying EQuS on edge devices, we conducted experiments using a commodity STMS2HT47 MCU on the Synthetic dataset. Significantly, this device could not accommodate other baseline methods due to their high demands on computation and memory. The results reveal the following operational metrics during the operational phase. The Flash memory consumption amounts to **63.72 KB**, a mere 3.11% of the available 2 MB. In terms of RAM, it uses **73.27 KB**, representing a modest 7.16% of the overall 11 MB capacity, signifying efficient memory utilization. Moreover, the latency for processing each sample is **44.95 ms**, which equates to a detection frequency close to 20 Hz. When assuming a state persists for 10 sampling intervals, E2Usd is capable of handling streaming USD scenarios below 200 Hz, encompassing a wide range of practical applications. These results constitute strong evidence of the efficient resource utilization of E2USD, underscoring its applicability in resource-limited scenarios.

## 5. Related Work

**Unsupervised State Detection for MTS**. Broadly, USD for MTS can be categorized into two groups: those that follow the two-stage pipeline outlined in Section 2.1 and those that deviate from it. Research in the former category typically places its focus on the MTS embedding stage. A notable example is HVGH (Zhou et al., 2017), which employs a variational autoencoder for MTS encoding and the Hierarchical Dirichlet Process for clustering. Besides, TICC (Zhou et al., 2017) proposes a novel correlation network based on Toeplitz inverse covariance for MTS embedding. Recently, Time2State(Zhou et al., 2017) introduced contrastive learning to enhance the learning of the embedding module, but it faces challenges of computational overhead and false negative samples, highlighting the need for efficient models for resource-constrained devices like MCUs. In contrast to the two-stage pipeline, methods like AutoPdlat(Zhou et al., 2017), HDP_HSMM (Zhou et al., 2017), and ClapTS_KMeans(Zhou et al., 2017) adhere to a one-stage framework but encounter significant scalability and stability issues (see Fig. 5 (a)), rendering them unsuitable for online usage.

**Compact Unsupervised Representation Learning for MTS**. While numerous DL studies have explored unsupervised representation learning for MTS data, the majority of current research has concentrated on innovating intricate structures to enhance representation effectiveness (Zhou et al., 2017; Zhou et al., 2017; Zhou et al., 2017) but has not considered developing compact models for this purpose. There are also studies dedicated to compact DL models for MTS, offering techniques that can be adapted for unsupervised MTS representation learning. For example, the recent LightCTS (Zhou et al., 2017) introduces compact architectures and operators for MTS forecasting. Similarly, LightTS(Zhou et al., 2017) employs adaptive ensemble distillation to achieve a compact architecture for MTS classification. However, these studies tend to exclusively explore DL approaches, overlooking the traditional methods that have been developed over the years, which often exhibit a higher level of compactness compared to DL structures.

Recognizing this gap, EQuS aims to take into account the nature of MTS data, bridging traditional MTS representation techniques and DL methods. The method leverages an FFT-based approach to obtain a sparse representation of MTS data and then employs a decomposed dual-view embedding module, which integrates classical time series decomposition and a lightweight DL model to produce the final embedding. This offers a promising avenue for compact unsupervised MTS representation learning.

## 6. Conclusion and Future Work

In this study, we present E2USD, an efficient-yet-effective method for unsupervised MTS state detection. An extensive empirical study offers detailed insight into the properties of the components of E2USD, including fftCompress, ddem, and frecLearning. Overall, E2USD achieves state-of-the-art accuracy and efficiency in diverse scenarios. The incorporation of an Adaptive Threshold Detection (adaTD) enables a harmonious balance between accuracy and computational requirements, positioning E2USD as the best choice for streaming state detection. As we move forward, we aim to investigate the false positive cases in frecLearning and explore wider real-world applications for E2USD.

Figure 7. Comparative analysis of adaTD with acD and sTD.