# Graph Anomaly Detection with Bi-level Optimization

Anonymous Author(s)

###### Abstract.

Graph anomaly detection (GAD) has various applications in finance, healthcare, and security. Graph Neural Networks (GNNs) are now the primary method for GAD, treating it as a task of semi-supervised node classification (normal vs. anomalous). However, most traditional GNNs aggregate and average embeddings from all neighbors, without considering their labels, which can hinder detecting actual anomalies. To address this issue, previous methods try to selectively aggregate neighbors. However, the same selection strategy is applied regardless of normal and anomalous classes, which does not fully solve this issue. This study discovers that nodes with different classes yet similar neighbor label distributions (NLD) tend to have opposing loss curves, which we term it as "loss rivalry". By introducing Contextual Stochastic Block (CSBM) and defining _NLD distance_, we explain this phenomenon theoretically and propose a **Bi**-level **o**ptimization **G**raph **N**eural **N**etwork (BioGNN), based on these observations. In a nutshell, the lower level of BioGNN segregates nodes based on their classes and NLD, while the upper level trains the anomaly detector using separation outcomes. Our experiments demonstrate that BioGNN outperforms state-of-the-art methods and effectively mitigates "loss rivalry". Codes are available at [https://anonymous.4open.science/r/BioGNN-12B4](https://anonymous.4open.science/r/BioGNN-12B4).

Graph Neural Networks, Anomaly Detection, Bi-level Optimization +
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

## 1. Introduction

Graph anomaly detection (GAD) is a learning-to-detect task. The objective is to differentiate anomalies from normal ones, assuming that the anomalies are generated from a distinct distribution that diverges from the normal nodes (Song et al., 2018). As demonstrated by (Song et al., 2018), GAD has various real-world applications including detecting spam reviews in user-rating-product graphs (Song et al., 2018), finding misinformation and fake news in social networks (Song et al., 2018), and identifying fraud in financial transaction graphs (Song et al., 2018; Wang et al., 2018).

A primary method is to consider GAD as a _semi-supervised node classification_ problem, where the edges play a crucial role. By examining the edges, we can divide an ego node's neighbors into two groups: (1) homophilous neighbors that have the same labels as the ego node, and (2) heterophilous neighbors whose labels are different from the ego node's label. For instance, in the case of an anomaly ego node, its interactions with anomaly neighbors display homophily, while its anomaly-normal edges demonstrate heterophily. Both homophily and heterophily are prevalent in nature. In transaction networks, fraudsters have heterophilous connections with their customers, while their connections with accomplices are homophilous.

From the standpoint of neighbor relationships, we can briefly describe the primary graph neural networks (GNNs)-based GAD solutions and their limitations as follows:

* Early studies (Song et al., 2018; Wang et al., 2018) aggregate over all neighbors without considering the impact of homophily and heterophily. That is, the representation of each node blindly aggregate the information from all neighbors, without discriminating the neighbor relationships. However, this approach can be disadvantageous to GAD as anomalies are more likely to be hidden among a large number of normal neighbors. Blindly aggregating information can dilute the suspiciousness of anomalies with normal signals, making them less discernible (Song et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018).
* To address the above-mentioned problem, recent studies (Bogu et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) draw inspiration from graph signal processing (GSP). They suggest that a low-pass filter may not be optimal for all graphs. Instead, they manipulate eigenvalues of the normalized graph Laplacian to amplify some frequency information and weaken others. However, these studies optimize node representations as a whole, without addressing differences in their distribution regarding neighbor labels. For instance, as shown in Figure 1, a normal node shares the same neighbors as an anomaly. Our analysis in SS3.2 reveals that nodes of different classes with the same neighbors retain rather different frequency components. While emphasizing a single frequency band can improve learning for some nodes, it can hinder the learning of others.

Thus, it is crucial to understand the impact of neighbor label distribution (NLD) on detector behavior. We introduce and reveal the phenomenon of "loss rivalry". Surprisingly, we observe opposite loss curves for anomalies and normal nodes holding similar NLDs.

Figure 1. The ego normal node and anomaly (marked in red circle) have comparable neighborhood label distributions (NLD). The probability of neighbor labels being 0 or 1 is denoted by \(p_{c}\) and \(q_{c}\), where the subscript represents the class label. For instance, \(p_{1}\) denotes the probability of a normal neighbor for anomalies.

These are separately highlighted around the maxima and minima of the curves in Figure 2. Our analysis emphasizes the importance of using distinct aggregation mechanisms for nodes with different classes but similar NLD.

Based on this finding, we propose a bi-level optimization model in SS4, named BioGNN. Specifically, it consists of two key components. The first component is a mask generator that separates nodes into mutually exclusive sets based on their classes and NLD. The second component contains two well-designed GNN encoders that adopt different mechanisms to learn node representations separately. In SS3.1, we define the NLD distance based on the Contextual Stochastic Block Model (CSBM) and verify its direct proportion to representation expressiveness. Due to the proved superiority of adaptive filters in heterophilic graphs(Chen et al., 2017; Wang et al., 2018; Wang et al., 2018), we approach the problem in the spectral domain. Specifically, we first explain the feasibility of acquiring NLD given the ego graph of a node in the spectral domain in SS3.2. Then, we distill the NLD of nodes from filter performance through the bi-level optimization process, as spectral filter performance depends on the concentration of spectral label distribution (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). In a nutshell, BioGNN distinguishes nodes with similar NLD but likely belong to different classes and feeds them into separate filters to prevent "loss rivalry". Our code is available at [https://anonymous.4open.science/r/BioGNN-12B4](https://anonymous.4open.science/r/BioGNN-12B4).

**Our contributions.** (1) We reveal the "loss rivalry" phenomenon, where nodes belonging to different classes but with similar NLD tend to have opposite loss curves, which can negatively impact model convergence. (2) We provide theoretical explanations regarding the importance of NLD and the benefits of using polynomial-based spectral filtering methods to capture the NLD of nodes. (3) We propose a novel bi-level optimization framework to address the problem, and the effectiveness of the proposed method is verified through experiments.

## 2. Preliminaries and Notations

In GAD, anomalous and normal nodes can be modeled as an attributed graph \(=(,,)\), where \(\) represents the set of anomalous and normal nodes, \(\) denotes edges, and \(\) is the attribute matrix. The objective of GAD is to identify anomalous nodes by learning from the attributes and structure of the graph. In SS3.1, we will discuss the impact of NLD on GAD and demonstrate the superiority of spectral filtering in addressing this issue. Therefore, we introduce basic knowledge of graph spectral filtering in this section.

**Graph-based Anomaly Detection.** A primary approach for GAD is to frame it as a semi-supervised node classification task (Wang et al., 2018). The goal is to train a predictive GNN model \(g\) that achieves minimal error in approaching the ground truth \(_{test}\) for unobserved nodes \(_{test}\) given observed nodes \(_{train}\), where \(_{train}_{test}=\) and \(_{train}_{test}=\):

\[g(,_{train})}_{test}. \]

Note that GAD is an imbalanced classification problem, which often results in similar NLD for normal nodes and anomalies: anomalies in the graph are rare, hence both normal nodes and anomalies are surrounded by numerous normal nodes.

**Graph Spectral Filtering.** Let \(\) be the adjacency matrix, and \(\) be the graph Laplacian, which can be expressed as \(-\) or as \(-^{-1/2}^{-1/2}\) (symmetric normalized), where \(\) is the identity matrix, and \(\) is the diagonal degree matrix. \(\) is positive semi-definite and symmetric, so it has an eigen-decomposition \(=^{T}\), where \(=\{_{1},,_{N}\}\) are eigenvalues, and \(=\{_{1},,_{N}\}\) are corresponding unit eigenvectors (Wang et al., 2018). Assuming \(=[_{1},,_{N}]\) is a graph signal, we call the spectrum \(^{T}\) the graph Fourier transform of the signal \(\)(Wang et al., 2018; Wang et al., 2018). In graph signal processing (GSP), the frequency is associated with \(\). Therefore, the goal of spectral methods is to identify a response function \(g()\) on \(\) to learn the graph representation \(\)(Wang et al., 2018):

\[=g()=[g()( ^{T})]=g()^{T}. \]

## 3. Theoretical Analysis

In this section, we introduce the Contextual Stochastic Block Model (CSBM), a widely used model for describing node feature formation. Based on CSBM, we define the NLD distance and verify its direct proportion to representation expressiveness. Furthermore, because adaptive filters have been shown to perform better in heterophilic graphs, we explore the feasibility of expressing NLD in the spectral domain to facilitate further study in later sections.

Figure 2. Illustration of the ‘loss rivalry’ phenomenon in YelpChi and Amazon Datasets with BWGNN (Wang et al., 2018). From the same-color circles around the maxima and minima, we observe that the two loss curves in the same dataset are opposite along the epochs. The curves are plotted

### Impact of NLD on Node classification

GNNs are widely used to learn node representations in networks, as they can capture graph topological and structural information effectively. However, GNNs distinguish nodes by averaging the node features of their neighborhood (Shi et al., 2018). Therefore, it is intuitive that the neighbor label distribution has a significant impact on GNN performance. To analyze NLD from a graph generation perspective, we introduce the Contextual Stochastic Block Model (CSBM) (Shi et al., 2018). CSBM is a random graph generative model commonly used to measure the expressiveness of GNNs (Shi et al., 2018).

**CSBM**. The Contextual Stochastic Block Model (CSBM) makes the following assumptions for an attributed graph \(}\): (1) For a central node \(u\) with label \(c\{0,1\}\), the labels of its neighbors are independently sampled from a fixed distribution \(_{c}(p_{c})\). \(p_{c}\) denotes the sampling probability of class \(c\), and the sampling process continues until the number of neighbors matches the degree of node \(u\). In this work, we refer to the distribution \(_{c}\) as the **neighborhood label distribution (NLD)**. (2) Anomalies and normal nodes have distinct node feature distributions, namely \(_{c}\).

For simplicity, we define the NLD distance as follows:

**Definition 3.1** (Neighborhood Label Distribution Distance): Given a graph \(}\) with label vector \(\), the neighborhood label distribution distance between nodes \(u\) and \(v\) is:

\[d(u,v)=dis(_{u_{c}}(u),_{_{c}}(v)), \]

where \(dis(,)\) measures the difference between distribution vectors, such as cosine distance or Euclidean distance; \(u_{c}\) and \(v_{c}\) denote the class of nodes \(u\) and \(v\), respectively.

In this work, we focus on the binary GAD classification problem, hence \(_{c}=\{_{0}=[p_{0},q_{0}],_{1}=[p_{1},q_{1}]\}\), where the symbol definitions are shown in Table 1. Furthermore, following previous works (Gulraj et al., 2017; Chen et al., 2018; Li et al., 2018), we suppose that \(_{c}\) are two Gaussian distributions of \(n\) variables, \(i.e.,x_{0} N_{n}(_{0},^{2}),x_{1} N_{n}(_{1},^{2})\). This problem setting leads us to the following proposition, which indicates the expressive power of GNNs.

**Proposition 3.1**: Given a graph \(}=(,},\{_{c}\},\{_{c}\})\), the distance between the means of the class-wise hidden representations is proportional to their NLD distance.

**Remark.** The detailed proof can be found in Appendix A.1. This proposition shows that the expressive power of the representation depends on the neighborhood label distribution. Specifically, for nodes \(u\) and \(v\) in different classes, a vanilla 2-layer GCN has the following distance between their hidden representations:

\[||_{u}-_{v}||_{2}=}{2}||_{1}-_{0}||_{2}, \]

where \(_{u}\) and \(_{0}\) are the mean values of the learned representations of nodes \(u\) and \(v\). Similarly, for spectral methods, whose general polynomial approximation form can be written as \(_{k}_{k}^{k}\)(Shi et al., 2018), we can achieve a much larger NLD distance with a second-order polynomial:

\[||_{u}-_{v}||_{2}=[1+}+}{2}] ||_{1}-_{0}||_{2}. \]

The larger the distance \(||_{u}-_{v}||_{2}\), the more expressive the representation and the better capability of the downstream linear detector. From (4) and (5), we observe two things: (1) the minimum value of \(||_{u}-_{v}||_{2}\) is achieved when \(d(u,v) 0\); (2) using second-order polynomial graph filtering can improve the ability to distinguish between nodes, especially when the NLD of nodes from different classes are similar. This finding aligns with previous research (Shi et al., 2018; Li et al., 2018) in this area.

### NLD in the Spectral Domain

The NLD of anomalous and normal nodes in four benchmark datasets is statistically reported in Table 2. We observe that the NLD for nodes from different classes are similar, especially in YelpChi and Amazon datasets. Our analysis justifies the need to filter out anomalies sharing similar neighborhood labels with normal nodes, so that the distribution of the remaining anomalies can be distinguished from that of normal nodes. Proposition 3.1 suggests that spectral methods are more effective. Therefore, we aim to address the problem in the spectral domain. To begin with, we express NLD in the spectral domain by bridging the gap between it and frequency. Specifically, we fragment a graph into a set of ego-graphs (Shi et al., 2018) and define the spectral label distribution as follows:

**Definition 3.2**: (Spectral Label Energy Distribution): Given an ego node \(u\) and its one-hop neighbor set \(_{u}\) with size \(N\), the spectral label energy distribution at \(_{k}\) is:

\[f_{k}(,)=_{k}^{2}/_{n=1}^{N}_{k}^{2}, \]

where \(f\) is a probability distribution with \(_{k=1}^{N}f_{k}=1\), \(\) is the Laplacian matrix of the ego-graph, and (\(x\)) denotes the ego-graph spectrum of the one-hot label vector \(\). Since \(_{k}=_{k}^{T}\), \(f_{k}(,)\) measures the weight of \(_{k}\) in y, a larger \(f_{k}\) indicates that the spectral label distribution concentrates more on \(_{k}\). With Definition 3.2, we now show the relationship between \(f(,)\) and NLD.

**Proposition 3.2**: For a binary classification problem, the expectation of the spectral label energy distribution \([f(,)]\) is positively associated with the NLD of the node. Specifically:

\[[f(,)]=}|(1-p_{0})}{N}&y=0,\\ }| p_{1}}{N}&y=1. \]

**Remark.** The detailed proof can be found in Appendix A.2. Proposition 3.2 indicates that capturing the difference in spectral label distribution is equivalent to measuring the similarity between NLDs. Furthermore, the proposition elucidates that different nodes with

   Symbol & Definition (Probability of) \\  \(p_{1}\) & normal neighbor for anomalies \\  \(q_{1}\) & anomaly neighbor for anomalies \\  \(p_{0}\) & normal neighbor for normal nodes \\  \(q_{0}\) & anomaly neighbor for normal nodes \\   

Table 1. NLD Symbol Definition.

similar NLD retain rather different frequency components. Based on this finding, separating nodes whose spectral label distributions are different could bring two benefits: (1) separate nodes in the same class but have different NLDs; (2) separate nodes in different classes but have similar NLDs. Both of these benefits alleviate the "loss rivalry" phenomenon and help with the convergence of GNNs.

### Validation on Real-World Graphs

To verify the correctness of our theoretical findings, we report the F1-macro and AUC performance of some general methods (triangle marker) (Zhou et al., 2017; Wang et al., 2018; Wang et al., 2018) and some polynomial spectral methods (Zhou et al., 2017; Wang et al., 2018; Wang et al., 2018) (star marker) in Figure 3. We make two observations: (1) As shown in Table 2, the NLD distance between the two classes is 0.0762 and 0.6462 for YelpChi and T-Finance, respectively. From Figure 3, we observe that most methods achieve better results on T-Finance than on YelpChi, demonstrating the importance of NLD. Moreover, the performance gap between models on YelpChi and Amazon is much larger than that on T-Finance. This suggests that we can achieve decent performance with less powerful models on datasets with larger NLD distances. Our finding supports the notion that NLD can influence the expressive power of the GNN model, and separating nodes with specific NLDs can improve the performance of the GNN model. (2) Spectral methods outperform spatial methods by a large margin. These tailored heterophilic filters further support our argument for the superiority of addressing the problem in the spectral domain.

## 4. Methodology

Guided by the analysis in SS3.1, we advocate for the necessity of treating nodes with distinct spectral label distributions separately. In this section, we introduce our **bi-**level optimization graph neural network BioGNN. To begin with, we introduce the learning objectives in SS4.1 and present the parameterization process in SS4.2. In SS4.4, we validate the effectiveness of the framework on golden-separated graphs.

### The Learning Objectives

To start with, we introduce Lemma 4.1 which is widely agreed upon in the literature (Zhou et al., 2017; Wang et al., 2018; Wang et al., 2018):

**Lemma 4.1** The prediction performance of a spectral filter is better when the spectral label energy distribution concentrates more on the pass band of the filter.

Building on Lemma 4.1, we could identify nodes according to the performance of different spectral filters through bi-level optimization. As shown in Figure 4, our learning objective is twofold: (1) Optimize the encoders \(\{(),_{1}(),_{2}()\}\) to maximize the probability of correctly classifying nodes separated by \((),(2)\) Optimize the encoder \(()\) which predicts the NLD of nodes and separate nodes to two sets. We set all the encoders as MLP with learnable parameters. Concretely, the learning objective of BioGNN is defined as follows:

\[_{_{1},,M_{1}}((g_{1}() _{1}(M_{1})),)\] \[+_{_{2},,M_{2}}((g_{2}() _{2}(M_{2})),),\] \[s.t. M_{1}+M_{2}=, \]

where \(M_{1}\) and \(M_{2}\) are hard masks given by learnable encoder \(()\), \(\) is an all-one vector, \(g_{1}(L)\) and \(g_{2}(L)\) are spectral filters, and \(\) denotes the element-wise multiplication.

### Instantiation of BioGNN

Given the two-fold objective, we propose to parameterize the encoder \(()\) and \(\{(),_{1}(),_{2}()\}\).

_Parameterizing \(()\)._ The encoder \(()\) serves as a separator that predicts the NLD of nodes and feeds nodes into different branches of filters. Consequently, to obtain informative input for \(\), we employ a label-wise message passing layer (Golov et al., 2013) which aggregates the labeled neighbors of the nodes label-wise. Concretely, for node \(u\), the aggregated feature \(h_{u,c}\) for class \(c\):

\[h_{u,c}=_{l,c}(u)|}_{_{l,c}(u)}x_ {0}, \]

where \(_{l,c}(u)\) is the set of neighbors labeled with \(c\). When there are no labeled neighbors belonging to class \(c\), we assign a zero

   &  &  \\  & \# Nodes & \# Edges & \# Features & \(p_{0}\) & \(q_{0}\) & \(p_{1}\) & \(q_{1}\) & Distance \\  YelpChi & 11,944 & 4,398,392 & 25 & 0.8683 & 0.1317 & 0.8144 & 0.1856 & 0.0762 \\ Amazon & 45,954 & 3,846,979 & 32 & 0.9766 & 0.0234 & 0.9254 & 0.0746 & 0.0724 \\ T-Finance & 39,357 & 21,222,543 & 10 & 0.9850 & 0.0150 & 0.5280 & 0.4720 & 0.6462 \\ T-Social & 5,781,065 & 73,105,508 & 10 & 0.7634 & 0.2366 & 0.9161 & 0.0839 & 0.2159 \\  

Table 2. Summary of the dataset statistics and the neighbor label distributions.

Figure 3. The influence of NLD on model performance.

embedding to \(h_{u,}\). Then we set

\[M_{1}(u)=*{argmax}(_{}([X_{u};h_{u,0};h_{u,1 }])). \]

To ensure smoothed and well-defined gradients \(\), we apply a straight-through (ST) gradient estimator (Brocker, 1991) to make the model differentiable. Note that BioGNN is trained in an iterative fashion, the encoders \(\{(),_{1}(),_{2}()\}\) are fixed as \(\{^{*}(),_{1}^{*}(),_{2}^{*}()\}\), the objective function in this phase is:

\[_{M_{1}}(^{*}(g_{1}()_{1}^{* }(M_{1})),)\] \[+_{M_{2}}(^{*}(g_{2}()_{2}^{ *}(M_{2})),)\] \[s.t. M_{1}+M_{2}=. \]

_Parameterizing \(\{(),_{1}(),_{2}()\}\)_. These three encoders serve as a predictor that assigns labels to input nodes. As we aim to distinguish between different spectral label distributions, which are closely related to the performance of filters with corresponding band-pass, we adopt low-pass and high-pass filters as \(g_{1}(L)\) and \(g_{2}(L)\), respectively. Here, we choose to use two branches and leave the multi-branch framework for future work. Therefore, the functions of \(M_{1}\) and \(M_{2}\) become the masking of nodes with high-frequency and low-frequency ego-graphs, respectively. In this iterative training phase, we freeze the masks as \(M_{1}^{*}\) and \(1-M_{1}^{*}\), and set the objective function as:

\[_{,_{1}}((g_{1}() _{1}(M_{1}^{*})),)\] \[+_{,_{2}}((g_{2}() _{2}((1-M_{1}^{*}))),). \]

A similar training process has also been used in graph contrastive learning (Zhu et al., 2017). For the choice of \(g_{1}()\) and \(g_{2}()\), we adopt Bernstein polynomial-based filters (Zhu et al., 2017; Wang et al., 2018) for their convenience to decompose low-pass and high-pass filters:

\[g()=U_{,}()U^{T}=/2)^{}(I-/2)^{}}{2_{0}^{1}^{-1}(1-t)^{-1} t}, \]

where \(_{,}\) is the standard beta distribution parameterized by \(\) and \(\). When \( 0\), we acquire \(g()\) as a low-pass filter; similarly, \(g()\) acts as a high-pass filter when \( 0\). For the choices of \(\) and \(\) on the specific benchmark and more training details, please refer to Appendix B.1 and B.2.

### Initialization of BioGNN

To embrace a more stable process of the bi-level optimization, we initialize the encoders before iterative training.

**Initialization of \(()\)**. \(()\) is initialized in a supervised fashion, where the supervision signal is obtained by counting the labeled inter-class neighbors:

\[Y_{sep}(u)=round(_{L}(u)|}_{0_{L}(u)}| \{y_{u} y_{0}\}|), \]

then the cross-entropy is minimized:

\[_{}-[Y_{sep} log(())+(1-Y_{sep}) log(1- ())]. \]

Note that in our experiments, although the supervision signal are calculated with ego-graphs, the input data is a **complete graph** rather than ego-graphs extracted from a larger graph. Each node in the complete graph connects directly to all other nodes, ensuring that all interactions are considered during the learning process. As nodes with high-frequency ego-graph are rare, to shield the separator from predicting all nodes as low-frequency ego nodes, we regularize the ratio of two sets of nodes by enforcing the following constraint: we treat \(_{sep}\) as the optimal known mask, and one term \(_{sep}-()\) is added to the objective. The final objective is:

\[_{}-[Y_{sep} log(())+(1-Y_{sep}) log(1- ())]+r(_{sep}-()). \]

**Initialization of \(\{(),_{1}(),_{2}()\}\).** In this phase, we treat \(Y_{sep}\) as the optimal known mask:

\[_{,_{1}}((g_{1}() _{1}(Y_{sep}),)\] \[+_{,_{2}}((g_{2}() _{2}((1-Y_{sep})),). \]
can alleviate the problem and boost the performance of GAD. We discovered that the training order is significant in achieving better performance. Training nodes with high-frequency ego-graphs before those with low-frequency ones leads to better results. One possible reason for this is the shared linear classifier \(\) between the two branches. Embeddings learned from the high-pass filter are noisier, and a classifier that performs well on noisy embeddings would most likely perform well on the whole dataset (LeCun et al., 2017). We consider this to be an intriguing discovery, yet leaving a comprehensive theoretical examination for future research.

## 5. Experiment

In this section, we conduct experiments on four benchmarks and report the results of our models as well as some state-of-the-art baselines to demonstrate the effectiveness of BioGNN.

### Experimental Setup

**Datasets.** Following previous works (Zhu et al., 2018; Zhang et al., 2019), we conduct experiments on four datasets introduced in Table 2. For more details about the datasets, please refer to Appendix B.3.

**Baselines.** Our baselines can be roughly categorized into three groups. The first group includes general methods, such as **MLP**, **GCN**(LeCun et al., 2017), **GAT**(LeCun et al., 2017), **ChebyNet**(Zhu et al., 2018), **GWNN**(Wang et al., 2019), and **JKNet**(Wang et al., 2019). As our focus is GAD, the second group considers tailored GAD methods including **CAREGNN**(Zhu et al., 2018), **PCGNN**(Wang et al., 2019), **GDN**(Wang et al., 2019), and **BWGNN**(Wang et al., 2019). The third group includes methods that consider neighbor labels, such as **H2GCN**(Wang et al., 2019), **GPRGNN**(Chen et al., 2019), and **MixHop**(Chen et al., 2019):

* **GCN**(Wang et al., 2019): GCN is a traditional graph convolutional network in spectral space.
* **GAT**(LeCun et al., 2017): GAT leverages masked self-attentional layers to weight the neighbors.
* **ChebyNet**(Zhu et al., 2018): ChebyNet generalizes CNN to graph data in the context of spectral graph theory.
* **GWNN**(Wang et al., 2019)1: GWNN leverages graph wavelet transform to address the shortcomings of spectral graph CNN methods that depend on graph Fourier transform. * **JKNet**(Wang et al., 2019): The jumping-knowledge network which concatenates or max-pooling the hidden representations. * **Care-GNN**(Zhu et al., 2018)2: Care-GNN is a camouflage-resistant graph neural network that adaptively samples neighbors according to the feature similarity, and the optimal sampling ratio is found through an RL module. * **H2GCN**(Wang et al., 2019)4: H2GCN is a tailored heterophily GNN which identifies three useful designs. * **GDN**(Wang et al., 2019)6: GDN deals with heterophily by leveraging constraints on original node features. * **MixHop**(Chen et al., 2019)7: MixHop repeatedly mixes feature representations of neighbors at various distances to learn relationships. * **GPRGNN**(Chen et al., 2019)8: GPR-GNN learns a polynomial filter by directly performing gradient descent on the polynomial coefficients. 

### Performance Comparison

The main results are reported in Table 3. Note that we search for the best threshold to achieve the best F1-macro in validation for all methods. In general, BioGNN achieves the best F1-macro score in all datasets, empirically verifying that it has a larger distance between predictions and the decision boundary, benefiting from measuring the NLD distance. For AUC, BioGNN did not achieve the best score in T-Social. We suppose the reason is that T-social has a complex frequency composition since the best performance is achieved when the frequency order is high according to BWGNN (Wang et al., 2019). We believe this issue could be alleviated if multi-branch filters are adopted, which we leave for future work. Furthermore, some methods could achieve high AUC while maintaining a low F1-Macro, indicating that the instances can be distinguished but hold tightly in the space.

Figure 5. Golden separated loss and real loss curves on YelpChi.

In such cases, it is hard to identify a classification threshold, which we consider unstable.

H2GCN, MixHop, and GPRGNN are three state-of-the-art spectral heterophilous GNNs that shed light on the relationship between the ego node and neighbor labels. We observe that they consistently outperform other groups of methods, including some tailored GAD methods. We ascribe this large performance gap to two reasons: (1) the harmfulness of heterophily where vast normal neighborhoods attenuate the suspiciousness of the anomalies; (2) the superiority of spectral filters to distinguish nodes with different NLD. However, they optimize the node representations as a whole, while BioGNN outperforms these methods, especially in FI-Macro, where the improvement ranges from 2.7% to 25.8%. This supports our analysis that different class nodes with similar NLD should be treated separately to alleviate "loss rivalry". Furthermore, among the tailored GNN methods (CAREGNN, PCGNN, GDN, BWGNN, and BioGNN), BWGNN and BioGNN are polynomial-based filters that perform better than others, further suggesting that spectral filtering is more promising in GAD.

In several datasets, MLP outperforms some GNN-based methods, indicating that blindly mixing neighbors can sometimes degrade the prediction performance. Therefore, structural information should be used with care, especially when the neighborhood label distributions for nodes are complex.

### Analysis of BioGNN

In this section, we take a closer look in BioGNN. We first verify the smoothness of the BioGNN loss curve to demonstrate its effectiveness in alleviating "loss rivalry". Then we plot the distribution of the separated nodes to elucidate that our model can successfully discriminate nodes with different NLD and set them apart. Making it more clear, we visualize some high-frequency ego-graphs.

#### Loss Rivalry Addressing

To answer the question of whether BioGNN can alleviate the "loss rivalry", we plot the training loss of BioGNN in Figure 4(b). Similar to Section 4.4, two separate sets of nodes are trained in a specific order: high-frequency nodes are trained first, followed by low-frequency nodes. Comparing Figure 2, 4(a), and 4(b), we find that the smoothness of BioGNN's training curve lies between golden-separate and mixed training, indicating that the new framework is effective in alleviating "loss rivalry" and improves the overall performance of GAD.

#### Distribution of the separated nodes

The core of BioGNN is node separation. To further validate its effectiveness, we report the

   Dataset &  &  &  &  \\ Metric & F1-Macro & AUC & F1-Macro & AUC & F1-Macro & AUC & F1-Macro & AUC \\  MLP & 0.4614 & 0.7366 & 0.9010 & 0.9082 & 0.4883 & 0.8609 & 0.4406 & 0.4923 \\ GCN & 0.5157 & 0.5413 & 0.5098 & 0.5083 & 0.5254 & 0.8203 & 0.6550 & 0.7012 \\ GAT & 0.4614 & 0.5459 & 0.5675 & 0.7731 & 0.8816 & 0.9388 & 0.4921 & 0.4923 \\ ChebyNet & 0.4608 & 0.6216 & 0.8070 & 0.9187 & 0.8017 & 0.8001 & & OOM \\ GWNN & 0.4608 & 0.6246 & 0.4822 & 0.9319 & 0.4883 & 0.9670 & & OOM \\ JKNet & 0.5805 & 0.7736 & 0.8270 & 0.8970 & 0.8971 & 0.9554 & 0.4923 & 0.7226 \\  CAREGNN & 0.5015 & 0.7300 & 0.6313 & 0.8832 & 0.7261 & 0.9105 & 0.4868 & 0.7939 \\ PCGNN & 0.6925 & 0.8118 & 0.8367 & 0.9555 & 0.4462 & 0.9200 & 0.4536 & 0.8917 \\ GDN & 0.7545 & 0.8904 & 0.9068 & 0.9709 & 0.8474 & 0.9462 & 0.7401 & 0.9287 \\ BWGNN & 0.7568 & **0.8967** & 0.9204 & 0.9706 & 0.8899 & 0.9599 & 0.7494 & 0.9275 \\  H2GCN & 0.6575 & 0.8406 & 0.9213 & 0.9693 & 0.8824 & 0.9553 & OOM & OOM \\ MixHop & 0.6534 & 0.8796 & 0.8093 & 0.9723 & 0.4880 & 0.9569 & 0.6471 & 0.9597 \\ GPRGNN & 0.6423 & 0.8355 & 0.8059 & 0.9358 & 0.8507 & 0.9462 & 0.5976 & **0.9622** \\ BioGNN & **0.7606** & 0.8947 & **0.9462** & **0.9766** & **0.9059** & **0.9670** & **0.8140** & 0.9325 \\   

Table 3. Performance Results. The best results are in boldface, and the 2nd-best are underlined.

Figure 6. The NLD distance between two separated sets of nodes.

empirical histogram of the NLD in four benchmarks in Figure 6. The x-axis represents the edge homophily, which explicitly represents the NLD around the ego node. The y-axis denotes the density, and the distribution curves are shown in dashed lines. From Figure 6, we observe that the two histograms seldom overlap, and the mean of two curves maintains a separable distance, demonstrating that BioGNN successfully sets the nodes apart.

**Visualization.** To show the results in an intuitive way, we report the ego-graph of some nodes in Figure 7. These nodes are assigned to the high-pass filter by \(()\). As observed from the figure where color denotes the class of the nodes, the ego node (red-circled) has more inter-class neighbors compared to the nodes assigned to the low-pass filter. This finding provides support for Equation 7 and verifies the effectiveness of our novel framework. More visualizations are in Appendix C.

**Time complexity analysis.** The time complexity of BioGNN is \(O(C||)\), where \(C\) represents a constant and \(||\) denotes the number of edges in the graph. This is due to the fact that the BernNet-based filter is a polynomial function that can be computed recursively, as explained in (Gardner et al., 2017).

## 6. Related Work

In this section, we introduce some static GAD networks and polynomial-based spectral GNNs.

### Static Graph Anomaly Detection

On static attributed graphs, GNN-based semi-supervised learning methods are widely adopted. For example, GraphUCB (Grabh et al., 2019) adopts contextual multi-armed bandit technology, and transforms graph anomaly detection into a decision-making problem; DCI (Zhu et al., 2019) decouples representation learning and classification with the self-supervised learning task. Recent methods realize the necessity of leveraging multi-relation graphs into GAD. EdGars (Zhu et al., 2019) and GraphConsis (Zhu et al., 2019) construct a single homo-graph with multiple relations. Likewise, Semi-GNN (Zhu et al., 2019), CARE-GNN (Grabh et al., 2019), and PC-GNN (Zhu et al., 2019) construct multiple homo-graphs based on node relations. In addition, some works discover that heterophily should be addressed properly in GAD. Semi-GNN and IHGAT (Grabh et al., 2019) employ hierarchical attention mechanisms for interpretable prediction, while based on camouflage behaviors and imbalanced problems, CARE-GNN, PC-GNN, and AO-GNN (Zhu et al., 2019) prune edges adaptively according to neighbor distribution. GDN (Grabh et al., 2019) and H2 -FDetector (Zhu et al., 2019) adopt different strategies for anomalies and normal nodes.

### Graph Spectral Filtering

Spectral GNNs simulate filters with different passbands in the spectral domain, enabling GNNs to work on both homophilic and heterophilic graphs (Zhu et al., 2019). GPRGNN (Grabh et al., 2019) adaptively learns the Generalized PageRank weights, regardless of whether the node labels are homophilic or heterophilic. FSGNN (Zhu et al., 2019) designs a feature selection graph neural network. FAGCN (Zhu et al., 2019) adaptively fuses different signals in the process of message passing by employing a self-gating mechanism. BernNet (Zhu et al., 2019) expresses the filtering operation with Bernstein polynomials. BWGNN (Gardner et al., 2017) observes the "right-shift" phenomenon and designs a band-pass filter to aggregate different frequency signals simultaneously. AdaGNN (Grabh et al., 2019) captures the varying importance of different frequency components to alleviate over-smoothing problem. AMNet (Bahdanau et al., 2015) aims to capture both low-frequency and high-frequency signals, and adaptively combine signals of different frequencies. GHRN (Grabh et al., 2019) design an edge indicator to distinguish homophilous and heterophilous edges.

## 7. Limitation and Conclusion

**Limitation.** Although we propose a novel network that treats nodes separately, it has some limitations. Our work only separates the nodes into two sets, and we hope to extend it to more fine-grained multi-branch neural networks in the future. Furthermore, our theoretical result largely relies on CSBM's assumptions; hence our model may fail in some cases where the graph generation process doesn't follow these assumptions.

**Conclusion.** This work starts with "loss rivalry", expressing the phenomenon that some nodes tend to have opposite loss curves from others. We argue that it is caused by the mixed training of different class nodes with similar NLD. Furthermore, we discover that spectral filters are superior in addressing the problem. To this end, we propose BioGNN, which essentially discriminates nodes that share similar NLD but are likely to be in different classes and feeds them into different filters to prevent "loss rivalry".