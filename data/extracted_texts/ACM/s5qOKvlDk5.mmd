# Prompt-enhanced Federated Content Representation Learning

Prompt-enhanced Federated Content Representation Learning

for Cross-domain Recommendation

Anonymous Author(s)

###### Abstract.

Cross-domain Recommendation (CDR) as one of the effective techniques in alleviating the data sparsity issues has been widely studied in recent years. However, previous works may cause domain privacy leakage since they necessitate the aggregation of diverse domain data into a centralized server during the training process. Though several studies have conducted privacy preserving CDR via Federated Learning (FL), they still have the following limitations: 1) They need to upload users' personal information to the central server, posing the risk of leaking user privacy. 2) Existing federated methods mainly rely on atomic item IDs to represent items, which prevents them from modeling items in a unified feature space, increasing the challenge of knowledge transfer among domains. 3) They are all based on the premise of knowing overlapped users between domains, which proves impractical in real-world applications. To address the above limitations, we focus on Privacy-preserving Cross-domain Recommendation (PCDR) and propose PFRC as our solution. For Limitation 1, we develop a FL schema by exclusively utilizing users' interactions with local clients and devising a Local Differential Privacy (LDP) method for gradient encryption. For Limitation 2, we model items in a universal feature space by their description texts. For Limitation 3, we initially learn federated content representations, harnessing the generality of natural language to establish bridges between domains. Subsequently, we craft two prompt fine-tuning strategies to tailor the pre-trained model to the target domain. Extensive experiments conducted on two real-world datasets consistently demonstrate the superiority of our PFRC method compared to the SOTA approaches.

Cross-domain Recommendation, Content Representation, Federated Learning +
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

## 1. Introduction

Cross-domain Recommendation (CDR) is a key area of recommender systems that aims to improve recommendation quality by leveraging knowledge from multiple domains. It relies on shared patterns and latent correlations, extending recommendations beyond single domains. Techniques such as domain adaptation and transfer learning play a vital role in information migration , and have achieved great success in attaining distinguished recommendations. However, a primary limitation of existing CDR methods is that they may leak domain privacy because of the centralized training schema. Direct data aggregation proves infeasible due to safeguards protecting trade secrets, exemplified by regulations such as the General Data Protection Regulation (GDPR) . Furthermore, prior studies grapple with another limitation by aligning domains through direct utilization of users' identity information, under the assumption of overlap. This approach is also unfeasible in many real-world CDR scenarios, chiefly due to user privacy concerns. For instance, users registering for personal services on one platform typically harbor reservations about exposing their identity to other platforms.

Recently, several studies have been focused on conducting privacy preserving in CDR tasks . For instance, Mai et al.  propose a federated GNN-based recommender system with random projection to prevent de-anonymization attacks, and a ternary quantization strategy to avoid user privacy leakage. However, previous methods solving PCDR still suffer from the following limitations: 1) They need to upload users' personal information, such as user embedding or user-related model parameters, to the central server. Despite the encryption of user information, there remains a potential for divulging users' private data, as achieving a balance between information encryption and method efficacy is challenging. 2) Existing federated methods predominantly rely on atomic IDs for item modeling, making it challenging to learn unified item representations. The uniqueness of items in different domains and the non-ID nature of data distributions pose difficulties in modeling items within a unified feature space. This limitation impedes the acquisition of universal item representations and hampers knowledge transfer across domains. 3) These methods are premised on the assumption of knowing overlapping users across domains, enabling domain alignment and CDR. However, this assumption carries the risk of serious privacy breaches, as it necessitates the use of users' identity information. Furthermore, identifying common users between domains can expose individuals to de-anonymization attacks, as organizations may exploit this information to infer user preferences in other domains.

To address these limitations, we target Domain-level Privacy-preserving Cross-domain Sequential Recommendation (DPCSR) and propose a Prompt-enhanced Federated Content Representation (PFRC) paradigm as our solution. We consider the sequential characteristic in PCDR since it is a common practice to organize users' behaviors into sequences. Specifically, to mitigate **Limitation 1**, we propose a federated content representation learning schema, treating domains as clients, with a central server responsible for parameter updates (FedAvg (Zhu et al., 2017) is applied). In PPCR, the gradients related to content representations can be shared across domains (the LDP strategy is also applied), while the user-related gradients are strictly prohibited to prevent privacy leakage. To deal with **Limitation 2**, we model items as language representations (i.e., _semantic ID_) by the associated description text of them so as to learn universal item representations, where the natural language plays the role of a general semantic bridging different domains. Compared with _atomic item ID_, _semantic ID_ enables us to represent different domain items in the same semantic space and simultaneously avoids the huge memory and storage footprint caused by the huge number of items in modern recommender systems. To tackle **Limitation 3**, we initially pre-train the federated content representations to fuse non-overlapped domains by leveraging the generality of natural languages, where a global _code embedding table_ under a universal semantic space is learned. Subsequently, to adapt the pre-learned knowledge to specific domains, we fine-tune the pre-trained content representations and model parameters with two kinds of prompting strategies.

The main contributions of this work can be summarized as:

* We target DPCSR and solve it by proposing PPCR, where a federated content representation learning schema and a prompt-enhanced fine-tuning paradigm are developed for domain transfer under the non-overlapping scenario.
* We model items in different domains as vector-quantified representations on the basis of their associated description texts, so as to unify them in the same semantic space.
* We develop a federated content representation learning framework for PCDR in the non-overlapping scenario by leveraging the generality of natural languages.
* We design two prompting strategies, namely full prompting, and light prompting, to adapt the pre-learned domain knowledge to the target domain.
* We conduct extensive experiments on two real-world datasets, and the experimental results consistently demonstrate the superiority of PPCR compared with other SOTA methods.

## 2. Related Work

### Federated Cross-domain Recommendation

Existing Federated Cross-domain Recommendation (FCDR) studies can be categorized into Cross-Silo Federated Recommendation (CSFR) and Cross-User Federated Recommendation (CUFR) methods according to the nature of clients. CSFR refers to FCDR among organizations and focus on preserving domain-level privacy (Bengio et al., 2017; Chen et al., 2018; Chen et al., 2019; Wang et al., 2019; Wang et al., 2019). For example, Wan et al. (Wang et al., 2019) devise a privacy-preserving double distillation framework (FedPDD) for CSFR to solve the limited overlapping user issue, which exploits a double distillation strategy to learn both explicit and implicit knowledge, and an offline training schema to prevent privacy leakage. In CUFR, each user is served as a client and tends to conduct user-level privacy preserving by FCDR (Wang et al., 2019; Wang et al., 2019). For instance, Yan et al. (Yan et al., 2019) train a general recommendation model on each user's personal device to avoid the leakage of user privacy and devise an embedding transformation mechanism on the server side for knowledge transfer. However, existing studies mainly rely on all or part of the overlapped users for CDR, and cannot be applied to scenarios in which users are non-overlapped across domains. Our solution falls into the CSFR category and tends to solve DPCSR by proposing a FL framework with federated content representations.

### Recommendation with Item Text

Recently, several recommendation methods have been focused on leveraging the content information to represent items to explore the generality of natural languages. Depending on whether text representation is directly used for recommendations, existing studies can be categorized into text representation (Huang et al., 2019; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019) and code representation-based methods (Huang et al., 2019; Wang et al., 2019). For example, Hou et al. (Huang et al., 2019) design a text representation-based method in universal sequence representation approach (UnisRec) by utilizing a Pre-trained Language Models (PLM), where the semantic item encoding is obtained and participates in sentence modeling. But this kind of method is too strict in binding item text and its representation, causing the model to pay much attention to the text features. To address this, Hou et al. (Huang et al., 2019) first convert the text of the item into a series of distinct indices called "item codes", and then learn these Vector-Quantized (VO) item representations by engaging with these codes. However, the above methods are all based on a centralized training schema, and none of them consider the generality of contents in helping FCDR.

### Recommendation with Prompt tuning

Prompt tuning, initially introduced in the field of NLP, involves designing specific input and output formats to guide PLM in performing specific tasks. Recent researchers have also used prompt tuning to solve the cold-start (Wang et al., 2019; Wang et al., 2019; Wang et al., 2019) and cross-domain (Wu et al., 2019; Wang et al., 2019) issues in recommender systems. For example, Wu et al. (Wu et al., 2019) devise a personalized prompt-based recommendation framework for cold-start recommendation, which builds a soft prompt via a prompt generator based on user profiles, and enables a sufficient training via prompt-oriented contrastive learning. Wang et al. (Wang et al., 2019) propose a prompt-enhanced paradigm for multi-target CDR, where a unified recommendation model is first pre-trained using data from all the domains, then the prompt tuning process is conducted to capture the distinctions among various domains and users. Though the prompt tuning methods have been widely studied, they are mainly utilized for domain adaption or zero-shot issues, and few of them focus on solving the FCDR task, which is one of the main purposes of this work.

## 3. Methodologies

### Preliminaries

Suppose we have two domains A and B. Let \(^{A}=\{u_{1}^{A},u_{2}^{A},,u_{m_{A}}^{A}\}\) and \(^{B}=\{u_{1}^{B},u_{2}^{B},,u_{m_{B}}^{B}\}\) be the user sets, \(=\{A_{1},A_{2},,A_{M_{A}}\}\) and \(=\{B_{1},B_{2},,B_{M_{B}}\}\) be the item sets in domains A and B, respectively, where \(m_{A}\), \(m_{B}\), \(M_{A}\) and \(M_{B}\) are the corresponding user number and item number in each domain. Each item \(A_{i}\) (or \(B_{j}\)) is identified by a unique item ID and associated with a description text (such as the product title, introduction, and brand).

Users can express their preferences by interacting with specific items. Take user \(u_{i}^{A}\) as an example, we record her sequential behaviors on items in domain A as \(_{i}^{A}=\{A_{1},A_{2},,A_{j},\}\). The description text of item \(A_{i}\) is denoted as \(_{i}=\{w_{i},w_{2},,w_{c}\}\), where \(w_{j}\) is the content word in natural languages, and \(c\) is the truncated length of item text. To represent items in a unified feature space, we share the language vocabulary in both domains. Compared with other ID-based traditional recommendation methods (Hendricks et al., 2014; Li et al., 2016; Wang et al., 2017), we only take item IDs as auxiliary information, and they will not be used for domain knowledge transfer. Instead, we represent items by deriving generalizable ID-agnostic representations from their description texts. Moreover, although users may simultaneously interact with items in multiple domains or platforms, we do not align them between domains, since it may compromise users' privacy. That is, we assume users and items are entirely non-overlapped in our setting. In this work, we tend to preserve domain privacy in a federated training schema and transfer domain knowledge by a pre-train & prompt learning paradigm with the help of the generalized content representations.

### Overview of PFCR

**Motivation.** To conduct PCD, we resort to FL by viewing domains as clients and local data can only be utilized within clients. Moreover, to prevent attackers from inferring user identities from uploaded public information, we only share the item-related gradients with the protection of LDP. To transfer domain information under the non-overlapping and privacy-preserving scenario, we first embed domain information into the distributed item representation in the pre-training stage, and then adapt the prompts in the fine-tuning stage, so as to meet the specific distribution of the target domain.

The system architecture of PFCR in the pre-training stage is shown in Fig. 1, which is a FL process that consists of Vector-Quantified Item Representation (VQIR), Sequence Encoder (SE), and Federated Content Representation (FCR). VQIR aims at representing items in different domains in the same semantic space. It is the foundation of modeling users' cross-domain personal interests. By unifying domains into the same language feature space, we are able to effectively integrate domain information (see Section 3.3). But as we focus on PCD, we do not follow the traditional centralized training schema. On the contrary, we devise a framework with the help of the federated content representations (see Section 3.4). In SE, we apply a transformer-style neural network to learn users' sequential interests in each client (see Section 3.4.1). The overall framework of PFCR in the fine-tuning stage is shown in Fig. 2. Two prompting strategies, i.e., full prompting and light prompting, are developed in this phase. In the full prompting strategy (as shown in Fig. 2 (a)), we explore the domain prompts and user prompts for fine-tuning, while in the light prompt learning (as shown in Fig. 2 (b)), only the domain prompts are reserved. In our design, the domain prompts are shared by all the users in the same domain, and the user prompts are related to specific users.

### Vector-Quantified Item Representation

As natural language is a universal way to represent items in different domains, it is intuitive to leverage the generality of natural language texts to bridge domain gaps, since similar items will have similar contents even if they are not in the same domain. To model the common semantic information across different domains, we unify items in the same semantic feature space and take the learned text encodings via PLMs as universal item representations. But such a "_text_\(\)_representation_" paradigm (Hendricks et al., 2014; Li et al., 2016) is too tight in binding

Figure 1. The system architecture of PFCR in the pre-training stage. In PFCR, the _code embedding table_ is pre-learned federally. The orange color within it denotes the index embeddings shared by all the clients, and the blue color indicates the index embedding that only appears in the current client.

item text and their representations, making the recommender might overemphasize the effect of text features. Moreover, the text encodings from different domains cannot naturally align in a unified semantic space. To address the above challenges, we exploit a "_text_ - \(\)_ode - \(\)presentation_" schema (Hang et al., 2017; Zhang et al., 2017), which first maps item text into a vector of discrete indices (called _item code_), and then employs these indices to lookup the _code embedding table_ for deriving item representations. But different from existing studies that learn it in a centralized server, we tend to develop a FL schema for privacy-preserving purposes (the details can be seen in Section 3.4). In this work, we deem the description texts of items as public data and the users' interaction behaviors as privacy information.

#### 3.3.1. Discrete Item Code Leaning

To obtain the discrete codes of items, we first encode their description texts into text encodings via PLMs to leverage the generality of natural language text. Then, we map the text encodings into discrete codes based on the optimized product quantization method (the Product Quantization (PQ) (Hang et al., 2017) algorithm is utilized).

For item text encoding, we utilize the widely used BERT model (Devlin et al., 2019) as the text encoder (the Huggingface model is exploited1). Specifically, for a given item \(i\), we first insert a [CLS] token at the beginning of its description text \(_{i}=\{w_{1},,w_{c}\}\) and subsequently feed it into BERT to obtain its textual encoding:

\[_{i}=(\{[CLS];w_{1};w_{c}\}]), \]

where \([:]\) represents the concatenation operation, \(_{i}^{d_{W}}\) is the representation of the given text, which is defined as the final hidden vector of the special input token [CLS].

To map the text encoding \(_{i}\) to discrete codes, the PQ method is applied. PQ defines \(D\) sets of vectors, within which each vector corresponds to \(M_{c}\) centroid embeddings with dimension \(d_{W}/D\). Let \(_{k,j}^{d_{W}/D}\) be the \(j\)-th centroid embedding for the \(k\)-th vector set. In the PQ method, the text encoding vector \(_{i}\) is first split into \(D\) sub-vectors \(_{i}=[_{i,1};_{i,D}]\). Then, for the \(k\)-th sub-vector of \(_{i}\), PQ selects the index of its nearest centroid embedding from the corresponding set to generate the discrete code of \(_{i,j}\). The selected index of \(_{i,k}\) can be defined as:

\[_{i,k}=_{j}\|_{i,k}-_{k,j}\|^{2}\{1,2,,M_{c }\}, \]

where \(_{i,k}\) is \(k\)-th dimension of the discrete code vector for item \(i\).

#### 3.3.2. Item Code Representation

Given the discrete item codes (e.g., \((c_{i1},,c_{i,D})\) of item \(i\)), we can derive item representations by directly performing lookup operation on a _code embedding table_ with average pooling.

**Code Embedding Table.** Let \(^{D M_{c} d_{W}}\) be the global _code embedding table_, where \(d_{0}\) denotes the dimension of the item embedding. There are \(D\) code embedding matrices within \(\), and each of them \(^{(k)}^{M_{c} d_{W}}\) is shared by the discrete codes of all the items (even if they are not in the same domain). This characteristic allows us to align different domains and embed the common domain information into item embeddings. Moreover, as we share the code embedding among all domains, we can represent items in the same code space, which forms the prerequisite for our subsequent FL endeavors. It is worth noting that we need to let the _code embedding table_ in all the clients and servers have the same initialization value to ensure they have the same update direction.

**Lookup Operation.** By performing the lookup operation on \(\), the code embeddings for item \(i\) can be denoted as \(\{_{1,_{i1}},,_{D,_{i,D}}\}\), where \(_{k,_{i,k}}^{d_{W}}\) is the \(c_{i,k}\)-th row of matrix \(^{(k)}\).

Then, we can arrive at the final item representation of item \(i\) by conducting the average pooling on the code embeddings:

\[_{i}=([_{1,_{i1}};;_{D,_{i,D}}]), \]

where \(_{i}^{d_{W}}\) is the final item representation, and \(():^{D d_{W}}^{d_{W}}\) is the mean pooling method on the \(D\) dimension.

### Federated Content Representation

Since we focus on the PCDR task, we do not follow the traditional centralized training schema for item representation learning. Instead, we resort to FL, where domains are viewed as clients, and the privacy of user data is strictly utilized only on local clients. To this end, a federated content representation learning paradigm is devised, which involves local training, uploading gradient, and gradient aggregation and synchronization.

#### 3.4.1. Local Training

To learn users' sequential interests, we first feed items' VQ representations \(\{_{1},_{2},,_{l},,_{n}\}\) to a transformer-style sequence encoder.

**Sequence Encoder.** It mainly consists of a multi-head self-attention layer (called MH) and a position-aware feed-forward neural network (called FFN) to model items' sequential dependencies. More formally, for each input item representation \(_{i}\), we first add it to the corresponding position embedding \(_{j}\) (\(j\) is the position of item \(i\) in the sequence).

\[_{j}^{0}=_{i}+_{j}. \]

Then, we feed \(_{j}^{0}\) to MH (Shen et al., 2017) and FFN (Shen et al., 2017) to conduct non-linear transformations. The encoding process is defined as follows:

\[^{l}=[_{0}^{l};;_{n}^{l}], \]

\[^{l+1}=((^{l})),l\{1,2,,L\}, \]

where \(^{l}^{n d_{W}}\) denotes the hidden representation of each sequence in the \(l\)-th layer, \(L\) is the total layer number. We take the hidden state \(_{i}^{A}=_{n}^{L}\) at the \(n\)-th position as the sequence representation (\(_{A}\) is the input sequence in domain A).

**Optimization Objective.** Given the input sequence \(_{i}^{A}\), we define the next item prediction probability in domain A as follows:

\[P(A_{i+1}|_{i}^{A})=(_{i}^{A}_{}), \]

where \(_{}\) is the representation of all the items in domain A.

We exploit the cross-entropy loss on all the domains as the local training objective:

\[L_{A}=-_{A}|}_{_{i}^{A}_{A}} P(A_{i+ 1}|_{i}^{A}), \]

where \(_{A}\) is the training set in domain A.

#### 3.4.2. Gradient Uploading and the Encryption Strategy

To enhance the local training process and enable the item representations to embed cross-domain information, we need to leverage the user preference information, such as users' interactions, in other domains. But as the privacy leakage concerns, we do not directly upload the user interaction data to the central server. Instead, we only upload model parameters' gradients for aggregation (the details can seen in Section 3.4.3). Then, the accumulated gradients will be passed back to clients to let them engage in the local training. In this distributed learning way, we can embed domain knowledge into pre-trained models, with which we can further conduct CDR.

However, as user-related gradients also have privacy leakage issues (attackers can obtain privacy features from model parameters or gradient through attach methods such as DLG (Shen et al., 2017)),we only upload item-related gradients in each client (i.e., the code _embedding table_) to the server for accumulation, and prohibit all the user-related parameters. The uploaded gradients of the _code embedding table_ are represented by \(g_{A}\) and \(g_{B}\) in domains \(A\) and \(B\), respectively.

**Encryption Strategy.** To prevent malicious actors from intercepting these gradients and then using them to infer item information, we further devise a LDP encryption method on these gradients. Traditional LDP methods (Han et al., 2017; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) mainly add Gaussian or Laplace noise to the gradients. But as they may significantly distort the gradient direction, few recent methods exploit the quantization (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) or randomized response (Chen et al., 2019; Wang et al., 2018) methods. However, they can only solve the problem of third-party attacks or untrustworthy partners, since only a single LDP solution is applied. To simultaneously consider both attack types, we propose a composite LDP method on \(g_{A}\) and \(g_{B}\), which consist of a quantization and a randomized response component.

_Quantization._ This component aims to map gradients to a finite number of discrete values to avoid third-party attacks, as the attackers cannot restore the gradient values without knowing the mapping method, even if they can intercept the uploaded gradients. For each element of \(g_{i}^{A}\) in client A (take domain A as an example), we first clip \(g_{i}^{A}\) to a certain range \([-,]\) according to its threshold:

\[g_{i}^{A}=(g_{i}^{A},-,), \]

where \(\) is the gradient threshold. Note that, we do not follow previous methods (Wang et al., 2018; Wang et al., 2018) that clip gradients to \(\), as gradients usually have richer values, and put them into a smaller interval will cause many gradient values to go to 0.

Then, we scale \(g_{i}^{A}\) by the following mapping function:

\[g_{i}^{A}=(^{A}+}{s}), \]

where round(\(\)) means rounding each element to its nearest integer. \(q_{i}^{A}\{0,1,,b-1\}\) is the quantized gradient element. \(s=\) is the scaling factor, \(b=2^{k}\) is the number of quantization buckets, \(k\) is the number of quantization bits.

_Randomized response._ To enable our method can also protect against attacks from untrusted partners, the randomized response method (Wang et al., 2018) is further applied. This method achieves protection by introducing more uncertainty via randomly flipping or displacing the quantized gradient so that its value can be randomly perturbed. The random noise adding process is defined as:

\[r_{i}^{A}=(q_{i}^{A}+) b, \]

where noise \((0,b)\) is the random noise that follows an Uniform distribution.

Then, to proceed with the flip operation, we generate a flip variable following the Bernoulli distribution. It determines whether the data should undergo the flip operation:

\[c_{i}^{A}(p),p=+1}{e^{}+2},q= +2}, \]

where \(p\) and \(q=1-p\) are the probability parameters, \(\) is the privacy parameter. Once \(c_{i}^{A}\) is obtained, the permutation operation is performed on the noisy gradient \(r_{i}^{A}\):

\[r_{i}^{A}=(r_{i}^{A}-c_{i}^{A}) b. \]

#### 3.4.3. Gradient Aggregation and Synchronization

To learn the global _code embedding table_ from distributed clients, the gradient aggregation operation is then applied on the server side. However, due to the LDP encryption method, the aggregated gradients may encompass inherent uncertainties and deviation. Therefore, the server needs to undertake further rectification, decode, and reconcile steps on the encrypted gradients.

To ensure each element in the gradient can be processed in the same way, we start by flattening the gradients from both clients, followed by a concatenation operation:

\[=(r^{A})(r^{B}), \]

where \(^{n_{e} d_{f}}\) is the flattened gradient, \(n_{e}\) is the client number, \(d_{f}=M_{e} d_{f}\) is the dimension of \(\), \(\) denotes the concatenation operation. \(^{A}=\{_{1}^{A},_{2}^{A},,_{M_{A}}^{A}\}\) and \(^{B}=\{_{1}^{B},_{2}^{B},,_{M_{B}}^{B}\}\) are the gradients of \(\) in clients A and B, respectively. Subsequently, to perform scaling and normalization operations during the denoising and recovery process of the gradients, we construct a constant matrix \(^{c}\) as follows:

\[^{c}=[(p-q)]_{i=1}^{n_{e} d_{f}}, \]

where \(^{c}^{2 d_{f}}\) is a constant matrix with the same shape as \(\). \(p\) and \(q\) are employed to introduce the probabilities of inversion and permutation operations. By multiplying this constant matrix with the gradients after random response, each element is effectively subjected to an inverse operation during the decoding and correction process, consequently restoring the denoised gradient information.

We utilize FedAVG to aggregate the gradients, and determine the aggregation weight based on the ratio of the client data to the total data:

\[w_{i}=}{_{i=1}^{m_{i}}m_{i}}. \]

The rectification and aggregation process can then be expressed as:

\[=_{i=1}^{m_{e}}_{i}_{i}^{c} w_{i}. \]

After that, we reshape the gradients back to their original dimensions and then decode them back to their previous range:

\[=() s-. \]Finally, we utilize the decoded gradients to update the embedding within the server, which can be defined as:

\[E E-. \]

We synchronize this updated embedding to all clients, followed by repeating the aforementioned training process until the pre-training phase convergences. Note that during the initialization phase of FL, we've ensured that the initial random parameters of the embedding on the server match those of the clients. As a result, the updated embedding is valid at this point.

### Domain-adaptive Prompting Paradigm

To retrieve the domain knowledge from pre-trained models, we further fine-tune the federated content representation through domain-adaptive prompts, i.e., full prompt and light prompt learning, to enhance CDR.

#### 3.5.1. The Full Prompting Schema

In this prompting paradigm, we exploit two kinds of soft prompts, i.e., domain prompt and user prompt, for domain adaption.

**Domain Prompt.** This is to extract the common preferences shared by all the users within each domain, which consists of the prompt context words and a domain prompt encoder. Suppose we have \(d_{W}\) context words in the domain prompt \(P_{}^{d_{W} d_{V}}\) (we set \(d_{W}\) as the batch size for the convenience of calculation). Then, we encode it by a multi-head attention layer (called MA). But different from the vanilla self-attention, we take the sequence embedding \(\) obtained by the pre-trained model as the queries. This encoding process can be defined as:

\[_{i}=(_{}_{i}^{Q},_ {}_{i}^{K},_{}_{i}^{V}), \]

where \(n_{h}\) denotes the number of heads, \(_{i}^{Q},_{i}^{V},_{i}^{V}^{d_{V}  d_{V}/n_{h}}\), and \(^{O}^{d_{W} d_{V}}\) are learnable parameters, \(_{d}\) is the representation of \(d_{W}\) sequences.

**User Prompt.** This is to model users' personal preferences in each domain. We represent it by the sequences of original item IDs, since they are unique to each user and can as supplementary information to user preferences (they are specific to each domain, and do not need to have cross-domain information). Then, we encode it (\(_{}\)) by a transformer-style user prompt encoder(called UPE), which is defined as:

\[P_{}=(). \]

To this end, we concatenate domain prompt, user prompt, and sequence embedding, followed by a fully connected work, to make predictions:

\[_{}=^{C}[P_{}_{}_{S}]+b^{C}, \]

where \(^{C}:^{3d_{W}}^{d_{W}}\) represents the weight matrix.

#### 3.5.2. The Light Prompting Paradigm

To reduce the storage and computational costs in the fine-tuning process, we further consider a light prompting paradigm by removing the item-level features (i.e., the user prompt) and concatenation layer. The final sequence representation in this schema is:

\[_{}=_{}+_{S}. \]

**Learning Objective.** For both paradigms, we minimize the following cross-entropy loss for learning optimal prompts (take domain A as an example):

\[L_{}=-_{A}|}_{_{i}^{A}_{A}} P (A_{i+1}|_{i}^{A}), \]

where \(P(A_{i+1}|_{i}^{A})=(_{}(_{ })_{i})\) is the probability of predicting the next item \(A_{i+1}\). The two-stage optimization algorithm is shown in Appendix B.

## 4. Experimental Setup

### Research Questions

We fully evaluate our PFCR method by answering the following research questions:

1. How does PFCR perform compared with the state-of-the-art baselines?
2. How do the key components of PFCR, i.e., Vector-Quantified Item Representation (VQIR), Federated Content Representation (FCR), and Domain-adaptive Prompting (DP), contribute to the performance of PFCR?
3. How do different federated learning algorithms affect PFCR?
4. What are the impacts of the key hyper-parameters on the performance of PFCR?

### Datasets and Evaluation Protocols

We conduct experiments on two pairs of domains, i.e., "_Office-Arts_", and "_OnlineRetail-Pantry_", in Amazon2 and OnlineRetail3 to evaluate our PFCR method. To conduct CDR, we select the Office and Arts domains in Amazon as our learning objective (i.e., the "_Office-Arts_" dataset). To further evaluate PFCR on the cross-platform scenario, we select Pantry as one domain and the data from OnlineRetail

Figure 2. The system architecture of PFCR in the prompt-tuning stage. The components with yellow color are the prompts to be fine-tuned.

[MISSING_PAGE_FAIL:7]

benefit of conducting CDR, and our method can effectively transfer domain knowledge in the non-overlapping scenario. 3) The cross-domain methods (i.e., CCDR and RecGURU) do not show impressive improvement over the single domains methods, denoting that non-overlapping CDR is a challenging task since there is no direct information to align domains. Our methods outperform CDR methods, demonstrating the usefulness of the contents in modeling the generality of domain information.

## 6. Model Analysis

### Ablation Studies (RQ2)

To show the importance of different model components, we further conduct ablation studies by comparing with the following variations of PPCR: 1) PPCR-VFD: This method excludes the VQIR, FCR, and DP components from PPCR. 2) PPCR-FD: This method removes the FCR and DP modules from PPCR. 3) PPCR-D: This method detaches the DP module from PPCR. 4) PPCR-F: This model does not use the FCR component in PPCR. The results of the ablation studies are shown in Table 2, from which we can observe that: 1) PPCR still has the best performance over its other variations. The gap between PPCR and PPCR-VFD indicates the importance of the components (i.e., VQIR, FCR, and DP) in learning the federated content presentations. 2) PPCR-VF performs better than PPCR-VFD, indicating the effectiveness of encoding items by semantic contents. 3) PPCR has a better performance than PPCR-D, showing the usefulness of conducting prompt learning in the DPCSR task. 4) The gap between PPCR and PPCR-F, indicating the importance of our FL strategy.

### Impacts of Different Federated Learning Strategies (RQ3)

To show the impact of different FL strategies, we further compare FedAVG (utilized in our PPCR method) with the following methods: FedProx (Zhou et al., 2017), Scaffold (Wang et al., 2018), and PopCode. PopCode is the method that only aggregates the code gradients with high frequencies in clients (i.e., popular codes). The experimental results are shown in Table 3, from which we can conclude that: 1) FedAVG outperforms FedProx and Scaffold, demonstrating the importance of simultaneously learning the common and domain-specific features of the code embeddings. Paying more attention to the common knowledge across domains (i.e., FedProx and Scaffold) cannot achieve better results. 2) FedAVG performs better than PopCode, indicating the benefit of aggregating all the codes. Only updating a subset of gradients in each client will distort the learning direction of the _code embedding table_, which results in sub-optimal results. 3) Beyond performance, we notice that FedProx and Scaffold upload more parameters than FedAVG, and have heavier computational costs.

### Impact of Hyper-parameters (RQ4)

We explore the impact of three key hyper-parameters, i.e., the number of communication round \(t\) in FL, the privacy parameter \(\) in LDP, and the number of quantified buckets \(b\) in the quantization component of LDP. The experimental results are reported in Fig 3. Due to the space limitation, we only show the results on _'OnlineRetail-Pantry'_, and similar results are achieved on _'Office-Arts_'. From Fig 3, we can observe that: 1) \(t\) significantly impacts the performance of PPCR. The value of \(t\) is not that the bigger the better. A higher value of \(t\) will result in excessive updates, leading to the performance decline. 2) PPCR does not have a definite chaining trend as \(\) changes since the randomized response method is different from Laplace or Gaussian noise that is added on all the gradients, while ours are only on part of them. 3) A proper value of \(b\) is important to PPCR. An over-big value of \(b\) will map the gradients to an overlarge range and will result in a performance decline. Similarly, an over-small \(b\) will limit the gradients in an over-small range, and let the gradients have less representational ability.

## 7. Conclusions

In this work, we target DPCSR and propose a PPCR paradigm as our solution with a two-stage training schema (the pre-training and prompt tuning stages). The pre-training phase is dedicated to achieving domain fusion and privacy preservation by harnessing the generality inherent in natural languages. Within this phase, we introduce a federated content representation learning method. The prompt tuning phase is geared towards adapting the pre-learned domain knowledge to the target domain, thereby enhancing CDR. To achieve this, we have devised two prompt learning techniques: the full prompt and light prompt learning methodologies. The experimental results on two real-world datasets demonstrate the superiority of our PPCR method and the effectiveness of our federated content representation learning in solving the non-overlapped PCDR tasks.

Figure 3. Impact of the hyper-parameters \(t,e,b\) on the _OnlineRetail-Pantry dataset_.

    &  &  \\ 
**Methods** &  &  &  &  \\    & @10 & @50 & @10 & @50 & @10 & @50 & @10 & @50 \\    & FullProx (Zhou et al., 2017) & 0.1197 & 0.1912 & 0.0731 & 0.0907 & 0.1132 & **0.2179** & 0.0353 & 0.0862 \\  & Scaffold (Wang et al., 2018) & 0.1146 & 0.1393 & 0.0742 & 0.0938 & 0.1115 & 0.3098 & **0.0036** & **0.0907** \\  & Footnote code & 0.1209 & 0.1937 & 0.0742 & 0.0900 & 0.1132 & 0.2533 & 0.0267 & 0.0350 \\    & **FdAvG** & **0.213** & **0.1943** & **0.0771** & **0.0900** & **0.1146** & 0.2776 & 0.0638 & 0.0862 \\   

Table 3. Results of different federated learning strategies on the _Office-Arts_ dataset.