# Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM

Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM

Anonymous Author(s)

###### Abstract.

In the literature, prior studies on Video Anomaly Detection (VAD) mainly focus on detecting whether each video frame is abnormal or not in the video, which largely ignore the structured video semantic information (i.e., what, when, and where does the abnormal event happen), though this structured information could be employed to construct a more precise and efficient system for abnormal event monitoring and retrieval. With this in mind, we propose a new chat-paradigm **M**ulti-scene **V**ideo **A**honormal **E**vent Extraction and Localization (M-VAE) task, aiming to extract the abnormal event quadruples (i.e., subject, event type, object, scene) and localize such event. Further, this paper believes that this new task faces two key challenges, i.e., global-local spatial modeling and global-local spatial balancing. To this end, this paper proposes a Global-local Spatial-sensitive Large Language Model (LLM) named Sherlock, i.e., acting like _Sherlock Holmes_ to track down the criminal events, for his M-VAE task. Specifically, this model designs a Global-local Spatial-enhanced MoE (GSM) module and a Spatial Imbalance Regulator (SIR) to address the above two challenges respectively. Extensive experiments on our constructed M-VAE instruction dataset show the significant advantages of Sherlock over several advanced Video-LLMs. This justifies the importance of global-local spatial information for the M-VAE task and the effectiveness of Sherlock in capturing such information.

Multi-scene Video, Video Abnormal Event, Spatial-sensitive LLM 1

## 1. Introduction

Video Understanding is a foundational task in artificial intelligence, which focuses on analyzing and interpreting the content of videos to enable various applications, including video classification, activity recognition, and scene understanding . As a critical branch of video understanding, **Video Anomaly Detection** (VAD) , which aims to automatically detect abnormal videos, has garnered significant research attention due to its wide range of applications in criminal activity detection and disaster response . Prior studies on VAD mainly focus on detecting whether each video frame is abnormal or not in the video . However, these studies overlook targeting at determining the underlying video semantic structure, i.e., "_what is the abnormal type, where they have occurred, which people or things are involved_" with a given video.

Motivated by these, this paper proposes a novel **M**ulti-scene **V**ideo **A**hormal **E**vent **E**xtraction and **Localization** (M-VAE) task1, aiming at localizing abnormal events (i.e., starting and ending times of the anomaly) and extracting event quadruples (i.e. [subject of the event, event type, object of the event, scene of the event]) through a chat paradigm. Take an example of _Street_ scene in Figure 1 (a), within 23s to 25s, a man bends down and pries the lock, then drives away from the street and the abnormal event quadruple is [_people_, _steal_, _car_, _street_]. Different scene (i.e., Residence scene) is also shown in Figure 1 (b). Within 15s to 17s, a man vandalizes a sculpture at one's residence and the quadruple is [_people_, _Vandalism_, _Sculpture_, _Residence_]. This structured processing for abnormal videos can significantly improve the practicality and efficiency of video anomaly localization systems. In fields such as real-time abnormal event monitoring that require high reliability and precision monitoring, using such structured processing can quickly search and screen for

Figure 1. (a) and (b) illustrate two surveillance video examples for our M-VAE task and Sherlock model in two scenes (**Street** and **Residence**). Sherlock precisely generates the abnormal event quadruples and their corresponding timestamps. (c) presents a circular ratio diagram illustrating different spatial information. From (c), we observe that the global spatial information and the local spatial information (i.e., action, object relation, and background) in our M-VAE dataset are imbalanced.

the required abnormal elements, which provides more convenient and intuitive evidence for further processing. Therefore, it is worthwhile to address this new task. Nevertheless, we believe that this new task faces two key challenges.

For one thing, it is challenging to model the global-local spatial information (named global-local spatial modeling challenge). Existing video understanding models (Sanchez et al., 2016; Wang et al., 2017; Wang et al., 2018) mainly focus on modeling general global information. However, local spatial information in our M-VAE task is often crucial compared to general global information, which are highly discriminative and essential for precise identification. Taking Figure 1 (a) as an example, the local spatial information, such as action (bend down), object relations (\(<\)man, near, car-), and background (street), can help better identify abnormal events. However, those local spatial information (e.g., actions, object relations, backgrounds) have different heterogeneous representations (i.e., different model structures and encoders). Therefore, a single, fixed-capacity transformer-based model, often makes it difficult to capture those critical local spatial information in videos. Recently, the Mixture of Expert (MoE) (Wang et al., 2017; Wang et al., 2018) paradigm has demonstrated scalability in multi-modal heterogeneous representation fusion tasks (Wang et al., 2017; Wang et al., 2018; Wang et al., 2018). Inspired by this, a well-behaved model for our task should adopt the MoE paradigm to not only consider global spatial information but also emphasize the importance of local spatial information.

For another, a straightforward approach is to employ a basic Mixture of Expert (MoE) mechanism (Wang et al., 2017; Wang et al., 2018; Wang et al., 2018) to treat global spatial information (i.e., general representations of videos) and local spatial information (e.g., actions) as the global expert and local experts for integrating those information. However, the data imbalance issue among local spatial information may lead to the basic MoE experts being biased towards the more frequently occurring spatial information in the dataset. The statistics in Figure 1 (c) can illustrate this imbalance. Certain frequently appearing local information (i.e., action at 45%), can lead to higher weight for the corresponding expert. However, in Figure 1 (a), the object relations information, with the smallest proportion (25%), but is the most discriminative for extracting and localizing _Theft_ events. More seriously, global spatial information is the most frequent and our preliminary experiments in Figure 7 (a) reveal global expert is often more thoroughly trained and often have the highest weights. Therefore, a better-behaved MoE expert fusion mechanism should mitigate this data imbalance (named global-local spatial balancing challenge), ensuring all experts are sufficiently trained to highlight their importance.

To tackle above challenges, we propose a Global-local Spatial-sensitive LLM named Sherlock, i.e., acting like _Sherlock Holmes_ to track down criminal events, for M-VAE. Specifically, this model designs a Global-local Spatial-enhanced MoE (GSM) module to address the global-local spatial modeling challenge, which includes four spatial experts to extract spatial information and an expert gate to weigh global and local spatial information. Furthermore, this model designs a Spatial Imbalance Regulator (SIR) to address the global-local spatial balancing challenge, which includes a Gated Spatial Balancing Loss (GSB) to further balance global and local experts. Particularly, we construct a M-VAE instruction dataset to better evaluate the effectiveness of our model. Detailed experiments show Sherlock can effectively extract and localize abnormal events and surpass advanced Video-LLMs in multiple evaluation metrics.

## 2. Related Work

\(\)**Video Anomaly Detection.** Video Understanding is a rapidly evolving research field which encompasses several tasks, including video grounding (Wang et al., 2017; Wang et al., 2018; Wang et al., 2018), spatial-temporal detection (Wang et al., 2018) and so on. As an important branch of video understanding, previous studies on Video Anomaly Detection (VAD) can be categorized into unsupervised, weakly-supervised, and fully-supervised categories. Unsupervised approaches focus on leveraging reconstruction techniques to identify anomalies (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). Weakly-supervised methods have shown promising results in identifying abnormal frames (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). Fully-supervised methods are scarce due to the expensive frame-level annotations required (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). Different from the above studies, our Sherlock model aims to target at determining the underlying video semantic structure, providing a structured quadruple that goes beyond previous methods, facilitating the rapid detection and early warning of abnormal events in real-time.

\(\)**Event Extraction** (EE) focuses on extracting structured information from given types of information. Traditional EE methods mainly extract from text documents (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). Recently, many studies (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) generate similar event structures from visual image data. Different from all the above studies, we are the first to focus on extracting the abnormal event from videos and constructing a quadruple dataset, incorporating information from multiple spatial information, enriching the task of event extraction, and making it more practical for real-world applications.

\(\)**Scene Recognition** is a fundamental task applied in remote sensing (Wang et al., 2018; Wang et al., 2018) and autonomous driving (Wang et al., 2018). Traditional methods rely on hand-crafted features for extracting visual attributes (Wang et al., 2018; Wang et al., 2018). Recently, ARCNet (Wang et al., 2018) and CapsNet (Wang et al., 2018) reinforcement, aim to locate important regions. Others, like using CapsNet in (Chen et al., 2018) and FACNN (Wang et al., 2018), focus on modeling global context. SCViT (Wang et al., 2018) and KPR combine fine-grained information. Recently, many studies (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) utilize LLMs to solve the illusion problem. Different from the above studies, we introduce scene classification into our M-VAE task and integrate scenes into event quadruples, greatly improving the applicability of our M-VAE task in the real world.

\(\)**Video-oriented Large Language Models.** The rise of Chat-GPT (Chen et al., 2018) has stimulated the prosperity of Video Large Language Models which can be categorized into four major types: firstly, Video Chat (Sanchez et al., 2016) and Video LLaMA (Wang et al., 2018), which utilize BLIP-2 (Wang et al., 2018) and Q-Former to map visual representations onto Vicuna; secondly, models like Video ChatGPT (Wang et al., 2018), Otter (Wang et al., 2018), Valley (Wang et al., 2018), mPLUG-Owl (Wang et al., 2018), and Chat-UniVi (Wang et al., 2018), which leverage CLIP (Wang et al., 2018) to encode visual features; thirdly, PandapT (Chen et al., 2018), which adopts Image-Bind (Wang et al., 2018) as its core architecture for video understanding; and fourthly, VideoLLaVA (Wang et al., 2018), which aligns image and video features into a linguistic feature space using LanguageBind (Wang et al., 2018). Recently, a few studies (Wang et al., 2018; Wang et al., 2018) consider incorporating spatial information in models. Besides, some studies (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) introduce the concept of MoE into LLMs, but they only focus on efficiency, without considering the balance between different information. Different from all the above studies, we design a new Sherlock model, to address our M-VAE task, which includes a Global-local Spatial-enhanced MoE module and a Spatial Imbalance Regulator to address the challenges of global-local modeling and balancing.

## 3. Our Sherlock Model

In this paper, we propose a Sherlock model to address the M-VAE task. Figure 2 illustrates the framework of Sherlock, which is composed of two core components (i.e., the Global-local Spatial-enhanced MoE (GSM) module (sec 3.1) for the global-local spatial modeling challenge and the Spatial Imbalance Regulator (SIR) (sec 3.2) for the global and local spatial balancing challenge). Subsequently, we present our training strategies to enhance the ability of understanding spatial information (sec 3.3).

**Backbone**. We choose Video-L1aVA2(Huang et al., 2019) and its visual encoder LanguageBind(Wang et al., 2019) as the core framework. Video-L1aVA, which is optimized with a mixed dataset of images and videos, demonstrates leading performance across most image and video benchmarks. We employ Video-L1aVA as the backbone to explore the potential of Video-L1a in extracting and localizing abnormal events.

**Task Formulation.** Given a video \(V\) for \(M\) frames, each frame is labeled with 1 or 0, where 1 and 0 represent whether this frame conveys an abnormal event. The goal of M-VAE is to interactively generate the quadruple (_sub_, _type_, _obj_, _see_) for each event along with the corresponding timestamp \(sta\) and \(end\), where \(sub\), \(type\), \(obj\), \(see\), \(sta\) and \(end\) are the subject, event type, object, scene, start time and end time of the abnormal event. As shown in Figure 1 (a), a man steals a car at street from 23s to 25s. Therefore, the output of our M-VAE task is {23s, 25s, (_people, steal, car, street_}).

### Global-local Spatial-enhanced MoE Module

As shown in Figure 2, we design a Global-local Spatial-enhanced MoE (GSM) Module for the global-local spatial modeling challenge. Inspired by Mixture-of-Experts (MoE) (MoE, 2019), we design three Local Spatial Experts (i.e., Local Action Expert, Local Object Relation Expert and Local Background Expert) and a Global Spatial Expert to extract spatial information, detailed as follows.

**Local Spatial Experts** contain three local spatial experts (i.e., action, object relation, and background), detailed as follows.

\(\)**Local Action Expert (Action Expert, AE)**. We leverage HigherHRNet(Wang et al., 2019), a well-adopted bottom-up human pose estimation network to extract local spatial action information. HigherHRNet can generate local spatial action tokens \(}=(t_{a}^{},...,t_{a}^{},...,t_{m}^{})\), and each token consists of 17 human joint nodes for each individual in every frame of a video sequence. Here, \(t\) denotes the \(i\)-th frame. Next, we apply Action Graph Attention to integrate \(}\) with the video tokens \(}=\{t_{a}^{},...,t_{a}^{},...,t_{m}^{}\}\) generated by the Video Encoder in Video-L1aVs. We start by calculating the attention weights \(_{kj}\) for each node \(e_{k}\) in \(t_{i}^{}\) relative to its neighboring node \(e_{j}\):

\[}=(}})(}})}{}) \]

where \(h_{k}\) and \(h_{j}\) is the features of \(e_{k}\) and \(e_{j}\) respectively. \(}\) denote the learnable weight matrix, and \(d\) is the feature dimension. Then we aggregate the feature \(_{k}\) of node \(e_{k}\): \(_{k}=_{j(e_{k})}_{kj} h_{j}\), where \((e_{k})\) is the neighboring nodes of \(e_{k}\). Finally the feature of

Figure 2. The overall framework of Sherlock. It consists of a Global-local Spatial-enhanced MoE (GSM) Module and a Spatial Imbalance Regulator (SIR). The SIR exerts a direct influence on the output weights of the expert gate. W SIR or W/o SIR means with or without Spatial Imbalance Regulator.

\(e_{k}\) is calculated by \(h_{k}^{}=(}[_{k},h_{k}])\), where \(}\) donates the weight matrix and \([_{k},h_{k}]\) is the concatenation of \(_{k}\) and \(h_{k}\).

After graph attention operation, we enhance \(}\) using the attention mechanism with query \(_{w}\), key \(}\), and value \(}\) calculation to obtain final action tokens: \(^{}}=(_{w}^{T}}) }\).

\(\)**Local Object Relation Expert (Object Relation Expert, ORE)**. We leverage ReITR (Kumar et al., 2017), a well-studied one-stage object relation graph generation method to extract local spatial object relation information. ReITR can generate an object relation token \(t_{t}^{}=(R_{i},E_{i})\), which represents the object relation graph of the \(i\)-th frame. Here, \(R_{i}=(\{e_{i,1},b_{i,1}\},...(e_{i,k},b_{i,k}\})\) is a set of \(k\) detected objects, with class e and corresponding bounding box \(b\). The set \(E_{i}=\{e_{i,p},r_{i,(pq)},c_{i,q}\}\) consists of the directed edges in the graph, representing two directional edges from \(c_{i,p}\) to \(r_{i,(p,q)}\) and from \(r_{i,(p,q)}\) to \(c_{i,q}\), where \(r_{i,(p,q)}\) denotes a relationship category. For example, an object might be represented as (_man_, \(\)_0.36, 0.24, 0.75, 1.625), and an edge as (_man_, _near_, _car_). Subsequently, we apply object-aware masking with Masked Graph Transformer Networks (MaskGTN) to fully utilize object relations. We mask irrelevant object parts based on the bounding box information, and aggregate information from neighbors using a graph transformer layer (GT). Given an input graph of region classes and edges, MaskGTN computes updated vectors for each region and edge. Assuming we use \(L\) layers of GT, with \(}\) representing the features of the \(\)-th layer, the final forward propagation is defined as follows:

\[}=(}}}}}}}) \]

where \(\) is the activation function on the graph. \(}\) is the adjacency matrix of the object-relation graph, derived from \(E_{i}\), and \(}\) is its degree matrix, with \(}_{ii}=_{i}}_{ij}\). \(}\) is a trainable weight matrix.

\(\)**Local Background Expert (Background Expert, BE)**. We leverage SAM2 (Shen et al., 2017), an advanced model for visual segmentation, to extract local spatial background information from videos. SAM2 can generate a background image for each frame of video. Then we leverage InternVit (Chen et al., 2018) to encode local spatial background information which is a large vision encoder extending the parameters of vision transformer (VIT) (Chen et al., 2018) to \(\), formally represented as:

\[}=((v_{i})) \]

where \(v_{i}\) is the \(i\)-th frame of video \(V\). This process results in the local spatial background tokens \(}=\{t_{1}^{},...,t_{b}^{},...,t_{m}^{}\}\) for the entire video sequence, with \(n\) representing the total number of frames.

**Global Spatial Expert** has a comprehensive understanding of the training data. Collaborate with local spatial experts to bring specialization and generalization capabilities to M-VAE tasks.

\(\)**Global Spatial Expert (Global Expert, GE)**. The weight assigned to the global spatial expert complements that of the local spatial experts. Consequently, the local spatial experts acquire specialized skills for specific tasks, whereas the global spatial expert develops a comprehensive understanding of the entire training corpus. The collaboration between these two types of experts provides both specialization and generalization for our M-VAE task. In this way, we leverage LanguageBind (Yang et al., 2019) in Video-LLaVA (Yang et al., 2019), which inherits the ViT-L/14 structure from CLIP and as equipped with powerful and universal visual encoding capabilities to extract global spatial information for our task. We subsequently leverage a pre-trained FFN layer by (Yang et al., 2019) to align the dimension with other spatial information, formally represented as:

\[}}=((v_{i})) \]

where \(v_{i}\) is the \(i\)-th frame of video \(V\). This process yields the full set of global tokens \(}}=\{t_{1}^{},...,t_{m}^{},...,t_{m}^{ }\}\) for the entire video sequence, with \(n\) representing the total number of frames.

After designing four experts, we ensure that the four Spatial Experts can dynamically adjust the weights of the four heterogeneous types of spatial information inspired by Mixture-of-Experts (MoE) (Shen et al., 2017). As shown in Figure 2, unlike methods that embed several FFNs within LLMs, our GSM put four experts outside the LLMs to adjust weights for global and local spatial information. Based on this, we introduce a dynamic Expert Gate (EG) (Shen et al., 2017), which controls the contribution of each expert by calculating gating weights as a soft gate. Finally, the output \(\) of GSM, based on four spatial experts and EG, is formally represented as:

\[=(}_{i=1}^{N}( })) \]

\[=(_{i=1}^{N}(g_{i}} )) \]

where LayerNorm (\(\)) indicates layer normalization (Chen et al., 2018). \(g_{i}\) (the \(i\)-th entry in \(\)) represents the weight of the \(i\)-th expert. \(}\) represents the outputs of the \(i\)-th Spatial expert. \(N\) is the total number of spatial expert, and \(}\) being the trainable weight matrix.

### Spatial Imbalance Regulator

After modeling the spatial information, we design a Spatial Imbalance Regulator (SIR) including a Gated Spatial Balancing Loss (GSB) for the global-local spatial balancing challenge, detailed as follows.

**Gated Spatial Balancing (GSB) Loss**. Previous researches employ a basic Mixture of Experts (MoE) (Shen et al., 2017; Chen et al., 2018) to model global and local spatial information. When faced with an imbalance between these two types of information, the weights assigned to experts tend to be biased toward those that appear more frequently. As shown in Figure 1 (c), there are the most spatial elements (_e.g._, _People_) related to local spatial action information in event quadruple. This implies that performance will deteriorate when faced with real-world data that is not processed by an action expert (_e.g._, _object relations_). More seriously, as shown in Figure 1 (c), global information holds significant weight in all data, which will lead to excessive training of global experts and weaken the abilities of local experts with lower weights. This imbalance phenomenon will greatly affect the performance of our model. Based on this, we should keep the weights of all spatial experts not too different and achieve the optimal state of relative balance where every expert is fully trained. Inspired by MoELORA (Yang et al., 2019), we propose a Gated Spatial Balancing (GSB) Loss to balance spatial weights, as follows:

\[_{}=(}}_{i=1}^{N_{}}-(g_{i}))-(g_{}) \]where \(N_{}\) is the number of local expert. \(g_{}\) is the weight of global expert. The first term of Eq.(7) is balancing between local experts, and the second term is balancing between local and global experts. The weights of four experts have already balanced when the loss is optimized to a minimum. This regulation achieves a better balance among all experts, reducing the impact of data imbalance, which effectively addresses the global-local balancing challenge. Finally, the overall loss of Sherlock can be represented as:

\[=_{}+*_{} \]

where \(\) is the hyper-parameter that controls the strength of \(_{}\), and \(_{}\) is the next-token prediction loss of Video-LLMs.

### Training Strategies for Sherlock

In order to enhance the ability of understanding spatial information, we design a two-stage training process. Stage 1 is to enhance the ability of understanding spatial information and Stage 2 is to address the M-VAE task, detailed as follows.

**Stage 1. Pre-Tuning for spatial understanding.** As shown in Figure 2, we first pre-tune Video-LLaV using four high-quality datasets. We aim for Video-LLaVA to have a good spatial understanding ability. Specifically, we selected four high-quality datasets: HumanML3D (Huang et al., 2019), Ref-I4 (Wang et al., 2019), RSI-CB (Wang et al., 2019), and COCO-Caption (Wang et al., 2019), as described in sec 4.1. For each pre-tuning dataset, we enable this dataset to understand corresponding spatial information.

**Stage 2. Instruction Tuning for M-VAE task.** We aim to enable the model to localize abnormal events and extract quadruples through the chat paradigm. We construct an instruction tuning dataset described in sec 4.1 and instruct the pre-tuned Video-LLaVA to _Extract quadruples and localize abnormal events. The quadruple includes subject, event type, object, and scene in abnormal events_. The instruction will undergo text embedding to obtain the textual tokens \(_{}\). Finally, the input of the LLM is "**O** from Eq.(5) + \(_{}\)".

## 4. Experimental Settings

### Instruction Data Construction

The training pipeline of Sherlock contains two stages. As shown in Figure 3, for each stage, we construct the corresponding instruction dataset for better tuning.

**For Stage 1.** We construct a special understanding dataset based on Ref-I4 (Chen et al., 2019), HumanML3D (Huang et al., 2019), RSI-CB (Wang et al., 2019) and COCO (Wang et al., 2019). Specifically, we manually design an instruction for each type of spatial information, for instance: **Instruction**: "_Judge the action of the characters in the image. Describe the image region_-_objs in the image_. _Judge the background of the image. Describe the image_". As HumanML3D has 25K videos with an average duration of 1 second, and we take 8 frames per second. For the data balance, we randomly select 20K images or frames from each dataset.

**For Stage 2.** We construct an M-VAE instruction dataset based on CUVA (Huang et al., 2019), which primarily consists of surveillance videos, with an average duration of **80** seconds per video. As this dataset includes five detailed video Q-A tasks (i.e., timestamp, classification, reason, result, and description tasks), it is highly beneficial for constructing our M-VAE dataset. **1)** For abnormal event quadruples, constructing quadruples involves two steps. **First**, we collect answers from the reason, result, and description tasks in CUVA for each video. Subsequently, we construct initial quadruples through ChatGPT (Wang et al., 2019) based on the answers to these tasks, with the instruction: "_Please extract the subject, object, and scene of the event based on the responses below_". **Second**, we create multiple candidate sets for subjects, objects, and scenes in quadruple. Specifically, **for subjects and objects elements**, we manually construct a set of around 40 for subjects and objects and filter elements based on this set. **For event types elements**, we adopt the 11 categories (i.e., Fighting, Animals, Water, Vandalism, Accidents, Robbery, Theft, Pedestrian, Fire, Violations, and Forbidden) from CUVA as the event types. **For scenes elements**, we assign two annotators to classify scenes for each abnormal event. If they cannot reach an agreement, an expert will make the final decision to ensure annotation quality. The _Kappa_ consistency check value of the annotation is 0.87. **2)** For localization task, we use the timestamp in the CUVA as labels for localization. Furthermore, we adhere to the split of CUVA for training and inference videos and take 8 frames per second, resulting in **800K** frames from 1K videos and each video contains **1.68** abnormal event on average. The statistics of the number of events and the duration in seconds (s) of events for each scene are shown in Table 1. Finally, we obtain our M-VAE instruction dataset. Our instruction for the M-VAE task is: _"Generate a quadruple and localize an abnormal event in the video. The quadruple includes subject, event type, object, and scene in abnormal events."_. Figure 1 (c) and Figure 4 show the top 20 quadruple elements, revealing the spatial imbalance.

### Baselines

In this paper, we select several advanced Video-LLMs as baselines which are introduced as follows. **VideoChat**(Wang et al., 2019) employs Q-Former (Wang et al., 2019) to map visual representations to Vicuna (Huang et al., 2019). **VideoChatGPT**(Wang et al., 2019) integrates LLMs with CLIP (Wang et al., 2019) for video representations. **Valley**(Wang et al., 2019) employs a temporal modeling module to bridge

_{}\) in global-local spatial balancing and encourages us to consider using SIR to better balance spatial information. **3)** In addition, we record the weights of four spatial experts after training in Figure 6 and Figure 7 (_a_). We can see that the weights of all experts have been relatively balanced, and each expert has demonstrated outstanding professional abilities when facing different types of abnormal videos.

**Effectiveness Study of Pre-tuning.** From Table 2, we can see that **w/o pre-tuning**, the performance is inferior to **Sherlock**. FNRs, F2, average mapp@toU, and average event extraction metrics have decreased by 17.63 (_p_-value < 0.01), 16.95 (_p_-value < 0.01), 10.92 (_p_-value < 0.01) and 11.48 (_p_-value < 0.01), respectively. This further

    &  &  \\   & &  &  &  &  &  \\  & 0.1 & 0.2 & 0.3 & & & & & & & & & & & & & & & & & & & & & \\  BiConvLSTM[(21)] & 52.74 & 37.31 & 31.12 & 40.39 & 68.05 & 44.48 & 784 & 784 & 784 & 784 & 784 & 784 & 784justifies the effectiveness of pre-tuning, as well as encourages us to use more high-quality datasets to enhance the spatial understanding ability of Video-LLMs before instruction-tuning.

### Convergence Analysis and Practical Assessment for Sherlock

In order to analyze the convergence of Sherlock, we record the loss of baseline Video-LLMs, Sherlock, and its variant without specific components over various training steps during the experiment. The results are shown in Figure 5 and we can see that: **1) Sherlock** demonstrates the fastest convergence compared to other Video-LLMs. At the convergence point, the loss of Sherlock is 1.05, while Video-LLAv is 2.06. This underscores the high efficiency of Sherlock over other advanced Video-LLMs, which hints at the potential of Sherlock for quicker training steps and less resource utilization. **2) Sherlock** demonstrates the fastest convergence compared to its variant without specific components in Figure 5. This justifies that the four types of spatial information along with GSM and SIR can accelerate the convergence process, which further encourages us to consider the spatial information in the M-VAE task.

To assess practicality, we analyze the FNRs of Sherlock for each scene. As shown in Table 3, we can observe that in every scene, Sherlock outperforms other Video-LLMs. This indicates that the possibility of misclassifying abnormal events as normal events is minimized, thereby demonstrating the importance of global and local spatial modeling of Sherlock. We also analyze the average inference time in seconds for a one-minute video. As shown in Figure 7 (b), Sherlock does not perform much differently from the other models in terms of inference time. This is reasonable, as some studies confirm that the MoE architecture can improve efficiency (Golovne et al., 2013; He et al., 2016). This suggests that introducing more information along with a MoE module for the M-VAE task does not increase the inference time and Sherlock can maintain good inference efficiency.

### Compared with Advanced Non-LLM Models on Public Dataset

In order to more comprehensively evaluate the effectiveness of Sherlock, we compare our **Sherlock** model with other advanced non-LLM models (Golovne et al., 2013; He et al., 2016; He et al., 2016; He et al., 2016) on traditional anomaly localization and anomaly classification task based on publicly available CUVA datasets (He et al., 2016). Specifically, we need Sherlock to determine whether each second of the video is abnormal or not without generating quadruples. As shown in Table 4, non-LLM models not only underperform relative to other Video-LLMs presented in Table 4 but also significantly inferior to our Sherlock model. This further demonstrates the importance of the global and local spatial information we proposed for the M-VAE task.

### Qualitative Analysis for Sherlock

As shown in Figure 8, we visualize and compare **Sherlock** with other Video-LLMs. We randomly select two samples from our dataset and ask these models to _Analyze the following video and localize the timestamp and extract the quadruple of the abnormal events_. From the figure, we can see that: **1)** Accurately localizing abnormal events and extracting correct quadruples is a huge challenge. For instance, example 2 captures a segment from 9s to 15s, where identifying the collision of the truck at road is particularly challenging, **2)** Compared with other advanced Video-LLMs, **Sherlock** shows excellent performance in localizing abnormal events. In example 1, **Sherlock** outperforms other models in terms of prediction accuracy. In example 2, it outperforms PandaGPT in terms of accuracy and can generate a correct quadruple. This further demonstrates the effectiveness of **Sherlock** in precisely extracting and localizing abnormal events in video segments.

## 6. Conclusion

In this paper, we firstly propose a new M-VAE task and a constructed M-VAE instruction dataset, making a significant contribution to future research on abnormal events. Secondly, we propose a Global-local Spatial-sensitive LLM named Sherlock to assist in localizing and extracting abnormal event quadruples, providing decision-makers with more intuitive and comprehensive information support. This model includes a Global-local Spatial-enhanced MoE module and Spatial Imbalance Regular to model and balance spatial information. In the end, our experimental results demonstrate the outstanding performance of Sherlock. In future work, we hope to consider the relationships between events and enrich our tasks with event inference to improve the performance of extraction. In addition, we also hope to improve the interpretability of our model by providing explanations for each abnormal event.

Figure 8. Two Visualized samples to compare Sherlock with other Video-LLMs.