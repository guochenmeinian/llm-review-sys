# ACM Reference Format:

InfoMAE: Pairing-Efficient Cross-Modal Alignment with Informational Masked Autoencoders for IoT Signals

Anonymous Author(s)

###### Abstract.

Standard multimodal self-supervised learning (SSL) algorithms regard cross-modal synchronization as implicit supervisory labels during pretraining, thus posing high requirements on the scale and quality of multimodal samples. These constraints significantly limit the performance of sensing intelligence in IoT applications, where the heterogeneity and the non-interpretability of time-series signals result in abundant unimodal data but scarce high-quality multimodal pairs. This paper proposes InfoMAE, a cross-modal alignment framework that tackles the challenge of multimodal pair efficiency under the SSL setting by facilitating efficient cross-modal alignment of pretrained unimodal representations. InfoMAE achieves _efficient cross-modal alignment_ with _limited data pairs_ through a novel information theory-inspired formulation that simultaneously addresses distribution-level and instance-level alignment. Extensive experiments on two real-world IoT applications are performed to evaluate InfoMAE's pairing efficiency to bridge pretrained unimodal models into a cohesive joint multimodal model. InfoMAE enhances downstream multimodal tasks by over 60% with significantly improved multimodal pairing efficiency. It also improves unimodal task accuracy by an average of 22% 1.Anonymous Author(s). 2024. InfoMAE: Pairing-Efficient Cross-Modal Alignment with Informational Masked Autoencoders for IoT Signals. In. ACM, New York, NY, USA, 16 pages. [https://doi.org/10.1145/nnnnnn.nnnn](https://doi.org/10.1145/nnnnnn.nnnn)

## 1. Introduction

Multimodal Self-Supervised Learning (SSL) algorithms, although achieving unprecedented performance in extensive sensing applications , present unique data challenges rarely encountered with unimodal SSL or vision-language domains due to the complexity in acquiring high-quality multimodal pair for IoT signals. The inherent properties of sensory data common in sensing applications result in abundant unimodal signals but scarce multimodal pairs. First, sensory modalities have heterogeneous properties, such as sampling rate, timestamp, or duration, that increase the likelihood of capturing asynchronous events. Consequently, standard IoT multimodal datasets require manual calibration to reduce temporal misalignments or to synchronize between the time-series signals. . Second, raw IoT signals often lack intuitive interpretability. Unlike images or text, where visual features can be easily matched to textual captions, capturing useful signatures between sensing modalities like vibration or frequency waves is challenging. Preprocessing and calibrating these signals requires modality-specific domain knowledge or technical expertise, which is labor-intensive and susceptible to operational errors. Finally, IoT sensors are subject to varying deployment conditions, leading to sparse and noisy data . Each modality can be independently affected by the deployment conditions or environmental factors. For instance, a loud noise source might significantly impact acoustic sensors while having minimal effect on seismic signals. This heterogeneity often results in poor-quality multimodal pairs that are uncorrelated with each other or incomplete datasets with significant gaps and missing data points. These factors contribute to significant challenges in IoT data collection. As IoT networks scale in quantity and the number of modalities, acquiring large-scale, high-quality multimodal pairs becomes increasingly time-consuming, error-prone, and less reliable. The limited multimodal pairs with potential misalignments can introduce uninformative false positive pairs , polluting the multimodal feature patterns extracted by the pretrained encoders.

Despite these challenges, most existing multimodal SSL frameworks  rely heavily on massive multimodal pairs to learn robust joint representations during the pretraining, but their capability could degrade significantly with insufficient synchronized pairs . On the other hand, independently pretraining each modality on their unimodal data and directly concatenating misaligned modality features for finetuning fails to capture cross-modal interactions that are critical to downstream multimodal tasks . Instead, we observe that with limited multimodal pairs, we can effectively convert independently trained unimodal encoders into a coherent model that sustains strong generalizability

Figure 1. Comparison of supervised learning, self-supervised learning, and pair-efficient self-supervised learning.

in multimodal tasks. We refer to this process as _pair-efficient SSL_. The relation of pair-efficient SSL for multimodal data compared to standard SSL draws an analogy to the evolvement of SSL compared to supervised learning, as visualized in Figure 1. In supervised learning, manual labels serve as supervision to train encoders for mapping inputs to task-specific labels. Its performance depends heavily on the quantity and quality of human annotations. Self-supervised learning (SSL) mitigates label scarcity by first designating proxy labels from the data properties to learn general semantics with massive unlabeled data, then calibrating the pretrained model to a downstream task with minimal human annotations. Similarly, in multimodal SSL contexts, cross-modal alignment acts as a special form of "supervision", where point-to-point modality correspondence is utilized to identify semantically meaningful and consistent sensory information. Taking another step forward, pair-efficient SSL takes advantage of abundant unimodal data for "independent pretraining", followed by "cross-modal finetuning" with limited multimodal pairs to align unimodal models into a cohesive multimodal model.

In this paper, we propose InfoMAE, a cross-modal learning framework designed to enhance the alignment of unimodal representations using a limited number of multimodal pairs. The key idea behind InfoMAE is to enforce alignment across modalities at both the _distribution_ and _instance_ levels. Existing contrastive learning frameworks adopt point-to-point alignment to map samples across different modalities to a proximate joint representation (Selak and Komodakis, 2016; Liu et al., 2017; Wang et al., 2018; Wang et al., 2019). These approaches focus on aligning individual samples, essentially viewing alignment as a local optimization problem that aims to minimize the geometric distances between corresponding samples in the representation space. However, such instance-level approaches face significant challenges with limited multimodal pairs, as they may overfit to the specific pairs available and result in poor generalization with pariring biases. These hinder capturing complex cross-modal relationships, especially when the multimodal pairs are sparse and unevenly distributed. In contrast, InfoMAE takes a more holistic approach by emphasizing _distribution-level_ alignment, considering the overall information content of the limited multimodal pairs rather than only focusing on the individual samples. We present a comprehensive analysis of distribution alignment and propose an _information theory-based approach_ to formally define the distribution alignment problem in the factorized information space. We formulate this as a differential learning objective to construct (i) shared joint representations as a compact common variable across modalities capable of performing any multimodal task and (ii) private representations holding implicit modality-specific information independent of shared representations. InfoMAE alleviates the strict requirement of exact multimodal sample pairs and can better accommodate potential misalignments in data collection or temporal synchronization, improving the representations learned even with a small-scale multimodal pair.

We extensively evaluate InfoMAE across various combinations of pretrained unimodal domains. InfoMAE achieves exceptional performance gain compared to the standard multimodal SSL paradigm under limited multimodal pairs and outperforms existing works when aligning the unimodal representations. Individual unimodal encoders, in return, can also benefit from the representational structures with improved downstream performance. Additionally, as the number of multimodal pairs scale, InfoMAE also demonstrates versatility as a standard multimodal SSL framework, achieving SOTA performance across real-world IoT applications.

## 2. Analysis of Cross-Modal Alignment

### Notation

Consider \(M\) sets of unsynchronized modality data \(=\{X_{i}\}_{i M}\), where each set \(X_{i}\) contains unlabeled samples of fixed-length windows partitioned from the time-series signals of the \(i\)-th modality. Let \(N_{i}=|X_{i}|\) denote the size of each modality set.

For the \(j\)-th sample of modality set \(i\), we apply Short-Time Fourier Transform (STFT) to obtain its time-frequency representation, \(_{ij}^{C_{i} I S_{i}}\), where \(C_{i}\) is the number of input channels, \(I\) is the number of time intervals within a sample window, and \(S_{i}\) is the spectrum length in the frequency domain. We have a set of modality encoders \(=\{E_{1},E_{2},,E_{M}\}\) to extract the modality embeddings of each sample and a set of modality decoders \(=\{D_{1},D_{2},,D_{M}\}\) to map the samples from the embedding space back to the time-frequency domain \(}=\{}\}_{i M}\) as a part of the reconstruction process. Additionally, there is a set of multimodal data \(^{s}=\{X_{i}^{s}\}_{i M^{s}}\) consisting of a subset of modalities \(M^{s} M\), where samples across the modalities are synchronized in time and have equal sizes \(|X_{1}^{s}|==|X_{M^{s}}^{s}|\). Note that each synchronized data of modality \(i\) can also be a subset of the unsynchronized unimodal, set such that \(X_{i}^{s} X_{i}\), as any synchronized multimodal data is inherently unsynchronized when considered independently. Finally, we have a set of labeled data for supervised learning and finetuning on a much smaller scale, where each sample has a corresponding label \(y_{j}\) for each downstream task.

### Problem Definition

Prior multimodal SSL practices require large-scale, fully synchronized multimodal sets \(^{s}\) to learn joint multimodal representations that perform well in downstream tasks. However, these approaches often overlook two challenges: (i) _Insufficient multimodal data_: When \(|^{s}|\) is small, existing methods struggle to learn effective joint representations, and (ii) _Unutilized unimodal data_. The abundance of available unimodal data is often neglected. In IoT applications, the scale of synchronized multimodal sets can be significantly limited due to signal heterogeneities, temporal misalignment, or domain variances, which result in incomplete modalities. This results in more available unimodal data than synchronized multimodal data (\(|X_{i}||X_{i}^{s}|\)). However, this abundant unimodal data is excluded from existing multimodal SSL pretraining techniques. To better utilize unimodal data, our problem falls under the SSL setting with unimodal pretrained models and limited multimodal pairs, which consists of two stages:

_Independent Unimodal Pretraining:_ For each independent modality data \(X_{i}\), we train a corresponding unimodal encoder \(E_{i}\). The goal is to learn a _holistic unimodal representation_ that maximizes downstream unimodal performance after finetuning. Since modality sets \(X_{i}\) are independent, this pretraining is not constrained by the number of synchronized pairs and can, therefore, fully leverage the abundant unimodal data.

_Efficient Cross-Modal Alignment:_ Given a set of synchronized modalities data \(^{s}\) of \(M^{s} M\) modalities, we aim to align the pretrained encoders efficiently. This alignment projects unimodal representations into joint representations that maximize the downstream multimodal performance after finetuning. The scale of the multimodal alignment should be significantly smaller than the unimodal pretraining \(|X_{i}^{s}||X_{i}|\). In contrast to prior multimodal SSL works focusing on learning robust joint representations on large-scale multimodal data, this work aims to improve the _data efficiency_ of learning robust joint representations given only limited multimodal pairs.

### Factorization & Distributional Alignment

This section analyzes multimodal representation factorization in the information space and demonstrates how it enables distribution-level alignment of unimodal representations.

#### 2.3.1. Connection between Factorization and Cross-modal Alignment

In aligning multimodal representations, prior approaches often rely on contrastive learning to minimize the _modality gap_(Zhou et al., 2017) by pulling representations of different modalities from the same sample closer together while pushing representations from different samples further apart. However, due to the inherent heterogeneity, each modality contains unique, modality-specific information, and enforcing perfect alignment across modalities could potentially hurt the performance in multimodal downstream tasks (Zhou et al., 2017). To address these challenges, recent works (Zhou et al., 2017; Zhou et al., 2017; Zhou et al., 2017) have proposed factorizing modality representations into shared and private subspaces. It preserves both common and modality-specific information and allows for the alignment of shared representations while maintaining independent private representations for downstream tasks. However, these works operate on _instance-level alignment_, and it remains unclear whether this is sufficient when only limited modality pairs are available for learning. The scarcity of paired samples introduces the risk of biased sampling, potentially misleading the alignment process. With this in mind, we analyze a different approach that factorizes the representation in the information space and enforces _distribution-level_ alignment to capture a more comprehensive correlation between modalities by _emphasizing their information content rather than just their geometric proximity_. The intuition behind this is that instead of individual sample pairs, we aim to align modalities by the global structure (as shown in Figure 2). When the multimodal pairs are scarce, the distributional alignment aims to be _resilient to sampling biases_ and capture meaningful cross-modal relationships.

#### 2.3.2. Distributional Alignment through Information-theory based Factorization

We now formally define the factorization problem in the information space. Without loss of generality, we state the definitions for two modalities, \(=\{X_{1},X_{2}\}\), but they can be generalized to more modalities.

First, we are interested in constructing a compact random variable \(U\) (shared representation) that can perform any task that can be achieved using \(X_{1}\) separately and \(X_{2}\) separately. Formally, we define a sufficient common variable as follows.

**Definition 2.1**.: (Sufficient Common Variable) \(U\) is defined as the sufficient common variable between \(X_{1},X_{2}\) if and only if \(U=s_{1}(X_{1})=s_{2}(X_{2})\) for some \(s_{1},s_{2}\), and

\[( f_{1},f_{2})[f_{1}(X_{1})=f_{2}(X_{2})][( f)f(U) =f_{1}(X_{1})=f_{2}(X_{2})], \]

namely, any common (shared) function between \(X_{1},X_{2}\) can be computed using \(U\). Building on the sufficient common variable, we define the shared representation to be the most compact form of \(U\) with the minimized entropy to ensure that \(U\) captures only the essential shared features across modalities.

**Definition 2.2**.: (Shared Representation) We refer to a sufficient common variable \(U\) with minimal entropy \(H(U)\) as the shared representation.

However, it is not clear how to find a sufficient common variable or a shared representation. We show that an approximation of the shared representation can be obtained by solving the following optimization problem, and later in Section 3, we propose the differentiable loss objectives with proof provided in Appendix A.

\[& H(U) \ X_{1}\!\!\! X_{2} U,\\ &\ ( s_{1},s_{2})\ U=s_{1}(X_{1})=s_{2}(X_{2}) \]

The conditional independence in Equation 2 enforces a form of distributional alignment, ensuring that given the shared representation \(U\) is the most compact aligned representation such that \(X_{1},X_{2}\) provide no additional information about each other.

Moreover, we define the private representations \(V_{1},V_{2}\) between \(X_{1},X_{2}\) as follows.

**Definition 2.3**.: (Private Representation) \(V_{1},V_{2}\) is the private representation of \(X_{1},X_{2}\) if they have minimal entropy among the random variables satisfying: \(V_{1}=p_{1}(X_{1}),V_{2}=p_{2}(X_{2})\) for some \(p_{1},p_{2}\) and there exist functions \(g_{1},g_{2}\) such that \(X_{1}=g_{1}(V_{1},U),X_{2}=g_{2}(V_{2},U)\), where \(U\) is the shared representation.

Similarly, we look for approximate representations. In particular, we replace equalities with a distance constraint \(d\), and independence is replaced by small mutual information. In Section 3, we discuss the detailed implementation of a differentiable loss function to find the approximate representations.

## 3. InfoMAE

This section introduces InfoMAE, a novel cross-modal alignment framework that efficiently aligns unimodal representations at the distribution and instance levels. We provide a detailed overview of InfoMAE's cross-modal alignment module in Figure 3.

Figure 2. An illustration of instance-level vs. distribution-level Cross-Modal Alignment

### Unimodal Pretraining

Unlike standard multimodal SSL that pretrains on synchronized multimodal pairs, we first initiate _unimodal pretraining_ on large-scale unsynchronized unimodal data. In the first stage, we pretrain each encoder \(E_{i}\) independently on unimodal data \(X_{i}\) with MAE, which applies mask reconstruction defined as the following for each modality \(i M\):

\[_{i}^{}=||}-X_{i}||^{2}}=D_{i }(E_{i}(X_{i})). \]

The pretrained unimodal encoders \(E_{i}\) extract a generalized representation for each modality \(M_{i}\). However, they do not guarantee information compatibility between modalities when used together in the downstream tasks. In the following sections, we present InfoMAE's different components (as illustrated in Figure 4) to calibrate the encoders to _explicitly align_ the modalities in both the distribution-level and instance-level with only a limited amount of multimodal pair \(^{s}\).

### Distribution-level Alignment

We begin with the differentiable objective function that we optimize to obtain the (approximate) shared (\(U\)) and private representations (\(V\)) defined in Section 2.3.2. To extract \(U\) that is a function of both \(X_{1},X_{2}\), we equivalently extract \(U_{1}=F_{1}^{}(E_{1}(X_{1})),U_{2}=F_{2}^{}(E_{2}(X_ {2}))\), where \(F_{1},F_{2}\) are 2-layer MLP projectors that maps the general representation into shared and private representations, and enforce a constraint that \(U_{1}=U_{2}\). Similarly, we extract \(V_{1}=F_{1}^{}(E_{1}(X_{1})),V_{2}=F_{2}^{}(E_{2}( X_{2}))\). We use \(=\{U_{1},U_{2}\}\) and \(=\{V_{1},V_{2}\}\) for the extracted shared and private representations, respectively.

#### 3.2.1. Shared Representation

As described in Section 2, we aim to find the shared representation \(U\) that solves the optimization problem in Definition (2.2). However, due to the difficulty of the optimization problem 2 and the possibility that a shared representation does not exist, we instead approximate the shared representation by minimizing the following objective

\[_{}^{}=& ad(U_{1},U_{2})+(H(U_{1})+H(U_{2}))\\ &+I(X_{1};X_{2} U_{1})+I(X_{1};X_{2} U_{2}), \]

where \(a\) and \(\) are the hyperparameters controlling the weight of each term, and \(d()\) is a distance measure. The first two terms in the loss function aim to find \(U_{1}=U_{2}\) with minimal entropy, while the last two terms aim to impose conditional independence of \(X_{1},X_{2}\) given \(U_{1}\) or \(U_{2}\). We would like to note that the entropy and conditional mutual information listed in Eq. (4) are not easy to compute or differentiate. To alleviate this, we reduce these terms into probabilistic density functions below:

\[_{}^{}=&  d(U_{1},U_{2})+_{l=1}^{2}_{X_{1},X_{2},U_{l}}[ _{X_{1},X_{2},U_{l}}}{p_{l}p_{l}p_{l}}.\\ &.+(1-)_{X_{1},U_{l}}}{p_{l}p_{l}}+_{X_{1},U_{l}}}{p_{l}-p _{l}}]. \]

Due to the space limit, we leave the detailed proof and discussion in Appendix A. To further enhance the differentiability of Eq. (5) by avoiding directly computing the probabilistic density ((_e.g._, \(_{X_{1},U_{1}}}{p_{l}p_{l}p_{l}p_{l}p_{l}p_{l}p_{l}p_{l}p_{l}p_{l}}\)), we follow [(31; 50; 60)] and utilize the _density-ratio trick_ to train a discriminator \(\), which given \(X_{1},X_{2},U\), outputs

Figure 4. Key learning objectives of InfoMAE Cross-Modal Alignment.

Figure 3. Overview of InfoMAE’s alignment in the information space. InfoMAE adopts an information theory-inspired objective to align the factorized representations. Best viewed in color:

[MISSING_PAGE_FAIL:5]

#### 4.1.2. Datasets

Our experiments focus on two real-world applications: Moving Object Detection (MOD) and Human Activity Recognition (HAR). The MOD application contains vibration-based datasets using seismic and acoustic sensors. The HAR application consists of publicly released IMU sensor datasets (accelerometer, gyroscope, and magnetometer) collected from many human subjects performing various daily activities. To evaluate cross-modal alignment, we simulate a practical scenario where the pretrained domains differ significantly to reflect the diverse signals across different IoT application domains. Under this setting, we have unsynchronized unimodal data from different domains: MOD consists of data from three separately collected domains (M, G, T), each with different targets, terrains, and environmental conditions. HAR consists of data from two publicly released datasets (RealWorld-HAR  and PAMAP2 ). We pretrain unimodal encoders with only the unimodal data from different domains and then use a limited amount of synchronized multimodal pairs for cross-modal alignment and downstream finetuning. For joint pretraining, we pretrain on the massive available synchronized multimodal pairs. We summarize the data used in Table 1 and describe these applications and domains in more detail in Appendix B.

#### 4.1.3. Baselines

We extensively evaluate InfoMAE with different SSL baselines including unimodal contrastive (SimCLR, MoCo), multimodal (CMC, GMC, FOCAL ) contrastive, temporal contrastive (TNC, TSTCC), and MAE based frameworks (MAE, CAV-MAE). We describe these baselines in more detail in Appendix F.

### Cross-Modal Alignment Evaluation

#### 4.2.1. Cross-Modal Alignment on MOD

We evaluate InfoMAE against prior CL works  on cross-modal alignment with various combinations of unimodal models pretrained with different domains. We align the encoders with a small scale of multimodal pairs (5% of the unimodal data scale) and an even smaller subset of labeled multimodal pairs from domain M for finetuning. This application involves two modalities (seismic and acoustic). Therefore we represent the domains of the unimodal representations with two letters (_e.g._, \(T_{}||G_{}\) means aligning seismic encoder pretrained on domain T and acoustic encoder pretrained on domain G).

In addition to the prior CL baselines, we also show the performance for direct concatenation of the pretrained unimodal representations without any alignment and for Joint Multimodal Pretraining on the same amount of synchronized multimodal pairs. We present the accuracy and F1-score after finetuning in Table 2, InfoMAE consistently outperforms the unimodal concatenation by a significant margin since direct concatenation fails to exploit cross-modal correspondence. CMC and other unimodal SSL frameworks even have negative impacts compared to direct concatenation, indicating that unimodal objectives or simply aligning the multimodal representations without considering the modality discrepancy could hurt the downstream performance. InfoMAE also achieves better results than FOCAL and GMC, underscoring the benefits of enforcing distribution-level alignment over instance-level alignment in downstream tasks with limited multimodal data. When the same amount of multimodal data is used for Joint Multimodal Pretraining, the significant gap between the aligned unimodal models and the joint pretrained multimodal model suggests the feasibility of transferring pretrained unimodal representations to multimodal representations with only limited (5%) synchronized multimodal data. It is noteworthy that some domain combinations ( e.g., GT, TT, TG) do not even overlap with data from the alignment and finetuning set (MOD).

#### 4.2.2. Cross-Modal Alignment on HAR

Besides MOD application, we also evaluate InfoMAE on HAR applications. In contrast to MOD evaluation, which aligns unimodal encoders pretrained on different domains, we analyze how additional unsynchronized data from the same domains could assist the downstream performance given the limited number of multimodal pairs. Here, we independently pretrain all unimodal encoders on unsynchronized IMU data from either PAMAP2, RealWorld-HAR, or Combined, which is the concatenation of the former two. Then, we use a small portion of the synchronized multimodal data pairs from PAMAP2 for cross-modal alignment and downstream finetuning. We present the results in Table 4. InfoMAE consistently achieves the best performance, with an average of 4.09% and 5.16% improvements in accuracy and the

 Framework &  & } M_{}\)} & } T_{}\)} & } T_{}\)} & } M_{}\)} & } G_{}\)} \\   & Joint & Modal & & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 \\
847 &  &  &  &  &  &  &  &  &  &  \\  Unimodal Concat & ✗ & ✗ & 0.6731 & 0.6699 & 0.5392 & 0.5281 & 0.4454 & 0.4366 & 0.7247 & 0.7217 & 0.6584 & 0.6543 & 0.6648 & 0.6543 & 0.666 \\  CMC  & ✗ & ✓ & 0.6792 & 0.6702 & 0.4313 & 0.4356 & 0.4173 & 0.4032 & 0.6919 & 0.6877 & 0.6497 & 0.6335 & 0.6479 & 0.6335 & 0.647 \\ FOCAL  & ✗ & ✓ & 0.7462 & 0.7432 & 0.6249 & 0.6249 & 0.5613 & 0.5579 & 0.7549 & 0.7527 & 0.7194 & 0.7160 & 0.649 \\ GMC  & ✗ & ✓ & 0.7354 & 0.7317 & 0.6591 & 0.6523 & 0.4756 & 0.4720 & 0.8044 & 0.8053 & 0.7247 & 0.7211 & 0.649 \\ SimCLR  & ✗ & ✓ & 0.3061 & 0.2742 & 0.2873 & 0.2609 & 0.2974 & 0.2758 & 0.2981 & 0.2698 & 0.2800 & 0.2308 & 0.2638 & 0.

F1-score compared to the best-performing baseline, FOCAL. The improvement is most significant in aligning unimodal encoders pretrained on RealWorld-HAR, which completely differs from the alignment set (PAMAP2). This further demonstrates InfoMAE's robustness as an alignment framework with a limited amount of multimodal pairs, reflecting its superior ability to utilize the unimodal data better even when they are from different domains.

### Unimodal Evaluation with Cross-Modal Alignment

We analyze how incorporating the multimodal correspondences into each unimodal encoder after alignment could benefit the downstream tasks. Figure 5 shows the accuracy for seismic and acoustic modalities before and after cross-modal alignment in the MOD application. With limited multimodal pairs, the pretrained unimodal encoders could gain the most significant performance improvements with InfoMAE. This emphasizes the InfoMAE's superior efficiency in enforcing cross-modal correspondence to each modality to improve their downstream performance, with only a few multimodal pairs required. With InfoMAE, the aligned unimodal model can generate the most holistic representations through distributional alignment compared to geometric alignment (CMC, FOCAL).

### Multimodal Pairing Efficiency

We also evaluate InfoMAE's alignment performance at varying amounts of multimodal data for MOD application in Table 3. We align both encoders pretrained from domain M (MM) and compare them to standard joint pretraining with different ratio of multimodal data. Additionally, we provide supervised training results on the same amount of labeled multimodal data used for finetuning. InfoMAE consistently achieves superior multimodal data efficiency, with minimal degradation as we reduce the number of multimodal data. In general, InfoMAE has an average of 3.42% gain over the highest-performing baselines and over 60% compared to joint model pretraining, which performs poorly in the absence of multimodal data. Note that the joint pretraining even performs worse than the supervised approach with only 5% of multimodal data, indicating the standard self-supervised pretraining fails to learn effective representations with an insufficient amount of synchronized multimodal data. In contrast, the two-stage learning paradigm of InfoMAE leveraging widely available unsynchronized unimodal data could effectively mitigate this problem.

### Standard Multimodal Pretraining on Large-scale Synchronized Dataset

While InfoMAE excels as an efficient cross-modal alignment framework under limited pairs, it also demonstrates remarkable flexibility as a standard multimodal SSL framework. We evaluate InfoMAE

    &  & Joint Pretrain & CMC & GMC & FOCAL & **InfoMAE** \\   & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 \\ 
5\% & & & 0.3329 & 0.3039 & 0.7087 & 0.6989 & 0.8614 & 0.8616 & 0.8694 & 0.8668 & **0.8828** & **0.8808** \\
15\% & & & 0.6142 & 0.6104 & 0.8111 & 0.8062 & 0.8781 & 0.8753 & 0.8727 & **0.9049** & **0.9028** & **7.681** \\
25\% & & & 0.7071 & 0.7938 & 0.8433 & 0.8372 & 0.8774 & 0.8758 & 0.8848 & 0.8831 & **0.9290** & **0.9270** \\
50\% & & & 0.8942 & 0.8920 & 0.8754 & 0.8724 & 0.8948 & 0.8938 & 0.9009 & 0.8994 & **0.9377** & **0.9367** \\   

Table 3. Alignment performance (MM) with different multimodal pair ratios from MOD.

    Unimodal \\ Pretrain Domain \\  } &  &  &  \\    Multimodal \\ Alignment Domain \\  } & & & & & & \\  Metric & Acc & F1 & Acc & F1 & Acc & F1 \\  Concat & 0.7843 & 0.7000 & 0.7763 & 0.6210 & 0.5675 & 0.4187 \\ CMC & 0.7334 & 0.6508 & 0.7285 & 0.6788 & 0.7010 & 0.5956 \\ FOCAL & 0.7922 & 0.7129 & 0.7354 & 0.6327 & 0.7643 & 0.6243 \\ GMC & 0.7314 & 0.5915 & 0.7344 & 0.5869 & 0.7414 & 0.5816 \\ SimCLR & 0.7299 & 0.6190 & 0.7075 & 0.5426 & 0.7225 & 0.5581 \\ TNC & 0.5431 & 0.4080 & 0.5889 & 0.4824 & 0.6378 & 0.5167 \\ TSTCC & 0.7299 & 0.6003 & 0.7065 & 0.5773 & 0.7354 & 0.5864 \\ 
**InfoMAE** & **0.8261** & **0.7303** & **0.8117** & **0.7175** & **0.7912** & **0.6901** \\   

Table 4. Linear Probing performance of HAR on PAMAP2 by aligning pretrained unimodal encoders.

Figure 5. Unimodal linear probing accuracy of MOD with and without cross-modal alignment.

    &  &  \\  Frameworks & Acc & F1 & Acc & F1 \\  CMC [(8)] & 0.7924 & 0.7897 & 0.6791 & 0.6776 & 770 \\ FOCAL [(40)] & 0.9137 & 0.9111 & 0.8156 & 0.8130 & 771 \\ GMC [(55)] & 0.7986 & 0.7947 & 0.3457 & 0.3387 & 772 \\ MoCo [(8)] & 0.8719 & 0.8688 & 0.7500 & 0.7483 & 773 \\ SimCLR [(6)] & 0.8418 & 0.8386 & 0.7288 & 0.7207 & 773 \\ TNC [(64)] & 0.6916 & 0.6797 & 0.5680 & 0.5625 & 778 \\ TSTCC [(15)] & 0.7080 & 0.7004 & 0.5804 & 0.5766 & 777 \\ MAE [(24)] & 0.6708 & 0.6642 & 0.4421 & 0.4365 & 778 \\ CAV-MAE [(18)] & 0.5507 & 0.5282 & 0.3457 & 0.3387 & 779 \\ 
**InfoMAE** & **0.9196** & **0.9186** & **0.8546** & **0.8535** & 780 \\   

Table 5. Performance of Joint Pretraining on MOD (seismic and acoustic) dataset and then finetuned on unseen domains.

against prior state-of-the-art works on Joint Multimodal Pretraining using abundant multimodal pairs, as shown in Table 5. We use synchronized, unlabeled multimodal data from the MOD dataset to pretrain backbone encoders. Then we freeze the pretrained encoders and perform linear probing using labeled multimodal data from domains \(G\) and \(T\), as described in Section 4.1. InfoMAE consistently outperforms the MAE-based framework and achieves better performance than other contrastive baselines. We leave more evaluation on Joint Multimodal Pretraining across four real-world datasets to Appendix G. Prior works, primarily designed for joint multimodal pretraining, often struggle with limited multimodal pairs and show significant performance degradation. In contrast, InfoMAE not only improves multimodal pairing efficiency but maintains high performance with minimal performance degradation.

### Ablation Studies

Finally, we study how each module of InfoMAE contributes to its performance through ablation studies. We evaluate four variants of InfoMAE by removing temporal, shared, private, and augmentation components in Table 6. The absence of either shared or private components leads to a significant degradation, implying the significance of factorized representation for cross-modal alignment. The drop in performance after removing temporal locality constraints also indicates the importance of learning temporal correspondence for time-series signals. Without temporal locality, the learned representations lose crucial temporal correspondence and can significantly compromise the ability to learn multimodal correspondences on top of the unimodal representations. Conversely, InfoMAE without augmentations does not significantly reduce the performance, demonstrating its robustness toward augmentation choices, in contrast to many contrastive learning frameworks that require careful selection of augmentations to avoid representational collapses.

## 5. Related Works

**Self-Supervised Multimodal Learning.** Self-supervised learning (SSL) techniques, such as Contrastive Learning (CL) and masked reconstructions, have achieved significant success in visual, textual, and time-series representation learning (Chen et al., 2017; Chen et al., 2018; Chen et al., 2019; Chen et al., 2019; Chen et al., 2020; Chen et al., 2021; Chen et al., 2020; Chen et al., 2021). Masked reconstruction learns informative representations by reconstructing masked inputs (Chen et al., 2019; Chen et al., 2019; Chen et al., 2020; Chen et al., 2021), with various masking strategies explored (Chen et al., 2019; Chen et al., 2020; Chen et al., 2021), and extended to time-frequency spectrograms (Chen et al., 2020; Chen et al., 2021) and videos (Chen et al., 2020; Chen et al., 2021). Multimodal representation learning has become increasingly important with diverse applications (Chen et al., 2019; Chen et al., 2021; Chen et al., 2021; Chen et al., 2021). Recent works leverage CL to learn correspondences between modalities (Chen et al., 2021; Chen et al., 2021; Chen et al., 2021; Chen et al., 2021; Chen et al., 2021), and others pretrain unified encoders for multimodal representations (Chen et al., 2021; Chen et al., 2021). Factorized Multimodal Learning (Chen et al., 2021; Chen et al., 2021; Chen et al., 2021) further decouples multimodal learning by acknowledging both modality-specific and modality-shared information. FOCCAL (Chen et al., 2021) proposed contrastive learning objectives to learn shared and private representation in the orthogonal space. FactorizedCL (Chen et al., 2021) separates the shared and private space based on their relevance to the downstream tasks. Some works (Chen et al., 2021; Chen et al., 2021) combine CL with MAE to capture cross-modal correspondence. Yet, these works minimize the geometric modality gap to learn cross-modal correspondences and rely on massive amounts of multimodal data for joint multimodal pretraining. In contrast, InfoMAE minimizes the information modality gap to further enhance the downstream performance. In reducing multimodal data pairs for training, many works (Chen et al., 2021; Chen et al., 2021; Chen et al., 2021) propose to impute missing modality pairs through feature generations. Wang _et al._(Wang et al., 2021) proposes using CL to align multimodal encoders through an anchor modality yet still overlooking unimodal data. In contrast, InfoMAE minimizes the reliance on multimodal data by taking advantage of a large amount of unimodal data.

**Multimodal Information Theory.** There has been a long history of exploring common information between random variables in information theory (Chen et al., 2021; Chen et al., 2021; Chen et al., 2021), and it is still an active research field (Chen et al., 2021; Chen et al., 2021; Chen et al., 2021; Chen et al., 2021). However, it remains challenging to compute the common information in practical applications. Kleinmanl _et al._(Kleinmanl et al., 2021) combines Variational Autoencoders with Gacs-Korner Common Information. Mai _et al._(Mai et al., 2021) proposes to measure the information redundancy for multimodal data. However, they do not explicitly consider the unique information for factorization. InfoMAE adopts the informational factorization considering both private and shared information to construct a joint representation in a task-agnostic manner rather than extracting task-related information like (Chen et al., 2021).

## 6. Discussion & Conclusion

We proposed InfoMAE, a pairing-efficient multi-stage SSL paradigm for multimodal IoT sensing. It first pretrains independent modality encoders on large-scale unimodal data sets. Then, it leverages a novel information theory-based optimization to achieve distributional cross-modal alignment with only limited multimodal pairs. Extensive evaluations compared to standard multimodal SSL frameworks demonstrated the superior efficiency and effectiveness of InfoMAE across multiple real-world IoT applications. We believe it opens new opportunities for developing more data-efficient and qualitative self-supervised multimodal models. In the Appendix, we provide more implementation details and evaluations.

    &  &  &  &  &  \\   & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 \\  noTemp & 0.6946 & 0.6902 & 0.5881 & 0.5884 & 0.5044 & 0.4888 & 0.7435 & 0.7432 & 0.6651 & 0.6570 \\ noShared & 0.7683 & 0.7595 & 0.6504 & 0.6515 & 0.5298 & 0.5232 & 0.8125 & 0.8116 & 0.7395 & 0.7351 \\ noPrivate & 0.5479 & 0.4732 & 0.4180 & 0.3402 & 0.2873 & 0.1812 & 0.6259 & 0.5519 & 0.5399 & 0.5519 \\ noAug & 0.7863 & 0.7823 & 0.6973 & 0.6967 & 0.5881 & 0.5868 & 0.2332 & 0.8252 & 0.7924 & 0.7879 \\ 
**InfoMAE** & **0.7950** & **0.7929** & **0.6986** & **0.7007** & **0.5928** & **0.5908** & **0.8326** & **0.8324** & **0.8326** & **0.8324** \\   

Table 6. Ablation Results of InfoMAE Cross-Modal Alignment.