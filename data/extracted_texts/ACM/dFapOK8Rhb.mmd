# Exploiting Language Power for Time Series Forecasting with Exogenous Variables

Anonymous Author(s)

###### Abstract

The World Wide Web thrives on intelligent services that depend heavily on accurate time series forecasting to navigate dynamic and evolving environments. Due to the partially-observed nature of real world, exclusively focusing on the target of interest, so-called _endogenous variables_, is insufficient for accurate forecasting, especially in web systems that are susceptible to external influences. Thus, utilizing _exogenous variables_ to harness external information, i.e., forecasting with exogenous variable (FEV), is imperative. Nevertheless, as the external environment is complex and ever-evolving, inadequately capturing external influences can even lead to learning spurious correlations and invalid prediction. Fortunately, recent studies have demonstrated that large language models (LLMs) exhibit exceptional recognition capabilities across open real-world systems, including a deep understanding of exogenous environments. However, it is difficult to directly apply LLMs for FEV due to challenges of task activation, exogenous knowledge extraction, and feature space alignment. In this work, we devise ExoLLM, an LLM-driven method to sufficiently utilize Exogenous variables for time series forecasting. We begin by Meta-task Instruction to activate the knowledge transfer of LLM from natural language processing to FEV. To comprehensively understand the intricate and hierarchical influences of exogenous variables, we propose Multi-grained Prompts, encompassing diverse external influences, including natural attributes, trend correlations, and period relationships between two types of variables. Additionally, a Dual TS-Text Attention is devised to bridge the feature gap between text and numeric data in LLM. Evaluation on real-world datasets demonstrates ExoLLM's superiority in exploiting exogenous information for forecasting with open-world language knowledge. Code is available at [https://anonymous.4open.science/r/ExoLLM](https://anonymous.4open.science/r/ExoLLM).

## CCS Concepts

* **Computing methodologies Artificial intelligence**.

## 1 Introduction

The World Wide Web, as a continuously and ever-changing physical system, heavily depends on the ability to forecast and respond to shifting patterns and user behaviors . Time series forecasting is essential to modern web technologies, utilizing historical data to anticipate future web patterns and trends . Its predictive accuracy not only enhances user experience but also drives the development of intelligent web services, ranging from personalized content recommendations  and web economics modeling  to microservice log analysis . These capabilities position time series forecasting as a cornerstone in creating adaptive, data-driven web platforms .

Recently, deep models have achieved promising progress in time series forecasting , with most of them focusing exclusively on the target of interest, known as _endogenous variables_, to make predictions . This approach often ignores the influence of _exogenous variables_ from the external environment. **Exogenous variables refer to observable data within a system that are not the target variable being predicted.** As shown in Figure 1 (a), the variations within web page views (endogenous variable) are often influenced by exogenous variables, such as traffic flow, hospitalization rate, and societal events . Thus, given the complex and changing physical system , incorporating exogenous factors, i.e., forecasting with exogenous variables (FEV) is becoming prevalent and indispensable .

Generally, the core of FEV is to effectively model the influence of exogenous variables on endogenous variable . Recent research in FEV proposes using attention among observed numerical exogenous series and endogenous series to capture this inherent relationship . Nevertheless, due to the _Intricate influences and interactions_ from external environment, relying solely on time series modality is insufficient for capturing these external influences: **(1) Multi-grained temporal dependencies**. The external influences and interactions from exogenous variables is multi-grained, such as periodicity and trends, which can be reflected by various aspects including complex human behaviors and living habits . It is difficult to model such changing and diverse impact only by observed numeric , highlighting the necessity of thoroughly learning multi-grained temporal features to effectively model these intricate patterns . **(2) Spurious correlation**. Noise and interventions in current data can lead to learning biased external influence, thereby affecting the accuracy of forecasting results . For example, traffic flows are positively correlated with exogenous weather variables, but mandatory controls can lead to less traffic even when the weather is good, resulting in spurious association that may be learned by models. Without any external knowledge from real world, a high prediction uncertainty tends to be inevitable .

Consequently, designing more intelligent and robust FEV framework that enable models to effectively understand the intricateexternal influence and avoid spurious correlation is in demand. Fortunately, with rapid development of large language models (LLMs) (Bahdanau et al., 2015), there have been more opportunities to leverage the vast language knowledge to comprehend external influence on endogenous variables. Through extensive training on large-scale text corpora, pre-trained LLMs have extensively acquired knowledge of multi-grained correlation between two types of variables. Intuitively, empowering FEV with these full-scale external knowledge can significantly enhance forecasting accuracy (Zhu et al., 2017). Nevertheless, as shown in Figure 1 (b), considering distinct task differences between NLP and time series forecasting (Bahdanau et al., 2015; Zhu et al., 2017), and distant data gap between discrete text and continues numeric (Bahdanau et al., 2015), employing LLMs to FEV faces several urgent challenges: **(1) Task activation.** How to construct task instruction to fully activate the potential of LLMs in FEV, enabling the knowledge transfer across tasks. **(2) Full-scale language-driven knowledge acquirement.** Given an LLM-based solution, how to devise effective and comprehensive prompts to acquire hierarchical and sufficient knowledge from exogenous variables. **(3) Feature space alignment.** Given the solution is concerned with two data modalities of both numerical and text data, how to construct a feasible encoding-decoding strategy to ensure the alignment between text space and time series space.

In this work, we devise ExoLLM to forecast with Exogenous variables using LLM, capturing diverse and changing external influences from exogenous variables with language-based knowledge. Technically, we elaborately craft domain-specific Meta-task Instructions to guide LLMs to process FEV tasks in different data domains. Subsequently, we establish Multi-grained Prompts to dynamically capture the natural attributes, periodic associations, trend correlations, and other granular external influence of exogenous variables, thereby adaptive transferring the dynamic auxiliary information into knowledge that can be understood by ExoLLM. Additionally, we design the Dual Time series-Text Attention Attention (DT\({}^{2}\)Attention) to mitigate data discrepancies during time series encoding and feature decoding, respectively. Comprehensive evaluation demonstrates that LLM can even act as an effective few-shot and zero-shot FEV learners when adopted through our elaborate design, outperforming specialized forecasting models. Our meticulous design enables LLMs to function even as a proficient few-shot and zero-shot FEV learner, surpassing specialized forecasting models in terms of effectiveness, as demonstrated by the comprehensive evaluation. Our contributions can be summarized as follows:

* Given the complex and evolving external environment of real-world system, i.e., web service, traffic, electricity and weather, we introduce LLMs to maximally explore the auxiliary information of exogenous variables.
* We propose ExoLLM, the first LLM-based forecasting model to accomplish FEV: 1) To fully exploit the potential of LLM in FEV, we elaborately design Meta-task Instruction and Multi-grained Prompt, realizing the pre-trained knowledge transfer from NLP to FEV and integrate dynamic context information into knowledge of time-series domain. 2) To deal with the distant data gap between discrete text and continues numeric, we design modality-aware encoding and decoding mechanisms, i.e., DT\({}^{2}\)Attention, to achieve aligned feature before and after LLM encoding.
* ExoLLM demonstrates outstanding predictive performance across various real scenarios, including long-term, short-term, few-shot, and zero-shot forecasting. Quantitatively, ExoLLM outperforms 10 state-of-the-art models for long-term forecasting, achieving top-1 performance in 51 settings and top-2 in 5 settings out of a total of 56 settings. In addition, ExoLLM reduces MAE by an average of 4.1%, 5.2%, and 4.5% in short-term, few-shot, and zero-shot forecasting tasks, respectively.

## 2. Related Work

### Forecasting with Exogenous Variables

In practical forecasting scenarios, the utilization of exogenous variables as auxiliary information for forecasting endogenous variables is more prevalent. Previous research has explored statistical methods such as ARIMAX (Zhu et al., 2017) and SARIMAX (Zhu et al., 2017), which understand relationships between exogenous and endogenous series along with auto-regression. Additionally, deep learning models like

Figure 1. (a) Illustration of Knowledge Reserve from Pre-trained LLM: The extensive pre-trained text data endows LLMs with the potential to understand intricate influence of exogenous variables on web page views. (b) Huge Gaps in Feature Space and Tasks: Text embeddings and time series features are usually mapped to different feature spaces, and it is challenging to fine-tune text-generation pre-trained LLM for FEV.

NBEATSx (Xiong et al., 2017) and TiDE (Xiong et al., 2017) argue that forecasting models can leverage future values of exogenous variables during the forecasting endogenous variables. Notably, TimeXer (Xiong et al., 2017) introduces external information into transformer architectures through well-designed embedding strategies to effectively incorporate external information into segmented representations of endogenous variables, accommodating temporal lags or missing data records. However, these approaches rely on establishing auxiliary information only based on numeric correlation between exogenous and endogenous series. In contrast, ExoLLM has the capability to extract multi-grained effects of exogenous variables on endogenous ones as auxiliary information from extensive world knowledge, thereby holding significant potential for enhancing accuracy and generalization in FEV.

### LLM-based Forecasting

The recent emergence of LLMs has opened up new possibilities for time series forecasting (Xiong et al., 2017; Zhang et al., 2017). GPT4TS (Xiong et al., 2017) utilizes a pre-trained language model without updating its self-attention and feedforward layers. The model undergoes fine-tuning and evaluation across various time series analysis tasks, demonstrating comparable or state-of-the-art performance by leveraging knowledge transfer from natural language pre-training. LLM4TS (Bahdanau et al., 2014) adopts a two-stage fine-tuning approach on the LLM to fully leverage time series data. TimelLM (Tai et al., 2016) introduces the concept of text prototypes and reprograms time series based on these prototypes to align them more naturally with language models. Tempo (Chen et al., 2016) decomposes the trend, seasonality, and residual components of time series while dynamically selecting prompts to address comprehension challenges for LLMs. UniTime (Xiong et al., 2017) proposes a language-empowered unified model to efficiently capture knowledge from cross-domain time series data. With their extensive knowledge base, LLMs exhibit tremendous potential in time series forecasting. However, as shown in Table 1, there has been no prior research exploiting LLM for forecasting with exogenous variables (FEV) to enhance the prediction accuracy. To address this gap, we propose ExoLLM which harnesses the power of language to capture the influence of exogenous variables on endogenous variables.

## 3. Problem Definition

In forecasting with exogenous variables, there is a historical endogenous series \(^{1 L}\) and its associated exogenous information \(\), where \(L\) is look-back window size. Concretely, \(^{M L}\) comprises multiple exogenous variables \(\{^{(1)},^{(2)},,^{(M)}\}\), where \(M\) is the variable num and \(^{(m)}^{1 L}\) is the \(m\)-th exogenous series. Our goal is to learn a forecasting model \(\) (\(\)), which predicts the future \(T\) time steps of endogenous series \(}^{1 T}\), based on its historical observation \(\) and the exogenous variables \(\).

## 4. Methodology

The detailed framework of ExoLLM is illustrated in Figure 2. Firstly, the Meta-task Instruction (MTI) and Multi-grained Prompt (MGP) text are embedded using frozen large language model to get uniform size embedding. Then, exogenous and endogenous series will be tokenized by shared Temporal-property preserved Tokenizer (TPT) to preserve temporal properties. Furthermore, a mainly frozen pre-trained LLM is utilized to integrate exogenous knowledge into endogenous token. It's worth noting that a Dual TS-Text Attention (DT\({}^{2}\)Attention) is devised to align TS-Text feature space before and after LLM encoding, which enables the model to aware of specific modality. The output endogenous token wil be finally mapped to the future time series by a lightweight forecasting head.

### Language-driven Exogenous Knowledge Utilization

#### Meta-task Instruction

To activate the knowledge transfer of LLM from nature language processing (NLP) to FEV, it is necessary to construct task instructions as guidance. As illustrated in Figure 3, the meta-task instruction comprises three key elements: (1) Overall description and analysis of dataset, offering explicit domain identification information to the model. (2) Brief summary of endogenous and exogenous variables, facilitating model to discern the source of each variables. (3) Introduction to the FEV task, fully activating LLM to accomplishing forecasting task with exogenous variables. We aim to activate the LLM's FEV capability in different domains through carefully designed meta-task instructions.

#### Multi-grained Prompt

To comprehensively understand the external environment of Entire systems, we need to consider not only

    & **ExoLLM** & **AntiTimes** & **TimeLLM** & **LLM4TS** & **UnTime** & **LLM4Time** & **TEST** & **TEMPO** & **GPT4TS** \\  & **(Ours)** & (2024) & (2023) & (2023) & (2024) & (2023) & (2023) & (2023) & (2023) \\  Exogenous Variables & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ Multimodal & ✓ & ✓ & ✓ & ✗ & ✗ & ✓ & ✓ & ✗ \\ Feature Alignment & ✓ & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\   

Table 1. Comparison between prior LLM-based time series forecasting models and ExoLLM.

Figure 2. Overall architecture of ExoLLM, which consists of Dual TS-Text Attention and pre-trained LLM to sufficiently exploit exogenous variables in FEV.

the apparent data correlation between numerical exogenous and endogenous variables, but also the natural properties, constant relationships, sequential trends, period influences, stability, and other multicolver factors. Therefore, we design multi-grained prompts (MGP) to exploit the LLM's comprehensive knowledge of the world to a diversified understanding of a specific environment. As shown in Table 2, the multi-grained prompt mainly consists of two elements: (1) Revealing the natural attribute of exogenous variables and their essential correlation with endogenous variables, endowing the model with prior knowledge of external environment. (2) Describing the dynamic characteristics of exogenous endogenous series in term of trends, period, stability, and noise intensity, enabling the model to consider dynamic external influences. Intuitively, MGP not only deepens the LLM's understanding of exogenous variables, but also enhances the LLM's perception of the external invisible environment.

#### Uniform-scale Text Encoding

After constructing the meta-task instruction and multi-grained prompt, the next step involves encoding the text to obtain embeddings of uniform dimensions. To integrate these text with adequate language knowledge, we use a pre-trained LLM to encode these text descriptions. Since the text length of each prompt is different, we design an ingenious method to obtain the same embedding size. Particularly, we add a special token <_EOS_> at the end of the prompt. Since all the previous tokens are visible to <_EOS_> throughout the causal attention in LLM, the embedding of <_EOS_> could represent the entire text. The text encoding process is given by:

\[ = ((, EOS)), \]

where SelectLast(\(\)) denotes selecting the embedding of the last <_EOS_> token, LLM(\(\)) represents encoding part of large language model, \( = \{t_{},td_{}^{(1)},td_{}^{(2)},...,td_{}^{(M)},td_{}\}\) is text description set of Meta-task Instruction and Multi-grained Prompt. \( = \{p_{},p_{}^{(1)},p_{}^{(2)},...,p_{}^{(M)},p_{}\}\) represents the uniform-scale text embeddings of TD, where \(p_{}  ^{1 D}\) is the embedding of meta-task instruction, \(p_{}^{(i)}  ^{k D}\) is the \(i\)-th exogenous prompt embedding set, \(p_{}  ^{k D}\) is endogenous prompt embedding set, \(k\) is multi-grained prompt number of one-type variable and \(D\) is the uniform hidden dimension.

### Temporal-property Preserved Tokenizer

To facilitate LLM's understanding of the different types of variable series, we need to compress each series into a single token. Recent studies (Zhou et al., 2017) use a linear layer to embed the entire time series as a token. However, this embedding approach neglects the temporal properties of data, resulting in the model's incomplete understanding of the relationships between exogenous and endogenous series. Therefore, we devise a Temporal-property Preserved Tokenizer (TPT) to obtain tokens reserving the temporal characteristics. Firstly, we partition the exogenous variables \(\) and endogenous variables \(\) into non-overlapping patches to enhance the local semantics at each time step (Zhou et al., 2017), resulting in \(_{}  ^{1 N P}\) and \(_{}  ^{M N P}\), where \(P\) is patch length, and \(N = \) is the corresponding numbers of patches. To compress the temporal representations, TPT employs Self-Attention to learn temporal interactions among patches and selects the the last patch as the output:

\[_{*}^{time} = (( + _{*})), \]

where Self-Attn(\(\)) denotes self-attention applied to time series, PE represents the position embedding, SelectLast(\(\)) denotes the operation of selecting the last patch, \(_{*}\) is patched exogenous or endogenous series and \(_{end}^{time}\) is the corresponding token. Selecting the last patch as the token representation of the entire series is justified by two reasons. (1) It interacts with all preceding patches through attention, thus possessing sequence-level temporal information; (2) It is closest to the future sequence, providing crucial near-term information. Finally, we obtain exogenous tokens \(_{}^{time}  ^{M D}\) and endogenous token \(_{end}^{time}  ^{1 D}\) in the time series feature space.

### Knowledge-retained LLM Encoder

Understanding the exogenous impact on endogenous variables is crucial for time series forecasting. We utilize meta-task instruction along with tokenized exogenous and endogenous variables to LLMs to fully exploit the prior knowledge in LLMs, thereby forming enhanced representations of endogenous token:

\[_{end}^{llm} = ((pt_{task},_{},_{end})), \]

where LLM(\(\)) denotes the encoder part of LLM. Each variable is treated as a token, and exogenous and endogenous variables are concatenated in a fixed order to form a "sentence" in a fixed order, like \([pt_{task},_{}^{(1)},_{}^{(2)},..., _{}^{(M)},_{}]\). Following (Zhou et al., 2017), we freeze the positional embedding layers and self-attention blocks in LLM to retain majority of learned knowledge from language pre-training. Ultimately, we obtain an exogenous variable enhanced representation of endogenous token, \(_{end}^{llm}  ^{1 D}\), which encapsulates rich information from prior exogenous knowledge.

### Feature Alignment with Dual TS-Text Attention

Given that LLM is pre-trained on discrete textual data and lack exposure to continuous numerical values, directly inputting tokens \(_{}^{time}\) and \(_{end}^{time}\) in time series featrue space into LLMs would increase the difficulties in understanding never-seen modality, thus resulting in degraded predictive performance. Besides, the output token from LLM in text space is difficult to decode into future series. Thus, a Dual TS-Text Attention (DT\({}^{2}\)Attention) is devised to align ts-text feature space before and after LLM encoder, respectively.

Figure 3. Example of Meta-task Instruction and headlines of Multi-grained Prompt in ETTh1.

#### TS-Text Attention

Intuitively, there should be a certain distinction between exogenous and endogenous tokens to avoid over-smoothing representation among different types of tokens, and absorb certain prior external knowledge to enhance the LLM's encoding ability. Thus, a TS-Text Attention is designed to achieve: 1) Mapping tokens from time series feature space to text feature space; 2) Distinguishing between endogenous and exogenous Tokens. Specifically, for any type of token, TS-Text Attention designs its Query as token in time series space, while the Key and Value are its corresponding multi-grained prompt. Then, we perform Cross Attention to align tokens:

\[_{}^{text}=(_{}^{time}, _{},_{}). \]

where \(_{}^{time}^{D}\) is the exogenous/endogenous token in time series space, \(_{}^{k D}\) is this variable's corresponding multi-grained prompt, \(_{}^{text}^{D}\) is the mapped token in text space and will be input into LLM in Eq (3).

#### Text-TS Attention

Denote \(_{end}^{}^{1 D}\) as the endogenous token encoded by LLM encoder in Eq (3). The \(_{end}^{}\) remains in the text space, directly decode \(_{}^{}\) for forecasting faces the challenge of converting textual semantics into time series. Thus, we use Text-TS Attention to alleviate such problem, decoding \(_{}^{}\) into time series space based on the temporal information of exogenous series. This can be expressed as:

\[_{}^{}=(_{ }^{}._{}^{time},_{ }^{time}), \]

where \(_{}^{}\) represents exogenous variables in time series space, \(_{}^{}^{1 D}\) is the decoded endogenous token. Through this approach, we can better utilize the representation capability of LLMs and combine exogenous series to enhance the endogenous forecasting.

### Lightweight Forecasting Head

Considering the richness of the encoded token and maximumly preserving exogenous information by LLMs, a simple linear layer is employed to transform \(_{}^{}\) for forecasting:

\[}=(_{}^{}). \]

where \(}^{1 T}\) is the future endogenous series.

## 5. Experiments

### Dataset and Experimental Settings

To verify the model's effectiveness, we extensively evaluate our proposed ExoLLM on a diverse range of FEV scenarios, including long-term, short-term, few-shot and zero-shot task.

#### Datasets and Experimental Setups

To completely evaluate the FEV capability of ExoLLM, we conduct experiments on \(12\) real-world datasets. These datasets are collected from web and especially the exogenous factors retrieved from are in the formation of language. In particular, seven well-established public long-term datasets from different domains, and five short-term datasets in electricity price are involved in our FEV experiments. The endogenous and exogenous variables of each dataset are summarized in detail in Appendix A.1. For short-term forecasting datasets, the input length is set as \(168\) and prediction length is \(24\). For long-term forecasting datasets, the input length is set as \(96\) and prediction length varies \(\{96,192,336,720\}\). More implementation details can be found at Appendix A.2.

#### Baselines

We compare ExoLLM with \(10\) baselines, which comprise the state-of-the-art forecasting methods, including LLM-based model: LLM4TS [(2)], GPT4TS [(45)], TimeLLM [(14)], Transformer-based model: TimeXer [(35)], TextTST [(26)],ITransformer [(21)], Transformer [(43)], AutoFormer [(39)], CNN-based model: S:ICNet [(19)], and Linear-based model: T1DE [(6)]. Among these models, TimeXer and

   Characteristics & Prompts & \\   & 1 & This Exogenous variable is High UeLess Load, representing external load that is inefficiently utilized. \\  & 2 & Exogenous High UeLess Load indicates a potential inefficiency in the system’s external load handling. \\  & 3 & Exogenous High UeLess Load can lead to increased energy consumption without corresponding output. \\  & 4 & Exogenous High UeLess Load might suggest that the system is operating under suboptimal external conditions. \\   & 3 & Exogenous High UeLess Load series shows an overall upward trend \\  & 4 & Exogenous High UeLess Load series initially rises and then declines. \\  & 5 & Exogenous High UeLess Load series inhibits an overall declining trend \\  & 6 & Exogenous High UeLess Load series initially declines and then rises \\   & 4 & Exogenous High UeLess Load series has no apparent periodicity. \\  & 5 & Exogenous High UeLess Load series exhibits shorter periodicity and higher frequency \\  & 6 & Exogenous High UeLess Load series displays clear periodicity. \\  & 7 & Exogenous High UeLess Load series exhibits relatively longer periodicity. \\   & 6 & Exogenous High UeLess Load series undergoes significant instability over all the time. \\  & 6 & Exogenous High UeLess Load series remains relatively stable while minimal fluctuations \\  & 6 & Exogenous High UeLess Load series experiences occasional bouts of volatility, interspersed with periods of relative calm. \\  & 6 & Exogenous High UeLess Load series shows consistent stability, with values remaining close to a steady mean. \\   & 6 & Exogenous High UeLess Load series is subject to very strong noise interference \\  & 6 & Exogenous High UeLess Load series has a low signal-to-noise ratio, where noise significantly affects the clarity of the underlying data. \\  & 6 & Exogenous High UeLess Load series experiences moderate noise, partially obscuring the underlying pattern. \\  & 6 & Exogenous High UeLess Load series is not influenced by any noise interference. \\   

Table 2. An example of Multi-grained Prompt of one variable in ETH1. Orange is chosen from exogenous and endogenous. Green is the variable name. Blue is prior knowledge about the variable’s nature attribute. Black is the fixed template.

[MISSING_PAGE_FAIL:6]

### Efficiency Analysis

We have compared ExoLLM with other LLM-based and linear-based methods in term of running time, and the results are provided in Table 7. As demonstrated, ExoLLM significantly reduces computational costs since it does not require repetitive text encoding during training. It saves considerable computational time compared to TimeLLM and is even comparable to Dlinear. We will discuss the computational efficiency of ExoLLM from theoretical perspectives: (a)The LLM used for frozen text embeddings does not participate in forward computation or backpropagation during each training iteration. For a given dataset, its MTI and MGP components are fixed, meaning their embeddings can be precomputed and stored on disk. During training, these embeddings only need to be loaded into memory, resulting in zero additional training time for this part. (b) The TPT (Time Patch Tokenization) used for encoding time-series features reduces the sequence length by a factor of \(P\) (where \(P\) is the patch length). This reduces the theoretical time complexity from \(O(L^{2})\) to \(O()\). Thus, ExoLLM is designed with a strong emphasis on resource efficiency, making it more computationally economical in practice.

### Ablation Study

We conduct ablation studies by removing each module from ExoLLM on six datasets. **w/o MGP** removes Multi-grained Prompt (MGP). **w/o MTI** removes Meta-task Instruction (MTI). **w/o DT\({}^{2}\)A** removes Dual TS-text Attention (DT\({}^{2}\)Attention) for feature space alignment. **w/o TPT** replaces Temporal-property Preserved Tokenizer (TPT), which could preserve temporal properties for each token, with a linear layerr. We analyze the results shown in Table 8. The observations are listed as follows: Obs.1) Removing MGP and MTI results in the most significant decrease in prediction metrics, 77 emphasizing their strong ability in activating LLM in FEV. Obs.2) DT\({}^{2}\)Attention also significantly improves the model performance, demonstrating the importance of feature space alignment. Obs.3) TST constantly promotes the forecasting accuracy, suggesting that reserving temporal properties in each token is needed.

### Exogenous Scale Analysis

Real-world time series often encounter challenges such as the absence of crucial exogenous data. In this section, we employ random masking to simulate these scenarios and further investigate the forecasting performance. As illustrated in Figure 4 (a) and (b), We

{c|c

[MISSING_PAGE_FAIL:8]