# Coherence-based Query Performance Measures for Dense Retrieval

Anonymous Author(s)

###### Abstract.

Query Performance Prediction (QPP) estimates the effectiveness of a search engine's results in response to a query without relevance judgments. Traditionally, _post-retrieval_ predictors have focused upon either the distribution of the retrieval scores, or the coherence of the top-ranked documents using traditional bag-of-words index representations. More recently, BERT-based models using dense embedded document representations have been used to create new predictors, but mostly applied to predict the performance of rankings created by BM25. Instead, we aim to predict the effectiveness of rankings created by single-representation dense retrieval models (ANCE & TCT-ColBERT). Therefore, we propose a number of variants of existing unsupervised coherence-based predictors that employ neural embedding representations. In our experiments on the TREC Deep Learning Track datasets, we demonstrate improved accuracy upon dense retrieval (up to 92% compared to sparse variants for TCT-ColBERT and 188% for ANCE). Going deeper, we select the most representative and best performing predictors to study the importance of differences among predictors and query types on query performance. Using existing distribution-based evaluation QPP measures and a particular type of linear mixed model, we find that query types further significantly influence query performance (and are up to 35% responsible for the unstable performance of QPP predictors), and this sensitivity is unique to dense retrieval models. In particular, we find that in the cases where our predictors perform lower than score-based predictors, this is partially due to the sensitivity of MAP@100 to query types. Our novel analysis provides new insights into dense QPP that can explain potential unstable performance of existing predictors and outlines the unique characteristics of different query types on dense retrieval models.

**ACM Reference Format:**

Anonymous Author(s). 2024. Coherence-based Query Performance Measures for Dense Retrieval. In _2024 ACM SIGIR International Conference on the Theory of Information Retrieval (ICTIR '24), July 13, 2024, Washington D.C., USA_. ACM, New York, NY, USA, 11 pages. [https://doi.org/10.1145/mmmmnn](https://doi.org/10.1145/mmmmnn).

## 1. Introduction

Retrieval effectiveness in search engines can vary across different queries . Being able to accurately predict the likely effectiveness of a search engine for a given query may facilitate interventions, such as asking the user to reformulate the query .

To this end, the task of _Query Performance Prediction (QPP)_ aims to predict the effectiveness of a search result in response to a query without having access to relevance judgments . In the last two decades, a number of _query performance predictors_ have been proposed, which can be grouped in two main categories: _Pre-retrieval_ predictors estimate query performance using only linguistic or statistical information contained in the queries or the corpus . On the other hand, _post-retrieval_ predictors use the relevance scores or contents of the top returned documents, by measuring, for example, the focus of the result list compared to the corpus , or the distribution of the scores of the top-ranked documents . Predictors based on NQC  (the standard deviation of relevance scores) have been found to be surprisingly accurate. A further group of predictors examine the pairwise similarities among the retrieved documents . Thus far, these predictors have been applied using traditional bag-of-words representations. While examining the coherence between returned documents is useful, as we show, these representations are not suitable for predicting the query performance of more advanced retrieval methods.

More recently, pre-trained language models (PLMs) have introduced neural network architectures that encode the embeddings of queries and documents , and have led to increased retrieval effectiveness. Often, a BERT-based model is trained for use as a reranker of the result retrieved by (e.g.) BM25  - such _cross-encoders_ include BERT_CLS  and monoT5 . On the other hand, _dense retrieval approaches_ are increasingly popular, whereby embedding-based representations of documents are indexed, and those with the similar embeddings to the query are

Figure 1. Schematic representation of recent QPP pipelines, together with our proposed approach (Step 2, bottom). Top: A BM25 ranking consisting of TF.IDF vector representations (Step 2) , and fine-tuning BERT-based models on top of existing rankings (Step 3) . Bottom: Dense retrieval ranking with dense embedded representations. Numbers denote each step in the pipeline.

identified through nearest-neighbour search (e.g. ANCE (Selvin et al., 2017), TCT-CoIBERT (Kumar et al., 2018)). Compared to reranking setups, dense retrieval is attractive as recall is not limited by the initial BM25 retrieval approach, and improvements in the PLM can improve all aspects of the retrieval effectiveness. Therefore, dense retrieval models inspire us to develop predictors that are effective for predicting their rankings.

In parallel, neural architectures have also been adopted as methods for predicting query difficulty. These post-retrieval methods are _supervised_, and use refined neural architectures in order to produce a final performance estimate (Kumar et al., 2018; Li et al., 2019; Li et al., 2019; Li et al., 2019). For instance, BERT-QPP (Kumar et al., 2018) fine-tunes BERT (Devlin et al., 2019) embeddings for QPP by estimating the relevance of the top-ranked document retrieved for each query. However, its performance is lower or outperformed by unsupervised predictors when using advanced retrieval methods and the TREC Deep Learning datasets (Kumar et al., 2018). In our view, the problem lies in the mismatch of representations between predictor and ranking, which is best described in Figure 1. On top, we see the pipeline resulting from a BM25 ranking, and, at the bottom, a ranking from a dense retrieval system (Selvin et al., 2017; Li et al., 2019). While BERT-based QPP techniques can be used to predict the effectiveness of BM25 (Kumar et al., 2018; Li et al., 2019; Li et al., 2019; Li et al., 2019), single-representation dense retrieval models already contain representations that can accurately predict their corresponding ranking, thus eliminating the need to apply step 3 (BERT-QPP). Instead, to create predictors applicable for dense retrieval, we could use the existing embedded representations (step 2). Indeed, by considering patterns among the embeddings of the retrieved documents, we can update existing unsupervised predictors from traditional sparse (Bishop, 2006; Krizhevsky et al., 2009) to dense representation-based.

At the same time, the selection of evaluation measure can have an impact on the conclusions of QPP experimental results. This observation is more prominent if we consider, for example, that unsupervised QPP predictors such as NQC (Nayak et al., 2018) were primarily optimised for MAP at deeper cutoffs (100 or 1000); on the other hand, more recent supervised predictors were either optimised for RR@10 (Kumar et al., 2018; Li et al., 2019) or used both NDCG@10 and RR@10 (Li et al., 2019) providing comparable results between the two measures, but in both cases, results for MAP were missing. As a result, it is impossible to provide insights that are fully generalisable, as missing to report either of them can lead to biased results and incomplete conclusions. We believe that designing experimental studies should be aligned with the idea that the different measures are not interchangeable, and that proposed predictors could be complemented with the case where the predictor fails, together with the explanation of the reasons why this happens.

One explanation could be that query performance is further mediated by query categorisation. Few works have examined how QPP varies with query categories (Chen et al., 2019; Li et al., 2019). Indeed, knowing which queries are more difficult to answer may inform us about how to develop more refined predictors. In a recent query taxonomy (Chen et al., 2019), certain categories were found to be more difficult to answer compared to others. Therefore, we also quantify the extent to which query categories are responsible for the unstable performance of QPPs across different evaluation measures.

In short, our contributions are the following: (i) We propose a number of embedding variants of existing coherence predictors and our own extension _pairRatio_, an unsupervised predictor which uses pairwise relations of embedding vectors. In this way, we create predictors designed for dense retrieval; (ii) We study existing predictors to two state-of-the-art single-representation dense retrieval models, namely ANCE (Selvin et al., 2017) and TCT-CoIBERT (Kumar et al., 2018), as well as BM25 using all three evaluation metrics currently used for QPP, and show that changing the representations increases performance significantly not just for dense but also sparse retrieval; (iii) By also comparing with supervised predictors, we show that applying a BERT-based model for dense QPP is an unnecessary step in the pipeline that decreases QPP performance; (iv) We apply multilevel statistical models (Li et al., 2019; Li et al., 2019; Li et al., 2019; Li et al., 2019) in QPP to quantify the relationship between query categorisation and the unstable QPPs. In our analyses, we measure the performance of different QPPs in relation to the total QPP variation that can be attributed to the categorisation or as we term _query types_. At the same time, we detect a unique sensitivity of dense retrieval methods, which are affected by query type (up to 35% increase in query performance variations due to query categorisation) and exhibit larger differences between predictors, a pattern which is not apparent in sparse retrieval.

In addition, we observe: (a) Our proposed predictors provide the highest correlations for the more precision-oriented NDCG@10 for all retrieval models, while NDCG@10 and MRR@10 provides similar results. (b) Our multilevel perspective proposes a solution to correlation instabilities between measures, by showing how the interplay with query types differently influences each of the measures. In other words, we provide an analytical point that can explain any predictor, and show how our proposed predictors mainly optimise the measure that is less influenced by query variations. The structure of the rest of this paper is as follows: We present related work in Section 2, and present our new extended predictors in Section 3. Then, we follow with traditional correlation analysis of QPP predictors in Sections 4 and 5, continue with an extended linear mixed model analysis to test for query type in Section 6, and conclude with some final remarks in Section 7.

## 2. Related Work

The focus of this paper is on post-retrieval QPPs, as they are in general more accurate than pre-retrieval QPPs (Li et al., 2019). Indeed, there are two main reason why we eliminate pre-retrieval predictors from our focus. First, existing unsupervised neural pre-retrieval predictors (Chen et al., 2019; Li et al., 2019; Li et al., 2019) propose, for example, geometric semantic similarities of query terms, which indicate query specificity or contextual similarity and are based on pre-trained neural embeddings. Since these predictors examine queries at the token-level, they are not applicable to single-representation dense retrieval. Second, information based on queries can, in general, provide quite limited information with respect to the effectiveness of the ranking.

In terms of post-retrieval QPP, earlier post-retrieval predictors examined the focus on the result list induced by language models (probability distributions of all single terms) (Li et al., 2019). For example, _Clarity_(Li et al., 2019) measures the divergence of the language model of top-ranked documents from the one of the corpus(irrelevant list) - the higher the divergence, the better the performance. _Utility Estimation Framework (UEF)_(Kumar et al., 2018) uses pseudo-effective reference lists induced by term probability-based language models and estimates their relevance using predictors such as NQC (see below in Section 2.1). Both of these rely upon term probabilities, and are, therefore, not feasible for extending our predictions to dense retrieval. _Query Feedback (QF)_(Selvin et al., 2017) refers to the overlap of the returned documents with those obtained after applying pseudo-relevance feedback - yet,pseudo relevance feedback approaches for dense retrieval are still in their infancy , so we do not consider QF further.

In the remainder of this section, we discuss the main types of query performance predictors that could be applied to dense retrieval, specifically score-based unsupervised predictors (Section 2.1) and document representation-based predictors (Section 2.2).

### Score-based QPP

Score-based predictors encode certain assumptions about how the scores should be distributed for high or low-performing queries. For instance, a simple predictor might be the _Maximum Score_ among the retrieved documents  - the higher the maximum score, the more confident the retrieval system is that it has found a document that matches well the query. The most commonly applied score-based predictor is _Normalised Query Commitment (NQC)_, based on the standard deviation of the retrieval scores, which is negatively correlated with the amount of query drift (the non-related information in the result list) . Several variations of NQC have been proposed that further enhance its accuracy , incorporate the scores magnitude , or estimate a more robust version of variance with bootstrapping. Indeed, _Robust Standard Deviation estimator (RSD)_ extends NQC results to multiple contexts (each with a bootstrap sample) representing a population of scores . Score-based predictors (step 1 in Figure 1) are easily applicable to dense retrieval, since scores are computed by each retrieval method.

### Document Representation-based QPP

Predictors based on document representations  capture semantic relations either between queries, documents, or their interaction  - we discuss unsupervised and supervised predictors below.

#### 2.2.1. Unsupervised Coherence Predictors

In general, effective unsupervised predictors that consider document representations are preferable, since they require less computation than supervised predictors. One example of an unsupervised predictor that examines the lexical representations of documents is _spatial autocorrelation_, which considers the spatial proximity of lexical document representations, by using their pairwise TF.IDF-based similarities to produce a new set of scores "diffused in space". The final predictor is obtained by correlating the original scores with the diffused scores. Indeed, a low correlation between scores of topically-close documents is assumed to imply a poor retrieval performance.

Another family of recent coherence-based predictors creates a graph of the most similar documents among the top-ranked documents , based on their TF-IDF representations. Specifically, metrics such as Weighted Average Neighbour Degree (WAND) and Weighted Density (WD) were found to enhance the performance of score-based predictors after linear interpolation. These predictors (applied step 2 in Figure 1, top) were proposed for sparse document representations and have not previously been applied to dense embedded representations.

#### 2.2.2. Supervised & Neural Predictors

In general, supervised models for QPP can be attractive due to the varying sources of indicators for inferring query performance . At the same time, they are computationally complex compared to unsupervised predictors. For example, Neural-QPP  is a multi-component supervised predictor as the output of existing unsupervised QPP predictors with weak supervision - we can think of this as a neural supervised aggregation predictor. More recently, BERT-QPP  fine-tunes a BERT model for the QPP task by adding cross-encoder or bi-encoder layers that estimate an effectiveness measure (e.g. NDCG) based on the contents of the top returned document in response to the query. While BERT-QPP can also be applied to the dense retrieval rankings, it uses a different model to that used by the dense retrieval approach itself. Out of the two BERT-QPP variants, the bi-encoder version is closer to the intuition of single-representation dense retrieval. Finally, qppBERT-PL  adds an LSTM network on top of the BERT representation to model both document contents and the progression of estimated relevance in the ranking. Compared to BERT-QPP, this approach has promise as it considers more information than just the top-ranked document.

To summarise, existing predictors have either focused on sparse document representations or retrieval scores on the unsupervised side, or have introduced neural pre-trained architectures to create more complex supervised predictors. However, no work has addressed unsupervised predictors using dense embedded representations, as are readily available in dense retrieval configuration. Instead, we argue that by using simple predictors that consider document representation resulting from dense models (step 2 of Figure 1, bottom), we can accurately predict effectiveness without the need for supervised cross-encoder-based methods (step 3). In the next section, we detail existing predictors that can be applied to dense retrieval.

## 3. Coherence Predictors for Dense Retrieval

In this section, we first describe some existing sparse coherence-based predictors in Section 3.1, and then show how these can be adapted to be better suited for dense retrieval settings in Section 3.2.

### Sparse Coherence-based Methods

#### 3.1.1. Spatial Autocorrelation (AC)

First, consider \(d\) to be a document's TF.IDF vector. Then, the inner product of two documents at ranks \(i\) and \(j\) is given by \(sim(d_{i},d_{j})\). We can obtain a pairwise similarity matrix among \(k\) top-ranked documents as follows:

\[W=sim(d_{11})&sim(d_{12})&...&sim(d_{1k})\\...&...&...&...\\ sim(d_{k1})&sim(d_{k2})&...&sim(d_{kk}) \]

where \(k\) is the cutoff number of the top-k documents. For brevity of notation, let \(sim(d_{ij})=sim(d_{i},d_{j})\). Projecting (multiplying) each element of the matrix \(W_{ij}\) on the vector of the original retrieved scores, \(Score()\), we can obtain a new list of _diffused_ scores as:

\[Score()=W*Score(d) \]

Thereafter, an estimate of the spatial autocorrelation (AC)  is obtained by using the Pearson correlation between the two vectors:

\[AC=corr(Score(),Score(d)) \]

which quantifies the relation between the initial and diffused scores. Indeed, as mentioned above, a low correlation between the original retrieval scores (i.e. \(Score(d)\)) and those weighted by their topical similarity (the diffused scores, \(Score()\)) was found to imply poor retrieval performance .

#### 3.1.2. Network Metrics

As mentioned above, the matrix \(W\) represents all pairwise similarities between the top-retrieved documents. This matrix is equivalent to a fully connected network, where each node \(}\) corresponds to the \(d\) TF.IDF vector, and each edge \(}\) corresponds to each entry \(sim(d_{ij})\)(Bordes, 2013), or more formally \((q,D_{q}^{(k)})=\{},},W\}\). In this regard, to avoid all edges being considered equal without attention to the edge weight, the network is further pruned via thresholding (Bordes, 2013), where the similarities higher than the mean similarity value are selected as neighbours.

Consequently, we have the following definitions, which correspond to some recently proposed network metrics (Bordes, 2013) for QPP:

\[AverageNeighbourDegree(AND)=_{i=1}^{k}(}|}_{j  N_{d_{j}}}|N_{d_{j}}|) \]

where \(N_{d_{i}}\) is the neighbourhood of document \(d_{i}\). Typically, Equation (4) is applied on the pruned graph that only contains edges between the most similar documents, and hence corresponds to the more accurate _Weighted AND_ (WAND) measure (Bordes, 2013).

Another way to think about coherence is to count the observed edges or similarities over the set of all possible edges. This results in the Density measure, as follows:

\[Density(D)=}|}{|}|(|}|-1)} \]

In short, a higher neighbourhood degree and a higher density of a graph network indicates a more coherent set of top-retrieved results. The general intuition behind these measures is that the presence of coherence, as reflected by highly similar documents in a top-retrieved set indicates the ability of the retrieval method to distinguish relevant from non-relevant documents, and therefore, return the relevant ones at the top of the list.

### Dense Coherence-based Methods

We now derive the embedding representation variants of the above predictors in order to make them suitable for the prediction of neural dense retrievers. We first create the variants for embedding-based AC and network metrics, and then introduce a new variant that extends AC by considering rank groupings.

#### 3.2.1. AC-embs

Let \(_{d}\) and \(_{q}\) respectively represent the dense embedded representation of a document and a query. Firstly, we adapt autocorrelation, such that instead of TF.IDF vectors we consider the embedded document representations. Let the inner product of two documents at ranks \(i\) and \(j\) (with embeddings \(_{i}\) and \(_{j}\)) be written \(sim(_{dij})\), then we can define the pairwise similarities of the top \(k\) ranked documents as:

\[W^{}=sim(_{d11})&sim(_{d12})&...&sim(_{d1k})\\...&...&...&...\\ sim(_{dk1})&sim(_{dk2})&...&sim(_{dkk}) \]

We can then apply autocorrelation (denoted as AC above) as per Equations (2) & (3). We denote this as _AC-embs_.

#### 3.2.2. Network-embs

Similarly, and as we showed that the similarity matrix is equivalent to a fully connected network set of edges, we can apply WAND and WD as per Equations (4) & (5), denoted as _WAND-embs_ and _WD-embs_, respectively.

#### 3.2.3. pairRatio

We now introduce an extension of AC-embs inspired by visually exploring embedding relations. Specifically, in Figure 2, we visualise the pairwise similarity matrix (\(W_{}\)) obtained using TCT-ColBERT (Zhu et al., 2019) embeddings for the top-100 passages for the one high and one low performing query in the TREC Deep Learning Track 2019 queryset. For the best performing query, there is higher pairwise similarity among documents of top ranks (top left corner, indicated by a group of lighter shading), and lower correlation for lower ranks (darker shading). On the other hand, for the worst query, elements of darker shading appear at high ranks, indicating that the top-ranked documents may not be as coherent). In addition, there is less dark shading in low ranks compared to the best query. These observations inspire us to explore the trend of average top vs. bottom rank pairwise similarities of top-ranked embeddings.

Specifically, let \(W_{1.._{2}}^{}\) denote the (diagonal) subset of \(W^{}\) between ranks \(_{1}\) and \(_{2}\). Then, for a given rank threshold \(\), we can measure the ratio between the mean pairwise similarity above and below rank \(\), i.e. \(W_{0..}^{}\) and \(W_{..k}^{}\) as follows:

\[pairRatio(W^{})=(W_{1.._{1}}^{})(W_{_{j}..k}^{})^{-1} \]

where \(W^{}\) denotes the mean of the given matrix, \(_{i}\) corresponds to the end of the upper matrix, and \(_{j}\) symbolises the start of the lower matrix (we use the two cutoff points as separate hyperparameters). We called this predictor _pairRatio_. Unlike WAND and WD, we consider the magnitude of this contrast as indicative of query performance. We believe that, since this relates to the retrieval method itself, it should be indicative of query performance especially for advanced retrieval methods.

Still, the similarity matrix \(W^{}\) can only provide information about the relative similarity of documents. Introducing some information about the document scores would increase performance prediction accuracy, since it relates to the absolute ranking of each document. Let \(A\) be an adjusted matrix, where each entry, for a document pair \(i\) and \(j\) is multiplied by the final similarity of the query to each of the documents:

\[A_{ij}=W_{ij}(_{i}_{q})(_{j}_{q}) \]

\(A\) better encodes similarity of the query among the pairwise document similarities. pairRatio (Equation (7)) can then be applied upon \(A\), which we denote as adjusted pairRatio, or _A-pairRatio_.

In short, we are interested in the effectiveness of these predictors based on dense document representations and how they perform

Figure 2. Heatmap of pairwise similarity matrix of the top-100 TCT-ColBERT document embeddings for returned for the best (query id 104861 with NDCG@10–1) and worst performing queries (query id 489204 with NDCG@10–0.189) from the TREC DL 19 queryset.

in relation to their sparse versions. We test their performance compared to score-based and supervised predictors in Section 5.

## 4. Experimental Setup

Our experiments address the following research questions:

**RQ1** How do unsupervised coherence-based predictors compare to unsupervised score-based predictors in dense and sparse retrieval?

**RQ2** How do unsupervised predictors perform compared to supervised predictors in dense and sparse retrieval?

To address these research questions, our setup is as follows:

**Datasets:** We use the MSMARCO passage ranking corpus, and apply the TREC Deep Learning track 2019 and 2020 query sets, containing respectively 43 and 54 queries with relevance judgements. In particular, each query in these quersets contains many judgements obtained by pooling various distinct retrieval systems.

**QPP Predictors:** As unsupervised score-based predictors, we apply Max score (MAX) (Zhou et al., 2019), and NQC (Zhou et al., 2019). As a representative variant of NQC, we choose _RSD_. This bootstrap-based predictor is the most recent NQC variant and was shown to outperform other score-based predictors. Specifically, we use the _RSD(uni)_ version which samples documents uniformly. For each cutoff, we sample from 0.60 to 0.80 of the initial result list size. We use spatial autocorrelation (AC) (Zhou et al., 2019), WAND and WD (Abadi et al., 2016), and the interpolation of WAND and WD with NQC (following the findings of the original paper Abadi et al. (2016), which suggest that network metrics further increase the performance of NQC). We also report our embedding variants (AC-embs, WAND-embs, WD-embs, PairRatio, A-PairRatio). For each unsupervised predictor, we tune the hyperparameters of each dataset on the other. Specifically, to tune the cutoff value for the top-\(k\) documents all unsupervised predictors including ours, we use a grid of values . For PairRatio and A-PairRatio, we also vary the other upper and lower rank threshold hyperparameters \(_{l}\) and \(_{j}\).

For supervised predictors, we report the bi-encoder and cross-encoder variants of BERT-QPP (Devlin et al., 2019). To achieve this, we retrained the BERT-QPP cross-encoder and bi-encoder models specifically for each of the dense retrieval models. These supervised predictors exhibit their highest correlations mainly for MRR, which means that they train models that estimate the relevance of the top document of a ranking. In this regard, we check whether an alternative supervised predictor (which we call _top-1(monoT5)_) that uses only the top-retrieved document to a monoT5 model (Zhou et al., 2019) - i.e. trained for relevance estimation and ranking rather than performance prediction - can perform well in dense retrieval. Note that we deliberately use the term _QPP Predictors_ instead of _baselines_, since our purpose is not to demonstrate the superiority of a single predictor, but rather how a _group of predictors_ behaves under different contexts and retrieval models.

**Retrieval Systems**: We deploy three retrieval approaches: BM25 sparse retrieval (applying Porter's English stemmer and removing standard stopwords) as implemented by Terrier (Terrier, 2018), and two single-representation dense retrieval approaches, namely ANCE (Zhou et al., 2019), and TCT-ColBERT (Zhou et al., 2019) with PyTerrier (Vaswani et al., 2017) integrations.1

**Measures:** Following the TREC 2019 Deep Learning Track Overview (Chen et al., 2019), we measure system effectiveness in terms of NDCG@10 and MAP@100. We further add MRR@10, following some recent work (Devlin et al., 2019; Zhou et al., 2019). To quantify the accuracy of the QPP techniques, we adopt Kendall's \(\) correlation measure, as typically reported in QPP literature (Kendall, 2018; Liang et al., 2019; Zhou et al., 2019; Zhou et al., 2019; Zhou et al., 2019; Zhou et al., 2019).2

## 5. Correlation Results

Tables 1 and 2 show the accuracy of all our examined predictors on the TREC DL 2019 and 2020 query sets, respectively. Within each table: groups of columns denote the various retrieval approaches; the uppermost row reports the mean effectiveness of each ranking approach for each evaluation measure; the next group of rows contains the Kendall's \(\) correlation of the score-based predictors, the next one the unsupervised lexical coherence-based predictors; then we report the results for the embedding-based predictors; and finally for the supervised predictors (Devlin et al., 2019).

### RQ1: Score-based vs Coherence-based Predictors

As expected, for BM25, distribution-based score predictors (NQC and RSD(uni) show high accuracy for MAP@100 and NDCG@10, while their accuracy is lower for MRR@10, especially for DL 19. However, unlike older datasets, sparse coherence predictors are very low for TREC DL datasets. As for dense coherence predictors, surprisingly, AC-embs variant is the best performing predictor for AP@100, and for NDCG@10 on 2020. As for our pairRatio variants, they are less effective than other unsupervised predictors, such as NQC and AC-embs (except for MRR@10), as well as supervised predictors on MRRR@10.

Next we consider the two dense retrieval settings, i.e. ANCE & TCT-ColBERT. For TCT-ColBERT, we observe that our pairRatio predictors outperform not only supervised predictors, but also NQC (the best performing unsupervised predictor) for NDCG@10 and MRR@10 for both datasets, are only behind RS(uni) for MRR@10 in the DL 2019 dataset, and are competitive for AP@100. Another observation is that A-pairRatio has increased the accuracy compared to pairRatio, particularly for the TCT-ColBERT model, which indicates the need for including document-query relations. In summary, for NDCG@10 and MRR@10, for TREC DL 2020, in all four cases our dense coherence-based predictors (any of them considered) outperform score-based predictors; for TREC DL 2019, in two of the four cases ours are higher, in one case RSD is higher, and in one case they are identical. For ANCE, WAND-embs and WD-embs are better than score-based predictors for NDCG@10 and MRR@10 for the 2020 dataset, while they are only slightly behind them in the 2019 dataset. Overall, for MAP@100, NQC or RSD (uni) consistently outperform coherence-based predictors, while for NDCG@10 and MRR@10, the picture is more unstable; however, in most cases, coherence-based predictors win for dense retrieval. Further, as might be expected, changing the type of representations from sparse to dense increases the performance of coherence-based predictors across the dense retrieval settings (for ANCE,in 7 out of 

[MISSING_PAGE_FAIL:6]

well for MAP@100, while coherence-based predictors show increased accuracy for NDCG@10 and MRR@10. For sparse retrieval, dense coherence predictors are in general better than score-based predictors.

### RQ2: Unsupervised vs. Supervised Predictors

Next, we compare the performance of unsupervised with supervised QPP predictors for each retrieval method. For BM25, we are able to reproduce the results of the bi-encoder and cross-encoder variants of BERT-QPP, as reflected by the higher values in MRR and the competitive correlation on the other two metrics. For BM25, we used the authors' checkpoints, while we re-trained the method for ANCE & TCT-ColBERT. However, their values are still lower than NQC, a simple score-based unsupervised predictor), and RSD(uni) (NDCG@10 on the TREC 2019 queryset), our pairRatio (MRR@10 on the 2019 queryset), AC-embs (AP@100 on 2019, AP@100 on 2020, NDCG@10 on 2020), and top-1 monoT5 (MRR@10 on both datasets). Most importantly, for the two dense retrieval methods, supervised predictors are not as effective as unsupervised predictors, such as Max and NQC. For TCT-ColBERT, supervised predictors are less effective than our pairRatio variants for NDCG@10 and MRR@10, and NQC and RSD(uni) for all metrics. The strongest observed correlations of BERT-QPP variants in dense retrieval are for AP@100. However, they have a cost to deploy (applying a BERT model on the top-ranked result). We argue that this resource would be better spent to re-rank the top results. In addition, the simpler'supervised' variant, _top-1(mono-T5)_, which uses the monoT5 score of the top-ranked document is a more accurate predictor than BERT-QPP across all retrieval methods, particularly for MRR@10, which is the metric that BERT-QPP is most competitive. This surprising result shows that BERT-QPP is itself just a relevance estimator for the top-ranked document that has been trained to predict MRR@10; using any effective relevance estimator can do as good a job, if not better. To answer RQ2, we find that the existing BERT-QPP supervised predictors are less accurate than unsupervised predictors (existing and ours) for dense retrieval.

## 6. Modeling Query Differences in QPP

The performance of dense coherence-based predictors is particularly accurate in certain dense retrieval settings (for TCT-ColBERT: pairRatio and A-pairRatio, for ANCE: WAND-embs and WD-embs) and shows superior performance for especially NDCG@10. Still, score-based predictors are often better for MAP@100. This difference in QPP correlations among evaluation metrics motivates us to explore whether the relationship between QPPs and retrieval effectiveness is mediated by the type of query (for instance queries of an Experience type have been found difficult to answer (Brock et al., 2018)). For this purpose, we apply a distribution-based QPP evaluation approach based on the scaled Absolute Rank Error (sARE) (Zhu et al., 2017). Specifically, the sARE value each query is calculated as: \(sARE_{q_{i}}=^{P}-x_{i}^{}|}{|Q|}\) where \(r_{i}^{P}\) and \(r_{i}^{}\) are the ranks assigned to query \(i\) by the QPP predictor and the evaluation metric, respectively (one sARE value is obtained per query, instead of a point estimate). This further allows using sARE in statistical models (Zhu et al., 2017; Zhu et al., 2017). Unlike (Zhu et al., 2017; Zhu et al., 2017) who use ANOVA, we use _Linear Mixed Effects (LME)_ models (Zhu et al., 2017; Zhu et al., 2017; Zhu et al., 2017), which also belong to Generalised Linear Models (GLM) (Zhu et al., 2017; Zhu et al., 2017), but split the total explained variance in s_ARE_ into 2 levels.

Specifically, Level 1 specifies the within-query variations (how each query changes or the per query variance over different QPP predictors). Level 2 specifies the between-query differences; it further explains each part of Level 1 by showing, how it changes according to a between-query factor - here we use the type of query or _query type_ as proposed in (Brock et al., 2018). A 2-Level approach is necessary to model the interplay of QPPs with query types; while each query receives a separate sARE value for each QPP predictor, multiple queries in the same type share the same sARE, and are, therefore, nested within their group (each query belongs to only one level of query type). Thus, the multilevel approach allows splitting the total variation in sARE into within (due to QPPs - Level 1)- and between-query (due to query types - Level 2) variation. Using separate models for each evaluation measure allows to check which measure is more affected by query types. Next, we describe LMEs in detail.

### Linear Mixed Model Definitions

First, our full model, denoted as \(LME_{full}\), is defined as :

**Level 1**

\[sARE_{ij}=_{0i}+_{1i}(QPPPredictor)+_{ij} \]

with \(_{ij} N(0,_{e}^{2})\)

where \(sARE_{ij}\) is the sARE of query \(i\) at QPP predictor measurement \(j\), \(_{0i}\) is the intercept (initial status) of query \(i\)'s change trajectory (reference QPP predictor, i.e., the first QPP measurement), \(_{1i}\) is the slope (rate of change) in sARE (per predictor unit), and \(_{ij}\) are the deviations of a query's equation on each measurement. This is also a way for Level 1 to check for statistically significant differences between predictors.

   Parameter & Interpretation \\   \\    } & average true sARE for the reference QPP predictor for the reference (without the effect of) query type \\  & average difference in sARE between different query types for the reference QPP predictor \\  & average true rate of change in sARE per unit change \\  & in QPP predictor for the reference (without the effect of) \\  & query type \\  & average difference in sARE between different query types per unit change in QPP predictor \\   \\    } & allow individual true query trajectories to be scattered around the average query true change trajectory \\  & allows individual query data to be scattered around individual query true change trajectory \\    } & Variance Components \\   & level 1 (residual) variance, variability around each \\  & query’s true change trajectory \\  & level 2 variance in reference predictor and rate of \\  & change per predictor measurement, how much between- \\  & query variability is left after accounting for query type \\  & residual covariance between true sARE for the reference \\  & (initial) predictor and rate of change, controlling for \\  & query type, across all queries \\   

Table 3. Explanation of terms included in the linear mixed effects full model.

[MISSING_PAGE_FAIL:8]

score-based compared to dense coherence-based predictors. This indicates that our proposed predictors are less sensitive to query type compared to score-based and supervised predictors. Note that while we plot the full model, for \(sARE_{NDCG}\), \(LME_{QPP}\) was preferred, i.e., only an effect of QPP predictor. This is complemented by Table 5, where \(sARE_{NDCG}\) contain a coefficient for QPPs, but not for query types or their interaction with QPPs.

To summarise, in Section 5.1, we observed that score-based predictors showed improved performance for MAP@100, but our LME analysis showed that this result is susceptible to influential query types. Instead, our dense coherence-based predictors showed higher correlations mainly for NDCG@10, and with the LME analysis (lack of query type terms and \(Pseudo-R^{2}\) terms at Level 2), we showed that this is more stable across different query types. Therefore, our predictors provide promising evidence for generalisability compared to existing predictors. In other words, while both MAP@100 and NDCG@10 are sensitive to QPPs, NDCG@10 is less sensitive to query type variations than MAP@100, thereby answering RQ4.

## 7. Conclusions

We examined the accuracy of QPP upon two single-representation dense retrieval methods (ANCE and TCT-ColBERT). In particular, we proposed new variants of unsupervised coherence-based predictors and managed to increase their performance for dense retrieval. In this way, we showed that changing the representations from TF.IDF to neural embeddings provided by the dense retrieval models together with some further modifications is enough to generalise performance of unsupervised predictors in relation to supervised ones. Indeed, with increasing effectiveness brought by dense retrieval methods, our proposed predictors becomes more competitive, especially for NDCG@10 and MRR@10. Also, we highlighted that focusing on a single evaluation measure to optimise a proposed predictor can be problematic and may falsely inform future studies, since MAP@100 and NDCG@10 cannot be used interchangeably. At the same time, we demonstrated the interplay between the different QPP predictors, evaluation metrics, and the particular types of queries. Importantly, we showed that while score-based predictors still remain very competitive for MAP@100, our examined statistical models indicate that MAP@100 is highly influenced by the type of query. Instead, using NDCG@10, QPP performance is more stable across queries, and since our proposed predictors show higher performance on this metric, this is a promising result for more generalisable performance in dense retrieval.

    &  &  &  \\  sARE \(\) & MAP & NDCG & MAP & NDCG & MAP & NDCG \\  \(Pseudo-R_{c}^{2}\) & 13.4\% & ✗ & 7.5\% & ✗ & 12.4\% & 14.6\% \\ \(Pseudo-R_{c}^{2}\) & ✗ & ✗ & 17.2\% & ✗ & 2.2\% & 9.9\% \\ \(Pseudo-R_{c}^{2}\) & ✗ & ✗ & 35.6\% & ✗ & 22.8\% & 8.1\% \\  \(Y_{00}\) & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\ \(Y_{01}\) & ✓ & ✗ & ✓ & ✗ & ✓ & ✗ \\ \(Y_{11}\) & ✗ & ✗ & ✓ & ✗ & ✓ & ✗ \\   

Table 6. Proportion of explained variance per component and included fixed effects in each LME for all three retrieval methods. ✓ indicates the presence of a fixed effect in LMEs, while ✗ shows the absence of either an important contribution of a factor (top) or a fixed effect (bottom).

Figure 3. LME results from the full model for TCT-ColBERT.

    &  \\  \(sARE_{MAP}\) & \(sARE_{ij}=[0.29-0.009(QPPPredictor_{ij})]+[_{0i}+_{i1}(QPPPredictor _{ij})+_{ij}]\) \\ \(sARE_{NDCG}\) & \(sARE_{ij}=0.26+_{0i}+_{ij}\) \\ \(sARE_{i1}=0.30+_{0i}+_{ij}\) \\  \(sARE_{MAP}\) & \(sARE_{ij}=[0.28-0.008(QPPPredictor_{ij})+0.25(NotAQ_{i})+0.05(NotAQ_{i})(QPPPredictor _{ij})]+[_{0i}+_{i1}(QPPPredictor_{ij})+_{ij}]\) \\ \(sARE_{NDCG}\) & \(sARE_{ij}=0.25+_{0i}+_{ij}\) \\ \(sARE_{i1}=[0.35-0.008(QPPPredictor_{ij})]+[_{0i}+_{i1}(QPPPredictor _{ij})+_{ij}]\) \\  TCT-ColBERT \\  \(sARE_{MAP}\) & \(sARE_{ij}=[0.32-0.01(QPPPredictor_{ij})+0.05(Experience_{i})(QPPPredictor _{ij})]+[_{0i}+_{i1}(QPPPredictor_{ij})+_{ij}]\) \\ \(sARE_{MAP}\) & \(sARE_{ij}=[0.32-0.01(QPPPredictor_{ij})+0.02(Reason_{i})(QPPPredictor_{ij})]+[ _{0i}+_{i1}(QPPPredictor_{ij})+_{ij}]\) \\ \(sARE_{NDCG}\) & \(sARE_{ij}=[0.32-0.008(QPPPredictor_{ij})]+[_{0i}+_{i1}(QPPPredictor _{ij})+_{ij}]\) \\ \(sARE_{i1}=0.32+_{0i}+_{ij}\) \\   

Table 5. Resulting LME models for each retrieval method and all metrics.

[MISSING_PAGE_EMPTY:10]

* Zhou and Croft (2007) Yun Zhou and W Bruce Croft. 2007. Query performance prediction in web search environments. In _Proc. SIGIR_.