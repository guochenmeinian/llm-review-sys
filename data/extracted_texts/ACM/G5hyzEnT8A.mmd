# A Multifaceted Look at Starlink Performance

Anonymous Author(s)

###### Abstract.

In recent years, Low-Earth Orbit (LEO) mega-constellations have emerged as a promising network technology and have ushered in a new era for democratizing Internet access. The Starlink network from SpaceX stands out as the only consumer-facing LEO network with over 2M+ customers and more than 4000 operational satellites. In this paper, we conduct the first-of-its-kind extensive multi-faceted analysis of Starlink network performance leveraging several measurement sources. First, based on 19.2M crowdsourced M-Lab speed test measurements from 34 countries since 2021, we analyze Starlink global performance relative to terrestrial cellular networks. Second, we examine Starlink's ability to support real-time web-based latency and bandwidth-critical applications by analyzing the performance of (i) Zoom video conferencing, and (ii) Luna cloud gaming, comparing it to 5G and terrestrial fiber. Third, we orchestrate targeted measurements from Starlink-enabled RIPE Atlas probes to shed light on the last-mile Starlink access and other factors affecting its performance globally. Finally, we conduct controlled experiments from Starlink dishes in two countries and analyze the impact of globally synchronized "15-second reconfiguration intervals" of the links that cause substantial latency and throughput variations. Our unique analysis provides revealing insights on global Starlink functionality and paints the most comprehensive picture of the LEO network's operation to date.

## 1. Introduction

Over the past two decades, the Internet's reach has grown rapidly, driven by innovations and investments in wireless access (Zhou et al., 2017; Wang et al., 2018; Wang et al., 2018) (both cellular and WiFi) and fiber backhaul deployment that has interconnected the globe (Bauer et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). Yet, the emergence of Low-Earth Orbit (LEO) satellite networking, spearheaded by ventures like Starlink (Solarlink, 2018), OneWeb (Web, 2018), and Kuiper (Bauer et al., 2018), is poised to revolutionize global connectivity. LEO networks consist of mega-constellations with thousands of satellites orbiting at 300-2000 km altitudes, offering ubiquitous _low latency_ coverage worldwide. Consequently, these networks are morphing into "global ISPs" capable of challenging existing Internet monopolies (Solarlink, 2018), bridging connectivity gaps in remote regions (Solarlink, 2018; Wang et al., 2018), and providing support in disaster-struck regions with impaired terrestrial infrastructure (Solarlink, 2018).

_Starlink_ from SpaceX stands out with its expansive fleet of 4000 satellites catering to 2M+ subscribers across 63 countries (Solarlink, 2018; Wang et al., 2018). The LEO operator plans to further amplify its coverage and quality of service (QoS) by launching \(\) 42,000 additional satellites in the coming years (Solarlink, 2018). However, despite significant global interest and the potential to impact the existing Internet ecosystem, only limited explorations have been made within the research community to understand Starlink's performance. The challenge stems from a lack of global vantage points required to accurately gauge the network's performance since factors such as orbital coverage, density of ground infrastructure, etc., can impact connectivity across regions. Initial studies have resorted to measurements from a handful of geographical locations (Solarlink, 2018; Wang et al., 2018; Wang et al., 2018) or extrapolated global performance through simulations (Solarlink, 2018) and emulations (Solarlink, 2018). However, the community agrees on the limited scope of such studies and has made open calls to establish a global LEO measurement testbed to address this challenge (Solarlink, 2018; Wang et al., 2018; Wang et al., 2018). Some researchers have navigated around this hurdle by exploring alternative measurement methods, e.g., by targeting exposed services behind user terminals (Zhou et al., 2017) or by mining speed test reports shared on social media platforms, such as Reddit (Reddit, 2018). While innovative, we argue that these techniques are insufficient to uncover the intricacies affecting the network, specifically its capability to support web applications.

This paper addresses this knowledge gap and provides the first comprehensive multi-faceted measurement study on Starlink. Our work is distinct from previous works in several ways. Firstly, we examine the global evolution of the network since 2021 by analyzing the M-Lab speed test measurements (Wang et al., 2018) from 34 countries (largest so far). We complement our investigation through active measurements over 98 RIPE Atlas (Solarlink, 2018) probes in 21 countries and conduct high-resolution experiments over controlled terminals in two European countries to investigate real-time web application performance and factors impacting Starlink's last-mile access. Specifically, we make the following contributions.

**(1)** We present a longitudinal study of global Starlink latency and throughput performance from M-Lab users in SS4. Our analysis, incorporating \(\) 19.2 M samples, reveals that Starlink performs competitively to terrestrial cellular networks. However, its performance varies globally due to infrastructure deployment differences, and is dependent on the density and closeness of ground stations and Point-of-Presence (PoP). We also observe signs of _bufferbloating_ as Starlink's latency increases by several factors under traffic load.

**(2)** We assess and compare the performance of real-time web applications, specifically Zoom video conferencing and Amazon Luna cloud gaming, to terrestrial networks (SS5). We find that, under optimal conditions, Starlink is capable of supporting such applications, matching the performance over cellular; however, we do observe some artifacts due to the network's periodic reconfigurations.

**(3)** We perform targeted measurements from Starlink RIPE Atlas (Solarlink, 2018) probes and leverage their diverse locations to characterize the satellite last-mile "bent-pipe" performance (SS6.1). We find that the "bent-pipe" latency within the dense 53' shell remains consistent worldwide (\(\) 40 ms), and is significantly lower to yet incomplete 70' and 97.6' orbits. We also find evidence of Starlink inter-satellite links (ISLs) connecting remote regions, showcasing superior performance to terrestrial paths in our case study.

**(4)** Our high-frequency measurements from terminals in two European countries confirm that Starlink performs network reconfigurations every 15s, leading to noticeable latency and throughput degradations at sub-second granularity. By correlating data from our terminals, one covered by 53' and the other restricted to 70' and 97.6' connectivity, we find that the reconfigurations are globally synchronized events and likely independent of satellite handovers.

Leveraging multi-dimensional, global, and controlled high resolution measurements, our findings distinctively advance the state-of-the-art by illuminating Starlink's global performance and the

## 2. Background

Starlink is a LEO satellite network operated by SpaceX that aims to provide global Internet coverage through a fleet of satellites flying at \(\) 500 km above the Earth's surface. The majority of Starlink's operational 4000 satellites lie within the 53's shell, which only covers parts of the globe (see Figure 1). The 70' and 97.6' orbits allow serving regions near the poles. These other shells however have fewer satellites (see Appendix A, Table 2 for constellation details).

Figure 2 shows the cross-section of Starlink end-to-end connectivity. To access the Internet over the Starlink network, end-users require a dish, a.k.a. "Dishy"1, that communicates with satellites visible above 25's of elevation through phased-array antennas using Ku-band (shown as User Link (UL)). Starlink satellites, equipped with multiple antennas subdivided into beams, can connect to multiple terminals simultaneously (Hendy et al., 2016) and relay all connections to a ground station (GS) on a Ka-band link (shown in green). The connection forms a direct "bent-pipe" in case the terminal and GS lie within a single satellite's coverage cone; otherwise, the satellites can relay within space to reach far-off GSs via laser inter-satellite links (ISLs), forming an "extended bent-pipe". Note that not all Starlink satellites are ISL-capable and it is difficult to effectively estimate ISL usage as Starlink satellites have no user visibility at IP layer and, therefore, do not show up in traceroutes.

Finally, the GSs relay traffic from satellites to Starlink point-of-presence (PoP) through a wired connection, which routes it to the destination server via terrestrial Internet (Berg et al., 2016). The public availability of GS deployment information differs across countries. No official source exists, so we rely on crowdsourced data for the geolocations of GSs and PoPs (Sanderson, 2016), which is also shown in Figure 1.

## 3. Measurement Methodology

### Global Measurements

_Measurement Lab (M-Lab)_ M-Lab (Marcourt et al., 2016) is an open-source project that allows users to perform end-to-end throughput and latency speed tests from their devices to 500+ servers in 60+ metropolitan areas (Sanderson, 2016). Google offers M-Lab measurements when a user searches for "speed test" (Sanderson, 2016), serving as the primary source of measurement initiations (Marcourt et al., 2016; Sanderson, 2016; Sanderson, 2016). At its core, M-Lab uses the Network Diagnostic Tool (NDT) (Sanderson, 2016), which measures uplink and downlink performance using a single 10 s WebSocket TCP connection. The platform also records fine-grained transport-level metrics (tcp_info), including goodput, round-trip time (RTT) and losses, along with IP, Autonomous System Number (ASN), and geolocation of both the end-user device and the selected M-Lab server. We identify measurements from the Starlink clients via their ASN (AS14593). The M-Lab dataset includes samples from 59 out of 63 countries where Starlink is operational. We restrict our analysis to ndT measurements, which use TCP BBR and countries with _at least 1000 measurements_, resulting in 19.2 M M-Lab measurement samples from 34 countries. Our analysis chronicles the global Starlink operation from its inception, as the first measurement samples in our dataset are dated to June 2021, which is closely aligned with the launch of Starlink v1.0 & v1.5 satellites (Sanderson, 2016). We find that the M-Lab server selection algorithm assigns the geographically closest server to the estimated client location (Sanderson, 2016), which might not always be optimal for Starlink, given its PoP-centered architecture. While we examine such artifacts by contrasting the M-Lab and RIPE Atlas results (86.1), we approached our analysis with caution, particularly when examining fine-grained region-specific insights.

_RIPE Atlas._ RIPE Atlas is a measurement platform that the networking research community commonly employs for conducting measurements (Sanderson, 2016). The platform comprises thousands of hardware and software probes scattered globally, enabling users to carry out active network measurements such as ping, traceroute, and DNS resolution to their chosen endpoints. In our study, we utilized 98 Starlink RIPE Atlas probes across 21 countries (see Figure 3). Our measurement targets were 145 data centers from _seven_ major cloud providers - Amazon EC2, Google, Microsoft, Digital Ocean, Alibaba, Amazon Lightsail, and Oracle (see Appendix B). The chosen operators represent the global cloud market (Sanderson, 2016; Sanderson, 2016; Sanderson, 2016) and ensure that our endpoints are close to Starlink Pops, which are usually co-located with Internet eXchange Point (IXP) or data center facilities (Sanderson, 2016; Sanderson, 2016). We perform ICMP traceroutes from Atlas probes to endpoints situated on the same or neighboring continent. We extract and track per-hop latencies between Starlink probe terminal-to-GS (identified by static 100.64.0.1 address), GS-to-PoP (172.16/12 address) and PoP-to-endpoint at 2 s intervals (Sanderson, 2016). Additionally, to improve PoP geolocations, we extract semantic location embeddings in reverse DNS PTR entry,

Figure 1. Orbits of three Starlink inclinations and crowdsourced Ground Station (GS) and Point-of-Presence (PoP) locations (Sanderson, 2016). Shaded regions depict Starlink’s service area.

Figure 2. Starlink follows “bent-pipe” connectivity as traffic traverses the client-side terminal, one or more satellites via inter-sat links (ISLs), nearest ground station (GS), ingressing with the terrestrial Internet via a point-of-presence (PoP).

e.g. tata-level3-seattle2.level3.net [(35)]. Our measurements over _ten_ months (Dec 2022 to Sept 2023) resulted in \(\) 1.8 M samples.

### Real-time Web Application Measurements

#### 3.2.1. Zoom Video Conferencing

We experimented with Zoom videoconferencing [(74)] due to its popularity in the Internet ecosystem [(12)] as well as latency and bandwidth-critical operational requirements. We set up a call between two parties, one using a server with access to an unobstructed Starlink dish and high-speed terrestrial fiber over 1 Gbps Ethernet. The other end was on an AWS machine located close to the assigned Starlink PoP. We set up virtual cameras and microphones on both machines, which were fed by a pre-recorded video of a person talking, resulting in bidirectional transmission. Both machines were time-synchronized to local stratum-1 NTP servers and we recorded (and analyzed) Zoom QoS leveraging the open-source toolchain from [(42)] that yields sub-second metrics.

#### Cloud Gaming

We also experiment with cloud gaming due to its demanding high throughput and low delay requirements [(43)]. We leverage the automated system by Ibql et al. [(18)] to evaluate the performance of playing the racing game "The Crew" on the Amazon Luna [(2)] platform. The measurements are based on a customized streaming client that records end-to-end information about media streams, such as frame and bitrate. The system also utilizes a bot that executes in-game actions at pre-defined intervals that trigger a predictable and immediate visual response. In post-processing, their analysis system detects the visual response and computes the _game delay_ as the time passed since the input action was triggered. Amazon Luna serves games at a resolution of up to 1920\(\)1080 at 60 FPS and adaptively reduces the resolution to, e.g., 1280\(\)720. We ran the game streaming client on the same machine as the Zoom measurements, additionally setting up a 5G modem to compare Starlink against cellular network. Similar to Zoom, the Luna game server was on AWS server close to our Starlink PoP (\(\) 1 ms RTT).

#### 3.2.2. Satisfiable

A significant limitation of our global measurements is their lack of sub-second visibility, which is essential for understanding the intricacies of Starlink network behavior. To allow us to obtain microscopic understanding, we orchestrated a set of precise, tailored, and controlled experiments, utilizing two Starlink terminals as vantage points (VPs) situated in two European countries. One connects to the 53" shell while the other, deployed in a high latitude location, can be shielded to confine its communication to the 70" and 97.6" orbits (see Figure 4). We placed a metal sheeting2 barrier at the Southfacing angle of the terminal, which obstructed its view from the 53" inclinations. We verify with external satellite trackers [(28; 54)] that the terminal only received connectivity from satellites in 97" or 70" inclinations, which resulted in brief _connectivity windows_ followed by periods of no service. We performed experiments using the Isochronous Round-Trip Tester (irtt) [(52)] and iperf [(17)] tools. The irtt setup records RTTs at high resolutions (3 ms interval) by transmitting small UDP packets. The irtt servers were deployed on cloud VMs in close proximity to the assigned Starlink PoP of both VPs (within 1 ms) - minimizing the influence of terrestrial path on our measurements. We used iperf to measure both uplink and downlink throughput and record performance at 100 ms granularity. Simultaneously, we polled the gRPC service on each terminal [(61)] every second to obtain the connection status information.

## 4. Global Starlink Performance

We use the minimum RTT (minRTT) reported during nd7 tests to the closest M-Lab server globally to quantify the baseline network performance. This metric is not affected by queuing delays prevalent during throughput measurements which results in elevated latencies. To put the Starlink latency into context, we select speedtests originating from terrestrial serving-ISPs to capture mobile network traffic. We filter measurements from devices connected to the top-3 mobile network operators (MNOs) in each country (see Appendix C for details). Note that our criterion results in a mix of wired and wireless access networks since M-Lab does not provide a way to distinguish between the two. Our endpoint selection remains the same for both Starlink and terrestrial networks (see SS3.1).

#### Global View

Figure 5 shows that, for a majority of countries, clients using terrestrial ISPs experience better latencies over Starlink. While the median latency of Starlink hovers around 40-50 ms in most countries, this distribution varies significantly across geographical regions. For instance, in Colombia, Starlink clients report better latencies than those utilizing established terrestrial networks. Conversely, in Manila (The Philippines), Starlink's performance is notably inferior (Figure 6). The uneven distribution of GSs and PoPs (Figure 1) may explain the latency differences; the USA, which experiences significantly lower latencies, also boasts a robust ground infrastructure. Similar trends are seen in Kenya and Mozambique, where the closest PoP is located in Nigeria.

Figure 4. Field-of-view experiment setup. Dishy, deployed at a high latitude location, is obstructed by a metal shielding, which restricts its connectivity to the 70° and 97.6° orbits.

Figure 3. Overview of global Starlink measurements in this study. Heatmap denotes M-Lab speedtest measurement densities. Starlink RIPE Atlas probes are shown as red circles.

_Well-Provisioned Regions._ Even though a significant portion of global Starlink measurement samples originate from Seattle (\( 10\%\)), the region shows consistently low latencies, with the 75th percentile well below 50 ms (Figure 6). Contributing factors can be dense GS availability or internal service prioritization for Starlink's headquarters. However, we observe that Starlink performance is fairly consistent across the USA, confirming that Seattle is not an anomaly but the norm (see Figure 21a in Appendix D). This result highlights the LEO network's potential to bridge Internet access disparities, which significantly affects the quality of terrestrial Internet in the USA (Song et al., 2018; Zhang et al., 2019). Europe is also relatively well covered with GSs but hosts only three PoPs that are in the UK, Germany, and Spain. Proximity to the nearest PoP correlates strongly with minRTT performance in Figure 7 - Dublin, London, and Berlin exhibit latencies comparable to the US, while for Rome and Paris, the 75th percentile is \( 20\) ms longer. Unlike US, Starlink in EU has significantly longer tail latencies, often surpassing 100 ms.

_Under-Provisioned Regions._ Starlink's superior performance in Colombia hints at its potential for connecting under-provisioned regions. However, Figure 6 shows that Starlink in South America (SA) trails significantly behind the US and Europe, with the 75th percentile exceeding 100 ms and tail at 200 ms. We observe similar performances in Oceania (see Figure 21b in Appendix D). By extracting the share of satellite vs. terrestrial path (from PoP to M-Lab servers, see Figure 18 in Appendix D)3, we find that the majority of SA Starlink latency comes from the bent-pipe. In contrast, latencies from Mexico and Africa (except Nigeria) show significant terrestrial influence, which we allude to non-optimal PoP assignments by Starlink routing policies.

We observed an interesting impact of ground infrastructure deployment in the Philippines, where a local PoP was deployed in May 2023. Prior to this, Starlink traffic from the Philippines was directed to the nearest Japanese PoP, traversing long submarine links to reach the geographically closest M-Lab server in-country - evident from Figure 19 in Appendix D which shows additional 50-70 ms RTT incurred by Philippine users to reach in-country vs. Japanese M-Lab servers. However, post-May 2023, the latencies to

Figure 8. RTT inflation (maxRTT) during M-Lab speedtests over Starlink: (a) download, (b) upload traffic.

Figure 7. Distributions of M-Lab minRTTs from select cities in Europe and South America, respectively.

average goodputs. Given its high measurement density at this location, this trend might be attributable to Starlink's internal throttling or load-balancing policies aimed at preventing congestion on the shared network infrastructure (Safania et al., 2017). We also find that over the past 17 months, Starlink goodputs have stabilized rather than increased, with almost all geographical regions demonstrating similar performance (shown in Figure 23 in Appendix D).

_Takeaway \( 1\)_ -- Starlink exhibits competitive performance to terrestrial ISPs on a global scale, especially in regions with dense GS and PoP deployment. However, noticeable degradation is observable in regions with limited ground infrastructure. Our results further confirm that Starlink is affected by bufferbloat. Over the past 17 months, Starlink appears to be optimizing for consistent global performance, albeit with a slight reduction in goodput, likely due to the increasing subscriber base.

## 5. Real-Time Application Performance

While the global Starlink performance in SS4 is promising for supporting web-based applications, it does not accurately capture the potential impact of minute network changes caused by routing, satellite switches, bufferbloating, etc., on application performance. Real-time web applications are known to be sensitive to such fluctuations (Boon and Amazon Luna, 2010; Safania et al., 2017; Safania et al., 2017). In this section, we examine the performance of Zoom and Amazon Luna cloud gaming over Starlink (see SS3.2 for details). This allows us to assess the suitability of the LEO network to meet the requirements of the majority of real-time Internet-based applications, as both applications impose a strict latency control loop. Cloud gaming necessitates high downlink bandwidth, while Zoom utilizes uplink and downlink capacity simultaneously.

_Zoom Video Conferencing._ Figure 10 shows samples from Zoom calls conducted over a high-speed terrestrial network and over Starlink. The total uplink throughput over Starlink is slightly higher, which we trace to FEC (Forward Error Correction) packets that are frequently sent in addition to raw video data (on average 146+-99 Kbps vs. 2+-2 Kbps over terrestrial). The frame rate, inferred from the packets received by the Zoom peer, does not meaningfully differ between the two networks (\(\) 27 FPS). Note that, since Zoom does not saturate the available uplink and downlink capacity, it should not be impacted by bufferbloating. Yet, we observe a slightly higher loss rate over LEO, which the application combats by proactively utilizing FEC. The uplink one-way delay (OWD) over Starlink is higher and more variable compared to the terrestrial connection (on average 52+-14 ms vs. 27+-7 ms). All observations also apply to the downlink except that Starlink's downlink latency (35+-11 ms) is similar to the terrestrial connection (32+-7 ms). Our analysis broadly agrees with (Safania et al., 2017) but our packet-level insight reveals bitrate functions partly caused by FEC. Further, our Starlink connection was more reliable and we did not experience second-long outages.

Interestingly, we observe that the Starlink OWD often noticeably shifts at interval points that occur at 15 s increments. Further investigation reveals the cause to be the Starlink _reconfiguration interval_, which, as reported in FCC filings (Safania et al., 2017), is the time-step at which the satellite paths are reallocated to the users. Other recent work also reports periodic link degradations at 15 s boundaries in their experiments, with RTT spikes and packet losses of several orders (Safania et al., 2017; Safania et al., 2017; Safania et al., 2017). We explore the impact of reconfiguration intervals and other Starlink-internal actions on network performance in SS6.

_Amazon Luna Cloud Gaming._ Table 1 shows 150 minutes of cloud gaming performance over terrestrial, 5G cellular, and Starlink networks. Overall, all networks realized close to 60 FPS playback rate at consistently high bitrate (\(\) 20 Mbps). Starlink lies in between the better-performing terrestrial and cellular in terms of bitrate fluctuations, frame drops and freezes4. Starlink exhibits the highest

    & Terrestrial & Cellular & Starlink \\  Idle RTT (ms) & 9 & 46 & 40 \\ Throughput (Mbps) & 1000 & 150 & 220 \\  Frames-per-second & 59\(\)1.51 & 59\(\)1.68 & 59\(\)1.63 & 27 \\ Bitrate (Mbps) & 23.08\(\)0.38 & 22.82\(\)2.42 & 22.81\(\)2.16 & 52 \\ Time at 1080p (\%) & 100 & 94.11 & 99.45 & 29 \\ Freezes (ms/min) & 0\(\)0 & 0.42\(\)20.34 & 0\(\)11.94 & 33 \\ Inter-frame (ms) & 17\(\)3.65 & 18\(\)11.1 & 16\(\)6.76 & 51 \\  Game delay (ms) & 133.53\(\)19.79 & 165.82\(\)23.55 & 167.13\(\)23.12 & 52 \\ RTT (ms) & 11\(\)11.41 & 39\(\)1.76 & 50\(\)16.28 & 53 \\ Jitter buffer (ms) & 15\(\)3.27 & 12\(\)1.33 & 15\(\)3.35 & 53 \\   

Table 1. The game metrics are aggregated over 150 minutes of playtime per connection. Values denote median\(\)SD and the worst performer is highlighted.

Figure 11. Cloud gaming over 5G (left) and Starlink (right). Vertical dashed lines show Starlink reconfiguration intervals.

Figure 10. Uplink Zoom video traffic over a terrestrial network (left) and Starlink (right). Vertical dashed lines show Starlink reconfiguration intervals.

Figure 9. Distribution of median (a) download and (b) upload goodput over Starlink from selected cities globally.

game delay, i.e., the delay experienced by the player between issuing a command and witnessing its effect. Specifically, the wired network delivers the visual response about 2 frames (\( 33\) ms) earlier than both 5G and Starlink. While examining the gaming performance over time, we observe occasional drops to \(<20\) FPS over Starlink (see Figure 11), that coincide with Starlink's reconfiguration interval. These fluctuations are only visible at sub-second granularity and, hence, are not reflected in global performance analysis (SS4).

Despite these variations, Starlink's performance remains competitive with 5G, highlighting its potential to deliver real-time application support, especially in regions with less mature cellular infrastructure. Note, however, that our Starlink terminal was set up without obstructions and the weather conditions during measurements were favorable to its operation (Sarlink, 2017). Different conditions, especially mobility, may change the relative performance of Starlink and cellular, which we plan to explore further in the near future.

_Takeaway_\(\)2 -- Starlink is competitive with the current 5G deployment for supporting demanding real-time applications. We also observe that Starlink experiences regular performance changes every 15s linked to its reconfiguration interval period. While these internal black-box parameters do influence performance to a certain extent, application-specific corrective measures, like FEC, are effective in mitigating these artifacts.

## 6. Dissecting the Bent-Pipe

We now attempt to uncover Starlink's behind-the-scenes operations and their impact on network performance. We follow a two-pronged approach to undertake this challenge. Our longitudinal traceroute measurements over RIPE Atlas accurately isolate the bent-pipe (terminal-to-PoP) global performance, allowing us to correlate it with parameters like ground station deployment, satellite availability, etc. (SS6.1). We then perform high-frequency, high-resolution experiments over Starlink terminals deployed in two EU countries to zoom in on bent-pipe operation and highlight traffic engineering signatures that may impact application performance (SS6.2).

### Global Bent-Pipe Performance

_Starlink vs. Cellular Last-mile_ We contrast our end-to-end M-Lab and real-time application analysis by comparing the Starlink bent-pipe latencies from RIPE Atlas traceroutes to cellular wireless last-mile (device-to-ISP network) access. Given the underrepresentation of cellular probes in RIPE Atlas, we augment our dataset with recent comprehensive measurements from Dang et al. (2018), which leveraged 115,000 cellular devices over the Speedchecker platform to analyze the performance of cellular networks worldwide. Figure 12 presents a comparative analysis of both networks across countries common in both datasets. Consistent with our previous findings, we find that the Starlink bent-pipe latencies fall within 36-48 ms, with the median hovering around 40 ms for almost all countries. Similarly, we find consistent cellular last-mile latencies across all countries, but almost 1.5\(\) less than Starlink. Recent investigations (Sarlink, 2017) report similar access latencies over WiFi and cellular networks. The bent-pipe latencies also corroborate our estimations in SS4 that the terminal-PoP path is the dominant contributor to the end-to-end latency. Out of the 21 countries with Starlink-enabled RIPE Atlas probes, the only exceptions where the bent-pipe latency is significantly higher (\(\) 100 ms) are the Virgin Islands (US), Reunion Islands (FR), and Falkland Islands (UK). Correlating with Figure 3, we find that Starlink neither has a GS nor a PoP in these regions, which may result in traffic routing over ISLs to far-off GS leading to longer bent-pipe latencies.

_Impact of Ground Infrastructure._ We extend our analysis by exploring the correlation between the distance from Starlink users to the GS and bent-pipe latencies. Recall that we rely on crowd-sourced data (Sarlink, 2017) for geolocating Starlink ground infrastructure since these are not officially publicly disclosed. We deduce through our traceroutes that Starlink directs its subscribers to the nearest GS relative to the PoP, as the GS-PoP latencies are \( 5\) ms almost globally (see Figure 22 in Appendix D - sole exceptions being US and Canada with 7-8 ms, likely due to abundant availability of GSs and PoPs resulting in more complex routing). Figure 13 shows the correlation of reported bent-pipe latency with the terminal-GS distance. Each point in the plot denotes at least 1000 measurements. We observe a directly proportional relationship as bent-pipe latencies tend to increase with increasing distance to the GS. Furthermore, we find that the predominant distance between GS and the user terminal is \(\) 1200 km, which is also the approximate coverage area width of a single satellite from 500 km altitude (Brockman et al., 2016) - suggesting that these connections are likely using direct bent-pipe, either without or with short ISL paths. Few terminals, specifically in Reunion, Falkland and the Virgin Islands, connect to GSs significantly farther away, possible only via long ISL chains, the impact of which we analyse further as a case study below.

_Case Study: Reunion Island._ The majority of Starlink satellites (starting from v1.5 deployed in 2021) are equipped with ISLs (Sarlink, 2017), and reports from SpaceX suggest active utilization of these links (Sarlink, 2017). Recent studies also agree with the use of ISLs (Sarlink, 2017), but point out inefficiencies in space routing (Sarlink, 2017). Nonetheless, the invisibility of satellite hops in traceroutes poses a challenge in accurately assessing the latency impact of ISLs. As such, we focus on a probe in Reunion Island (RU), which connects to the Internet via Frankfurt PoP (\(\) 9000 km). Figure 14 segments the bent-pipe RTT between

Figure 12. Last-mile latencies for different countries. “Starlink” denotes satellite bent-pipe over RIPE Atlas while “Cellular” wireless access from Speedchecker (2017).

Figure 13. Correlation between Starlink bent-pipe latency and Dishy-GS distance. Red line denotes linear regression fit.

the user terminal (Dishy) to GS (non-terrestrial), and from GS to the PoP (terrestrial). For comparison, we also plot the RTTs from a probe within Germany (DE) connecting to the same PoP (\( 500\) km, in red). The vertical lines represent the median RTT over terrestrial infrastructure from both probe locations to the PoP. Firstly, we observe minimal GS-PoP latency for both locations, verifying that the RU satellite link is using ISLs. Secondly, in RU, Starlink shows significant latency improvement over fiber (\( 60\) ms). This is because the island has limited connectivity with two submarine cables routing traffic 10,000 km away, either in Asia or South America (Santzik et al., 2018). Starlink provides a better option by avoiding the terrestrial route altogether, directly connecting RU users to the dense backbone infrastructure in EU (Brocker et al., 2018). However, since the bent-pipe incurs at least 30-40 ms latency in the best-case, Starlink is less attractive in regions with robust terrestrial network infrastructure (also evident from the DE probe where fiber achieves better latencies).

_Impact of Serving Orbit._ Recall that the majority of Starlink satellites are deployed in the 53' inclination (see Table 2 in Appendix A). Consequently, network performance for clients located outside this orbit's range may vary widely as they are serviced by fewer satellites in \(70^{}\) and \(97.6^{}\) orbits. Figure 15 contrasts the bent-pipe latencies of probe in Alaska (61.5685N, 149.0125W) ["A"] to probes within 53' orbit. Despite dense GS availability, the bent-pipe latencies for Alaska are significantly higher (\( 2\)). The Swedish probe ["B"] at 59.6395N is at the boundary of 53' orbit but still exhibits comparable latency to Canada, UK, and Germany. Furthermore, the Alaslan probe experiences intermittent connectivity, attributed to the infrequent passing of satellite clusters within the \(70^{}\) and \(97.6^{}\) orbits. These findings indicate substantial discrepancies in Starlink's performance across geographical regions, which may evolve for the better as more satellites are launched in these orbits. Nevertheless, we leverage the sparse availability of satellites at the higher latitude to further dissect the bent-pipe operations in SS6.2.

_Takeaway \(\)3_ - The Starlink "bent-pipe" accounts for (on average) 40 ms of latency almost consistently globally. In certain cases where ISLs are being used, the latencies might escalate yet still outshine traditional terrestrial networks when bridging remote regions. The satellite link yields stable latencies, provided that the client is served by the dense 53' orbit.

### Controlled Experiments

We now investigate the cause of periodic disruptions to real-time applications (SS5). Specifically, we perform high-resolution measurements to gain insights into Starlink network operation.

_Global Scheduling._ We performed simultaneous iRTT measurements from two countries that are sufficiently geographically removed that both cannot be connected to the same serving satellite. We also verify that both terminals are assigned different PoPs located within their country. The resulting RTTs, shown in Figure 16a, vary in a consistent pattern, being comparatively stable within each Starlink reconfiguration interval but potentially changing significantly between intervals. Moreover, the time-wise alignment of reconfiguration intervals for both vantage points indicates that Starlink operates on a globally coordinated schedule, rather than on a per-Dishy or per-satellite basis. These results are in line with other recent studies (Santzik et al., 2018), which also hint that Starlink utilizes a global network controller. Previous studies (Santzik et al., 2018) have noticed drops in downlink throughput every 15s but have not correlated these with the reconfiguration intervals. We also observe throughput drops on both downlink and uplink, shown in Figure 16b, that occur at the reconfiguration interval boundaries. Similar to the RTT, the throughput typically remains relatively consistent within an interval, but can experience sudden changes between interval transitions. These also corroborate the periodic performance degradation in our real-time application experiments.

_Disproving Satellite Handoff Hypothesis._ Previous works have suggested satellite or beam changes at reconfiguration interval boundaries to be the root-cause of network degradation (Santzik et al., 2018; Santzik et al., 2018; Santzik et al., 2018). To investigate this hypothesis, we deliberately obstructed the field-of-view of our high latitude Dishy to prevent it from connecting to the dense 53' orbital shell (see SS3.3 for details). The restriction curtailed the number of candidate (potentially connectable) satellites to 13%. This limitation led to intermittent connectivity, characterized by brief connectivity windows with long service downlines. By synchronizing the timings of each connectivity window with the overhead positions of candidate satellites (from CelesTrak (2018) and other sources (CelesTrak, 2018)), we identify several windows where the terminal can be served only by a single satellite. Figure 16c (upper) shows RTTs from one such window. The fact that there is significant RTT variance between intervals invalidates the hypothesis that the changes in RTT are caused by satellite handovers (considering a single candidate satellite during the observed period, leaving no room for hand-off occurrences). Separately, we perform the same experiment but focus on (both uplink and downlink) throughput. Similar to RTT, we also witness throughput drops at interval boundaries even when only one candidate satellite is visible.

_Scheduling Updates._ Figure 16c (lower) shows the distribution of start and end times of the connectivity windows during our restricted field-of-view experiments. We observed a strong correlation between connectivity end times and reconfiguration interval

Figure 14. Bent-pipe RTT segments from Reunion Island (yellow) vs. Germany (red) connecting to Germany PoP. Vertical lines show latency over Atlas probes connected via fiber from both locations to the Frankfurt server (PoP location).

Figure 15. Bent-pipe latencies for “A” (in Alaska) covered by the 70’ and 97.6’ while the rest (Sweden “B”, Canada “C”, UK “D”, and Germany “E”) are also covered by 53’.

(RI) boundary, which is not seen with start times5. The result hints at internal network scheduling changes at reconfiguration interval boundaries, i.e., Starlink assigns its terminals new satellites (or frequencies) every 15s. We hypothesize that with an obstructed view, the scheduler cannot find better alternatives in the 70' and 97.6' orbits, resulting in connectivity loss at the end of the window.

_Analysis Summary._ Putting together our various observations, we theorize that Starlink relies on a global scheduler that re-allocates the user-satellite(s)-GS path every 15s. An FCC filing from Starlink implies this behavior (Gilton et al., 2016) and recent studies also suggest that the LEO operator performs periodic load balancing at reconfiguration boundaries, reconnecting all active clients to satellites (Zhou et al., 2016; Zhou et al., 2016). The theory also explains our observed RTT and throughput changes when only a single candidate satellite is in view. It is plausible that Starlink may have rescheduled the terminal to the same satellite but with reallocated frequency and routing resources. Regardless, these reconfigurations result in brief sub-second connection disruptions, which may become more noticeable at the application-layer as the number of subscribers on the network increases over time.

_Takeaway \(\)4_ -- Starlink uses 15s-long reconfiguration intervals to globally schedule and manage the network. Such intervals cause latency/throughput variations at the interval boundaries. Handoffs between satellites are not the sole cause of these effects. Indeed, our findings hint at a scheduling system reallocating resources for connections once every reconfiguration interval.

## 7. Related Work

LEO satellites have become a subject of extensive research in recent years, with a particular focus on advancing the performance of various systems and technologies. Starlink, the posterchild of LEO networks, continues to grow in its maturity and reach with \(>2\)M subscribers as of September 2023 (Selig et al., 2020). Despite its growing popularity, there has been limited exploration into measuring Starlink's performance so far. Existing studies either have a narrow scope, employing only a few vantage points (Gilton et al., 2016; Gilton et al., 2016; Gilton et al., 2016) or focus on broad application-level operation (Zhou et al., 2016; Gilton et al., 2016) without investigating root-causes. Ma et al. (2016) embarked on a journey across Canada with four dishes to scrutinize various factors, such as temperature and weather, that might influence Starlink's performance.

A few endeavors have attempted to unveil the operations of Starlink's black-box network. Pan et al. (Pan et al., 2016) revealed the operator's internal network topology from traceroutees, whereas Tanveer et al. (Tanveer et al., 2016) spotlighted a potential global network controller. The absence of global measurement sites poses a predominant challenge hampering a comprehensive understanding of Starlink's performance. As we show in this work, Starlink's performance varies geographically due to differing internal configurations and ground infrastructure availability. Some researchers have devised innovative methods to combat this. For example, Izhikevich et al. (Izhikevich et al., 2016) conducted measurements towards exposed services behind the Starlink user terminal, while Taneja et al. (Taneja et al., 2016) mined social media platforms like Reddit to gauge the LEO network's performance. Our study not only corroborates and extends existing findings but also stands as the most extensive examination to date. Our approach - anchored in detailed insights from 34 countries, leveraging 19.2 million crowdsourced M-Lab measurements, 2.9 million active RIPE Atlas measurements, and two controlled terminals connecting to different Starlink orbits - provides a deeper understanding of the Starlink "bent-pipe" and overall performance.

## 8. Conclusions

Despite its potential as a "global ISP" capable of challenging the state of global Internet connectivity, there have been limited performance evaluations of Starlink to date. We conducted a multi-faceted investigation of Starlink, providing insights from a global perspective down to internal network operations. Globally, our analysis showed that Starlink is comparable to cellular for supporting real-time applications (in our case Zoom and Luna cloud gaming), though this varies based on proximity to ground infrastructure. Our case study shows Starlink inter-satellite connections helping remote users achieve better Internet service than terrestrial networks. However, at sub-second granularity, Starlink exhibits performance variations, likely due to periodic internal network reconfigurations at 15s intervals. We find that the reconfigurations are synchronized globally and are not caused only by satellite handovers. As such, this first-of-its-kind study is a step towards a clearer understanding of Starlink's operations and performance as it continues to evolve.

Figure 16. (left, a) iRTT latencies with Dishys in two countries connected to different ground infrastructure; (middle, b) Maximum uplink and downlink throughput over a 195-second (13 interval) period; (right, c) (upper) RTTs for a connectivity window where the Dishy was connected to only a single satellite; (lower) Probability distribution of the time between the connectivity window start / end and the previous reconfiguration interval (RI). Vertical dashed lines show Starlink reconfiguration intervals.