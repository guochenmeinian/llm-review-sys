# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:2]

## 4 Experimental Evaluation

### Dataset

We utilize the Naturalistic Driving Action Recognition Dataset from the AI City Challenge , which consists of approximately 62 hours of footage, acquired from 69 participants. Each participant performed 16 different tasks, including but not limited to telephonic conversations, eating, and reaching backward, in a randomized order, as specified in Table 1.

The data includes three camera positions installed within a vehicle, as in Figure 2, positioned to capture from varied angles and synchronized to record simultaneously. The data collection was executed in two phases for each participant: the first without any visual obstructions and the second incorporating visual obstructions to appearance (e.g., sun-glasses, hats). Thus, six videos were collected per participant--three from the non-obstructed phase and three from the obstructed phase.

 
**Class** & **Activity Label** & **Dist. \%** \\ 
0 & Normal Forward Driving & 59.01 \\
1 & Drinking & 1.49 \\
2 & Phone Call(right) & 2.78 \\
3 & Phone Call(left) & 2.97 \\
4 & Eating & 3.29 \\
5 & Text (Right) & 3.44 \\
6 & Text (Left) & 3.56 \\
7 & Reaching behind & 1.40 \\
8 & Adjust control panel & 2.42 \\
9 & Pick up from floor (Driver) & 1.31 \\
10 & Pick up from floor (Passenger) & 2.15 \\
11 & Talk to passenger at the right & 3.52 \\
12 & Talk to passenger at backseat & 3.46 \\
13 & Yawning & 1.87 \\
14 & Hand on head & 3.45 \\
15 & Singing or dancing with music & 3.85 \\  

Table 1: Table of Driver Activity Classifications.

Figure 1: The Semantic Representation Late Fusion Network (SRLF-Net) takes images from multiple perspectives as input. Each image is sent to a CLIP encoder. Our experiments use the Vision Transformer backbone, base size, with size 32 patches. These representations are then further encoded using independent (non-shared-weight) fully-connected layers, each followed by batch normalization, ReLU activation, and dropout (rates 0.5 and 0.6 respectively). We use input size 768, and use two layers, compressing once to 512 and then to 256. These representations are then concatenated and used as input to another series of fully-connected layers (fusion step), again using batch normalization and ReLU activation between each. The size of these layers are 768, 768, 512, 256, 128, then \(n\) (number of classes), which is 16 for our experiments.

### Training Details

We detail our evaluation data splits in the following sections, with care to have images of individuals binned only to one set out of training and test. We divide our training set into two groups; 80% to train and 20% to validation, with possible overlap in individuals (though no same frames are shared). With our training set, we train SRLF-Net for up to 100 epochs, employing early stopping on a validation loss criteria. We use the adam optimizer (learning rate of 0.0001), 1cycle learning rate schedule policy , and cross-entropy loss.

For testing, we utilize the 7-fold data split provided in the dataset, dividing into 7 near-even groups of participants. This allows us to approximate generalizability with a 7-fold average.

### Evaluation Over All Classes

The results for 7-fold test are seen in Table 2. We achieve an average accuracy of 71.64 %, showcasing the promising use of the method, notable in comparison to 6.25% expected accuracy of random selection for sixteen classes.

As illustrated in Figure 3, the model observes a large favorability for class 0 (Normal Forward Driving) likely due to the skewed distributions of the data, as portrayed in Table 1, with phone call and hand-on-head the next most-correctly-classified classes. Adjusting the control panel shows the most confusion with the default driving class. Straight forward driving accounts for 59.01 % of the data, resonating binary test to differentiate between straight forward driving and all other classes in Figure 4. For more accurate classification, it would be beneficial to mitigate the effects of the confounding majority class ("normal driving"); we explore experiments in class-weighting, but find these effects to not be strong enough to counter the adverse learning effect. As another solution, we consider the use of an early-stage binary classifier to separate normal driving from distracted driving. The binary classifier is imperfect (as shown in Figure 4, and in the next section, we carry out an additional distraction-classification experiment excluding the "normal driving" class, on the assumption that some strong binary classifier may be achieved with further architectural exploration.

### Distracting Activities Only: Evaluating Without Normal Driving Class

Our architecture, in combination with a dataset heavily skewed towards normal driving, tends to overpredict the normal driving class. To understand how well the model separates between the distracting activity classes, we run an experiment by which we assume there is some "perfect" binary classifier which can distinguish between normal driving and distracted driving, and then use our model to classify only between these distraction classes. The results of

  
**K-fold** & **Accuracy** \\ 
1 & 68.09 \% \\
2 & 74.40 \% \\
3 & 73.60 \% \\
4 & 71.37 \% \\
5 & 70.15 \% \\
6 & 75.34 \% \\
36 & 7 & 68.53 \% \\  
**Average:** & 71.64 \\
**Standard Deviation:** & 2.88 \\   

Table 2: Table of k-fold cross-validation accuracies and average accuracy.

Figure 3: Confusion matrix for best performing k-fold 6 including a mode filter, resulting in a performance of 77.10 %.

Figure 2: Illustration of multi-perspective in-cabin camera views for monitoring driver behavior under the class ’0: Normal Forward Driving’. (1) Dashboard view. (2) Rear-view. (3) Side view.

this experiment are illustrated in Figure 5. The model, in general, predicts the correct class with the greatest likelihood for any given activity class, though for some individual classes, this likelihood may be less than \(>\)50%. Phone call and hand-on-head again show the best performance.

We also highlight the importance of the mode-filter post-processing step; without the mode filter, the accuracy is 63.66%, and with the mode filter, this accuracy rises to 70.06%. This filter leverages the knowledge that there is a certain rate at which a driver can reasonably change between tasks (i.e. it would be unexpected for a driver to oscillate between different distracting activities at 30 Hz, even if the camera captures and model infers at that rate).

## 5 Concluding Remarks and Future Research

To begin, we highlight some recommended opportunities for future research:

1. Comparison to text-encoding methods, such as vector products between text and image encodings, or even the evaluation of prompted vision-language systems to determine classes of images. We note that we have began a series of experiments using LLaVA, but the computation time on such methods _significantly_ exceeds the method shown in this paper, without offering stronger preliminary results. In relation to these methods, our presented algorithm does carry the benefit of immediate applicability to multiple simultaneous views.
2. The integration of temporal information (either as post-processing, or addition of LSTM or Transformer models early in the architecture) may be very useful, since driver activities occur over time, with valuable information in these action dynamics.
3. Evaluation on combinations of non-consistent views. It would be interesting to merge multiple datasets which share some classes in common, so that we can evaluate generalizability to further views and subjects.
4. Integration into open-set novelty detection methods, such that the system can expand its number of classes, retraining if necessary, when new activities are introduced.

In this research, we present a new perspective of the vision-language contrastively-learned encoding as a fundamental new representation of an image, which contains both visual information as well as semantic information. We show that from this information, it is possible to classify driver activity into a variety of distraction classes with fairly strong accuracy, and further, that our algorithm can adapt to any number of simultaneous views. Vision-language models may lead to driver monitoring systems which are more accurate, robust, and generalizable; suitable for an open-set of possible distractions; and directly explainable  via language.

Figure 4: Binary Confusion matrix for best performing k-fold 6 only including class 0 for straight forward driving and a combination of all other activity classes, performing 77.22 % accuracy.

Figure 5: Confusion matrix for best performing k-fold 6 without class 0 for straight forward driving and including a mode filter, performing 70.06% accuracy. By removing the forward driving class, the accuracy metric decreases slightly (simply because the over-predicted forward driving class accounted for a majority of the dataset), but the average performance over classes actually increases from 50.44% to 70.13%. The alignment of average per-class accuracy and overall accuracy is a strong indicator of the model’s effective learning.

[MISSING_PAGE_EMPTY:6]

[MISSING_PAGE_FAIL:7]