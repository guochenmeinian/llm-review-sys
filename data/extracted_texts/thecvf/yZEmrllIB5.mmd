# Intrinsic LORA: A Generalist Approach for Discovering Knowledge in Generative Models

Xiaodan Du\({}^{1}\)  Nicholas Kolkin\({}^{2}\)  Greg Shakhnarovich\({}^{1}\)  Anand Bhattad\({}^{1}\)

\({}^{1}\)Toyota Technological Institute at Chicago \({}^{2}\)Adobe

###### Abstract

Generative models have been shown to be capable of creating images that closely mimic real scenes, suggesting they inherently encode scene representations. We introduce Intrinsic LoRA (I-LoRA), a general approach that uses Low-Rank Adaptation (LoRA) to discover scene intrinsics such as normals, depth, albedo, and shading from a wide array of generative models. I-LoRA is lightweight, adding minimally to the model's parameters and requiring very small datasets for this knowledge discovery. Our approach, applicable to Diffusion models, GANs, and Autoregressive models alike, generates intrinsics using the same output head as the original images.

## 1 Introduction

Generative models can produce high-quality images almost indistinguishable from real-world photographs. They seem to demonstrate a profound understanding of the world, capturing nuances of realistic object placement, appearance, and lighting conditions. Yet, it remains an open question how these models encode such detailed knowledge, and whether representations of scene intrinsics exist in these models and can be extracted explicitly.

**Our Contribution.** We conduct our inquiry across a spectrum spanning diffusion, GANs, and autoregressive models - to understand whether they encode fundamental scene in

   Model & Pretrain Type & Domain & Normal & Depth & Albeda & Shuffering \\  VQGAN  & Autoregressive & FFHQ & - & - & ✓ & ✓ \\ SO-2  & GAN & FFHQ & - & ✓ & ✓ & ✓ \\ SO-2  & GAN & LSUBM & - & ✓ & ✓ & ✓ \\ SO-3  & GAN & FFHQ & - & ✓ & ✓ & ✓ \\ SO-3  & GAN & ImageNet & - & - & ✓ & ✓ \\ SD-3  & Diffusion & Open & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Summary of scene intrinsics found across different generative models without changing generator head. ✓: Intrinsics can be extracted with high quality. \(\): Intrinsics cannot be extracted with high quality. \(\): Intrinsics cannot be extracted with high quality. \(\): Intrinsics cannot be extracted.

Figure 1: Intrinsic LoRA (I-LoRA) is a general approach for extracting visual knowledge from generative models of many types. Our method applies targeted, lightweight fine-tuning to modulate key feature maps, using low-rank adaptation (LoRA) on attention layers in VQGAN (a) and Stable Diffusion (d), and affine layers in StyleGAN (b and c). This process helps us discover fundamental scene intrinsics – normals, depth, albedo, and shading – directly from the models’ learned representations, avoiding the need for additional task-specific design of decoding heads or layers.

trinsics of normals, depth, albedo, and shading . Our method, Intrinsic LoRA (I-LoRA), a Low-Rank Adaptation (LoRA) technique, efficiently extracts these intrinsics across different model types with minimal computational overhead and data requirements. Detailed results and a summary are presented in Tab. 1 and elaborated further in Sec. 4. Our experiments suggest that the intrinsic knowledge within generative models is not accidental but a byproduct of large-scale learning to mimic image data. In summary, our work broadens the understanding of visual knowledge within generative image models and our contributions are:

* **Wide Applicability:** We validate I-LoRA's capability to extract scene intrinsics (normals, depth, albedo, and shading) across a broad spectrum of generative models, highlighting its adaptability to diverse architectures.
* **Efficient and Lean Approach** to knowledge extraction: I-LoRA is highly efficient, requiring a little increase in parameters (less than 0.17% for Stable Diffusion) and minimal training data, as few as 250 images.
* **Insights from Learned Priors:** Through control experiments, we illustrate the critical role of learned priors, suggesting the quality of intrinsics extracted is correlated to the visual quality of the generative model.
* **Competitive Quality of Intrinsics:** Our method, supervised with hundreds to thousands of labeled images, generates intrinsics on par with or even better than those produced by the leading supervised techniques requiring millions of labeled images.

## 2 Related Work

**Generative Models:** Generative Adversarial Networks (GANs)  have been widely used for generating realistic images. Variants like StyleGAN , StyleGAN2  and GigaGAN  have pushed the boundaries in terms of image quality and control.

Diffusion models, such as Denoising Score Matching  and Noise-Contrastive Estimation , have been used for generative tasks and are perhaps the most popular at the moment .

Autoregressive models like PixelRNN  and PixelCNN  generate images pixel-by-pixel, offering fine-grained control but at the cost of computational efficiency. More recently, VQ-VAE-2  and VQGAN  have combined autoregressive models with vector quantization to achieve high-quality image synthesis.

**Scene Intrinsics Extraction:** Barrow and Tenenbaum  highlighted several fundamental scene intrinsics including depth, albedo, shading, and surface normals. A large body of work has focused on extracting some related properties, like depth and normals from images  using labeled annotated data. Labeled annotations of albedo and shading are hard to find and as the recent review in  shows, methods involving little or no learning have remained competitive until fairly recently. However, these methods often rely on supervised learning and do not explore the capabilities of generative models in this context.

Many recent studies have used generative models  as pre-trained feature extractors or scene prior learners. They use generated images to enhance downstream discriminative models, fine-tune the original generative model for a new task, learn new layers or decoders to produce desired scene intrinsics.

**Knowledge in Generative Models:** Several studies have explored the extent of StyleGAN's knowledge, particularly in the context of 3D information about faces . Yang et al.  show GANs encode hierarchical semantic information across different layers. Further research has demonstrated that manipulating offsets in StyleGAN can lead to effective relighting of images  and extraction of scene intrinsics . Chen et al. found internal activations of the LDM encode linear representations of both 3D depth data and a salient-object / background distinction. Recently,  found correspondence emerges in image diffusion models without any explicit supervision.

**LoRA (Low-Rank Adaptation).** LoRA  introduces trainable low-rank decomposed matrices into specific layers of the model architecture. These matrices are the only components updated during task-specific optimization. This results in a significant reduction in the number of trainable parameters, ensuring only slight modifications to the model, and preserving its core functionality and accessibility.

## 3 Intrinsic LoRA

A generative model \(G\) maps noise/conditioning information \(z\) to an RGB image \(G(z)^{H W 3}\). We seek to augment \(G\) with a small set of parameters \(\) that allow us to produce, using the same architecture as \(G\), an image-like map with up to three channels, representing scene intrinsics.

**I-LoRA's Learning Framework.** Our method, I-LoRA, learns to extract intrinsic properties of an image (such as depth) using a small number of labeled examples (image/depth map pairs) as supervision. In cases where we do not have access to the actual intrinsic properties, we use models trained on large datasets to generate estimated intrinsics as pseudo-ground truth, used as training targets for \(G_{}\).

To optimize \(\) of \(G_{}\) using a pseudo-ground truth predictor \(\) (e.g., a network trained to predict depth from an image), we minimize the objective:

\[_{}_{z}[d(G_{}(z),(G(z)))], \]

where \(d\) is the distance metric.

Diffusion models require special treatment since they are effectively image-to-image and not noise-to-image. During inference, diffusion models repeatedly receive a noisy image as input. Thus instead of conditioning noise \(z\) we feed an image \(x\) (generated or real) to a diffusion model \(G\). In this case, given a real image \(x\), our objective function becomes \(_{}_{x}[d(G_{}(x),(x))]\).

For surface normals \(\) is Omnidatav2-Normal . For depth we use ZoeDepth  as the predictor \(\). For Albedo and Shading \(\) is Paradigms . For SG2, SGXL and VQGAN, \(d\) in Eq.1 is

\[d(x,y)=1-cos(x,y)+\|x-y\|_{1} \]

for normal and MSE for other intrinsics. For latent diffusion based methods, there isn't a clear physical meaning to the relative angle of latent vectors in encoded normal maps, so we use the standard objective of MSE for all intrinsics.

We use LoRA to recover image intrinsics from generative models. LoRA introduces a low-rank weight matrix \(W^{*}\), which has a lower rank than the original weight matrix \(W^{d_{1} d_{2}}\). This is achieved by factorizing \(W^{*}\) into two smaller matrices \(W^{*}_{u}^{d_{1} d^{*}}\) and \(W^{*}_{l}^{d^{*} d_{2}}\), where \(d^{*}\) is chosen such that \(d^{*}(d_{1},d_{2})\).

The output \(o\) for an input activation \(a\) is then given by:

\[o=Wa+W^{*}a=Wa+W^{*}_{u}W^{*}_{l}a. \]

Applying I-LoRA.For GANs, I-LoRA modules are integrated with the affine layers that map from w-space to s-space . In the case of **VQGAN, an autoregressive model**, I-LoRA is applied to the convolutional attention layers within the decoder. For **diffusion models**, I-LoRA adaptors are learned atop cross-attention and self-attention layers. The UNet is utilized as a dense predictor, transforming an RGB input into intrinsics in one step. This approach, favoring simplicity and effectiveness, delivers superior quantitative results. Depending on the intrinsics of interest, the textual input varies among "surface normal", "depth", "albedo", or "shading".

## 4 Experiments

In this section, we outline I-LoRA's contributions, demonstrating its general applicability across generative models (Sec. 4.1). Control experiments provide evidence of I-LoRA's effectiveness (Sec. 4.2). Note: our analysis in Sec. 4.2 primarily utilizes a single-step I-LoRA model for intrinsic image extraction. In Sec. 5, we discuss the challenge of naively applying I-LoRA to a multi-step Stable Diffusion model. We propose a simple modification to the architecture by adding an extra layer (that is not learned) for improved intrinsic image extraction. We refer to this model as **Augmented I-LoRA** (I-LoRA\({}_{}\)).

### I-LoRA is General and Universally Applicable

We evaluate I-LoRA across diverse generative models, including StyleGAN-v2 , StyleGAN-XL , and VQGAN , trained on datasets like FFHQ , LSUN Bedrooms , and ImageNet . I-LoRA adaptors are tailored to each model and dataset to extract intrinsics: surface normals, depth, albedo, and shading, demonstrating broad applicability and robustness in both qualitative assessments (Fig. 1, 2, 4) and quantitative (Tab. 2 on generated images, Tab. 3 on real images). In all experiments - covering both generated and real images - we use pseudo-ground truth from off-the-shelf models as a supervisory signal. We use I-LoRA with Rank 8 as default for all generative models.

We find I-LoRA can unearth intrinsic knowledge across almost all models tested, the notable exception is StyleGAN-XL trained on ImageNet. Where it yields qualitatively poor results, which we attribute to the model's limited ability to generate realistic images (Fig. 3). This suggests the quality of intrinsic extraction is correlated with the generative model's fidelity (see Sec. 4.2).

In evaluations of generated images, our method is benchmarked against pseudo-ground truths derived from existing models, compensating for the lack of true ground truths.

Diffusion models excel as powerful image generators, thanks to their architecture as image-to-image translators. This feature simplifies their application to real images. Tak

Figure 4: Scene intrinsics from I-LoRA applied to randomly generated images. I-LoRA accurately predicts the table’s normal in the first row when compared to . The comparison highlights I-LoRA’s ability to closely align with, and sometimes surpass, these supervised SOTA monocular predictors.

Figure 3: StyleGAN-XL trained on ImageNet. Top: pan, bottom: laptop, with the corresponding scene intrinsics (pseudo ground truth and extracted) alongside. The surface normals and depth maps, while capturing the basic shape and volume, lack precise detail and exhibit artifacts. Albedo and Shading extractions fail. These difficulties are correlated with the overall worse realism and consistency of the generated images.

Figure 2: Scene intrinsic properties extracted from StyleGAN-v2 trained on LSUN bedroom images using I-LoRA.

[MISSING_PAGE_FAIL:4]

[MISSING_PAGE_FAIL:5]

*  Daqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba, and Sanja Fidler. Semantic segmentation with generative models: Semi-supervised learning and strong out-of-domain generalization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021.
*  Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. _arXiv preprint arXiv:2305.08891_, 2023.
*  Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2015.
*  Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. _arXiv preprint arXiv:2211.01095_, 2022.
*  Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell. Diffusion hyperfeatures: Searching through time and space for semantic correspondence. In _Advances in Neural Information Processing Systems_, 2023.
*  Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In _International Conference on Learning Representations_, 2021.
*  Thao Nguyen, Yuheng Li, Utkarsh Ojha, and Yong Jae Lee. Visual instruction inversion: Image editing via visual prompting. _arXiv preprint arXiv:2307.14331_, 2023.
*  Atsuhiro Noguchi and Tatsuya Harada. Rgbd-gan: Unsupervised 3d representation learning from natural image datasets via rgbd image synthesis. In _International Conference on Learning Representations_, 2020.
*  Xingang Pan, Bo Dai, Ziwei Liu, Chen Change Loy, and Ping Luo. Do 2d gans know 3d shape? unsupervised 3d shape reconstruction from 2d image gans. In _International Conference on Learning Representations_, 2021.
*  Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.
*  Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2021.
*  Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. _Advances in neural information processing systems_, 32, 2019.
*  Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.
*  Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, and Yannis Kalantidis. Fake it till you make it: Learning transferable representations from synthetic imagenet clones. In _CVPR 2023-IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023.
*  Axel Sauer, Katja Schwarz, and Andreas Geiger. Styleganxl: Scaling stylegan to large diverse datasets. In _ACM SIGGRAPH 2022 conference proceedings_, 2022.
*  Luming Tang, Menglin Jia, Qianqian Wang, Cheng Peng, Phoo, and Bharath Harahram. Emergent correspondence from image diffusion. _arXiv preprint arXiv:2306.03881_, 2023.
*  Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. _Advances in neural information processing systems_, 29, 2016.
*  Aaron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In _International conference on machine learning_. PMLR, 2016.
*  Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Z Dai, Andrea F Daniele, Mohammadeza Mostajabi, Steven Basart, Matthew R Walter, et al. Diode: A dense indoor and outdoor depth dataset. _arXiv preprint arXiv:1908.00463_, 2019.
*  Pascal Vincent. A connection between score matching and denoising autoencoders. _Neural computation_, 2011.
*  Zongze Wu, Dani Lischinski, and Eli Shechtman. Stylespace analysis: Disentangled controls for stylegan image generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021.
*  Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models. _arXiv preprint arXiv:2303.04803_, 2023.
*  Ceyuan Yang, Yujun Shen, and Bolei Zhou. Semantic hierarchy emerges in deep generative representations for scene synthesis. _International Journal of Computer Vision_, 2021.
*  Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. _arXiv preprint arXiv:1506.03365_, 2015.
*  Ning Yu, Guilin Liu, Aysegul Dundar, Andrew Tao, Bryan Catanzaro, Larry S Davis, and Mario Fritz. Dual contrastive loss and attention for gans. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2021.
*  Ye Yu and William AP Smith. Inverserendernet: Learning single image inverse rendering. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, 2019.
*  Yuxuan Zhang, Wenzheng Chen, Huan Ling, Jun Gao, Yinan Zhang, Antonio Torralba, and Sanja Fidler. Image gans meet differentiable rendering for inverse graphics and interpretable 3d neural rendering. In _International Conference on Learning Representations_, 2021.
*  Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio Torralba, and Sanja Fidler. Datasetgan: Efficient labeled data factory with minimal human effort. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021.
*  Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing text-to-image diffusion models for visual perception. _ICCV_, 2023.

**Appendices**

## Appendix A I-LoRA Pipeline

Fig. 7 illustrates the I-LoRA pipeline applied to Stable Diffusion's UNet in a single-step manner.

## Appendix B Additional Ablation Studies

### Rank Efficiency

Our single-step I-LoRA model, distinguished by its high quantitative performance, serves as the basis for ablation studies that assess the influence of rank and labeled data quantity on intrinsic extraction efficiency. We verify that the requirements for compute, parameters, and data to learn I-LoRA are minimal.

Fig. 8 shows surface normal predictions across LoRA ranks. The highest accuracy is achieved with Rank 8, balancing accuracy and memory. Notably, a Rank 2 LoRA with only 0.4M additional parameters (a mere 0.04% increase) still yields good performance. Note that across different generative models, Rank 8 adaptors adds only 0.17% to 0.57% additional parameters (Tab. 2).

### Label Efficiency

The impact of the labeled data size is analyzed in Fig. 9. I-LoRA reaches peak performance using a modest 4000 training examples, with credible predictions visible from as few as 250 samples.

### Number of Diffusion Steps

To assess the impact of the number of diffusion steps on the performance of the multi-step I-LoRA\({}_{}\) model, we conducted an ablation study. The results are presented in Fig. 10. For all our experiments in the main text, we used DPMSolver++. Interestingly, the quality of results did not vary significantly with an increased number of steps, indicating that 10 steps are sufficient for extracting better surface normals from the Stable Diffusion. Nevertheless, we use 25 steps for all our experiments because it is more stable across different image intrinsics.

### CFG scales

When working with the multi-step I-LoRA\({}_{}\), the quality of the final output is influenced by the choice of classifier-free guidance (CFG) scales during the inference process. In Fig. 11, we present a comparison of the effects of using different CFG scales. Based on our experiments, we found that using CFG=3.0 results in the best overall quality and minimizes color-shift artifacts.

## Appendix C Baselines

### Superiority of I-LoRA over Fine-tuning and Linear Probing

We compare I-LoRA with two common baselines: linear probing and full model fine-tuning. Following Chen et al. for linear probing and employing standard fine-tuning practices, we train all methods with a small dataset of 250 samples to 16000 samples. Our findings, detailed in Tab. 4 and illustrated in Fig. 12, indicate that I-LoRA significantly outperforms these baselines in low-data regimes, validating its superior efficacy and data efficiency.

### Other Ablations and Baselines

We extensively study the effect of applying LoRA to different attention layers within Stable Diffusion models. Specifically, we investigate the outcomes of targeting up-blocks, mid-block, down-blocks, cross-attention, and self-attention layers individually. We find (Fig. 13) that isolating LoRA to up or down blocks or the mid-block alone is less effective or diverges, and applying to either cross- or self-attention layers yields decent results, though combining them is best.

Additionally, we evaluated other image editing methods such as Textual Inversion  and VISII , alongside InstructPix2Pix's response to "Turn it into a surface normal map" instruction . As shown in Fig. 14, these methods perform poorly for intrinsic image extraction, demonstrating the effectiveness of our I-LoRA approach in extracting scene intrinsics.

### Baseline of Directly Applying SDEdit

In addition to baselines we discussed above, here we show that directly applying SDEdit  will also fail to extract reasonable image intrinsics. We take the model from the SDv1-5 column in Fig. 6 of the main paper and apply SDEdit. In Fig. 15, we show directly applying SDEdit results in severe artifacts, regardless of strength.

## Appendix D Hyper-parameters

In Table 5, we show the hyperparameters we use for each model.

## Appendix E Generated Images Used for Quantitative Analysis

In Tab. 2 of the main paper, we report quantitative results on synthetic images. For Autoregressive models and GANs, we first randomly sample 500 noises and use them to generate 500 RGB images. The same 500 noises will then be used to generate intrinsics with our learned LoRAs loaded. For Stable Diffusion experiments (both single-step and multi-step), we use a single dataset with 1000 synthetic images withFigure 8: Parameter Efficiency of I-LoRA. We evaluate I-LoRA across various rank settings for surface normal extraction. Lower ranks such as 8 offer a balance between efficiency and effectiveness. All model variants are trained using SD’s UNet (v1.5) with 4000 samples. Performance metrics, such as Mean Angular Error and L1 Error for normals, and additional parameter counts are detailed below each variant.

   &  &  &  &  \\   & Mean Error\({}^{}\)\(\) & L1 \(\) = 00 \(\) & Mean Error\({}^{}\)\(\) & L1 \(\) = 00 \(\) & Mean Error\({}^{}\)\(\) & L1 \(\) = 00 \(\) \\  Linear Probe & 29.10 & 23.74 & 28.45 & 23.25 & 28.52 & 23.26 & 28.22 & 23.11 \\ Fine-tuning & 34.40 & 27.58 & 25.19 & 20.28 & 28.03 & 22.17 & 27.39 & 22.24 \\  LoRA (Ours) & **27.73** & **22.46** & **22.22** & **18.05** & **20.31** & **16.53** & **21.26** & **17.33** \\  

Table 4: We find LoRA to consistently outperform all baselines for different number training samples (first row).

Figure 7: Overview of I-LoRA applied to Stable Diffusion’s UNet in a single-step manner. We adopt an efficient fine-tuning approach, specifically low-rank matrices corresponding to key feature maps – attention matrices – to reveal scene intrinsics. Distinct low-rank adaptors (LoRA) are optimized for each intrinsic (_violet_ adaptors for surface normals; swappable with other intrinsics). We use a few labeled examples for this fine-tuning and directly extract scene intrinsics using the same decoder that generates images, circumventing the need for specialized decoders or comprehensive model re-training.

[MISSING_PAGE_FAIL:9]

Figure 11: Ablation study analyzing the impact of different classifier-free guidance (CFG) on I-LoRA\({}_{}\) surface normal prediction. For efficiency, we experimented with a step of 10. We observed that CFG=1 sometimes led to incorrect semantic predictions, particularly in the case of stairs in row 4. On the other hand, using large CFGs (5 and beyond) results in more severe color shift problems.

Figure 12: Comparison with baselines. All models are trained with 250 samples. Note LoRA effectively extracts better normals compared to other baselines.

Figure 13: Ablation study on the effect of applying LoRA on different types of attention layers. We started all models with SD v1-5, 4000 training samples and LoRA rank=8.

Figure 14: Comparison of image editing techniques for surface normal mapping. VISII and Textual Inversion yield unsatisfactory results, while InstructPix2Pix fails to interpret the task, resulting in near-original output.

Figure 15: We observe applying SDEdit method on the SDv1-5 model alone, without incorporating the additional input image latent encoding, fails to produce satisfactorily aligned and high-quality scene intrinsics. The reason for this might be the considerable domain shift that exists between RGB images and surface normal maps, which results in severe artifacts when using SDEdit. The variable “s” represents the strength of SDEdit.

   Model & Dataset & Resolution & Rank & LR & BS & LoRA Params & Generator Params & Convergence Steps \\  VQGAN & FFHQ & 256 & 8 & 1e-03 & 1 & 0.13M & 873.9M & \(\) 4000 \\ StyleGAN-v2 & FFHQ & 256 & 8 & 1e-03 & 1 & 0.14M & 24.8M & \(\) 4000 \\ StyleGAN-v2 & LSUN Bedroom & 256 & 8 & 1e-03 & 1 & 0.14M & 24.8M & \(\) 4000 \\ StyleGAN-XL & FFHQ & 256 & 8 & 1e-03 & 1 & 0.19M & 67.9M & \(\) 4000 \\ StyleGAN-XL & ImageNet & 256 & 8 & 1e-03 & 1 & 0.19M & 67.9M & \(\) 4000 \\ I-LoRA\({}_{}\) (multi step) & Open & 512 & 8 & 1e-04 & 4 & 1.59M & 943.2M & \(\) 30000 \\ I-LoRA (single step) & Open & 512 & 8 & 1e-04 & 4 & 1.59M & 943.2M & \(\) 15000 \\   

Table 5: Hyper-parameters for each model. LR refers to the learning rate and BS refers to the batch size. Please note that the number of steps required to reach convergence reported above is for normal/depth. However, it is worth noting that albedo and shading tend to require significantly fewer steps to converge (usually half of normal/depth). Additionally, I-LoRA\({}_{}\) (multi-step) and I-LoRA (single-step) are trained on real-world DIODE dataset, while the other models are trained on synthetic images within a specific domain. (Num. of params of VQGAN counts transformer + first stage models; Num. of params of I-LoRA\({}_{}\) and I-LoRA counts VAE+UNet)Figure 16: Additional results after applying improved diffusion techniques with I-LoRA\({}_{}}\). I-LoRA\({}_{}}\) was found to significantly reduce color shift artifacts observed in I-LoRA1-5\({}_{}}\) during the extraction of detailed scene intrinsic results.

Figure 17: Scene intrinsics from different generators – VQGAN, StyleGAN-v2, and StyleGAN-XL – trained on FFHQ dataset: The “image” column shows the synthetic images produced by each model. Subsequent columns show four scene intrinsics extracted by a SOTA non-generative model and I-LoRA(ours).

Figure 19: Additional results of scene intrinsics extraction from Stylegan-v2 trained on LSUN bedroom images.

Figure 18: Additional results of scene intrinsics from different generators – VQGAN, StyleGAN-v2, and StyleGAN-XL – trained on FFHQ dataset.

Figure 21: Additional results for StyleGAN-XL trained on ImageNet. StyleGAN-XL’s inability to produce image intrinsics may be due to its inability to create high-quality plausible images.

Figure 20: Additional results of scene intrinsics extraction from Stable Diffusion I-LoRA (single-step).

Figure 22: Results of I-LoRA\({}_{}\) models applied on unseen \(1024^{2}\) synthetic images. Left: original image; middle: ours; right: pseudo-ground truth.

Figure 23: Cont. results of I-LoRA\({}_{}}\) models applied on unseen \(1024^{2}\) synthetic images. Left: original image; middle: ours; right: pseudo-ground truth.

Figure 24: Cont. results of I-LoRA\({}_{}\) models applied on unseen \(1024^{2}\) synthetic images. Left: original image; middle: ours; right: pseudo-ground truth.

Figure 25: Cont. results of I-LoRA\({}_{}}\) models applied on unseen \(1024^{2}\) synthetic images. Left: original image; middle: ours; right: pseudo-ground truth.

Figure 26: Cont. results of I-LoRA\({}_{}\) models applied on unseen \(1024^{2}\) synthetic images. Left: original image; middle: ours; right: pseudo-ground truth.

Figure 27: Cont. results of I-LoRA\({}_{\!{U}\!{G}}\) models applied on unseen \(1024^{2}\) synthetic images. Left: original image; middle: ours; right: pseudo-ground truth.

Figure 28: Cont. results of I-LoRA\({}_{\!{U}\!{G}}\) models applied on unseen \(1024^{2}\) synthetic images. Left: original image; middle: ours; right: pseudo-ground truth.

Figure 29: Cont. results of I-LoRA\({}_{}\) models applied on unseen \(1024^{2}\) synthetic images. Left: original image; middle: ours; right: pseudo-ground truth.

Figure 30: Cont. results of I-LoRA\({}_{}\) models applied on unseen \(1024^{2}\) synthetic images. Left: original image; middle: ours; right: pseudo-ground truth.

Figure 31: Cont. results of I-LoRA\({}_{\!{U}\!{G}}\) models applied on unseen \(1024^{2}\) synthetic images. Left: original image; middle: ours; right: pseudo-ground truth.