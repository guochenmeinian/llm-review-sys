# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

in deployment . Furthermore, unlike today's AD models, humans do not navigate based on geometrically precise bird's-eye view (BEV) representations . Instead, humans implicitly perform object-centric perception, prediction, and planning (which we refer to as \(P_{1-3}\)): a rough identification and localization of key objects, followed by reasoning about their possible movement and aggregation of this information into a driving action .

Simultaneously, another field has been forging ahead: Vision-Language Models (VLMs) . These models have several strengths. First, they hold a base understanding of the world from internet-scale data that could potentially facilitate generalization for planning in AD. In fact, this sort of generalization has already been achieved by VLMs for simpler robotics tasks . Second, the use of language representations as an input and output offers a platform for human-friendly interaction with these models, unlike bounding boxes or trajectories that are more common to current methods . Finally, VLMs are able to make decisions in multiple steps linked by logical reasoning . Importantly, even though they reason in multiple separate steps, VLMs are end-to-end differentiable architectures, a characteristic that is highly desirable for autonomous driving .

Recent work towards enabling the application of VLMs to AD systems falls into two categories: scene-level or single object-level Visual Question Answering (VQA). Scene-level VQA refers to the task of describing the driving behavior by one or two supporting reasons, _e.g._, "The car is moving into the right lane because it is safe to do so." . Single object-level VQA formulates the understanding of the ego vehicle's response to a single object by a chain of QAs in the form of "what-which-where-how-why", _e.g._, "The ego vehicle stops because there is a pedestrian in a white shirt crossing the intersection in front of the ego vehicle and it does not want to crash into the pedestrian." . Unfortunately, neither of these paradigms provides a suitable proxy task to mimic the \(P_{1-3}\) reasoning process in humans, who consider multiple objects and reason about each in multiple steps. Therefore, in this paper, we propose a new task, along with corresponding datasets and a baseline model architecture (Fig. 1).

**Task. Graph Visual Question Answering (GVQA)** involves formulating \(P_{1-3}\) reasoning as a series of question-answer pairs (QAs) in a directed graph. Its key difference to the aforementioned VQA tasks for AD is the availability of logical dependencies between QAs which can be used to guide the answering process. GVQA also encompasses questions regarding behavior and motion planning, with dedicated metrics (details in Section 2).

**Datasets. DriveLM-nuSeenes** consist of annotated QAs, arranged in a graph, linking images with driving behavior through logical reasoning. In comparison to existing benchmarks, they provide significantly more text annotations per frame (Fig. 2). We pair these training datasets with challenging test data for evaluating zero-shot generalization.

**Model. DriveLM-Agent** employs a trajectory tokenizer that can be applied to any general VLM , coupled with a graph prompting scheme that models logical dependencies as context inputs for VLMs. The result is a simple, elegant methodology to effectively repurpose VLMs for end-to-end AD.

Our experiments provide encouraging results. We find that GVQA on DriveLM is a challenging task, where current methods obtain moderate scores and better modeling of logical dependencies is likely necessary to achieve strong QA performance. Even so, DriveLM-Agent already performs competitively to state-of-the-art driving-specific models  when tested in the open-loop planning setting, despite its task-agnostic and generalist architecture. Furthermore, employing a graph structure improves zero-shot generalization, enabling DriveLM-Agent to correctly handle novel objects unseen during training or deployment on the Waymo dataset  after training only on nuScenes  data. From these results, we believe that improving GVQA holds great potential towards building autonomous driving agents with strong generalization.

## 2 DriveLM: Task, Data, Metrics

Human drivers usually decompose their decision-making process into distinct stages that follow a logical progression which encompasses the identification and localization of key objects, their possible future action and interaction, and ego planning based on all this information . This inspires us to propose the GVQA as the critical ingredient of DriveLM, which serves as a suitable proxy task to mimic the human reasoning process. Within this section, we illustrate the formulation of the GVQA task (Section 2.1) and introduce DriveLM-Data (Section 2.2) to exemplify the instantiation of GVQA using prominent driving datasets.

### DriveLM-Task: GVQA

We organize all the Question Answer pairs (QAs) for an image frame into a graph structure, denoted by \(G\!=\!(V,E)\). \(V\) stands for the set of vertices, where each vertex represents a QA pair \(v\!=\!(q,a)\) associated with one or more key objects in the scenario. The key difference between GVQA and ordinary VQA is that the QAs in GVQA have logical dependencies, which we formulate as the edges between the vertices. \(E\!\!V\!\!V\), is a set of directed edges, where each edge \(e\!=\!(v_{p},v_{c})\) connects the parent QA and the child QA. We formulate the edge set \(E\) by incorporating two dimensions: object-level and task-level edges. At the object level, we construct the logical edges \(e\!\!E\) to represent the impact of interactions between different objects. For example, the planning QA node for the sedan is influenced by the perception QA node of the pedestrian in the illustration from Fig. 1 (center). At the task-level, we establish the logical edges \(e\!\!E\) to capture the logical chain of different reasoning stages:

* **Perception** (\(P_{1}\)): identification, description, and localization of key objects in the current scene.
* **Prediction** (\(P_{2}\)): estimation of possible action/interaction of key objects based on perception results.
* **Planning** (\(P_{3}\)): possible safe actions of the ego vehicle.
* **Behavior** (\(B\)): classification of driving decision.
* **Motion** (\(M\)): waypoints of ego vehicle future trajectory.

The concepts of perception, prediction, and planning (\(P_{1-3}\)) are similar to those in end-to-end AD , while the concepts of motion and behavior are based on the ego vehicle future trajectory. Specifically, we define the motion \(M\) as the ego vehicle future trajectory, which is a set of \(N\) points with coordinates \((x,y)\) in bird's-eye view (BEV), denoted as \(M\!=\!\{(x_{0},y_{0}),(x_{1},y_{1},...,(x_{N},y_{N})\}\). Each point is the offset between the future position and the current position by a fixed time interval. Then, the distance for \(x,y\) at each time interval is computed as:

\[\{x,y\}_{}=\{(_{x,1},_{y,1}),...,(_{x,N},_ {y,N})\}, \]

where \(_{x,i}=x_{i}-x_{i-1}\) and \(_{y,i}=y_{i}-y_{i-1}\), for \(i=1,2,,N\). The goal of the behavior representation is to serve as an interface from \(P_{1-3}\) to \(M\). To obtain a behavior representation, we map the mean of \(x_{}\) and \(y_{}\) to one of the predefined bins, where each bin corresponds to a category in either speed or steering. These are denoted as \(B_{sp}\) and \(B_{st}\) respectively. In this work, we consider 5 bins:

\[B_{sp}\{_{2},_{1},,_{1},_{2}\},\] \[B_{st}\{_{2},_{1},,_{1},_{2}\},\]

where the number in the subscript indicates the intensity. The combination of the speed and steering categories for a trajectory form its behavior category as \(B\!=\!(B_{sp},B_{st})\). While we use a simple definition of \(B\) as a starting point for research on driving with VLMs, we note that our formulation supports the incorporation of more abstract behaviors such as a lane changes or overtaking.

### DriveLM-Data

We introduce DriveLM-nuScenes to provide QAs with the graph structure defined in Section 2.1,

**DriveLM-nuScenes.** We divide the annotation process into three steps: selecting key frames from video clips, choosing key objects within these key frames, and subsequently annotating the frame-level \(P_{1-3}\) QAs for these key objects. A portion of the Perception QAs are generated from the nuScenes  and OpenLane-V2  ground truth, while the remaining QAs are manually annotated. As we manually annotate the vast majority of data in DriveLM-nuScenes, quality is particularly crucial for this portion. When annotating, we conduct multiple rounds of rigorous quality checks. In each round, we categorize the data into different batches and inspect ten percent of the data in each batch. If the qualification rate of manually annotated data in this ten percent does not meet expectations, we request the annotators to re-label all data in the batch. In Fig. 2 (left), we showcase an example of the QA annotation pipeline, where all questions undergo quality checks according to our standards. As a result, DriveLM-nuScenes stands out from previously proposed datasets with its larger scale, greater comprehensiveness, and more complex structure. These QAs cover various aspects of the driving process, ranging from perception and prediction to planning, providing a comprehensive understanding of autonomous driving scenarios as shown in Fig. 2 (right).

Figure 2: **(Left) Annotation Pipeline:** In DriveLM-nuScenes, we adopt a semi-rule-based QA labeling pipeline, where both the ground truth annotation in nuScenes/OpenLane-V2 and feedback from human annotators are used. A critical part of our pipeline is the multi-round quality check, which guarantees high data quality at reasonable costs. In DriveLM-CARLA, we meet the same standards while exploiting a fully rule-based QA labeling pipeline instead. **(Right) Question Distribution:** The questions in our dataset cover various specific aspects of driving tasks, most of which are annotated by human annotators, making this a suitable proxy for human-like driving reasoning.

## 3 Experiments

In this section, we present our experimental results that aim to address the following research questions: (1) How can VLMs be effectively repurposed for end-to-end autonomous driving? (2) Can VLMs for driving generalize when evaluated with unseen sensor setups;

**Setup.** We now briefly overview the key implementation details for the two settings used in our experiments (additional details are provided in the supplementary material). All fine-tuning is implemented with LoRA . On DriveLM-nuScenes, we finetune BLIP-2 on the train split for 10 epochs. We use a batch size of 2 for each GPU, and the entire training process spans approximately 7 hours with 8 V100 GPUs.

### VLMs for End-to-End Driving

In our first experiment, we aim to assess the ability of VLMs to perform open-loop planning on DriveLM-nuScenes. In particular, we investigate the impact of the context provided to the behavior and motion stages. Given sensor data (and in the case of VLM methods, a text input), the model is required to predict the ego-vehicle future trajectory in the form of waypoints.

**Baselines.** As a reference for the difficulty of the task, we provide a simple **Command Mean** baseline. Each frame in nuScenes is associated with one of 3 commands, 'turn left', 'turn right', or 'go straight'. We output the mean of all trajectories in the training set whose command matches the current test frame command. Further, we compare our approach to the current state-of-the-art on nuScenes, UniAD . Besides the author-released checkpoint, which requires video inputs, we train a single-frame version ('**UniAD-Single**') for a fair comparison to our single-frame VLMs. Finally, **BLIP-RT-2** denotes BLIP-2  fine-tuned on DriveLM-Data with the trajectory tokenization scheme. This acts as an indicator for the performance when using an identical network architecture as DriveLM-Agent, but no context inputs or VQA training data.

**DriveLM-Agent.** We consider 3 variants of DriveLM-Agent incorporating our proposed changes in steps: (1) a 2-stage version that predicts behavior and then motion (as described in Section 2.1), but without any \(P_{1-3}\) context for behavior prediction ('None'); (2) a 'Chain' version that builds the \(P_{1-3}\) graph, but only passes the final node (\(P_{3}\)) to the behavior stage; (3) the full model ('Graph') that uses all QAs from \(P_{1-3}\) as context for \(B\).

**Results.** We show the results for the methods listed above in Table 1. Among the baselines, BLIP-RT-2 is unable to match UniAD-Single (though both methods perform well relative to Command Mean). This shows that the single-stage approach without any reasoning is unable to compete with the prior state-of-the-art on nuScenes. However, the proposed DriveLM-Agent, which predicts behavior as an intermediate step for motion, provides a significant boost in performance, surpassing UniAD-Single. This indicates that with the appropriate prompting, VLMs can be surprisingly competitive for end-to-end driving. Interestingly, in the experimental setting of Table 1 which does not involve generalization, the Chain and Graph versions of DriveLM-Agent do not provide any further advantage over no context. Further, single-frame VLMs fall short in comparison to the privileged video-based UniAD model, indicating that VLMs with video inputs may be necessary for this task.

### Generalization Across Sensor Configurations

As a more challenging setting for evaluating the models from Section 3.1, we now apply them without any further training to a new domain: the Waymo dataset . Waymo's sensor setup does not include a rear camera, so we drop this input from UniAD-Single. The VLM methods only use the front view and do not require any adaptation.

**Results.** As shown in Table 2, UniAD-Single does not cope well with the new sensor configuration, and drops below BLIP-RT-2 in performance. The multi-stage approach of DriveLM-Agent provides further improvements. In particular, the accuracy of speed predictions rises from \(43.90\) with no context to \(54.29\) with the full graph. On the other hand, the chain approach does not provide sufficient useful information, with a speed accuracy of only \(41.28\).

    & **Behavier** & **Motion** &  &  \\  & **Context** & **Context** & \(Acc.:\) & Speed \(\) & Saser \(\) & ADE \(\) & Cal \(\) \\   Conventional Mean \\ UniAD-Single \\ BIP-RT-2 \\  } & - & - & - & - & 4.57 & 5.72 \\  & - & - & - & - & 1.80 & 2.62 \\  & - & - & - & - & - & 2.63 & 2.77 \\   Conventional Mean \\ UniAD-Single \\ BIP-RT-2 \\  } &  Nane \\