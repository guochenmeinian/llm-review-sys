# Two-Person Interaction Augmentation with Skeleton Priors

Anonymous CVPR submission

Paper ID 8

###### Abstract

Close and continuous interaction with rich contacts is a crucial aspect of human activities (e.g. hugging, dancing) and of interest in many domains like activity recognition, motion prediction, character animation, etc. However, acquiring such skeletal motion is challenging. While direct motion capture is expensive and slow, motion editing/generation is also non-trivial, as complex contact patterns with topological and geometric constraints have to be retained. To this end, we propose a new deep learning method for two-body skeletal interaction motion augmentation, which can generate variations of contact-rich interactions with varying body sizes and proportions while retaining the key geometric/topological relations between two bodies. Our system can learn effectively from a relatively small amount of data and generalize to drastically different skeleton sizes. Through exhaustive evaluation and comparison, we show it can generate high-quality motions, has strong generalizability and outperforms traditional optimization-based methods and alternative deep learning solutions.

+
Footnote †: Abstract

Close and continuous interaction with rich contacts is a crucial aspect of human activities (e.g. hugging, dancing) and of interest in many domains like activity recognition, motion prediction, character animation, etc. However, acquiring such skeletal motion is challenging. While direct motion capture is expensive and slow, motion editing/generation is also non-trivial, as complex contact patterns with topological and geometric constraints have to be retained. To this end, we propose a new deep learning method for two-body skeletal interaction motion augmentation, which can generate variations of contact-rich interactions with varying body sizes and proportions while retaining the key geometric/topological relations between two bodies. Our system can learn effectively from a relatively small amount of data and generalize to drastically different skeleton sizes. Through exhaustive evaluation and comparison, we show it can generate high-quality motions, has strong generalizability and outperforms traditional optimization-based methods and alternative deep learning solutions.

+
Footnote †: Abstract

Close and continuous interaction with rich contacts is a crucial aspect of human activities (e.g. hugging, dancing) and of interest in many domains like activity recognition, motion prediction, character animation, etc. However, acquiring such skeletal motion is challenging. While direct motion capture is expensive and slow, motion editing/generation is also non-trivial, as complex contact patterns with topological and geometric constraints have to be retained. To this end, we propose a new deep learning method for two-body skeletal interaction motion augmentation, which can generate variations of contact-rich interactions with varying body sizes and proportions while retaining the key geometric/topological relations between two bodies. Our system can learn effectively from a relatively small amount of data and generalize to drastically different skeleton sizes. Through exhaustive evaluation and comparison, we show it can generate high-quality motions, has strong generalizability and outperforms traditional optimization-based methods and alternative deep learning solutions.

## 1 Introduction

Skeletal motion is a crucial data modality in many applications, such as human activity recognition, motion analysis, security and computer graphics . However, capturing high-quality skeletal motions often requires expensive hardware, professional actors, costly post-processing and laborious trial-and-error processes . Affordable devices such as RGB-D cameras can reduce the cost but usually provide data with jittering and tracking errors . As a result, the majority of available skeletal data is based on single-person  or multiple people with short, simple and almost-no-contact interactions . Datasets with close and continuous interactions  are rare, limiting the research of motion generation , prediction , classification  within such motions.

One way to tackle the challenge is to carefully capture the motion of actors and retarget it onto different skeletons . With a single skeleton, the problem can be formulated as optimizations with respect to keeping key geometric and dynamic constraints . However, this process quickly becomes intractable with the increase of constraints such as foot contact and hand-environment contact, let alone retargeting two people with close and continuous interactions like wrestling and dancing, where inter-character geometric/topological constraints need to be retained . Consequently, multiple runs of complex optimization with careful hand-tuning of objective function weights are needed  for a single motion, which is prohibitively slow and therefore can only be used to generate small amounts of data.

Meanwhile, data-driven approaches for single body retargeting , despite being successful, cannot be directly extended to two-character interaction. Methodologically, these methods do not model inter-character geometric constraints, which is key to the semantics of interactions . From the data point of view, these approaches, especially those using deep learning , require a large amount of data, which is largely absent for two-character interaction. Existing two-character interaction datasets are for action recognition  and low-quality, or only consist of a small amount of data with limited variations in body sizes , hardly covering the distribution of possible body variations. Considering the high cost of obtaining interaction data, a method that can learn effectively from limited data and generate interactions with diversified body variations is highly desirable.

We propose a novel lightweight framework for two-character skeletal interaction augmentation, easing the need to capture a large amount of data. Our key insight is the joint relations evolving in time (e.g. relative positions, velocities, etc.) can fully describe an interaction, e.g. hugging always involves wrapping one's arms around the other's body. These relations change when the body size changes, but the _distribution_ of them should stay similar in the sense that one's arms should still wrap around the other, such that the hand-to-body distance is always smaller than e.g. the foot-to-body distance. Meanwhile, this distribution should be very different from other types of interactionse.g. wrestling. Therefore, to generate motions from different skeleton sizes, the key is being able to predict the joint relation distributions based on a given skeleton.

To this end, we propose a conditional motion generation approach, where the generated motions are conditioned on the joint relation distribution which is further conditioned on a skeleton prior, allowing a skeleton change to propagate through the joint relation distribution and finally influence the final motion. We start by modeling the joint probability of two-body motions and proposing a novel factorization to decompose it into three distributions. The three distributions are realized as neural networks, which together form an end-to-end model that conditions two-body motions on one person's body size. Further, to address the data scarcity challenge, we capture new two-body data and employ an existing optimization-based method for initial data augmentation. After training our model on the data, it can be employed for further motion data augmentation for many downstream tasks.

We evaluate our method in multiple tasks. Since there is no similar method for baselines, we compare our method with adapted baselines and optimization-based approaches, demonstrating that our method is accurate in generating desired motions, can generate diversified interactions while respecting interaction constraints, is much faster for inference and generalizes to large skeletal changes than optimization-based methods. In addition, our model benefits downstream tasks including motion prediction and activity recognition. Formally, our contributions include:

1. A new factorization of two-character interactions that allows for effective modelling of interaction features.
2. a new deep learning method for interaction retargeting/generation to the best of our knowledge, which learns and generalizes effectively from a small number of training samples.
3. A new dataset augmented from single interaction examples, containing interactions with different body sizes and proportions.

## 2 Related Work

### Deep Learning for Skeletal Motion

Neural networks have been successful in modeling skeletal motions. Convolutional neural networks can learn latent representations for denoising and synthesis . Recurrent neural networks improve the robustness and enable long horizon synthesis . Graph neural networks capture the joint relations . Generative flows combine the style and content in the latent space . Transformers co-embed human motion and body parameters into a latent representation . Diffusion models provide a larger capacity and are less prone to mode collapse in generation . But all the above research is on a single body. While there is some research in modeling human-environment interactions , two-body interactions are more complex. Very recent research shows successful synthesis of two interacting characters, but their focus is either on single character control , or fix one while generating the other . None of them models interactions, especially under varying body sizes and proportions. To our best knowledge, there is no deep-learning method for complex two-character interactions.

### Motion Retargeting

Motion retargeting adapts a character's motion to another of a different size while maintaining the motion semantics. Early research employs space-time optimization based on contact , purposefully-designed inverse kinematics solver for different morphologies , data-driven reconstruction of poses based on end-effectors , or physical filters  and physical-based solvers  considering dynamics constraints. Recently, deep learning has achieved great success, e.g. recurrent neural networks with contact modeling , skeleton-aware operators without explicitly pairing the source and target motions , and variational autoencoders for motion features preservation during retargeting . Beyond skeletal motions, the skeleton structure is also effective in video based retargeting . Fast deep learning methods are pursued for real-time robotic control . Unlike previous research, we propose a novel deep learning architecture for motion retargeting/generation of two-character interactions, which are intrinsically more complex than single-character retargeting.

### Interaction

Interaction retargeting involving more than one person is more challenging than single-body retargeting, due to their complex motion constraints  such as topological constraints , but these constraints involve heavy manual designs. As a more general solution, InteractionMesh  uses dense mesh structures to represent the spatial relations between two characters and minimizes the mesh change during retargeting  and synthesis of character-environment interactions . As it may result in unnatural movements when the skeleton is significantly different from the original one, a prioritization strategy on local relations is proposed . Nevertheless, optimisation-based methods require careful design of constraints, and incur large run-time costs.

Recently, there is a surge of deep learning methods on interactions, including human-object interaction , motion generation as reaction , from texts  and by reinforcement learning . Interaction has also been investigated in motion forecasting . Among these papers, the closest work is interaction motion generation but existing work either cannot deal with skeletons of different sizes or does not focus on continuous and close interactions.

To our best knowledge, there is no deep learning method for interaction modeling as proposed in this research.

Another key bottleneck of two-character interaction re-targeting/generation is the lack of data. Existing datasets focus on action recognition  with simple interactions. While some datasets with complex interactions are available , they include limited variations of body sizes/proportions and have a limited amount of data. In this research, we present a new dataset and a method that learns efficiently from small amounts of data.

## 3 Methodology

We denote a motion with \(T\) frames as \(q=\{q^{0},,q^{T}\}^{}^{T N 3}\) where \(q^{t}\) is the \(t^{}\) frame, and each frame \(q^{t}=\{p^{t}_{0},,p^{t}_{N}\}\) consists of \(N\) joints and \(p_{j}\) is the \(j^{}\) joint position. An interaction motion of two characters \(A\) and \(B\) is represented by \(\{q_{A},q_{B}\}\). For a specific interaction, different body sizes and proportions should not change the semantics, e.g. one character always having its arms around the other in hugging. These invariant semantics are often captured by topological/geometric features . Therefore, a skeletal change in \(B\) should cause changes in both \(q_{A}\) and \(q_{B}\) to retain the semantics. We represent a \(B\) skeleton by its bone length vector \(B_{s}^{n}\) where \(n\) is the number of bones. The aim is to model the joint probability \(p(B_{s},q_{A},q_{B})\). We propose a simple yet effective model, shown in Fig. 1.

### A New Factorization of Interaction Motions

Directly learning \(p(B_{s},q_{A},q_{B})\) would need large amounts of data containing different interactions with varying both lengths. Therefore, we first make it learnable on limited data by introducing a new factorization. First, we represent skeletons with different bone lengths as heterogeneously scaled versions of a _template_ skeleton with a bone length scale vector \(=\{1,...,1\}^{n}\), i.e. we treat the bone lengths of the template skeleton as scale 1. We abuse the notation and denote a skeleton variation by \(B_{s}\), indicating how each bone is scaled with respect to \(\).

Next, we represent motion data as deviations from some _template_ motion \(\{_{A},_{B}\}\) with the template skeleton \(\). A skeleton variation \(B_{s}\) corresponds to a distribution of motions \(\{q^{}_{A},q^{}_{B}\}\), where not only the \(B\) motion deviates from \(_{B}\), the \(A\) motion also deviates from \(_{A}\) accordingly to maintain the interaction. So we can split data into template motions and others \(\{q_{A},q_{B}\}=\{_{A},q^{}_{A}\}\{_{B},q^{}_ {B}\}\), so that \(p(B_{s},q_{A},q_{B})=p(q^{}_{A},q^{}_{B},B_{s},_{A}, _{B})\). Given \(\{_{A},_{B}\}\), \(p(q^{}_{A},q^{}_{B},B_{s},_{A},_{B})\) is an easier distribution to learn than the original \(p(B_{s},q_{A},q_{B})\), as \(\{_{A},_{B}\}\) serves as an anchor motion with an anchor skeleton, so that all other motion variations can be described by offsets from the template motion, restricting \(p(q^{}_{A},q^{}_{B},B_{s},_{A},_{B})\) to only model the distribution of offsets from \(\{_{A},_{B}\}\).

There are many ways to factorize \(p(q^{}_{A},q^{}_{B},B_{s},_{A},_{B})\) theoretically. Our new factorization follows:

\[p(q^{}_{A},q^{}_{B},B_{s},_{A},_{B})\] \[(i)=p(q^{}_{A}|q^{}_{B},B_{s},_{A},_{B}) p(q^{}_{B},B_{s},_{A},_{B})\] \[(ii)=p(q^{}_{A}|q^{}_{B},_{A})p(q^{}_{B} |B_{s},_{B})p(B_{s},_{A},_{B})\] \[(iii)=p(q^{}_{A}|q^{}_{B},_{A})p(q^{}_{B} |B_{s},_{B})p(B_{s}) \]

where (i) gives the conditional probability of \(p(q^{}_{A}|q^{}_{B},B_{s},_{A},_{B})\), and its prior \(p(q^{}_{B},B_{s},_{A},_{B})\). Further, \(p(q^{}_{B},B_{s},_{A},_{B})\) can be factorized into \(p(q^{}_{B}|B_{s},_{B})p(B_{s},_{A},_{B})\) in (ii), assuming \(q^{}_{B}\) does not depend on \(_{A}\). Given the template motion \(\{_{A},_{B}\}\) and a changed skeleton \(B_{s}\), \(\{B_{s},_{A},_{B}\} p(B_{s},_{A},_{B})\), we can sample a new \(q^{}_{B} p(q^{}_{B}|B_{s},_{B})\) that satisfies the desired skeleton change, then further sample a new \(q^{}_{A} p(q^{}_{A}|q^{}_{B},_{A})\) that maintains the interaction with \(q^{}_{B}\). Further, (iii) is obtained when \(\{_{A},_{B}\}\) is given.

The three distributions in Eq. (1) have explicit meanings. \(p(B_{s})\) is the _skeleton prior_ which captures skeletal variations that are likely to be observed; \(p(q^{}_{B}|B_{s},_{B})\) is for _motion reargeting_, i.e. modeling the distribution of possible \(B\) motions w.r.t. \(_{B}\), given a skeletal variation \(B_{s}\); \(p(q^{}_{A}|q^{}_{B},_{A})\) is for _motion adaptation_, i.e. modeling the possible \(A\) motions w.r.t. \(_{A}\), given a specific \(B\) motion \(q^{}_{B}\). Among many possible ways of factorization, our particular choice in Eq. (1) conforms to a plausible workflow where user input can be injected at multiple stages. The input can be a skeletal change \(B_{s}\) to \(p(q^{}_{B}|B_{s},_{B})\), or a keyframed new motion \(q^{}_{B}\) to \(p(q^{}_{A}|q^{}_{B},_{A})\). Alternatively, the \(B_{s}\) can be drawn from \(p(B_{s})\) for unlimited motion generation.

To keep our model small, inspired by the recent research in human motions , we learn a generative model by assuming \(p(B_{s})\), \(p(q^{}_{B}|B_{s},_{B})\) and \(p(q^{}_{A}|q^{}_{B},_{A})\) to have well-behaved latent distribution, e.g. Gaussian, shown in Fig. 1 Compared with other alternative networks such as flows and Transformers, our model is especially suitable since our data is limited. We introduce the general architecture and refer the readers to the supplementary material (SM) for details.

### Network Architecture

In Fig. 1, MLP1 and MLP2 are a five-layer (16-32-64-128-256) fully-connected (FC) network, and a five-layer (256-128-64-32-dim(\(B_{s}\))) FC network, respectively. As \(B_{s}\) is a simple n-dimensional vector with fixed structural information, i.e. each dimension representing the scale of a bone, simple MLPs work well in projecting \(B_{s}\) into a latent space where it conforms to a Normal distribution.

Next, we choose two types of networks as key components of our model to learn motion dynamics and interactions. First, spatio-temporal Graph Convolution Networks(ST-GCN) extract features by conducting spatial and temporal convolution on graph data and have been proven effective in analyzing human motions . We use ST-GCNs as encoders to extract reliable features. The other network is a Recurrent Neural Network named Graph Gated Recurrent Unit or G-GRU . G-GRU models time-series data by Gated Recurrent Unit on graph structures and have the ability to stably unroll into the future on predicting human motions . We use it as decoders in our model. This choice is again for reducing the required amount of data for training, which would be much larger if other networks, e.g. ST-GCNs are used as decoders based on our experiments.

Instead of directly learning the distribution of \(q^{}_{B}\), learning the distribution of the differences \( q_{B}=q^{}_{B}-_{B}\) is easier : \(p(q^{}_{B}|B_{s},_{B})=p( q_{B}|B_{s})\), which is easier as it becomes learning the distribution of offsets from the template motion \(_{B}\) and a skeleton variation \(B_{s}\). We encode \( q_{B}\) into a latent space then decode it back to the data space by:

\[z=((( q_{B},B_{s}), _{B}^{0},_{B}^{T})))\] \[ q^{}_{B}=(z,_{B}^{0}, _{B}^{T},B_{s}))\] \[z(0,) \]

where in both the encoding and decoding processes, we also incorporate the first and last frame of the template motion \(_{B}^{0},_{B}^{T}\) because they help stabilize the dynamics based on our results. After decoding, we add the predicted \( q^{}_{B}\) back to the template motion to get the new motion \(q^{}_{B}=q_{B}+ q^{}_{B}\).

Next, given a motion \(q^{}_{B}\), character \(A\) needs to adjust its motions to keep the interaction, leading to a distribution of possible \(q^{}_{A}\). Similarly, we focus on learning \( q_{A}=q^{}_{A}-_{A}\) by an autoencoder:

\[z=((( q_{A}),_{A}^{0},_{A}^{T},(q^{}_{B}))\] \[ q^{}_{A}=(z,_{A}^{0}, _{A}^{T}))z(0,) \]

where after decoding we compute the new motion \(q^{}_{A}=_{A}+ q^{}_{A}\).

We give more detailed architectures of ST-GCN1 and G-GRU1 in Figure 2, and the detailed architectures of ST-GCN2, ST-GCN3 and G-GRU2 in Figure 3.

### Loss functions

Training our model involves three loss terms for the three autoencoders:

\[=_{B_{S}}+_{B_{M}}+_{A_{M}}. \]

Figure 1: Overview of our model. The key components include Spatial-temporal Graph Convolution Networks (ST-GCN), Multi-layer perceptrons (MLP) and G-GRU networks. Details are in the supplementary material (SM).

Figure 3: The architecture of ST-GCN2, ST-GCN3 and G-GRU2. More details are in the supplementary material.

Figure 2: The architecture of ST-GCN1 and G-GRU1. More details are in the supplementary material.

Minimizing \(_{B_{s}}\) learns MLP1 and MLP2 to learn the distribution of possible skeleton variations \(B_{s}\):

\[_{B_{S}}=||B^{}_{s}-B_{s}||_{1}^{2}+D_{KL}[z|| (0,)], \]

where \(z\) is the output of MLP1, \(B^{}_{s}\) is the output of MLP2, \(B_{s}\) is the ground-truth skeleton variation and \(D_{KL}\) is the KL-divergence.

Next, \(_{B_{M}}\) is for training ST-GCN1 and G-GRU1:

\[_{B_{M}}= \{_{1}||^{}_{B}-q^{}_{B} ||_{1}+_{2}||^{}_{B}-^{}_{B}||_{1}\] \[+_{3}BL(^{}_{B},q^{}_{B})\}+_{4}D _{KL}[z||(0,)], \]

where \(z\) is the latent variable, \(_{4}=1-_{1}-_{2}-_{3}\), \(M\) is the total number of motions. \(^{}_{B}\) and \(q^{}_{B}\) are the predicted and the ground-truth B motion. \(_{1}=0.75\), \(_{2}=0.1\) and \(_{3}=0.05\). \(||||_{1}\) is the \(l_{1}\) norm and \(p(z|c)(0,)\). \(BL(^{}_{B},q^{}_{B})\) is the bone-length loss between \(^{}_{B}\) and \(q^{}_{B}\):

\[BL(,q)=_{t}||bone\_len(^{t})-bone\_len(q^{})||_{2} ^{2}, \]

where \(bone\_len\) computes the bone lengths of frame \(t\) of \(\) and \(q\). Note we minimize the difference between the ground-truth and prediction on the _zero-order_ and _first-order_ derivative in Eq. 6.

Summarily for \(_{A_{M}}\):

\[_{A_{M}}= [_{1}||^{}_{A}-q^{}_{A}|| _{1}+_{2}||^{}_{A}-^{}_{A}||_{1}\] \[+_{3}BL(^{}_{A},q^{}_{A})]+_{4}D_{KL }[z||(0,)], \]

where \(z\) is the latent variable. \(_{4}=1-_{1}-_{2}-_{3}\), \(M\) is the total number of motions. \(^{}_{A}\) and \(q^{}_{A}\) are the predicted and the ground-truth B motion. \(_{4}=1-_{1}-_{2}-_{3}\), and \(_{1}=0.75\), \(_{2}=0.1\) and \(_{3}=0.05\). \(BL(^{}_{A},q^{}_{A})\) is the same bone length loss as in Eq. 7.

## 4 A New Interaction Dataset

To our best knowledge, there are few public datasets focusing on close and continuous interactions except . To construct our dataset, we first obtain base motions and augment them. The base motion details are shown in the SM. We obtain "Judo". From CMU , we choose "Face-to-back", "Turn-around" and "Hold-body". From ExPI , we choose "Around-the-back", "Back-flip", "Big-ben", "Noser" and "Chandelle". These interactions are sufficiently complex to fully evaluate the robustness and generalizability of our model. They show the need for automated motion retargeting/generation as it requires hiring professional actors. Also, these motions contain rich and sustained contacts and close and continuous interactions, where single-body motion retargeting methods can easily lead to breach of contact and severe body penetrations.

After obtaining the base motions, a number of variations of each motion are collected to form a dataset. Our method is independent of how the variations are obtained. One may consider motion capture with actors of different body sizes, or manual keyframing with different characters. We employ a semi-automated approach. We manually change the skeleton to generate variations, after which we adapt an iterative and interactive optimization approach called InteractionMesh  to generate new motions based on the changed skeletons. This allows us to precisely control the bone sizes for rigorous and consistent evaluation.

For each base motion, we vary the bones by scales within [0.75, 1.25] with a 0.05 spacing, where the original skeleton is used as the scale-1 template skeleton. This spans the +25% range of the original skeleton, covering most of the population. The process is semi-automatic, involving the use of an optimisation engine to carefully retarget an interaction to different body sizes, with manual adjustment of constraint weights and inspection of results. Synthesizing a few seconds of interaction generally requires around 2 minutes of computation. This is done multiple times for one variation of a base motion, due to the need for manual weighting tuning.

## 5 Experiments

### Tasks, Metrics and Generalization Settings

**Tasks**. Since our model can generate motions with or without user input to specify a skeleton variation, we test different model variants for motion augmentation. Specifically, we evaluate our model on motion augmentation via retargeting and generation. If \(B_{s}\) is given, we refer to the task as _retargeting_ where we only use G-GRU1 and G-GRU2 for inference; if \(B_{s}\) is not given, we use the full model (MLP2+G-GRU1+G-GRU2) and refer to it as _generation_.

**Metrics**. We employ four metrics for evaluation: joint position reconstruction error (\(E_{r}\)), bone-length error (\(E_{b}\)), and Frechet Inception Distance (FID), and joint-pair distance error (JPD). \(E_{r}\), \(E_{b}\) and JPD are based on \(l_{2}\) distance. FID is used to compare the distributional difference between the generated motions and the data. JPD measures the key joint-pair distance error. The key joint pairs are the body parts in continuous contact. It is to investigate the key spatial relations between joint pairs in different motions (Judo: A) 409 right hand to B's spine; Face-to-back: A's left hand to B's right hand; Turn-around: A's left hand to B's right hand; Hold-Body: A's right hand to B's spine; Around-the-back: A's left hand to B's right hand; Back-flip: A's left hand to B's right hand; Back-flip: A's left hand to B's right hand; Big-ben: A's right hand to B's right hip; Noser: A's right hand to B's right hip; Chandelle: A's right hand to B's right hip. All results reported are per joint results averaged over A and B.

**Generalization Settings**. Our dataset has two different skeletal topologies shown in the SM. Therefore, we divide them into two datasets: D1 (M1-4) and D2 (M5-M9) and conduct experiments on them separately. We employ four different settings to evaluate our model: _random_, _cross-scale_, _cross-interaction_ and _cross-scale-interaction_:
1. _Random_ means a random split on the data for training and testing where we keep 20% data for testing.
2. _Cross-scale_ means we train on moderate bone scales but predict on larger skeleton variations. Our training data is within the scale [0.95, 1.05] and our testing data is both much smaller [0.75, 0.85] and larger [1.15, 1.25]. Note the testing varies up to +/- 25% of the bone lengths covering a wide range of bodies.
3. _Cross-interaction_ is splitting the data by interaction types, e.g. training on Judo and tested dancing. When we choose one or several interactions for testing, the other interactions are used for training the model. Specifically, in D1, we split the data into two sets: M1-M2 and M3-M4; in D2, we split them into two sets: M5-M7 and M8-M9. In both, when one group is used for training, the other is used for testing.
4. _Cross-scale-interaction_ is both cross-scale and cross-interaction, which is the hardest setting. This means that the scale [0.95, 1.05] of some interactions are used for training, and the scale [0.75, 0.85] and [1.15, 1.25] in the other interactions are for testing. For instance, in D1, when the scale [0.95, 1.05] of M1-M2 is used for training, the scale [0.75, 0.85] and [1.15, 1.25] in M3-M4 are for testing.

### Evaluation

#### 5.2.1 Retargeting and Generation

We present the main results here and refer the readers to the SM for more results and details.

We first show quantitative evaluation in Tab. 2. Across the two tasks, generation is harder than retargeting, as the bone scales are not given in generation. Naturally, the bone length error \(E_{b}\) is almost always slightly worse than Retargeting and so is JPD. But even the worst case is 330% in \(E_{b}\) and 206.89% worse in JPD which suggests the model generalizability on unseen scales and interactions in general is strong. We show visual results in Fig. 4 and the video. Together with the scaled skeleton, the poses are automatically adapted on both characters to keep the geometric relations of the interaction.

In terms of generation settings, the overall difficulty should be Cross-scale-interaction \(>\) Cross-interaction \(>\) Cross-scale \(>\) Random, as more and more information is included in training data from Cross-scale-interaction to 466. Random. The metrics in Tab. 2 are consistent with this expectation. Cross-scale-interaction is the most challenging task which is testing the model on both unseen bone sizes and interactions simultaneously. Its metrics are worse than the other three in general as expected. Despite the worse results, the visual results of cross-scale-interaction are of good quality. We show one example (with the worst metrics) in Fig. 5 in comparison with ground-truth.

 Base Motion & M1 & M2 & M3 & M4 & M5 & M6 & M7 & M8 & M9 & Total \\  Original frames & 91 & 536 & 561 & 488 & 294 & 248 & 238 & 518 & 345 & 3,319 \\  Augmented motion & 160 & 119 & 119 & 119 & 90 & 90 & 90 & 90 & 90 & 967 \\  Augmented frames & 14,560 & 63,784 & 66,759 & 58,072 & 26,460 & 22,320 & 21,420 & 46,620 & 31,050 & 351,045 \\ 

Table 1: M1: Judo, M2 Face-to-back, M3 Turn-around, M4: Hold-body, M5 Around-the-back, M6 Back-flip, M7 Big-ben, M8 Noser, M9 Chandelle. More details are in the SM.

  & \(E_{r}\) & \(E_{b}\) & **JPD** & \(ID\) & \(E_{b}\) & **JPD** \\    & Random & 1.069 & 0.171 & 3.008 & 2.934 & 0.18 & 3.421 \\  & Cross-scale & 2.017 & 3.040 & 4.248 & 3.973 & 0.354 & 3.404 \\  & Cross-scale interaction & 2.843 & 0.476 & 4.434 & 4.071 & 0.092 & 4.903 \\  & Cross-scale-interaction & 3.021 & 0.659 & 4.754 & 4.369 & 0.753 & 5.607 \\   & Random & 0.067 & 0.094 & 0.104 & 1.719 & 0.005 & 0.101 \\  & Cross-scale & 0.344 & 0.018 & 0.241 & 2.364 & 0.023 & 0.645 \\  & Cross-interaction & 0.071 & 0.067 & 0.625 & 3.077 & 0.097 & 1.044 \\  & Cross-scale interaction & 1.051 & 0.318 & 0.456 & 3.256 & 0.143 & 1.317 \\   & Random & 1.076 & 0.204 & 2.757 & 5.573 & 0.093 & 2.134 \\  & Cross-scale & 1.563 & 0.066 & 2.948 & 6.556 & 0.094 & 2.872 \\  & Cross-scale interaction & 1.644 & 0.098 & 3.417 & 6.712 & 0.127 & 3.095 \\  & Cross-scale interaction & 1.928 & 0.133 & 3.498 & 6.863 & 0.153 & 3.317 \\   & Random & 0.191 & 0.017 & 0.264 & 1.579 & 0.03 & 0.297 \\  & Cross-scale & 0.471 & 0.079 & 0.418 & 2.148 & 0.087 & 1.971 \\  & Cross-scale interaction & 0.617 & 0.104 & 0.589 & 2.648 & 0.111 & 1.347 \\  & Cross-scale interaction & 0.897 & 0.112 & 0.624 & 3.094 & 0.129 & 1.915 \\   & Random & 1.975 & 0.038 & 0.398 & 0.69 & 0.091 & 0.604 \\  & Cross-scale & 2.674 & 0.016 & 0.837 & 1.283 & 0.031 & 1.157 \\  & Cross-correlation & 3.067 & 0.042 & 1.623 & 1.431 & 0.05 & 1.894 \\  & Cross-scale interaction & 3.864 & 0.067 & 2.268 & 1.897 & 0.094 & 3.068 \\   & Random & 1.878 & 0.088 & 0.448 & 0.688 & 0.013 & 0.624 \\  & Cross-scale & 1.615 & 0.029 & 0.997 & 1.228 & 0.028 & 1.273 \\  & Cross-scale interaction & 4.013 & 0.031 & 1.923 & 1.523 & 0.039 & 2.044 \\  & Cross-scale interaction & 4.076 & 0.265 & 2.641 & 1.667 & 0.083 & 2.54 \\   & Random & 2.746 & 0.006 & 0.495 & 0.645 & 0.015 & 0.702 \\  & Cross-scale & 5.204 & 0.017 & 1.163 & 1.153 & 0.03 & 2.14 \\  & Cross-correlation & 5.648 & 0.299 & 2.329 & 1.492 & 0.042 & 2.32 \\  & Cross-scale interaction & 5.757 & 0.066 & 2.759 & 1.475 & 0.069 & 3.762 \\   & Random & 2.272 & 0.004 & 0.060 & 0.476 & 0.012 & 0.634 \\  & Cross-scale & 3.132 & 0.021 & 0.964 & 1.349 & 0.088 & 1.374 \\  & Cross-interaction & 3.389 & 0.04 & 1.541 & 1.671 & 0.057 & 1.862 \\  & Cross-scale interaction & 3.971 & 0.103 & 2.341 & 2.965 & 0.103 & 2.657 \\   & Random & 2.234 & 0.005 & 0.430 & 0.634 & 0.099 & 0.561 \\  & Cross-scale interaction & 2.935 & 0.01 & 0.934 & 1.412 & 0.043 & 1.259 \\   & Cross-scale interaction & 3.256 & 0.023 & 1.674 & 1.842 & 0.051 & 1.903 \\   & Cross-scale interaction & 3.623 & 0.064 & 2.842 & 2.854 & 0.114 & 2.971 \\   

Table 2: Retargeting (left) and Generation (right). Here is the result of D1 (M1-4) and D2 (M5-9).

#### 5.2.2 Extrapolating to Large Unseen Scales

We predict larger scales. The scales are beyond our dataset (including the testing data). We show one example of Turn-around on 0.65 and 1.3 in the SM, which shows that our model can extrapolate to larger skeletal variations when trained only using data on scales [0.95, 1.05]. More examples can be found in the video. Although larger scale variations e.g. 0.5 and 1.5 might lead to unnatural motions, the SM already demonstrate the generalizability of our model.

### Comparison

To our best knowledge, it is new for deep learning to be employed for interaction augmentation with varying body sizes. So there is no similar research. Therefore, we adapt two single-body methods () which provide conditioned generation and are the only methods we know that could potentially be adapted for handling varying bone lengths, i.e. we train the model by labelling different scales as different conditions and train the model on scale [0.75, 1.25]. More specifically, both models require action type (i.e. a class label) as input, so we label data at different scales as different classes. Note  and  cannot generate motions for unseen action types, which means they cannot predict on unseen scales like our method.

We show the metrics in Tab. 3. After trying our best to train , it still generates jittering motions. It can preserve the bone-length better than  but its FID and JPD  are much worse.  generate better results but it is still much worse than our method. We show one example of Hold-Body in Fig. 6 in comparison with . Overall, single-body methods even when adapted cannot easily generate interactions.

We also compare with InteractionMesh . Since our ground-truth is from InteractionMesh, comparisons on the aforementioned evaluation metrics would be meaningless. Instead, we compare the speed and motion quality on unseen extreme scales. The inference time of our model is 0.323 seconds, while InteractionMesh needs \(\)120 seconds on average per optimization, plus the time needed for manual tuning of the weighting. Admittedly, our model needs overheads for training. However, once trained, it is very fast and can be used for interactive applications. Further, InteractionMesh needs to do optimization for every given \(B_{s}\), while our model is trained once then does inference for any \(B_{s}\). Last but not least, InteractionMesh sometimes fails to

Figure 4: In the original Judo motion (top), the red character is augmented for a bigger body (middle) and a smaller body (bottom), while retaining the key features of the interaction semantics. The black boxes in column **a** highlight how the “Judo holding” semantics, i.e., the red character holding the blue one, are adapted. The black boxes in column **b** show a similar example.

Figure 5: Comparison between ground-truth (top) and cross-scale-interaction (bottom). The skeleton of the red character is changed. Both of them are Back-flip on scale 0.85.

  &  &  \\   & Our method &  &  & Our method &  &  \\ FID & **0.412** & 2.257 & 40.351 & **0.267** & 1.998 & 28.459 \\ Eb & **0.002** & 0.541 & 0.389 & **0.118** & 0.334 & 0.311 \\ JPD & **0.168** & 1.463 & 4.903 & **3.401** & 4.532 & 5.648 \\ 

Table 3: Results at Scale 1.25, averaged over 10 randomly generated motions.

Figure 6: Scale 1.25 comparison. Left: ground-truth, mid: ours, right: .  generates unnatural poses and break contact (enlarged parts). Zoom-in for better visualization.

converge due to its optimization set up, resulting in either numerical explosion or very unnatural motions (see video). This requires careful manual tuning. Comparatively, our model does not need manual intervention.

### Downstream Tasks

Motion augmentation can benefit various downstream tasks. Here we show two downstream tasks: motion prediction and activity recognition. In motion prediction we train two models  and  on the ExPI dataset  with/without our data augmentation, following their settings. The testing protocols and evaluation metrics follow . The results are shown in Tab. 4, where 90 of 100 metrics are improved by our augmentation, with a maximum 47.88% improvement on JME (M5-AB-0.2sec) and a maximum 47.74% improvement on AME (M5-AB-0.6sec).

In activity recognition, we train three latest activity classifiers HD-GCN , STGAT  and TCA-GCN  on ExPI with/without data augmentation, following two data splits: 80/10/10 and 50/20/30 split on training/validation/testing data. The results are shown in Tab. 5. The data augmentation improves the accuracy across all models and all split settings. As the training data is reduced from 80% to 50%, the results with data augmentation have a small deterioration (less than 1.49%). Without data augmentation, it quickly drops by as much as 3.42%.

We further show the quality of the augmented motions via a trained classifier. If a trained classifier can correctly recognize the generated motions, then it suggests the generated features have similar features to the original data. We train the aforementioned classifiers on the original ExPI data and use the generated motions as testing data. Tab. 6 shows the action recognition result. Our method outperforms the other two methods in all three action recognition classifiers, which shows that our generated data has more similar features to the ground-truth. Given close interaction data is new  and its limited variety and amounts, our method provide an efficient way of augmenting such data for activity recognition.

### Alternative Architectures

Our model combines existing network components in a novel way for interaction augmentation, so a natural question is if there are other better alternative architectures. We test several alternative network architectures inspired by existing research. The selection criteria is they need to be data efficient for learning, so we exclude some data-demanding architectures such as Transformers or Diffusion models. The details and results are shown in the SM, but overall our model outperforms the alternative architectures.

## 6 Conclusion, Limitations & Discussion

To our best knowledge, our research is the very first deep learning model for interaction augmentation. It has high accuracy in generating desired skeletal changes, great flexibility in generating diversified motions, strong generalizability to unseen and large skeletal scales, and benefits to multiple downstream tasks. One limitation is that we need some data samples to start and require the same skeletal topology to do cross-motion motion augmentation. However, considering the difficulties of interaction motion capture, our method provides a new and fast way of iteratively augmenting a single captured motion then learning to generate infinite number of variations. Next, although we use InteractionMesh to generate training data, our method can easily incorporate other data sources such as captured motions from different subjects as well as manually created motions by animators. Given the small number of motions needed by our method, this is still a fast pipeline to acquire a large number of interactions with varying body sizes.

**Settings/Classifiers** & **HD-GCN ** & **STGAT ** & ** TCA-GCN ** \\ 
**ACTOR** & 97.68 & 98.03 & 97.22 \\ 
**Action2** & 97.43 & 96.90 & 96.43 \\  Our method & **98.64** & **98.53** & **97.93** \\  

Table 6: Activity recognition accuracy on 3 different methods from D2 (M5-9). Training on the ground-truth and testing on generated 200 motions.

 Predicatev & 0.2 & 0.4 & 0.6 & 0.8 & 1.0 \\   & JME & **0.2340**,409 & **0.4270**,771 & **6.930**,103 & **0.7221**,365 & **0.8481**,594 \\  & AME & **0.4200**,670 & **0.5760**,100 & **1.4349**,149 & **1.259**,187 & **1.4274**,176 \\   & **0.5200**,552 & **0.5480**,870 & **1.5091**,187 & **1.458**,153 & **1.5761**,779 \\  & AME & **0.6710**,682 & 1.7701**,**168 & **1.530**,157 & **1.5981**,968 & **2.2532**,326 \\   & **0.5380**,056 & 0.9710**,**1280 & 1.302 & 1.7301 & 1.9261**,248 \\  & AME & **0.7900**,727 & 1.3340 & 1.8809 & 2.3192 & **2.290**,260 & **2.6733** \\   & JME & **0.5700**,502 & **0.5279**,035 & 1.3724 & **1.468**,197 & **1.862**,013 \\  & AME & **0.5700**,535 & 1.3330 & **1.790**,189 & **1.998**,198 & **2.3532**,423 \\   & JME & **0.5800**,500 & **0.5840**,030 & **1.280**,132 & **1.867**,725 & **1.902**,201 \\  & AME & **0.7200**,723 & 1.4681 & **1.849**,153 & **1.849**,152 & **2.413**,625 \\   & JME & **0.7200**,507 & **0.4440**,767 & **0.6521**,721 & **0.763**,789 & **0.8671**,641 \\  & AME & **0.7600**,678 & **0.7481**,109 & **1.408**,163 & **1.435**,154 & **1.551**,123 \\   & JME & **0.5800**,548 & **0.5880**,030 & **1.267**,806 & **1.536**,729 & **1.536**,729 \\  & AME & **0.6830**,050 & **1.494**,194 & **1.2520**,566 & **1.969**,973 & **1.256**,335 \\   & **0.5800**,549 & **1.2670**,100 & 1.323 & 1.464**,651 & **1.859**,197 & **1.940** \\  & AME & **0.7230**,746 & **1.4667**,419 & **1.398**,190 & 3.291/**3.297**,**268 & **2.621**,612 \\   & JME & **0.5970**,605 & **1.4580**,168 & **1.2481**,315 & **1.701**,167 & **1.892**,148 \\  & AME & **0.7200**,748 & **1.3437**,184 & **1.3849**,180 & **2.4667**,102 & **2.332**,425 \\   & JME & **0.5240**,528 & **0.6820**,892 & **1.378**,192 & **1.4761**,702 & **1.932**,036 \\  & AME & 0.7180**,713 & **1.4861**,419 & **1.8671**,901 & **2.067**,209 & **2.532**,672 \\  

Table 4: Motion prediction of  (top) and  (bottom) in JME (joint mean error) and AME (aligned mean error) from D2 (M5-9). In each test, xx/xx is with/without data augmentation.

**Methods/Classifiers** & **HD-GCN ** & **STGAT ** & **TCA-GCN ** \\ 
**ACTOR** & 97.68 & 98.03 & 97.22 \\ 
**Action2** & 97.43 & 96.90 & 96.43 \\  Our method & **98.64** & **98.53** & **97.93** \\  

Table 5: Activity recognition accuracy on 3 different classifiers from ExPI . In each test, xx/xx is with/without data augmentation.