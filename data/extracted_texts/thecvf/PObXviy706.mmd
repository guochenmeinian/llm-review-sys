# DAFT: Data-Aware Fine-Tuning of Foundation Models for Efficient and Effective Medical Image Segmentation

DAFT: Data-Aware Fine-Tuning of Foundation Models for Efficient and Effective Medical Image Segmentation

Alexander Pfefferle0009-0003-5457-7526

1University of Freiburg, Freiburg, Germany 1

Lennart Purucker0009-0001-1181-0549

2ELLIS Institute Tubingen, Tubingen, Germany 2{pfeffera, purucker, fh}@cs.uni-freiburg.de

Frank Hutter0000-0002-2037-3694

2ELLIS Institute Tubingen, Tubingen, Germany 2{pfeffera, purucker, fh}@cs.uni-freiburg.de

###### Abstract

Efficient and effective medical image segmentation supports faster and better decision-making of medical experts. In this work, we propose data-aware fine-tuning (DAFT), a method for enabling efficient and effective inference with foundation models, and apply it to medical image segmentation tasks. Following concepts from meta-learning for algorithm selection and dynamic selection, DAFT aims to fine-tune several versions of a foundation model on subsets of all available data instead of fine-tuning just one larger model. Then, at inference time, we select which fine-tuned model to use for the prediction depending on the distribution of the input data. DAFT enables us to create more efficient and effective models for each subset than when creating one model for all data. In our implementation of DAFT for the "Segment Anything In Medical Images On Laptop" competition as part of the CVPR24 Workshop on "Foundation Models for Medical Vision", we use the EfficientViT architecture, knowledge distillation, and OpenVINO runtime to further improve the inference. Additionally, we optimized the efficiency of our method through a flood of improvements, including an optimized inference runtime, caching, optimizing the docker deployment container, and better inference code. DAFT improved the average dice similarity coefficient from 78.64% to 83.29% and the normalized surface distance from 80.58% to 85.59% compared to the baseline on the test data. Our final submission secured first place on the post-challenge leaderboard. Finally, and more importantly, we improved the average inference speed over the baseline by a factor of 6.5 (14.69 to 2.25 seconds) on the test set.

Keywords:Data Aware Fine Tuning Efficient Image Segmentation

## 1 Introduction

Medical experts in various medical applications have to spot and detect patterns in medical images from computer tomography (CT), Microscopy, and X-ray on a daily basis. Clinical applications that rely on image segmentation to detect regions of interest in medical images enable experts to make faster and better decisions. Such clinical applications can be powered by state-of-the-art image segmentation foundation models like SAM  or MedSAM .

The problem with foundation models for image segmentation is that they often are large, expensive models, e.g., MedSAM has more than 93 Million parameters and requires more than 10GB RAM when run on CPU. Furthermore, trends like the ever-increasing size of foundation models, as seen in the field of large language models3, will likely make new image segmentation models only more expensive to use in real-world inference in a clinical application. Yet, critically, medical images are always sensitive patient data. Such images are not easily shared with others and often cannot leave the hospital's network or _even leave an expert's laptop_.

Therefore, it is crucial for the viability and usability of clinical applications to enable image segmentation models that are resource-efficient and effective in supporting the decisions of experts. Our goal is to enable even the most resource-constrained experts to benefit from image segmentation models.

Our goal perfectly aligns with the challenge Segment Anything In Medical Images On Laptop, organized by Jun Ma, Yuyin Zhou, Bo Wang, Feifei Li, and Sumin Kim as a part of the CVPR24 Workshop on Foundation Models for Medical Vision. In this manuscript, we, the automlfreiburg Team from the University of Freiburg, present _data-aware fine-tuning_ (DAFT), our proposed method to enable efficient and effective inference with foundation models applied to medical image segmentation tasks to solve the challenge.

DAFT aims to fine-tune _several versions_ of a foundation model on _subsets_ of all available fine-tuning data to produce models that need to understand and remember less while also being more effective for their specific subset's distribution. Then, at inference time, we _select_ which fine-tuned model to use for the prediction depending on the distribution of the input data. DAFT follows traditional concepts from meta-learning algorithm selection  and dynamic selection , which we adapted to the age of foundation models.

The rest of this manuscript is structured as follows: the remainder of this section introduces the challenge's background, the approach we used, and related work. In Section 2, we present our method in more detail, describing our fine-tuning pipeline and how we improved the runtime speed of our approach. Section 3 contains the implementation details and our protocol for evaluating submissions. Our results are demonstrated in section 4. Finally, we present our improvements for the post-challenge "performance booster" in section5 before concluding our manuscript.

### Competition Background

The Segment Anything In Medical Images On Laptop competition challenges participants to create a universal promptable medical image segmentation predictor, that is deployable on a laptop. Hereby, deployable on a laptop meansthat we _do not_ have access to a GPU and only 8GB of RAM and a CPU with 6 cores.

The desired universal promptable medical image segmentation predictor must be able to produce predictions for a wide variety of medical imaging modalities, including 3D modalities, such as Computer Tomography (CT), Magnetic Resonance Tomography (MR), Positron Emission Tomography (PET), 2D greyscale images like Ultrasonic (US), X-Ray, Optical Coherence Tomography (OCT), Mammography and 2D RGB images like Dermoscopy, Endoscopy, Fundus and Microscopy. The prompts are boxes (2D or 3D) surrounding the area of the to-be-segmented area of the image.

The universal predictor, deployed in a Docker  container, is evaluated based on the average of the rank of three metrics: the Dice Similarity Coefficient (DSC), Normalized Surface Distance (NSD), and running time.

The organizers provided a preprocessed dataset we could use for training. Furthermore, they shared a list of additional datasets and a list of pretrained models that we were allowed to use. Both lists were extended and curated by the community up until one month before the submission deadline. Moreover, the challenge was hosted on Codabench  with a validation leaderboard with up to 6 submissions per day. The organizers also supported up to six docker submissions on the validation data in total.

When submitting to Codabench, participants would upload the predictions of their model and receive the average DSC and NSD for each modality. The organizers would execute Docker submissions in the evaluation environment, and participants received the predictions and runtime for each data point as well as any error messages.

### Our Approach

We implemented DAFT for this challenge by following a training protocol of 1) knowledge distillation, 2) general fine-tuning, and 3) data-aware fine-tuning for 11 subsets of the data.

In detail, we defined 11 subsets by separating the data based on the origin of the image, like CT or MR. Then for each subset, we 1) created an EfficientViT  backbone for our foundation model by knowledge-distilling and using pre-trained weights; 2) fine-tuned the model on all available data; and 3) fine-tuned only on the training data of the respective subset. Then, at inference, we associated the input image with one of our 11 subsets and selected the respective fine-tuned foundation model for segmenting the input image.

Besides DAFT, we implemented a flood of improvements for inference efficiency: using EfficientViT as a faster neural network architecture, an optimized inference runtime based on OpenVINO, caching, optimizing the docker deployment container, and enhancing the inference code.

On the test data, we show that DAFT improved the average across all modalities for the dice similarity coefficient from 78.64% to 83.29% and for the normalized surface distance from 80.58% to 85.59% compared to the baseline. Ourperformance booster submission secured first place on the post-challenge leaderboard. Finally, and more importantly, we improved the average inference speed over the baseline by a factor of 6.5 (14.69 to 2.25 seconds) on the test set.

### Related Work

In general, fine-tuning  has become more important in recent years due to the prevalence of large and expensive foundation models that need to be adjusted for specific applications at hand. Specifically, fine-tuning has shown to be extremely powerful for medical image segmentation tasks. MedSAM  is a segmentation foundation model for medical images created by the organizers of the competition. It was created by fine-tuning the segment anything model (SAM)  on over 1 Million medical images. The creators of MedSAM also released LiteMedSAM4, a lightweight version of MedSAM that was used as a baseline in the competition.

At the same time, there has been research into making segmentation foundation models faster. One area of research in this regard focuses on finding more efficient architectures, e.g. EfficientViT-SAM  or MobileSAM  for SAM. Speeding up inference of a model can also be achieved by using a runtime that is better optimized for deployment on certain hardware, e.g. OpenVINO  or the ONNXRuntime .

Besides fine-tuning, knowledge distillation  enables a model that is being trained to leverage knowledge gained by other models that have been trained before. LiteMedSAM was created by distilling the vision transformer in MedSAM to a TinyViT  and performing additional fine-tuning afterward.

Furthermore, DAFT is highly related to meta-learning for algorithm selection  and dynamic selection . In the former, a meta-model is learned to select one algorithm from a fixed set of potential algorithms to solve a problem. For example, a specific SAT solver is selected to solve a specific SAT instance. This motivated our approach in that we treat different subsets of the data as different problems that certain foundation models might solve better than others. In dynamic selection, a meta-model selects which model is used to obtain predictions _per data point_ of a machine learning task. This specifically motivated our inference setup. So far, to the best of our knowledge, no one applied the concepts of dynamic selection or meta-learning algorithm selection to fine-tuning.

A mixture of experts (MoE) model  is the closest related work for DAFT with foundation models . But MoE models differ fundamentally as the selection, i.e., routing, happens during the inference and training but not before, as we propose with DAFT.

## 2 Method: Data-Aware Fine-Tuning

The concept of data-aware fine-tuning (DAFT) is to select differently fine-tuned foundation models for different _foundation modalities_.

Foundation modalities are subsets of the data that differ in their distribution for the application of a foundation model. Generally, foundation modalities can be understood as different clusters of data points in the data for which a foundation model would be used. In other words, a collection of data points that are sufficiently similar based on their intrinsic characteristics. For this challenge and the application of medical images, we chose the origins of medical images, e.g., CT or MR, as our foundation modalities.

To then decide which fine-tuned foundation model to select given a new input image, we created a _meta-model_. The meta-model predicts which foundation modality the input image belongs to, which in turn decides which fine-tuned foundation model we select to segment the image. Due to our choice of foundation modalities, the meta-model was extremely simple in this challenge, as detailed in the following subsection. Figure 1 provides a general overview of our method.

### Data Subset Selection for Data-Aware Fine-Tuning

For this challenge and the application of medical images, we chose the origins of medical images, e.g., CT or MR, as our 11 foundation modalities. This choice is based on our hypothesis that fine-tuning a foundation model that, for example, focuses only on learning Dermoscopy data might perform better on Dermoscopy data than a model that was trained on both CT and Dermoscopy data. Likewise, we hypothesize that the model fine-tuned only on a subset of the data can be made more efficient at inference as it also only requires a subset of the capacity, enabling us, in principle, to use smaller models or prune fine-tuned models more aggressively.

We shortly investigated subdividing X-ray images into X-ray upper extremity, lower extremity, etc. but stopped due to time constraints. Likewise, we considered creating more general foundation modalities by splitting images by {3D modalities, 2D greyscale, 2D RGB}, {3D, 2D}, or {RGB, not RGB}. For the sake of simplicity, we stick with our original choice and leave it to future work to further subdivide these foundation modalities for medical images. Still, we would like to highlight that more general foundation modalities are likely useful to avoid overfitting.

For data modalities with less obvious human-perceivable differences in the data, like with foundation models for tabular data  and time series , we suggest using unsupervised learning to cluster the data into foundation modalities, or compute meta-features of the data and cluster these meta-features.

Given a set of foundation modalities, we require a foundation model selector to determine when to use which fine-tuned foundation model for an image. To this end, during the initial phase of the challenge, we created a selector for medical images by training a tabular meta-model to predict the foundation modality of an input image. We also considered training an image classifier to predict theFigure 1: Overview of our implementation of DAFT for medical image segmentation.

modality but decided that the increase in runtime would be too expensive. To create a meta-dataset for training the selector, we computed meta-features (like entropy, number of boxes, or percentage of black pixels) of all data points in the training data and stored their foundation modalities (like CT or MR) as target labels. We decided to create two meta-models since the meta-features for 2D and 3D data points can be different. Our model selector checked whether we have a 2D or 3D data point at hand and used the corresponding meta-model afterward. We used a scikit-learn MLPClassifier3 for both meta-models and achieved an accuracy score of 64% on 2D and 88% on 3D when training on the dataset provided by the organizers and evaluating on the official validation set. This shows that we are able to differentiate the foundation modalities reasonably well with our straightforward meta-features and meta-models.

While the selector was working as intended, we realized that our choice of foundation modalities would always be known in a realistic use case for the medical domain. In real-world settings where medical image segmentation is used, we would know the modality within our software, as the different modalities are clearly separable medical applications (and software products). With this in mind, we instead opted to select the foundation model based on the file name of the image during inference. The file name of all images in the challenge used a naming convention that indicated their origin (e.g., 3D images start with 3DBox_ and 2D images start with 2DBox_ followed by the modality and case number: 3DBox_PET_0001). We confirmed with the organizers that this approach is allowed and in the spirit of the competition before focusing on it as our final meta-model to predict the foundation modality.

The final implementation of our meta-model is a tree of if-else cases based on parsing the file name and mapping a leaf in the tree to a foundation model fine-tuned on a subset of the data. As the naming conventions were not always consistent and since we believed that there might be unknown naming conventions at test time, we devised several additional naming checks and a general fallback case. The fallback case would use the provided LiteMedSAM baseline model to segment an image.

### Fine-Tuning Based on Data-Aware Subsets

To obtain a fine-tuned foundation model for each of the foundation modalities, we set up a fine-tuning pipeline including model distillation , re-using weights of pre-trained models, general fine-tuning and data-aware fine-tuning.

In detail, our fine-tuning pipeline was a three-step process. The first two steps were done once, and the last step was executed for each of the 11 foundation modalities. The pipeline is visualized in Figure 2 and consisted of the following steps:1. **Knowledge Distillation:** Distill the TinyViT  image encoder of LiteMedSAM6 to EfficientViT  and copy the weights of the prompt encoder/mask decoder. 2. **General Fine-Tuning:** Fine-tune the initial foundation model from the previous step on the entire dataset of images provided by the organizers. This step makes up for errors or forgetting during knowledge distillation and provides us with a pre-trained-like model.
3. **Data-Aware Fine-Tuning:** Further fine-tune the foundation model from general fine-tuning on a subset of the dataset based on the origin of the image.

We initialized EfficientViT-SAM  with its pretrained weights7 and leveraged the training done for LiteMedSAM by distilling the image encoder of LiteMedSAM to the image encoder of EfficientViT-SAM. Since the architecture of the prompt encoder and mask decoder were the same in both architectures, we were able to copy the corresponding weights after knowledge distillation. We used EfficientViT since we found it to be faster at inference speed than TinyViT. All available data was used for the distillation step. We also considered using MedSAM as a teacher network but decided that the distillation process would take too long.

We added the general fine-tuning, the second step in our pipeline, because we used the EfficientViT-SAM architecture as a backbone. Since the EfficientViT-SAM was not pre-trained on medical images, we first need to guarantee that our distilled model achieves similar general performance on medical images to LiteMedSAM. Thus, we also fine-tuned (or, depending on your perspective, retrained) the distilled model on the entire dataset. For general and data-aware fine-tuning runs, we froze the prompt encoder and only updated the image encoder and mask decoder. Data-aware fine-tuning directly after distillation would likely perform worse as the foundation model might not be properly adjusted to the general distribution of medical images.

During steps one and two, we only trained on a single random slice of each 3D data point in an epoch. This reduced the training time significantly and also ensured that modalities with deep 3D data points that contained many slices did not dominate the training. In the last step, we trained on all slices of 3D data points if the corresponding subset consisted of 3D data only. Thus, DAFT also enabled us to have a more efficient fine-tuning pipeline, especially reducing the time required to obtain production-ready foundation models for the foundation modalities with 2D images or only a small number of data points in their respective subset.

#### 4.2.1 Pre-Processing, Post-Processing, and Loss Function

For pre-processing, we resized and padded images to a size of \((256,256)\) and normalized the intensities. In all three steps of our pipeline, we augmented images by flippingFigure 2: Overview of our fine-tuning pipeline.

them horizontally with a probability of 50% and vertically with a probability of 50%. Following the organizer's baseline, we also randomly increased the size of the prompt's bounding boxes in all directions by up to five pixels. Additionally, for inference, we post-process our prediction by resizing the logits predicted by our model to the original size of the image using bilinear interpolation with a threshold of 0 afterward. Our loss function depended on the step of our pipeline. During knowledge distillation, we minimized the mean squared error between the embeddings predicted by both image encoders. For general fine-tuning and data-aware fine-tuning, we optimized the unweighted sum of the binary cross-entropy loss, dice loss, and intersection over union loss.

### Inference Optimization for CPU

We optimized the runtime of our model through a flood of improvements, including using a faster neural network architecture, an optimized inference runtime, caching, optimizing the docker deployment container, and inference code. All our improvements were specifically for CPU or would apply to CPU and GPU.

ArchitectureWe used EfficientViT instead of TinyViT as an image encoder, which made computing image embeddings faster, which is particularly important for 3D images, where we need to compute an image embedding for multiple slices.

Optimized Inference RuntimeWe replaced PyTorch  by using the OpenVINO8 runtime, which made inference faster and also reduced the latency of loading the execution environment before inference. OpenVINO achieves the latter by reducing the number of imports and import dependencies used at inference, avoiding loading the entire PyTorch library, which takes a considerable amount of time. Specifically, this allows us to avoid loading dependencies required only for training. For this challenge, we noticed that reducing loading the execution environment before inference is very important because the docker container will be run once per data point. Hence, if we manage to speed up the latency to the first inference, we save time on every single data point.

CachingMoreover, we used OpenVINO Model Caching9 to speed up loading our models. This increases the runtime the first time a model is loaded since the cache needs to be created, but all subsequent runs using the same model will be faster since it will be loaded from the system's cache.

Optimized Docker ContainerWe optimized the docker container by reducing its size and the number of layers to increase the efficiency of running commands with the docker container. In detail, we used python:3.11-slim instead of pytorch/pytorch:latest as a parent image to avoid loading code irrelevant for inference. Using newer Python versions, especially 3.11110, increases the speed of Python itself. Besides, using a more lightweight version of Python, we opted for the headless version of OpenCV11 to reduce the number of default packages installed. To further reduce the image size, we specifically removed caching for apt and pip while building the image. Finally, we combined multiple RUN commands and further made sure with docker-squash12, to get a container that only consisted of a single layer.

The training checkpoints used in the docker image were also converted to more optimized deployment artifacts beforehand. In detail, we converted training checkpoints of all fine-tuned foundation models to ONNX13 and only then to OpenVINO artifacts.

#### 2.2.3 Optimized Inference Code

Last but not least, we improved the inference code that was originally provided by the organizers. In this challenge, if given a 3D image, the code first sliced it into multiple 2D images before segmenting each 2D image individually. Then, it would segment each 2D image for each input prompt box _individually_ going from the midpoint of the z-dimensions outward in both directions. Thereby, it would use predictions of a prior 2D image as the prompt box for the next 2D image. As a result, the original pipeline would re-compute the image embedding for every 2D image and every prompt box. If two or more provided prompt boxes span across the same sliced 2D images (the same z-dimension of the 3D image), these shared slices would be re-computed for each such prompt box. To optimize this when predicting for 3D images, we avoid redundant computation by caching the image embeddings computed for each sliced 2D image. Thus, we guarantee to compute the image embedding at most once per sliced 2D image across all prompt boxes, i.e., segmentation tasks.

Additionally, we adjusted the original training pipeline for loading 2D or 3D images to work directly on.npz files14. Without this adjustment, we would need to convert 2D images to.npy files and extract and store the sliced 2D images as.npy files from the original 3D image.npz files.

## 3 Experimental Setup

We follow the experimental design provided by the organizers to obtain results. Additionally, we explain the process behind our development protocol and any remaining implementation details in the following.

### Model Development Evaluation Protocol

For all training runs during development, we restricted ourselves to the dataset prepared by the competition organizers and did not include any other external public datasets. Thus, we also did not include any of the allowed public datasets gathered by the community during the competition's initial phase.

During development, we evaluated the accuracy of our approach by submitting its predictions to the validation leaderboard, treating the leaderboard as our validation data to obtain validation performance. Like in traditional hyperparameter optimization, our evaluation might have overfitted to the validation data as a result of re-using a fixed validation set over the course of the challenge.

To evaluate the runtime during development in a realistic setting, we used the organizer's evaluation script15 with our docker container on a basic DigitalOcean droplet with 4 vCPUs with 8GB RAM16; simulating deployment on a laptop.

### Implementation details

Our development code is available on the _finalsubmission_ branch of our GitHub repository17. The rest of this section details the used environment settings and training protocols, concluding with the results of our training protocol: an overview of the final set of data-aware fine-tuned foundation models.

#### 3.2.1 Environment Settings

The training environment and requirements are presented in Table 1. We used this specific environment since it was available on our compute cluster, the JUWELS Hardware Booster18, which we used for training. Table 2 details the environment we used to create the final model artifacts for deployment in the docker image; we executed this conversion locally on a consumer-grade personal computer. Finally, Table 3 details the requirements used as part of our docker image.

#### 3.2.2 Training Protocols

We followed the workflow presented in Figure 1 and described in Section 2.2. Within each step, we optimized for training performance as described in Section 2.2. Finally, we selected the best-fine-tuned model per foundation modality by optimizing for validation performance across all steps of our pipeline. As a result, if general fine-tuning does not improve over knowledge distillation, we stick to the model from knowledge distillation. Likewise, if data-aware fine-tuning does not improve over general fine-tuning, we stick to the model from general fine-tuning.

The details of our training protocols are shown in the following tables: Table 4 presents the protocol for knowledge distillation; Table 5 presents the protocol

  System & Rocky Linux release 8.9 (Green Obsidian) \\  CPU & AMD EPYC Rome 7402 CPU, 2\(\) 24 cores, 2,7 GHz \\  RAM & 100GB of 512 GB DDR4, 3200 MHz \\  GPU (number and type) & Four NVIDIA A100 40G \\  CUDA version & 12.0 \\  Programming language & Python 3.11.3 \\  Deep learning framework & torch 2.1.2 \\  Specific dependencies & monai 1.3.2, numpy 1.25.1, opencv-python 4.10.0.84 \\  & Branch of efficientvit\({}^{A}\) \\   \({}^{A}\)Link to specific branch

Table 1: Training Environment and Requirements

  Python Version & 3.10.13 \\  Specific dependencies & numpy 1.24.1, openvino 2024.0.0, \\  & torch 2.2.0, onnxruntime 1.17.1 \\  & efficientvit\({}^{A}\) \\   \({}^{A}\)Link to specific branch

Table 2: Model Conversion Environment

  Parent image & python:3.11-slim \\  Specific dependencies & numpy 1.26.4, openvino 2024.0.0, \\  & opencv-python-headless 4.9.0.80 \\  

Table 3: Docker Image Requirementsused for general fine-tuning and data-aware fine-tuning (DAFT), we used the same protocol and only changed the input data for DAFT; and finally Table 6 presents the results of our DAFT-based training protocol.

In detail, Table 6 shows how we obtained the final fine-tuned foundation model per foundation modality and the respective number of training epochs. For all but X-ray, Ultrasonic, Dermoscopy, and Endoscopy, DAFT improved validation performance. For Endoscopy, not even general fine-tuning improved validation performance in the first place. Likewise, we noticed that if we use only MR or PET data, we start to overfit for MR and PET, respectively. Hence, we used a larger subset of data, merging several foundation modalities for DAFT in these two cases. We note that CT, MR, and PET are similar in the images they produce and their application, which motivated merging these specific foundation modalities. Furthermore, we found that none of the models we trained were able to beat LiteMedSAM, the baseline, on ultrasonic data. Thus, we decided to use the LiteMedSAM version provided by the organizers for ultrasonic data instead of our fine-tuned EfficientViT-SAM models.

Furthermore, for X-ray, we trained knowledge distillation and general fine-tuning only on 80% of all data for only 20 epochs and 46 epochs, respectively, due to using an older version of our code for training. We did not use the l0-checkpoint of EfficientViT-SAM during knowledge distillation of Microscopy for the same reason.

## 4 Results and Discussion

We first present the quantitative results in Section 4.1; next, the qualitative results in Section 4.2; followed by the efficiency results in Section 4.3. The quantitative and efficiency results were obtained on the validation set provided by the organizers. Finally we present the results on the final test set in Section 4.4.

 p{113.8pt}}  Pre-trained Model & EfficientViT l0, LiteMedSAM \\   Batch size & 7 \\  Patch size & 256\(\)256\(\)3 \\  Total epochs & 24 \\  Optimizer & AdamW (\(=(0.9,0.999)\), \(=10^{-8}\)) \\  Initial learning rate (lr) & \(5 10^{-5}\) \\  Lr decay schedule & ReduceLROnPlateau (\(mode=min\), \(factor=0.9\), \(patience=5\), \(coldown=5\)) \\  Training time & 13.7 hours \\  Loss function & mean squared error \\  Number of model parameters 30M \\  

Table 4: Training Protocol for Knowledge Distillation

[MISSING_PAGE_EMPTY:15]

### Quantitative Results

We present the quantitative results on validation data for the baseline (i.e., LightMedSAM19), an ablation study, and our final submission based on DAFT.

Our ablation study consists of an EfficientViT-SAM model _without_ DAFT, that is, we created one general, large foundation model for all foundation modalities by performing knowledge distillation for 24 epochs and general fine-tuning for 24 epochs on the whole dataset. For the ablation study, we used a PyTorch runtime. The ablation study provides insights across our presented results into how well our method would have been without DAFT.

Table 7 shows the results on validation data. DAFT improved the average dice similarity coefficient from 82.6% to 88.07% and the normalized surface distance from 81.61% to 89.16% compared to the baseline. Our EfficientViT+DAFT approach is specifically effective for Microscopy (65.39% to 87.14% NSD) and PET (16.07% to 56.31% NSD). Our proposed method made no improvements for ultrasonic (US) data as we used the baseline model for this data (the observed differences are noise). Yet, the ablation study shows that our EfficientViT model performs much worse for this data, which explains why we failed to improve over the baseline with DAFT for EfficientViT. For all other data modalities, we noticed that our ablation study, EfficientViT backbone, improved over the baseline. And DAFT further improves over our EfficientViT backbone.

### Qualitative Results

Figure 3 contains examples of good segmentation results on Dermoscopy, Endoscopy, and Fundus data. The corresponding DSC and NSD scores were 97.28%

   &  &  &  \\   & DSC(\%) & NSD(\%) & DSC(\%) & NSD(\%) & DSC(\%) & NSD (\%) \\  CT & 92.19 & 94.77 & 91.09 & 94.58 & 93.14 & 95.48 \\ MR & 89.13 & 92.66 & 86.98 & 91.28 & 88.21 & 91.73 \\ PET & 46.54 & 16.07 & 70.46 & 55.24 & 71.46 & 56.31 \\ US & 94.78 & 96.81 & 83.89 & 88.63 & 94.77 & 96.81 \\ X-Ray & 75.83 & 80.39 & 71.98 & 77.7 & 77.07 & 82.83 \\ Dermoscopy & 92.47 & 93.85 & 94.94 & 96.38 & 94.97 & 96.41 \\ Endoscopy & 96.04 & 98.11 & 95.24 & 97.94 & 96.60 & 98.61 \\ Fundus & 94.8 & 96.41 & 94.75 & 96.4 & 95.59 & 97.16 \\ Microscopy & 61.63 & 65.39 & 78.12 & 84.62 & 80.86 & 87.14 \\  Average & 82.6 & 81.61 & 85.27 & 86.98 & 88.07 & 89.16 \\  

Table 7: Quantitative Evaluation Results On Validation Data. The baseline is LiteMedSAM, the ablation study a knowledge-distilled and fine-tuned version of EfficientViT, and our proposed method uses DAFT in addition.

and 98.16% for Dermoscopy, 97.83% and 98.29% for Endoscopy, and 97.96% and 98.7% for Fundus. Figure 4 depicts two examples with bad segmentation results. The Mammography example had a DSC of 81.37% and a NSD of 84.58%. The whole 3D CT datapoint had scores of 76.77% and 91.63%. The bad segmentation results show that our predictions are too large and convex, which our model did not seem to expect for these data points.

Figure 3: Examples of Good Segmentation Results: The first row contains a Dermoscopy data point, the second row is an Endoscopy data point, and the last row is a Fundus data point.

### Inference Efficiency Results

Table 8 records the runtime of the baseline and our final submission for a list of example data points from the validation set. We observe the biggest relative improvements for 3D images. This likely follows from caching the computation of the image embeddings and using EfficientViT instead of TinyViT as the backbone, since the image embedding is the most expensive part of the network architecture.

We are also significantly faster on 2D data points. This is likely because our code initializes faster since we replaced the heavy PyTorch library with OpenVINO; this drastically reduced the latency to the first inference. This is more important for 2D data points than for 3D data points, as the inference time of the image encoder is the dominating factor for 3D data points. Our results for ultrasonic (US) 2D data points, where we used the baseline model, also show that our improvements do not only come from using EfficientViT but from the flood of our improvements described in Section 2.3.

Figure 4: Examples of Bad Segmentation Results: The first row contains a Mammography example, and the second row a slice of a CT example.

For the presented ablation study w.r.t. inference efficiency, we ran our proposed model but replaced OpenVINO with ONNXRuntime20. ONNXRuntime is over two times slower on all 3D data points except 3DBox_MR_0121 and also slightly slower on all 2D data points. This shows that OpenVINO with caching dominates the ONNXRuntime for this application.

### Results on final testing set

Table 9 contains our results on the final testing set. The average DSC across all modalities improved from 76.1% to 79.84% and the NSD from 78.63% to 82.35%. The runtime decreased significantly across all modalities. The average runtime improved from 22.81 to 4.01, which demonstrates a speedup of factor 5.7. When looking at the specific modalities, we observe the biggest increases in accuracy for CT and OCT. We can also observe significant improvements in both DSC and NSD for MR, PET and Microscopy data. We see a significant decrease in accuracy for X-Ray data. Our final submission achieved second place on the testing leaderboard.

  Case ID & Size & Num. Objects & Baseline21 & Ablation Study & Proposed \\ 
3DBox\_CT\_0566 & (287, 512, 512) & 6 & 814.9 & 266.2 & 113.9 \\
3DBox\_CT\_0888 & (237, 512, 512) & 6 & 219.8 & 89.6 & 38.2 \\
3DBox\_CT\_0860 & (246, 512, 512) & 1 & 40.2 & 22.6 & 10.2 \\
3DBox\_MR\_0621 & (115, 400, 400) & 6 & 389.0 & 96.6 & 45.5 \\
3DBox\_MR\_0121 & (64, 290, 320) & 6 & 247.6 & 56.1 & 42.5 \\
3DBox\_MR\_0179 & (84, 512, 512) & 1 & 38.3 & 24.2 & 11.1 \\
3DBox\_PET\_0001 & (264, 200, 200) & 1 & 30.5 & 16.8 & 7.8 \\
2DBox\_US\_0525 & (256, 256, 3) & 1 & 11.3 & 3.7 & 3.5 \\
2DBox\_X-Ray\_0053 & (320, 640, 3) & 34 & 13.1 & 5.9 & 5.0 \\
2DBox\_Dermoscopy\_0003 & (3024, 4032, 3) & 1 & 10.8 & 4.7 & 3.3 \\
2DBox\_Endoscopy\_0086 & (480, 560, 3) & 1 & 11.0 & 4.0 & 2.7 \\
2DBox\_Fundus\_0003 & (2048, 2048, 3) & 1 & 11.4 & 3.8 & 3.0 \\
2DBox\_Microscope\_0008 & (1536, 2040, 3) & 19 & 13.1 & 5.3 & 4.1 \\
2DBox\_Microscope\_0016 & (1920, 2560, 3) & 241 & 35.8 & 29.4 & 27.5 \\   Average Runtime & - & - & 134.8 & 44.9 & 22.7 \\  

Table 8: Quantitative evaluation of segmentation efficiency in terms of running time (s). We used our own evaluation on CPU, as described in Section 3.1, to obtain the running time per method.

### Limitations and Future work

The biggest limitation of our final submission is the amount of training and validation data we used in our model development protocol. The validation dataset was missing Mammography and OCT data and only had a few data points for certain modalities (e.g., ten for Fundus, or only three for 3D PET); since we used the validation scores to pick our final set of models, we are likely overfitting to the validation data. Likewise, we might overfit to our training data, as our training data was quite limited (e.g., Microscopy had only 1000 data points during training). Furthermore, due to our focus on DAFT, we did not perform large-scale re-training or fine-tuning runs across a collection of all publicly shared training datasets, which would likely have resulted in further improvements.

An interesting area for further research is automatically determining the subsets used for data-aware fine-tuning. We initially explored creating a meta-dataset by computing the meta-features of our data points. Afterward, we could compute clusters in the metadata and use the data points corresponding to a cluster as a subset for DAFT. Likewise, exploring applications of DAFT in application areas, such as tabular data, time series, or NLP, seems very promising.

## 5 Post-Challenge Performance Booster

Following the announcement of the competition results, the organizers invited participants to retrain their models on an enlarged dataset and incorporate potential improvements to try and beat their old submission. They also added data to the validation and test set.

### Changes

We incorporated early stopping into all three training steps to avoid overfitting. The provided dataset was split into 80% for training and 20% for validation for

    &  &  \\   & DSC(\%) & NSD(\%) & Runtime & DSC(\%) & NSD (\%) & Runtime \\  CT & 55.4 & 58.34 & 43.58 & 74.92 & 80.41 & 7.33 \\ MR & 64.83 & 62.84 & 18.75 & 73.5 & 69.22 & 3.58 \\ PET & 61.35 & 57.93 & 84.4 & 66.96 & 60.33 & 10.62 \\ US & 85.25 & 89.73 & 10.72 & 85.29 & 89.73 & 3.72 \\ X-Ray & 85.75 & 94.03 & 9.07 & 71.42 & 81.84 & 2.21 \\ OCT & 67.23 & 73.33 & 7.74 & 79.05 & 85.71 & 2.42 \\ Endoscopy & 94.41 & 96.95 & 6.8 & 94.08 & 96.68 & 1.91 \\ Fundus & 86.33 & 88.39 & 8.05 & 86.74 & 88.79 & 1.98 \\ Microscopy & 84.36 & 86.15 & 16.19 & 86.6 & 88.48 & 2.32 \\  Average & 76.1 & 78.63 & 22.81 & 79.84 & 82.35 & 4.01 \\   

Table 9: Quantitative Evaluation Results On final testing set. The baseline is LiteMedSAM, and our proposed method uses DAFT in addition.

[MISSING_PAGE_FAIL:21]

the average down from 14.69 to 2.25, which equals a speedup of factor 6.5. We improved the average DSC from 78.64% to 83.29% and the average NSD from 80.58% to 85.59%. The biggest change in accuracy happened for the CT modality, the DSC increased from 55.75% to 73.53% and the NSD from 58.48% to 78.4%. Our performance booster submission achieved first place on the post-challenge leaderboard.

## 6 Conclusion

In this paper, we proposed data-aware fine-tuning (DAFT), a method for enabling efficient and effective inference with foundation models, and apply it to medical image segmentation tasks as part of the "Segment Anything In Medical Images On Laptop" competition. Following concepts from meta-learning for algorithm selection and dynamic selection, DAFT aims to fine-tune several versions of a foundation model on subsets of all available data instead of fine-tuning just one larger model. Then, at inference time, we select which fine-tuned model to use for the prediction depending on the distribution of the input data. In our implementation of DAFT we use the EfficientViT architecture, knowledge distillation, and OpenVINO runtime to further improve the efficiency and effectiveness of inference.

In our experiments on the validation data provided by the competition, we show that DAFT enables us to create more effective models for each subset than when creating one model for all data. Moreover, we show that we can outperform the baseline by a wide margin. Likewise, we detail the large improvement in inference obtained by our implementation.

Our results show the potential of DAFT and optimizing foundation models for inference. Both concepts enable us to deploy efficient and effective segmentation foundation models on the laptops of medical experts.

   &  &  \\   & DSC(\%) & NSD(\%) & Runtime & DSC(\%) & NSD (\%) & Runtime \\  CT & 55.75 & 58.48 & 38.78 & 73.53 & 78.4 & 5.59 \\ MR & 64.80 & 62.75 & 18.57 & 72.84 & 70.36 & 2.81 \\ PET & 76.94 & 66.98 & 14.90 & 78.75 & 69.38 & 2.4 \\ US & 85.24 & 89.73 & 8.96 & 89.32 & 93.34 & 1.6 \\ X-Ray & 85.51 & 94.40 & 9.95 & 83.94 & 93.87 & 2.08 \\ OCT & 73.31 & 80.20 & 8.39 & 81.64 & 88.75 & 1.32 \\ Endoscopy & 94.41 & 96.95 & 7.56 & 94.24 & 96.85 & 1.25 \\ Fundus & 87.47 & 89.58 & 8.77 & 86.36 & 88.53 & 1.38 \\ Microscopy & 84.36 & 86.15 & 16.34 & 89 & 90.84 & 1.84 \\  Average & 78.64 & 80.58 & 14.69 & 83.29 & 85.59 & 2.25 \\  

Table 12: Quantitative Evaluation Results of our Booster Submission on final testing set.

#### Acknowledgments

We thank all the data owners for making the medical images publicly available and CodaLab  for hosting the challenge platform. We also thank all authors of prior work for making their weights and code public. The authors gratefully acknowledge funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under SFB 1597 (SmallData), grant number 499552394. Furthermore, the authors gratefully acknowledge the Gauss Center for Supercomputing eV (www.gauss-centre.eu) for funding this project by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS at Julich Supercomputing Center (JSC). Lastly, the authors gratefully acknowledge funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 417962828. We are additionally grateful to the organizers for setting up the challenge and for the great effort and time put into running the competition.