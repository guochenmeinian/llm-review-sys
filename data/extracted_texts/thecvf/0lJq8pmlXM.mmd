# ScribbleGen: Generative Data Augmentation Improves

Scribble-supervised Semantic Segmentation

 Jacob Schnell\({}^{1}\)

Work done during an internship at UC Merced.

\({}^{1}\)University of Waterloo

Jieke Wang\({}^{2}\)

Lu Qi\({}^{2}\)

Vincent Tao Hu\({}^{3}\)

Meng Tang\({}^{2}\)

\({}^{2}\)University of California, Merced

\({}^{3}\)CompVis Group, LMU Munich

###### Abstract

Recent advances in generative models, such as diffusion models, have made generating high-quality synthetic images widely accessible. Prior works have shown that training on synthetic images improves many perception tasks, such as image classification, object detection, and semantic segmentation. We are the first to explore generative data augmentations for scribble-supervised semantic segmentation. We propose ScribbleGen, a generative data augmentation method that leverages a ControlNet diffusion model conditioned on semantic scribbles to produce high-quality training data. However, naive implementations of generative data augmentations may inadvertently harm the performance of the downstream segmentor rather than improve it. We leverage classifier-free diffusion guidance to enforce class consistency and introduce encode ratios to trade off data diversity for data realism. Using the guidance scale and encode ratio, we can generate a spectrum of high-quality training images. We propose multiple augmentation schemes and find that these schemes significantly impact model performance, especially in the low-data regime. Our framework further reduces the gap between the performance of scribble-supervised segmentation and that of fully-supervised segmentation. We also show that our framework significantly improves segmentation performance on small datasets, even surpassing fully-supervised segmentation. The code is available at [https://github.com/mengtang-lab/scribblegen](https://github.com/mengtang-lab/scribblegen).

## 1 Introduction

With the massive leaps forward in modern deep learning, machine learning model capacity has never been higher, with some models reaching billions of parameters . However, for many tasks, the size and complexity of datasets have not kept up with the explosion in model capacity. Since machine learning models perform with a large and rich training dataset, the question of scaling datasets to match model sizes is increasingly pressing. For tasks like fully-supervised semantic segmentation (FSSS), however, this is especially expensive due to the need for dense pixel-level annotations. These annotations must often also be produced by experts with domain-specific knowledge, exacerbating the costs of data labeling even further.

Weakly-supervised semantic segmentation (WSSS) seeks to reduce the requirement for dense annotations by using weak annotations. Such methods include scribble-supervised semantic segmentation, where only a fraction of pixels along some lines (scribbles) are provided. However, these methods still lag behind fully-supervised alternatives

Figure 1: Segmentation model performance on PascalVOC and its subsets. All results other than full-mask supervision use scribble-supervision with ResNet-based RLoss  model. Naive data augmentation (i.e., fixed encode ratio \(=1.0\)) harms model performance, especially in the low-data regime, while our augmentation scheme (Adaptive \(\) Sampling) improves performance.

regarding segmentation quality, with state-of-the-art methods still achieving 2-4% lower mIoU  relative to fully-supervised models.

Another strategy is to produce synthetic training data using image-generative models. Prior works have shown that using Generative Adversarial Networks (GANs) to produce training data improves results in image classification  and semantic segmentation , among other tasks. Diffusion models , a well-known type of generative models, have demonstrated strong performance in terms of controllability  and fidelity . Several studies have successfully applied diffusion models to synthesize training data for image classification , object detection , and fully-supervised segmentation . This raises the question: _Can we also leverage the power of diffusion models to synthesize training data to further enhance the performance of scribble-supervised segmentation_?

In this work, we introduce ScribbleGen, a diffusion model conditioned on semantic scribbles to generate high-fidelity synthetic training images. Deep image-generative models such as diffusion models commonly used today, often require large datasets to produce high-quality images. This leads to a paradox where to upscale our training dataset, we need to already have access to a large training dataset. We address this problem by including a new parameter in the generative process, the encode ratio, which trades off image diversity for image photorealism.

Our contributions are summarized as follows:

* We are the first to leverage denoising diffusion models for generative data augmentation for scribble-supervised semantic segmentation. Our approach produces a spectrum of synthetic images conditioned on scribbles using different guidance scales and encode ratios.
* We provide detailed analyses and propose several schemes to combine synthetic and real data effectively for scribble-supervised semantic segmentation. We also identify the limitations of naive data augmentation schemes that can harm segmentation performance relative to not using synthetic training data at all.
* We achieve state-of-the-art results in scribble-supervised semantic segmentation, closing the gap between weakly-supervised and fully-supervised models as shown in Fig. 1. In particular, our framework significantly improves segmentation results in the low-data regime, where only a limited number of images are available.

## 2 Related work

Synthetic training dataNumerous efforts have been dedicated to leveraging synthetic data for training perception models. IT-GAN  shows that GAN-generated samples can help classification models learn faster and improve performance. DatasetGAN , BigDatasetGAN , and HandsOff  employ GANs  for jointly generating synthetic images and their corresponding labels for segmentation tasks.

Recent advances in diffusion models have brought notable stability during training, robust synthesis capabilities , and enhanced controllability . As a result, there has been a significant shift towards the use of diffusion models for data synthesis, including for image classification , object detection , instance segmentation , and semantic segmentation . For example, by fine-tuning an Imagen  model on ImageNet ,  generates synthetic images from text prompts to improve the performance of image classification. Similarly, D3S  introduces a novel synthetic dataset specially designed to mitigate the foreground and background biases prevalent in real images.  jointly generate synthetic images and associated mask annotation, akin to DatasetGAN, using a StableDiffusion  image-generative model. GroundedDiffusion  further generates the triplet of image, mask, and texts to adapt the pre-trained diffusion model for open-vocabulary segmentation. FreeMask  utilizes FreestyleNet  to synthesize images conditioned on full mask annotations.

Our work diverges from these initiatives by focusing on sparse labels (e.g., scribbles) from real images as generative conditions, encouraging the creation of realistic and diverse synthetic images. While FreeMask  similarly conditions synthetic images on real data annotations, our method uses sparse rather than dense annotations, allowing for broader applications where dense labeling is expensive.

Guidance in Diffusion modelsDiffusion models excel in various tasks due to their controllability . They're used to generate image content , image layout , audio content , human motion , etc. Guidance signals can also be incorporated to enhance image fidelity  relative to unconditional generation. It has been shown that diffusion models can be guided by pretraining a noisy-data-based classifier, known as Classifier-guidance . On the other hand, classifier-free guidance  removes the need for extra pretraining by randomly dropping out the guidance signal during training. We develop a framework that utilizes classifier-free guidance for generative data augmentation to improve scribble-based segmentation.

Weakly-supervised segmentationWeakly-supervised segmentation methods use weak annotations rather than full segmentation masks to train segmentation networks for images  or point clouds . Forms of weak annotations include points , scribbles , bounding boxes , image-level tags , and text . These methods can be roughly categorized into two groups. The first group proposes various unsupervised or semi-supervised losses such as entropy loss , CRF loss , and contrastive-learning losses . The second group iteratively refines full-mask pseudo-labels  during training to mimic full supervision. Many weakly-supervised approaches rely on class activation maps (CAMs)  that gives localization cues from classification networks. Our generative data augmentation approach complements any existing weakly-supervised segmentation methods, as we show improved performance of several methods with our synthetic data.

Weak annotations can also be provided as input for segmentation networks at test time for interactive segmentation . For example, Segment Anything  allows prompts including clicks, bounding boxes, masks, or text. While Segment Anything  provides many masks in a semi-automatic way for training interactive segmentation, we focus on synthetic image synthesis for training weakly-supervised segmentation.

## 3 Method

In this section, we describe our method of generative data augmentation for weakly-supervised semantic segmentation outlined in Fig. 2. First, in Sec. 3.1, we provide a background on sampling from diffusion models. Then, in Sec. 3.2, we introduce a variant of ControlNet  conditioned on scribble labels and text prompts. We further discuss how to achieve semantically consistent images and trade off diversity and photorealism through guided diffusion and encode ratio in Sec. 3.3 and Sec. 3.4, respectively. Sec. 3.5 proposes several schemes to effectively combine synthetic and real images for training segmentation networks.

### Background

Diffusion models.Diffusion models  learn to reverse a forward process that gradually adds noise to an image \(_{}\) until the original signal is fully diminished. After training, following the reverse process allows us to sample an image \(_{0}\) given noise \(_{T}(0,I)\). Learning this reverse process reduces to learning a denoiser \(_{}\) that recovers the original image from a noisy image \(_{t}\) as

\[_{} f_{}(_{t},t):=(_{t }-(1-_{t})_{}(_{t},t))/ _{t}}. \]

To get high-quality samples, the standard diffusion model sampling process  requires many (often \(T=1,000\)) neural function evaluations. Using a non-Markovian forward process, Denoising Diffusion Implicit Model (DDIM)  samplers forego several intermediate function calls, accelerating sampling. Let \(\) be an increasing subsequence of \([T,,1]\) and define the DDIM forward process for some stochasticity parameter \(_{ 0}^{T}\) as

\[q_{}(_{_{i-1}}|_{_{i}},_{0})=}}_{0}\\ +}-_{_{i}}^{2}}_{_{i}}-}}_{0}}{}}},\ _{_{i}}^{2}I. \]

We can then sample from the generative process using the abovementioned forward process. In particular, using \(f_{}(x_{t},t)\) as defined in Eq. (1) we can sample \(_{_{i-1}}\) from \(_{_{i}}\) by

\[p_{}^{(_{i})}(_{_{i-1}}|_{_{i}})= (f_{}(_{_{i}},_{i}),\ _{_{i}}^{2}I)&i=1\\ q_{}(_{_{i-1}}|_{_{i}},f_{}( _{_{i}},_{i}))&i>1 \]

We slightly abuse notation here and define \(_{0}=0\) so that when \(i=1\), we sample the denoised image \(_{0}\).

Classifier-free guidance.To trade off mode coverage and sample fidelity in a conditional diffusion model,  proposes to guide the image generation process using the gradients of a classifier, with the additional cost of training the classifier on noisy images. To address this drawback, classifier-free guidance  does not require any classifier. They obtain a conditional and unconditional network combination in a single model by randomly dropping the guidance signal \(\) during training. After training, it empowers the model with progressive control over the degree of alignment between the guidance signal and the sample by varying the guidance scale \(w\) when a larger \(w\) leads to greater alignment with the guidance signal:

\[}_{}(_{t},t;\ ,w)=(1+w)_{}(_{t},t;\ )-w_{}(_{t},t). \]

### Scribble-conditioned Image Synthesis

We consider a semantic synthesis approach to generating our synthetic training data. The synthetic training data is generated conditioned on real segmentation labels from the training dataset. We leverage a typical denoising diffusion model, ControlNet , to achieve image synthesis conditioned on the segmentation scribbles. Our model is trained using the usual DDPM  object: given a noisy image \(_{t}\) (in reality \(_{t}\) is a latent representation as in , but we omit this detail for brevity) and conditioning input \(\) it predicts the added noise \(\). Our segmentation scribbles on which the model is conditioned are represented as RGB images in \(^{h w 3}\) with different colors for every class, though we explore other representations in Sec. 4.3.

Finally, we note that it is difficult for the ControlNet model to produce semantically consistent images with the given scribble labels. We hypothesize that this is due to the difficulty of encoding class information in RGB images, especially in the early stages of training. Therefore, we supplement our model with text prompts that include all the classes within the image. Adding these prompts significantly improves image class consistency and leads to higher-quality images relative to an unchanging default prompt. We explore the effect of this prompt in Sec. 4.3.

Our ControlNet training objective is thus

\[_{}()=_{(_{}, _{s},_{t}),t,}[\|-_{}(_{t},t,_{s},_{t})\|_{2}^{2} ], \]

where \((_{},_{s},_{t})\) is the triplet of the original (unnoised) image, the conditioning scribble label, and the conditioning text prompt and \(_{}\) is our ControlNet diffusion model.

### Classifier-free Scribble Guidance

We leverage diffusion guidance to further improve semantic consistency between the generated synthetic image and conditional input. Following the proposals from Classifier-free Guided Diffusion , we randomly drop out 10% of all conditioning scribble inputs \(_{s}\), replacing them with a randomly initialized, learned embedding \(}\), when training the ControlNet model. By modifying Eq. 4, we arrive at a new guided noise prediction function:

\[}_{}(_{t},t;_{s},_{t},w)=(1+w)_{}(_{t},t;_{s},_{t })-w_{}(_{t},t;}). \]

While ControlNet uses a pre-trained Stable-Diffusion model , which is trained conditionally and unconditionally, scribble drop-out during training can be viewed as fine-tuning the unconditional diffusion model to our dataset. We have found that the guidance scale, \(w\), can significantly impact the quality of generated images, especially with respect to the fine-grain details of the produced image. We further ablate this hyperparameter's impact in Sec 4.3.

### Control Image Diversity via Encode Ratio

The vanilla diffusion model denoises sampled Gaussian noise \(_{T}(0,I)\) iteratively until \(_{0}\) at inference time. In practice, synthetic images generated this way may be unrealistic, particularly when training data is limited for our scribble-conditioned diffusion model. To improve photorealism at the cost of diversity, we propose another forward diffusion process parameter, the encode ratio \((0,1]\). Specifically, we perform \( T\) noise-adding forward diffusion steps to the input images and, during inference, denoise \(_{ T}\) iteratively until \(_{0}\). Thus, for \(=1\), there is no change, but for small choices of \(\), there is less noise added to the image \(_{0}\). As \( 0\), the sampled image will become increasingly similar to the original \(_{}\). Therefore, a whole spectrum of synthetic images with varying levels of similarity to the reference image can be achieved by varying our choice of \(\). We outline our sampling algorithm, which combines the accelerated DDIM sampling from Sec. 3.1, the scribble guidance from Sec. 3.3, and the encode ratio from Sec 3.4 in Algorithm 1. Fig. 3 shows synthetic images generated with varying guidance scales and encode ratios.

### Combine synthetic images with real images

Generative data augmentation can, in principle, produce an infinite amount of synthetic images. However, naively combining real and synthetic images can harm rather than benefit weakly-supervised segmentation models, as we have observed. In particular, it is not clear which choices of the guidance scale \(w\) and encode ratio \(\) are optimal. We choose the optimal guidance scale, as determined in Sec. 4.3. For encode ratio \(\), we propose and systematically evaluate two strategies for combining synthetic with real images.

Let \(=\{_{1},,_{n}\}\) denote the set of all real im

Figure 2: Given a limited number of real scribbles, we pretrain a ControlNet-based diffusion model for high-fidelity synthesis of images conditioned on scribbles. We can control the image synthesis with the encode ratio \(\) and the guidance scale \(w\). These image-scribble pairs can then be smoothly integrated into the training of scribble-based semantic segmentation.

ages and \(=\{_{1},,_{n}\}\) denote the set of all (scribble) labels. Then we produce a set of synthetic images \(}=\{}_{1},,}_{n}\}\) where \(}_{i}=_{}(_{i},_{i};w,)\) is the output of our trained diffusion model, \(_{}\), conditioned on the scribble \(_{i}\) and prompt-condition \(_{i}\), given guidance scale \(w\) and encode ratio \(\). We may then produce a new, augmented dataset \(^{}=(,})\) and \(^{}=(,)\). Note this means each label, \(_{i}\), appears twice in our dataset, once for the real image \(_{i}\) and once for the synthetic image \(}_{i}\).
* **Fixed encode ratio \(\)**: We choose a fixed encode ratio which gives a fixed synthetic dataset \(}\). Using the default value of \(=1\) yields the most diverse synthetic images with possibly inferior image fidelity. We find the optimal \(\) that gives the best segmentation in our experiments.
* **Adaptive encode ratio \(\)**: To avoid hyper-parameter search, we also propose an adaptive scheme for choosing \(\). We gradually increase the encode ratio \(\) while training downstream segmentation networks, similar to curriculum learning. Initially, synthetic images used for training are similar to real images, which are considered an easier curriculum to learn. Synthetic images diverge increasingly from the real images as training progresses. For this case, the synthetic dataset is formed at epoch \(e\) as \(}=\{}_{1,_{e}},,}_{1,_{e}}\}\) where we follow the encode ratio schedule \([_{1},,_{E}]^{E}\) where \(E\) is the number of training epochs.

## 4 Experiments

Sec. 4.1 summarize our main results that show improvements on several scribble-supervised segmentation methods using our generative data augmentation. In Sec. 4.2, we further explore the challenging scenario with limited number of real images. We show that naive implementations of generative data augmentation may harm the performance, whereas our data augmentation scheme improves. Sec. 4.3 gives an ablation study on guidance scale and encode ratio, two critical degrees of freedom for our image synthesis.

Dataset and Implementation DetailsWe report results on the standard PASCAL VOC12 segmentation dataset

Figure 3: Left: Our sampled synthetic images conditioned on the ground-truth scribble. By sampling using different guidance scales and encode ratios we are able to generate a whole spectrum of realistic synthetic training images. Right: The ground-truth real image and corresponding scribble label.

which contains 10 582 images for training and 1 449 images for validation. We utilize scribbles from ScribbleSup dataset  with only 3% pixels labeled on average.

For image synthesis, we use a latent diffusion model  with a downsampling rate of \(f=8\), so that an input image of size \(512 512\) is downsampled to \(64 64\). We use Stable Diffusion 1.5 as the backbone for ControlNet  and finetune ControlNet for 200 epochs with a batch size of 16 using two A100 80GB GPUs. We set \(T=1000\) discrete timesteps for ControlNet and use a linear learning rate scheduler from an initial rate of \(10^{-4}\) during training. For scribble conditioning, we randomly dropout 10% of scribbles, replacing them with a learned embedding of the same size. Scribble labels are represented as RBG images in \(\{1,,255\}^{512 512 3}\). We also provide the text prompt "a high-quality, detailed, and professional image of [list of classes]" as suggested in . We provide visualizations of our synthetic dataset in the supplementary material.

**Evaluation metric.** We evaluate both the diversity and fidelity of the generated images by the Frechet Inception Distance (FID) , as it is the _de facto_ metric for the evaluation of generative methods, e.g., . It provides a symmetric measure of the distance between two distributions in the feature space of Inception-V3 . We use FID as our primary metric for the sampling quality. We realize, however, that FID should not be the only metric for evaluating the downstream impact of synthetic data for training segmentation networks. Hence, we also report segmentation results trained with synthetic data only to evaluate synthetic data, similar to the Classification Accuracy Score (CAS) proposed by  but for semantic segmentation. We report the standard mean Intersection Over Union (mIOU) metric for segmentation results.

### Generative data augmentation improves scribble-supervised semantic segmentation

For our experiments, we consider two methods of weakly-supervised semantic segmentation, including simple regularized losses (RLoss)  and the current state-of-the-art in scribble-supervised segmentation, Adaptive Gaussian Mixture Models (AGMM) . For both methods, we jointly train them on the original training set and our augmented training set. Both methods also follow a polynomial learning rate scheduler. The sampling of synthetic training images is outlined in Sec. 3.5. Table 1 shows improved results using generative data augmentation for both RLoss and AGMM. Our method with synthetic data further reduces the gap between weakly-supervised and fully-supervised segmentation. We show visualizations of our segmentation results with and without using our generative data augmentation in Fig. 6. We also include further visualizations in the supplementary material.

### Low-data Regime Results

For the low-data regime, we only consider the RLoss method due to its simplicity and speed to train. We consider three different reduced datasets with 50%, 25%, and 12.5% of all training images used, respectively. For each of these cases, we train a ControlNet diffusion model on the limited dataset (following the same experimental setup described at the start of Sec. 4) and sample synthetic images as usual. The results of training RLoss on each of the reduced datasets for our different proposed augmentation schemes are reported in Fig. 1.

We notice that the naive data augmentation fails to help in all of our reduced datasets and instead reduces model performance in all but the 50% case. Conversely, our proposed _Adaptive \(\) Sampling_ improves or matches performance for all four datasets. We hypothesize this is due to the lack of training images required to ensure high-quality generation from our diffusion model. This hypothesis is confirmed by the significantly higher FID scores for synthetic datasets generated with limited training data reported in Fig. 5 middle. We also confirm this hypothesis qualitatively in Fig. 4, where we observe that fully synthetic images deteriorate in quality as the number of training images decreases. How

[MISSING_PAGE_FAIL:7]

synthetic-only case, the quality of the synthetic images is more important, so decreasing the encode ratio to improve data realism matters more than data diversity. We include further visualizations of the impact of the encode ratio on image synthesis in our supplementary material.

Conditioning InputWe also ablate modifying the conditioning input to ControlNet. We try representing scribble labels as one-hot embeddings in \(\{0,1\}^{h w C}\) where there are \(C\) total classes. Using these one-hot embeddings, we obtained a higher FID by 4.4 points relative to RGB embeddings, but we found no improvement in mIoU results using our Fixed \(\) augmentation scheme. We also try using text prompts that don't include the classes in the image. Using unchanging prompts (i.e., "a high-quality, detailed, and professional image") yields lower FID by 3.1 points relative to prompts that include the classes in the image and 1.9% lower mIoU using our Fixed \(\) augmentation scheme.

## 5 Conclusion and Future Work

We propose leveraging diffusion models conditioned on scribbles to produce high-quality synthetic training data for scribble-supervised semantic segmentation. We advocate the use of classifier-free guided diffusion and introduce the encode ratio to control the generative process, allowing us to generate a spectrum of images. We report state-of-the-art performance on scribble-supervised semantic segmentation with our generative data augmentation.

In the future, it will be interesting to train generative models for open-vocabulary image synthesis conditioned on sparse annotations. Our generative data augmentation has the potential to improve semi-supervised segmentation. We are also interested in end-to-end training of generative data augmentation and perception models, as metrics like FID are loosely related to perception performances.

Figure 5: Left: The FID of our full training dataset when generated with different classifier-free guidance scales. Results are reported for ControlNet trained all 10582 images. Middle: The FID of our training dataset when generated with different encode ratios. Results are reported for four ControlNet models trained on a different number of images. Right: The mIoU of a downstream segmentation model when trained on datasets of varying encode ratios. Note \(=0.0\) corresponds to training on real images only. Results are reported for training on both naive data augmentation and only on synthetic images. In both cases, we use all 10582 images for training.

Figure 6: Qualitative results on PASCAL dataset. Our generative data augmentation method improves scribble-supervised semantic segmentation methods such as AGMM .

AcknowledgementThe authors would like to thank Prof. Yuri Boykov, Prof. Olga Veksler, and Prof. Ming-Hsuan Yang for their helpful discussion and comments that improved the quality of this work.