# A Cross-Dataset Study for Text-based 3D Human Motion Retrieval

Anonymous CVPR submission

Paper ID 13

###### Abstract

We provide results of our study on text-based 3D human motion retrieval and particularly focus on cross-dataset generalization. Due to practical reasons such as dataset-specific human body representations, existing works typically benchmark by training and testing on partitions from the same dataset. Here, we employ a unified SMPL body format for all datasets, which allows us to perform training on one dataset, testing on the other, as well as training on a combination of datasets. Our results suggest that there exist dataset biases in standard text-motion benchmarks such as HumanML3D, KIT Motion-Language, and BABEL. We show that text augmentations help close the domain gap to some extent, but the gap remains. We further provide the first zero-shot action recognition results on BABEL, without using categorical action labels during training, opening up a new avenue for future research.

## 1 Introduction

Dataset bias is a known phenomenon in machine learning research. The pioneering work of Torralba and Efros  shows that given a sample from an object recognition dataset, both a human researcher and a computer (SVM classifier) can guess which dataset the image comes from, known as the 'Name That Dataset' task. In a similar spirit, we observe that 3D human motion description datasets typically have a language style that distinguishes them from each other. KIT Motion-Language (KITML)  is dominated by locomotive motions and often starts by 'A person is...'. HumanML3D  similarly contains such full-sentence descriptions, but tends to be more verbose, and covers a larger vocabulary of motions. BABEL  language style is distinct, concisely describing with a single verb such as'sit'. The t-SNE  visualization in Figure 1 confirms this observation, where we plot MPNet  text embeddings of random subset of 400 labels from each dataset. BABEL textual labels appear clearly distinct from HumanML3D and KITML. In this work, we perform cross-dataset evaluations to quantify these gaps, and attempt reducing them via text augmentations.

We instantiate our study with the text-to-motion retrieval task. While there is a large literature on text-to-motion synthesis , text-to-motion retrieval is relatively new . TMR  employs a contrastive training, similar to CLIP , to learn a cross-modal embedding space. In this work, we train TMR models and show several improvements. Similar to ActionGPT  which improves text-to-motion synthesis with text augmentations, we leverage large language models (LLMs) to increase robustness of retrieval models via label augmentations such as paraphrasing (see Table 1). Furthermore, we study the ability of a model trained with free-form text labels to generalize to the zero-shot1 action recognition task, by performing motion-to-text retrieval.

Our contributions are the following: (i) We report

Figure 1: **3D human motion descriptions per dataset:** The t-SNE plot of text embeddings corresponding to motion descriptions clearly shows a domain gap between the concise raw labels of the BABEL dataset and the full-sentence labels of HumanML3D and KITML datasets.

cross-dataset retrieval performance using TMR on a unified SMPL  representation, and assess the effect of training on a combination of datasets, leveraging HumanML3D, KITML and BABEL. (ii) We perform data augmentation on the textual labels and show that training TMR with these augmented data improves the results. (iii) We perform zero-shot action recognition on the BABEL-60, BABEL-120 benchmarks by training only on HumanML3D, and provide several ablations, again confirming the improvements from text augmentations.

## 2 Related Work

We briefly describe few works on (i) 3D human motions and language, with a particular emphasis on datasets in this domain, and (ii) zero-shot classification with natural language supervision in other domains of computer vision. For a broader overview, we refer to the survey of .

3D human motions and language.Following advances in natural language processing, there has been an increased interest in building models to control 3D human motion generation with language inputs , and more recently on text-based motion search . The performance of these models naturally depend on the datasets they are trained on. KITML  is one of the first 3D human motion description datasets, collecting annotations for a relatively small amount of motions, with a relatively small vocabulary of words, thus limiting its generalization to out-of-distribution samples. More recently, two concurrent works HumanML3D  and BABEL  collected manual labels for the large AMASS  motion collection. Since these efforts were in parallel, the resulting annotations differ in style, incurring a domain gap. As mentioned in Section 1, HumanML3D follows KITML-style verbose full sentence descriptions, while BABEL introduces concise labels, typically with verbs in an imperative form (e.g., 'wave hands' vs 'A person is waving hands'). In this work, we focus on a cross-dataset study investigating generalization performance of text-to-motion retrieval models, instantiated by the recent method of TMR .

In a similar spirit to our work, Action-GPT  investigates text augmentations using LLMs for improving robustness. However, their study is on a single dataset, BABEL, with only qualitative results on unseen text descriptions. Here, we provide quantitative cross-dataset results, showing improvements on the zero-shot setting with text augmentations.

Zero-shot classification with natural language supervision.CLIP  image-text retrieval model is a popular example of training contrastively with free-form language labels and successfully applying on categorical labels for zero-shot classification on various downstream datasets. CLIP observes a small performance gain by appending the string 'a photo of' to the class labels, simply to reduce the domain gap between training and test time. Similar multimodal contrastive models were built by ActionCLIP  for video action recognition, using additional prompts such as 'human action' of. In 3D human motions domain, MotionCLIP  leverages CLIP image-text joint space by turning 3D motions into rendered images. Similar to this work, MotionCLIP  reports results on BABEL action recognition benchmarks by posing the problem as motion-to-text retrieval; however, they work with the fully-supervised setting, where they use training labels of BABEL, adapting to the textual domain of action classes. In contrast, our target is the zero-shot setting, where the set of labels are unknown.

   Original label & Paraphrases & Action \\  A person & -Someone trips and falls but continues moving forward by getting up and running. & \\ stumbles on the ground but & -An individual experiences a misstep while running but continues onward. & Trip and run. \\ gets up and keeps on running. & -A person stumbles while running but gets back up and continues to move forward. & \\  A person knees on the floor. & -A person is crouching or squatting on the ground. & \\ A person knees on the floor. & -Someone is bending their knees to lower themselves to the ground. & Kneel. \\  & -The individual is kneeling on the ground. & \\  Punch. & -A person clenches their fist and strikes something with the closed hand, using & \\  & the arm and shoulder muscles for force. & N/A \\  & -A person extends their arm and fist in a punching motion. & \\  & -A person thrusts one fist forward then pulls it back. & \\   

Table 1: **Example LLM paraphrasing: We prompt Llama-2 as described in Section 3 in order to augment the original motion descriptions on the left. Middle column shows results of instructing the LLM to paraphrase. The right column is the result of instructing to convert into the style of action labels. The three example input labels are taken from HumanML3D, KITML, and BABEL datasets from top to bottom.**

## 3 Methodology

We build on the recent method of TMR , and make several improvements: mainly the use of text augmentations and using a hard-negative variant (HN-NCE ) of the InfoNCE  loss. We also train on a combination of datasets (instead of a single dataset) using motion representation of Guo et al.  computed on the SMPL  body skeletons (instead of dataset-specific skeletons). When training jointly on multiple datasets, we simply append training sets and sample disproportional to training set size to balance the distributions. In the following, we detail our text augmentation procedure.

We perform text augmentation by paraphrasing each motion text label several times. First, given a motion, for each of its text annotations, we use Llama-2  to generate paraphrases of this text. We prompt Llama-2 by instructing to paraphase a given motion description with the paraphrasing style defined by few-shot examples that we provide in the form of text pairs. This procedure applies to HumanML3D and KITML sentences. When paraphrasing concise BABEL text annotations, we alter the prompt by instructing to describe the motion, and providing few-shot examples in the form of _"Sentence: Point." Paraphrased:" A person motions forward with their hand."_.

For HumanML3D and KITML, that are annotated with full sentences, we additionally generate action-style annotations. For example, an action-style annotation for _"The person sprits down the track, their feet pointing against the ground"_ is _"Sprint"_. We refer to Table 1 for more text augmentation examples.

We have two sources for providing few-shot examples in the prompts. First, we generate example pairs using GPT-3.5 . Second, we leverage the multiple annotations corresponding to the same motion segment (either within or across datasets), and assume that such annotations may be paraphrases of each other.

As a final augmentation strategy, we sample uniformly at random, among a set including all the annotations (ground truth and its augmentations). We then encode all the texts in this set and average their associated text embeddings.

During training, for each motion in a batch, we sample with probability \(p_{gt}\), one of the ground truth annotations (in case of multiple manual labels); with probability \(p_{par}\), one of the paraphrased versions; with probability \(p_{act}\), the action-style annotation version; and with probability \(p_{avg}\), the averaged text embedding as described above. In our experiments, these are set as \(p_{gt}\!=\!0.4\), \(p_{par}\!=\!0.2\), \(p_{act}\!=\!0.1\) and \(p_{avg}\!=\!0.3\), unless stated otherwise. We illustrate this procedure in Figure 2.

## 4 Experiments

We first describe the datasets (Section 4.1) and evaluation metrics (Section 4.2) used in our experiments. Next, we report the main results on text-to-motion retrieval (Section 4.3) and zero-shot action recognition (Section 4.4). We then provide ablations on text augmentations (Section 4.5) and conclude with qualitative analyses (Section 4.6).

### Datasets

We experiment with **HumanML3D** and **KITML** standard text-motion datasets. We also benchmark this task on **BABEL** raw textual labels, and report on its action recognition benchmarks, BABEL-60 and BABEL-120 for 60 and 120 action labels, respectively. The source of these captioned motions largely overlap with the AMASS  collection that unifies motions from multiple MoCap sources into SMPL body format . We therefore simply extract motion representation from Guo et al.  on SMPL skeletons for each of these datasets, alleviating the issue of dataset-specific skeleton definitions, e.g., for KITML .

HumanML3D includes 23384, 1460 and 4384 motions for the training, validation and testing sets, respectively. The original KITML dataset includes 6018 motions processed using the Master Motor Map (MMM) framework, split into sets of

Figure 2: **Model overview:** We simply employ TMR  for text-motion retrieval, but unify several text augmentation approaches to increase its robustness across domains. For each ground truth (GT) textual label, we generate \(n\) paraphrased versions, as well as a short action-style description using Llama-2 prompting. During training, we randomly sample either of these augmented labels with probabilities defined by \(p_{gt}\),\(p_{par}\),\(p_{avg}\),\(p_{act}\). With probability \(p_{avg}\), we also randomly subsample paraphrased versions and average their text embeddings. The selected text embedding \(z^{T}\) is then matched to the motion embedding \(z^{M}\) using contrastive loss. Note that we do not visualize the motion decoder for simplicity, but we keep the original architecture as in .

[MISSING_PAGE_FAIL:4]

the evaluation of BABEL motion retrieval (i.e., large fluctuation when repeating the same experiment), we provide average over three repeated runs with different random seeds and report the standard deviation. Given the high variance on BABEL, we refrain from making conclusions on this new benchmark, but find its action retrieval evaluation to be more stable (see Section 4.4).

Combining datasets.Jointly training on HumanML3D and KITML (H+K) outperforms training only with one or the other when testing on the small-vocabulary KITML dataset. This does not impact performance on the larger HumanML3D. Adding BABEL to training does not bring a consistent boost, and mainly helps the same-domain BABEL evaluation.

Text augmentation.Text augmentations bring an overall improvement, especially significant on HumanML3D (14.47 vs 11.68 R@1). On the other hand, the impact on BABEL is inconclusive due to large variance in the BABEL retrieval benchmark. As will be seen in Section 4.4, the BABEL action recognition benchmark highly benefits from text augmentations. For more details on text augmentation parameters, we refer to Section 4.5.

HN-NCE.When replacing the InfoNCE loss with HN-NCE , we observe the best performance for H+K joint training when tested on HumanML3D and KITML. The best results on BABEL are also with HN-NCE, but when training on H+K+B.

To the best of our knowledge, these results represent state-of-the-art performance, with 3% improvement on HumanML3D over TMR  (11.63 vs 14.89), and with 7% improvement on KITML (21.75 vs 29.39).

### Zero-shot action recognition results

We study the ability of a model trained on text labels, here HumanML3D, to generalize to categorical action labels, when evaluating on BABEL action recognition through motion-to-text retrieval. Following the original work describing the dataset and the action recognition benchmark , we report Top-1 and Top-5 accuracy metrics (equivalent to R@1 and R@5), as well as Top-1 class-balanced version (Top-1 CB). Results are summarized in Table 3. In the first block, we list the previous works reporting on this benchmark , using the BABEL action labels for training (B-actions). We first check that TMR reaches 282 their performance on this fully-supervised setting. We then provide intermediate results by using BABEL motions, but their free-form textual labels, instead of the categorical action labels. Both 'raw' and 'proc' (processed) labels provided by this dataset match the performance of action labels (perhaps due to action labels being derived from those). In the last block, we report the zero-shot setting by training on HumanML3D texts. Here, we observe significant improvements via text augmentations (e.g., 290, 22.44 vs 26.30). We also ablate our average embedding strategy described in Section 3 (\(p_{gt}=0.4\), \(p_{par}=0.3\), \(p_{act}=0.3\), \(p_{avg}=0\)) and see its benefits (last two rows).

### Text augmentation ablations

We first study the impact of the choice of probabilities used in our augmentation strategy, \(p_{par}\), \(p_{sum}\) and \(p_{act}\). Next, we compare our text augmentation approach to the one of Action-GPT , the method we find to be most related to ours. We conduct these ablations by training on the combination of HumanML3D + KITML training, and by evaluating on HumanML3D.

Augmentation probabilities.Table 4 studies the impact of the probability used for picking the augmentation approach when sampling the text label, among which are picking the ground truth (\(p_{gt}\)), picking one paraphrase (\(p_{par}\)), picking the action-type label (\(p_{act}\)), and picking the average of a random

    &  &  \\  & Training data & Augm & Top-1 CB & Top-1 & Top-5 & Top-1 CB & Top-1 & Top-5 \\ 
2s-AGCN  CE & B-actions & ✗ & 24.46 & 41.14 & 73.18 & 17.56 & 38.41 & 70.49 \\
2s-AGCN  Focal & B-actions & ✗ & 30.42 & 33.41 & 67.83 & 26.17 & 27.91 & 57.96 \\ MotionCLIP  & B-actions & ✗ & - & 40.90 & 57.71 & - & - & - \\  TMR & B-actions & ✗ & 25.14 & 40.21 & 62.99 & 20.61 & 37.27 & 55.93 \\ TMR & B-text (raw) & ✗ & 25.36 & 37.93 & 54.14 & 20.88 & 34.03 & 47.95 \\ TMR & B-text (proc) & ✗ & 24.73 & 40.91 & 56.63 & 20.88 & 38.15 & 50.93 \\   TMR & H-text & ✗ & 22.44 & 27.10 & 53.73 & 16.23 & 23.66 & 44.67 \\ TMR & H-text & ✓w/o avg & 25.02 & 33.46 & 62.75 & 20.10 & 29.59 & 55.11 \\ TMR & H-text & ✓ & **26.30** & **36.08** & **64.18** & **22.20** & **32.46** & **56.32** \\   

Table 3: **Motion-to-text retrieval for action recognition:** Best results on BABEL action recognition in the _zero-shot_ setting (last 3 rows) are obtained when training on HumanML3D (H-text) with all the text augmentations. We also provide results with the fully-supervised setting using action labels (B-actions). Benchmarking TMR  on this task obtains comparable performance to the state of the art. Finally, we report the intermediate setting of using raw or processed (proc) BABEL textual labels (B-text), from which action labels are inferred.

[MISSING_PAGE_FAIL:6]

Figure 4: **Qualitative results on BABEL action recognition:** We apply zero-shot action classification via motion-to-text retrieval by treating class labels as text. The model is trained on HumanML3D free-form textual labels, and tested on BABEL actions. On the right of each input motion example, we display the ground truth (GT) action, along with the top-5 retrieved actions and their motion-text similarity scores. We observe that the high similarities among the top retrieved actions are mainly due to ambiguities across categories, e.g., “Grasp object” motion retrieves action classes involving hand motions such as “Touch object” and “Hand movements”.

Figure 3: **Qualitative results on HumanML3D text-to-motion retrieval with and without augmentation:** In both examples, while none of the retrieved motions are extremely remote from the text description, the model trained with augmentation captures more of the requested details for most motions in the top 5 ranks. In the example above, the model captures the interaction between elbow and knee, while the baseline model only captures the implication of the legs. In the below example, the model retrieves both parts of the movement – putting the box down and running – while the baseline only retrieves the running portion.

when training with the HumanML3D dataset. We observe that many more classes show a significant improvement than a loss of performance. For example, the rare classes in BABEL such as 'crossing limbs', 'wave', and 'knee movement' are substantially improved, as well as the frequent'stand' category. Figure 6 further shows the most frequent confusion between categories, which demonstrates the finegrained nature of this benchmark. This allows to ponder the importance of some of the classification mistakes, by looking at the category an action is most confused with. As already outlined with Figure 4, some actions tend to be mostly mistaken for an action with similar meaning. For instance, the action 'jog', is mostly confused with 'run', which mitigates the fact that the performance of our model drops significantly on 'jog'. We also point in the confusion matrix a wide area corresponding to actions all related to hand-object interaction.

## 5 Conclusion and Limitations

We presented our work analyzing the generalization performance of text-motion retrieval models. Specifically, we perform cross-dataset experiments using standard benchmarks. Our results suggest that significant gains are observed when applying text augmentations to overcome the domain gap across datasets. Moreover, we benchmarked the popular TMR model on BABEL action recognition evaluation, and obtained promising zero-shot performance by only training on HumanML3D dataset. One potential limitation of our approach is the text augmentation which is not necessarily grounded in the motion. That is, the LLM can hallucinate details which are not visible in the motion. Future work can explore motion captioning as a way to incorporate grounded augmentations. Another avenue for future research is to expand this analysis to investigate the domain gap across motions, and not only across textual labels.

Figure 5: **Per-action performance improvement:** We plot the per-action R@1 scores for the 60 BABEL actions, comparing with/without the text augmentations. The dashed line represents the frequency of test labels for each class (y-axis on the right), showing the unbalanced nature of this benchmark.

Figure 6: **Action classification confusion matrix portions:** We visualize several sources of classification mistakes, easily explained by the presence of ambiguous or related action labels. On the left, we display the full 60-categories of BABEL-60, and zoom into interesting regions on the right, highlighting the most confused actions in red. For example, the bottom row shows that hand-object interaction categories are confused frequently.