# RLNet: Adaptive Fusion of 4D Radar and Lidar for 3D Object Detection

Ruoyu Xu

1Zhejiang University, 38 Zheda Road, Hangzhou, China

1{xuruoyu,xiangzy}@zju.edu.cn

Zhiyu Xiang

1Zhejiang University, 38 Zheda Road, Hangzhou, China

1{xuruoyu,xiangzy}@zju.edu.cn

###### Abstract

Lidar based 3D object detection has made great progress in recent years and has become the mainstream configuration for autonomous vehicles. However, Lidar can experience substantial performance degradation in the case of adverse weather or long-distance object detection, due to its short wavelength and the limitation of energy emission. 4D millimeter-wave radar is capable of providing 3D point clouds similar to Lidar, with much more robustness against adverse weather conditions. However, 3D object detection with only 4D radar is less satisfactory due to the high sparsity and flickering nature of the measurements. In this paper, we propose a novel 3D object detection method termed RLNet, which effectively integrates 4D radar and Lidar through adaptive feature fusion. An adaptive radar point speed compensation and a modality dropout training strategy are further introduced to improve the performance. RLNet achieves the state-of-the-art performance in the experiments, outperforming baseline method by 7.35 and 2.76 percent in mAP on the popular VoD and ZJUODset dataset, respectively. The code will be available.

Keywords:4D Radar Adaptive Fusion 3D Object Detection

## 1 Introduction

As a popular sensor for autonomous driving, Lidar is well-known for its capability of providing accurate 3D information of surrounding environments. As a result, Many successful Lidar-based 3D object detection methods  have been proposed and achieve state-of-the-art performance in a variety of public datasets . However, due to the short wavelength and energy emission limitations, Lidar can experience substantial performance degradation in the case of adverse weather  or long-distance object detection .

In recent years, 4D millimeter wave radar has received widespread attention. Due to the penetrative nature of millimeter waves, radar can well handle adverse weathers such as rain, snow, and fog, and achieve a longer detection distance . Unlike the traditional 3D radar, 4D radar is capable of providing 3D point clouds similar to Lidar, providing the possibility for accurate 3D object detection. Some public datasets such as Astyx , VoD , Tj4dradset  and ZJUODset  have been published to boost the research. However, the high sparsity and noisynature of the acquired 4D radar point clouds pose a great challenge to the robust detection.

As shown in Fig. 1, the information provided by Lidar and radar sensors can be complementary for the object detection task. Radar can address the limitations of Lidar in the case of part obstruction and objects in the distance, as well as supplying valuable Doppler velocity to facilitate detection of dynamic targets. Current Lidar and radar fusion methods [14; 15] do not well consider the different characteristics of the two modalities during fusion, and they also don't take into account the extreme case of failure of one sensor. Therefore, it is crucial to explore the differences in modalities and design appropriate fusion approaches. To address these problems, we propose a novel 3D object detection method termed RLNet, which effectively fuses 4D radar and Lidar features through adaptive weighting. A radar point speed compensation and a modality dropout training policy are further introduced to improve the detection performance. The experimental results on the VoD dataset  and ZJUODset  demonstrate the effectiveness of our method.

In summary, our main contributions are as follows:

* We introduce a lightweight Lidar-radar fusion network, which fulfills the 3D object detection task in complex environment by adaptively weighting the importance of 4D radar and Lidar features.
* We propose an effective speed compensation method for radar point cloud preprocessing. We estimate the ego-speed from the raw Doppler speed and then obtain the compensated radial speed for each point.
* We propose a special training method with random modality dropout, which enhances the feature of each single-modality and improves the robustness of the network.

Figure 1: A typical scenario on the ZJUODset . The orange and green points in the left image denote the point clouds acquired by Lidar and 4D radar, respectively. The three enlarged bounding boxes on the right show the resampling objects with only radar points, only Lidar points and both of them, respectively.

* We conduct extensive experiments on the VoD  and ZJUODset  datasets. Experimental results show that our method achieves state-of-the-art performance.

## 2 Method

In this section, we introduce our RLNet network in detail, which fuses 4D radar and Lidar point clouds for 3D object detection. The network framework is introduced first, followed by description of each module.

### Network Framework

We implement our RLNet based on SECOND , as shown in Fig. 2. Besides the backbone, the framework consists of a speed compensation module, an adaptive feature fusion module, and a random modality dropout strategy. Before inputting to the network, the 4D radar point clouds are preprocessed by the speed compensation module to obtain the absolute radial velocity for each point. After feature extraction by the sparse encoders, the features from both the radar and Lidar branches are fed into the adaptive feature fusion module, which generates suitable weights to each modal feature before concatenation. We design a random modality dropout strategy during training, thereby enhancing the robustness of the network when the failure of one modality happens. The input for each Lidar point is a 4D-vector with [x,y,z,r], where the first 3 components are the 3D coordinates and the last one is the reflection magnitude. For the 4D radar point, the input is regarded as a 6D-vector, with additional speed and timestamp components over the Lidar point.

### Speed compensation

The extra Doppler speed provided by the radar can be valuable for the network to detect and classify the moving objects. However, the raw Doppler speed obtained

Figure 2: Framework of our RLNet.

from 4D radar is the relative radial speed to the ego-vehicle. The unknown motion of the ego-vehicle can greatly influence the measured Doppler speed, causing difficulty to the network when utilizing the speed information. To address this issue, we propose a speed compensation module for the radar points based on the assumption that most of the radar points are static in the environment. The process is illustrated in Fig. 3 and described as follows.

1. Assuming that the ego-vehicle moves only on x-y plane and heads along the x direction, we project the raw Doppler speed of the points onto the x-y plane, then project the resulting \(V_{r}\) to x-axis to obtain the relative speed \(V_{x}\) along the vehicle's motion direction with \(V_{x}=V_{r}/cos\);
2. Assuming the majority of points in the scene are from the background, which is static relative to the world coordinate system, we divide the speed space of \(V_{x}\) into several bins and count the votes in each bin, and the speed value with the highest count is regarded as the estimated ego-vehicle velocity \(V_{e}\);
3. Compensate the radial velocity of each point based on the estimated ego-vehicle velocity, with \(V_{comp}=V_{r}-V_{e}*cos\). The resulting \(V_{comp}\) can better model the speed field of the scene in that most of the static background objects' speeds are zero.

### Adaptive Fusion Module

Following the sparse convolution layer on the backbone, the network obtains the Lidar and radar features with each of them having dimensions of B\(\)C\(\)H\(\)W, where B, C, H and W are the batch size, the channel number, the height and width of the feature map, respectively. The simplest way to fuse these two features is concatenation, which combines features along the channel dimension to form a feature of size B\(\)2C\(\)H\(\)W. However, this simple way does not consider the large difference of the sensor characteristics, such as the multi-path noise of the radar data and occlusion sensitiveness of the Lidar data. Simple concatenation of their features can confuse the learning of the network and degrade the performance.

To tackle this problem, we propose an Adaptive Fusion module to combine the Lidar and radar features by adaptive weights. Specifically, Lidar feature

Figure 3: Illustration of speed compensation.

map \(F_{L}\) and radar feature map \(F_{R}\) are first processed through a channel average pooling layer, yielding two channel feature maps \(F_{LA}\) and \(F_{RA}\) as follows:

\[F_{LA}=AvgPool(F_{L}) \]

\[F_{RA}=AvgPool(F_{R}) \]

Then these two feature maps are concatenated along the channel dimension and fed into a 3\(\)3 convolution layer, followed by batch normalization and softmax. The result is the weight map of \(W_{L}\) and \(W_{R}\), with each having the size of B\(\)H\(\)W:

\[F_{mix}=Concat[F_{LA},F_{RA}] \]

\[[W_{L},W_{R}]=Softmax(BN(Conv(F_{mix})) \]

The weight maps represent the importance of each spatial location of the feature maps. The modality features are then weighted and concatenated by Eq.(5), yielding the fused feature map \(F_{fusion}\) with size B\(\)2C\(\)H\(\)W:

\[F_{fusion}=Concat[W_{L}*F_{L},W_{R}*F_{R}] \]

This fused feature map is then fed into the subsequent backbone to perform 3D object detection.

### Random Modality Dropout

The data from multimodal sensors can effectively improve the performance of 3D object detection. However, in some adverse scenarios when a certain sensor is constrained or degraded, the network may receive input from only one sensor. In such case, the full multimodal trained network can experience significant degradation in performance. Inspired by CramNet , we introduce random modality dropout strategy during training. The process is described with Eq.(6) and (7), where \(F^{}_{L}\) and \(F^{}_{R}\) represent the modal features after dropout gate, \(()\) is the indicator function with outputting values 0 or 1, other parameters are explained in the following.

\[F^{}_{L}=(p_{1}>P_{drop} p_{2}>P_{L})F_{L} \]

\[F^{}_{R}=(p_{1}>P_{drop} p_{2} P_{L})F_{R} \]

First a random keeping probability \(p_{1}\) is generated. If \(p_{1}\) is greater than the dropping probability threshold \(P_{drop}\), both features of the two modalities are retained and regular training process proceeds. Otherwise, one of the modality feature should be dropped. In this paper, \(P_{drop}\) is set to 0.2.

When deciding which modality feature to drop, another random keeping probability \(p_{2}\) is generated. If \(p_{2}\) is greater than the Lidar's keeping probability threshold \(P_{L}\), the Lidar features are kept and the 4D radar features are dropped; otherwise the Lidar features are dropped in this training epoch. Unlike CramNet  which does not consider the difference of the modalities and uses the same dropout probability for the camera and radar features, we set \(P_{L}\) to 0.2 in this work considering that Lidar features play a dominant role in the fusion. In addition, the experimental results in Section IV suggest that modality dropout strategy can not only mitigate degradation caused by the sensor failure, but also enhance the feature of each modality, thereby promoting the performance of multimodal 3D object detection.

## 3 Experiments

### Dataset and Evaluation Metrics

We conduct experiments on the popular VoD dataset and ZJUODset.

**VoD Dataset .** The VoD dataset is collected in Delft, which includes a substantial number of car, pedestrian, and cyclist objects to detect. Most of the cars are aligned on the roadside with different extent of occlusions, which pose challenges to the Lidar. We follow the official partition and divide the dataset into training and validation set with 5139 and 1296 frames respectively. Based on the target's distance to the ego-vehicle, VoD separately evaluates the performance on the entire annotated area and the driving corridor. The latter refers to the narrow area within the range of [-4m, 4m] on the x-axis and [0, 25m] on the z-axis (front) in the camera coordinate.

**ZJUODset .** ZJUODset collects data on the real traffic scenes of Hangzhou city, aiming at addressing complex and long distance detection requirements for autonomous driving. It collects point clouds acquired from a solid state Livox Lidar and an Oculii Eagle 4D radar, and evaluates the detection performance up to 150 meters. We define the evaluation area as extreme level. 'Easy', 'Moderate' and 'Hard' levels represent objects within 30, 50 and 80 meters, respectively. Within the 3800 annotated frames, we split the first 2660 frames as the training set and the last 1140 frames as the validation set.

AP40 metric is employed for the evaluation. On the VoD Dataset, we use IOU thresholds of 0.5/0.25/0.25 for car, cyclist and pedestrian, respectively, in order to be in line with the previous works. On the ZJUODset, we instead use IOU thresholds of 0.7/0.5/0.5 for car, cyclist and pedestrian respectively, to evaluate the performance with a higher standard than VoD.

### Implementation Details

On the VoD Dataset, the entire detection range is set as (0, 51.2m) on the x-axis, (-25.6m, 25.6m) on the y-axis, and (-3m, 2m) on the z-axis in the Lidar coordinate. We set the voxel size to (0.05m, 0.05m, 0.1m) and the maximum number of points in each voxel to 5. On the ZJUODset, we define the detection range as (0, 158.4m) on the x-axis, (-39.6m, 39.6m) on the y-axis, and (-5m, 3m) on the z-axis. The space is partitioned into voxels of (0.075m, 0.075m, 0.2m) for encoding. We use the random flipping along the x-axis and random global scaling with the scaling factor in [0.95,1.05] for data augmentation.

We implement our RLNet based on mmdetection3d  and OpenPCDet  framework. We employ the Adam optimizer for parameter updates with an initial learning rate 0.001 and a weight decay factor 0.01. The learning rate is updated using a cyclical decay method, with the maximum learning rate being 0.01 and the minimum being \(10^{-7}\). Just as SECOND , the loss for the model comprises three components: classification loss, detection regression loss, and angular loss. Specifically, we adopt Focal Loss for classification, Smooth L1 Loss for location regression and Cross-Entropy loss for angular regression.

### Experimental Results

The experimental results on the VoD dataset are shown in TABLE 1. Besides comparing to other Lidar+radar methods, we also list methods using other modalities, such as pure radar, pure Lidar and radar+image for reference. As shown in the TABLE 1, our method achieves the best performance among all of the methods, with 3.91% and 3.30% improvements on mAP over the second place method (the feature cascade version of SECOND  with Lidar+radar), for the entire annotated area and driving corridor, respectively. Comparing to the pure radar version of SECOND , our method has about 35% improvement on mAP in the entire area, showing the importance of the Lidar in the task. Our RLNet also performs much better than the pure Lidar method, showing the critical role of the 4D radar in detecting some hard occluded objects. The qualitative results shown in Fig. 4 also validate this statement. Meanwhile, our

    &  &  &  \\   & & _Car_ & _Pedestrian_ & _Cyclist_ & _mAP_ & _Car_ & _Pedestrian_ & _Cyclist_ & _mAP_ \\   & PointPillars  & 34.88 & 31.62 & 63.23 & 43.24 & 72.04 & 41.38 & 88.64 & 67.35 \\  & Second  & 35.05 & 29.19 & 55.24 & 39.83 & 73.57 & 43.08 & 83.47 & 66.71 \\   & PointPillars  & 59.11 & 37.71 & 64.49 & 53.77 & 92.35 & 48.02 & 89.08 & 76.48 \\  & Second  & 66.95 & 59.90 & 76.88 & 67.91 & 94.69 & 71.15 & 95.63 & 87.16 \\   & RCFusion  & 41.70 & 38.95 & 68.31 & 49.65 & 71.87 & 47.50 & 88.33 & 69.23 \\  & LXL  & 42.33 & 49.48 & 77.12 & 56.31 & 72.18 & 58.30 & 88.31 & 72.93 \\   & PointPillars  & 60.65 & 48.89 & 73.07 & 60.87 & 91.96 & 51.84 & 91.37 & 78.39 \\  & Second\(\) & 68.70 & 63.56 & 81.20 & 71.35 & 94.94 & 72.37 & 94.04 & 87.12 \\   & Interfusion  & 55.86 & 49.42 & 70.39 & 58.56 & 84.32 & 55.08 & 91.58 & 76.99 \\   & Ours & **74.26** & **68.98** & **82.57** & **75.26** & **97.35** & **78.10** & **95.82** & **90.42** \\   

Table 1: 3D Detection Results on the VoD dataset. \(\)means we change it to multi-modal method by feature cascade.

    &  &  &  \\   & & _Car_ & _Pedestrian_ & _Cyclist_ & _mAP_ & _Car_ & _Pedestrian_ & _Cyclist_ & _mAP_ \\   & PointPillars  & 42.61 & 10.25 & 29.68 & 27.51 & 62.06 & 12.78 & 35.91 & 36.92 \\  & Second  & 42.62 & 13.76 & 39.77 & 32.05 & 62.94 & 21.35 & 48.64 & 44.31 \\   & PointPillars!  & 44.42 & 15.31 & 40.98 & 33.57 & 62.99 & 20.69 & 50.25 & 44.64 \\  & Second†  & 43.14 & 15.36 & 39.64 & 32.71 & 63.10 & 21.67 & 50.93 & 45.24 \\   & Interfusion  & **45.16** & 13.05 & 41.39 & 33.20 & **65.11** & 18.17 & 51.22 & 44.83 \\   & Ours & 44.81 & **17.51** & **42.11** & **34.81** & 64.93 & **22.49** & **52.79** & **46.74** \\   

Table 2: Experimental Results on the ZJUODset. \(\)means we change it to multi-modal method by feature cascade.

method shows superior performance over the radar+image method like RCFusion  and LXL , which should thank to the accurate geometric information provided by the Lidar over the image.

Similar phenomenon can be observed on the ZJUODset, as shown in TABLE 2. The difference is that the improvements are much harder to achieve than on the VoD dataset, due to more complex environment, much longer detecting distance requirements (3 times longer than VoD) and higher standard of evaluation metrics of the task. Our RLNet still stands 1.61% higher on mAP over the second best method Interfusion , validating the effectiveness of our method. The qualitative results shown in Fig. 5 also reveal less false positive and false negative detections of our method over its counterparts.

### Ablation Study

We conduct ablation study on the ZJUODset and the results are in TABLE 3.

**Effects of speed compensation.** We employ a speed compensation strategy to rectify the relative speed caused by the ego-motion of the vehicle. Comparing (a) with (b), or (c) with (d) in TABLE 3, we see that introducing speed compensation can add 0.6\(\)0.7 percent on 3D mAP, which suggests that removing the influence of ego-motion on the Doppler speed is beneficial for 3D object detection.

**Effects of adaptive fusion module.** The simple feature cascade (FC) cannot fully explore the complementary nature of the Lidar and radar data, resulting in inferior performance. Comparison of (a) with (c) or (b) with (d) in TABLE 3 shows that adaptive feature fusion module (AF) can better fuse the features of the two modalities, with 0.4\(\)0.7 percent improvements on mAP.

**Effects of random modality dropout.** Comparing (d) with (e) in TABLE 3, we see that introducing random modality dropout (RD) effectively improves the performance, with an increase of 3D and BEV mAP by 1.04% and 0.75%, respectively. Finally, the complete configuration of our method (e) obtains 2.1% higher than the baseline (a) on 3D mAP, revealing the effectiveness of our model design.

    &  &  &  \\   &  &  &  &  &  &  &  &  &  &  &  &  \\ (a) & & & & & & & & & & & \\ (b) & & & & & & & & & & & \\ (c) & & & & & & & & & & & \\ (d) & & & & & & & & & & & \\ (e) & & & & & & & & & & & \\   

Table 3: Ablation Studies(SC refers to Speed Compensation, AF refers to Adaptive Fusion, FC refers to Feature Cascade, and RD refers to Feature Random Dropout).

## 4 Conclusions

In this paper we introduce an effective method for 3D object detection by fusing 4D radar point clouds to the popular Lidar. Although both sensors have the similar form of 3D point, the high sparsity and range noise contained in the 4D radar has to be well treated. Based on the popular SECOND  backbone, we design speed compensation module for radar points to provide the rectified speed to the network, and propose adaptive fusion module to well balance and enhance the multi-modal features. A special random modality dropout training strategy is further employed to strengthen the robustness of the feature. The experimental results on VoD and ZJUODset datasets demonstrate our success.