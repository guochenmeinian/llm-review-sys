# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

portant for applications such as video games or robotics. We can find many different condition types such as actions , audio , or natural text . In contrast to discrete conditioning means such as actions, utilizing text is advantageous due to its capacity to convey detailed descriptions of specific motions. Natural text allows for the specification of movements in different body parts, at varying velocities, and within diverse contexts or emotional states. Recent advancements with Large Language Models (LLMs) have underscored the potency of text as a versatile tool across various applications .

Generating realistic individual human motion conditioned on a textual description is a very challenging task due to the complexity of the intra-personal dynamics as well as the difficulty of aligning a textual description with a specific motion. Additionally, motion is rarely done in isolation in the real world. As an intelligent species, we adapt our motions depending on several factors, such as the environment and other individuals that we might interact with . Modeling such interactions is extremely difficult due to the intricacy of inter-personal dynamics . More specifically, a single person might behave in many different ways under the same interaction. This _individual diversity_ can arise from variations in the joints trajectories, velocities, or even the action semantics. For example, two people can salute each other by having the left or the right hand, slowly or quickly, or even bowing instead. Controlling such intra-personal dynamics when generating human-human interactions is an important and underexplored capability. Available annotated interaction datasets such as InterHuman  contain a significant amount of annotated interactions. However, neither of them  provides enough individual diversity nor detailed textual descriptions of the individual motions of the interaction. As a consequence, recent human-human interaction generation methods  tend to replicate the interactions from the training datasets, showing limited diversity in the individual motions that encompass the interactions, and lack individual control capabilities. To address all these problems, we could scale up by collecting bigger and more diverse datasets. This work, instead, proposes a new methodology that effectively exploits the individual diversity already present in the available datasets to improve the performance and control when generating human-human interactions. More particularly, our main contributions1 are:

* We propose in2IN, a novel diffusion model architecture that is not only conditioned on the overall interaction description but also on the descriptions of the individual motion performed by each interactant, as illustrated in Fig. 1. To do so, we extend the InterHuman dataset  with LLM-generated textual descriptions of the individual human motions involved in the interaction. Our approach allows for a more precise interaction generation and achieves state-of-the-art results on the InterHuman dataset.
* We introduce a diffusion conditioning technique based on the Classifier Free Guidance (CFG)  that allows weighting independently the importance of each condition during the interaction generation. This enables a higher control over the influence of individual and interaction descriptions on the sampling process.
* We propose DualMDM, a new motion composition technique to further increase the individual diversity and control. By combining our in2IN interaction model with a single-person (individual) motion prior, we generate interactions with more diverse intra-personal dynamics.

## 2 Related Work

### Text-Driven Human Motion Generation

A review of recent literature  reveals significant progress in this domain over the past two years, with a plethora of methodologies being explored. The first set of methodologies that have been explored is based on aligning the latent spaces of text and motion using the Kullback-Leibler divergence loss . A decoder is trained to convert the text latent representation into the corresponding motion. The main limitation of these approaches is that the scarcity of motion data might lead to latent space misalignments and therefore semantic mismatches between the text and the generated motion.

Based on the recent success of auto-regressive approaches in domains like language, with the advent of LLMs  powered by Transformers , new approaches have emerged in the motion field . In these, motions are tokenized into discrete codes from a learned codebook, and a Transformer architecture is used to convert text tokens into motion tokens in an autoregressive manner. While these approaches generate more realistic motions, they have some downsides. Firstly, while tokenizing text is a relatively simple task, tokenizing motion is not straightforward because there are no clear individual logic units as can be the words or lemmas in a text. Additionally, due to the nature of auto-regressive models, they cannot model bi-directional dependencies. MMM  and MoMask  address this limitation using masked attention in BERT  style.

Diffusion Models  have emerged as the best option for many generative tasks , also achieving excellent results in the text-to-motion field. FLAME  and MotionDiffusion  employ a traditional diffusion model with a Transformer as the noise predictor, achieving state-of-the-art results. Instead of predicting the noise, MDM predicts the fully denoised motion at each step. This strategy, typically called \(x_{0}\) reparametrization , enables the use of kinematic loss functions, leading to better human motion generation. Other methods propose incorporating physical constraints into the diffusion process , using latent diffusion models for speeding up the sampling , or leveraging retrieval-based methods . Although the sequential multi-step nature of diffusion models during inference makes them very slow, it also empowers them to generate very realistic samples with high diversity  and fine-grained control capabilities. As a result, diffusion models are very powerful for human interaction generation.

### Text-Driven Human Interaction Generation

ComMDM  extends MDM's capabilities to generate multi-human interactions. ComMDM is a cross-attention module integrated into specific layers of the denoisers in two frozen MDM models. This module processes the activations from the two models and adjusts them to foster interaction. In , a similar concept is employed but this time with two distinct models. Interaction modeling is achieved through a shared cross-attention module that connects both models, an architecture particularly suited for asymmetric interactions involving an actor and a receiver. However, they observed that their method overfitted to the training dataset due to the lack of annotated interaction datasets. Recently, InterHuman  was released, becoming the most extensive annotated dataset of human interactions up to date. The authors also propose a baseline method called InterGen, which is based on two cooperative denoisers with shared weights. Finally, MoMat-MoGen  extends the retrieval diffusion model proposed in  and adapts it to human interactions, becoming the current state of the art on InterHuman. In contrast to the previous approaches, we propose a diffusion model (in2IN) that conditions the generation on both the general interaction description and a fine-grained description providing more details on the action performed by each individual involved in the interaction. This results in a model that generates adequate inter-personal dynamics and, at the same time, enables precise control on the intra-personal dynamics.

### Human Motion Composition

The iterative paradigm underlying diffusion models provides them the capability to combine data, such as multiple images or motions, in a harmonized way . In the realm of motion, the literature has traditionally differentiated between temporal and spatial composition. Temporal composition refers to combining multiple individual motions into a larger sequence , making smooth and realistic transitions among them emerge. On the other hand, spatial composition refers to combining multiple motions to generate a new motion of the same length that combines certain elements of the original motions, such as the actions, the trajectory, or joint-specific movements . All of them share the same limitation though: they apply to single-person motion composition. In a more broad sense,  proposed a generic _model composition_ technique to combine the sampling processes of two different diffusion models, thus generating a harmonized motion. However, they used a fixed score-merging technique along the whole denoising process, which we prove is a suboptimal strategy in more complex scenarios like ours. Instead, we propose a novel model composition technique (DualMDM) that can combine 1) individual motions generated with a prior pre-trained on a single-person motion dataset, and 2) the interactive motions generated by a human-human interaction model like in2IN. The interactions generated with DualMDM show higher diversity of intra-personal dynamics while still maintaining the inter-personal coherence of the overall interaction.

## 3 Method

In this section, we introduce our main methodological contributions. First, in Sec. 3.1, we describe in2IN, our proposed interaction-aware diffusion model conditioned on both the interaction and the individual textual descriptions. Then, we introduce the multi-weight CFG technique, which increases the user control over the influence that each condition has over the generation process. Finally, in Sec 3.2, we discuss how our second contribution, DualMDM, can increase the control and diversity of the intra-personal dynamics generated by pre-trained interaction models such as in2IN.

### in2IN: Interaction diffusion model

The architecture of our interaction diffusion model (in2IN) is founded on the principle that interactions between two persons exhibit a commutative property , denoted as \(\{x_{a},x_{b}\}\), which is considered to be equivalent to \(\{x_{b},x_{a}\}\). Building on this concept, we introduce a Transformer-based diffusion model in a Siamese configuration . Two copies of the diffusion model are made, sharing parameters. Each network is responsible for processing its respective noisy motion inputs, \(_{a}^{t}\) and \(_{b}^{t}\), and aims to produce the denoised versions, \(_{a}^{0}\) and \(_{b}^{0}\). We predict the \(x_{0}\) directly  as this allows us to use kinematic losses. Once the losses have been calculated, the motion is soaked back to \(x^{t-1}\) to become the input of the next denoising iteration.

Similarly to , our diffusion model architecture (Fig. 2) has a multi-head self-attention module where it learns the intra-personal dynamics of the motion, and a multi-head cross-attention module that combines the self-attention output with the motion of the other individual in the interaction, thus modeling the inter-personal dynamics. We also condition the generation with adaptative normal ization layers . However, in contrast to previous approaches, we introduce different conditions for the different attention modules. For the self-attention module, where only the individual motion matters, we provide the specific textual description of the individual motion as conditioning. On the other hand, in the cross-attention module, where the whole interaction is important, we provide the interaction textual description as conditioning. This allows for a more precise control of the intra- and inter-personal dynamics.

**Multi-Weight Classifier-Free Guidance.** Our conditioning strategy for the diffusion model builds upon CFG, initially proposed by Ho _et al_. . Generally, diffusion models have a significant dependency on CFG to generate high-quality samples. However, incorporating multiple conditions using CFG is not trivial. We address this by employing distinct weighting strategies for each condition. The equation representing our model's sampling function, denoted as \(G_{s}(x^{t},t,c)\), is as follows:

\[ G_{s}(x^{t},t,c)&=G( x^{t},t,)\\ &+w_{c}(G(x^{t},t,c)-G(x^{t},t, ))\\ &+w_{I}(G(x^{t},t,c)-G(x^{t},t, ))\\ &+w_{i}(G(x^{t},t,)-G(x^{t},t, )), \]

where \(G(x^{t},t,)\) is the unconditional output of the model, and \(G(x^{t},t,c)\), \(G(x^{t},t,c_{})\), and \(G(x^{t},t,c_{})\) denote the model outputs conditioned on the whole conditioning \(c=\{c_{I},c_{i}\}\), only the interaction, and only the individual, respectively. The weights \(w_{c}\), \(w_{I}\), and \(w_{i}\) adjust the influence of each conditioned output relative to the unconditional baseline. A notable limitation of this approach is the necessity to perform quadruple sampling from the denoiser, as opposed to the dual sampling required in a conventional CFG methodology. In exchange, this method allows for more refined control over the generation process, ensuring that the model can effectively capture and express the nuances of both individual and interaction-specific conditions. If a weight is set to 0, then that particular conditioning is ignored during the generation process.

### DualMDM: Model composition

In our second contribution, we propose a motion model composition technique that allows us to combine interactions generated by an interaction model with the motions generated by an individual motion prior trained with a single-person motion dataset. This method uses a single-person human motion prior to provide the generated human-human interactions with a higher diversity of intra-personal dynamics. This model composition technique is built on top of the method proposed in DiffusionBlending :

\[ G^{a,b}(x^{t},t,c_{a},c_{b})&=G^{a} (x^{t},t,c_{a})\\ &+w(G^{b}(x^{t},t,c_{b})-G^{a}(x^{t},t,c_{a})), \]

where \(w\) is the blending weight, \(G^{a}(x^{t},t,c_{a})\) and \(G^{b}(x^{t},t,c_{b})\) are the outputs of the diffusion models \(a\) and \(b\), respectively. Since the original proposal was made to combine single-person diffusion models, we adapt the previous formula to our scenario:

\[ G^{I,i}(x^{t},t,c)&=G^{I}(x^{t},t,c) \\ &+w(G^{i}(x^{t},t,c_{i})-G^{i}(x_{t},t,c)), \]

where \(G^{I}(x^{t},t,c)\) is the output of the interaction diffusion model and \(G^{i}(x^{t},t,c_{i})\) is the output of the individual motion prior. By choosing \(w\) to be constant, authors from

Figure 2: **in2IN diffusion model.** Our proposed architecture consists of a Siamese Transformer that generates the denoised motion of each individual in the interaction (\(x_{a}^{a}\) and \(x_{b}^{b}\)). In the first stage, a self-attention layer models the intra-personal dependencies using the encoded individual condition and noisy motion of each person (\(x_{a}^{t}\) and \(x_{b}^{t}\)). In the second stage, a cross-attention module models the inter-personal dynamics using the encoded interaction description, the self-attention output, and the noisy motion from the other interacting person.

 assumed that the optimal blending weight is the same along the whole sampling process. However, in line with , we argue that the optimal blending weight might vary along the denoising chain, depending on the particularities of each scenario. To account for this, we propose to replace the constant \(w\) with a weight scheduler \(w(t)\) that parameterizes the blending weight used to combine the denoised motion from both models, making it variable on the sampling phase (Fig. 3). As a generalization of the DiffusionBlending technique, DualMDM is a more flexible and powerful strategy to combine two diffusion models.

## 4 Experimental Evaluation

### Data

Our experiments are conducted with the InterHuman  and HumanML3D  datasets. InterHuman is the largest annotated interaction dataset in which each motion is represented as \(x^{i}=[_{g}^{};_{g}^{};^{f}]\), where \(x^{i}\), the \(i\)-th motion timestep, encompasses joint positions \(_{g}^{p}^{3N_{j}}\) and velocities \(_{g}^{}^{3N_{j}}\) in the world frame, \(6D\) representation of local rotations \(^{r}^{6N_{j}}\) in the root frame, and binary foot-ground contact features \(^{f}^{4}\). \(N\) is the number of joints. In our case \(N=22\). As InterHuman does not provide individual textual descriptions of the motions pertaining to the interaction, we have automatically generated them using LLMs.

InterHuman dataset is focused on providing a wide range of interactions rather than individual diversity in its motions. We have trained an individual motion prior with the HumanML3D dataset, which contains a much wider range of annotated individual motions. For compatibility purposes, we converted the HumanML3D motion representation to the one used in the InterHuman dataset. More details on the LLM-based generation of the individual descriptions and the implementation details of our individual motion prior can be found in the Supplementary Material.

### Evaluation Metrics

We utilize the evaluation metrics proposed in . R-precision and Multimodal-Dist evaluate how semantically close the generated motions are to the input prompts. The FID score is used to measure the dissimilarity between the distributions of generated motions and the actual ground truth motions. Diversity is assessed to gauge the range of variation within the generated motion distribution, while MultiModality calculates the average variance for motions generated from a single text prompt. To compute these metrics, we need encoders that align the text and motion latent representation, which we borrow from .

None of the previous evaluation metrics assesses the alignment of the generated interactions with the individual descriptions. Due to the lack of ground-truth individual annotations, we cannot train single-person motion and text encoders for InterHuman. Therefore, we cannot reliably assess the individual alignment with the R-Prec, Multimodal-Dist, or FID metrics. We argue though that the interaction metrics are not only sensitive to the global quality of the interaction but also to the coherence of the intra-personal dynamics with the interaction context. If an interactant is _kicking a ball_, the _salute to each other_ interaction is not coherent, and the generated motion will have low R-Prec. Thus, interaction metrics are indeed sensitive to wrong intra-personal dynamics in an interaction. What they do not capture are the intra-personal differences promoted by the usage of distinct individual descriptions. More specifically, the interaction generated with {\(\)=_salute to each other_, \(_{}=\)\(c_{}=\)\(wave\)_right hand_} will be different from the one generated with the same set with \(c_{}=\)_bowsw forward_ instead. However, these differences might come 1) from the intrinsic diversity of the generative model, quantified by the MultiModality metric (i.e., different ways of 38aving right hand, and not bowing at all), or 2) from the extrinsic diversity caused by differences in the individual descriptions used, thus showing control capabilities over the generated intra-personal dynamics. With the motivation of quantifying the latter, we introduce a new evaluation metric called _Extrinsic Individual Diversity (EID)_.

**Extrinsic Individual Diversity (EID).** In order to assess the extrinsic diversity of the model, we need to disentangle it from the intrinsic one. To do so, we generate two empirical distributions that will serve as a proxy for quantifying the intrinsic diversity of 1) the ground-truth scenario, and 2) a synthetic scenario where the individual descriptions are randomly changed. In particular, for every set of interaction and individual descriptions {\(,_{},_{},_{}\)} in the dataset, we proceed as follows: 1) we build \(D_{}\) as the set of \(N\) motions generated with {\(_{},_{},_{}, _{}\)}, and 2) we build \(D_{}\) as the set of \(N\) motions generated randomly replacing \(c_{}\)

Figure 3: Different weights schedulers tested for DualMDM. Oranges: Exponential. **Blues:** Inverse Exponential. **Greens:** Constant. **Magenta:** Linear.

and \(c_{}}\) with other individual descriptions from the dataset. Then, we define the _EID_ as the Wasserstein distance between \(D_{}\) and \(D_{}\). A higher distance means more influence of the individual descriptions on the diversity of the generated motions, arguably leading to higher control on the intra-personal dynamics of the interaction. This metric can be combined with others such as the R-Precision and FID to analyze the trade-off between individual diversity and interaction quality and fidelity.

In our experiments, we set \(N{=}32\). To quantify the additional extrinsic diversity provided by the DualMDM technique, we build \(D_{}\) with in2IN and \(D_{}\) with in2IN combined with the DualMDM.

### Implementation Details

Our in2IN models consist of 8 consecutive multi-head attention layers with a latent dimension of 1024 and 8 heads. We utilize a frozen CLIP-Vi\(LT/14\) model  as our text encoder. We set the number of diffusion timesteps to 1,000 and employ a cosine noise schedule . During inference, we use DDIM sampling  with \(=0\) and 50 timesteps, and our proposed multi-weight CFG variation. To enable the latter, 10% of the CLIP embeddings are randomly set to zero during training.

All models have been trained using the AdamW optimizer  with betas of \((0.9,0.999)\), weight decay of \(2 10^{-5}\), maximum learning rate of \(10^{-4}\), and a cosine learning rate schedule with an initial 10-epoch linear warm-up period. They have been trained using the L2 loss and, thanks to the use of the \(x_{0}\) parameterization, kinematic losses have also been used. These include the foot contact and the velocity losses from the MDM framework , and the bone length, the masked joint distance map, and the relative orientation losses suggested in InterGen . Additionally, we have used the kinematic loss scheduler from InterGen. All models have been trained for 2,000 epochs with a batch size of 64 with 16-bit mixed precision. Two Nvidia 3090 GPUs have been required for the span of 5 days.

**DualMDM schedulers.** We test these functions:

\[& w(t)=\\ & w(t)=t/T\\ & w(t)=e^{-(T-t)},\\ & w(t)=1-e^{-(T-t)},  \]

where \(t\) is the actual denoising step, \(T\) is the total number of denoising steps, and \(\) is the parameter that determines the slope of our scheduler function. We visualize them in Fig. 3.

### Quantitative Analysis

#### 4.4.1 in2IN: Interaction Generation

Tab. 1 shows the quantitative evaluation of our in2IN architecture with respect to the previously existing methods evaluated on the InterHuman dataset. It can be observed that by using individual information we have been able to obtain better results than all previous methods. As might reasonably be anticipated, the additional information used only by in2IN in form of LLM-generated individual descriptions reduces the spectrum of valid motions fulfilling the interaction description, which reflects as a lower MultiModality.

With respect to the Multi-Weight CFG, we evaluate the isolated effect of each weight on the evaluation metrics in Fig. 4. As can be observed, for weights \(w_{c}\) and \(w_{I}\), 4 is the best weight individually. On the other hand, for weight \(w_{i}\), 2 is the best weight. More than that turns into a decrement in performance. We find the best combination with a grid search in a validation subset: \(w_{c}{=}3\), \(w_{I}{=}3\), and \(w_{i}{=}1\).

#### 4.4.2 DualMDM: Individual Diversity

In Tab. 2, the EID metric is compared with the R-Precision and FID using different schedulers in our DualMDM method. In general, we can observe that in all the schedulers, the ones that assign more weight to the individual model obtain higher individual diversity, in exchange for a lower interaction quality. While a constant scheduler with \(0.25\) seems to achieve good quantitative values, we can observe that the exponential weight scheduler with \(0.00875\) provides a better trade-off between individual diversity and interaction quality. This is fundamental, as we want to have high intra-personal diversity while keeping the inter-personal coherence. We hypothesize that the good

Figure 4: Comparison of **R-Precision** and **FID** for the different weights on the Multi-Weight CFG tested in isolation. Each column represents a different weight (\(w_{c}\), \(w_{I}\), \(w_{i}\)). \(w_{c}\) has been tested with \(w_{I}{=}w_{i}{=}0\). \(w_{I}\) and \(w_{i}\) have been tested with \(w_{c}{=}1\), and the other weight set to 0.

[MISSING_PAGE_FAIL:7]

Figure 5: **Interaction Description:** The two guys meet, grip each otherâ€™s hand, and nod in agreement. The X-axis represents time.

Figure 6: **Interaction Description:** One person spots the other person on the street, lifts the right hand to greet, and the other person glances towards one person. The X-axis represents time.

Figure 7: **Interaction Description:** Two persons are in an intense boxing match. **Individual Description #1:** An individual throws a kick with his right leg. **Individual Description #2:** An individual is boxing. The X-axis represents time.