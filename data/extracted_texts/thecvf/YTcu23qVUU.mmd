# Region-Based Emotion Recognition via Superpixel Feature Pooling

Zhihang Ren\({}^{1}\), Yifan Wang\({}^{1}\), Tsung-Wei Ke\({}^{2}\), Yunhui Guo\({}^{3}\), Stella X. Yu\({}^{1,4}\), David Whitney\({}^{1}\)

\({}^{1}\)University of California, Berkeley, \({}^{2}\)Carnegie Mellon University,

\({}^{3}\)University of Texas at Dallas, \({}^{4}\)University of Michigan, Ann Arbor

\({}^{1}\){peter.zhren,wyf020803,zhimin,dwhitney}@berkeley.edu,

\({}^{2}\)tsungwek@andrew.cmu.edu, \({}^{3}\)yunhui.guo@utdallas.edu, \({}^{4}\)stellayu@umich.edu

###### Abstract

Perceiving other people's emotional states is fundamentally important for successful social interactions and robotics. Traditional emotion recognition algorithms exclusively focus on facial expressions, ignoring the critical role of background context, which is now known to be necessary to accurately represent and understand the emotions of others. More recent studies have utilized different fusing techniques to combine facial and contextual information in visual scenes, but these approaches are limited to detection-based methods. In this study, we propose a new region-based emotion recognition method via superpixel feature pooling that does not rely on detection. Our proposed method consists of three types of blocks, including an initial over-segmentation block, the superpixel pooling block, and the emotion recognition block. On EMOTIC and VEATIC datasets, our proposed method improves state-of-the-art performance by \(68.57\%\) and \(11.79\%\) respectively. We also achieve competitive performance on the CAER-S dataset.

## 1 Introduction

Recognizing human emotions is routine and necessary to successfully navigate social interactions on a daily basis. Nowadays, as robotic techniques grow fast, it is natural to make future intelligent machines socially aware in the human-populated world. Moreover, emotion recognition may help the autonomous driving system to anticipate pedestrians' or drivers' intentions and react properly. Therefore, understanding emotion perception mechanisms and designing automatic emotion recognition methods are essential for future robotics and autonomous driving developments.

Over the past several years, the interest in utilizing deep learning models to automatically recognize emotional states has grown rapidly. Following a long tradition of research on emotion recognition in the fields of psychology, neuroscience, and vision science, previous computer vision research focused almost exclusively on facial expressions as the key information for emotion recognition. This is unsurprising, as facial expressions seem to be the most direct and inherent way for humans to understand the emotions of others. Consequently, many early datasets annotated only the emotional states of character faces or lab-controlled human interactions, treating them as if they are independent of the context . With this massive annotated data involving facial expressions, researchers primarily focused on the analysis of facial expression to predict emotions . Later studies also found extra information, such as shoulder location and body pose, could be utilized to infer emotional states . Overall, though, early datasets and recognition models focus strictly on character-specific information to infer emotional states.

Although the character itself--including the facial expression--contains a great deal of information about its emotional state, many studies in Psychology have proven that context information is critically important for accurate emotion perception . In many scenarios, scene context influences the perception of human emotion even though the facial expression is unchanged or very similar . And, scene context can explain as much of the variance in human emotion perception as facial expression . For these reasons, annotations of isolated faces may not accurately reflect true human emotion, and context-based emotion datasets are necessary.

In light of the importance of context in emotion recognition, several datasets that include contextual information have emerged recently . Those datasets not only contain characters but also large areas of the surrounding context. In turn, recent algorithms  then focus on different feature extraction methods and fusing techniques for various types of information and visual features.

To locate different types of visual information, such as the face, body, and background scene, previous methodsrely heavily on object detection methods . They often utilize rectangular bounding boxes to select or mask out certain image blocks (Figure 1(b)). Then, visual feature encoding and fusing modules are utilized to represent the visual information of the whole scene.

When humans try to perceive the emotions of other people, they rely on bottom-up and top-down visual processes, where fine-scale visual features and coarse-scale object and scene knowledge mutually facilitate each other . In this process, there are no bounding box structures; instead, there are fine or coarse scales of object and scene regions (superpixels). With this insight, researchers have started to utilize superpixels in a variety of computer vision models .

So far, superpixel-based models have been successful in understanding and grouping semantically similar image regions, achieving good performance on part parsing, saliency detection, and image segmentation tasks. As emotion recognition requires the understanding of characters' facial expressions, as well as their interactions with different regions of objects and background scenes, the superpixel approach could be useful for emotion recognition. But, to the best of our knowledge, no emotion recognition method has utilized superpixel-based methods.

In this paper, we propose a new emotion recognition method that utilizes superpixel-level visual features. Our proposed method consists of three types of blocks: 1) an over-segmentation block to initialize the fine-grained segments and generate initial superpixel features; 2) the superpixel pooling block to learn the grouping policy and aggregate the current level finer-scale features to the next level coarser-scale features; and 3) the emotion recognition block for either emotion regression or classification tasks based on the final aggregated feature. We test our proposed method on three public context-aware emotion recognition datasets, EMOTIC , CAER , and VEATIC . We achieve state-of-the-art performance on EMOTIC  and VEATIC , with \(68.57\%\) improvement on EMOTIC and \(11.79\%\) improvement on VEATIC. We also achieve competitive results on CAER-S . Moreover, we show that by using superpixels as feature extraction anchors, we can naturally obtain semantically similar superpixels for free with the learned grouping policy.

In summary, our contribution of this work lies in three aspects:

1. We propose the first region-based emotion recognition method via superpixel feature pooling.
2. We achieve state-of-the-art emotion recognition performance on VEATIC and EMOTIC datasets with \(68.57\%\) improvement on EMOTIC and \(11.79\%\) improvement on VEATIC.
3. We show that the method can also provide us with clusters that contain semantically similar superpixels via the learned grouping policy.

## 2 Related Work

### Context-Aware Emotion Recognition

When inferring emotion states, the context-aware emotion recognition methods do not only rely solely on the face or body information but also consider the emotion cues from scene context and background information. Traditional context-aware emotion recognition methods invariably extract multiple representations from various visual information sources and then apply feature fusion to make the final prediction . Object detection methods are widely utilized to identify the information sources, marking them with rectangular bounding boxes. For example, the model released along with the EMOTIC dataset  fused the body region feature and the whole image as the context feature via a Convolutional Neural Network (CNN). In this study, we utilize superpixel as the feature anchor for subsequent feature fusing, which does not rely on object detection or bounding boxes.

### Vision Transformers

Vision Transformers (ViT)  have achieved amazing performance in image recognition. They treat images as sets of rectangular patch tokens and employ an attention mechanism in learning . ViTs can be computationally expensive. To improve their efficiency, hierarchical transformers aim to reduce the number of tokens by spatial pooling . Other approaches directly prune tokens away according to their significance scores . Our grouping procedure looks like the latter approach. However, we focus on grouping different visual regions for emotion recognition while those methods aim for efficiency. Additionally, we use superpixels as input units instead of square patches.

### Superpixels

Superpixels are sets of locally connected pixels that encapsulate coherent structures, such as colors . Intuitively, superpixels have been utilized in various computer vision tasks that involve dense labeling, including part parsing , saliency detection , and image segmentation . Recent studies have replaced patches with superpixel tokens in ViT architectures to achieve semantic segmentation . In this study, we adopt superpixels as the visual feature extraction anchors for feature fusing in emotion recognition tasks.

## 3 Method

Inspired by the human emotion recognition process, we propose the first region-based emotion recognition method via superpixel feature pooling. Our idea revolves around utilizing superpixels to enhance our understanding of characters' facial expressions, along with their interactions within various regions of objects and background scenes. Figure 2 illustrates an overview of the method. The image/frame is at first over-segmented to obtain the fine-grained segments. At each level, the finer-scale superpixels are grouped into coarser-scale superpixels via the superpixel pooling block. The corresponding superpixel features are aggregated according to the learned pooling policy, processed by the visual transformer block to learn better features, and then sent to the next level pooling and aggregation. At last, the emotion recognition block can take the final aggregated feature to complete the emotion recognition task. Now, we introduce each block respectively.

### Over-segmentation Block

Each time, we start with the finest-level pixel grouping, denoted as \(G_{0}\), i.e., the initial image region grouping. These groupings are based on low-level visual cues and designed to align with image contours. In this paper, we utilize SEEDS  to obtain the locally connected and color-wise coherent regions, i.e., the superpixels. Then, we progressively group these superpixels into coarser regions and fuse the corresponding superpixel features to get the final aggregated feature for emotion recognition.

The initial pixel features of the input image, \(X_{cnn}\), are obtained via a convolutional neural network (CNN). These pixel features are then aggregated within each superpixel in \(G_{0}\) to create the initial superpixel features, referred to as \(X_{G}\). The aggregation is achieved by averaging each pixel feature within a specific superpixel. After this, we append a class token \(X_{class}\), and positional encodings \(E_{pos}\) into the initial features \(X_{G}\). We set \(E_{pos}\) to align with the resolution of the CNN features \(X_{cnn}\) and then average it within each superpixel. The resulting input segment features are defined as \(Z_{0}=[X_{class};X_{G}]+[;E_{pos}]\)

### Superpixel Pooling Block

To form better features for the aggregated superpixel features at each grouping level, such that semantically similar superpixel features at each grouping level would be more similar in the feature space and vice versa, we apply two visual transformer (ViT) blocks before the superpixel pooling. Then, we pool the similar fine-scale regions into a coarser scale and move to the next level. Various pooling strategies can be applied here. In this paper, we adopt a graph pooling strategy . The similarities between different superpixel features in neighbor levels are computed and utilized to quantify the soft assignment probability \(P_{l}\) from a finer level \(l-1\) to a coarser level \(l\). Then, the next level of coarser groupings \(G_{l}\) can be determined by the finer level grouping \(G_{l-1}\) and the soft assignment probability \(P_{l}\).

\[G_{l}=G_{l-1} P_{l}=G_{0}_{i=1}^{l}P_{i} \]

### Emotion Recognition Block

At the final stage, we will have an aggregated feature that contains the combined visual information from separate visual regions. Then, we utilize a multilayer perceptron (MLP) to achieve the emotion recognition task. At last, we utilize either categorical emotion states or continuous emotion ratings to guide the training. We emphasize that no segmentation maps are utilized in the training. The grouping of superpixels is only trained to make good emotion recognition results, though the method naturally learned how to group superpixels efficiently.

Figure 1: **Comparison of detection-based methods and our proposed region-based method. Detection-based methods rely on the bounding box to encode the character and context visual information separately, while our proposed region-based method directly utilizes initial pixel-level features and gradually aggregates similar superpixel features to represent the visual information.**

## 4 Experiment

### Datasets and Evaluation Metrics

**Datasets:** We conduct our experiments on three standard datasets for the context-aware emotion recognition task, namely EMOTIC , CAER-S , and VEATIC . EMOTIC contains 23,571 images of 34,320 annotated subjects in uncontrolled environments. The annotation of these images contains the bounding boxes of the target subjects' body regions and 26 discrete emotion categories. CAER-S includes 70k static images extracted from video clips of 79 TV shows to predict emotional states. These images are annotated with 7 emotion categories: Anger, Disgust, Fear, Happy, Sad, Surprise, and Neutral. VEATIC has 124 video clips from Hollywood movies, documentaries, and home videos with continuous valence and arousal ratings of each frame via real-time annotation.

**Evaluation Metrics:** Following , we utilize the standard classification accuracy to evaluate performance on CAER-S. For VEATIC, the root mean square error (RMSE) is used. At last, we utilize the mean Average Precision (mAP) to evaluate the classification results on the EMOTIC.

### State-of-the-art Methods

Given the fact that our model is tested on three datasets, we select several models with different structures tested on each of the corresponding datasets for comparison. For the EMOTIC dataset, we select seven distinct models for comparison. EMOT-Net  is a two-branch Convolutional Neural Network model, whose unique branches capture the body features and context features separately. GCN-CNN  is a Graph Convolutional Network trying to infer emotion relationships utilizing the affective graph constructed by context elements. CAER-Net  is a double-stream Convolutional Neural Network model with an adaptive fusion module focusing on inferring emotion by integrating context information with facial information. RRLA  proposed the Body-Object Attention module and Body Part Attention module to estimate the importance of body parts and background information. VRD  utilizes both the spatial and semantic features by attention mechanism to learn the impact of each part on emotion recognition. EmotiCon  takes advantage of visual attention and depth maps to obtain multi-modal information. And CCIM  utilizes causal inference for model training. For CAER-S Dataset, we have added two more models for comparison in addition to the ones mentioned above. SIB-Net  is inspired by the study of context-containing order, interaction, and bias relationships. GRERN  proposes a framework based on a Graph Convolutional Network to do emotion classification utilizing the region-wise semantic relationships. For VEATIC Dataset, We compare our model to VEATIC-NET , which is a two-stream Video Transformer using the attention mechanism to learn

Figure 2: **Overview of the method:** In our proposed method, We start with over-segmented regions, i.e., the superpixels, and then gradually group similar superpixels, and aggregate features of corresponding superpixels. We utilize the final aggregated feature for emotion recognition. Along with the training, we also obtain clusters that contain semantically similar pixels, e.g., the red face region in \(G_{2}\).

the contextual relationships between frames. We reproduce the results on the corresponding datasets based on the details given by the models above.

### Implementation Details

We conducted supervised training following the setup of DeiT . The model is trained on 4 NVIDIA GeForce RTX 2080 Ti GPUs. For the hyperparameters, we have 4 levels in total for the superpixel pooling. There are 64, 32, 16, and 8 clusters respectively at each grouping level and the batch size of the data is 64. For the superpixel inputs, we utilize 196 pixels as default. We resize the input images into \(224 224\) and apply normalization to the images. Our model is trained using the AdamW optimizer . For the learning rate schedule, we use a linear warmup of 5 epochs to reach a peak learning rate of \(5.0 10^{-4}\) from \(1.0 10^{-6}\), followed by a cosine decay of 30 epochs to decay the final learning rate to minimum learning rate of \(1.0 10^{-5}\).

### Comparison with State-of-the-art Methods

#### 4.4.1 Results on the EMOTIC Dataset.

In Table 1, we see that our proposed method significantly improves the recognition precision of most emotion categories. In particular, compared to EMOT-NET , GCN-CNN , CAER-NET , RRLA , VRD , EmotiCon , and CCIM , our proposed method improve the mAP scores by \(136.16\%\), \(134.23\%\), \(176.56\%\), \(103.52\%\), \(87.60\%\), \(86.96\%\), and \(68.57\%\) respectively. For

  
**Category** & EMOT-NET & GCN-CNN & CAER-NET & RRLA & VRD & EmotiCon & CCIM & Ours \\   Affection & 26.47 & 47.52 & 22.36 & 37.93 & 44.48 & 38.55 & 40.77 & **64.22** \\ Anger & 11.24 & 11.27 & 12.88 & 13.73 & 30.71 & 14.69 & 15.48 & **65.62** \\ Annoyance & 15.26 & 12.33 & 14.42 & 20.87 & 26.47 & 24.68 & 24.47 & **65.81** \\ Anticipation & 57.31 & 63.2 & 52.85 & 61.08 & 59.89 & 60.73 & 95.15 & **70.81** \\ Aversion & 7.44 & 6.81 & 3.26 & 9.61 & 12.43 & 11.33 & 19.38 & **71.67** \\ Confidence & **80.33** & 74.83 & 72.68 & 80.08 & 79.24 & 68.12 & 75.81 & 60.79 \\ Disapproval & 16.14 & 12.64 & 15.37 & 21.54 & 24.54 & 18.55 & 23.65 & **65.50** \\ Disconnection & 20.64 & 23.17 & 22.01 & 28.32 & 34.24 & 28.73 & 31.93 & **70.84** \\ Disquirement & 19.57 & 17.66 & 10.84 & 22.57 & 24.23 & 22.14 & 26.84 & **66.76** \\ Doubt/Confusion & 31.88 & 19.67 & 26.07 & 33.5 & 25.42 & 38.43 & 34.28 & **59.45** \\ Embarrassment & 3.05 & 1.58 & 1.88 & 4.16 & 4.26 & 10.31 & 16.73 & **60.59** \\ Engagement & 86.69 & 87.31 & 73.71 & 88.12 & 88.71 & 86.23 & **97.41** & 61.17 \\ Esteem & 17.86 & 12.05 & 15.38 & 20.5 & 17.99 & 25.75 & 27.44 & **63.51** \\ Excitement & 78.05 & 72.68 & 70.42 & 80.11 & 74.21 & 80.75 & **81.59** & 70.22 \\ Fatigue & 8.87 & 12.93 & 6.29 & 17.51 & 22.62 & 19.35 & 15.53 & **78.61** \\ Fear & 15.7 & 6.15 & 7.47 & 15.56 & 13.92 & 16.99 & 15.37 & **66.46** \\ Happiness & 58.92 & 72.9 & 53.73 & 76.01 & 83.02 & 80.45 & **83.55** & 67.35 \\ Pain & 9.46 & 8.22 & 8.16 & 14.56 & 16.68 & 14.68 & 17.76 & **68.99** \\ Peace & 22.35 & 30.68 & 19.55 & 26.76 & 28.91 & 35.72 & 38.94 & **71.30** \\ Pleasure & 46.72 & 48.37 & 34.12 & 55.64 & 55.47 & **67.31** & 64.57 & 63.93 \\ Sadness & 18.69 & 23.9 & 17.75 & 30.8 & 42.87 & 40.26 & 45.63 & **57.78** \\ Sensitivity & 9.05 & 4.74 & 6.94 & 9.59 & 15.89 & 13.94 & 17.04 & **66.48** \\ Suffering & 17.67 & 23.71 & 14.85 & 30.7 & 46.23 & 48.05 & 21.52 & **65.32** \\ Surprise & 22.38 & 8.44 & 17.46 & 17.92 & 16.27 & 19.6 & 26.81 & **59.14** \\ Sympathy & 15.23 & 19.45 & 14.89 & 15.26 & 15.37 & 16.74 & 47.6 & **65.01** \\ Yearning & 9.22 & 9.86 & 4.84 & 10.11 & 10.04 & 15.08 & 12.25 & **67.76** \\  mAP & 27.93 & 28.16 & 23.85 & 32.41 & 35.16 & 35.28 & 39.13 & **65.96** \\   

Table 1: Average precision (%) of seven recent methods, and our proposed method for each emotion category on the EMOTIC dataset . Overall, our proposed method improves state-of-the-art performance by \(68.57\%\).

    &  \\   & **Valence** & **Arousal** & **Overall** \\  VEATIC-NET & 0.3084 & 0.2410 & 0.2747 \\
**Ours** & **0.2577** & **0.2268** & **0.2423** \\   

Table 3: Comparison of our proposed method with the baseline model proposed in VEATIC . Our method outperforms \(11.79\%\) compared to the baseline method.

certain emotion categories, the emotion recognition average precision is even improved drastically compared to SOTA performances, such as Yearning (\(+349.34\%\)), Sensitivity (\(+290.14\%\)), Pain (\(+288.46\%\)), Anger (\(+269.81\%\)), Embarrassment (\(+262.16\%\)), and Fatigue (\(+247.52\%\)).

#### 4.4.2 Results on the CAER-S Dataset.

Table 2 shows our proposed method performs competitively with recently released emotion recognition methods. It is worth noting that the CAER dataset utilized very few annotators (six) and has little control over the annotation quality

Figure 3: **Visualization of Grouping:** Column (a) raw images; (b) over-segments; (c) finer groupings; (d) coarser groupings; (e) overlaying coarser groupings onto the raw images. Surprisingly, without the supervision of segmentation maps, the proposed method learned the grouping policy for superpixels guided by visual emotion recognition training.

compared to EMOTIC and VEATIC datasets. As annotation uncertainty and bias may result from the insufficiency of annotators , this may influence the interpretation of any model's performance to some extent.

#### 4.4.3 Results on the VEATIC Dataset.

We also test our proposed method on a recently released dataset, VEATIC . Our proposed method improves the RMSE of the overall rating by \(11.79\%\). In terms of valence and arousal, our method improves \(16.44\%\) and \(5.89\%\) respectively.

### Grouping Visualization

Although our supervised training process does not utilize segmentation maps as guidance, through the learned grouping policy, we show that the semantically similar finer-scale superpixels are pooled to form coarser-scale regions at the next level. As in Figure 3(e), we can find groupings of facial regions and the object/scene regions which the character is interacting with.

Compared to traditional detection-based methods, where the emotion recognition module passively encodes the visual information selected by bounding boxes, our proposed method proactively learns which superpixels to group and aggregate. It is clear to see which regions contribute similarly to the final emotion prediction. Thus, by utilizing superpixels to enhance our understanding of characters' facial expressions, we can achieve more accurate emotion recognition.

## 5 Conclusion

In this paper, we proposed the first region-based emotion recognition method via superpixel feature pooling. It achieves state-of-the-art emotion recognition performance on VEATIC and EMOTIC datasets. It also achieves competitive results on CAER-S dataset. Moreover, the proposed method can also provide us with clusters that contain semantically similar superpixels via the learned grouping policy.