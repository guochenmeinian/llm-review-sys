ID: rSSpEmrN0C
Title: A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 7, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the LayTextLLM method for document understanding, integrating spatial layout information and text within a large language model (LLM). It encodes bounding box information into a single token interleaved with text tokens, addressing the challenge of long sequences. The experimental results indicate improved performance on Key Information Extraction (KIE) and Visual Question Answering (VQA) tasks compared to prior works, demonstrating the method's effectiveness.

### Strengths and Weaknesses
Strengths:  
- The interleaving of layout information with text is a novel approach.  
- The proposed Shuffled-OCR Supervised Fine-tuning may benefit other OCR-based methods.  
- LayTextLLM achieves state-of-the-art performance on various text-rich tasks, validating its effectiveness and reducing input length.  
- The paper is well-written, with sufficient experimental details and discussions.

Weaknesses:  
- Comparisons to existing models are unclear, raising questions about the observed performance improvements.  
- The reliance on OCR-derived text and spatial layouts limits the model's ability to incorporate visual information.  
- The exploration of the shuffling ratio is limited to KIE tasks, requiring validation on VQA datasets.  
- The effectiveness of the proposed training tasks (LNTP and SSFT) needs more ablation studies for substantiation.  
- The paper lacks comprehensive comparisons with other relevant OCR-based models and methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of comparisons by using the same LLM backbone across different methods or at least analyzing the number of parameters in each model. Additionally, it would be beneficial to clarify the OCR engine used to address performance gaps. We suggest providing a more detailed explanation of the training process, particularly regarding pre-training and SFT distinctions. The authors should also consider using positional encodings that directly represent (x, y)-positions of text to enhance layout understanding. Furthermore, we encourage the authors to include a broader range of comparisons with other OCR-based models and to conduct more ablation studies on the effectiveness of LNTP and SSFT. Lastly, addressing minor grammatical errors and clarifying terms in tables would enhance the paper's overall presentation.