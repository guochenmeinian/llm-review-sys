ID: Sp9cj4pNYD
Title: SurgicAI: A Hierarchical Platform for Fine-Grained Surgical Policy Learning and Benchmarking
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SurgicAI, a comprehensive framework for data collection and benchmarking in surgical policy learning focused on suturing. Key contributions include a set of (sub-)tasks for suturing policy training, hierarchical task decomposition for multi-stage surgical procedures, and scripts for expert trajectory collection. The authors propose a simulation environment that integrates imitation learning and reinforcement learning, benchmarking various methods and providing a clear framework for dataset collection.

### Strengths and Weaknesses
Strengths:
- The paper introduces a standardized environment for complex surgical tasks, enhancing the field of robotic learning.
- It effectively benchmarks task definitions using sensible baselines for reinforcement learning and imitation learning.
- The hierarchical decomposition of the suturing task into subtasks significantly improves performance on longer horizon tasks.
- Clear presentation of experiments and provision of expert trajectory data for the research community.

Weaknesses:
- The repository lacks clear organization and comprehensive documentation, making it difficult to map results to code implementations.
- Computational costs and wall time for training policies are not reported, which are essential for assessing resource requirements.
- The work does not validate the transferability of learned policies to real-world applications, limiting its practical implications.

### Suggestions for Improvement
We recommend that the authors improve the organization of the repository and provide comprehensive documentation, including a Dockerfile with a tutorial for training, evaluation, and demo generation to enhance reproducibility. Additionally, reporting wall time for training policies and considering domain randomization or curriculum learning could strengthen the training process. It would also be beneficial to explore the effectiveness of using dense rewards in RL benchmarks and to clarify how the simulation environment differs from prior works, particularly in terms of low-level action space and reward definitions. Lastly, we suggest discussing the main research questions that the community should investigate using this platform, given the high performance of behavior cloning on subtasks.