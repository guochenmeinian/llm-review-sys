ID: DmrIEHJxN5
Title: From Complex to Simple: Unraveling the Cognitive Tree for Reasoning with Small Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Cognitive Tree (CogTree), a methodology designed to enhance the capabilities of large language models (LLMs) in logical reasoning by employing a dual-process framework inspired by cognitive science. It features an implicit extraction module for generating potential answers and an explicit reasoning module for evaluating these answers. The experimental results indicate that CogTree improves efficiency by reducing the number of parameters needed for query resolution while maintaining accuracy. The evaluation utilized datasets such as the Entailment Bank and GSM8K, demonstrating that smaller models can outperform larger ones under certain conditions.

### Strengths and Weaknesses
Strengths:
- The paper addresses significant issues regarding LLMs' performance in reasoning tasks and their computational costs.
- It effectively incorporates cognitive science principles, enhancing understandability.
- The experimental design is robust, yielding promising results regarding accuracy and parameter efficiency.

Weaknesses:
- The method's key aspect, query decomposition, lacks detailed explanation, making it difficult to understand how sub-questions are generated.
- The novelty of the contribution is questioned, as it resembles existing methodologies like Least-to-Most prompting and Decomposition Distillation.
- The paper does not adequately address the limitations of LMs in handling higher-order logical reasoning tasks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the decomposition process, particularly in Section 3.1, by providing a detailed step-by-step example. Additionally, we suggest rephrasing lines 209-219 for better comprehension and including descriptions for the axes in Figure 5. The authors should also elaborate on the limitations section to accurately reflect the challenges faced by the methodology. Lastly, addressing the concerns regarding the generalizability of LMs in logical reasoning tasks would strengthen the paper's argument.