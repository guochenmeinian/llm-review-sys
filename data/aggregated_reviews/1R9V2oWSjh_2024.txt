ID: 1R9V2oWSjh
Title: LoReUn: Data Itself Implicitly Provides Cues to Improve Machine Unlearning
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 9, 6, 8
Original Confidences: 4, 3, 5

Aggregated Review:
### Key Points
This paper presents a novel method called **Loss-based Reweighting Unlearning (LoReUn)**, aimed at enhancing machine unlearning (MU) by dynamically reweighting data points based on their difficulty levels. LoReUn utilizes the loss of each data point on the unlearned model compared to a reference loss from the original model to guide the unlearning process. The method seeks to reduce the performance gap between approximate and exact MU methods while ensuring computational efficiency. Evaluations on image classification and generation tasks demonstrate improved results over existing baselines.

### Strengths and Weaknesses
Strengths:
1. **Innovative Approach**: LoReUn shifts the focus from traditional gradient-based methods to using data point loss values, simplifying the unlearning strategy and enhancing efficiency.
2. **Plug-and-Play Strategy**: The method can be easily integrated into existing unlearning frameworks with minimal computational overhead, making it suitable for large-scale applications.
3. **Dynamic Reweighting**: The approach maintains a balance between forgetting and retaining data, crucial for preserving model utility.
4. **Comprehensive Evaluation**: The versatility of LoReUn is demonstrated through evaluations across multiple tasks, including image classification and text-to-image generation.
5. **Reduction in Computational Cost**: The method significantly lowers computational demands by avoiding explicit gradient-based evaluations.
6. **Well Validation**: Results show that LoReUn achieves a smaller gap with exact unlearning methods compared to baselines, indicating its efficiency.

Weaknesses:
1. **Potential for Optimization**: The heuristic nature of the exponential decay and growth functions for weight adjustment lacks rigorous mathematical justification.
2. **Comparison with Diverse Baselines**: The selected baselines may not encompass the full range of state-of-the-art unlearning methods, limiting the evaluation's strength.
3. **Dependency on Loss Metrics**: Relying on the difference between current and reference loss may yield suboptimal results if the loss function inadequately captures memorization levels.
4. **Subjectivity in Weight Adjustment**: The choice of exponential functions for weight adjustment could benefit from a clearer mathematical explanation.

### Suggestions for Improvement
We recommend that the authors improve the robustness of the method by providing a more rigorous mathematical justification for the weight adjustment functions. Additionally, including a broader range of state-of-the-art unlearning methods in the evaluation would strengthen the comparisons. The authors should also address the implications of not having access to the retaining dataset in stricter unlearning scenarios. Clarifying the criteria for classifying data points as "difficult" and providing ablation results on Eq 6 would enhance the paper's clarity. Furthermore, the authors should specify the classes forgotten in "class-wise unlearning" in Table 1 and consider revising the text for improved coherence.