ID: scYa9DYUAy
Title: VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 3, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an automatic pipeline for generating captions for videos, integrating multiple modalities such as audio, video, and subtitles. The authors initially train separate captioners for audio and video, then use a pretrained LLM to aggregate these captions into a single output. The resulting dataset is employed to train a multimodal model that addresses various text-to-video/audio retrieval, generation, and question-answering tasks. Additionally, the paper introduces the VAST-27M dataset, which combines audio, video, subtitles, and generated captions, demonstrating the effectiveness of both the dataset and the model through extensive experiments.

### Strengths and Weaknesses
Strengths:
- The work confirms that unimodal caption generation models serve as strong baselines, reducing the need for costly multimodal data collection.
- The downstream experiments are comprehensive and yield promising results, achieving state-of-the-art performance on 22 tasks.

Weaknesses:
- The paper is difficult to follow due to poor naming conventions for components and abbreviations.
- The contribution is highly incremental, relying heavily on existing pretrained models with minimal technical novelty.
- There is insufficient discussion on the interaction of the proposed loss function components and their impact on learned representations.
- The dataset generation relies on the capabilities of pretrained models, raising questions about the fairness of comparisons with state-of-the-art methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by refining the naming conventions for components and abbreviations. Additionally, the authors should provide a more detailed discussion on the novelty of their contributions, particularly in relation to existing models. We suggest that the authors clarify how different components of the loss function interact and affect the quality of representations. Furthermore, it would be beneficial to address the potential biases in the pretrained models and their implications for the dataset's quality. Lastly, we encourage the authors to include a comparison of the performance of their model with and without the VAST-27M dataset to better illustrate its impact.