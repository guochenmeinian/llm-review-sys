ID: 5eu00pcLWa
Title: Quantification of Uncertainty with Adversarial Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 4, 8, 3, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Quantification of Uncertainty with Adversarial Models (QUAM), a novel method for estimating epistemic uncertainty in deep learning. The authors argue that traditional methods like Deep Ensembles and variational inference underestimate uncertainty by sampling from limited posterior modes. QUAM introduces adversarial models that differ in predictions from a reference model while fitting the training data, using mixture importance sampling to enhance uncertainty estimation. Empirical results demonstrate QUAM's effectiveness in out-of-distribution (OOD) detection and suggest that it outperforms traditional methods in estimating uncertainty. The authors also discuss the calibration of QUAM, noting that while it was not explicitly designed for calibration, it shows empirical calibration benefits, particularly when evaluated on the ImageNet dataset.

### Strengths and Weaknesses
Strengths:
- The empirical results on OOD detection are promising and indicate that QUAM outperforms existing methods.
- The paper is well-written, with design choices supported by existing literature, and the experiments are reproducible and robust.
- The introduction of adversarial models as a new concept for uncertainty quantification is significant.
- The authors effectively address reviewer concerns regarding clarity and related works, enhancing the paper's overall quality.

Weaknesses:
- The calibration of the resulting uncertainty is unclear; the authors should address whether QUAM provides well-calibrated uncertainty estimates.
- The argument regarding the underperformance of ensembles and Bayesian Neural Networks (BNNs) is not sufficiently clear, particularly the claim that they miss important posterior modes.
- There is a lack of comparison with other ensemble methods, raising questions about the contributions of the mixture of Gaussians versus adversarial search.
- Important definitions and implementation details are introduced too late in the paper, hindering comprehension.
- The description of aleatoric uncertainty is deemed conflicting and potentially misleading, as it mixes aleatoric and epistemic uncertainties without clear distinction.
- There is a suggestion that all baselines should be predicted uniformly for better comparison, which the authors acknowledge but have not yet implemented.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing an earlier definition of adversarial models in both the abstract and introduction. Additionally, the authors should include essential implementation details, such as the number of adversarial models and MC samples, in the main text. We suggest conducting an ablation study on the number of adversarial models to clarify their impact on performance. Furthermore, the authors should explain why the search algorithm may not always converge to the same solution and consider enforcing diversity among solutions to enhance performance. Lastly, we recommend improving the clarity of the section discussing aleatoric uncertainty, as it may mislead future readers, and consider removing or revising this section to avoid confusion. Additionally, we suggest that the authors explore predicting all baselines uniformly on a per-test-sample basis to facilitate more sensible comparisons with QUAM.