ID: 9B6J64eTp4
Title: Articulate your NeRF: Unsupervised articulated object modeling via conditional view synthesis
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 2, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an unsupervised method for learning the pose and part-segmentation of articulated objects with rigid parts through conditional view synthesis. The authors propose a two-stage approach where a static NeRF is fitted to an object's initial state, followed by an optimization process that alternates between pose estimation and part segmentation. The method utilizes a 3D voxel grid heuristic to aid in part assignment and aims to achieve high-quality results through a decoupled optimization process.

### Strengths and Weaknesses
Strengths:
- The pipeline is end-to-end and completely unsupervised, which is a significant advantage.
- The use of 3D voxels instead of meshes offers a novel approach.
- The method shows promising quantitative and qualitative results, particularly in view synthesis and pose estimation.

Weaknesses:
- The novelty and contribution of the work are questioned, particularly in comparison to the PARIS method, with concerns that the method regresses by separating pose and segmentation optimization.
- The segmentation results are still noisy, and artifacts are present in the visualizations.
- The voxel grid refinement appears to be a simple engineering post-processing step rather than a robust solution.
- There are discrepancies in reported experimental results compared to existing methods, and the method is limited to rigid deformations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation, particularly in explaining the stage-wise training schema and the details of the optimization process. Providing a more detailed explanation of Figure 3 and addressing the formula in Lines 167-168 would enhance understanding. Additionally, we suggest that the authors compare their method against a broader range of state-of-the-art articulated object reconstruction methods, not limited to NeRF-based approaches. Addressing the robustness of the method for finding U (Line 170) is crucial, as any issues could compromise subsequent operations. Finally, discussing the efficiency of the proposed method in terms of training and inference times, as well as the impact of the number of iterations at each stage, would strengthen the paper.