ID: LMU2RNwdh2
Title: An Inverse Scaling Law for CLIP Training
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 7, 8, 3, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of training efficiency in CLIP-style vision-language models by investigating the impact of token length on performance. The authors propose that larger models can maintain performance with shorter token sequences, termed the "inverse scaling law." They evaluate various token reduction strategies, including random token dropping and grid masking, and find that reducing image resolution yields the best results. The study claims that these methods allow for training models that match or exceed OpenCLIP performance while significantly reducing compute requirements. Additionally, the paper analyzes CLIPA models in comparison to OpenCLIP models, revealing inconsistent performance across various datasets in the VTAB benchmarks. The authors assert that CLIPA achieves competitive results despite being trained with limited resources and emphasize the novel inverse scaling law, although the framing of this law is questioned due to limited data points on the x-axis and the necessity of special techniques for H-14 models.

### Strengths and Weaknesses
Strengths:
- The work is well-motivated, addressing the high costs of training foundation models and proposing significant reductions in training time.
- The authors have developed a new state-of-the-art open-source model, CLIPA, which shows competitive performance under constrained computational resources.
- The experiments are comprehensive and clearly presented, with valuable comparisons of different token reduction methods.
- The claim regarding "vision embeddings" not requiring long sequence lengths in text embedding is well-framed and valuable for publication.
- The findings regarding the inverse scaling law are intriguing and could have a substantial impact on future research in vision-language models.

Weaknesses:
- The novelty of the work is questionable, as it closely resembles findings from the concurrent paper "RECLIP," which also discusses token reduction strategies.
- Performance comparisons between CLIPA and OpenCLIP appear random, with significant drops in retrieval performance noted.
- The evaluation is limited to zero-shot classification and retrieval, neglecting other important tasks that CLIP is typically used for, such as linear probing and fine-tuning.
- The explanation of hyperparameter tuning lacks clarity, particularly regarding the selection process.
- The limited discussion of results at only three points on the x-axis raises concerns about the robustness of the inverse scaling law claim.
- The presentation contains inaccuracies and polemical statements that misrepresent the impact of CLIP on accessibility and the definition of "academic resources."

### Suggestions for Improvement
We recommend that the authors improve the discussion of their work in relation to the concurrent "RECLIP" paper, including a thorough comparison and acknowledgment of its findings. Additionally, we suggest expanding the evaluation to include a wider range of tasks, such as linear probing and fine-tuning, to better assess the model's general representation quality. The authors should clarify the definition of the "inverse scaling law" and expand the analysis to include more data points on the x-axis to strengthen the claims made. Furthermore, we advise addressing the significant drop in retrieval performance more comprehensively and enhancing the clarity of the hyperparameter tuning process, specifically detailing how parameters were selected. Lastly, addressing the inaccuracies in the presentation regarding CLIP's accessibility and the characterization of academic resources, as well as enhancing the clarity of figures and ensuring consistent terminology throughout the paper, would improve overall comprehension.