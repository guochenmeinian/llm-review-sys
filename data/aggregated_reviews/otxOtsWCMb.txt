ID: otxOtsWCMb
Title: From an Image to a Scene: Learning to Imagine the World from a Million 360Â° Videos
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a large-scale, real-world, multiview, 360-degree outward dataset, termed 360-1M, designed for static novel view synthesis and 3D reconstruction. The dataset comprises over 380 million image pairs derived from 1 million 360-degree YouTube videos. To facilitate data collection, the authors develop a pipeline utilizing Dust3R for identifying corresponding frames, which enhances scalability. The authors propose a diffusion-based model, ODIN, that achieves state-of-the-art performance in novel view synthesis by incorporating motion masking and viewpoint conditions.

### Strengths and Weaknesses
Strengths:
- The dataset is significantly larger and more diverse than existing datasets for static novel view synthesis, making it crucial for modern data-driven methods.
- The innovative pipeline for data collection enhances scalability and efficiency in identifying corresponding frames.
- The ODIN model demonstrates superior performance in generating long-range novel views and realistic 3D reconstructions.

Weaknesses:
- The dataset's reliance on pre-captured 360-degree YouTube videos limits viewpoint diversity, as the camera trajectory is controlled by the operator.
- The quality of the data is inconsistent, with potential instability across different data points, raising concerns about the impact of low-quality frames.
- Claims regarding the sufficiency of 1 fps lack experimental support, and the effects of varying fps rates need further investigation.
- The motion masking method lacks thorough analysis, particularly regarding the hyperparameter's significance and its impact on synthesizing dynamic objects.

### Suggestions for Improvement
We recommend that the authors improve the dataset's robustness by discussing the impact of low-quality data and ensuring no overlap between training and testing sets. Additionally, we suggest conducting experiments to validate the claim that 1 fps is sufficient by comparing performance across different fps rates. The authors should provide a more detailed analysis of the motion masking hyperparameter in Eq. 3 and its tuning process. Furthermore, including qualitative examples of generated images and comparisons with existing methods would enhance the paper's comprehensiveness. Lastly, clarifying the role of Dust3R in 3D reconstruction and addressing discrepancies in performance metrics would strengthen the findings.