ID: kuCY0mW4Q3
Title: VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 8, 5, 7, 8, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents VB-LoRA, a parameter-efficient method for fine-tuning large language models (LLMs) by utilizing a shared vector bank to construct adapter parameters. The authors demonstrate that VB-LoRA achieves comparable or superior performance to existing methods like LoRA, VeRA, and Tied-LoRA while significantly reducing the number of trainable parameters. The approach is validated across multiple benchmarks (GLUE, E2E, and MT-Bench) and model families (RoBERTa, GPT-2, and Llama2). The paper includes ablation studies to analyze the impact of vector selection strategies and sub-vector lengths.

### Strengths and Weaknesses
Strengths:
- Proposes a novel and effective method for reducing parameter count in LoRA, enhancing parameter efficiency while maintaining performance.
- The use of a shared vector bank and differentiable top-k softmax is innovative and allows for end-to-end training.
- Comprehensive evaluation across various tasks and models, showcasing the method's applicability.

Weaknesses:
- The paper lacks clarity in some sections, particularly regarding the complexity of the proposed method and its relation to existing theoretical works.
- Evaluation is not comprehensive; it relies on limited datasets and models, particularly for instruction-tuned and NLU tasks.
- The scalability of VB-LoRA is questionable, as increasing the bank size or vector length does not yield significant performance improvements.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper, especially in Section 3, by simplifying the explanation of VB-LoRA and its theoretical underpinnings. Additionally, we suggest conducting experiments on a broader range of datasets and models to enhance the comprehensiveness of the evaluation, particularly for instruction-tuned tasks. It would also be beneficial to explore the scalability of VB-LoRA further by including ablation studies with larger ranks and discussing the implications of vector selection on performance. Lastly, addressing the limitations regarding the model's performance on knowledge-intensive tasks in the limitations section would strengthen the paper.