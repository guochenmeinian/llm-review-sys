ID: 5x5Vxclc1K
Title: SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SMoP, a mixture-of-expert method aimed at parameter-efficient fine-tuning (PEFT) through the use of short soft prompts combined via a gating mechanism. The authors argue that while longer soft prompts improve performance, they also increase computational costs. SMoP addresses this inefficiency by tuning multiple short prompts jointly. Experiments conducted on two T5 models demonstrate an improved cost-performance trade-off across various SuperGLUE datasets.

### Strengths and Weaknesses
Strengths:
- The methodology is reasonable and effectively addresses inefficiencies associated with longer soft prompts.
- The proposed method is largely plug-and-play for various soft prompt tuning methods, enhancing its applicability.
- Experiments, including ablation studies, are well-executed and bolster confidence in the method's efficacy.

Weaknesses:
- The relation to previous works, particularly AdaMix, is insufficiently discussed, despite notable similarities.
- Comparisons with other lightweight methods like LoRA are lacking, raising questions about the method's relative performance.
- Important baselines such as SPoT, Attempt, and MPT are not included, limiting the novelty and soundness of the work.
- The method requires additional hyper-parameter tuning, which may increase training costs and complexity.

### Suggestions for Improvement
We recommend that the authors improve the discussion of the relationship between SMoP and AdaMix, particularly regarding the similarities in methodology. Additionally, we suggest including comparisons with other lightweight methods like LoRA to clarify the advantages of SMoP. It would also be beneficial to incorporate important baselines such as SPoT, Attempt, and MPT to enhance the soundness of the work. Finally, addressing the hyper-parameter tuning concerns and providing empirical evidence for the method's orthogonality to other prompt tuning methods would strengthen the paper.