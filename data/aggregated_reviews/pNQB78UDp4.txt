ID: pNQB78UDp4
Title: CVPT: Cross-Attention help Visual Prompt Tuning adapt visual task
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 4, 4, 3, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Cross Visual Prompt Tuning (CVPT) as an improvement over the widely-used Visual Prompt Tuning (VPT) method. The authors analyze the limitations of VPT, particularly its reliance on prompt tokens that lack rich semantic information in visual tasks, leading to computational inefficiency and distortion of self-attention. CVPT introduces a cross-attention module to effectively decouple prompt tokens from embedded tokens, thereby addressing these issues and enhancing model performance in downstream tasks.

### Strengths and Weaknesses
Strengths:
1. Good performance on image classification and semantic segmentation tasks.
2. Comprehensive analysis of the weaknesses of prompt-based methods and VPT.
3. Introduction of a cross-attention module to mitigate the issues associated with prompt-based methods.
4. Effective comparison with VPT demonstrating the advantages of CVPT when using multiple prompts.

Weaknesses:
1. Lack of experimental evidence supporting the claim that prompts in visual tasks lack representation information, leading to ambiguity in the motivation.
2. No experiments establish the relationship between weight distribution and model performance, necessitating further discussion.
3. Cross-attention should be positioned as a preliminary concept rather than a primary contribution.
4. Insufficient discussion regarding E2VPT, which is closely related to the proposed method.
5. Inconsistencies in experimental setup, particularly in the analysis of self-attention weights without comparative studies involving CVPT.
6. Limited demonstration of CVPT's robustness across other hierarchical transformer architectures, such as Swin.

### Suggestions for Improvement
We recommend that the authors improve the experimental validation of the claim regarding the lack of representation information in visual prompts by providing relevant citations and evidence. Additionally, we suggest including experiments that correlate weight distribution with model performance to clarify the implications of prompt weights. The authors should also consider repositioning the discussion of cross-attention in the paper and enhance the analysis of E2VPT. Furthermore, we advise including comparative studies in the experimental setup to support claims about self-attention weights and demonstrating CVPT's performance across various hierarchical architectures. Lastly, more detailed implementation information would enhance the clarity of the methodology.