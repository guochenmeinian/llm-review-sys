ID: l5FDMofecw
Title: OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 7, 8, 6, 7
Original Confidences: 4, 4, 4, 3

Aggregated Review:
### Key Points
This paper presents OpenMathInstruct-2, a significant advancement in mathematical reasoning for LLMs through the introduction of a large, high-quality open-source dataset. The authors demonstrate strong performance improvements supported by extensive experiments, including data scaling laws for supervised fine-tuning (SFT) and the robustness of SFT. The dataset, containing over 14 million question-answer pairs, addresses limitations of previous smaller datasets and enhances open-source AI development in this domain.

### Strengths and Weaknesses
Strengths:
- The dataset is generated using the Llama 3-405B model, ensuring reproducibility.
- The paper provides significant performance improvements and contributes to open-source resources.
- The methodology is robust, with thorough experimental setups and clear evaluations on multiple benchmarks.
- The introduction and evaluation methodology are well-structured and logically presented.

Weaknesses:
- There is limited exploration of dataset quality metrics, lacking insights into the diversity of questions and depth of reasoning.
- The paper does not provide a stopping criterion for SFT, which would be valuable for practical applications.
- There is a lack of comprehensive comparisons to closed-source models, limiting the context for performance interpretation.
- The rationale for the specific size of the generated dataset is absent, and the impact of varying dataset sizes is not discussed.

### Suggestions for Improvement
We recommend that the authors improve the exploration of dataset quality metrics by analyzing the diversity of questions and the depth of reasoning required. Additionally, providing guidance on a stopping criterion for SFT would enhance practical applicability. We also suggest including comprehensive comparisons to closed-source models to contextualize performance gains. Finally, discussing the rationale for the dataset size and the potential effects of scaling it could further strengthen the paper.