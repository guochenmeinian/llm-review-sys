ID: 42Cc5s71zl
Title: D$^2$TV: Dual Knowledge Distillation and Target-oriented Vision Modeling for Many-to-Many Multimodal Summarization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Many-to-Many Multimodal Summarization (M$^3$S) task, which aims to generate summaries across languages using document inputs and corresponding image sequences. The authors propose a dual knowledge distillation and target-oriented vision modeling framework (D2TV) that enhances both multimodal monolingual summarization (MMS) and multimodal cross-lingual summarization (MXLS). The paper introduces the M$^3$Sum dataset, comprising 44 languages, to facilitate future research. Experimental results demonstrate state-of-the-art performance, although the evaluation is limited to four languages.

### Strengths and Weaknesses
Strengths:
- The introduction of the M$^3$S task and the corresponding M$^3$Sum dataset is significant for the field.
- The D2TV framework effectively combines knowledge distillation and visual feature filtering, addressing key challenges in multimodal summarization.
- The paper is well-written and presents thorough experimental results, including both automated and human evaluations.

Weaknesses:
- The motivation for the Dual Knowledge Distillation module and the target-oriented contrastive objective is insufficiently explained, lacking vivid examples and comprehensive analysis.
- The evaluation on the M$^3$Sum dataset is limited to four languages, raising concerns about the generalizability of the results.
- Comparisons with existing work are inadequate, and the significance of visual features in the summarization task is questionable.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind the Dual Knowledge Distillation module and the target-oriented contrastive objective by providing detailed explanations and relevant examples in the Introduction. Additionally, we suggest revising Figure 2 for clarity and ensuring that the DKD and TCO modules are adequately represented. The authors should also expand the evaluation on the M$^3$Sum dataset to include a broader range of languages to enhance the reliability of the results. Finally, a more comprehensive comparison with existing literature, such as "CFSum: A Coarse-to-Fine Contribution Network for Multimodal Summarization," would strengthen the paper's positioning within the field.