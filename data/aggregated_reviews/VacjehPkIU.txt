ID: VacjehPkIU
Title: Superlim: A Swedish Language Understanding Evaluation Benchmark
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark for Swedish language models, inspired by GLUE and SuperGLUE, detailing the features of the benchmark, dataset collection, and annotation. The authors propose a new set of NLP tasks for evaluating general-purpose language models and a feature-rich leaderboard for tracking progress. They also introduce an evaluation measure that scores tasks on a uniform scale and a new documentation paradigm for datasets.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents significant progress in evaluating Swedish models.
- The range of selected tasks is broad and well-maintained, supporting the need for more evaluation data.
- The thoughtful design of the leaderboard and dataset documentation addresses the needs of various stakeholders.
- The tasks and datasets are described with sufficient detail.

Weaknesses:
- Concerns regarding the reliability of gold data creation and inter-annotator agreement, particularly for word-level tasks.
- Potential underestimation of the negative impact of using post-edited test data.
- Missing details on the cultural adaptation of data and the qualifications of native speakers involved in translations.

### Suggestions for Improvement
We recommend that the authors improve the description of the gold data creation process, including inter-annotator agreement metrics for all tasks. Specifically, for tasks like anon-texttask-8, anon-wordtask-1, anon-wordtask-2, and anon-wordtask-4, provide clarity on the number of annotators and their agreement levels. Additionally, we suggest including more details on the cultural adaptation process and the qualifications of native speakers involved in translations. Lastly, consider addressing the potential risks of data leakage and clarify the values presented in Table 3.