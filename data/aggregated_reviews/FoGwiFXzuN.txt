ID: FoGwiFXzuN
Title: How Far Can Transformers Reason? The Globality Barrier and Inductive Scratchpad
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 5, 7, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new metric for measuring the difficulty of tasks for transformer models, termed distribution locality, which quantifies the amount of input required for a model to accurately predict the corresponding output. The authors introduce the concept of a locality barrier, indicating that tasks exceeding a certain locality threshold become unlearnable for transformers. To address this barrier, the authors propose various scratchpads, particularly an inductive scratchpad, which enables models to iteratively apply learned rules to solve problems beyond their training distribution. The paper also investigates the conditions under which transformers can learn algorithms with length-generalization and provides a formal definition of distribution locality, supported by theoretical and empirical results. Additionally, the authors propose a theoretical framework involving conjectures and theorems related to transformer models and their use of scratchpads, including a method for defining correctness in the context of scratchpad entries.

### Strengths and Weaknesses
Strengths:
- The paper offers strong theoretical contributions and addresses fundamental issues in transformer learning.
- It defines a coherent metric, distribution locality, which intuitively explains the limits of transformers and aids in predicting future capabilities.
- The inductive scratchpad concept is novel and potentially applicable across various language model tasks.
- The clarity of mathematical formulations and the demonstration of the locality problem enhance the paper's impact.
- The authors provide careful responses to reviewer questions, indicating a willingness to improve the paper.
- The proposed definitions and conjectures are relevant to the field and show potential for advancing understanding of transformer models.

Weaknesses:
- The language used to describe ideas is occasionally awkward, affecting readability.
- Visual representation of scratchpad outputs is difficult to parse, lacking consistency in notation.
- The paper does not provide clear implementation steps for applying the inductive scratchpad to other problems.
- There is insufficient discussion on the scalability of the proposed methods and their applicability to real-world tasks.
- The presentation lacks clarity, particularly in section 2, where definitions could be simplified.
- Remark 2 is loosely connected to the main content and may benefit from being moved to a separate discussion section or supplemental materials.
- Conjecture 2 is difficult to grasp without additional context.

### Suggestions for Improvement
We recommend that the authors improve the clarity of language throughout the paper, focusing on precise sentence construction to enhance readability. Additionally, providing consistent visual representations of scratchpad outputs would facilitate comparison and understanding. Including real-world task examples could strengthen the claims regarding the inductive scratchpad's efficacy. We suggest that the authors elaborate on the implementation steps for the inductive scratchpad in various transformer applications and discuss the scalability of their approach in greater detail. To enhance the presentation, we recommend using shortened definitions and placing extended versions in an appendix. We also suggest creating a separate discussion section for Remark 2 or relocating it to supplemental materials. To clarify Conjecture 2, we advise breaking it down into a definition followed by the conjecture/theorem. Lastly, we encourage the authors to clarify the necessity of the label names in Theorem 1 to improve the theorem's readability.