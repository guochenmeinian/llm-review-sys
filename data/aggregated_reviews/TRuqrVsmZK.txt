ID: TRuqrVsmZK
Title: BayesTune: Bayesian Sparse Deep Model Fine-tuning
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 5, 7, 5, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents BayesTune, a method for selecting parameters to fine-tune in pre-trained models using Bayesian inference. The authors employ a Laplace prior over model weights and a hyper Gamma prior for the scale, utilizing Langevin dynamics for posterior inference. The method's effectiveness is demonstrated through experiments in Computer Vision and NLP tasks, showing competitive performance against existing techniques. The authors also detail their process for selecting hyperprior values $\alpha=0.01$ and $\beta=100$ based on the Gamma distribution's statistical properties, noting that while significant performance variations occur with large changes in these values, sensitivity is minimal near optimal points. Empirical execution times for fine-tuning tasks on a Tesla-V100 GPU indicate comparable performance to other methods like LoRA.

### Strengths and Weaknesses
Strengths:
- The problem of efficient fine-tuning is highly relevant given the prevalence of large pre-trained models.
- The method is intuitive and interpretable, grounded in a Bayesian framework.
- A comprehensive set of experiments supports the method's utility.
- The authors provide a clear rationale for the selection of hyperprior values and cut-off points, enhancing the transparency of their methodology.
- Empirical execution times for fine-tuning tasks are presented, allowing for a better understanding of the model's efficiency.

Weaknesses:
- The approach closely resembles existing work like SP-regularization, and the authors should compare BayesTune against it to clarify its novelty.
- The efficiency of the method is unclear; sparse updates in attention layers may not significantly reduce computation time, and the authors should clarify this in the paper.
- There is an inconsistency between Algorithm 1 and the method described in the text, indicating that Algorithm 1 may be incomplete.
- Performance appears competitive for NLP but less so for Computer Vision, as indicated in Table 2.
- The choice of hyperprior values lacks extensive empirical validation across different tasks, which may raise questions about their generalizability.
- The complexity of using variational inference (VI) versus Stochastic Gradient Langevin Dynamics (SGLD) is not fully addressed, potentially leaving gaps in understanding the trade-offs involved.

### Suggestions for Improvement
We recommend that the authors improve the comparison with related work, particularly SP-regularization, to clarify the novelty of BayesTune. Additionally, the authors should specify the computational efficiency of sparse updates in attention layers and provide empirical measures of time and memory consumption. Clarifying the discrepancies in Algorithm 1 and the text is essential for coherence. The authors should also consider restructuring the related work section for better flow and include discussions on limitations and hyperparameter choices. Furthermore, we suggest improving the empirical validation of hyperprior values across various tasks to demonstrate their robustness, and providing a more detailed comparison of the complexities associated with variational inference and SGLD, clarifying the rationale behind the chosen approach. Lastly, more concise explanations in the abstract and tables would enhance readability.