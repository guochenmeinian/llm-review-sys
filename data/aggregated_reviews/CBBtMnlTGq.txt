ID: CBBtMnlTGq
Title: Neural Data Transformer 2: Multi-context Pretraining for Neural Spiking Activity
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 5, 7, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the 'Neural Data Transformer 2' (NDT2), a model designed to integrate neural intracortical recordings across sessions, tasks, and subjects by applying attention mechanisms across space and time. The authors demonstrate that cross-session/task/subject pretraining enhances decoding performance with minimal finetuning data, drawing parallels to advancements in the language domain. The NDT2 model shows promise for generalizing across contexts in systems neuroscience, potentially leading to the development of neural 'foundation models' for brain-computer interface (BCI) applications and fundamental neuroscience research.

### Strengths and Weaknesses
Strengths:  
- NDT2 effectively utilizes large-scale unsupervised pretraining, allowing for the aggregation of neural data from diverse contexts and rapid adaptation to new environments.  
- The model demonstrates high-performance decoding capabilities in both monkey reaching behavior and human iBCI applications, showcasing its utility in clinical settings.  
- The paper contributes significantly to the BCI field by applying techniques from machine learning to improve generalizability and performance in real neural datasets.  

Weaknesses:  
- The study is limited by the small amount of data, primarily from a few monkey tasks and a single human dataset, which may lead to overfitting.  
- The presentation lacks clarity in several areas, including the description of the model, the use of context tokens, and the specifics of the pretraining methodology.  
- The computational efficiency of the full attention mechanism is questioned, and the method's practical applicability for iBCI usage remains unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their presentation by expanding figure legends and providing detailed explanations of the model architecture and methodology. Specifically, clarify the definition and role of context tokens, and ensure that the distinctions between NDT1 and NDT2 are explicitly stated. Additionally, we suggest optimizing the computational efficiency of the model to address concerns regarding the full attention mechanism's costs. To enhance the robustness of the findings, consider aggregating data across multiple experiments and providing a clearer rationale for the choice of pretraining datasets. Finally, we encourage the authors to include precise mathematical descriptions and pseudocode to facilitate reproducibility and understanding of the NDT2 model.