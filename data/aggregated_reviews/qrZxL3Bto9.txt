ID: qrZxL3Bto9
Title: Evaluating language models as risk scores
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 7, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents folktexts, a software package designed to evaluate the calibration of large language models (LLMs) using datasets derived from the American Community Survey (ACS). The package features an extensible API for testing various models, metrics, and prompting strategies, and it demonstrates that while LLMs exhibit strong predictive abilities, their outputs are often miscalibrated. The evaluation employs a prompting QA setting, measuring calibration through the Expected Calibration Error (ECE) and includes a large-scale analysis of multiple language models. Enhancements to the folktexts framework include compatibility with closed-source LLMs via web APIs and the introduction of a numeric risk prompting scheme for computing risk scores, which the authors propose improves calibration for instruction-tuned models, albeit with a small decrease in predictive power (AUC). Results for the GPT-4o-mini model are included, demonstrating high risk score predictive power and miscalibration, along with discussions on the implications of these findings across various tasks and datasets.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and provides clear explanations of the methodology and tools included in the folktexts package.  
- It offers valuable insights into LLM calibration, revealing significant miscalibration issues despite strong predictive capabilities.  
- The package features an easy-to-use API and allows for subgroup calibration evaluation, which is important for assessing algorithmic fairness.  
- The integration of web API support for closed-source LLMs enhances usability.  
- The numeric risk prompting scheme shows improved calibration for instruction-tuned models.  
- Comprehensive results and analyses are provided, including fairness assessments and comparisons across multiple models.  

Weaknesses:  
- The scope of the evaluation is limited to ACS data, raising questions about the generalizability of the findings to other datasets and real-world applications.  
- The documentation is insufficient, lacking clarity on the intended scope and usage, and the package does not adequately support fine-tuning or a wider range of models.  
- The evaluation metrics and design choices, such as the exclusive use of ECE, require further justification and could benefit from additional uncertainty quantification methods.  
- The numeric prompting scheme leads to a reduction in predictive power (AUC).  
- The scope of datasets evaluated may be limited, raising concerns about generalizability.  
- The paper does not fully leverage the potential of indefinite answers in its evaluation protocol.  

### Suggestions for Improvement
- We recommend that the authors improve the documentation to clarify the intended scope and usage of the package, including detailed explanations of the included features and how to interpret outputs.  
- The authors should consider expanding the evaluation to include other popular QA datasets to enhance the generalizability of their findings.  
- We suggest that the authors provide suggestions on how to add support for models from providers like OpenAI and Anthropic, as well as implement additional uncertainty quantification methods, such as confidence intervals or conformal prediction methods.  
- The authors should address the limitations of the current instruction-tuning processes and explore advanced calibration techniques to improve model predictions.  
- We recommend that the authors clarify the rationale behind their choice of ECE as the primary metric and consider integrating fairness evaluation efforts to ensure equitable model performance across different population groups.  
- Additionally, the authors should expand the analysis of the numeric prompting scheme to better understand its impact on predictive performance across different model types.  
- Finally, we suggest enhancing the evaluation protocol to fully utilize the benefits of indefinite answers, which could provide deeper insights into model calibration and performance.