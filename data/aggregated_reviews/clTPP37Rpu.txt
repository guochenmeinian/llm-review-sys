ID: clTPP37Rpu
Title: Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive knowledge evaluation framework (CONNER) designed to assess the knowledge generation capabilities of large language models (LLMs). CONNER incorporates both intrinsic evaluations—focusing on the quality of generated knowledge—and extrinsic evaluations that consider the applicability of this knowledge to downstream tasks such as question answering and knowledge-grounded dialogue. The authors propose automatic metrics for six evaluation aspects: Factuality, Relevance, Coherence, Informativeness, Helpfulness, and Validity, and they evaluate generated knowledge from three LLMs (Flan-T5, Llama, and ChatGPT) as well as text from a retrieval system (DPR). The findings indicate that while LLM-generated text is less factual, it is more relevant than retrieved text, and this relevance is crucial for downstream tasks.

### Strengths and Weaknesses
Strengths:
- The paper is the first to propose an automatic evaluation framework for LLM-generated knowledge, addressing a significant gap in prior research.
- It provides thorough evaluations and interesting observations regarding the impact of generated knowledge on downstream tasks.
- The authors conduct a sufficient number of manual experiments, demonstrating a high correlation between manual and automatic evaluation scores.

Weaknesses:
- Some metrics lack clear explanations and grounding in prior research, raising concerns about their validity and applicability.
- The evaluation framework may not generalize well to languages other than English due to reliance on specific assistant models.
- The paper does not validate the effectiveness of the proposed metrics before applying them to LLMs, which is essential for establishing their reliability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the metrics by providing detailed explanations and justifications for their use, particularly for the Factuality and Relevance metrics. Additionally, it is crucial to validate the proposed metrics against ground-truth knowledge before application to ensure their effectiveness. We suggest incorporating comparisons with established evaluation measures to strengthen the validity of the findings. Furthermore, addressing the generalizability of the framework to other languages and providing more robust baselines for the proposed metrics would enhance the paper's contributions. Lastly, we encourage the authors to clarify the relationship between the Helpfulness score and the baseline used, as well as to correct any typographical errors throughout the manuscript.