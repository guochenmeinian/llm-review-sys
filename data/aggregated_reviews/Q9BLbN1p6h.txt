ID: Q9BLbN1p6h
Title: Mixture-of-Linguistic-Experts Adapters for Improving and Interpreting Pre-trained Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method that integrates linguistic structure injection with parameter-efficient fine-tuning (PEFT) of pre-trained language models, utilizing parallel adapter modules to encode various linguistic structures through a Mixture-of-Linguistic-Experts architecture. The method prunes experts based on importance scores to manage the parameter budget. Results indicate that the proposed approach outperforms baselines on high-resourced datasets, with an analysis of expert selection providing insights into the effectiveness of linguistic knowledge at different layers.

### Strengths and Weaknesses
Strengths:
- The incorporation of linguistic knowledge into PLMs using adapter structures is well-motivated and novel.
- The study of the necessity of linguistic knowledge through an Edgeless Graph expert is intriguing, challenging the assumption that such knowledge is always beneficial.
- The paper is well-written, clear, and easy to follow, with gains over baselines on various tasks.

Weaknesses:
- Experimental results lack robustness; improvements of less than 1% are not convincingly significant without reporting standard deviations.
- The absence of a comparison with full fine-tuning limits the evaluation of the proposed method's effectiveness.
- An ablation study is missing, which is crucial for understanding the impact of different linguistic information sources and the expert pruning component.
- The methods have only been tested on English, raising questions about applicability to lower-resource languages.

### Suggestions for Improvement
We recommend that the authors improve the experimental results by reporting standard deviations in Table 2 to validate their claims. Additionally, a comparison with full fine-tuning should be included to assess the benefits of the proposed linguistic knowledge integration. Conducting an ablation study to clarify the contributions of various linguistic structures and the Gumbel-Softmax mechanism is essential. Lastly, expanding Section 4 to provide more detail on the analysis would enhance the paper's depth.