ID: gAP52Z2dar
Title: Inverse Preference Learning: Preference-based RL without a Reward Function
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel algorithm, inverse preference learning (IPL), which learns a policy directly from offline pairwise preference data without the need to learn a separate reward function. The authors argue that this approach simplifies the traditional two-step process of reward inference followed by policy learning. They demonstrate that IPL performs comparably or better than existing methods across various simulated robotics tasks. Additionally, the paper provides an enhanced evaluation of the IPL method through experiments on a preference benchmark, introducing three new baselines/ablations: 1) IPL + XQL, which achieves comparable performance to IPL + IQL but suffers from instability on walker2d-expert; 2) IPL Without Reg, which highlights the importance of regularization; and 3) BREX, which learns a reward function using IQL for consistency. The results indicate that IPL can achieve similar performance to other methods without learning a reward network, emphasizing its robustness and the necessity of regularization.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a straightforward and innovative method for preference-based reinforcement learning, eliminating the need for an intermediate reward function.
2. The experimental results indicate that IPL outperforms or matches the performance of more complex methods with numerous hyperparameters.
3. The authors provide comprehensive experimental results that address previous concerns about empirical evaluation and demonstrate the robustness of IPL across different RL algorithms.
4. The authors have made their source code available, enhancing reproducibility and accessibility.

Weaknesses:
1. The theoretical foundations of the method are inadequately addressed, particularly regarding the optimality of the learned policy and the implications of using a large regularization weight.
2. The novelty of the approach is questionable, as some ideas presented have been previously introduced in prior work, particularly regarding the interchangeability of reward and Q functions.
3. The comparison with prior works on reward learning from preferences is limited, and the experimental setup lacks sufficient baselines.
4. There are concerns regarding the instability of performance in certain environments, particularly with walker2d-expert.
5. The presentation of results in tables is inconsistent, potentially leading to confusion regarding the performance metrics reported.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for their method, particularly by providing proof that the learned policy is optimal for the implicit Q function. Additionally, clarifying how the large regularization weight affects the combined objective would enhance understanding. We suggest that the authors explicitly acknowledge prior works that have explored similar insights regarding reward and Q functions to clarify the novelty of their contribution. Addressing the performance instability in specific environments, such as walker2d-expert, would strengthen the overall robustness of the findings. Expanding the experimental comparisons to include more baselines from the literature on reward learning from preferences would also strengthen the empirical validation of their method. Finally, we recommend revising the presentation of results in tables for consistency, ensuring that both final and best success rates are reported uniformly.