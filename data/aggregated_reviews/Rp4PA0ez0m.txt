ID: Rp4PA0ez0m
Title: Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 5, 5, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Transfer Sequential VAE (TranSVAE) framework aimed at addressing unsupervised domain adaptation (UDA) for action recognition by disentangling spatial and temporal domain divergences. The authors propose two sets of latent factors: static information encoding and dynamic information encoding, facilitating the adaptation process through adversarial learning. The method demonstrates effectiveness across several action recognition benchmarks, including UCF-HMDB, Jester, and Epic Kitchens. Additionally, the authors discuss the calculation of parameters and computational complexity in the context of video-based UDA, clarifying that all trainable parameters, including those of the TranSVAE decoder, are included in the calculations. They emphasize that performance gains are attributed to the interaction of various optimization constraints rather than solely to the component $L_{svae}$. The authors also address the terminology regarding "backbone," explaining that it refers to the model used for data pre-processing, and they highlight the differences between TranSVAE and VideoMAE, noting the need for a deliberate design if VideoMAE is to be used as a backbone.

### Strengths and Weaknesses
Strengths:
- The focus on unsupervised video domain adaptation is a compelling and challenging topic.
- The paper is well-written and organized, with extensive experiments showcasing the proposed model's effectiveness.
- The use of visualizations and comparisons enhances the clarity of the results.
- The authors provide clear and detailed responses to reviewer questions, enhancing the understanding of their methodology.
- They acknowledge the importance of computational efficiency in UDA and express willingness to explore new backbone models like VideoMAE.

Weaknesses:
- The proposed method appears to be a combination of existing modules, lacking a clear novelty or complexity analysis compared to current UDA methods.
- The end-to-end training process of the model is unclear, particularly regarding the various components and loss functions.
- Hyper-parameter selection and analysis are insufficiently discussed, raising questions about the choice of values for T, $\eta$, and $\lambda$s.
- The choice of I3D as the backbone is outdated, and the paper does not explore newer models.
- The performance of TranSVAE with VideoMAE as a backbone is reported as unsatisfactory, indicating potential limitations in the current approach.
- Some tables, such as Table 4, are not referenced or analyzed adequately, and comparisons with state-of-the-art methods are lacking.
- The computational costs across methods are similar, which may suggest a lack of innovation in efficiency.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the model's novelty and complexity analysis in relation to existing UDA methods. A detailed end-to-end training explanation should be provided, including the roles of different components and loss functions. Additionally, the authors should discuss the rationale behind hyper-parameter choices and conduct a thorough analysis of these parameters. We suggest considering more recent backbone models for comparison and ensuring that all tables and figures are properly referenced and analyzed. Furthermore, we recommend that the authors improve the discussion on the performance gap observed when using VideoMAE as a backbone, including a thorough analysis of hyperparameter choices. We also suggest including the preliminary study results in the main manuscript to provide a clearer context for their findings. Finally, we encourage the authors to enhance the clarity of their methodology by adding more annotations in figures and tables to facilitate understanding.