ID: gZhvtIRu7i
Title: MILDSum: A Novel Benchmark Dataset for Multilingual Summarization of Indian Legal Case Judgments
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the MILDSum (Multilingual Indian Legal Document Summarization) dataset, comprising 3,122 aligned judgment-summary pairs in English and Hindi, sourced from the reputable LiveLaw website. The authors detail their data collection methodology, including two pipelines for alignment and cleaning, and provide baseline results using nine summarization models. The study emphasizes cross-lingual summarization to address the needs of India's population, many of whom lack English proficiency. The authors benchmark various models, finding that extractive methods outperform abstractive ones on this dataset.

### Strengths and Weaknesses
Strengths:
- The introduction of the MILDSum dataset fills a significant gap in multilingual summarization resources for the Indian legal domain, ensuring high-quality, curated summaries.
- The dataset addresses a pressing societal issue by making legal judgments accessible to Hindi speakers.
- The paper provides insights into the unique characteristics of Indian legal documents and benchmarks multiple summarization models.

Weaknesses:
- The dataset's reliance on a single source, LiveLaw, may limit its diversity and generalizability.
- The paper lacks rigor in describing the dataset's characteristics and the evaluation metrics used for benchmarking.
- The quality of translations from English to Hindi is not adequately discussed, raising concerns about their accuracy.
- The paper does not compare MILDSum with other datasets, which would strengthen its claims.

### Suggestions for Improvement
We recommend that the authors improve the related work section to include more comprehensive discussions of existing summarization datasets, particularly those involving Indian languages. Additionally, the authors should provide quantitative measures to describe the dataset's characteristics, such as extract/abstract-iveness and vocabulary size. Clarifying the evaluation metrics for ROUGE and BERTScore, particularly their adaptation for Hindi, is essential. We also suggest that the authors address the quality of translations and consider including a comparative analysis with other datasets to enhance the robustness of their findings. Finally, the ethics statement should be elaborated to clarify the implications of model outputs and the consent required for data collection.