ID: Q8Z04XhDdL
Title: MoE Jetpack: From Dense Checkpoints to Adaptive Mixture of Experts for Vision Tasks
Conference: NeurIPS
Year: 2024
Number of Reviews: 20
Original Ratings: 5, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MoE Jetpack, a method for converting dense checkpoints into mixture of experts (MoE) models, utilizing checkpoint recycling to enhance fine-tuning efficiency and accuracy. The authors introduce the hyperspherical adaptive MoE (SpheroMoE) layer and the Adaptive Dual-path structure, which optimize the MoE architecture. Extensive experiments validate the proposed methods, demonstrating significant performance improvements across various datasets. Additionally, the paper compares MoE Jetpack with Dense ViT models, highlighting the performance of randomly initialized MoE models and the importance of distinguishing between different training processes involving pre-trained weights and fine-tuning.

### Strengths and Weaknesses
Strengths:  
- The paper provides a thorough explanation of checkpoint recycling and the SpheroMoE layer, with clear and replicable methodologies.  
- The innovative approach of decomposing pre-trained dense models into experts enhances flexibility and leverages existing pre-training costs.  
- The Adaptive Dual-path MoE effectively balances speed and accuracy, improving overall model performance.  
- Comprehensive experiments across multiple datasets ensure robustness and generalizability of results.  
- The paper effectively demonstrates the effectiveness of the proposed methods through various tables, showcasing improvements in model performance and convergence.  
- The authors have addressed most reviewer concerns, enhancing the clarity and soundness of the experimental results.  

Weaknesses:  
- The main results suffer from unfair comparisons, as MoE Jetpack is initialized from a pre-trained model while the baseline (SoftMoE) is trained from scratch.  
- There is some confusion regarding the terminology and processes related to pre-trained weights and fine-tuning, leading to contradictory statements in the responses to reviewer comments.  
- The experimental settings for each table lack clarity, which may hinder understanding for meticulous reviewers.  
- Formatting issues and missing references to related work need addressing, particularly in comparing the proposed methods with existing techniques.  
- The computational overhead of checkpoint recycling requires quantification, and the effectiveness of using pre-trained weights in practical applications remains questionable.

### Suggestions for Improvement
We recommend that the authors improve the fairness of comparisons by including a baseline of a dense model pre-trained on ImageNet-21k and fine-tuned on the evaluation dataset. Additionally, consider comparing MoE Jetpack with a SoftMoE model pre-trained on ImageNet-21k and fine-tuned on the evaluation dataset. It is essential to detail and quantify the additional computational overhead introduced by checkpoint recycling. Furthermore, we suggest that the authors improve the clarity of the terminology used in describing the training processes, particularly regarding the distinction between using pre-trained weights and fine-tuning. Lastly, we recommend including clearer explanations of the experimental settings for each table in future revisions or supplementary materials to enhance transparency and comprehensibility, while also ensuring that formatting issues are corrected and that relevant related work is discussed to contextualize the contributions of the proposed methods.