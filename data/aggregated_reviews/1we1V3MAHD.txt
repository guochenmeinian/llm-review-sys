ID: 1we1V3MAHD
Title: MotionBooth: Motion-Aware Customized Text-to-Video Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 20
Original Ratings: 3, 6, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MotionBooth, a method for fine-tuning a pre-trained text-to-video model using a collection of images of a specific object to generate controlled videos. The fine-tuning process incorporates three losses: diffusion loss on image data focused on the object's region, video preservation loss to prevent overfitting to static images, and subject token cross-attention loss for improved controllability during generation. The inference process allows control of the object's motion by editing cross-attention maps and managing camera movement through shifting noised latents. Additionally, the authors propose a motion control module that applies suppression and amplification techniques to enhance subject representation during the denoising process. They clarify that the reflect method's failure in their model is due to the distinct initialization of the latent variable $z_T$ for each frame, which is necessary for capturing frame-by-frame variations. The authors also address the similarities and differences between their approach and the Mix-of-Show paper regarding attention regularization losses.

### Strengths and Weaknesses
Strengths:
- The problem of controllable video generation is significant, and the proposed method demonstrates improved controllability, as evidenced by visual results.
- The paper is well-structured, with clear motivation and effective figures that enhance understanding.
- The authors provide thorough clarifications regarding their methodology, particularly the motion control module and the suppression formula.
- The paper demonstrates effective techniques for achieving zoom effects through bounding box adjustments.
- The limitations of the method are adequately discussed in the appendix.

Weaknesses:
- **Novelty**: The paper lacks discussion and comparisons with similar training-free motion control methods, which diminishes its novelty.
- **Method**: The attention suppression technique lacks clarity, particularly regarding its implications for the rest of the prompt.
- **Evaluation**: The absence of quantitative evaluations and limited object classes in the dataset undermine the robustness of the claims. More ablation studies are needed to illustrate contributions effectively.
- **Results**: Issues with bounding box coverage raise concerns about the effectiveness of the proposed control method.
- The camera movement control module's contribution is perceived as limited, particularly in achieving nuanced camera motions like rotation. Some reviewers feel that the innovation presented in the paper is somewhat constrained and that the authors did not fully grasp the implications of their suppression method.

### Suggestions for Improvement
We recommend that the authors improve the discussion of the connection between MotionBooth and prior works with similar training-free controls, including more ablation studies to clarify contributions. Additionally, the authors should provide more details on the subject motion control attention suppression technique, especially regarding its impact on prompt tokens outside the subject's bounding box. Clarifying the efficiency of the fine-tuning process, including training time for adapting the model to new subjects, would also enhance the paper. Furthermore, we suggest enhancing the camera movement control module to allow for more sophisticated subject motion controls, such as walking and jumping, rather than relying solely on bounding box adjustments. Finally, expanding the evaluation dataset to include a broader range of object classes and conducting more comprehensive quantitative evaluations would strengthen the findings. We also encourage the authors to explore and explicitly compare the effects of attention regularization losses with those in the Mix-of-Show paper to provide a more comprehensive analysis.