ID: NnMEadcdyD
Title: Understanding Diffusion Objectives as the ELBO with Simple Data Augmentation
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 8, 8, 9, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a systematic investigation of different weighting schemes for the loss functions of generative diffusion models. The authors explore the relationship between a weighted loss function and the Evidence Lower Bound (ELBO), demonstrating that a monotonic weighting factor allows for an interpretation of the loss as log-likelihood optimization with respect to data augmentation, particularly through noise perturbation. They leverage this interpretation to propose new weighting schemes and validate their findings empirically.

### Strengths and Weaknesses
Strengths:
* The paper contributes significantly to the understanding of diffusion models, addressing critical aspects of their implementation systematically.
* The authors provide a theoretical framework that recasts the diffusion model objective as an ELBO objective, which is beneficial for the research community.
* The empirical results indicate that the proposed monotonic weighting functions yield high-quality samples, achieving competitive performance on benchmark datasets.

Weaknesses:
* The paper suffers from several technical issues related to mathematical rigor, including unclear definitions of key terms and inconsistent notation.
* The derivations lack sufficient detail, particularly in the main conclusions regarding the equivalence of the diffusion objective and the ELBO.
* The experimental validation is limited, as it relies on a single dataset (ImageNet 64x64), raising questions about the generalizability of the results.

### Suggestions for Improvement
We recommend that the authors improve the clarity and rigor of their mathematical exposition, particularly in defining the set of time indices and the conditional distributions involved. It would be beneficial to provide detailed derivations for key results, especially those related to the KL divergence and its implications for the ELBO. Additionally, we suggest expanding the experimental validation to include multiple datasets to strengthen the generality of the conclusions drawn from the proposed weighting schemes.