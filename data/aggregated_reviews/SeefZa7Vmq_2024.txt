ID: SeefZa7Vmq
Title: Unlearnable 3D Point Clouds: Class-wise Transformation Is All You Need
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Unlearnable Multi-Transformations (UMT), the first approach aimed at making 3D point cloud data unlearnable for unauthorized deep learning models through class-wise transformations. It introduces a data restoration scheme for authorized users to train on the unlearnable data effectively. Theoretical analysis and extensive experiments across various datasets and models validate UMT's effectiveness in protecting sensitive 3D data while allowing legitimate access. Additionally, the paper explores the effectiveness of RIConv++ in processing 3D point clouds, particularly focusing on the ModelNet10 dataset and the impact of UMT data ratios. The authors propose that while RIConv++ is designed to be rotation-invariant, its performance is hindered by the characteristics of the KITTI dataset, where limited diversity and point count lead to anomalies in transformation effects. The main contributions include the introduction of the first 3D unlearnable scheme, a novel data restoration approach, theoretical insights into the unlearnability mechanism, and empirical validation of UMT's superiority.

### Strengths and Weaknesses
Strengths:
1. The paper is clearly written, making complex concepts accessible.
2. The proposed unlearnable scheme is straightforward and effectively leverages 3D data characteristics, combining four types of transformations to protect point cloud data while allowing restoration for authorized users.
3. Extensive experiments demonstrate the method's effectiveness, successfully preventing unauthorized users from achieving high performance on the UMT dataset.
4. The method includes theoretical proof, enhancing its credibility and robustness.
5. The authors provide a comprehensive analysis of the ModelNet10 dataset and present detailed results for varying UMT data ratios, demonstrating a consistent trend in performance.
6. The response to reviewer concerns includes a thoughtful exploration of the limitations of RIConv++ on the KITTI dataset, highlighting the unique challenges posed by the dataset's characteristics.

Weaknesses:
1. The experiments do not assess the UMT dataset's performance when mixed with other datasets, raising questions about maintaining performance if only part of the training set consists of UMT data.
2. The reliance on randomness in experiments is concerning; while the appendix shows different random seeds, it does not indicate the variance of results in other experiments.
3. The proposed transformation patterns may be easily detectable, allowing unauthorized models to adapt and potentially compromise the method's effectiveness.
4. The settings studied may not reflect real-world applications requiring high safety standards, such as 3D face recognition and scene-level point clouds for autonomous driving.
5. The initial lack of clarity regarding the dataset and the absence of a comparison using 0% UMT data raised concerns that needed addressing.
6. The claim that the combination of transformations is ineffective for RIConv++ on the KITTI dataset requires further empirical validation.

### Suggestions for Improvement
We recommend that the authors improve the experimental design to investigate the impact of mixing UMT data with other datasets to assess performance consistency. Additionally, we suggest providing clarity on whether results are averaged over multiple runs to ensure reliability. The authors should consider addressing the predictability of transformation patterns and explore more complex transformation strategies to enhance robustness against unauthorized adaptation. Furthermore, we encourage the authors to refine the presentation and writing for clarity, particularly in theoretical contributions, and to supplement the introduction with more intuitive explanations of how the proposed method addresses the identified challenges. We also recommend that the authors improve clarity by explicitly stating the dataset used in the experiments and ensuring that comparisons with 0% UMT data are consistently included. Lastly, we suggest enhancing the discussion on the counterintuitive results observed with RIConv++ on the KITTI dataset, potentially incorporating more empirical data to support their claims regarding the limitations of local feature extraction in this context.