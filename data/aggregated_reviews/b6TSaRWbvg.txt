ID: b6TSaRWbvg
Title: Pencils Down! Automatic Rubric-based Evaluation of Retrieve/Generate Systems
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: 1, 0, 1, -1, -1
Original Confidences: 4, 4, 5, 4, 4

Aggregated Review:
### Key Points
This paper presents RUBRIC, a novel evaluation metric for information retrieval systems that integrates large language models (LLMs) with human judgment. The authors propose decomposing queries into detailed subquestions, allowing for a more nuanced assessment of relevance. The methodology is empirically validated against TREC benchmarks, demonstrating high correlation with traditional evaluation methods. However, the paper raises questions about the robustness of its grading rubrics and the implications of its evaluation strategy compared to existing metrics.

### Strengths and Weaknesses
Strengths:
- The problem addressed is timely and relevant, with a well-articulated motivation.
- The related work section is comprehensive, showcasing a thorough review of existing methods.
- The GitHub documentation is well-structured, facilitating reproducibility.

Weaknesses:
- The evaluation methodology's reliance on LLMs without passage-level relevance judgments may undermine confidence in the results.
- The integration of relevance labels through the maximum function in Equation 1 is questionable, potentially skewing relevance assessments.
- The paper lacks clarity on the unique contributions of RUBRIC compared to existing methods, raising concerns about its novelty.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their evaluation methodology by addressing the potential biases introduced by the automated labeling process. It would be beneficial to analyze the distribution of graded passages in Table 2 to ensure balanced relevance scoring. Additionally, we suggest revisiting the integration method for relevance labels in Equation 1 to ensure it accurately reflects the relevance of passages. The authors should also clarify the rationale behind limiting the number of subquestions to 10 and consider expanding their experiments to include in-context learning examples. Furthermore, we encourage the authors to provide a more detailed exploration of the robustness and redundancy of their grading rubrics, as well as a clearer justification for the designation of the "best performing implementation" of the RUBRIC framework. Lastly, we recommend including a discussion on the effort required to implement RUBRIC compared to traditional evaluation methods to better contextualize its applicability.