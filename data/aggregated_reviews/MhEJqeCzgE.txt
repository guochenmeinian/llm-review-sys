ID: MhEJqeCzgE
Title: Unraveling Feature Extraction Mechanisms in Neural Networks
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical perspective on feature extraction mechanisms in fundamental neural modules using gradient descent for text classification tasks. The authors analyze learning dynamics for binary and multi-class classifiers, validating their theory through experiments that explore factors affecting feature extraction. The study focuses on the neural tangent kernel (NTK) at the infinite-width limit for models like MLP, CNN, and self-attention, revealing insights into how biases and activation functions influence feature learning.

### Strengths and Weaknesses
Strengths:
- The theory is well-articulated and encompasses both binary and multi-class classification.
- Experimental results provide deep insights into the behavior of neural network modules, particularly regarding the performance decline of MLP with ReLU.
- The paper is well-written, with clear motivation and intuitive explanations of derivations.

Weaknesses:
- The reliance on the appendix for key theories and experiments may hinder accessibility for readers.
- The empirical results primarily offer correlational insights rather than a mechanistic understanding of the phenomena.
- The implications of learning dynamics without the infinite width condition remain unclear.

### Suggestions for Improvement
We recommend that the authors improve the accessibility of the paper by integrating key theories and experiments from the appendix into the main text. Additionally, we suggest that the authors provide a clearer mechanistic understanding of the observed phenomena rather than relying solely on correlational analysis. It would also be beneficial to comment on how the theory could guide the engineering of useful biases in feature extraction. Lastly, please enhance the captions for figures, particularly Figure 1, and consider increasing the size of Figure 3 for better readability.