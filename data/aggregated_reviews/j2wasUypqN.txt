ID: j2wasUypqN
Title: MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 7, 8, 9, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 1, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MetaBox, a benchmark platform for Meta-Black-Box Optimization (MetaBBO) using Reinforcement Learning (RL). The platform offers a unified algorithmic template, facilitating the evaluation of BBO algorithms through over 300 problem instances and 19 baseline methods. It includes three standardized performance metrics to consistently assess the algorithms. The authors clarify that MetaBBO extends beyond hyperparameter tuning to encompass various optimization dimensions, such as operator selection and algorithm configuration, and can be viewed as a meta-level decision process formulated as a Markov Decision Process (MDP). The authors aim to simplify and standardize the benchmarking process in this domain while introducing innovative metrics like Meta Generalization Decay (MGD) and Meta Transfer Efficiency (MTE).

### Strengths and Weaknesses
**Strengths:**
1. The benchmark is comprehensive, featuring numerous instances and baselines for comparison.
2. The documentation is detailed, providing sufficient guidance for users and enhancing user experience.
3. The writing is clear and well-structured, improving readability and understanding.
4. The paper effectively differentiates between MetaBBO, hyperparameter optimization (HPO), and traditional BBO methods.

**Weaknesses:**
1. The distinction between Meta BBO tasks and traditional hyperparameter search methods is unclear, raising questions about the practical significance of the benchmark given existing BBO benchmarks.
2. The limitations section lacks depth and specificity regarding the challenges of performance evaluation in BBO.
3. The presentation of results could be improved by including error bars or confidence intervals in the performance charts.

### Suggestions for Improvement
We recommend that the authors clarify the differences between Meta BBO tasks and traditional hyperparameter search methods to enhance understanding of the benchmark's unique contributions. Additionally, we suggest expanding the limitations section to provide more context on the challenges of performance evaluation in BBO. Including a dedicated background section with a figure to illustrate the bi-level optimization framework would also improve clarity. Furthermore, providing pseudocode for usage in the main text, rather than just in the appendix, would benefit users. Lastly, incorporating error bars in the performance charts would enhance the transparency of the results. Maintaining ongoing updates and support for the MetaBox platform will be crucial for its long-term success and usability.