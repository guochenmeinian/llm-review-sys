ID: yTTomSJsSW
Title: Aligning Large Language Models with Representation Editing: A Control Perspective
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 6, 5, 8, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents RE-CONTROL, a novel approach for aligning Large Language Models (LLMs) through representation editing, treating LLMs as discrete-time stochastic dynamical systems. The authors propose inserting control signals into internal representations, allowing for precise output manipulation during test time. Empirical results indicate that RE-CONTROL outperforms existing alignment methods on the HH dataset without significant inference time increases. The method learns a value function from human-preference data to enhance model performance at inference.

### Strengths and Weaknesses
Strengths:
- The innovative application of control theory to align LLMs through representation editing is a significant contribution.
- The method allows for dynamic adjustments during generation, improving practical usability.
- Empirical evidence demonstrates that RE-CONTROL surpasses both training-time and test-time alignment baselines.

Weaknesses:
- Certain expressions and notations in the paper are unclear, leading to confusion (e.g., a_t, V_{phi}, and mismatched figure legends).
- The method's performance heavily relies on the value model's reliability, which is not adequately discussed.
- The theoretical analysis lacks rigor, particularly in interpreting generated tokens as random variables rather than as determined by logits.

### Suggestions for Improvement
We recommend that the authors improve clarity by defining all notations and ensuring that all figures are referenced appropriately. Additionally, a discussion on the reliability of the value model should be included to address its critical role in alignment effectiveness. The authors should enhance the theoretical framework to provide a more rigorous interpretation of the generated tokens. Furthermore, we suggest conducting a compute-performance tradeoff analysis to clarify RE-CONTROL's efficiency compared to other methods. Lastly, diversifying the experimental datasets and models would strengthen the findings and address potential overfitting concerns.