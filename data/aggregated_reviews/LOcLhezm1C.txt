ID: LOcLhezm1C
Title: Is Function Similarity Over-Engineered? Building a Benchmark
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 5, 6, 7, -1, -1
Original Confidences: 4, 3, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents a new baseline model, REFuSe, which utilizes only raw bytes as features and a basic convolutional neural network, achieving state-of-the-art performance on binary function similarity detection (BFSD) tasks. The authors also introduce a new benchmark, REFUSE-BENCH, aimed at facilitating future research in BFSD. The paper addresses limitations in existing models and emphasizes the importance of dataset construction and feature engineering.

### Strengths and Weaknesses
Strengths:
- The introduction of REFuSe demonstrates that simpler models can achieve competitive performance, suggesting that complex features may not always be necessary.
- The release of REFUSE and REFUSE-BENCH under an open-source license promotes transparency and encourages further research and collaboration.
- The paper effectively addresses critical shortcomings in existing datasets by considering real-world binaries and their inter-procedural relations.

Weaknesses:
- The paper does not adequately discuss discrepancies in performance between REFuSe and the baselines GNN and jTrans, particularly regarding dataset variations.
- Comparisons are limited to two existing models, and the lack of additional metrics beyond MRR weakens the evaluation of REFuSe's performance.
- The analysis of why jTrans outperforms REFuSe in certain scenarios is insufficient, and the paper does not explore the impact of smaller pool sizes on model performance.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the performance discrepancies between REFuSe and the baselines, particularly addressing the impact of dataset variations. Additionally, including comparisons with more baseline models and exploring different scenarios, such as varying jTrans pool sizes, would strengthen the paper's contributions. Incorporating additional performance metrics, such as AUC or Recall@1, would provide a more comprehensive evaluation. Lastly, we suggest enhancing the clarity of Figure 1 and ensuring that table titles are descriptive enough to convey their contents effectively.