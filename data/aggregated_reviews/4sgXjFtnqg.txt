ID: 4sgXjFtnqg
Title: Efficient Multilingual Language Model Compression through Vocabulary Trimming
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents vocabulary trimming (VT), a method for compressing multilingual language models (LMs) by removing tokens irrelevant to a target language. The authors provide extensive empirical evidence demonstrating that VT minimally impacts monolingual tasks such as question answering and sentiment analysis. They also explore the timing of VT application and its potential as a debiasing technique, comparing the trimmed multilingual model to a monolingual model and showing reduced bias.

### Strengths and Weaknesses
Strengths:
- The paper includes extensive empirical work across multiple high-resource languages.
- Figures effectively illustrate the method.
- The method is well-contextualized within existing literature.

Weaknesses:
- The justification for VT over existing monolingual models is unclear, particularly in the introduction and section 4.
- The transition to discussing VT's impact on bias is abrupt and lacks clarity.
- The analysis does not address the tradeoff between extracting monolingual LMs and potential loss of cross-lingual transfer.
- The paper lacks evaluation on larger LMs and low-resource languages.

### Suggestions for Improvement
We recommend that the authors improve the justification for VT by providing a clearer comparison with existing monolingual models in the introduction and section 4. Additionally, clarify the transition to discussing bias impact by referencing existing literature. The authors should analyze the tradeoff between monolingual extraction and cross-lingual transfer, and include evaluations on larger LMs and low-resource languages. Furthermore, consider addressing the potential for memory reduction through alternative methods, such as loading vocabulary embeddings into CPU memory.