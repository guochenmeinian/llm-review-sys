ID: DRWCDFsb2e
Title: INTERPRETABILITY OF LLM DECEPTION: UNIVERSAL MOTIF
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 6, 5
Original Confidences: 4, 5

Aggregated Review:
### Key Points
This paper presents a protocol aimed at inducing deception in Large Language Models (LLMs) to enhance user safety. The authors dissect latent activations and employ a three-stage iterative refinement process to achieve deception. Through experiments on 20 models, including Gemini and Llama, they demonstrate that LLMs possess an internal representation of truth, evidenced by 'truth directions' in early-middle layers. The authors identify that a sparse set of layers in the third stage is causally responsible for lying and show that contrastive activation steering can effectively reduce lying.

### Strengths and Weaknesses
Strengths:
- The experiments are thorough, testing the protocol across 20 models.
- The protocol is innovative, demonstrating manipulation of LLMs.
- Promising results pave the way for future research.

Weaknesses:
- The focus is limited to deception when explicitly instructed, lacking exploration of other forms of unintentional deception.
- The framing around "deception" may be too narrow; a broader term like "instructed misrepresentations" could be more appropriate.
- The paper's novelty is diminished by reliance on existing work, particularly Campbell et al. (2023), without adequate citation or differentiation.

### Suggestions for Improvement
We recommend that the authors expand the scope of their investigation to include other types of unintentional deception. Additionally, we suggest rebranding the focus to "instructed misrepresentations" for greater accuracy. To enhance the scientific rigor of the paper, we advise the authors to clarify how their findings differ from existing literature, particularly in relation to Campbell et al. (2023), and to properly cite relevant methodologies that have been previously established.