ID: iiWP7khhwP
Title: Long-Range Language Modeling with Selective Cache
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents three strategies to build a selective cache of key-value pairs in a transformer XL style language model, enhancing the model's ability to utilize longer histories with a fixed-size cache. The strategies include selecting words based on eye-fixation duration, employing a selector network to prune key-value pairs, and a refinement called the replacement network for removing pairs from the cache. Experimental results on PG-19, Wikitext, and CMU-DoG datasets demonstrate that all strategies improve performance over Transformer XL and Compressive Cache, with the replacement network yielding the best results. Additionally, clustering similar tokens for selection shows minor improvements.

### Strengths and Weaknesses
Strengths:
* Proposes three novel approaches to enhance transformer language models by effectively caching key-value pairs without increasing attention computation context.
* Experimental results indicate perplexity improvements over Transformer XL and Compressive Cache.
* The methods intuitively store named entities and rare tokens in the cache.

Weaknesses:
* Some experimental comparisons are weak or incomplete, particularly regarding cache size consistency in Table 1.
* The complexity of the proposed method raises questions about the clarity of quantitative improvements, and the experiments are limited in scope.

### Suggestions for Improvement
We recommend that the authors improve the consistency of cache sizes used in their experiments, particularly for the selector network and replacement network in Table 1. Additionally, providing a clearer explanation of the quantitative improvements and the effects of each design choice on performance gains would enhance the paper's clarity. It would also be beneficial to compare against simpler baselines, such as using information entropy as a selection criterion, and to include a column for "data condition" in Table 1 for better clarity. Finally, addressing the training and inference costs associated with the proposed method would provide valuable insights into its practicality.