ID: bH4LVNVXUo
Title: Asymmetric Certified Robustness via Feature-Convex Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 6, 5, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for certifying robustness in binary classification, particularly focusing on asymmetric adversarial attacks. The authors propose a feature-convex neural network architecture that combines Lipschitz and convex ReLU networks to ensure convexity in the function \( g(x) \). The method aims to compute a robustness certification efficiently and is applicable to various adversarial classification tasks, such as malware and spam detection. The authors argue that their approach can outperform existing methods in certain scenarios.

### Strengths and Weaknesses
Strengths:
- The computation of a robust certificate is significantly faster than competing methods while achieving comparable performance.
- The paper formalizes the asymmetric nature of adversarial classification, which is relevant in many real-world applications.
- The exposition is clear, and the bibliography is thorough.

Weaknesses:
- The paper lacks a comparison with SVMs, which are simpler and often yield robust certifications more easily. The authors should provide examples where their method outperforms SVMs in accuracy or robustness.
- The choice of the parameter \( \ell \) in the network architecture is not well justified, particularly regarding its implications for depth and performance.
- There is insufficient discussion on selecting the transformation \( \phi \) when the dataset dimension is less than the number of data points.
- The experimental section is poorly presented, obscuring the benefits of the proposed method and lacking clarity on baseline comparisons.

### Suggestions for Improvement
We recommend that the authors improve the comparison with SVMs by including accuracy and robustness metrics to substantiate their claims. Additionally, we suggest elaborating on the choice of \( \ell \) and its implications for the network's depth. The authors should also discuss how to select \( \phi \) in cases where the dataset dimension is less than the number of data points. Furthermore, we encourage the authors to enhance the experimental section's clarity and presentation, ensuring that baseline methods are clearly defined and compared.