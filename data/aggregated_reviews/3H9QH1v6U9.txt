ID: 3H9QH1v6U9
Title: DFRD: Data-Free Robustness Distillation for Heterogeneous Federated Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 6, 5, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called Data-Free Robust Distillation (DFRD) aimed at learning a robust global model in heterogeneous federated learning (FL) environments. The authors propose a conditional generator to simulate the training space of local models, addressing both data and model heterogeneity. The method incorporates an exponential moving average (EMA) strategy to mitigate catastrophic forgetting. The experimental evaluation demonstrates that DFRD outperforms existing methods on several image classification datasets. The authors emphasize the empirical validation of their method while acknowledging the absence of comprehensive theoretical analysis in existing literature regarding data generation boundaries. They aim to clarify the applicability of their experimental setup to various real-world scenarios, such as mobile devices, IoT, and autonomous driving, while noting limitations in handling personalized federated learning.

### Strengths and Weaknesses
Strengths:
1. The problem of heterogeneous federated learning is significant and well-addressed.
2. The proposed data-free knowledge distillation method is detailed, with a thorough analysis of the conditional generator's characteristics.
3. Extensive experiments on six real-world datasets validate the proposed approach.
4. The authors provide robust empirical validation of their method, demonstrating its utility in federated learning.
5. They clarify the applicability of their experimental setup to real-world scenarios, enhancing the paper's relevance.

Weaknesses:
1. The motivation for the conditional generator is insufficiently elaborated.
2. The contributions of the proposed losses appear incremental, lacking novelty.
3. The choice of hyper-parameters in the EMA method is unclear, and some experimental results are difficult to interpret.
4. The overall performance is low, raising concerns about the method's practicality in non-IID settings.
5. The connection between the stages in DFRD lacks intuitive explanations, making it difficult to understand the method's effectiveness.
6. The omission of comparisons with regularization-based solutions is not convincingly justified, particularly given their relevance to the non-IID problem in federated learning.

### Suggestions for Improvement
We recommend that the authors improve the theoretical support for the impact of knowledge transfer on synthesizing different datasets, particularly clarifying the relationship between synthetic data and decision boundaries. Additionally, the authors should provide more detailed explanations regarding the conditional generator's design and its contributions to fidelity, transferability, and diversity. We suggest including visual experiments to enhance the credibility of the generator's training. Furthermore, the authors should clarify the hyper-parameter choices for the EMA method and ensure that the experimental settings are consistently presented across tables and figures. We also recommend improving the clarity of the problem formulation and the scenarios considered in the paper, explicitly addressing applications where clients may share the same test data distribution but have different training data distributions. It would be beneficial to elaborate on how Dynamic Weighting and Label Sampling address the non-IID setup in the main manuscript. Lastly, we encourage the authors to consider including comparisons with regularization-based solutions to strengthen their findings.