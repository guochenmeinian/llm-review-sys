ID: pEhvscmSgG
Title: Constrained Latent Action Policies for Model-Based Offline Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 5, 7, 6, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents C-LAP, a model-based offline reinforcement learning method that addresses the distributional shift problem without using uncertainty penalties or modifying the Bellman update. It learns a joint distribution of latent states and actions, constraining the latent action policy to the dataset distribution through a linear transformation. The authors provide empirical results on D4RL and V-D4RL datasets, demonstrating performance comparable to strong offline RL baselines.

### Strengths and Weaknesses
Strengths:
- The paper effectively tackles the distributional shift issue in offline RL by incorporating latent actions, leading to enhanced policy learning with fewer gradient steps.
- The combination of a latent action prior and a bounded policy to derive a constrained policy is both novel and intuitive.
- The methodology is well-structured and clearly articulated, with strong empirical results, particularly in pixel-based observations.

Weaknesses:
- Figure 1 lacks clarity; it should simultaneously display the prior and posterior in Figure 1(a) and the latent action prior $p_\theta(u_t|s_t)$ and policy $\pi_\psi(u_t|s_t)$ in Figure 1(b). The source of imagined trajectories during policy training is ambiguous and needs clarification.
- The authors should provide distribution results to substantiate claims regarding the closeness of latent actions to the dataset's action distribution and discuss any performance degradation compared to models using real actions.
- The differences between the proposed method and Dreamer should be more explicitly highlighted.
- The experiments primarily focus on Mujoco tasks; performance on navigation tasks like AntMaze should be addressed.
- Some experiments in Figures 4 and 5 show unexpectedly high returns at the start of policy learning, which requires explanation.
- In Tables 5 and 6, C-LAP's performance is inferior to some baselines in medium datasets, suggesting its effectiveness may depend on the quality of the offline dataset.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Figure 1 by showing both the prior and posterior simultaneously. Additionally, the authors should provide distribution results to support claims about latent actions and clarify the source of imagined trajectories during training. It would be beneficial to explicitly compare the proposed method with Dreamer and to include performance results on navigation tasks like AntMaze. The authors should also address the unexpected high returns observed in some experiments and discuss the performance discrepancies in Tables 5 and 6 in relation to dataset quality. Lastly, a more thorough explanation of the motivation for using latent action spaces over actual action spaces would strengthen the paper.