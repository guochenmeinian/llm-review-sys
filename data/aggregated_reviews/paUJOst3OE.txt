ID: paUJOst3OE
Title: MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a compelling argument that different LLMs perform better with specific prompts, introducing the model-adaptive prompt optimizer (MAPO) to tailor prompts for each LLM in downstream tasks. The authors quantify performance variations across LLMs and demonstrate that MAPO, which includes supervised fine-tuning (SFT) and reinforcement learning (RL), leads to significant improvements in various tasks. However, the method's alignment with model-specific adaptation is questioned, as it resembles existing prompt tuning methods.

### Strengths and Weaknesses
Strengths:
- The observation that different LLMs benefit from distinct prompts is intriguing.
- The proposed MAPO shows promising performance improvements across multiple downstream tasks.

Weaknesses:
- The method may not be suitable for low-resource tasks due to data requirements for SFT and RL stages.
- The necessity of RL in the proposed method is unclear, and comparisons against stronger SFT models are lacking.
- The computational cost of MAPO is potentially high, and the paper does not adequately address this concern.
- The presentation is messy, with issues in clarity and formatting, including small font sizes in tables and figures.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the "match degree" term before its introduction. Additionally, it would be beneficial to report the computational cost of MAPO and include results for +SFT in Tables 3, 4, and 5 to clarify the source of improvements. The authors should also consider incorporating randomness during the generation process of LLMs. Furthermore, we suggest enhancing the overall presentation and proofreading the paper to address typos and improve readability.