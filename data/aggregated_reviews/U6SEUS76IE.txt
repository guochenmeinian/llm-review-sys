ID: U6SEUS76IE
Title: FedID: Federated Interactive Distillation for Large-Scale Pretraining Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel training algorithm called Federated Interactive Distillation (FedID) within the federated distillation framework. The authors propose using a public proxy dataset to gather client knowledge and adjust the gradient of a small labeled dataset to address confirmation bias. Extensive experiments on various GLUE tasks demonstrate that the proposed method is effective and robust, outperforming several baseline approaches.

### Strengths and Weaknesses
Strengths:
- The FedID approach is a useful privacy-preserving training method with good performance.
- The algorithm design is reasonable and effective.
- Comprehensive experiments provide valuable insights into public dataset distribution and communication aspects in federated learning.

Weaknesses:
- The contribution appears incremental compared to existing Federated Distillation approaches, primarily focusing on the applied aspect.
- There is a lack of case-level comparisons to justify how the proposed method alleviates confirmation bias.
- The paper does not discuss resilience to malicious clients or servers.
- There is insufficient reproducible content, including hyperparameter settings and code.

### Suggestions for Improvement
We recommend that the authors improve the paper by providing a case-level comparison to analyze the alleviation of confirmation bias in detail. Additionally, the authors should include comparisons with alternative designs, such as using the proxy dataset for server-to-client interaction. A discussion on resilience to malicious clients or servers would enhance the robustness of the contributions. Furthermore, we suggest that the authors ensure reproducibility by including hyperparameter settings and code. Lastly, adding a summarized version of section 2.2 to the introduction could help clarify the paper's contributions.