ID: E9dH0BP5VW
Title: Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the scaling properties of various network architectures in transformer models, focusing on their performance across upstream and downstream tasks. The authors empirically evaluate up to 100 models, including transformers and their efficient variants, to derive insights into how architecture affects scaling behavior. They find that while certain architectures excel at specific scales, the vanilla transformer generally performs best overall. The study emphasizes the importance of considering model scale when proposing new architectures.

### Strengths and Weaknesses
Strengths:
- The paper features extensive empirical evaluations across a wide range of models, providing valuable insights into scaling laws.
- It is well-written and presents a clear framework for model evaluation.
- The findings contribute significantly to understanding model selection in both research and engineering contexts.

Weaknesses:
- The analysis lacks consideration of dataset size, which is crucial for understanding scaling properties.
- The focus is limited to encoder-decoder architectures, missing opportunities to explore other architectural approaches.
- The discussion on inductive biases is minimal, which may mislead readers regarding the paper's title and focus.
- Reproducibility is hindered due to insufficient details on experimental setups and data availability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of all figures to enhance visibility. Additionally, the authors should define all metrics used in the paper with mathematical formulas. It would be beneficial to include a discussion on how hyperparameter settings can influence scaling laws, as this could aid in practical applications. Furthermore, we suggest that the authors consider analyzing the effects of dataset size on scaling and explore the implications of using a broader range of model architectures, including decoder-only models. Lastly, providing more detailed information on the computational resources used in the study would address concerns regarding reproducibility and environmental impact.