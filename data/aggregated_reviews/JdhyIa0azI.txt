ID: JdhyIa0azI
Title: Neural Functional Transformers
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel architecture called Neural Functional Transformer (NFT) that utilizes an attention-based layer equivariant to neuron permutations for processing neural network weights. The authors propose INR2Array, a method for learning an invariant latent space for implicit neural representations (INRs). The architecture aims to enhance expressivity and classification accuracy on INRs compared to existing methods. The authors report that while NFTs achieve better training accuracy than Neural Functional Networks (NFNs) with fewer parameters, NFNs outperform NFTs in test performance on simpler tasks. Additionally, NFTs show lower training reconstruction errors in more complex tasks like INR2Array, suggesting their potential in challenging scenarios. However, discrepancies in reported results for DWS on MNIST raise questions about the effectiveness of NFTs, particularly when compared to previous benchmarks.

### Strengths and Weaknesses
Strengths:
1. The paper is well-structured and clearly written, presenting an interesting problem with significant potential.
2. The combination of Transformers with minimal equivariance is a promising approach.
3. The proposed architecture achieves competitive performance in various tasks, particularly showing lower training reconstruction errors than NFNs in complex tasks.
4. The paper includes ablation experiments that validate the importance of minimal equivalence.

Weaknesses:
1. The novelty of the proposed method is limited compared to prior works, and the evaluation results are insufficient for a fair comparison.
2. The proposed architecture does not consistently outperform NFNs in all tasks, raising doubts about its overall effectiveness.
3. Missing results for NFT without INR2Array hinder the assessment of the method's contributions.
4. The evaluation against DWS is flawed due to discrepancies in the number of INR copies used.
5. The equivariance proof lacks sufficient detail, and runtime comparisons with linear equivariant layers are absent.
6. The paper does not adequately clarify the features used in the weight space or the implications of the INR2Vec limitation.
7. There is insufficient empirical evidence to support the primary motivation for NFTs, particularly in demonstrating their advantages over linear layers on simpler benchmarks.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contributions by providing clearer comparisons with prior works. Specifically, include missing evaluation results for NFT without INR2Array and ensure fair comparisons against DWS by using the same number of INR copies. Additionally, we suggest enhancing the equivariance proof with more details and concrete examples, as well as providing runtime comparisons with linear equivariant layers. It would be beneficial to clarify the features utilized in the weight space and to include a comparison to Spatial Functa to further motivate the NFT architecture. Finally, we encourage the authors to provide a more detailed description of the weight-space in the Introduction or Preliminaries to enhance clarity and to address the practical applications of the proposed method more convincingly.