ID: zAXg8dW8ZO
Title: One-Line-of-Code Data Mollification Improves Optimization of Likelihood-based Generative Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 4, 6, 3, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to enhance the sample quality of likelihood-based generative models by employing data mollification, which addresses manifold overfitting and improves density estimation in low-density regions. The authors propose a progressive introduction of Gaussian noise during training, guided by Theorem 1, allowing for a refined estimation of the target density. They connect data mollification with Gaussian homotopy, demonstrating that this technique can be implemented with minimal code changes and leads to improved FID scores on datasets like CIFAR10 and CelebA. The methodology shows promise across various generative models, including variational autoencoders and normalizing flows, with empirical results indicating significant improvements across multiple datasets and models, particularly with a sigmoid noise schedule.

### Strengths and Weaknesses
Strengths:
- The proposed technique significantly improves FID scores across multiple datasets without incurring additional computational costs.
- The methodology is straightforward to implement, supported by clear instructions and pseudo code.
- The authors provide empirical evidence of the method's effectiveness through various experiments, including informative toy examples and real datasets.
- The robustness of the method to noise schedule choices is a notable advantage.
- The paper is well-written, with a satisfactory literature review and clear presentation of results.

Weaknesses:
- The datasets used may be too simplistic, raising concerns about the generalizability of results to larger-scale datasets like ImageNet.
- The novelty of the approach is questioned, as it closely resembles existing literature on generative modeling.
- There is insufficient theoretical discussion on why data mollification alleviates issues related to manifold overfitting and density estimation.
- The relationship between the proposed method and noise augmentation is not explicitly addressed, and the choice of noise function lacks robust theoretical justification.
- The paper could benefit from a clearer distinction between data mollification and score-based models.

### Suggestions for Improvement
We recommend that the authors improve the theoretical discussion surrounding the efficacy of data mollification in addressing manifold overfitting and density estimation issues. Additionally, it would be beneficial to empirically validate the proposed method against alternative noise schedules and demonstrate its superiority over conventional data augmentation techniques. We suggest including experiments on more challenging datasets to enhance the generalizability of the findings. Clarifying the relationship between the proposed approach and noise augmentation, as well as providing a more detailed description of the model's implementation, would strengthen the paper. Lastly, we encourage the authors to improve the clarity of the differences between data mollification and score-based models by adding a bold header in the paper. Addressing the concerns regarding the novelty of the approach in relation to existing literature would also be crucial for its acceptance.