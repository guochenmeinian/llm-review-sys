ID: 5k9XeHIK3L
Title: Text2CAD: Generating Sequential CAD Designs from Beginner-to-Expert Level Text Prompts
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 6, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Text2CAD, a framework for generating parametric CAD models from designer-friendly text instructions. The authors introduce a data annotation pipeline that utilizes LLM models like Mistral and LLaVA-NeXT to create text prompts for the DeepCAD dataset, which includes approximately 170K CAD models. The proposed method employs a transformer-based autoregressive network to bridge the gap between text prompts and CAD models, demonstrating its effectiveness through various metrics.

### Strengths and Weaknesses
Strengths:  
- The paper is well organized and easy to understand.  
- The visualization of the data annotation pipeline is clear.  
- The structure and logic of the paper are rigorous.  
- The annotation method is innovative, leveraging VLM and LLM to enhance the DeepCAD dataset with diverse text descriptions.  
- The Text2CAD transformer effectively generates CAD construction sequences from text inputs.  

Weaknesses:  
- Some typographical errors need correction (e.g., “Computer-Aided Design (CAD) play”).  
- The quantitative experiments are insufficient, reducing the impact of the findings.  
- Figures could be improved with better color usage to enhance clarity.  
- The paper lacks details on training computational costs and inference time.  
- The claim of being the first AI framework for CAD image generation is inaccurate, as previous work (SketchGen) exists.  
- There is no analysis of the contribution of different levels of language prompts to the final generation.  
- Limited baseline comparisons weaken the novelty of the proposed approach.  
- The quality of the annotated text and its evaluation is not sufficiently addressed.  

### Suggestions for Improvement
We recommend that the authors improve the quantitative experiments to enhance the impact of their findings. Additionally, correcting typographical errors and enhancing figures with more colors would improve clarity. It is crucial to include an analysis of the contribution of the four levels of prompts to validate their necessity. We suggest that the authors provide a comparison with SketchGen to clarify the distinctions between the two approaches. Furthermore, discussing the quality of the annotated text and how it can be evaluated would strengthen the paper. Finally, including typical failure examples and discussing them could provide valuable insights for future work.