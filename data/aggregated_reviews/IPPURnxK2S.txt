ID: IPPURnxK2S
Title: Improving generalization in large langue model by learning prefix subspaces
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper introduces the first adaptation of the subspace method to large language models through subspace adjustment of prefixes, proposing to leverage the advantages of this method to guide model adjustment via validation metrics. The authors demonstrate that their approach leads to improvements in natural language understanding tasks. They also present a novel method for few-examples fine-tuning of LLMs, termed "Prefix Subspaces," which coordinates training across several models to enhance generalization while addressing the challenges of large parameter counts in LLMs.

### Strengths and Weaknesses
Strengths:  
- The motivation for the study is clear and reasonable, with well-written and easy-to-follow content.  
- The inclusion of 5-fold replication and statistical significance enhances the support for the authors' claims.  
- The proposed method shows favorable performance compared to existing PEFT methods and addresses pressing issues in data-efficient transfer learning.

Weaknesses:  
- The improvements observed are limited, particularly when K is small (<100), and the application scenarios are restricted to prefix methods.  
- The novelty of the proposed framework is questioned, as it builds on previous methods with minimal modifications.  
- The lack of source code limits the ability to validate the approach and hinders future research.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contributions to address the concerns raised in the "Reasons to Reject." Additionally, the authors should consider conducting experiments on larger pre-trained language models and comparing their method with previous techniques under their original settings for a fair evaluation. Including a comparison of the number of parameters in Table 1 would also strengthen the paper. Furthermore, we suggest adding experiments with the full GLUE dataset and applying their method alongside other PEFT techniques to better illustrate the limitations and potential of their main hypothesis.