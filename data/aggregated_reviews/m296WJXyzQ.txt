ID: m296WJXyzQ
Title: Scanning Trojaned Models Using Out-of-Distribution Samples
Conference: NeurIPS
Year: 2024
Number of Reviews: 22
Original Ratings: 4, 6, 6, 3, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for detecting trojaned models, named TRODO, which utilizes adversarial shifts in out-of-distribution (OOD) samples. The authors argue that trojaned classifiers mistakenly classify OOD samples as in-distribution (ID) and leverage this characteristic by applying adversarial attacks to perturb OOD samples towards the ID direction. The authors clarify that the classifier maintains higher confidence in in-distribution samples compared to OOD samples, asserting that their signature remains valid even with temperature rescaling. The effectiveness of TRODO is validated through extensive experiments across various datasets and architectures, demonstrating its robustness even against adversarially trained models. However, the authors also note that adaptive attacks can bypass detection mechanisms, with their method showing a significant performance improvement in certain scenarios.

### Strengths and Weaknesses
Strengths:
- The proposed method is general and applicable to various types of trojan attacks and label mapping strategies, functioning effectively with or without clean training data.
- The paper includes extensive empirical results, theoretical analysis, and clear explanations supported by figures, making it accessible and informative.
- The authors provide a detailed explanation of their method and its theoretical underpinnings, including the significance of ID scores and the impact of perturbations on OOD samples.
- Additional experiments demonstrate the effectiveness of their approach against adaptive attacks, showing a notable performance improvement.

Weaknesses:
- The motivation and definitions, particularly regarding near-OOD and far-OOD samples, are not clearly articulated, leading to potential confusion.
- The presentation is challenging to follow, with unclear formatting and a lack of intuitive explanations connecting theoretical analysis to the proposed method.
- The method's vulnerability to adaptive attacks, including temperature rescaling, raises concerns about its robustness and is not adequately addressed.
- The discussion around calibration and its implications for decision-making in high-stakes scenarios is insufficiently addressed, particularly regarding the assumptions made about classifier behavior.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation and definitions in the introduction, particularly regarding near-OOD and far-OOD samples. Additionally, restructuring Sections 1 and 2 to provide clear definitions early on could enhance readability. The authors should address the potential vulnerabilities of their method to adaptive attacks, including temperature rescaling scenarios, and experimentally test and discuss the robustness of their approach against these attacks. Furthermore, we suggest incorporating the proposed minor extension of applying their own softmax function to the logits of the input classifier to mitigate the identified vulnerabilities. Lastly, including results for all architectures mentioned and comparing with a broader range of state-of-the-art methods would strengthen the paper's contributions.