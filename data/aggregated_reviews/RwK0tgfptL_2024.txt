ID: RwK0tgfptL
Title: Shaving Weights with Occam's Razor: Bayesian Sparsification for Neural Networks using the Marginal Likelihood
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 8, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to sparsifying neural networks through a Bayesian framework, specifically utilizing the marginal likelihood via Laplace approximation, termed SpaM. The authors propose weight/node/layer-wise Gaussian priors, optimizing these scales adaptively during training. A new pruning criterion, Optimal Posterior Damage (OPD), is introduced, leveraging the approximated posterior for effective weight pruning. The empirical results demonstrate that SpaM generally outperforms traditional MAP training methods in terms of performance after pruning.

### Strengths and Weaknesses
Strengths:
- The SpaM algorithm and OPD criterion show effectiveness across various network types, supported by extensive and relevant experiments.
- The approximation allowing KFAC to work with non-scalar priors contributes to the KFAC literature beyond just pruning.
- The paper is well-written, with intuitive explanations of complex concepts.

Weaknesses:
- The empirical advantages of the more complex KFAC with non-scalar priors over simpler diagonal precision matrices are not clearly established.
- The paper lacks a comparison with Bayesian inference approaches that utilize hyper-priors, which could provide a more comprehensive understanding of the proposed method's effectiveness.
- Some methodological aspects, such as the integration with existing pruning criteria and the computational costs of Laplace approximation, require further clarification.

### Suggestions for Improvement
- We recommend that the authors improve clarity on the "interleaved" training of prior hyperparameters and network parameters, as well as the specifics of the MAP comparisons.
- Consider removing less relevant figures and discussions to include more experimental plots in the main text, enhancing the paper's focus.
- Address the typo in Eq. 5, where $\mathbf Q^T$ should be $\mathbf Q^T_A$, and clarify the notation in the equation to avoid confusion regarding elementwise addition.
- We suggest expanding on the differences between uniform and global pruning for better understanding.
- The authors should explore and compare the performance of different prior distribution families, such as horseshoe or spike-and-slab priors, to strengthen the experimental analysis.
- Clarify the sequence of dense training followed by pruning, as this is critical for reader comprehension.