ID: jCMYIUwprx
Title: INDICT: Code Generation with Internal Dialogues of Critiques for Both Security and Helpfulness
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 5, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 2, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a dual-critic prompting framework, INDICT, designed to enhance both helpfulness and security in code generation. The authors propose two critics—one focused on helpfulness and the other on security—augmented with external tools like search engines and code executors to mitigate hallucination issues. INDICT is evaluated across eight tasks and five benchmarks, demonstrating its effectiveness in improving code quality.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow.
2. The topic is valuable, addressing a gap in the literature where many works highlight security issues in LLM-generated code but few propose solutions.
3. The ablation studies effectively demonstrate the contributions of each module, including the helpfulness critic, security critic, external tools, and iterative refinement.

Weaknesses:
1. The novelty of the multi-agent collaborative system is weakened by the existence of prior works that have successfully employed similar approaches.
2. The evaluation is limited to comparisons with pure LLMs, lacking comparisons with other relevant works, which undermines the validity of INDICT's effectiveness.
3. The absence of experiments comparing the use of two separate LLMs as critics versus a single LLM raises questions about the necessity of employing two distinct critics.

### Suggestions for Improvement
We recommend that the authors improve the comparison baselines by including advanced prompting methods such as CodeRL, Self-Correct, and Self-Collaboration to provide a more robust evaluation of INDICT's performance. Additionally, we suggest conducting experiments that compare the effectiveness of using two critics versus one, as well as evaluating the summarization agent's impact on performance. It would also be beneficial to clarify the methodology regarding the thought-action-observation process and provide more details on the efficiency trade-offs of INDICT compared to pure LLMs. Lastly, we encourage the authors to address potential data leakage concerns and include a failure case analysis to deepen the understanding of INDICT's limitations.