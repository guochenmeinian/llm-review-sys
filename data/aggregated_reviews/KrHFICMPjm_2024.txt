ID: KrHFICMPjm
Title: GUIDE: Real-Time Human-Shaped Agents
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 3, 5, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GUIDE, a reinforcement learning with human feedback (RLHF) framework designed for real-time, continuous human feedback. The authors propose a novel feedback mechanism that integrates dense continuous feedback with human input, translating this feedback into dense rewards and incorporating a parallel training model for simulated human feedback. The effectiveness of GUIDE is demonstrated through a user study involving 50 participants across three challenging sparse reward environments, including an Atari game and tasks similar to Minecraft, showing improvements over baseline methods. The authors argue that their main contribution lies in grounding this method through large-scale human experiments, despite acknowledging that prior work has utilized human feedback for reward shaping.

### Strengths and Weaknesses
Strengths:
1. The paper is easy to read and introduces a novel approach to continuous human feedback.
2. GUIDE is evaluated with human annotators, demonstrating improvements in three environments and providing cognitive test analyses.
3. The extensive user study is a significant contribution, correlating cognitive performance with the ability to guide an RL agent.
4. The proposed feedback mechanism is described as elegant and novel, with a strong emphasis on real-time human interaction.
5. The authors conducted extensive experiments with 50 human subjects, demonstrating significant improvements over discrete feedback methods.

Weaknesses:
1. The practicality of GUIDE is questionable, as the simplicity of the continuous feedback model may not apply to complex real-world scenarios. The environments used may not adequately validate RLHF algorithms due to their simplicity.
2. The requirement for constant scalar feedback from annotators is challenging and may lead to biases, making pair-wise annotations a more feasible alternative.
3. The proposed method for synthesizing feedback through supervised learning may not accurately capture the complex reward distribution.
4. The paper lacks clarity in defining key symbols and concepts, leading to confusion.
5. There is insufficient discussion of recent related works in RLHF, which could enhance the context of the research.
6. The technical contribution is perceived as limited due to prior work in the area of human feedback for reward shaping.
7. The experiments are criticized for being incomplete, particularly regarding the limitations of the c-DeepTamer baseline and the lack of additional comparative baselines.

### Suggestions for Improvement
We recommend that the authors improve the practicality of GUIDE by addressing concerns regarding its application in complex environments. Consider exploring pair-wise annotations as a more feasible alternative to constant scalar feedback. Additionally, we suggest refining the method for synthesizing feedback to better model the reward distribution. Clarifying the definitions of symbols and concepts in the paper will enhance understanding. We encourage the authors to include a more comprehensive discussion of recent related works in RLHF to strengthen the paper's context and relevance. Furthermore, we recommend improving the completeness of their experiments by including additional baselines that can utilize environment rewards, as this would strengthen their comparative analysis. Lastly, addressing the concerns regarding the cognitive load of the continuous feedback mechanism with empirical data could enhance the robustness of their claims.