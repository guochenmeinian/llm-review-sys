ID: F738WY1Xm4
Title: Deep linear networks for regression are implicitly regularized towards flat minima
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of deep linear neural networks in the context of overdetermined univariate regression. The authors derive a lower bound on the sharpness of the empirical risk minimizer and demonstrate that gradient flow, with small initialization, leads to minimizers with sharpness bounded by a constant times this lower bound. This constant is dependent on the condition number of the data covariance matrix. The results are shown for both small-scale and residual initializations, indicating an implicit regularization towards flat minima.

### Strengths and Weaknesses
Strengths:
- The exposition is excellent, with three main results clearly delineated in separate sections.
- The mathematical clarity in the problem setting is commendable.
- The first main result relaxes previous assumptions and employs a simpler proof.
- The characterization of gradient flow minimizers is of independent interest, particularly the connection to sharpness.
- The theoretical analysis is substantial, providing novel insights into the interplay between step size, initialization scale, and sharpness.

Weaknesses:
- A rigorous theoretical connection between the main results and gradient descent behavior is lacking; if empirical, this should be highlighted.
- The definition of sharpness appears limited; the results may be overly specific to the largest eigenvalue of the Hessian.
- Motivation for the two initialization schemes is unclear; are they merely borrowed from existing work?
- The title may mislead, as it implies an architecture preference for flat minima, which is contingent on specific learning configurations.
- The implications for learning rate design seem weak, as practical implementation based on these results is not evident.

### Suggestions for Improvement
We recommend that the authors improve the theoretical connection between the results and gradient descent behavior, clarifying if it is purely empirical. Additionally, we suggest expanding the definition of sharpness to encompass broader interpretations beyond the largest eigenvalue of the Hessian. The authors should provide clearer motivation for the chosen initialization schemes and consider revising the title to better reflect the findings. Finally, we encourage the authors to strengthen the implications for learning rate design to enhance practical relevance.