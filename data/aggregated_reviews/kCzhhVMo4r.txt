ID: kCzhhVMo4r
Title: Length-Adaptive Distillation: Customizing Small Language Model for Dynamic Token Pruning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a two-stage knowledge distillation framework, LAD, that transfers general and task-specific knowledge to the student while adapting to dynamic token pruning. The authors propose a reconstruction loss and alignment loss during knowledge distillation from a pre-trained teacher, demonstrating superior performance and better performance-speed trade-off on the GLUE benchmark compared to MiniLMv2 and TinyBERT. Comprehensive ablation studies are conducted to validate the effectiveness of the proposed additional losses.

### Strengths and Weaknesses
Strengths:
- The simultaneous consideration of model compression and token pruning is innovative and not previously addressed.
- The paper shows strong performance on the GLUE benchmark and is well-written, facilitating comprehension.
- Comprehensive ablation studies enhance understanding of the proposed method's components.

Weaknesses:
- The novelty of the proposed method is questioned as it appears to be a simple combination of existing techniques.
- The experiments are limited, using only RoBERTa-base as the teacher model and a 6-layer transformer, neglecting larger models.
- Clarity regarding the selection of important hyper-parameters ($\lambda_2$ and $\lambda_3$) and implementation details is lacking, which may hinder reproducibility.
- The method roughly doubles training costs with only marginal improvements in inference speed.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by including larger teacher models and comparing with cascade methods (1 two-stage distillation -> pruning; 2 pruning -> two-stage distillation) to better showcase effectiveness. Additionally, the authors should clarify how the hyper-parameters $\lambda_2$ and $\lambda_3$ are chosen and their impact on performance. We also suggest providing more detailed implementation information to enhance reproducibility and addressing the concerns regarding the non-deterministic nature of the inference process.