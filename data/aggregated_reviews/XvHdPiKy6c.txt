ID: XvHdPiKy6c
Title: SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 7, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SAFESORA, a novel text-video pair dataset aimed at aligning text-to-video (T2V) generation with human values, focusing on helpfulness and harmlessness. The dataset comprises 14,711 unique text prompts and 57,333 videos generated by four large vision models, along with 51,691 pairs of human preference annotations. The authors demonstrate the dataset's utility through applications like training a text-video moderation model and aligning large vision models with human preferences.

### Strengths and Weaknesses
Strengths:
- The SAFESORA dataset addresses a significant gap in AI by providing a comprehensive resource for aligning T2V generation with human values.
- The detailed annotation process, subdividing helpfulness and harmlessness into multiple sub-dimensions, ensures high-quality annotations.
- The methodology is robust, incorporating LLMs for prompt refinement and a thorough analysis of human preference correlations.
- The paper effectively demonstrates the dataset's applications and proposes future research directions, emphasizing ethical considerations.

Weaknesses:
- The dataset may exhibit demographic biases among annotators, which could affect the annotations' objectivity.
- The definition of "natural laws" in the context of correctness is ambiguous and requires clarification.
- The paper lacks inter-annotator agreement statistics and quality control measures for the dataset.
- There is a gap in motivating the correspondence between helpfulness and harmfulness in T2V models, which could be better articulated.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definition of "natural laws" to specify whether it refers to laws of physics or general nature facts. Additionally, the authors should address potential biases in the dataset by analyzing how demographic attributes of crowdworkers may influence annotations. Including inter-annotator agreement statistics and quality control measures would enhance the dataset's credibility. Furthermore, we suggest expanding the dataset to include more diverse scenarios and developing more efficient alignment algorithms. Finally, adding an example of the dataset in the main text and discussing the tension between helpfulness and harmfulness in the introduction would strengthen the paper's motivation.