ID: vBlzen37i0
Title: Optimal deep learning of holomorphic operators between Banach spaces
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 8, 8, 7, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on learning holomorphic operators between Banach spaces, utilizing encoder-decoder architectures with deep neural networks (DNNs) to achieve optimal approximation rates. The authors provide theoretical results on sample efficiency for learning operators from $\mathcal{X}$ to $\mathcal{Y}$, demonstrating that non-adaptive training is optimal up to logarithmic terms. The work includes numerical experiments validating theoretical claims, particularly for the diffusion equation, Navier-Stokes-Brinkman equation, and Boussinesq equation.

### Strengths and Weaknesses
Strengths:
* The paper is well-written, offering a comprehensive review of current research on holomorphic operators.
* The results are novel and extend previous work, allowing standard training methods for DNNs.
* The theoretical findings are well-supported by practical experiments, and the proof techniques are clearly explained.

Weaknesses:
* Some assumptions and results are technical, which may pose accessibility challenges.
* The significance of numerical confirmations is not clearly articulated.
* Certain figures are too small, hindering the recognition of results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the discussion surrounding the encoder and decoder, particularly regarding their definitions and roles. It would be beneficial to include a precise reference discussing the holomorphicity of the Navier-Stokes-Brinkman and Boussinesq equations. Additionally, we suggest that the authors summarize the differences between Banach and Hilbert space cases for $\mathcal{Y}$, and clarify the implications of Theorems 4.1 and 4.2 in relation to the techniques used in [7]. Addressing the performance of different activation functions and the potential extension of Theorem 3.2 to include optimization procedures would enhance the practical relevance of the findings. Lastly, we encourage the authors to review the size of figures for better visibility and to introduce the notation for the Pettis norm to alleviate confusion.