ID: vA0vj1mY77
Title: MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 7, 6, 7, 5, -1, -1, -1
Original Confidences: 4, 5, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MVDiffusion, a novel diffusion framework designed to generate multi-view images with pixel-to-pixel consistency. The authors propose a correspondence-aware attention mechanism that enhances multi-view consistency through a latent diffusion process. The method demonstrates superior performance in multi-view image generation tasks, particularly in maintaining consistency across different views.

### Strengths and Weaknesses
Strengths:
1. The idea of ensuring pixel correspondence in the diffusion process is both novel and reasonable.
2. The supplementary material includes code that details the implementation.
3. The proposed method shows potential for improving results in text-to-texture generation given meshes.
4. Users can input different prompts for various view images.
5. The paper is well-written, especially in clearly demonstrating the method's design.

Weaknesses:
1. The method requires input depth maps to achieve pixel correspondence, which may lack detail for certain objects like leaves and strings.
2. The authors do not address how to handle depth occlusion or pixels in target images that are not visible from the source image.
3. The interpolation module is time-consuming, yet the time consumption for each module is not provided.
4. The evaluation of multi-view consistency relies on pixel-level similarity, raising questions about the determinism of depth-to-image generation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the pipeline figure (Figure 2) to better illustrate the workflow of the modules. Additionally, including a brief section on common failure cases would benefit the community. Clarifying the roles of the generation and interpolation modules during inference, particularly regarding the number of images generated by each, is essential. Addressing how conflicts in overlap regions between generated images are managed would also enhance the paper. Conducting an ablation study to assess the influence of each module would provide valuable insights. Lastly, showcasing more densely generated multi-view images and creating an animation could facilitate a better qualitative evaluation of multi-view consistency.