ID: qRnmLJQHgx
Title: 4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 8, 6, 5, -1, -1, -1, -1
Original Confidences: 5, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an advanced vision model capable of handling a wide range of tasks and modalities, demonstrating the potential to train a single model on diverse modalities without performance loss compared to specialized models. The authors propose a model trained on various modalities, including RGB images, depth, semantic segmentation, and CLIP features, enabling tasks such as image generation and understanding. Additionally, the paper addresses limitations of current multimodal models by introducing a model that can manage at least three times more tasks and modalities, utilizing discrete tokenization techniques.

### Strengths and Weaknesses
Strengths:
- The presentation is clear and detailed, with good performance on various downstream tasks.
- The workload involved in organizing large datasets and conducting extensive training is commendable.
- The model shows promising results that are on-par or exceed strong baselines.

Weaknesses:
- The writing could be improved for clarity, particularly regarding the training framework and multimodal masked training.
- The paper lacks detailed configurations of the model architectures and does not measure the quality of generated images against existing models.
- Claims regarding the model's capabilities may be overstated, as simpler methods exist for unifying tasks, and the differences between completing 5 tasks versus 10+ tasks need further discussion.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly by detailing the training framework and including a simple diagram to enhance understanding. Additionally, the authors should measure the quality of generated images using metrics like FID and compare their model's performance against existing diffusion-based models. It would also be beneficial to include a discussion on the encoder-decoder paradigm versus decoder-only structures, referencing relevant literature. Furthermore, we suggest providing a table comparing the performance of tokenizers and discussing the selection process for vocabulary sizes and metadata choices. Lastly, including quantitative evaluations on the new claimed capabilities would strengthen the paper significantly.