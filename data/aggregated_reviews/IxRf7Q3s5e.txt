ID: IxRf7Q3s5e
Title: NeuralSolver: Learning Algorithms For Consistent and Efficient Extrapolation Across General Tasks
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "NeuralThink," a novel architecture aimed at enhancing same-size task generalization and different-size task extrapolation performances. The authors propose a method comprising three components: (1) a recurrent module utilizing an LSTM network for processing inputs of varying scales, (2) a processing module that aggregates processed information, and (3) a curriculum learning training scheme that incrementally increases observation dimensionality. Experimental results indicate that NeuralThink surpasses baseline methods, particularly in different-size task extrapolation, while also demonstrating improved training and parameter efficiency.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant problem in algorithm learning and extrapolation, achieving oracle-level performance on different-size tasks.
- The proposed method outperforms selected baselines in an algorithm learning benchmark, indicating its effectiveness, with an ablation study showing positive contributions from all components.
- The introduction is well-written, and Figures 1-3 are effectively illustrated.

Weaknesses:
- The rationale behind the improved performance compared to prior approaches is unclear; further explanation of the fundamental differences and their implications would be beneficial.
- Experimental results appear counter-intuitive, as the proposed method achieves high performance on different-size tasks while showing imperfect results on same-size tasks. An analysis of this discrepancy is needed, along with additional case studies to clarify performance gaps.
- The comparison with a limited number of baselines restricts understanding; insights into how NeuralThink compares with other recent methods would be valuable.
- Some figures require improved clarity, particularly Figure 5, where the meaning of "x/y" in the legend is ambiguous.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the rationale behind the performance improvements by explicitly stating the differences between their approach and prior methods. Additionally, an analysis addressing the counter-intuitive results regarding same-size and different-size tasks would enhance understanding. Expanding the number of baselines compared and including a detailed comparison with recent methods, such as [1], would strengthen the paper. We also suggest enhancing the clarity of figures, particularly Figure 5, and including a dedicated limitations section to discuss the transferability of the architecture to real-world applications. Finally, providing a full set of equations describing the model in the appendix would add significant value.