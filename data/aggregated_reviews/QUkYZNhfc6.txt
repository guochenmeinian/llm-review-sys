ID: QUkYZNhfc6
Title: FaceDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and Relighting with Diffusion Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 4, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FDNeRF, a generative method for reconstructing Face NeRFs with capabilities for semantic editing and relighting. FDNeRF employs 3D GAN inversion and a trained 2D latent-diffusion model to facilitate zero-shot learning without explicit 3D data. The method incorporates designed loss functions and multi-modal pre-training, allowing extensive control over editing processes using single-view images, text prompts, and target lighting. Experimental results indicate realistic outcomes and editing flexibility. Additionally, the paper discusses the trade-offs between distortion, perception, and editability in image manipulation, demonstrating that these trade-offs exist not only between $W$ and $W_k^*$ but also within the $W_k^*$ space itself. It references section 7 of the StyleSpace paper, indicating that latent optimization in $W+$ and $S$ spaces offers more flexibility than in $W$, although this flexibility can lead to latent codes that do not reside on the generated image manifold, resulting in unnatural artifacts during manipulation.

### Strengths and Weaknesses
Strengths:
- The inclusion of an illumination loss using spherical harmonics (SH) and a differentiable renderer preserves lighting conditions and allows explicit control over lighting in a view-consistent manner.
- The method demonstrates fair identity preservation, editing fidelity, and 3D geometry consistency from a single image.
- The paper effectively highlights the intricate trade-offs between distortion, perception, and editability, providing a nuanced understanding of these concepts.
- It references relevant literature, enhancing the credibility of its claims regarding latent optimization and reconstruction accuracy.

Weaknesses:
- The paper primarily combines conventional losses, limiting its contributions and making it challenging to derive valuable insights.
- Performance evaluation is restricted to angles between -30 to 30 degrees, lacking assessment on larger poses, which hinders understanding of its effectiveness for 3D reconstruction and editing.
- A comprehensive evaluation of the inversion process is missing, particularly a comparison with state-of-the-art EG3D inversion methods, suggesting inadequacies in the implemented inversion method.
- The editing results exhibit issues with identity and skin color alterations, raising concerns about the trade-offs in semantic editing.
- The discussion on the trade-offs could benefit from a more detailed exploration of the implications of choosing $W+$ over $W$ and $S$ for manipulation tasks.
- The potential for unnatural artifacts when manipulating latent codes in $S$ is mentioned but not sufficiently elaborated upon.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of the inversion process by comparing it with state-of-the-art EG3D inversion methods to establish its performance. Additionally, expanding the performance evaluation to include larger pose angles would provide a more comprehensive understanding of the method's effectiveness. It would also be beneficial to clarify the advantages of the SDS loss compared to CLIP guidance, demonstrating its superiority through clear examples. Furthermore, we suggest that the authors improve the discussion on the implications of selecting $W+$ for manipulation tasks, providing clearer insights into how this choice impacts the overall quality of image manipulation. Finally, elaborating on the nature of the unnatural artifacts that may arise from manipulating latent codes in $S$ would better inform readers of the practical challenges involved. Addressing the identity preservation issues and providing evidence for the trade-offs in semantic editing would enhance the paper's contributions.