ID: bKqrWLCMrX
Title: Uncovering the Hidden Dynamics of Video Self-supervised Learning under Distribution Shifts
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive study of six prominent self-supervised methods in video representation learning, assessing their performance under various natural distribution shifts. The authors conduct extensive experiments across 17 tasks, revealing insights into the generalizability of these methods, including their robustness to viewpoint changes and temporal modeling. Specifically, the analysis highlights the robustness of contrastive methods, particularly v-SimCLR and v-MOCO, to viewpoint shifts, with v-SimCLR demonstrating robustness across all setups, while v-MOCO is robust to two of them. The authors also emphasize the superior performance of v-Supervised and v-MAE in out-of-context generalization, particularly in Context (10 class) and Context (50 class) setups. Furthermore, the paper discusses the comparative performance of single-stream networks versus Siamese frameworks in learning temporal dynamics and viewpoint invariance, contributing valuable empirical data for the video representation learning community.

### Strengths and Weaknesses
Strengths:
1. The paper includes extensive experiments and sufficient data analysis, grounding its conclusions.
2. It covers most cutting-edge video pre-training approaches, addressing an important problem in large foundation models.
3. The training and evaluation protocols are clearly outlined, providing a useful resource for future robustness benchmarking.
4. The statistical tests robustly support the claims made regarding the performance of contrastive methods under various conditions.
5. There is a clear delineation of the strengths of v-Supervised and v-MAE as strong temporal learners in out-of-context scenarios.
6. The analysis provides insightful comparisons between single-stream networks and Siamese frameworks.

Weaknesses:
1. The effects of data size and model size are not discussed, which limits the analysis.
2. There is no proposed improved VSSL method, which could enhance the paper's contribution.
3. The hyper-parameter settings for each method are not fully explored, potentially diminishing the value of the findings.
4. The presentation of results, particularly in figures, is unclear without zooming in.
5. The performance of v-Supervised significantly decreases under multiple concurrent viewpoint shifts, which may limit its applicability.
6. The paper could benefit from a more detailed exploration of the reasons behind the performance drop when finetuning is reversed in source shifts.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the effects of data size and model size on the conclusions. Additionally, proposing an improved VSSL method or guidelines for developing such methods would enhance the paper's impact. Including a summary table of the six methods and their performance would aid reader comprehension. We also suggest conducting a thorough hyper-parameter sweep for each method to ensure comprehensive pre-training exploration. Improving the readability of figures and ensuring that all statements are backed by appropriate statistical tests would strengthen the paper's claims. Furthermore, we recommend that the authors provide more detailed explanations regarding the robustness of v-MOCO and v-Supervised, and include a discussion on the implications of the performance drop observed when finetuning is reversed, as this could provide valuable insights into the model's behavior under varying conditions.