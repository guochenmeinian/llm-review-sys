ID: C1d3VVfdVG
Title: Unchosen Experts Can Contribute Too: Unleashing MoE Modelsâ€™ Power by Self-Contrast
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 8, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach called Self-Contrast Mixture-of-Experts (SCMoE) aimed at enhancing the performance of Mixture-of-Experts (MoE) models by leveraging unchosen experts in a self-contrast manner during inference. The authors demonstrate through exploratory studies that merely increasing the number of activated experts does not necessarily improve output quality and that different routing strategies yield significantly different output distributions. The SCMoE method is shown to improve reasoning capabilities across various domains, with experimental results indicating consistent performance gains compared to baseline methods.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated, with exploratory studies highlighting the complexities of expert selection in MoE models.
- The SCMoE method is simple, intuitive, and incurs minimal performance overhead while yielding significant gains on evaluation benchmarks.
- The evaluation tasks are representative of key LLM applications, making the results meaningful and interpretable.
- The paper is well-written, with clear methodology and organized experimental results.

Weaknesses:
- The concept of SCMoE bears some similarity to the cited work "Contrastive decoding: Open-ended text generation as optimization," necessitating a more thorough discussion of the differences.
- Clarity is lacking regarding the selection of certain hyperparameters, particularly the choice of k in rank-k routing.
- The paper does not adequately address the memory overhead associated with using two models for inference, which could be significant for larger models.
- The relationship between commonsense reasoning tasks and the proposed method's performance remains unclear, particularly regarding its applicability to logic reasoning tasks.

### Suggestions for Improvement
We recommend that the authors improve the discussion in the related work section to clarify the distinctions between SCMoE and similar approaches. Additionally, the authors should provide a rationale for the choice of hyperparameters, particularly the selection of k in rank-k routing. It would also be beneficial to address the memory overhead implications of using two routing strategies more comprehensively. Finally, the authors should explore the potential for the proposed algorithm to enhance performance in logic reasoning tasks and clarify its applicability to general world knowledge reasoning.