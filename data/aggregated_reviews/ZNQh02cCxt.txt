ID: ZNQh02cCxt
Title: Enhancing Abstractiveness of Summarization Models through Calibrated Distillation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel teacher-student method for abstractive summarization, enhancing abstractiveness while maintaining the quality of the original BART model. The authors propose a sequence-level knowledge distillation method that generates diverse summaries, improving both informativeness and abstractiveness. The results indicate that the proposed method outperforms existing distillation techniques, achieving higher scores in ROUGE and novel n-grams. However, the improvements in abstractiveness are primarily observed in 1-grams, raising questions about the effectiveness of the method for larger n-grams.

### Strengths and Weaknesses
Strengths:
- The proposed approach demonstrates superior performance over teacher models and other distillation methods in terms of informativeness and abstractiveness.
- Extensive experiments, including ablation studies and hyperparameter sensitivity analyses, support the claims made.
- The paper is well-structured and clearly written, facilitating comprehension.

Weaknesses:
- The method's effectiveness on models other than BART remains untested.
- Improvements in abstractiveness are limited to 1-grams, suggesting that conventional word replacement methods may be more effective.
- Evaluation lacks consideration of additional criteria such as factuality, coherence, and fluency, which are crucial for assessing the quality of generated summaries.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by incorporating additional criteria, particularly factuality, to address potential hallucinations in generated summaries. It would be beneficial to clarify whether the BART model is trained from scratch or fine-tuned, as this impacts the interpretation of results. Additionally, we suggest exploring the effectiveness of the proposed method on other summarization models and investigating the impact of varying the balancing hyperparameter lambda based on the dataset. Lastly, addressing the typo in line 10 and enhancing clarity in the results section regarding the significance of ROUGE scores would strengthen the paper.