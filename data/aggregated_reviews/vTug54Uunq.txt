ID: vTug54Uunq
Title: Faster Margin Maximization Rates for Generic Optimization Methods
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 6, 8, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 2, 2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on efficient algorithms for margin maximization within a general geometry, leveraging the phenomenon of "implicit bias" observed in optimization algorithms like gradient descent. The authors propose a novel analytical framework by reformulating the problem as a bilinear game, demonstrating that techniques from online learning can yield fast convergence rates for various optimization algorithms, including mirror descent and steepest descent.

### Strengths and Weaknesses
Strengths:  
- The paper offers an innovative perspective by analyzing optimization algorithms through online learning and game theory, particularly highlighted by Theorem 1, which effectively encapsulates the reduction from optimization to online learning.  
- The presentation is clear, detailing prior literature and contextualizing the contributions of this work, enhancing comprehension and appreciation of its versatility.  
- The theoretical analysis is rigorous, providing significant insights into the implicit biases of generic optimization methods.

Weaknesses:  
- The notation is dense and can be challenging to interpret, with specific concerns regarding the introduction of terms like $D_E$ and the interchangeability of $p$-norms and $q$-norms.  
- The theorem statements are lengthy and may appear intimidating, with insufficient discussion on directional errors in Section 4.  
- The paper primarily focuses on exponential loss, limiting its applicability, and lacks empirical validation of the theoretical results, particularly regarding the tightness of convergence rates.

### Suggestions for Improvement
We recommend that the authors improve the clarity of notation and definitions, particularly by addressing the dense notation and introducing $D_E$ more clearly in the Preliminaries section. Additionally, it would be beneficial to clarify the use of $p$-norms and $q$-norms to avoid confusion. 

We suggest that the authors provide a more detailed discussion on directional errors and consider shortening theorem statements to enhance readability. Including simple experiments to verify theoretical results, such as using synthetic datasets with known max-margin solutions, would strengthen the paper. 

Finally, we encourage the authors to explore the extension of their framework beyond exponential loss and to provide empirical comparisons to validate the theoretical convergence rates obtained in this work.