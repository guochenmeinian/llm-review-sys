ID: VUgXAWOCQz
Title: Randomized algorithms and PAC bounds for inverse reinforcement learning in continuous spaces
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 6, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into inverse reinforcement learning (IRL) within discounted Markov Decision Processes (MDPs) characterized by continuous state and action spaces. The authors propose a novel optimization framework that aims to learn a reward function, ensuring the expert's policy remains approximately optimal while incorporating a linear normalization constraint. The study further explores the computational challenges of deriving this solution using the expert's policy and transition dynamics, culminating in a sample complexity analysis based on available expert trajectories and a generative model of the environment. Additionally, the authors extend previous work by accommodating unknown, nonstationary, and randomized expert policies, clarifying that their approach simplifies to established formulations in finite tabular MDPs while offering more general applicability. They emphasize the significance of their optimization-based framework, which allows for efficient learning from finite expert trajectories without querying the expert during training.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant gap in the theoretical understanding of IRL, particularly in continuous state and action spaces.
- It introduces an original optimization perspective that distinguishes it from prior works and provides formal guarantees.
- The theoretical results, including probabilistic convergence guarantees, contribute to bridging the gap between theory and practical applications in continuous IRL.
- The authors provide a comprehensive comparison with recent literature, highlighting the advantages of their approach in terms of flexibility and formal guarantees.
- The optimization-based framework is well-suited for modern stochastic optimization methods, potentially benefiting future algorithm development.
- The paper addresses the challenges of extracting a single cost function in continuous MDPs, incorporating normalization constraints to avoid trivial solutions.

Weaknesses:
- The presentation is often confusing and could benefit from improved clarity to enhance accessibility for the intended audience.
- The results predominantly yield negative outcomes, raising concerns about the practicality of the proposed methods, especially given the exponential sample complexity in state-action dimensions.
- The motivation behind certain design choices, such as the linear normalization constraint, lacks sufficient justification, which may weaken the overall argument.
- The assumption of Lipschitz continuity in Assumption 2.1 may be perceived as strong, and the authors should clarify its implications for various MDP models.
- The exponential sample complexity in relation to state-action dimensions may be seen as a limitation, particularly in high-dimensional settings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation to make the theoretical concepts more accessible to the reinforcement learning community. Additionally, we suggest including comparisons with existing benchmarks to better contextualize the contributions of the paper. It would also be beneficial to provide insights into the hardness of the problem, potentially through lower bounds, to substantiate claims regarding sample complexity. Furthermore, we encourage the authors to elaborate on the implications of their solution concept, particularly how the normalization constraint aids in achieving practical utility for downstream applications. We also recommend improving the clarity of Assumption 2.1 by providing references to support its standard nature and discussing its implications for both tabular and continuous MDPs. Additionally, we suggest rephrasing the discussion on identifiability to clearly differentiate between mathematical artifacts and the issue of underdeterminacy, ensuring that reward shaping is not conflated with these artifacts. Finally, we encourage the authors to elaborate on the practical implications of their exponential sample complexity results, particularly in low-dimensional settings where convergence may still be achievable.