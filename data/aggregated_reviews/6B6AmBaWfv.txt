ID: 6B6AmBaWfv
Title: Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based Code Selection
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the hierarchical vector quantized graph autoencoder (HVQ-GAE) for graph self-supervised learning, claiming that previous methods may generate inappropriate perturbations during view generation, which HVQ can help mitigate. The authors propose a two-layer codebook to learn hierarchical structures in graphs. Experimental results indicate that HVQ-GAE outperforms all baselines in link prediction tasks and shows competitive results in node classification.

### Strengths and Weaknesses
Strengths:
1. The paper is the first to apply vector quantization (VQ) technology to graph self-supervised learning.
2. HVQ-GAE demonstrates significant performance improvements in link prediction tasks and maintains competitive results in node classification.
3. The motivation for using VQ is well-founded, addressing disruptions in graph representations and empirically validating its effectiveness in capturing topological structures.
4. The writing is clear and logically structured, with comprehensive experimental results.

Weaknesses:
1. The motivation for using VQ in graph self-supervised learning is unclear, particularly regarding how it avoids inappropriate perturbations.
2. The paper does not explore the potential benefits of employing more levels in the codebook, which could enhance representation power.
3. The two-layer codebook design increases parameter complexity, and the lack of a weight coefficient in the loss function may hinder effective balancing of contributions.
4. The scientific problem and research gap are not clearly articulated, and the challenges of each method module are insufficiently summarized.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation for using VQ in graph self-supervised learning, specifically defining "inappropriate perturbations." Additionally, we suggest conducting an ablation study to assess whether other graph autoencoders could benefit from edge-level reconstruction. To enhance the paper's rigor, please include a limitations analysis covering error analysis and method performance. Furthermore, we encourage the authors to explore the effects of using more levels in the codebook and to provide a clearer justification for the design choices from a graph perspective. Lastly, an analysis of the overhead and running time, particularly on larger-scale graphs, would strengthen the evaluation of the proposed method's scalability.