ID: kj33zJ9Vue
Title: On permutation symmetries in Bayesian neural network posteriors: a variational perspective
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 7, 7, 6, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 2, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of mode connectivity in Bayesian neural networks (BNNs) through the lens of variational inference, extending linear mode connectivity modulo permutation to BNN posteriors by imposing a Wasserstein metric on the distribution space. The authors propose a matching algorithm based on the Linear Assignment Problem to establish linear connectivity between low-loss solutions, demonstrating that such connections exist in the context of BNNs. They investigate how log likelihood varies along the geodesic between distributions derived from approximate Bayesian inference, focusing on BNNs parameterized by multivariate Gaussians with diagonal covariance. The experimental results validate the proposed heuristic for computing permutations that minimize changes in log likelihood along the geodesic, aligning with previous findings by Entezari et al. (2022) and extending the concept of mode connectivity to the distribution level.

### Strengths and Weaknesses
Strengths:
- The authors introduce a novel approach by applying permutation alignment to BNNs, which is well-formulated and methodologically sound.
- The paper is well-written and the experimental design is thorough, including various ablation studies that provide a comprehensive understanding of the effects of different parameters.
- Experimental results validate the proposed heuristic for computing permutations across the studied network architectures and datasets.

Weaknesses:
- The paper lacks significant novelty, as it largely reiterates existing results without offering substantial new insights into BNNs beyond what is already known.
- The title should specify "variational BNNs" to clarify the focus, and the definition of barrier loss appears misaligned with the intended loss computation.
- The reliance on a Gaussian variational family limits the generalizability of the findings, and the implications of using KL-divergence versus Wasserstein distance are inadequately addressed.
- The limitations of the proposed heuristic are not thoroughly discussed, particularly regarding model architectures with batch normalization, which may fail without additional adjustments.
- It remains unclear whether the proposed algorithm outperforms standard methods such as weight-matching or activation matching.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their conjecture by restricting it to mean-field variational Bayesian inference, as the current formulation is overly broad. Additionally, we suggest that the authors clarify the connection between the interpolation and alignment processes, as well as provide a more detailed discussion on the implications of using different distance metrics. It would also be beneficial to include comparisons of test accuracy alongside log-likelihood in the results to enhance interpretability. Furthermore, we recommend that the authors improve the discussion on the limitations of their heuristic, particularly in relation to batch normalization. It would also be beneficial to empirically evaluate the importance of incorporating the covariance matrix in the objective for computing permutations, potentially including a comparative table of different approaches. Lastly, we encourage the authors to clarify the evaluation metrics used, particularly in relation to connectivity values, and to ensure a direct comparison with results from Ainsworth et al.