ID: Op8UJUVf8G
Title: Multimodal Learning from Egocentric Videos and Motion Data for Action Recognition
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: 4, 4, 6, -1
Original Confidences: 5, 4, 5, 5

Aggregated Review:
### Key Points
This paper presents a multi-modal approach for egocentric activity recognition by integrating vision data with motion data from sensors attached to the subjects' heads. The authors demonstrate the contribution of gaze sensor data in enhancing multi-modal classification performance. They evaluate a hybrid model that fuses various sensor inputs, comparing it with standalone models. While the paper is well-structured and easy to read, it lacks a comprehensive evaluation against state-of-the-art models and does not discuss the practical implications of the results.

### Strengths and Weaknesses
Strengths:
- Clear demonstration of the contribution of gaze sensor data in multi-modal egocentric activity classification.
- Well-structured and easy-to-read manuscript.
- Collection of a new multimodal dataset from 17 subjects, which is a significant contribution.

Weaknesses:
- Evaluation is self-contained, lacking comparisons with broader state-of-the-art egocentric models.
- Limited dataset size raises concerns about the generalizability of the findings.
- Discussion lacks insights into the performance of state-of-the-art models and the implications of the achieved scores.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by comparing their hybrid model against existing state-of-the-art models on their dataset or by including common datasets from the literature. Justifying the selected classes and the length of data collection for each class would strengthen the claims. Additionally, clarifying the definition of LOSO, the implications of the achieved performance rates, and the rationale for using a weighted F1 Score would enhance the manuscript. The authors should also consider revising the methodology to avoid reinventing basic ML concepts and ensure clarity in their explanations. Lastly, providing access to the dataset and code would benefit the community.