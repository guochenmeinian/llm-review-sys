ID: HbV5vRJMOY
Title: Mixture of Nested Experts: Adaptive Processing of Visual Tokens
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 4, 7, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Mixture of Nested Experts (MoNE) framework, which enhances the Mixture-of-Experts paradigm by introducing a hierarchical structure for processing visual tokens in images and videos. The authors propose a method where experts are defined as nested subsets of channels in Feed Forward Networks, allowing for varying compute costs. A dynamic budget allocation heuristic is introduced to adapt the classical load balancing loss for experts with different capacities. The evaluation is primarily conducted on ImageNet-21K and video classification benchmarks, demonstrating competitive performance in terms of accuracy and efficiency. However, the paper lacks a comprehensive comparison with existing dynamic pruning methods and does not include throughput data for these methods.

### Strengths and Weaknesses
Strengths:
- The routing strategy is learned early in the transformer layers, optimizing hardware efficiency by determining channel activation upfront.
- The rationale for having experts with different compute costs is well-founded, enhancing the accuracy-efficiency trade-off.
- The proposed MoNE framework demonstrates a technically sound approach to dynamic computing without increasing parameter count.
- The paper includes robust ablation studies on router placement and provides substantial gains in latency and efficiency.

Weaknesses:
- **Fixed number of experts:** The rationale for using a fixed number of nested blocks is unclear; a more flexible approach could enhance load balancing and capacity distribution.
- **Baselines:** The experimental section lacks sufficient baseline comparisons, particularly for dynamic routing in video classification tasks. Comparisons with simpler mixtures of experts and token pruning methods are notably absent.
- **Dynamic budget allocation relevance:** The proposed heuristic shows performance comparable to a uniform allocation, raising questions about its significance.
- **Inference speed:** The paper primarily focuses on FLOPs rather than actual throughput, leaving the inference speed of the models unaddressed.
- The evaluation is limited to specific benchmarks, making it difficult to assess the method's performance against existing dynamic models. The reliance on ImageNet-21K, which is less standard, may affect the paper's robustness.

### Suggestions for Improvement
We recommend that the authors improve the clarity around the fixed number of experts by exploring dynamic sparsity literature and considering a more flexible routing mechanism. Additionally, we suggest including more baseline comparisons, particularly with dynamic routing methods and token pruning techniques, to strengthen the evaluation. The authors should also provide direct comparisons of training and inference speeds to address concerns about practical deployment. Furthermore, we recommend improving the discussion and comparison of their method with existing dynamic computing approaches, particularly by including throughput data for dynamic pruning methods. Reporting results on ImageNet-1K alongside ImageNet-21K would provide a more comprehensive evaluation. Finally, clarifying the implementation details regarding padding and the handling of tokens with varying dimensions in the manuscript would enhance understanding.