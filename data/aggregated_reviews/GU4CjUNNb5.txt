ID: GU4CjUNNb5
Title: Reflection System for the Abstraction and Reasoning Corpus
Conference: AAAI
Year: 2024
Number of Reviews: 3
Original Ratings: 7, 6, 7
Original Confidences: 5, 4, 4

Aggregated Review:
### Key Points
This paper presents an augmentation technique that enhances performance for LLMs on the Abstraction and Reasoning Corpus (ARC) and demonstrates that fine-tuning based on these augmentations further improves results. Additionally, it introduces a reflection system that effectively combines multiple independent solvers. The authors propose AugARC, an enhanced benchmark that broadens the scope of ARC tasks and achieves a record accuracy of 166/400.

### Strengths and Weaknesses
Strengths:
- Identification of augmentations that improve ARC performance for LLMs.
- Comprehensive evaluation of QLoRA fine-tuning on AugARC across different models.
- In-depth analysis of solution overlap among various algorithms.
- Proposal of a reflection system that selects the most promising outputs from candidates.
- Evaluation on a complete dataset, facilitating comparison.
- Strong limitations section.

Weaknesses:
- Similarities between the test-time augmentation scheme in AugARC and that proposed by Bober-Irizar and Banerjee (2024).
- Minor performance improvements over DSL search with significantly higher computational resources.
- Lack of clarity regarding the prompting scheme for the reflection model.
- Insufficient insights into the performance of the reflection model.
- Ablation studies lack scientific rigor.

### Suggestions for Improvement
We recommend that the authors clarify the exact prompting scheme for the reflection model and provide insights into its performance. The authors should address the inaccuracies in Figure 3, where the reflection model selects the wrong solution, and revise the caption accordingly. On page 4, we suggest referring to AugARC as an augmentation for ARC rather than a benchmark to avoid confusion. Additionally, sorting Figure 4 by performance would enhance readability. The authors should normalize Figure 5a by the total number of tasks and ensure that the models are ordered correctly based on their contributions. We encourage the authors to explore the impact of permutations as augmentations more rigorously, considering potential dataset imbalances. Detailed ablations on the reflection system's contributions and theoretical insights into its effectiveness would strengthen the paper. Finally, the conclusion should better reflect the paper's content and contributions to the original goals of ARC.