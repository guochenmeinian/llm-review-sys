ID: 79eWvkLjib
Title: Zero-Shot Reinforcement Learning from Low Quality Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 3, 6, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 2, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates methods for zero-shot reinforcement learning (RL) trained on small, homogeneous datasets, addressing the practical challenges when large, heterogeneous datasets are unavailable. The authors identify that existing methods overestimate out-of-distribution (OOD) state-action values in low-quality datasets and propose incorporating conservatism into zero-shot RL algorithms to mitigate these issues. Their experimental results show that conservative zero-shot RL methods outperform non-conservative counterparts on low-quality datasets while maintaining competitive performance on high-quality datasets.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant gap in zero-shot RL by focusing on the challenges of small, homogeneous datasets, which are common in real-world applications.
- The proposed conservative zero-shot RL methods consistently outperform non-conservative counterparts on low-quality datasets without degrading performance on high-quality datasets.
- The writing is clear, and the experiments effectively validate the proposed methods.

Weaknesses:
- The problem definition is questionable, as the work introduces another dataset, which may not align with the concept of zero-shot RL. Additionally, the code link is invalid.
- The introduction of conservatism increases algorithm complexity, potentially posing implementation challenges for practitioners.
- There is limited validation of the proposed methods in real-world settings, affecting their applicability.
- The paper lacks a clear definition for "low-quality data," which could be better described as "coverage." A more explicit definition is recommended.
- The comparison of baselines is incomplete, missing an offline goal-conditioned baseline, which is relevant to the tasks considered.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the problem definition to align with zero-shot RL principles and ensure the code link is functional. Additionally, we suggest providing a more explicit definition of "low-quality data" and including a representative offline goal-conditioned reinforcement learning baseline, such as GC-IQL, to strengthen the comparative analysis. Furthermore, a discussion of related works in the offline GCRL domain would enhance the comprehensiveness of the study. Lastly, consider providing more background and intuition about zero-shot RL to make the paper accessible to a broader audience.