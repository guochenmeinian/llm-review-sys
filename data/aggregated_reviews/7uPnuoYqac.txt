ID: 7uPnuoYqac
Title: Federated Learning with Manifold Regularization and Normalized Update Reaggregation
Conference: NeurIPS
Year: 2023
Number of Reviews: 23
Original Ratings: 5, 5, 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel federated learning algorithm, FedMRUR, which utilizes hyperbolic graph manifold regularization and a normalized aggregation scheme to address model inconsistency caused by local data heterogeneity. The authors claim that FedMRUR achieves state-of-the-art performance with improved convergence properties, including a linear speedup. They provide a theoretical contribution that enhances the constant of the optimal bound and guarantees convergence for non-convex objectives under partial client participation. Empirical results demonstrate that FedMRUR achieves higher test accuracy more quickly than competitors, supported by extensive numerical evidence, particularly outperforming existing algorithms like FedCM and MoFedSAM.

### Strengths and Weaknesses
Strengths:
- The proposed method is theoretically robust, providing convergence guarantees for smooth non-convex scenarios, which are prevalent in the literature.
- The work is clearly articulated, addressing a significant issue in federated learning, namely model inconsistency.
- The paper offers a significant theoretical contribution regarding the optimal bound and hyperbolic regularization.
- Experimental results indicate that FedMRUR outperforms existing algorithms, particularly in terms of accuracy across various Dirichlet settings and faster convergence compared to FedCM and MoFedSAM.

Weaknesses:
- The presentation is often confusing, with unclear connections between the proposed algorithm and theoretical arguments, particularly in Section 3.2.
- Some technical details, such as the use of "hyperbolic" and "Lorentzian" terms, are inadequately explained, leading to potential misunderstandings.
- The convergence theory may be perceived as flawed due to the use of a regularized function, which differs from the original federated learning problem.
- The comparison with MoFedSAM lacks clarity regarding the convergence rate, raising questions about the novelty and effectiveness of the proposed refinements.
- The complexity of hyperbolic regularization in conjunction with neural networks raises concerns about practical applicability and parameter tuning.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation, particularly in linking Algorithm 1 with the arguments in Section 3.2. Specifically, elucidate how "mapped Lorentzian vectors" are utilized in Algorithm 1 and clarify the definitions of terms like "hyperbolic" and "Lorentzian" in the context of the algorithm. Additionally, we suggest providing a more detailed discussion on the convergence rate comparison with MoFedSAM to substantiate the claimed improvements. It would also be beneficial to clarify the notation in figures and algorithms, ensuring consistency throughout the paper. Furthermore, we recommend that the authors enhance the clarity of their theoretical arguments regarding the convergence rates and the implications of using a regularized function. Providing further empirical comparisons to strengthen the claims about hyperbolic regularization, particularly addressing concerns about its complexity and applicability in broader contexts, would also be advantageous. Lastly, enhancing the organization of the paper could help in presenting their ideas more cohesively.