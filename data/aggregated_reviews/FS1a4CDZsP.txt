ID: FS1a4CDZsP
Title: PAC-tuning: Fine-tuning Pre-trained Language Models with PAC-driven Perturbed Gradient Descent
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents PAC-tuning, a two-stage fine-tuning process for pre-trained language models (PLMs) aimed at enhancing generalization performance in few-shot learning scenarios. The authors propose to minimize the PAC-Bayes bound in the first stage and utilize the learned noise variance for further fine-tuning in the second stage. Experimental results on the GLUE benchmark demonstrate competitive performance, particularly in small-sample text classification tasks.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured, providing comprehensive explanations of the methodology and experimental design.
- PAC-tuning shows strong practicality and outperforms conventional fine-tuning methods, indicating its effectiveness.
- The technical robustness of the proposed method is evident, particularly in its approach to minimizing the PAC-Bayes bound.

Weaknesses:
- The method introduces a lengthy additional stage in the fine-tuning process, which may hinder practicality; an ablation study on the necessity of this phase is recommended.
- There is a lack of in-depth analysis regarding the stability and convergence properties of PAC-tuning, as well as insufficient details on hyperparameter settings.
- The experimental comparisons are limited, particularly regarding state-of-the-art methods and the performance of the tuned model after the first stage.
- The results on the GLUE tasks are sometimes contradictory, and the low GPT-2 scores suggest potential issues with the model's suitability for few-shot learning.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the necessity of the two-stage design and provide a comparison of tunable parameters and tuning time budgets across methods. An ablation study should be conducted to assess the impact of Phase 1 on overall performance. Additionally, the authors should explore the incorporation of PAC-tuning with other fine-tuning techniques, such as pruning and data augmentation. A more comprehensive analysis of hyperparameter settings and the computational complexity associated with PAC-Bayes bounds is also advised. Finally, expanding the experimental evaluation to include more tasks, such as NLG, and comparing with current state-of-the-art methods would strengthen the paper.