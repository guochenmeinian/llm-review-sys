ID: wMbe8fVjgf
Title: Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators
Conference: NeurIPS
Year: 2023
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: 5, 3, 4

Aggregated Review:
### Key Points
This paper presents a novel approach to matrix multiplication in the context of quantization, specifically focusing on the product accumulation aspect. The authors propose accumulating in lower precision, arguing that high-precision accumulation in FP16 is a computational bottleneck, particularly when weights and activations are in FP8. The work aims to influence future ML accelerator designs by demonstrating that 12 bits is the lowest effective precision for accumulation, while also highlighting the limitations of lower bit-widths.

### Strengths and Weaknesses
Strengths:
- The authors define and rigorously test a methodology for lower precision accumulation.
- Empirical evidence supports the efficacy of their methods across various tasks, including ImageNet with ResNet and SQuAD with BERT.
- The problem statement and the use of the Straight-Through Estimator (STE) for backpropagation are clearly articulated.

Weaknesses:
- The motivation regarding accumulation as a bottleneck requires more substantiation, particularly in relation to gate count and its implications.
- The quantized FMA is only evaluated in the forward pass, raising questions about its applicability in fine-tuning and pre-training contexts.
- The paper lacks a comprehensive evaluation of overflow, underflow, and swamping effects across different architectures.
- There is insufficient clarity on certain terms, such as "W/A quantization," and the characterization of FP16 accumulation as experimental does not align with its mainstream adoption.

### Suggestions for Improvement
We recommend that the authors improve the motivation section by providing clearer evidence and explanations regarding the bottleneck nature of accumulation in matrix multiplication. It would be beneficial to include a discussion on the impact of quantizing different parts of the matrix multiplication process. We suggest testing the quantized FMA in the backward pass to validate the conclusions drawn for fine-tuning. Additionally, the authors should consider demonstrating the effects of overflow, underflow, and swamping across various architectures for a more comprehensive evaluation. Including compression rates and a comparison of training and inference times in the results would enhance the paper's clarity and depth. Lastly, we advise eliminating instances of tautology to improve readability.