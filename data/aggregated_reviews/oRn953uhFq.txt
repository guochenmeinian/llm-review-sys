ID: oRn953uhFq
Title: Analyzing the Sample Complexity of Self-Supervised Image Reconstruction Methods
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 5, 5, 7, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of the sample complexity involved in learning a linear denoiser using a self-supervised learning loss, and it verifies these bounds through experiments on linear denoising. The authors also investigate the performance gap between self-supervised and supervised learning in image reconstruction, particularly for denoising and compressive MRI tasks. The findings indicate that self-supervised losses, when unbiased estimators of supervised losses, can achieve comparable performance to supervised learning with sufficient sample sizes.

### Strengths and Weaknesses
Strengths:
- A theoretical bound demonstrates the significance of sample complexity in self-supervised learning for image denoising, presenting a bound that scales as 1/N, where N is the dataset size.
- The empirical evaluation of the gap between self-supervised and supervised learning shows that self-supervised training can match supervised performance as the number of training examples increases.

Weaknesses:
- There is a disconnect between the theoretical analysis of linear denoisers and the empirical results on non-linear denoising with deep networks, raising questions about the applicability of the linear theory to non-linear settings.
- Some assumptions in the main theorem appear unrealistic for deep learning contexts, such as training networks on multiple epochs rather than a single pass.
- The theoretical approach seems to extend existing work without clear contributions, and the paper does not address the implications of using a SURE-based loss, which could also serve as an unbiased estimator for denoising.

### Suggestions for Improvement
We recommend that the authors improve the linkage between the theoretical analysis of linear denoisers and the empirical results on non-linear denoising to clarify the applicability of their findings. Additionally, we suggest that the authors address the unrealistic assumptions in their main theorem to better align with deep learning practices. It would also be beneficial to discuss the limitations of applying linear denoiser analysis to non-linear denoisers more thoroughly. Lastly, we encourage the authors to explore the implications of using a SURE-based loss in their analysis.