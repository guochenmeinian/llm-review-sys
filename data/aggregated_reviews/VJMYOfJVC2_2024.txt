ID: VJMYOfJVC2
Title: WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents WISE, a novel technique for editing and inserting facts in autoregressive language models (LMs) within a lifelong learning framework. The approach balances reliability, generalization, and locality by utilizing a dual memory system comprising a main memory and multiple side-memories, with a router to determine the appropriate memory for a given query. The authors evaluate WISE on various datasets, demonstrating its effectiveness in updating knowledge while addressing challenges inherent in lifelong editing.

### Strengths and Weaknesses
Strengths:
- WISE offers a balanced performance in reliability, generalization, and locality for fact editing in LMs.
- The paper provides clear motivation, solid technical contributions, and a variety of experimental configurations.
- The routing mechanism and knowledge-sharding approach are innovative and effectively mitigate performance degradation due to knowledge overlap.

Weaknesses:
- Clarity on technical details, such as the routing activation and parameter overlap, is lacking.
- The complexity of the WISE system, including its numerous components and configurations, may hinder understanding.
- The experiments are limited to small LLMs, raising concerns about the scalability and behavior of WISE with larger models.

### Suggestions for Improvement
We recommend that the authors improve clarity on the routing activation by specifying which token in the prompt it corresponds to and addressing the parameter overlap calculation more comprehensively. Additionally, we suggest that the authors explore the implications of sequential editing scenarios, such as conflicting knowledge over time and multi-hop edits. To enhance the experimental robustness, consider testing WISE with larger LLMs to uncover potential caveats. Lastly, we advise reducing the negative vertical space in formatting by relocating less critical analyses to the appendix or utilizing the extra content page for the final version.