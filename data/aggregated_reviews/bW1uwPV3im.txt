ID: bW1uwPV3im
Title: LoRA: A Logical Reasoning Augmented Dataset for Visual Question Answering
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 6, 7, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LoRA, a novel Logical Reasoning Augmented VQA dataset aimed at enhancing the evaluation of complex logical reasoning in Visual Question Answering (VQA) models. The dataset comprises 200,000 diverse questions generated using SROIQ Description Logic, focusing on intricate logical operations beyond simple conjunctions and disjunctions. The authors propose a generalizable framework for dataset creation based on the OWL format, enabling compatibility with various ontologies. They highlight the dataset's emphasis on layered logical complexities, distinguishing it from existing knowledge-based and visual reasoning datasets. The authors also provide automated scripts for knowledge extraction, question generation, and image generation, promoting community engagement and usability for researchers.

### Strengths and Weaknesses
Strengths:
- The dataset provides a significant advancement in assessing logical reasoning capabilities of VQA models, introducing a comprehensive evaluation setup.
- The novelty and focus on complex logical reasoning are well-recognized, filling a gap in real-world logical reasoning with added complexity.
- The automated dataset creation mechanism is adequately described and offers substantial advantages for customization.
- Comprehensive evaluation metrics for diverse question types and logical difficulties are provided.
- The use of realistic kitchen and food visuals enhances the dataset's applicability to real-world scenarios.

Weaknesses:
- The focus on a specific domain (food and kitchen) raises concerns about the generalizability of findings across diverse fields and may limit its perceived applicability to other areas.
- The methodology for determining question difficulty lacks rigor and requires further justification, with a current lack of performance reporting for a simple knowledge-augmented model.
- The reliance on third-party tools for key experimental stages, particularly image generation, may limit scalability, reproducibility, and transparency.
- The current methods may not fully address uncertainty and ambiguity in logical reasoning, and the dataset exhibits a lower complexity in visual reasoning compared to existing benchmarks.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of the dataset by expanding its focus beyond food and kitchen scenarios to include a broader range of real-world contexts. Additionally, we suggest incorporating uncertainty into the dataset to enhance realism and challenge. An in-depth elaboration on the dataset generation process, particularly regarding ontology creation and question generation, would enhance clarity. We also recommend reassessing the methodology for determining question difficulty with a more rigorous approach, potentially incorporating human validation, to strengthen the findings. Furthermore, including performance metrics for a simple knowledge-augmented model could better demonstrate the dataset's complexity and the challenges it presents in logical reasoning. Lastly, while Blender is a suitable tool for image generation, exploring advanced generative tools like stable diffusion could broaden the dataset's visual diversity.