ID: Srt1hhQgqa
Title: Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 6, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Cappy, a small classifier model designed to predict answers from a set of possibilities or multiple generations from a large language model (LLM). Trained on data from T0, Cappy utilizes LLMs to generate partially correct responses alongside positive and negative examples. The authors evaluate Cappy's performance on T0 evaluation tasks and BIG-Bench, demonstrating that it outperforms same-size models and improves upon a base model when selecting from generations. Cappy acts as a scoring mechanism, enhancing the quality of generated responses without requiring backpropagation through the LLM. Additionally, the authors emphasize a multi-task learning approach that incorporates contrastive information without relying on costly human annotations during pretraining. They compare Cappy with established pretrained multi-task LLMs like T0, FLAN, and OPT-IML, highlighting that these models are not directly comparable to those using human preference data. Experiments using publicly available reward models trained on human preference datasets show that Cappy outperforms these baselines and some multi-task LLMs.

### Strengths and Weaknesses
Strengths:
- The straightforward approach of using a trained classifier as a reranker for model generations is compelling.
- The data gathering scheme for Cappy’s training data and the use of ROUGE-L as a proxy for gold labels are innovative and effective.
- Cappy's ability to adapt a single LLM to various domains using lightweight filters is a notable advantage.
- The incorporation of contrastive information shows a clear advantage over models relying solely on ground truth data.
- The experimental results indicate that Cappy performs well compared to established models, enhancing its credibility.

Weaknesses:
- The novelty of Cappy is questionable, as it closely resembles best-of-N sampling methods previously proposed, lacking sufficient comparison to these prior works.
- Cappy's performance improvements over a frozen LLM are modest compared to fine-tuning, raising concerns about its overall significance.
- The reliance on ROUGE-L as the sole evaluation metric may not provide a comprehensive assessment of performance, and the discussion of tasks where Cappy underperforms is insufficient.
- The initial lack of clarity regarding the adaptation experiments may have hindered understanding of the model's versatility.
- The comparison with human preference data models could be perceived as limited due to the different application domains.

### Suggestions for Improvement
We recommend that the authors improve the novelty aspect by explicitly comparing Cappy to prior best-of-N sampling methods in a related work section, integrating these comparisons as baselines or ablations. Additionally, exploring further baselines and ablation studies, such as contrastive training and encoding multiple samples at once, would enhance the robustness of the findings. It would also be beneficial to include a broader range of evaluation metrics beyond ROUGE-L to provide a more nuanced understanding of Cappy's performance. Furthermore, we suggest that the authors improve the clarity of their adaptation experiments by explicitly stating that they involve “adapting a single LLM to several domains using a lightweight filter.” Finally, we recommend that the authors explore and discuss the potential of Cappy as a filter for generations from multiple LLMs in future versions, including related experiments to support this idea.