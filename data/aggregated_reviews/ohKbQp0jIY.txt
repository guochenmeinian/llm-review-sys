ID: ohKbQp0jIY
Title: Successor-Predecessor Intrinsic Exploration
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 3, 5, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel intrinsic reward mechanism termed Successor-Predecessor Intrinsic Exploration (SPIE) aimed at enhancing exploration in reinforcement learning. Unlike existing methods that primarily utilize prospective information, SPIE integrates both successor representation (SR) and predecessor representation (PR) to compute intrinsic rewards. The authors evaluate SPIE through experiments on discrete grid worlds, continuous environments like MountainCar, and several Atari games, demonstrating its effectiveness in promoting exploration. Additionally, the authors explore the effects of intrinsic rewards on the performance of SARSA algorithms, investigating the relationship between the scale and sign of intrinsic rewards and the exploration efficiency of SARSA-SR, particularly in fixed SR settings. They acknowledge the potential confounding factor of reward scales and have conducted ablation studies to normalize and rescale intrinsic rewards, addressing the need for clearer references to baseline performance scores and the implications of terminal states in their analysis.

### Strengths and Weaknesses
Strengths:
- The integration of both prospective and retrospective information in exploration is a compelling approach.
- The paper is generally clear and well-structured, with thorough experiments in discrete state spaces showing the benefits of retrospective information.
- The authors demonstrate a willingness to engage in discussions and clarify points raised by reviewers.
- They have conducted ablation studies that provide insights into the effects of intrinsic rewards on exploration efficiency.
- The paper acknowledges the importance of referencing performance statistics from the literature and plans to include additional results for better comparison.

Weaknesses:
- The motivation for the proposed method is unclear, with inconsistent focus between continual exploration and singleton tasks.
- The paper lacks sufficient citations and comparisons with recent baselines, particularly in non-stationary continuous state environments.
- Results on the Atari benchmark are not convincing, requiring more comprehensive evaluations and insights.
- The inability to include anonymous links to visualizations may limit accessibility.
- The exploration performance of SARSA-SR remains significantly lower than SARSA-SRR, indicating room for improvement.
- The analysis of trade-offs involving non-rewarding terminal states is still preliminary and lacks clear patterns.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation by consistently addressing the focus on continual exploration versus singleton tasks throughout the paper. Additionally, the authors should include more citations and comparisons with recent methods, such as NovelD, NGU, and E3B, to contextualize their contributions. It is also essential to provide more detailed insights into the Atari results, including comparisons with other environments like Gravitar and Solaris, and to discuss the performance discrepancies observed. Furthermore, we suggest conducting experiments to validate claims regarding bottleneck states and the implications of the intrinsic reward structure in both discrete and continuous settings. We recommend improving the clarity of references to baseline performance scores in the main text or captions and including training curves to enhance understanding of RND's performance over time. We encourage the authors to conduct further investigations into the trade-offs between extrinsic rewards and the sum of intrinsic rewards, particularly in scenarios with multiple non-rewarding terminal states. Lastly, ensuring that all visualizations are included in the camera-ready version will enhance the overall presentation of their findings.