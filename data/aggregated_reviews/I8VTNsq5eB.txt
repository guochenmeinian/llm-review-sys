ID: I8VTNsq5eB
Title: CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 6
Original Ratings: -1, -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CESAR, a novel framework for the automatic induction of compositional instructions in multi-turn dialogs. The authors apply CESAR to the InstructDial benchmark, demonstrating its scalability and ability to generate rich instructions. The study enhances InstructDial with new datasets and tasks, showcasing CESAR's performance on both atomic and compositional tasks. The paper also introduces InstructDial++, a benchmark to evaluate models' capabilities in handling compositional tasks.

### Strengths and Weaknesses
Strengths:  
1. CESAR addresses the challenge of generating complex instructions for dialog tasks, unifying multiple dialog tasks in a consistent format.  
2. The paper expands the InstructDial benchmark significantly, contributing 68 new downstream tasks and demonstrating strong experimental results.  
3. The framework is well-defined and scalable, with abundant experiments validating the authors' claims.  

Weaknesses:  
1. The paper lacks a detailed explanation of the CESAR framework, making it difficult for readers to grasp the methodology fully.  
2. There is insufficient evaluation and comparison with existing methods, particularly regarding the performance of open-source models versus closed-access models.  
3. The individual quality of the new tasks is unclear, raising concerns about the overall contribution of the majority of tasks.  
4. The paper does not adequately differentiate itself from FLAN-T5, which already includes several dialogue tasks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the CESAR framework by providing more detailed explanations of its methodology and implementation. Additionally, we suggest including thorough evaluations and comparisons with existing methods, particularly focusing on open-source models like Alpaca, Vicuna, and Llama2-Chat. It would be beneficial to present qualitative examples comparing CESAR and GPT-3.5-turbo to analyze performance trends. Furthermore, we encourage the authors to clarify how compositional tasks are generated in a scalable manner and to ensure that the dataset used for evaluations is robust and well-defined.