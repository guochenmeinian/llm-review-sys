ID: MCVfX7HgPO
Title: Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 6, 7, 7, 6, -1, -1, -1
Original Confidences: 5, 4, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a programmable natural language reasoning dataset, PrOntoQA-OOD, designed to evaluate the out-of-distribution generalization abilities of large language models (LLMs). The authors test four LLMs—FLAN-T5, LLaMA, GPT-3.5, and PaLM—across various deduction rules, proof width/depth, and rule compositions. The findings indicate that LLMs exhibit mixed generalization capabilities to unseen deduction rules and demonstrate more robust generalization to longer, wider, and compositional proofs.

### Strengths and Weaknesses
Strengths:
- The dataset construction is solid, considering various deduction rules and enabling control over proof complexity.
- The experimentation is thorough, providing valuable insights into LLMs' reasoning capabilities.
- The paper is well written and organized, making it easy to follow.

Weaknesses:
- The failure to contextualize the paper with relevant prior work, particularly the benchmark for theorem proving, is a significant oversight.
- The omission of three rules from the deduction framework raises concerns about the completeness and soundness of the claims made.
- The presentation of OOD generalization performances in terms of $\Delta$ proof accuracies lacks clarity, particularly for FLAN-T5, which may mislead interpretations of generalization.
- Some figures lack context, as they present predicted and expected answers without the corresponding questions.
- The claim that theorem proving and medical diagnosis are purely deductive reasoning problems lacks rigor, as both tasks also require inductive reasoning.

### Suggestions for Improvement
We recommend that the authors improve the contextualization of their work by discussing relevant prior research, particularly the benchmark for theorem proving. Additionally, the authors should clarify the differences between the deduction rules used and those in Gentzen's natural deduction to support their completeness and soundness claims. To enhance clarity, we suggest omitting uncertain size claims for models and presenting absolute OOD proof accuracies. Furthermore, including the questions alongside predicted and expected answers in figures would improve comprehension. Lastly, we encourage the authors to refine their claims regarding reasoning types to accurately reflect the complexity of the tasks discussed.