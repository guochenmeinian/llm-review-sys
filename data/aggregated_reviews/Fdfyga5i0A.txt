ID: Fdfyga5i0A
Title: Mnemosyne: Learning to Train Transformers with Transformers
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Mnemosyne, a novel learnable optimizer utilizing transformers to train neural networks without task-specific tuning. The authors propose a spatio-temporal low-rank implicit attention mechanism to enhance optimization efficiency, claiming it outperforms LSTM optimizers and matches state-of-the-art methods. The architecture leverages attention to encode long-term dependencies and includes a theoretical analysis of the compact associative memory involved.

### Strengths and Weaknesses
Strengths:  
- The innovative approach of using transformers for optimization is compelling and addresses practical limitations in existing methods.  
- Mnemosyne shows superior performance in fine-tuning various models, indicating broad applicability and robustness.  
- The paper includes a substantial amount of work, including theoretical results and practical hacks to improve performance, which seems sufficient for acceptance at NeurIPS.  

Weaknesses:  
- The effectiveness of the method on broader model training is unclear, as it only fine-tunes a subset of transformer layers, raising concerns about its generalizability.  
- The lack of source code is a significant drawback, as reproducibility is increasingly important in research.  
- The authors do not adequately motivate their architectural choices or discuss the trade-offs involved, limiting the methodological depth of the work.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the associative memory section to better articulate its significance and implications. Additionally, providing source code would enhance the paper's reproducibility. It would be beneficial to include a more detailed discussion of the meta-training procedure and its robustness, as well as a critical analysis of the strengths and weaknesses of the proposed method. Finally, addressing the scalability of Mnemosyne to larger models and providing empirical validation for claims regarding long-term dependencies would strengthen the paper.