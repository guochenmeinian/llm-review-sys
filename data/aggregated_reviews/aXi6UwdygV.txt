ID: aXi6UwdygV
Title: Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a transformer-based model for supervised phonological reconstruction and cognate reflex prediction, utilizing a method from computational biology (MSA transformer). The authors report experimental results demonstrating improvements over existing models, particularly in the context of a SIGTYP 2022 dataset and a protolanguage reconstruction dataset. The paper includes a detailed error analysis and explores zero-shot reconstruction, contributing to the understanding of diachronic sound changes.

### Strengths and Weaknesses
Strengths:
- The method effectively adapts novel ideas from computational biology, showing improvements over state-of-the-art models.
- The paper is meticulously documented, facilitating reproducibility.
- The error analysis is comprehensive and provides valuable insights into sound change errors.

Weaknesses:
- The novelty of the work is limited, primarily applying an existing method to a new domain without sufficient justification for adaptations made from previous models.
- Some sections, such as the description of Multiple Sequence Alignment and performance decrease during trimming, lack clarity and justification.
- The paper does not adequately address significance testing for all experimental results, and some details regarding the model architecture and its advantages are insufficiently discussed.

### Suggestions for Improvement
We recommend that the authors improve the clarity and justification in Section 3.1 regarding the choice of Multiple Sequence Alignment methodology. Additionally, the authors should provide a clearer rationale for the modifications made to the model architecture compared to previous works. It would be beneficial to include significance testing for all results and to clarify the performance gains attributed to the model architecture versus pretraining. Furthermore, addressing the questions regarding the t-test values and providing scores per language family in an appendix would enhance the paper's rigor. Lastly, we suggest correcting typographical errors and ensuring uniformity in spacing throughout the document.