ID: zIEaOZ0saA
Title: New Complexity-Theoretic Frontiers of Tractability for Neural Network Training
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 7, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 2, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the computational complexity of training ReLU and linear neural networks, focusing on conditions under which the training problem can be solved in polynomial time. The authors propose two main contributions: a polynomial-time algorithm for training constant-size ReLU networks with hidden neurons having an out-degree of exactly one, extending previous results for depth-2 networks, and the identification of a condition termed "untangling" for linear networks, which allows for efficient learning if provided. While finding an untangling is NP-hard in general, it can be determined in linear time for networks with a constant number of hidden neurons or constant treewidth. Additionally, the paper examines upper bounds related to neural network architectures, specifically addressing potential misinterpretations of claims regarding "bounded-size" architectures. The authors plan to clarify these claims in the final version and discuss their "blow-up theorem" (Theorem 10), which asserts that the size of the resulting architecture depends solely on the original architecture's size, ensuring that if the original architecture is bounded, the new one will be as well.

### Strengths and Weaknesses
Strengths:
- The paper addresses significant questions in neural network training, contributing to the understanding of when efficient solutions are possible for linear and ReLU networks.
- It is well-written and presents a clear exposition of the results and related literature.
- The authors demonstrate a commitment to clarity by planning to elaborate on the term "bounded-size" to avoid misinterpretation.
- The formal statements in the paper effectively convey the intended meanings, providing a solid foundation for the claims made.
- The blow-up theorem is well-articulated, linking the size of the new architecture to the original, which is a significant contribution to solving the NNT problem.

Weaknesses:
- The requirement for hidden neurons to have an out-degree of exactly one limits the applicability of the results, as practical deep networks do not conform to this structure.
- The paper's reliance on previously established techniques raises questions about the novelty of the contributions, particularly regarding the technical differences from prior works like Arora et al. (2018).
- The algorithms presented are primarily of theoretical interest, as the tractable cases are often unrealistic and the running times depend heavily on fixed constants.
- There is a potential for confusion regarding the implications of the blow-up theorem, particularly in relation to the construction of networks that fit distinct input-output pairs.
- The authors may need to address the nuances of the blow-up theorem more explicitly to prevent misinterpretation.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the implications of transforming networks to those with out-degree one, clarifying the motivation behind this approach. Additionally, we suggest elaborating on the main technical differences between their results and those of Arora et al. (2018), particularly regarding the contributions of Lemmas 1-4. It would also be beneficial to address the limitations of the assumptions made, such as the out-degree condition and the implications of focusing solely on minimizing training error without considering generalization. Furthermore, we recommend that the authors improve the clarity of the claims regarding "bounded-size" architectures by expanding on this term in the final version. Lastly, we suggest that the authors provide a more detailed discussion of the implications of Theorem 10, particularly concerning the construction of networks that perfectly fit distinct inputs and outputs, to enhance understanding and address potential confusion.