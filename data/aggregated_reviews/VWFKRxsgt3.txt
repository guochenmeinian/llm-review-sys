ID: VWFKRxsgt3
Title: ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper introduces ZeroSCROLLS, a benchmark for evaluating zero-shot natural language understanding over long texts, comprising 10 datasets with 500 samples each. The authors evaluate several state-of-the-art large language models (LLMs) and find that while GPT-4 achieves the highest scores, there remains significant room for improvement in challenging tasks like sentiment aggregation. The paper also addresses output formatting discrepancies among models.

### Strengths and Weaknesses
Strengths:  
- The paper presents a comprehensive benchmark for long text understanding and includes rigorous validation of dataset construction methods.  
- The overall presentation is clear and easy to follow, with well-supported claims.  
- Extensive experimentation is conducted, and data and code will be made available.

Weaknesses:  
- The motivation for the new benchmark compared to the existing SCROLLS benchmark is unclear, particularly regarding the rationale for not using SCROLLS' test set for zero-shot evaluation.  
- The contribution appears limited, as the benchmark seems like a subset of SCROLLS, lacking clarity on dataset selection and evaluation aspects.  
- The analysis of experimental results is insufficient, with a need for more in-depth insights, particularly regarding performance variations among models.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation and differences between ZeroSCROLLS and the SCROLLS benchmark, specifically addressing why a new dataset is necessary. Additionally, we suggest providing more detailed analysis of the experimental results, including ordering the difficulty of tasks and explaining performance anomalies, such as the minimal performance gain of FLAN-T5 when increasing token limits. Finally, we encourage the authors to include the dataset in the submission materials for better reproducibility.