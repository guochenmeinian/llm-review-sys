ID: kWTvdSSH5W
Title: A Data-Centric Perspective on Evaluating Machine Learning Models for Tabular Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 8, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper benchmarks tree, neural network, and AutoML methods on 10 Kaggle tabular datasets, comparing them with hyperparameter optimization (HPO), manual feature engineering, and test-time adaptation techniques. The authors demonstrate that performance gaps diminish with advanced techniques, emphasizing the significance of feature engineering and test-time adaptation for top solutions. They find that while some methods achieve top 1% results without dataset-specific tricks, all methods reach this level when all tricks are applied. The authors provide code for experiment reproduction and data preprocessing, proposing an open-source framework to facilitate further research and benchmarking in tabular data analysis. They acknowledge the need for improved reproducibility by including package versions and additional datasets, while also addressing performance discrepancies observed with AutoGluon compared to their implementations. The paper also highlights the progress made in general-purpose neural network architectures, such as MLP-PLR and FTTransformer, particularly when feature engineering is applied to categorical data.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, clear, and presents a sound experimental design with extensive evaluations.
- It includes multiple model families and an AutoML system, providing a comprehensive comparison.
- The use of private leaderboard scores and provided code enhances the trustworthiness of the results.
- The methodology and presentation are well-recognized, showcasing the potential impact of the contribution.
- The work addresses a relevant and timely topic, contributing significantly to the understanding of tabular data modeling.

Weaknesses:
- The reproducibility of the provided code is limited due to the absence of specified package version ranges or a frozen requirements file.
- The dataset selection is insufficient, with only 10 datasets, which may not adequately expose the strengths and weaknesses of different methods and raises concerns about overfitting.
- There is minimal discussion on training time and compute resource differences among methods.
- The reliance on Kaggle leaderboard metrics may obscure absolute performance differences and lacks repeated evaluations for robustness.
- Some claims regarding the progress of general-purpose architectures and the challenges posed by categorical features require further elaboration and clearer evidence.

### Suggestions for Improvement
We recommend that the authors improve the reproducibility of their code by including package version details, such as those for CatBoost, XGBoost, LightGBM, and AutoGluon, along with a `requirements.txt` and `requirements_frozen.txt` file. Additionally, expanding the benchmark to include more datasets, particularly multiclass and regression tasks, will enhance its impact and applicability. We suggest incorporating a Pareto frontier plot of leaderboard placement versus training time to provide insights into the trade-offs between performance and resource usage. Furthermore, a discussion on the differences in training time and compute resources for each method would be beneficial. Clarifying the distinction between preprocessing and feature engineering, as well as providing more details on the dataset selection process, will improve the paper's clarity. Lastly, we recommend that the authors elaborate on the claims regarding the progress of general-purpose architectures and clarify the conclusions drawn about categorical features by explicitly linking them to relevant experimental results. Including the exact date of the leaderboard snapshot in the main text and visualizing performance variations more effectively would also strengthen the paper's conclusions.