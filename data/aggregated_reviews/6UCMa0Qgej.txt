ID: 6UCMa0Qgej
Title: Adversarial Model for Offline Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 4, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Adversarial Model for Offline Reinforcement Learning (ARMOR), a model-based framework designed to robustly learn policies that improve upon any reference policy through adversarial training of a Markov decision process (MDP) model. ARMOR optimizes for worst-case performance relative to the reference policy, ensuring robust policy improvement regardless of data quality. The authors provide theoretical proofs supporting ARMOR's performance and demonstrate its effectiveness through a scalable implementation that achieves competitive results on D4RL benchmarks using only a single model. Additionally, the paper discusses the relationship between the objective $\max_\pi \min_M \max_{\pi_{ref}} J_M(\pi) - J_M(\pi_{ref})$ and regret minimization, highlighting concerns about overly conservative policies. The authors clarify that the objective can lead to conservativeness due to the nature of the optimization process and the assumptions made in the analysis, particularly regarding the fixed reference policy $\pi_{ref}$ in ARMOR. Theorem 3 is emphasized as crucial for establishing the non-negativity of the objective, which is necessary for proving the RPI property.

### Strengths and Weaknesses
Strengths:
1. The proposed method is theoretically supported and offers a novel perspective on learning dynamics models in offline RL.
2. Practical implementation details, including hyperparameter selection, are discussed thoroughly.
3. ARMOR achieves competitive performance while utilizing only a single model, which is significant for sophisticated dynamics modeling.
4. The authors effectively address most reviewer concerns and clarify complex theoretical points, particularly regarding the implications of the non-negativity of the objective in ARMOR.
5. The discussion includes a relevant counterexample that illustrates the limitations of approximating the optimal policy in offline reinforcement learning.

Weaknesses:
1. The assumptions regarding the realizability of the model and policy classes may be overly demanding, and it is unclear how these hold in practice, particularly with misspecified models or policies.
2. The distinction between the reference policy and the behavior policy appears artificial, as the reference policy is often derived from behavior-cloned policies on similar datasets.
3. Claims of robustness to hyperparameters lack direct experimental evidence, necessitating further details on the selection and scope of hyperparameter sets.
4. Differences in practical implementation from baseline methods raise questions about whether performance gains stem from ARMOR or from these implementation choices.
5. Some reviewers express confusion regarding the relationship between the assumptions and the conclusions drawn, particularly about the ill-posed nature of the optimization when negative values are involved.
6. The clarity of the writing, especially in Section 4.1, is noted as needing improvement to enhance reader comprehension.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their assumptions regarding model and policy realizability and provide empirical evidence demonstrating the robustness of ARMOR across various hyperparameter settings. Additionally, we suggest that the authors clarify the conceptual distinction between the reference and behavior policies, possibly by exploring scenarios where they differ significantly. It would also be beneficial to include experiments that validate the claim of robust policy improvement across multiple reference policies rather than a single one. Furthermore, we encourage the authors to enhance the presentation of their results, particularly in Figures 2 and 3, to ensure that they effectively communicate the underlying motivations and findings. We also recommend improving the clarity of Section 4.1 by restructuring it to enhance readability, specifically by placing the "Connection to the Theoretical Formulation" first to provide a coherent transition from Eq (1) to the implementation details. Additionally, we suggest including more discussion on related studies that utilize adversarial approaches for model learning, particularly the study [2], which presents an opposing objective in adversarial model learning. Finally, we encourage the authors to provide a specific revision plan with line numbers or section references to address the written issues raised by reviewers.