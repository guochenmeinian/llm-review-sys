ID: PoMCId4iez
Title: From Dissonance to Insights: Dissecting Disagreements in Rationale Construction for Case Outcome Classification
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel dataset that annotates facts leading to specific legal case outcomes and analyzes the disagreement between two human experts in the context of Case Outcome Classification (COC). The authors introduce a taxonomy to categorize the reasons for disagreement and measure the alignment of state-of-the-art models with expert rationales. The findings indicate low correlation between model results and expert annotations, highlighting the complexities in COC.

### Strengths and Weaknesses
Strengths:
- The dataset and taxonomy are valuable contributions to explainable NLP and provide insights into human subjectivity in legal contexts.
- The annotation process encourages specificity regarding the impact of text on COC, which is a significant benefit.
- The paper is well-written, clear, and presents engaging discussions on the challenges of COC.

Weaknesses:
- The presentation of the paper can be improved for better clarity and understanding.
- The methodology for studying misalignment between experts and models relies solely on Integrated Gradients, which may not provide strong evidence of misalignment.

### Suggestions for Improvement
We recommend that the authors improve the paper's presentation to enhance clarity and understanding. Specifically, please present the dataset characteristics in a table, including statistics on the number of annotated facts per case and the case class outcome distribution. Additionally, clarify the role of the model performance in Table 2 and the context of the "greedy input packing strategy" mentioned in the text. We suggest examining more explainability methods beyond Integrated Gradients to support claims of misalignment between models and expert rationales. Finally, consider involving more experts in the annotation process to mitigate individual bias and enhance the robustness of the findings.