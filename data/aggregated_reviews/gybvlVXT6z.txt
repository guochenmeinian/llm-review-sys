ID: gybvlVXT6z
Title: Black-Box Tuning of Vision-Language Models with Effective Gradient Approximation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Collaborative Black-Box Tuning (CBBT), a method for optimizing textual prompts and adapting output features in vision-language models without access to their internal parameters. The authors demonstrate that CBBT can achieve performance comparable to white-box methods across eleven datasets, addressing the challenges of black-box tuning effectively.

### Strengths and Weaknesses
Strengths:  
- The proposed method shows strong performance in black-box tuning of pre-trained vision-language models, outperforming previous methods in image classification tasks.  
- The authors provide valuable insights into the black-box tuning process by comparing results with white-box settings.  
- Additional results were included during the rebuttal, enhancing the paper's comprehensiveness.

Weaknesses:  
- There is a lack of in-depth theoretical analysis regarding the new gradient estimation method and its implications for convergence.  
- The experimental analysis is insufficient, particularly concerning the impact of prompt length and the absence of key baselines like CoCoOp.  
- Some reviewers noted inconsistencies in conclusions drawn from newly added experiments compared to the main paper.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of the upper bound and its implications for optimization convergence. Additionally, including more comprehensive experimental comparisons with established baselines, such as CoCoOp, and addressing the performance drop associated with prompt length would strengthen the paper. Clarifying the rationale behind the observed performance differences when using learnable prompts and providing details on the computational budget required for the method would also enhance the paper's robustness.