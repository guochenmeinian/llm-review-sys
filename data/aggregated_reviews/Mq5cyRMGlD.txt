ID: Mq5cyRMGlD
Title: VERVE: Template-based ReflectiVE Rewriting for MotiVational IntErviewing
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a template-driven reflective rewriting technique aimed at enhancing motivational interviewing, a counseling strategy that improves counseling outcomes. The authors propose the VERVE framework, which utilizes template editing techniques and introduces methods like paraphrase-augmented training and adaptively template updating. The proposed technique shows significant improvements over baseline methods in both human and automatic evaluations. However, the paper lacks a clear explanation of the importance of reflective listening and motivational interviewing, which may hinder reader comprehension.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- The proposed framework is effective, demonstrating superior performance in reflection scores while retaining original content.
- The study addresses an essential problem in counselor training within the sensitive mental health domain.

Weaknesses:
- There is a notable gap in explaining why reflective listening is beneficial and the significance of motivational interviewing.
- The absence of comparisons with Large Language Models (LLMs) limits the evaluation's comprehensiveness and relevance.
- The selection of baselines is limited, primarily focusing on naive models, which may not adequately represent the current state of the field.

### Suggestions for Improvement
We recommend that the authors improve the introduction by briefly explaining what motivational interviewing is and elaborating on the benefits of reflective listening. Additionally, the authors should consider incorporating comparisons with LLMs to provide a more comprehensive evaluation of their approach. Expanding the selection of baselines to include more contemporary models would strengthen the robustness of their performance comparisons. Lastly, addressing the intricacies of paraphrase-augmented training and including domain-centric metrics for validation would enhance the paper's contributions.