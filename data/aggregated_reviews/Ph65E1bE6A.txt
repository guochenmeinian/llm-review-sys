ID: Ph65E1bE6A
Title: Overcoming Recency Bias of Normalization Statistics in Continual Learning: Balance and Adaptation
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 7, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Adaptive Balance of Batch Normalization (AdaB2N) as a solution to the sub-optimality of Batch Normalization (BN) in continual learning. The authors propose a Bayesian-based strategy for determining task-wise contributions in normalized representations and introduce an adaptive parameter that balances the constant and normalized momentum of exponential moving averages (EMA). The paper includes a comprehensive theoretical analysis, a well-motivated problem statement, and promising experimental results on small-scale datasets.

### Strengths and Weaknesses
Strengths:
- **Comprehensive theoretical analysis:** The paper thoroughly analyzes the behavior of batch normalization in continual learning, highlighting the balance-adaptation dilemma.
- **Well-motivated problem statement:** The limitations of batch normalization in continual learning are clearly articulated.
- **Promising experimental results:** Experimental outcomes on CIFAR and Mini-ImageNet datasets show significant improvements.
- **Interesting analysis experiment:** The proposed method achieves normalization statistics comparable to those of models trained on all tasks jointly.

Weaknesses:
- **Insufficient empirical significance analysis:** The paper lacks a thorough statistical significance analysis of results, particularly regarding standard deviations. Clarification on optimal conditions for the proposed method's performance is needed.
- **Limited method scalability analysis:** Experiments are confined to small-scale datasets, raising questions about scalability to larger datasets like ImageNet.
- **Missing important baselines:** Key baselines, including a simple fine-tuning baseline and the upper bound of no-continual learning (NoCL) performance, are absent.
- **Limited investigation of subtle distribution shifts:** The focus on drastic changes between tasks overlooks the performance of BN under more subtle distribution shifts.

### Suggestions for Improvement
- We recommend that the authors improve the statistical significance analysis by providing detailed conditions under which the proposed method performs optimally and reporting means and standard deviations for offline CL experiments.
- We suggest that the authors address the scalability of the proposed method by conducting experiments on larger datasets, such as the ImageNet subset, and explaining any limitations regarding scalability.
- We encourage the authors to include the missing baselines and compare their method against these baselines to strengthen their findings.
- We recommend that the authors investigate the performance of Batch Normalization in scenarios with more subtle distribution shifts within individual tasks or in a task-free continual learning setup.
- We suggest that the authors clarify the normalization methods used by the joint training baseline and discuss the applicability of their proposed normalization method in the non-continual learning scenario.