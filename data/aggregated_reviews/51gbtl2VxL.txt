ID: 51gbtl2VxL
Title: Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 6
Original Ratings: -1, -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on multimodal machine translation (MMT), focusing on enhancing the utilization of visual information through the generation of Visual Question-Answering (VQA) pairs from source text. The authors propose a multi-task learning framework to incorporate these VQA signals into MMT, demonstrating effectiveness across multiple datasets. A significant contribution is the introduction of the Multi30k-VQA dataset, which is intended to facilitate further research in VQA and MMT.

### Strengths and Weaknesses
Strengths:
- The idea of using VQA to ground MMT in visual contexts is a valuable contribution.
- The Multi30k-VQA dataset and the proposed multi-task learning framework are beneficial for advancing research.
- The paper is well-written and presents clear experimental results that support its claims.

Weaknesses:
- The novelty of the approach is limited, as the concept of incorporating additional signals for training is not new.
- There is insufficient discussion on the quality of the generated Multi30k-VQA corpus, and the results lack statistical significance.
- The paper does not adequately compare its methods with recent state-of-the-art approaches, and the experimental settings are underspecified.
- The assessment of cross-modal interactions relies solely on quantitative analysis, lacking qualitative evidence.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their work by providing a more comprehensive comparison with recent state-of-the-art methods. Additionally, including an ablation study would clarify the contributions of the VQA pairs versus visual features. It is essential to address the quality of the Multi30k-VQA dataset and provide statistical significance for the results. We also suggest enhancing the analysis of cross-modal interactions by incorporating qualitative evidence or visualizations. Lastly, please ensure that the prompt design phase explores in-context learning examples to improve the quality of the synthetic data.