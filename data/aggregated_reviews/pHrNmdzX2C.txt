ID: pHrNmdzX2C
Title: FinGPT: Large Generative Models for a Small Language
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the development of large language models (LLMs) for Finnish, a low-resource language, by gathering and preprocessing data, training models based on the GPT architecture, and extending the BLOOM model. The authors introduce FIN-bench, a Finnish variant of BIG-bench, and create a dataset for NLP studies. The work emphasizes the importance of making model checkpoints freely available and provides a well-documented setup for building custom LLMs.

### Strengths and Weaknesses
Strengths:
- Comprehensive documentation of data sources, preprocessing, and model training.
- Availability of checkpoints for the research community.
- Inclusion of experiments addressing biases and human alignment in model outputs.

Weaknesses:
- The work does not introduce novel methodologies or analyses of prior models.
- Small test set sizes limit the ability to analyze generation patterns effectively.
- Some aspects of the evaluation benchmark and dataset composition could be better clarified.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the evaluation benchmark by adding examples. Additionally, consider conducting significance tests for performance comparisons mentioned on lines 471-472. It would be beneficial to address the potential impact of non-Finnish corpora on bias measurements and to clarify the process of creating and validating test sets. We suggest condensing the "Pretraining" section to focus on key hyperparameter choices and moving less critical details to the appendix. Finally, reformatting tables for improved readability and ensuring uniform font sizes would enhance presentation quality.