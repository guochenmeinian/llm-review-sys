ID: 2z4s0W375H
Title: Tuna: Instruction Tuning using Feedback from Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for fine-tuning an Instruction Tuning model (Student Model) using feedback from a Large Language Model (Teacher Model). The framework includes two ranking methods: Probabilistic Ranking, which utilizes log likelihood ranking from GPT-4 to generate diverse responses for fine-tuning, and Contextual Ranking, where the Student Model generates responses evaluated by GPT-4 based on relevance and accuracy. The main contribution is an NLP Engineering experiment demonstrating the effectiveness of the proposed methods.

### Strengths and Weaknesses
Strengths:
- The experiments are reasonably executable, despite the reliance on proprietary LLMs.
- The paper employs an ablation study to validate the proposed framework and discusses the risks of GPT-4-based evaluations.
- The proposed model "Tuna" shows improved performance over state-of-the-art LLMs on multiple benchmarks.

Weaknesses:
- The framework's heavy dependence on proprietary LLMs raises concerns about accessibility and evaluation fairness.
- The evaluation methodology is biased, as it compares aligned models with those not aligned to GPT-4, potentially skewing performance results.
- There is a lack of human evaluation to substantiate the quality of model outputs, relying solely on automatic metrics.

### Suggestions for Improvement
We recommend that the authors improve the evaluation methodology by incorporating human assessments to validate the quality of the generated responses. Additionally, we suggest providing a clearer rationale for the choice of the Teacher Model and the Alpaca 52K instruction set. The authors should also consider using pairwise comparisons in the Probabilistic Ranking method instead of ranking multiple responses simultaneously, as this may yield more reliable results. Finally, including metrics like BertScore for semantic similarity could enhance the evaluation framework.