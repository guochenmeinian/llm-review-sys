ID: Vbm5UCaYeh
Title: Efficient Algorithms for Generalized Linear Bandits with Heavy-tailed Rewards
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 6, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on generalized linear bandits with heavy-tailed rewards, proposing two algorithms, CRTM and CRMM, that utilize truncation and mean of medians strategies. The algorithms achieve near-optimal regret bounds of $\tilde{O}(dT^{\frac{1}{1+\epsilon}})$ and improve upon previous methods in terms of both regret bounds and computational complexity. The authors provide simulations to support their claims.

### Strengths and Weaknesses
Strengths:
1. The problem of generalized linear bandits with heavy-tailed rewards is well-motivated and relevant to real-world applications.
2. Both algorithms achieve near-optimal regret bounds, with proofs appearing correct.
3. The paper is clearly written, and the related work section is particularly helpful.
4. The algorithms demonstrate lower computational complexity compared to previous truncation-based methods, requiring only $\mathcal O(d^2)$ time for each round.

Weaknesses:
1. The algorithms appear to be direct combinations of existing techniques, limiting their novelty.
2. The main technical contributions are unclear, particularly regarding the CRTM algorithm's relation to existing methods.
3. Certain input values for the algorithm, such as $S$, $\epsilon$, and $v$, are not known in practice.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the technical contributions by elaborating on the differences in theoretical analysis compared to existing works, particularly in relation to TOFU/BTC. Additionally, we suggest addressing the practical implications of unknown input values and exploring whether the dependency on $\kappa$ in the regret bound can be relaxed.