ID: sgVOjDqUMT
Title: MiniCache: KV Cache Compression in Depth Dimension for Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 7, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MiniCache, a novel method for compressing the Key-Value (KV) cache in large language models (LLMs) by merging cache states across layers. The authors leverage the high similarity of KV states between adjacent layers in the middle-to-deep portions of LLMs, achieving significant memory savings and enhanced inference throughput. MiniCache is training-free and complements existing compression strategies, demonstrating a compression ratio of up to 5.02x, a 5x increase in inference throughput, and a 41% reduction in memory footprint, all while maintaining near-lossless performance across various models and benchmarks.

### Strengths and Weaknesses
Strengths:
- The proposed merging strategy introduces a novel perspective on KV cache compression, with substantial implications for the field.
- The method is training-free and exhibits low computational overhead, making it practical for integration into existing systems.
- Extensive experiments validate the effectiveness of MiniCache across multiple models and benchmarks, showcasing its robustness and compatibility with existing quantization strategies.
- The paper is well-written and clearly presents the methodology and results.

Weaknesses:
- Some methodological design choices appear ad-hoc, particularly the decision to merge only consecutive layers after the mid-layer. This could be improved by addressing the criteria for layer selection.
- The comparison of efficiency post-quantization may overstate the method's effectiveness, as the contribution does not involve quantization. Efficiency should be measured without quantization.
- The paper lacks baseline comparisons with other KV cache eviction methods and does not evaluate MiniCache on instruction-following benchmarks, which is crucial for assessing its generalizability.

### Suggestions for Improvement
We recommend that the authors improve the methodological design by providing a principled approach for selecting layers to merge, potentially based on cosine similarity thresholds. Additionally, the authors should measure efficiency improvements without relying on quantization to avoid misleading comparisons. Including comparisons to early layer-exiting methods and evaluating MiniCache on instruction-following benchmarks would enhance the paper's relevance. Finally, providing the implementation source code and justifying the choice of Spherical Linear Interpolation (SLERP) over other methods would strengthen the paper's impact.