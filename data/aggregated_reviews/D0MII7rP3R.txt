ID: D0MII7rP3R
Title: Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 22
Original Ratings: 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two novel dataset pruning (DP) methods, label mapping and feature mapping, aimed at enhancing pretraining efficiency and fine-tuning accuracy in transfer learning. The authors evaluate their methods across various transfer learning tasks, demonstrating significant improvements in pretraining speed without compromising performance on downstream tasks. Additionally, the authors propose a comparison of computational costs between their customized pre-training approach and generic pre-training. However, the paper raises questions about the meaningfulness of the topic, given the small dataset size and the focus on computational efficiency, as well as concerns regarding the clarity of the introduction and the thoroughness of the experimental results.

### Strengths and Weaknesses
Strengths:  
+ The paper addresses a critical issue in dataset pruning for transfer learning, which has been underexplored.  
+ The proposed methods yield notable enhancements in pretraining efficiency and fine-tuning accuracy.  
+ A unified perspective is provided to integrate DP with transfer learning, showcasing broad applicability across multiple tasks.  
+ The experimental setup is clear and reproducible, with promising results on various benchmarks.  
+ The authors have conducted additional experiments that have addressed some reviewer concerns, particularly regarding the accuracy of surrogate models and the robustness of their approach on CIFAR-C.  
+ The acknowledgment of contributions and the willingness to revise based on feedback demonstrate a commitment to improving the manuscript.

Weaknesses:  
+ The paper does not explore other transfer learning scenarios, such as domain adaptation or meta-learning, which may present different challenges.  
+ There is a lack of thorough ablation studies or sensitivity analyses regarding design choices impacting performance.  
+ Limitations and failure cases of the proposed methods are not discussed, which could hinder practical application understanding.  
+ The necessity of a surrogate source model for the proposed DP methods may limit their applicability in certain contexts.  
+ The effectiveness of the methods is uncertain with low-quality or biased data, and the generalizability to other tasks remains unclear.  
+ The computational savings from pruning are minimal, raising concerns about the trade-off between efficiency and potential biases introduced.  
+ Some reviewers feel the paper lacks thoroughness in experiments, particularly in areas like domain adaptation and meta-learning.  
+ The theoretical analysis of the methods is considered insufficient, with calls for a generalization analysis that the authors believe is beyond the scope of their current work.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the introduction, particularly Lines 21-26, to ensure it does not appear to oppose generic large-scale pre-training. Additionally, we suggest revising Table 3 and the discussion in Lines 332-346 to focus on the computation costs of dataset pruning methods rather than making direct comparisons to generic pre-training. We also recommend conducting a thorough ablation study to analyze the impact of various design choices on performance. Furthermore, the authors should consider expanding their experimental scope to include comprehensive analyses in domain adaptation, multi-task learning, and meta-learning. It would be beneficial to include a discussion on the limitations and failure cases of the proposed methods to provide a clearer understanding of their practical applications. Lastly, we suggest that the authors clarify the necessity of the surrogate model and its implications for the method's generalizability, especially in contexts with low-quality data, along with a more detailed analysis of the impact of pruning on dataset size and potential biases to strengthen the paper's contributions.