ID: yAZSZob2dN
Title: Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ABEL, an unsupervised method designed to enhance passage retrieval in zero-shot settings through an iterative training strategy that alternates the roles of a retriever and a reranker. The core contribution lies in the "Boot and Switch" mechanism, where the retriever learns from the reranker's supervision signals, and vice versa, leading to mutual performance enhancement. The authors demonstrate that starting from unsupervised labels using BM25, both components can achieve competitive effectiveness compared to many supervised models.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written and well-organized, with sufficient technical details for reproducibility.
- Extensive experiments support the methodology, showcasing the contributions of each component and visualized results, including zero-shot retrieval results on BEIR.

Weaknesses:
- The novelty of the approach is questioned, as it appears to combine existing methods without significant innovation. The contributions are not well articulated, particularly regarding the internal mechanisms of the Boot and Switch process.
- The overall structure is clear, but some definitions and connections among ABEL, ABEL+, and ABEL++ are confusing. The choice of baselines is limited, with only BM25 and Contriever included in the unsupervised setting.

### Suggestions for Improvement
We recommend that the authors improve the articulation of the novelty and contributions of ABEL, particularly by discussing the internal reasons for the Boot and Switch mechanism. Additionally, consider expanding the baseline comparisons to include a broader range of models to strengthen the evaluation. Clarifying the definitions and relationships among ABEL variants will enhance reader understanding. Finally, addressing the potential for error accumulation in the iterative training loop would be beneficial.