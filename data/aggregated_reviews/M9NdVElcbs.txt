ID: M9NdVElcbs
Title: Bridging the Digital Divide: Performance Variation across Socio-Economic Factors in Vision-Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the economic and geographic performance distribution of a Vision-Language model using a geo-coded dataset of images with category labels. The authors reveal a significant performance gap between the poorest households and other socio-economic levels, emphasizing the importance of geographic and demographic approaches in model evaluation. The study also highlights performance disparities in OpenAI's CLIP model, correlating model scores with income levels and noting that image retrieval quality varies across different income groups and countries.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and addresses an important topic regarding economic/geographic bias in pre-trained models.  
- It presents a comprehensive evaluation of performance disparities across income levels, supported by robust experiments.  
- The findings contribute valuable insights into the need for more inclusive AI models and the importance of diverse datasets.

Weaknesses:  
- The evaluation lacks rigor, particularly in the normalization of CLIP scores, which may affect the soundness of the findings.  
- The paper's contributions may not be sufficiently novel, as similar performance disparities have been reported in prior works without deeper exploration of underlying causes or improvement strategies.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by normalizing the CLIP scores to ensure accurate comparisons across object categories. Additionally, we suggest that the authors delve deeper into the reasons behind the observed performance disparities and explore potential strategies for improvement, which could enhance the paper's impact and attract further interest in addressing these biases. Furthermore, we encourage the authors to consider investigating larger model variations of CLIP and multilingual versions to address unexamined questions regarding model size and training impact on performance disparities.