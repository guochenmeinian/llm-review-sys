ID: SlXKgBPMPn
Title: HiGen: Hierarchical Graph Generative Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 6, 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a hierarchical generative graph model that employs a clustering process followed by a GNN model to estimate probabilities, referred to as HiGen. The model generates graphs by progressively expanding nodes and edges, with nodes representing communities and edge weights dictating connections. The authors propose a stick-breaking process for community weight generation and utilize an auto-regressive approach for intra-community structure, while inter-community edges are generated using GNN. The authors argue that HiGen enhances community generation and inter-cluster interactions, achieving state-of-the-art results on benchmark datasets. The evaluation includes comparisons across various datasets, demonstrating HiGen's efficiency in sampling speed and generation quality, although concerns remain regarding potential overfitting and the model's ability to handle specific graph structures.

### Strengths and Weaknesses
Strengths:
1. The hierarchical clustering approach enhances the graph generation process effectively.
2. HiGen outperforms GRAN in multiple metrics despite smaller or equal model sizes.
3. The model demonstrates significantly faster sampling times, especially for larger graphs.
4. The introduction of parallelization minimizes sequential steps, improving efficiency.
5. The experimental results demonstrate improvements across multiple datasets, and the authors provide detailed results and clarify several technical aspects in their responses.

Weaknesses:
1. The claim of being the first hierarchical method for generic graphs is questionable, as prior works exist.
2. The time complexity analysis is incomplete, focusing only on sequential steps without considering overall computational requirements; the worst-case scenario may still reach $O(n^2)$.
3. The paper lacks a study on community size distribution during generation, which would enhance understanding.
4. There are unresolved issues regarding the model's ability to handle specific graph structures, such as d-regular graphs.
5. The potential for overfitting remains a concern, particularly with respect to the memorization of training data and the independence of generated components.
6. The evaluation metrics used have limitations and should be better addressed in the main text.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their main contributions, explicitly stating whether it is the model or the sampling process that is novel. Additionally, the authors should provide a comprehensive comparison against other baselines, not just the mean of the distribution, to substantiate claims regarding overfitting, particularly in relation to the training dataset size. It would be beneficial to include empirical results demonstrating the time complexity of the model on larger networks and to clarify the worst-case scenario of $O(n^2)$. We also suggest conducting an ablation study to isolate the contributions of the GNN architecture used and a thorough analysis of overfitting, possibly by reporting average likelihood values on train and test datasets. Lastly, the authors should clarify the loss function, the determination of weights at level 0, and the generation of the node embedding matrix, while ensuring consistent notation throughout the paper, and explicitly state the limitations of HiGen in handling certain graph structures and clarify the implications of independence assumptions in their model, particularly in relation to Theorem 3.1.