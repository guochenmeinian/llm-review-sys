ID: XHdwlbNSVb
Title: MMSite: A Multi-modal Framework for the Identification of Active Sites in Proteins
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 5, 8, 6, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MMSite, a multimodal framework designed to predict active sites in proteins by integrating amino acid sequences and textual descriptions. The authors introduce the ProTAD dataset, which contains over 570,000 pairs of protein sequences and corresponding textual annotations. The MMSite framework employs a "First Align, Then Fuse" strategy to align and combine these modalities, demonstrating improved performance over existing sequence-based and multimodal baselines. The paper includes extensive ablation studies to validate the proposed methodology. Additionally, the authors conduct a comparative analysis of training with human-annotated texts versus Prot2Text-generated texts, showing that human-annotated data yields superior performance across most metrics. They propose that while Prot2Text is effective for generating descriptions, human-annotated texts should be prioritized during training and inference to leverage pre-trained knowledge more effectively.

### Strengths and Weaknesses
Strengths:
- The authors curated the ProTAD dataset, providing a comprehensive resource for protein sequences and textual annotations.
- The two-stage multimodal framework effectively aligns and fuses sequence and text embeddings, enhancing active site predictions.
- Results indicate that the proposed method outperforms baseline models, even with predicted textual contexts during inference.
- The integration of protein sequences with biomedical texts for fine-grained, token-level prediction of active sites is innovative and potentially impactful.
- The authors effectively address reviewer concerns and provide additional evaluation results that support their claims.

Weaknesses:
- The reliance on an external model to generate textual context during inference for novel proteins is a limitation.
- Insufficient details are provided for replicating the implementation of MMSite and training baseline methods.
- The performance gap between MMSite and baseline methods narrows with higher dataset clustering thresholds, warranting deeper analysis.
- The originality of the work compared to previous research appears marginal.
- The rationale for the extensive dataset with 17 attributes is not fully justified, especially since training on the function attribute alone achieves similar performance.

### Suggestions for Improvement
We recommend that the authors improve the related work section by including methods that learn embeddings using sequence and structure inputs, highlighting the unique multimodality of their approach. Additionally, we suggest providing reasoning for the performance differences between MMSite and ProtST, as well as clarifying the use of embeddings from pre-trained PLMs and BLMs in the introduction. 

We also advise enhancing the clarity of the neural network architecture by detailing the dimensions after each module and explaining the skip concatenate and prediction module in Fig. 2. Furthermore, we recommend including information on how baseline methods were trained, addressing reproducibility by providing code access, and discussing the necessity of manual prompting for text descriptions.

Lastly, we encourage the authors to elaborate on the KL divergence loss function's role in addressing the potential semantic associations in batch processing and to consider a temporal evaluation approach for a more realistic assessment of their methodology. We also recommend improving the justification for the large dataset by clearly articulating the unique contributions of each attribute beyond the function attribute. Additionally, we suggest enhancing the clarity of the manuscript by addressing writing issues and modifying captions in Figures 3 and 8 to better explain the meaning of each color. Lastly, we encourage the authors to further explore the influence of the dataset used for pre-training the PLM and BLM on the generalization of MMSite, particularly regarding the performance of Prot2Text in applications to de novo proteins.