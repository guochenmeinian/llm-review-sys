ID: TqIDmoIzLT
Title: CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an effort to improve the quality of the CoNLL-2003 corpus annotations through a comprehensive relabeling process. The authors propose a methodology involving several two-step correction rounds, utilizing automatic methods to identify potential errors and then engaging annotators to assess these cases. They describe three methods for error highlighting: comparison of CoNLL NE labels with AIDA EL labels, cross-checking by training NER tools and label classifiers, and training a NER model for adjectival affiliation. The authors evaluate quality improvements through manual reassessment and comparisons with state-of-the-art (SOTA) methods.

### Strengths and Weaknesses
Strengths:
- The paper provides a new, corrected version of the CoNLL-03 dataset, enhancing its reliability for NER evaluation.
- The methodology is well-articulated and convincing, with thorough statistical analysis of the dataset's quality.
- The authors demonstrate significant reductions in annotation errors, which improves model performance.
- The paper is clearly written and presents a valuable resource for the community.

Weaknesses:
- The methodology may not be applicable to other domain-specific corpora.
- Some examples focus heavily on sports, raising questions about the generalizability of the improvements.
- Potential biases in the correction process due to the involvement of the same annotators who developed the framework.

### Suggestions for Improvement
We recommend that the authors clarify whether the annotators were the same individuals who conceived the correction framework to address potential bias. Additionally, please provide more examples beyond sports to illustrate the general applicability of the improvements. It would be beneficial to explain the source of the Wikipedia labels provided to annotators and to clarify the process of boundary corrections mentioned in the paper. Finally, consider addressing the significance of modification rates between train and test splits and the choice of examining errors in FLERT over Biaffine, which appears to yield better results.