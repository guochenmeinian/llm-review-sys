ID: EfMyf9MC3t
Title: Speculative Decoding with Big Little Decoder
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 8, 4, 6, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents BiLD, a framework that enhances autoregressive text generation by utilizing a small model for efficient token generation and invoking a larger model to refine predictions when the small model exhibits low confidence. The authors implement two policies: a fallback policy that activates the large model based on the small model's uncertainty, and a rollback policy that corrects inaccurate predictions from the small model. The proposed method achieves up to 2x speedup in tasks like machine translation and summarization with minimal quality loss. Additionally, the paper compares the BiLD framework with CALM, emphasizing the advantages of token-level batching/routing over basic batching. The authors argue that token-level batching is a more efficient solution, already implemented in various LLM serving systems, and that integrating the BiLD framework into these systems requires minimal engineering changes. A comparative analysis demonstrates BiLD's superior performance over CALM, particularly when using a large model.

### Strengths and Weaknesses
Strengths:
- The integration of small and large models is intuitive and effectively addresses the challenge of maintaining quality while improving efficiency.
- The fallback and rollback policies are innovative and empirically demonstrate superior performance compared to speculative sampling.
- The framework does not require additional training, and the prediction alignment technique enhances performance with minimal effort.
- The paper effectively demonstrates the efficiency of token-level batching/routing, supported by existing frameworks.
- A detailed comparison table illustrates BiLD's performance advantages over CALM, providing empirical evidence for their claims.

Weaknesses:
- The evaluation benchmarks primarily focus on translation tasks, lacking diversity in QA tasks which could provide a more comprehensive assessment.
- The method's scalability is questionable, particularly regarding batch sizes greater than one, which may hinder practical applicability.
- The paper does not adequately discuss prior works that share similar ideas, limiting contextual understanding and comparison.
- The reviewer perceives fundamental similarities between CALM and the method proposed in this paper, suggesting a lack of distinction.
- The performance of CALM is limited by its early exiting decisions, which introduce latency overhead.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including more QA tasks such as TriviaQA, SQuAD, WebQS, and NQS to demonstrate the framework's versatility. Additionally, we suggest conducting experiments with batch sizes larger than one to assess the method's scalability and provide evidence of its performance in such scenarios. A quantitative comparison with CALM would also strengthen the paper by highlighting the differences in effectiveness between the two approaches. Furthermore, we recommend that the authors improve the clarity of the distinctions between BiLD and CALM to address concerns about their similarities. Providing further insights into the implications of the latency overhead associated with CALM's early exiting decisions could also strengthen the argument for BiLD's efficiency. Lastly, a more detailed discussion of the relationship between this work and existing literature on non-autoregressive models would enhance the context and relevance of the contributions.