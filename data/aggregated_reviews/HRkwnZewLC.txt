ID: HRkwnZewLC
Title: Navigating the Maze of Explainable AI: A Systematic Approach to Evaluating Methods and Metrics
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 5, 7, 6, 7, -1, -1, -1
Original Confidences: 3, 4, 3, 5, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive benchmark analysis of 17 existing XAI methods, addressing key shortcomings in current benchmarks, such as small datasets and inadequate evaluation metrics. The authors propose the LATEC benchmark, which evaluates these methods across 20 distinct metrics and various model architectures, providing insights into the generalization of XAI performance. The study highlights inconsistencies in current evaluations and emphasizes the importance of robust evaluation criteria, including faithfulness, robustness, and complexity.

### Strengths and Weaknesses
Strengths:
1. The paper offers a large-scale evaluation of XAI methods, incorporating diverse model architectures and input modalities.
2. It identifies inconsistencies in existing XAI evaluations, providing valuable insights for practitioners.
3. The writing is clear and the evaluation framework is comprehensive, covering multiple perspectives and datasets.

Weaknesses:
1. The paper does not sufficiently discuss how easily the benchmarking approach can be applied to new XAI methods or what interface developers should use.
2. It lacks a detailed comparison with other existing comprehensive XAI benchmarks, which could enhance its contribution.
3. The evidence for frequent metric disagreement is not robust enough, as Figure 2 may not adequately showcase significant discrepancies.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the application of their benchmarking approach for new XAI methods, including the interface for developers. Additionally, a detailed comparison with existing benchmarks should be included to contextualize their contributions. To strengthen the evidence of metric disagreement, we suggest providing more substantial data beyond what is presented in Figure 2. Furthermore, a systematic tuning of hyperparameters for all methods, along with a discussion on the implications of metric agreement and disagreement, would enhance the robustness of the findings. Lastly, including median ranks alongside average ranks in the results could provide a clearer picture of method performance.