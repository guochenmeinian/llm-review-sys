ID: 3ZrGmenVM2
Title: Deep learning with kernels through RKHM and the Perron-Frobenius operator
Conference: NeurIPS
Year: 2023
Number of Reviews: 5
Original Ratings: 7, 6, 7, 5, -1
Original Confidences: 3, 3, 3, 3, -1

Aggregated Review:
### Key Points
This paper presents deep Reproducing kernel Hilbert $\mathcal{C}^*$-module (RKHM), a deep learning framework for kernel methods that generalizes RKHS using $\mathcal{C}^*$-algebra. The authors construct a map as the composition of functions in RKHMs and develop a new Rademacher generalization bound using the Perron-Frobenius norm, which exhibits milder dependency on output dimensions compared to existing bounds. They also establish a representer theorem and explore connections to existing studies, including CNNs and benign overfitting, while providing numerical experiments to support their theoretical findings.

### Strengths and Weaknesses
Strengths:
- Clear writing and well-introduced concepts.
- Solid theoretical results, including a better generalization bound for RKHM compared to known bounds for vvRKHS.
- Novel tools and approaches that can inspire further research.
- Experiments substantiate theoretical claims.

Weaknesses:
- The paper can be difficult to read due to unclear terminology and notation, such as the definition of '$\mathcal{A}$-valued positive definite kernel'.
- Some derivations are standard and do not offer significant new insights, raising questions about the necessity of the additional abstraction.
- The connection to CNNs is unconvincing, and the term "benign overfitting" is misapplied in the context of the paper.

### Suggestions for Improvement
We recommend that the authors improve the clarity of terminology and notation, particularly regarding the domain of kernels and the definitions of key terms. Additionally, we suggest providing a more thorough explanation of the theoretical advantages of deep RKHM over non-deep counterparts. It would be beneficial to clarify the connections to CNNs and benign overfitting, ensuring that the discussion aligns with established definitions and observations in the literature. Lastly, we encourage the authors to elaborate on the motivation for formalizing compositions of linear models and the implications of their findings on the broader understanding of deep kernel methods.