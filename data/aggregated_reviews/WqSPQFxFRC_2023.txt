ID: WqSPQFxFRC
Title: LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 7, 8, 8, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LegalBench, a new benchmark for evaluating large language models (LLMs) in the legal domain, consisting of 162 tasks that assess various types of legal reasoning. The authors propose a comprehensive organization of these tasks, developed through collaboration with legal professionals, which enhances the benchmark's relevance and practical applicability. The paper provides empirical results from evaluations across 20 different LLMs, including GPT-4 and GPT-3.5, demonstrating their performance on these tasks. However, the authors acknowledge that the benchmark is primarily focused on the US legal system and express intentions to expand to other jurisdictions in future work.

### Strengths and Weaknesses
Strengths:
- LegalBench is comprehensive, covering a wide range of legal reasoning tasks crafted by legal experts, which adds significant value to the research community.
- The collaborative construction process involving legal experts enhances the benchmark's legitimacy and relevance.
- A significant increase in the number of evaluated LLMs (20 models) provides a broader understanding of performance across different families and sizes.
- The clear organization of tasks and detailed documentation, including structured evaluation formats like gradebooks, improves usability and understanding.

Weaknesses:
- The benchmark is primarily focused on the US legal system and English language, limiting its applicability to other jurisdictions and languages.
- The tasks currently lack subjective elements, which may not reflect real-world legal complexities.
- There is a noted absence of long-document tasks, which are increasingly relevant due to advancements in LLM context windows.
- The evaluation of LLMs is limited in diversity, with few models and sizes considered, potentially skewing performance insights.
- The evaluation of subjective legal tasks remains challenging due to the lack of concrete ground truth labels.

### Suggestions for Improvement
We recommend that the authors improve the benchmark's applicability by discussing its limitations regarding non-US legal systems and multilingual capabilities. Actively soliciting contributions from legal scholars in various jurisdictions could help create a more globally representative dataset. Additionally, we suggest expediting the inclusion of long-document tasks to align with recent advancements in LLM capabilities. Incorporating tasks with subjective outcomes would enhance the benchmark's realism, and exploring methods for evaluating subjective tasks more effectively, potentially by developing a framework for gathering expert evaluations at scale, would be beneficial. Furthermore, we encourage the authors to evaluate a broader range of LLMs, including those with varying parameter sizes and legal domain-specific models, to provide a more comprehensive performance analysis. Lastly, we recommend addressing reproducibility concerns by providing the necessary code and documentation for evaluation and dataset maintenance.