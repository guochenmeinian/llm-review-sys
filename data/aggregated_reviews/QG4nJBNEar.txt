ID: QG4nJBNEar
Title: CAT-Walk: Inductive Hypergraph Learning via Set Walks
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 7, 6, 7, 7, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a hyperedge learning method for modeling temporal hyperedges, featuring a novel hyperedge-centric random walk called SetWalk, a permutation-invariant pooling method termed SetMixer, and a technique for anonymizing node and hyperedge identities. The authors empirically demonstrate that SetWalk generates sequences of hyperedges, enhancing expressiveness compared to traditional node-based random walks. The pooling method, a variation of MLP-mixer, is shown to outperform mean pooling and RNNs. The experimental validation supports the effectiveness of the CAT-Walk for hyperedge prediction, with ablation studies confirming the significance of each component.

### Strengths and Weaknesses
Strengths:
- The proposed method includes components that could extend beyond hyperedge prediction, particularly the hyperedge-centric random walk and set-based pooling, which may benefit other architectures.
- The paper is well-structured and visually clear, making complex concepts accessible.
- Novel concepts are introduced, with experimental results highlighting their advantages.

Weaknesses:
- The connection between SetWalk and classical graph-based random walks remains unclear.
- A detailed discussion of the limitations of self-attention and RNNs, as well as how SetWalk overcomes them, is necessary.
- The expressivity comparison between SetMixer pooling and classical approaches lacks clarity, particularly regarding its limitations.
- The definition of permutation invariance in Eq.5 is questioned, as it may not align with Theorem 3.
- Theorem 4's claim about expressivity compared to existing anonymization strategies is ambiguous.
- Additional ablation studies are needed to strengthen the case for CAT-Walk, including comparisons with self-attention/transformer and sum-based universal approximators.
- The experimental evaluation of components across datasets is limited, and some definitions could benefit from schematic illustrations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the connection between SetWalk and classical random walks, providing a thorough discussion of the limitations of self-attention and RNNs. Additionally, we suggest clarifying the expressivity of SetMixer pooling and addressing its limitations. The authors should ensure that Eq.5 adheres to permutation invariance as stated in Theorem 3. Furthermore, we advise refining Theorem 4 to explicitly state the expressivity comparison with existing methods. Including more ablation studies, particularly with self-attention/transformer and sum-based universal approximators, would enhance the robustness of the findings. Lastly, we encourage the authors to evaluate each component across all datasets and to include schematic illustrations for complex definitions to improve understanding.