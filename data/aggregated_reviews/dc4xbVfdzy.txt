ID: dc4xbVfdzy
Title: Decision Mamba: A Multi-Grained State Space Model with Self-Evolution Regularization for Offline RL
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Decision Mamba (DM), a novel application of State Space Models (SSMs) to the offline reinforcement learning (RL) problem. The authors propose a three-stage architecture that includes trajectory embedding with MLPs and time embeddings, a coarse-grained SSM for sequential dependency understanding, and a fine-grained approach using a 1D-convolutional layer for contextual precision. Additionally, they introduce progressive self-evolution regularization (PSER) to enhance robustness against noisy labels. Experimental results indicate that DM outperforms various baselines, including transformer-based methods.

### Strengths and Weaknesses
Strengths:
- The application of SSMs to offline RL tasks is innovative and promising.
- The architecture is straightforward and the empirical results demonstrate solid performance across multiple environments.
- The paper is clearly written and easy to follow.

Weaknesses:
- There is insufficient reasoning provided for the "fine-grained" control attributed to the Conv1D module.
- The relationship between PSER and existing methods like PPO and DPO lacks formal justification.
- The design choices for DM are not well-explained, and the contributions appear somewhat independent rather than tightly integrated.

### Suggestions for Improvement
We recommend that the authors improve the explanation of how the Conv1D module contributes to fine-grained control and provide a more detailed theoretical justification for PSER in relation to existing methods. Additionally, we suggest conducting experiments with longer context lengths to explore potential performance variations and enhancing the methodology section to clarify Mamba-related terminology for better reader comprehension. Finally, visual experiments demonstrating the impact of Formula 16 on action changes would strengthen the paper's claims regarding overfitting prevention.