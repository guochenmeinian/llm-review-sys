ID: 7WeCyYy9TL
Title: Joint processing of linguistic properties in brains and language models
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 2, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the alignment between large language model (LLM) embeddings, specifically BERT, and fMRI reading data by examining the impact of removing specific linguistic properties from BERT's representations. The authors demonstrate that the removal of these properties significantly decreases alignment with fMRI data, suggesting that BERT's encoding of linguistic information is crucial for this alignment. The study employs both macro and micro-level analyses, contributing a finer-grained understanding of the relationship between linguistic properties and brain activity.

### Strengths and Weaknesses
Strengths:
- The paper innovatively analyzes LLM representations' predictive ability regarding fMRI data by systematically removing linguistic information.
- It conducts analyses at both macro (whole-model and whole-brain) and micro (layer-wise, ROI, and sub-ROI) levels.
- The study is well-structured, utilizing a large public fMRI dataset and providing clear explanations of methods and results.

Weaknesses:
- The Pearson correlations in Figure 3 are low (averaging less than 0.1), raising questions about their significance and interpretation.
- The linguistic annotations used are derived from an off-the-shelf NLP tool (stanza), resulting in "silver" rather than "gold" annotations.
- The experiments demonstrating the removal of linguistic properties are inconclusive, lacking sufficient comparative data, such as a baseline with random parameters.
- The choice of BERT, an encoder-only model, may limit the generalizability of findings, as autoregressive models may better reflect the reading process.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the word length probing task by ensuring consistent naming and providing a clear definition of the task. Additionally, we suggest including more baselines, such as a most-frequent-class baseline for each task, to enhance the comparative analysis. To strengthen the findings, we encourage the authors to explore methods from the causality+NLP literature to establish causal relationships rather than relying solely on correlational metrics. Finally, we advise considering alternative methods for identifying the effect of linguistic information, as the current projection method may be misleading.