ID: j2EaW49Rk7
Title: Orthogonal Gradient Boosting for Interpretable Additive Rule Ensembles
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 6, 3, 5, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a gradient-boosting algorithmic framework for rule ensemble learning, focusing on the interpretability of the resulting rule set. The authors propose Fully-Corrective Orthogonal Gradient Boosting (FCOGB) as a suitable method for rule boosting, allowing adjustments to the weights of preceding rules in each iteration, which may reduce the number of rules needed for a certain accuracy. The authors derive a stepwise boosting objective function for single rule search, similar to existing gradient boosting objectives but with a different regularization term. Additionally, the paper introduces an approach for learning smaller ensembles of additive rule ensembles, aiming to improve interpretability and runtime complexity. The authors propose an algorithm that optimizes the accuracy-complexity trade-off and introduces an efficient cut-point search algorithm. The effectiveness of FCOGB is demonstrated through experimental comparisons across various tasks, although some reviewers express concerns that the contributions are oversold, particularly regarding interpretability and computational costs.

### Strengths and Weaknesses
Strengths:
- The application of FCOGB to rule learning is novel, with a clear justification provided. Figure 2 aids in understanding the differences between FCOGB and existing algorithms.
- The authors consider optimal rule search in each iteration, proposing a strategy that leverages the structure in the boosting objective function to enhance efficiency.
- The algorithm is evaluated on diverse datasets, with detailed result analyses showing that FCOGB achieves a better accuracy-risk trade-off than existing methods.
- The proposed algorithm shows potential for improving generalization performance and provides a novel objective function that accounts for weight correction, enhancing risk reduction per rule.

Weaknesses:
- The paper's presentation requires improvement, particularly in clearly distinguishing between general gradient boosting and rule boosting, which may confuse readers unfamiliar with the literature.
- There is an insufficient comparison with rule induction algorithms based on column generation, which could provide valuable insights into FCOGB's performance.
- The claim regarding the efficient computation of the prefix optimization problem needs clarification and supporting evidence.
- Several minor errors and missing references detract from the paper's quality.
- The claims regarding interpretability and computational efficiency are perceived as exaggerated, and the complexity of additive rule ensembles is questioned, with concerns that they may not be as interpretable as conventional rule sets.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the "Rule Boosting" section by separating general gradient boosting from rule boosting to enhance reader comprehension. Merging the "Single rule optimization" subsection with "4.3 Efficient Implementation" could also improve fluency. Additionally, providing more details about the BnB/beam search for a single rule would strengthen the algorithm's description. We suggest including comparisons with recent rule induction algorithms based on column generation to contextualize FCOGB's performance. The authors should clarify the computation of the prefix optimization problem and provide a proof or reference to support their claims. We also recommend revising the title to "Orthogonal Gradient Boosting for Simpler Additive Rule Ensembles" and avoiding the term "cognitive complexity," opting for "complexity" instead. Furthermore, we suggest providing a more explicit summary of computational costs, clarifying that the overhead is modest, with a factor of about 2 in most cases. Lastly, we encourage the authors to analyze their algorithm objectively, acknowledging its limitations to foster a more favorable evaluation, and addressing the minor errors and including the missing references will enhance the overall quality of the paper.