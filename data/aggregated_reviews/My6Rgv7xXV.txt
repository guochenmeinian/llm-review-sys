ID: My6Rgv7xXV
Title: Contextual Interaction for Argument Post Quality Assessment
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the influence of contextual interaction on human assessment of argument quality. The authors propose combining language models with supervised contrastive learning and discourse-sensitive embeddings, evaluating these methods on the IBM-Rank-30k dataset for argument quality ranking and comparison. The results indicate that contrastive learning and discourse-sensitive embeddings significantly outperform previous approaches, particularly in distinguishing arguments with minor quality differences, while prompt-based methods perform poorly.

### Strengths and Weaknesses
Strengths:
- The paper contributes a novel approach that outperforms state-of-the-art results by incorporating contextual interaction.
- It is well-structured and clearly written, with comprehensible experiments and well-summarized results.
- The analysis of LLMs' ability to distinguish argument quality reveals significant limitations, particularly their tendency to guess randomly when arguments are similar.
- The effect of contextual interaction is effectively illustrated through case studies and comparisons.

Weaknesses:
- The introduction lacks emphasis on contextualization and its integration into social science research.
- The rationale for using ChatGPT and Davinci as indicators of argument quality performance is insufficiently explained.
- LLMs are not explored in depth, particularly regarding their random performance when argument score differences are small.

### Suggestions for Improvement
We recommend that the authors improve the introduction by emphasizing the importance of contextualization and its relevance to social science research. Additionally, the authors should clarify the rationale for using ChatGPT and Davinci, detailing how their performance on text summarization relates to argument quality assessment. Discussing the limitations of incorporating contextual interactions and expectations for performance improvements would enhance the analysis. Furthermore, including DAGN in the comparison in Section 5 could provide valuable insights. Lastly, we suggest providing more context details for LLMs, exploring the impact of varying the number of arguments provided, and considering the implications of topic similarity on scoring.