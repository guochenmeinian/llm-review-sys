ID: VGb2RhMFAI
Title: Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time Controllable Text Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Attribute Collapse problem encountered during controllable text generation (CTG) and proposes air-decoding lightweight methods to address this issue. The authors introduce Attribute Distribution Reconstruction methods to balance probability distributions, demonstrating that air-decoding achieves high performance compared to other baselines. The paper is well-structured and includes calculation examples for clarity.

### Strengths and Weaknesses
Strengths:  
- The paper defines the Attribute Collapse problem, contributing novel methods to mitigate it.  
- It is well-written and easy to follow, with sufficient ablations and human evaluations.  
- The identified problem is interesting and well-motivated, with good coverage of baseline methods.

Weaknesses:  
- The criteria for "lightweight" and "low-resource" are not clearly defined, and comparisons with other baselines are lacking.  
- The explanation of the Attribute Collapse problem relies solely on PPL and accuracy curves, which may not adequately represent the issue.  
- The distribution reconstruction method may lack theoretical support and could be too simplistic.  
- Detailed guidelines for human evaluations and inter-annotator agreement scores are missing.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definitions for "lightweight" and "low-resource" and provide comparisons with relevant baselines. Additionally, the authors should present CTG results for models affected by the Attribute Collapse problem, or at least include relevant results in the appendix. Enhancing the theoretical foundation of the Attribute Distribution Reconstruction method would strengthen the paper. Finally, we suggest including detailed guidelines for human evaluations and measuring inter-annotator agreement scores to justify the experimental results.