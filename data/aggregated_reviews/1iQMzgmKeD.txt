ID: 1iQMzgmKeD
Title: Extrapolating Multilingual Understanding Models as Multilingual Generators
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents methods to enhance multilingual understanding models (encoder) for generation tasks, specifically through a Semantic-Guided Alignment-then-Denoising (SGA) approach. The authors adapt a multilingual encoder (XLM-R) to function as a generator with minimal new parameters. Experimental results across various tasks, including machine translation and question generation, demonstrate the effectiveness of their method. Notably, the authors observe that XLM-R performs worse than mBART in supervised settings despite superior zero-shot performance.

### Strengths and Weaknesses
Strengths:
- The paper addresses an important topic and proposes a novel solution that shows strong experimental results.
- The method is parameter-efficient, allowing significant improvements in generation quality with minimal additional parameters.
- The approach is generalizable to various tasks and exhibits enhanced zero-shot knowledge transfer capabilities.

Weaknesses:
- Some parts of the paper lack clarity, leading to confusion regarding specific results and terminology.
- The paper does not adequately compare with significant prior work, particularly in the context of pre-trained encoders for generation.
- The writing is unclear in distinguishing between model architectures and training objectives, which may mislead readers.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their writing, particularly in distinguishing model architectures (encoder, decoder, enc-dec) and training objectives (autoregressive, auto-encoding). Additionally, the authors should address the confusion regarding the results of mTransformer in Tables 1 and 2 and clarify how they handle inconsistent input and output lengths. It is also essential to include comparisons with relevant prior work, such as BiBERT and prompt tuning on mBART, to strengthen their claims. Finally, we suggest that the authors specify whether they will release their codes and models to enhance reproducibility.