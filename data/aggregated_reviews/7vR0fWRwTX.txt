ID: 7vR0fWRwTX
Title: Exploring Discourse Structure in Document-level Machine Translation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to document-level neural machine translation (NMT) that utilizes a rhetorical structure theory (RST) parser to capture discourse information. The authors propose a pipeline that segments documents using the TextTiling algorithm, parses paragraphs into RST trees, linearizes these trees into sequences, and employs a multi-granularity attention mechanism to enhance context utilization. The RST-Att model demonstrates superior performance on various datasets, achieving state-of-the-art results in document-level translation.

### Strengths and Weaknesses
Strengths:
- The method is well-motivated and introduces a principled paragraph-level translation approach.
- The RST-Att model outperforms existing methods, showcasing its potential to advance the state-of-the-art in document-level NMT.
- The paper is well-written, clear, and includes extensive experiments and ablation studies.

Weaknesses:
- The experimental section lacks sufficient analysis and comparison with simple baselines to justify the improvements.
- The reliance on automatic RST parsing raises concerns about error-proneness and limits applicability to languages with available parsers.
- The notation for the "Para2Para Baseline (BART)" is not clearly defined, and the paper lacks detailed discussions on the linguistic aspects of context that contribute to quality improvements.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by incorporating additional baselines, such as a simple separator approach using artificial <sep> tokens, to better justify the RST parsing and linearization steps. Additionally, including a column in Table 2 that details the data conditions for each model would enhance clarity. We also suggest providing a formal definition for the "Para2Para Baseline (BART)" and expanding the analysis on how the RST parser's performance impacts the overall results. Lastly, addressing the decoding efficiency compared to traditional methods and comparing the proposed approach with large language models (LLMs) would strengthen the paper's contributions.