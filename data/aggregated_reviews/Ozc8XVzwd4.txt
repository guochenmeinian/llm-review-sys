ID: Ozc8XVzwd4
Title: VCC: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 6, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents VIP-token centric compression (VCC) aimed at enhancing the efficiency of Transformers for ultra-long sequences by focusing on a small subset of important tokens, termed VIP tokens. The authors propose a multi-resolution compression technique to reduce computational costs while maintaining performance. Extensive experiments validate the effectiveness of the proposed method, demonstrating its ability to compress long sequences into smaller representations.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant issue in current LLM applications regarding long sequence inputs, showcasing high application value.
- It is well-written, with a novel method that is validated through thorough experiments.
- The authors plan to release code and checkpoints, enhancing reproducibility.

Weaknesses:
- The approach is limited to Transformer encoders, raising concerns about its applicability to popular decoder-only models like GPT and PaLM.
- The compression method appears to impede the use of relative positional encoding, with insufficient details provided on its handling in experiments.
- The theoretical guarantees and numerical simulations regarding the approximation error of the multi-resolution compression technique are lacking, despite being a central contribution.
- Missing hardware-independent metrics such as FLOPs, and insufficient discussion on the compatibility of VCC with KV Cache and other recent works.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their approach by explicitly discussing its applicability to decoder-only models. Additionally, the authors should provide clarity on how relative positional encoding is managed in their experiments. We encourage the inclusion of theoretical analysis and numerical experiments on the approximation error in the main body of the paper. Furthermore, we suggest adding hardware-independent metrics like FLOPs and discussing the relationship between VCC and KV Cache. Lastly, addressing the limitations of the token importance evaluation method and providing a more rigorous modeling approach would strengthen the paper.