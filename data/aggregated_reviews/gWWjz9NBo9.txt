ID: gWWjz9NBo9
Title: PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a data augmentation approach called PromptMix for generating additional training data for text classification using large language models (LLMs). The authors hypothesize that a robust text classifier requires a good mix of borderline examples, proposing a two-step method where GPT-3.5-turbo first generates mixed-class examples and then relabels them to enhance label reliability. The authors evaluate their method across four diverse text classification datasets, focusing on 2-shot and zero-shot scenarios, and demonstrate that 2-shot PromptMix outperforms multiple 5-shot or higher data augmentation baselines.

### Strengths and Weaknesses
Strengths:  
- The approach is innovative and effective without needing additional unlabelled data or fine-tuning of the generative model.  
- The novelty is well justified, showing improvements through generating class border samples and relabeling.  
- The paper is detailed, well-written, and includes extensive analyses and ablation studies.  
- The experimental setup is straightforward and leads to significant performance gains over baselines.

Weaknesses:  
- The dependence on paid OpenAI models raises concerns about accessibility and performance variability with open-source models.  
- The methodology for writing prompts and the adaptability of the approach to diverse datasets are not adequately discussed.  
- The performance variance when changing the number of selected classes is not addressed.  
- The paper lacks standard deviations for the main results, which is crucial for understanding result stability.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the methodology used for writing prompts and clarify the model's dependency on dataset characteristics. Additionally, an analysis of how PromptMix performs with open-source models and a parameter count cutoff for LLMs would enhance the paper. Reporting standard deviations for all main results is essential for assessing the stability of findings. Finally, we suggest including a discussion on the applicability of PromptMix to tasks like Hate Speech detection, as this could provide valuable insights into its limitations.