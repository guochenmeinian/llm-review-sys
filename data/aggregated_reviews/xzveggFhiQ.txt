ID: xzveggFhiQ
Title: Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity Alignment
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Transformer-based method, Meaformer, for multi-modal knowledge graph entity alignment (MMEA). It introduces a novel Multi-Modal KG Transformer (MMKT) that effectively integrates heterogeneous information through a hierarchical modifiable self-attention block and entity-type prefix injection. The evaluation demonstrates that the proposed framework achieves state-of-the-art performance on public multi-modal entity alignment datasets, specifically FB15K-DB15K and FB15K-YAGO15K.

### Strengths and Weaknesses
Strengths:
1. The innovative Meaformer framework leverages a multi-modal KG Transformer to address challenges in integrating diverse information types within MMKGs.
2. The hierarchical modifiable self-attention block preserves unique semantics, enhancing the integration of multiple information sources.
3. The entity-type prefix injection methods improve alignment accuracy by focusing on relevant entity types.
4. The framework shows superior performance compared to existing methods, indicating its effectiveness in identifying equivalent entity pairs.

Weaknesses:
1. The technical contribution is deemed incremental, lacking a thorough comparison with recent similar works.
2. Some textual descriptions are unclear, making it difficult to follow key concepts and terminology.
3. The evaluation is limited to only two cross-MMKG EA benchmarks, suggesting a need for broader testing.
4. The analysis of related work is insufficient, and the order of attention layers is not fully explored in experiments.

### Suggestions for Improvement
We recommend that the authors improve the clarity of textual descriptions, particularly regarding terms like "distinct types" and "first-order neighbors." Additionally, we suggest expanding the evaluation to include more cross-lingual MM EA benchmarks. It would be beneficial to provide a more detailed analysis of the order of attention layers and to include a dedicated section on related work to contextualize the contributions of this paper. Finally, we advise the authors to cite and compare their work with the other "Meaformer" method published in December 2022.