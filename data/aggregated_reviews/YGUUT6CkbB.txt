ID: YGUUT6CkbB
Title: StrAE: Autoencoding for Pre-Trained Embeddings using Explicit Structure
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Structured AutoEncoder framework (StrAE) that leverages hierarchical compositions to produce multi-level representations, utilizing an encoder-decoder architecture with contrastive loss. The model computes two representations for each node in a binary parse tree: one from a bottom-up traversal and another from a top-down traversal, adhering to the principle of faithfulness. The authors propose a variant, Self-StrAE, which learns to compose tokens without explicit structural input. Experimental results demonstrate that StrAE outperforms traditional models like IORNNs and Tree-LSTMs, particularly in linguistically motivated tasks, and shows competitive performance against word-only encoders like RoBERTa.

### Strengths and Weaknesses
Strengths:
- The motivation for the research is clearly articulated, and the paper is well-structured and easy to follow.
- The use of contrastive loss to incorporate structural information is innovative and effective.
- The experimental results are robust, showing that StrAE outperforms other tree-based models.

Weaknesses:
- The experimental design in Section 5 raises concerns, particularly regarding the averaging of RoBERTa outputs for sentence embeddings, which may yield misleading results.
- The benefits of StrAE in terms of parameter efficiency and inference speed are not adequately addressed.
- The embedding process for Self-StrAE lacks clarity and requires further explanation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the Self-StrAE embedding process to distinguish it from StrAE. Additionally, please provide a discussion on training and decoding times compared to other methods, as this information is currently missing. We suggest conducting experiments with inconsistent tree structures to evaluate model robustness further. Furthermore, consider integrating the NSP objective during transformer pretraining to enhance performance and provide a more comprehensive comparison by including the total number of parameters for all competitors in Table 4. Lastly, clarify the notation N upon its first appearance and address the potential noise introduced by the contrastive loss objective.