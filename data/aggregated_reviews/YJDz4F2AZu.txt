ID: YJDz4F2AZu
Title: ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 5, 7, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the ContiFormer, a continuous-time transformer model designed for irregular time series data. The model integrates Neural ODEs with a continuous dynamic attention mechanism, allowing for the handling of time-dependent inputs through vector-valued functions. The authors demonstrate that various transformer variants for irregular time series are special cases of ContiFormer and evaluate its performance across multiple datasets, showing superior results compared to baseline models. Additionally, the paper explores the integration of concepts from differential equation-based neural networks, contributing significantly to the field by proposing a novel architecture that encompasses various existing models.

### Strengths and Weaknesses
Strengths:
- The proposed work is original, introducing a novel continuous-time attention mechanism (CT-MHA).
- The authors provide a thorough complexity analysis and extensive empirical evaluation, demonstrating improved performance over state-of-the-art models in event forecasting and classification tasks.
- The paper is generally well-structured and presents a unique theoretical approach.
- The writing is clear, and the numerical experiments are extensive.

Weaknesses:
- The related work section is convoluted and could benefit from clearer organization into subsections.
- Key aspects of the model, such as the initialization of keys and values, lack clarity.
- Some claims regarding the limitations of existing models lack sufficient support and precision, particularly concerning cumulative errors in Neural ODEs and the performance of baseline models.
- The assumption that every observation influences the dynamic system before its occurrence is not well-explained, raising questions about the model's causality and online applicability.
- The paper lacks standard deviations in reported results, raising concerns about the statistical rigor of the evaluations.
- There is no evaluation on regularly sampled datasets, which would clarify the model's performance across different scenarios.
- Limitations of the model are not discussed.

### Suggestions for Improvement
We recommend that the authors improve the organization of the related work section by dividing it into three subsections: Transformers for time-series modeling, Transformers for irregular time series, and Continuous time models (NODEs). Additionally, please improve the clarity of key model components, particularly the initialization of keys and values. We suggest rephrasing claims regarding cumulative errors in Neural ODEs to accurately reflect the dependency on the solver used and providing a thorough explanation of the assumptions regarding observation influence to address potential causality issues.

Furthermore, we recommend including comparisons with state-of-the-art models like S5 in the benchmarks and providing full results with standard deviations for all experiments to substantiate claims of "overall statistical superiority." Evaluating the model on regularly sampled datasets would provide valuable insights into its versatility. Lastly, a discussion of the model's limitations should be incorporated to enhance the paper's comprehensiveness.