ID: qFIs4hYZaZ
Title: Learning Interatomic Potentials at Multiple Scales
Conference: NeurIPS
Year: 2023
Number of Reviews: 3
Original Ratings: 9, 5, -1
Original Confidences: 4, 4, 3

Aggregated Review:
### Key Points
This paper presents the MTS-Allegro model, a novel machine learning interatomic potential (MLIP) algorithm designed to accelerate Molecular Dynamics (MD) simulations by separating fast-scale and slow-scale interactions. The method involves co-training two MLIPs: one for efficient but less accurate predictions and another for accurate but time-consuming predictions. Simulation results on an ab initio water system demonstrate a threefold speedup while maintaining comparable accuracy to conventional methods. The paper is well-written and shows significant potential for enhancing MD simulation efficiency.

### Strengths and Weaknesses
Strengths:
1. The innovative approach of scale separation in MLIP-driven MD simulations is well-justified and novel, with limited prior literature in this area.
2. The experimental validation on the ab initio water system convincingly demonstrates the method's effectiveness in terms of speed and accuracy.
3. A comprehensive analysis of energy conservation is provided, which is critical for MD simulations.

Weaknesses:
1. The experiments are limited to a single ab initio water system; testing on a broader range of systems would enhance the evaluation of generalizability.
2. The paper hints at the potential for more complex scale separations but lacks a detailed discussion on the complexity and scalability of these approaches.
3. Clarity is needed on hyperparameter determination, including the radial cutoff, number of parameters, and inner timesteps, specifically whether these can be tuned or are theoretically predetermined.
4. There is a lack of exploration regarding the trade-off between accuracy and computational time based on the relative importance of the inner model, which could benefit from visual representation.

### Suggestions for Improvement
We recommend that the authors improve the discussion on hyperparameter determination, specifically addressing how to decide the radial cutoff, number of parameters, and inner timesteps, and clarify whether these are tunable or predetermined. Additionally, we suggest exploring a more diverse set of systems to assess the generalizability of the method. A deeper discussion on the complexity and scalability of potential multi-level scale separations would also be beneficial. Finally, we encourage the authors to include a visual representation of the accuracy versus computational time trade-off as the importance of the inner model varies.