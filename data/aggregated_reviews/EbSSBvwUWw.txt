ID: EbSSBvwUWw
Title: Matching the Statistical Query Lower Bound for $k$-Sparse Parity Problems with Sign Stochastic Gradient Descent
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the $k$-sparse parity problem over the $d$-dimensional boolean hypercube, utilizing a two-layer neural network trained with a modified online stochastic gradient descent (SGD) algorithm known as "sign SGD." The authors demonstrate that this approach can learn the $k$-parity problem with $n = \tilde O(d^{k-1})$ samples and a network width of $2^{\Theta(k)}$, achieving a total computation time of $\Theta(d^{k} \text{polylog}(d))$. This result aligns with the established Statistical Query (SQ) lower bound of $d^k$.

### Strengths and Weaknesses
Strengths:
- The paper contributes to the understanding of the k-sparse parity problem, matching the SQ lower bound, which is significant in the context of recent interest in deep learning theory.
- The writing is clear and the theoretical analysis is compact and elegant, making the proof accessible.
- The results indicate that SGD on neural networks can achieve the same runtime complexity as SQ algorithms, adding value to the literature on learning with SGD.

Weaknesses:
- The choice of the Sign SGD algorithm is problematic as it lacks rotational invariance, which could limit its applicability to other problems beyond k-parity.
- Comparisons to prior work, particularly Glasgow (2023), may be misleading, as that work demonstrates a more general approach using vanilla SGD.
- The use of the activation function $\sigma(z) = z^k$ is seen as unrealistic, as it hardcodes the problem structure, diverging from more commonly used activations like ReLU.
- The analysis may not provide novel insights compared to existing literature, and the results may feel incremental.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of the Sign SGD algorithm, particularly its lack of rotational invariance, and clarify how this affects the applicability of their results. Additionally, we suggest revising the comparisons to prior work to avoid misleading implications, especially regarding the capabilities of Glasgow (2023). The authors should also consider using a more standard activation function, such as ReLU, to enhance the generalizability of their findings. Finally, we encourage the inclusion of more intuition in the main text for Lemma 5.3, specifically regarding the sufficiency of a batch size of $\tilde O(d^{k-1})$ for gradient concentration.