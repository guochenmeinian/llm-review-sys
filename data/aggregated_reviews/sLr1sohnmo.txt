ID: sLr1sohnmo
Title: Error Bounds for Learning with Vector-Valued Random Features
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 9, 6, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical error analysis for learning with vector-valued random features, specifically focusing on random feature ridge regression. The authors derive a general bound for the population risk functional, demonstrating that $O(\sqrt{N})$ random features suffice to achieve error bounds of the order $O(1/\sqrt{N})$, extending previous results from real-valued to vector-valued outputs. The analysis includes convergence rates for well-specified and mis-specified problems, statistical consistency, and almost sure bounds. The work also introduces a new proof technique and provides strong consistency in the misspecified setting.

### Strengths and Weaknesses
Strengths:
- The paper offers a comprehensive analysis, addressing approximation, generalization, misspecification, and noise.
- It implies minimax optimal convergence rates in well-specified settings and presents improved parameter complexity.
- The writing is clear and the analysis appears rigorous, with mild and standard assumptions.

Weaknesses:
- The focus is limited to random feature ridge regression with square loss, raising questions about the applicability of the technique to other loss functions, such as logistic loss.
- The results exhibit a saturation phenomenon, particularly in Theorem 3.12, where error bounds improve only under specific regularity conditions.
- The obtained bounds do not explicitly account for the output dimension $p$, leaving its impact on learning performance unclear.

### Suggestions for Improvement
We recommend that the authors improve the scope of their analysis by exploring the extension of their technique to other loss functions, such as logistic loss. Additionally, addressing the saturation phenomenon in Theorem 3.12 could enhance the robustness of their results. It would also be beneficial to clarify how the output dimension affects convergence rates in learning with vector-valued outputs. Finally, including concrete examples of feature maps and visual aids could enhance comprehension and accessibility for a broader audience.