ID: wSpFTuqSRF
Title: Model Pairing Using Embedding Translation for Backdoor Attack Detection on Open-Set Classification Tasks
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 8, 8
Original Confidences: 4, 4

Aggregated Review:
### Key Points
This paper presents a novel approach to backdoor detection in open-set classification tasks, particularly in biometric systems. The authors propose using model pairs and embedding translation to detect backdoor attacks without assuming clean models. The method is rigorously evaluated across various architectures, demonstrating effectiveness even when both models are compromised. The theoretical foundation, including cosine similarity and affine transformations, is well-supported, and the experiments provide strong evidence of the method's capabilities.

### Strengths and Weaknesses
Strengths:
1. The approach is original, introducing model pairs and embedding translation, marking a significant departure from existing methods reliant on clean data.
2. The method is flexible, applicable across different architectures and datasets, enhancing its real-world relevance.
3. The focus on biometric systems underscores the practical importance of the work in security-critical environments.
4. The experimental design is thorough, clearly demonstrating the method's effectiveness against various backdoor trigger sizes.

Weaknesses:
1. The method's performance is partially dependent on the quality of backdoor training, which may limit robustness in certain scenarios.
2. There is limited discussion on false positives, which is crucial for real-world deployment.
3. The scope is primarily focused on backdoor detection in biometric systems, potentially limiting broader applicability.
4. The computational overhead of using model pairs may be significant in resource-constrained environments.

### Suggestions for Improvement
We recommend that the authors improve the discussion on false positives to provide a clearer understanding of the methodâ€™s real-world applicability and limitations. Additionally, expanding the exploration of the method to include other types of attacks, such as adversarial examples, would enhance its significance. Finally, addressing the computational overhead associated with running two models in parallel could improve the method's feasibility in real-time applications.