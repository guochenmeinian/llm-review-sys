ID: J66ptjMkAG
Title: Kernel Quadrature with Randomly Pivoted Cholesky
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 8, 6, 7, 4, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 2, 5, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new method for kernel quadrature based on the randomly pivoted Cholesky (RPCholesky) decomposition, utilizing rejection sampling for quadrature point selection. The authors extend the RPCholesky algorithm to continuous domains, providing theoretical guarantees and empirical evaluations that demonstrate its efficiency and accuracy compared to existing methods. The authors derive an error bound for the quadrature method and present comprehensive comparisons against state-of-the-art techniques.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with clear theoretical results and proofs.
- The proposed method shows significant improvements in sampling time and accuracy over continuous volume sampling.
- The experiments provide convincing evidence of the method's performance, achieving spectral accuracy with a logarithmic number of nodes.

Weaknesses:
- There are no empirical results on real datasets, limiting the applicability of the proposed method.
- The discussion on the method's application in machine learning is insufficient.
- The experiments are limited to one type of kernel and low-dimensional settings, lacking broader evaluations.

### Suggestions for Improvement
We recommend that the authors improve the empirical evaluation by including results on real datasets and demonstrating the method's applicability in practical machine learning tasks. Additionally, we suggest expanding the experiments to include various Mercer kernels and higher-dimensional settings to better assess the method's generalizability. Clarifying the scalability of the proposed algorithms as the number of quadrature points increases would also enhance the paper's impact.