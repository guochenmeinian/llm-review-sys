ID: rhIfzCZoXG
Title: Counterfactual Evaluation of Peer-Review Assignment Policies
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 4, 5, 6, 8, -1, -1, -1
Original Confidences: 2, 3, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on counterfactual evaluation of peer-review assignment strategies utilizing randomness to mitigate fraud. The authors adapt existing off-policy evaluation methods to address challenges such as non-positivity, missing reviews, and attribution. They provide a framework for off-policy evaluation with various imputation choices and conduct empirical evaluations on two datasets to analyze the effects of randomness and weights on similarity components. The findings indicate that higher text similarity correlates with better review quality, while randomization has a marginal impact on quality.

### Strengths and Weaknesses
Strengths:  
1. The paper addresses a significant issue, as online A/B experiments for evaluating peer-review assignments are costly and challenging to implement.  
2. The authors propose a comprehensive off-policy evaluation framework with various imputation methods tailored for peer review.  
3. The writing is clear, and the experiments conducted are convincing and yield interesting insights.  

Weaknesses:  
1. There is a lack of evaluation on the quality of the counterfactual evaluation; incorporating random A/B experiments would enhance credibility.  
2. The paper would benefit from a clearer comparison of the proposed off-policy evaluation method with existing evaluation methods to analyze its effectiveness.  
3. Assumptions such as monotonicity and Lipschitz continuity should be stated more explicitly, and the challenges subsection in Section 3 is somewhat disorganized.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of their assumptions and limitations, making them more prominent in the manuscript. Specifically, the authors should explicitly state the assumptions related to reviewer-paper assignments and interference. Additionally, we suggest enhancing the organization of Section 4 by clearly delineating the proposed approaches and their corresponding assumptions. The authors should also provide a more detailed discussion of how uncertainty estimates are generated and consider comparing their method with a broader range of existing evaluation techniques. Lastly, we encourage the authors to acknowledge the limitations of using self-reported expertise and confidence as proxies for review quality and to discuss how these limitations might affect their findings.