ID: OQQoD8Vc3B
Title: Are aligned neural networks adversarially aligned?
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 4, 6, 5, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the alignment of large language models (LLMs) and multimodal models in the context of adversarial attacks, specifically focusing on their ability to produce harmful content. The authors find that while existing NLP attacks are insufficient against aligned text models, multimodal models are more vulnerable to adversarial image manipulations. The research aims to inform alignment techniques by evaluating the robustness of these models against adversarial inputs.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant and timely issue regarding the adversarial robustness of language models.
- It introduces a novel approach to evaluating adversarial alignment, providing empirical evidence that can benefit future research.
- The findings on multimodal models reveal critical weaknesses, highlighting the need for careful deployment.

Weaknesses:
- The paper's structure is disorganized, making it challenging to follow, with unclear explanations and insufficient details on experimental setups.
- The evaluation primarily focuses on toxicity, neglecting other potential metrics of misalignment, and only employs a limited number of NLP attacks.
- Key concepts are not clearly defined, leading to confusion regarding the contributions and objectives of the research.

### Suggestions for Improvement
We recommend that the authors improve the paper's organization to enhance clarity, particularly by providing a more detailed background on existing attack methods and their relevance to the proposed approach. Additionally, we suggest clarifying the experimental setup, including specific details such as the meaning of "StableVicuna" and the parameters used in the attacks. The authors should also address the unclear expressions in lines 171-175 and provide a more comprehensive explanation of the challenges mentioned. Lastly, we encourage the authors to expand their evaluation to include a broader range of adversarial attacks and metrics beyond toxicity to strengthen their contributions.