ID: xXfDB8kJUs
Title: Learning Curves for Noisy Heterogeneous Feature-Subsampled Ridge Ensembles
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 5, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of ensemble learning with linear ridge regression, focusing on heterogeneous feature subsampling and the interplay between readout noise and feature subsampling in neural networks. The authors derive generalization error expressions using the replica trick from statistical physics, demonstrating their findings through simulations that align with numerically calculated errors. They explore the double descent phenomenon, suggesting that heterogeneous feature subsampling can mitigate its effects. Additionally, the authors propose that readout noise can coexist with feature subsampling in both biological neural circuits and artificial neural networks, particularly when conceptualizing features as representations in the visual cortex or dimensions of a feature map. The analysis includes the impact of noise and regularization on generalization error, providing insights into the behavior of ridge regression ensembles.

### Strengths and Weaknesses
Strengths:
- The work offers novel insights into the double descent phenomenon, addressing key questions in modern machine learning theory.
- The mathematical analysis is rigorous, employing the replica trick as a new tool for studying this phenomenon.
- The authors clearly define the basic setting and assumptions, and adequately cover related work, establishing the novelty of their contribution.
- The authors' revisions and responses to feedback are appreciated, particularly the inclusion of a table of key parameters and terms for clarity.
- The justification for readout noise in physical neural networks is compelling and strengthens the theoretical model.
- The analysis is considered novel and useful for researchers.

Weaknesses:
- The paper relies on simplifying assumptions (e.g., Gaussian data, linear teacher function) that may not reflect real-world scenarios, limiting practical applicability.
- The presentation is challenging to follow, with unclear definitions of key terms such as "readout noise" and "readout dimensionality."
- The results lack practical implications for analysts wishing to apply heterogeneous feature-subsampled ridge ensembles optimally.
- The novelty of the findings is somewhat unclear, as similar approaches have been studied previously without clear differentiation.
- The justification for readout noise in the context of feature subsampling remains weak, particularly in relation to artificial and biological neural networks.
- The use of non-standard symbols for sample size and feature dimension may confuse readers.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their presentation by defining key terms such as "readout noise" and "readout dimensionality" upon their first introduction. Additionally, simplifying the explanation of main results and conclusions, perhaps through examples or tables, would enhance reader comprehension. It would also be beneficial to include clearer examples where feature subsampling and readout noise coexist, as well as citations to biological literature supporting the existence of structures corresponding to their model. Furthermore, we suggest that the authors explicitly state the limitations of the replica method used in their analysis, clarify how their work differs from existing studies on feature subsampling ensembles, and mention the additional results in the supplementary materials within the main paper, particularly summarizing experiments on Cifar 10 to validate their findings on real data. Lastly, we recommend revising the presentation to make it less busy and more accessible for readers, and using commonly accepted symbols in machine learning for sample size and feature dimension.