ID: aYJ2T5TXoX
Title: Generalizability of experimental studies
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 5, 3, 5, 5, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a formalization of generalizability in experimental studies within machine learning, utilizing three types of kernels to quantify differences in experimental rankings. A significant contribution is the development of an algorithm that estimates the number of studies required to achieve a desired level of generalizability. The authors also conduct experiments on large language models (LLMs) to demonstrate the framework's efficacy.

### Strengths and Weaknesses
Strengths:
- The topic is relevant and addresses a significant issue in machine learning.
- The paper is clearly written, with a well-defined formalization of generalizability through kernel methods.
- The theoretical framework is robust, extending beyond standard reproducibility definitions.

Weaknesses:
- The practical usefulness of the algorithm remains unclear, with insufficient empirical evidence supporting its effectiveness.
- There is a lack of discussion regarding the computational costs associated with the algorithm.
- The Python package is improperly configured, leading to import errors.
- The paper does not adequately address the issue of train/test splits in machine learning experiments.
- The proposed kernel-based testing for rankings lacks standard statistical validation, with no comparison to established tests like Kendall's τ or Spearman's ρ.

### Suggestions for Improvement
- We recommend that the authors improve the clarity of the writing by defining symbols before use and ensuring consistent terminology throughout the paper.
- Please include a discussion on the computational costs of the algorithm to enhance transparency.
- We suggest addressing the train/test split issue explicitly, clarifying the assumptions regarding training and validation sets.
- Consider comparing the proposed kernel methods to standard statistical tests to justify their use.
- We recommend revising figure captions to provide more detailed descriptions of the plots and the variables involved.
- Please restructure Section 6 to place conclusions before limitations and future work for better flow.
- We advise the authors to clarify the limitations of the MMD kernel in quantifying effect sizes and to explore how this framework can measure practical differences between alternatives.