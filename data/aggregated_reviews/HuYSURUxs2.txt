ID: HuYSURUxs2
Title: Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 8, 8, 6
Original Confidences: 3, 4, 3

Aggregated Review:
### Key Points
This paper investigates the effectiveness of using weaker, less compute-intensive language models (LMs) for generating synthetic training data compared to stronger, more expensive models. It evaluates trade-offs in coverage, diversity, and false positive rates, demonstrating that training with WC-generated data consistently outperforms SE-generated data across multiple benchmarks, including MATH and GSM-8K. The authors propose a novel "weak-to-strong" improvement method, suggesting that weaker models can effectively enhance stronger models, leading to state-of-the-art performance while optimizing computational resources.

### Strengths and Weaknesses
Strengths:  
- **Innovative Approach:** The introduction of the "weak-to-strong" improvement method provides a fresh perspective on model training, achieving state-of-the-art results compared to stronger models.  
- **Comprehensive Evaluation:** The study evaluates various setups, including knowledge distillation and self-improvement, across multiple models and benchmarks.  
- **Cost-Effective:** The argument for using weaker models presents a viable path for making training more affordable and accessible.

Weaknesses:  
- **Higher False Positive Rates:** The use of weaker models results in higher false positive rates in synthetic data, which may impact downstream tasks.  
- **Limited Model Inclusion:** The focus on the *Gemma* series models restricts the findings' generalizability, as including a broader range of models like the *LLaMa* series could strengthen the conclusions.  
- **Generalizability Concerns:** The narrow set of models tested raises concerns about the broader applicability of the results.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by including a wider range of models, such as the *LLaMa* and *Mistral* series, in their experiments. Additionally, we suggest conducting experiments with higher computational budgets to determine if the SE model can outperform the WC model under different conditions.