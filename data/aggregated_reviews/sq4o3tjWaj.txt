ID: sq4o3tjWaj
Title: Whatâ€™s Left? Concept Grounding with Logic-Enhanced Foundation Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Neuro-FOL, a method that integrates a large language model (LLM) for generating first-order logic (FOL) programs, an FOL executor, and trainable concept grounding modules to enhance performance on various visual and 3D question-answering tasks. The method employs a differentiable FOL executor to evaluate the generated programs hierarchically, utilizing domain-specific grounding modules. Evaluations across 2D question answering, 3D referring expressions, temporal sequence reasoning, and robotic manipulation demonstrate that Neuro-FOL performs competitively, particularly excelling in low data settings.

### Strengths and Weaknesses
Strengths:
- The core idea of leveraging a pre-trained LLM for logical query breakdown is innovative and well-motivated, offering both generality and adaptability to new domains.
- The evaluation across diverse datasets showcases the method's applicability across various domains.
- Strong performance in low data scenarios compared to end-to-end methods is a notable advantage.
- The paper is generally well-presented and clearly motivated.

Weaknesses:
- The claim of universality across domains is potentially misleading, as it relies on sufficient training data for domain-specific modules, which is not adequately addressed.
- The limited set of domains and narrow reasoning types may restrict the method's applicability, particularly in more complex visual reasoning tasks.
- The comparison with pure neural-symbolic (NS) approaches lacks clarity on the advantages of Neuro-FOL, particularly regarding the importance of FOL versus the absence of predefined modules.
- Some claims, such as the ability to perform across all domains, are misleading since components are trained for specific domains.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the training process of Neuro-FOL, particularly how it handles domain-independent learning and the implications of training on multiple domains versus individual domains. Additionally, the authors should provide more detailed discussions on the limitations of their methodology, particularly concerning the separation of language and perception aspects. It would also be beneficial to clarify the role of FOL in capturing context-dependent meanings and to address the potential issues with concepts that are underrepresented in training data. Lastly, we suggest explicitly stating the training loop for Neuro-FOL and the implications of invalid or incorrect programs during evaluation.