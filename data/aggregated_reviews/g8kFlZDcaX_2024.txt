ID: g8kFlZDcaX
Title: Decision-Focused Learning with Directional Gradients
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 5, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new family of surrogate losses, termed perturbation gradient losses (PG loss), for the predict-then-optimize framework in contextual optimization problems. The authors provide a theoretical framework demonstrating that uniform convergence allows for the identification of sub-optimal hypotheses with low empirical PG loss, which also yields low expected decision loss. The paper includes theoretical analysis to establish bounds on approximation errors and regret, alongside extensive experiments demonstrating the method's advantages. The authors acknowledge the NP-Hardness of the problem, indicating that while gradient descent may not guarantee a global optimum, empirical experiments suggest it can find high-quality local optima. Theorems 3.4 and 3.7 provide insights into the relationship between the empirical PG loss and expected decision loss, emphasizing the importance of choosing an appropriate decay rate for the parameter \( h \). 

### Strengths and Weaknesses
Strengths:  
- The proposed PG loss is more efficient than standard DFO loss, requiring only the computation of the optimal decision variable without implicit differentiation through $\hat{\pi}(t)$.  
- The approximation error of the surrogate functions decreases with an increasing number of samples, beneficial for large-sample applications.  
- The paper offers a solid theoretical foundation with uniform convergence results, enhancing understanding of the relationship between empirical and expected losses.  
- The empirical evidence supports the theory, showing that gradient descent can yield high-quality solutions.  
- The theoretical results are relevant and non-trivial, and the paper is well-motivated, addressing performance guarantees under model misspecification.  

Weaknesses:  
- The (sub)differentiability of the PG loss is inadequately established, particularly in Line 172, where the differentiability is questionable, and the directional derivative in Line 148 is not well-defined.  
- The NP-Hardness of the problem limits the ability to provide guarantees on global optimization, which may undermine the practical applicability of the theoretical results.  
- The paper's empirical evaluation is limited, primarily focusing on synthetic data and discrete decision variables, lacking sufficient exploration of continuous decision scenarios.  
- Some reviewers express skepticism regarding the rigor of certain proofs, particularly Lemma 3.2, suggesting a need for clearer justification.  
- The method's performance appears to deteriorate on more complex problems, and the role of \( h \) in approximations is unclear, with insufficient details on the proof of part c) in Theorem 2.1.  

### Suggestions for Improvement
We recommend that the authors improve the theoretical rigor regarding the (sub)differentiability of the PG loss, particularly addressing the concerns raised about Lines 148 and 172. Additionally, we suggest conducting further experiments on real-world and large-scale problems to better illustrate the advantages of the proposed method. Clarifying the role of \( h \) in the approximations and providing detailed proofs for Theorem 2.1 would enhance the paper's clarity and robustness. We also recommend improving the rigor of the proofs, particularly for Lemma 3.2, to enhance the overall credibility of the theoretical claims. Furthermore, making the implicit facts regarding the smoothness of decision-loss curves more explicit, potentially as standalone lemmas, would aid in the intuition surrounding the performance of first-order methods. Lastly, addressing the concerns related to the curse of dimensionality in the context of sample growth would strengthen the paper's quantitative claims.