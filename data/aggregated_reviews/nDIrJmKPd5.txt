ID: nDIrJmKPd5
Title: Private Distribution Learning with Public Data: The View from Sample Compression
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 8, 7, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies differentially private learnability of probability distributions, establishing significant connections between public-private learnability, sample compression schemes, and list learning. The authors demonstrate that a class of distributions is public-private learnable if it is realizably compressible and list learnable, providing concrete bounds on sample sizes. The results have notable implications, such as proving that mixtures of Gaussians are public-private learnable, and establishing lower bounds for learning d-dimensional Gaussians. The paper also explores robust compressible classes and their relation to public-private learning.

### Strengths and Weaknesses
Strengths:
- The paper presents original and significant contributions to the learning community, particularly through the elegant equivalence of three properties of distribution classes.
- The results are technically interesting and novel, with clear proofs that enhance understanding.
- The writing is clear and the paper is enjoyable to read, effectively distilling complex concepts.

Weaknesses:
- The reliance on the appendix for proof techniques limits the depth of discussion in the main body, particularly in Sections 6 and 7, which could be more insightful.
- The characterizations are confined to pure differential privacy, which restricts the applicability of some results.
- The reductions between different notions of learning are inefficient and not always constructive.

### Suggestions for Improvement
We recommend that the authors improve the discussion of proof techniques in the main body, particularly for Proposition 3.2, to enhance reader comprehension. Additionally, consider simplifying the bounds presented to allow for more detailed explanations. It would be beneficial to provide definitions for terms like "TV" and "$SC$" within the main text rather than in the appendix. We suggest adding a conclusion section to summarize findings and propose open problems. Lastly, addressing the limitations of the current work in the main body could provide a clearer understanding of its implications.