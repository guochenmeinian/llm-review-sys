ID: zrUEHZ6s9C
Title: Algorithm Selection for Deep Active Learning with Imbalanced Datasets
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 7, 6, 6, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TAILOR, a novel algorithm for active learning that adaptively selects from multiple candidate algorithms using a multi-armed bandit framework. The authors propose new reward functions that balance class diversity and informativeness in selecting unlabeled samples. The theoretical justification through regret analysis supports the effectiveness of TAILOR, which is evaluated on various multiclass and multilabel datasets, demonstrating comparable or superior accuracy to the best candidate active learning strategies.

### Strengths and Weaknesses
Strengths:
1. The unification of existing active learning algorithms into the TAILOR framework alleviates the challenge of selecting an effective algorithm for specific problems.
2. The novelty of the reward function design in the context of deep active learning is significant and non-trivial.
3. Extensive experimentation across multiple datasets enhances the credibility of TAILOR's effectiveness.
4. The theoretical analysis indicating better regret bounds compared to linear contextual bandits is a notable contribution.

Weaknesses:
1. The reliance on a single ResNet-18 architecture for experimentation limits the generalizability of the results; results from multiple architectures, especially more powerful ones like ResNet101 or ViT, would strengthen the findings.
2. The impact of \(\gamma\) (line 281) is not explored; an ablation study on varying \(\gamma\) values would be beneficial.
3. The reward function aimed at maximizing participation from minority classes lacks empirical validation; an ablation study comparing minority class performance against different baselines would provide valuable insights.
4. The motivation for algorithm selection is questioned, particularly regarding the dominance of BADGE across datasets, and the time efficiency of TAILOR compared to single algorithms is unclear.

### Suggestions for Improvement
We recommend that the authors improve the experimental evaluation by including results from multiple architectures, particularly more powerful models like ResNet101 and ViT. Additionally, conducting an ablation study to analyze the impact of varying \(\gamma\) values would enhance the understanding of the reward function's effectiveness. It would also be beneficial to include an ablation study focused on the performance of minority classes to validate the proposed reward function's impact. Finally, addressing the motivation behind algorithm selection, particularly in relation to BADGE's performance, and clarifying the time efficiency of TAILOR compared to single algorithms would strengthen the paper.