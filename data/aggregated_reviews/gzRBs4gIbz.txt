ID: gzRBs4gIbz
Title: Non-autoregressive Streaming Transformer for Simultaneous Translation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to Non-autoregressive Streaming Transformer (NAST) for Simultaneous Translation (SiMT), addressing issues of aggressive anticipation in previous autoregressive models that lead to poor translation quality and high latency. The authors propose a flexible READ/WRITE strategy and alignment-based latency loss, which significantly improves translation quality and latency, as demonstrated by experimental results on WMT15 German → English and WMT16 English → Romanian benchmarks. The paper also introduces a non-autoregressive model that utilizes CTC training for monotonic alignment and incorporates latency control, non-monotonic latency alignments, glancing, and multi-stage training, yielding substantial improvements over baseline models.

### Strengths and Weaknesses
Strengths:
- The proposed methods are novel and represent one of the first applications of non-autoregressive models to simultaneous translation.
- The paper is well-motivated, clearly organized, and presents experimental results with effective visualizations.
- The approach shows significant improvements in BLEU scores and reduces hallucination compared to autoregressive models.

Weaknesses:
- The paper lacks in-depth analysis of results and examples to illustrate the methods, which could enhance reader understanding.
- There are concerns regarding the adaptability of the proposed method to other translation tasks, such as multilingual or low-resource translation.
- The novelty of using non-autoregressive techniques in SiMT may require further clarification, particularly regarding the contribution of the proposed loss function.

### Suggestions for Improvement
We recommend that the authors improve the analysis of model behavior by including case studies and generation examples to better illustrate methods and results. Additionally, the authors should clarify the novelty of their approach, particularly in relation to existing literature on loss functions. It would also be beneficial to demonstrate that the non-autoregressive model maintains fluency and coherence in translations, potentially using tools like GPT-4 for evaluation.