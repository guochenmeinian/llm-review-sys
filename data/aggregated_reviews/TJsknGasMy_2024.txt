ID: TJsknGasMy
Title: Differentially Private Stochastic Gradient Descent with Fixed-Size Minibatches: Tighter RDP Guarantees with or without Replacement
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 4, 7, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the RÃ©nyi differential privacy (RDP) guarantees of the subsampled Gaussian mechanism with fixed-sized random minibatches, considering both with and without replacement scenarios. The authors improve upon previous results by Wang et al. (2019) by a factor of four, demonstrating that fixed-size subsampling without replacement yields better privacy-utility trade-offs compared to Poisson subsampling. Additionally, the paper proposes a method for calculating RDP bounds more efficiently, particularly in the context of Gaussian mixtures, offering tighter bounds than those in prior works. The authors provide both analytical and numerical investigations, highlighting the variance analysis and the empirical benefits of fixed-size subsampling, while also providing insights into privacy accounting.

### Strengths and Weaknesses
Strengths:
- The analysis is solid, and the paper is well-written, providing thorough discussions.
- The results are significant and likely impactful in the domain of differential privacy in deep learning, aligning closely with practical applications.
- The proposed method offers a more efficient calculation of RDP bounds compared to numerical methods.
- The bounds presented are tighter than those previously established, providing useful insights into privacy accounting.
- The numerical experiments illustrate the advantages of fixed-size subsampling without replacement.

Weaknesses:
- The paper overlooks recent relevant research, particularly the work by Zhu et al. (2022), which presents a dominating pair of distributions for the Gaussian mechanism that could enhance the paper's contributions.
- The upper bounds for "with replacement" scenarios appear conservative, and the analysis lacks clarity regarding the accuracy of these bounds.
- The derivation of the bounds in Section D.4 lacks clarity and detail, particularly regarding the assumptions made.
- There is confusion regarding the definition of "worst-case" bounds, and the paper does not adequately address the potential numerical challenges associated with the proposed methods.

### Suggestions for Improvement
We recommend that the authors improve the literature review to include recent advancements, particularly Zhu et al. (2022), to strengthen the paper's relevance. Additionally, we suggest providing clearer explanations of the upper bounds for "with replacement" scenarios, including their accuracy and implications. It would also be beneficial to discuss the relationship between the results presented and those from previous works, such as Mironov et al. (2019), to contextualize the contributions more effectively. Furthermore, we recommend improving the clarity of the derivation in Section D.4 by providing a detailed explanation of the assumptions and calculations involved. Adding more discussions on the Taylor expansion terms in the main paper to clarify the reasons for the constant improvement observed would also enhance the paper. Lastly, addressing the potential numerical challenges of the proposed method in relation to existing techniques would further strengthen the paper's contribution.