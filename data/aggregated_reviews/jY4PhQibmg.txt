ID: jY4PhQibmg
Title: Gated Slot Attention for Efficient Linear-Time Sequence Modeling
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Gated Slot Attention (GSA), an enhancement of Gated Linear Attention (GLA), designed to improve sequence modeling efficiency. GSA employs a selective gating mechanism and a two-pass GLA structure, allowing for better context awareness while maintaining a bounded memory footprint suitable for long sequences. A significant improvement is the incorporation of a softmax operation to retain sharp attention distributions, which enhances modeling by reducing dilution. The architecture is validated with up to 2.7 billion parameters from scratch and 7 billion in continual training, showing promising results on standard LM-eval harness benchmark tasks.

### Strengths and Weaknesses
Strengths:
- GSA's gated update mechanism and two-pass structure provide a straightforward enhancement over GLA, ensuring manageable memory usage critical for long sequences.
- Experimental results validate GSA's effectiveness in improving performance in sequence modeling tasks.

Weaknesses:
- GSA builds largely on existing GLA techniques, making its enhancements appear incremental rather than revolutionary, which may limit its perceived impact.
- The softmax operation on $QK^T$ retains quadratic complexity in training, raising concerns about the authors' claim of linear-time sequence modeling.
- The paper lacks explicit clarification on whether GSA inherits GLA's recurrent architecture, necessitating comparisons with other recurrent models like RWKV for a comprehensive evaluation.
- The evaluation is limited to LM-eval harness, with no exploration of tasks requiring longer context reasoning or retrieval, leaving GSA's performance in such scenarios unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the significance of GSA's proposed method and its advantages over prior work. It would be beneficial to include comparisons with models lacking context-aware query vectors to demonstrate GSA's real-world applicability. Additionally, we suggest evaluating GSA on tasks that stress-test memory utilization, such as retrieval and long-context tasks, to substantiate claims of memory efficiency. An error analysis on the continually trained models could help clarify the performance discrepancies observed in MMLU scores. Finally, we advise defining "slots" in the introduction for better reader comprehension and addressing the presentation issues noted in the reviews.