ID: B7S4jJGlvl
Title: Symbolic Regression with a Learned Concept Library
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 8, 7, 6, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LASR, a symbolic regression framework that enhances PySR by incorporating Large Language Models (LLMs) to discover and evolve concepts from high-performing equations. The authors propose using LLMs to guide genetic operations through a library of natural language concepts, evaluated on the Feynman equations dataset and synthetic tasks, demonstrating improved performance over existing symbolic regression baselines. The authors also provide a detailed comparison of performance metrics, emphasizing that LASR's equations are closer to the ground truth than those discovered by PySR. They argue that evaluating performance based on the number of iterations is more reliable than wall clock time due to the nature of equation evaluation in scientific simulations. Furthermore, they address concerns regarding the consistency of discovered equations, clarifying that while the equations may differ in format, they consistently fit the underlying dataset.

### Strengths and Weaknesses
Strengths:
- The integration of LLMs into symbolic regression to learn and utilize abstract concepts is innovative and well-structured.
- LASR demonstrates superior performance compared to PySR, even with fewer iterations.
- The methodology is clear, with a strong focus on data contamination concerns, effectively explaining how LLMs are unlikely to retrieve solutions from training data.
- The paper effectively uses correlation metrics to assess the closeness of discovered equations to ground truth.
- The authors provide a clear rationale for their choice of performance evaluation metrics.

Weaknesses:
- The evaluation lacks comprehensive analysis and raises concerns about potential data leakage, particularly when using well-known physics equations.
- Claims regarding state-of-the-art performance and the impact of LLM size need to be tempered, as they may not be fully substantiated by the experiments.
- The introduction of a concept library may not be universally beneficial, especially in unknown environments where traditional evolutionary algorithms might excel.
- The marginal improvement of LASR may not seem significant in some contexts.
- Concerns about the consistency of discovered equations could lead to questions about the reliability of the results.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by addressing potential data leakage more thoroughly and providing a more comprehensive analysis of the method's generalizability. Additionally, we suggest toning down strong claims regarding performance and clarifying the distinction between results based on exact matches versus metrics like MSE. It would also be beneficial to explore the effectiveness of simpler prompting strategies as baselines and to include a discussion on the implications of using LLMs in environments with unknown functions. Furthermore, we recommend improving the clarity of the discussion regarding the consistency of discovered equations, perhaps by providing more examples or visualizations to illustrate the differences in format while maintaining performance. Lastly, expanding the manuscript to include a more in-depth discussion of the physical concepts underlying the equations discovered by LASR would enhance the contextual understanding of their findings, along with providing a comparison of running times between LASR and PySR, particularly under varying iterations, to enhance the understanding of computational trade-offs.