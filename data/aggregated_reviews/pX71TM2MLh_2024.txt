ID: pX71TM2MLh
Title: Data Free Backdoor Attacks
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 6, 5, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents data-free backdoor attacks (DFBA), a method for injecting backdoors into pre-trained neural networks by modifying parameter weights without requiring fine-tuning or access to clean data. The backdoor is activated by a trigger calculated from the weights of a designated neuron in the first layer, allowing the model to classify inputs as the target class when the trigger is present. DFBA reportedly achieves a nearly 100% attack success rate (ASR) in under one second of injection time, with minimal impact on clean accuracy (CA). The authors propose that further scaling of model size does not significantly increase the gap between clean accuracy and backdoor accuracy, as evidenced by experiments on the ResNet-152 model.

### Strengths and Weaknesses
Strengths:  
1. The DFBA attack is straightforward, intuitive, and demonstrates strong performance, making it a significant contribution to the study of backdoor attacks.  
2. The paper addresses an increasingly relevant topic in security, proposing a novel attack vector that warrants further investigation.  
3. Empirical evidence shows that increasing model size does not significantly affect the CA/BA gap.  
4. Section 4.1 provides clear technical details, particularly in interpreting the mathematical concepts presented.  
5. DFBA shows high effectiveness with nearly 100% ASR and minimal performance degradation.

Weaknesses:  
1. Claims regarding the undetectability and unremovability of the attack are overly optimistic, as they assume defenders are unaware of the attack and not actively searching for it.  
2. The paper lacks clarity on the number of features required for the trigger and how this choice ensures stealthiness, as well as a fair comparison to baseline methods.  
3. The theoretical analysis assumes no clean samples will activate the backdoor, which was contradicted by experimental results showing that at least one clean image did activate it.  
4. Key results regarding DFBA's effectiveness against state-of-the-art defenses are not included in the main text, despite being a major contribution.  
5. The method appears limited to specific architectures, such as convolutional neural networks, which should be explicitly stated in the main text.  
6. The method currently requires specific designs for different architectures, limiting its extensibility, and the authors acknowledge the inability to quickly adapt DFBA to other architectures.  
7. Efficiency goals in Section 3.3 are vaguely defined and would benefit from quantifiable metrics.  
8. The paper contains grammatical and syntactical errors that need correction.

### Suggestions for Improvement
We recommend that the authors improve the clarity of claims regarding the undetectability and unremovability of the attack by including caveats about defender awareness. Additionally, provide detailed information on the trigger features and their stealthiness, and ensure a fair comparison with baseline methods. It is crucial to include results on DFBA's effectiveness against state-of-the-art defenses in the main text. The authors should also clarify the method's limitations regarding architecture specificity and improve the adaptability of DFBA to various architectures to enhance its applicability. Furthermore, defining efficiency goals with quantifiable metrics and providing more empirical results on different models would strengthen the claims regarding the method's performance and extensibility. A thorough proofreading is necessary to correct grammatical and syntactical errors throughout the paper.