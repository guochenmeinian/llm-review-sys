ID: pLwYhNNnoR
Title: PRODIGY: Enabling In-context Learning Over Graphs
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: 6, 6, 6, 8
Original Confidences: 3, 4, 2, 4

Aggregated Review:
### Key Points
This paper presents PRODIGY, a framework designed for in-context learning on graph classification tasks. It introduces a prompt graph as a unified representation for various tasks and develops a graph neural network architecture alongside a family of in-context pre-training objectives. The experimental results indicate that the pretrained model demonstrates strong in-context learning performance across multiple tasks within the same domain as the pre-training data, without requiring fine-tuning.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a significant issue in enabling in-context learning for diverse graph machine learning tasks.
2. The approach is novel and technically sound, particularly the "Task graph Message Passing" step, which enhances label information propagation.
3. The paper is well-structured, with clear presentation and descriptive figures.

Weaknesses:
1. The performance may be contingent on the similarity between test and pre-training data, raising questions about transferability to different datasets.
2. The comparison with fine-tuning is inadequate, lacking explicit evaluation of the differences between in-context learning and fine-tuning settings.
3. Performance improvements may be partially due to a larger model size, as indicated by the architecture's complexity compared to baseline models.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of transferability by testing the model on datasets that differ from the pre-training data. Additionally, we suggest conducting a more comprehensive comparison between in-context learning and fine-tuning to clarify the advantages of the proposed approach. It would also be beneficial to address the potential influence of model size on performance by including a detailed analysis of parameter counts and their effects. Lastly, we encourage the authors to consider incorporating more recent self-supervised graph learning baselines, such as GraphMAE, to strengthen their comparative analysis.