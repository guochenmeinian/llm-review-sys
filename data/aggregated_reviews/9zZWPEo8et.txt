ID: 9zZWPEo8et
Title: LogicAttack: Adversarial Attacks for Evaluating Logical Consistency of Natural Language Inference
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for assessing LLMs beyond task performance, highlighting that high performance may arise from recognizing spurious correlations. The authors propose using propositional logic constructs to evaluate model consistency across various logical forms, generating negations and combining premise-hypothesis pairs with an LLM. They demonstrate that models struggle with approximately 51% of rewritten yet logically consistent examples. Additionally, the paper explores nine data augmentation techniques to test the robustness of NLI models, revealing vulnerabilities in existing models. The authors evaluate their adversarial attack methodology on multiple models, showing that larger models are less susceptible to attacks.

### Strengths and Weaknesses
Strengths:
- The paper identifies a significant flaw in existing LLMs and proposes a systematic method for assessing logical discrepancies.
- A variety of propositional logical constructs are tested for model assessment.
- Extensive experiments are conducted to evaluate the robustness of NLI models against different premise-hypothesis combinations.

Weaknesses:
- The negation generation using an LLM raises concerns about quality and potential hallucinations, impacting the reliability of results.
- The paper's reliance on a single dataset (SNLI) for evaluation may not suffice for definitive conclusions.
- The perturbation strategies and their effectiveness are not clearly justified, and the modified data significantly differs from the original, challenging the stealthiness principle in attack papers.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their negation generation process and provide a more robust justification for the quality of generated perturbations. Additionally, the authors should consider expanding their evaluation beyond SNLI to strengthen their conclusions. Clarifying the relationship between perturbations and formal expressions through better formatting and additional whitespace would enhance readability. Finally, addressing the legibility of figures and providing clearer distinctions between baseline and proposed results would improve the overall presentation of the paper.