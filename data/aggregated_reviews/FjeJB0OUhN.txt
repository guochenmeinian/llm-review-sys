ID: FjeJB0OUhN
Title: Can Long-Context Language Models Subsume Retrieval, SQL, and More?
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 2, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LOFT, a benchmark for evaluating Long Context Language Models (LCLMs) across six diverse tasks and 35 datasets, requiring context lengths from 32k to 1M tokens. The authors propose a novel prompting strategy, Corpus-in-Context (CiC), and conduct experiments using the latest LCLMs, Gemini 1.5 Pro and GPT-4o. The findings indicate that while LCLMs excel in various tasks, they struggle with SQL-like reasoning. The paper provides a comprehensive analysis of the strengths and limitations of LCLMs, including positional analysis of CiC prompting.

### Strengths and Weaknesses
Strengths:
- The benchmark covers a wide range of tasks relevant to LCLMs, with clear documentation and analysis.
- The introduction of CiC prompting and detailed ablation studies enhance the understanding of model performance.
- The paper effectively outlines the advantages of LCLMs over traditional models in many tasks.
- All datasets are open source, promoting reproducibility and accessibility for the research community.

Weaknesses:
- The SQL performance is subpar compared to traditional models, and the analysis lacks illustrative examples of failures.
- The reliance on existing datasets raises concerns about data leakage and the reliability of conclusions.
- The evaluation metrics and experimental settings, particularly for SQL tasks, are questioned for their realism and rigor.
- The motivation for evaluating LCLMs on information retrieval tasks is not clearly articulated.

### Suggestions for Improvement
We recommend that the authors improve the SQL analysis by incorporating more recent datasets and providing clearer explanations of how non-unique gold-SQLs impact their conclusions. A thorough error analysis would help elucidate the challenges faced by LCLMs in SQL reasoning. Additionally, we suggest expanding the context length to ~8M tokens to future-proof the benchmark. Clarifying the motivation for evaluating LCLMs on IR tasks and detailing the dataset filtering techniques would enhance the paper's clarity. Lastly, we encourage the authors to consider using SPARQL instead of SQL for richer query capabilities in the LOFT setting.