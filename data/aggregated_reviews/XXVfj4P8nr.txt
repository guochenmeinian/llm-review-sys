ID: XXVfj4P8nr
Title: Training-Free Open-Ended Object Detection and Segmentation via Attention as Prompts
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 4, 5, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a training-free open-ended object detector that utilizes a vision-language model (VLM), specifically CogVLM, to recognize and roughly locate objects through attention maps, subsequently prompting the Segment Anything Model (SAM) with these coarse results. The authors propose techniques such as head aggregation, attention flow, attention score regularization, iterative refinement, and scale/prompt ensemble to enhance the accuracy of point prompts. VL-SAM demonstrates state-of-the-art performance on open-ended datasets like LVIS and CODA, with ablation studies confirming the contribution of each component to detection performance.

### Strengths and Weaknesses
Strengths:
- The integration of pre-trained large models to accomplish tasks that individual models cannot is a promising direction.
- The proposed techniques—head aggregation, attention flow, attention score regularization, iterative refinement, and scale/prompt ensemble—are intuitively effective for improving zero-shot performance.
- The method is well-explained, and the ablation study of individual components is thorough.

Weaknesses:
- Speed concerns arise as the VL-SAM approach requires multiple passes through CogVLM-17B and SAM-huge, raising questions about the model's frames per second (FPS) and whether the performance gains justify the computational cost.
- Experimental details are unclear, particularly regarding the significance of question ensemble and multi-scale operations, which are common techniques, and the lack of examples for question prompts.
- The paper does not adequately compare VL-SAM's performance against other methods like DetClipV3, and the evaluation metrics raise questions about the necessity of VL-SAM.
- The reliance on large foundation models may skew comparisons, and the impact of using smaller models on VL-SAM's performance is not discussed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of experimental details, particularly by providing examples of question prompts and discussing the significance of the question ensemble and multi-scale operations. Additionally, a more comprehensive comparison with other methods, including DetClipV3, should be included to contextualize VL-SAM's performance. We suggest that the authors address the trade-offs between speed and accuracy more thoroughly, considering the significant delays introduced by operations like "Iterative Refine" and "Multi-scale." Lastly, a discussion on how to select an appropriate vision-language model based on specific characteristics would enhance the paper's contribution.