ID: f71xXsoG1v
Title: Provable convergence guarantees for black-box variational inference
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 8, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the first convergence results for black-box variational inference (VI), establishing convergence rates under specific assumptions on the log model, $\log p$, and utilizing a Gaussian variational family of distributions. The authors address challenges related to non-smoothness in the objective function and provide theoretical results for two stochastic optimization algorithms, proximal and projected stochastic gradient descent. The paper includes case studies to validate the assumptions and demonstrates the significance of the results in practical Bayesian applications.

### Strengths and Weaknesses
Strengths:  
- The work fills a critical gap in the literature by providing convergence guarantees for black-box VI, a widely used framework.  
- The paper is well-organized and presents a clear narrative, making it accessible to readers.  
- The introduction of a proximal operator for optimizing the covariance matrix is a novel contribution to the Gaussian VI literature.  
- The case studies effectively illustrate the validity of the assumptions made in the analysis.  

Weaknesses:  
- The scope of the analysis is somewhat limited to Gaussian variational families and does not consider data subsampling on $\log p$, which could enhance the impact of the work.  
- There are concerns regarding the clarity of certain technical aspects, such as the implications of the covariance matrix being zero and the relevance of the score-type estimator.  
- The paper may not appeal to a broader audience due to its niche focus, and the novelty of the results compared to existing literature on stochastic optimization is questioned.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the implications of the covariance matrix being zero and the role of the score-type estimator in the context of their results. Additionally, consider expanding the scope of the analysis to include data subsampling, as this could significantly enhance the paper's relevance and applicability. We also suggest integrating a comparison with existing works, such as [Xu & Campbell, 2022], to contextualize the contributions of this paper within the broader landscape of optimization theory for Gaussian VI. Finally, providing additional case studies involving more complex Bayesian models could further substantiate the practical implications of the findings.