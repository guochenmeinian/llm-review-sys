ID: eowkjKVPoH
Title: Mission Impossible: A Statistical Perspective on Jailbreaking LLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 6, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new theoretical framework that conceptualizes prompts as combinations of concepts and queries, facilitating a detailed analysis of how and why LLMs can be manipulated into producing unsafe responses. The authors propose a statistical metric for alignment and introduce Enhanced Reinforcement Learning from Human Feedback (E-RLHF), which modifies existing RLHF methods to enhance the likelihood of safe responses from LLMs without incurring additional training costs. Through experiments on standard benchmarks like AdvBench and HarmBench, the paper demonstrates that E-RLHF significantly outperforms traditional RLHF in mitigating jailbreaking attempts.

### Strengths and Weaknesses
Strengths:  
- The paper offers a unique statistical framework that allows for a nuanced understanding of LLM input processing and unsafe output generation.  
- The introduction of E-RLHF as an alignment strategy that does not require extra computational resources is a significant strength.  
- Empirical tests using established benchmarks provide solid evidence that E-RLHF can significantly reduce the Attack Success Rate (ASR) compared to traditional RLHF.  
- The mathematical presentation is clear, with well-motivated formulations and comprehensive proofs supporting the claims.  

Weaknesses:  
- E-RLHF's effectiveness in complex conversational scenarios remains untested.  
- The connection between the theoretical framework and the experimental results appears tenuous, with some reviewers finding the proposed solutions to be simplistic or lacking depth.  
- There is a perceived disconnect between the theory and experiments, with some reviewers questioning how the theoretical insights translate into practical applications.

### Suggestions for Improvement
We recommend that the authors improve the connection between the theoretical framework and the experimental results to clarify how the theory informs the practical application of E-RLHF. Additionally, we suggest exploring the effectiveness of E-RLHF in more complex conversational scenarios to validate its robustness. It would also be beneficial to provide more detailed analyses of the performance of E-RLHF across different concepts and clarify the implications of the findings on MT-Bench performance. Lastly, we encourage the authors to consider more sophisticated methods for expanding the safety zone beyond the current approach.