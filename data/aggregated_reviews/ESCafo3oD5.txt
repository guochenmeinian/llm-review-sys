ID: ESCafo3oD5
Title: PrimDiffusion: Volumetric Primitives Diffusion for 3D Human Generation
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 4, 6, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents PrimDiffusion, the first diffusion model for generating 3D human models using a Mixture of Volumetric Primitives (MVP). The method employs a two-stage training process: the first stage learns volumetric primitives from multi-view images, while the second stage utilizes these primitives in a diffusion model to generate high-quality 3D representations. The authors emphasize a compact parameter space for efficient training and rendering, and they propose a scalable framework that employs cross-modal attention, enabling volumetric primitive learning without per-subject optimization on large datasets. The authors demonstrate the model's potential in training-free generative tasks, such as texture transfer and 3D inpainting, which surpass limitations of prior methods requiring retraining. They clarify that their method does not need explicit 3D supervision during training, relying instead on multiview 2D images and camera poses. The effectiveness of their approach is evaluated against existing methods on the RenderPeople dataset.

### Strengths and Weaknesses
Strengths:
- The paper tackles the significant challenge of generating clothed 3D humans and is well-structured and easy to follow.
- The model introduces a novel approach to 3D human generation using diffusion techniques and effectively utilizes multiview images to enhance 3D representation and view synthesis.
- The use of cross attention to fuse pose and texture information is effective and enhances performance.
- The framework allows for high-performance rendering without the need for a decoder, achieving superior results compared to existing techniques.

Weaknesses:
- The motivation for using volumetric primitive representation over other methods (e.g., point clouds) is unclear, and the authors need to provide fundamental reasons for its superiority beyond computational efficiency.
- Claims regarding the method's real-time capabilities and its comparison to existing methods are overstated; the generation time of the diffusion model is slow, and the paper lacks a fair comparison with baselines like gDNA.
- The dependency on multiview images for training raises concerns about cost and accessibility, and the sampling strategy for poses from the AMASS dataset may not yield a representative distribution, potentially affecting model effectiveness.
- The quality of generated 3D geometry and texture is low, with high-frequency details lacking, and the results appear blurry. The authors should clarify the reasons for these issues.
- The implementation lacks clarity in several areas, such as how reposing is achieved while preserving identity and the handling of texture outside SMPL body parts. Additionally, the GPU memory consumption unit was incorrectly stated in the initial submission.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation for using volumetric primitives by providing a thorough analysis of their advantages over other representations. Additionally, please ensure that claims regarding real-time performance are accurately represented and supported by empirical evidence. We suggest including a detailed comparison with gDNA and other relevant methods to substantiate claims of state-of-the-art performance. Furthermore, addressing the quality of generated results by enhancing high-frequency details and providing insights into the blurriness observed in outputs would strengthen the paper. We also recommend improving clarity regarding the necessity of multiview images, addressing the cost implications while emphasizing the advantages for 3D representation. Additionally, please provide further insights into the pose sampling strategy to ensure a representative distribution from the AMASS dataset. Lastly, please correct the GPU memory consumption unit in the revised version to 'MB' as noted, and consider incorporating multiview consistency losses in EVA3D to enhance performance further.