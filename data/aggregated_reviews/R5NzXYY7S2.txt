ID: R5NzXYY7S2
Title: Modeling Legal Reasoning: LM Annotation at the Edge of Human Agreement
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a significant contribution to the field of legal argument retrieval, specifically regarding statutory interpretation in US Supreme Court judgments. The authors propose a legal classification dataset and benchmark the performance of language models (LMs) on this dataset. The study reveals that generic LMs have limited capabilities in specialized tasks without task-specific fine-tuning. The paper also includes a comprehensive comparison of various models and a time-series analysis of court case opinion styles.

### Strengths and Weaknesses
Strengths:
- The paper introduces a high-quality dataset annotated by legal experts, which is valuable for testing LLMs.
- The experimental setup is valid, and the comparison across models is insightful, revealing the challenges in classification tasks.
- The quantitative analysis of shifts in opinion style in court cases is well-executed and interesting.

Weaknesses:
- The novelty of the paper is questioned, particularly regarding the framing and comprehensiveness of the benchmarking experiments.
- The classification task may be perceived as trivial, with existing models already achieving high accuracy on similar tasks.
- The authors do not include a wide range of state-of-the-art models, limiting the comprehensiveness of the benchmarking.

### Suggestions for Improvement
We recommend that the authors improve the conceptual framework by distinguishing between 'literal' and 'purposive' interpretation instead of 'grand' and 'formal' styles. This could enhance clarity and rigor in the discussion of legal interpretation. Additionally, we suggest including more recent state-of-the-art models in the experiments, such as seq2seq models (e.g., BART, T5) and larger models (e.g., LLaMA), to provide a more comprehensive benchmark. The authors should also consider shortening the historical context section and moving it to the appendix to focus on the dataset's strengths. Finally, we encourage the authors to conduct further exploratory studies with proper prompting techniques to better support their claims regarding model performance.