ID: s4xIeYimGQ
Title: Large Language Models are Better Reasoners with Self-Verification
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to enhance the reasoning abilities of large language models (LLMs) through self-verification, which includes forward reasoning and backward verification. The authors propose that LLMs can generate candidate answers using chain-of-thought (CoT) prompting and then verify these answers by masking original conditions and predicting outcomes. The effectiveness of this method is demonstrated through experiments on arithmetic, commonsense, and logical reasoning tasks.

### Strengths and Weaknesses
Strengths:
- The self-verification method is novel and shows potential to improve performance.
- The paper provides a clear process for the proposed method and includes comprehensive experiments across multiple datasets.
- The method can be integrated with existing techniques like self-consistency and PAL, leading to enhanced performance.

Weaknesses:
- The improvement over CoT baselines, particularly Self-Consistency Decoding, is minimal, averaging around 0.5%.
- The method is less effective on general reasoning tasks compared to mathematical reasoning.
- There is a lack of deep analysis regarding the conditions under which backward verification is effective.
- The iterative nature of the method results in high querying costs, and the models used in experiments are somewhat outdated.

### Suggestions for Improvement
We recommend that the authors improve the analysis of self-verification accuracy, particularly addressing the potential for LLMs to return incorrect answers in both forward and backward reasoning. Additionally, we suggest providing a separate analysis of the benefits derived from the backward verification process, as well as updating the experimental models to include newer versions like text-davinci-003 and gpt-3.5-turbo. Finally, we encourage the authors to conduct a deeper qualitative analysis to clarify when backward verification is most effective and how similar approaches can be generalized to other reasoning tasks.