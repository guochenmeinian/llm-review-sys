ID: xtK3gZjQDC
Title: Towards Human-AI Complementarity with Prediction Sets
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 5, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of decision support systems utilizing prediction set algorithms. The authors demonstrate that (i) conformal prediction techniques are generally sub-optimal for accuracy; (ii) finding optimal prediction sets with human assistance is NP-hard; and (iii) a greedy algorithm is proposed that guarantees improved prediction sets over those from conformal predictors. Experimental evaluations on synthetic and real datasets validate the effectiveness of the proposed approach.

### Strengths and Weaknesses
Strengths:
1. The paper's contribution is well-framed.
2. The theoretical analysis is sound.
3. The proposed algorithm shows improvement over existing methods.

Weaknesses:
1. The presentation is at times unclear, with dense sections and missing explanations (e.g., lines 135-146, lines 40/43/48).
2. The paper lacks results for the BRUTE FORCE baseline in Table 2 and Figure 3.
3. The theoretical analysis may not be practical, as it does not address the expected regret metric $\mathbb{E} g(S|x)$.
4. The proposed solution does not maintain the distributionally-free guarantee typical of conformal prediction.
5. The experimental analysis could benefit from comparisons with additional datasets and classical score functions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation, particularly in dense sections, and provide concrete examples to aid understanding. Additionally, please include the BRUTE FORCE SEARCH results in Table 2 and Figure 3. It would be beneficial to discuss the exploitation-exploration dilemma and include comparisons with alternative scores like RAPS and SAPS. We also suggest validating the algorithm on more realistic datasets such as ImageNet and CIFAR100, and reporting the empirical coverage of the greedy algorithm to ensure valid coverage guarantees. Finally, consider adjusting the selection of $\alpha$ based on a separate hold-out dataset to avoid potential biases in hyper-parameter tuning.