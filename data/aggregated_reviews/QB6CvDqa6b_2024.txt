ID: QB6CvDqa6b
Title: An Offline Adaptation Framework for Constrained Multi-Objective Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 6, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework for multi-objective reinforcement learning (MORL) that eliminates the reliance on handcrafted target preferences by using demonstrations to implicitly indicate expected policy preferences. The proposed offline adaptation framework, Preference Distribution Offline Adaptation (PDOA), can handle safety-critical objectives through safe demonstrations, even when safety thresholds are unknown. Empirical results demonstrate that the framework effectively infers policies that align with real preferences and meet constraints implied by the demonstrations.

### Strengths and Weaknesses
Strengths:
1. The framework's approach to bypass handcrafted preferences is innovative and potentially beneficial for real-world applications.
2. Extensive experimental evaluations across various benchmarks showcase the flexibility and effectiveness of the proposed method.

Weaknesses:
1. The relationship between preference and reward function definition raises concerns about preference estimation accuracy, particularly in cases of correlated reward functions.
2. The method assumes access to explicit preferences and demonstrations during training, which may not be practical in real-world settings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the relationship between preference definitions and reward functions, addressing potential correlation issues that could affect preference estimation. Additionally, the authors should consider discussing how the method could adapt to scenarios where explicit preferences are not available during training. A clearer distinction between past work and the contributions of this paper would enhance understanding, possibly supported by pseudocode for the algorithm. Furthermore, we suggest providing a broader discussion on the implications of the method in real-world applications, especially regarding safety and adaptation challenges.