ID: XMpzcC9L5z
Title: How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the predictability of Large Language Models (LLMs) performance prior to training, proposing a novel method based on BIG-bench experiment records. The authors demonstrate high predictability using an MLP-based predictor with an R² score exceeding 95%, indicating strong patterns in the data. They define the prediction problem with four inputs: model family, number of parameters, evaluated tasks, and number of in-context examples. The authors also introduce "small-bench," a condensed subset of BIG-bench tasks designed to maintain predictive power while reducing computational demands. 

### Strengths and Weaknesses
Strengths:  
- The paper addresses a significant problem in LLM evaluation, providing a method to predict performance without running experiments and selecting effective benchmark subsets.  
- The results show high R² scores, validating the authors' approach and indicating the presence of predictable patterns in the BIG-bench dataset.  
- The concept of "small-bench" could lower computational barriers for model evaluation.  
- The presentation is clear, with comprehensive experimental results supporting the claims.  

Weaknesses:  
- The paper only reports RMSE and R² scores, which do not fully capture model accuracy.  
- There is a limited number of model families in BIG-bench, raising concerns about the generalizability of results.  
- The discussion lacks depth regarding the diversity of models and tasks in BIG-bench.  
- The choice of explanatory variables in the regression model is insufficiently justified, and the impact of LLMs' emergent abilities on performance is not addressed.  
- The experiments are confined to one dataset, which may introduce biases and limit generalization.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the variety of models and tasks in BIG-bench to clarify their differences. Additionally, the authors should provide a more detailed explanation of the chosen explanatory variables in the regression model and consider the implications of LLMs' emergent abilities on their predictions. It would be beneficial to analyze the minimal data requirements for applying their proposed approach and to compare the performance of "small-bench" directly with BIG-bench. Finally, we suggest including additional performance metrics beyond RMSE and R² to provide a more comprehensive evaluation of model accuracy.