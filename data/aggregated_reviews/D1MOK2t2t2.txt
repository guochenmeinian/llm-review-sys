ID: D1MOK2t2t2
Title: BEDD: The MineRL BASALT Evaluation and Demonstrations Dataset for Training and Benchmarking Agents that Solve Fuzzy Tasks
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 8, 6, 10, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the BASALT Evaluation and Demonstrations Dataset (BEDD), which includes 26 million image-action pairs from nearly 14,000 videos of human players completing BASALT tasks in Minecraft. The dataset is designed to facilitate systematic assessment of algorithms that learn from human feedback, featuring over 3,000 dense pairwise evaluations of human and algorithmic agents. The authors propose a streamlined codebase for benchmarking new algorithms against a preliminary leaderboard and conduct a detailed analysis to guide algorithm development.

### Strengths and Weaknesses
Strengths:
- The dataset represents a remarkable data collection effort, comprising 14,000 videos with extensive annotations that assess human behavior.
- The topic is highly relevant, providing a benchmark for algorithms learning from human feedback in complex open-world environments.
- The authors offer a streamlined codebase for easy evaluation and comparison of new models against existing benchmarks.
- The open-sourcing of the dataset and workflows enhances accessibility for the research community.

Weaknesses:
- The paper lacks annotated examples of failure cases, which could provide valuable counterfactual information for users.
- There is insufficient clarity regarding the open availability of the dataset and benchmarking procedures, as references to open-source repositories are missing.
- The relationship to MineDojo is not discussed in sufficient depth, leaving the main novelties of the proposed work unclear.

### Suggestions for Improvement
We recommend that the authors improve the dataset by including annotated examples of failure cases to enrich the dataset's utility. Additionally, we suggest providing more fine-grained annotations of successful episodes, including critical success keypoints. Clarifying the open availability of the dataset and benchmarking procedures, along with adding references to open-source repositories, would enhance transparency. Finally, we encourage the authors to elaborate on the relationship to MineDojo, highlighting the unique contributions of their work compared to existing benchmarks.