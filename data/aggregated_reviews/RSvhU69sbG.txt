ID: RSvhU69sbG
Title: MathPile: A Billion-Token-Scale Pretraining Corpus for Math
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 9, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a large-scale, high-quality, and diverse dataset for mathematical reasoning, named MathPile, constructed from various math sources such as textbooks, Stack Exchange, Wikipedia, Common Crawl, and ProofWiki. The dataset contains approximately 9.5 billion tokens and is developed through a comprehensive multi-stage data processing pipeline that includes file parsing, language identification, cleaning, filtering, deduplication, and contamination detection. The authors assess data quality through detailed statistics and model pretraining experiments, demonstrating the dataset's significance for enhancing educational tools and large language models (LLMs).

### Strengths and Weaknesses
Strengths:
- The dataset is large-scale and collected from diverse math sources, ensuring high quality.
- A comprehensive data processing pipeline is implemented, enhancing reliability.
- The inclusion of a robust data contamination detection mechanism ensures dataset integrity.
- The framework is well-structured, with clear delineation of stages and processes.
- The use of specific technical methods, such as regex for text cleaning and MinHash LSH for deduplication, demonstrates a robust approach to data quality.
- The authors provide detailed logging mechanisms, enhancing transparency and traceability of the data processing steps.
- The open-source nature of MathPile provides a valuable resource for the research community.
- The paper is well-structured and clearly presented.

Weaknesses:
- Limited experiments are conducted, using only one base model (Mistral) for pre-training, which raises questions about the dataset's effectiveness and generalizability.
- There is a lack of strong baselines and limited performance improvements as more data is added.
- The paper does not include performance comparisons with other math pre-training corpora.
- The quality definition is primarily based on heuristic rules without a formal framework.
- Some filtering rules, while comprehensive, may benefit from further empirical validation to ensure their effectiveness across diverse datasets.

### Suggestions for Improvement
We recommend that the authors improve the paper by including concrete dataset examples from different sources at various levels in Section 4.1. Additionally, conducting experiments with a broader range of base models, such as those from the LLama or Qwen families, would better demonstrate the corpus's effectiveness. The authors should also clarify the limitations of the dataset, particularly regarding data mixing and its impact on performance. Providing a detailed explanation of the data cleaning scripts, including their input, output, and specific decisions made, would enhance reproducibility. Furthermore, including performance comparisons with other pre-training corpora and addressing performance on non-math benchmarks would strengthen the paper's contributions. Lastly, consider providing empirical evidence or case studies to support the efficacy of the filtering rules implemented, enhancing the robustness of the claims made regarding data quality and suitability for training language models.