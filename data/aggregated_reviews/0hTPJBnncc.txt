ID: 0hTPJBnncc
Title: MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark, MQuAKE, for evaluating knowledge editing in large language models (LLMs) without altering their weights. The authors propose an external memory-based method, MeLLo, to facilitate knowledge editing, focusing on multi-hop questions that depend on edited information. The study introduces the concept of "chains-of-facts" to assess the impact of knowledge edits on the entire chain of reasoning. Two datasets, mquake-t and mquake-cf, are constructed for GPT-J to explore whether the model genuinely learns new knowledge or merely hard-codes it.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated, clearly written, and presents a novel benchmark and method that are relevant to the community.
- The dataset construction and experimental design are well-detailed, supporting future research in knowledge editing.
- The findings indicate that existing methods struggle to utilize updated facts for multi-hop questions, highlighting a significant gap in current knowledge editing techniques.

Weaknesses:
- The absence of prompt templates for the main experiments limits reproducibility.
- The dataset's applicability is restricted as it primarily focuses on GPT-J, potentially overlooking the variability in world knowledge across different models.
- The evaluation metrics rely heavily on accuracy, which may not adequately capture the nuances of knowledge editing, especially with aliasing issues.

### Suggestions for Improvement
We recommend that the authors improve the dataset's applicability by including a broader range of language models beyond GPT-J. Additionally, consider employing a probability-based evaluation approach, such as CounterFact (ROME), to better assess the accuracy of knowledge edits. It is crucial to investigate the robustness of MeLLo against retriever errors, particularly how irrelevant knowledge retrieval might affect the model's output. Finally, we suggest providing prompt templates for the main experiments to enhance reproducibility and clarity in the methodology.