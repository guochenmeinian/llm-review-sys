ID: x69O84Df2G
Title: Multi-Reward Best Policy Identification
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 4, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach to identifying optimal policies in reinforcement learning (RL) with multiple rewards, proposing an instance-dependent lower bound and a provably-correct algorithm for convex approximation in tabular settings. The authors also discuss extensions to deep RL, supported by numerical results. The study aims to optimize performance across a set of rewards, addressing the complexities of multi-reward best policy identification.

### Strengths and Weaknesses
Strengths:
- The paper effectively identifies optimal policies across multiple rewards, addressing a significant practical concern.
- It establishes an instance-dependent lower bound for any Probably-Correct algorithm in the multi-reward context.
- The theoretical and empirical analyses are comprehensive, with supplementary evidence in the appendix that strengthens the arguments.

Weaknesses:
- The environments used for deep RL are overly simplistic, limiting the demonstration of the algorithm's performance in complex scenarios.
- The definition of policy optimality is restrictive, only considering stationary deterministic policies, raising questions about the applicability of the proposed lower bound.
- The lack of theoretical guarantees for the deep RL extension and insufficient empirical studies weaken the claims made.
- The relationship between the MR-BPI problem and reward-free RL remains unclear, necessitating a more rigorous comparison.
- The paper lacks clarity in certain areas, including the computational costs of convex optimization methods and the innovative aspects that are relegated to the appendix.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing more detailed explanations of the background and main ideas, particularly in the main text rather than the appendix. Additionally, it would be beneficial to include experiments in more challenging environments to better assess scalability and practical applicability. The authors should also clarify the implications of their assumptions regarding policy optimality and discuss the computational costs associated with their methods. Furthermore, a more thorough comparison with reward-free RL and multi-objective RL is necessary to articulate the differences and strengths of their approach.