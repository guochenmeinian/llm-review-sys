ID: qq6ctdUwCX
Title: Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to mitigate length bias in reinforcement learning from human feedback (RLHF) by employing the Product-of-Experts (PoE) technique. The authors identify that the reward model often assumes humans prefer longer responses, leading to misleading outputs. Their approach separates the reward model from sequence length influences and is validated through experimental results that show improved language model performance.

### Strengths and Weaknesses
Strengths:
- The paper addresses the significant issue of length bias in response generation.
- The innovative use of the PoE technique offers a practical solution to mitigate this bias.
- Comprehensive experiments demonstrate the effectiveness of the proposed method.

Weaknesses:
- The paper primarily combines existing concepts of products-of-experts and reward modeling, lacking sufficient depth for a longer paper.
- The improvement in reducing output length is marginal (1% to 2%), necessitating more examples for validation.
- Clarity issues in Figure 2 hinder understanding of the results, and the evidence for length bias requires further analysis.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Figure 2, possibly by converting it to a distribution plot to better illustrate the trends. Additionally, further experimentation is needed to provide stronger evidence of the proposed method's effectiveness in real-world scenarios. The authors should also explore the impact of the bias-only expert model's size on results and provide more detailed analyses to support their claims. Finally, including examples in the appendix that highlight the advantages of their method over existing approaches would enhance the paper's contributions.