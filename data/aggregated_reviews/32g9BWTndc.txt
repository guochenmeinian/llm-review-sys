ID: 32g9BWTndc
Title: LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 4, 5, 8, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TEA-GLM, a framework that aligns GNN representations with LLM token embeddings for zero-shot graph machine learning. TEA-GLM facilitates cross-dataset and cross-task learning without requiring LLM fine-tuning. The authors employ contrastive learning to pretrain a GNN, which is then mapped to graph tokens for LLM downstream tasks. The experimental results indicate state-of-the-art performance on various unseen tasks and datasets.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and effectively communicates the design and motivation behind the proposed framework.
- The use of a linear projector for LLM integration is efficient.
- The PCA-based alignment with LLM input is intriguing and may inspire future research.
- The model demonstrates cross-task transferability, achieving notable performance on link prediction despite being pretrained solely on node classification.

Weaknesses:
- The experimental results raise concerns, particularly regarding the performance of the vanilla Vicuna-7B model, which outperformed LLaGA, and the lack of significant differences between Vicuna-7B and TEA-GLM on certain datasets.
- The novelty of the approach is questioned, as several components are already established in the field, and the feature-wise contrastive learning contribution is not thoroughly examined.
- The mapping of a single GNN node representation to multiple graph tokens lacks justification, and the pooling mechanism may lead to non-decodable embeddings.

### Suggestions for Improvement
We recommend that the authors improve the clarity and depth of the experimental evaluation, particularly by conducting additional experiments that align more closely with LLaGA's settings. It would be beneficial to explore the mechanism behind feature-wise contrastive learning in greater detail. To substantiate claims regarding the inductive knowledge transfer from the pretraining process, we suggest demonstrating the model's performance without node text inputs or comparing it against baseline models that fine-tune the LLM on node texts. Furthermore, we encourage the authors to provide a more thorough discussion on the pooling methods used, particularly addressing the implications of sum versus max pooling in relation to the model's performance.