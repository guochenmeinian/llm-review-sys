ID: SGKbHXoLCI
Title: Collaborative Learning via Prediction Consensus
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 5, 6, 7, 5, -1, -1
Original Confidences: 3, 4, 2, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents a decentralized learning algorithm inspired by social science to facilitate information exchange among agents and improve prediction accuracy on a shared target domain. Each agent shares its prediction, which is weighted according to trust scores, and these proxy labels are used to enhance local model training via distillation. The authors demonstrate that their trust measure based on cosine similarity achieves an ideal trust matrix, facilitating effective consensus. Empirical results indicate that the proposed method outperforms classical baselines under heterogeneous conditions.

### Strengths and Weaknesses
Strengths:
- The paper addresses a collaborative learning setting where agents maintain data privacy while improving predictive performance, proposing a decentralized algorithm that is particularly useful when data and model sharing are restricted.
- The incorporation of a trust mechanism inspired by social science is innovative, and the theoretical analysis appears sound, supported by sufficient experimental evidence demonstrating the method's effectiveness.
- The approach significantly enhances individual model performance and mitigates the negative impact of poor models on collective outcomes.

Weaknesses:
- The potential for information leakage through prediction exchange raises concerns; the authors should explore more private data-sharing methods.
- The necessity of collective prediction could be better motivated, and the possibility of ensemble algorithms like bagging being more effective should be considered.
- The claim regarding minimal communication needs further clarification, and the explanation of co-training lacks convincing detail, particularly regarding the treatment of heterogeneous data.
- A comprehensive comparison with recent state-of-the-art collaborative learning methods is missing, limiting the evaluation of the proposed method's relative performance.

### Suggestions for Improvement
We recommend that the authors improve the discussion on privacy concerns related to prediction exchange and explore alternative methods for data sharing. Additionally, the authors should provide a more robust motivation for the necessity of collective prediction and consider the effectiveness of ensemble methods like bagging. Clarifying the claim about minimal communication and providing a more detailed explanation of co-training would strengthen the paper. Furthermore, including a comprehensive comparison with other state-of-the-art methods and a detailed analysis of the computational complexity and scalability of the proposed method would enhance its practical applicability.