ID: OzcPJz7rgg
Title: STARSS23: An Audio-Visual Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new audio-visual dataset called Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23) aimed at advancing research in audio-visual sound event localization and detection (SELD). The authors propose a SELD task that leverages multichannel audio and video to estimate the temporal activation and direction of arrival (DOA) of target sound events. The dataset includes 13 annotated sound event classes, loosely aligned with the Audioset ontology, and demonstrates that the audio-visual system outperforms audio-only systems in localization tasks. The authors also analyze the importance of visual information for improving performance in specific classes, particularly human body-related sounds, and acknowledge the limitations of the STARSS23 dataset, noting its scarcity for training purposes. They outline plans to enhance training data through multichannel audio-visual simulation and other data sources while addressing issues of dataset imbalance.

### Strengths and Weaknesses
Strengths:  
- The dataset construction pipeline is clear, feasible, and reproducible.  
- The integration of visual data into the SELD task is intuitive and beneficial for temporal activation and DOA estimation.  
- The paper is well-written and easy to follow, with sufficient documentation on data collection and organization.  
- The paper provides a clear demonstration of the necessity of visual information in enhancing SELD performance for certain classes.  
- The authors have a well-defined plan for addressing data scarcity and imbalance, utilizing various data sources and techniques.  
- The inclusion of competitive benchmarks adds value to the research.

Weaknesses:  
- The research motivation is not clearly articulated, and the coverage of related works is insufficient.  
- The novelty of the dataset is questioned, as it appears incremental over the previous STARSS22 dataset, primarily adding visual information without significant methodological changes.  
- The reliance on a limited set of visual classes, particularly the person class, may restrict the generalizability of the findings.  
- The experimental results indicate that visual information may not always enhance detection performance, raising concerns about its necessity.  
- The dataset is imbalanced, with a predominance of certain sound event classes, which may limit the model's learning for rare categories.  
- The paper lacks detailed metrics definitions and mathematical formulations, which could hinder replication efforts.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the research motivation and provide a more comprehensive discussion of related works and potential tasks, such as audio-visual cross-modal retrieval and classification. Additionally, we suggest conducting further experiments to validate the necessity of visual information in SELD tasks and addressing the dataset's imbalance and long-tail issues. The authors should also provide definitions and formulas for the metrics used in their evaluations and enhance the benchmark section with clearer illustrations of the baseline methods. Furthermore, we suggest expanding the discussion on the limitations of using only the person class for visual detection and exploring potential enhancements in visual feature development and model structure to improve performance across other classes. Finally, careful proofreading is advised to correct grammatical errors throughout the paper.