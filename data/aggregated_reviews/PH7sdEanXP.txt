ID: PH7sdEanXP
Title: Scaling Laws in Linear Regression: Compute, Parameters, and Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 8, 5, 6, 5, 8, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the neural scaling laws in the context of linear regression using one-pass SGD, addressing the contradiction between statistical theory, which suggests that variance error increases with model size, and empirical observations indicating improved predictive performance with larger models. The authors derive scaling laws while accounting for a variance term that grows with model complexity, which is shown to be of higher order than typical approximation and bias terms. The work includes numerical simulations to support the theoretical findings.

### Strengths and Weaknesses
Strengths:
- The authors reconcile empirical scaling laws with classical statistical learning theory, providing a novel perspective that is relevant to both the learning theory and LLM communities.
- A general framework is introduced in Section 6 to handle the general spectrum case of the data covariance matrix, showcasing technical innovation in controlling the effect of data sketch.
- The paper is well-written and easy to understand.

Weaknesses:
- The assumptions, particularly regarding linear regression and Gaussian design, may be overly restrictive; extending the analysis to kernel settings with relaxed feature assumptions could enhance the work.
- The focus on one-pass SGD, which is less common in LLM training, raises questions about the applicability of results to algorithms like Adam.
- The findings may lack novelty, as they primarily reframe existing bounds without introducing significant new technical contributions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the variance term's significance when first introduced, as its higher-order nature is only explicitly stated later in Theorem 4.1. Additionally, validating the theoretical results on a large benchmark dataset, even with modified assumptions, would strengthen the paper. We suggest extending the analysis to kernel settings and exploring the implications of different step-size schedules, such as constant or polynomial decaying, on the regularization effects of SGD. Lastly, a more explicit discussion of the limitations and broader context of the work, including comparisons with existing literature on generalization error rates, would enhance the manuscript's depth.