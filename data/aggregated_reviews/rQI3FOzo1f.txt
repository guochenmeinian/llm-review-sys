ID: rQI3FOzo1f
Title: Efficient Learning of Linear Graph Neural Networks via Node Subsampling
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 5, 8, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to efficiently scale graph neural networks (GNNs) for regression tasks on large graphs through node subsampling and leverage score sampling. The authors demonstrate that their technique provides a good spectral approximation to the full computation of \(AX\), avoiding the \(O(n^2d)\) matrix multiplication cost. The method is based on a two-layer linear GNN, where the mean squared error (MSE) of the reduced problem is bounded by the true MSE times \((1+\epsilon)\).

### Strengths and Weaknesses
Strengths:
- The proposed technique addresses the practical issue of scaling GNNs, offering useful theoretical guarantees and originality in its approach.
- The clarity of the algorithm's explanation and the numerical experiments conducted on large datasets validate the proposed method's effectiveness.

Weaknesses:
- The focus on linear GNNs limits the applicability of the work; there is a lack of discussion or results for non-linear models.
- The paper does not provide practical wall-clock runtimes for the proposed method, which is essential given the computational context.
- The complexity analysis is based on theoretical assumptions that may not hold in practice, particularly regarding sparse matrix multiplication.

### Suggestions for Improvement
We recommend that the authors improve the paper by including a summary table that compares the computational and memory complexities of the various techniques used in the experiments. Additionally, it would be beneficial to provide experimental results on the runtimes of the proposed method compared to sparse matrix multiplication. We suggest that the authors explore the extension of their method to non-linear GNNs and multi-layer models, as well as include evaluations on tasks beyond regression, such as link prediction and node classification. Finally, addressing the identified typos and formatting issues will enhance the overall presentation of the manuscript.