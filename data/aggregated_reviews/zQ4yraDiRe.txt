ID: zQ4yraDiRe
Title: Multi-scale Diffusion Denoised Smoothing
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 6, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 1, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new technique for certified adversarial robustness based on randomized smoothing, proposing two certification schemes: cascaded randomized smoothing, which aggregates multiple smoothed classifiers with varying smoothing factors, and diffusion calibration, a fine-tuning method to mitigate overconfidence in the original randomized smoothing approach. Experimental results indicate that the proposed method surpasses the previous baseline of Diffusion Denoised Smoothing.

### Strengths and Weaknesses
Strengths:
1. Clarity: The paper is well-written, with a clear description of the method and concise presentation of results.
2. Originality: The novel cascading scheme for smoothed classifiers and the introduction of two losses for calibrating the diffusion model are promising contributions.
3. Significance: The proposed method addresses practical limitations of randomized smoothing, potentially enhancing its applicability.

Weaknesses:
1. The paper lacks discussion on the computational requirements of the proposed method, particularly regarding the cascading smoothed classifier and fine-tuning processes.
2. The definition of the prediction from the cascaded smoothed classifier differs from the original randomized smoothing, necessitating a clearer explanation of the certification bounds.
3. The limitations of randomized smoothing discussed in Section 3.1 may not be novel and could be better positioned in the background section.
4. Empirical evidence supporting the claims about overconfidence and the effectiveness of the proposed losses is insufficient.
5. The title "multi-scale diffusion denoised smoothing" does not accurately reflect the contributions of the work.

### Suggestions for Improvement
We recommend that the authors improve the discussion on computational overhead associated with the proposed methods. Additionally, we suggest adding a subsection or paragraph that clearly defines the certification bounds for the cascaded smoothed classifier. It would also be beneficial to provide empirical evidence supporting the claims regarding overconfidence and to include evaluations confirming the effectiveness of the fine-tuning process. Finally, we advise revising the title to better represent the contributions of both cascaded randomized smoothing and diffusion calibration.