ID: Ror9xJhbdc
Title: Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial Applications
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper discusses the evaluation methods and application scenarios of Instruction Fine-tuning models (IFT models) in industrial contexts. The authors propose two new evaluation metrics, Comparability Across Task (CAT) and Task and Format Agnostism (TFA), and validate these metrics using LLMs as scoring agents. The findings indicate that LLMs can serve as effective alternatives for evaluating IFT models. Additionally, the paper presents a method for enhancing model learning efficiency through synthetic data in low-data environments.

### Strengths and Weaknesses
Strengths:
- The introduction of two new evaluation metrics and a comprehensive evaluation method for IFT models, supported by extensive experiments.
- Strong evaluation results with a detailed experimental setup that allows for reproducibility.
- Practical insights on using synthetic data to improve IFT model performance in low-data scenarios.

Weaknesses:
- Some experimental results lack sufficient explanation and justification, particularly in section C.2.
- The paper does not adequately connect the novel measures to the scenarios outlined in the introduction (S_0, S_1, and S_2).
- The rationale for repurposing an Instruction-following LLM for specific tasks is insufficiently explained.

### Suggestions for Improvement
We recommend that the authors improve the clarity and justification of the experimental results, particularly in section C.2. Additionally, we suggest providing more detailed explanations on why conventional metrics are inferior to GPT4, especially in relation to the proposed requirements (CIT, CAT, TFA). It would also be beneficial to explicitly link the novel measures to the scenarios described in the introduction and to provide concrete examples supporting the need for task-specific repurposing of Instruction-following LLMs.