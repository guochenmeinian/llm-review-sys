ID: WGoCZl2itU
Title: ClashEval: Quantifying the tug-of-war between an LLMâ€™s internal prior and external evidence
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 7, 6, 5, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ClashEval, a benchmark dataset designed to evaluate the ability of language models (LLMs) to discern correct from incorrect information in a retrieval-augmented generation (RAG) context. The dataset includes over 1200 questions across six domains, with content containing deliberate errors. The authors explore how LLMs respond to both correct and erroneous external content, revealing that they adopt incorrect information over 60% of the time, particularly when their initial confidence is low.

### Strengths and Weaknesses
Strengths:
- The dataset is well-designed, providing a detailed examination of LLMs' handling of conflicting information, which is a relatively uncommon resource.
- Extensive evaluation of six leading LLMs offers deep insights into their strengths and weaknesses.
- The research quantitatively measures how deviations in contextual accuracy affect model choices, contributing significantly to the field.

Weaknesses:
- The paper lacks a comparative analysis of the performance differences among the six models, which would enhance understanding of their varying behaviors.
- The rationale for selecting these specific models for comparison is not provided.
- The dataset's diversity in domains and perturbation methods appears limited, potentially affecting the generalizability of the findings.
- The generation process of the dataset and experimental setups require further elaboration.

### Suggestions for Improvement
We recommend that the authors improve the documentation of the dataset generation process and provide clearer guidelines for creating new variations. Additionally, it would be beneficial to report per-category accuracy scores and mean prior probabilities to better interpret the results. Exploring how models handle multiple conflicting sources of information would also enrich the analysis. Finally, addressing the potential for fine-tuning models to enhance accuracy could provide valuable insights into improving LLM performance in RAG contexts.