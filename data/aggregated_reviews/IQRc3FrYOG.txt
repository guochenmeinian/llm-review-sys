ID: IQRc3FrYOG
Title: Mutual-Information Regularized Multi-Agent Policy Iteration
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new method, LIMI, which employs mutual information as a regularizer to enhance multi-agent reinforcement learning (MARL) algorithms by reducing reliance on team-related information and improving generalization across varying team compositions. The authors provide both theoretical proofs and practical implementation details, demonstrating the method's effectiveness through experiments in multiple environments, particularly showing superior zero-shot generalization compared to existing methods.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with detailed theoretical proofs and practical insights.
- Empirical results indicate better zero-shot generalization on SMAC tasks compared to other methods.

Weaknesses:
- The paper lacks results from a broader range of environments, as SMAC is sensitive to hyper-parameters, which undermines the persuasiveness of the claims.
- Important baselines, such as MAPPO and HATRPO, are omitted from the analysis.
- The definition of "team composition" is unclear, and the literature review lacks references to relevant works like InfoPG and MOA, which diminishes the context of the proposed method.
- The approach of minimizing mutual information appears counterintuitive for cooperative MARL, potentially leading to reduced collaboration among agents.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the term "team composition" by providing a formal definition early in the introduction. Additionally, including a real-world example of a multi-agent scenario would enhance the motivation behind the solution. The authors should also expand the literature review to include relevant works such as InfoPG and MOA, discussing how LIMI differs from these approaches. To address concerns about cooperation, we suggest evaluating the algorithm on tasks that require high collaboration, such as those provided by COPA. Finally, the authors should include results from more complex environments and clarify the role of the hyperparameter alpha in their experiments.