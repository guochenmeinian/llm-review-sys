ID: XlpipUGygX
Title: Amortized Planning with Large-Scale Transformers: A Case Study on Chess
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 6, 6, 5, -1, -1, -1, -1
Original Confidences: 3, 3, 5, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an open dataset of real chess positions annotated with evaluation scores, best moves, and scores for each legal move according to StockFish 16, using a 50ms evaluation time. The authors train large transformers on this dataset and provide extensive analysis, including model performance on Lichess and hyperparameter ablation studies. The contributions include a large-scale dataset and a transformer model that outperforms baselines, demonstrating the potential of large models in complex planning tasks.

### Strengths and Weaknesses
Strengths:
- The paper offers a comprehensive analysis of the trained Transformer on ChessBench, establishing a strong foundation for future research.
- Promising results indicate that the Transformer architecture can effectively replicate StockFish's evaluations.
- The extensive dataset introduced could be beneficial for future research endeavors.

Weaknesses:
- The dataset may be of poor quality due to the 50ms evaluation limit, potentially leading to inaccuracies that could affect model performance.
- The paper's primary goal is unclear; while it suggests a focus on Transformers' capabilities in planning, it primarily provides a dataset and an initial case study.
- The methodological contribution is limited, as the techniques employed are incremental and previously established.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper's objectives, explicitly stating the primary contributions in the introduction. Additionally, consider addressing the dataset's quality concerns by evaluating the impact of the 50ms limit on model performance. We suggest conducting ablation studies on training objectives to enhance the methodological rigor. Furthermore, clarify the importance of the transformer structure versus the dataset in performance improvements, and ensure that the semantic interconnectedness of training targets is adequately modeled.