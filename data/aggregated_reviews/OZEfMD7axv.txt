ID: OZEfMD7axv
Title: SyncDiffusion: Coherent Montage via Synchronized Joint Diffusions
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 6, 7, 7, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SyncDiffusion, a module designed to synchronize multiple outputs from diffusion models to create coherent montages. The authors propose guiding the reverse diffusion process by shifting \(x_t\) with gradients of a perceptual loss between target and anchor windows. Experimental results indicate that SyncDiffusion enhances coherence in montages compared to prior methods.

### Strengths and Weaknesses
Strengths:
- The study addresses the critical issue of synchronizing multiple estimates from diffusion models, which is essential for applications like panorama image generation.
- The method's simplicity, using gradients of a perceptual loss to ensure global coherence, allows for compatibility with various output combination methods.
- The paper is well-written, with clear theoretical and algorithmic explanations, and presents comprehensive qualitative and quantitative results demonstrating its effectiveness.

Weaknesses:
- The algorithm's reliance on the homogeneity of the target scene may limit its applicability, as indicated by the use of LPIPS for guidance, which could lead to failure cases.
- The evaluation metrics, while interesting, do not clearly demonstrate the superiority of the proposed method, especially since Mean-CLIP-S and Mean-GIQA perform comparably, and FID results are subpar.
- The effectiveness of SyncDiffusion remains ambiguous, as user studies focus solely on coherence rather than image quality or prompt fidelity.
- The claim that SyncDiffusion is a plug-and-play module is undermined by its evaluation being limited to MultiDiffusion.

### Suggestions for Improvement
We recommend that the authors improve the algorithm's applicability by exploring guidance methods beyond LPIPS to mitigate reliance on scene homogeneity. Additionally, we suggest providing a detailed explanation of how stable diffusion can generate wider images and clarifying the text-to-panorama generation task. It would also be beneficial to discuss the impact of gradient descent weight settings on qualitative results and consider experimenting with alternative weights. Lastly, addressing the computational overhead associated with the proposed method in comparison to baselines would enhance the paper's rigor.