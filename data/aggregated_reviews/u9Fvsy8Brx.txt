ID: u9Fvsy8Brx
Title: mmT5: Modular Multilingual Pre-Training Solves Source Language Hallucinations
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents mmT5, which enhances multilingual language pre-training by incorporating language-specific components and freezing the decoder during cross-lingual transfer fine-tuning. Comprehensive experiments demonstrate that this method significantly outperforms standard training in classification tasks and addresses the issue of incorrect language generation in smaller models. The authors conducted extensive experiments across various tasks, including Question Answering and generation, providing valuable insights for adapting language models to languages with limited task-specific training data.

### Strengths and Weaknesses
Strengths:
- The method is simple and elegant, yielding superior results compared to the baseline with a minimal increase in parameters.
- The paper is well-written, with clear notations and self-contained figures and tables.
- It effectively addresses two critical limitations of current seq-to-seq models: the curse of multilinguality and source language hallucination.

Weaknesses:
- The paper offers limited novelty, primarily extending existing methods without substantial new contributions.
- The fixed number of language-specific modules (100) raises concerns about generalization to new languages.
- The experimental comparisons lack robustness, with insufficient encoder-decoder models and weak performance against models like XLM-R.

### Suggestions for Improvement
We recommend that the authors improve the discussion in section 6.1 by providing insights on why freezing the FFN component of the decoder yields the largest gains. Additionally, the authors should clarify the relationship between their method and existing models, particularly "Multilingual unsupervised neural machine translation with denoising adapters," and include comparisons with "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer." It would also be beneficial to address how the model can generalize to novel languages and provide results on other popular tasks, such as machine translation. Finally, we suggest revising Figure 4 to a table or bar chart for better clarity.