ID: mJZH9w8qgu
Title: In-Trajectory Inverse Reinforcement Learning: Learn Incrementally Before an Ongoing Trajectory Terminates
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 4, 8, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework for Inverse Reinforcement Learning (IRL) from ongoing trajectories, allowing for the learning of a reward function that induces an optimal policy based on expert demonstrations without waiting for complete trajectories. The authors propose an online learning algorithm featuring meta-regularization, supported by both theoretical and empirical analyses to validate their results. The paper introduces a new problem setting termed in-trajectory IRL, where a reward function and corresponding policy are learned incrementally.

### Strengths and Weaknesses
Strengths:  
- The framework for learning from ongoing trajectories is innovative and applicable to real-world scenarios.  
- The theoretical analysis is robust, ensuring convergence and addressing non i.i.d. input data.  
- The algorithm is practical, easy to reproduce, and demonstrates good performance in experiments, including a real-world stock market example.  

Weaknesses:  
- The paper lacks a clear discussion on the nature of the extracted reward function and its applications, particularly regarding identifiability issues.  
- The presentation of the content is inadequate, making it difficult to grasp the contributions fully.  
- Assumption 1 regarding the smoothness of the parameterized reward may be overly restrictive, especially for neural networks with non-smooth activations.  
- The rationale behind different initial points for algorithms in experiments is unclear, raising questions about the training process.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the discussion surrounding the extracted reward function, specifically addressing its potential applications and the identifiability concerns. Additionally, we suggest enhancing the overall presentation to better convey the contributions of the work. Clarifying the experimental setup, particularly regarding the initial points for different algorithms and the completion of trajectories in MuJoCo, would also strengthen the paper. Lastly, we encourage the authors to reconsider the implications of Assumption 1 and provide more context on how data for meta-regularization is collected.