ID: viktK3nO5b
Title: SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue Agents
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 9, 9, 5, 7, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
The paper introduces SpokenWOZ, a large-scale spoken task-oriented dialogue (TOD) dataset comprising over 5.7k dialogues and 249 hours of audio. Key contributions include the dataset's extensive size, the identification of new challenges in spoken dialogue such as cross-turn and reasoning slot detection, and comprehensive experiments on various models, including LLMs. The authors received 1,520 applications, but only 250 participants passed the qualification test, which assesses task completion and dialogue audio quality. Crowd-sourcing is utilized to evaluate audio quality, removing any audio with poor communication defined by inaudible utterances. The authors do not collect personal information, adhering to regional laws, and believe gender and age do not correlate with audio quality. In Section 6.1, the authors compare various models, including extractive and generative models, as well as text-modal and dual-modal models. Detailed statistics on audio-related features from the SpokenWOZ dataset are provided, highlighting variations in pitch, intensity, speech rate, and duration across different dialogue acts.

### Strengths and Weaknesses
**Strengths:**
1. The dataset is a significant resource for advancing spoken TOD research.
2. The rigorous methodology for dataset creation and quality control is commendable.
3. The qualification process for participants ensures high-quality dialogue audio.
4. The use of crowd-sourcing for audio evaluation enhances the reliability of the data.
5. Comprehensive comparisons of model types and sizes demonstrate thoroughness in the research approach.
6. Detailed statistical analysis of audio features provides valuable insights into dialogue act characteristics.

**Weaknesses:**
1. Clarity issues exist regarding the qualification test and quality control criteria.
2. The dataset lacks detailed demographic information about speakers.
3. The rationale for model selection in experiments is insufficiently explained.
4. The authors' assumption that gender and age do not impact audio quality may overlook potential influencing factors.
5. The current focus on text data statistics may limit the depth of audio analysis.

### Suggestions for Improvement
1. The authors should clarify the qualification test process in Section 3.1, specifically the criteria for filtering participants.
2. The explanation of quality control measures should be expanded to detail how "poor communication quality" is assessed.
3. The authors are encouraged to include demographic attributes such as gender and age of speakers in the dataset.
4. In Section 6.1, the authors should provide a rationale for model selection for comparison.
5. The authors might consider exploring the impact of different ASR systems on the dataset and including more varied conversation templates to enhance robustness.
6. The evaluation metrics for LLMs should be revisited to ensure they align with realistic generation rather than de-lexicalized responses.
7. The authors should consider incorporating an analysis of how gender and age might affect audio quality, as this could provide a more nuanced understanding of the data.
8. The authors are encouraged to expand the statistical information on audio data in the Appendix to enhance the depth of their analysis.