ID: lbFQ2QJHNY
Title: Boosting Graph Convolution with Disparity-induced Structural Refinement
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework, DSR-GNN, aimed at enhancing graph neural networks (GNNs) for both homophilous and heterophilous graphs. The core technical contribution is the Disparity-Induced Structural Refinement (DSR), which adaptively refines graph structures during message passing using a disparity score that combines feature similarity and structural characteristics. Theoretical analysis supports the method, and empirical evaluations show superior performance compared to state-of-the-art approaches on benchmark datasets.

### Strengths and Weaknesses
Strengths:
- The proposed DSR method effectively addresses limitations of existing GNNs in handling heterophilous graphs, supported by a solid theoretical foundation.
- The authors derive error bounds for node classification, grounding their design decisions in rigorous theoretical insights.
- The evaluation includes diverse datasets, demonstrating the method's robustness and generality, with detailed ablation studies isolating the impact of each component.
- The paper is well-structured, with clear problem statements, methodology explanations, and result discussions.

Weaknesses:
- The literature review lacks coverage of important related works in Homo/Heterophile graph learning, particularly regarding topology optimization and layer-wise propagation.
- Significance tests are needed to further validate the improvements shown by the proposed DSR method.
- The interpretability and convergence properties of layer-wise weights $\lambda$ are not sufficiently analyzed.
- There is no analysis of the computational overhead introduced by the disparity score computation and graph refinement, especially for large-scale graphs.

### Suggestions for Improvement
We recommend that the authors improve the literature review by analyzing more related works and distinguishing their differences. Additionally, providing significance tests on results would better demonstrate the improvements. An efficiency analysis should be included, along with discussions on adapting the method to large-scale graphs. Furthermore, we suggest offering theoretical interpretability of Layer-wise Weight Learning and clarifying differences against GReTo.