ID: DSmHC8bi3j
Title: Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for fine-tuning pre-trained language models (PLMs) using noisy labels, leveraging large language models (LLMs) as external guidance to filter clean and noisy data. The authors propose a framework that categorizes training examples into easy clean, hard clean, and true noisy, employing distinct objectives for each category. The paper includes extensive experiments on both synthetic and real-world datasets to validate the effectiveness of the proposed method.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to understand.
- It conducts exhaustive experiments on various datasets, demonstrating solid methodology.
- The topic of mitigating noise in training data is of significant relevance and potential impact.

Weaknesses:
- The method's reliance on high-quality LLMs limits its applicability.
- Key concepts, such as the confidence of both the pre-trained model and the external LLM, are not clearly defined.
- There is insufficient discussion on the efficiency of the proposed approach compared to other methods for handling noisy labels.
- The paper lacks an exploration of its implications for other areas in NLP and does not adequately address the potential for information leakage from LLMs.

### Suggestions for Improvement
We recommend that the authors improve the clarity of key concepts, particularly the definitions of confidence for both the pre-trained model and the LLM. Additionally, the authors should discuss the efficiency of their method in comparison to other noise-robust approaches. We suggest including comparisons with knowledge distillation as a baseline to enhance the paper's empirical context. Furthermore, the related work section should be expanded to incorporate discussions on label smoothing and other relevant methodologies. Lastly, we encourage the authors to provide more detailed analyses of their experimental results, particularly regarding the accuracy of the easy clean category under various noise conditions.