ID: L4uaAR4ArM
Title: Simple and Effective Masked Diffusion Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 31
Original Ratings: 6, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a masked diffusion language modeling (MDLM) framework that enhances the performance of diffusion models in generating high-quality text, achieving state-of-the-art results on language benchmarks and approaching autoregressive methods. The authors derive a simplified Rao-Blackwellized objective, establishing a connection between generative diffusion models and encoder-only BERT models. MDLM simplifies the noise process and denoising mechanism, leading to a tighter evidence lower bound (ELBO) and improved performance metrics, including a test perplexity of 27.04 on LM1B, significantly better than D3PM's 28.56. The method is evaluated on both classification and generation tasks, demonstrating competitive performance, including applications in biological sequence modeling. The paper also emphasizes the efficiency of MDLM's inference algorithm and its ability to support arbitrary sequence lengths.

### Strengths and Weaknesses
Strengths:
- The simplified diffusion objective effectively stabilizes training and is well-supported by detailed derivations.
- MDLM introduces significant algorithmic innovations, such as a simplified noise process and a specialized denoising process, which enhance performance.
- The connection between masked language modeling (MLM) and diffusion language modeling is clearly articulated.
- The paper provides a clear theoretical foundation for the improvements in perplexity, linking them to the continuous time ELBO and the SUBS parameterization.
- Empirical results show superior performance compared to other diffusion language models and indicate that MDLM can outperform non-autoregressive baselines while matching state-of-the-art performance in fewer training steps.

Weaknesses:
- The presentation is unclear, particularly regarding the differences between the proposed method and existing models like DiffusionBert and D3MM.
- The novelty of MDLM relative to existing methods could be further clarified, particularly in distinguishing its contributions from those of D3PM and DiffusionBERT.
- Experimental details, such as network structure and tokenizer changes, are overly complex, obscuring the method's simplicity.
- The paper lacks a comprehensive theoretical analysis explaining why MDLM outperforms previous methods and does not adequately address the scalability of MDLM to larger models or its practical speed-up compared to autoregressive models.
- Important aspects, such as sampling steps and memory usage, are inadequately addressed in tables and discussions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by providing a detailed overview of the algorithm, including pseudocode to illustrate the training and sampling processes. Additionally, the authors should clarify the differences between MDLM and existing models like D3MM and DiffusionBert, ensuring that the motivation for semi-autoregressive models is well-articulated. We suggest including a mathematical proof for the continuity equation and addressing the missing citations for related works. Furthermore, the authors should conduct a more comprehensive comparison with state-of-the-art non-diffusion models and include evaluations beyond perplexity to assess the quality of generated text. To address concerns about scalability, we encourage the authors to explore and report on scaling laws for MDLM in future work. Lastly, we recommend providing more detailed comparisons of MDLM's speed and efficiency relative to autoregressive models to strengthen the claims regarding practical speed-ups.