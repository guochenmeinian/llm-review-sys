ID: hilGwNabqB
Title: A Bayesian Approach for Personalized Federated Learning in Heterogeneous Settings
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 4, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FedBNN, a personalized federated learning framework that utilizes Bayesian principles to tackle challenges associated with heterogeneous data and computational resources among clients. The authors propose a novel collaboration mechanism that shares priors in the functional space of the networks rather than model parameters, enhancing local training on small datasets while ensuring differential privacy. The effectiveness of FedBNN is demonstrated through experiments on standard datasets, showing superior performance in heterogeneous settings.

### Strengths and Weaknesses
Strengths:
1. The paper provides a comprehensive evaluation of the FedBNN framework across multiple datasets and heterogeneous settings, demonstrating its robustness and adaptability.
2. A formal privacy analysis is included, showcasing the framework's applicability in privacy-sensitive scenarios.
3. The writing is clear, and the motivation, methodology, and experimental settings are well-explained.

Weaknesses:
1. The novelty of the approach is somewhat diminished by existing methods like FedPop, necessitating clearer differentiation and more comprehensive comparisons.
2. The experimental evaluation lacks diversity in dataset types and real-world applications, limiting generalizability.
3. Scalability concerns regarding the computational overhead of Bayesian neural networks are not adequately addressed.
4. The reliance on a public auxiliary dataset raises questions about its availability in practical scenarios.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their work by providing clearer differentiation from existing methods and conducting more comprehensive comparisons. Additionally, expanding the experimental evaluation to include a wider variety of datasets and real-world applications would enhance generalizability. The authors should also address scalability concerns related to the computational overhead of Bayesian neural networks. Finally, we suggest clarifying the privacy analysis, particularly regarding the trade-offs between noise levels and model accuracy, and providing more thorough ablation studies to isolate the impact of individual framework components.