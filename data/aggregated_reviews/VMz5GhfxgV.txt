ID: VMz5GhfxgV
Title: Hyperbolic Space with Hierarchical Margin Boosts Fine-Grained Learning from Coarse Labels
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 4, 8, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method, Poincar√© embeddings with hierarchical cosine margins (PE-HCM), aimed at addressing the challenge of fine-grained learning from coarse labels. The authors propose embedding image representations in hyperbolic space to leverage its hierarchical structure for coarse label classification. The method includes a second branch that clusters instances sharing the same coarse labels to generate fine-grained pseudo-labels, utilizing a hierarchical loss to align cosine distances with label relations. Experimental validation is conducted on multiple benchmarks, including CIFAR-100 and BREEDS, with ablation studies assessing various components of the method.

### Strengths and Weaknesses
Strengths:
- The method is well justified, with an interesting adaptive hierarchical distance that performs well experimentally.
- Experimental results are encouraging, demonstrating superior recognition accuracy across datasets.
- The inclusion of ablation studies enhances the understanding of the method's components.

Weaknesses:
- The paper overlooks relevant works in the related literature, such as MaskCon and Grafit, which also address coarse-to-fine tasks.
- There is a lack of comparison with state-of-the-art methods and unsupervised baselines, limiting the contextual evaluation of the proposed approach.
- Minor issues include insufficient detail on benchmark datasets and typographical errors.

### Suggestions for Improvement
We recommend that the authors improve the literature review by including relevant works such as MaskCon, Grafit, and UnHyp, and conducting comparisons with these state-of-the-art methods. Additionally, we suggest performing experiments on larger datasets like iNaturalist-18 and including comparisons with unsupervised baselines, such as Dino and iBot. Furthermore, the authors should clarify the classes present in benchmark datasets and address minor typographical errors. Lastly, we encourage the authors to provide empirical evidence for their theoretical claims, particularly regarding the interpretability of the learned embeddings and the impact of coarse label granularity on performance.