ID: wbSweS7I5f
Title: Incentive and Dynamic Client Selection for Federated Unlearning
Conference: ACM
Year: 2023
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an innovative approach for federated unlearning through incentive and dynamic client selection, utilizing evolutionary game theory to model the dynamic participation process and employing deep reinforcement learning (DRL) for client selection. The effectiveness of the proposed method is demonstrated through various experiments, addressing practical problems in federated learning (FL). However, several concerns regarding the methodology and clarity remain.

### Strengths and Weaknesses
Strengths:
- The approach is innovative and addresses urgent practical problems in federated unlearning.
- The technical details and design structure are thoroughly described, making the paper easy to follow.

Weaknesses:
- The number of baselines is limited, with only three provided; additional up-to-date methods should be included.
- Certain figures, such as Figure 5c, present unusual accuracy curves that require further analysis.
- The font size in experimental result graphs is too small, hindering readability.
- The necessity of the rewarding evolutionary game and its implications for fairness among clients are not adequately clarified.

### Suggestions for Improvement
We recommend that the authors improve the number of baseline comparisons by adding 2-3 up-to-date methods of federated unlearning. Additionally, further analysis is needed for the accuracy curve in Figure 5c, and the font size in the experimental result graphs should be enlarged for better readability. We also suggest that the authors clarify the necessity of the rewarding evolutionary game and discuss measures to ensure fairness for disadvantaged clients in the selection process. Finally, addressing the questions regarding the central server's determination of unlearning completion, communication costs, and implications of unlearning on model performance and stability would enhance the manuscript's clarity and depth.