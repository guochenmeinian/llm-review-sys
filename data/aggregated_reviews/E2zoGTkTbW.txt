ID: E2zoGTkTbW
Title: Reward Imputation with Sketching for Contextual Batched Bandits
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 7, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on contextual batched bandit (CBB) problems, focusing on reward imputation for non-executed actions to enhance learning. The authors propose an efficient reward imputation approach, which is solved using imputation regularized ridge regression, and introduce sketching to reduce time complexity. Theoretical guarantees indicate a sublinear regret bound against the optimal policy, and experiments demonstrate its superiority over existing algorithms.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and clearly written, with a self-contained presentation.
- The proposed reward imputation method is innovative and shows strong empirical benefits over existing algorithms.
- Theoretical analysis supports the claims, and the method is backed by solid experimental results.

Weaknesses:
- A high-level explanation of why reward imputation is statistically effective is lacking, as it appears counter-intuitive that imputed rewards derived from observed rewards could yield improvement.
- Certain notations, such as $L_j^n$ and $\nu$ in Theorem 2, are introduced before being defined, complicating comprehension.
- The improvement from imputation is not clearly articulated, particularly regarding the constants $C_{\text{imp}}$ and $C_{\text{reg}}$, which require further analysis to understand their impact on regret bounds.
- The batch size $B$ is not clearly defined as a tunable parameter, leading to confusion about its implications in the analysis.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the statistical rationale behind reward imputation to address its counter-intuitive nature. Additionally, providing definitions for all notations prior to their usage would enhance readability. A more detailed analysis of the constants $C_{\text{imp}}$ and $C_{\text{reg}}$ is necessary to clarify their roles in the regret bounds. Furthermore, we suggest explicitly stating whether the batch size $B$ is a tunable hyper-parameter and discussing its theoretical advantages when set to $B=1$. Lastly, we encourage the authors to visualize the bias, variance, and regret for each method in their synthetic experiments to support their theoretical claims.