ID: qqU8WPw44f
Title: CURE4Rec: A Benchmark for Recommendation Unlearning with Deeper Influence
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 8, 7, 7, 7, -1
Original Confidences: 4, 4, 3, 4, -1

Aggregated Review:
### Key Points
This paper presents a benchmark for evaluating the impact of machine unlearning methods on recommender systems, focusing on five existing unlearning methods and introducing an unlearn data selection method. The evaluation framework encompasses recommendation effectiveness, unlearning effectiveness, unlearning efficiency, and fairness, utilizing three datasets and three recommendation models. The authors have also open-sourced the benchmark code.

### Strengths and Weaknesses
Strengths:
- The topic of unlearning is gaining popularity and holds significant potential in recommender systems, advancing research in this area.
- The paper evaluates fairness as a side effect of unlearning, providing insightful findings.
- The introduction of fairness and robustness as metrics in the benchmark offers a comprehensive perspective on recommendation unlearning.
- The paper is well-organized, clearly written, and the experiments are easy to understand.

Weaknesses:
- The evaluation of fairness on different shards may not accurately reflect the unlearning model's performance due to distinct unlearning tasks.
- The necessity of unlearning data selection strategies is questioned, as they show limited impact beyond unlearning time.
- The fairness metrics used are based on recommendation performance, which may not accurately reflect the impact of unlearning methods on model fairness.
- There are discrepancies between the paper version in the system and the supplementary materials.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of fairness by using commonly used metrics like statistical parity or equal opportunity instead of relying solely on recommendation performance. Additionally, we suggest conducting experiments on exact unlearning to evaluate completeness, as the proposed MIA-based evaluation could apply here. The authors should also provide more detailed documentation in the GitHub repository, including hyperparameter settings and usage instructions. Lastly, we advise addressing the rationale for evaluating fairness on different shards, ensuring that the observed unfairness is not influenced by varying forgetting tasks.