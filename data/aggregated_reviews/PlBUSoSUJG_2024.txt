ID: PlBUSoSUJG
Title: Policy Gradient with Tree Expansion
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel policy parameterization for reinforcement learning called SoftTreeMax, which modifies traditional softmax policies by utilizing small horizon trajectory reward values instead of logit values. The authors claim that the SoftTreeMax policy gradient exhibits lower variance than conventional policy gradients, thereby improving sample complexity. They analyze two variants, C-SoftTreeMax and E-SoftTreeMax, and provide theoretical analysis demonstrating that the policy gradient variance decays exponentially with the planning horizon. Experiments conducted using the PPO algorithm on Atari games show the efficacy of SoftTreeMax compared to traditional methods. However, the paper faces criticism regarding its presentation and contextualization within existing related work.

### Strengths and Weaknesses
Strengths:
- The originality of integrating tree search with policy gradient methods is notable, representing a departure from traditional value function-based approaches.
- The theoretical analysis supports the claim that the proposed method can reduce variance in policy gradients, which is corroborated by empirical results.
- The paper is well-written, with clear theorem statements and implementation details that ensure reproducibility, including a parallel GPU-based simulator.

Weaknesses:
- Clarity is lacking in how the various components of the proposed algorithm interact, which hinders understanding.
- The related work section does not adequately contextualize the significance of the proposed method, and comparisons with established variance reduction techniques are missing.
- Insufficient implementation details for SoftTreeMax, including specifics on the exploration policy, optimizer choices, and initialization of function approximators, are crucial for RL performance.
- The empirical tests are incomplete, focusing only on C-SoftTreeMax without evaluating E-SoftTreeMax, and the number of seeds used is limited.
- The claim of outperforming PPO in all tested Atari games is misleading, as results in some games show comparable performance.
- The paper does not adequately address the computational expense and convergence speed of SoftTreeMax compared to traditional methods.

### Suggestions for Improvement
We recommend that the authors improve clarity by summarizing how the components of the algorithm work together in a schematic. Additionally, the related work section should be rewritten and extended to include comparisons with established variance reduction methods and tree search techniques. 

To enhance empirical robustness, we suggest including more seeds in the experiments and testing both C-SoftTreeMax and E-SoftTreeMax. Furthermore, it would be beneficial to provide guidance on selecting the planning length parameter $d$ based on empirical results. 

We also recommend adding a dedicated future work section discussing efficient tree expansion implementations and forward model learning, as well as a more detailed explanation of the empirical settings and weight updates in the experiments. Lastly, addressing the concerns regarding the similarity between the proposed method and n-step returns would strengthen the paper's contributions. Additionally, we suggest improving the discussion on the implementation details of SoftTreeMax, including specifics on the exploration policy, optimizer, and initialization of function approximators. Finally, clarifying the performance claims in the results section and addressing the computational complexity and convergence speed in relation to the variance reduction would provide a more comprehensive understanding of the method's efficiency.