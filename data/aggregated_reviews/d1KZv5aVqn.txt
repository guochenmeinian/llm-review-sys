ID: d1KZv5aVqn
Title: Sparse Backpropagation for MoE Training
Conference: NeurIPS
Year: 2023
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: 3, 5, 4

Aggregated Review:
### Key Points
This paper presents SparseMixer, a gradient estimator for the Mixture-of-Experts (MoE) style network loss that activates only one expert, enhancing scalability compared to methods like Straight-Through, which require all experts to be activated. The estimator employs first and second order approximations using the Euler and Mid-point methods, respectively. The authors demonstrate that SparseMixer leads to improved training and faster convergence relative to the baseline Switch Transformer.

### Strengths and Weaknesses
Strengths:
1. The estimator is supported by solid theoretical foundations.
2. Empirical results indicate that the technique performs well.
3. The scalability of the proposed estimator is crucial for MoE networks.

Weaknesses:
1. The writing quality, particularly in the theoretical sections, needs significant improvement; many notations are undefined, leaving readers to interpret equations independently, especially in Section 2.
2. The claim regarding the disparity introduced by second order estimation in training versus inference lacks clarity, as the authors' proposed solution appears somewhat hand-wavy.
3. The instability noted in Section 4.3.2 when using only the second order estimator requires further investigation.

### Suggestions for Improvement
We recommend that the authors improve the clarity and definition of notations in the theoretical sections, particularly in Section 2. Additionally, we suggest providing a more robust justification for the proposed mixing of first and second order estimations to address the concerns about disparity in training and inference. Lastly, we encourage the authors to conduct a more thorough study on the instability associated with the second order estimator as mentioned in Section 4.3.2.