ID: IacxcFpvWQ
Title: Can Language Models Teach? Teacher Explanations Improve Student Performance via Personalization
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 7, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into whether stronger language models (LLMs) can effectively teach weaker ones through explanations, specifically exploring the use of a teacher LLM to enhance the reasoning capabilities of a student LLM. The authors examine various methods, including generating explanations for a subset of test data, personalizing these explanations based on student behavior, and assessing the student's ability to generalize from these explanations. The findings indicate that teacher intervention at test-time improves student accuracy and that a stronger teacher can enhance the performance of a weaker model, particularly through the introduction of an expected utility metric that optimizes which examples to explain. The study evaluates these methods on datasets such as StrategyQA, GSM8k, and CommonsenseQA using multiple LLMs.

### Strengths and Weaknesses
Strengths:  
- The experimental setup is sound and comprehensive, utilizing few-shot demonstrations, multiple seeds, and diverse reasoning datasets.  
- The paper addresses significant research questions regarding the teaching capabilities of LLMs, personalized explanations, and distillation approaches for identifying student understanding.  
- The introduction of the expected utility concept is compelling, showing significant performance improvements over random selection of examples.  
- The innovative approach to generating personalized explanations based on human feedback is noteworthy, although the results are mixed.

Weaknesses:  
- The paper lacks sufficient detail, making it difficult to assess the soundness of the setup and conclusions.  
- The definition of "teaching" as merely providing explanations during testing is misleading; true teaching should be evaluated based on the model's ability to generalize to new examples.  
- Performance improvements are often marginal, with overlapping confidence intervals between interventions, raising questions about statistical significance.  
- The evaluation of the effectiveness of personalized explanations lacks comparisons to unpersonalized explanations and other baselines, making it unclear whether accuracy gains stem from personalized interventions or simply from few-shot prompting.  
- Claims regarding Theory of Mind (ToM) lack proper backing, as there are no baselines to support the assertion that ToM enhances explanations.  
- Limitations regarding the necessity of gold labels for explanations are inadequately addressed, impacting the practical applicability of the method.  

### Suggestions for Improvement
We recommend that the authors improve the clarity and detail of the experimental design to facilitate better evaluation of the results. Specifically, addressing the following questions would enhance understanding:  
- How is the confidence of the teacher model assessed?  
- What specific metrics are used to measure the performance improvements?  
- Can the authors provide clearer distinctions between the roles of teacher and student models in the experiments?  
We also suggest incorporating statistical analyses, such as multi-level regression models, to validate the findings and demonstrate generalizability. Additionally, we recommend improving the clarity of the teaching definition, emphasizing the importance of generalization to new examples. Including comparisons to unpersonalized teacher explanations for randomly chosen examples in RQ4 would better assess the impact of personalization. A qualitative analysis distinguishing the differences between personalized and unpersonalized explanations is necessary, particularly given the marginal gains observed. Furthermore, we encourage the authors to provide real-world scenarios that motivate the need for reduced communication costs between teacher and student. Lastly, we advise removing references to "theory of mind" and adjusting the language in the paper to focus on personalization, ensuring that the terminology accurately reflects the study's contributions.