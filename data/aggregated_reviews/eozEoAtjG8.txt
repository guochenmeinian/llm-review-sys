ID: eozEoAtjG8
Title: Understanding and Improving Feature Learning for Out-of-Distribution Generalization
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 7, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on out-of-distribution (OOD) generalization in the context of spurious correlations, providing a theoretical analysis of feature learning dynamics under Empirical Risk Minimization (ERM) and Invariant Risk Minimization (IRM). The authors demonstrate that ERM learns both spurious and invariant features at different rates, impacting subsequent OOD optimization performance. They propose a novel training method called Feature Augmented Training (FAT) that enhances feature learning by separating training data into augmentation and retention sets, which allows for the retention of previously learned features while incorporating new ones. This method shows empirical improvements over standard ERM pre-training across various datasets. The authors acknowledge the importance of clarifying task-relevant features in their visualizations and commit to including more examples in the revised version to illustrate their findings.

### Strengths and Weaknesses
Strengths:
- The paper effectively combines theoretical insights with empirical evaluations, offering valuable contributions to understanding feature learning dynamics.
- The method's innovative approach to feature learning is well-articulated, emphasizing the importance of capturing all predictive features.
- The authors effectively address reviewer concerns, enhancing the clarity of their visualizations and explanations.

Weaknesses:
- The clarity and presentation of the paper could be improved for broader accessibility. For instance, details in figures and terminology need better explanations, such as the meaning of the “Feature Learning” axis in Fig. 2 and the definition of “OOD objectives” prior to L122.
- The theoretical framework relies on a linear activation function, limiting its applicability. More details should be provided on extending results to non-linear networks.
- The connection between the theoretical analysis and the FAT methodology is unclear, as the theory does not sufficiently inform the method's design.
- The authors admit limitations in their expertise regarding specific domains, which may affect the depth of their feature analysis. Some reviewers still express concerns about the theoretical limitations and the extent of empirical improvements.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing more detailed captions for figures, particularly Fig. 2, and explicitly defining terms like “OOD objectives.” Additionally, the authors should elaborate on the FAT methodology, including the rationale for design choices such as reinitializing classifier weights and the specifics of the iFAT method. To strengthen the theoretical contributions, we suggest including quantitative results in Theorem 4.1 and discussing the computational complexity of FAT compared to competitors. Furthermore, we recommend improving the clarity of the task-relevant features in their visualizations, ensuring that the descriptions are intuitive and comprehensive. Including more examples in the revised version to provide a broader understanding of the model's performance across different datasets would also be beneficial. Finally, we encourage the authors to address the theoretical limitations and explore potential extensions to their work that could enhance its applicability in real-world scenarios. Addressing the reproducibility issue by providing code would greatly enhance the verification of findings.