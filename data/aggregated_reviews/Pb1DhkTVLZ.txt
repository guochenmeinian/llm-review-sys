ID: Pb1DhkTVLZ
Title: Estimating Large Language Model Capabilities without Labeled Test Data
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on estimating the in-context learning (ICL) ability of large language models (LLMs) without labeled test data. The authors formalize the problem of few-shot ICL accuracy estimation, proposing a meta-model that utilizes LLM confidence and accuracy for dataset-level estimation. The evaluation includes three datasets and two architectures (OPT and LLaMA), demonstrating the method's effectiveness against baselines and its potential in practical applications.

### Strengths and Weaknesses
Strengths:
- The problem addressed is significant and practically relevant, with a well-motivated approach that could save time and resources.
- The comparative evaluation against baselines showcases state-of-the-art performance, supported by thorough experiments and an ablation study.
- The confidence-based evaluation strategies enhance the paper's value, providing insights into the model's workings.

Weaknesses:
- The proposed benchmark lacks diversity, needing more tasks and better organization to strengthen its generalizability.
- The observed improvements in accuracy appear modest, especially when compared to simpler methods like the Oracle, which requires fewer examples for accurate estimation.
- The evaluation framework is limited to three tasks and two models, raising concerns about broader applicability and the high variance in accuracy reported.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by including a wider variety of tasks and organizing them more effectively. Additionally, we suggest enhancing the clarity of Table 1 and considering alternative metrics for better readability. The authors should elaborate on the estimation variance across different prompts and settings to further validate the method's effectiveness. Finally, expanding the evaluation to include diverse datasets, such as Natural-Instruction V1/2, and integrating statistical tests for comparisons would strengthen the findings.