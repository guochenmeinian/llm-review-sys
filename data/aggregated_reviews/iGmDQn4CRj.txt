ID: iGmDQn4CRj
Title: Simplifying Neural Network Training Under Class Imbalance
Conference: NeurIPS
Year: 2023
Number of Reviews: 22
Original Ratings: 8, 6, 6, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the class imbalance problem, demonstrating that tuning standard hyperparameters can yield state-of-the-art performance on various imbalanced datasets without the need for specialized loss functions or samplers. The authors identify key findings: (1) smaller batch sizes benefit imbalanced data, (2) data augmentation strategies must be carefully chosen as they can negatively impact minority class performance, (3) larger models tend to overfit on imbalanced datasets, (4) integrating self-supervised loss during training enhances performance, (5) the SAM optimizer improves accuracy for minority classes, and (6) label smoothing mitigates overfitting to minority classes. The study is validated through extensive experiments across multiple datasets, including both image and tabular data. Additionally, the authors investigate the effect of batch size on regularization during imbalanced training, observing that larger batch sizes increase overfitting, particularly for minority classes, and that imbalanced training results in lower gradient noise, contributing to overfitting. The analysis of the Hessian spectrum indicates that larger batch sizes lead to sharper minima, increasing the risk of overfitting. The paper also presents a modified version of the SAM technique that treats majority and minority classes differently to optimize decision boundaries, emphasizing that not all techniques effective in balanced setups translate well to imbalanced scenarios. Various evaluation metrics, including accuracy and class-wise accuracy, are reported to enhance model performance assessment.

### Strengths and Weaknesses
Strengths:
- The authors effectively challenge existing assumptions about hyperparameter tuning in imbalanced datasets, prompting a reevaluation of experimental settings.
- Comprehensive experiments across diverse datasets provide robust evidence for the proposed methods.
- The paper is well-structured and clearly articulates its contributions and distinctions from prior work.
- The authors conducted extensive experiments that provide concrete justification for their observations regarding batch size effects.
- The integration of multiple architectures and datasets strengthens the robustness of the conclusions.
- The paper offers significant insights into the effects of batch size on overfitting and generalization in imbalanced training.

Weaknesses:
- The paper lacks detailed reporting of all hyperparameter settings and tuning methods, which hinders reproducibility.
- Some findings, particularly regarding data augmentation and model architecture, are not sufficiently novel or well-supported.
- The evaluation metrics primarily focus on accuracy, which may not adequately reflect performance in imbalanced contexts.
- The inability to share figures directly may limit the clarity of the presented results.
- The complexity of hyperparameter tuning for different classes remains a concern, complicating the implementation of some proposed methods.
- The discussion on the types of imbalances in language models could be expanded for clarity.

### Suggestions for Improvement
We recommend that the authors improve the reproducibility of their results by providing a complete list of hyperparameter settings and tuning methods used in all experiments, along with the code for replication. Additionally, we suggest including a more diverse set of datasets, such as ImageNet-LT and iNaturalist, to validate the findings further. It would also be beneficial to explore the underlying mechanisms of how batch size affects performance in imbalanced settings and to provide theoretical insights into these observations. We recommend that the authors improve the presentation of their findings by including figures in the camera-ready version to enhance clarity. Furthermore, we suggest that the authors explicitly address the novelty of their results in relation to existing literature to strengthen their argument. Lastly, consider providing a more detailed discussion on the implications of the observed overfitting in the context of practical applications and elaborating on the complexities of hyperparameter tuning for different classes to enhance the clarity of the proposed methods.