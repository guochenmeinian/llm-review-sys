ID: LGG1IQhbOr
Title: Non-geodesically-convex optimization in the Wasserstein space
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 3, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an optimization scheme called "semi-FB Euler" for minimizing a functional over the Wasserstein space \( P_2(R^d) \), defined as \( \mathcal{F}(\mu) = \int (G-H) d\mu + \mathcal{H}(\mu) \), where \( G \) and \( H \) are convex functions and \( \mathcal{H} \) is a convex functional. The scheme involves a two-step process: first computing \( \nu_{n+1} \) using a forward Euler step on the concave term \( -H \), followed by a backward step to minimize the convex term. The authors establish convergence to critical points of \( \mathcal{F} \) under mild regularity assumptions, both asymptotically and with non-asymptotic rates. Additionally, the paper explores DC structures in the context of Wasserstein-geodesically-convex functionals, proposing to clarify the focus on DC structures and the importance of discussing DC splittings in Bayesian posterior sampling, while acknowledging the need for empirical comparisons with traditional sampling techniques.

### Strengths and Weaknesses
Strengths:  
- The work provides a mathematically rigorous foundation for an infinite-dimensional optimization scheme, detailing explicit regularity conditions for well-definedness and convergence.  
- The theoretical contributions offer valuable insights into DC structures and their implications for practical applications.  
- The paper is clearly written and well-structured, with adequate background on Wasserstein geometry.  
- The authors demonstrate awareness of the challenges in implementing their methods, particularly regarding the JKO step.  
- The proposed approach is elegant and theoretically sound, offering strong guarantees in a broader context than traditional convex optimization.

Weaknesses:  
- The "semi-FB Euler" scheme closely resembles existing methods, particularly the Wasserstein proximal gradient algorithm, raising questions about its novelty.  
- The reliance on a computationally expensive JKO step limits practical applicability, and numerical experiments are conducted on toy datasets without addressing computational costs.  
- The theoretical contributions may not be sufficiently significant, as some results could be guaranteed under smoother conditions rather than the proposed concavity of \( -H \).  
- The presentation is somewhat muddled, lacking a clear focus on either sampling applications or a comprehensive treatment of all Wasserstein-geodesically-DC functionals.  
- The connection to Maximum Mean Discrepancy (MMD) is not convincingly established, and the practical performance of the proposed methods remains unaddressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the contributions by explicitly addressing the similarities with existing methods and emphasizing the unique aspects of their approach. Additionally, we suggest incorporating more detailed discussions on the computational aspects of the JKO step and its implications for practical applications. It would be beneficial to explore the extension of results to cases where \( H \) is smooth instead of merely concave, as this could enhance the theoretical guarantees. We also recommend that the authors clarify their focus in the abstract by explicitly stating their consideration of the DC structure and provide a more thorough discussion of DC splittings for Bayesian posterior sampling. To enhance the practical applicability of their work, we encourage the authors to empirically compare their method with traditional sampling methods on target measures that are DC but non-smooth. Furthermore, we advise adding citations to relevant literature to strengthen the connection to MMD and to simplify proofs by leveraging existing results. Finally, we encourage the authors to elaborate on the "interesting avenues for future work" mentioned in the conclusion to provide clearer insights into potential applications and implications of their findings.