ID: fi3aKVnBQo
Title: Efficient Leverage Score Sampling for Tensor Train Decomposition
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 6, 4, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a randomized alternating least squares (TT-ALS) algorithm for computing tensor factorizations, leveraging an exact characterization of leverage scores through an intermediate orthonormal representation. The authors propose a leverage score sampling-based method to reduce computational complexity, supported by rigorous theoretical analysis and empirical results demonstrating performance improvements over traditional methods.

### Strengths and Weaknesses
Strengths:  
- The approach is natural, with powerful bounds and a well-structured theoretical analysis.  
- Empirical evaluations show significant runtime improvements without sacrificing fit, validating the proposed method's efficiency.  
- The paper introduces a novel data structure for efficiently computing leverage scores.

Weaknesses:  
- The experimental setting primarily focuses on 3-dimensional dense tensors, raising concerns about applicability to more complex, sparse, and higher-dimensional tensor instances.  
- The contribution relative to existing work, particularly [Malik and Becker, 2021], is not clearly articulated, especially regarding the necessity of a new method for TT decomposition.  
- Comparisons with key competitors, such as Chen et al. 2023, are lacking, and the empirical evaluation is limited to a small number of baselines.  
- The analysis does not fully address how subsampling affects ALS convergence, and implementation details are insufficiently described.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contribution by explicitly stating the necessity of developing a new method for TT decomposition beyond existing leverage score sampling approaches. Additionally, we suggest including comparisons with Chen et al. 2023 to strengthen the empirical evaluation. It would be beneficial to provide a comprehensive analysis of how the proposed method's runtime improvements relate to convergence and to clarify the implementation details that are currently omitted. Furthermore, addressing the concerns regarding the experimental settings and exploring the performance on sparse and higher-dimensional tensors would enhance the paper's applicability. Lastly, improving the presentation by defining tensor jargon and correcting minor typographical errors would benefit the overall clarity.