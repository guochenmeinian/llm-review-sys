ID: SjQ1iIqpfU
Title: CoBo: Collaborative Learning via Bilevel Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to collaborative learning by framing it as a bilevel optimization problem, enhancing the training efficacy of multiple clients. The authors propose the COBO algorithm, a scalable and elastic Stochastic Gradient Descent (SGD) type alternating optimization algorithm, which addresses challenges in client selection and model training. Theoretically guaranteed for convergence, COBO outperforms established personalization algorithms, particularly in heterogeneous federated learning environments and when fine-tuning Large Language Models (LLMs).

### Strengths and Weaknesses
Strengths:
1. The authors formulate collaborative learning using an innovative bilevel optimization framework, yielding more universally applicable solutions.
2. COBO is introduced as a scalable and elastic SGD-type algorithm that efficiently addresses the bilevel problem, maintaining performance with an increasing number of clients.
3. The authors provide theoretical convergence guarantees for COBO in collaborative learning scenarios with clustered client structures.
4. Empirical results demonstrate that COBO surpasses established personalized federated learning benchmarks.

Weaknesses:
1. The paper lacks an estimate of the step size in equation (1), which is critical for understanding the collaboration weights update.
2. The theoretical proof of COBO is based on simplified assumptions, such as the use of full gradient information, which may not reflect practical complexities.
3. The experiments conducted do not include repeated trials to assess robustness under varying conditions, potentially skewing results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the step size estimation in equation (1) to enhance understanding. Additionally, we suggest providing a more comprehensive comparison of the theoretical convergence performance between COBO and state-of-the-art algorithms, particularly addressing the limitations of the simplified assumptions in the proof. Furthermore, conducting repeated experiments would strengthen the robustness of the results, and including comparisons with fully decentralized algorithms could provide a more holistic evaluation of COBO's performance.