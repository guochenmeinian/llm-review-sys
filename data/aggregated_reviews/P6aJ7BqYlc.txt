ID: P6aJ7BqYlc
Title: GACL: Exemplar-Free Generalized Analytic Continual Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 8, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new exemplar-free generalized continual learning (GCIL) technique called generalized analytic continual learning (GACL). It employs analytic learning to provide a closed-form solution to the GCIL scenario, achieving equivalence between incremental learning and joint training by decomposing incoming data into exposed and unexposed classes. The GACL method is theoretically validated and demonstrates strong empirical performance across multiple datasets.

### Strengths and Weaknesses
Strengths:
1. The technique establishes a valuable equivalence between GCIL and joint training, despite requiring a pre-trained network.
2. GACL is both accurate and exemplar-free, which is a significant advancement in the field.
3. The approach is well-motivated, clearly explained, and supported by a comprehensive literature review.
4. The theoretical framework is clearly articulated, particularly regarding the separation of weights into W_unexposed and W_ECLG.

Weaknesses:
1. The average accuracy on CIFAR-100 is significantly lower than the final accuracy, warranting further explanation.
2. The concept of "weight-invariance" in Theorem 3.1 needs clearer elaboration for accessibility to a broader audience.
3. The assumption that the pre-trained backbone is generalizable may not hold in all scenarios, particularly where a large domain gap exists.
4. The experimental setup lacks clarity, particularly regarding whether comparisons were made in an online scenario.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental setup and specify the backbone network used for reported results. Additionally, we suggest that the authors address the motivation for integrating analytic learning into GCIL more explicitly, and clarify the specific design considerations that make GACL suitable for GCIL. Furthermore, including comparisons with existing ACL methods, particularly RanPAC, would strengthen the paper's contributions. Lastly, we encourage the authors to provide a comparison of memory costs associated with their method versus replay-based approaches.