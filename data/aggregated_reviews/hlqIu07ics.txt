ID: hlqIu07ics
Title: Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of multi-step reasoning in large language models (LLMs) through a novel probing approach called MechanisticProbe, which recovers reasoning trees from the models' attention patterns. The authors investigate two models, GPT-2 and LLaMA, on a synthetic task (finding the kth-smallest element) and a synthetically generated real-world task (ProofWriter). The findings suggest that LLMs engage in multi-step reasoning, with higher probing scores correlating with improved performance and robustness.

### Strengths and Weaknesses
Strengths:
- The probing approach is novel and timely, providing insights into the reasoning processes of LLMs.
- Comprehensive experiments support the existence of reasoning trees and their impact on model performance.

Weaknesses:
- The applicability of the probing task is limited due to assumptions about task format and tree structure.
- The experiments are constrained by the small number of datasets and models tested, raising concerns about generalizability.
- The interpretability of attention patterns is debated, and the method's scalability to larger models is unclear.

### Suggestions for Improvement
We recommend that the authors improve the general applicability of the probing task by addressing the assumptions regarding task format and tree structure. Additionally, we suggest expanding the experiments to include more datasets and models to strengthen the claims made. It would also be beneficial for the authors to clarify how the method scales to larger models and to provide a more robust discussion on the interpretability of attention patterns. Furthermore, we encourage the authors to consider including datasets like HotpotQA and Musique for a more comprehensive evaluation. Lastly, please correct the terminology from "k-smallest" to "kth-smallest" and ensure all figures are properly labeled.