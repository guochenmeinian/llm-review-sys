ID: SJw4Da8BuR
Title: Transformer Compression via Subspace Projection
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 3, 4, 3, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a model compression approach for transformers called Transformer Compression via Subspace Projection (TCSP), which reduces hidden size through low-rank factorization. The authors propose a method that projects the model onto a lower-dimensional subspace using a projection matrix derived from sample training data. Experimental results on BERT and T5 demonstrate a compression ratio of 44% with a maximum accuracy loss of 1.6%. The paper claims compatibility with other compression techniques like model pruning and head size compression.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and clear, making it easy to understand.
- The proposed algorithm is general and compatible with other compression strategies.
- Experimental results validate the effectiveness of the method, achieving significant compression with minimal performance loss.

Weaknesses:
- The novelty of the approach is limited, as it closely resembles existing low-rank factorization techniques like SVD.
- The paper lacks a thorough comparison with recent techniques and does not provide sufficient experimental results on inference speed.
- The ablation studies are inadequate, particularly regarding the comparison with SVD and other pruning methods.
- Some notation is introduced before definitions, and there are writing mistakes that detract from clarity.

### Suggestions for Improvement
We recommend that the authors improve the novelty discussion by addressing the similarities to existing low-rank approximation methods and providing a clearer justification for the proposed approach. A comprehensive comparison with recent techniques such as LORA and additional compression metrics should be included. We also suggest conducting a robust ablation study with SVD and other relevant methods to assess the impact of different components of the approach. Furthermore, clarifying the sampling method for training data and the choice of rank value in the main text would enhance the paper's clarity. Lastly, addressing the writing mistakes and refining the overall presentation would improve readability.