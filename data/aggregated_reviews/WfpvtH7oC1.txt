ID: WfpvtH7oC1
Title: Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 7, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 2, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method that discretizes action trajectories from offline data into skills using a Byte-Pair Encoding (BPE)-inspired tokenization approach. The discrete action trajectory skills are provided to a high-level agent for solving downstream tasks. The method demonstrates improved exploration in sparse reward settings, computational efficiency in skill creation, and generalization due to its non-state conditioning. The evaluation across various environments shows performance on par with or better than existing non-state conditioned skill-based reinforcement learning methods.

### Strengths and Weaknesses
Strengths:
- The writing is clear, and the method is intuitive and easy to understand, supported by effective visualizations.
- The method is computationally efficient, allowing for rapid skill creation and downstream reinforcement learning applications.
- The results convincingly demonstrate the method's effectiveness across multiple environments, with comprehensive ablation studies.

Weaknesses:
- The method lacks state conditioning, limiting its flexibility and performance in certain scenarios, as evidenced by comparisons with state-conditioned methods like SPiRL.
- The policy must learn associations between action sequence tokens from scratch, potentially wasting environment steps.
- The presentation of baselines and experimental details is unclear, particularly regarding the number of demonstrations used and the state-of-the-art nature of the baselines.
- The paper does not adequately address the impact of environment stochasticity on performance or the scalability of the method with larger datasets.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental setup by providing details on the number of demonstrations used and a more thorough explanation of the baselines. Additionally, addressing the limitations of the method, particularly regarding state conditioning and the effects of stochastic environments, would strengthen the paper. It would also be beneficial to explore the potential for variable-length skills and to clarify the advantages of BPE over simpler methods like k-means clustering in skill discovery. Finally, including evaluations in highly stochastic environments could provide valuable insights into the robustness of the proposed method.