ID: tIlDF5B6T4
Title: Learning Mathematical Rules with Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 9, 7, 6, 6
Original Confidences: 4, 5, 4, 3

Aggregated Review:
### Key Points
This paper presents an exploration of large language models' (LLMs) ability to learn and apply mathematical rules, such as distributivity and equation simplification, particularly in the context of word problems. The authors propose an innovative methodology for generating synthetic data that mimics mathematics textbook styles, allowing for effective fine-tuning of models. The study demonstrates that while LLMs can generalize mathematical rules to some extent, they encounter challenges when translating natural language into mathematical expressions.

### Strengths and Weaknesses
Strengths:
1. The creation of synthetic data reflecting mathematical textbook problems provides a controlled environment for testing models' abilities to learn specific mathematical rules.
2. The study addresses a practical challenge in AI education technology by improving LLMs' handling of word problems, a fundamental aspect of mathematical learning.
3. A thorough analysis of models' performance on both synthetic and real-world style data offers valuable insights into their capabilities and limitations in mathematical reasoning.

Weaknesses:
1. The models struggle with applying learned mathematical rules to new problems, particularly in translating natural language into mathematical equations, a critical step not sufficiently addressed.
2. While synthetic data is beneficial for controlled experiments, it may not capture the complexity of real-world data, potentially limiting practical applicability.
3. There is a risk that fine-tuning on specific tasks may lead to overfitting, reducing effectiveness on generalized tasks or unseen data types.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the observed decrease in performance on pre-training tasks after fine-tuning, including potential mitigation strategies for catastrophic forgetting. Additionally, exploring methodologies that enhance the models' ability to translate natural language problems into mathematical expressions would strengthen the study. Finally, we suggest considering the scalability of synthetic data generation to encompass a broader range of mathematical scenarios with less effort.