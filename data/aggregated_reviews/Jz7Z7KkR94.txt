ID: Jz7Z7KkR94
Title: REDUCR: Robust Data Downsampling using Class Priority Reweighting
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 7, 7, -1, -1, -1, -1
Original Confidences: 3, 1, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a data downsampling method called REDUCR, which is designed to be robust against class imbalance and distribution shifts. The authors propose an online algorithm that employs class priority reweighting to enhance worst-class performance while maintaining efficiency in data selection. Experiments across six benchmarks demonstrate the method's effectiveness in improving worst-class generation performance in both vision and text tasks.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and organized, with a clear motivation for the proposed method.
- The originality of the online batch selection approach is notable, as it effectively addresses noise, imbalance, and distributional shifts while preserving worst-class performance.
- Positive experimental results are supported by comprehensive formulas and visual aids that clarify the algorithm and evaluation outcomes.

Weaknesses:
- The experiments do not include datasets with varying distribution shifts, despite claims of robustness to such shifts.
- The paper lacks detailed ablation studies to justify the necessity of each component in the online batch selection process.
- All experiments are conducted on small-scale datasets, and there is insufficient exploration of the method's performance in larger-scale scenarios or with other model architectures.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including numerical results on datasets with distribution shifts to substantiate claims of robustness. Additionally, discussing the applicability of REDUCR to other model architectures, such as ConvNeXt and Swin, would enhance the generalizability of the findings. Further, we suggest conducting ablation studies on hyperparameters, such as the fraction of data points selected for training, to clarify the impact of each component on the method's performance. Lastly, exploring the balance between compute efficiency and data downsampling efficiency across different model architectures and class group numbers would strengthen the paper's contributions.