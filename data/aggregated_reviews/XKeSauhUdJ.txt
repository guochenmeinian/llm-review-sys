ID: XKeSauhUdJ
Title: DiffUTE: Universal Text Editing Diffusion Model
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 5, 6, 6, 6, 6, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DiffUTE, a universal self-supervised text editing diffusion model aimed at enhancing language-guided image editing. The authors propose modifications to the network structure to improve multilingual character rendering and utilize a self-supervised learning framework to leverage extensive web data. The experimental results indicate that DiffUTE achieves high-fidelity and controllable editing on diverse images, marking a significant advancement in the field.

### Strengths and Weaknesses
Strengths:
- The problem addressed is realistic and relevant, as current diffusion models struggle with accurate text rendering.
- The incorporation of LLM into the inference process is compelling.
- The proposed method demonstrates impressive editing performance and is organized clearly, making it easy to follow.

Weaknesses:
- The novelty of the core idea is limited, as latent diffusion has been previously successful in various tasks.
- The paper lacks details on the position control module, making the ablation study unconvincing.
- There is insufficient evaluation using established metrics like SSIM, MSE, and PSNR, which are necessary for a comprehensive quality assessment.
- Missing comparisons with relevant works such as Krishnan et al. (2023) and Ji et al. (2023) weaken the contribution.
- The reliance on a pretrained OCR encoder appears arbitrary, and an ablation study with different encoders is needed.

### Suggestions for Improvement
We recommend that the authors improve the novelty by clearly articulating the unique contributions of their approach compared to existing methods. Additionally, please provide more details regarding the position control module and conduct a thorough ablation study. We suggest including a wider range of evaluation metrics, such as SSIM, MSE, and PSNR, to better assess output quality. Furthermore, consider comparing your results with those of Krishnan et al. (2023) and Ji et al. (2023) to strengthen the paper's claims. Lastly, an ablation study with different pretrained OCR encoders should be included to clarify the impact of this choice.