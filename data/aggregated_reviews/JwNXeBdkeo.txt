ID: JwNXeBdkeo
Title: Provably (More) Sample-Efficient Offline RL with Options
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of sample complexity in offline reinforcement learning (RL) with options, focusing on the suboptimality of hierarchical policies compared to optimal policies. The authors propose the PEVIO algorithm, which implements a pessimistic value iteration approach for learning with options, and derive theoretical bounds for two data collection methods: state-option-utility and state-action-reward. The findings suggest that using options can enhance sample efficiency and lead to faster convergence rates under certain conditions.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant issue in hierarchical RL, exploring the impact of options on offline learning efficiency.
- It is well-written and thorough in its mathematical formulation, assumptions, and definitions.
- The analysis of different data collection methods provides valuable insights into their respective advantages and challenges.

Weaknesses:
- The practical applicability of the PEVIO algorithm appears limited, particularly due to the data splitting technique that discards a substantial portion of data.
- The theoretical framework relies on the assumption that a set of options is pre-defined, which may affect the overall performance of the hierarchical policy.
- Clarity regarding the assumptions and the implications of the penalty function in the algorithm could be improved.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the assumptions regarding the availability and quality of options, as the performance of the hierarchical policy is contingent upon these factors. Additionally, we suggest providing more details on the penalty function and its intuitive meaning, as well as clarifying how the algorithm updates this function. A dedicated section discussing the limitations of the study, particularly in relation to the assumptions made, would enhance the paper's comprehensiveness. Finally, we encourage the authors to explore the influence of option complexity on the effectiveness of PEVIO and to expand the experimental setup to validate its performance across varied environments and datasets.