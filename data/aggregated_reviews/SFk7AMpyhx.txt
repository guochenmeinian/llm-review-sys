ID: SFk7AMpyhx
Title: 4Diffusion: Multi-view Video Diffusion Model for 4D Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 5, 4, 5, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 5, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a 4D generation pipeline, 4Diffusion, designed to create spatial-temporally consistent 4D content from monocular video. The authors propose a unified diffusion model that integrates a learnable motion module into a frozen 3D-aware diffusion model, effectively capturing multi-view spatial-temporal correlations. The model is trained on a curated dataset, achieving reasonable temporal consistency while maintaining the generalizability and spatial consistency of the 3D-aware diffusion model. Additionally, a 4D-aware Score Distillation Sampling loss is introduced to optimize 4D representation parameterized by dynamic NeRF, with the framework outperforming optimization-based baselines.

### Strengths and Weaknesses
Strengths:
- The paper proposes a reasonable direction for generating multiview video to guide 4D generation.
- It is well-written and easy to follow.
- A new subset of animatable Objaverse is introduced, enhancing the model's generation capabilities.
- The architectural extension of the 3D-aware diffusion model is a valuable contribution.

Weaknesses:
- The paper's novelty is moderate, and the results exhibit temporal inconsistencies, such as color flickering.
- The proposed method requires extensive training time (12 hours on A100) and a limited dataset of 996 training samples, raising concerns about overfitting.
- The evaluation design is unclear, particularly regarding the purpose of comparing synthesized videos to input videos and the potential overlap with existing datasets.
- Some technical details, such as the architecture of the learnable motion module, are inadequately described.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the technical details, particularly regarding the architecture and parameters of the motion module. Additionally, addressing the temporal inconsistencies and exploring the impact of increasing the training dataset size could enhance the model's performance. We suggest providing a more thorough explanation of the evaluation metrics and ensuring that the validation and test datasets are clearly defined to avoid potential data leakage. Lastly, further evaluation of statistical significance over the improvements compared to Consistent4D would strengthen the paper's contributions.