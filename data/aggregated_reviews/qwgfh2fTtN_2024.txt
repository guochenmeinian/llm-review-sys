ID: qwgfh2fTtN
Title: Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 5, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to "easy-to-hard generalization," where the authors propose training a verifier on simpler tasks to supervise a generator on more complex tasks. They introduce the Outcome & Process Reward Model (OPRM) to leverage the strengths of outcome and process reward models, demonstrating its effectiveness through extensive experiments primarily on the MATH dataset. The findings suggest that easier evaluators can maintain their effectiveness when applied to harder tasks, providing a potential pathway for scalable alignment without human supervision.

### Strengths and Weaknesses
Strengths:
- The problem addressed is both promising and challenging.
- The proposed approach is intuitive and well-motivated.
- The paper is well-written, presenting clear ideas and thorough experimental validation.
- The introduction of OPRM effectively combines ORMs and PRMs, showing improved efficiency.

Weaknesses:
- The definition of "difficult problems" lacks clarity and may be biased based on the MATH dataset's subsets.
- The distinction between "easy-to-hard" and "weak-to-strong" generalization is not adequately addressed.
- Some experimental results, such as the performance of the Full ICL setting compared to the Easy-To-Hard ICL setting, require better explanation.
- The paper could benefit from more case studies and clearer navigation in tables and figures.

### Suggestions for Improvement
We recommend that the authors refine the definition of difficult problems and provide performance metrics across different subsets of the MATH dataset to demonstrate the model's ability to tackle truly challenging problems. Additionally, we suggest clarifying the differences between "easy-to-hard" and "weak-to-strong" generalization, and addressing the performance discrepancies noted in Table 1 with more robust explanations. Reporting the average accuracy of verifying each step in a solution would enhance the evaluation of the evaluators. Lastly, improving the readability of tables and figures, as well as providing details on the training runs, would strengthen the presentation of the results.