ID: JWMIm1EyaE
Title: Explaining with Contrastive Phrasal Highlighting: A Case Study in Assisting Humans to Detect Translation Differences
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to model explainability in machine translation (MT) by focusing on the differences between input sentences to elucidate output translations. The authors propose a method called phrase alignment guided erasure, which generates contrastive phrasal highlights to explain predictions of a semantic divergence model. This method enhances post-hoc explanation techniques by providing unsupervised contrastive explanations of translation discrepancies. The design employs a phrase-alignment guided occlusion strategy that ranks bilingual sentence pairs based on semantic similarity, extracting candidate counterfactual inputs through a statistical MT-based phrase table. The paper also introduces application-grounded evaluation measures to assess the effectiveness of the proposed method.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, coherent, and easy to follow, with thorough experiments and ample analysis.
- The proposed contrastive phrasal-pair erasure method accounts for the structure of inputs, improving upon existing MT explainability techniques.
- Results indicate that contrastive phrasal highlights significantly aid bilingual speakers in identifying fine-grained translation differences and errors.
- The method is applicable to multiple language pairs and leverages pre-existing phrasal alignment packages.

Weaknesses:
- The reliance on external phrasal alignment models may limit applicability in low-resource scenarios.
- The small sample sizes in application-grounded evaluations (60 translation pairs in the first study and 50 in the second) may weaken the generalizability of findings.
- Potential biases in user responses due to the phrasing of prompts could affect the validity of results.
- Some details regarding statistical evaluations and definitions are unclear, and the discussion lacks depth in connecting results across studies.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their statistical evaluations by providing full details on the tests used and addressing the significance testing in section 4. Additionally, the authors should consider expanding the discussion to connect findings from the two studies, clarifying how the results relate to the stated hypotheses. To enhance reproducibility, we suggest that the authors release their code and the data used in the studies, as well as provide more comprehensive details on the ChatGPT prompts. Finally, addressing the potential biases in user responses by revising the phrasing of prompts could strengthen the validity of the evaluations.