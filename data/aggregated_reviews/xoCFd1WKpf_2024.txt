ID: xoCFd1WKpf
Title: Unified Lexical Representation for Interpretable Visual-Language Alignment
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 5, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LexVLA, a method for Visual-Language Alignment (VLA) that integrates a pretrained vision model (DINOv2) and a pretrained language model (Llama 2) using a unified lexical representation. The authors propose an overuse penalty to mitigate false discoveries and introduce the PatchDis metric for evaluating patch-level alignment. Evaluation on zero-shot cross-modal retrieval datasets demonstrates state-of-the-art performance compared to selected baselines, even with modest dataset fine-tuning.

### Strengths and Weaknesses
Strengths:
- The authors proposed an effective and interpretable Lexical Representation approach for VLA.
- The framework is clear and accessible, requiring no complex design or training configurations.
- LexVLA outperforms baselines on cross-modal retrieval benchmarks, even with less training data.
- Extensive experiments validate the proposed components and their effectiveness.

Weaknesses:
- The novelty of the work is unclear, particularly regarding the lexical representation and its integration with existing codebook strategies.
- The vocabulary relies on the Llama tokenizer, which may lead to the omission of longer relevant words and meaningless sub-word tokens.
- The alignment was tested on only one task; exploring additional multimodal tasks would enhance the study's impact.
- The effectiveness of the proposed PatchDis metric in reflecting interpretability is not rigorously verified.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the novelty of their approach, particularly in relation to existing codebook strategies. Additionally, the authors should explore the implications of using the Llama tokenizer on vocabulary completeness and report any findings regarding missing long words. It would also be beneficial to test the alignment on other multimodal tasks, such as zero-shot classification or grounding. Furthermore, we suggest providing a quantitative or qualitative verification of the PatchDis metric's effectiveness in evaluating interpretability.