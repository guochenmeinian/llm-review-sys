ID: vAOgaPvgYr
Title: OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step
Conference: NeurIPS
Year: 2024
Number of Reviews: 38
Original Ratings: 7, 6, 3, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents OccamLLM, a framework that integrates a symbolic architecture called OccamNet into Large Language Models (LLMs) to enhance their arithmetic capabilities. The approach allows for exact arithmetic computations in a single autoregressive step, achieving 100% accuracy on various arithmetic tasks while outperforming models like GPT-4. The authors evaluate OccamLLM on simple arithmetic tasks and demonstrate its effectiveness in both single-step and multi-step arithmetic reasoning. Additionally, the paper details the generation of OccamLLM switch training data, comprising 25% numerical expressions, 5% multi-step reasoning problems, and 70% from 27 manually created prompts. The authors assert that OccamLlama 70B consistently outperforms Llama 3 70B, particularly in challenging datasets such as MultiArith Float and MATH401, and shows strong generalization capabilities, particularly in handling out-of-distribution problems.

### Strengths and Weaknesses
Strengths:
1. The integration of OccamNet allows for high accuracy in arithmetic without the need for fine-tuning the LLM, thus avoiding catastrophic forgetting.
2. OccamLLM demonstrates robust performance in single-step arithmetic tasks, consistently outperforming the baseline Llama3-8B model.
3. The framework is interpretable, providing insights into the computational process through predicted computational Directed Acyclic Graphs (DAGs).
4. OccamLLM exhibits strong generalization on out-of-distribution tasks, performing well on benchmarks with both numeric and text-based problems.
5. The model shows improved generation speed and cost-effectiveness, requiring fewer tokens than LLMs that generate code.
6. The authors provide a clear rationale for their choice of datasets, asserting that they were selected to ensure robust testing of the model's capabilities.

Weaknesses:
1. The decoder's effectiveness is contingent on its training across diverse prompts, raising concerns about its performance with unfamiliar inputs.
2. The framework struggles with compound expressions, as it relies on LLMs to decompose them, which may lead to incorrect operations if simplification fails.
3. The novelty of the approach is limited, as it combines existing architectures without introducing fundamentally new methodologies.
4. The focus on single-step arithmetic raises concerns about the model's performance on more complex, multi-step problems.
5. The claim that OccamLlama ensures correct arithmetic is challenged, as it does not achieve 100% accuracy on benchmarks.
6. There are concerns about over-engineering a solution that could be addressed with simpler models.

### Suggestions for Improvement
We recommend that the authors improve the robustness of the decoder by training it on a wider variety of prompts to enhance its ability to handle unfamiliar inputs. Additionally, the authors should address the limitations related to compound expressions by providing experimental validation that LLMs can effectively simplify these expressions. It would also be beneficial to include a comparison of OccamNet with other baselines, such as a trained Transformer for multi-step calculations, to better justify the proposed methodology's advantages. Furthermore, we suggest that the authors improve clarity regarding the model's performance on multi-step arithmetic problems by providing additional examples and results. We encourage the authors to clarify the role of the switch mechanism and its potential for improvement in future work, and to provide a deeper experimental analysis to substantiate claims about the advantages of OccamLlama over code generation, particularly addressing concerns about novelty and performance in general mathematical reasoning tasks. Lastly, we recommend that the authors explicitly acknowledge the limitations in achieving 100% correctness on arithmetic tasks.