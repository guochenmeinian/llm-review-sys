ID: 2dw3zQ3nk9
Title: Vript: A Video Is Worth Thousands of Words
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 8, 5, 9, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Vript, a novel video-text dataset comprising 12K high-resolution videos with detailed script-like captions. The authors propose a new model, Vriptor, trained on this dataset, and introduce the Vript-Hard benchmark, which includes three tasks: Vript-HAL (Hallucination Evaluation), Vript-RR (Retrieval and Reasoning), and Vript-ERO (Event Re-ordering). The core contributions are significant for advancing video understanding research, with the dataset achieving state-of-the-art performance among open-source models.

### Strengths and Weaknesses
Strengths:
- The Vript dataset features long, detailed captions, enhancing the quality of video descriptions.
- The introduction of innovative benchmarks addresses critical gaps in existing video understanding tasks.
- Vriptor demonstrates competitive performance in video captioning tasks, indicating the dataset's effectiveness.

Weaknesses:
- The paper lacks detailed statistics about the Vript dataset, such as topic distributions and caption lengths.
- Important information regarding the framework and training pipeline is omitted, leading to ambiguity in concepts like the "three training paradigms."
- The presentation of results is incomplete, particularly in Table 3, where only partial results for Vript-ERO are shown, and inconsistencies in QA ranks are noted.

### Suggestions for Improvement
We recommend that the authors improve the paper's organization by providing a comprehensive description of the data collection and cleaning process, including specific sources and criteria for video selection. Additionally, please include detailed statistics about the Vript dataset, such as instance counts for each benchmark and the number of frames used in evaluations. Clarifying ambiguous concepts, such as the "three training paradigms," and ensuring consistency in the presentation of results will enhance clarity. Lastly, we suggest including representative open-source baselines like PLLaVA and VILA-1.5 in the experiments for a more robust evaluation.