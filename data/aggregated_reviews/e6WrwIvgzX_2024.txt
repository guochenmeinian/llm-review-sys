ID: e6WrwIvgzX
Title: AutoMix: Automatically Mixing Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 5, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AutoMix, a method designed to optimize the performance and computational cost of large language models (LLMs) by adaptively selecting models based on task difficulty. AutoMix employs a few-shot self-verification mechanism to assess the correctness of outputs from smaller models and utilizes a POMDP-based router to determine whether to query a larger model. The experimental results demonstrate that AutoMix can reduce computational costs by over 50% while maintaining performance across various datasets and models.

### Strengths and Weaknesses
Strengths:
- The approach of adaptively selecting model sizes is innovative and effectively reduces overhead by initially using a smaller model.
- The formulation of the query routing optimization problem as a POMDP is insightful, aligning well with the challenges of model performance evaluation during testing.
- The empirical analysis shows that AutoMix outperforms existing baselines, such as FrugalGPT and HybridLLM, across multiple datasets.

Weaknesses:
- The experimental setup lacks rigor, particularly regarding the influence of cost settings on the primary metric, $\Delta_{IBC$, raising concerns about the validity of the conclusions.
- There is insufficient analysis of the absolute performance decay when using smaller models, which is critical as performance often outweighs cost considerations.
- The integration of the POMDP-based router may not significantly enhance performance compared to simpler methods, and the additional learning costs associated with it are not adequately addressed.
- The generalizability of AutoMix is limited, as experiments primarily focus on dialogue and context-grounded reasoning tasks without exploring other task types.

### Suggestions for Improvement
We recommend that the authors improve the rigor of the experimental setup by providing a more detailed analysis of how different cost settings affect the results. Additionally, the authors should include a quantified analysis of the absolute performance decay when adopting outputs from smaller models. Clarifying the conditions under which the POMDP-based router operates effectively and discussing its learning costs in more detail would also strengthen the paper. Finally, expanding the experimental scope to include a wider variety of tasks would enhance the generalizability of AutoMix.