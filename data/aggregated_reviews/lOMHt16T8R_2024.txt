ID: lOMHt16T8R
Title: PaCE: Parsimonious Concept Engineering for Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 3, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Parsimonious Concept Engineering (PaCE), a novel framework for aligning LLMs by modifying their activation space. PaCE constructs a large-scale concept dictionary and partitions concepts into benign and undesirable categories. During inference, it decomposes activations into these concept directions, removing undesirable components to align LLM behavior while preserving linguistic capabilities. The authors demonstrate PaCE's effectiveness in tasks such as response detoxification, faithfulness enhancement, and sentiment revision.

### Strengths and Weaknesses
Strengths:
- The paper proposes a unique activation manipulation framework, distinct from existing methods like vector addition and orthogonal projection.
- The authors clearly articulate the deficiencies of existing methods and how PaCE addresses these issues.
- Validation shows that PaCE outperforms existing methods in achieving alignment goals while maintaining linguistic capabilities.

Weaknesses:
- The main weakness is the efficiency of the framework, as using PaCE results in token predictions taking approximately 2-3 times longer than without it, compared to a 30-40% increase with vector addition.
- The quality and coverage of the concept dictionary may overly influence performance, and the static nature of the dictionary limits handling context-dependent concepts.
- The method does not adequately address polysemy, potentially compromising the accuracy of extracted concept directions.
- There are concerns regarding the misuse of the representation reading algorithm in extracting concept directions, which could lead to inaccurate pairings.

### Suggestions for Improvement
We recommend that the authors improve the discussion on computation time and storage space required for constructing the direction dictionary. Additionally, clarifying why OrthoProj requires significantly more computation time compared to other methods would be beneficial. The authors should also address how PaCE can handle context-dependent concepts and ensure the concept dictionary is comprehensive and free from biases. Furthermore, we suggest including more diverse models in the evaluation and expanding linguistic capability assessments to include generation tasks like open domain QA and summarization. Lastly, enhancing the clarity of the presentation and addressing the computational efficiency of PaCE compared to VecAdd would strengthen the paper.