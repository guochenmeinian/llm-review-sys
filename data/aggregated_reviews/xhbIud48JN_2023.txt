ID: xhbIud48JN
Title: SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 7, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
The paper presents SituatedGen, a generative commonsense benchmark focused on geographical and temporal contexts. It introduces a new task of generating contrastive sentence pairs from existing commonsense benchmarks, utilizing an automatic pipeline for dataset collection. The authors benchmark several generative language models on this dataset, highlighting the importance of external context in understanding commonsense relationships.

### Strengths and Weaknesses
**Strengths:**
1. The dataset is high-quality and meaningful for commonsense reasoning research.
2. The method for automatically collecting contrastive sentences is simple and effective, ensuring minimal supervision.
3. The paper is well-written, with clear definitions and extensive experiments demonstrating the task's difficulty.

**Weaknesses:**
1. The generative models often fail to include the specified keywords in their outputs, likely due to the unrelated nature of the sentences.
2. The evaluation metrics do not adequately capture the specific aspects of contrast tailored for this dataset.
3. The dataset size is limited, with a significant portion being context-dependent antithesis.

### Suggestions for Improvement
The authors should improve the models' performance by providing additional information in the form of a prefix to help instruction-tuned models include keywords in their outputs. They should also explore the robustness of models to small manipulations affecting commonsense relationships, potentially incorporating reasoning steps or explicit classification of keywords into two subgroups before generation. Furthermore, the authors should consider refining evaluation metrics to better assess the contrast between sentences, possibly by computing similarity between entity-masked sentences.