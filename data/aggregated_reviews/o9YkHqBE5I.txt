ID: o9YkHqBE5I
Title: SAM meets Gaze: Passive Eye Tracking for Prompt-based Instance Segmentation
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: 5, 8, 8, -1
Original Confidences: 4, 3, 2, 4

Aggregated Review:
### Key Points
This paper presents the integration of gaze data obtained via passive eye tracking as prompt input for the Segment Anything Model (SAM). The authors investigate two variants of gaze inputs: mask-based and point-based prompts, aiming to fine-tune the prompt encoders and mask decoder of SAM. The study employs a non-interactive training procedure, comparing gaze-based prompts against finetuned versions of SAM using bounding boxes or sampled foreground points. Quantitative results indicate that the fixation point method yields the best segmentation accuracy, although it still lags behind other methods. The paper also notes a 2-second speed improvement in the annotation process at the cost of a 3% decrease in segmentation accuracy.

### Strengths and Weaknesses
Strengths:
- The paper clarifies the application of eye tracking with the SAM model, testing various input methods and comparing results with additional finetuning.
- It stratifies results based on segmentation time relative to object characteristics, providing insights into user variability.

Weaknesses:
- Insufficient details regarding IRB exemption and data collection protocols raise concerns about ethical compliance.
- The segmentation results using eye tracking show a notable difference compared to mouse segmentations, contradicting claims of comparability.
- Timing information lacks usability due to the absence of instant feedback during segmentation, potentially affecting data accuracy.
- The impact of eye tracker calibration on results remains unclear, including time requirements and precision metrics.

### Suggestions for Improvement
We recommend that the authors improve the documentation of the data collection process to address IRB concerns and clarify ethical compliance. Additionally, the authors should provide a more detailed analysis of the timing data and its implications, particularly regarding the lack of instant feedback. It would also be beneficial to discuss the impact of eye tracker calibration on segmentation results, including time and precision metrics. Lastly, we suggest that the authors acknowledge and discuss the findings of the earlier paper "GazeSAM: What You See is What You Segment" to enhance the originality and context of their work.