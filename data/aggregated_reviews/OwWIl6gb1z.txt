ID: OwWIl6gb1z
Title: CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 7
Original Ratings: -1, -1, -1, -1, 8, 8, 5
Original Confidences: -1, -1, -1, -1, 4, 3, 4

Aggregated Review:
### Key Points
This paper presents CRoW, a benchmark for evaluating commonsense reasoning in real-world NLP tasks. The authors propose a multi-stage data collection pipeline that utilizes commonsense-violating perturbations based on Winograd schemas to construct CRoW, which encompasses six practical NLP tasks and six dimensions of commonsense knowledge. The evaluation of various language models on CRoW reveals that they significantly underperform compared to human benchmarks in commonsense reasoning.

### Strengths and Weaknesses
Strengths:  
- The paper effectively bridges the gap between commonsense reasoning and practical NLP applications, providing valuable insights into real-world implications.  
- The CRoW dataset is diverse, covering six NLP tasks and dimensions of commonsense knowledge, with clear annotation guidelines and robust quality verification.  
- The methodology for data collection and evaluation is comprehensive, with a thorough analysis of model performance across different commonsense dimensions.

Weaknesses:  
- The clarity of the paper's structure and statements is lacking, making it challenging to follow; for instance, inconsistencies exist regarding task classifications as binary or multi-choice.  
- The human evaluation is limited to only two annotators, raising concerns about the reliability and rigor of the evaluation process.  
- The performance analysis of language models lacks depth, particularly in distinguishing between different commonsense dimensions and justifying task selections.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper's structure and statements, particularly regarding the classification of tasks. Additionally, increasing the number of annotators in the human evaluation would enhance the reliability of the results. We suggest providing more detailed performance analysis across commonsense dimensions and justifying the selection of tasks included in CRoW. Furthermore, including examples of curated data would help illustrate the uniqueness of the data generation approach. Lastly, expanding the analysis of the performance differences between models and incorporating more in-depth tasks could further demonstrate the reasoning capabilities of the LLMs.