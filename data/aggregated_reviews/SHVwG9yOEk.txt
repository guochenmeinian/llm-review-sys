ID: SHVwG9yOEk
Title: Hyperbolic Graph Neural Networks at Scale: A Meta Learning Approach
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 5, 5, 8, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 2, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Hyperbolic GRAph Meta Learner (H-GRAM), a model designed to learn transferable information from local subgraphs using hyperbolic meta gradients and label hyperbolic protonets, facilitating faster learning over disjoint query subgraphs. The authors evaluate H-GRAM on node classification and link prediction tasks, demonstrating its effectiveness in few-shot settings and its superiority over Euclidean models. The work addresses the scalability of hyperbolic neural networks (HNNs) to large graphs and unseen graphs, proving that local neighborhood information suffices for node classification and link prediction.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and introduces novel contributions, particularly in its approach to scalability and generalization of HNNs.  
- H-GRAM consistently outperforms baseline models in link prediction and node classification tasks, supported by informative ablation studies.  
- The theoretical justification and comprehensive experiments across various datasets enhance the paper's quality.

Weaknesses:  
- The Related Work section lacks a critical reference on HNNs for large-scale datasets.  
- The problem setup needs clearer explanations regarding graph properties.  
- The experiments should include performance metrics for both inductive and transductive tasks, along with detailed dataset statistics.  
- Concerns about model scalability and computational complexity remain unaddressed, particularly regarding Mobius computations.  
- The introduction contains excessive detail about model architecture, detracting from the key innovations.  
- Some tables lack standard deviation data, and comparisons with other meta-learning approaches are insufficient.

### Suggestions for Improvement
We recommend that the authors improve the Related Work section by including the missing reference on HNNs for large datasets. Additionally, clarify the properties of the graph in the problem setup. It would be beneficial to provide performance metrics on both inductive and transductive tasks and include detailed dataset statistics. We also suggest conducting a model complexity analysis to address scalability concerns. Furthermore, we advise the authors to streamline the introduction by focusing on key innovations and to ensure all tables include standard deviation data. Lastly, a broader comparison with other meta-learning approaches would provide valuable context for H-GRAM's performance.