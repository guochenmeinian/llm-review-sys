ID: Y6IGTNMdLT
Title: Model Shapley: Equitable Model Valuation with Black-box Access
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 4, 6, 4, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on model valuation, introducing the *Model Shapley Value* based on the classic Shapley Value from cooperative game theory. The authors propose a *Dirichlet abstraction* to facilitate efficient comparisons of predictive models and explore practical application scenarios in model marketplaces, particularly focusing on query sets and blackbox access. They empirically evaluate their method on datasets such as MNIST and CIFAR-10, identifying three main challenges in model valuation: developing suitable abstractions, ensuring equitability properties, and leveraging Shapley value in large marketplaces. Theorem 1 is introduced to demonstrate the learnability of model Shapley through its Lipschitz continuity, with Gaussian process regression (GPR) suggested as a suitable learning approach. The authors suggest a learning approach for predicting model Shapley values (MSVs) based on a small subset of models.

### Strengths and Weaknesses
Strengths:
- The problem of valuing predictive models is well-motivated, with a clear background and supporting statements.
- The proposed solution is solid and well-thought-out, utilizing Dirichlet distributions innovatively.
- The paper introduces a novel theoretical framework that addresses relevant questions in model marketplaces.
- The empirical analyses are clearly explained, and the paper is well-structured and written, facilitating a good read.
- The authors provide clear definitions and clarifications regarding key concepts such as "task" and "query set."
- The use of Theorem 1 to establish the learnability of model Shapley is well-articulated, with a solid connection to empirical evaluations.

Weaknesses:
- The presentation is a substantial barrier to understanding, with a lack of formal description of the model valuation task.
- The description of Dirichlet abstractions is unclear, raising questions about its purpose and implementation.
- The paper assumes the availability of a large query set for accurate maximum likelihood estimation (MLE), neglecting scenarios with sparse or unbalanced data.
- There is insufficient discussion on the trade-off between abstraction level and query set size, and the assumption of model homogeneity post-abstraction may not hold in practice.
- The relationship between the theoretical guarantees of Theorem 1 and practical learning approaches could be further elaborated.
- The paper lacks a comparison with existing model valuation methods and does not explicitly address its limitations or broader societal impacts.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by formally defining the model valuation task, including input, output, and trivial solutions. Additionally, we suggest providing a clearer explanation of the Dirichlet abstraction, including its purpose and justification. The authors should address the implications of sparse or unbalanced data on their method and provide guidelines for determining the optimal trade-off between abstraction level and query set size. Furthermore, a comparative study with existing model valuation techniques would enhance the paper's contribution. We also recommend that the authors improve the clarity of the formal problem statement by explicitly detailing the inputs (i.e., $N$ classification models) and desired outputs (i.e., an equitable valuation function $\phi_i$). It would be beneficial to clearly define the list of notations in the Appendix and ensure that $\mathcal{Q}^*$ is defined prior to its formal use. Lastly, we suggest that the authors further elaborate on how the theoretical guarantees of Theorem 1 connect to practical learning approaches, particularly in the context of GPR.