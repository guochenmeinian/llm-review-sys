ID: 7sjLvCkEwq
Title: Discriminative Entropy Clustering and its Relation to K-means and SVM
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 4, 5, 7, 3, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the relationship between the regularized information maximization (RIM) clustering framework and K-means and SVM-based clustering methods, revealing stronger similarities to SVM-based clustering. The authors propose a new loss function and an associated EM optimization algorithm that utilizes reverse cross-entropy/KL divergence to achieve more robust and fair clustering, demonstrating improved performance on several balanced image classification benchmarks. Additionally, the paper evaluates a self-labeling mechanism in entropy clustering, showing improved results while maintaining consistency across datasets. The authors claim that their method outperforms prior approaches, achieving top performance on nearly all benchmarks and providing a comprehensive empirical evaluation.

### Strengths and Weaknesses
Strengths:  
1. The authors identified an error regarding the missing normalization term in the proof of the equivalence theorem from Jabi et al. (2021).  
2. The novel use of reverse cross-entropy in the RIM loss has the potential to effectively address the impact of uncertain/noisy pseudo-labels.  
3. The proposed method shows improved performance on various image classification benchmarks compared to several baselines.  
4. The method demonstrates consistent top performance across various datasets, indicating robustness.  
5. The authors provide a mathematically transparent formulation that enhances understanding of the improvements.  
6. Comprehensive comparisons with prior methods highlight the advantages of their approach.

Weaknesses:  
1. The manuscript is poorly written, with excessive discussion on general background that obscures the actual contributions. Key terms like H and KL divergence are not clearly defined, complicating understanding. The conceptual and algorithmic contributions appear independent, lacking clarity on their interrelation.  
2. The disproof of the equivalence theorem in Jabi et al. (2021) lacks convincing evidence, as it does not rule out the validity of the equivalence itself. The focus on the standard K-means objective contrasts with the soft and regularized K-means loss considered by Jabi et al. (2021).  
3. The claim linking L2 regularization to margin maximization is questionable, as margin maximization is a property of the loss function, not the regularization.  
4. Experimental validation is limited, with a need for a more comprehensive evaluation of the proposed loss function modifications across diverse datasets and clustering scenarios. The authors only tested on balanced datasets with ground truth cluster numbers, which limits robustness.  
5. Tables 2 and 3 show results that are mostly within standard errors, raising questions about the significance of improvements.  
6. There is a lack of clarity regarding the contributions of the self-labeling mechanism versus changes to the KL-term.

### Suggestions for Improvement
We recommend that the authors improve the manuscript's clarity by reducing background discussions and explicitly defining key terms. The authors should ensure that the conceptual and algorithmic contributions are clearly linked. Additionally, we suggest providing a more convincing argument regarding the disproof of the equivalence theorem and addressing the questionable claim about L2 regularization. Expanding the experimental validation to include unbalanced datasets and varying numbers of clusters would enhance robustness. Standardizing experimental settings, including model architecture, is essential for comparability with existing results. Furthermore, we recommend that the authors improve the clarity of their results by unifying the labels in Tables 2, 3, and 4 to better distinguish between the contributions of the self-labeling mechanism and the KL-term modifications. Providing more explicit comparisons with earlier works could strengthen the argument for the effectiveness of their method.