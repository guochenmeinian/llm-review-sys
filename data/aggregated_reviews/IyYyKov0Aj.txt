ID: IyYyKov0Aj
Title: Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 8, 5, 5, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Conditional Adapter (CODA), which enhances parameter-efficient fine-tuning and improves inference speed. CODA employs a lightweight routing adapter path for all tokens while selectively routing certain tokens through the original path using a “Soft Top-K” operation, derived from an optimization problem involving a generalized entropy function. Experimental results across NLP, vision, and speech tasks demonstrate significant inference acceleration with minimal performance degradation.

### Strengths and Weaknesses
Strengths:
1. The concept of CODA is both simple and powerful, supported by extensive experimental validation.
2. The “Soft Top-K” operation is a unique and effective approach, outperforming traditional methods like sigmoid gating.
3. The paper is well-written and presents a clear methodology, showcasing versatility across multiple modalities.

Weaknesses:
1. The discussion on the differences between CODA and CoLT5 is insufficient.
2. The paper lacks thorough comparisons with existing parameter-efficient fine-tuning and dynamic inference methods.
3. The organization of the paper deteriorates after Section 3.2, affecting clarity.
4. The relationship between the adapter and inference acceleration is not clearly established, making it seem like a combination of two independent methods.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the differences between CODA and CoLT5 for clarity. Additionally, a more thorough comparison with existing parameter-efficient fine-tuning methods and dynamic inference techniques would strengthen the paper. We suggest enhancing the organization of the paper post-Section 3.2 to improve readability. Furthermore, addressing the clarity of the relationship between the adapter and inference acceleration would provide a more cohesive understanding of the contributions. Lastly, we expect CODA to also reduce memory consumption for both training and inference, and a comparison regarding memory usage would enhance the paper's value.