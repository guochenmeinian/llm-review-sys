ID: mjzss9Xg76
Title: GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs
Conference: ACM
Year: 2024
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the GraphCLIP framework, which addresses the challenges of transferability in Graph Foundation Models (GFMs) for Text-Attributed Graphs (TAGs). The authors propose a self-supervised contrastive graph-summary pretraining method that utilizes Large Language Models (LLMs) to generate graph-summary pairs, enhancing cross-domain zero/few-shot transferability. The research quality is high, demonstrated through extensive experimentation against 17 baselines, validating GraphCLIP's effectiveness across various settings. The paper is well-structured and clearly written, with a comprehensive introduction and detailed methodology.

### Strengths and Weaknesses
Strengths:
1. The innovative use of LLMs for generating graph-summary pairs addresses limitations of existing methods.
2. The integration of invariant learning into the contrastive loss enhances model transferability and generalization.
3. Extensive experiments across 12 datasets validate the framework's performance in zero-shot and few-shot scenarios.

Weaknesses:
1. Some mathematical formulations are dense and may be difficult for readers to grasp; more intuitive examples are needed.
2. The paper lacks a deep comparison with generative models, particularly regarding the impact of using source data for pre-training LLMs.
3. The graph summary generation process raises concerns about hallucination issues and the quality of responses from the LLM, which could affect model performance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of mathematical formulations by providing more intuitive examples. Additionally, the authors should address the potential hallucination issues in the graph-summary generation process and clarify how they evaluate the accuracy of the LLM's responses. It would be beneficial to provide specific examples of how GraphCLIP is tested on tasks like node classification and link prediction, including task-specific instructions. Furthermore, we suggest that the authors explore alternative invariant learning techniques and different text encoders to assess their impact on performance. Lastly, clarifying the notation in the equations and expanding the related work section to include studies on graph contrastive learning would enhance the paper's comprehensiveness.