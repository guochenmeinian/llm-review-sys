ID: T1lFrYwtf7
Title: Latent Paraphrasing: Perturbation on Layers Improves Knowledge Injection in Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 5, 6, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LaPael, a novel method for enhancing knowledge injection in large language models (LLMs) by introducing a latent paraphraser layer. Unlike traditional methods that rely on explicit paraphrasing, LaPael operates at the latent level, minimizing the need for high-quality paraphrased data and reducing computational costs. The authors demonstrate that LaPael outperforms standard fine-tuning and paraphrasing approaches across three question-answering datasets, including SQuAD, StreamingQA, and ArchivalQA.

### Strengths and Weaknesses
Strengths:
1. The introduction of a latent paraphraser layer is a significant innovation that reduces the reliance on costly paraphrased data.
2. The empirical results indicate that LaPael consistently outperforms existing methods, supported by extensive ablation experiments that clarify the contributions of various components.

Weaknesses:
1. The distinction between LaPael and previous perturbation methods in the feature space is inadequately explained.
2. The assumption of a Gaussian output distribution for Transformers in Section 4.2 lacks justification, and the implications of directly calculating KL divergence without considering distribution types need clarification.
3. The paper does not adequately address the computational cost of training the latent paraphraser compared to traditional methods, nor does it provide a thorough comparison with paraphrasing methods regarding computational efficiency.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the differences between LaPael and existing perturbation methods. Additionally, the authors should justify the assumption of a Gaussian output distribution in Section 4.2 and discuss the implications of their KL divergence calculations. A comparison of the computational costs between LaPael and traditional paraphrasing methods should be included to provide a clearer understanding of efficiency. Finally, addressing the potential negative impact of LaPael on the existing knowledge of LLMs would enhance the paper's depth.