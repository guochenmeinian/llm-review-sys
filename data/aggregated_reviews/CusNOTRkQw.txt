ID: CusNOTRkQw
Title: Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 5, 4, 5, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach for enhancing test-time prompting through distribution alignment in Vision-Language (V-L) models, termed PromptAlign. The authors propose a distribution alignment loss that utilizes offline computed statistics from a proxy source dataset, specifically ImageNet, to align test sample token distributions with source data distributions. Evaluation results indicate improved performance on domain generalization and cross-dataset generalization tasks compared to existing methods.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and easy to follow, with extensive experiments validating the proposed approach.
- The concept of combining test-time adaptation with distribution alignment is technically sound and holds practical value.
- PromptAlign demonstrates consistent performance improvements over baseline methods in various benchmark datasets.

Weaknesses:
- The method heavily relies on ImageNet statistics, raising concerns about its applicability to other V-L models beyond CLIP and the potential for overfitting due to reliance on mean and variance of single test examples.
- The novelty of the approach is somewhat low, as it primarily combines existing methods (MaPLE and TPT) with an additional alignment loss.
- There is a lack of theoretical justification for the choice of proxy source dataset and the distribution alignment strategy, which could benefit from deeper analysis and insights.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for using ImageNet as a proxy source dataset and explore the implications of using alternative datasets like LAION400M. Additionally, we suggest providing more insights into the choice of mean and variance for distribution alignment and considering the inclusion of more sophisticated statistics or losses. Furthermore, we encourage the authors to conduct additional evaluations against state-of-the-art methods and clarify the performance of the proposed method across different CLIP backbone architectures. Lastly, a FLOPs and compute analysis should be included to assess the efficiency of the proposed method compared to TPT.