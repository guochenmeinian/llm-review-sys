ID: g1Q9Uu8lCp
Title: xDial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents xDial-Eval, a multilingual open-domain dialogue evaluation benchmark that includes datasets, evaluation methods, and an analysis of current metrics. The authors propose a resource contribution by translating 18 English dialogue datasets into 9 languages and evaluating various BERT-based and generation-based LLMs on this benchmark. Additionally, the paper introduces an ensemble metric that combines generative and discriminative models.

### Strengths and Weaknesses
Strengths:  
- The construction of xDial-Eval includes 12 turn-level and 6 dialogue-level open-source datasets, demonstrating a good choice of languages and types of LLMs for comparison.  
- The paper addresses a gap in the evaluation of multilingual open-domain conversations, contributing to the field.  
- It is well organized and clearly written.

Weaknesses:  
- The reliance on machine translation models raises concerns about their impact on dataset quality and the evaluation process.  
- The focus on coherence in the proposed dataset overlooks other important properties of dialogue evaluation.  
- The evaluation metrics are primarily based on automatic translations, lacking sufficient human evaluation to assess quality.  
- Some claims are not sufficiently supported, and the reproducibility of results is hindered by underspecified parameters and limited access to training/evaluation data.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by incorporating human annotations to assess the quality of the machine-translated datasets. Additionally, please clarify how human evaluations are obtained and provide scores from the metrics used for correlation calculations in the experimentation section. It would be beneficial to mention the languages in the abstract and address the potential bias in results due to similar translation methods used for training and evaluation. Finally, we suggest providing more details on the parameter settings to enhance reproducibility.