ID: OOItbUUQcd
Title: A Cross-Domain Benchmark for Active Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 6, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new benchmark for active learning across three domains: vision, language, and tabular datasets. The authors propose a subset selection algorithm to serve as an upper bound for active learning performance. Key findings indicate that averaging over only three trials can significantly hinder accuracy in relative performance assessments, and that the best active learning strategy varies across domains, suggesting the need for domain-specific algorithms.

### Strengths and Weaknesses
Strengths:
- The benchmark is comprehensive, covering multiple domains and allowing for extensive experimental runs.
- It highlights the importance of multiple runs for meaningful conclusions and reveals that state-of-the-art methods may underperform in this benchmark.
- The inclusion of synthetic datasets and a new oracle evaluation algorithm adds value to the framework.

Weaknesses:
- The contribution of synthetic benchmarks is unclear, as results are unsurprising and demonstrate that uncertainty sampling methods fail under adversarial conditions.
- The greedy oracle algorithm lacks clear guarantees, and its description is difficult to follow.
- The evaluation relies on relatively small models and datasets, raising questions about the applicability of results to larger contexts.

### Suggestions for Improvement
We recommend that the authors improve the discussion of related benchmarks, particularly by including studies like [1] and [2], which explore active learning in tabular datasets and semi-supervised learning. Additionally, we suggest revising the proposed metric of average ranking to better reflect accuracy and label-efficiency, as it currently penalizes algorithms that perform relatively worse without capturing their actual performance. Furthermore, we encourage the authors to provide clearer explanations of the oracle and its role earlier in the manuscript, and to address the choice of validation on the entire dataset, as this is a significant critique in active learning research. Lastly, we recommend enhancing the clarity of figures and tables, particularly regarding legends and terminology.