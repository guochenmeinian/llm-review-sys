ID: vZHk1QlBQW
Title: ForkMerge: Mitigating Negative Transfer in Auxiliary-Task Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 7, 6, 7, 4, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 2, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach called ForkMerge, aimed at improving performance on target tasks by leveraging auxiliary tasks while mitigating negative transfer. The method involves a dual optimization process where model parameters are forked and updated separately for the target and auxiliary tasks, with a dynamic adjustment of task weights based on validation accuracy. The authors conduct extensive experiments across various benchmarks, demonstrating the effectiveness of ForkMerge compared to existing methods. Additionally, the paper provides a comprehensive analysis of the model's sensitivity to hyperparameters, particularly task weights in auxiliary-task learning. The authors propose that these weights are crucial for optimizing model performance, as evidenced by various methodologies such as Uncertainty Weighting (UW), Dynamic Weighted Averaging (DWA), Gradient-Cosine Similarity (GCS), and Auto-$\lambda$. They explore the implications of task weights from both multitask optimization and distribution perspectives, emphasizing the need for a balance to avoid overfitting and negative transfer. The performance of larger models, specifically ViT-Base, is also investigated in relation to the ForkMerge approach, which consistently outperforms other methods.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant challenge in auxiliary task learning, providing a well-motivated and intuitive solution.
- The analysis of task interference and distribution shifts offers valuable insights into the underlying issues of negative transfer.
- Extensive empirical evaluations across multiple datasets reinforce the proposed method's effectiveness.
- The paper provides a thorough exploration of task weight sensitivity and its impact on model performance.
- The authors effectively utilize various methodologies to support their findings.
- Empirical results demonstrate the robustness of the ForkMerge approach across different architectures.

Weaknesses:
- The computational cost associated with the ForkMerge approach, particularly regarding memory efficiency and the number of forward/backward passes, is not adequately discussed.
- The connection between the observations in Section 3 and the proposed algorithm lacks clarity, limiting the perceived contribution.
- Some critical details are omitted from the main text, which affects the overall understanding of the method and its implications.
- The distinction between distribution and hypothesis shift remains unclear, leading to potential confusion.
- The definitions of $\lambda$ and $\Lambda$ are inconsistent, which may hinder reader comprehension.
- The motivation for Equation (13) lacks clarity, particularly regarding its practical implications.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the computational overhead of ForkMerge, particularly in terms of time and memory efficiency, and clarify how the method filters out harmful parameter updates post-merging. Additionally, we suggest providing a theoretical formulation that connects the observations in Section 3 to the ForkMerge algorithm to strengthen the contribution. Furthermore, enhancing clarity in the presentation by reducing the use of abbreviations and ensuring that all critical details are included in the main text would significantly improve readability. We also recommend improving the clarity surrounding the difference between distribution and hypothesis shift to reduce confusion. Redefining the intuition of $\lambda$ as a distribution mixture coefficient to align with its usage throughout the paper would be beneficial. It would also be advantageous to better formalize the connection between "mixture of losses" and "mixture of distributions." Lastly, we advise rephrasing the introduction of Equation (13) to highlight its role in pruning and efficiency, as well as to mention the extended time for lambda estimation and enhanced flexibility compared to Equation (12).