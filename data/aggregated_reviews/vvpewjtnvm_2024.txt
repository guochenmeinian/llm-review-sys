ID: vvpewjtnvm
Title: Low Precision Local Training is Enough for Federated Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an efficient federated learning (FL) paradigm that utilizes low-precision operations for local model training while employing high-precision computation for model aggregation at the server. The authors claim that this approach not only maintains comparable performance to full-precision training but also alleviates overfitting issues. The method is theoretically proven to converge, even with non-IID data distributions, and extensive experiments validate its effectiveness.

### Strengths and Weaknesses
Strengths:  
1. The application of low-precision training within each cycle for local FL is both meaningful and effective.  
2. The paper includes theoretical analysis on convergence.  
3. Comprehensive experiments demonstrate the proposed method's effectiveness.  

Weaknesses:  
1. The novelty of the method is questionable, particularly in relation to SWALP, as significant sections of the paper closely resemble prior work.  
2. Writing quality requires improvement, with several typographical errors and unclear equations.  
3. The experiments are limited to image datasets, lacking evaluation on other data types, and there is insufficient discussion of existing literature on quantization in FL.  

### Suggestions for Improvement
We recommend that the authors improve the novelty justification against SWALP, particularly addressing concerns regarding the similarities in Sections 3.2 and 4.1. Additionally, clarify the differences in Equation (5) and ensure all equations are clearly defined. The authors should enhance the writing quality by correcting typographical errors and ensuring clarity in the presentation of algorithms and equations. Expanding the experimental evaluation to include diverse data types and integrating discussions on existing quantization techniques in FL would strengthen the paper. Lastly, consider exploring mixed precision strategies and the implications of low-precision training on privacy and security aspects in federated learning.