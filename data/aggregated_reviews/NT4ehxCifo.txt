ID: NT4ehxCifo
Title: Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the use of ChatGPT for out-of-domain (OOD) Intent Discovery and General Intent Discovery (GID) tasks. The authors find that while ChatGPT performs adequately in zero-shot scenarios, it does not outperform fine-tuned models. The paper presents challenges associated with using ChatGPT for clustering tasks and suggests future research directions. It also proposes heuristic prompts and conducts qualitative analyses to understand the factors affecting ChatGPT's performance.

### Strengths and Weaknesses
Strengths:
- This paper is the first to comprehensively evaluate ChatGPTâ€™s performance on OOD Intent Discovery and GID tasks.
- The qualitative analyses provide valuable insights into the sensitivity of ChatGPT to prompts and the impact of in-domain knowledge.
- The presentation is clear and relatively easy to follow.

Weaknesses:
- The contributions are somewhat limited, as the findings regarding ChatGPT's performance on clustering tasks are not particularly novel.
- The evaluation relies on a single dataset and straightforward prompting strategies, raising concerns about generalizability and robustness.
- There is insufficient analysis of the reasons behind ChatGPT's performance, particularly regarding the counterintuitive results observed.

### Suggestions for Improvement
We recommend that the authors improve the analysis of prompt engineering to clarify whether ChatGPT's performance issues stem from suboptimal prompts or inherent limitations of LLMs. Additionally, we suggest that the authors explore variations in prompt phrasing and exemplar choice to assess the generalizability of their findings. It would also be beneficial to provide a more thorough investigation into the reasons for ChatGPT's poorer performance with more in-domain knowledge. Finally, we urge the authors to make their code, data, and evaluation scripts publicly available to enhance reproducibility.