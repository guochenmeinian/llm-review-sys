ID: jTiJPDv82w
Title: ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TOXICCHAT, a benchmark for toxicity detection derived from real-world human-AI interactions, utilizing a human-AI collaborative annotation framework to classify prompts as toxic, non-toxic, or uncertain. The dataset includes specific labeling for "jailbreaking" prompts. The authors conduct experiments on existing toxicity classifiers, revealing their inadequate performance on TOXICCHAT, while fine-tuning on this dataset enhances performance on a real-world validation set.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant issue in detecting toxic inputs to chatbots, particularly with the inclusion of jailbreaking prompts, which have not been extensively studied in this context.
- The dataset is valuable and demonstrates improved performance of models trained on TOXICCHAT compared to existing benchmarks.

Weaknesses:
- There is a lack of in-depth analysis of the collected toxic prompts, particularly regarding the diverse types of jailbreaking prompts and the failure modes of LLMs.
- The motivation and writing are unclear, with a primary focus on prompt toxicity detection rather than user-AI interaction as suggested by the title.
- The experiments utilize weaker toxicity detection methods, primarily open-source APIs or fine-tuned RoBERTa models, neglecting recent advancements in LLMs that outperform previous state-of-the-art baselines.
- The jailbreaking prompts constitute only 3.89% of the dataset, raising concerns about their representativeness.

### Suggestions for Improvement
We recommend that the authors improve the analysis of the collected toxic prompts, including a detailed examination of the fine-grained types of jailbreaking prompts and LLM failure modes. Additionally, we suggest clarifying the motivation and focus of the paper to better align with the title, possibly by incorporating experiments on previous dialogue safety benchmarks. The authors should also explore stronger toxicity detection methods, leveraging recent advancements in LLMs. Finally, we advise increasing the representation of jailbreaking prompts in the dataset and providing clearer explanations of the experimental results, including error measures and cross-domain analyses.