ID: JMuKfZx2xU
Title: On Slicing Optimality for Mutual Information
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 5, 6, 6, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new dependence measure called optimal sliced mutual information ($\mathcal{SI}^*$), which improves upon the existing sliced mutual information ($\mathcal{SI}$) by optimizing the distribution of slicing directions. The authors formalize this optimization as a regularized problem, demonstrating that $\mathcal{SI}^*$ retains relevant information while being scalable to high dimensions. The paper claims that $\mathcal{SI}^*$ outperforms $\mathcal{SI}$ in various machine learning tasks, including representation and reinforcement learning.

### Strengths and Weaknesses
Strengths:  
- The paper is well-structured and clearly written, providing a solid introduction and motivation for the proposed measure.  
- The theoretical analysis is detailed, and the existence of an optimal slicing policy is a significant contribution.  
- Empirical results demonstrate the effectiveness of $\mathcal{SI}^*$ across different domains, indicating its practical applicability.

Weaknesses:  
- The justification for the two criteria of optimality in the definition of $\mathcal{SI}^*$ is inadequate, particularly regarding the importance of maximally informative regions and the necessity of slice diversity.  
- The experiments primarily focus on low-dimensional cases, lacking scalability analysis in higher dimensions.  
- Certain sections, such as the neural network estimator and experimental setup, require clearer explanations.  
- The claim of better sample and slice efficiency lacks theoretical backing, as the convergence rate appears similar to that of $\mathcal{SI}$.  
- The paper's evaluation against state-of-the-art methods is insufficient, particularly in representation learning tasks.

### Suggestions for Improvement
We recommend that the authors improve the justification for the criteria of optimality in $\mathcal{SI}^*$, clearly explaining the relevance of maximally informative regions and the need for diverse slices. Additionally, we suggest conducting experiments in higher dimensions to assess the scalability of $\mathcal{SI}^*$ effectively. The authors should provide clearer explanations of the neural network estimator and the experimental setup to enhance readability. We also recommend including theoretical results supporting the claims of sample and slice efficiency. Finally, a more comprehensive evaluation against state-of-the-art methods in representation learning is necessary to substantiate the significance of the results.