ID: 6lXuQBMsyM
Title: DetGPT: Detect What You Need via Reasoning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new task called reasoning-based object detection, introducing a method named DetGPT that employs vision-language models (VLMs) to interpret high-level language instructions and localize objects in images. The authors also introduce a dataset, RDBench, to evaluate the effectiveness of DetGPT, which resolves indirect queries by prompting a large language model (LLM) and fine-tuning a VLM. Evaluations are conducted on both in-domain (COCO) and out-of-domain (Object365) images, exploring various LLMs and the significance of LLM-based reasoning.

### Strengths and Weaknesses
Strengths:
- The task of localizing objects based on abstract language descriptions is innovative and relevant to the vision-language community.
- The proposed method shows potential for integration into robotic systems, enhancing user interaction through high-level task goals.
- The paper is well-organized, clearly written, and provides comprehensive results with various language models as backbones.

Weaknesses:
- The quality of the automatically generated dataset raises concerns about the reasoning and commonsense knowledge required for effective object detection.
- The absence of a majority-class baseline and comparisons to existing methods limits the evaluation's significance.
- The theoretical contribution is limited, as the network design and training procedure largely follow previous work, with insufficient evidence demonstrating the advantages of the trained VLM over existing models.

### Suggestions for Improvement
We recommend that the authors improve the analysis of the dataset by providing a breakdown of model performance based on different question types and difficulties. Additionally, we suggest including a majority-class baseline in the evaluations and investigating whether training on RDBench affects performance on direct queries. It would also be beneficial to compare the proposed method against existing benchmarks or methods, such as OK-VQA, to establish significance. Lastly, we encourage the authors to provide more evidence on the advantages of their VLM compared to existing LLMs capable of composing vision-and-language models.