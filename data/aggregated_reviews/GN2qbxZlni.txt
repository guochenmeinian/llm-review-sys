ID: GN2qbxZlni
Title: MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in LLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MR.BEAN, a benchmark designed to evaluate the meta-reasoning capabilities of large language models (LLMs). It emphasizes the models' ability to detect and correct errors in reasoning steps, moving beyond traditional outcome-based evaluations. The benchmark comprises 6,006 questions across various subjects, requiring LLMs to analyze and rectify errors in automatically generated reasoning. The authors conducted extensive evaluations of 15 LLMs, revealing significant limitations in their reasoning abilities and highlighting the potential for improvement through high-quality synthetic data.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel benchmark, MR.BEAN, focusing on meta-reasoning, which assesses the reasoning process rather than just final outcomes.
- The dataset is comprehensive, covering diverse subjects and ensuring high-quality evaluations through expert annotations.
- The extensive empirical study reveals previously unidentified weaknesses in LLM reasoning, providing valuable insights for future AI development.

Weaknesses:
- The $ACC_{reason}$ metric shows inconsistencies due to its reliance on varying judgments from LLMs or human evaluators.
- There is insufficient validation of the model's automatic annotations against human evaluations, raising concerns about reliability.
- The paper lacks transparency regarding the annotation process, including details on annotator qualifications and inter-annotator agreement rates.
- The combination of metrics into a single MR-score may obscure individual model performance across different reasoning aspects.

### Suggestions for Improvement
We recommend that the authors improve the validation of the MR metric by incorporating human evaluations to support its credibility. Additionally, providing more transparency regarding the annotation process, including inter-annotator agreement rates, would enhance the reliability of the findings. The authors should also clarify the rationale behind combining the three metrics into a single MR-score and consider presenting them individually for clearer insights into model performance. Finally, including specific test cases using GPT-4 could demonstrate the benchmark's effectiveness in identifying strengths and weaknesses in state-of-the-art models.