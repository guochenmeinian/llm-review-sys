ID: YXXmIHJQBN
Title: 4DBInfer:  A 4D Benchmarking Toolbox for Graph-Centric Predictive Modeling on RDBs
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 7, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents 4DBInfer, a benchmark for GNN-based predictive modeling on relational databases (RDBs), detailing how to apply GNNs for predictive tasks. It includes a systematic evaluation framework, diverse datasets, and extensive experimental results demonstrating the effectiveness of GNN methods compared to traditional approaches. The benchmark is publicly available with comprehensive documentation, aiming to enhance research in RDB methods. Additionally, the authors address challenges in running experiments in lower resource environments by proposing down-sampling larger datasets to create manageable temporal splits without compromising task integrity, as demonstrated in the Illinois Graph Benchmark. They note that some benchmarks, particularly those using the Seznam dataset, are relatively small and can be executed on standard laptops with approximately 1GB of memory.

### Strengths and Weaknesses
Strengths:
- The benchmark features large, open RDBs with clear tasks, significantly contributing to future research quality.
- Extensive experimental section with interesting baselines and ablation studies.
- Well-documented repository with tutorials and installation guides.
- The authors provide a clear rationale for down-sampling datasets, enhancing accessibility for users with limited resources.
- The inclusion of detailed peak RAM usage data for various benchmarks improves transparency regarding resource requirements.

Weaknesses:
- The paper suffers from poor presentation, with dense and sometimes unclear writing.
- It shows a bias towards GNNs, neglecting alternative methods and their drawbacks.
- Lack of information on computational resources required to run the benchmark.
- The initial communication regarding the resource requirements of benchmarks was unclear, potentially leading to misconceptions about their accessibility.
- The lack of specific details on which tasks are less resource-intensive may deter some users from engaging with the benchmarks.

### Suggestions for Improvement
We recommend that the authors improve the presentation by simplifying the writing to enhance clarity and readability. Additionally, we suggest including a discussion on alternative methods for generating graph embeddings and converting tabular data into graph form to mitigate perceived bias towards GNNs. Please add a section detailing the computational resources necessary for running the benchmark, as this information is crucial for potential users. Furthermore, clarifying the tasks evaluated in the benchmark and the meaning of "billion-scale" in Figure 1 would enhance understanding. Lastly, we recommend improving the clarity of resource requirements by explicitly stating which tasks are cheaper to run and adding peak RAM usage to Table 2 in Appendix 2 for a clearer overview. Ensure that the text reflects the availability of smaller benchmarks to prevent readers from dismissing the benchmarks as universally resource-intensive.