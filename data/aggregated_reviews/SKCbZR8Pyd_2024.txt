ID: SKCbZR8Pyd
Title: SpeechAlign: Aligning Speech Generation to Human Preferences
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 8, 8, 5, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for enhancing speech generation in speech language models through preference optimization techniques. The authors propose using a dataset of "gold" speech tokens from a neural codec model, contrasting them with synthetic tokens from a speech generation model. Various preference optimization strategies, including RLHF-PPO and DPO, are evaluated, with DPO showing the best performance. The iterative training process further refines the model using a challenging preference dataset, leading to improved speech generation quality.

### Strengths and Weaknesses
Strengths:
- The integration of human feedback to align speech outputs with preferences is a novel approach yielding convincing results.
- The study effectively demonstrates the distribution mismatch between gold and synthetic tokens, providing valuable insights.
- A thorough evaluation of multiple preference optimization algorithms is conducted, supported by extensive experimental results.

Weaknesses:
- The relationship between the preliminary analysis of the 'distribution gap' and SpeechAlign is unclear.
- The presentation of Best-of-N Sampling (BoN) is confusing, as it appears more aligned with decoding than model alignment.
- The description of preference optimization algorithms is difficult to understand without prior knowledge of relevant literature.
- Variance in results is not reported, despite multiple evaluations of each model.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the relationship between the distribution gap and SpeechAlign. Additionally, the presentation of Best-of-N Sampling should be revised to clarify its role in the context of preference optimization. It would be beneficial to include variance in the results to provide a more comprehensive understanding of the model's performance. Furthermore, a discussion of the vocoder used for generating speech from acoustic tokens should be included, and the authors should consider sharing the preference optimization datasets. Lastly, addressing the scalability of the methodology with larger datasets and evaluating its performance against recent personalized speech synthesis technologies would enhance the paper's contributions.