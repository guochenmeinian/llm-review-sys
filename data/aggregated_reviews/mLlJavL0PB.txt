ID: mLlJavL0PB
Title: InstructExcel: A Benchmark for Natural Language Instruction in Excel
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "INSTRUCT EXCEL," a benchmark dataset designed for evaluating language models' interactions with Microsoft Excel. It includes links to approximately 2,000 public spreadsheets, around 10,000 sample action instructions, and corresponding OfficeScript code generated through annotator actions. The authors conduct baseline experiments using various prompting techniques and model configurations, revealing insights into the capabilities of existing models like GPT-3.5 and GPT-4. The study emphasizes a novel function-based evaluation approach, which prioritizes models' functional understanding over exact output matching.

### Strengths and Weaknesses
Strengths:
- The paper introduces a valuable benchmark for generating code from natural language instructions, contributing to the field.
- It provides a comprehensive evaluation of multiple models and configurations, offering insights into their performance.
- The writing quality is generally good, and the empirical results are of interest to the community.

Weaknesses:
- There is ambiguity in the evaluation criteria, particularly regarding the manual annotation process and inter-annotator agreement.
- The scope may be perceived as limited, focusing solely on Excel without broader applicability to other software.
- The evaluation metrics used may not adequately reflect model performance, and the paper lacks a comparative analysis with existing benchmarks.

### Suggestions for Improvement
We recommend that the authors improve the clarity and consistency of the evaluation criteria to address the ambiguities in the manual annotation process. Additionally, providing further details on how dataset quality was controlled and the inter-annotator agreement rate would enhance the paper's rigor. It would be beneficial to discuss the potential for expanding the benchmark to other software applications, as well as to include a comparative analysis with existing benchmarks to highlight its uniqueness. Finally, we suggest incorporating qualitative insights and detailed error analyses to complement the quantitative metrics used.