ID: ZV4tZgclu8
Title: Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a curated dataset of Connecting Walls (CWs) from the Only Connect quiz show, comprising N=618 CWs, which surpasses the previous largest dataset. The dataset includes ground truth answers and performances from human contestants. The authors propose an evaluation of pre-trained language models (LLMs) on creative problem-solving tasks, particularly in identifying connections among words. Additionally, the paper introduces a challenging task for LLMs involving word grouping with heterogeneous connections, supported by various baseline results. The authors also analyze language model performance in quiz show contexts, addressing potential biases and limitations while proposing enhancements to their dataset and methodology, including clarifications on data sources and ethical considerations regarding the use of their dataset.

### Strengths and Weaknesses
Strengths:
- The authors have significantly augmented an existing dataset, filling gaps with data from the show.
- The operationalization of LLM-based and embedding-based clustering benchmarks is commendable.
- The task is novel and relevant, with comprehensive evaluations that are easy to follow.
- The authors have effectively addressed reviewer comments, enhancing clarity and depth in their limitations section regarding potential US bias and cultural considerations.
- They have improved the documentation of their dataset, making it more accessible for users, particularly non-Only Connect viewers.
- Ethical concerns about dataset reuse have been acknowledged and addressed.

Weaknesses:
- There are limitations in the evaluation process, including potential USA-centric bias in LLMs and a lack of clarity in the JSON parameters.
- The analysis of distractors (red herrings) and their influence on model performance is insufficiently supported by empirical evidence.
- The framing of human-like behavior may detract from the task's complexity.
- There remains a need for clearer attribution of sources used in the study, particularly regarding fan websites and production teams.
- The JSON file, while mostly self-documenting, still requires additional clarity for some elements.

### Suggestions for Improvement
We recommend that the authors improve the analysis of red herrings by providing statistics on their types and ratios within the dataset to support claims about their influence on model performance. Additionally, we suggest conducting experiments comparing performance on tasks with and without red herrings to validate the fixation effect. Clarifying the parameters in the JSON file, particularly regarding date and performance, is essential. We also recommend improving the clarity of attributions for all sources used, particularly the curators of ocdb.cc and the Only Connect production teams. Furthermore, we encourage the authors to enhance the JSON documentation to ensure all elements are easily understood by users unfamiliar with the content. Lastly, we suggest explicitly outlining any potential negative social impacts of their work in the datasheet to prevent unethical reuse of the dataset and to address potential biases in LLMs more thoroughly.