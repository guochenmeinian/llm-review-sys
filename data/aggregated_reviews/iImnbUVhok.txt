ID: iImnbUVhok
Title: Joint Prompt Optimization of Stacked LLMs using Variational Inference
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 4, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework called Deep Language Network (DLN) that stacks multiple large language models (LLMs) as stochastic layers, utilizing prompts at each layer as tunable parameters. The authors propose using variational inference for joint training of these LLMs, achieving performance that can rival larger models like GPT-4. The paper discusses the limitations of LLMs, explores prompt learning techniques, and outlines future research directions, including testing with different models and expanding the architecture. The authors acknowledge the need for ethical considerations and practical deployment strategies for such models. Additionally, the authors propose a two-layer architecture (DLN-2) that aims to enhance performance compared to single-layer models, involving a retrieval step using 400 examples for training, while the prediction step is limited to 32 examples due to context length constraints. They express willingness to implement additional baselines and datasets as suggested by reviewers.

### Strengths and Weaknesses
Strengths:
- The introduction of Deep Language Networks (DLNs) offers a novel perspective on leveraging LLMs, demonstrating improved performance over single-layer models.
- The paper effectively details prompt engineering and in-context learning techniques, enabling better adaptation to various tasks.
- Experimental results validate the effectiveness of DLNs across multiple datasets, showcasing their potential for enhancing language modeling tasks.
- The authors demonstrate a clear understanding of the complexities involved in training and testing language models and are open to feedback.

Weaknesses:
- The complexity of DLNs compared to single-layer models may pose challenges in implementation and require significant computational resources.
- The comparison with baselines is currently inadequate, lacking a comprehensive range of models, making it difficult to assess DLNs' relative performance.
- The methodology of using a subset of the test set raises questions about the robustness of the results.
- Claims regarding the generalization to more than two layers may be misleading, as the authors have not tested beyond this configuration.
- Evaluation metrics are limited, and the experiments may not generalize across all language modeling tasks, necessitating a detailed analysis of DLNs' limitations.
- The computational efficiency of DLNs, including training time and memory requirements, is not thoroughly discussed, which could impact scalability.

### Suggestions for Improvement
We recommend that the authors improve the comparison of DLNs with a broader range of state-of-the-art models to better contextualize their performance. Conducting an ablation study with the same number of examples as the baselines would provide a more equitable assessment. Additionally, we suggest clarifying the rationale for using a subset of the test set and considering testing on all nine datasets mentioned in their paper to strengthen their results. We also recommend providing a detailed analysis of the computational and memory requirements of DLNs compared to single-layer models. Furthermore, including more comprehensive evaluation metrics and conducting experiments across diverse datasets to assess generalizability would enhance understanding. Lastly, we advise a more cautious representation of the capabilities of their method, particularly regarding the generalization to more than two layers, and encourage the authors to report the 95% confidence intervals for their results to provide a clearer picture of the performance variability.