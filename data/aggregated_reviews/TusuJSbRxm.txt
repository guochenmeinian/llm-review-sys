ID: TusuJSbRxm
Title: Trajectory Data Suffices for Statistically Efficient Learning in Offline RL with Linear $q^\pi$-Realizability and Concentrability
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 7, 5, -1, -1, -1
Original Confidences: 3, 2, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a significant advancement in offline reinforcement learning (RL) under linear q-realizability, demonstrating that efficient learning of an Îµ-optimal policy is possible with trajectory data, contrasting sharply with previous results showing exponential sample complexity when only individual transitions are available. The authors establish that under the linear q-realizability assumption and bounded concentrability, the sample complexity scales polynomially with respect to the horizon, feature dimension, and concentrability coefficient, while inversely relating to the desired accuracy.

### Strengths and Weaknesses
Strengths:  
The paper addresses a long-standing question in offline RL, providing a rigorous theoretical contribution that enhances understanding of statistical complexity in this domain. The results are of high interest to the RL theory community, and the assumptions are clearly articulated and discussed.

Weaknesses:  
The proof sketches in Sections 4 and 5 are complex and could benefit from clearer explanations. Specific concerns include the reasoning behind $G=\bar{G}$ satisfying condition (14) and how the algorithm effectively eliminates incorrect guesses $G'$. The notation system is not user-friendly, and the absence of an algorithm block hinders clarity. Additionally, the lack of experimental results limits the practical demonstration of the theoretical findings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the proof sketches by making the intuitions behind key lemmas more explicit, particularly regarding the implications of the concentrability assumption. It would be beneficial to include a concrete definition of non-trajectory data for direct comparison with trajectory data and to elaborate on the hardness results without trajectory data. Furthermore, we suggest adding pseudocode for the algorithm and providing a high-level summary of the main ideas to enhance readability. Finally, including toy examples, such as tabular MDPs, would help illustrate the necessity of skipping certain states.