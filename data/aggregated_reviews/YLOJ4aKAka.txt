ID: YLOJ4aKAka
Title: Connecting Pre-trained Language Model and Downstream Task via Properties of Representation
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 6, 6, 6, 6, -1, -1
Original Confidences: 2, 3, 3, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the transferability of representations from pre-trained language models (LLMs) to downstream tasks, emphasizing the significance of the "anchor vector hypothesis." The authors argue that effective transfer requires addressing two conditions: the insensitivity of downstream tasks to "super-small" probability words and the stabilization of the softmax function through the anchor vector. Empirical evidence supports the existence of the anchor vector, demonstrating that excluding low-probability words reduces the error in approximating the partition function.

### Strengths and Weaknesses
Strengths:
- The paper is well-written with a clear flow, effectively addressing the properties of learned representations and their impact on downstream performance.
- Empirical verifications lend substantial credibility to the anchor vector hypothesis, reinforcing its validity.

Weaknesses:
- The empirical verification lacks clarity regarding the experimental setup, particularly the rationale for using auto-regressive models like GPT-2 and OPT instead of masked language models, leading to potential confusion.
- The paper's assumptions may not hold universally, particularly regarding the relevance of words to downstream tasks, which could limit the generalizability of the findings.
- The scope of the paper is somewhat limited, focusing primarily on binary classification tasks without broader empirical exploration.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental setup, specifically justifying the choice of auto-regressive models for verification. Additionally, addressing the assumptions made regarding word relevance and expanding the empirical analysis to include a wider range of downstream tasks would enhance the paper's robustness. Furthermore, including a broader range of activation functions, such as swish and gelu, could provide insights into the anchor vector hypothesis's applicability beyond ReLU. Lastly, providing labels for the axes in Figure 1 would improve the presentation.