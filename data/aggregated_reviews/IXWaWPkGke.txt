ID: IXWaWPkGke
Title: Optimization or Architecture: How to Hack Kalman Filtering
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new Kalman Filter (KF) algorithm called the Optimized Kalman Filter (OKF), which enhances the supervised learning loss optimization typically used for training non-linear models. The authors argue that the significant improvement of OKF over the Neural Kalman Filter (NKF) is due to this optimization approach, as demonstrated in Doppler radar and video tracking problems. The paper highlights a methodological error in the literature regarding the comparison between optimized filtering models and unoptimized KFs, and establishes that OKF consistently achieves superior accuracy across various problem scenarios.

### Strengths and Weaknesses
Strengths:
- The paper addresses a notable oversight in the literature regarding the comparison of linear KF and NKF, revealing that linear models can perform comparably to non-linear ones when learned properly.
- The writing is clear, and the experiments are comprehensive, supporting the main narrative effectively.
- The motivation for the study is well-articulated, and the proposed model appears novel.

Weaknesses:
- The theoretical contribution is limited, with results largely derived from existing literature or simple algebra. There is a lack of formal definitions for Neural KF and their optimization goals.
- The paper does not adequately compare OKF with KFs that have known covariance matrices or with NKF in non-linear scenarios.
- The clarity of the main message is sometimes obscured, and the broader application of the Kalman filter in supervised learning settings needs better explanation.

### Suggestions for Improvement
We recommend that the authors improve the theoretical framework by providing a formal definition of Neural KF and clarifying their optimization goals. Additionally, it would be beneficial to analyze the error introduced by estimated covariance matrices and explore the closed form of the minimum-variance unbiased estimator when these matrices are unknown. We suggest including comparisons with KFs that have known covariance matrices and with NKF in non-linear contexts to strengthen the findings. Finally, enhancing the clarity of the main message and the broader implications of the work would improve the paper's impact.