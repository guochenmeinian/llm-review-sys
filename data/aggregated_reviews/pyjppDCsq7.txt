ID: pyjppDCsq7
Title: Influence Scores at Scale for Efficient Language Data Sampling
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an evaluation of influence scores for data reduction in language classification tasks, specifically benchmarking five influence scores on the Natural Language Inference (NLI) task. The authors find that VoG (variance of gradients) outperforms a random-pruning baseline and successfully apply VoG in a real-world setting without regression on key metrics. The study aims to determine optimal data selection strategies and evaluates the influence scores against a random sampling baseline in both noisy and clean data settings.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- It provides extensive experimental results, contributing valuable insights to the NLP community.
- The practical orientation of the study, including offline and online A/B tests, enhances its relevance for practitioners.

Weaknesses:
- The major conclusion relies heavily on the performance of the BERT encoder on the SNLI dataset, raising concerns about the generalizability of findings to other datasets or tasks.
- The relevance of the challenge regarding the selection of important examples for fine-tuning transformer-based models is questioned, particularly for larger models like LLMs.
- The study's controlled setting may limit the applicability of its findings to a broader range of problems.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the relevance of the challenge presented in the introduction, providing more context and justification for its significance. Additionally, the authors should acknowledge the limitations of their study's scope and discuss the implications for generalizability to other datasets and experimental settings. Addressing these concerns could strengthen the paper's overall contribution.