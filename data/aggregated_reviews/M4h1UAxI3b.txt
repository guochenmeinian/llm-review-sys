ID: M4h1UAxI3b
Title: H-nobs: Achieving Certified Fairness and Robustness in Distributed Learning on Heterogeneous Datasets
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 4, 6, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a protocol aimed at addressing fairness and Byzantine robustness in federated learning. The authors propose an objective function that incorporates fairness and an algorithm that averages gradients with large norms while mitigating their influence. The theoretical analysis is comprehensive, covering various assumptions, and the paper discusses the complexities of achieving both fairness and robustness. Additionally, the authors derive convergence guarantees under different convexity assumptions and evaluate the proposed method through experiments on the Spambase dataset.

### Strengths and Weaknesses
Strengths:
1. The paper provides an in-depth discussion of the intricate relationship between fairness and robustness, offering valuable insights into previously unexplored questions.
2. The theoretical analysis is thorough, with multiple levels of assumptions and corresponding guarantees.
3. The empirical results demonstrate the effectiveness of the proposed method, particularly in terms of accuracy and fairness.

Weaknesses:
1. The algorithm appears to be a straightforward combination of prior works, lacking significant novelty.
2. Convergence theorems are derived under specific conditions (alpha < 1/3), and the implications of these theorems are not sufficiently elaborated.
3. The focus is limited to one fairness notion, egalitarian fairness, without considering implications for other fairness definitions.
4. The experimental evaluation is constrained to a single dataset and lacks comprehensive details on hyper-parameters, which limits reproducibility and generalizability.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contributions by providing a clearer distinction between their work and prior research. Additionally, we suggest elaborating on the implications of the convergence theorems to enhance clarity. To strengthen the fairness analysis, consider incorporating multiple fairness notions beyond egalitarian fairness. We also encourage the authors to conduct more extensive experiments across various datasets and hyper-parameter settings to bolster the generalizability of their findings. Finally, providing detailed information on the experimental setup, including hyper-parameters and the rationale for selected values, will enhance reproducibility.