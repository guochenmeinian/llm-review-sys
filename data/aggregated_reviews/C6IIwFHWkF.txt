ID: C6IIwFHWkF
Title: Dynamic Sparsity Is Channel-Level Sparsity Learner
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the practical deployment and speedup of Dynamic Sparse Training methods on general hardware, proposing a sparse-inducing method that gradually removes entire channels during training. This results in a network with structured sparse characteristics, enabling favorable acceleration. The authors validate their method through experiments on extensive datasets, achieving significant inference throughput speedups without compromising accuracy. The paper introduces Channel-aware dynamic sparse (Chase), which translates unstructured dynamic sparsity to GPU-friendly channel-level sparsity during end-to-end training.

### Strengths and Weaknesses
Strengths:
1. The motivation for the proposed method is reasonable, addressing the deployment challenges of unstructured sparsity on off-the-shelf hardware.
2. The novel observation regarding the sparsity distribution across channels provides valuable insights for the community.
3. The method is easy to implement and surpasses traditional non-structured pruning methods while achieving practical GPU acceleration.
4. Chase achieves up to 1.7Ã— inference throughput speedup on common GPU devices without accuracy loss, establishing a new state-of-the-art performance for unstructured sparse training.

Weaknesses:
1. The paper does not compare the proposed method with structured pruning methods, which would be beneficial given its focus on practical acceleration.
2. Experiment choices, particularly using over-parameterized models like VGG19 and ResNet50 for CIFAR-10/100, may undermine the reliability of results.
3. Important details regarding the computation of "GPU-supported FLOPs" and "Throughput" are missing.
4. The method requires additional memory for channel masks, which could be limiting for models with many channels.
5. There is insufficient discussion on the limitations of the proposed method and its potential societal impacts.

### Suggestions for Improvement
We recommend that the authors improve the comparison of their method with structured pruning techniques to clarify its advantages. Additionally, consider using more appropriate models for CIFAR-10/100 to enhance the reliability of experimental results. It is crucial to provide detailed explanations of how "GPU-supported FLOPs" and "Throughput" are computed. Addressing the memory requirements for channel masks and discussing the limitations of the method in greater detail would also strengthen the paper. Lastly, we encourage the authors to replicate experiments on larger datasets like ImageNet and extend validation to other models and tasks to establish broader applicability.