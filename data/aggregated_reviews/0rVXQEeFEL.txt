ID: 0rVXQEeFEL
Title: Transformer-based Planning for Symbolic Regression
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 7, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TPSR, a Transformer-based Planning strategy for Symbolic Regression that integrates Monte Carlo Tree Search (MCTS) into the transformer decoding process. This allows for the incorporation of non-differentiable feedback, such as accuracy and complexity. Experimental results indicate that TPSR surpasses existing methods regarding fitting-complexity trade-off, extrapolation abilities, and robustness to noise.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to understand.
- The enhancement of large-scale pre-trained Transformers with improved search capabilities is promising for symbolic regression.
- The model demonstrates strong performance compared to both the end-to-end (E2E) baseline and genetic programming (GP) methods.

Weaknesses:
- The novelty of the approach is questionable, as a similar idea has been explored in recent literature, specifically in [1], where MCTS is combined with pre-trained Transformers. Clarification on the differences between the two approaches is needed.
- The selection of the parameter $\lambda$ appears to significantly impact the experiments, yet guidance on its practical selection is unclear.
- The method primarily showcases a new application of existing techniques, lacking substantial methodological innovation.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the novelty of their approach by explicitly detailing the differences from the work in [1]. Additionally, we suggest providing a more comprehensive explanation of how to select the parameter $\lambda$ in practice. It would also be beneficial to include an ablation study to demonstrate the effects of $\beta(s)$. Furthermore, the authors should consider discussing the potential integration of MCTS into the training or fine-tuning processes of the transformer models. Lastly, addressing the minor issues noted, such as clarifying terminology and ensuring self-containment of the methodology, would enhance the paper's overall quality.