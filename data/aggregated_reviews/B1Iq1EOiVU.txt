ID: B1Iq1EOiVU
Title: DeformableTST: Transformer for Time Series Forecasting without Over-reliance on Patching
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DeformableTST, a novel time series forecasting method that replaces patching techniques with a Deformable Attention mechanism. This mechanism dynamically selects significant time points using learnable offsets, thus reducing computational costs and enhancing performance across various forecasting tasks. The authors validate their approach through comprehensive experiments on multiple benchmarks, demonstrating state-of-the-art performance while minimizing reliance on patching.

### Strengths and Weaknesses
Strengths:
1. The paper effectively addresses the limitations of existing patch-based forecasting models, presenting a reasonable problem definition.
2. The Deformable Attention mechanism is an advanced sparse attention method that significantly reduces computational costs by focusing on important time points.
3. The methodology is clear and well-structured, with thorough experiments covering both short-term and long-term forecasting scenarios.

Weaknesses:
1. The paper lacks novelty, as numerous new attention mechanisms have been proposed previously.
2. Concerns exist regarding the uniform attention prior's effectiveness in scenarios where key information is clustered within specific time windows, which lacks systematic experimental validation.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their work by providing a more detailed comparison with existing attention mechanisms, particularly in relation to 2D deformable attention in vision transformers. Additionally, conducting an ablation study to demonstrate the impact of patching on their method would strengthen their claims. Clarifying the term "important points" in the main text and providing citations for claims regarding patch-based transformers would enhance the paper's credibility. Finally, addressing the effectiveness of the model in scenarios with clustered attention distributions through targeted experiments would be beneficial.