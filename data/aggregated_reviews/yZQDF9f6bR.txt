ID: yZQDF9f6bR
Title: Diplomat: A Dialogue Dataset for Situated PragMATic Reasoning
Conference: NeurIPS
Year: 2023
Number of Reviews: 31
Original Ratings: 6, 7, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
The paper presents Diplomat, a comprehensive annotated dataset derived from NPR interviews, aimed at enhancing pragmatic reasoning in conversational agents. It introduces two tasks: Pragmatic Identification and Reasoning (PIR) and Conversational Question Answering (CQA). The PIR task includes subtasks that reveal current language models' limitations in pragmatic awareness, while the CQA task assesses models' ability to answer questions without access to intended meanings. The study highlights the challenges faced by state-of-the-art models in these areas. We appreciate the opportunity for further discussion on the development of pragmatic reasoning and recognize the complexity of collecting pragmatic data, which necessitates native speakers for accurate representation. The dataset is described as valuable, open, and diverse, showcasing a wide array of topics and rigorous filtering, with a vocabulary size of 48,900. However, we note discrepancies in model performance and the unexpected results of larger models compared to smaller ones, attributed to domain discrepancies and the low-resource nature of pragmatic data. The dataset addresses pragmatic reasoning in multi-turn conversations, filling a gap in existing literature and providing a unified framework for understanding figurative language and conversational question answering.

### Strengths and Weaknesses
Strengths include the dataset's rigorous creation process, which involved automatic selection, fine-grained annotation, and human refinement, resulting in a diverse set of dialogues. The tasks are well-designed and relevant, addressing an underexplored area in NLP. The paper is generally well-structured and clear, facilitating understanding. The authors have shown responsiveness to feedback, clarifying and expanding explanations in the revised materials. However, weaknesses include inconsistencies in dataset naming, unclear connections between tasks, and a lack of detailed analysis on reasoning types. We also identify the absence of an inter-annotation agreement study as a serious shortcoming. Concerns were raised regarding the quality of answers generated by crowdworkers and the potential influence of AI tools like ChatGPT on data quality. Additionally, the evaluation does not sufficiently address human pragmatic reasoning, and the distinction between "general" and "diverse" datasets remains unclear.

### Suggestions for Improvement
We suggest further polishing the paper to resolve inconsistencies, such as clarifying the dataset name and establishing the NLI task's relevance. Enhancing the dataset by leveraging additional sources and incorporating more dialogue would improve diversity, robustness, and adaptability. We recommend conducting a more detailed analysis of models' performance across the categorized reasoning types and clarifying the reasoning types and their theoretical foundations by including relevant citations. Including a section on "Safety and Ethical Implications" would enhance the paper's comprehensiveness. We urge the authors to correct remaining typos and ambiguities in figures and supplementary materials, explicitly list English-speaking countries, and provide examples for unclear terms. Additionally, including training time information in the supplementary materials and ensuring that all references to tables are accurate would improve clarity. Finally, we recommend conducting a formal inter-annotator agreement study to strengthen the validity of the findings.