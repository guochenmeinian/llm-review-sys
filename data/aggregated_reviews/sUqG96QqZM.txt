ID: sUqG96QqZM
Title: Weakly-Supervised Audio-Visual Segmentation
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 5, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a weakly-supervised audio-visual segmentation framework called WS-AVS, which predicts sounding source masks from audio and images without requiring pixel-level ground truth masks. The authors employ multi-scale contrastive learning to capture audio-visual alignment and refine pseudo masks to guide training, demonstrating superior performance on the AVS Bench compared to existing methods. The proposed methodology addresses the challenge of costly pixel-level annotations by utilizing a weakly-supervised approach.

### Strengths and Weaknesses
Strengths:  
- The paper introduces an interesting approach to audio-visual segmentation that eliminates the need for pixel-level annotations, addressing a significant challenge in the field.  
- The methodology is well-structured and easy to follow, with convincing results and a detailed ablation study.  
- Comprehensive evaluations across various settings illustrate the performance of the proposed method.

Weaknesses:  
- Several important details are lacking, such as the specifics of how multi-scale features are generated and how negative examples are selected in the multi-scale multiple-instance contrastive learning.  
- The novelty of the paper is unclear; the authors should better define their unique contributions, particularly the significance of multi-scale multiple-instance contrastive learning.  
- The evaluation is limited to a single dataset, and the authors do not report performance on off-screen-only videos, which could impact the robustness of their model.  
- The results in Table 1 lack clarity regarding the number of sources used and the methodology for obtaining these examples.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their methodology by elaborating on the generation of multi-scale features and the selection of negative examples in multi-scale multiple-instance contrastive learning. Additionally, we encourage the authors to clearly define the unique technical contributions of their work, particularly the role of multi-scale multiple-instance contrastive learning in weakly-supervised segmentation. Expanding the evaluation to include additional datasets, such as Flickr-SoundNet or AudioSet, would enhance the robustness of their findings. Furthermore, the authors should assess their model's performance on off-screen-only videos to better understand its capabilities in real-world scenarios. Lastly, including additional failure cases in the supplementary material would provide a more comprehensive view of the model's limitations.