ID: KNFG5KLXD3
Title: We Are What We Repeatedly Do: Inducing and Deploying Habitual Schemas in Persona-Based Responses
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to dialogue generation for virtual conversational agents, aiming to create personas that generate natural and consistent responses. The authors propose leveraging event schemas to represent habitual knowledge in a narrative-like form, enhancing the agent's persona. A bootstrapping method is introduced for schema creation from simple natural language facts, and in-context prompting with large language models (LLMs) is utilized to produce more engaging and controllable responses. The main contributions include the explicit representation of personas through event schemas, the bootstrapping method, and improved dialogue generation performance demonstrated through evaluations.

### Strengths and Weaknesses
Strengths:
- The approach for generating sophisticated personas for LLMs is interesting and grounded in a strong theoretical background, particularly the use of event schemas.
- The paper is well-structured and clearly presents its main contributions.
- Good reproducibility and rich evaluation methods, including both automatic and human evaluations, suggest the approach yields more engaging and diverse responses.

Weaknesses:
- Clarity issues arise in the introduction regarding the need for topical coherence in dialogue, which is not sufficiently addressed throughout the paper.
- The methods section contains ambiguities due to the numerous concepts presented; better illustrated examples could enhance understanding.
- The inclusion of gold standard responses in evaluation may skew results, particularly in BLEU score measurements, as responses often resemble the gold standard.

### Suggestions for Improvement
We recommend that the authors improve clarity in the introduction by consistently addressing the need for topical coherence in dialogue. Additionally, we suggest providing clearer examples in the methods section to elucidate the response generation process. To address the potential bias introduced by gold standard responses, we recommend exploring alternative evaluation metrics that do not rely on direct comparisons to gold responses. Lastly, we encourage the authors to clarify the dialogue context used in their methodology and provide more details on participant compensation and demographics in their study.