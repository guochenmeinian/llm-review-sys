ID: ckKuQDW2RZ
Title: Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CONIC10K, a challenging math problem dataset focused on conic sections in Chinese senior high school education. The dataset includes a variety of problems with differing reasoning depths, requiring only knowledge of conic sections. Experimental results indicate that existing large language models (LLMs), including GPT-4, demonstrate weak performance on complex reasoning tasks.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a comprehensive dataset of 10,000 problems on conic sections, serving as an interesting benchmark that highlights the limitations of LLMs like GPT-4.
2. It provides a detailed performance analysis of multiple state-of-the-art language models across subcategories of the dataset.
3. The inclusion of formal representations and reasoning steps adds merit, while the senior high school educational context acts as a quality checker.

Weaknesses:
1. The paper lacks insights into data quality analysis, particularly since the dataset is not available for external examination.
2. Questions arise regarding the reliability of the evaluation protocol; there is no human study to assess its efficacy.
3. The study does not explore the relationship between semantic parsing and mathQA, which diminishes the dataset's utility as a semantic parsing benchmark.
4. There is a need to investigate the effectiveness of adding more demonstrations to improve model performance.
5. The paper lacks results on self-consistency, which involves sampling multiple reasoning paths and conducting a majority vote on predictions.

### Suggestions for Improvement
We recommend that the authors improve the insights into data quality analysis, especially considering the dataset's unavailability for external review. Additionally, it would be beneficial to include a human study to evaluate the benchmark's effectiveness. We suggest exploring the relationship between semantic parsing and mathQA to enhance the dataset's relevance. Furthermore, we encourage the authors to investigate the impact of adding more demonstrations on model performance. Lastly, we recommend including results on self-consistency to provide a more comprehensive evaluation of reasoning capabilities.