ID: 2z4U9reLm9
Title: FEABench: Evaluating Language Models on Real World Physics Reasoning Ability
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 6, 5, 7
Original Confidences: 4, 5, 3

Aggregated Review:
### Key Points
This paper presents FEABench, a benchmark designed to evaluate large language models (LLMs) on real-world physics and engineering problems using finite element analysis (FEA) software. The authors propose a multi-faceted evaluation scheme to assess LLMs' capabilities in interacting with COMSOL Multiphysics software through its API, focusing on a set of 13 quantitatively verifiable problems across various physics domains. The paper introduces an LLM agent capable of API interaction and iterating on solutions, alongside a range of metrics to evaluate performance, including code executability and physics-specific metrics.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a significant gap in evaluating LLMs on real-world engineering tasks, particularly in finite element analysis.
2. The benchmark dataset is well-curated, providing a comprehensive evaluation of LLM performance across different physics domains.
3. The innovative multi-faceted evaluation scheme and the introduction of an LLM agent with API interaction offer valuable insights into LLM capabilities and limitations.
4. The methodology, metrics, and results are generally well-explained and illustrated effectively.

Weaknesses:
1. The benchmark is limited to problems solvable with COMSOL Multiphysics, which may restrict generalizability.
2. The dataset's small size of only 13 problems may not capture the diversity of real-world FEA tasks.
3. The lack of a human baseline comparison diminishes the contextual understanding of LLM performance.
4. The criteria for problem selection require more thorough justification to ensure representativeness.
5. There is limited analysis of failure modes, which could provide insights into LLM shortcomings.
6. Reproducibility concerns arise from reliance on proprietary software, potentially limiting broader research applicability.
7. The paper does not adequately demonstrate how generating COMSOL API calls translates to practical engineering problem-solving capabilities.

### Suggestions for Improvement
We recommend that the authors improve the benchmark's generalizability by including problems from other FEA software packages. Expanding the number and diversity of problems would enhance the dataset's robustness. Additionally, providing a human performance baseline would strengthen the evaluation context. We suggest a more detailed justification of the problem selection criteria and a deeper analysis of failure modes to uncover specific challenges faced by the models. Addressing potential biases and limitations in the benchmark, as well as discussing how LLMs integrate with traditional numerical analysis techniques, would further enhance the paper's relevance and applicability.