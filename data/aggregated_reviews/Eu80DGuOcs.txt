ID: Eu80DGuOcs
Title: Understanding and Improving Training-free Loss-based Diffusion Guidance
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of training-free loss-based diffusion guidance, analyzing its mechanisms and limitations while proposing enhancements such as random augmentation and Polyak step size to improve convergence and smoothness. The authors provide a theoretical framework explaining the dependence of guidance strength on $\sqrt{\alpha_t}$ and the impact of adversarial gradients on generation quality. They conduct a detailed evaluation of adversarial gradient issues using an adversarially robust ResNet-50, indicating that a significant loss gap between standard and robust ResNet-50 suggests susceptibility to adversarial gradients. Experimental results demonstrate that the proposed methods outperform existing loss-based guidance techniques, including Universal Guidance and FreeDoM, while also comparing the convergence of training-free guidance with training-based methods, specifically PPAP, highlighting the slower convergence of FreeDoM at lower sampling steps, which is improved by incorporating the Polyak step size.

### Strengths and Weaknesses
Strengths:
- The theoretical analysis offers valuable insights into the mechanisms of training-free guidance.
- The proposed techniques, random augmentation and Polyak step size, show effectiveness across various applications.
- The paper provides a quantitative evaluation of adversarial gradients and demonstrates the effectiveness of the proposed method through comprehensive analysis.
- The theoretical foundation supporting random augmentation is robust and applicable across various modalities, enhancing the paper's contribution to the field.

Weaknesses:
- The empirical evidence supporting the claims is insufficient, particularly regarding the assumptions of Lipschitz continuity for complex neural networks.
- The paper lacks a comprehensive comparison with recent literature and related works, such as [A] and [B], which address similar configurations and methodologies.
- The selection of hyper-parameters and their optimization processes are not adequately detailed.
- The random augmentation technique may have limitations in its applicability across different domains.
- The completion of additional experiments under class conditions may not be feasible before the review period ends, potentially limiting the scope of the findings presented.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation of their claims by providing more extensive experiments, particularly for the oscillation of loss curves and the careful selection of guidance weights. It is crucial to include comparative experiments with classifier-guided tasks to assess the significance of the proposed improvements. Additionally, we suggest that the authors clarify the rationale behind the Polyak step size and provide more details on its implementation. Addressing the limitations of random augmentation in various domains would also enhance the robustness of the proposed methods. Furthermore, we recommend that the authors improve the clarity of the results in Table 3 by explicitly stating the implications of the loss values in relation to adversarial gradients. Lastly, prioritizing the completion of ongoing experiments under class conditions would strengthen their findings before the final submission.