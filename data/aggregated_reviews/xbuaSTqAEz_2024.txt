ID: xbuaSTqAEz
Title: Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 7, 5, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel end-to-end multiple clustering approach, termed Multi-Sub, which incorporates a multi-modal subspace proxy learning framework. The authors leverage the synergistic capabilities of CLIP and GPT-4 to align textual prompts that express user preferences with corresponding visual representations. The main contributions of Multi-Sub include capturing user-specific clustering interests, optimizing representation learning and clustering simultaneously, and providing extensive experimental validation that demonstrates superior performance over existing methods.

### Strengths and Weaknesses
Strengths:
1. The integration of CLIP and GPT-4 for multi-modal subspace proxy learning effectively addresses the limitations of traditional multiple clustering methods.
2. Multi-Sub captures diverse user interests, yielding tailored clustering results with minimal manual interpretation.
3. The paper is well-structured, with clear writing and well-drawn figures that facilitate understanding.
4. Extensive experiments on public datasets validate the robustness and generalizability of the proposed method.

Weaknesses:
1. Multi-Sub employs a two-phase iterative approach for alignment and clustering, raising questions about whether this constitutes a two-stage task.
2. The description of Clustering Loss in Section 3.4 lacks clarity regarding how samples are classified into the same class, particularly concerning the origin of pseudo-labels.
3. The potential introduction of a priori information from pre-trained large language models may compromise the integrity of unsupervised scenarios.
4. A more detailed analysis of hyperparameter sensitivity and the use of regularization techniques to mitigate overfitting risks on smaller datasets is warranted.
5. The reasons behind performance differences among various text encoders, as shown in Table 3, could be explored in greater depth.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the Clustering Loss description in Section 3.4, specifically addressing how samples are classified and the source of pseudo-labels. Additionally, we suggest providing a more detailed analysis of hyperparameter sensitivity and discussing any regularization techniques employed to prevent overfitting. The authors should also explore the reasons behind the performance variations observed with different text encoders in greater detail. Lastly, enhancing the clarity of the contributions and discussing broader impacts beyond limitations would strengthen the paper's overall presentation.