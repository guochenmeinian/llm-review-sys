ID: aE7feUD7o7
Title: Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FISH-DIP, a method for parameter-efficient fine-tuning of pretrained language models (PLMs) in sequence labeling tasks under few-shot settings. The authors propose a sample-aware dynamic sparsity selection metric based on Fisher information to update a small fraction of model parameters. Experimental results across various sequence labeling tasks demonstrate that FISH-DIP outperforms full fine-tuning and existing parameter-efficient fine-tuning (PEFT) methods.

### Strengths and Weaknesses
Strengths:  
- Novelty in applying Fisher information for dynamic parameter selection in sequence labeling tasks.  
- Clear presentation of the methodology and comprehensive empirical results across seven datasets.  
- Demonstrated effectiveness of FISH-DIP in low-resource settings, showing competitive or superior performance compared to traditional methods.

Weaknesses:  
- Lack of a clear definition of "low-resource," leading to ambiguity regarding the paper's focus.  
- Insufficient motivation for the proposed method, with unclear distinctions from existing PEFT methods.  
- Limited baseline comparisons, particularly with other relevant few-shot learning methods and PEFT techniques.  
- Time and memory efficiency of the dynamic parameter selection process is not discussed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the term "low-resource" by explicitly defining it in the context of their work. Additionally, the authors should strengthen the motivation for FISH-DIP by providing a more in-depth analysis of its advantages over existing PEFT methods. Including more baseline comparisons, particularly with methods like UIE and other PEFT techniques, would enhance the robustness of their findings. Finally, we suggest discussing the computational overhead introduced by FISH-DIP and its implications for training efficiency.