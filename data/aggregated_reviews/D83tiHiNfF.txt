ID: D83tiHiNfF
Title: Formal Theorem Proving by Rewarding LLMs to Decompose Proofs Hierarchically
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 4, 6, 6, 6
Original Confidences: 4, 4, 4, 3

Aggregated Review:
### Key Points
This paper presents a reinforcement learning (RL) approach, specifically the Proof Decomposer (ProD-RL), aimed at enhancing large language models (LLMs) in theorem proving by requiring them to autonomously generate and prove relevant lemmas. The authors argue that this method better assesses proof planning abilities compared to existing evaluations that provide necessary lemmas. The results suggest that the approach shows promise, although performance improvements on the AFP 2023 dataset are marginal.

### Strengths and Weaknesses
Strengths:  
- The paper introduces an original RL-based methodology for theorem proving that emphasizes hierarchical decomposition of proofs, aligning with how mathematicians approach problem-solving.  
- The technical aspects, including the model architecture and reward mechanisms, are well-articulated, and the experimental results indicate improvements over baselines.  
- The topic is relevant and significant for future research in automated theorem proving.

Weaknesses:  
- The paper lacks clarity in several areas, such as the differences between baseline methodologies and the specifics of the ProD-SFT method.  
- Key experimental details are omitted from the main text, leading to potential misunderstandings about the reward structure and training processes.  
- The results on the AFP 2023 dataset are barely positive, raising concerns about the generalizability of the approach.  
- The paper does not adhere to the required format, lacking a conclusion in the main text.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology by providing detailed explanations of the ProD-SFT method and the differences between baseline models. Additionally, it is crucial to include all key experimental details in the main text to avoid misleading interpretations. We suggest that the authors consider including a conclusion section and address the formatting issues to comply with submission guidelines. Furthermore, conducting ablation studies on exploration versus exploitation adjustments for RL methods and testing other base models, such as gemma or deepseek, could provide valuable insights into the model's performance. Lastly, including generated proof examples and discussing the model's performance with varying theorem complexities would enhance the paper's depth.