ID: 08zf7kTOoh
Title: Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 7, 7, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an extensive experimental study on various image-based generative models, revealing that existing evaluation metrics do not strongly correlate with human evaluations of perceptual realism. The authors propose alternative self-supervised feature extractors, particularly focusing on the FD_DINOv2 score, to enhance evaluation accuracy and investigate data memorization in generative models. They emphasize that while human assessments serve as ground truth, the correlation between human error rates (HER) and metrics like FID is inconsistent across datasets. The study highlights the limitations of traditional metrics like FID, particularly its reliance on the Inception-V3 network, and suggests that DINOv2 may provide a more reliable evaluation framework. Additionally, the authors explore the Rarity Score (RS) and its implications on human perception of image authenticity, revealing that dataset issues significantly influence the correlation between RS and HER. The authors also release datasets and a library for further research.

### Strengths and Weaknesses
Strengths:
- The authors conducted comprehensive experiments evaluating generative models from multiple perspectives, including encoders, human evaluations, diversity, and memorization.
- The paper offers significant insights into the inadequacies of existing evaluation metrics and proposes a promising alternative with DINOv2.
- The incorporation of the Rarity Score analysis adds depth to the understanding of human perception of generated images.
- The release of generated image datasets and human evaluation data facilitates further research.

Weaknesses:
- No existing metric showed strong correlation with human evaluations, raising concerns about the calculation of the human error rate.
- The human evaluation methodology may be flawed, as it could confuse fakeness with unlikeliness, potentially skewing results.
- There is a lack of clear definitions and measures for some claims, particularly regarding the treatment of human ratings as ground truth.
- The authors' justification for using human assessments may not universally apply, as not all applications of generative models involve human end-users.
- The paper lacks clarity in presenting key conclusions, particularly in the extensive conclusion section, and Figure 1 is not intuitive.
- There is insufficient discussion on the limitations of the study, particularly regarding the dataset and participant bias.

### Suggestions for Improvement
We recommend that the authors improve the calculation of the human error rate by separately treating real-to-fake and fake-to-real classifications, emphasizing the importance of the fake-to-real case. Additionally, we suggest enhancing Figure 1 for better intuitiveness and summarizing key conclusions more effectively in the introduction. We also recommend improving the clarity of definitions and measures related to claims made in the paper, particularly concerning the treatment of human ratings as ground truth. Furthermore, we encourage the authors to explicitly address the limitations of their metrics in contexts where human perception may not be the primary concern, such as data augmentation or debiasing applications. Conducting a bias analysis on participant demographics could strengthen the reliability of the results. Lastly, we encourage the authors to discuss the limitations of their work in a dedicated section to clarify the implications of their findings.