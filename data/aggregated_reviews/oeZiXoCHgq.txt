ID: oeZiXoCHgq
Title: ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the ACT-SQL method, which automatically generates chain-of-thought reasoning steps for text-to-SQL tasks, enhancing LLMs' reasoning capabilities. The authors propose using hybrid demonstration examples, combining static and dynamic examples, and conduct a comprehensive comparison of various database prompt styles in both zero-shot and few-shot settings. The results indicate that ACT-SQL outperforms baseline few-shot methods across multiple datasets, achieving state-of-the-art performance on the Spider dev set.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel approach to chain-of-thought prompt design, enhancing efficiency by automating the generation of exemplars without manual labeling.
2. It demonstrates versatility by extending the method to multi-turn text-to-SQL tasks.
3. The experiments are thorough, showing that ACT-SQL achieves state-of-the-art results among existing in-context learning approaches.

Weaknesses:
1. The effectiveness of ACT-SQL is questioned due to minimal performance differences compared to few-shot methods, necessitating statistical significance tests for validation.
2. The novelty of the work is unclear, as similar concepts have been explored in prior research, and a comprehensive comparison with existing methods is lacking.
3. The paper does not adequately clarify the significance of performance improvements, which may affect its perceived contribution to the field.

### Suggestions for Improvement
We recommend that the authors improve the statistical analysis by conducting significance tests to validate the performance differences between ACT-SQL and few-shot methods. Additionally, a more detailed comparison with related works that utilize chain-of-thought or in-context learning techniques should be included to clarify the novelty and contributions of this research. Finally, we suggest that the authors provide clearer metrics and context regarding the claimed state-of-the-art performance to enhance the robustness of their findings.