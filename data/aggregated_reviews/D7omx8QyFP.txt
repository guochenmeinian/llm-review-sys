ID: D7omx8QyFP
Title: The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new CoT collection training dataset aimed at enhancing the reasoning capabilities of large language models (LLMs). The dataset is generated through human annotation and augmented with LLMs, demonstrating effectiveness in improving reasoning and generalization abilities across various pre-trained language models (PLMs). Additionally, the authors propose a novel instruction tuning approach that fine-tunes smaller LMs using the CoT collection, yielding improved performance on unseen tasks and in few-shot learning scenarios.

### Strengths and Weaknesses
Strengths:  
- The CoT dataset is a significant contribution that facilitates research on chain-of-thought reasoning without reliance on commercial models.  
- Extensive experiments validate the effectiveness of the collected data, showing notable improvements in model performance.  
- The paper is well-organized and presents sound methodology.

Weaknesses:  
- The dataset's production is costly, and the quality of rationales generated may be inconsistent, particularly in multi-step reasoning tasks.  
- There is a lack of investigation into in-domain performance after CoT training, and comparisons with other publicly available datasets are insufficient.  
- The use of deprecated Codex for generating CoT data raises concerns about reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the size, nature, and quality of other publicly available datasets with rationales, and compare the effectiveness of the CoT Collection against these datasets. Additionally, we suggest conducting experiments to evaluate in-domain performance after CoT training and exploring the impact of incorrect intermediate inference steps on model fine-tuning. Finally, addressing the concerns regarding the use of deprecated Codex and providing alternative API options for future research would enhance the paper's reproducibility.