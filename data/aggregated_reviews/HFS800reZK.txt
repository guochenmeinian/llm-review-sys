ID: HFS800reZK
Title: Learning Representations for Hierarchies with Minimal Support
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 5, 3, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 2, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for identifying a minimal subset of entries in the adjacency matrix that uniquely distinguishes a directed acyclic graph (DAG) among all transitively-closed DAGs. The authors propose an algorithm, FINDMINDISTINGUISHER, to compute this minimal set and leverage it for efficient training of node embedding models through hierarchy-aware sampling. The method shows significant reductions in training data—up to 99%—while maintaining or improving model performance and convergence rates on both synthetic and real-world datasets. Additionally, the paper addresses efficiency challenges related to super nodes in graphs with skewed degree distributions, proposing a graph-theoretically optimal reduction that significantly reduces the negative support set by over 99% in large real-world hierarchies like MeSH, while demonstrating faster convergence compared to the full negative support set. The authors validate their theoretical findings through experiments on three distinct families of transitively-closed DAGs, emphasizing that the key novelty lies in the minimal size of the support set rather than the hierarchy-aware sampling procedure.

### Strengths and Weaknesses
Strengths:
- The theoretical foundation of the proposed framework is robust, with key properties formally stated and proven.
- The authors provide a substantial reduction in the negative support set, demonstrating its effectiveness in real-world applications.
- Empirical studies systematically evaluate the impact of algorithm design choices and data characteristics on model performance, with experimental validation on diverse synthetic graphs supporting the theoretical claims made in the paper.
- The substantial reduction in training data required for effective graph representation learning is conceptually valuable.

Weaknesses:
- The end-to-end efficiency of the proposed method is not clearly evaluated, making it difficult to assess its practical significance.
- The hierarchy-aware sampling process requires more elaboration, and the novelty of this sampling method is unclear as it closely resembles existing methods.
- The loss function in Section 5 relies on several poorly defined terms, raising concerns about repeatability.
- Concerns regarding the impact of super nodes in large-scale graphs remain unaddressed, particularly in relation to the density of the equivalent sidigraph.
- The presentation needs improvement, including missing verbs and unclear experimental aims, particularly regarding comparisons with existing models.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the hierarchy-aware sampling process and provide a more detailed evaluation of the end-to-end efficiency of their method. Additionally, the authors should clarify the loss function in Section 5 to ensure all terms are well-defined and enhance the overall presentation by correcting grammatical errors and ensuring that experimental aims are clearly articulated. We also suggest that the authors explicitly distinguish the novelty of the hierarchy-aware sampling from previous methods and address the concerns regarding the impact of super nodes in large-scale graphs, particularly in relation to the density of the equivalent sidigraph, to strengthen the argument for the proposed method's effectiveness. Finally, we encourage the authors to explore the applicability of their framework to other graph properties beyond transitivity to broaden the scope of their contributions.