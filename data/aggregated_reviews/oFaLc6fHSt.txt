ID: oFaLc6fHSt
Title: Gradient Informed Proximal Policy Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 5, 6, 4, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two novel approaches in reinforcement learning: the GI-PPO algorithm, which integrates analytical gradients into the Proximal Policy Optimization (PPO) framework through an adaptive α-policy, and the MBPO algorithm, which utilizes an ensemble of neural networks to approximate environment dynamics without assuming a fully known model. The GI-PPO method aims to enhance policy network performance by dynamically adjusting based on the variance and bias of analytical gradients, while the MBPO algorithm emphasizes the complexities introduced by model approximation and discusses the potential for using analytical gradients derived from these approximated models. The authors also highlight the trade-offs between accuracy, efficiency, robustness, and generalization, suggesting methods like Monte Carlo Tree Search (MCTS) for future exploration.

### Strengths and Weaknesses
Strengths:  
- The theoretical exploration of analytical gradients and the α-policy in GI-PPO is innovative and contributes significantly to the field.  
- The empirical results for GI-PPO indicate that the new algorithm can outperform baseline methods in certain environments.  
- The paper provides a clear explanation of the MBPO algorithm and effectively addresses misconceptions regarding the assumption of a fully known model.  
- The discussion on future directions, including the use of MCTS, highlights the potential for further research in model-based reinforcement learning.  
- The paper is generally well-written and presents a solid technical foundation, including theoretical derivations and extensive experiments.

Weaknesses:  
- The focus on PPO in GI-PPO raises questions about the applicability of the proposed method to off-policy algorithms, which requires further justification.  
- The assumption of differentiable environment dynamics in GI-PPO limits the practical applicability of the algorithm in real-world scenarios.  
- Some mathematical claims and algorithmic details in GI-PPO lack clarity, such as the adjustment of α and the implications of the determinant ratio in policy updates.  
- The writing quality is inconsistent in GI-PPO, with some sections lacking motivation and intuitive explanations, making it difficult to follow.  
- The MBPO response could benefit from more detailed examples or empirical evidence to support claims about the advantages of their approach over fully known models.  
- The complexity introduced by model approximation in MBPO may require further elaboration to clarify its implications on the training process.

### Suggestions for Improvement
We recommend that the authors improve the clarity of mathematical claims in GI-PPO, particularly regarding the definition of the advantage function A-hat and the adjustment mechanism for α. Additionally, the authors should provide a thorough evaluation of the practical implications of the differentiability assumption and explore the potential of the proposed method in off-policy contexts. It would also be beneficial to include more challenging tasks in the experiments for GI-PPO and to clarify the relationship between completeness and bias in analytical gradients. For MBPO, we suggest that the authors improve the clarity of their explanations by providing specific examples or empirical results that demonstrate the advantages of their method compared to fully known models. Furthermore, we recommend that the authors elaborate on the implications of model approximation on the training process to enhance understanding of its complexity. Finally, we suggest moving Algorithm 1 from the appendix to the main text for better accessibility.