ID: wlcm21C4nk
Title: Advancing Training Efficiency of Deep Spiking Neural Networks through Rate-based Backpropagation
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 6, 5, 6, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel rate-based backpropagation method for training spiking neural networks (SNNs), which effectively reduces computational and memory costs by separating the time-dependent backpropagation process. The authors propose leveraging rate-encoded approximations to capture essential information, validated through empirical experiments across multiple datasets, demonstrating superior training efficiency and accuracy compared to traditional Backpropagation Through Time (BPTT). The method's theoretical and empirical analyses indicate that it maintains performance comparable to BPTT while enabling more scalable SNN training.

### Strengths and Weaknesses
Strengths:  
1. The paper is well-written and organized, providing comprehensive documentation of the contributions.  
2. Empirical results on various datasets (CIFAR-10, CIFAR-100, ImageNet, CIFAR10-DVS) support the theoretical claims, ensuring accuracy while reducing memory and time costs.  
3. The proposed method addresses high time and memory costs in SNN training, offering solid theoretical insights into error bounds related to BPTT training.

Weaknesses:  
1. Important details, such as the top-level algorithm of the proposed method and experimental setup, are relegated to the appendix and should be included in the main manuscript.  
2. The novelty of the approach is questioned due to similarities with existing methods, necessitating a clearer distinction from prior works.  
3. There is a lack of experimental proof regarding the claimed reduction in training time, and generalization results on hyperparameters are not presented.  
4. The method's applicability to sequential tasks is limited, and additional experiments are needed to demonstrate scalability across different datasets and hyperparameter values.

### Suggestions for Improvement
We recommend that the authors improve the manuscript by moving critical details from the appendix to the main text for better clarity. Additionally, please clarify the differences between your approach and those outlined in references [1] and [2]. To strengthen your claims, we suggest including experimental proof of the training time reduction and generalization results on hyperparameters, particularly for values of $\lambda$ beyond 0.2. Furthermore, consider supplementing experiments with additional datasets to validate the scalability of the proposed method. Lastly, please ensure that all figures, such as Fig. 3, are correctly labeled, including any missing placeholders.