ID: Pjky9XG8zP
Title: One Less Reason for Filter Pruning: Gaining Free Adversarial Robustness with Structured Grouped Kernel Pruning
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 6, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Smoothly Robust Grouped Kernel Pruning (SR-GKP), a method aimed at enhancing adversarial robustness in convolutional neural networks (CNNs) through a novel approach to filter grouping and kernel selection based on Grouped Kernel Pruning (GKP). The authors claim that SR-GKP operates in a one-shot and data-free manner, followed by fine-tuning on the original training set, without incurring additional costs compared to other pruning methods. The empirical results indicate that SR-GKP improves adversarial robustness while maintaining benign accuracy. The paper also demonstrates superior performance compared to existing methods like HARP and MAD, with detailed experimental setups and results showing consistent outperformance across various metrics, even with a higher fine-tuning budget. The authors clarify the differences between structured and unstructured pruning, emphasizing that SR-GKP's approach is distinct and effective in the context of adversarial robustness.

### Strengths and Weaknesses
Strengths:
1. The problem addressed is significant and relevant to current research in adversarial robustness.
2. The proposed method is innovative and technically sound, demonstrating merit through empirical evidence.
3. The paper provides a comprehensive benchmarking study of adversarial robustness across various pruning methods.
4. The paper effectively demonstrates the superiority of SR-GKP over HARP and MAD through comprehensive experiments and clear data presentation.
5. The authors address reviewer concerns thoroughly, enhancing the clarity and depth of the discussion regarding pruning methodologies.

Weaknesses:
1. The baselines used for ImageNet experiments are inadequate, lacking recent comparisons such as [a], [b], [c].
2. There is a notable absence of ablation studies concerning the hyperparameter $\alpha$.
3. Some reviewers noted ambiguity in the term "one-shot," which could confuse readers unfamiliar with pruning terminology.
4. The novelty of the method was questioned, as it appears to combine existing concepts without a strong theoretical foundation.
5. The paper does not adhere to the NeurIPS 2023 template, particularly in Figure 2.
6. The writing quality requires improvement, with several typos and unclear phrasing.

### Suggestions for Improvement
We recommend that the authors improve the baseline comparisons for ImageNet by incorporating recent methods like [a], [b], and [c]. Additionally, conducting ablation studies regarding the hyperparameter $\alpha$ would provide valuable insights into its selection and impact. We suggest improving the clarity of the term "one-shot" by explicitly distinguishing their method from others, particularly the OTO library, to avoid confusion among end-users. Furthermore, including a more comprehensive discussion of existing unstructured pruning methods and their potential insights for structured pruning would enhance the paper's depth and relevance. We also suggest adhering to the NeurIPS 2023 template for formatting, particularly for Figure 2. Lastly, enhancing the clarity of the writing and correcting typographical errors will improve the overall presentation of the paper.