ID: 8WvYAycmDJ
Title: MixFormerV2: Efficient Fully Transformer Tracking
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 8, 5, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an efficient pure-Transformer-based tracking framework, MixFormerV2, which replaces dense prediction heads and complex updating-score mapping modules with simple four-box tokens, streamlining the tracking pipeline. The authors propose Dense-to-Sparse Distillation and Deep-to-Shallow Distillation to alleviate the computational burden of the transformer architecture. Experimental results indicate a favorable trade-off between tracking accuracy and speed.

### Strengths and Weaknesses
Strengths:
- The motivation for achieving efficient and effective tracking with Transformers is significant for balancing performance and speed.
- Experimental results demonstrate that the proposed method enhances inference speed while maintaining excellent performance.
- The writing is clear and well-organized, making it easy to follow.

Weaknesses:
- The method is only applied to one baseline tracker, MixFormer, limiting its generality across other Transformer trackers.
- A detailed analysis of the pruned parameters is lacking, particularly regarding the relationship between performance and prune ratio.
- The training time for different distillation strategies is not provided, impacting the method's practicability.
- The clarity of Table 3(e) is questionable, particularly regarding the performance implications of using or not using PMDP.
- The paper does not include comparisons with the LaSOT extension benchmark or the HCAT method, and the amount of parameters is not reported.

### Suggestions for Improvement
We recommend that the authors improve the generality of the proposed method by applying it to additional Transformer trackers. Additionally, please provide a detailed analysis of the pruned parameters and the training time for different distillation strategies. Clarifying the description of Table 3(e) and including comparisons with the LaSOT extension benchmark and HCAT method would enhance the paper. Finally, we suggest including the amount of parameters and discussing limitations, including failure cases and areas for improvement.