ID: 43s8hgGTOX
Title: OpenDebateEvidence: A Massive-Scale Argument Mining and Summarization Dataset
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 7, 7, 7, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the OpenDebateEvidence (ODE) dataset, the largest publicly available dataset for argument mining and summarization, comprising 3.5 million documents from various debate formats. The authors demonstrate that fine-tuning large language models (LLMs) on this dataset enhances performance in argument summarization tasks, as evidenced by improvements in ROUGE and perplexity scores. The dataset is rich in metadata, which supports various argumentation tasks beyond summarization.

### Strengths and Weaknesses
Strengths:
- The ODE dataset is the largest of its kind, significantly contributing to the computational argumentation community.
- The dataset's rich metadata enhances its utility for various tasks.
- Empirical evidence shows that fine-tuning LLMs on ODE improves performance on other datasets like DebateSum and BillSum.

Weaknesses:
- The originality of ODE is questioned due to similarities with existing datasets like DebateSum and BillSum.
- The deduplication process lacks modern NLP techniques, potentially leaving semantic redundancies.
- The paper does not clearly define summarization tasks, leading to confusion about the nature of the outputs.

### Suggestions for Improvement
We recommend that the authors improve the clarity of definitions for abstractive and extractive summarization and provide a clear task formulation. Additionally, a comparison of ODE with existing datasets, including a table highlighting differences, would enhance understanding. The authors should also address potential semantic duplication by employing modern deduplication techniques and provide explicit train/test/validation splits to assess overlaps. Furthermore, including evaluations on tasks beyond summarization, such as argument mining and assessment, would broaden the dataset's applicability. Lastly, we suggest that the authors clarify the reliability of GPT-4 as a judge by providing inter-human agreement rates for comparison.