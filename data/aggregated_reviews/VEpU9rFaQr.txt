ID: VEpU9rFaQr
Title: Auditing for Human Expertise
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 7, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a statistical framework to assess whether human experts utilize additional information beyond what is captured by algorithmic predictions. The authors formalize this inquiry through a hypothesis test that evaluates the independence of human predictions from outcome variables, conditioned on input features. The method, termed ExpertTest, is applied to real-world medical data from emergency departments, revealing that physicians indeed incorporate valuable external information not included in standard algorithms.

### Strengths and Weaknesses
Strengths:
- The statistical approach is well-motivated, clearly demonstrating how to test for reliance on extra information by humans.
- The test's flexibility due to the choice of L allows for various statistical properties.
- The paper is concisely written, with a clear exposition of ideas and candid discussion of limitations.
- The experimental evaluation on real-world data provides compelling evidence for the validity of the proposed test.

Weaknesses:
- The method's reliance on datasets with pairs that are close in input space but distinct in feature space may limit generalizability.
- The simplicity of the conditional independence framework may not yield novel technical insights, and the results may not extend to high-dimensional settings.
- The paper does not adequately explore the interplay between human and algorithmic decision-making, nor does it address how the test could improve decision outcomes.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their method by conducting evaluations on multiple datasets and exploring synthetic data to better understand the test's sensitivity. Additionally, we suggest that the authors clarify the assumptions regarding the independence of variables and the functional forms of predictions. Addressing the interplay between human and algorithmic decision-making could strengthen the paper's contribution, particularly in demonstrating the implications for achieving complementarity in human-AI systems. Finally, enhancing the clarity of notation and providing more descriptive table captions would improve readability.