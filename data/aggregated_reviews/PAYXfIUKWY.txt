ID: PAYXfIUKWY
Title: Effective Robustness against Natural Distribution Shifts for Models with Different Training Data
Conference: NeurIPS
Year: 2023
Number of Reviews: 24
Original Ratings: 7, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a modification to effective robustness evaluations by incorporating multiple training sets, allowing for a more accurate comparison of models trained on different datasets. The authors empirically demonstrate that their method improves the prediction of out-of-distribution (OOD) accuracy. They specifically compare CLIP-based models with traditional ImageNet-trained models and propose a multi-ID evaluation approach, arguing it is superior to single-ID evaluations. The analysis of models trained on ImageNet, YFCC, and LAION indicates that CLIP models generally do not show greater effective robustness compared to ResNets, reinforcing existing empirical findings. However, concerns are raised regarding the clarity of the motivation for using multiple ID test sets, the validity of the comparisons, and the implications of different training losses.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with a clear introduction that effectively motivates the research problem.
- Section 3 offers novel insights into the problem of effective robustness comparisons among models with varying training data.
- The motivating experiment illustrates the limitations of using a single in-distribution (ID) test set.
- The authors have conducted a Kendall's rank correlation test to assess the relationship between single-ID and multi-ID effective robustness, providing quantitative evidence for their claims.
- The correlation results between single-ID and multi-ID effective robustness provide valuable insights into model performance.

Weaknesses:
- The manuscript contains numerous typos and grammatical errors that need correction.
- The motivation for the multi-ID evaluation is unclear, as it does not adequately address the fundamental differences in training loss between CLIP-based and traditional models.
- The fitting quality of the proposed method is not convincingly demonstrated, raising questions about its practical utility.
- The definition and selection of OOD test sets lack clarity, raising concerns about the theoretical backing of the experimental results.
- The methodology may not reliably reflect single-ID effective robustness for certain dataset and model combinations, leading to potential misinterpretations of model effectiveness.
- The claims made in the manuscript may overstate the relationship between multi-ID effective robustness and effective robustness.

### Suggestions for Improvement
We recommend that the authors improve the manuscript by conducting a thorough proofreading to address the identified typos and grammatical errors. Additionally, please clarify the practical implications of fitting quality in your experiments and provide a more detailed discussion on the choice of ID test sets, especially for models pre-trained on larger datasets. We suggest improving the clarity of the motivation by addressing the differences in training loss between CLIP-based and traditional models, rather than merely increasing the number of ID test sets. Furthermore, we recommend providing a more robust theoretical framework for the definition of OOD test sets to support the experimental results. To enhance generalizability, consider conducting experiments that demonstrate the consistency of effective robustness rankings across different ID test sets, potentially using a statistical test like Kendall's rank correlation to validate the interpretability of multi-ID effective robustness. Lastly, we suggest incorporating a limitations section that emphasizes that multi-ID effective robustness should be used as a complement to single-ID effective robustness and revising the abstract to reflect that the new evaluation metric provides a different estimate of effective robustness rather than a better one.