ID: 7hSVLwNbWT
Title: Coverage-based Example Selection for In-Context Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for selecting informative examples in in-context learning using a BERTScore-based set metric, which is proven to be submodular. The authors conduct experiments across multiple datasets and large language models (LLMs), demonstrating improvements in performance. The proposed set-based sample selection strategy aims to enhance the coverage of salient aspects in demonstration selection.

### Strengths and Weaknesses
Strengths:
1. The research problem of demonstration selection is significant.
2. The proposed method is simple and does not require training.
3. Extensive experiments with various LLMs and tasks are conducted.
4. The paper is clearly presented with concrete examples illustrating improvements.

Weaknesses:
1. The performance of the proposed method is heavily reliant on the scoring mechanism, with poor results noted for cosine and BM25 metrics.
2. The definition of "salient aspect" for cosine is unclear, particularly in its application in Equation (6).
3. Comparisons to related methods like Set-BM25 and Set-cosine are not included in Table 1.
4. There is a lack of significance tests and evaluation of computational efficiency.

### Suggestions for Improvement
We recommend that the authors clarify the definition of "salient aspect" for cosine in Section 5 and explain how Equation (6) is calculated. Additionally, we suggest including results for Set-BM25 and Set-cosine in Table 1 to enhance clarity in comparisons. Furthermore, we encourage the authors to provide significance tests and an analysis of computational efficiency in their experiments. Lastly, discussing related strategies from the domain of active learning, such as those in "Batch Active Learning at Scale," would enrich the paper's context.