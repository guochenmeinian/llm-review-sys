ID: iwOdPBePxL
Title: Concept Denoising Score Matching for Responsible Text-to-Image Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 6, 8, 4
Original Confidences: 4, 3, 3

Aggregated Review:
### Key Points
This paper presents a training objective that incorporates a learnable condition \( c \) added to the bottleneck layer of the UNet, referred to as the CodSMa objective. This allows the model to generate gender-neutral representations by sampling \( c \) during inference. For instance, when generating "An image of a doctor," the model samples \( c \) as "man" or "woman" equally. The authors demonstrate their method's effectiveness through extensive results, particularly in addressing safety and bias benchmarks.

### Strengths and Weaknesses
Strengths:
- The CodSMa objective is clear and sensible.
- The analysis of guidance in the h-space builds effectively on existing literature.
- The results are extensive and validate the proposed approach.

Weaknesses:
- The theoretical explanations in sections 3.2 and 3.3 lack clarity and can be misleading.
- In section 3.2, the connection to Classifier-Free Guidance is necessary, as the roles of \( \delta_n \) and \( \delta_p \) as guidance fields need clarification. The surprising nature of \( \delta_p \) guiding away from \( y_p \) and the difference in behavior between \( y \) and \( y_p \) require further explanation.
- The discussion of the gradient in section 3.3 is misleading, and the justification for equation 7 is unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the theoretical explanations in sections 3.2 and 3.3, particularly by elucidating the connection between the score differences and the CodSMa objective. Additionally, justifying the presence of equation 7 would enhance the paper's value. Clarifying when the additional guidance condition \( c \) is sampled, especially in relation to sensitive prompts, would also be beneficial. Finally, sharing loss curves and the distance between targets could provide insights into training dynamics and the faithfulness of identity-specific prompts. Regularization to maintain distance between sensitive attributes should be considered. 

Nits:
- Introduce \( L_{diff} \) explicitly above line 198.
- Address the missing square of the \( L_2 \) loss in equations 4 and 5, or adapt equations 6 and 7 accordingly.