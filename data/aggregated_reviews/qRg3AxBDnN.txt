ID: qRg3AxBDnN
Title: Learning the Visualness of Text Using Large Vision-Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the task of determining the visualness of sentences, creating a dataset through crowdsourced annotations for English sentences. The authors propose a fine-tuning strategy for large vision-language models, such as CLIP, to enhance the identification of visual aspects in text. Quantitative and qualitative experiments demonstrate the effectiveness of this approach compared to competitive baselines while maintaining performance in downstream text-to-image retrieval.

### Strengths and Weaknesses
Strengths:
1. The introduction of a novel task focused on identifying the visualness of text, relevant for applications like text-to-image retrieval and generation.
2. The curation of a dataset with human annotations for visualness, which can aid future research.
3. A fine-tuning strategy for large vision-language models that preserves downstream performance.
4. Comprehensive experimental validation showcasing the effectiveness of the proposed method.

Weaknesses:
1. The corpus size is relatively small, particularly the high-quality annotated portion.
2. The innovation appears limited, with the fine-tuning strategy being somewhat simplistic.
3. Insufficient quantitative analysis using generative models like DALL-E to validate the effectiveness of the text-visual semantic discrimination method.
4. Lack of clarity regarding the significance of the task and its potential benefits for other applications.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the significance of their work and outline the potential benefits of recognizing visual information in text. Additionally, we suggest conducting more extensive quantitative analysis using generative models, such as DALL-E, to substantiate the effectiveness of their method in downstream tasks. Furthermore, we encourage the authors to consider using a stronger baseline, such as BERT-large, instead of BERT-base, and to provide additional explanations regarding the filtering thresholds used in their experiments. Lastly, we advise addressing the concerns regarding the representation of visual scores and ensuring that the fine-tuning strategy optimizes both classification and generation performance under an open vocabulary setting.