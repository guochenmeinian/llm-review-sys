ID: odPKQiL2X8
Title: Exploring All-In-One Knowledge Distillation Framework for Neural Machine Translation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents All-in-One Knowledge Distillation (AIO-KD), which involves randomly extracting subnetworks from a teacher model to train multiple student models simultaneously. The authors propose a two-stage training scheme that includes dynamic gradient detaching and mutual learning to mitigate the negative impact of poorly performing students. The empirical results indicate that AIO-KD achieves state-of-the-art performance and the analyses of various hyper-parameters are insightful.

### Strengths and Weaknesses
Strengths:
- The engineering aspect of AIO-KD is well thought out and executed, demonstrating effectiveness through empirical results.
- The paper is well-organized and provides sufficient experimental analysis to support its claims.
- The ablation studies and analyses of hyper-parameters enhance understanding of the method's advantages and limitations.

Weaknesses:
- The paper assumes familiarity with neural machine translation, omitting essential preliminaries that could hinder broader comprehension.
- Some analyses lack thorough explanations or clear conclusions, raising questions about the randomness observed in hyper-parameter trends.
- The compared baselines are outdated, and more recent works should be included for a comprehensive evaluation.

### Suggestions for Improvement
We recommend that the authors improve the accessibility of the paper by including necessary preliminaries related to neural machine translation. Additionally, we suggest providing clearer explanations for the observed randomness in hyper-parameter trends and ensuring that all analyses are thoroughly discussed. Finally, we encourage the authors to compare AIO-KD against more recent baselines to strengthen the evaluation of their method.