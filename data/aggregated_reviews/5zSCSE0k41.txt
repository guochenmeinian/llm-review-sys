ID: 5zSCSE0k41
Title: VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 8, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents VASA, a novel method for generating talking head videos using a diffusion model based on Transformer architecture. The authors propose a disentanglement approach to control gaze, expression, and camera position, achieving impressive qualitative results that outperform existing methods. The paper emphasizes the practical application of generating highly realistic talking head avatars with real-time capabilities.

### Strengths and Weaknesses
Strengths:
- The qualitative results are impressive, likely surpassing state-of-the-art methods.
- The idea of disentangling the representation from existing work is innovative.
- The model demonstrates fast inference on consumer-grade GPUs.
- The paper is well-structured and easy to understand.

Weaknesses:
- Many details necessary for reproducibility are missing, including specifics on the CAPP score and training procedures.
- The dataset usage and preprocessing details are unclear, particularly regarding the VoxCeleb dataset and the new OneMin-32 dataset.
- The architecture's modifications for disentanglement are not well-explained.
- Comparisons with recent methods are limited, and the fairness of comparisons against smaller datasets is questionable.
- The paper lacks clarity on the implementation of condition signals in the network.

### Suggestions for Improvement
We recommend that the authors improve the reproducibility of their work by providing detailed information on the training procedures and datasets used, including the size and type of the OneMin-32 dataset. Clarifying the architecture modifications and the implementation of condition signals within the network is essential. Additionally, including comparisons with more recent methods, such as EMO, would strengthen the paper. Finally, we suggest that the authors consider open-sourcing their code to promote further research in the field.