ID: 6nLdWdTeos
Title: Learning Dynamic Representations for Discourse Dependency Parsing
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a hierarchical transition-based model for discourse dependency parsing, utilizing a GAT-based encoder to represent sub-trees from parsing history. The authors propose a novel parsing model that leverages these representations, demonstrating superior performance across multiple datasets compared to previous methods. The analysis indicates that the model effectively handles complex discourse structures.

### Strengths and Weaknesses
Strengths:
- The model is well-described, and empirical results provide strong evidence of its effectiveness.
- The approach to modeling partial structures during parsing is intuitive and yields better results than prior methods.

Weaknesses:
- The paper lacks sufficient baseline comparisons and ablation studies, particularly regarding the impact of specific model designs.
- There is no analysis of the model's efficiency, raising concerns about potential computational costs associated with encoding sub-tree structures.
- Clarity issues exist, particularly in explaining the algorithm and hyperparameters, which could hinder reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the algorithm by providing a fully-worked step-by-step example in an appendix. Additionally, the authors should include a discussion of hyperparameters, specifying dimensions of parameter matrices and the number of EGAT layers used. We also suggest conducting ablation studies to analyze the contributions of different model designs, particularly in comparison to similar hierarchical models like “Zhou22.” Furthermore, addressing the efficiency of the model and considering a commitment to release code would enhance reproducibility.