ID: ZX6CEo1Wtv
Title: Latent Diffusion for Neural Spiking Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 6, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Latent Diffusion Model for Neural Spiking Data (LDNS), which integrates autoencoders for low-dimensional representation with denoising diffusion models to generate realistic neural spiking data. The authors apply LDNS to three datasets: a synthetic dataset from the Lorenz system and two neuroscientific datasets, demonstrating its capability for conditional generation based on behavioral covariates. The model adapts to variable-length discrete time series and incorporates a Poisson loss for discrete spiking data, utilizing structured state space sequence (S4) models for effective training.

### Strengths and Weaknesses
Strengths:
- The work is original, utilizing latent diffusion models (LDMs) for generating neural population activity, which has not been previously explored.
- The technical implementation is solid, with appropriate adaptations for the discrete nature of neural data and effective handling of variable-length sequences.
- The manuscript is well-written, clearly explaining key concepts and experimental designs, with high-quality figures that enhance understanding.

Weaknesses:
- The evaluation of the model lacks comprehensiveness, primarily comparing against LFADS in only one of the three experiments, which limits the assessment of LDNS's overall utility.
- The reliance on basic statistics (firing rates, pairwise correlations) for evaluation does not adequately capture the dynamics of neural population activity, and comparisons with other models are insufficient.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of the LDNS model by incorporating comparisons with additional generative models beyond LFADS, such as Poisson Interpretable VAE and Targeted Neural Dynamical Modeling. This would provide a clearer context for assessing the model's capabilities. Additionally, we suggest that the authors analyze the sampled neural population dynamics using established methods for dimensionality reduction to better understand the model's performance. 

Furthermore, we encourage the authors to clarify the significance of the spike-history filter, including why it is trained post-hoc and the implications of using the softplus approximation over the exponential function. Joint training of the spike-history filter with the autoencoder could also be explored to enhance model stability. Lastly, addressing the privacy concerns related to human data usage is essential for ethical compliance.