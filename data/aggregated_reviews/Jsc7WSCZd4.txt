ID: Jsc7WSCZd4
Title: SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 7, 7, 6, 9, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SUGARCREPE, a benchmark designed to address biases in existing benchmarks for evaluating the compositional understanding of vision-language models (VLMs). The authors identify that current benchmarks allow for exploitation of structural biases, leading to misleading evaluations. SUGARCREPE utilizes ChatGPT to generate fluent hard negatives and employs an adversarial refinement mechanism to mitigate these biases. The authors re-evaluate state-of-the-art VLMs, revealing that previous performance improvements may have been overestimated. Additionally, the paper introduces a novel dataset, SugarCrepe, aimed at generating hard negatives for evaluation in image captioning tasks. The authors propose a semi-automatic dataset curation pipeline that combines language model (LLM) generation with manual filtering to ensure the quality and validity of generated captions. They address concerns regarding the reliability of LLM-generated data, potential biases, and the controllability of the generation process, providing quantitative definitions for fluency and reasonableness, utilizing grammar and plausibility scores to evaluate the generated captions.

### Strengths and Weaknesses
Strengths:
1. The innovative use of language models for generating hard negatives with an adversarial refinement mechanism is both practical and effective.
2. The incorporation of a human filtering stage enhances the reliability of the dataset.
3. The paper provides strong empirical evidence demonstrating the limitations of existing benchmarks, particularly ARO and CREPE.
4. The use of adversarial refinement techniques demonstrates a proactive approach to minimizing biases in generated captions.
5. The methodology is well-documented, with clear details on prompts and operations for implementation, and includes detailed explanations in response to reviewer queries, improving overall transparency.

Weaknesses:
1. The paper lacks detailed analysis of the computational costs associated with using language models, particularly regarding query costs and dataset effectiveness.
2. There is a need for a concrete definition and quantitative analysis of fluency and reasonableness for the generated hard negatives.
3. The dependency on the language model's capabilities raises concerns about consistency due to potential updates from OpenAI.
4. The potential for textual bias from the language model remains unaddressed.
5. There is a lack of solid quantitative evidence to demonstrate that manual verification can consistently overcome the challenges posed by potential shifts in the behavior of the LLM.
6. The discussion on the implications of model changes in ChatGPT could be further elaborated to strengthen the argument.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the computational costs of using language models, including evaluating query costs and their impact on dataset effectiveness. Additionally, providing a concrete definition and quantitative analysis of fluency and reasonableness for the generated hard negatives would strengthen the paper's motivation. The authors should clarify how they ensure consistency in experiments given the variability of the language model used. Addressing the potential for textual bias in LLM-generated content in the limitations section would enhance the paper's rigor. Furthermore, we recommend that the authors provide more solid arguments and quantitative evidence regarding the robustness of manual verification against model behavior shifts. It would also be beneficial to add a section discussing the limitations and challenges posed by potential biases in LLMs, as well as the implications of model updates on the dataset's quality.