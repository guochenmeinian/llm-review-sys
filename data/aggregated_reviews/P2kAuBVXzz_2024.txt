ID: P2kAuBVXzz
Title: It's All Relative: Relative Uncertainty in Latent Spaces using Relative Representations
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 6, 5, 6, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for addressing uncertainty in neural networks' latent spaces by utilizing relative representations, which compare features against fixed anchor points. The authors propose an alignment score, $\rho{}$, to measure the separability of latent observations. Empirical results indicate that transforming embeddings into a space of relative proximity enhances alignment between latent observations, particularly when sampling along a curve connecting two independently trained neural networks.

### Strengths and Weaknesses
Strengths:  
- The paper introduces a novel alignment score and provides a clear mathematical formulation of the problem.  
- It offers practical insights into reducing uncertainty in neural network ensembles and discusses limitations effectively.  
- The work is well-organized and mostly well-written, with a good citation of previous research.

Weaknesses:  
- The results and conclusions lack robustness, primarily validating existing methods without significant new insights.  
- The choice of older models (VGG-16 and Preactivation-ResNet-110) limits the generalizability of findings, and the relevance of these architectures to current tasks is not adequately addressed.  
- Clarity issues persist, particularly regarding the elimination of uncertainty $\sigma_R^2(z_i)$ and the rationale behind certain equations.

### Suggestions for Improvement
We recommend that the authors improve clarity in the introduction by explicitly discussing the motivation behind their work and addressing how the relative representation framework eliminates $\sigma_R^2(z_i)$. Additionally, we suggest incorporating more recent architectures, such as Vision Transformers or EfficientNet, to strengthen the findings. The authors should also provide a more comprehensive discussion of the limitations of the models used and explore the impact of varying the number of models (K) and anchors on the results. Finally, further clarification of the rationale behind Eqs. (3â€“5) is necessary to enhance understanding.