ID: TjZwZ6xUbw
Title: Scalable and Provably Fair Exposure Control for Large-Scale Recommender Systems
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a scalable recommendation method for fair exposure, addressing the limitations of current fair recommendation methods in large-scale scenarios. The authors utilize an iALS method as the backbone and optimize the model with an ADMM-based technique, claiming that the scalability issue in recommendation systems remains a significant challenge. However, concerns arise regarding the novelty of the contributions beyond classical methods like iALS and ADMM. The authors extend iALS to exADMM, effectively addressing fairness concerns and providing theoretical convergence assurances through extensive experiments on three datasets.

### Strengths and Weaknesses
Strengths:
- The research question is significant and relevant to web applications.
- The introduction of exADMM effectively addresses fairness concerns with theoretical guarantees.
- Extensive experiments demonstrate improved fairness outcomes and computational efficiency.

Weaknesses:
- The paper lacks clarity in writing and reader-friendliness, particularly in explaining related work and positioning within the field.
- There is confusion regarding the regularizer R_ex, with unclear definitions and implications.
- The reliance on the Gini index for measuring unfairness is questioned, and the absence of relevant baselines in the fairness domain is noted.
- The experimental evaluation lacks clarity on data splitting and model training choices, which affects the reliability of comparisons.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in the related works section, to better position their contributions within the field of Large-Scale Recommender Systems. Additionally, the authors should clarify the definition and implications of the regularizer R_ex, ensuring logical consistency throughout the paper. It would be beneficial to explore other fairness definitions, such as user-side fairness, and to include more relevant baselines in the fairness domain for comprehensive comparisons. Furthermore, the authors should clarify how the data split is performed and provide a more systematic explanation of model training choices to enhance the reliability of their experimental results.