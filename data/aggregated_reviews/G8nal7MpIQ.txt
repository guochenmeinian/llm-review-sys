ID: G8nal7MpIQ
Title: Guide Your Agent with Adaptive Multimodal Rewards
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 6, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework called Multimodal Reward Decision Transformer (MRDT) that utilizes visual-text alignment scores from pre-trained vision-language models as reward signals in visual-based reinforcement learning. The authors propose training a return-conditioned policy based on Decision Transformer (DT) with improved reward signals learned from fine-tuned VLMs. The method demonstrates better generalization to unseen goals in environments from the OpenAI Procgen benchmark, addressing the goal misgeneralization problem. Additionally, the authors indicate that they will include RLBench experiments and additional experimental results across a wider range of tasks in the final manuscript to address reviewer concerns.

### Strengths and Weaknesses
Strengths:
1. The proposed method effectively leverages pre-trained VLMs for reward learning, presenting a promising direction for decision-making.
2. The framework is well-structured, with clear and informative figures, and shows significant performance improvements over prior work, such as InstructRL.
3. The analysis of multimodal rewards provides valuable insights into their quality and effectiveness in distinguishing different cases.
4. The authors demonstrate a willingness to incorporate feedback and improve their work by including RLBench experiments and diverse task results.

Weaknesses:
1. The experimental section lacks breadth, primarily focusing on three simple environments, which may not adequately demonstrate the method's effectiveness across diverse benchmarks.
2. There is insufficient comparison with existing literature, particularly with closely related works like Zest, which could provide a more comprehensive evaluation.
3. Concerns arise regarding the potential for goal misrepresentation and the unexpected behavior of multimodal rewards post-fine-tuning, which could lead to arbitrary policy learning.
4. The initial manuscript may have lacked sufficient experimental diversity, which the authors are now addressing.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by evaluating MRDT on a wider range of benchmarks, including more complex tasks that involve multimodal inputs, such as those from RLBench. Additionally, we suggest conducting comparisons with relevant literature, particularly Zest, to strengthen the evaluation. It would also be beneficial for the authors to address the goal misrepresentation problem more thoroughly and provide insights into the unexpected behavior of the fine-tuned reward model, particularly regarding its implications for online training and policy learning. Furthermore, we recommend ensuring that the additional experimental results are comprehensive and clearly demonstrate the effectiveness of their proposed methods across the diverse tasks mentioned.