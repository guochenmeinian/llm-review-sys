ID: erQDc72vyi
Title: Frozen-DETR: Enhancing DETR with Image Understanding from Frozen Foundation Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Frozen DETR, which utilizes frozen foundation models as feature enhancers to improve the DETR object detection framework. By integrating feature maps from models like CLIP into the pyramid feature maps and feeding them into the encoder, Frozen DETR enriches contextual information, thereby enhancing DETR's performance. The authors propose a decoupled design that allows for asymmetric input sizes, significantly reducing computational load while enabling smaller detectors to leverage larger foundation models. Additionally, the paper compares the performance and latency of DAB-DETR-DC5 and Frozen-DETR within the context of a Frozen DETR pipeline. The authors argue that the introduction of multi-scale feature maps in Frozen-DETR should result in increased latency due to the foundation model, while also addressing the computational complexities associated with single-scale and multi-scale detectors. They clarify that the use of deformable attention in multi-scale detectors mitigates latency, leading to a minimal FPS difference between the two models.

### Strengths and Weaknesses
Strengths:  
1. The paper effectively uses foundation models as plug-and-play modules, enhancing performance across various detectors.  
2. Extensive experiments and comparisons with state-of-the-art methods demonstrate the proposed method's effectiveness and robustness.  
3. The writing is clear, and the experiments validate design choices thoroughly.  
4. The article presents a simple yet clever design that effectively addresses performance-speed trade-offs.  
5. The authors have shown responsiveness to reviewer feedback, clarifying potential misunderstandings regarding the experimental setup and computational complexities.  

Weaknesses:  
1. The reliance on foundation models increases inference costs, necessitating a detailed analysis of the computational costs introduced.  
2. Performance improvements diminish with advanced models like Co-DETR during extended training periods, requiring further explanation.  
3. The feature fusion approach appears incremental, with limited substantial advancements beyond the primary claim. Testing with additional pivotal DETR models is essential for a comprehensive evaluation.  
4. There are inconsistencies in reported latency values for DAB-DETR-DC5 and Frozen-DETR, leading to confusion about their performance metrics.  
5. The claim regarding the parallelization of computations using cuda.Stream() lacks clarity, as it depends on GPU core availability, which may not guarantee improved performance.  

### Suggestions for Improvement
We recommend that the authors improve the analysis of the computational costs associated with using foundation models. Additionally, providing results from pivotal DETR methods, such as DN-DETR and Conditional-DETR, would strengthen the evaluation. An explanation for the diminishing performance with advanced models during longer training periods should be included. We also suggest improving the clarity of the latency comparisons between DAB-DETR-DC5 and Frozen-DETR, ensuring that the rationale behind the reported performance metrics is explicitly articulated. Furthermore, a more detailed explanation of the implications of using cuda.Stream() for parallel computations should be provided, addressing potential limitations in GPU core utilization. Finally, we recommend reorganizing the introduction for clarity and enhancing figure captions for better comprehension.