ID: hfgdwxbNOW
Title: Adapting a Generative Pretrained Transformer Achieves SOTA Performance in Assessing Diverse Physiological Functions Using Only Photoplethysmography Signals: A GPT-PPG Approach
Conference: AAAI
Year: 2024
Number of Reviews: 4
Original Ratings: 5, 7, 7, 7
Original Confidences: 4, 2, 4, 3

Aggregated Review:
### Key Points
This paper presents architectural variants of transformers, pretraining, and fine-tuning methods specifically for the PPG domain. The authors propose a foundation model that utilizes photoplethysmography signals to assess diverse physiological functions, employing a decoder-only transformer with a next sample prediction pre-training objective. The model is fine-tuned for various downstream tasks, including heart rate estimation and atrial fibrillation detection.

### Strengths and Weaknesses
Strengths:
- Extensive experiments conducted in the PPG domain.
- Development of PPG foundation models using a decoder-only transformer with a tailored loss function, embedding, and linear head.
- Results in Table 1 demonstrate promising outcomes.
- The paper is well-written, and the block diagram effectively represents the proposed method.
- Impressive qualitative results presented in the appendix.

Weaknesses:
- Lack of comparison with other transformer architectures for timeseries data.
- Writing clarity issues, particularly regarding the distinction between linear and attention-based prediction heads, and identifying SOTA algorithms in experiments.
- Table 1 is difficult to read; different performance metrics should be separated.
- Missing introduction of BP-SBP and unclear significance of the highlighted value of 9.56.
- Insufficient support for the claim that the foundation model can be used for downstream tasks without further fine-tuning.
- Missing details on model parameters, training time, and GPUs used, which are crucial for understanding scalability and feasibility.
- Absence of fine-tuning time comparisons with SOTA specialist models.
- Lack of ablation studies to clarify the impact of design choices like RMSNorm over LayerNorm and PoPE over traditional Positional Encoding.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in distinguishing between the linear prediction head and the attention-based prediction head. Additionally, we suggest including comparisons with other transformer architectures for timeseries data to strengthen the paper's contributions. To enhance Table 1, consider separating the different performance metrics for better readability. We also advise introducing BP-SBP and clarifying the significance of the highlighted value of 9.56. To support the claim regarding the foundation model's applicability to downstream tasks without fine-tuning, please provide relevant experimental evidence. Furthermore, including details about model parameters, training time, and GPUs used would greatly aid in assessing the model's scalability. Lastly, we recommend conducting ablation studies to elucidate the effects of design choices on the foundation model's training.