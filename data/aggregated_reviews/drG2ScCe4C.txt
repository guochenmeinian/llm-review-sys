ID: drG2ScCe4C
Title: Spoiler Detection as Semantic Text Matching
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel NLP task of spoiler matching, linking specific spoilers to episodes from TV shows, and introduces a dataset comprising text episode summaries and related user comments. The authors propose a two-step algorithm for identifying irrelevant comments before matching spoilers to episodes and benchmark the performance of BM25 alongside four other models. The dataset, which includes both hand-labeled and auto-labeled comments, is positioned as beneficial for long-range language models and related studies.

### Strengths and Weaknesses
Strengths:
- The paper defines an interesting and novel NLP task and dataset.
- The dataset preparation procedure is well-articulated and innovative.
- The results analysis is compelling, with clear examples provided.
- The paper is well-explained, and the case study identifies relevant challenges.

Weaknesses:
- The two-step solution may be unnecessary; a multiclass approach could simplify the problem.
- The claim regarding the auto-labeler's independence from episode-specific context is questionable, as performance could improve with additional context.
- The auto-labeler's performance metrics (77% precision and 81% recall) are considered low for its intended use.
- Concerns exist regarding the novelty of the approach, which may be solvable through few-shot prompt engineering.
- The dataset may be biased, with a limited number of hand-labeled comments for unseen shows.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the abstract, as it currently appears convoluted. Additionally, we suggest incorporating more evaluation metrics, specifically precision and recall, to provide a comprehensive assessment of the model's performance. The authors should also consider adding details on the data sampling process and explicitly state the contribution of the novel dataset in the introduction. Lastly, we encourage the authors to address the potential bias in the dataset and explore methods to mitigate summary truncation issues that could affect matching results.