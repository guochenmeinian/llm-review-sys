ID: gfqM0MyzLn
Title: SEHG: Bridging Interpretability and Prediction in Self-Explainable Heterogeneous Graph Neural Networks
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Self-Explainable Heterogeneous Graph Neural Network (SEHG) that addresses the interpretability and performance challenges of heterogeneous graph neural networks (HGNNs). SEHG integrates explanation generation into the learning process through two stages: generating explanations alongside predictions and enhancing prediction accuracy via contrastive learning. A significant feature of SEHG is its learnable heterogeneous masks, which provide explanations without relying on predefined metapaths. The authors introduce a synthetic dataset, HetBA, to evaluate explanations for heterogeneous graphs. Experimental results indicate that SEHG outperforms strong baselines in node classification tasks, achieving performance improvements of up to 9.44% and enhancing explanation fidelity by up to 46.57%.

### Strengths and Weaknesses
Strengths:
- The integration of explanation generation directly into the HGNN framework is novel and effectively presented.
- The comprehensive experimental evaluation demonstrates significant improvements in both performance and interpretability.
- The paper is well-written, with clear figures and sufficient theoretical details.

Weaknesses:
- The paper lacks an analysis of computational complexity, which is essential given the method's complexity.
- The selection of baseline methods is outdated, limiting the evaluation's robustness.
- The construction details of the HetBA dataset are minimal, raising concerns about its credibility and replicability.
- The justification for the range-based penalty in the training phase is unclear, and the effectiveness of joint training is not adequately compared to separate-stage training.

### Suggestions for Improvement
We recommend that the authors improve the analysis of computational complexity to provide insights into space and time requirements. Additionally, updating the baseline comparisons to include more recent HGNN methods would strengthen the evaluation. Clarifying the construction process of the HetBA dataset and providing more details on its uniqueness is crucial for validation. We also suggest justifying the range-based penalty's design and conducting a direct comparison between joint and separate-stage training approaches to support claims about the benefits of joint training. Lastly, addressing the adaptability of the proposed method to diverse datasets and providing a theoretical justification for model convergence would enhance the paper's rigor.