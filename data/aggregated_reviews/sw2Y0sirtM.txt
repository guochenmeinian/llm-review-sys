ID: sw2Y0sirtM
Title: A Unified, Scalable Framework for Neural Population Decoding
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 7, 8, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for neural population decoding using a transformer architecture that enhances decoding accuracy by representing input spike trains with event-based tokens and utilizing a latent representation to reduce attention matrix size. The model can identify new neurons from various experiments by freezing network parameters during gradient descent, contingent on the availability of behavioral recordings. The method is validated through multiple experiments involving motor cortical recordings from monkeys engaged in arm movement tasks.

### Strengths and Weaknesses
Strengths:
- A novel framework for neural population decoding.
- Efficient representation of recordings via event-based tokenization.
- Identification of units from new experiments consistent with previous findings.
- Demonstrated effectiveness across multiple experiments from different labs.
- Significant improvement in inference accuracy compared to strong baselines.

Weaknesses:
- The reliance on behavioral recordings limits the model's applicability to other contexts, such as visual cortex recordings.
- Clarity issues regarding the nature of commands (line 165) and the relationship between context window T and Tmax.
- Suggested experiments to probe unit identification fidelity are not conducted.
- Heavy reliance on large neural recordings may not be feasible for basic neuroscience research.
- Limited exploration of the model's application across diverse tasks beyond motor functions.

### Suggestions for Improvement
We recommend that the authors improve clarity by providing a mathematical description of commands if they are not given via natural language. Additionally, we suggest conducting a simple experiment to assess the fidelity of unit identification by shuffling unit order and comparing embeddings. It would also be beneficial to clarify the relationship between context window T and Tmax, and to provide more details on extending the model to self-supervised tasks. Finally, we encourage the authors to discuss the implications of their work in the context of generalizing across different brain regions and tasks, and to ensure that the term "foundation model" is used judiciously to avoid confusion.