ID: eeeqORvJbf
Title: Learning and processing the ordinal information of temporal sequences in recurrent neural circuits
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 6, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for training recurrent neural networks (RNNs) to extract ordinal sequences by leveraging tree-structured attractor dynamics. The authors propose that suitable training allows the network to learn these dynamics, facilitating the encoding of temporal sequences and improving robustness in tasks like keyword spotting. The study demonstrates that once the network learns the temporal structure, it can generalize this knowledge to new sequences, showing potential applications in machine learning and neuroscience.

### Strengths and Weaknesses
Strengths:
1. The manuscript articulates its key ideas clearly and effectively.
2. The core concept of training RNNs to recognize ordinal structures through varied temporal intervals is intriguing and has potential applications.

Weaknesses:
1. The results in Figure 3 lack interpretability due to insufficient control models, making it unclear if the tree structure is responsible for performance improvements.
2. Figure 4's training regime may present a "warped" version of inputs, raising concerns about the network's ability to generalize beyond the training data.
3. The evaluation is limited to short sequences and supervised learning, which may not reflect the complexities of human cognitive processes.
4. The requirement for training on a wide range of temporal intervals is a significant limitation in modeling brain functions.

### Suggestions for Improvement
We recommend that the authors improve the interpretability of their results by including a systematic bank of control models in Figures 3 and 4. Additionally, providing further analysis on how the training regime affects the spectral structure of inputs would clarify the network's generalization capabilities. Expanding the evaluation to include longer sequences and unsupervised learning approaches could enhance the relevance of the findings to cognitive processes. Lastly, addressing the neuroscientific plausibility of the model's assumptions regarding tree-structured templates would strengthen the manuscript's arguments.