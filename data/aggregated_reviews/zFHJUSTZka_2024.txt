ID: zFHJUSTZka
Title: Direct Language Model Alignment from Online AI Feedback
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 4, 4, 5, 3, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents OAIF, an online method designed to align language models with human preferences by utilizing feedback from language models as a surrogate for human feedback. The core innovation of OAIF lies in the use of online-generated preference pairs throughout the training process. Experimental results indicate that switching from offline preference datasets to those labeled by other language models leads to responses that better align with human preferences. The work extends offline preference learning methods, such as DPO, to an online variant, demonstrating significant performance improvements over traditional offline methods.

### Strengths and Weaknesses
Strengths:
1. OAIF introduces on-the-fly generated preference pairs and AI-provided labels, enhancing the alignment process.
2. The paper is well-written and easy to follow, with comprehensive evaluations showing significant performance improvements over offline DAP and RLHF methods.
3. The experiments are well-designed, supporting the main claims of the paper.

Weaknesses:
1. The novelty of OAIF is questionable, as it appears to function similarly to existing methods that utilize preference models, raising concerns about its differentiation from prior work.
2. The rationale for performance gains from on-policy learning is inadequately clarified, lacking strong theoretical foundations or experimental evidence.
3. The performance comparison with other online methods like RSO and Iterative DPO is insufficient, and the limitations of using LLMs as annotators are not thoroughly addressed.
4. Some experimental comparisons are perceived as unfair, and the computational overhead introduced by on-policy sampling is not discussed.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of OAIF to clarify its novelty and differentiate it from existing methods. Additionally, we suggest including performance comparisons with other online methods such as RSO and Iterative DPO to strengthen the claims made. The authors should also provide experimental validation to support the effectiveness of using LLMs over traditional reward models, particularly addressing the distributional shift problem. Furthermore, we encourage the authors to analyze and discuss how on-policy data can mitigate overfitting and improve performance. Lastly, ensuring equal training data across different methods in experiments would enhance the validity of the results.