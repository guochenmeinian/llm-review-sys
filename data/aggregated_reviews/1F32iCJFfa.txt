ID: 1F32iCJFfa
Title: Schrodinger Bridge Flow for Unpaired Data Translation
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents $\alpha$-IMF, an incremental version of the IMF method for solving the Schrödinger Bridge problem, demonstrating its convergence properties and implementing it through an online learning approach. The authors propose parameterizing forward and backward control simultaneously by introducing a forward/backward indicator as an input variable to the neural network. Additionally, the paper introduces a novel theoretical framework for analyzing online schemes to solve the Schrödinger Bridge, with the $\alpha$-IMF scheme being the first online training algorithm for this purpose. A practical implementation, $\alpha$-DSBM, simplifies the process by using a single bidirectional network instead of two, enhancing scalability. The method shows promising results on toy data and image-to-image translation tasks, specifically on the Cat <-> Wild dataset, and additional experiments on CelebA validate the findings, showing improved performance metrics for $\alpha$-DSBM compared to DSBM.

### Strengths and Weaknesses
Strengths:
- The introduction of $\alpha$-IMF expands the theoretical framework and enables online learning.
- The results demonstrate significant improvements in alignment and quality metrics for $\alpha$-DSBM over DSBM.
- The paper connects the Schrödinger bridge with reinforcement learning and Sinkhorn flow, providing a comprehensive theoretical basis.
- Quantitative data supporting claims regarding parameter reduction and convergence rates are provided.

Weaknesses:
- The experimental evaluation on practical data is limited, with only incremental performance improvements noted on the Cat <-> Wild dataset, necessitating more evaluations on diverse datasets, such as Male <-> Female or Handbags <-> Shoes.
- The paper lacks initial quantitative data to substantiate claims of simplicity and faster convergence, and the fine-tuning duration raises questions about the efficiency of the proposed method, as it exceeds the base training time.
- A quantification of the advantages of bidirectional or online learning over DSBM, such as comparisons of wall-clock time or GPU memory usage, is lacking.
- Some theoretical assumptions may require further clarification, and the algorithm reflects $\alpha$ implicitly, with no analysis provided on its role. Insights or an algorithm to explicitly control $\alpha$ would enhance understanding.

### Suggestions for Improvement
We recommend that the authors improve the experimental evaluation by including more diverse datasets to assess the performance of $\alpha$-IMF comprehensively. Additionally, we suggest providing more quantitative data to support claims of simpler implementation and faster convergence, as well as clarifying the convergence rates in comparison with baseline methods, particularly addressing the fine-tuning duration. Quantifying the advantages of bidirectional learning or online learning over DSBM through metrics like wall-clock time or GPU memory usage would strengthen the paper. Furthermore, we recommend ensuring that all theoretical assumptions are clearly articulated in the revised version, along with a detailed discussion on parameter reduction and comparisons with DSBM and related methods. Finally, providing insights into the role of $\alpha$ and developing an algorithm that explicitly controls it would enhance the overall understanding of the proposed method.