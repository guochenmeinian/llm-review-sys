ID: AKAMNDe2Sw
Title: Boosting Adversarial Transferability by Achieving Flat Local Maxima
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 5, 3, 5, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called Penalizing Gradient Norm (PGN) aimed at enhancing the transferability of adversarial perturbations by encouraging flatness in the local landscape of adversarial examples. The authors empirically validate that adversarial examples situated at flat local minima exhibit improved transferability. The method employs a regularization technique that penalizes the gradient norm around the input sample and approximates the second-order Hessian matrix using the finite difference method for computational efficiency. Empirical results demonstrate that PGN significantly boosts transferability compared to existing methods.

### Strengths and Weaknesses
Strengths:
- The motivation for the proposed method is clear, and the writing is well-structured, facilitating understanding.
- The empirical validation of the assumption that flat local minima enhance transferability is original and insightful.
- Extensive experiments support the effectiveness of PGN in improving adversarial transferability.

Weaknesses:
- The method lacks rigorous theoretical support, particularly regarding the analogy drawn between perturbation optimization and model training.
- The paper does not provide standard variance for reported results, which undermines the significance of the findings.
- There is insufficient empirical and theoretical analysis of the finite difference method's effectiveness in optimization, particularly regarding its impact on the gradient norm.

### Suggestions for Improvement
We recommend that the authors improve the theoretical foundation of the proposed method by providing rigorous support for the claims made, particularly in Section 3.2. Additionally, we suggest including standard variance in the reported results to validate their significance. The authors should also conduct a detailed analysis of the finite difference method's effectiveness in speeding up optimization, including computational consumption and attack success rates with and without this method. Furthermore, a comparison between PGN and previous work, such as Reverse Adversarial Perturbation (RAP), should be included to clarify the novelty of the approach. Lastly, we encourage the authors to evaluate the performance of PGN on transformer-based models and clarify technical details in Section 3.3.