ID: 9cFyqhjEHC
Title: A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products and Graph Coarsening
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 6, 2, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CS-GNN, a subgraph GNN approach that utilizes graph coarsening and graph Cartesian products, allowing for flexible generation and processing of various bag sizes. The authors discover new permutation symmetries in the node feature tensor during generalized message passing, supported by both theoretical and empirical analyses. The authors propose a novel method leveraging message passing over product graphs, enhanced by symmetry-based updates, and provide a basis for equivariant layers. Experimental results indicate competitive performance with state-of-the-art methods.

### Strengths and Weaknesses
Strengths:
1. The topic of subgraph GNNs is interesting and relevant.
2. The paper is generally well-written with clarity and strong mathematical formulation.
3. The proposed method is novel, with solid theoretical foundations.
4. Empirical results appear promising, demonstrating competitive performance.
5. Implementation code is provided for reproducibility.

Weaknesses:
1. The size of the coarsened graph, though sparse, remains large with $2^n$ nodes, potentially leading to high computational complexity for large input graphs.
2. Key definitions, particularly regarding graph coarsening, are unclear and could benefit from improved presentation.
3. The significance of the theoretical analysis in Section 5 is not strongly demonstrated.
4. Limited experimental datasets and baselines raise questions about the robustness of the results.
5. The motivation for using subgraph GNNs is weak, particularly given the subpar experimental results compared to existing methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of key definitions, especially regarding graph coarsening, to enhance understanding. Additionally, providing a more detailed theoretical analysis in Section 5 would strengthen the paper's contributions. We suggest incorporating more diverse datasets, such as PCQM4Mv2, to validate the proposed method's effectiveness and comparing against a broader range of baselines to substantiate claims of competitive performance. Finally, addressing the motivation for subgraph GNNs in light of the experimental results would significantly bolster the paper's impact.