ID: 9cQ6kToLnJ
Title: Learning threshold neurons via edge of stability
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 4, 3, 6, 7, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the dynamics of neural network training, specifically focusing on the edge of stability (EoS). The authors explore gradient descent behavior in a simple sparse coding model, revealing a phase transition at the EoS where training dynamics shift from stable convergence to chaotic oscillations. They demonstrate that this transition is characterized by a power-law behavior and is relevant to practical neural network training scenarios, including a two-layer ReLU network and a deep neural network trained on CIFAR-10. The authors also analyze the dynamics of pairs of ReLU neurons, showing that large learning rates are necessary for learning non-zero biases and that EoS induces a phase transition in bias behavior.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a complex problem in neural network learning, providing novel insights into the EoS and the transition from dense to sparse representations.  
- The investigation is methodologically sound, with findings well-supported by data and analysis.  
- The presentation is clear and structured, making the research accessible to readers.  

Weaknesses:  
- The assertion that a large learning rate is necessary to learn the bias raises questions, particularly regarding the effects of freezing the bias during training, which the authors do not explore.  
- The simplistic model used may not fully capture the complexities of real-world deep learning applications, limiting the generalizability of the findings.  
- The focus on biases may be misleading, as freezing them did not significantly impact results, suggesting their role may not be as critical as implied.

### Suggestions for Improvement
We recommend that the authors improve their investigation into the effects of freezing the bias during training and discuss its implications more thoroughly. Additionally, extending the analysis to more complex models could enhance the relevance of their findings. Clarifying the role of biases in the dynamics of neural network training, especially in relation to the EoS, would strengthen the paper. Furthermore, addressing the questions raised about the initial bias in Fig. 1, the definition of "sharpness," and the step size for the $A$-dynamic would improve the manuscript's clarity and depth.