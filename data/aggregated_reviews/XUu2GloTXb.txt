ID: XUu2GloTXb
Title: Implicit Contrastive Representation Learning with Guided Stop-gradient
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 6, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Guided Stop-Gradient (GSG) method, which enhances self-supervised learning (SSL) algorithms like BYOL and SimSiam by selectively applying asymmetric predictors based on computed distances between embeddings. The authors demonstrate that GSG stabilizes training and improves performance across various downstream tasks, particularly at lower batch sizes and without predictors.

### Strengths and Weaknesses
Strengths:
1. The method is simple and practical, yielding consistent improvements in performance on pretraining and downstream tasks.
2. The experiments are solid, showcasing the effectiveness of GSG and its ability to enhance training stability.
3. The combination of asymmetric architectures with contrastive loss is intriguing and well-motivated.

Weaknesses:
1. The advantages of implicit contrastive loss over explicit methods remain unclear; further comparisons are needed to solidify claims.
2. The paper lacks in-depth analysis of the learned representation space and the effects of batch size and negative samples.
3. Some experimental choices, such as the use of large batch sizes, raise questions about optimality and benchmarking accuracy.

### Suggestions for Improvement
We recommend that the authors improve the analysis of the representation space to clarify why GSG outperforms baselines. Additionally, providing more empirical evidence regarding the selection of stop-gradient locations and the impact of batch size on performance would strengthen the paper. We also suggest including comparisons with explicit contrastive methods and exploring variations of the algorithm using more than two examples to enhance the robustness of the findings. Lastly, addressing the concerns regarding the accuracy benchmarks and the rationale behind batch size choices would further solidify the paper's contributions.