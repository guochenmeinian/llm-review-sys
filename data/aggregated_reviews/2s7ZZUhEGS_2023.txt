ID: 2s7ZZUhEGS
Title: MARBLE: Music Audio Representation Benchmark for Universal Evaluation
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 5, 6, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Music Audio Representation Benchmark for universaL Evaluation (MARBLE), aimed at enhancing AI's understanding of music through a comprehensive benchmark for Music Information Retrieval (MIR) tasks. The authors establish a unified protocol across 14 tasks and 8 publicly available datasets, categorized into four levels: acoustic, performance, score, and high-level description. The benchmark includes source separation and emphasizes the importance of robust model comparisons, particularly with Jukebox, which has shown superior performance due to its larger parameter set. While the benchmark focuses on discriminative tasks, it lacks generative tasks, which are also essential for music processing. The authors acknowledge the need for diversified metrics and dataset diversity, particularly across different music genres and eras, while maintaining a focus on mainstream music for the initial benchmark. They provide a toolkit and leaderboard to facilitate further research in music AI.

### Strengths and Weaknesses
Strengths:
- The paper offers a robust foundation for comparing models with a comprehensive task structure and unified protocols.
- It includes a toolkit and leaderboard, promoting transparency and reproducibility in the field.
- The benchmark encompasses a wide range of MIR tasks, including source separation, facilitating comprehensive evaluation.
- The authors have addressed concerns regarding data augmentation and industry relevance.
- Plans to include best practice guidelines for task deployment are a positive addition.

Weaknesses:
- The focus is primarily on coarse-grained discriminative tasks, neglecting fine-grained tasks and generative tasks, which are critical for music processing.
- The datasets used are limited in size and diversity, particularly across various music genres, periods, and cultures, potentially affecting the benchmark's generalizability.
- There is insufficient analysis of noise and augmentation techniques, which could enhance the benchmark's value.
- The lack of noise robustness analysis and the absence of a code library for replicability hinder the benchmark's effectiveness.
- Concerns about the originality of the work remain unaddressed.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by incorporating both fine-grained discriminative tasks and generative tasks, such as music generation. Additionally, the authors should consider expanding the dataset size and diversity to ensure better representation across different music genres, periods, and cultures. We suggest including a table summarizing the diversity of music genres, periods, and cultures in the benchmark. Conducting an analysis of noise robustness, including background and label noise, would enhance the benchmark's applicability in real-world scenarios. Furthermore, we encourage the authors to conduct a more in-depth analysis of noise and augmentation techniques and to include a few lines in the paper suggesting best practices for deployment for each task based on their analyses. Lastly, we suggest that the authors provide a detailed code library and additional information in the appendix to facilitate replicability and further research.