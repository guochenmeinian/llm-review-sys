ID: loJM1acwzf
Title: MMLONGBENCH-DOC: Benchmarking Long-context Document Understanding with Visualizations
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 9, 7, 7, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new benchmark, MMLONGBENCH-DOC, for assessing the long-document understanding ability of vision-language models (VLMs). The dataset comprises 1,062 expert-annotated questions derived from 130 lengthy documents, averaging 49.4 pages and 20,971 tokens each. The benchmark evaluates VLMs across three aspects: Information Identification (44.0% single-page questions), Cross-Page Comprehension (33.2% cross-page questions), and Hallucination Severity (22.8% unanswerable questions). The authors empirically investigate the performance of various large vision-language models (LVLMs), revealing a significant performance gap between models and human evaluators, underscoring the benchmark's relevance for long-range document understanding. Experiments on 14 VLMs reveal that the best-performing model, GPT-4o, achieves an F1 score of 42.7%, indicating significant challenges in long-context document understanding.

### Strengths and Weaknesses
Strengths:
- Clear and comprehensive overview of the benchmark construction and quality assurance.
- Rigorous methodology with extensive experiments and detailed analysis of results.
- Thorough annotation pipeline with well-trained human annotators ensuring high-quality labels through effective cross-checking.
- Focus on long-context document understanding, addressing a significant gap in existing literature.
- High-quality annotations ensured through expert involvement and quality control processes.
- The average document length of 49 pages is substantial and representative of real-world scenarios, balancing between trivial and overly complex documents.
- Granular evaluation includes a diverse range of models and evaluation metrics, effectively reflecting the current landscape of LVLMs.
- The benchmark's difficulty (generalized accuracy below 41%) provides valuable insights into the limitations of LVLMs in document understanding.

Weaknesses:
- Lack of clarity on whether the benchmark will be shared with the community.
- Insufficient details on annotator compensation and the time required for dataset construction.
- Limited error analysis depth and absence of discussion on potential biases in the dataset.
- Limited document diversity, as a significant portion of the dataset is sourced from existing datasets, leading to a lack of diversity in categories, particularly in academic papers.
- Visualization clarity issues, particularly with Figure 1, which lacks clarity and makes interpretation difficult.

### Suggestions for Improvement
We recommend that the authors improve the paper by providing a clear description of the benchmark's availability to the community. Additionally, include details on how annotators were compensated and the total annotation time required. Expanding the error analysis to include specific examples and root causes of errors would enhance the paper's insights. Furthermore, addressing potential biases in the dataset and discussing strategies for mitigation would strengthen the ethical considerations of the work. We suggest improving the diversity of the dataset by augmenting it with questions from underrepresented categories, particularly across various scientific disciplines. Enhancing Figure 1 for clarity and communication effectiveness is also recommended. We encourage the inclusion of a "human baseline" in future evaluations to contextualize the difficulty of the tasks for human respondents. Lastly, we recommend categorizing questions not only by the number of evidence pages but also by the specific capabilities required to answer them, such as grounding or reasoning abilities.