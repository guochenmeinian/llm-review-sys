ID: R635gF7lXD
Title: StructGPT: A General Framework for Large Language Model to Reason over Structured Data
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an Iterative Reading-then-Reasoning (IRR) framework for large language models (LLMs) that enhances their inference capabilities on structured data through external interfaces. The framework focuses on two main functions: collecting relevant evidence (*reading*) and inferring answers or planning steps (*reasoning*). The authors implement this framework across various tasks, including KG-based question answering, Table-based question answering, and DB-based Text-to-SQL, demonstrating improved reasoning performance in zero-shot and few-shot settings, although not outperforming prior supervised-tuning systems.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel combination of KG, table, and text-to-SQL approaches utilizing LLMs.
- It features a logical and well-structured writing style, with rich experimental results and case analyses.
- The experiments and error analysis are thorough and informative.

Weaknesses:
- The description of the approach is unclear, particularly regarding the interaction of prompts with the interface and the overall methodology.
- The performance of the framework is only on par with existing systems, and the reproducibility is hindered by the non-determinism of the LLM used.
- The content is somewhat limited, with instances of redundancy and a lack of small-scale validation experiments.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology, particularly in explaining how prompts interact with the interface and whether an agent-based approach is employed. Additionally, we suggest including discussions or experiments addressing the strong assumption regarding entity extraction and linking, as this impacts the end-to-end property of semantic parsers. It would also be beneficial to mention the retrieval date of ChatGPT to account for model version variations. Lastly, we encourage the authors to compare their method with existing open-source toolkits like langchain and consider releasing their code to enhance community engagement.