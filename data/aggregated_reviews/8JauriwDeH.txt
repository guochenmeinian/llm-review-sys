ID: 8JauriwDeH
Title: Near-Optimal Streaming Heavy-Tailed Statistical Estimation with Clipped SGD
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 5, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new concentration method to enhance the convergence bounds of clipped-SGD in online/streaming settings, particularly when the noise has bounded second moments. The authors propose bootstrapping Freedman's inequality through iterative refinement using a PAC Bayesian argument, which yields improved bounds closer to optimal than previous methods. The work also extends to applications in streaming heavy-tailed statistical estimation, including mean estimation and regression.

### Strengths and Weaknesses
Strengths:  
- The paper makes a significant contribution by sharpening known bounds for heavy-tailed clipped-SGD, with an interesting approach of bootstrapping Freedman's inequality via PAC Bayesian arguments.  
- The proposed method has a broad range of applications, which are presented thoroughly.  
- Improved error bounds for Clipped-SGD are achieved, extending applicability to smooth convex and Lipschitz convex objectives.  
- The clarity of presentation is commendable, making complex concepts accessible.  

Weaknesses:  
- The second moment bound assumption appears stronger than the variance bound used in prior work, necessitating clearer discussion on the implications of using Freedman's high-dimensional version.  
- The algorithm requires knowledge of problem-dependent parameters like $D_1$ and time horizon $T$, which complicates its application in streaming settings.  
- The presence of an extra $\log\log T$ term in the bounds is undesirable.  
- The proofs, particularly in section F, could benefit from a more streamlined presentation.  
- Minor details, such as missing square roots in equations and citation omissions, need addressing.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the second moment bound assumption and its implications in comparison to prior work. Additionally, consider exploring ways to reduce reliance on problem-dependent parameters like $D_1$ and $T$, possibly by incorporating parameter-free algorithms from the online learning community. Addressing the extra $\log\log T$ term in the bounds should also be a priority. Furthermore, we suggest including technique-proof sketches in the main paper to enhance clarity and integrating theoretical developments with practical applications more cohesively. Finally, incorporating experimental validation would strengthen the paper's claims regarding the proposed algorithms' effectiveness.