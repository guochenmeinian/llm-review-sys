ID: QWTCcxMpPA
Title: Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 8, 5, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "MATH-Vision (MATH-V): A Large-Scale Benchmark for Multimodal Mathematical Reasoning," introducing a dataset of 3,040 mathematical problems with visual contexts from real competitions. The dataset encompasses 16 mathematical disciplines and is graded across 5 difficulty levels, facilitating a comprehensive evaluation of large multimodal models (LMMs). The authors reveal a significant performance gap between LMMs and human performance on MATH-V, which has not been previously recognized, underscoring the need for advancements in LMMs. Comprehensive evaluations conducted across various settings and models further validate the dataset's quality and relevance. The categorization of problems allows for detailed error analysis.

### Strengths and Weaknesses
Strengths:
- MATH-V is a large-scale benchmark with high-quality problems from competitions, covering 16 disciplines and 5 difficulty levels, ensuring a comprehensive evaluation of LMMs.
- The dataset's diversity and challenge enhance its potential for future analyses, similar to existing textual datasets.
- The dataset provides valuable insights into the performance gaps between LMMs and human capabilities.
- Comprehensive evaluations across different models enhance the credibility of the findings.
- The paper is well-written, providing thorough details on data collection and evaluation, showcasing the limitations of current models in visual math reasoning.
- The authors have effectively addressed reviewer concerns, leading to improved scores from some reviewers.

Weaknesses:
- The division of difficulty levels lacks explicit operational criteria, relying too heavily on original competition grade requirements.
- Some problems contain irrelevant images, which may not be necessary for solving the questions, potentially affecting evaluation results.
- The manual annotation and voting methods for error analysis present scalability and objectivity limitations.
- There are lingering concerns regarding the dataset's potential data leakage, despite the authors' assurances.
- Clarification on the ethical use of collected problems may still be needed to fully satisfy all reviewers.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the difficulty level division by providing explicit evaluation rules based on mathematical concepts, reasoning steps, and computational complexity. Additionally, we suggest quantifying the extent of irrelevant images in the dataset and considering options such as removing them, reducing their prevalence, or creating separate test sets for relevant and irrelevant images. Furthermore, developing an automated multimodal mathematical reasoning evaluation framework could enhance scalability and objectivity in error analysis by decomposing the reasoning process into subtasks and designing dedicated evaluation metrics for each. We also recommend that the authors improve clarity regarding the dataset's integrity by providing more detailed evidence against data leakage concerns. Lastly, further elaboration on the ethical considerations surrounding the use of collected problems could strengthen the paper's position, and addressing the qualifications of annotators for competition-level math questions and conducting a rigorous data contamination analysis would strengthen the study's validity.