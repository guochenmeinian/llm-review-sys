ID: 3EcjsgPq74
Title: Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of design choices for sparse feed-forward networks (S-FFN) in large language models, focusing on memory block size and selection methods within a unified framework of sparse neural memory. The authors propose a new gating method, Avg-K, which selects memory blocks based on averaged hidden states and demonstrates improved perplexity over existing methods like the Switch Transformer. The paper provides a comprehensive evaluation of various S-FFN architectures, offering insights into their effectiveness and efficiency.

### Strengths and Weaknesses
Strengths:
- The paper introduces a unified framework for analyzing S-FFN architectures, providing innovative insights into the impact of design choices.
- The proposed Avg-K method shows strong empirical results, outperforming existing methods and contributing to the understanding of MoE approaches.
- The writing is clear and the analyses of hyper-parameters and baselines are comprehensive.

Weaknesses:
- The analysis lacks integration with engineering implementation, particularly regarding scalability and communication costs in larger models.
- There is insufficient exploration of the communication overhead among experts and the potential impact of load balancing on the Avg-K method.
- The findings may become dated without consideration of rapid advancements in the field of natural language processing.

### Suggestions for Improvement
We recommend that the authors improve the exploration of the scalability of the Avg-K method, particularly in the context of model parallelism and larger language tasks. It would be beneficial to include experiments that address the communication overhead and load balancing issues associated with the Avg-K method. Additionally, we suggest providing deeper insights into the rationale behind the selection of the Avg-K approach and comparing it with more recent MoE routing methods like X-MoE.