ID: UDqHhbqYJV
Title: Can Language Models Solve Graph Problems in Natural Language?
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 8, 6, 8, 7, 8, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Natural Language Graph (NLGraph) benchmark, which evaluates the graph reasoning capabilities of large language models (LLMs) through 29,370 problems across eight graph reasoning tasks of varying complexity. The authors investigate whether LLMs can effectively map textual descriptions of graphs to grounded conceptual spaces and solve graph algorithm problems. The findings indicate that while LLMs exhibit preliminary graph reasoning abilities, the effectiveness of advanced prompting diminishes with complex tasks, and LLMs are susceptible to spurious correlations. The authors propose two new prompting techniques, Build-a-Graph and Algorithmic Prompting, which enhance performance on simpler tasks.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated and clearly written, addressing a significant gap in evaluating LLM capabilities with a comprehensive benchmark.
- The NLGraph benchmark is extensive, covering a range of graph reasoning tasks and providing valuable insights into LLM performance.
- The experiments are well-designed, revealing counter-intuitive findings about LLM behavior and prompting strategies.

Weaknesses:
- Evaluations are limited to text-davinci-003, which may restrict the generalizability of the conclusions.
- The motivation for selecting specific graph reasoning tasks is insufficiently justified, and the proposed prompting strategies lack testing on graphs with real-world semantics.
- The paper does not systematically compare the performance of different LLM generations, which could provide deeper insights into advancements in LLM capabilities.

### Suggestions for Improvement
We recommend that the authors improve the justification for the selection of the eight graph reasoning tasks to clarify their relevance to real-world applications. Additionally, the authors should test the proposed prompting strategies on graphs with real-world semantics to enhance their applicability. To strengthen the study, we suggest including a systematic comparison of performance across multiple LLM generations, such as GPT-3, GPT-3.5, and GPT-4, to better understand the impact of instruction fine-tuning and advanced prompting techniques. Finally, providing complete prompts for more complex strategies would enhance reproducibility and clarity.