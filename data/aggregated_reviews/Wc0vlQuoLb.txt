ID: Wc0vlQuoLb
Title: I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 3, 6, 4, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a training-based confidence calibration method, named IDK-tuning, aimed at improving the factuality of large language models (LLMs). The authors introduce a special `[IDK]` ("I Don't Know") token to the model's vocabulary and modify the training objective to shift some probability mass from incorrect predictions to this token, thereby encouraging explicit expression of uncertainty. The results indicate that IDK-tuned models perform better on factual benchmarks, showing increased precision with only a slight decrease in recall.

### Strengths and Weaknesses
Strengths:
- The connection between confidence calibration and LLM hallucination is compelling and well-articulated.
- The IDK-tuning method is intuitive and presents a novel approach to uncertainty modeling without requiring extensive human annotation or synthetic data.
- Extensive experiments on scaling behavior and ablation studies are conducted, providing robust insights into the method's effectiveness.
- The authors acknowledge limitations and singularities in their results, demonstrating transparency in their findings.
- The writing is clear and the paper is easy to follow.

Weaknesses:
- Precision improvements in multiple-choice QA tasks are lower compared to factual sentence completion tasks, suggesting a need for more detailed results on `lm-eval-harness` tasks.
- The absence of code or tuned model checkpoints limits reproducibility.
- The theoretical grounding of the method is questioned, particularly regarding the mathematical expressions used and the handling of uncertainty.
- The evaluation lacks comparisons with other uncertainty quantification methods, which could provide a broader context for the findings.
- The IDK-tuning setup may require prohibitive additional fine-tuning, and its application in pretraining from scratch remains unclear.

### Suggestions for Improvement
We recommend that the authors improve the theoretical foundation of their approach by addressing the mathematical expressions and ensuring they align with common practices. Additionally, we suggest that the authors compare their method against established uncertainty quantification techniques to contextualize their findings better. Providing detailed results for each `lm-eval-harness` task, especially for multiple-choice QA, would enhance the paper's applicability. Finally, including code or tuned model checkpoints would significantly improve reproducibility and facilitate further research in this area.