ID: MCkUS1P3Sh
Title: Nash Regret Guarantees for Linear Bandits
Conference: NeurIPS
Year: 2023
Number of Reviews: 24
Original Ratings: 6, 6, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into Nash regret within the framework of stochastic linear bandits, defining Nash regret as the difference between the optimal reward and the geometric mean of expected rewards. The authors propose an algorithm that employs elimination phases and utilizes the John ellipsoid technique for sampling, alongside a G-optimal design to minimize confidence interval widths. The theoretical analysis shows that the proposed algorithm achieves a Nash regret of approximately $\tilde{O}(\sqrt{d/T})$ for finite arms and $\tilde{O}(d^{5/4}/\sqrt{T})$ for infinite arms.

### Strengths and Weaknesses
Strengths:
- The paper addresses the novel concept of Nash regret in stochastic linear bandits and provides a detailed algorithm with regret analysis.
- The technical foundation is solid, particularly in the finite-arm case where the upper bound is tightly bounded.

Weaknesses:
- The paper lacks a discussion on the computational aspects of finding the G-optimal distribution and sampling via the John ellipsoid method.
- There are no experimental results to support the theoretical claims.
- The regret bound for infinite arms may not be tight, as it involves a factor of $d^{5/4}$ rather than just $d$.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the computational aspects of the G-optimal distribution and the John ellipsoid method. Additionally, including experimental results comparing their algorithm with existing bandit algorithms would strengthen the submission. It would also be beneficial to elaborate on the reason for the factor of $5/4$ in the infinite arms case and to provide more context regarding the related work, particularly concerning Nash regret in multi-armed bandits.