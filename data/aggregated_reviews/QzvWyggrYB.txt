ID: QzvWyggrYB
Title: Large Language Models Must Be Taught to Know What They Donâ€™t Know
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 3, 7, 6, 7, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the calibration of large language models (LLMs) by estimating reliable confidence in their responses. The authors demonstrate that existing methods are inadequate for accurate confidence estimates, particularly as LLMs scale. They argue for the necessity of fine-tuning, exploring three approaches: Probe, LoRA, and LoRA + Prompt, confirming their effectiveness through experiments. The study also highlights the generalizability of these methods and their implications for user decision-making. Additionally, the authors clarify their fine-tuned calibration model's distinctions from the work of Kadavath et al. and emphasize their focus on open-ended generation, which is crucial for understanding their conclusions.

### Strengths and Weaknesses
Strengths:
- Originality: The paper introduces three novel fine-tuning methods for confidence estimation in LLM responses, particularly the combination of LoRA and prompts.
- Quality: The experiments are well-motivated and effectively validate the authors' hypotheses.
- Clarity: The writing is clear and accessible, making the paper easy to follow.
- Significance: The conclusion that fine-tuning is essential for accurate confidence prediction is valuable, and the exploration of user decision-making benefits is significant.
- Distinction: The authors effectively clarify their model's training process and its differences from related works, enhancing the reader's understanding.

Weaknesses:
- Applicability: The proposed methods require access to LLM parameters, limiting their use with black-box models. A comparison with techniques that do not require such access, like sampling, would be beneficial.
- Performance Gaps: The claim that "(LoRA + Prompt) performs almost as well in transfer as on in-distribution data" is contradicted by significant performance gaps in open-ended generation tasks.
- Confusion in Section 6.1: The experimental investigation appears problematic, as the results suggest that the model can predict reliable confidence based solely on the question rather than the generation context.
- Focus on Open-Ended Generation: The paper could benefit from a more prominent discussion of the focus on open-ended generation to prevent potential confusion.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of their methods regarding black-box LLMs and consider including comparisons with alternative techniques like sampling. Additionally, we suggest clarifying the performance claims related to transfer and in-distribution data, particularly addressing the observed gaps in ECE and AUROC metrics. It would also be beneficial to enhance Section 6.1 for clarity and address the experimental design concerns raised. We encourage the authors to highlight their focus on open-ended generation early in the text and, if space permits, include discussions on how their settings and conclusions differ from those in Kadavath et al. Lastly, we recommend including a more comprehensive discussion on the applicability of their methods across various datasets beyond MMLU.