ID: Iq0DvhB4Kf
Title: Emergent and Predictable Memorization in Large Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 8, 6, 5, 6, 6, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical study on predicting the memorization of examples in large language models, specifically using the Pythia model. The authors test two methods for memorization prediction: (1) prediction via smaller models and (2) prediction via partially trained models. The experiments reveal that while both methods yield non-trivial results, neither can reliably predict memorization with high recall. The paper also includes experiments to identify the best method concerning recall and uncovers interesting trends.

### Strengths and Weaknesses
Strengths:
1. This work is an extensive empirical study on a significant and timely topic.
2. The analysis is closely tied to real use cases, leading to practical suggestions.
3. The paper is well-written, with clear motivation, organization, and an insightful limitations/future work section.
4. The additional analysis in the appendix is interesting and could enhance the paper's value; moving it to the main text is encouraged.

Weaknesses:
1. The use of k-extractible and memorization scores simplifies the problem, as attackers may not need to extract the same information as the training data.
2. The analysis does not address data properties or contents, which are important for future work.
3. Despite extensive experiments, the generalizability of findings to other models and data remains uncertain.
4. The takeaway in lines 308-310 is hard to understand.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the takeaway in lines 308-310. Additionally, we suggest that the authors address the limitations regarding the use of k-extractible and memorization scores by discussing how these methods may not fully capture the complexities of real-world scenarios. It would also be beneficial to explore the implications of data properties and contents in future work. Lastly, we encourage the authors to clarify the value of their findings for other large language models beyond the Pythia model.