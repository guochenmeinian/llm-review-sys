ID: arkmhtYLL6
Title: Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 7, 6, 6, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a methodology for training a student model to sensitize or desensitize specific concepts by leveraging a teacher model. The authors introduce a concept distillation loss that utilizes Concept Activation Vectors (CAVs) from a high-performing teacher classifier, aiming to reduce concept activation in that direction. They also employ a prediction loss term based on class prototypes and an autoencoder to minimize discrepancies between the latent spaces of the teacher and student models. Additionally, the authors enhance the original TCAV method by utilizing intermediate layer sensitivity through prototypes, allowing for fine-tuning models for (de)sensitization towards concepts at any layer. Empirical evidence, including TCAV scores and cosine similarity analyses, demonstrates the robustness of the concept-distilled student model against biases across various datasets.

### Strengths and Weaknesses
Strengths:
- The authors propose a novel approach to distill concept information from a well-trained teacher to a student model, effectively reducing spurious correlations learned by deep neural networks (DNNs).
- The use of prototypes for capturing concepts in intermediate layers is a significant contribution, providing a more comprehensive understanding of model sensitivity.
- The methodology shows superior performance on biased datasets, with clear experimental validation of its effectiveness in mitigating bias.
- Empirical results, including TCAV scores and cosine similarity metrics, effectively illustrate the method's efficacy in enhancing concept learning.
- The paper is well-written, and the selection of baselines is extensive, demonstrating the applicability of using CAVs for model improvement.

Weaknesses:
- The paper lacks detailed analysis of the student's model performance, particularly regarding its accuracy on specific datasets like ColorMNIST, raising questions about potential biases and limitations.
- The overall pipeline and methodology are somewhat unclear, and the paper explores too many ideas without sufficient justification or clarity.
- The explanation of the role of prototypes in the introduction lacks clarity, which may hinder reader comprehension.
- The presentation of results, particularly in Table 6, is difficult to interpret, potentially obscuring key findings.
- There is a need for more recent related work to be discussed, particularly concerning alignment between different models' feature spaces.
- The quality of figures, especially Fig 4, should be improved for better clarity.

### Suggestions for Improvement
- We recommend that the authors improve the analysis of the student's model performance, particularly addressing why it still exhibits relatively poor accuracy on datasets like ColorMNIST and exploring potential limitations.
- We suggest incorporating a clearer description of the training pipeline to enhance understanding, particularly regarding the interactions between the teacher and student models.
- We recommend that the authors improve the introduction by elaborating on the importance of prototypes for capturing concepts in intermediate layers, clarifying that the original CAV method focuses solely on the last layer.
- We encourage the authors to include more recent related work in the discussion, such as the paper "Text-To-Concept (and Back) via Cross-Model Alignment," to provide context for their approach.
- We advise conducting a proper ablation study to verify the claims made in the paper and to justify the design choices explicitly.
- To enhance clarity, we suggest providing exact numerical values for cosine similarities and reformatting Table 6 for better readability, possibly relocating it to an earlier section of the paper for improved intuition.
- We recommend improving the quality of figures, especially Fig 4, to enhance visual clarity.