ID: sWNOvNXGLP
Title: DISCOVER: Making Vision Networks Interpretable via Competition and Dissection
Conference: NeurIPS
Year: 2023
Number of Reviews: 24
Original Ratings: 6, 7, 4, 2, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, 5, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DISCOVER, a framework for creating interpretable vision networks using stochastic local winner-takes-all (LWTA) layers and multimodal vision-text models. It aims to enhance neuron specialization and interpretability through a new similarity metric based on Jensen-Shannon divergence. The authors evaluate the framework on various architectures, demonstrating improved classification performance and interpretability compared to traditional methods. Additionally, the paper analyzes neuron activations in conventional and LWTA-based DeiT networks, emphasizing the relationship between sparsity and interpretability. The authors report that, on average, 98% of neurons are active in the DeiT-T model when evaluated on the ImageNet-1k validation set, leading to challenges in interpretability due to the conflicting functionalities of activated neurons. They argue that while sparsity does not guarantee interpretability, it facilitates the examination of individual neurons and provide empirical evidence from neuron activation analysis, highlighting differences in activated concepts between networks with varying competitor settings.

### Strengths and Weaknesses
Strengths:
1. The proposed framework is novel and combines LWTA layers with multimodal models, offering a unique approach to neuron specialization.
2. The classification performance is comparable to existing methods, even without hyperparameter tuning.
3. The introduction of a new similarity metric enhances the interpretability of neuron representations.
4. The paper provides substantial empirical evidence regarding neuron activation and sparsity in DeiT networks.
5. The authors effectively clarify the relationship between neuron activation and interpretability, referencing relevant literature.
6. The analysis of neuron activations for specific examples from the ImageNet-1k validation set enhances the understanding of the proposed mechanisms.

Weaknesses:
1. The novelty of the DISCOVER method is limited, as it resembles existing approaches like DMoE and requires clarification on differences.
2. The evaluation lacks thoroughness, with insufficient visualization results and comparisons to other methods to substantiate claims of interpretability.
3. The clarity of the presentation could be improved, particularly in figures and mathematical explanations, which may hinder understanding.
4. The paper lacks concrete case studies demonstrating how sparser CVNs improve interpretability.
5. Some reviewers feel that not all points raised in their feedback have been adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by simplifying figures, particularly Figure 1, and providing a more focused discussion on how LWTA is integrated into transformer architectures. Additionally, we suggest including more comprehensive evaluations, such as comparing the interpretability of different values of $U$ and providing empirical evidence to support the claim that sparsity enhances interpretability. Specifically, including a case study that lists all concepts corresponding to activated neurons for selected images would be beneficial. Lastly, we recommend that the authors ensure all reviewer feedback is thoroughly addressed to avoid any perceived gaps in their responses.