ID: nErbvDkucY
Title: Training and inference of large language models using 8-bit floating point
Conference: NeurIPS
Year: 2023
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: 4, 4, 4

Aggregated Review:
### Key Points
This paper presents a method to enhance the training and inference speed of large language models (LLMs) by utilizing reduced bit representations for weights and activations, specifically employing FP8 E4 format for weights and activations and FP8 E5 for gradients. To mitigate underflow or overflow issues when converting from FP16 to FP8 E4, the authors introduce scaling bias schemes. The proposed method is evaluated on GPT and Llama 2 models across three GLUE tasks, demonstrating minimal performance degradation compared to FP16. 

### Strengths and Weaknesses
Strengths:  
* The evaluations indicate small or no performance drop.  
* The paper includes extensive analysis and discusses potential limitations of FP8-AMAX and FP8-CSCALE methods.  
* The findings are promising for both industry and academia.  

Weaknesses:  
* The scientific novelty of the proposed schemes is limited.  
* A significant amount of material is relegated to the Supplementary section, which may not be critical.  
* The title is perceived as overly bold, suggesting broader applicability than the focus on linear layers.

### Suggestions for Improvement
We recommend that the authors improve the title to accurately reflect the focus on linear layers in FP8. Additionally, we suggest exploring the applicability of the proposed methods to other layers of LLMs, particularly attention layers, to enhance the paper's relevance and impact.