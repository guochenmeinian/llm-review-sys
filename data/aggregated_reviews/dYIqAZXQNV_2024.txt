ID: dYIqAZXQNV
Title: Generalizing CNNs to graphs with learnable neighborhood quantization
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 7, 6, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 1, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Quantized Graph Convolution Network (QGCN) that extends CNNs to graph data by decomposing convolution operations into non-overlapping sub-kernels. The authors demonstrate that QGCN is equivalent to a 2D CNN layer for local pixel neighborhoods and generalize this to graphs of arbitrary dimensions. By integrating the QGCN into a residual network architecture, they achieve performance that matches or exceeds current GCN models on benchmark datasets.

### Strengths and Weaknesses
Strengths:  
- The authors provide a novel approach by quantizing convolution kernels into non-overlapping sub-kernels, which is relatively innovative.  
- The presentation is clear and logical, making the paper easy to follow.  
- The method shows strong performance compared to baseline models across various datasets.  

Weaknesses:  
- The motivation for integrating convolution kernels into GCNs is unclear, especially given the existing performance of GCNs on graph tasks.  
- The downstream tasks of the proposed network are ambiguous, lacking clarity on whether it addresses image classification or graph-related tasks.  
- The experimental settings are insufficiently detailed for reproducibility, and the manuscript lacks definitions for some notations and references to baseline models.  
- The heavy computational burden of the QGCN raises concerns about its practical application, with inference latency significantly higher than that of GCNs.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind integrating convolution kernels into the GCN framework. Additionally, please provide a detailed description of the downstream tasks the proposed model addresses. We suggest enhancing the experimental section by including more comprehensive settings, such as learning rates and network layers, and ensuring that all notations are defined. Furthermore, we encourage the authors to compare their method against more recent state-of-the-art models and address the high computational complexity of the QGCN, potentially exploring strategies to reduce it.