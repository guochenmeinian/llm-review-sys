ID: RAtrnAtAsM
Title: LEGO: A Multi-agent Collaborative Framework with Role-playing and Iterative Feedback for Causality Explanation Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a multi-agent collaborative framework for causality explanation generation, comprising five agents: cause analyst, effect analyst, knowledge master, explainer, and critic. The cause and effect analysts engage in bi-directional reasoning, while the explainer and critic refine the causality explanation through iterative feedback. The framework is well-motivated, with comprehensive experiments conducted on two datasets, including ablation studies and human evaluations.

### Strengths and Weaknesses
Strengths:  
- The bi-directional reasoning mechanism is novel and effectively addresses spurious causal associations in LLMs.  
- The framework is clearly articulated, with motivating examples and thorough experimental design.  
- Insights on optimal interactions between LLM agents contribute to the research community.  

Weaknesses:  
- The evaluation metrics and results presentation require improvement, particularly regarding the clarity of precision, recall, and BERT-f1 scores in Table 1.  
- The human evaluation guidelines are ambiguous, and the rationale for using different metrics across datasets is unclear.  
- The impact of the role-playing framework is not sufficiently demonstrated, and the presentation of ablation experiments lacks clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the evaluation metrics, ensuring that the computation of precision, recall, and BERT-f1 scores is explicitly defined. Additionally, we suggest separating the results of automatic and human evaluations into distinct tables to avoid confusion. The authors should also clarify the human evaluation process, including whether annotators worked independently or in discussions. To strengthen the empirical evaluation, we encourage the inclusion of additional ablation studies, such as examining the effects of communication between cause and effect analysts. Finally, we recommend providing a more detailed discussion of the significance of results and the rationale behind the choice of evaluation metrics for different datasets.