ID: n6ztJ3Lrdj
Title: Learning with Explanation Constraints
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 4, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical investigation into learning with constraints on model gradients, focusing on gradient-based explanations within neural networks. The authors propose an analytic framework that defines explanation constraints as stochastic constraints depending on data points, contrasting with deterministic constraints like l2 regularization. They introduce the EPAC model to analyze the impact of these constraints on sample efficiency and generalization bounds, showing that incorporating explanation constraints can effectively reduce the hypothesis class, leading to improved model performance, particularly in requiring fewer labeled data. The study also includes new definitions and bounds related to explanation constraints, enhancing the understanding of how these constraints can improve label efficiency. Furthermore, the authors analyze the relationship between explanation constraints and semi-supervised learning, providing insights into learnable constraints and their implications for model generalization. They propose a variational method to solve the learning problem under these constraints.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant and timely problem in machine learning, contributing to the understanding of learning from explanations.
- The theoretical framework is well-structured, and the analysis provides valuable insights into the benefits of explanation constraints.
- The authors provide a clear definition of explanation constraints and their theoretical significance, contributing new bounds for Rademacher complexity in this context.
- The connection between explanation constraints and semi-supervised learning is well-articulated.
- The presentation is clear and the writing is generally well-organized, making the concepts accessible.

Weaknesses:
- The paper lacks a thorough discussion of the stringency of the conditions in theorems, complicating the assessment of the contribution's significance.
- There is insufficient exploration of model shape constraints, such as monotonicity and convexity, and the relationship between explanation constraints and traditional label supervision is not adequately clarified.
- The rationale for using non-standard datasets is insufficiently justified, raising concerns about the generalizability of the results.
- The connection between theoretical analysis and experimental results is weak, particularly regarding the advantages of the variational approach over the Lagrangian method.
- The explanation of how hyperparameter λ is determined lacks clarity.
- The experimental section does not effectively connect with the theoretical analysis, and several key prior works are not cited or discussed.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the stringency of their conditions in theorems to clarify their significance. Additionally, the authors should address the lack of exploration regarding model shape constraints and provide a clearer justification for why explanation constraints are distinct from standard label supervision. We also suggest that the authors improve the justification for selecting non-standard datasets and conduct more thorough evaluations to demonstrate generality to established benchmarks. Furthermore, we recommend strengthening the connection between their theoretical analysis and experimental findings, particularly in clarifying why the variational approach outperforms the Lagrangian method. Finally, we advise providing a clearer explanation of the process for selecting the hyperparameter λ in the Augmented Lagrangian objective and incorporating intuitive examples of explanation constraints early in the paper to aid understanding.