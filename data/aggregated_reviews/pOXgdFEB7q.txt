ID: pOXgdFEB7q
Title: What Variables Affect Out-of-Distribution Generalization in Pretrained Models?
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 4, 7, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the "tunnel effect" in neural networks, proposing that deeper layers compress representations, which limits out-of-distribution (OOD) generalization. The authors challenge the universality of the tunnel effect, suggesting it is influenced by training data diversity. They conduct extensive experiments across various neural network architectures and datasets, introducing three metrics to quantify the tunnel effect's strength. Their findings indicate that increasing training data diversity mitigates the tunnel effect and enhances OOD generalization, while depth and overparameterization have adverse effects.

### Strengths and Weaknesses
Strengths:  
- The paper presents original research that challenges existing assumptions about the tunnel effect, potentially impacting the understanding of OOD generalization in neural networks.  
- The experiments are extensive, employing rigorous statistical analysis to support the hypothesis, and the writing is clear, with helpful figures and tables.  

Weaknesses:  
- The empirical evaluations raise questions about their conclusiveness regarding the generality of the hypothesis.  
- The use of linear probing and SHAP for evaluation is criticized, as they may not provide reliable results.  
- The paper lacks significant theoretical contributions and is criticized for being poorly organized, limiting the clarity of the relationship between arguments and experiments.  
- Certain findings appear trivial or already established, and the paper does not provide absolute accuracy numbers, which are essential for validating claims.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their definitions, particularly for terms like "linear probe" and "probe accuracy." Additionally, we suggest providing absolute accuracy numbers in the results discussion to substantiate claims regarding OOD performance. The authors should also address the confounding variables in their experiments and expand the continual learning analysis to include different architectures and datasets to strengthen their claims. Furthermore, we encourage the authors to clarify the reasoning behind "Metric 3: ID/OOD Alignment" and the term "more hierarchical features" to enhance reader comprehension. Lastly, we advise revising the organization of the paper to better connect arguments with experimental results.