ID: ZJWQfgXQb6
Title: The ToMCAT Dataset
Conference: NeurIPS
Year: 2023
Number of Reviews: 25
Original Ratings: 6, 6, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 1, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive dataset for studying human-machine teaming, integrating multi-modal data from collaborative task scenarios, including physiological signals, brain data, and game performance metrics. The dataset aims to advance research in both human-human and human-AI collaboration, particularly in search-and-rescue contexts. The authors propose that their dataset enables the study of affect in teams through actual physiological data, rather than relying solely on self-reports. They acknowledge the unimodal nature of their initial analyses, which focus on predicting performance scores and affective states from brain signals, and plan to incorporate multimodal approaches in future work. The paper outlines future research avenues, including multiparty phonetic entrainment analysis and sentiment classification in task-related dialogue, while also addressing representation and biases in the dataset.

### Strengths and Weaknesses
Strengths:
- The motivation for the dataset is clear, and its potential utility for computer and social scientists is convincing.
- The dataset's combination of physiological and cognitive data fills a notable gap in current research.
- The data collection process is well-documented, and the dataset is accessible through various means.
- Initial analyses show interesting results, indicating the dataset's richness and providing valuable insights into affective computing and human-machine teaming.
- The authors have made significant improvements in their manuscript based on reviewer feedback, including clarifications and additional analyses.

Weaknesses:
- The link between exploratory experiments and the dataset's intended use is unclear, particularly regarding data annotation and ground-truth labels.
- The experimental section lacks depth, focusing primarily on unimodal analyses despite the dataset's multimodal nature.
- The baseline analyses lack grounding in prior work, making it unclear why specific analyses were chosen.
- Clarity issues persist in the writing, particularly regarding the description of the dataset, experimental setup, and the terminology used, such as "rescue" and "Human-AI collaboration."
- There is a lack of clarity regarding the motivation for specific tasks and the connection between exploratory experiments and the dataset.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind choosing the rescue task and the rationale for using Minecraft as a simulation. A figure illustrating the experimental setup would enhance understanding. Additionally, we suggest including more multimodal analyses and addressing the subjective bias in self-reported emotion labels by incorporating objective measures from other modalities. The authors should provide a detailed description of the recruitment process and participant demographics to assess representation and potential biases. We also recommend that the authors improve the grounding of baseline analyses in prior work by clearly explaining the rationale for their chosen methods and how they relate to state-of-the-art techniques. Furthermore, we encourage the authors to streamline the descriptions and justifications for tasks and modalities to improve readability and enhance the discussion surrounding the reliability of self-reported labels and their implications for the study's findings. Lastly, we suggest that the authors further emphasize the distinction between cooperative and competitive tasks in their analysis to enhance the understanding of their contributions.