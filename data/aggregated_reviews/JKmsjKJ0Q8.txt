ID: JKmsjKJ0Q8
Title: Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a dataset simulating *Avalon: The Resistance*, aimed at exploring LLMs' ability to understand long-horizon dialogue in complex multi-party scenarios. The dataset includes dialogue from the game and self-reported persuasion strategies, with tasks focused on identifying roles and the role of Merlin. The authors experiment with three LLMs, comparing their performance against human and random baselines.

### Strengths and Weaknesses
Strengths:
- The dataset introduces a unique resource for dialogue understanding in hidden role games, which is distinct from traditional task-oriented dialogues.
- Experiments are well-designed, executed, and analyzed, providing valuable insights into model performance.
- The paper is well-written, and the authors effectively addressed reviewer concerns in their rebuttal.

Weaknesses:
- There is a lack of analysis regarding model performance, particularly concerning the impact of bad faith actors and deception on results.
- The input composition for multi-turn conversations is limited, as it only includes the last turn's utterances, which may hinder capturing the nuances of human interaction.
- The novelty of the dataset is questionable, given the existence of prior datasets for similar games, and the paper does not adequately justify the need for this new dataset.
- Some experimental results lack clarity, particularly regarding the metrics used (e.g., average precision vs. F1 score) and how human evaluations were aggregated.

### Suggestions for Improvement
We recommend that the authors improve the analysis of model performance by explicitly examining how bad faith actors and deception affect outcomes. Additionally, consider revising the input composition to include previous utterances, as this could enhance the understanding of multi-turn dialogue. To address novelty concerns, we suggest providing a clearer justification for the dataset's necessity and including comparative experiments with existing datasets. Lastly, ensure that all reported metrics are clearly labeled and consider reporting F1 scores alongside average precision for a more comprehensive evaluation.