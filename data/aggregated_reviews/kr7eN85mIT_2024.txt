ID: kr7eN85mIT
Title: Tell What You Hear From What You See - Video to Audio Generation Through Text
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a multimodal video-to-audio generative model called **VATT**, which generates audio and audio captions from silent videos and optional text prompts. The framework comprises two components: i) **VATT converter**, an instruction fine-tuned LLM that projects video features into language space, and ii) **VATT audio**, a bi-directional transformer audio decoder based on **Maskgit** for generating audio tokens. The authors demonstrate state-of-the-art performance in audio quality and inference speed through experiments on the VGGSound dataset.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow, with a clear motivation and detailed explanations.
- It features significant technical contributions, particularly in the V2A Instruction Tuning, which facilitates text control in video-to-audio generation.
- Comprehensive experiments and ablations validate the proposed method's superiority over baselines in both quantitative evaluations and human studies.

Weaknesses:
- The primary contribution appears to be the V2A Instruction Tuning, while the maskgit-based audio decoder seems incremental.
- The VATT converter may lose temporal information from videos, leading to less synchronized audio generation.
- The paper's content is densely packed, making it somewhat challenging to comprehend, and it contains grammatical errors.
- The novelty is limited, as some techniques were previously proposed, raising questions about the distinctiveness of the authors' contributions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by reducing the density of content and addressing grammatical errors. Additionally, we suggest including an ablation study to analyze how the quality of generated captions affects audio results. To enhance the robustness of the findings, the authors should conduct experiments on more datasets beyond VGGSound. Furthermore, verifying the correctness of LTU-generated captions used as ground truth is essential. Lastly, we encourage the authors to compare their model against large language models specifically designed for video understanding, such as VideoChat or Video-LLaMA, to provide a more comprehensive evaluation.