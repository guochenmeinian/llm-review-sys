ID: X3oeoyJlMw
Title: Mixture of Link Predictors on Graphs
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 4, 7, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an ensemble model, Link-MoE, for link prediction on graphs, which combines various existing link prediction methods. The authors argue that different node pairs require distinct pairwise representations for accurate predictions, leading to the need for a mixture of experts approach. The gating model assigns weights to the outputs of these expert models based on their relevance to the specific node pairs. The authors propose a two-stage strategy for recalling and scoring edges, acknowledging the inherent complexity of link prediction tasks, which is \(O(N^2)\). They emphasize the importance of sparse gating to enhance inference efficiency and report that Link-MoE outperforms existing methods on several benchmarks, particularly in specific scenarios. However, concerns remain regarding its scalability and inference time, especially with the inclusion of SEAL as an expert.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and clearly articulates its motivations.
2. The experimental design is comprehensive, validating the proposed method's effectiveness against multiple baselines.
3. The authors provide a practical training strategy for the experts and gating network, achieving state-of-the-art performance.
4. Sparse gating is highlighted as a significant feature that can improve efficiency.
5. Experimental results indicate that Link-MoE can outperform existing experts in certain settings.

Weaknesses:
1. The novelty of the approach is limited, as it primarily combines existing models rather than introducing fundamentally new methodologies.
2. The experimental evaluation lacks comprehensive baselines, omitting relevant models like the ensemble of link prediction methods.
3. Concerns exist regarding the computational efficiency of the method, particularly due to the extensive training required for multiple expert models.
4. The scalability of Link-MoE remains uncertain, with no concrete performance or inference time metrics provided under the proposed settings.
5. The current version does not prioritize sparse gating as the main method, which could enhance efficiency.
6. The choice of SEAL as an expert raises concerns due to its high inference time.
7. The readability of figures and notation inconsistencies detract from the overall presentation.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contribution by addressing specific challenges associated with mixture of experts (MoE) models, such as the collapse problem. Additionally, the authors should consider including a broader range of baseline models in their experiments, particularly those that are relevant to their approach. To enhance computational efficiency, we suggest exploring the implementation of conditional computation techniques like MoE. Furthermore, we encourage the authors to clarify the differences between Link-MoE and existing methods, particularly Global-Ensemble, in the main text. We also recommend that the authors improve the discussion on sparse gating, including detailed method descriptions and experimental results to better illustrate its effectiveness compared to other methods. Conducting experiments to benchmark the time complexity of Link-MoE and providing concrete evidence of its efficiency would strengthen the paper. Clarifying the rationale behind the choice of experts, particularly SEAL, and addressing the inference time concerns more thoroughly would also be beneficial. Finally, addressing the readability of figures and ensuring consistent notation throughout the paper would enhance clarity, and including HeaRT results in the main paper would demonstrate Link-MoE's capabilities in more challenging settings.