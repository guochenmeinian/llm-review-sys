ID: 8R4z3XZt5J
Title: Forcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks
Conference: NeurIPS
Year: 2023
Number of Reviews: 2
Original Ratings: 4, 6
Original Confidences: 3, 4

Aggregated Review:
### Key Points
This paper presents early work on poisoning attacks in text summarization and text completion settings, focusing on the security vulnerabilities of generative LLMs through both full fine-tuning and prefix-tuning methods. The authors propose new metrics to evaluate stealthiness and attack success, but the evaluation metrics may not adequately capture these concepts.

### Strengths and Weaknesses
Strengths:
- The paper addresses an important topic in natural language generation (NLG) tasks.
- It considers multiple trigger possibilities, including fixed, floating, and pieces.
- The comparison of security vulnerabilities using different tuning methods is a valuable contribution.

Weaknesses:
- Clarity issues arise regarding the low target match percentages in Figure 4, particularly for the xsum dataset, which lacks explanation.
- The significance of the differences between trigger types in Figure 5 is questionable.
- The evaluation of trigger stealthiness is insufficient, as it does not include assessments from NLP checks.
- The metrics used to evaluate poisoning success are unclear and may not effectively represent the attack's success.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their results, particularly in Figure 4, by providing explanations for the low target match percentages. Additionally, the authors should clarify the significance of the differences between trigger types in Figure 5. To enhance the evaluation of trigger stealthiness, we suggest including assessments from NLP checks, such as perplexity measurements of modified text. Finally, we encourage the authors to reconsider the metrics used to evaluate poisoning success, as even one effective trigger phrase should be considered a success if it exceeds a certain token length.