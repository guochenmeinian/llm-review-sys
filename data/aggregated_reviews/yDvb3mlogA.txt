ID: yDvb3mlogA
Title: Closing the gap between the upper bound and lower bound of Adam's iteration complexity
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 3, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a convergence analysis of the Adam optimization algorithm under smoothness and bounded variance conditions, addressing the gap between upper and lower bounds of iteration complexity. The authors demonstrate that Adam achieves an optimal iteration complexity of \(O(1/\epsilon^4)\) without additional assumptions, contrasting with previous analyses that required bounded gradients or other conditions. Theorems 1 and 2 establish upper bounds that align with lower bounds, enhancing the theoretical understanding of Adam's performance. Additionally, the authors provide a theoretical analysis focusing on the dependence of Adam's performance on the parameter \(\beta_1\), arguing that setting \(\beta_1\) close to 1 is not necessary for optimal performance. Their experimental results on the transformer with WIKITEXT and VGG 13 on CIFAR 10 indicate that performance remains stable as \(\beta_1\) varies from 0 to 0.9, but shows a significant drop when \(\beta_1\) increases to 0.9999.

### Strengths and Weaknesses
Strengths:
- The paper effectively closes the gap between existing upper and lower bounds for Adam's iteration complexity, making a significant contribution to the field.
- The proof techniques, including the peeling-off strategy, are innovative and may have broader applications in analyzing adaptive algorithms.
- The presentation is clear and accessible, particularly for experts in stochastic optimization.
- The authors provide empirical evidence demonstrating the performance of Adam across a range of \(\beta_1\) values, contributing valuable insights to the optimization community.
- The detailed experimental results with statistical measures enhance the reliability of the findings.

Weaknesses:
- The results may not be surprising given the existing body of work on Adam, leading to questions about the novelty of the contributions.
- The relationship between the paper's findings and prior results, particularly those indicating Adam's non-convergence, is unclear.
- The proof complexity may hinder verification, and the bounded variance condition could be seen as restrictive.
- Some reviewers express skepticism about the relevance of the empirical results from training GANs to the theoretical claims made in the paper.
- Concerns are raised regarding the tightness of the upper bound presented, with suggestions that it does not align with practical results observed in previous studies.
- The current experiments are limited to 50 epochs, which may not fully capture the model's performance dynamics, and the reliance on a small number of random seeds could affect the robustness of the conclusions drawn.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the implications of Theorems 1 and 2, specifically how they relate to practical hyperparameter choices such as \(\beta_1\) and \(\beta_2\). It would be beneficial to provide lower bounds for the gradient norm of Adam and to bridge the theoretical and practical aspects of the analysis. Additionally, we suggest that the authors conduct further experiments to compare the performance of Adam with \(\beta_1 \approx 0\) against \(\beta_1 \approx 1\), as this could strengthen their argument. We also recommend increasing the number of epochs beyond 50 to gain a more comprehensive understanding of model performance and incorporating a larger number of random seeds to enhance the robustness of the results. Finally, addressing the relationship with existing literature, particularly the work by Arjevani et al., in more detail would strengthen the theoretical framework of the paper.