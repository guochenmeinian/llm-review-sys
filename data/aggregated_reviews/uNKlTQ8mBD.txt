ID: uNKlTQ8mBD
Title: Learning Formal Mathematics From Intrinsic Motivation
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 6, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to formal theorem proving (FTP) where a learning agent, starting from axioms, learns to generate conjectures and prove them using a single transformer model. The authors propose an alternating process that samples conjectures through constrained decoding, followed by proof searches that yield data for further training. The model demonstrates the ability to generate increasingly difficult conjectures and improve its proof-writing capabilities across arithmetic and propositional logic domains. The incorporation of hindsight relabeling enhances the learning process. Additionally, the authors explore self-improving systems capable of theorem proving, particularly in the context of "alien mathematics," arguing that their system can self-improve and prove numerous theorems not initially provided, although these may not yet be of significant interest to humans. They acknowledge that while their work does not resolve fundamental questions about making alien mathematics interesting, it opens avenues for further study. The authors also highlight that their compute requirements are modest compared to prior works, allowing for interesting questions to be explored with less computational power.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant question in AI-for-mathematics: how to create systems that learn from scratch. The exploration of conjecturing and proving together is novel and potentially impactful.
- The self-improvement paradigm of LLMs is well-articulated, and the MCTS-guided policy for proof search is a clever approach that facilitates efficient learning.
- The authors effectively address reviewer concerns and clarify their work, leading to increased scores from some reviewers.
- The proposed setup allows for exploration of novel strategies in theorem conjecturing and proving, potentially leading to significant insights in the field.

Weaknesses:
- Lack of empirical evidence: The paper does not provide examples of conjectures generated at various stages, making it difficult to assess their quality and the model's improvement over time.
- The results presented are not particularly convincing, especially regarding performance on extrinsic problems, and the conjectures generated may lack interesting outcomes.
- The scalability of the approach is questionable, as improvements are modest, particularly in the propositional logic task.
- The evaluation of conjecture difficulty relies on the learned policy, which raises concerns about the validity of the claims regarding increasing difficulty.
- The compute requirements, while modest, still raise questions about the adequacy of the attempts at solving complex problems.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation of their approach by including examples of conjectures generated at different stages to assess their quality. Additionally, providing a broader set of known theorems for evaluation, particularly for the abelian group task, would enhance the robustness of the findings. 

Clarifying the details of the MCTS policy and its operators would help readers understand the generalizability of the approach beyond Peano-based domains. We also suggest that the authors explore the applicability of their method to popular theorem proving environments like Coq and Lean, potentially including a discussion or demonstration of this application.

To better track learning dynamics, we recommend measuring and reporting metrics such as the average length of proofs and conjectures across iterations. Lastly, addressing the concerns regarding the evaluation of conjecture difficulty and the model's performance on standard benchmarks like mathlib or mini-f2f would strengthen the paper's contributions. Furthermore, we suggest that the authors improve the discussion on how their setup connects to existing theorem provers, as this is crucial for reader understanding. Additionally, exploring strategies to reward conjectures that generalize previous results and penalize those that do not, as well as considering data augmentation techniques for the prover, could enhance the impact and applicability of the work to other domains.