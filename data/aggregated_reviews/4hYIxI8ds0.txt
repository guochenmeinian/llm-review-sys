ID: 4hYIxI8ds0
Title: Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 5, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel unsupervised domain adaptation method called invariant CONsistency learning (ICON). ICON utilizes labeled source samples and clustered target samples, employing BCE losses to align features of the same class/cluster within mini-batches. Evaluation on 10 benchmark datasets (Office-Home, VisDA-2017, WILDS 2.0) demonstrates that ICON achieves superior performance compared to several representative methods. The authors claim that ICON provides an optimal classifier under certain assumptions, although the realism of these assumptions is questioned.

### Strengths and Weaknesses
Strengths:
- ICON is simple and easy to use, achieving high accuracy across various datasets.
- The method effectively utilizes clustering on target features, reducing noise in pseudo labels and mitigating overfitting.
- The paper is well-written and presents sufficient experiments validating ICON's effectiveness.

Weaknesses:
- ICON's performance on Office-Home and VisDA-2017 is inferior to state-of-the-art methods, raising concerns about its competitiveness.
- The strong assumptions underlying the theoretical properties of ICON may not hold in practical scenarios.
- The similarity of ICON's principle to contrastive learning methods is noted, but differences are not adequately discussed.
- The paper lacks exploration of extending ICON to more complex domain adaptation problems, such as universal and source-free domain adaptation.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the performance comparisons with state-of-the-art methods, particularly addressing the noted shortcomings on Office-Home and VisDA-2017. Additionally, we suggest providing a clearer theoretical justification for the accuracy of low-dimensional pseudo labels and exploring the necessity of the BCE task for UDA classification. It would also be beneficial to include ablation studies on the impact of hyper-parameter selection and the use of self-training loss. Finally, discussing the potential for ICON's application in more advanced domain adaptation scenarios would enhance the paper's contribution.