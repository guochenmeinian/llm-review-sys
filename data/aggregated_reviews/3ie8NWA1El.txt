ID: 3ie8NWA1El
Title: HyperPrism: An Adaptive Non-linear Aggregation Framework for Distributed Machine Learning over Non-IID Data and Time-varying Communication Links
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 4, 7, 5, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents HyperPrism, a novel framework for decentralized machine learning (DML) that addresses challenges posed by non-IID data and time-varying communication links. The authors propose a non-linear aggregation method based on Kolmogorov Means and adaptive mapping functions, which they argue enhances convergence speed and scalability compared to traditional linear aggregation methods. The framework's theoretical analysis and experimental results support its effectiveness in handling decentralized DML challenges.

### Strengths and Weaknesses
Strengths:
1. The originality of the HyperPrism framework in addressing the dual challenges of non-IID data and unstable communication links is notable.
2. The research quality is high, with rigorous theoretical analysis and comprehensive experimental validation.
3. The clarity of the paper is commendable, with well-structured explanations and supportive figures, although some dense sections could benefit from simplification.
4. The results demonstrate significant improvements in convergence speed and scalability, providing valuable contributions to the field.

Weaknesses:
1. The use of hypernetworks for predicting the exponent of the mapping function deviates from common practices, suggesting a need to simplify this aspect to a basic MLP.
2. The analyses of hypernetworks are limited; further ablation studies on the value set of p, its variance across distributed machines, and results with preset p values should be included.
3. The baselines for comparison are outdated, primarily pre-2021; incorporating more recent baselines from 2022 or 2023 would enhance the paper's relevance.
4. The experimental setup is limited to a narrow range of datasets and models, which restricts the generalizability of HyperPrism's performance improvements.
5. Clarity issues exist, particularly in introducing HyperPrism and explaining the motivation behind using mirror descent and Kolmogorov Means.
6. Insufficient details on hyperparameter tuning hinder the assessment of comparison fairness.

### Suggestions for Improvement
We recommend that the authors improve the explanation of hypernetworks by simplifying their role to a basic MLP. Additionally, we suggest including ablation studies to analyze the influence of p and its variance across distributed machines. To strengthen the paper, the authors should update the comparison baselines to include more recent studies from 2022 or 2023. Expanding the experimental setup to include diverse datasets and models would also enhance generalizability. Furthermore, we advise improving clarity in sections discussing HyperPrism and providing more details on hyperparameter tuning to ensure fair comparisons.