ID: ouiQX2XWYc
Title: Watermarking LLMs with Weight Quantization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel watermarking strategy aimed at protecting open-source large language models (LLMs) from malicious usage by embedding watermarks in model weights during quantization. The watermarks remain hidden in the quantized model but are revealed in the full-precision model, allowing public release of the quantized version while safeguarding the full model. The method is demonstrated on GPT-Neo and LLaMA models without predefined triggers, with experimental results validating its effectiveness.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a timely and significant problem of protecting intellectual property in open-source LLMs.  
- It proposes a novel watermarking technique that leverages model quantization gaps, avoiding the need for predefined triggers.  
- Experimental results show the method's effectiveness on major models like GPT-Neo and LLaMA.  

Weaknesses:  
- There is limited discussion on watermark robustness under fine-tuning, which is critical for real-world applications.  
- The analysis of the tradeoff between watermark capacity and model performance is insufficient.  
- Important comparisons with traditional watermarking techniques are missing, and details regarding the specific watermarks used in experiments are unclear.  
- The experimental setup is constrained, limiting the generalizability of the results.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the impact of fine-tuning on watermark retention, as this is crucial for practical applications. Additionally, we suggest including a more thorough analysis of the tradeoff between watermark capacity and model performance. It would also be beneficial to compare the proposed method with traditional watermarking techniques and clarify the specifics of the watermarks used in the experiments. Finally, consider testing the quantized model using the LLM Harness benchmark to evaluate performance degradation more comprehensively.