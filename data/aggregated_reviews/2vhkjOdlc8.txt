ID: 2vhkjOdlc8
Title: Dinomaly: The Less Is More Philosophy in Multi-Class Unsupervised Anomaly Detection
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 6, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "Dinomaly: The Less Is More Philosophy in Multi-Class Unsupervised Anomaly Detection," which introduces a minimalist reconstruction-based framework for unsupervised anomaly detection (UAD) in multi-class settings. The authors propose four key components: Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction. Extensive experiments on MVTec-AD, VisA, and Real-IAD datasets demonstrate that Dinomaly outperforms state-of-the-art multi-class and some class-separated UAD methods. Additionally, the authors provide a comprehensive evaluation of various methods for anomaly detection, including RD4AD, ViTAD, and ReContrast, highlighting their performance metrics across different resolutions. They argue that while pixel-level metrics may not benefit from increased resolution, image-level detection performance remains crucial for practical applications. The authors also discuss the scaling ability of their proposed Dinomaly framework, demonstrating its efficiency and state-of-the-art results across varying computational budgets, while addressing concerns regarding dense layer-wise distillation's negative impact on generalization in UAD and committing to including additional ablation studies in the final version.

### Strengths and Weaknesses
Strengths:
1. The use of simple components like Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction is original and challenges the notion that complex architectures are necessary for better performance in anomaly detection.
2. The methodology is well-detailed, with robust experimental design and comprehensive performance metrics, convincingly showing Dinomaly's superiority over existing methods.
3. The paper provides a variety of model selections for users based on computational resources, showcasing the scalability of Dinomaly.
4. Clear articulation of the drawbacks of dense layer-wise distillation in the context of UAD.
5. The paper is generally clear, well-organized, and relatively reproducible, with substantial significance in addressing a major challenge in UAD by achieving high performance in multi-class settings without complex architectures.

Weaknesses:
1. The paper lacks important justification and discussion, making it difficult for readers to recognize the novelty and improvements of Dinomaly. A comparison to specific previous methods is needed to highlight differences and improvements.
2. The motivations for choosing Noisy Bottleneck and Loose Reconstruction are not deeply explored, particularly regarding how Noisy Bottleneck prevents identity mapping.
3. There is no discussion of parameter number, computational complexity, or time complexity in the experiments.
4. The claim regarding Loose Constraint's feature grouping lacks clarity, particularly in how to categorize features into low-semantic-level and high-semantic-level groups.
5. Insufficient explanation of why dense layer-wise distillation is detrimental to UAD.
6. Some writing shortcomings in clarity regarding innovative ideas and motivations.
7. Limited inclusion of fairness resolution experiments and results from additional datasets.

### Suggestions for Improvement
1. We recommend that the authors improve the justification for the choice of Noisy Bottleneck and Loose Reconstruction, explaining how these components enhance anomaly detection performance.
2. Provide a more explicit comparison to recent methods, detailing performance and conceptual differences, and highlight specific limitations of prior work that Dinomaly addresses.
3. Increase the discussion on simplicity and scalability, particularly regarding the computational costs associated with the proposed method.
4. We suggest improving the explanation of the negative effects of dense layer-wise distillation on UAD to enhance clarity.
5. Include fairness resolution experiments and results from the VisA and Real-IAD datasets in the revised version.
6. We encourage the authors to upload the logs of the main experiments (excluding ablation studies) to facilitate replication and comparison by future researchers.
7. The credibility of the paper may benefit from deeper experimental analysis, especially regarding Dinomaly's performance under varying conditions, such as video inputs.