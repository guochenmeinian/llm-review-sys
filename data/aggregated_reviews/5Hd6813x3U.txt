ID: 5Hd6813x3U
Title: Copy Suppression: Comprehensively Understanding a Motif in Language Model Attention Heads
Conference: EMNLP/2024/Workshop/BlackBoxNLP
Year: 2024
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: 3, 4, 5

Aggregated Review:
### Key Points
This paper presents a thorough examination of "copy suppression," a behavior of attention heads in language models that down-weights the prediction of previously generated tokens. The authors identify that Layer 10 Head 7 (L10H7) in GPT-2 small implements this mechanism, with 76.9% of its behavior attributed to copy suppression. They demonstrate its significance in anti-induction and self-repair, and show that this behavior is present across different model architectures. The authors claim to provide the most comprehensive description of a language model component to date.

### Strengths and Weaknesses
Strengths:
- Novelty: The analysis of copy suppression is a previously unexplored behavior of attention heads.
- Thoroughness: The detailed analyses are convincing and well-supported.
- Cross-model generalization: The demonstration of copy suppression across multiple architectures (GPT-2, Pythia, and SoLU) is particularly noteworthy.
- Clarity: The paper is well-written, with a linked glossary for technical terms.

Weaknesses:
- Presentation: The results can be difficult to follow, particularly for readers not well-versed in mechanistic interpretability techniques, which may limit accessibility.
- Clarity of Results: It remains unclear if copy suppression is the "best" explanation for L10H7, as other ablations explain more of the head's behavior.
- Information Overload: The extensive number of experiments can lead to a "too much information" phenomenon, making it hard to follow and potentially lacking depth in some discussions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the technical explanations to enhance accessibility for a broader audience. Additionally, we suggest expanding Section 2.2 and providing more guidance for Figure 2 to aid interpretation. The authors should also clarify the implications of results in Section 3.3.2 regarding the comparative effectiveness of different ablations. Furthermore, we encourage the authors to explore and report on copy suppression in other models, as this could provide valuable insights. Lastly, we recommend refining figures and descriptions to ensure they effectively illustrate the concepts discussed, particularly in Figure 4.