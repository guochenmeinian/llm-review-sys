ID: rTAIgZe3wo
Title: ACTOR: Active Learning with Annotator-specific Classification Heads to Embrace Human Label Variation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a multi-head model for active learning, where each head represents an individual annotator. The authors propose various active learning methods compatible with their model and demonstrate its competitiveness against single-head models. They highlight that their model excels in modeling individual opinions and shows a superior correlation between model uncertainty and annotator disagreement compared to single-head models.

### Strengths and Weaknesses
Strengths:
- The approach of allowing the model to select both instances and annotators is innovative and promising.
- The research direction of modeling annotator confidence alongside opinions is significant.
- Comprehensive evaluations and experiments across different settings are conducted.

Weaknesses:
- The representation of entropy lacks clarity, as it should indicate dependence on a data point.
- The calculation of Group-level entropy is flawed due to the addition of logits from different heads without normalization, risking dominance by a single head.
- The formulation for vote variance is limited to binary classification, which may mislead interpretations in multi-class scenarios.
- The paper relies heavily on existing tools without presenting sufficiently novel contributions or important baselines.

### Suggestions for Improvement
We recommend that the authors improve the clarity of entropy representation by indicating its dependence on data points, e.g., $H(p^a|x)$. Additionally, consider recalculating Group-level entropy using joint entropy of posterior probabilities or normalizing logits before summation to avoid skewed results. The authors should also address the limitations of the vote variance formulation for multi-class scenarios. Furthermore, we suggest including more experiments with other large language models to strengthen the conclusions and adding important baselines, such as active learning without the head-per-annotator setup, to provide a more comprehensive evaluation.