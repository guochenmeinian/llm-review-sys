ID: 1c9XHlHTs7
Title: Warm-up Free Policy Optimization: Improved Regret in Linear Markov Decision Processes
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 7, 6, 7, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new policy optimization algorithm called Contracted Features Policy Optimization (CFPO) for linear Markov Decision Processes (MDPs). The authors propose a contraction mechanism that eliminates the costly warm-up phase found in previous methods, achieving improved regret bounds. The CFPO algorithm demonstrates a $\sqrt{H^3 d}$ enhancement in regret for adversarial settings and matches the performance of value-iteration algorithms. The algorithm is computationally efficient, utilizing all samples and being reward-aware, thus avoiding information waste.

### Strengths and Weaknesses
Strengths:
1. The paper significantly improves the regret bound for policy optimization algorithms in linear MDPs, achieving $O(\sqrt{H^4 d^3 K})$ regret.
2. The proposed feature shrinkage technique is simple and does not incur additional overhead, enhancing the estimated state-action value's boundedness.
3. The paper is clearly written, with a thorough discussion of related work and a detailed explanation of the CFPO algorithm.

Weaknesses:
1. The paper lacks experimental results to validate the theoretical claims, particularly regarding the ease of implementation compared to existing algorithms.
2. There are concerns about the clarity of certain assumptions, such as whether the regret bound significantly depends on $|\mathcal{X}| < \infty$.
3. The paper could benefit from a more nuanced discussion of the gap between upper and lower bounds and potential approaches to address this gap.

### Suggestions for Improvement
We recommend that the authors improve the paper by including experimental results to corroborate the theoretical findings, particularly demonstrating the improved regret and ease of implementation. Additionally, clarifying the dependence of the regret bound on $|\mathcal{X}| < \infty$ in the main text would enhance understanding. The authors should also consider discussing the gap between upper and lower bounds more thoroughly and explore whether previous variance reduction techniques can be applied to their method. Finally, adding a conclusion section to summarize key findings would strengthen the paper.