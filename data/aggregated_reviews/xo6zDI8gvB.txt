ID: xo6zDI8gvB
Title: A Multi-modal Global Instance Tracking Benchmark (MGIT): Better Locating Target in Complex Spatio-temporal and Causal Relationship
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 8, 7, 7, 6, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 5, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
The authors present MGIT, a multi-modal global instance tracking benchmark featuring 150 long video sequences and over 2.03 million frames, annotated with three semantic grains: action, activity, and story. This aims to enhance understanding of complex spatio-temporal relationships. The paper includes comparative experiments with existing benchmarks to validate the proposed annotation method and its impact on tracking performance. Evaluation methods with specific formulas are introduced, and key challenges in tracking are discussed from the perspectives of multi-modal evaluation mechanisms and comparisons between single-modal and multi-modal approaches. The authors analyze the performance of combining mechanisms B and D, revealing that the effective range of story-level information primarily covers the initial action, leading to insignificant score differences for most algorithms. They also address the poor performance of SNLT, attributing it to its local search basis and issues within its open-source code, while describing strategies for ensuring annotation quality.

### Strengths and Weaknesses
**Strengths:**
1. The paper is well-structured, featuring effective diagrams and detailed tables that illustrate the dataset's multi-granular annotations.
2. MGIT introduces a challenging benchmark that addresses limitations in previous language-based tracking tasks by providing more accurate language descriptions through multi-granular annotations.
3. The motivation for the dataset's construction is clearly articulated, and the experimental analyses support further research in multi-modal object tracking.
4. The authors have clearly articulated the challenges and limitations of current tracking mechanisms and benchmarks, implementing a structured approach to ensure the quality of semantic annotations.
5. The inclusion of bad case scenarios enhances the understanding of algorithm failures and the importance of accurate semantic annotations.

**Weaknesses:**
1. The paper lacks a standard for measuring annotation errors from different annotators, which could lead to ambiguous annotations.
2. There is insufficient discussion on 'bad case' scenarios where trackers fail due to language inaccuracies, and a visual analysis of these challenges is missing.
3. The necessity and effectiveness of the proposed multi-granular annotation strategy are not convincingly demonstrated, particularly in relation to existing datasets.
4. The performance of multi-modal algorithms still lags behind single-modal algorithms, indicating a gap in the effectiveness of current multi-modal approaches.
5. The authors acknowledge limitations in existing benchmarks, which may hinder the evaluation of multi-modal trackers.

### Suggestions for Improvement
1. The authors should discuss potential issues with ambiguous annotations that may arise from fine-grained annotations and consider establishing a standard for measuring annotation errors.
2. The authors are encouraged to include a comparison of tracker performance in 'bad case' scenarios and conduct a visual analysis to identify specific challenges within the dataset.
3. The authors need to provide specific formulas for evaluation methods to enhance clarity and understanding.
4. Further exploration of the interactions between the three annotations (action, activity, and story) is recommended to assess their combined impact on tracking performance.
5. The authors should clarify the video selection strategy and provide more statistical support for claims regarding semantic ambiguity in existing datasets.
6. The authors should improve the design of multi-modal benchmarks to address the completeness of semantic information and video complexity, facilitating a more favorable experimental environment.
7. The authors are encouraged to further explore the integration of high-quality semantic understanding modules to better align semantic and visual information in multi-modal trackers.
8. The authors should consider expanding their analysis of 'bad case' scenarios to include a broader range of examples, which may provide deeper insights into the challenges faced by current algorithms.