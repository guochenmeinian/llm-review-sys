ID: dLr4H7Uj4H
Title: Learning Optimal Lattice Vector Quantizers for End-to-end Neural Image Compression
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 3, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to optimize lattice vector quantization (LVQ) for neural image compression, focusing on adaptive learning of codebook structures for improved rate-distortion performance. The authors claim that their approach outperforms existing quantization methods, demonstrating significant gains in various neural image codecs. The proposed method utilizes orthogonal basis functions and Babaiâ€™s Rounding Technique, with experimental results indicating effectiveness in BD-rate comparisons against scalar and classical quantizers.

### Strengths and Weaknesses
Strengths:
- The introduction of learnable codebook bases in LVQ is innovative and shows significant performance improvements.
- The method is compatible with various neural image codecs and demonstrates efficiency in enhancing rate-distortion performance.

Weaknesses:
- Concerns arise regarding the correctness of the entropy model design, particularly in modeling the integer vector as a continuous distribution and the estimation of rates in the ablation studies.
- The effectiveness of the orthogonal constraint is questioned, as it may reduce decorrelation ability, resembling uniform scalar quantization.
- The paper lacks detailed rate-distortion curves and a comprehensive analysis of the quantized values' characteristics.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the entropy model design, specifically addressing how the integer vector is modeled as a continuous distribution. Additionally, providing a thorough explanation of the rate estimation process in the ablation studies would enhance understanding. We suggest including rate-distortion curves for various models, particularly for stronger models like LIC-TCM, to better illustrate performance across different bitrates. Finally, a more detailed discussion on the implications of the orthogonal constraint on performance and the quantization process would be beneficial.