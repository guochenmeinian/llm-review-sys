ID: ftPoVcm821
Title: Demo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 6, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Demo2Code, a framework that synthesizes code from user language instructions and demonstrations by recursively summarizing the demonstrations into a task specification, which is then expanded into execution code. The method is evaluated across various tasks, including table-top manipulation, a cooking simulator, and the EPIC Kitchens dataset, showing improvements over previous state-of-the-art methods. The authors claim that their approach can generalize to longer-horizon tasks and learn user intents. Additionally, the paper provides a comprehensive analysis of task specification patterns in robotic planning through the Demo2Code pipeline, proposing various avoidance conditions and reaction types, such as global avoidance and different forms of reactions (instantaneous, delayed, and patrolling). Each pattern is illustrated with specific examples from the Robotouille tasks, demonstrating how these specifications can guide robotic actions in complex scenarios.

### Strengths and Weaknesses
Strengths:
1. The framework is novel, effectively summarizing demonstrations and language instructions using LLMs for action generation.
2. The recursive and hierarchical summarization method is reasonable and well-executed.
3. The experiments are comprehensive, covering a range of tasks and demonstrating robustness.
4. The paper effectively addresses the gap in evaluation standards between LLM planning and classical planning, setting a precedent for future research.
5. It provides a detailed exploration of task specification patterns, enhancing the understanding of robotic planning capabilities.

Weaknesses:
1. The evaluation metrics raise concerns, particularly regarding the clarity of task success rates versus unit-test pass rates.
2. The assumption of complete state descriptions in demonstrations may not hold in real-world scenarios, limiting practical applicability.
3. The lack of a concrete definition for task specifications and summarization criteria undermines the evaluation's rigor.
4. The method's reliance on privileged low-level state information and manual annotations in the EPIC Kitchens dataset raises questions about its scalability to real-world applications.
5. There is a divergence in evaluation methodologies between LLM and classical planning, which could hinder the comparison of complexities across these domains.
6. The paper lacks a thorough discussion on how to evaluate Learning from Demonstration (LfD) approaches, which is essential for validating the proposed methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the evaluation metrics, specifically defining the relationship between unit-test pass rates and task success rates. Additionally, the authors should provide a concrete definition of task specifications and the criteria for summarization to enhance the evaluation's rigor. It would be beneficial to address the assumptions regarding complete state descriptions in demonstrations and explore how the method can handle under-specified or noisy task descriptions. We also suggest evaluating the framework against a broader set of tasks that incorporate diverse temporal properties, as outlined in Menghi et al.'s taxonomy, to strengthen the generalization claims. Furthermore, we recommend that the authors improve the discussion on the evaluation gap between LLM-based planners and classical planners, incorporating specific methodologies for evaluating LfD approaches in the final version. Lastly, including more examples of pipelines transitioning from demonstrations to robot policies in the classical planning domain could provide valuable insights into the evaluation of inductive learning problems on complex tasks.