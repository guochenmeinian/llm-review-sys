ID: OHimIaixXk
Title: Building the Bridge of Schrödinger: A Continuous Entropic Optimal Transport Benchmark
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 6, 7, 9, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 1, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a methodology for constructing benchmark pairs for entropic optimal transport (EOT) and Schrödinger Bridge (SB) problems, addressing the existing gap in standardized evaluation methods for neural solvers. The authors emphasize the complexity of creating benchmark pairs for optimal transport/SB and argue that their main contribution lies in the methodology for generating benchmark datasets. They propose a novel approach to create pairs of probability distributions with known EOT solutions, facilitating the evaluation of existing solvers against a theoretically grounded benchmark. The authors acknowledge the importance of scenarios that resemble real-world data and suggest that their methodology can generate benchmark pairs based on single-cell biological data. They also address concerns regarding the generalizability of their benchmarks and the evaluation metrics used.

### Strengths and Weaknesses
Strengths:  
- The topic is highly relevant, and the proposed methodology is novel and theoretically sound.  
- The constructed pairs of distributions with known SB solutions provide a valuable resource for researchers.  
- The paper demonstrates the potential for creating benchmarks that are relevant to real-world applications, particularly in single-cell biology.  
- The writing is clear, and the codebase is well-organized, enhancing reproducibility.  

Weaknesses:  
- The focus on dataset creation rather than benchmarking existing methods limits the paper's impact.  
- The choice of potential function $f^*$ as a weighted log-sum-exp of quadratic functions may restrict the generalizability and representativeness of the benchmarks.  
- The performance of solvers is heavily dependent on hyperparameter tuning, which has not been adequately addressed, raising concerns about the validity of the comparisons made between solvers.  
- The methodology may not adequately address complex or non-standard distributions, questioning the benchmark's applicability in broader contexts.

### Suggestions for Improvement
We recommend that the authors improve the generality of their benchmark by considering alternative methods for constructing distribution pairs that are more representative of real-world scenarios. Additionally, we suggest that the authors conduct a more thorough hyperparameter tuning process to enhance the validity of their experimental comparisons. It may also be beneficial to test a wider range of synthetic situations or those closely related to real-world data. Furthermore, we encourage the authors to explore alternative potential functions to enhance generalizability and representativeness. Lastly, we suggest that the authors further elaborate on the practical applications of their benchmarking tool in various machine-learning contexts, thereby illustrating its potential impact on the field.