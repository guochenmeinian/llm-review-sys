ID: arHJlYiY2J
Title: Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 7, 6, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for generating consistent multi-view videos of the same scene using a cross-video synchronization module (CVD) that leverages epipolar geometry. The authors propose a hybrid training strategy that utilizes both static and dynamic video datasets, specifically RealEstate10K and WebVid10M, to enhance geometric and semantic consistency across generated videos. Experimental results indicate that the proposed method outperforms existing approaches in generating coherent multi-view content, even in scenarios with large camera changes and non-overlapping regions. The authors adjust the homography transformation parameters to closely simulate camera movements in RealEstate10K, ensuring that differences in camera trajectories do not hinder performance. They acknowledge limitations related to synchronization issues in generated content, attributing these to the base model, AnimateDiff, and suggest that newer models like OpenSORA/CogVideoX may mitigate these problems.

### Strengths and Weaknesses
Strengths:
- The originality of utilizing epipolar geometry for consistency across generated videos is noteworthy.
- The model effectively handles large camera motion and non-overlapping scenarios.
- Training on diverse datasets (RealEstate10K and WebVid10M) supports robust generation performance.
- The qualitative results demonstrate high-quality consistency in the generated videos.
- Adjustments to homography transformation parameters enhance simulation accuracy.
- The paper is well-organized and clearly written, with a comprehensible derivation in the supplementary material.
- The proposed method shows significant performance improvements over baseline methods, particularly in geometry and semantic consistencies.

Weaknesses:
- Clarification is needed regarding the application of the training strategy to the cross-view synchronization module (CVSM) and whether the CVSM blocks in different figures are initialized from one another.
- There are issues in the derivation from Eq. (16) to (17) in the supplementary material, particularly regarding the independence of videos capturing the same scene.
- The paper lacks a discussion of recent related works that address multi-view consistency through latent space manipulation.
- The absence of ablation studies makes it difficult to assess the impact of crucial data augmentation strategies on performance.
- The model's performance is still influenced by the limitations of the base model, leading to potential synchronization issues.
- More detailed information about the datasets used could enhance understanding.
- The use of different LoRA modules for training on various datasets raises questions about their integration during inference.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the training strategy in Section 4.1, specifically addressing the initialization of CVSM blocks. Additionally, please correct or clarify the derivation issues identified from Eq. (16) to (17). We suggest including a discussion of recent related works on multi-view consistency to contextualize the contribution of this paper. Furthermore, we encourage the authors to conduct ablation studies to evaluate the effectiveness of data augmentation strategies. We also recommend providing more detailed information about the datasets presented to enhance understanding. Lastly, please address the synchronization issues more thoroughly and explore the integration of newer video generation models to enhance the overall robustness of the results.