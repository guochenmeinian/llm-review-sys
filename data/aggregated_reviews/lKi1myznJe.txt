ID: lKi1myznJe
Title: ReasoningLM: Enabling Structural Subgraph Reasoning  in Pre-trained Language Models for Question Answering over Knowledge Graph
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach to enhance subgraph reasoning in Knowledge Graph Question Answering (KGQA) by developing ReasoningLM, a Pre-trained Language Model (PLM) that incorporates subgraph-aware self-attention and an adaptation tuning strategy. The authors demonstrate the effectiveness of their model through extensive experiments, showing superior performance compared to existing baselines in KGQA tasks.

### Strengths and Weaknesses
Strengths:  
1. The proposed approach is well-motivated and addresses a significant challenge in KGQA, demonstrating broad applicability across different PLMs.  
2. The experimental results validate the effectiveness of ReasoningLM, showcasing strong performance on multiple datasets.  
3. The paper introduces efficient training strategies that contribute to performance improvements.

Weaknesses:  
1. The novelty of the proposed method is somewhat incremental, as similar concepts have been explored in prior works, particularly regarding structural self-attention and sparse masks.  
2. Concerns about the fairness of comparisons with baseline methods arise due to the use of additional training data during adaptation tuning.  
3. The rationale behind certain methodological choices, such as the use of KL divergence loss and the BFS-based subgraph serialization, lacks sufficient explanation.

### Suggestions for Improvement
We recommend that the authors improve the discussion on how their approach differs from existing methods, particularly in relation to structural self-attention and sparse masking. Additionally, conducting a fair comparison by training baseline models with the same additional data would strengthen the evaluation of their approach. The authors should also provide performance metrics for the ablated system with PET (Partial Rule-SYN) for a fair comparison with PET (LLM-SYN). Clarifying whether the position embeddings respect the structural positions of subgraph elements and elaborating on the advantages of KL divergence loss in this context would enhance the paper. Finally, addressing concerns regarding BFS-based subgraph serialization and the requirement for knowledge of the Golden topic entity would improve the understanding of the model's capabilities.