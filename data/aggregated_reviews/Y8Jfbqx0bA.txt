ID: Y8Jfbqx0bA
Title: On Consistent Bayesian Inference from Synthetic Data
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 3, 3, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 2, 2, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for performing consistent Bayesian inference from synthetic data generated under differential privacy (DP) and non-DP settings. The authors propose mixing posterior samples from multiple synthetic datasets, demonstrating that this technique converges to the posterior of downstream analysis under specific conditions. The theoretical foundation relies on the Bernstein-von Mises theorem, with experimental validation through non-private Gaussian mean estimation and DP logistic regression. The authors argue that their method provides necessary corrections for various downstream analyses while allowing for the reuse of non-DP analysis methods. They highlight challenges in achieving accuracy under DP with synthetic data, particularly for tasks like computing the sample median, and discuss the implications of using synthetic data sizes and the advantages of Bayesian methods over frequentist approaches.

### Strengths and Weaknesses
Strengths:
- The paper addresses a timely and significant topic, providing a novel approach to Bayesian inference in the context of synthetic data.
- The theoretical framework is mostly well-presented, and the narrative is relatively easy to follow.
- The authors contribute significantly to the theoretical understanding of Bayesian inference with synthetic data and address practical limitations of existing DP methods, highlighting the flexibility of synthetic data for multiple analyses.
- The authors provide empirical evidence supporting their claims, particularly in the UCI Adult experiment, and offer code, which is beneficial for future research.

Weaknesses:
- The motivation for using synthetic data, particularly without DP, is unclear, raising questions about its advantages over direct DP summaries.
- The theoretical contribution lacks clarity, particularly regarding the claim that the distribution of synthetic data can closely approximate the original distribution as sample size increases.
- The reliance on strong assumptions, such as Gaussian models and the congeniality assumption, may limit the applicability of the proposed method, and more evidence is needed to demonstrate robustness against violations of these assumptions.
- The requirement for synthetic dataset sizes to approach infinity is a notable disadvantage not present in frequentist settings, and the practical appeal of the method is diminished by its performance under violations of the congeniality assumption.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind their research, explicitly stating the unique benefits of Bayesian inference in this context. Additionally, the rationale for using synthetic data without DP should be better articulated. The authors should clarify the theoretical contributions and provide more extensive evidence of robustness concerning congeniality violations, potentially through additional examples where the posterior distribution is not Gaussian. We also suggest improving the discussion on the robustness of their method regarding violations of the congeniality assumption, providing nuanced examples and illustrations. Clarifying the role of $\theta$, the differences between $X^{Syn}$ and $X^*$, and the conditions outlined in the paper will enhance clarity. Including more extensive experiments and discussions on the implications of synthetic dataset sizes, as well as addressing the limitations of the Bernstein-von Mises theorem, will strengthen the paper's contributions. Finally, we suggest that the authors elaborate on the advantages of Bayesian methods in comparison to frequentist approaches to further justify their framework.