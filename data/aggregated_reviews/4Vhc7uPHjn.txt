ID: 4Vhc7uPHjn
Title: ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 7, 6, 7, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the REXTIME benchmark, aimed at evaluating the temporal reasoning capabilities of Video-LLMs through a multi-choice VideoQA format. The authors generate questions and answers using LLMs from caption inputs of ActivityNet-captions and QVHighlight, revealing limitations in advanced MLLMs' reasoning across time. The dataset comprises 9,695 training, 921 validation, and 2,143 test samples, with a focus on diverse question types categorized into sequential, cause-effect, and means-to-an-end.

### Strengths and Weaknesses
Strengths:
- The benchmark is well-developed and provides strong baselines, offering valuable insights into MLLMs' performance.
- The automated pipeline for dataset generation minimizes manual annotation efforts while ensuring high-quality samples.
- The paper is well-written, with clear supplementary materials detailing implementation and dataset specifics.

Weaknesses:
- There is insufficient differentiation between REXTIME and NExT-GQA, particularly regarding the smaller number of annotated samples and the omission of causal questions.
- The analyses do not adequately address the designed question types, and the connection between "means-to-an-end" questions and video understanding is weak.
- The lack of examples illustrating QA quality raises concerns about potential dataset biases.

### Suggestions for Improvement
We recommend that the authors improve the differentiation of REXTIME from NExT-GQA by providing a more detailed comparison and addressing the limitations in sample size and question types. Additionally, we suggest conducting a "BlindQA" study to assess the dataset's biases and to include more examples of event pairs to clarify the question categories. It would also be beneficial to analyze model performance across different question types and to clarify the distinction between Intentionality and Purpose in the scoring criteria. Finally, we encourage the authors to provide clearer explanations regarding the dataset's construction and the implications of their findings on model performance.