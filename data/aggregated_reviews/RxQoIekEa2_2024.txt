ID: RxQoIekEa2
Title: Statistical and Geometrical properties of the Kernel Kullback-Leibler divergence
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 5, 5, 6, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the regularized kernel KL divergences, extending the kernel KL divergence introduced by Francis Bach. The authors derive a closed-form formula that facilitates computational efficiency through gradient methods and establish a new finite sample estimation convergence bound. Numerical simulations indicate that the proposed divergences are easy to implement and effective for sampling complex distributions.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- A finite sample approximation bound is derived.
- Numerical simulations demonstrate superior performance compared to related divergences.
- The explicit formula for discrete measures is a significant contribution.

Weaknesses:
- Proposition 1 assumes $p$ is absolutely continuous with respect to $q$.
- The bound in Proposition 3 becomes trivial as $\alpha \to 1$.
- Applications of the proposed methods appear underwhelming, with gradient flows not performing well.
- Lack of experiments demonstrating scalability to high-dimensional problems.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Proposition 4 by recalling the definition of $\varphi(x)$ and motivating the condition on the Radon-Nikodym derivative. Additionally, we suggest addressing the typo at line 301 regarding the regularization parameter and clarifying how the performance of KKL compares with skewed KL divergence. It would be beneficial to provide insights into the scalability of their flows to high dimensions and to revise the title to reflect the focus on "regularized kernel KL divergence."