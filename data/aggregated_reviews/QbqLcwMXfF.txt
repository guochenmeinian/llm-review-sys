ID: QbqLcwMXfF
Title: Selective Attention: Enhancing Transformer through Principled Context Control
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 7, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Selective Self-Attention (SSA), which introduces trainable temperature functions to modulate the contextual sparsity of attention weights in self-attention mechanisms. The authors demonstrate that SSA can improve perplexity and optimization in language modeling tasks with only a slight increase in parameters. The approach includes token-aware and position-aware gating for queries, keys, and values, aiming to enhance control over attention dynamics.

### Strengths and Weaknesses
Strengths:  
- The technical contribution is an interesting and intuitive enhancement to standard attention mechanisms, benefiting both toy examples and real-world data.  
- The motivation for SSA is clear, and the paper is well-written, making it accessible.  
- The mathematical exploration is comprehensive, providing solid reasoning for SSA's advantages.  

Weaknesses:  
- The claim that performance improvements are due to SSA rather than increased parameter count lacks sufficient argumentation.  
- The evaluation is limited to language modeling tasks, with no comparisons to other selective or sparse attention methods.  
- The mathematical sections are dense, introducing terms and propositions quickly without adequate background.  
- The terminology used, such as "soft sparsity," is misleading and could be clarified.  
- There is no detailed description of datasets or the computational overhead associated with the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the argumentation supporting the claim that SSA provides benefits beyond just increased parameters by testing models scaled up with standard attention. Additionally, providing a more comprehensive evaluation across various NLP tasks would strengthen the paper. Clarifying the terminology used, especially regarding "soft sparsity," and offering a detailed description of datasets and their characteristics would enhance understanding. Finally, addressing the computational costs associated with the proposed method, particularly for larger models, is essential.