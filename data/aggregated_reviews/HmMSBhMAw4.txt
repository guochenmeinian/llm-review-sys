ID: HmMSBhMAw4
Title: Periodic agent-state based Q-learning for POMDPs
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 8, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel reinforcement learning (RL) algorithm, PASQL (Periodic Agent-State Based Q-Learning), designed for Partially Observable Markov Decision Processes (POMDPs). The authors argue that traditional methods relying on belief states are impractical in model-free settings, as they require knowledge of the system model. Instead, PASQL learns non-stationary, periodic policies using agent states that do not satisfy the Markov property. The authors provide theoretical convergence guarantees and demonstrate the algorithm's effectiveness through numerical experiments.

### Strengths and Weaknesses
Strengths:
1. The paper creatively integrates concepts from periodic Markov processes and Q-learning, addressing non-Markovian dynamics effectively.
2. It offers a rigorous theoretical framework, including convergence proofs and bounds on sub-optimality, supporting the claims about PASQL's efficacy.
3. The numerical experiments convincingly demonstrate that PASQL can outperform standard Q-learning in POMDP settings.
4. The authors provide a comprehensive comparison with existing methods, highlighting PASQLâ€™s advantages.

Weaknesses:
1. The introduction of periodic policies increases complexity and may require more computational resources, potentially limiting practical applications.
2. The focus on tabular representations may not scale well to high-dimensional problems or those requiring function approximation.
3. The proposed algorithm leads to a limited class of policies, primarily deterministic, which may not fully exploit the potential of stochastic policies.
4. The numerical experiments are based on small examples, raising questions about their generalizability.

### Suggestions for Improvement
We recommend that the authors improve the exploration of stochastic policies to enhance the expressiveness of the proposed algorithm. Additionally, we suggest expanding the numerical experiments to include larger and more complex scenarios to better assess the algorithm's scalability and practical applicability. Clarifying the choice of the period \( L \) in the experiments and its sensitivity to performance would also strengthen the paper. Finally, addressing the limitations of the current approach, particularly regarding the reliance on deterministic policies, would provide a more comprehensive understanding of PASQL's capabilities.