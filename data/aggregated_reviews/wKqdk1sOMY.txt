ID: wKqdk1sOMY
Title: Execution-Based Evaluation for Open-Domain Code Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ODEX, a dataset comprising 945 NL-Code pairs across four languages: English, Spanish, Japanese, and Russian. It includes 1,707 human-written test cases for execution-based evaluation and addresses challenges in open-domain code execution, such as irreproducible runs and randomized outputs. The authors claim that ODEX covers a broader range of open-domain libraries compared to existing datasets, and it is tested on CODEX and CODEGEN.

### Strengths and Weaknesses
Strengths:
- ODEX's execution-based evaluation aligns closely with human preferences, enhancing its contribution to the community.
- The dataset's comprehensive analysis of domain distribution and data complexity is commendable.
- The meticulous dataset creation process, including quality checks during annotation, is well-designed.

Weaknesses:
- The novelty of ODEX is unclear, as it appears to be a subset of existing datasets with added human-written test cases.
- The small size of the dataset raises concerns about the credibility of the results, particularly when divided by language.
- The experiments do not adequately demonstrate the sufficiency or diversity of the test cases, and potential biases in test case creation are noted.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the dataset's novelty by explicitly comparing it to existing datasets. Additionally, consider addressing the dataset's size limitations by expanding the number of code pairs and ensuring a more representative sample across libraries. To enhance the analysis of test cases, provide detailed metrics on their diversity and sufficiency. Furthermore, we suggest revising the test case creation process to minimize bias by allowing annotators to focus solely on the query without exposure to the canonical code. Lastly, include a thorough examination of the results to demonstrate the effectiveness of the test cases in evaluating code correctness.