ID: GWOCiRkjCF
Title: Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model Collaboration
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a precedent-enhanced Legal Judgment Prediction (LJP) framework called PLJP, which integrates the strengths of both large language models (LLMs) and domain models to improve LJP tasks. The authors propose a pipelined framework that includes fact summarization, judgment prediction, precedent retrieval, and in-context LJP. Experiments on real-world datasets, including CAIL2018 and CJO22, demonstrate the effectiveness of the proposed method, showing improved results in penalty term and law article prediction, with comparable performance in charge prediction.

### Strengths and Weaknesses
Strengths:
- The paper is the first to utilize precedents to enhance LJP tasks effectively.
- The integration of LLMs and domain models presents a promising direction for legal AI.
- Extensive experiments validate the proposed method's effectiveness.

Weaknesses:
- The writing lacks clarity, particularly in explaining how labels are derived from precedents.
- There is a lack of comparison with relevant recent work, such as TOPJUDGE and CTM.
- The method's heavy reliance on a fine-tuned BERT-based classifier raises concerns about its robustness.
- The ablation studies reveal inconsistencies in model performance when generating explanations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their writing, especially in Section 4.2.1, to better explain the derivation of labels from precedents. Additionally, the authors should compare their framework with relevant recent works, such as TOPJUDGE and CTM, to substantiate their claims of effectiveness. It is also crucial to analyze the reliability of candidate results from the first-stage prediction model, given the pipeline nature of their approach. Finally, we suggest conducting a more detailed analysis of the ablation experiments, particularly regarding the impact of explanation generation on model performance.