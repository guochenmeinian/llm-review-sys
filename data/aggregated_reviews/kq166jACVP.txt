ID: kq166jACVP
Title: Aligner: Efficient Alignment by Learning to Correct
Conference: NeurIPS
Year: 2024
Number of Reviews: 46
Original Ratings: 7, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Aligh, a lightweight and model-agnostic alignment method that learns correctional residuals between preferred and dispreferred answers using a separate model. The authors demonstrate its effectiveness through extensive experiments across 11 different LLMs, focusing on metrics such as helpfulness, harmlessness, and honesty. The paper also includes interpretability experiments and examines benefits in multi-round RLHF. Additionally, the authors propose a novel alignment paradigm that fine-tunes a pre-trained LLM as an 'aligner' module, mapping misaligned zero-shot responses to corrected aligned responses, which is agnostic to model size and architecture. Furthermore, the Aligner integrates residual learning principles to enhance performance across various tasks, including code generation, mathematics, and instruction-following, and addresses the OOD reward model collapse problem in reinforcement learning from human feedback (RLHF) through a dual Copy and Correction mechanism. The authors provide extensive experimental results demonstrating that the Aligner consistently enhances model capabilities, as evidenced by comparative performance metrics across several datasets.

### Strengths and Weaknesses
Strengths:
- The method is well-motivated, lightweight, and demonstrates effectiveness in iterative updating during multi-round RLHF.
- Extensive experiments across various datasets and scenarios provide robust validation of the approach.
- The incorporation of residual learning principles allows the Aligner to learn from both good and bad responses, enhancing alignment performance.
- The authors conducted thorough experiments, including human evaluations and comparisons with existing methods, which enhance the paper's credibility.
- The paper includes interpretability techniques that yield interesting insights into the alignment process.

Weaknesses:
- The discussion on the out-of-domain (OOD) extent of training data concerning test data is insufficiently addressed, raising questions about the aligner's performance on OOD datasets.
- Some experimental results, particularly with Llama2-70B-Chat, showed negative impacts due to issues with response length and EOS token generation.
- The reliance on human annotators for corrections raises concerns about the cost and scalability of the Aligner.
- There is a lack of concrete examples of prompts and before/after alignment responses, which would enhance qualitative understanding.
- The claim that correcting misaligned answers is easier than fine-tuning the target LM lacks sufficient empirical support.

### Suggestions for Improvement
We recommend that the authors improve the discussion regarding the out-of-domain performance of the aligner, particularly how it handles OOD datasets. Additionally, including concrete examples of prompts and the corresponding responses before and after alignment would provide clearer insights into the aligner's effectiveness. We suggest documenting the baseline effectiveness of simply prompting the model to output the same answer, as this could clarify the necessity of QAA fine-tuning. Furthermore, demonstrating the aligner's performance on adversarially crafted OOD test sets would strengthen the claims of its robustness. We also recommend improving clarity regarding the cost implications of using the Aligner compared to traditional methods, emphasizing its efficiency in utilizing preference datasets. Lastly, we encourage the authors to clarify the loss landscape representation to avoid potential misunderstandings and to provide targeted experimental results supporting the claim that correcting misaligned answers is easier than fine-tuning the target LM directly.