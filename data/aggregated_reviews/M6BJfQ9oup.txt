ID: M6BJfQ9oup
Title: Conceptor-Aided Debiasing of Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach to mitigate bias in language models using conceptors, introducing two methods: a post-processing method and a continuing training method. The authors utilize the logical semantics of conceptors (AND and OR) to explore the combination of different bias attribute word lists and address intersectional bias. Comprehensive experiments are conducted to evaluate the impact of various debiasing corpora, bias word lists, and their combinations on different language models, revealing that the proposed methods effectively mitigate bias while maintaining model performance.

### Strengths and Weaknesses
Strengths:  
- The proposed methods are novel and demonstrate effective debiasing across multiple language models.  
- The use of conceptors' logical semantics to study bias mitigation is innovative.  
- Extensive experiments provide valuable insights into bias mitigation practices.  
- The introduction of CI-BERT shows a proactive approach to bias reduction during training.

Weaknesses:  
- Some details regarding Algorithm 1 are insufficient, particularly concerning the creation of the bias subspace.  
- The "Conceptor Intervention and Continued Training" method may be computationally expensive and lacks clarity on the recalibration of word vectors.  
- Key results are relegated to the appendix, which could hinder presentation and accessibility.  
- The methods are limited to English and focus primarily on North American biases, which may restrict applicability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Algorithm 1 by providing more details on the bias subspace creation process. Additionally, we suggest clarifying the computational implications of the "Conceptor Intervention and Continued Training" method, particularly regarding the recalibration of word vectors. To enhance the paper's presentation, we advise reorganizing the content to include significant results from the appendix in the main text. Finally, we encourage the authors to broaden the scope of their study to include multilingual models and a wider range of biases beyond North America.