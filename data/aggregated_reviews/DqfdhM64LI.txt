ID: DqfdhM64LI
Title: Decentralized Randomly Distributed Multi-agent Multi-armed Bandit with Heterogeneous Rewards
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 6, 7, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on decentralized multi-agent multi-armed bandit (MAB) problems, focusing on heterogeneous rewards and time-varying random graphs. The authors propose algorithms that incorporate a burn-in period followed by a learning phase, utilizing upper confidence bounds (UCB) to optimize arm selection. The work claims to be the first to address these challenges with heavy-tailed reward distributions and provides regret analysis demonstrating optimal performance in various settings. Additionally, the authors emphasize communication efficiency and regret bounds, proposing to integrate communication cost analysis and relevant references to enhance the paper's depth. They acknowledge the importance of existing works that characterize the trade-off between regret and communication costs, particularly in scenarios involving random graphs and heterogeneous rewards.

### Strengths and Weaknesses
Strengths:
- The paper tackles a complex and relevant problem in multi-agent MAB, addressing both heterogeneous rewards and dynamic communication graphs.
- The presentation is clear, making the theoretical contributions accessible to readers.
- The proposed algorithms are intuitively sound and supported by solid theoretical results.
- The authors provide a thorough response to reviewer feedback, indicating a willingness to improve the manuscript.
- They plan to incorporate valuable references that enhance the theoretical foundation of their work.
- The proposed modifications to the introduction and conclusion sections strengthen the overall argument and completeness of the paper.

Weaknesses:
- The technical aspects may exceed the expertise of some reviewers, raising concerns about the correctness of proofs.
- The paper does not address communication complexity, which is a significant aspect in multi-agent bandit literature.
- The focus on global optimal arms may overlook the importance of local optimal arms for individual agents.
- Lack of experimental validation to support theoretical claims.
- The paper may benefit from a more detailed communication analysis, particularly regarding algorithms that achieve lower communication costs in multi-agent bandits.
- The current assumptions about homogeneous rewards may limit the applicability of the findings to more complex scenarios.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the motivation and practical applications of the chosen graph types, particularly regarding the connection between random graphs and real-world scenarios. Additionally, clarifying the rationale behind using a constant probability of "1/2" in Algorithm 1 would enhance understanding. We suggest exploring the challenges of extending the analysis to directed graphs and discussing the scalability of the proposed methods for larger multi-agent systems. Furthermore, we recommend improving the communication analysis in the updated version by incorporating insights from works that achieve lower communication costs, such as those by Wang et al. (2020) and Wang et al. (2022). It would also be beneficial to expand the discussion on the implications of heterogeneous rewards and the complexities introduced by random graph structures. Finally, incorporating synthetic simulations to visualize information flow, the impact of heterogeneity on regret, and the effects of different graph structures would substantiate the theoretical contributions and ensure that the conclusion section reflects the advancements in communication efficiency as a promising direction for future research.