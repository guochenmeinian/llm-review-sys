ID: 97E3YXvcFM
Title: Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 6, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for sample-efficient exploration in sparse-reward deep reinforcement learning by maximizing value-conditioned state entropy. The authors propose a value-conditioned state entropy objective that combines state entropy maximization with task rewards, implemented through a non-parametric conditional entropy estimator. The paper compares this approach against standard entropy maximization across various environments, including MiniGrid, DeepMind Control, and Meta-World.

### Strengths and Weaknesses
Strengths:
- The intrinsic reward can be easily integrated into existing methods.
- Experiments demonstrate that the value-conditioned objective enhances or matches performance across multiple domains.
- The paper articulates its ideas clearly and includes compelling visualizations of conditional entropy estimation.

Weaknesses:
- The computation of intrinsic rewards relies on robust value function estimates, which is challenging in sparse-reward contexts.
- The theoretical foundation for value-conditioned state entropy remains unclear, raising questions about the generality of the results.
- Some aspects of the writing, particularly certain formulas, may be difficult to understand.

### Suggestions for Improvement
We recommend that the authors improve the theoretical grounding for value-conditioned entropy bonuses, particularly regarding how value function estimation errors impact the learning process. Additionally, we suggest discussing the choice of using samples from the replay buffer versus the latest samples for value-conditional entropy, including the pros and cons of both approaches. It would also be beneficial to analyze the sensitivity of VCSE to the parameter $\beta$ and to compare their method with recent work on constrained entropy maximization, highlighting the advantages and disadvantages of each approach. Finally, we encourage the authors to clarify the motivation behind their method and consider including more complex environments for experimental validation.