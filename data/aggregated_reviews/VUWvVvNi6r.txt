ID: VUWvVvNi6r
Title: Unveiling the Hidden Structure of Self-Attention via Kernel Principal Component Analysis
Conference: NeurIPS
Year: 2024
Number of Reviews: 28
Original Ratings: 5, 7, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 2, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new method, Kernel PCA Attention (RPC-Attention), aimed at enhancing self-attention mechanisms in transformers for sequence modeling in natural language processing and computer vision. The authors derive self-attention from kernel principal component analysis (kernel PCA), demonstrating that the attention outputs project query vectors onto the principal component axes of the key matrix. They empirically validate RPC-Attention's effectiveness through improved performance in object classification, language modeling, and image segmentation tasks. Furthermore, the authors address scalability concerns by clarifying that the feature dimension per head is often smaller than the sequence length, and they provide a detailed table of feature dimensions for various ViT models. Additional experiments on a downstream natural language understanding task show that RPC-Attention significantly outperforms baseline models in sentiment classification tasks.

### Strengths and Weaknesses
Strengths:
1. The paper provides a novel perspective on self-attention through kernel PCA, supported by thorough mathematical explanations.
2. The derivation of RPC-Attention is convincing and well-articulated.
3. The experimental results demonstrate RPC-Attention's robustness against data corruption and adversarial attacks.
4. The authors effectively address scalability concerns with empirical evidence and detailed explanations.
5. The additional experiments on the SST-5 task validate the effectiveness of RPC-Attention in practical applications.

Weaknesses:
1. The practicality of RPC-Attention is questionable, as experiments show limited scalability and no significant performance advantage over traditional ViT models.
2. Concerns arise regarding the computational speed of RPC-SymViT, which is slower than ViT, and the scalability of the model as performance gains diminish with increased size.
3. The iterative algorithm used in RPC-Attention raises questions about stability and gradient flow during back-propagation.
4. Some reviewers initially expressed concerns about scalability, indicating that this aspect may require further clarification for broader acceptance.

### Suggestions for Improvement
We recommend that the authors improve the scalability of RPC-Attention by exploring larger models and conducting experiments with more standardized settings, such as ViT-Base with extended training epochs. Additionally, we suggest clarifying the backward pass definition and addressing the stability of the PAP algorithm during back-propagation to enhance reproducibility and clarity. Improving the clarity of explanations regarding scalability will help preemptively address potential reviewer concerns. Finally, providing a deeper analysis of the computational costs associated with varying iterations and layers would help justify the practical applicability of RPC-Attention in multi-layer settings and provide more context on the implications of the experimental results to enhance the overall impact of the findings.