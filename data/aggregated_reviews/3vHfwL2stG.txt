ID: 3vHfwL2stG
Title: The Ladder in Chaos: Improving Policy Learning by Harnessing the Parameter Evolving Path in A Low-dimensional Space
Conference: NeurIPS
Year: 2024
Number of Reviews: 30
Original Ratings: 5, 5, 4, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel deep reinforcement learning algorithm called Policy Path Trimming and Boosting (PPTB), which employs a temporal singular value decomposition (SVD) to analyze the evolution of policy network parameters in TD3 and RAD agents. The authors investigate how policy parameters change over time, revealing significant detours and concentrated learning dynamics in a low-dimensional space. The proposed method aims to enhance performance by boosting updates in primary parameter directions while trimming those in secondary directions. The findings indicate that PPTB significantly improves performance metrics such as score and AUC in various environments. Additionally, the paper explores oscillatory patterns in learning dynamics across different machine learning paradigms, particularly focusing on reinforcement learning (RL) and supervised learning (SL). The authors observe that Behavior Cloning (BC) policies exhibit different oscillation patterns compared to RL methods, particularly with Adam and SGD optimizers, and that empirical evidence suggests RL and SL do not exhibit distinctly different oscillatory patterns.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, clear, and methodical, making it easy to understand the analysis and findings.
- The innovative use of temporal SVD to study parameter evolution is a novel contribution to the field.
- The proposed PPTB method shows promising improvements in performance on standard deep reinforcement learning benchmarks.
- The inclusion of additional results and learning curves enhances the clarity and completeness of the findings.
- The authors conducted additional experiments demonstrating that PPTB improves learning performance in MinAtar tasks, which adds credibility to their claims.

Weaknesses:
- The evaluation is limited to TD3 and RAD, raising questions about the universality of PPTB across other DRL algorithms.
- Some figures are unclear, particularly the last two in Figure 2(b), and certain reported improvements in AUC appear to contain errors.
- The paper lacks a theoretical explanation for why the proposed approach accelerates convergence and does not adequately address the implications of using large neural network dimensions in experiments.
- The interpretation of oscillation patterns in BC and RL training dynamics requires clarification, as some reviewers noted that oscillations are present in both paradigms.
- The use of terms like "wavelets" and "harmonics" is imprecise and has led to misunderstandings.
- The paper's narrative needs reworking to clarify the relationship between oscillations and known sources in RL, such as TD learning and actor-critic dynamics.

### Suggestions for Improvement
We recommend that the authors improve the clarity of figures, particularly those in Figure 2(b), to enhance comprehension. Additionally, please provide learning curves for TD3 and TD3-PPTB to clarify performance comparisons. It would be beneficial to include results for 1000k or 2000k steps in the RAD results to provide a more comprehensive evaluation. We also suggest discussing the determination of the parameter \( P_b \) in your experiments and its potential impact on training stability. Furthermore, consider expanding the experimental scope to include other DRL algorithms and environments, as well as providing a more detailed analysis of the limitations and future directions for the PPTB method. Lastly, we encourage the authors to clarify the algorithm's mechanics in Section 4.1 and Section 4.2 for better understanding. We also recommend improving the clarity of claims regarding oscillation patterns, emphasizing that these phenomena may not be exclusive to RL but could also apply to supervised learning. Strengthening the connection to related work on low-dimensional subspace training would enhance the paper's relevance. Finally, we suggest that the authors present their findings in a more organized manner, starting from a general deep learning perspective before delving into specific empirical observations.