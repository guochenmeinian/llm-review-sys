ID: e5icsXBD8Q
Title: Large Language Model Unlearning via Embedding-Corrupted Prompts
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 6, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach called Embedding-COrrupted (ECO) Prompts for efficient unlearning in large language models (LLMs). ECO employs a prompt classifier to maintain an unlearning state during inference, allowing for the identification, corruption, and safeguarding of prompts that need to be forgotten without modifying the LLMs themselves. The authors demonstrate the effectiveness of ECO Prompts through extensive experiments across various LLMs and unlearning benchmarks, achieving effective unlearning with minimal side effects. Additionally, the paper explores the forget quality metric, which is derived from the output distributions of retained and unlearned models rather than multiple-choice assessments. The forget quality is computed using the truth ratio and the Kolmogorovâ€“Smirnov (KS) test to evaluate distribution similarity. The authors provide a comprehensive evaluation of the ROUGE-L metric and real generated examples to illustrate the effects of unlearning, alongside probing experiments on WMDP to show that attackers cannot recover correct answers from raw model logits. They also explore task-agnostic selection of surrogate metrics to enhance the robustness of their approach.

### Strengths and Weaknesses
Strengths:
- The authors propose a direct method for unlearning during inference, addressing the challenges of retraining LLMs, which contributes significantly to the field of unlearning.
- ECO Prompts demonstrate strong generalizability across various knowledge entanglement and unlearning tasks, with potential for future integration with other techniques.
- The paper is well-organized, clearly written, and includes comprehensive experimental results and literature review.
- The design choices, such as thresholding the classifier with conformal prediction, are well thought out.
- The paper offers a clear and thorough explanation of the forget quality metric and its computation, enhancing understanding of the unlearning process.
- The evaluation metrics, including ROUGE-L and probing experiments, are well-articulated and demonstrate the effectiveness of the proposed method.
- The authors show a commitment to addressing reviewer feedback and improving the clarity and robustness of their approach.

Weaknesses:
- The reliance on a pre-trained prompt classifier complicates end-user application and raises concerns about trustworthiness and safety.
- The approach does not guarantee that harmful responses or copyrighted content will not be triggered, as some prompts may still elicit such outputs.
- Certain concepts, such as the meaning of labels in Figure 1 and the embedding function, require further clarification.
- The optimization objective raises concerns about the fairness of the employed metric function, which could favor the proposed method over baselines.
- The selection of surrogate metrics appears somewhat heuristic and may be task-specific, potentially limiting generalizability.
- The reliance on additional classifiers may pose challenges in more complex unlearning scenarios, raising concerns about the method's robustness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of key concepts, such as the definition of knowledge entanglement and the specifics of the embedding function. Additionally, the authors should provide further justification for the claim regarding the "potential fuzzy boundary between retaining and forgetting." It would be beneficial to include experiments that directly probe the privacy and hallucination aspects of the model. We suggest clarifying the benefits of the corruption mechanism over a simple classifier + template approach, as this is central to the paper's contributions. Furthermore, we recommend improving the generalizability of the surrogate metrics by exploring additional datasets and tasks to validate their approach further. Consider incorporating external moderation techniques like LlamaGuard or ShieldGemma to enhance the robustness of the classifiers against more complicated unlearning scenarios. Finally, ensure that the information regarding the use of copyrighted material is prominently emphasized in the main body of the revised paper to address ethical concerns effectively.