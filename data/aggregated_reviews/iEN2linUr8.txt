ID: iEN2linUr8
Title: II-Bench: An Image Implication Understanding Benchmark for Multimodal Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 8, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Image Metaphor Understanding Benchmark (II-Bench), a novel benchmark aimed at evaluating the higher-order perception of multimodal large language models (MLLMs) through a comprehensive dataset of 1,222 images and 1,434 questions across various domains. The authors propose that current MLLMs exhibit significant deficiencies in understanding complex image semantics and sentiment comprehension, with human performance averaging 90% accuracy compared to the best MLLM's 74.8%. The study emphasizes the importance of evaluating models' capabilities in understanding abstract and metaphorical images. Additionally, the authors implement a rigorous multi-step annotation process to ensure the quality and difficulty of questions, addressing concerns about subjectivity and the potential ease of questions due to distinctive distractor options. Performance evaluations of various large language models (LLMs) and MLLMs highlight the necessity of multimodal approaches for effective task completion.

### Strengths and Weaknesses
Strengths:
1. The paper identifies a critical gap in MLLM evaluation regarding metaphor understanding in images, introducing an innovative test set.
2. A meticulous image selection strategy enhances the dataset's quality, incorporating diverse modalities and manual judgment.
3. The study employs multiple evaluation methods, including few-shot and CoT, allowing for a comprehensive exploration of factors affecting metaphor understanding.
4. The authors implement a comprehensive annotation process involving multiple reviewers to ensure question quality and difficulty.
5. Performance evaluations demonstrate the significant gap between MLLM and LLM capabilities, emphasizing the need for multimodal integration.
6. The paper addresses the importance of high-order perception in advancing artificial general intelligence (AGI).

Weaknesses:
1. The multiple-choice format of the dataset may hinder evaluation, as models might understand questions but fail to select the correct option.
2. Some questions in II-Bench may lack standard answers, leading to potential inconsistencies in evaluation.
3. The benchmark may not be sufficiently challenging for MLLMs, as indicated by their performance metrics.
4. The dataset's expansion is challenging due to the time-consuming manual creation process.
5. Potential data leakage risks exist, as images sourced from well-known illustration websites may overlap with models' training data.
6. The explanation of the answer extraction process is currently insufficient, which may hinder understanding.
7. The multiple-choice format can lead to overly long contexts, which some models may struggle to handle effectively.

### Suggestions for Improvement
We recommend that the authors improve the dataset by including tests on model performance after fine-tuning with data highly relevant to understanding image implications, which would clarify the adaptability of the models. Additionally, addressing the concerns regarding the multiple-choice format by exploring alternative evaluation methods could enhance the robustness of the assessment. We suggest revising the question design to enhance challenge and reduce subjectivity. It would also be beneficial to provide more details about the annotation process to ensure consistency in subjective questions and to clarify the steps for extracting options in the CoT evaluation method. Furthermore, including a comprehensive explanation of the steps involved in option extraction would ensure transparency and robustness in evaluations.