ID: CMgxAaRqZh
Title: Accelerating Greedy Coordinate Gradient and General Prompt Optimization via Probe Sampling
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 8, 4, 7, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to accelerate discrete prompt optimization algorithms by utilizing a smaller draft model to filter candidates based on an agreement score with the target model. The approach, applied to Greedy Coordinate Gradient (GCG) and other algorithms like AutoPrompt, ADE, and AutoDAN, demonstrates improved runtime while maintaining or enhancing the attack success rate (ASR) on AdvBench.

### Strengths and Weaknesses
Strengths:
- The problem addressed is significant, enhancing efficiency in finding prompts for large models.
- The method is simple and broadly applicable to various discrete prompt optimization algorithms.
- The paper is well-presented, with minor typos, and is easy to read.

Weaknesses:
- The paper does not address transferability, a crucial aspect of adversarial attacks.
- The related work section inadequately presents discrete prompt optimization algorithms and omits prior work using a pair of models for optimization.
- The draft model's alignment with the target model is not sufficiently explained, raising concerns about the generation of meaningless suffixes.

### Suggestions for Improvement
We recommend that the authors improve the discussion on transferability, particularly regarding the impact of the draft model on adversarial attacks across different LMs. Additionally, we suggest enhancing the related work section to include a more comprehensive overview of discrete prompt optimization algorithms and relevant prior research. Clarifying the alignment between the draft and target models, including the implications of using a smaller batch size for GPU efficiency, would strengthen the paper. Finally, we encourage the authors to compare their method with other efficient techniques, such as advprompter and Amplegcg, to contextualize their contributions better.