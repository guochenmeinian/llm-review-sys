ID: Cjnirz5pan
Title: SAFE: Slow and Fast Parameter-Efficient Tuning for Continual Learning with Pre-Trained Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SAFE (Slow And Fast parameter-Efficient tuning), a novel framework for continual learning that integrates slow parameter-efficient tuning (S-PET) to inherit general knowledge from pre-trained models and fast parameter-efficient tuning (F-PET) to adapt to new tasks. The authors argue that their method is the first to effectively utilize these techniques in the context of PTM-based continual learning, providing theoretical foundations using information bottleneck theory. They demonstrate how the slow learner inherits generalizable knowledge from PTMs, enabling effective generalization to novel classes. The authors claim that SAFE achieves state-of-the-art performance across six benchmark datasets, showcasing significant improvements over existing methods. They also address concerns about the slow learner's training, loss functions, and hyperparameters, committing to include comparisons with O-LoRA and DAP in the revised manuscript to enhance the discussion of their contributions.

### Strengths and Weaknesses
Strengths:
- The integration of slow and fast parameter-efficient tuning within a unified framework effectively addresses the stability-plasticity trade-off in continual learning.
- The authors offer unique theoretical insights and practical applications of slow and fast learning in continual learning.
- The paper is well-structured, providing extensive experimental validation that demonstrates the method's superiority across multiple datasets.
- The authors show a willingness to address reviewer concerns and improve the manuscript based on feedback.

Weaknesses:
- The slow learner's training only in the initial session may limit its adaptability to new tasks, potentially affecting model flexibility.
- The complexity of multiple loss functions and hyperparameters complicates the optimization process, making practical tuning challenging.
- Some aspects of the correlation matrix and its learnability remain unclear, requiring further analysis.
- The sensitivity of aggregation inference for already-trained models needs more exploration.
- Limitations of the method, particularly regarding the reliance on strong feature extractors and hyperparameter optimization, are not fully addressed.
- The experimental results do not adequately support the claimed contributions, with marginal improvements observed in ablation studies and insufficient comparisons with the latest methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation and effects of the proposed techniques, particularly regarding the correlation matrix and its relationship to existing methods like RanPAC. Additionally, we suggest conducting further experiments or ablations to clarify the impact of each loss function and hyperparameter choice on the overall performance of SAFE. It would also be beneficial to analyze the stability of hyperparameters and discuss the importance of the aggregation process based on the ablation study. Furthermore, we recommend providing a more thorough examination of the importance and sensitivity of aggregation inference for already-trained models. Finally, we encourage the authors to discuss essential limitations more comprehensively, particularly concerning the method's reliance on strong feature extractors and the theoretical foundation for hyperparameter selection.