ID: i4gqCM1r3z
Title: Reconstruction Attacks on Machine Unlearning: Simple Models are Vulnerable
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 4, 6, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents reconstruction attacks on machine unlearning, proposing a method that accurately recovers deleted samples using linear models before and after sample deletion. The authors leverage a closed-form single-sample training algorithm for linear regression and estimate the covariance matrix from a public dataset. The attack is extended to include models with a fixed embedding function followed by a linear layer.

### Strengths and Weaknesses
Strengths:
- The problem studied is interesting and relevant to privacy concerns in machine unlearning.
- The proposed attack demonstrates strong performance for linear regression models.
- The theoretical derivations are rigorous, and the experiments across various tasks and datasets showcase the attack's effectiveness.

Weaknesses:
- The attack relies on strong assumptions, including knowledge of the training data distribution and model parameters, which may not be realistic in practice.
- The focus is primarily on linear models, raising questions about the attack's applicability to more complex models.
- The experimental evaluation lacks diversity in metrics and does not clarify which unlearning methods were used, potentially affecting the attack's performance.
- The reliance on a public dataset with the same distribution as the training data is questionable, particularly regarding dataset size and its impact on performance.

### Suggestions for Improvement
We recommend that the authors improve the justification for their strong assumptions regarding the attacker's knowledge and access to model parameters. Clarifying the practical scenarios in which these assumptions hold would enhance the paper's credibility. Additionally, the authors should explore the attack's performance with different unlearning methods and provide a more diverse set of evaluation metrics. Evaluating the effect of dataset size on attack performance and discussing potential defenses against the proposed attack would also strengthen the paper. Finally, considering the implications of approximate unlearning in real-world scenarios could provide a more comprehensive understanding of the privacy risks involved.