ID: GqXbfVmEPW
Title: Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 7, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach, MDP (masking-differential prompting), to defend pre-trained language models (PLMs) as few-shot learners against backdoor attacks. MDP leverages the sensitivity of sentence masking to distinguish between triggered and non-triggered sentences, demonstrating exceptional performance across various benchmark datasets. The authors argue that existing defenses are inadequate for few-shot scenarios, and MDP provides a lightweight, effective solution that creates a dilemma for attackers regarding the effectiveness of their attacks versus detection evasion.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a significant security concern regarding the vulnerability of PLMs in few-shot learning contexts.
2. It provides a well-motivated and theoretically grounded approach, supported by comprehensive experimental analysis against various baseline methods.
3. The clarity of presentation, including well-structured explanations and visual aids, enhances reader comprehension.

Weaknesses:
1. The authors should include a formal algorithm for MDP to improve clarity and move key parameters from the appendix to the main body for easier reference.
2. Additional ablation studies are needed to validate the method, particularly varying the poisoning rate, number of triggers, and exploring different model architectures.
3. The evaluation should consider adaptive attacks, including mask-invariant backdoors, and clarify whether MDP can classify all test data as clean when the model is not poisoned.
4. The paper lacks instructions in the readme for running the code.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the proposed method by including a formal algorithm and relocating key parameters to the main body of the paper. Additionally, conducting crucial ablation studies to validate the method's effectiveness across varying conditions is essential. The authors should also consider evaluating MDP against adaptive attacks, particularly those that are mask-invariant, and clarify its classification capabilities for unpoisoned data. Lastly, including detailed instructions in the readme for running the code would enhance usability.