ID: BVN9Kgvwzv
Title: From Cloze to Comprehension: Retrofitting Pre-trained Masked Language Models to Pre-trained Machine Reader
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 6, 7, 7, 4, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel pretraining method for span-extraction tasks, leveraging Wikipedia hyperlinks to construct a large-scale dataset for training. The authors propose using this dataset to continue training a pretrained MLM model, enabling it to extract entity spans from given descriptions and articles. Experimental results demonstrate that pretraining RoBERTa with this dataset improves performance on various machine reading comprehension (MRC) and named entity recognition (NER) tasks. The proposed PMR model also shows effectiveness in classification tasks by extracting supporting spans.

### Strengths and Weaknesses
Strengths:
- The unsupervised data collection process is attractive and may inspire further research.
- The method is straightforward and effective, with comprehensive evaluations and results.
- The proposed pre-trained machine reader achieves state-of-the-art performance on few-shot NER and extractive question answering tasks.

Weaknesses:
- The constructed dataset could also benefit seq2seq models like T5, but only MLM models are explored.
- There are limited classification experiments presented, particularly lacking full-resource results.
- The paper does not include comparisons with smaller LLMs or generative models, limiting its scope.
- The gains from the WAE pretraining task versus simply having more MRC-style training data remain unclear.

### Suggestions for Improvement
We recommend that the authors improve the paper by including experiments with smaller few-shot settings for EQA benchmarks to assess the impact of shot numbers on performance. Additionally, incorporating comparisons with smaller versions of LLMs and other generative models would enhance the robustness of the findings. It would also be beneficial to clarify the differences between PMR and similar works like Splinter in the related work section. Lastly, we suggest adding a limitations section in the main body of the paper to ensure readers can quickly identify potential constraints of the study.