ID: fs28jccJj5
Title: SpikedAttention: Training-Free and Fully Spike-Driven Transformer-to-SNN Conversion with Winner-Oriented Spike Shift for Softmax Operation
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 6, 7, -1, -1, -1, -1
Original Confidences: 2, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SpikedAttention, a novel method for converting pre-trained transformers, such as Swin Transformer and BERT, into spiking neural networks (SNNs) without additional training. Key innovations include trace-driven matrix multiplication and winner-oriented spike shift (WOSS) for softmax, enabling the conversion of attention modules into spike-based computations while preserving the original architecture. The authors demonstrate that SpikedAttention achieves state-of-the-art accuracy on ImageNet (80.0%) with a 42% energy reduction for image classification and a 58% reduction for NLP tasks on the GLUE benchmark, maintaining only a 0.3% accuracy loss.

### Strengths and Weaknesses
Strengths:
1. The introduction of innovative techniques for implementing attention mechanisms in SNNs addresses challenges in softmax computation and matrix multiplication.
2. SpikedAttention achieves impressive accuracy and energy efficiency, outperforming existing SNN-based transformers.
3. The method allows for direct conversion of pre-trained transformers into SNNs without requiring additional training or architectural modifications.
4. The versatility of SpikedAttention is showcased through successful conversions of both vision and language models.

Weaknesses:
1. SpikedAttention requires a longer timestep than directly trained SNNs to maintain high accuracy, potentially impacting latency and efficiency.
2. The current implementation does not support functions like GeLU and LayerNorm, limiting its applicability to various models.
3. The absence of an ablation study raises questions about the necessity of trace-driven matrix multiplication and WOSS softmax.
4. Limited evaluation on diverse datasets, particularly in vision tasks, restricts the generalizability of the findings.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including additional datasets for vision tasks beyond ImageNet to enhance the robustness of their findings. Additionally, we suggest conducting an ablation study to clarify the contributions of trace-driven matrix multiplication and WOSS softmax. To address the limitations of longer timesteps, we encourage the authors to explore the impact of timestep selection on accuracy and energy efficiency, potentially providing a figure to illustrate this relationship. Lastly, a more detailed discussion on hardware implementation and optimizations would strengthen the practical applicability of SpikedAttention.