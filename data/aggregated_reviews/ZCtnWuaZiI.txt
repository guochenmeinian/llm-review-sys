ID: ZCtnWuaZiI
Title: Universal Functional Regression with Neural Operator Flows
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 8, 4
Original Confidences: 5, 3

Aggregated Review:
### Key Points
This paper presents an infinite-dimensional extension of normalizing flows, termed Neural Operator Flows, which map to Gaussian process base processes. The authors propose a framework for Bayesian functional regression utilizing priors learned through Neural Operator Flows, akin to posterior inference in Gaussian processes with tuned prior hyperparameters. The method relies on MAP estimation and SGLD for posterior sampling. The work aims to enhance flexible functional prior specification and functional inference in approximate Bayesian inference.

### Strengths and Weaknesses
Strengths:  
- The proposed method addresses important aspects of Bayesian functional regression and has the potential to improve existing approximate inference methods.

Weaknesses:  
- The paper lacks clarity and structure, making it challenging to follow the main contributions. The explanation of the method is insufficient, with only a simple and poorly described diagram. The originality is unclear, and the related work section does not adequately address existing literature on normalizing flows and GPs. The examples are limited, particularly the seismic example, which does not convincingly demonstrate the method's performance.

### Suggestions for Improvement
We recommend that the authors improve the clarity and structure of the paper, providing a more detailed explanation of the model and its components, particularly Equation 2. It would be beneficial to elaborate on the connections to existing literature, including Maro√±a et al. (2021) and related works on normalizing flows and GPs. Additionally, enhancing the readability of figures and including more diverse examples would strengthen the paper. Finally, addressing the questions regarding the impact of GP choice in the latent space, extrapolation uncertainty, and the performance of uncertainty intervals would provide valuable insights.