ID: h8goI8uPXM
Title: decoupleQ: Towards 2-bit Post-Training Uniform Quantization via decoupling Parameters into Integer and Floating Points
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 3, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents decoupleQ, a novel post-training quantization method that decouples model parameters into integer and floating-point parts, transforming quantization into a constrained optimization problem. DecoupleQ achieves significant improvements in large language models (LLMs), particularly at extreme low-bit quantization (2-bit), and demonstrates comparable accuracy to higher precision formats like fp16/bf16. The authors also released the W2A16 CUDA kernel.

### Strengths and Weaknesses
Strengths:
1. DecoupleQ eliminates the need for ad-hoc techniques to handle outliers, focusing on optimizing model accuracy under extreme low-bit quantization.
2. The method shows notable advancements over existing techniques in LLMs, particularly at 2-bit quantization.
3. The approach can be extended to supervised fine-tuning (SFT) to further enhance model accuracy.

Weaknesses:
1. The method appears to combine existing approaches, such as using Adaround and GPTQ for obtaining integer parts, and integrating PTQ and QAT.
2. The novelty is limited, as the core idea resembles learnable normalization techniques.
3. Further experiments on LLMs are necessary, including evaluations in multi-task settings and comparisons with more existing quantization methods like NWQ and PD-Quant.
4. Additional ablation studies are needed, particularly experiments without training norm layers.
5. The paper lacks clarity and cohesion, with several spelling mistakes and unclear distinctions between decoupleQ and traditional methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity and organization of the paper, particularly in Sections 1 and 3, to enhance readability. Additionally, we suggest conducting further experiments on lower activation bitwidths (<=8bit) and providing more comprehensive comparisons with existing quantization methods. More ablation studies, especially regarding the impact of training norm layers, should be included. Lastly, addressing the spelling errors and ensuring a clear distinction between decoupleQ and traditional quantization methods will strengthen the manuscript.