ID: c8HOQIMwKP
Title: UnSeg: One Universal Unlearnable Example Generator is Enough against All Image Segmentation
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework called Unlearnable Segmentation (UnSeg) designed to generate unlearnable examples (UEs) for image segmentation tasks, addressing key challenges such as data efficiency, generation efficiency, and transferability. The authors propose leveraging the Segment Anything Model (SAM) through bilevel optimization to create noise that renders images unusable for training segmentation models. The effectiveness of UnSeg is validated through extensive experiments across multiple segmentation tasks, datasets, and architectures.

### Strengths and Weaknesses
Strengths:
- The motivation and proposed method effectively address practical concerns regarding privacy in training large-scale segmentation models.
- The use of SAM for generating UEs is innovative and shows promise in achieving good results across various benchmarks.
- Comprehensive experiments and ablation studies are conducted, demonstrating the method's effectiveness.

Weaknesses:
- The clarity of the key experiment setup is lacking, particularly regarding the benefits of using SAM and the initialization of the surrogate model.
- The practical application of the unlearnable samples generated is questionable, especially given the disparity between the segmenter used for training and the one used for generating UEs.
- The paper does not adequately discuss the challenges of obtaining object masks for generating UEs and lacks a thorough comparison with more state-of-the-art methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental setup, particularly regarding the advantages of using SAM as the backbone for both the unlearnable example generation model and the surrogate model. Additionally, the authors should discuss the practicality of using bounding boxes or clicks as alternatives to object masks. It would be beneficial to provide more results from different networks and datasets using the clean-unlearnable mixed training dataset. Finally, including a broader comparison with state-of-the-art methods for segmentation and classification would strengthen the validation of the proposed method.