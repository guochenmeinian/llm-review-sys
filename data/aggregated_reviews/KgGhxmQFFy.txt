ID: KgGhxmQFFy
Title: UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 2, 4, 5, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Uni-MoT, a unified structure designed to align molecules with text using a VQ tokenizer and Q-Former from BLIP-2. By treating molecules as new word tokens, Uni-MoT aligns discrete token representations for both modalities while adhering to the autoregressive framework of large language models (LLMs). The training process consists of four stages: Causal Q-Former Pretraining, Molecule Tokenizer Pretraining, Unified Molecule-Text Pretraining, and Task-Specific Instruction Tuning. Experimental results indicate that Uni-MoT outperforms selected baselines.

### Strengths and Weaknesses
Strengths:
- The performance of Uni-MoT is generally good, surpassing baseline models.
- Uni-MoT offers an alternative approach for aligning molecules with text.

Weaknesses:
- The authors claim a novel structure, yet it closely resembles the widely used BLIP-2 framework, limiting the technical contribution and novelty of the work. The paper appears to merely swap inputs in the Q-Former.
- Experiments are conducted solely on Llama-2, which does not convincingly demonstrate the model's agnosticism; other LLMs like Mistral and Meditron could yield better results.
- The dataset selection is questionable, as results on ChEBI-20 are relegated to the appendix, and the main experiments are conducted on PubChem, despite ChEBI-20 having a larger data scale.
- Comparisons with baselines lack fairness, particularly since MolCA uses a smaller backbone model, and several state-of-the-art models like BioT5 and BioT5+ are not discussed.
- The ablation study is insufficient, lacking naive fine-tuning performance metrics for Llama-2 and not addressing the size of the codebook used in the VQ tokenizer.
- Data leakage during pre-training stages is a concern, given overlaps between pre-training and fine-tuning datasets.
- Some claims and definitions are unclear, such as referring to the model as an "LLM" when it is still based on Llama-2.

### Suggestions for Improvement
We recommend that the authors improve the novelty discussion by clearly differentiating Uni-MoT from existing models like MolCA and 3D-MoLM. Conducting experiments on additional backbone LLMs, such as Mistral and Meditron, would strengthen the claims of model agnosticism. Justifying the dataset selection and conducting more experiments on ChEBI-20 would enhance the robustness of the findings. The authors should ensure fair comparisons with stronger baselines and include a more comprehensive ablation study that provides naive fine-tuning performance metrics. Addressing potential data leakage concerns is crucial, and clarifying ambiguous claims and definitions would improve the paper's clarity.