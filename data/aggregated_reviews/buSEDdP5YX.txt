ID: buSEDdP5YX
Title: Avoiding Pitfalls for Privacy Accounting of Subsampled Mechanisms under Composition
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 6, 5, 3, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 2, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an examination of discrepancies between privacy accounting methods and their implementations, particularly focusing on the privacy guarantees of subsampled differential privacy (DP) mechanisms. The authors propose that the composition of subsampled mechanisms may not be defined by worst-case datasets and highlight significant differences in privacy guarantees between Poisson subsampling and sampling without replacement. They also address challenges in computing tight DP bounds under the substitution relation of neighboring datasets.

### Strengths and Weaknesses
Strengths:
- The paper addresses a timely and important topic in differential privacy, with strong practical implications for preventing unintended privacy breaches.
- The authors articulate their findings well, promoting better practices among DP practitioners while maintaining a respectful tone.

Weaknesses:
- The paper's messages can be convoluted, making it difficult to follow.
- It lacks viable technical solutions for the identified issues, which may present a challenging research problem.
- There is limited technical novelty, with many results echoing previous works.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their definitions, particularly in Definition 1, where the "only if" condition does not hold. In Definition 3, consider using "of P from Q" to address the asymmetry of hockey-stick divergence. Additionally, clarify assumptions about absolute continuity and the existence of densities in Definition 5, and revise the notation to use $\mathrm{d}$ for the differential operator. 

In Definition 6, ensure that the footnote specifies the **actual** batch size should remain confidential. On line 160, avoid referring to $D \sim D'$ as dominating pairs of datasets; instead, clarify that the distributions under D, D' are dominating pairs. Consistently use "databases" rather than mixing with "datasets." 

In line 244, provide a more detailed explanation for non-DP theorists about why the crossing vanishes when stating that "they converge to a Gaussian distribution." In Figure 2, clarify that mechanisms satisfying the same delta and epsilon are not necessarily identical or comparable without equal privacy curves. 

Address the limitations more thoroughly; the discussion should include an experimental section demonstrating the real-world implications of the pitfalls identified. Lastly, consider providing guidelines for practitioners on handling potential overestimations of privacy guarantees when using different sampling methods.