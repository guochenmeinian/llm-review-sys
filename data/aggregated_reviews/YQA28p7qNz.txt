ID: YQA28p7qNz
Title: 3D-LLM: Injecting the 3D World into Large Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 7, 6, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework called 3D-LLM, which utilizes large language models (LLMs) to comprehend 3D scenes. The authors propose a series of 3D-language data generation pipelines to create a dataset of 300K 3D-language pairs, enabling the model to perform various tasks such as 3D captioning, question answering, and navigation. The framework leverages 3D point clouds as inputs and incorporates techniques for extracting 3D features from 2D multi-view images, achieving state-of-the-art performance on benchmarks like ScanQA.

### Strengths and Weaknesses
Strengths:
1. The motivation for the research is clear and well-articulated.
2. The experimental results demonstrate substantial coverage across multiple 3D tasks, showcasing solid performance.
3. The organization and writing of the paper are fluent, making it easy to read.
4. The technical contributions, including data generation and experimental analysis, are convincing.

Weaknesses:
1. There may be potential data leakage due to the use of object semantic information and bounding boxes in generating 3D-language pairs, which could skew downstream task performance.
2. The reliance on 2D multi-view images for gathering 3D representations introduces complexity and potential information loss.
3. The ablation study lacks comprehensiveness, as it does not explore all combinations of position embeddings, location tokens, and localization.
4. The performance of 3D-LLMs in zero/few-shot scenarios on held-out datasets is not adequately presented.
5. The results on certain tasks, such as ScanRefer, are not satisfactory, and the baselines used may not be robust enough.

### Suggestions for Improvement
We recommend that the authors improve the comprehensiveness of the ablation study by starting with a baseline that removes all position embeddings, location tokens, and localization, and then showing all possible combinations. Additionally, we suggest that the authors explore the effectiveness of the three approaches used to generate 3D features by including experimental results. It would also be beneficial to discuss the limitations of the 3D-LLM framework more thoroughly. Furthermore, we encourage the authors to report performance metrics under pre-trained weights across all sources of pretraining data to better understand the effects of fine-tuning. Lastly, we recommend investigating the possibility of training the 3D encoder directly using the language-3D dataset without relying on images, as this could yield more promising results.