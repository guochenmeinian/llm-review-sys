ID: S9Qrrxpy6z
Title: CiteME: Can Language Models Accurately Cite Scientific Claims?
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the CiteMe benchmark, which evaluates citation contributions through 130 curated questions assessed on four dimensions: Attributable vs Unattributable, Unambiguous vs Ambiguous, Non-Trivial vs Trivial, and Reasonable vs Unreasonable. The authors propose CiteAgent, an autonomous system utilizing modern LLMs and tools like the Semantic Scholar API to perform evaluations on the CiteMe benchmark. The evaluation metrics are designed to assess the quality of citation excerpts, with a focus on addressing issues found in previous benchmarks. The authors employ 20 experts for human evaluation, primarily machine learning PhD students, and acknowledge the potential for future comparisons of the agent's performance against humans with varying expertise levels. The paper highlights a significant performance gap between LLMs and human citation accuracy, with human performance at 69.7% compared to LLMs achieving 18.5% and CiteAgent at 35.3%. Additionally, the authors provide a comparison table detailing the characteristics of various citation prediction benchmarks, showcasing CiteMe's larger query set and domain specificity.

### Strengths and Weaknesses
Strengths:
- The paper introduces a manually curated citation attribution benchmark, addressing limitations of existing automated benchmarks.
- CiteAgent is a well-structured system that demonstrates effective citation retrieval capabilities.
- The benchmark CiteMe is well-motivated and addresses critical gaps in existing citation attribution systems.
- The authors provide a clear discussion of related work and the necessity of their approach, highlighting the limitations of current models.
- The comprehensive error analysis enhances understanding of LLM performance in citation tasks.
- The inclusion of inter-rater agreement metrics strengthens the evaluation process.

Weaknesses:
- The motivation for the CiteMe task design could be better articulated, particularly regarding its implications for improving scientific assistants and linking paper discovery tools to broader scientific discovery applications.
- The human evaluation lacks clarity on the expertise of the 20 experts involved and does not include inter-rater agreement metrics.
- The evaluation relies heavily on human judgment, raising questions about the consistency and reliability of the curation process.
- Some aspects of the evaluation metrics require further clarification regarding their selection and relevance.

### Suggestions for Improvement
We recommend that the authors improve the motivation for the CiteMe task by clearly connecting better performance on this benchmark to enhanced capabilities in scientific assistance and explicitly linking paper discovery to scientific discovery. It may be beneficial to reference existing frameworks for scientific assistants to strengthen this argument. Additionally, clarify the expertise of the human evaluators and consider comparing the LM agent's performance against individuals with varying levels of expertise. Including inter-rater agreement metrics would also enhance the reliability of the human evaluations. Furthermore, we suggest that the authors clarify the rationale behind the chosen evaluation metrics in the revised draft, ensuring that readers understand their significance. Lastly, a deeper discussion on the choice of SPECTER models as baselines and their limitations, along with an exploration of the implications of using outdated models, would provide valuable context. Including more details about the benchmarks referenced, such as their size and domain, will enhance the clarity and comprehensiveness of the paper.