ID: HwWkIwzzKF
Title: Contextual Bandits with Knapsacks beyond Worst Cases via Re-Solving
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 4, 6, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on Contextual Bandits with Knapsacks (CBwK), proposing an algorithm that achieves logarithmic regret under specific conditions while maintaining optimal worst-case regret bounds. The authors identify sufficient conditions for achieving $o(\sqrt{T})$ regret, contrasting with previous works that primarily addressed $\sqrt{T}$ worst-case regrets. The algorithm is based on a sequential refinement of the underlying best static linear programming (LP) problem. Additionally, the authors model "partial feedback" in the context of CBwK, clarifying that their model allows for skipping rounds and stopping early, effectively making it a "full feedback" scenario when no null arm is present. The motivation for their approach is rooted in auto-bidding markets with budget constraints, where feedback can be revealed after each auction. The authors discuss the stability factor \(D\) as a gap-like parameter influencing their regret bounds, which are \(O(1)\) for full information and \(O(\log T)\) for partial information.

### Strengths and Weaknesses
Strengths:
- The identification of conditions for $o(\sqrt{T})$ regret is a significant contribution.
- The algorithm is adaptive, not requiring prior knowledge of whether conditions are satisfied.
- The exposition is generally clear, although more detail would enhance understanding.
- The approach is intuitive and computationally tractable.
- The authors provide a clear motivation for their model, linking it to practical applications in auto-bidding.
- The clarification of the feedback model and its implications for regret bounds is well-articulated.

Weaknesses:
- The bounds are presented in expectation rather than with high probability, which is less common.
- There is insufficient discussion on the stopping time of the algorithm, which is only briefly mentioned in the appendix.
- The algorithm's efficiency is questionable, as it requires solving a potentially large LP at each step.
- The assumption of a unique and non-degenerate solution is not well-defined and may not hold in practical scenarios.
- The terminology surrounding "partial feedback" is misleading and may confuse readers.
- The gap-like parameters are not sufficiently detailed, and their dependence on public parameters is unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the assumptions, particularly Assumption 3.1, by providing a formal definition and justifying its applicability to the bandit with knapsack problem. Additionally, a thorough comparison with existing literature on constrained reinforcement learning should be included to contextualize the contributions of this work. We suggest moving the full-information results to the appendix to clarify the main contributions. Furthermore, the authors should elaborate on the algorithm's running time and explore potential extensions to algorithms based on Online Convex Optimization (OCO). We also recommend improving the clarity of their terminology by renaming "partial feedback" to avoid confusion with "bandit feedback." The authors should explicitly detail the gap-like parameters and their dependence on public parameters, particularly in the absence of resources. It would be beneficial to explore how the stability factor \(D\) scales with the number of arms, contexts, and resources, and include this analysis in the final version. Finally, citing specific ad markets where feedback is revealed post-auction would strengthen the motivation section.