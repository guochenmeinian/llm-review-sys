ID: kda8szucLZ
Title: Continual Dialogue State Tracking via Example-Guided Question Answering
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to alleviate the catastrophic forgetting problem in continual learning for dialogue state tracking (DST) by reformulating it as an Example-Guided Question Answering (EGQA) task. The authors propose a dialogue-level sampling strategy for memory selection and demonstrate the effectiveness of their method through experiments on the SGD dataset, achieving state-of-the-art performance with a T5-small model. The study emphasizes the importance of in-context learning and memory techniques in enhancing model generalization across domains.

### Strengths and Weaknesses
Strengths:
- The reformulation of DST as EGQA is effective in improving continual learning performance and model generalization.
- Detailed experiments analyze various factors influencing performance, such as retriever types and in-context learning samples.
- The proposed dialogue-level sampling strategy is a novel contribution that enhances memory selection.

Weaknesses:
- The retrieval process for in-context examples lacks clarity, raising concerns about the necessity of storing all samples from previous domains, which implies the use of "infinite memory."
- The evaluation is limited to a single dataset (SGD), and additional datasets should be included to validate findings.
- The dialogue-level sampling strategy and its implications are not sufficiently explained, and the presentation of results, particularly in tables and figures, requires improvement.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the retrieval process for in-context examples and consider limiting the number of samples stored in the database to avoid the drawbacks of infinite memory. Additionally, we suggest that the authors evaluate their method on larger models, such as FlanT5/T5-large, and include more datasets like MultiWOZ to strengthen their claims. The authors should also provide a more detailed discussion of the dialogue-level sampling strategy and enhance the presentation of tables and figures for better readability.