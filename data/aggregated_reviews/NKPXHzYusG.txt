ID: NKPXHzYusG
Title: VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths Vision Computation
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 4, 4, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach called VideoLLM-MoD, which aims to efficiently scale up vision resolution for online video large language models (VideoLLMs) while minimizing computational costs. The authors propose a mixture-of-depths strategy that allows for dynamic allocation of computation to critical vision tokens, thereby reducing the number of tokens processed in each layer. Experiments demonstrate that this method achieves comparable or improved performance with significantly less computational effort across various datasets.

### Strengths and Weaknesses
Strengths:
1. The explored problem of handling long videos with large language models is meaningful and has wide applications.
2. The motivation for using different depths of networks to process visual tokens is clear and logical.
3. The proposed method effectively reduces computational costs while maintaining performance, enabling high vision resolution for VideoLLMs.

Weaknesses:
1. The presentation of Section 3.1 and Figure 6 closely resembles VideoLLM-online, raising concerns about originality. Additionally, the rationale behind the multiplication of indicator and probability terms in Eq.1 is unclear.
2. The quantitative performance improvement over VideoLLM-online is marginal, with full computation yielding only slight gains on some datasets. This raises questions about the sufficiency of preserving very few visual tokens.
3. Some visualizations, such as Figures 1 and 5, are redundant, and there is a lack of visualization on selected visual tokens across different layers, which is necessary to demonstrate the effectiveness of the LayerExpert.

### Suggestions for Improvement
We recommend that the authors improve the originality of their presentation by clearly distinguishing their contributions from existing works, particularly VideoLLM-online. Additionally, the authors should provide a more thorough explanation of the performance results, especially regarding the marginal improvements observed. Including visualizations of selected visual tokens across layers would enhance understanding of the LayerExpert's functionality. Finally, we suggest that the authors compare their method with other video LLMs that compress frames into fewer tokens, such as LLaMA-VID, to provide a more comprehensive evaluation of performance and efficiency.