ID: AWpWaub6nf
Title: Every Parameter Matters: Ensuring the Convergence of Federated Learning with Dynamic Heterogeneous Models Reduction
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 5, 5, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 2, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a general convergence analysis for a heterogeneous federated learning (FL) algorithm that trains a shared global model using time-varying and client-dependent local models. The authors establish sufficient conditions for the convergence of this heterogeneous FL algorithm to the neighborhood of a stationary point of the standard FL framework. They propose practical suggestions for designing heterogeneous FL algorithms and conduct thorough experiments to validate their claims. Additionally, the paper analyzes FedAvg with model pruning, showing that pruning techniques that evenly update parameters yield better convergence results, supported by numerical experiments on various datasets.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and provides a first convergence result for heterogeneous FL algorithms, offering valuable insights into the optimality gap related to the minimum coverage index and model reduction noise.  
- The theoretical proofs are sound, and the experimental results align well with the theoretical findings.  
- The introduction of the minimum covering index concept enhances the analysis of model reduction methods.

Weaknesses:  
- The last term in Theorem 1 (and similarly for Theorem 2) regarding the average norms of global parameters is not straightforwardly small, requiring further discussion.  
- The term $E\Vert \theta_q\Vert^2$ lacks clarity regarding its potential size, undermining the strength of the theoretical results.  
- Connections between the theorems and the general convergence result of FedAvg are not adequately discussed, weakening clarity.  
- Some parts of the paper appear rushed, with missing references and typographical errors that need correction.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the last term in Theorem 1 and Theorem 2 by discussing its tightness and conditions under which it can be considered small. Additionally, please provide a theoretical discussion or numerical justification for the term $E\Vert \theta_q\Vert^2$. It would be beneficial to clarify the connection between the theorem and the standard FedAvg convergence rate, particularly when $\delta = 0$ and $\Gamma_{min} = T$. We also suggest addressing the relationship between pruning and lossy compression more explicitly. Furthermore, please ensure all assumptions and lemmas are properly referenced, correct any typographical errors, and clarify the definitions of terms like $\Gamma_q^{(i)}$ and $\Gamma^*$. Lastly, consider expanding the empirical evaluation to provide more insights into the model reduction methods used.