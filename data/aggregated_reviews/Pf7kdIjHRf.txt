ID: Pf7kdIjHRf
Title: Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers
Conference: NeurIPS
Year: 2024
Number of Reviews: 24
Original Ratings: 6, 3, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Heterogeneous Pre-trained Transformers (HPT), a large-scale transformer model pretrained on diverse robotic data to address the challenges of heterogeneity across different robot embodiments and tasks. The architecture comprises a dataset-specific token encoder for images and proprioceptive information (Stem), a shared Trunk for processing latent tokens, and a dataset-specific action head. This design allows for improved processing of data from various robot embodiments and demonstrates scalability using the Open-X-Embodiment Dataset (OXE). The authors also propose a novel approach to tokenizing embodiments and aligning them in a shared latent space, utilizing a lightweight encoder-decoder architecture inspired by recent literature on mixture of experts and multimodal models. The proposed architecture is validated through pretraining on OXE and adapted for both simulation and real-world robot experiments. The authors report that HPT performs well in comparison to existing generalist policies on various tasks, although they faced challenges with fine-tuning in real-world experiments.

### Strengths and Weaknesses
Strengths:
- The method is simple yet effective, demonstrating good scaling properties for a multi-embodiment robot transformer.
- The extensive pretraining study encompasses 50 datasets and various robot embodiments, showcasing the most diverse pretraining in robotics to date.
- An insightful scaling analysis is provided, which is novel in the context of robotics.
- The paper introduces a well-motivated architecture that effectively addresses the challenges of heterogeneous input data.
- Additional ablation studies highlight the potential of the architecture for future analysis with discrete action spaces and fleet learning experiments.
- The integration of proprioceptive and visual information is well-explained, highlighting its significance for dexterous robotic tasks.
- The paper is well-structured and easy to follow.

Weaknesses:
- The absence of adequate baselines in real-world experiments undermines the comparison of the pretrained model against relevant alternatives, such as Octo, and models trained from scratch like Diffusion Policy or ACT.
- The experiments only involve a single robot embodiment, limiting the demonstration of the method's advantages across heterogeneous robot data.
- In simulation, comparisons against state-of-the-art models like Diffusion Policy or ACT are lacking, leaving unclear insights on the proposed model's improvements.
- There is a lack of comprehensive comparisons with baseline models, particularly in the context of the Simpler Tasks.
- The rationale behind the performance of the model without visual input remains unclear, raising questions about the evaluation setup.
- Concerns about the suitability of an $80$ layer transformer model for real robot applications are raised, particularly regarding inference time.
- Several aspects of the image encoder are inadequately described, missing critical details.
- The paper does not sufficiently address potential overfitting or causal confusion issues associated with proprioceptive input.

### Suggestions for Improvement
We recommend that the authors improve the experimental comparisons by including adequate baselines such as Octo for both simulation and real-world settings to better justify the performance of HPT. Additionally, we suggest providing more detailed descriptions of the image encoder, including whether the ResNet is pretrained on ImageNet, if fine-tuning has been attempted, and clarifying the choice of using ResNet over ViT despite its lower validation loss. Furthermore, the authors should address the inference time for the different models to assess their practicality in real-world applications. We also recommend improving the comparison with baseline models by including results from additional Simpler Tasks to provide a more comprehensive evaluation. Clarifying the evaluation setup, particularly regarding the differences between fine-tuning and testing, would enhance understanding of the model's performance without visual input. Additionally, elaborating on the effectiveness of proprioceptive input in light of potential overfitting or causal confusion would strengthen the discussion. Finally, releasing the training pipeline alongside the pre-trained models would significantly benefit the robotics community.