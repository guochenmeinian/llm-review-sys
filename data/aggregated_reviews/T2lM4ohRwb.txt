ID: T2lM4ohRwb
Title: Connecting Certified and Adversarial Training
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 5, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TAPS, an unsound certified training method that integrates certified training via Interval Bound Propagation (IBP) and adversarial training using Projected Gradient Descent (PGD). TAPS divides the neural network into a feature extractor and a classifier, employing IBP to propagate over-approximations through the feature extractor and PGD to generate adversarial examples for training. The method can be combined with the state-of-the-art SABR to enhance both natural and certified accuracy. Experimental results indicate that TAPS and STAPS (TAPS+SABR) achieve the highest certified accuracy on datasets like MNIST, CIFAR10, and TinyImageNet, with some exceptions.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a training method that surpasses existing state-of-the-art approaches.
2. Extensive ablation studies are conducted to support the findings.

Weaknesses:
1. The results in Table 1 show that STAPS only outperforms in two out of five settings, suggesting that SABR is equally important as TAPS. The justification in Section 3.5 requires more rigor, particularly regarding the claim about the exponential growth of BOX abstractions, which lacks experimental support. A layer-by-layer comparison in Figure 5 would be beneficial.
2. The empirical improvements in certified accuracy are marginal and sometimes negative, particularly when compared to SABR, raising concerns about the effectiveness of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the justification in Section 3.5 regarding the importance of SABR in STAPS, possibly by including a layer-by-layer comparison in Figure 5. Additionally, the authors should address the marginal improvements in certified accuracy and provide standard deviation metrics to clarify the significance of their results. It would also be beneficial to discuss how to efficiently select hyper-parameters, as this is a critical challenge for training methods like TAPS. Lastly, we suggest that the authors clarify the details of combining TAPS and SABR in Section 3.5 and consider including results for larger perturbation bounds to demonstrate scalability.