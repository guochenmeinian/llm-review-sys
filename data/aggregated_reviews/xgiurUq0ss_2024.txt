ID: xgiurUq0ss
Title: DDK: Distilling Domain Knowledge for Efficient Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 6, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DDK, a knowledge distillation (KD) framework that distills large language models (LLMs) into smaller LMs by dynamically adjusting domain weights during distillation. DDK enhances knowledge transfer by tailoring the distillation dataset based on domain performance differences, addressing the issue of excessive focus on domains with minimal performance gaps. Extensive evaluations demonstrate that DDK significantly outperforms existing KD methods and continuously pretrained baselines across various tasks.

### Strengths and Weaknesses
Strengths:  
1. The paper is well written, and the method is easy to follow.  
2. The proposed dynamic dataloader for KD is technically sound, and the experiments validate the efficacy of DDK across various benchmarks.  
3. A complete training algorithm is designed, and the process is straightforward to generalize.  

Weaknesses:  
1. The extra computation introduced by DDK should be considered, as it requires inference of a large LM during the training of the small LM, leading to potentially higher inference costs than training the student model.  
2. The dynamic dataloader relies on prior knowledge of the training data distribution and category, which may limit its applicability.  
3. The novelty of DDK is somewhat limited compared to existing methods like ShearedLLaMA, which also adjust domain proportions for training smaller models.  
4. The results for Qwen 1.5 are not convincingly robust, as discrepancies exist between reported and official results.  

### Suggestions for Improvement
We recommend that the authors improve the discussion on the differences between DDK and existing methods such as ShearedLLaMA, particularly regarding domain sampling techniques. Additionally, the authors should provide a more robust comparison with similar domain sampling methods and clarify the advantages of DDK over previous approaches. It would be beneficial to include experiments on the dataset and the scale property of the teacher model to further validate the method. Lastly, addressing the computational costs and ensuring that the performance metrics are consistent with official benchmarks will strengthen the paper's contributions.