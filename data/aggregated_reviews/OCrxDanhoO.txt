ID: OCrxDanhoO
Title: BIGOS V2 Benchmark for Polish ASR: Curated Datasets and Tools for Reproducible Evaluation
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 7, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive framework for curating speech datasets and evaluating Automatic Speech Recognition (ASR) systems, specifically for the Polish language. It introduces the BIGOS V2 dataset, which expands on the previous V1 dataset by including 12 datasets and evaluating 25 ASR system-model combinations, compared to V1â€™s 10 datasets and 7 systems. The work aims to equip developers and researchers with the necessary tools to build ASR models for Polish, addressing the challenges posed by the scarcity of high-quality datasets for less commonly spoken languages. The paper also reports results on a conversational dataset for the first time and includes extensive performance comparisons and insights into data-centric AI methods.

### Strengths and Weaknesses
Strengths:  
- The introduction of the BIGOS V2 dataset represents a significant advancement in Polish ASR development, being the largest unified speech dataset to date.  
- The paper is well-written and organized, providing a significant contribution to the field by offering a comprehensive benchmark for Polish ASR.  
- It adheres to advanced practices in dataset curation and provides tools for benchmarking, enhancing accessibility and reproducibility.  
- The thorough analysis of existing ASR systems contributes valuable insights into their performance and enhances the reproducibility of ASR evaluations.

Weaknesses:  
- The paper lacks significant academic insights or novel contributions, with claims not supported by diverse experiments or substantial evidence.  
- There is insufficient analysis of the datasets' characteristics, and the selection criteria for datasets are not clearly articulated.  
- The evaluation focuses primarily on quantitative metrics, neglecting a deeper analysis of error types and the quality of transcriptions.  
- The theoretical discussions regarding the impact of the work on the community and future contributions are insufficiently developed.

### Suggestions for Improvement
We recommend that the authors improve the paper by incorporating comprehensive experiments and providing more rigorous evidence to support their claims. It would be beneficial to include detailed probing experiments that explore the strengths and limitations of the datasets, as well as a qualitative error analysis to identify typical errors made by ASR systems for Polish. 

Additionally, we suggest expanding the description of the dataset selection criteria and providing a detailed analysis of the datasets' characteristics, including aspects such as speaker demographics and transcription quality. The authors should also consider integrating more theoretical discussions that link dataset characteristics with potential impacts on the ASR community and explore expanding evaluation metrics beyond WER to include proper noun recognition and transcription speed. 

Furthermore, we recommend that the authors improve the benchmarking section by expanding it with additional details, including definitions and context for metrics (SER, WER, etc.) in Section 2.4.2. Lastly, we advise the authors to clarify the terminology used in the paper, particularly regarding the "framework," to accurately reflect the contribution as a comprehensive ASR benchmark for Polish.