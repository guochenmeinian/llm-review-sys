ID: Z1wGHeHBrk
Title: VIPHY: Probing “Visible” Physical Commonsense Knowledge
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a large, machine-generated dataset called VIPHY, aimed at evaluating Vision-Language Models (VLMs) in understanding object color, size, and spatial relationships. The authors propose an automatic pipeline for dataset generation, leveraging existing datasets like Visual Genome and ADE20K. They report significant performance gaps between VLMs and human understanding, particularly in spatial reasoning tasks, and explore the performance of a language-only model, which outperforms VLMs in inferring size and spatial knowledge.

### Strengths and Weaknesses
Strengths:
1. The dataset has the potential to be a valuable resource for the community.
2. The innovative pipeline for constructing the visual physics commonsense dataset utilizes diverse tools effectively.
3. Empirical evidence highlights the performance gap between VLMs and humans, emphasizing the need for improved understanding of visible physics in NLP.
4. The inclusion of source code and data is commendable, with hopes for public availability post-publication.

Weaknesses:
1. The lack of recent models in the evaluation limits the comprehensiveness of the assessment, weakening claims about VLMs' struggles with physical knowledge.
2. The paper does not clarify how the larger, potentially noisier dataset provides unique insights compared to smaller, manually curated datasets.
3. Absence of human validation in the automated process raises concerns about potential errors and limited coverage of "visible commonsense" understanding.
4. The dataset may be prone to significant text biases, questioning its effectiveness in testing visible physics commonsense knowledge.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including recent models such as OFA and Unified-IO to enhance the robustness of their analysis. Additionally, the authors should clarify how the larger dataset adds unique value compared to existing benchmarks. It is crucial to incorporate human validation in the automated process to mitigate potential errors. We also suggest addressing the concerns regarding text biases and providing more qualitative examples that demonstrate when visual understanding is essential for the tasks. Lastly, we encourage the authors to address the identified typos and presentation issues to enhance clarity.