ID: fDiZJ7mmOV
Title: Non-Stationary Learning of Neural Networks with Automatic Soft Parameter Reset
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 4, 7, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for non-stationary learning in neural networks, proposing a parameter drift model based on the Ornstein-Uhlenbeck process. This model enables a form of “soft parameter reset” that adapts to the data stream, with an adaptive parameter $\gamma_t$ learned online. The authors illustrate the update rule for neural network parameters within both Bayesian and non-Bayesian frameworks, demonstrating the method's efficacy through numerical experiments on plasticity benchmarks and reinforcement learning tasks. Additionally, the authors clarify that the local optimum $\theta^{\ast}_t$ is not assumed to be unique, acknowledging the presence of multiple local optima in overparameterized settings. They provide a linearization derivation that aids in understanding the model's behavior and plan to include a full derivation in the appendix to address reviewer concerns.

### Strengths and Weaknesses
Strengths:
- The proposed method introduces a novel approach to online learning of parameter resetting, applicable to continual learning and reinforcement learning.
- The paper is well-presented, with a clear discussion of the problem's relevance and the methodology's novelty.
- The authors effectively clarify the concept of the drift model and its implications for parameter optimization.
- The inclusion of a linearization derivation enhances the technical rigor of the paper.

Weaknesses:
- The experimental validation lacks clarity, failing to convincingly demonstrate the method's benefits and leaving questions about the choice of baselines and benchmark problems.
- The reinforcement learning section is unclear, particularly regarding the performance of the proposed method in on-policy settings.
- The theoretical analysis is insufficient; there is no proof that the method outperforms traditional SGD in terms of efficiency or mitigating plasticity loss and catastrophic forgetting.
- The explanation of local optima may still appear vague, suggesting a deterministic view rather than acknowledging the inherent randomness in deep learning.
- The paper is not self-contained, with several equations lacking derivations and explanations, leading to difficulties in understanding.
- The manuscript may require significant rewriting to address all reviewer concerns adequately.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental validation, specifically addressing the choice of baselines and benchmark problems, and clearly stating which method they propose. Additionally, the authors should clarify the reinforcement learning performance, particularly in on-policy settings, and provide a more detailed explanation of the intended take-away from Figure 2. We also suggest including a theoretical analysis to support claims of efficiency and effectiveness compared to traditional methods. Furthermore, we recommend that the authors improve the clarity of the discussion surrounding local optima by explicitly acknowledging the non-uniqueness of $\theta^{\ast}_t$ and the randomness involved in data sampling. Including a more detailed derivation of the linearization in the revised manuscript would facilitate understanding. Finally, we believe that a complete rewrite of the manuscript should undergo another review process to ensure all concerns are thoroughly addressed.