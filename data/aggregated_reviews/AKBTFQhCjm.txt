ID: AKBTFQhCjm
Title: DEFT: Efficient Fine-tuning of Diffusion Models by Learning the Generalised $h$-transform
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for conditional generation using diffusion models, called DEFT, which combines a fixed, pre-trained unconditional model with a learned conditional correction term. The authors provide extensive experiments demonstrating DEFT's effectiveness across various domains, achieving state-of-the-art performance with faster inference times. The theoretical foundation is based on Doob's $h$-transform, which enhances understanding of diffusion models and facilitates future improvements. The authors emphasize the significance of their refined approach to the DEFT architecture, highlighting strengthened experimental results and addressing concerns regarding theoretical novelty and clarity in parameterization.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- The proposed method is simple, easy to understand, and potentially very useful for practical applications, as it requires only the inference of a pre-trained model without fine-tuning.
- The authors provide a solid theoretical motivation for their method and conduct comprehensive experiments, including an ablation study, which enhances the robustness of their findings.
- Detailed explanations of the DEFT architecture, particularly regarding the inductive bias and its implications for training efficiency, are included.
- Code is made available for reproducibility.
- The manuscript includes comparisons with both training-free and fine-tuned methods, clarifying the computational advantages of DEFT.

Weaknesses:
- DEFT requires additional training compared to some other approaches like DPS, which may limit its applicability.
- Some discussed image-to-image generative models demonstrate strong performance in conditional generation but require full training; a comparison of their performance and training budget with DEFT would enhance completeness.
- The presentation of certain claims is unclear, particularly regarding the unification of existing methods, the differences between $X_t$ and $H_t$, and the rationale behind the DEFT network parametrization.
- The paper does not adequately address the computational overhead associated with the fine-tuning process.
- Some reviewers found the initial comparisons with training-free methods to be unfair without satisfactory explanations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their presentation by addressing the following questions: 
1. How does Doob’s $h$-transform unify existing methods for diffusion-based inverse solvers?
2. What is the difference between $X_t$ and $H_t$?
3. Clarify the rationale behind the DEFT network parametrization in Section 3.3 and provide a derivation of equation (13) from Doob’s $h$-transform.
4. Explain how DEFT can assume no knowledge of the forward operator, as this seems contradictory to the requirement for computing $\ln p(y|X_t)$.
5. Include a detailed analysis of the computational overhead involved in the fine-tuning process.
6. Further elaborate on the rationale behind the DEFT network parameterization and its inductive bias to enhance understanding among readers.
7. Explicitly distinguish between trained and training-free methods to prevent any misinterpretations.