ID: DpuphOgJqh
Title: Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 6, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SUPER, a communication-based cooperative multi-agent reinforcement learning method that employs semi-decentralized training. The authors propose that agents share high-td-error experiences to enhance learning efficiency. The experimental evaluation indicates that SUPER outperforms several reasonable baselines, suggesting its potential impact on the multi-agent reinforcement learning (MARL) community. Additionally, the authors report new experiments on the Atari videogame Space Invaders, demonstrating a significant performance improvement using SUPER over DDQN, with SUPER achieving a clipped reward of 59.98 (+- 1.07) at 5M timesteps, compared to DDQN's 47.46 (+-5.23). The authors plan to include these results in the next revision, believing they enhance the experimental evaluation of their approach.

### Strengths and Weaknesses
Strengths:
- The proposed method is simple, elegant, and easy to reproduce, making it widely applicable in various cooperative RL environments.
- The paper is well-organized, clear, and presents convincing experimental results.
- The addition of Space Invaders experiments provides further validation of the SUPER approach.
- The reported performance improvements are statistically significant, strengthening the paper's claims.

Weaknesses:
- The evaluation domains are overly simplistic for an empirical study, lacking complexity found in environments like Starcraft or MeltingPot.
- The experimental comparison does not include communication-related algorithms, limiting the assessment of SUPER's effectiveness.
- The paper does not adequately discuss the implications of heterogeneous agents or the communication costs associated with SUPER.
- The authors seek feedback on whether their revisions adequately address previous concerns, indicating potential unresolved issues.

### Suggestions for Improvement
We recommend that the authors improve the complexity of the evaluation domains by including more challenging environments that require division of labor, such as MeltingPot or other complex simulations. Additionally, we suggest incorporating a line graph to illustrate the performance of algorithms over time rather than using arbitrary training points. A deeper discussion on the applicability of SUPER in various multi-agent scenarios, including the need for observation translation among heterogeneous agents, would enhance the paper. We also recommend that the authors improve clarity regarding how the new experiments address prior feedback and ensure that the integration of the new results into the paper is seamless and effectively highlights the advancements made. Finally, we encourage the authors to compare SUPER against other transfer learning methods that utilize similar amounts of transferred data to provide a more comprehensive evaluation.