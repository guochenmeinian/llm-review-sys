ID: KOxEqQzvOZ
Title: Debias NLU Datasets via Training-free Perturbations
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for training models that are robust to spurious correlations by conducting perturbations on biased examples. The authors evaluate their proposed method on MNLI and FEVER, demonstrating improvements in several out-of-domain (OOD) test sets. The method is characterized as a data-centric approach that can be combined with model-centric debiasing methods.

### Strengths and Weaknesses
Strengths:
- The paper is well written and easy to follow.
- The proposed debiasing method is benchmarked against multiple state-of-the-art strategies and shows competitive performance on both in-domain and OOD datasets.
- The framework is cost-effective and introduces a novel data-augmentation method to reduce spurious correlations.

Weaknesses:
- The novelty of the method is limited, feeling like a trivial extension of prior work (Wu et al., 2022).
- Performance improvements are marginal, and the method is considerably more complex than simpler prior methods (e.g., Mahabadi et al., 2020).
- There are no experiments on QQP/PAWS, and the analysis/ablations are not comprehensive.
- The focus on NLU is too broad given the datasets used; a more suitable title and text would be NLI or additional experiments on tasks like QA.

### Suggestions for Improvement
We recommend that the authors improve the novelty of the method by conducting experiments on QQP and PAWS to provide a broader evaluation. Additionally, we suggest enhancing the comprehensiveness of the analysis and ablations to better support the claims made. It would also be beneficial to quantitatively demonstrate the cost-effectiveness of the method. Finally, consider exploring the generalization of the method to enable automatic discovery of spurious correlations and features responsible for bias, as well as investigating the potential for further performance improvements through extended perturbation generation.