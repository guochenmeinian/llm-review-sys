ID: sLzD2rw9Ce
Title: DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving World Model
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 8, 6, 7, 7, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the DrivingDojo dataset, a large-scale video dataset aimed at enhancing world models for autonomous driving. The dataset includes comprehensive driving maneuvers, multi-agent interactions, and open-world knowledge, divided into three subsets: DrivingDojo-Action, DrivingDojo-Interplay, and DrivingDojo-Open. The authors propose an Action Instruction Following (AIF) benchmark to evaluate world models' capabilities in generating action-controlled future predictions. Benchmark results indicate significant opportunities for future development in driving world models.

### Strengths and Weaknesses
Strengths:
- The dataset is significant, providing a rich resource for training and evaluating world simulators, especially in complex scenarios.
- It includes diverse driving maneuvers and multi-agent interactions, curated from various cities and weather conditions, ensuring high realism.
- The paper is well-written, with clear documentation and insightful limitation analysis.

Weaknesses:
- The dataset lacks information such as maps, traffic lights, and trajectories of other agents, which may limit its application beyond video generation tasks.
- The generated video quality is subpar, with issues like blurriness in critical elements.
- The dataset is collected using a single front-view camera, which may restrict its utility compared to multi-camera setups.

### Suggestions for Improvement
We recommend that the authors improve the dataset by including additional information such as maps, traffic lights, and trajectories of other agents to enhance its applicability. It would be beneficial to demonstrate the dataset's impact on training a driving policy, such as an end-to-end model, to showcase its potential beyond video generation. We also suggest expanding the text descriptions in the Open-world Knowledge split for better scene understanding and clarifying its potential applications. Additionally, addressing the video quality issues and considering the inclusion of more sensor modalities could further enhance the dataset's value. Finally, we encourage the authors to provide error bars for their experiments and evaluate more methods against ground truth data to strengthen their findings.