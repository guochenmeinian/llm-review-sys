ID: Pu41DTYo2f
Title: An Examination of AI-Generated Text Detectors Across Multiple Domains and Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 6, 6, 7
Original Confidences: 4, 5, 4

Aggregated Review:
### Key Points
This paper evaluates the effectiveness of four open-source AI text detectors (RADAR, Wild, Fast-DetectGPT, and GPTID) across seven domains, analyzing both plain and adversarial prompting scenarios. The authors highlight the inadequacy of the AUROC metric for assessing detection accuracy at low false positive rates, advocating for the TPR@FPR metric as a more reliable measure.

### Strengths and Weaknesses
Strengths:
- The breadth of experimental results provides significant insights into the performance variability of detection methods across different text domains.
- The investigation of multiple adversarial prompts, while not yielding significantly different results, adds depth to the analysis.
- The identification of AUROC as a poor predictor of detection accuracy at low false positive rates is a valuable contribution, emphasizing the importance of TPR at low FPR for practical applications.

Weaknesses:
- The technical contribution is limited as the paper does not propose new methods for addressing the problem.
- The study lacks evaluation of commercial APIs for AI text detection, which would provide a comparative performance analysis against open-source detectors.
- Basic details regarding the evaluated text detection methods are missing from the main paper, with relevant information relegated to the appendix.
- Some explanations in Section 3 are unclear, particularly regarding the impact of dataset imbalance on TPR@FPR estimation and the calculation of TPR at varying FPR values without threshold control.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the dataset selection process to ensure representation of new, unseen data and prevent data leakage. Additionally, consider incorporating a broader range of attack methods in the red teaming experiments. It would be beneficial to discuss the potential impact of model size and training data size on detector performance. We suggest providing a rationale for the TPR@FPR thresholds used in evaluations, ideally based on previous research or empirical observations. A more thorough discussion of related work and background should be included in the revised version. Finally, we encourage the authors to address the limitations of the detectors themselves, including their reliance on specific features and susceptibility to adversarial attacks, and how these factors influence overall performance.