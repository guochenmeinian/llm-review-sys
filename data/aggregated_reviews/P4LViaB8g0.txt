ID: P4LViaB8g0
Title: Enabling On-Device Large Language Models with 3D-Stacked Memory
Conference: NeurIPS
Year: 2024
Number of Reviews: 1
Original Ratings: 7
Original Confidences: 3

Aggregated Review:
### Key Points
This paper presents an exploration of 3D-stacked memory (DRAM and SRAM) aimed at enabling efficient deployment of large language models (LLMs) on augmented reality (AR) and edge devices. The authors propose solutions to address the large die area requirement of SRAMs and the high power consumption associated with off-chip LPDDR-DRAM. Through experiments, the authors successfully demonstrate significantly lower memory power consumption for 3D-stacked memory setups, providing insights into power consumption across various target inference rates.

### Strengths and Weaknesses
Strengths:  
- The paper effectively showcases the power savings achieved with 3D-stacked memory configurations.  
- It provides a comprehensive analysis of power consumption relative to different compute requirements.

Weaknesses:  
- The analysis lacks performance metrics related to inference latency and throughput of LLMs when using 3D-SRAM and 3D-DRAM.  
- The scalability of the 3D-stacked memory solution for larger models, particularly those with billions of parameters, is not addressed.  
- There is insufficient discussion on practical challenges for real-world deployment of 3D-DRAM or hybrid memory configurations in AR or wearable devices.

### Suggestions for Improvement
We recommend that the authors improve the analysis by including performance metrics that detail how 3D-SRAM and 3D-DRAM configurations affect inference latency and throughput. Additionally, the authors should address the scalability of their 3D-stacked memory solution for larger models exceeding 200 million parameters. Finally, we suggest that the authors discuss foreseeable practical challenges in deploying 3D-DRAM or hybrid memory configurations in commercially available AR or wearable devices.