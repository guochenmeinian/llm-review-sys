ID: Bk2nbTDtm8
Title: CRAFT-MD: A Conversational Evaluation Framework for Comprehensive Assessment of Clinical LLMs
Conference: AAAI
Year: 2024
Number of Reviews: 4
Original Ratings: 8, 6, 7, 7
Original Confidences: 4, 3, 4, 5

Aggregated Review:
### Key Points
This paper presents a novel automated evaluation framework for assessing the conversational ability of large language models (LLMs) in diagnosing medical conditions, utilizing a patient AI agent to simulate doctor-patient interactions. The framework evaluates the diagnosis through both a grader LLM and a medical expert across four conversational setups using 140 skin disease vignettes. The authors claim this highlights LLMs' shortcomings in gathering patient histories, synthesizing information, and clinical reasoning without answer choices.

### Strengths and Weaknesses
Strengths:
- The development of robust automated evaluation of LLMs is crucial for safe technology adoption.
- The framework enhances scalability and allows for broader, quicker testing compared to human evaluations.

Weaknesses:
- No human baseline is provided, making it difficult to assess performance levels in Table 1. The extent of performance drop-off with increasing task complexity remains unclear.
- The evaluation dataset is limited to 140 skin disease cases, with potential test set leakage due to sourcing from the internet.
- The paper lacks clarity on the accuracy and bias of the grader AI agent and the patient AI agent, which are critical assumptions for the framework's validity.
- Details on the construction of the evaluation MCQs are missing, raising concerns about their realism and potential overlap with model training data.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the framework by explicitly stating the foundation models used in the grader AI agent and the outcomes of expert evaluations. Additionally, the authors should provide a detailed breakdown of the test set vignettes, including the proportion sourced from dermnet and bespoke cases. It would be beneficial to clarify how the MCQs were constructed and to consider using single best answer (SBA) questions for realism. To validate the framework, we suggest running the same evaluations with human agents and reporting those results. Finally, exploring the applicability of CRAFT-MD against Retrieval-Augmented Generation (RAG) models could enhance the paper's insights and generalizability.