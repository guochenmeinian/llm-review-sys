ID: dqdffX3BS5
Title: An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework named Mecoin for graph few-shot class-incremental learning, addressing catastrophic forgetting and overfitting. Mecoin incorporates two main components: the structured memory unit (SMU) for efficient prototype storage and updating, and the memory representation adaptive module (MRaM) for separating the learning of prototypes from class representations. The framework effectively maintains representative prototypes and minimizes extensive finetuning, thus alleviating the issues associated with few-shot settings and class-incremental learning.

### Strengths and Weaknesses
Strengths:  
1. Efficiently stores past knowledge with low storage requirements using the SMU.  
2. Prevents catastrophic forgetting by separating the learning of prototypes and class representations.  
3. Experimental results demonstrate significant performance improvements over state-of-the-art methods.  

Weaknesses:  
1. The abbreviations for the proposed method are inconsistent, with MRaM and MRAM used interchangeably, and the model name Mecoin being too similar to the Memory Construction module (MeCo).  
2. Figure 1 lacks clarity regarding the details and advantages of the SMU and MRaM structures.  
3. Equation 6 in Section 3.1 lacks explanation on how it integrates edge information into the loss function.  
4. The proof of Theorem 1 is overly brief; a detailed derivation should be provided instead of citing a lemma from another paper.  
5. Equations 8 and 9 in Section 3.2 are unclear, particularly regarding how new knowledge is updated.  
6. The pretraining process of the GNN in Section 4.1 needs clarification on whether it is a standard graph node classification task or a few-shot node classification task. Additionally, there is a redundancy in mentioning MAG-Meta in Table 3, and the inclusion of Mecoin among comparison methods in Tables 2, 3, and 4 is confusing.  
7. The toy dataset in Section 4.2 is too simplistic; sampling at least 10 classes is recommended to better demonstrate the model's capabilities in handling overlapping problems.  

### Suggestions for Improvement
We recommend that the authors improve the consistency of abbreviations used throughout the paper, particularly for MRaM and MRAM, and clarify the naming of Mecoin and its submodules. We suggest enhancing Figure 1 to better illustrate the SMU and MRaM structures. Additionally, we advise providing a detailed explanation of Equation 6 and a more comprehensive proof for Theorem 1. Clarifying the incomprehensible aspects of Equations 8 and 9 is essential, as is elaborating on the pretraining process in Section 4.1. We also encourage the authors to sample more than four classes in the toy dataset to adequately showcase the model's performance in overlapping scenarios.