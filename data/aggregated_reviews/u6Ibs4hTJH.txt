ID: u6Ibs4hTJH
Title: Real-World Image Variation by Aligning Diffusion Inversion Chain
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 7, 5, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework called RIVAL (Real-world Image Variation by Alignment) designed to generate variations of real images without requiring tuning. The method incorporates two main components: cross-image self-attention injection, which inverts the real image using DDIM and samples a random chain by mixing values from the real inverted chain and denoised values, and a latent chain alignment process to address out-of-distribution issues. Experiments demonstrate improvements in text alignment and user preference over baseline methods. The authors assert that RIVAL enhances performance in various text-to-image applications, including example-based inpainting.

### Strengths and Weaknesses
Strengths:
- The method does not require training or finetuning, making it practical despite increased inference time.
- RIVAL shows superior identity preservation and style matching compared to other baselines, with impressive editing results on text conditioning.
- Detailed ablation studies clarify the impact of alignment steps and framework components.

Weaknesses:
- The authors do not analyze the editability of the proposed method, which is a significant feature.
- There is no ablation study on the sensitivity to CFG guidance weight.
- The presentation of the cross-image self-attention injection and parameter training is unclear, and the shuffle operation description lacks clarity.
- The necessity and interpretability of the proposed method are not fully discussed, and the writing in Section 3 is difficult to understand.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation, particularly in Section 3, by using more diagrams to explain the methods. Additionally, we suggest providing a thorough analysis of the editability of the proposed method and conducting an ablation study on the CFG guidance weight. Clarifying the training of parameters in the cross-image self-attention injection and providing a more detailed algorithmic description of the generation process would enhance understanding. Finally, we encourage the authors to explore and validate the applicability of RIVAL to other diffusion-based generation tasks.