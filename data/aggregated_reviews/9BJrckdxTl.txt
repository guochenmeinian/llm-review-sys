ID: 9BJrckdxTl
Title: Gaussian Splatting Under Attack: Investigating Adversarial Noise in 3D Objects
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 6, 9, 7, 6
Original Confidences: 3, 3, 4, 3

Aggregated Review:
### Key Points
This paper presents a novel approach by investigating adversarial attacks in the context of 3D Gaussian Splatting, an area that has received less attention compared to traditional 2D adversarial attacks. The authors propose a comprehensive experimental framework that reveals significant drops in model accuracy and confidence levels due to adversarial perturbations, highlighting critical vulnerabilities in 3D vision models. The work is well-structured, with clear results and a concise presentation, contributing meaningfully to the field of adversarial machine learning.

### Strengths and Weaknesses
Strengths:  
- The paper introduces a fresh perspective on adversarial attacks in 3D models, expanding the understanding of vulnerabilities in this domain.  
- Comprehensive experiments yield intriguing results, suggesting potential avenues for future research.  
- The writing is clear and easy to understand, with a novel application of a simple formula for adversarial attacks.  

Weaknesses:  
- Certain aspects, such as the specific model ùëì in "Algorithm 1: Masked Iterative FGSM Adversarial Attack," lack clarity.  
- The description of Figure 1 is insufficient, making it difficult to grasp the detailed process of the proposed method.  
- There is a lack of experiments demonstrating the efficacy of the untargeted attack, and comparisons between targeted and untargeted methods are missing.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the model ùëì in the algorithm description and explicitly state that it refers to the CLIP model. Additionally, clarify the operation of Clip_{min,max} regarding pixel intensity clipping. To enhance the work, consider covering a more diverse range of objects and datasets, including outdoor structures. We also suggest exploring the transferability of adversarial attacks to determine if the attacked 3D object misleads other pretrained object classification models. Finally, provide a more detailed description of Figure 1 and include direct comparisons with related work to strengthen the analysis.