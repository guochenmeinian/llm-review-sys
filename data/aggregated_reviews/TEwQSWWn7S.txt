ID: TEwQSWWn7S
Title: Fast, accurate training and sampling of Restricted Boltzmann Machines
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a pre-training method for Restricted Boltzmann Machines (RBMs) that integrates principal directions of the dataset into a low-rank RBM through convex optimization. The authors propose that this method, combined with Persistent Contrastive Divergence (PCD) training, mitigates issues related to second-order phase transitions. The effectiveness of the pre-training method is demonstrated on the MNIST 01 dataset, a synthesized “Mickey” dataset, and the Human Genome dataset (HGD), showing improvements over PCD and Jarzynski reweighting methods. Additionally, a new sampling technique, Parallel Trajectory Tempering (PTT), is introduced and compared to Annealed Importance Sampling (AIS). The paper also analyzes the learning dynamics of RBMs trained with maximum likelihood, emphasizing the relationship between the singular vectors of the weight matrix and the principal component analysis (PCA) of datasets. The authors argue that while PCA alignment occurs initially, long-term training may lead to decorrelation from PCA, particularly in complex datasets like images. They acknowledge limitations in their method, especially for multimodal datasets like CIFAR, and commit to including comprehensive MNIST experiments in the final version.

### Strengths and Weaknesses
Strengths:
- The proposed pre-training method enhances RBM training compared to standard PCD and Jarzynski methods.
- The novel PTT method shows improved sampling efficiency over standard Gibbs sampling.
- The paper provides a thorough exploration of the dynamics of RBMs and their learning processes.
- The authors demonstrate a clear understanding of the limitations of their approach and commit to addressing them in future work.

Weaknesses:
- The practical utility of RBMs for generative modeling remains unclear.
- PTT appears to require more memory than traditional RBM methods.
- The novelty of the pre-training method is questionable, given prior work by Decelle and Furtlehner.
- The paper lacks comprehensive comparisons against baseline methods, particularly regarding log-likelihood values and normalization constants.
- The rebuttal does not adequately address concerns regarding the performance of pre-training compared to persistent contrastive divergence (PCD).
- The analysis of phase transitions in binary RBMs lacks clarity, particularly in differentiating between first-order and second-order transitions.
- Language and terminology may not be accessible to the broader machine learning community, potentially limiting impact.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the primary motivation for studying RBMs and their potential advantages over state-of-the-art generative models, including specific scenarios and metrics for comparison. Additionally, we suggest providing a clearer distinction between the current work and the referenced study by Decelle and Furtlehner, particularly regarding contributions. The authors should enhance empirical evaluations by including comparisons with Parallel Tempering (PT) and ensuring that log-likelihood claims are supported by sufficient data. We also recommend that the authors improve their rebuttal by clearly addressing the performance comparison between pre-training and PCD, providing empirical evidence from multiple runs to support claims of better log-likelihood (LL). Furthermore, we encourage the authors to clarify the analysis of phase transitions in binary RBMs, particularly regarding the implications of continuous versus discrete topologies. Lastly, we advise revising the language to make it more accessible to the machine learning community, replacing jargon from statistical physics with more universally understood terms, and reducing the learning rate for practical applications, as a high learning rate exacerbates thermalization issues during PCD training.