ID: UtbOzHZFAv
Title: Self-Paced Pairwise Representation Learning for Semi-Supervised Text Classification
Conference: ACM
Year: 2023
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to semi-supervised text classification (SSTC) that addresses overfitting and data annotation costs through a Self-Paced PairWise (SPPW) representation learning model. The authors propose a prototype-based classifier that utilizes affinity learning to improve label inference and filtering of pseudo-labeled data. The effectiveness of the SPPW model is demonstrated through superior performance on multiple benchmark datasets, although the framework remains somewhat standard.

### Strengths and Weaknesses
Strengths:
- The prototype-based semi-supervised approach effectively mitigates overfitting issues associated with parametric classifiers in scenarios with limited labeled data.
- The design of the parameter-free classifier is simple and well-motivated, grounded in representation similarity and affinity learning without significant overhead.
- The motivation for prototype-based label inference and the self-paced training procedure is clear, although sections 4.1 and 4.2 could benefit from improved clarity.

Weaknesses:
- The selection of baselines for comparison is limited, with potential alternatives like ALBERT, DistilBERT, and RoBERTa not being considered.
- The rationale for the "overfitted classifier" claim is not convincingly supported, as the final classifier in BERT-based models typically has a limited number of parameters.
- The novelty of the "Self-Paced" concept appears weak, resembling a new data filtering rule rather than a substantial innovation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of sections 4.1 and 4.2 to better connect the training procedure and loss functions. Additionally, we suggest expanding the spectrum of baseline models to include variants like ALBERT and DistilBERT for a more comprehensive comparison. It would also be beneficial to provide a stronger justification for the "overfitted classifier" claim and to clarify the novelty of the "Self-Paced" approach. Finally, consider testing the model on larger backbone models, such as BERT-large or T5 XXL, to further validate performance.