ID: FJlrSZBMCD
Title: Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 5, 6, 6, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MOHAWK, a three-stage distillation method designed to transfer knowledge from pretrained transformer models to subquadratic models like Mamba-2. The core concept involves treating both transformers and SSMs as utilizing different forms of mixing matrices over token sequences. Experimental results indicate that Phi-Mamba, a variant of Mamba-2 tailored for distillation, achieves strong performance on downstream tasks using less than 1% of the training data compared to training from scratch.

### Strengths and Weaknesses
Strengths:
1. The paper introduces an effective method for distilling knowledge from pretrained transformer models to subquadratic models, allowing the linear attention community to develop better models at a lower cost.
2. Mamba-2 effectively aligns with self-attention by transforming the forget gate of GLA into a data-dependent value, achieving linear attention-like structures through the removal of softmax and the addition of a forget gate.
3. The well-designed three-stage distillation process is supported by ablation studies that highlight the significance of each stage, offering valuable insights for future research.

Weaknesses:
1. The paper lacks a detailed comparison between the proposed distillation method and direct fine-tuning approaches like SUPRA. While the three-stage distillation shows better performance with only 2.8B tokens, the authors should discuss the trade-offs, including the computational overhead of using both a teacher and a student model during distillation.
2. Numerous typos are present throughout the manuscript, such as "Mamba-2 2" in line 159, which should reference "Figure" before the second "2". Careful proofreading is necessary.
3. There is an inconsistency regarding the reported model size of Phi-1.5, which is stated as 1.5B in the paper but is officially documented as 1.3B. The authors should clarify this discrepancy.
4. The performance of the Phi-Mamba model on the MMLU benchmark is not reported, which is essential for assessing its performance across a broader range of tasks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of terms such as "mixing matrices," which is not formally defined, and provide a clearer description of the "Matrix Orientation" and "Hidden-State Alignment" steps, including definitions for terms like TeacherMixer and StudentMixer. Additionally, we urge the authors to include code or trained model artifacts to facilitate reproducibility, as the current presentation does not allow for straightforward replication of experiments. Lastly, we suggest that the authors address the limitations regarding the scaling properties of MOHAWK, its performance with larger models, and the impact of the training data used.