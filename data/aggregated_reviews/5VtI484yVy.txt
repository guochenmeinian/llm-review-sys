ID: 5VtI484yVy
Title: A Neuro-Symbolic Benchmark Suite for Concept Quality and Reasoning Shortcuts
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents **rsbench**, a benchmark suite designed to evaluate reasoning shortcuts (RSs) in neural and neuro-symbolic models. It includes eight tasks, with three newly developed by the authors: MNMath, MNLogic, and SDD-OIA. The benchmark provides existing datasets or generators for data creation, alongside three evaluation metrics and a novel algorithm, countrss, for counting RSs. The tasks span arithmetic, logic, and high-stakes scenarios, such as autonomous driving, and are applied to five models (DPL, LTN, CBM, NN, CLIP).

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and accessible, providing sufficient background for readers unfamiliar with reasoning shortcuts.  
- **rsbench** offers a diverse range of tasks and allows for customizable datasets, enhancing its applicability across different scenarios.  
- The introduction of new metrics and verification methods improves the assessment of model reliability.  
- The focus on high-stakes tasks increases the relevance of the research to real-world applications.  

Weaknesses:  
- The benchmark was only applied to three tasks, with only one being new, raising questions about the evaluation of the other introduced tasks.  
- The complexity of the documentation and experimental setup may overwhelm new users, making it less user-friendly.  
- The utilization of TCAV for visualizing concepts in black-box models is insufficiently explained, limiting its clarity and potential impact.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by applying the benchmark to all eight tasks to validate their contributions fully. Additionally, simplifying the explanations in Section 3 and providing concrete examples of instance structures in **rsbench** would enhance clarity for readers. The authors should also clarify how task properties are determined and elaborate on the experimental setup, particularly regarding how models trained on datasets like MNLogic utilize concepts to identify RSs. Finally, developing interactive dashboards and visualization tools to interpret RS evaluation results would significantly aid researchers in understanding and refining their models.