ID: CaAJeNkceP
Title: Dispelling the Mirage of Progress in Offline MARL through Standardised Baselines and Evaluation
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 5, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a critical examination of offline multi-agent reinforcement learning (MARL), identifying inconsistencies in baselines and evaluation protocols as major barriers to progress. The authors conduct a comparative study demonstrating that well-implemented simple baselines often outperform claimed state-of-the-art (SOTA) algorithms across various tasks. They propose a standardized evaluation methodology and provide robust baseline implementations to enhance future research.

### Strengths and Weaknesses
Strengths:  
1. The paper reveals that many purported SOTA algorithms are misleading, as simple baselines frequently outperform them, highlighting critical shortcomings in offline MARL.  
2. The authors benchmark simple baselines against SOTA algorithms, convincingly showing superior performance in most cases.  
3. A straightforward and standardized evaluation protocol is proposed, which could improve the fairness of empirical comparisons in offline MARL.  
4. The paper addresses consistency in baseline implementation, evaluation environments, and training steps/seeds, providing strong evidence of inconsistencies across current literature.

Weaknesses:  
1. The criteria for selecting standard baselines and environments appear naive, relying on the authors' judgment without statistical backing.  
2. The paper uses performance figures from previous SOTA papers, but varying evaluation protocols may lead to biased comparisons; replicating results with original or reproduced codes would yield more reliable outcomes.  
3. The authors do not explicitly highlight the limitations of their work.

### Suggestions for Improvement
1. We recommend that the authors provide statistical evidence to support the selection of standard baselines and environments, rather than relying solely on subjective judgment.  
2. We suggest that the authors replicate results using original or reproduced codes to ensure accurate comparisons, addressing potential biases from varying evaluation protocols.  
3. We encourage the authors to clarify their definitions of "simple" and "complex" baselines to resolve ambiguities.  
4. We recommend that the authors provide thorough hyperparameter details for all methods discussed, as this can significantly affect performance.  
5. We suggest that the authors clarify their reasoning for selecting specific algorithms and environments, such as QMIX, MADDPG, MPE, SMAC, and MAMuJoCo, to enhance transparency in their methodology.  
6. We recommend that the authors elaborate on the term "inconsistent use of baselines" to improve clarity and understanding.