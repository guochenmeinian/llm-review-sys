ID: 57yfvVESPE
Title: Tunable Soft Prompts are Messengers in Federated Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel federated learning (FL) approach named FedSP, which utilizes tunable soft prompts to facilitate knowledge exchange among participants while maintaining model privacy. The authors employ knowledge distillation initialization, cross-layer parameter sharing, alternative training, and parameter-efficient fine-tuning. Through experiments on two datasets and two large language models (LLMs), the authors demonstrate a method for training LLMs in FL with protected model privacy.

### Strengths and Weaknesses
Strengths:
- The topic of model privacy in FL is timely and significant.
- The paper is well-written, with clear presentation of methods and comprehensive evaluation.
- The experiments validate the effectiveness of FedSP across multiple datasets.

Weaknesses:
- The writing is difficult to follow, and the main contributions are not clearly articulated.
- The performance of FedSP is not the best in some datasets, as shown in Tables 1 and 2.
- The settings for the global and client models lack clarity, including model structures and configurations.
- Limited discussion on the safety of soft prompts and their potential to leak information.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing and explicitly outline the contributions at the end of the Introduction. Additionally, if the focus is on FedSP, the authors should conduct further experiments on various FL settings, such as the number of clients and communication rounds. Clarifying the model structures for both global and client models is essential. We also suggest addressing the concerns regarding the auxiliary model's construction and its potential information leakage. Finally, incorporating relevant missed related work and comparing with additional baselines would strengthen the paper.