ID: SFTvQQA4KJ
Title: FLatS: Principled Out-of-Distribution Detection with Feature-Based Likelihood Ratio Score
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FLATS, a novel method for out-of-distribution (OOD) detection based on likelihood ratios, enhancing existing methods by incorporating out-distribution density estimation. The authors argue that current state-of-the-art (SOTA) methods are suboptimal as they focus solely on in-distribution density. FLATS demonstrates strong performance on various benchmarks, establishing a new SOTA in OOD detection.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and organized, providing both theoretical and empirical contributions.
- FLATS is backed by rigorous theoretical analysis and establishes itself as a new SOTA, outperforming benchmarks like KNN and Maha.
- The versatility of FLATS is highlighted through its potential to enhance other OOD detection methodologies.

Weaknesses:
- The relationship between FLATS and existing works, particularly "Likelihood Ratios for Out-of-Distribution Detection," requires further discussion.
- The reliance on RoBERTa as the sole backbone for experiments limits the generalizability of results; a broader comparison with other language models is necessary.
- The experimental validation is perceived as weak, and a more detailed analysis of FLATS compared to a wider range of methods is needed.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the relationship between FLATS and "Likelihood Ratios for Out-of-Distribution Detection." Additionally, the authors should provide a comprehensive comparison with other language models such as T5-3b, 11b, llama-3b, and 7b to justify their choice of RoBERTa. Furthermore, we suggest including a short explanation in Section 5 to clarify the superiority of FLATS over existing methods and conducting experiments with various backbones beyond NLP classification tasks to strengthen the empirical validation.