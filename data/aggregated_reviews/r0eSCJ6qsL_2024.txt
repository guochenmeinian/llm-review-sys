ID: r0eSCJ6qsL
Title: AsCAN: Asymmetric Convolution-Attention Networks for Efficient Recognition and Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 26
Original Ratings: 7, 8, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Asymmetric Convolution-Attention Networks (AsCAN), a hybrid architecture that combines convolutional and transformer blocks to achieve favorable performance-throughput trade-offs for tasks such as image classification and text-to-image generation. The authors propose an asymmetric distribution of these blocks, utilizing more convolutional layers in the early stages and transformer layers in the later stages. They argue that traditional metrics like MACs and FLOPs do not adequately capture throughput gains due to inefficiencies from operations such as reshape and permute, as well as the absence of efficient CUDA operators for specialized building blocks. The empirical analysis demonstrates that AsCAN achieves better latency on GPUs while maintaining competitive accuracy compared to state-of-the-art models. Additionally, insights from this architecture are applied to a diffusion model for image generation, yielding improved performance metrics.

### Strengths and Weaknesses
Strengths:
1. The paper is well-motivated, analyzing the benefits of a simple neural network architecture across various tasks on current hardware, providing insights of broad interest.
2. Extensive empirical analysis supports the design choices, with detailed experimental setups aiding reproducibility.
3. The authors effectively address reviewer concerns with detailed explanations and additional experiments.
4. The novelty of the asymmetric architecture is well-articulated, highlighting its potential advantages over symmetric designs.
5. The paper includes a thorough discussion on the limitations of MACs and FLOPs as performance indicators.

Weaknesses:
1. The contribution of the training pipeline versus architecture in enhancing image generation performance is unclear; a more detailed discussion in Section 3.2.2 would enhance clarity.
2. The originality of the approach is limited, as AsCAN is built from existing components with known benefits.
3. The rationale behind certain architectural choices, such as the fixed order of convolutional and transformer blocks, lacks sufficient explanation.
4. The rationale for the architectural choice remains unclear, particularly regarding the effectiveness of the asymmetric distribution beyond specific tasks like ImageNet.
5. The experimental design in Table 2 lacks control over the number of parameters, which could skew the perceived advantages of the proposed architecture.

### Suggestions for Improvement
We recommend that the authors improve clarity by providing a more detailed discussion on the impact of the training pipeline versus architecture on performance, particularly in Section 3.2.2. Additionally, we suggest that the authors clarify the rationale for the "C before T" design constraint and provide more evidence of the transferability of the asymmetric distribution to other tasks or domains. It would also be beneficial to conduct more controlled experiments to better highlight the trade-offs in performance and throughput, particularly by constraining the number of parameters across architectures. Finally, we encourage the authors to expand their discussion on the architectural choices, particularly regarding the merging of stages S1 and S0, and to address minor typographical errors and improve terminology for clarity to enhance the overall presentation of the paper.