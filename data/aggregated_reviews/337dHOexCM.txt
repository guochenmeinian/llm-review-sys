ID: 337dHOexCM
Title: Retrieval & Fine-Tuning for In-Context Tabular Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 7, 6, 2, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Locally-Calibrated PFN (LoCalPFN), an enhancement of the TabPFN model for tabular data that integrates retrieval and fine-tuning techniques. The authors propose using k-Nearest Neighbours (kNN) to select local contexts for each test point, followed by fine-tuning the model on these contexts. Extensive evaluations on 95 datasets demonstrate that LoCalPFN achieves state-of-the-art performance compared to both neural networks and tree-based methods. The contributions include addressing TabPFN's scaling issues and showcasing improved context utilization through rigorous experimentation and ablation studies.

### Strengths and Weaknesses
Strengths:
- The novel combination of retrieval and fine-tuning enhances transformer-based in-context learning for tabular data.
- Comprehensive evaluations across 95 datasets provide robust evidence of LoCalPFN's effectiveness against strong baselines.
- The paper is well-organized and clearly written, making complex concepts accessible.

Weaknesses:
- The fine-tuning process increases computational complexity and runtime, particularly with larger datasets.
- Reliance on TabPFN as the base model limits the generalizability of the proposed method to other in-context learning models.
- Several details are missing, such as runtime computation methods, hyperparameter optimization clarity, and the selection criteria for the datasets used.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Section 2.4 regarding the fine-tuning steps. Additionally, it would be beneficial to report both training and inference times separately, as inference speed is a critical factor for practical applications. We suggest including more aggregation metrics, such as mean rank and mean normalized score, to enhance the evaluation. Furthermore, the authors should clarify whether LocalPFN undergoes hyperparameter optimization and provide the hyperparameter space if applicable. Lastly, addressing the limitations regarding the model's cost and latency at inference time would strengthen the paper's applicability in real-world scenarios.