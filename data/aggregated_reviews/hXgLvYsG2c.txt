ID: hXgLvYsG2c
Title: CoMERA: Computing- and Memory-Efficient Training via Rank-Adaptive Tensor Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 28
Original Ratings: 5, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CoMERA, a novel training method for compressing large AI models, specifically large language models (LLMs), during training. CoMERA optimizes computing and memory efficiency through rank-adaptive tensor compression, aiming to reduce training costs and environmental impact while maintaining accuracy. The authors highlight that compressed models may exhibit larger training losses but can achieve better testing accuracy due to smaller generalization gaps. Empirical validation indicates significant speedup, with claims of a $2.5\times$ speedup and a $4.25$ overall compression ratio, alongside memory savings compared to existing methods like GaLore and LTE. The authors provide evidence showing that their tensorized BERT model outperforms standard BERT on two out of three downstream tasks while achieving a significant reduction in GPU hours.

### Strengths and Weaknesses
Strengths:  
1. Experimental results show significant improvements in training speed and memory efficiency, outperforming recent methods.  
2. The method addresses both computing and environmental costs, enhancing the practical utility of tensor models in machine learning.  
3. The multi-objective optimization formulation balances compression ratio and model accuracy, allowing customization for varying resource requirements.  
4. The authors effectively address the complexity of compressing models during training, emphasizing its significance in the LLM domain.  
5. The paper provides a clear comparison with existing methods, highlighting the advantages of CoMERA.

Weaknesses:  
1. Scalability to larger models and diverse architectures remains unexplored.  
2. The clarity of certain sections, particularly regarding the design of the “tensor-index level” and contraction path optimization, needs improvement.  
3. Comparisons with baseline times and additional experiments, especially with transformers, are limited.  
4. Concerns remain regarding the accuracy of CoMERA on more challenging tasks, particularly SQuAD, where it underperformed compared to standard BERT.  
5. The authors' claims about speedup and compression ratios may not hold for larger models, raising questions about the generalizability of their results.  
6. The lack of large-scale experiments limits the robustness of the findings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the “tensor-index level” design in lines 171-173. Additionally, please provide a detailed explanation of the rank adaptation procedure, including how ranks are chosen during training. It would also be beneficial to include comparisons with baseline times in Figures 5 and 8, and to conduct further experiments with transformers, such as pre-training Roberta on C4. Furthermore, we suggest including task-specific fine-tuning results to better demonstrate the model's performance across various downstream tasks. We encourage the authors to clarify the use of CUDA graphs in Section 4.3 and discuss the limitations of the proposed algorithm in detail, particularly regarding its performance on larger datasets. Lastly, we encourage the authors to consider releasing the code to enhance the contribution and impact of their work within the research community.