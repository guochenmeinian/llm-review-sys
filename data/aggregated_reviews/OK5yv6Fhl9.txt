ID: OK5yv6Fhl9
Title: MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel benchmarking dataset called MULTITuDE for multilingual machine-generated text detection, comprising 74,081 texts in 11 languages, including 7,992 human-written texts and 66,089 machine-generated texts. The authors provide a comprehensive multilingual benchmark for several state-of-the-art (SOTA) detection methods and evaluate the cross-language generalization of fine-tuned models in a multilingual setting. The study also conducts a large-scale evaluation of machine-generated text detection techniques using 11 languages and 11 large language models (LLMs), focusing on linguistic similarities.

### Strengths and Weaknesses
Strengths:
- The introduction of the MULTITuDE dataset is a valuable contribution, fostering further research in multilingual machine-generated text detection.
- The paper provides a thorough multilingual benchmark for SOTA detection methods, enhancing understanding of these techniques.
- The related works section is well-written and covers relevant literature comprehensively.
- The selection of experimental variables is well-considered, yielding useful insights.

Weaknesses:
- The dataset is limited to one domain (news texts), which may restrict its applicability to other domains.
- There is a limited amount of training data available for only three languages, and the human-written texts are also scarce.
- The findings, while extensive, are somewhat straightforward and expected, lacking novelty.
- The quality of experiments is questionable due to the absence of hyperparameter optimization, which could affect the reliability of results.

### Suggestions for Improvement
We recommend that the authors improve the dataset by including a broader range of domains beyond news texts to enhance its applicability. Additionally, increasing the amount of training data for more languages would strengthen the study's findings. We suggest that the authors conduct hyperparameter optimization for the models used, as relying on a single set of hyperparameters may not yield optimal performance across diverse architectures. Finally, addressing the straightforward nature of the findings could enhance the paper's novelty and impact.