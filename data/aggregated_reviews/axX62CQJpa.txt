ID: axX62CQJpa
Title: Streaming Long Video Understanding with Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents VideoStreaming, a Vision-Language Large Model (VLLM) designed to understand arbitrary-length videos by streamingly encoding video tokens and adaptively selecting a constant number of them. The authors propose a novel methodology that includes a small LLM streaming encoder, prefix task, modified attention, memory propagation, and Gumbel-Topk token selection. The method demonstrates superior performance on long video benchmarks, showcasing precise temporal comprehension for detailed question answering.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant computational challenge in long video understanding by effectively managing the number of tokens.
- The methodology is innovative, utilizing causal LLMs for streaming and query-related vision summarization with explicit timestamp cues.
- The Memory-Propagated Streaming Encoding technique captures long-term video content while maintaining a manageable computational footprint.
- The paper is well-written and easy to follow, with sufficient experiments across varied benchmarks.

Weaknesses:
- VideoStreaming may be unnecessary for common short/medium video benchmarks, as it uses significantly more training data than comparable models without demonstrating clear advantages.
- The paper lacks sufficient experiments on long video benchmarks, particularly in competitive baselines.
- Important applications, such as long-term, real-time streaming QA, are not addressed.
- The use of summarization tokens and the necessity of a small language model for the streaming encoder require better justification and clarity.
- The ablation studies are not comprehensive, and the paper does not include recent instruction-tuning baselines for fair comparison.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including more results on long video benchmarks and competitive baselines with the same training data and procedures. Additionally, the authors should address the potential redundancy in key clip selection by integrating a regularization function to promote diversity. We suggest providing clearer explanations of the summarization tokens and the necessity of the small language model for visual dependencies. Furthermore, the authors should enhance the ablation studies to validate their design choices comprehensively. Finally, including discussions on recent works related to long video streaming optimization would strengthen the paper's contributions.