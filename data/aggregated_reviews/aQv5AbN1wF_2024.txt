ID: aQv5AbN1wF
Title: On Feature Learning in Structured State Space Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 6, 6, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the scaling behavior of structured state space models (SSMs) as their width approaches infinity. The authors demonstrate that existing scaling rules, such as maximal update parameterization (μP) and spectral scaling conditions, fail to ensure feature learning in SSMs. They propose a new scaling rule, μP SSM, which facilitates feature learning and shows improved stability, generalization, and hyperparameter transferability through empirical results. The analysis includes a detailed examination of signal propagation in SSMs, both forward and backward, and highlights the importance of proper initialization and parameterization for stable outputs across multiple layers.

### Strengths and Weaknesses
Strengths:
- Addresses a significant theoretical question regarding SSM scaling behavior.
- Provides rigorous analysis of signal propagation in SSMs.
- Identifies limitations of existing scaling approaches and proposes a principled correction.
- Empirically validates results on real SSM architectures like Mamba.
- Offers practical implications for training larger, more efficient SSMs.

Weaknesses:
- Presentation can be difficult to follow, with many terms introduced without sufficient explanation.
- Theoretical analysis is limited to the N_u then N_x approaching infinity setting.
- Narrow scope of empirical validation, focusing only on text generation with Mamba on a single dataset.
- Lacks comparison to other recent SSM variants and insufficient discussion of potential negative implications of enabling feature learning in larger SSMs.
- No clear roadmap for extending results to more practical settings.

### Suggestions for Improvement
We recommend that the authors improve the presentation by providing more introductory material on state space models to assist readers unfamiliar with the topic. Additionally, including diagrams could enhance visualization of the SSM forward pass. On the mathematical side, we suggest that the authors organize the analysis more clearly, establishing notation and results explicitly, and modularizing proofs into basic operations. More empirical comparisons among standard parameterization, μP, spectral scaling, and μP SSM across various datasets and tasks would strengthen the findings. Finally, addressing the assumptions regarding the input dimensions and providing a clearer motivation for the necessity of considering N_x and N_u approaching infinity would enhance the paper's clarity and applicability.