ID: hz33V7Tb2O
Title: CLeAR: Continual Learning on Algorithmic Reasoning for Human-like Intelligence
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 5, 6, 7, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CLeAR, a novel methodology for continual learning (CL) of abstract logical concepts and automata recognition (AR) tasks through a one-to-many mapping strategy that aligns various tasks in a shared mapping space. The authors address challenges specific to CL for algorithmic reasoning tasks, including decorrelated input data and the need for generalization to out-of-distribution data. The method demonstrates near-zero forgetting and improved accuracy during subsequent training, outperforming existing CL methods in image classification and achieving high accuracy with minimal forgetting in diverse AR scenarios. The authors propose 15 benchmarks based on the Chomsky hierarchy to evaluate their approach, and the mapping function \( m_t \) enables effective adaptation across a sequence of diverse AR tasks while ensuring a uniform distribution and preserving all relevant information for complete reconstruction of original inputs.

### Strengths and Weaknesses
Strengths:
- The introduction of CLeAR fills a gap in CL research focused on abstract logical concepts and automata recognition.
- The one-to-many mapping strategy effectively addresses unique challenges in abstract concept learning and shows superior performance and stability in CL-AR tasks.
- The experiments show promising results, including backward transfer and improved performance across various tasks, laying the groundwork for future exploration in the novel field of CL-AR.

Weaknesses:
- The framing around catastrophic forgetting may not accurately reflect current empirical findings in deep learning, and relevant literature should be cited.
- The experiments lack clarity on their real-world applicability and the specific scenarios where sequential training is beneficial.
- CLeAR appears to be outperformed by multitask and single-task training, and comparisons with these baselines are insufficiently detailed.
- The authors have not yet fully explored the performance limits of their mapping function with an extensive range of alphabets, and clearer descriptions of the novelty and methodology of the \( m_t \) learning method are needed.

### Suggestions for Improvement
We recommend that the authors improve the framing of catastrophic forgetting by incorporating recent empirical studies to provide a more nuanced perspective. Additionally, the authors should clarify how their experiments relate to real-world problems and specify scenarios where sequential training is advantageous. It is essential to include a comprehensive comparison with multitask and single-task training to better demonstrate the effectiveness of CLeAR. Furthermore, the authors should elaborate on the interpretation of results in Table 3 and ensure that all experimental evaluations are conducted across multiple runs to avoid confirmation bias. Lastly, we recommend that the authors improve the clarity of their description regarding the novelty of the \( m_t \) learning method, particularly its incorporation of perfect reconstruction loss and mean and covariance losses, and expand the alphabet's range to test the mapping function's performance to its limits for both single tasks and CL.