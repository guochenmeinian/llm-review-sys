ID: cmSNX47aEH
Title: DeiSAM: Segment Anything with Deictic Prompting
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 3, 6, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 7, 8, 7
Original Confidences: 5, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, 4, 3

Aggregated Review:
### Key Points
This paper presents DeiSAM, a novel framework that integrates large pre-trained neural networks with differentiable logic reasoners for deictic promptable segmentation. The authors leverage Large Language Models (LLMs) to generate first-order logic rules and perform forward reasoning on scene graphs to enhance image segmentation based on complex textual prompts. Additionally, the paper introduces the Deictic Visual Genome (DeiVG) dataset for evaluating deictic segmentation and demonstrates that DeiSAM outperforms existing neural baselines.

### Strengths and Weaknesses
Strengths:
1. The paper is well-structured and easy to read.
2. The integration of LLMs with differentiable logic reasoners for deictic prompt-based segmentation is a novel approach, addressing limitations in neural network-based segmentation models.
3. Empirical results show significant improvements over existing baselines, validating the effectiveness of the proposed method.

Weaknesses:
1. The motivation and technical novelties are limited, as the approach combines existing methods without substantial new insights.
2. The clarity of the paper could be improved, particularly regarding the training process and the role of the scene graph generator.
3. The reliance on scene graph quality raises concerns about generalizability and the potential for misleading performance results.
4. The dataset's construction may be overly artificial, limiting its applicability to real-world scenarios.
5. The paper lacks detailed ablation studies and evaluations on more challenging benchmarks, which would provide a comprehensive assessment of DeiSAMâ€™s effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by annotating which modules are tuned during training and providing clearer explanations of the Semantic Unifier and forward reasoning components. Additionally, the authors should evaluate the proposed method on other referring image segmentation benchmarks, such as RefCOCO, RefCOCOg, and RefCOCO+, to assess its effectiveness in broader contexts. Finally, we suggest including detailed ablation studies for each component to elucidate the benefits of the combined logical reasoner and the effects of the input scene graph.