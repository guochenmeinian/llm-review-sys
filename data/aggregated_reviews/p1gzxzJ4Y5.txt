ID: p1gzxzJ4Y5
Title: FlowPG: Action-constrained Policy Gradient with Normalizing Flows
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel action-constrained reinforcement learning (ACRL) algorithm called FlowPG, which utilizes a normalizing flow model to generate actions within the feasible action region. The authors propose to learn an action mapping that respects constraints with high probability, training a normalizing flow offline on samples generated via Hamiltonian Monte-Carlo (HMC) and probabilistic sentential decision diagrams (PSDD). FlowPG demonstrates superior performance in terms of fewer constraint violations and reduced wall clock time across various tasks.

### Strengths and Weaknesses
Strengths:
- The paper addresses a critical problem in reinforcement learning, particularly in robotics and decision-making tasks.
- FlowPG is a novel and effective solution that improves performance while reducing constraint violations and training time.
- The use of a uniform prior distribution is insightful, enhancing the performance of policy gradient algorithms.
- The organization and clarity of the paper are commendable, with effective visualizations supporting the findings.

Weaknesses:
- The tasks evaluated are standard benchmarks with relatively simple constraints, limiting the assessment of scalability to more complex, state-dependent constraints.
- The sample generation procedure may struggle with higher-dimensional action spaces, necessitating further exploration of iterative refinement approaches.
- There is insufficient discussion on handling discrete actions in the BSS environment, raising concerns about potential constraint violations.
- The lengthy discussion on gradient computation for the normalizing flow appears unnecessary unless it directly contributes to training efficiency.
- The rationale for choosing a normalizing flow over other generative models lacks depth, particularly in the context of deterministic policies.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including more complex tasks with state-dependent constraints to better demonstrate the scalability of FlowPG. Additionally, a clearer discussion on how discrete actions are managed in the BSS environment is necessary to address potential constraint violations. An ablation study on the effectiveness of the HMC and PSDD methods should be included to validate their contributions. Furthermore, the authors should consider incorporating more recent state-of-the-art methods in the experiments to enhance the comparative analysis. Lastly, a more robust justification for the choice of normalizing flows over other generative models would strengthen the paper's argument.