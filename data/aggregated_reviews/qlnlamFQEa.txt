ID: qlnlamFQEa
Title: Aligning Synthetic Medical Images with Clinical Knowledge using Human Feedback
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 6, 7, 8, 7, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 5, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a pathologist-in-the-loop framework for generating clinically plausible synthetic medical images using diffusion models. The authors propose a three-step process: pretraining a conditional diffusion model on real medical images, training a clinical plausibility reward model based on expert feedback, and fine-tuning the diffusion model with a reward-weighted likelihood loss. Extensive evaluations demonstrate significant improvements in clinical plausibility, fidelity, diversity, and downstream utility of the generated images.

### Strengths and Weaknesses
Strengths:
- The integration of clinical expert knowledge into the generative process is innovative and addresses a critical problem in medical image synthesis.
- The paper is well-organized, clearly presented, and includes convincing empirical evaluations and ablations.
- Extensive evaluation metrics and visualization of synthetic images highlight the effectiveness of incorporating pathologist feedback.

Weaknesses:
- Reproducibility is a concern; making the code public would enhance reproducibility.
- The reliance on manual rules for expert feedback lacks clarity regarding their definition and effectiveness in capturing complex image quality.
- The evaluation is limited to a single downstream task, and comparisons with models trained on both synthetic and real images are absent.
- The criteria for clinical plausibility could be more precisely defined, and the use of binary feedback from pathologists may limit the richness of the supervision signal.

### Suggestions for Improvement
We recommend that the authors improve reproducibility by making the code publicly available. Additionally, please elaborate on the manual rules used for expert feedback and consider evaluating the model's performance with varying amounts of rules. It would be beneficial to include more downstream tasks in the evaluation and to compare the performance of models trained on both synthetic and real images. Furthermore, we suggest providing more precise definitions for the clinical plausibility criteria and exploring the use of more granular ratings from pathologists for richer feedback. Finally, consider conducting evaluations on multiple datasets and higher resolution images to validate the generalizability of the method.