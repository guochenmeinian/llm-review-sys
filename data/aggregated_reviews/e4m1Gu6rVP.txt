ID: e4m1Gu6rVP
Title: Distilling ChatGPT for Explainable Automated Student Answer Assessment
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Automated Explainable Student Response Assessment (AERA) framework, designed to automate student answer assessment by utilizing ChatGPT for scoring and generating rationales. The framework involves three steps: (1) prompting ChatGPT for rationales, (2) refining these rationales, and (3) fine-tuning a smaller language model, Long T5, using the refined outputs. The authors demonstrate that their method improves the Quadratic Weighted Kappa (QWK) score by 11% compared to ChatGPT and that the rationales produced are of comparable quality to those from ChatGPT.

### Strengths and Weaknesses
Strengths:
- The framework has the potential to significantly enhance efficiency and interpretability in educational assessments.
- The methodology is straightforward and effectively addresses data scarcity in short answer grading.
- Clear writing and presentation facilitate understanding of the proposed method and results.

Weaknesses:
- The choice of the ASAP-SAS dataset raises concerns, as it lacks rationales for each score, limiting evaluation.
- Comparisons with baselines, particularly ChatGPT, may not be fair due to differences in training and capabilities.
- The use of a closed-source model like ChatGPT complicates replicability and generalizability of findings.
- Lack of significance testing for qualitative results and insufficient clarity in the rationale quality measurement.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the rationale for choosing the ASAP-SAS dataset and consider using an open-source LLM for better replicability. Additionally, we suggest conducting experiments with varying numbers of demonstration examples to provide insights into model performance. It would also be beneficial to include significance testing for the evaluation results and to address the trade-off between model performance and rationale generation more explicitly. Lastly, we encourage the authors to reference existing distillation literature to contextualize their contributions more effectively.