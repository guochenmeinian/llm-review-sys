ID: sW8yGZ4uVJ
Title: Ordering-based Conditions for Global Convergence of Policy Gradient Methods
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 8, 8, 9, 5, 7, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of global convergence for policy gradient (PG) methods, specifically standard softmax PG and natural policy gradient (NPG), in the context of finite-arm bandits with linear function approximation. The authors demonstrate that global convergence is achievable without reliance on policy or reward realizability and that approximation error is not critical for global convergence. They propose new ordering-based conditions for Softmax PG, which require the representation to preserve reward ranking, and for NPG, which necessitate that the projection of the reward onto the representation space maintains the optimal actionâ€™s rank. This finding is supported by numerical simulations and establishes new conditions for convergence, challenging the traditional reliance on approximation error as a key factor.

### Strengths and Weaknesses
Strengths:
- The paper provides a novel perspective on the convergence of PG methods, emphasizing that global convergence is determined by ordering conditions rather than approximation error.
- The research question is significant, and the results are likely to inspire further developments in function approximation for more general Markov decision processes (MDPs).
- The proof strategies and algorithm analysis techniques introduced are innovative, and the writing is clear and well-structured, with helpful examples and rigorous definitions of new conditions.

Weaknesses:
- The analysis is limited to a single-state scenario, raising concerns about the applicability of results to more complex, multi-state MDPs.
- The paper lacks sufficient evidence to generalize findings beyond the studied case, and discussions on extending results to broader contexts are insufficient.
- The paper would benefit from larger-scale experiments, and further discussion on the implications of the ordering-based conditions is needed.
- Some proofs, particularly for Theorem 1, require further clarification to enhance understanding.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by providing more extensive discussions on how the results might apply to multi-state MDPs. Additionally, we suggest constructing larger-scale experiments to validate their findings and including more persuasive evidence or simulations to support the applicability of the proposed ordering-based conditions in broader contexts. Clarifying the proofs, especially for Theorem 1, would enhance the paper's rigor and accessibility. We also encourage the authors to discuss the implications of the ordering-based conditions in more depth, particularly for Softmax PG, and whether there is a systematic way to check the existence of a weight vector that preserves reward ranking. Furthermore, addressing the size of the subspace of \(X\) in \(\mathbb{R}^{d}\) that satisfies the order-preserving condition for both Softmax PG and NPG would be beneficial. Lastly, we suggest revising the proof in the appendix for clarity and addressing the ordering of features in examples to show that they are not solely based on column permutations.