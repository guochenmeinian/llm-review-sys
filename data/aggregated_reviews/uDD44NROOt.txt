ID: uDD44NROOt
Title: SPRINQL: Sub-optimal Demonstrations driven Offline Imitation Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the SPRINQL algorithm, which addresses offline imitation learning from sub-optimal demonstrations by utilizing a reference reward function and inverse soft-Q learning. The authors propose a method that combines reward regularization and weights based on trajectory expertise rankings, demonstrating that SPRINQL achieves superior performance compared to existing baselines in simulated environments. The evaluation includes extensive ablation studies that validate the robustness of the method across various dataset sizes.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and provides strong experimental results, with extensive ablation studies justifying each component of SPRINQL.  
- The method effectively leverages multiple suboptimal demonstrations, showing improved performance as more suboptimal data is included.  
- The theoretical foundation is solid, with the main objective admitting a unique saddle point solution over \(Q, \pi\).

Weaknesses:  
- Certain aspects of the proposed approach remain unclear, particularly regarding the guarantees on the converged policy due to heuristic weight selection.  
- The influence of the reward regularization term is not fully explored, raising questions about its impact on performance.  
- The suboptimal trajectories are generated by adding noise to expert actions, which may not convincingly represent under-trained RL policies.  
- The reporting on baseline performance raises concerns about reproducibility, as results deviate significantly from original papers.

### Suggestions for Improvement
We recommend that the authors improve clarity on the guarantees regarding the converged policy in relation to the heuristic weight selection. Additionally, we suggest a more thorough investigation into the effects of the reward regularization term on performance. It would also be beneficial to generate suboptimal trajectories from under-trained RL policies to strengthen the experimental validation. Finally, we encourage the authors to provide clearer commentary on the trends observed in the extensive evaluation presented in Table 1 and to consider releasing the datasets used for experiments to enhance reproducibility.