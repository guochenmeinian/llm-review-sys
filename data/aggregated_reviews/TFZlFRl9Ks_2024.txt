ID: TFZlFRl9Ks
Title: CAT3D: Create Anything in 3D with Multi-View Diffusion Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 8, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CAT3D, a pipeline for generating 3D representations from one or a few input views. It includes a multi-view generation model for synthesizing novel images and a Zip-Nerf for 3D reconstruction. The authors demonstrate impressive 3D results and provide extensive experiments validating CAT3D's effectiveness. The method is noted for its ability to handle single images and sparse multi-view captures, achieving state-of-the-art performance on few-view 3D reconstruction tasks.

### Strengths and Weaknesses
Strengths:
1. CAT3D achieves impressive results in 3D generation, showcasing high quality and efficiency.
2. The pipeline is straightforward yet robust, as verified through comprehensive experiments.
3. The method demonstrates good generalization despite being trained on constrained datasets.
4. The ablation study is convincing and well-executed.

Weaknesses:
1. Many techniques used in CAT3D are derived from previous works, raising questions about originality.
2. The source of results in key figures is unclear, necessitating clarification on whether they stem from ZIP-Nerf or the multi-view diffusion process.
3. Inference efficiency is reported only with 16 A100 GPUs, which is not representative for most users; results with a single GPU are needed.
4. The training conditions and targets are limited, and the ability to handle arbitrary conditions without fine-tuning is uncertain.
5. The generalization of the model, particularly for text-to-image samples, is not adequately addressed.
6. The authors did not commit to releasing the code, leaving implementation details vague.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the source of results in Figures 1, 2, 4, and 5, specifying whether they are derived from ZIP-Nerf or the multi-view diffusion process. Additionally, please provide inference efficiency results with a single GPU to enhance accessibility for users. It would also be beneficial to explore the model's capability to handle arbitrary conditions and targets without fine-tuning. We suggest including a more thorough discussion on the generalization of the model, particularly concerning text-to-image samples. Finally, we encourage the authors to clarify their plans for code release and provide more details about the training procedure and hyperparameters to facilitate reproducibility.