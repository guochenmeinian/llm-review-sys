ID: 4272bEn4Q0
Title: Large Language Models are Complex Table Parsers
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for question answering on complex tables, specifically those with multi-level headers and numerous merged cells. The authors propose using advanced generative language models, such as GPT-3.5, and introduce a novel complex table representation method along with a prompting mechanism to enhance performance and address the maximum input token limitation. Experimental results on HiTab and AIT-QA demonstrate the effectiveness of the proposed approach, supported by a fine-grained empirical analysis.

### Strengths and Weaknesses
Strengths:
- The problem addressed is highly valuable and relevant.
- The paper is clearly written and easy to follow.
- The proposed approach achieves state-of-the-art performance on HiTab and AIT-QA.

Weaknesses:
- The motivation for the proposed approach is unclear, lacking elaboration on the new challenges faced when applying LLMs to complex table QA compared to flat table QA.
- Important baselines, such as the table linearization method used in Tapex, are missing from the experiments.
- The effectiveness of the multi-turn dialogue schemes is not adequately demonstrated.
- The evaluation relies solely on accuracy, which does not provide insights into the generated answers' precision and recall.

### Suggestions for Improvement
We recommend that the authors improve the motivation for their approach by clearly articulating the new challenges in applying LLMs to complex table QA compared to flat table QA. Additionally, we suggest including comparisons with established baselines, such as the table linearization method from Tapex, to better demonstrate the effectiveness of their table reconstruction method. It would also be beneficial to provide evidence of the effectiveness of the multi-turn dialogue schemes. Furthermore, we encourage the authors to explore alternative evaluation metrics beyond accuracy to capture the nuances of generated answers. Lastly, we advise enhancing the clarity of the title to reflect the specific contributions of the paper.