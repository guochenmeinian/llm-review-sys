ID: XswQeLjJo5
Title: Unraveling the Gradient Descent Dynamics of Transformers
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of the training dynamics of Transformer models, demonstrating that with appropriate initialization, gradient descent can achieve a global optimal solution using either Softmax or Gaussian kernels. The authors highlight that the Gaussian attention kernel exhibits more favorable behavior than Softmax in certain scenarios, establishing different convergence theorems for a single-layer Transformer with varying trainable weight matrices and kernel functions. Empirical experiments validate the theoretical claims, showing that the Gaussian kernel converges faster than the Softmax kernel.

### Strengths and Weaknesses
Strengths:
- The paper provides a thorough theoretical analysis of Transformer training dynamics, offering conditions for global convergence.
- The comparison between Gaussian and Softmax kernels is novel and insightful, suggesting significant implications for future research and applications.

Weaknesses:
- The insights from Theorem 2 regarding global convergence are unclear, especially in light of existing work by Wu et al. [2024], which allows for a trainable weight $W^O$.
- The empirical evaluation is limited, relying on simplified models and small datasets, which may not convincingly validate the theoretical claims.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the insights gained from Theorem 2, particularly in relation to Wu et al. [2024], by comparing the conditions and discussing the implications of fixed versus trainable weights. Additionally, we suggest enhancing the empirical evaluation by testing on larger, more complex Transformer architectures and standard NLP benchmarks to better validate the theoretical results. Furthermore, including critical steps from the proofs in the main text could clarify the differences in convergence rates between the Gaussian and Softmax kernels.