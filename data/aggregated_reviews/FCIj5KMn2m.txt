ID: FCIj5KMn2m
Title: Cognitive Steering in Deep Neural Networks via Long-Range Modulatory Feedback Connections
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 8, 5, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to incorporating long-range modulatory feedback connections into deep CNNs, specifically AlexNet, evaluated on ImageNet. The authors propose two modes of feedback: a default mode and a cognitive steering mode, demonstrating improvements in ImageNet accuracy, adversarial robustness, and performance on a composite image recognition task. Additionally, the paper compares additive and multiplicative long-range modulation (LRM) models, focusing on their performance in detecting targets and managing false alarms. The authors argue that while the LRA model shows improved detection rates, it also results in increased false alarms compared to the LRM model. They clarify that LRM models can still experience hallucinations under certain conditions and introduce a new section linking their models to human cognitive behaviors, demonstrating how LRM models can replicate neural signatures of top-down attention.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and clear, with thoughtfully prepared visualizations. The motivation for the modulatory connections is well-articulated, and the empirical results are compelling.
- The authors effectively address anticipated questions within the paper, particularly in Section 2.2, which explains decision-making processes and explored options.
- The exploration of long-range feedback connections is a significant and timely contribution to the field of neural networks.
- The thorough comparison of LRM and LRA models highlights their respective strengths and weaknesses in terms of detection and false alarm rates, enhancing the manuscript's clarity and depth.
- The addition of a new results section detailing performance metrics for both models under varying attentional gains is a valuable enhancement.

Weaknesses:
- The implementation details in Section 2.1 lack mathematical equations or pseudocode, leading to ambiguity regarding the feedback connections' computations.
- A thorough discussion of the costs associated with these connections (e.g., time, memory) is missing, which may mislead readers into thinking there are no drawbacks to their use.
- The Related Research section could benefit from a clearer identification of the 1-3 most closely related works and how this implementation differs from them.
- The empirical evaluation lacks robustness, as the paper does not sufficiently benchmark against standard CV datasets or existing approaches.
- The authors have not yet conducted AblationCam tests during cognitive steering, which could provide valuable insights into the models' attentional mechanisms.
- There is a need for more initial results on standard computer vision tasks and Brain-Score to further substantiate the significance of the work.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the implementation details in Section 2.1 by including mathematical equations or pseudocode to eliminate ambiguity. Additionally, a comprehensive discussion of the computational costs associated with the feedback connections should be included to provide a balanced view of their implications. The authors should also refine the Related Research section to highlight the most relevant prior works and articulate the distinctions of their approach. Furthermore, we suggest conducting additional experiments on standard datasets and comparing their model against larger or deeper architectures to strengthen the empirical evaluation. We also recommend that the authors conduct AblationCam tests during cognitive steering to gain insights into what the hallucinating additive networks attend to compared to the LRM. Lastly, presenting initial results on standard computer vision tasks, such as Visual Question Answering (VQA), and further elaborating on their Brain-Score findings would better support the significance of their work.