ID: Eib6OOeVJI
Title: Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SciMult, a multi-task contrastive learning framework designed for scientific literature understanding tasks, including multi-label classification, link prediction, and search. The authors propose task-aware specialization and instruction tuning to reduce interference among tasks. Extensive experiments demonstrate that SciMult outperforms state-of-the-art scientific language models across various benchmark datasets.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and clearly written, making it easy to follow.
- It introduces an innovative approach by unifying multiple scientific literature understanding tasks within a contrastive learning framework, offering four model variants for task-aware representations.
- The authors effectively address the issue of undesirable backbone parameter sharing, preventing task interference, and propose a reasonable method of instruction tuning to enhance performance.
- Comprehensive experiments validate the effectiveness of SciMult, showing superior performance in both in-domain and cross-domain settings.

Weaknesses:
- The motivation for the proposed method is not sufficiently strong, particularly in distinguishing it from the recent work SciRepEval, which shares similar objectives.
- The assumption that each label's name and definition must be available is somewhat strong.
- The concepts of multi-task learning and contrastive learning lack novel insights from a task nature perspective, resulting in a marginal contribution to the field.
- Performance results indicate that SciMult underperforms or matches SPECTER 2.0 on certain datasets, suggesting that improvements may be contingent on the contrastive learning setup.

### Suggestions for Improvement
We recommend that the authors clarify the differences between SciMult and SciRepEval to strengthen the motivation for their contributions. Additionally, addressing the strong assumptions regarding label definitions and providing more novel insights into multi-task learning could enhance the paper's impact. Finally, we suggest that the authors investigate the performance of SciMult when trained solely on specific datasets to better understand its advantages over traditional methods.