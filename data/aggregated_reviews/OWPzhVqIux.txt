ID: OWPzhVqIux
Title: Can large language models explore in-context?
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 5, 4, 5, 7, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the exploration capabilities of large language models (LLMs) like GPT-3.5, GPT-4, and LLAMA2 in multi-armed bandit tasks, utilizing in-context learning where environment descriptions and interaction histories are provided within the prompt. The findings reveal that most configurations fail to explore effectively, exhibiting "suffix failures" or "uniform-like failures," with only GPT-4 using chain-of-thought reasoning and an externally summarized history showing satisfactory exploratory behavior. The authors conclude that algorithmic interventions may be necessary for LLMs to function as effective decision-making agents in more complex settings.

### Strengths and Weaknesses
Strengths:
- The study employs multiple LLMs and various prompt designs, providing a comprehensive evaluation against established baselines.
- The introduction of "suffix failure frequency" and "MinFrac" as metrics for identifying exploration failures is innovative.
- The research addresses an important question regarding LLM capabilities, with implications for their use in decision-making tasks.

Weaknesses:
- The research question has been previously explored, raising concerns about the novelty and contribution of the work.
- The empirical focus may not fully represent the complexities of broader reinforcement learning problems, limiting generalizability.
- The analysis of failure modes and prompt design elements is underdeveloped, and the lack of parameter tuning for baselines raises questions about the fairness of comparisons.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their work by more thoroughly discussing related literature and addressing the limitations of previous studies. A more systematic investigation of failure modes and prompt design elements could yield valuable insights. Additionally, exploring the effects of fine-tuning smaller models like GPT-2 or LLaMa-2 on simple RL tasks may enhance understanding of exploration capabilities. We also suggest including a mixed-effects regression analysis to assess the contributions of various factors to exploration outcomes, which would provide a more nuanced understanding of the results.