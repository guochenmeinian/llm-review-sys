ID: HRGd5dcVfw
Title: Guiding The Last Layer in Federated Learning with Pre-Trained Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 6, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to fine-tuning classifiers in Federated Learning (FL) using pre-trained models, specifically through a two-stage head-tuning and fine-tuning technique. The authors propose the nearest class means (NCM) method, which is claimed to enhance computational efficiency and communication costs while improving performance in heterogeneous settings. Empirical evaluations demonstrate that the proposed method outperforms standard federated learning techniques, particularly in terms of convergence speed and robustness to hyperparameters.

### Strengths and Weaknesses
Strengths:
1. The exploration of using pre-trained models in FL is innovative and timely, particularly given the rise of foundation models.
2. The experiments are rigorously conducted and detailed, providing insights into the discrepancies observed in previous studies.
3. The proposed method shows significant improvements in communication efficiency and convergence speed compared to traditional fine-tuning methods.

Weaknesses:
1. The explanation of the proposed approach, particularly regarding the mixed tuning process, lacks clarity.
2. The method's comparison with existing federated algorithms specifically designed for heterogeneity is insufficient.
3. The evaluation focuses on linear probing and NCM without adequately distinguishing their differing methodologies, leading to ambiguity about the paper's primary message.
4. The experimental scope is limited to small-scale datasets, and there is a lack of theoretical analysis regarding communication costs and convergence rates.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the explanation regarding the mixed tuning approach, specifically whether the final tuning applies to all weights or just the head. Additionally, we suggest that the authors compare their method with established federated algorithms tailored for heterogeneous data to strengthen their argument. It would also be beneficial to clarify the distinct contributions of linear probing and NCM in the results to enhance the paper's focus. Finally, we encourage the authors to expand their experimental evaluation to include larger datasets and provide a more comprehensive theoretical analysis of communication costs and convergence rates.