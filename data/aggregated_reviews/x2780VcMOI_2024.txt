ID: x2780VcMOI
Title: A Polar coordinate system represents syntax in large language models
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 8, 5, 5, -1, -1, -1, -1
Original Confidences: 4, 2, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents polar probes, a structural probe that learns a distance and rotation function to classify syntactic structures from language model representations more accurately than previous methods. The authors demonstrate that their polar probe significantly outperforms prior approaches, addressing whether direction can represent syntactic relationships. The evaluation includes metrics such as UUAS, LAS, and Balanced Accuracy on a controlled dataset, providing a robust comparison. The findings suggest that the direction and angle of representations in language models effectively encode syntactic relationships, raising the bar for understanding syntax in language models.

### Strengths and Weaknesses
Strengths:  
- The paper is well-presented, with strong empirical findings supporting the hypothesis regarding the representation of syntactic relationships.  
- The controlled dataset allows for a clean comparison in a challenging setting, serving as a valuable resource for future research.  
- The potential for exploring syntax through language models, especially in multilingual contexts, points to exciting future directions.  

Weaknesses:  
- The focus on dependency parses and limitations of current tokenizers are acknowledged but remain weaknesses.  
- The analysis lacks depth, particularly in exploring how different model sizes capture syntactic complexity and the implications of a shared representation.  
- The distinction between probing and parsing is unclear, and the paper suffers from a lack of convincing baselines, with low cosine similarity between identical syntactic categories presenting unexpected patterns that are not adequately discussed.  

### Suggestions for Improvement
We recommend that the authors improve the model analysis by exploring how models of different sizes capture varying levels of syntactic complexity and identifying any critical points where syntax representation stabilizes. Additionally, we suggest providing a clearer distinction between probing and parsing, potentially by comparing results directly to parsers on the same data. Enhancing the discussion around the implications of shared representations and addressing the surprising patterns in cosine similarity would strengthen the paper. Finally, we encourage the authors to clarify how trees are predicted and to ensure that the meaningfulness of hierarchical structures is adequately emphasized in the discussion.