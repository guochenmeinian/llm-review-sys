ID: 7VjR70sxti
Title: LLGformer: Learnable Long-range Graph Transformer for Traffic Flow Prediction
Conference: ACM
Year: 2024
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LLGformer, a spatio-temporal graph model designed to predict traffic flow in road networks. The authors propose a learnable spatiotemporal graph construction method that incorporates historical data, effectively capturing long-range dependencies and periodic patterns. The architecture includes an encoder-decoder structure and experimental results demonstrate that LLGformer outperforms existing methods on multiple datasets, including METR-LA and PEMS-BAY. However, the novelty of the approach is questioned, as some components are not groundbreaking and lack theoretical justification.

### Strengths and Weaknesses
Strengths:
- The paper clearly outlines limitations in existing traffic flow prediction methods and provides a strong motivation for LLGformer.
- The introduction of a learnable spatiotemporal graph construction method based on Dynamic Time Warping (DTW) is innovative.
- Extensive experiments show that LLGformer significantly outperforms existing methods across various datasets.
- The manuscript is well-structured and clear, enhancing readability.

Weaknesses:
- The novelty of LLGformer compared to Trafformer is limited, as many components are identical, and this similarity is not acknowledged.
- The efficiency evaluation of LLGformer is insufficient, as it constructs a larger graph and introduces complex feature construction without a fair comparison of training and inference times.
- The paper does not adequately explain the selection of time alignment algorithms or the relevance of the time regularization algorithm to traffic flow prediction.
- The connection between traffic prediction and the Web is tenuous, and the related work section is overly lengthy and should be integrated into the main text.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for their architectural decisions to enhance the novelty of the work. Additionally, the authors should clarify the efficiency evaluation by including training and inference times in their comparisons. It would be beneficial to provide a more detailed explanation of the time alignment algorithms used and their relevance to the task. We also suggest integrating the related work section into the main text for better context and clarity. Lastly, the authors should strengthen the connection between their work and its relevance to the Web, as this is a critical requirement for the submission.