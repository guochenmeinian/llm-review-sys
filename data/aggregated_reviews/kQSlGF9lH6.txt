ID: kQSlGF9lH6
Title: Investigating Efficiently Extending Transformers for Long Input Summarization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an optimization of the block attention-based efficient long-context Transformer model, focusing on various design choices such as block size, global tokens, block arrangement, positional encoding, and pre-training schemes. The authors propose PEGASUS-X, an adaptation of the pre-trained PEGASUS model, which shows improved performance on long document summarization tasks, particularly on datasets like GovReport and PubMed. The contributions include a thorough study of design choices and the introduction of PEGASUS-X, which supports 16K input tokens and balances task performance with efficiency.

### Strengths and Weaknesses
Strengths:
- The paper conducts an extensive ablation study on design choices, revealing their effects on long document summarization.
- PEGASUS-X achieves state-of-the-art performance on key datasets, demonstrating its effectiveness.
- The architecture design choices are well-studied and contribute to a better understanding of long input summarization tasks.

Weaknesses:
- The study's domain coverage is limited, lacking exploration of more challenging tasks like meeting and narrative summarization.
- Empirical effectiveness compared to models like BART-LS shows only marginal improvements, raising questions about its significance.

### Suggestions for Improvement
We recommend that the authors expand the domain coverage of their study to include more complex tasks such as meeting and narrative summarization. Additionally, we suggest that the authors clarify the performance comparison of PEGASUS-X with BART-LS, particularly regarding computational efficiency and speed. Furthermore, we encourage the authors to investigate the impact of different global token embeddings on PEGASUS-X's performance for long-document summarization tasks.