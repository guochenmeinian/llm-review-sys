ID: N56hAiQvot
Title: PackQViT: Faster Sub-8-bit Vision Transformers via Full and Packed Quantization on the Mobile
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 4, 6, 5, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents PackQViT, a framework for sub-8-bit quantization of Vision Transformers (ViTs) aimed at efficient deployment on mobile devices. The authors propose various quantization schemes for weights and activations, alongside an outlier-aware training process. The framework utilizes a SIMD-based 4-bit packed multiplier to enhance inference speed while maintaining accuracy, demonstrating significant improvements on the ImageNet dataset. Additionally, the paper provides a comprehensive analysis of the PackQViT method, showcasing its superior performance compared to GPUSQ-ViT across various models and bit precisions. Detailed results indicate that PackQViT outperforms GPUSQ-ViT by up to 2.2% and 1.2% for DeiT and Swin models, respectively, at 8-bit precision, and remains competitive at 4-bit precision. The authors also address challenges related to INT8 inference on mobile GPUs, highlighting the lack of support from mainstream inference engines and mobile GPU architectures.

### Strengths and Weaknesses
Strengths:
- The paper effectively addresses challenges in deploying ViTs on mobile devices, providing practical solutions and demonstrating real hardware implementation.
- It showcases improved latency and accuracy in ViT inference, particularly under 8-bit and 4-bit quantization.
- The insights into the distribution of model weights and activations are valuable for the field.
- The paper provides clear and detailed experimental results, showcasing the effectiveness of PackQViT.
- The authors effectively clarify the limitations of current mobile GPU architectures and inference engines regarding INT8 support.

Weaknesses:
- The evaluation lacks clarity, particularly regarding the comparison between Quantization Aware Training (QAT) and Post-Training Quantization (PTQ) methods, which may mislead readers.
- The novelty of the proposed methods is questionable, as many techniques appear to be adaptations of existing approaches.
- Presentation issues, including confusing figures and unclear explanations, detract from the overall readability and comprehension of the paper.
- The explanation of the BitShift matmul kernel lacks clarity and does not convincingly demonstrate its advantages over traditional methods.
- Some reviewers express confusion regarding the classification of NVIDIA Jetson Orin as a mobile GPU and the overall mobile GPU narrative.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the evaluation section by clearly distinguishing between QAT and PTQ results, ensuring that comparisons are fair and well-supported. Additionally, the authors should provide more detailed discussions on the kernel implementation and the performance of their proposed methods across various hardware platforms. To enhance the novelty claim, it would be beneficial to explicitly differentiate their contributions from existing literature and clarify the unique aspects of their approach. We also suggest improving the clarity of the BitShift matmul kernel design by providing a more detailed comparison with traditional matmul implementations to substantiate claims of speedup. Finally, addressing the concerns regarding the classification of mobile GPUs and providing a more robust discussion on the implications of using architectures like Apple Neural Engine for low-precision fixed-point operations will significantly improve the paper's accessibility.