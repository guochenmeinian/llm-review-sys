ID: DHucngOEe3
Title: Pre-Trained Multi-Goal Transformers with Prompt Optimization for Efficient Online Adaptation
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 6, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach that applies prompt optimization to reinforcement learning by pre-training a transformer-based policy on a tasks-agnostic dataset, incorporating subgoals to fine-tune a meta-policy for optimizing goal trajectories. The authors propose a pretrain-and-prompt-tuning paradigm to address generalization challenges in RL, utilizing a goal-conditioned transformer and multi-arm bandit algorithms for prompt tuning. The experiments demonstrate the proposed method's superior performance and efficient adaptation in unseen environments.

### Strengths and Weaknesses
Strengths:
- The integration of tasks-agnostic pre-training with transformer architectures is innovative and aligns with future trends in AI research.
- The conceptualization of prompt and prompt-tuning in RL is intriguing and well-formulated, supported by comprehensive experiments.
- The experimental results validate the proposed method's effectiveness in achieving fast online adaptation and goal learning.

Weaknesses:
- Clarity and overall writing need improvement; the introduction is overly abstract, and a running example would enhance understanding.
- The process of prompt optimization using multi-arm bandit modeling lacks detail and appears confusing.
- The relationship between the proposed method and hierarchical or skill-based RL is not adequately discussed.
- Limitations regarding the volatility of prompt-based architectures and the necessity of task goals at test time are insufficiently addressed.

### Suggestions for Improvement
We recommend that the authors improve clarity by providing a running example in the introduction to illustrate the motivation. The concept of goal relabeling should be explained earlier, especially since Fig. 1 is referenced in the introduction. Additionally, we suggest restructuring the paper by switching sections 2 and 3 for better flow. 

The authors should enhance the definition of M by including H to define a finite horizon MDP and clarify the goal function's type signature to improve comprehensibility. Addressing the limitations of requiring data that accomplishes the intended task and comparing with other task-agnostic pre-training approaches, such as DIAYN, would strengthen the paper. 

Furthermore, we encourage the authors to elaborate on the multi-arm bandit modeling process and its complexity, ensuring that the theoretical guarantees are clearer. Finally, providing more discussion on the advantages of hindsight relabeling and the implications of using different policy backbones would enrich the analysis.