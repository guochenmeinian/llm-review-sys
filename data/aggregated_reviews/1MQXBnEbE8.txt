ID: 1MQXBnEbE8
Title: M$^3$-Impute: Mask-guided Representation Learning for Missing Value Imputation
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 6, 5, 6, 5, 7, 6, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, 4, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents M3-Impute, a mask-guided representation learning method for missing value imputation that leverages missingness information as an explicit input through innovative masking schemes. M3-Impute effectively learns feature-wise and sample-wise correlations, accommodating various types of data missingness (MCAR, MAR, MNAR). The model employs a variant of GraphSAGE for graph representation learning, incorporating edge embeddings via neighborhood aggregation, and demonstrates superior performance over traditional tabular data models across multiple benchmark datasets.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel approach to missing value imputation through M3-Impute, utilizing missingness information and innovative masking schemes to capture correlations effectively.
- Comprehensive experiments across various datasets validate the model's performance, showing consistent improvements over baseline methods.
- The inclusion of a code package and datasets enhances the reproducibility of the research.

Weaknesses:
- The paper lacks detailed sensitivity analysis for critical hyperparameters beyond the initialization parameter Ïµ, such as learning rate and dropout rate.
- There is insufficient contextualization of M3-Impute relative to prior work, particularly regarding the evolution and limitations of baseline models like GRAPE and IGRM.
- Relying solely on MAE for performance evaluation is limiting; incorporating RMSE would provide a more nuanced understanding of model performance, especially concerning outliers.

### Suggestions for Improvement
We recommend that the authors improve the sensitivity analysis by including a detailed examination of other critical hyperparameters, such as learning rate, batch size, and dropout rate. Additionally, we suggest enhancing the contextualization of M3-Impute by providing a deeper analysis of baseline models, including their strengths and weaknesses, to clarify the novelty of the proposed method. To address the limitations of using MAE, we recommend incorporating RMSE in the evaluation metrics to better capture the model's performance across different error distributions. Lastly, summarizing the limitations of M3-Impute in the conclusion would provide readers with a clearer understanding of the method's applicability and contexts where it may underperform.