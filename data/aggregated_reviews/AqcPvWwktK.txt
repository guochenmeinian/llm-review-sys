ID: AqcPvWwktK
Title: Semi-supervised Multi-label Learning with Balanced Binary Angular Margin Loss
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into semi-supervised multi-label learning (SSMLL), highlighting a significant variance-bias issue where the variance difference between positive and negative samplesâ€™ feature distributions is notably higher than in supervised settings. The authors propose a novel SSMLL method that addresses this issue through a balanced binary angular margin (BBAM) loss, which balances variance bias from the feature angle distribution perspective. Additionally, they introduce a prototype-based negative sampling method to ensure high-quality negative samples. Experimental results across various image and text benchmarks validate the effectiveness of the proposed approach.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with clear descriptions of the empirical phenomenon, theoretical analyses, and experimental design.
- The motivation behind addressing the variance bias problem is compelling, leading to the development of the BBAM loss and a reasonable SSMLL method.
- Comprehensive experimental results demonstrate the proposed method's effectiveness across multiple benchmarks.

Weaknesses:
- There is a lack of quantitative analysis regarding the variance difference and insufficient experiments on benchmarks with more positive classes per sample.
- The complexity of linear Gaussian transformations and estimating label angle variances may hinder practical applications, necessitating efficiency experiments.
- Some minor typographical errors and inconsistencies in mathematical symbols detract from the overall presentation.

### Suggestions for Improvement
We recommend that the authors improve the quantitative analysis of the variance difference to demonstrate how the BBAM loss addresses the variance bias issue. Additionally, conducting experiments on benchmarks with a higher average number of positive classes per sample, such as AWA, would strengthen the findings. We also suggest performing efficiency experiments to compare the running time of the proposed method against other baselines, such as CAP. Finally, addressing the minor typographical errors and standardizing mathematical symbols would enhance the clarity of the paper.