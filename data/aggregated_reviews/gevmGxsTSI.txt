ID: gevmGxsTSI
Title: Learning From Biased Soft Labels
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 5, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the learnability properties of biased soft labels in knowledge distillation and weakly-supervised learning. The authors propose two indicators, unreliability degree and ambiguity degree, to measure the effectiveness of these labels and establish conditions for classifier consistency and ERM learnability. Additionally, a heuristic method for training "Skillful but Bad Teachers" (SBTs) is introduced, demonstrating that these teachers can effectively train students to achieve high accuracy on datasets like CIFAR-10/100.

### Strengths and Weaknesses
Strengths:
- The investigation into learning from biased soft labels is highly relevant and original.
- The theoretical framework is comprehensive, providing valuable insights into weakly-supervised learning paradigms.
- The paper is well-written and presents a thorough theoretical analysis across multiple settings.

Weaknesses:
- The distinction between incomplete supervision and partial label learning is confusing and inconsistent with existing literature.
- The empirical analysis lacks baseline comparisons, making it difficult to assess the validity of the proposed indicators and SBT loss.
- The experiments are limited to CIFAR-10/100, with no exploration of larger datasets or different architectures, which restricts the generalizability of the findings.
- Clarity issues exist regarding terminology and assumptions, particularly around the definition of biased soft labels and the conditions for classifier consistency.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the distinction between incomplete supervision and partial label learning, aligning it with established literature. Additionally, the empirical analysis should include baseline comparisons to better evaluate the effectiveness of the proposed indicators and SBT loss. Expanding experiments to include larger datasets and various architectures would enhance the generalizability of the findings. We also suggest providing more visualizations of prediction results and elaborating on the assumptions made in the theoretical framework to strengthen the paper's arguments. Lastly, addressing the page limit by condensing content is necessary to meet submission guidelines.