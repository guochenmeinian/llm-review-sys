ID: z4FaPUslma
Title: Guiding Neural Collapse: Optimising Towards the Nearest Simplex Equiangular Tight Frame
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel algorithm for neural network training that leverages Riemannian optimization to guide the final layer weights toward a simplex Equiangular Tight Frame (ETF) orientation. The authors propose a method that penalizes the distance to the ETF and addresses solution non-uniqueness through a proximal term. Experimental results demonstrate improvements in training and testing accuracy across various architectures and datasets, highlighting the benefits of the neural collapse phenomenon.

### Strengths and Weaknesses
Strengths:
1. The proposed approach effectively frames the transition of weights to a simplex ETF as a differentiable Riemannian optimization problem, enabling an end-to-end training pipeline.
2. The algorithm shows significant improvements in convergence speed and stability, supported by well-executed experiments on synthetic and real-world datasets.

Weaknesses:
1. The paper lacks detailed numerical data on the additional memory and step-time costs associated with the deep declarative layer, which could be addressed with further empirical analysis.
2. The presentation of results could be clearer, particularly regarding the computational overhead and the effectiveness of the proposed method compared to standard training procedures.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the computational costs associated with the proposed method, including specific numerical data on memory and step-time increases. Additionally, consider providing a clearer comparison of running times and computational costs against standard and fixed ETF approaches. An ablation study should be included to clarify the contributions of various components, such as the proximal term and the deep declarative layer. Furthermore, we suggest addressing the questions raised regarding the optimization of equation 8, the implications of Proposition 1, and the clarity of hyperparameter choices, particularly concerning the role of temperature \(\tau\). Lastly, including error bars in figures and providing more detailed insights into the performance of the proposed method across different architectures would enhance the paper's clarity and impact.