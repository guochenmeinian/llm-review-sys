ID: pVACX02m0p
Title: Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 8, 7, 6, 7
Original Confidences: 3, 3, 3, 3

Aggregated Review:
### Key Points
This paper presents SimNPO, a novel optimization framework for large language model (LLM) unlearning that addresses the limitations of the existing Negative Preference Optimization (NPO) approach, particularly the bias introduced by a reference model. The authors argue that this bias can lead to ineffective unlearning and propose SimNPO as a simpler, reference-free method. Experimental results validate that SimNPO outperforms standard NPO on the TOFU benchmark and shows promise across various datasets, although results on MUSE and WMDP are more mixed.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, logically structured, and features high-quality figures, enhancing clarity.
- The removal of the reference model effectively addresses bias and efficiency issues associated with NPO.
- Thorough experimental evaluation supports the effectiveness of SimNPO, particularly on the TOFU benchmark.

Weaknesses:
- The similarities between SimNPO and SimPO raise concerns about the novelty of the approach, as both methods share key techniques like removing the reference model and incorporating length normalization.
- The omission of RMU's performance on TOFU and MUSE benchmarks limits the ability to fully assess SimNPO's utility.
- Clarity issues exist in the "mixture of Markov chains" experiment, particularly regarding the trade-offs between SimNPO and NPO for long sequences.
- The paper implies that SimNPO does not generate repeated texts, which is misleading as this occurs under both methods.

### Suggestions for Improvement
We recommend that the authors improve the novelty discussion by clearly differentiating SimNPO from SimPO, emphasizing unique contributions. Additionally, evaluating RMU on the TOFU and MUSE benchmarks would provide a more comprehensive understanding of SimNPO's performance. Clarifying the "mixture of Markov chains" experiment and explicitly addressing the repeated text phenomenon in the main body would enhance the paper's clarity. Lastly, we suggest refining the terminology around "forget quality" to avoid confusion and ensuring that figures, particularly Figure 1a and Figure 6, are clearer and more informative.