ID: XpmJNP8BVA
Title: Regularized Behavior Cloning for Blocking the Leakage of Past Action Information
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 5, 5, 7, 8, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 5, 3, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Past Action Leakage Regularization (PALR) to address the leakage of past action information in behavior cloning (BC). PALR aims to prevent BC agents from merely memorizing past actions by learning a representation of history that removes unnecessary observations through conditional independence. The authors utilize the Hilbert-Schmidt Conditional Independence Criterion (HSCIC) to measure this independence, which offers advantages over traditional information-theoretic metrics. The experiments demonstrate that PALR outperforms several baselines across various continuous control tasks. Additionally, the authors propose a general solution for offline imitation learning with offline high-dimensional action spaces, acknowledging the potential for future comparisons with methods like implicit behavioral cloning and diffusion policies. They provide experimental results applying their regularization method to the Decision Transformer and have expanded their evaluations to include pixel-based imitation tasks and additional baselines.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and presents a clear flow from problem definition to evaluation.  
- The use of HSCIC is well-justified, showing significant performance improvements in BC.  
- Comprehensive ablation studies are included, enhancing the understanding of the regularization's effectiveness.  
- The results indicate a strong correlation between HSCIC and agent performance, supporting the proposed method's validity.  
- The authors have convincingly addressed reviewer concerns and provided additional experimental results, enhancing the paper's soundness.  
- The inclusion of diverse evaluations, such as pixel-based tasks and comparisons with multiple baselines, strengthens the findings.

Weaknesses:  
- The contribution appears incremental, primarily introducing the HSCIC regularization term, with prior work already proposing similar formulations.  
- The experiments focus solely on state-based continuous control tasks, which do not adequately support the claims regarding POMDPs. More complex environments should be included for a robust evaluation.  
- There is a lack of comparison with non-regularization-based methods, limiting the scope of the findings.  
- Some experimental results are difficult to interpret, and the paper lacks a thorough discussion of limitations.  
- The paper lacks detailed descriptions of the experimental setup and model architecture for the CARLA environment.  
- Inconsistent terminology regarding the problems investigated may hinder clarity and coherence.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contribution by comparing PALR with advanced imitation learning algorithms, such as DAC and PWIL, to demonstrate its effectiveness in more complex scenarios. Additionally, expanding the experimental evaluation to include more challenging environments, such as visual imitation learning or robotic manipulation tasks, would strengthen the claims made. We also suggest addressing the interpretability of experimental results, particularly regarding the relationship between training data and HSCIC scores. Furthermore, we recommend providing a detailed description of the experimental setup, environment, and model architecture for CARLA. Finally, adopting a consistent nomenclature for the problem under investigation, specifically using "copycat problem" throughout the paper, could enhance clarity and coherence.