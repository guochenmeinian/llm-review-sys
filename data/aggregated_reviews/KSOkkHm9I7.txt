ID: KSOkkHm9I7
Title: Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass
Conference: NeurIPS
Year: 2024
Number of Reviews: 24
Original Ratings: 7, 4, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Superposed Decoding, a novel method for generating multiple drafts (k drafts) in a single inference pass using autoregressive models. The approach consists of two iterative steps: (1) performing inference on fused tokens with top-k sampling to produce k candidate tokens, and (2) extending each draft with one candidate token based on combined n-gram and LLM probability scores. The method shows significant improvements in generation perplexity, downstream task performance, and is preferred by human annotators, achieving results 2.44 times faster. The authors also propose a resetting mechanism to reduce repetitions in long drafts.

### Strengths and Weaknesses
Strengths:
- The method is innovative and addresses a task with practical applications and real-world relevance.
- Experiments are well-designed, covering text generation, question answering, and human evaluation, demonstrating the method's effectiveness in both quality and efficiency.

Weaknesses:
- The application is limited to text drafting and does not extend to code generation, which could benefit from this method.
- The problem scope may not have significant optimization potential, and the proposed method's performance in restricted scenarios lacks strong justification.
- The reliance on an additional n-gram model for coherence detracts from the method's elegance and raises concerns about its practical usage.

### Suggestions for Improvement
We recommend that the authors improve the justification for the proposed method's advantages over existing techniques, such as tree attention masks, which could achieve similar goals without the proposed superposition. Additionally, the authors should address the concerns regarding the n-gram model's dependency and its impact on coherence, possibly by providing more experiments to evaluate the method's performance without this additional step. Clarifying how diversity in generated drafts compares to other decoding methods would also strengthen the paper.