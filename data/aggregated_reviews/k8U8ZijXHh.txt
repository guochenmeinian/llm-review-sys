ID: k8U8ZijXHh
Title: PDF: Point Diffusion Implicit Function for Large-scale Scene Neural Representation
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 6, 6, 4, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for reconstructing large-scale scenes using a point cloud-based representation and a point cloud diffusion model for upsampling. The approach combines a Point-NeRF-style rendering for the foreground and a background rendering based on Mip-NeRF 360, achieving state-of-the-art performance in tested scenes. Extensive experiments validate the method's effectiveness on the OMMO dataset and five synthetic scenes from the BlendedMVS dataset, demonstrating superior performance in detailed texture reconstruction and realistic lighting effects compared to InstantNGP. The algorithm effectively limits the sampling region to the surface, enhancing reconstruction quality. The authors employ a uniform set of parameters across all scenes and plan to include a comprehensive evaluation of parameter ablation in the final manuscript.

### Strengths and Weaknesses
Strengths:
1. The use of a diffusion model for point cloud upsampling and completion is innovative and demonstrates compelling results.
2. The method achieves better reconstruction quality in large-scale scenes compared to existing NeRF baselines, supported by both qualitative and quantitative experiments.
3. Extensive validation on both real and synthetic datasets supports the effectiveness of the proposed approach.
4. The paper is well-organized, with clear visualizations that effectively illustrate the method's performance.
5. The authors acknowledge and plan to address the need for comprehensive parameter ablation evaluation.

Weaknesses:
1. The implementation details, particularly regarding the training dataset size for the diffusion model, are unclear, raising concerns about the model's performance in texture-less regions.
2. The combination of the diffusion model with the NeRF model lacks depth, appearing as a straightforward modification rather than a significant contribution.
3. The absence of comparisons with Point-NeRF and other upsampling methods limits the evaluation of the proposed method's effectiveness.
4. The manuscript lacks thorough explanations for figures and results, which diminishes context and clarity.
5. Long training times per scene and scene size limitations may affect the practical applicability of the method.
6. Some sections of the paper, including the explanation of background rendering and the influence of randomness in the diffusion model, lack clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the implementation details, particularly regarding the training dataset size and the training strategy for the diffusion model. Additionally, including a comparison with Point-NeRF and other point cloud upsampling methods would strengthen the evaluation. The authors should also address the concerns about reconstruction quality in texture-less regions and provide a more detailed explanation of the background rendering process. Furthermore, improving the explanations accompanying all figures and results in the main manuscript would provide necessary context. Finally, discussing the computational costs and training times in relation to existing methods could enhance the paper's practical relevance.