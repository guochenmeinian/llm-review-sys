ID: 0BfQT652sC
Title: Stochastic Multi-armed Bandits: Optimal Trade-off among Optimality, Consistency, and Tail Risk
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 7, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of the stochastic multi-armed bandit (MAB) problem, focusing on the trade-off between worst-case optimality, instance-dependent consistency, and light-tailed risk in policy design. The authors propose a novel policy, Successive Elimination with Random Permutation (SEwRP), which achieves optimal regret tail risk while balancing these properties. They provide theoretical guarantees and generalize their findings to non-stationary scenarios, demonstrating the interplay between these key factors.

### Strengths and Weaknesses
Strengths:  
- The paper contributes significantly to the understanding of the MAB problem by characterizing the relationships among worst-case optimality, instance-dependent consistency, and light-tailed risk.  
- The SEwRP policy introduces innovative bonus terms, achieving optimal regret and tail risk for various scenarios.  
- The authors effectively generalize their analysis to include non-stationary baseline rewards, enhancing the applicability of their findings.  
- The presentation is clear, with fluent arguments that facilitate understanding.

Weaknesses:  
- The reliance on theoretical analysis may limit the paper's impact; empirical validation within the main text is necessary to substantiate theoretical claims.  
- The practicality of the proposed algorithm remains uncertain, particularly in 'any-time' scenarios without prior knowledge of the time horizon $T$.  
- Some sections, such as Section 4, may not contribute significantly to the main arguments and could be moved to the appendices for clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by considering a tail-risk constraint first, then proposing an algorithm that meets this constraint with optimal regret guarantees. Additionally, extended discussions on the proofs and technical points would enhance understanding. We suggest removing Section 4 to focus on the main results and their proofs. Furthermore, incorporating a more extensive set of experiments in the main body, including comparisons with standard bandit algorithms, would provide a clearer picture of the algorithm's practical performance.