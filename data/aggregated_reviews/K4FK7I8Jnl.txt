ID: K4FK7I8Jnl
Title: MAG-GNN: Reinforcement Learning Boosted Graph Neural Network
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 4, 6, 6, 7, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 2, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MAG-GNN, a subgraph-based Graph Neural Network (GNN) method that employs reinforcement learning (RL) to identify discriminative subgraphs efficiently. The authors empirically evaluate MAG-GNN's performance on both synthetic and real-world datasets, demonstrating its effectiveness in reducing complexity while maintaining expressivity.

### Strengths and Weaknesses
Strengths:
- The main elements of the RL framework are well-explained.
- The empirical section includes clear and valuable research questions.
- The motivation for reducing complexity in subgraph GNNs is significant, and the combination of RL with subgraph GNNs is original.

Weaknesses:
- The writing lacks clarity and context, making it difficult to follow.
- The presentation of results is unclear, with vague table captions and insufficient detail on metrics.
- The paper lacks thorough comparisons with random policies and other subgraph GNN methods, which are crucial for validating MAG-GNN's performance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing and presentation, particularly in the results section, by providing clearer captions for tables and figures. Additionally, please include a thorough comparison with a random policy that uses the same architecture and parameter settings as MAG-GNN. It is essential to report the performance of the random baseline across all datasets considered. We also suggest including comparisons with the I-MLE approach from k-OSAN and other relevant subgraph GNNs, such as those by Bevilacqua et al. (2022) and Zhang et al. (2023). Furthermore, please clarify the notation used throughout the paper, especially in equations and figures, and consider adding an algorithm to illustrate the training procedure for MAG-GNN. Lastly, we encourage the authors to explore the training efficiency and runtime of MAG-GNN in relation to other methods, as well as provide visualizations of the subgraph structures selected by the RL agent.