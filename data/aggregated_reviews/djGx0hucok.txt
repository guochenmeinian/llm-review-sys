ID: djGx0hucok
Title: FedLLM-Bench: Realistic Benchmarks for Federated Learning of Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 6, 8, 7, 8, -1
Original Confidences: 5, 4, 5, 4, -1

Aggregated Review:
### Key Points
This paper presents four federated datasets aimed at benchmarking federated learning methods on large language models (LLMs). The datasets exhibit naturally occurring heterogeneity and address language-specific challenges in model training. The authors benchmark federated LoRA fine-tuning against various optimization methods and local training. However, the claim of being the first to provide realistic language modeling datasets for federated learning is contested, as similar datasets exist in the literature.

### Strengths and Weaknesses
Strengths:
- The proposed datasets are useful for federated learning (FL) research, particularly the multilingual dataset, which is underexplored.
- The benchmarks effectively compare various methods and include local training as a baseline, which is often missing in other works.
- The paper is well-structured, clearly written, and provides comprehensive empirical studies.

Weaknesses:
- The authors inaccurately claim to be the first to propose useful datasets for LLM + FL research, overlooking existing works.
- The datasets are relatively small due to filtering based on sample size, which reduces their heterogeneity and utility for cross-device FL research.
- The focus on breadth in algorithmic comparison leads to noisy results, suggesting a lack of nuanced tuning.

### Suggestions for Improvement
We recommend that the authors improve the following aspects:
1. Clarify how their datasets fit into the existing landscape of realistically heterogeneous datasets for FL + LLM research.
2. Explain the distinction between their dataset partitioning and that of other works, particularly regarding dataset-agnostic versus dataset-specific partitioning.
3. Reconsider the filtering criteria for dataset inclusion, allowing users to experiment with different sample sizes rather than imposing a cap.
4. Provide more detailed descriptions of the fine-tuning methods and baseline choices, including hyper-parameters.
5. Consider including perplexity as a metric for evaluating the datasets and discuss the implications of using different LLMs in their experiments.