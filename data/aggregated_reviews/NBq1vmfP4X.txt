ID: NBq1vmfP4X
Title: The Power of Hard Attention Transformers on Data Sequences: A formal language theoretic perspective
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 6, 6, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the expressive power of unique hard attention transformers (UHAT) through formal languages, particularly focusing on data sequences rather than traditional finite alphabets. The authors propose three main findings: (1) UHATs over data sequences with positional encodings belong to the circuit complexity class $TC^0$, while those without positional encodings can recognize non-regular languages; (2) UHATs can recognize all languages definable in an extension of linear temporal logic with unary predicates; and (3) the results highlight the increased computational expressiveness of transformers when applied to infinite alphabets.

### Strengths and Weaknesses
Strengths:
- **S1: Important Contribution**: The paper significantly advances the understanding of transformers' expressive power, particularly in the context of data sequences, which is a natural extension of existing research.
- **S2: Clarity**: The writing is clear for those familiar with the topic, effectively explaining concepts and proof ideas.
- **S3: Elegant Proofs**: The proofs linking UHATs to $TC^0$ are well-structured and utilize polynomial equations and polyhedral analysis effectively.

Weaknesses:
- **W1: Coverage of $TC^0$**: The characterization of UHATs in $TC^0$ is incomplete; a more precise delineation of UHATs' capabilities would enhance the paper.
- **W2: Accessibility**: The paper lacks accessibility for the broader machine learning community, making it feel more suited for a theoretical computer science audience.
- **W3: Practical Implications**: There is insufficient discussion on the practical implications of the theoretical findings for transformer design, which could enhance the paper's relevance.

### Suggestions for Improvement
We recommend that the authors improve the characterization of UHATs' power in relation to $TC^0$ to provide a more comprehensive understanding. Additionally, enhancing the paper's accessibility for a wider audience, perhaps by simplifying some technical jargon, would be beneficial. Finally, we suggest that the authors include a discussion on the practical implications of their theoretical analysis to better connect their findings to real-world applications in transformer design.