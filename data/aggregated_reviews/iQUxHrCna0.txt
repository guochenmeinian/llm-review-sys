ID: iQUxHrCna0
Title: UDON: Universal Dynamic Online distillatioN for generic image representations
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 3, 6, 6, 5, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Universal Dynamic Online distillatioN (UDON), a multi-teacher distillation method aimed at enhancing universal image representations. UDON utilizes a knowledge distillation strategy to transfer information from multiple domain-specific teacher models to a universal student model, incorporating a dynamic domain sampling strategy to balance domain contributions. The experimental results validate its effectiveness across various tasks, including instance recognition and retrieval.

### Strengths and Weaknesses
Strengths:  
- The introduction of multi-teacher distillation with a shared backbone is novel and addresses significant challenges in universal image representation.  
- The dynamic sampling method enhances learning efficiency and addresses training data distribution imbalances.  
- The paper is well-written, with clear descriptions of design and implementation, and extensive experiments demonstrate UDON's superiority over existing methods.  
- Many ablation studies provide insights into the contributions of different design choices.

Weaknesses:  
- The paper overlooks important works in embedding learning, particularly the Matryoshka representation learning (MRL), which limits its potential applications.  
- The evaluation lacks clarity, particularly in Table 1, and does not include recent state-of-the-art embedding models like SigLIP.  
- The rationale for UDON's superiority over the Universal Separate Classifier Training method (USC) is not sufficiently clear, particularly regarding the role of distillation.  
- There is no ablation study on critical hyperparameters such as temperature and embedding dimension, which should be explored.  
- The performance gains appear sensitive to the dataset used, with dynamic sampling negatively impacting some benchmarks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Table 1 by providing clearer definitions for terms like "Off-the-shelf" and "Specialist+Oracle." Additionally, the authors should address the omission of significant works in embedding learning, particularly MRL, to enhance the paper's relevance. We suggest conducting an ablation study on the temperature and embedding dimension to explore their impacts. Furthermore, the authors should clarify the advantages of their distillation approach over USC, ensuring that the explanation aligns with the paper's claims. Lastly, we encourage the authors to investigate the scalability of their approach and its performance across various datasets to strengthen their conclusions.