ID: YEtstXIpP3
Title: Model-Free Active Exploration in Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 5, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 2, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a model-free approach to exploration in reinforcement learning (RL) that derives a new lower bound on the number of samples required to identify a near-optimal policy. The authors propose exploration strategies applicable to both tabular and continuous Markov Decision Processes (MDPs), validated through experiments that demonstrate their competitiveness against existing methods. The authors also introduce tighter PAC bounds for model-free RL, leading to practical exploration methods utilizing ensembles of Q-values.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with a clear presentation of theoretical results and empirical validation across various RL domains.
- The proposed methods adapt automatically to specific problems, showcasing relevance and importance in the field.
- The theoretical contributions transform the original lower bound into a more manageable form, enhancing practical applicability.

Weaknesses:
- The paper lacks clarity regarding the use of ensembles in the proposed algorithms, which may lead to initial disappointment for readers expecting a purely theoretical approach.
- There is insufficient discussion on the runtime of the ensemble methods compared to model-based approaches, raising questions about their efficiency.
- The exploration strategy does not include experiments in complex environments, limiting its applicability and robustness.
- The theoretical results are dense and may benefit from more intuitive explanations to aid reader comprehension.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by explicitly stating the use of ensembles in the introduction and discussing their runtime compared to model-based methods. Additionally, including experiments in more complex environments, such as Montezuma's Revenge, would strengthen the validation of the proposed methods. We suggest enhancing the readability by providing intuitive explanations for the theoretical results and ensuring that each theorem or derivation in the main text references the corresponding appendix section for clarity. Lastly, a more thorough discussion of the limitations and assumptions of the proposed approach would be beneficial.