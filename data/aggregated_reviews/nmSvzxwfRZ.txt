ID: nmSvzxwfRZ
Title: FinePrompt: Unveiling the Role of Finetuned Inductive Bias on Compositional Reasoning in GPT-4
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for transferring task-specific inductive biases from fine-tuned models to prompts, aiming to enhance the performance of large language models like GPT-4 on compositional reasoning tasks. The authors propose prompt templates that facilitate this transfer and demonstrate competitive zero-shot and few-shot performance against existing prompting strategies. The experimental results on the MuSiQue and DROP datasets indicate that some proposed prompts outperform traditional elicitive prompting approaches.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a significant challenge in natural language processing, specifically compositional reasoning, and proposes a novel approach to improve GPT-4's performance.  
- It provides experimental results that validate the effectiveness of the FinePrompt method, offering valuable insights into various prompt strategies.  
- The work bridges the gap between fine-tuning and prompting, which is crucial given the current limitations of large language models.

Weaknesses:  
- The paper lacks clarity regarding the specific prompt-infusion methods and how inductive biases are extracted from fine-tuned models.  
- There is insufficient comparison between different FinePrompt configurations and original fine-tuned models, as well as with current state-of-the-art methods.  
- The evaluation lacks rigor, particularly in exploring alternative approaches to prompt tuning, which hinders a comprehensive understanding of the gains and trade-offs.

### Suggestions for Improvement
We recommend that the authors improve clarity by detailing how fine-tuned models are differentiated and how inductive biases are extracted. Specifically, address the following questions:  
A. How are inductive biases obtained? Are they task-specific or instance-specific?  
B. Does the pattern obtained for the numerical reasoning task vary with different few-shot examples?  
C. In Section 2.3, clarify how nodes within the text are identifiedâ€”are they manually identified or part of the dataset?  
D. Explain how pipeline-infused prompts differ from existing strategies like CoT and self-ask.  
E. Clarify whether fineprompting is done on GPT-4 out-of-the-box or a fine-tuned version.  
F. Elaborate on how this method is more reproducible than others.  

Additionally, we suggest including a table that delineates which model/dataset corresponds to each of the three prompt infusion strategies introduced in the paper.