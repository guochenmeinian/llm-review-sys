ID: qgv56R2YJ7
Title: Diffusion Self-Guidance for Controllable Image Generation
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 5, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for detailed image editing and controlled image synthesis using large-scale text-to-image diffusion models, introducing the concept of self-guidance. The authors propose that self-guidance utilizes the internal representations of the denoiser neural network to guide image generation, allowing for modifications in object size, shape, position, and appearance without noticeable artifacts. This technique does not require additional training or paired data, making it a powerful tool for image editing. The authors validate their method primarily through qualitative examples, demonstrating its effectiveness across various editing tasks.

### Strengths and Weaknesses
Strengths:
- The introduction of the self-guidance technique enables advanced image editing with minimal requirements, achieving fine control over object properties while introducing almost no artifacts.
- The paper is well-written, clear, and presents a novel approach that is conceptually simple and effective.
- The method supports simultaneous manipulation of multiple image attributes and allows for the editing of real images, showcasing its versatility.

Weaknesses:
- Editing is limited to objects and words present in the text prompt, restricting fine-grained control over specific object parts.
- The method involves numerous hyperparameters that require manual tuning, with a lack of detailed ablation studies on their effects.
- The heuristic nature of defining edits based on attention/feature maps could benefit from a more principled approach.
- The validation is primarily conducted on pixel-space models, raising questions about its applicability to latent diffusion models like Stable Diffusion.

### Suggestions for Improvement
We recommend that the authors improve the discussion of limitations, particularly regarding the consistency of edits and feature entanglement when multiple controls are applied. A thorough analysis of these issues should be included in the main text. Additionally, we suggest conducting ablation studies on hyperparameters to visualize their effects on editing quality. It would also be beneficial to explore the applicability of the self-guidance method to latent diffusion models and provide examples using Stable Diffusion to calibrate performance. Finally, we encourage the authors to acknowledge and cite related techniques such as "paint-with-words" to contextualize their contributions.