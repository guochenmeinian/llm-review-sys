ID: IfpNsorodK
Title: PFDiff: Training-free Acceleration of Diffusion Models through the Gradient Guidance of Past and Future
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 4, 5, 6, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents PFDiff, a training-free denoising method aimed at accelerating sampling speed in diffusion models. PFDiff utilizes gradients from past time steps to update intermediate states, effectively reducing unnecessary neural function evaluations (NFEs) while addressing discretization errors. Experimental results indicate that PFDiff improves classic samplers without requiring training computation, demonstrating effectiveness across various pre-trained diffusion models.

### Strengths and Weaknesses
Strengths:
1. The training-free approach to reducing discretization errors in diffusion models is both attractive and practical.
2. The motivation for using previous gradients to guide current sampling is intuitively plausible, and the method is technically sound.
3. The presentation is clear, with readable figures, and the experimental results show significant acceleration, particularly in the low NFE regime.

Weaknesses:
1. The theoretical analysis appears naive; further explanation is needed regarding the utility of previous gradients in guiding current sampling, especially considering varying noise levels.
2. The contribution is limited as many prior works have explored using previous gradients for improving sampling speed.
3. The justification for using future gradient information is flawed, as the mean value theorem does not support the authors' claims regarding its superiority over current gradients.
4. The method requires extensive parameter tuning, which can be computationally expensive and case-specific, limiting its practicality.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis by providing a more robust justification for the effectiveness of using previous gradients in guiding current sampling. Additionally, we suggest including more experiments on ImageNet 256, covering both conditional and unconditional settings, and comparing PFDiff with the DEIS sampler. Clarifying the relationship between the proposed method and distillation models would also enhance the paper. Finally, testing additional metrics such as recall and precision would strengthen the evaluation of the method's performance.