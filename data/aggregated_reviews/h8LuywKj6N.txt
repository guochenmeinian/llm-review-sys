ID: h8LuywKj6N
Title: GUI-World: A Dataset for GUI-Orientated Multimodal Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 7, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GUI-World, a benchmarking dataset comprising over 12,000 GUI videos aimed at evaluating and enhancing the capabilities of multimodal large language models (MLLMs) in understanding graphical user interfaces (GUIs). The authors propose a model, GUI-Vid, trained on this dataset to demonstrate potential improvements in GUI understanding tasks. Additionally, the paper discusses fine-tuning video language models (Video-LLaVA) using limited computational resources, specifically two A800 GPUs, and reports challenges with LoRA fine-tuning, including catastrophic forgetting in model checkpoints. The authors emphasize the significance of performance gaps in benchmark results, particularly in GUI understanding tasks, and the need for clearer interpretation of these results.

### Strengths and Weaknesses
Strengths:
- The dataset comprises videos, enhancing its relevance as GUI agents increasingly interact with dynamic content.
- The extensive size of the dataset reflects significant effort in data collection across various platforms.
- The dataset is not tied to specific applications, making it broadly applicable to general GUI interactions.
- The authors successfully conducted supplementary experiments that validate the use of Automated Keyframe Identification Methods, enhancing the coherence of the paper.
- The engagement with reviewers and responsiveness to feedback demonstrates a commitment to improving the work.

Weaknesses:
- The paper lacks clarity on how the benchmark relates to the actual performance of models as agents, raising questions about the practical implications of the results.
- The multiple-choice format may allow large models to guess answers without thorough understanding, potentially undermining the assessment's rigor.
- The necessity of a mixed benchmark combining different GUI types is questioned, as it may dilute the effectiveness of model evaluation.
- Limited computational resources hindered the ability to perform full-scale fine-tuning, impacting the depth of the study.
- The interpretation of performance gaps in benchmarks, particularly in GUI tasks, remains unclear and requires further elucidation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how the benchmark correlates with the ability of models to act as generalist agents, addressing whether a model's performance in the benchmark translates to real-world task execution. We suggest revisiting the multiple-choice format to ensure it accurately assesses knowledge and understanding rather than allowing for guessing. We encourage the authors to clarify the rationale behind mixing different GUI types in the benchmark and consider whether separate models for each type would yield more meaningful evaluations. Additionally, we recommend that the authors improve the clarity of their interpretations regarding the significance of performance gaps in benchmarks, especially for GUI understanding tasks. We suggest exploring fine-tuning on more advanced foundational MLLMs to better showcase the dataset's value. Finally, enhancing the discussion phase with detailed explanations of automated Keyframe Identifier methods and related multimodal LLM fine-tuning papers would strengthen the paper further.