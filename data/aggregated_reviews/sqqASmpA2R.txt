ID: sqqASmpA2R
Title: Stable and low-precision training for large-scale vision-language models
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 8, 6, 6, 6, 3, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, 2, 5, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents methods to enhance the training of large vision-language models, specifically through the introduction of a new linear layer for int8 quantized training called SwitchBack and a novel optimizer, StableAdamW, which combines AdamW with update clipping from AdaFactor. These innovations aim to achieve faster and more stable training, with reported speedups of 13-25% while maintaining performance.

### Strengths and Weaknesses
Strengths:
- The SwitchBack method effectively utilizes 8-bit precision for the first two matrix multiplies, significantly accelerating training without sacrificing performance.
- The StableAdamW optimizer addresses known instabilities in training large vision-language models, potentially impacting future research positively.
- The paper provides detailed comparisons and results, contributing valuable insights to the community.

Weaknesses:
- The generalizability of the results to downstream tasks, such as object detection or semantic segmentation, is not sufficiently discussed, raising questions about the applicability of SwitchBack and StableAdamW in other transformer architectures.
- The complexity of the proposed quantization method may not justify the relatively modest speedup, and further exploration of remaining bottlenecks is warranted.
- Concerns about the novelty of the SwitchBack approach and the lack of methodological differences between StableAdamW and AdaFactor have been raised.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the generalizability of their methods to downstream tasks and other transformer architectures. Additionally, providing more details on the complexity versus speedup trade-off of the quantization method would enhance the paper's clarity. It would also be beneficial to address the novelty concerns regarding SwitchBack and StableAdamW by comparing them more explicitly with existing methods in the literature.