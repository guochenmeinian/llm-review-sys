ID: eWiGn0Fcdx
Title: Exploring Token Pruning in Vision State Space Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 6, 5, 6, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a token pruning method for vision state space models (SSMs) aimed at enhancing computational efficiency while preserving performance. The authors identify that traditional token pruning techniques designed for vision transformers (ViTs) fail in SSMs due to the disruption of sequential token positions. To address this, they propose a pruning-aware hidden state alignment method to stabilize the neighborhood of remaining tokens and a tailored token importance evaluation method. Extensive experiments demonstrate the method's effectiveness, achieving significant computation reduction with minimal performance impact, exemplified by 81.7% accuracy on ImageNet with a 41.6% reduction in FLOPs for pruned PlainMamba-L3.

### Strengths and Weaknesses
Strengths:
- The proposed method effectively stabilizes token neighborhoods, maintaining model performance post-pruning.
- The tailored importance evaluation method guides the pruning process effectively for SSM-based models.
- Comprehensive experiments validate the method's efficacy, achieving notable computation reductions with minimal performance loss.
- The paper is well-organized, with clear motivation and extensive visualizations supporting the findings.

Weaknesses:
- The method is limited to plain, non-hierarchical SSM-based models, raising questions about its generalizability to hierarchical variants like VMamba.
- The comparison is primarily with EViT, a baseline from 2022; more recent pruning methods should be considered for a comprehensive evaluation.
- There is a lack of clear explanation regarding the effectiveness of the pruning-aware hidden state alignment strategy, particularly concerning the role of pruned tokens.
- The proposed token importance evaluation lacks justification, particularly regarding the choice of a clipped threshold and its implications for performance.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of the proposed method by exploring its application to hierarchical SSM variants. Additionally, we suggest incorporating comparisons with more recent pruning methods to strengthen the evaluation. The authors should provide a clearer explanation of the pruning-aware hidden state alignment strategy, particularly addressing how changes in indices lead to performance improvements. Furthermore, we encourage the authors to elaborate on the rationale behind the token importance evaluation method, including the choice of threshold and its impact on the results. Lastly, including actual running latency data would enhance the practicality assessment of the proposed methods.