ID: czwZnNf60r
Title: Exploring Diverse In-Context Configurations for Image Captioning
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 6, 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a case study on prompt design in generative visual language models for image captioning tasks. The authors investigate four methods for image selection and four for caption assignment to create contextually relevant learning samples. Utilizing OpenFlamingo, they conduct experiments to assess the optimal integration of images and captions in the Visual Language Model (VLM). Key findings include: (1) multi-modal mutual synergy significantly influences model performance; (2) caption descriptiveness and language patterns affect performance variably, with simpler patterns yielding better results when images compensate for descriptiveness; and (3) excessive similarity between in-context images and test images can lead the VLM to rely on in-context captions rather than learning to generate captions.

### Strengths and Weaknesses
Strengths:
- The paper provides a rigorous experimental investigation into prompt design, contributing valuable insights into in-context learning for image captioning.
- It explores a distinct aspect of the research questions, yielding findings that align with prior works while addressing more complex tasks.
- The comprehensive experiments demonstrate the impact of different sampling strategies on performance.

Weaknesses:
- The writing is often challenging to comprehend, particularly in specific sections, and the use of numerous abbreviations complicates readability.
- The paper relies solely on OpenFlamingo, limiting the generalizability of the findings to other models.
- Some experimental claims lack sufficient backing, particularly regarding the effectiveness of simpler caption patterns and the implications of image similarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, especially in lines 141-150, by rephrasing or incorporating visual aids. Additionally, consider swapping sections 4.2.1 and 4.2.2 to align with the typical image selection pipeline. It would be beneficial to confirm the accuracy of claims regarding model performance comparisons and to reduce the use of abbreviations for better readability. Expanding figure captions and including more elaborate explanations of the methods would enhance comprehension. Furthermore, we suggest exploring additional visual language models, such as FROMAGe, and conducting ablations with smaller language models to assess the robustness of the findings across different configurations.