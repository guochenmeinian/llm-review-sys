ID: Ogw1sSo9FP
Title: TEG-DB: A Comprehensive Dataset and Benchmark of Textual-Edge Graphs
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 6, 6, 6, -1, -1, -1
Original Confidences: 3, 5, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TEG-DB, a novel dataset and benchmark collection designed to address the limitations of existing Text-Attributed Graphs (TAGs) by incorporating rich textual annotations on both nodes and edges. The authors propose a standardized pipeline for data preprocessing, loading, and evaluation, facilitating extensive benchmarking across various domains, including citation, social, and e-commerce graphs. The introduction of TEG-DB is positioned as a significant advancement in graph representation learning, encouraging collaboration within the research community.

### Strengths and Weaknesses
Strengths:
- The originality of TEG-DB as the first open dataset with rich textual-edge features is noteworthy.
- The comprehensive benchmarking across diverse domains enhances the applicability of the research.
- The clear writing and structured presentation effectively convey the research objectives and significance.

Weaknesses:
- The paper lacks sufficient explanation of results, particularly in Sections 5.2 and 5.3, where performance differences among methods are not adequately discussed.
- There is an overuse of numerical data, which can overwhelm readers; a more concise presentation would improve clarity.
- The title of Section 4.3 suggests adaptation of existing methods, but concrete solutions are not proposed, leading to confusion.

### Suggestions for Improvement
We recommend that the authors improve the explanation of results in Sections 5.2 and 5.3 by providing insights into why certain methods perform better or worse. Additionally, consider summarizing numerical data to enhance readability and reduce redundancy. It is crucial to clarify the adaptation of existing methods in Section 4.3 by proposing concrete solutions to the challenges identified. Furthermore, we suggest normalizing the dataset formats for ease of use, adding more baseline models such as GCN, GAT, and RoBERTa, and providing a clearer definition of LLM as Predictor. Lastly, addressing inconsistencies between observations and experimental results will strengthen the manuscript's credibility.