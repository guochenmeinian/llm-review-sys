ID: sdlh4gVOj8
Title: On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling and Beyond
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 7, 5, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel notion of data diversity that enhances existing conditions in offline reinforcement learning (RL). The authors develop algorithms based on value sampling, regularized optimization, and posterior sampling, demonstrating that these can achieve comparable sample efficiency. The work also introduces a model-free posterior sampling algorithm for offline RL, contributing to the understanding of sample-efficient learning from historical datasets. Furthermore, the authors clarify that the definition of $\hat{\pi} = (\hat{\pi}_1, \ldots, \hat{\pi}_H)$, where $\hat{\pi}_h(a_h|s_h) = \frac{1}{T} \sum_{t=1}^T \pi^t_h(a_h|s_h)$, is inherently Markovian. They note that in the example, $\pi^t_h(a^t | s) = 1$ for all $t = 1,2$ and $h = 1,2,3$, leading to $\hat{\pi}_h(a_h = a^t | s_h = s) = 0.5$ for all $t = 1,2$ and $h = 1,2,3$.

### Strengths and Weaknesses
Strengths:
1. The new data diversity condition is general and tighter than previous conditions, potentially inspiring future research.
2. The writing is clear, providing a well-structured presentation that enhances comprehensibility.
3. The paper effectively connects offline RL to transfer learning, highlighting its relevance in the broader context of function approximation.
4. The authors provide a clear mathematical clarification regarding the Markovian nature of their proposed definition.
5. They acknowledge and appreciate the reviewer's efforts in addressing the confusion.

Weaknesses:
1. There are typos and inconsistencies in notation, such as the use of $p_{0,h}$ and the definitions of $f_h$ and $g_h$ in Algorithms 2 and 3.
2. Concerns exist regarding the practical performance and time complexity of the posterior sampling algorithms, which require further discussion.
3. The comparison with state-of-the-art (SOTA) algorithms, particularly LCB-based algorithms, may not be accurate, as recent literature suggests improved dependencies on $d$.
4. The example presented in the review is criticized for being confusing and not supporting the conclusions drawn.
5. There are miscalculations identified in the example that detract from its validity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of notation throughout the paper, particularly addressing the inconsistencies noted in the algorithms. Additionally, we suggest expanding the discussion on the practical issues related to the performance and time complexity of the posterior sampling algorithms. It would also be beneficial for the authors to ensure accurate comparisons with recent SOTA algorithms and to clarify the implications of their findings regarding the Markovian property of the proposed policies. Furthermore, we recommend that the authors improve the clarity of the example used in the discussion to ensure it accurately supports their conclusions and address the identified miscalculations to enhance the overall presentation of the paper.