ID: cB0BImqSS9
Title: Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 8, 8, 8, -1, -1, -1
Original Confidences: 3, 5, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Monarch Mixer (M2), a new neural network layer that operates efficiently on modern GPUs and demonstrates strong performance against state-of-the-art models across various benchmarks. The layer utilizes second-order Monarch matrices, which combine permutation and block diagonal matrices to represent dependencies across features and temporal dimensions. The authors propose a novel initialization method for these matrices, enabling their application in causal language modeling while maintaining computational efficiency. Performance comparisons are made against transformer and fully connected layers in language and image classification tasks.

### Strengths and Weaknesses
Strengths:  
The paper makes a significant contribution by addressing the computational complexity of neural networks from both sequence length and feature dimension perspectives. The discussion on runtime performance factors on modern GPUs is particularly valuable. The proposed M2 layer achieves superior performance with fewer trainable parameters and demonstrates potential as a competitive architectural inductive bias, similar to attention mechanisms.

Weaknesses:  
The paper is somewhat difficult to read without prior knowledge of the referenced work [7], particularly in Section 4, which is dense. Additionally, the evaluation is limited to transformer tasks, neglecting relevant speech applications. There is also a lack of comprehensive experiments in non-causal language modeling and image classification, and the paper does not provide sufficient details on model configurations and comparisons.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by simplifying dense sections, especially Section 4, to enhance readability for those unfamiliar with [7]. Explicitly state whether the expressivity of M increases with order p and clarify if M can express any dense matrix. Additionally, provide a more intuitive explanation of the initialization for the causal scenario and ensure that the experiments for non-causal language modeling and image classification are more comprehensive, considering learnable Monarch matrices. Finally, include comparisons of inference latency and performance metrics with smaller parameter configurations of BERT and provide detailed architecture information for BERT-s and ViT-s.