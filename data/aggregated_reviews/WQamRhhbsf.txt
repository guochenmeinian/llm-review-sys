ID: WQamRhhbsf
Title: Impact of Co-occurrence on Factual Knowledge of Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the co-occurrence bias in large language models (LLMs), specifically how such biases affect the models' ability to answer factual questions correctly. The authors demonstrate that LLMs tend to favor answers with higher co-occurrence statistics over the correct answers, a bias that persists across models of varying sizes. They conduct a correlation study to show that as the co-occurrence of subject and object decreases, accuracy also declines. The paper explores two mitigation strategies: debiased fine-tuning, which yields limited improvements, and knowledge editing, which shows promise but may require careful handling to avoid unintended changes.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant and timely issue regarding LLMs' performance as knowledge bases.
- The related work section is comprehensive and informative.
- The writing is generally clear, and the methodology for studying co-occurrence bias is novel.

Weaknesses:
- The exact claims regarding the bias are unclear; while correlation is established, it does not definitively prove that co-occurrence statistics lead to incorrect predictions.
- The results of the mitigation strategies are not robust, and the section on these strategies is relatively brief and lacks methodological insights.
- Some claims are insufficiently substantiated, and additional clarity is needed on the relationship between co-occurrence and memorization.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims regarding the co-occurrence bias and provide more substantial evidence to demonstrate how surface statistics interfere with model predictions. Specifically, we suggest examining cases where the model is expected to answer correctly based on training data prevalence and showing error patterns related to co-occurrence. Additionally, we encourage the authors to elaborate on the existing literature to clarify their contributions, particularly regarding fine-tuning and its expected effects. Finally, we advise including quantitative results over the entire dataset to support claims about co-occurrence and model performance, as well as providing a more formal definition of the filtering method in Section 6.1.