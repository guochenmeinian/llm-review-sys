ID: catfRXDWcb
Title: HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 7, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a high-quality dataset for human image animation, combining crafted real-world and synthetic data, and introduces a rule-based camera trajectory generation method for precise camera motion annotation. The dataset includes 2,300 copyright-free 3D avatar assets, enhancing existing resources. The authors propose a benchmark for 2D human animation generation with moving cameras, demonstrating superior performance with their model trained on this dataset.

### Strengths and Weaknesses
Strengths:
- The motivation for the dataset is sound, addressing the challenges of obtaining high-quality camera motions from real-world data.
- The dataset features high resolution and a substantial volume of data, as evidenced in Table 2.
- The paper is well organized, with comprehensive references and a clear presentation of the dataset's construction and implementation.

Weaknesses:
- The exploration of synthetic data advantages is limited; the authors could consider outputting additional annotations like 2D and 3D pixel motion trajectories.
- Missing detailed statistics about the synthetic data, such as average video clip duration and motion styles, hinder a complete understanding of the dataset.
- The rendering quality of some synthetic data (Anime) is subpar, affecting its utility for performance.

### Suggestions for Improvement
We recommend that the authors improve the clarity and detail of the data collection and annotation pipeline, particularly regarding the Internet video filtering process and human pose fitting. Additionally, providing metrics related to the diversity and distribution of both real-world and synthetic datasets would enhance the paper's rigor. The authors should also consider improving the rendering quality of the synthetic dataset and explore generating more diverse camera trajectories. Finally, a dedicated dataset page with comprehensive documentation and a download link for all data would significantly enhance the work's impact.