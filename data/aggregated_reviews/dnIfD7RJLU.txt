ID: dnIfD7RJLU
Title: GEM: Gestalt Enhanced Markup Language Model for Web Understanding via Render Tree
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to training a pre-trained language model for understanding markup language-based text, particularly HTML. The authors propose two pre-training tasks inspired by Gestalt psychology: Binary Textual Style Prediction (predicting style similarity between HTML nodes) and Proximate Note Prediction (predicting proximity between nodes). These tasks leverage visual appearance information to enhance the model's performance on downstream tasks such as web question answering and information extraction. The results indicate that the Gestalt-inspired pre-training methods improve downstream performance.

### Strengths and Weaknesses
Strengths:
- The proposed method enhances the model's ability to understand HTML text, which is increasingly relevant for applications like web browsing.
- The clarity of the paper's exposition effectively communicates the motivation and benefits of the approach.
- Ablation studies demonstrate the importance of the proposed pre-training tasks in improving performance.

Weaknesses:
- The performance gain is modest compared to MarkupLM, particularly in web information extraction tasks.
- The paper lacks detailed comparisons of pre-training performance metrics, such as loss or perplexity, against existing models like MarkupLM or RoBERTa.
- There is insufficient discussion on webpage preprocessing, data collection methods, and sampling strategies for training.
- The paper does not adequately analyze error patterns or provide examples of where the approach may fall short.

### Suggestions for Improvement
We recommend that the authors improve the clarity of webpage preprocessing details, including how long webpages are truncated into sub-pages. Additionally, the authors should clarify the selection criteria for the 100k pages in the pre-training corpus and address the sampling methods for node pairs in the pre-training tasks. We suggest conducting further ablation studies specifically on the Information Extraction task to evaluate the model's performance in discerning correct elements. Finally, we encourage the authors to include an analysis of error patterns to better understand the limitations of their approach.