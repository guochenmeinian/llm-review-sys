ID: EhhPtGsVAv
Title: f-Policy Gradients: A General Framework for Goal-Conditioned RL using f-Divergences
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 4, 6, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for goal-conditioned reinforcement learning (GCRL) that minimizes the f-divergence between the agent's state visitation distribution and the goal distribution. The authors demonstrate that this approach leads to the optimal policy and discuss its theoretical foundations and empirical evaluations across various environments, including grid worlds and mazes. Additionally, the paper introduces a novel approach to imitation learning by utilizing f-divergence as a regularizer with an existing reward function, distinguishing it from previous methods. The authors prove that minimizing the f-divergence results in the optimal policy and introduce a mathematically stable objective that does not rely on stationary rewards during policy optimization. The learning signals encourage exploration and do not assume coverage between the goal distribution and the agentâ€™s visitation, addressing limitations in discriminative methods.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow, with clear motivation and thorough analysis.
2. The novel contribution of utilizing f-divergence in both GCRL and imitation learning is noteworthy, and the theoretical results indicate that minimizing this divergence yields optimal policies.
3. Empirical evaluations show that the proposed f-Policy Gradients outperform standard policy gradient methods and baseline shaping rewards in the tested environments.
4. The mathematical proof provided supports the claim of achieving optimal policy through f-divergence minimization, and the proposed objective's stability is a significant improvement over previous methods that assume stationary rewards.

Weaknesses:
1. The framework's effectiveness in strictly sparse-reward settings is questionable, as the authors acknowledge limitations when the goal distribution becomes a Dirac distribution.
2. The advantages of f-PG over traditional GCRL methods remain unclear, particularly in dense-reward scenarios where standard policy gradient methods also achieve optimal policies.
3. The experimental environments are relatively simple, raising concerns about the generalizability of the results to more complex tasks.
4. The novelty of the approach may not be sufficiently highlighted, potentially leading to underappreciation of its contributions, and there may be a lack of clarity in how the proposed method compares to existing techniques beyond the mention of Dual-DICE.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the advantages of the proposed framework, particularly in sparse-reward settings, and provide a more detailed comparison with existing shaping reward methods. Additionally, the authors should consider conducting experiments in more complex environments to validate the robustness of their approach. It would also be beneficial to clarify the role of the introduced parameters, such as $\gamma$, and to address the limitations of using f-divergence as a metric. Finally, including a limitations section would enhance the paper's comprehensiveness and ensure that the novelty of the approach is clearly articulated to strengthen the paper's impact.