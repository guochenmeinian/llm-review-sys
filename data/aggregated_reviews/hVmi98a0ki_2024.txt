ID: hVmi98a0ki
Title: Optimizing Automatic Differentiation with Deep Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 7, 7, 7, 5, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method called AlphaGrad for optimizing computational graphs derived through automatic differentiation (AD) algorithms using deep reinforcement learning (RL). The authors formulate the optimization of a computation graph as a single-player game where an AlphaZero agent aims to minimize the number of required multiplications. They propose that their agent learns to recognize recurring connectivity patterns across various graphs, enhancing its ability to generalize and optimize elimination procedures. The paper demonstrates that AlphaGrad can improve over preexisting forward-mode AD methods in various domains, although it shows limited improvements in deep learning tasks that utilize reverse-mode AD. The authors also introduce Graphax, an AD interpreter for JAX, facilitating the optimization and execution of computational graphs discovered from elimination orders. They highlight the importance of understanding the learning and generalization within the same graph, suggesting that analyzing elimination orders and attention maps could yield insights into the optimization process.

### Strengths and Weaknesses
Strengths:
- The proposed approach demonstrates benefits across tasks and serves as a new benchmark for RL applications in AD.
- The paper is well-organized and provides thorough experimental details, ensuring reproducibility.
- The introduction of Graphax allows for practical application of the method, enhancing its accessibility.
- The approach effectively leverages reinforcement learning to optimize vertex elimination orders.
- The authors provide a clear rationale for the need to train on multiple graphs to enhance generalization.
- The discussion on the evolving nature of the computational graph during training is insightful.

Weaknesses:
- The method primarily optimizes the number of multiplications, which does not capture the full complexity of runtime factors like memory access patterns.
- The approach requires separate training for each computational graph, making it computationally expensive and potentially prohibitive in practice.
- Results for reverse-mode AD, particularly in deep learning contexts, do not show significant improvements, limiting the method's applicability.
- The evaluation methodology in Table 1 lacks clarity regarding whether it assesses fixed input instances or a distribution of inputs.
- The explanation of the learning process could be more explicit to avoid confusion about the distinction between search and learning.

### Suggestions for Improvement
We recommend that the authors improve the generalization performance of the approach to reduce the need for separate training runs for each graph. Additionally, we suggest exploring optimization metrics beyond the number of multiplications, such as execution time or memory usage, to enhance practical applicability. Clarifying the representation of actions and rewards in the RL framework could also strengthen the paper. We recommend improving the clarity of the evaluation methodology in Table 1 by explicitly stating whether each row corresponds to fixed input instances or a distribution of inputs. Furthermore, we suggest that the authors explore the actual elimination orders and correlate them with executed functions to gain deeper insights into the optimization process. Implementing tools to analyze attention maps could further enhance understanding of the graph components that influence elimination order optimization. Finally, addressing the limitations regarding the performance of the method on larger and more complex computational graphs would provide a more comprehensive evaluation.