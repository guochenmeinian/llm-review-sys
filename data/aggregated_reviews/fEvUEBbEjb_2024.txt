ID: fEvUEBbEjb
Title: TARP-VP: Towards Evaluation of Transferred  Adversarial Robustness and Privacy on Label  Mapping Visual Prompting Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 7, 7, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the adversarial robustness and privacy aspects of models trained using the Language Model Visual Prompting (LM-VP) technique, a novel approach that has not been previously explored. The authors propose that LM-VP models trained with transferred adversarial training (AT) demonstrate advantages in AI security, achieving a favorable trade-off between adversarial robustness and privacy protection across various pre-trained models. The study highlights the challenges of evaluating these models under different attack scenarios, particularly concerning membership inference attacks (MIAs). Additionally, the authors clarify that they do not claim transfer adversarial attacks should replace white-box attacks for evaluating adversarial robustness, emphasizing the importance of studying transferred adversarial robustness for consistent evaluation results.

### Strengths and Weaknesses
Strengths:
- The paper connects the significant topics of privacy and prompt learning, presenting high novelty in its findings.
- It contributes to the understanding of transferred adversarial training, which is crucial for domain adaptation and label mapping.
- The authors demonstrate that applying transferred AT to LM-VP models leads to improved adversarial robustness and privacy protection.
- The ablation study on prompt generation types indicates a thorough examination of the topic.
- The authors provide clear clarifications regarding the relationship between transfer adversarial robustness and privacy, as well as the effects of different pre-trained models on robustness.
- The paper addresses a significant gap in understanding the trade-offs between adversarial robustness and privacy in LM-VP models.

Weaknesses:
- The discussion on the MIA success rate lacks clarity regarding its significance and whether it represents an improvement over a baseline.
- The experimental setup does not clearly specify which MIA attacks were utilized, leading to potential confusion.
- The paper does not adequately address the theoretical foundations and interpretability of the LM-VP models, limiting the understanding of their security implications.
- The evaluation primarily relies on empirical findings, which may not fully capture the theoretical relationships between adversarial robustness and privacy.
- The current version does not adequately address white-box adversarial robustness, which may lead to confusion regarding the overall robustness evaluation.
- The title may imply a broader focus on adversarial robustness than what is covered in the paper.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the MIA success rate discussion by providing a baseline for comparison. Additionally, the authors should specify the MIA attacks used in the experimental setup to enhance transparency. We suggest incorporating an ablation study on different epsilon sizes to provide insights into the limitations of the adversarial attack setup. Furthermore, we encourage the authors to strengthen the theoretical support and interpretability of their analysis to deepen the understanding of the security implications of LM-VP models. We also recommend revising the title to better reflect the focus on transferred adversarial robustness. Lastly, we suggest incorporating an analysis of white-box adversarial robustness in the revised manuscript, including the impact of different pre-trained models and the failure of standard adversarial training on LM-VP models.