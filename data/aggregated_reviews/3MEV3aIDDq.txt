ID: 3MEV3aIDDq
Title: Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a visual question answering (VQA) dataset, INFOSEEK, focusing on visual info-seeking questions. The authors evaluate state-of-the-art models on this dataset, analyzing their performance in answering intricate questions. The dataset comprises over 1 million visual information-seeking QA pairs, leveraging the Wikidata database, and aims to enhance VQA research with cross-modal knowledge.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel VQA dataset that covers fine-grained knowledge and visual understanding, potentially accelerating community development.
- The experimental evaluations are abundant and validate the dataset's effectiveness through fine-tuning on various models.

Weaknesses:
- The work exhibits limited innovation beyond the dataset creation, with evaluations lacking some of the latest multimodal models, such as MiniGPT4 and LLaVA.
- The dataset construction relies heavily on human effort, raising concerns about reproducibility and objectivity.
- Some descriptions are obscure or lack rigor, and the soundness of the dataset requires clarification.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the dataset's construction process and provide detailed information regarding the experimental configurations, including the types of questions and images involved. Additionally, we suggest that the authors include a broader range of models in their evaluations, specifically the latest multimodal models like MiniGPT4 and LLaVA. Furthermore, we encourage the authors to clarify the automatic generation claims of INFOSEEK_Wikidata and provide a comprehensive introduction to the model fine-tuning settings. Lastly, we urge the authors to address the reproducibility concerns by specifying parameter settings and ensuring the training/evaluation data are more widely accessible.