ID: KkHY1WGDII
Title: Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for obtaining structured output from large language models (LLMs) using Grammar-Constrained Decoding (GCD). The authors propose 'input-dependent grammars' to enhance flexibility in constraining output for various structured prediction tasks, evaluated through empirical results on closed information extraction and entity disambiguation. The study demonstrates that constraining output can significantly improve LLM accuracy without the need for fine-tuning in most cases.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents a simple yet effective approach to a common NLP problem.
- It provides strong experimental results, particularly in few-shot learning scenarios.
- The method is applicable to a wide range of structured NLP tasks, offering informative insights into constrained decoding.

Weaknesses:
- The contribution size is considered modest, and the paper could benefit from exploring more tasks or comparing its approach to fine-tuning.
- The baselines for structured prediction tasks are not comprehensive, limiting understanding of existing methods.
- The application of the Grammatical Framework (GF) requires more detailed explanation, and the efficiency of the method with complex grammars needs quantitative analysis.

### Suggestions for Improvement
We recommend that the authors improve the comprehensiveness of the baselines for various structured prediction tasks to enhance the understanding of existing methods. Additionally, we suggest providing a more detailed explanation of the application of the Grammatical Framework (GF). It would also be beneficial to quantitatively analyze the method's efficiency in handling complex grammars, including metrics such as the number of generation steps and parsing failures required to generate correct outputs. Finally, we encourage the authors to explore whether a small amount of fine-tuning could further enhance model performance on downstream tasks, potentially surpassing previous supervised methods.