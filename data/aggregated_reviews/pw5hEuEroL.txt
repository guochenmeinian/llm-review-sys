ID: pw5hEuEroL
Title: Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 7, 8, 6, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for analyzing mixture distributions and privacy amplification in deep learning models using $f$-Differential Privacy (DP) and its tradeoff function. The authors enhance bounds for shuffling mechanisms in differential privacy and demonstrate the privacy implications of a single-step gradient descent from random initialization. They leverage the connection between DP and hypothesis testing to derive tighter bounds for privacy amplification through shuffling and random initialization. The authors argue that their Lemma 4.1 and Lemma 4.7 provide a more general analysis than existing works, particularly in the context of sub-sampling mechanisms and shuffling models, and emphasize that their results extend the existing literature on privacy amplification by iteration, shuffling, and sub-sampling.

### Strengths and Weaknesses
Strengths:
- The paper addresses critical issues in DP by providing a unified approach to analyze additive mixtures of random variables, moving beyond ad-hoc methods.
- It offers significant improvements in existing statements and presents a clear and mathematically rigorous framework, which could influence future research in DP.
- The authors establish significant enhancements in shuffling models, presenting near-optimal results compared to state-of-the-art analyses.
- The framework is claimed to be applicable to any mixture model, providing a comprehensive approach to privacy analysis.

Weaknesses:
- The logical flow of the paper could be clearer, particularly regarding the presentation of results and proofs.
- Certain results, especially those concerning random initialization, may lack practical significance and require more robust theoretical backing.
- The presentation of numerical evaluations and baseline comparisons in Figure 1 is unclear and could mislead readers.
- There is a lack of clear comparisons with existing works, particularly in terms of asymptotic behavior and empirical results, which may leave the improvement unclear.
- The analysis of sub-sampling with fixed batch sizes is insufficiently discussed, raising concerns about the applicability of their results to more complex scenarios.

### Suggestions for Improvement
We recommend that the authors improve the logical flow by clearly outlining the main results and their derivations, possibly by reordering content or providing sketches of proofs in the main body. Additionally, consider moving the DP-GD analysis to an appendix to streamline the main narrative. Clarifying the relationship between the outputs of neighboring datasets and the assumptions regarding the coupling of random variables would enhance understanding. We also suggest extending the discussion on the relationship to prior results, particularly regarding the implications of varying $\epsilon_0$ and the comparison with existing works. Furthermore, we recommend that the authors improve the clarity of their contributions by providing a more thorough analytical exposition that details the enhancements over state-of-the-art methodologies, including explicit comparisons with existing results. Addressing the concerns regarding the applicability of their framework to fixed batch sizes and providing clearer empirical results would strengthen the paper's impact. Finally, ensuring that the baseline comparisons are relevant and well-explained would bolster the paper's contributions.