ID: w8LoOWsbU7
Title: Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Adaptive Language-guided Multimodal Transformer (ALMT), which addresses sentiment-irrelevant and conflicting information in Multimodal Sentiment Analysis (MSA) by utilizing an Adaptive Hyper-modality Learning (AHL) module. The AHL module learns an irrelevance/conflict-suppressing representation from visual and audio features, guided by language features at different scales. The authors claim that ALMT achieves state-of-the-art performance on datasets such as MOSI, MOSEI, and CH-SIMS.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents a novel approach to MSA, significantly contributing to the field.
- The incorporation of the AHL module is innovative and could inspire future research.
- Empirical evidence demonstrates the model's effectiveness, achieving state-of-the-art performance on several datasets.

Weaknesses:
- The improvement of ALMT over the CHFN method on the MOSI dataset appears marginal, raising concerns about statistical significance.
- Some reviewers noted novelty limitations, suggesting that the work may be incremental as using Transformers for multimodal fusion is common.

### Suggestions for Improvement
We recommend that the authors improve the paper by conducting significance tests to validate the statistical significance of the observed improvements. Additionally, the authors should provide more details on how the AHL module suppresses sentiment-irrelevant information and include a theoretical justification for the chosen architecture, particularly the design of the AHL in the proposed method.