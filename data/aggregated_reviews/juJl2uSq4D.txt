ID: juJl2uSq4D
Title: RL in Latent MDPs is Tractable: Online Guarantees via Off-Policy Evaluation
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 8, 3, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies latent Markov decision processes (LMDP) where a set of MDPs is randomly selected at the beginning of each episode, requiring the agent to infer the selected MDP from feedback. The authors propose an algorithm with sample complexity poly(S,A)^M, matching the \Omega((SA)^M) lower bound. The algorithm is a model-based exploration method that collects data to shrink the confidence interval of model parameters. It shows that after poly(SAH) rounds of exploration, a near-optimal policy can be constructed. The paper also introduces a new coverage coefficient for analyzing LMDPs and demonstrates its link to sample complexity.

### Strengths and Weaknesses
Strengths:
- The exposition of the main ideas is clear, particularly through the instantiation on tabular MDPs.
- The OMLE algorithm is well-structured, providing a general framework for model-based exploration.
- The polynomial sample complexity result for LMDPs with constant latent states is significant and interesting.

Weaknesses:
- The algorithm lacks computational efficiency, as it requires enumeration of parameters and policies within the confidence interval.
- Section 4 is difficult to follow without prior knowledge of related works, and the justification for the choice d = 2M-1 is insufficient.
- The title may overstate the results, as the sample complexity remains exponential in M, and the finite M assumption is not intrinsic to the LMDP setup.
- The connection to existing studies on the coverage coefficient is not sufficiently concrete, and the meaning of segmented policies is unclear.

### Suggestions for Improvement
We recommend that the authors improve Section 4 to make it more self-contained and provide clearer justifications for technical choices. Additionally, we suggest revising the title to avoid overclaiming the results, given the exponential sample complexity in M. Clarifying the relationship between the algorithm and existing work on off-policy evaluation (OPE) would also enhance the paper. Finally, addressing the questions regarding the generalization of the coverage coefficient and its implications for complexity measures would strengthen the analysis.