ID: eNM94i7R3A
Title: Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 6, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the learning dynamics in minimal neural networks, focusing on how layer-specific initialization variances and learning rates influence the transition between lazy and rich learning regimes. The authors derive exact solutions and identify a conserved quantity, $\delta$, which plays a crucial role in determining the learning dynamics. The findings are extended to more complex networks, demonstrating that unbalanced initialization promotes feature learning and enhances interpretability in deep networks.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written, with clear visuals and a logical progression from simple to complex models.  
- The identification of the conserved quantity $\delta$ provides a clean framework for understanding the dynamics of different learning regimes.  
- Experimental results align closely with theoretical predictions, reinforcing the validity of the authors' claims.  

Weaknesses:  
- The assumption of whitened input for deriving exact solutions is unrealistic, and the low-rank case requires additional constraints for accurate solutions.  
- Some equations are omitted for brevity, which detracts from completeness, particularly those related to NTK dynamics.  
- The connection between $\delta$ and NTK dynamics could be made more explicit, and the paper could benefit from a clearer discussion of the implications of the conserved quantities.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their results by addressing the following points:  
- Clarify whether Theorem 4.1 holds throughout training and explicitly define what the "exact solution" refers to.  
- Expand on the meaning of "rapid feature learning" and provide more mathematical detail to support this claim.  
- Include missing equations that relate to NTK dynamics and the kernel distance for completeness.  
- Consider moving some content from the appendix to the main text to enhance readability, while ensuring that the essential details are clearly presented.  
- Compare the results in Section 4 with those derived for SGD in deep linear models to provide additional insights into the dynamics discussed.  
- Reassess the terminology "initialization geometry" to better reflect the role of learning rates in optimization, possibly adopting "optimization geometry" instead.