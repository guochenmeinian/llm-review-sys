ID: qmvtDIfbmS
Title: WhodunitBench: Evaluating Large Multimodal Agents via Murder Mystery Games
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 8, 6, 8, 6, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents WhodunitBench, a benchmark for evaluating large multimodal agents (LMAs) in multi-agent settings through murder mystery games. The benchmark assesses LMAs' capabilities in perception, interaction, and cognition within dynamic environments. The study highlights that while LMAs can handle simple tasks, they struggle with complex interactions. The authors effectively leverage GPT-4 to generate plausible distractors in multiple-choice questions, and the paper details the construction and evaluation of the benchmark.

### Strengths and Weaknesses
Strengths:
1. The benchmark offers a novel and rich context for evaluating multiple AI capabilities simultaneously.
2. Clear documentation of the dataset and evaluation procedures enhances reproducibility.
3. The study provides insights into the reasoning chains developed by GPT-4V in the context of the game.

Weaknesses:
1. The focus on LMAs competing against each other without real participants limits the study's applicability to real-world scenarios.
2. Some findings are predictable, given the models tested, which could be acknowledged more explicitly.
3. The presentation of the paper requires improvement for clarity and coherence.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by addressing the following:
- Explicitly state whether real participants were involved in the evaluation alongside LMAs, as this would enhance the study's implications for real-world reasoning and perception.
- Discuss the practical implications of the benchmark results and how they should be interpreted in relation to real-world tasks.
- Expand the limitations section to include a more detailed analysis of how social skills and reasoning abilities impact performance.
- Consider including comparisons of LMA performance against human players to strengthen the benchmark's relevance.
- Improve the overall presentation by addressing typos, clarifying acronyms, and ensuring consistency in terminology throughout the paper.