ID: 8hUUy3hoS8
Title: StreamBench: Towards Benchmarking Continuous Improvement of Language Agents
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents StreamBench, a benchmark for evaluating LLM agents' capabilities to improve performance over time in an online learning environment. The authors augment five datasets to create a stream-based evaluation framework, introducing techniques such as a cost-effective multi-agent round-robin prompting method (MAM-StreamICL) that leverages multiple LLM agents. The authors demonstrate that streaming can enhance the performance of both smaller and larger LLM agents.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- It addresses a significant gap in existing benchmarks for LLM agents, focusing on continuous improvement rather than static capabilities.
- The framework is generalizable and applicable to various aspects of LLM agent performance.
- The MAM-StreamICL method effectively combines the strengths of multiple LLM agents while maintaining cost efficiency.

Weaknesses:
- The code repository is not finalized, and the dataset ordering sequences are difficult to locate.
- There is no error quantification in the results presented in Figure 1 and Table 2.
- The feedback signal implementation for datasets in StreamBench is unclear.
- The rationale behind the random assignment of time steps in the streaming sequence is not well justified.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the streaming sequence design and its real-world relevance. Additionally, providing error quantification for different sequence orderings in the results would enhance the analysis. It would be beneficial to explore alternative feedback signals beyond the binary feedback currently required by the proposed algorithms (Self-StreamICL and MAM-StreamICL). We suggest including an example of the prompt template in the main paper and clarifying the implementation of the shared memory among different LLMs. An analysis of the embeddings in memory could provide further insights into performance. Lastly, we encourage the authors to consider including more non-streaming methods in their benchmarks and to clarify the exclusion of certain fine-tuning methods.