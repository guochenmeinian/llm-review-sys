ID: QMaLS4VeY3
Title: Aligning Audio-Visual Joint Representations with an Agentic Workflow
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 3, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for improving audio-visual representation learning by reducing misalignment between audio and video streams through a data-centric agentic workflow (AVAgent) controlled by large language models (LLMs). The method involves analyzing audio and visual content separately, planning corrective actions, and applying preprocessing techniques to enhance alignment. Experimental evaluations across various audio-visual tasks demonstrate that the proposed method outperforms established benchmarks.

### Strengths and Weaknesses
Strengths:  
- The problem addressed is significant, as audio-visual representation learning is crucial for multi-modal tasks.  
- The proposed method is simple and provides a preprocessing tool applicable to various audio-visual techniques.  
- The innovative agentic workflow is clearly outlined, and extensive empirical evaluations support its effectiveness across multiple tasks.  

Weaknesses:  
- The paper lacks detailed descriptions of the "Planning" phase, particularly regarding the specifics of actions to be taken for audio-visual pairs.  
- The reliance on LLMs raises concerns about the absence of technical tricks or innovations beyond LLM capabilities.  
- The proposed workflow's limited capability is evident, as it primarily considers basic signal-processing actions, which may not robustly address complex audio-visual sync issues.  
- Clarity regarding dataset specifications and the computational demands of the proposed method is insufficient.

### Suggestions for Improvement
We recommend that the authors improve the documentation of the "Planning" phase to include specific details about the actions required for audio-visual pairs, such as noise levels and wavelet selection. Additionally, consider enhancing the complexity of the proposed method by incorporating more advanced actions that can handle multi-source audio scenarios effectively. It would be beneficial to clarify which subset of the AVSBench dataset was used and to discuss how features processed by the proposed method could enhance existing audio-visual frameworks. Finally, including a brief discussion on the expected computational demands and presenting comparisons with previous methods focusing solely on preprocessing steps would provide a more comprehensive view of the method's contributions.