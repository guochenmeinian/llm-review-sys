ID: FTPDBQuT4G
Title: Generalized Linear Bandits with Limited Adaptivity
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 8, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of the generalized linear contextual bandit problem under limited adaptivity constraints, proposing two algorithms: B-GLinCB for predetermined update times and RS-GLinCB for adaptive updates. B-GLinCB divides the timeline into batches, ensuring a $\tilde{O}(\sqrt{T})$ regret when the number of updates is $\Omega(\log \log T)$, independent of the problem-dependent instance $\kappa$. RS-GLinCB achieves a $\kappa$-independent regret bound by employing two criteria for action selection. The theoretical results are validated through comparisons with baseline algorithms in logistic bandit settings.

### Strengths and Weaknesses
Strengths:
- The paper addresses a crucial aspect of real-world decision-making by focusing on limited adaptivity constraints.
- It extends results from linear to non-linear reward models, achieving $\tilde{O}(\sqrt{T})$ regret under specific conditions, with the leading term independent of $\kappa$.
- The algorithms are computationally efficient, maintaining a constant sample size for reward parameter estimation.

Weaknesses:
- The requirement for prior knowledge of $\kappa$ in achieving a $\kappa$-independent regret bound is a limitation, especially when compared to existing methods that do not require this information.
- The computational complexity related to the dimension $d$ is not adequately addressed, particularly in calculating optimal design and distributional optimal design at each time step.
- The numerical experiments lack comparisons with randomized algorithms like Thompson sampling and EVILL, which would enhance the analysis of regret performance.

### Suggestions for Improvement
We recommend that the authors improve the algorithm's applicability by eliminating the need for prior knowledge of $\kappa$, potentially drawing on methods from the MNL contextual bandit literature. Additionally, we suggest providing a detailed explanation of the algorithm's computational complexity concerning dimension $d$. To strengthen the empirical analysis, we recommend including comparisons with randomized algorithms and addressing the warmup stage's impact on regret, particularly in the context of $\kappa$-dependent scenarios. Lastly, we encourage the authors to clarify the differences in the upper bound of the reward parameter used in experiments and consider including a table of contents for better organization.