ID: 9KtX12YmA7
Title: The Behavior and Convergence of Local Bayesian Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 8, 8, 6, 8, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the behavior and convergence properties of local Bayesian optimization (BO) methods, particularly focusing on the GIBO variant. The authors provide theoretical convergence rates for GIBO in both noiseless and noisy settings, demonstrating that local solutions can be found more rapidly compared to conventional global BO methods. Empirical experiments validate the tightness of the derived bounds, and the paper addresses the implications of observational noise on convergence.

### Strengths and Weaknesses
Strengths:
- The study addresses an important problem by providing theoretical analysis for local BO methods.
- The writing is clear and facilitates understanding of complex theoretical concepts.
- The convergence rates derived for GIBO are sound and empirically validated, showing how observational noise affects convergence.

Weaknesses:
- The analysis is limited to the GIBO approach, which differs significantly from other local BO methods like TurBO, and this distinction should be made clearer.
- The theoretical framework relies on strong assumptions about the objective function, which may not hold in practical scenarios, potentially limiting the applicability of the results.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the focus on the GIBO method and its distinction from other local BO approaches like TurBO. Additionally, the authors should consider relaxing the assumptions regarding the objective function to enhance the generalizability of their theoretical analysis. Addressing the questions raised about hyperparameter settings and the potential for parallel processing would also strengthen the paper.