ID: iiYadgKHwo
Title: Variational Distillation of Diffusion Policies into Mixture of Experts
Conference: NeurIPS
Year: 2024
Number of Reviews: 21
Original Ratings: 6, 5, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Variational Diffusion Distillation (VDD), a method for distilling denoising diffusion policies into Mixtures of Experts (MoE) using variational inference. The authors aim to combine the strengths of diffusion models, which excel in learning complex distributions but suffer from slow inference times, with the efficiency of MoEs. VDD demonstrates superior performance across nine behavior learning tasks compared to existing methods. The authors also propose improvements to the VDD methodology based on reviewer feedback, including an expanded discussion on VDD's relation to recent diffusion distillation approaches, enhanced baseline comparisons with the Consistency Trajectory Model (CTM), and a completed results table for DDPM and VDD-DDPM on kitchen and block push datasets. Additional evaluations on diffusion teacher models with various samplers, detailed training cost information, improved visualizations of gating probabilities, and an open-sourced codebase for reproducibility are also included.

### Strengths and Weaknesses
Strengths:
- The technical approach is sound, with a rigorous methodology and clear motivation for combining diffusion models and MoEs.
- The experimental section is impressive, showcasing substantial evaluations and meticulous visualizations.
- The proposed method achieves faster inference while maintaining performance comparable to the original diffusion policy.
- Comprehensive expansion on related work enhances the novelty of VDD.
- Strong baseline comparisons demonstrate VDD's distillation performance.
- Detailed training cost information provides clarity on resource requirements.
- Improved visualizations aid in understanding the methodology.
- Availability of an open-sourced codebase facilitates reproducibility.

Weaknesses:
- The paper does not adequately address the diversity of policies, focusing primarily on performance metrics without justifying the need for diverse policies.
- The novelty of applying variational distillation in reinforcement learning is limited, as it is a known technique in other fields.
- Several experimental results are incomplete, and inconsistencies in reported metrics raise questions about the comprehensiveness of the evaluation.
- The paper lacks illustrative visualizations that could help readers understand the method's performance in real-world scenarios.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of policy diversity by including comparisons of the diversity of different algorithms and justifying the need for diverse policies in their context. Additionally, we suggest emphasizing the novel aspects of applying variational distillation for policy extraction. It would be beneficial to provide complete experimental results, particularly for the DDPM component, and to clarify any inconsistencies in the reported metrics. Including illustrative visualizations of good and bad case studies would enhance the reader's understanding of the method's performance. Finally, we recommend improving clarity in the presentation of results and ensuring that all technical details are easily accessible to the reader, as well as further elaborating on the implications of the enhanced evaluations to provide deeper insights into the significance of their findings.