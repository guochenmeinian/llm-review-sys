ID: 3b9sqxCW1x
Title: A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated Class Incremental Learning for Vision Tasks
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to class-incremental learning (CIL) in a federated setting, utilizing a server-side generative model to create synthetic images that replace data from older classes. The authors claim their method outperforms existing approaches, supported by empirical results. They introduce a new benchmark dataset, SuperImageNet, and conduct an ablation study to analyze the contributions of different components of their approach.

### Strengths and Weaknesses
Strengths:
- The paper effectively addresses the challenges of CIL within the federated learning (FL) framework and articulates the training paradigms and loss functions used.
- The proposed method demonstrates significant performance improvements on existing benchmarks.
- The introduction of the SuperImageNet dataset is a valuable contribution to the field.

Weaknesses:
- The claim that training the generative model in a data-free manner leads to better performance lacks clarity and requires further discussion and experimentation.
- Potential privacy issues related to the shared generative model are not adequately analyzed.
- The nature of the SuperImageNet dataset is unclear, raising questions about its originality compared to existing datasets.
- The performance discrepancies in Figure 5, particularly regarding FedCIL's results, are not sufficiently discussed, including the number of runs and hyperparameter tuning.
- The paper does not provide a theoretical analysis or sufficient clarity on the concept of "task" in the context of FL.

### Suggestions for Improvement
We recommend that the authors improve the discussion on how the data-free training of the generative model contributes to performance gains, possibly through additional experiments. An analysis of privacy implications related to the shared generative model should be included. Clarifying the nature of the SuperImageNet dataset and its distinction from existing datasets is essential. The authors should also provide more context regarding the results in Figure 5, including details on experimental runs and hyperparameter settings. Additionally, we suggest that the authors define the term "task" earlier in the paper to enhance clarity for readers unfamiliar with the concept. Finally, including a theoretical analysis of the method's convergence and performance in more realistic settings would strengthen the paper.