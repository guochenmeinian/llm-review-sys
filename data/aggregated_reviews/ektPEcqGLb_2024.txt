ID: ektPEcqGLb
Title: Poisson Variational Autoencoder
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 7, 7, 7, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel variational autoencoder (VAE) model, the Poisson variational autoencoder ($\mathcal{P}$-VAE), which utilizes Poisson-distributed latent variables and a Poisson reparameterization for efficient training. The authors argue that their model is inspired by biological neural networks and connects to sparse coding, enabling insights into sensory processing in the brain. The experimental results demonstrate that the $\mathcal{P}$-VAE learns sparse representations and performs well on downstream classification tasks compared to existing VAE baselines.

### Strengths and Weaknesses
Strengths:
- The introduction of a Poisson variational autoencoder is a novel contribution to the VAE landscape, with strong motivation from sparse coding and biological representation.
- The paper is well-written, with clear explanations and effective figures that enhance understanding.
- The experimental evaluation is thorough, providing strong evidence for the model's capabilities and limitations.

Weaknesses:
- A significant weakness is the insufficient exploration of the temperature parameter in the Poisson reparameterization, including its effects on gradients and the lack of motivation for its annealing during training.
- There is a lack of discussion regarding the noise level for the likelihood $p(x | z)$, which raises questions about the model's adaptability to different datasets.
- The paper does not adequately address potential challenges or limitations of the method, particularly in relation to its biological plausibility and comparisons with existing models.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the temperature parameter in the Poisson reparameterization, specifically addressing how it affects the gradient and providing a rationale for its annealing. Additionally, a clearer explanation of the noise level for the likelihood $p(x | z)$ should be included to enhance the model's applicability to various datasets. We suggest incorporating a balanced discussion of both strengths and weaknesses of the method, particularly regarding its biological implications. Furthermore, it would be beneficial to clarify the significance of the global prior parameter $r$ and to consider including a simple linear probe as a baseline for downstream classification tasks. Lastly, a more explicit connection to neuroscience concepts and clearer notation would enhance accessibility for machine learning audiences.