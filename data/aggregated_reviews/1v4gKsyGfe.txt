ID: 1v4gKsyGfe
Title: Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 8, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 2, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of the linear probing and fine-tuning (LP-FT) framework based on neural tangent kernel (NTK) theory, supported by experiments with transformer-based models on natural language processing benchmarks. The authors decompose the NTK matrix into FT-effective and pre-train-effective components, demonstrating an approximation between the fine-tuning matrix and the LoRA matrix. The analysis highlights the significance of linear head norm and its impact on model calibration, which can be mitigated through temperature scaling.

### Strengths and Weaknesses
Strengths:  
1. The application of NTK theory to analyze LP-FT in complex models like Transformers is intriguing and well-supported by mathematical proofs.  
2. The paper is well-written, with detailed technical content and comprehensive benchmarks that substantiate the claims.  
3. The originality of the research is notable, as it bridges established theory with recent advancements in NLP, potentially impacting fine-tuning practices across various fields.  

Weaknesses:  
1. The experiments predominantly focus on classification tasks; exploring additional domains and more challenging tasks (reasoning, math, code generation) could enhance the generalizability of the findings.  
2. The paper could benefit from improved organization, particularly regarding the introduction of concepts and clarity in figures and tables.  
3. More detailed explanations of the experimental setup and hyperparameter tuning would enhance transparency and reproducibility.  

### Suggestions for Improvement
We recommend that the authors improve the organization of the material, ensuring that concepts like the linear model in Proposition 1 are introduced earlier. Additionally, we suggest providing precise descriptions for each table and figure to enhance clarity. Expanding the range of experiments to include various transformer models and domains would strengthen the empirical evidence. Furthermore, we encourage the authors to elaborate on the practical applications of their findings, offering specific examples for practitioners. Lastly, a deeper exploration of temperature scaling and its implications for model performance would provide more comprehensive recommendations.