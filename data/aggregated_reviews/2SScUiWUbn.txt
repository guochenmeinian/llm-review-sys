ID: 2SScUiWUbn
Title: On the Connection between Pre-training Data Diversity and Fine-tuning Robustness
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical study on visual pre-training, examining the effects of various factors such as data quantity, label granularity, label semantics, image diversity, and data sources on fine-tuning robustness. The authors conduct extensive experiments to analyze how pre-training datasets, both real and synthetic, influence performance across tasks like image classification and out-of-distribution detection. The findings indicate that pre-training data quantity and label granularity are significant determinants of fine-tuning robustness.

### Strengths and Weaknesses
Strengths:
- The novelty of the project lies in its experimental findings regarding visual pre-training, revealing critical insights into data quantity and label granularity.
- The paper is well-written, with clear conclusions that provide valuable guidance for future research.
- The comprehensive exploration of dataset construction offers important insights into deep learning models.

Weaknesses:
- The study predominantly utilizes CNN architectures, neglecting the potential benefits of implementing vision transformer (ViT) architectures, which are increasingly relevant in current research.
- The reliance on a single metric for evaluating robustness may limit the comprehensiveness of the findings.
- The figures lack sufficient definition, making it challenging to interpret their implications, particularly in Figures 4, 5, 6, 7, 8, 10, and 11.
- The focus on supervised pre-training overlooks the growing significance of self-supervised learning methods.

### Suggestions for Improvement
We recommend that the authors improve the experimental setup by incorporating vision transformer (ViT) architectures to enhance the relevance of their findings. Additionally, addressing the influence of different fine-tuning methods on robustness would provide a more nuanced understanding of the results. The authors should also consider discussing the impact of long-tailed data distributions on fine-tuning robustness, as this could significantly affect their conclusions. Furthermore, enhancing the clarity and definition of the figures would aid in better conveying the results. Lastly, exploring the generalizability of their findings across various downstream tasks and datasets would strengthen the paper's contributions.