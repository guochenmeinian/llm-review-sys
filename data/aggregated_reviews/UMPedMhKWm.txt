ID: UMPedMhKWm
Title: Rapid Plug-in Defenders
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 7, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two distinct contributions: CeTaD (Considering Pre-trained Transformers as Defenders), a method designed to address the Rapid Plug-in Defender (RaPiD) problem in deep neural networks (DNNs) against adversarial attacks, and a framework mechanism involving vision embeddings and a decoder-only transformer model. CeTaD utilizes pre-trained transformer models, fine-tuning only a limited set of parameters with few-shot clean and adversarial examples, allowing for rapid defense without modifying the deployed model. The authors evaluate CeTaD across various datasets and attack methods, demonstrating its effectiveness, transferability, and generalization capabilities, achieving superior performance in both clean and adversarial accuracy compared to existing baselines. Additionally, the framework mechanism clarifies that the output of the last encoder layer is shaped as (batch_size, patch/sequence_size, hidden_size), which is then rearranged by PixelShuffle to increase spatial resolution and form the shape of the images (batch_size, H, W).

### Strengths and Weaknesses
Strengths:
- CeTaD effectively counters adversarial perturbations without altering the deployed model, addressing a significant challenge in the field.
- The method is computationally efficient, requiring minimal parameter tuning and few-shot examples.
- Comprehensive evaluations validate CeTaD's effectiveness, transferability, and the impact of different components across various scenarios and datasets.
- The authors have conducted additional evaluations to support their research, enhancing the clarity of their framework mechanism.

Weaknesses:
- The comparison of performance across different \( l_p \) bound perturbations is lacking.
- Older defense methods, such as JPEG compression, should be included for comparison.
- There are typographical errors and issues with the placement of tables relative to their content.
- The experimental settings for adaptive attacks are flawed, as the plug-in defense is assumed to be known to the attacker.
- The paper does not sufficiently address how CeTaD performs against stronger adaptive attacks or provide a clear comparison with existing defenses.
- There is a lack of clarity regarding how features are extracted from the decoder-only transformer model, particularly concerning the output activation per token and its concatenation for PixelShuffle.

### Suggestions for Improvement
We recommend that the authors improve the comparison of different \( l_p \) bound perturbations and include older defense methods like JPEG compression in their evaluations. Additionally, addressing typographical errors and ensuring that tables are placed closer to their relevant content would enhance clarity. The authors should also correct the experimental settings for adaptive attacks to ensure a fair comparison and provide a more in-depth analysis of CeTaD's performance against stronger adaptive attacks. Furthermore, we recommend that the authors improve the explanation of feature extraction from the decoder-only transformer model, specifically detailing whether the output activation per token is used before the next-token predictor and how these activations are concatenated for PixelShuffle. Incorporating and clarifying the rearrangement process of the last encoder layer's output in the revised version would also strengthen the paper.