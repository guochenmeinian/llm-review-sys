ID: qWi33pPecC
Title: Most Influential Subset Selection: Challenges, Promises, and Beyond
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 2, 7, 7, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of the Most Influential Subset Selection (MISS) problem, highlighting the failures of existing greedy additive methods in identifying influential subsets due to their assumption of linearity and neglect of interactions among samples. The authors propose an adaptive greedy algorithm that dynamically updates individual influence scores to better capture these interactions, ensuring consistent influence across selected samples. The effectiveness of this algorithm is demonstrated through experiments on synthetic and real-world datasets, including MNIST.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and provides a thorough theoretical analysis of the failure modes in existing greedy approaches to the MISS problem.
- The problem addressed is timely and relevant, particularly in the context of large datasets and data-centric AI.
- The adaptive updating of influence weights is a reasonable and sensible approach, supported by clear visualizations and examples.

Weaknesses:
- The paper primarily focuses on theoretical aspects, with limited empirical validation, particularly for subset sizes larger than 2.
- The computational efficiency of the proposed algorithm is questionable, as it is inefficient for moderately sized datasets, with reported long runtimes.
- There is insufficient discussion on the applicability of the proposed method to non-linear models and the influence of randomness in training.
- The experimental assessment lacks robustness checks and comparisons with established baselines, limiting the ability to evaluate the proposed solution's practical relevance.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation of their adaptive greedy algorithm by testing it on larger subset sizes and more diverse datasets to demonstrate scalability. Additionally, a comparison with established methods in data subset selection, such as SELCO, would provide context for the proposed approach's performance. The authors should also address the implications of their method in non-linear contexts and discuss the influence of randomness in model training more thoroughly. Finally, enhancing the clarity of figures with axis labels and grounding comparisons with "ground truth influence" would improve the paper's overall presentation and impact.