ID: h0a3p5WtXU
Title: Loss Landscape Characterization of Neural Networks without  Over-Parametrization
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 8, 4, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new condition, referred to as the $\alpha-\beta$ condition, to describe the optimization landscape of deep neural networks. It alleviates the restrictive consequences of alternative conditions like the PL-condition, particularly regarding overparameterization and saddle points. The authors prove convergence for SGD and other first-order stochastic methods under this condition and provide empirical evidence demonstrating its applicability in realistic networks. The paper also highlights that many prior conditions fail to hold in practice, which is supported by both theoretical counter-examples and extensive empirical analysis.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written and tackles the critical issue of understanding the loss landscape structure that enables efficient optimization.
- It provides compelling arguments for the $\alpha-\beta$ condition, backed by theoretical results and experimental insights.
- The extensive empirical investigation showing the failure of prior conditions in realistic networks is a significant contribution.

Weaknesses:
- The convergence for SGD is shown with a non-vanishing error term $O(\beta \sigma^2)$, and further insights into this term's implications are lacking.
- The theoretical implications of the convergence results are weak, particularly regarding the upper bound of minimum loss being potentially large.
- The empirical justification for the $\alpha-\beta$ condition is limited, with concerns about its applicability in harder tasks.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the non-vanishing error term $O(\beta \sigma^2)$, particularly its relationship with SGD, SPS, and NGN. Clarifying whether this term is an artifact of the proof technique or related to the $\alpha-\beta$ condition would enhance understanding. Additionally, including experiments with popular optimizers like SGD+momentum and Adam would provide valuable insights into the $\alpha-\beta$ condition's applicability. We also suggest that the authors address the theoretical connection between the $\alpha-\beta$ condition and neural network training more robustly, as this is a significant limitation. Finally, we encourage the authors to plot the $\alpha$ and $\beta$ values across different optimizers to elucidate their impact on convergence rates.