ID: OFMPrCAMKi
Title: Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 6, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Jaccard Metric Losses (JMLs) as a flexible alternative to traditional Intersection over Union (IoU) losses for semantic segmentation, particularly addressing the limitations of IoU losses when processing soft labels. The authors demonstrate that JMLs can effectively incorporate training techniques such as label smoothing, knowledge distillation, and semi-supervised learning. Experiments across multiple datasets show consistent improvements over baseline methods.

### Strengths and Weaknesses
Strengths:
- The motivation for addressing the limitations of IoU losses is compelling, and the novelty of JMLs is clear.
- The paper shows good results across various datasets and architectures, including both CNNs and transformers.
- The theoretical analysis of JMLs enhances understanding of their properties.

Weaknesses:
- The paper lacks direct comparisons between IoU losses and JMLs, which weakens the justification for the proposed method.
- Some experimental results are confusing, particularly discrepancies between tables and unexpected performance outcomes.
- The focus on older models in experiments limits the demonstration of JMLs' scalability and effectiveness with more recent architectures.

### Suggestions for Improvement
We recommend that the authors improve the clarity of experimental results by addressing discrepancies between tables and providing direct comparisons between IoU losses and JMLs. Additionally, we suggest including results from larger and more recent architectures, such as Mask2Former, to better demonstrate the scalability and relevance of JMLs in current research. Furthermore, adding failure case analyses and discussing limitations would enhance the depth of the paper.