ID: bA3iR0jQO9
Title: Accelerate Multi-Agent Reinforcement Learning in Zero-Sum Games with Subgame Curriculum Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a subgame curriculum learning framework aimed at accelerating multi-agent reinforcement learning (MARL) training for zero-sum Markov games. The authors propose an adaptive initial state distribution that allows agents to reset to previously visited states for improved learning efficiency. A subgame selection metric is derived to approximate the squared distance to Nash equilibrium (NE) values, and a particle-based state sampler is utilized for subgame generation. The proposed Subgame Automatic Curriculum Learning (SACL) algorithm is shown to converge faster and achieve lower exploitability in various zero-sum environments.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant challenge in reducing the computational cost of solving complex zero-sum games with MARL, offering potential applications and implications.
- A motivating example illustrates the effectiveness of the subgame curriculum learning framework, leveraging concepts from goal-conditioned reinforcement learning and prioritized experience replay.
- The SACL algorithm is compatible with various MARL algorithms, preserving convergence properties while demonstrating improved performance in experiments.

Weaknesses:
- There is no evidence presented that the proposed method improves final performance, despite accelerated convergence.
- The assumption that the environment can be reset to any desired state for subgame generation may not be feasible in all settings.
- The link between equations in the theoretical contributions is weak, and there are concerns regarding the clarity of the exploitability computation and the choice of metrics for state weighting.

### Suggestions for Improvement
We recommend that the authors improve the evidence supporting the final performance enhancement of the proposed method. It would be beneficial to include comparisons with other metrics, such as the least visited times of a state, to strengthen the analysis of the state weighting approach. Additionally, clarifying the computation of approximate exploitability and providing quantitative results on the impact of hyperparameters would enhance the evaluation of SACL's performance. Finally, addressing the theoretical underpinnings of the subgame sampler and its relationship to existing literature on subgame solving would solidify the contribution of this work.