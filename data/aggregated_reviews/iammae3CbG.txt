ID: iammae3CbG
Title: Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Prototype-based HyperAdapter (PHA) framework aimed at enhancing sample-efficient multi-task learning through adapter-tuning and hypernetworks. The authors propose a method that generates parameters for adapter layers based on task-specific prototypes, which are retrieved and transferred to new tasks for further tuning, thereby improving generalization capacity. Extensive empirical experiments indicate that PHA outperforms existing methods.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized, clearly written, and easy to follow.
- It addresses a significant issue in multi-task learning with a novel approach.
- Comprehensive experiments demonstrate the effectiveness of the proposed method.
- Strong motivation for the framework and robust ablation studies, particularly regarding the instance-dense retriever module.

Weaknesses:
- The novelty of the methods is limited, as many techniques used, such as InfoNCE, are established in other fields.
- Experimental results lack significance testing and are primarily focused on NLU tasks, with no exploration of NLG tasks.
- The applicability of the method to different model architectures, such as decoder-only LMs, is uncertain.
- Marginal improvements on benchmarks like SuperGLUE raise questions about the method's effectiveness.
- The motivation for using prototypes requires further clarification, particularly regarding their relationship with instance embeddings.

### Suggestions for Improvement
We recommend that the authors improve the significance testing of their experimental results and expand their evaluation to include NLG tasks. Additionally, the authors should clarify the experimental settings and provide explanations for the observed performance discrepancies on different benchmarks. Further elaboration on the motivation for using prototypes and their interaction with instance embeddings is necessary. We also suggest including comparisons with methods like ATTEMPT and MPT in the evaluation. Lastly, addressing the minor typographical errors and enhancing the explanation of the evaluation procedure in the camera-ready version would be beneficial.