ID: ubXaboYnzN
Title: QTSumm: Query-Focused Summarization over Tabular Data
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark for query-focused table summarization and introduces a table summarization approach named REFACTOR. The authors identify the gap between automatic and human evaluation results, proposing QTSUMM-EVAL to assess the performance of automatic metrics. The study includes a dataset of 5,625 query-summary pairs from 2,437 tables, emphasizing the importance of generating tailored summaries based on user queries. Experimental results indicate that REFACTOR enhances state-of-the-art models by incorporating relevant facts into the model input.

### Strengths and Weaknesses
Strengths:
- The manuscript builds a valuable dataset that closely mirrors real-world scenarios.
- The proposed method achieves state-of-the-art performance on the dataset.
- Comprehensive evaluation of instruction data quality and the dataset construction process is well-documented, including multi-round validation.

Weaknesses:
- The novelty of the proposed method is limited compared to existing work on TableQA and Table-to-text generation.
- The writing lacks clarity, with redundant paragraphs and a convoluted structure that complicates understanding.
- Details regarding the REFACTOR method are insufficiently presented in the main paper, requiring readers to refer to the appendix for critical information.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the manuscript by simplifying redundant paragraphs and enhancing the logical flow of the writing. Additionally, we suggest that the authors include more details about the REFACTOR method in the main paper to aid reader comprehension, particularly regarding the formulation and input/output of each step. Furthermore, we encourage the authors to compare their method with other TableQA/Table-to-text generation methods in their experiments and to explore the use of more examples beyond the 2-shot setting for testing LLMs.