ID: AA1xrgAP5z
Title: Universal Online Learning with Gradient Variations: A Multi-layer Online Ensemble Approach
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 8, 9, 7, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an algorithm for online convex optimization (OCO) that adapts to the curvature of loss functions without prior knowledge. The authors aim to achieve dynamic adaptability to strongly convex, exp-concave, and convex loss functions while ensuring computational efficiency with only one gradient query per round. They introduce a three-layer algorithm that estimates loss function types and parameters, leveraging existing structures like MSMWC and optimistic Online Mirror Descent. Notably, the authors provide a novel decomposition of regret, reducing gradient queries from \(O(\log^2 T)\) to \(O(1)\) per round, which can be applied to various meta algorithms for worst-case guarantees. The authors also acknowledge the need for clearer definitions, such as the Bregman divergence and the notation $\|\cdot\|_{U_t}$, and commit to reordering content for better clarity.

### Strengths and Weaknesses
Strengths:  
- The paper makes significant contributions to OCO, providing universal guarantees and non-trivial improvements in analysis.  
- It is well-written, with detailed explanations of methods and clear connections to existing literature.  
- The proofs are clear, and the appendix demonstrates practical applications, bridging stochastic and adversarial OCO with competitive regret bounds.  
- The authors demonstrate responsiveness to feedback, indicating a willingness to improve clarity and rigor in their work.  
- They plan to enhance the presentation by reorganizing content and providing additional explanations, which will benefit readers' understanding.

Weaknesses:  
- While comparisons with existing literature are provided, comparisons with lower bounds, particularly for small loss bounds dependent on \(F\) and \(V\), are lacking.  
- The paper would benefit from experimental results comparing its performance with Zhang et al. (2022).  
- Some sections, particularly Section 4, are presented hastily, potentially hindering reader comprehension.  
- The definition of "optimism" and the base learners are not adequately introduced, leading to confusion.  
- Some definitions and concepts are currently unclear, such as the Bregman divergence and specific notations, which could confuse readers.  
- There are several mathematical errors and typos that need correction, which may detract from the paper's overall quality.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Section 4 to enhance reader understanding, possibly by providing more context and revising existing methods. Additionally, please include comparisons with lower bounds for small loss bounds and consider adding experimental results to compare with Zhang et al. (2022). We suggest defining "optimism" and the base learners more clearly in the main text to avoid confusion. Furthermore, we recommend improving the clarity of definitions, particularly for the Bregman divergence and the notation $\|\cdot\|_{U_t}$. Please ensure that the presentation order is revised to provide an overview before delving into specifics. We also suggest clarifying the conditions in Lemma 3 and the rationale behind the step size $\eta_t$. Lastly, addressing the identified mathematical errors and typos throughout the manuscript will enhance its accuracy and readability.