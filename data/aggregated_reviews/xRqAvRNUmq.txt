ID: xRqAvRNUmq
Title: Learning to Scale Logits for Temperature-Conditional GFlowNets
Conference: NeurIPS
Year: 2023
Number of Reviews: 3
Original Ratings: 7, 7, -1
Original Confidences: 4, 3, 5

Aggregated Review:
### Key Points
This paper presents an application of temperature scaling to GFlowNets, enhancing the sampling of unique trajectories by rescaling the logits of the forward policy network. The authors propose two methods, Layer-GFN and LSL-GFN, which improve the model's ability to generate diverse high-reward samples, particularly in drug discovery contexts. Empirical results demonstrate that these methods outperform existing approaches on the QM9 and TFBind8 tasks.

### Strengths and Weaknesses
Strengths:
- The paper features a well-structured introduction and background on GFlowNets and the proposed extensions.
- An extensive empirical comparison is conducted between Layer-GFN, LSL-GFN, and five baseline methods across two tasks.
- An ablation study highlights the advantages of training over a mixture of temperature scales.

Weaknesses:
- The *number of modes* metric may be misleading as it reflects unique samples above a reward threshold, which could belong to the same mode; renaming it to "number of unique samples" would be more accurate. Clarification on how the threshold is determined is needed.
- The calculation of Top-100 Diversity is not explained, nor is it clear if larger or smaller values are preferable, requiring further elaboration in the text.
- The explanation of the ablation over the number of epochs per training round is insufficient, particularly regarding the *inner_loop* parameter, making it difficult to assess the significance of setting it to four.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the reward threshold used in the number of modes metric and visualize the distribution of rewards generated by each method. Additionally, the authors should clarify how the reward is computed for QM9 and the permissiveness of the TFBind8 evaluation. Further, it would be beneficial to explain how diversity is calculated, as the scales appear different in the QM9 and TFBind8 plots. Lastly, including a plot of the mapping of $f_\theta:\beta\to T$ could enhance understanding of the learned functions at various training epochs.