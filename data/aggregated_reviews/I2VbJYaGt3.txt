ID: I2VbJYaGt3
Title: Descriptive Kernel Convolution Network with Improved Random Walk Kernel
Conference: ACM
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an improved graph kernel called RWK$^+$, which incorporates color-matching random walks and is utilized in a Kernel Convolution Network (KCN) for unsupervised graph feature learning. The authors derive efficient computation for RWK$^+$ and propose a new GNN layer, RWK$^+$Conv. Experiments demonstrate RWK$^+$'s effectiveness across various tasks, particularly in graph-level tasks, and its adaptability to real-world applications such as Twitter bot detection and Reddit community classification.

### Strengths and Weaknesses
Strengths:
1. The introduction of color-matching random walks is novel and effectively enhances the descriptive learning ability of the kernel.
2. The paper is well-structured, clearly written, and provides a thorough analysis of the proposed method across multiple datasets.
3. The proposed RWK$^+$Conv layer shows significant performance improvements over GCN in graph-level tasks.

Weaknesses:
1. The complexity of Algorithm 1 is quadratic with respect to $m$, posing challenges for large-scale graph applications.
2. RWK$^+$Conv is only compared to GCN, neglecting stronger baselines like GCNII, WL subtree kernels, and Wasserstein WL kernels, which raises concerns about the validity of the performance claims.
3. The learnable parameters of RWK are not clearly explained, and the computational complexity analysis between the proposed method and RWK is missing.

### Suggestions for Improvement
We recommend that the authors improve the presentation by providing a more detailed mathematical definition of "learnable hidden graphs" on page 3, line 240, and clarifying the term "the subgraph around node G" in line 241. Additionally, we suggest correcting the typo in line 66 to "Weisfeiler-Lehman" and discussing figures in ascending order while moving Figure 4 to the main content. It would also be beneficial to address different sections of the Appendix in the main content, ensure consistent font usage throughout the paper, and relocate the dataset descriptions to the main content. Finally, we advise including more robust baseline comparisons and a detailed explanation of the GCNConv method and its experimental settings.