ID: SViJgzox1z
Title: Parameter Efficient Multi-task Fine-tuning by Learning to Transfer Token-wise Prompts
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called token-wise prompt tuning (TPT) aimed at enhancing prompt-based methods for natural language processing tasks. The authors introduce token-wise prompt banks, which consist of fine-grained prompts that can be combined to generate instance-dependent prompts. TPT addresses two main limitations of previous methods: the generation of individualized prompts tailored to specific examples and the leveraging of cross-task features for multi-task learning. The effectiveness of TPT is demonstrated through experiments on various NLP tasks, showing significant performance improvements over existing methods.

### Strengths and Weaknesses
Strengths:
- The concept of token-wise prompt banks is a novel contribution that enhances the contextual relevance of prompts.
- Experimental results indicate significant improvements in performance across various NLP tasks, validating the proposed method.
- The paper is well-structured and clearly presents the methodology, experiments, and results.

Weaknesses:
- The implementation complexity is high due to several intricate components, such as memory networks and the token-wise prompt bank.
- The method's ability to effectively transfer cross-task features may pose challenges.
- The focus on a limited range of models, primarily T5-small and T5-large, feels outdated, and the comparison with existing methods is limited.
- The experimental evaluation lacks a comprehensive range of models and relevant baselines, leading to questions about the robustness of the findings.

### Suggestions for Improvement
We recommend that the authors improve the comparison with a wider range of state-of-the-art methods to provide a clearer understanding of the relative performance. Additionally, exploring more recent decoder-only models, such as the LLaMa model, would enhance the relevance of the analysis. We suggest including a broader evaluation across various model sizes to reflect the continuous growth of pre-trained language models. Furthermore, the authors should provide a more detailed analysis of the performance degradation observed in certain tasks and clarify the implications of their method on the usage of cutting-edge large language models.