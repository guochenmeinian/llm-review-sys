ID: k7G42gLuab
Title: Understanding and Detecting File Knowledge Leakage in GPT App Ecosystem
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a large-scale analysis of file knowledge leakage in GPT applications, utilizing a tool called GPTs-Filtor to detect vulnerabilities at both the prompt and network levels. The authors constructed a dataset of 8,000 popular GPTs across eight categories, revealing that approximately half are susceptible to leakage, with 3,645 files identified as leaked. The study aims to enhance security practices within the GPT ecosystem.

### Strengths and Weaknesses
Strengths:
- The paper is the first comprehensive study on file knowledge leakage in GPTs.
- The methodology is clear, and the paper is well-organized and easy to follow.
- GPTs-Filtor effectively uncovers instances of file knowledge leakage in real-world applications.

Weaknesses:
- The analysis of what constitutes leakage is superficial, lacking categorization of the type and criticality of exposed information.
- The threat model is limited, focusing only on prompt injection and transmitted data.
- Ethical concerns arise from the potential violation of proprietary data rights and the lack of clarity on how developers will be informed of vulnerabilities.
- The evaluation design is flawed, with questions regarding the reliability of the metadata used and the effectiveness of the harmful prompts library.

### Suggestions for Improvement
We recommend that the authors improve the depth of their analysis by categorizing the type and criticality of the information exposed during leakage. Additionally, the authors should clarify their threat model and consider expanding it beyond prompt injection and transmitted data. We suggest revisiting the ethical implications of their methodology, particularly regarding the disclosure of proprietary data and the process for informing developers. Furthermore, we encourage the authors to validate the effectiveness of their harmful prompts library and provide a more robust evaluation design, possibly including a ground truth dataset to ensure the reliability of their findings. Lastly, we recommend enhancing the clarity of the presentation, particularly in figures and terminology used throughout the paper.