ID: IpzCUvade7
Title: Interventional Rationalization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents interventional rationalization (Inter-RAT) as a novel method that utilizes causal theory and intervention techniques to improve explanations for neural network predictions. It addresses the issue of spurious correlations and demonstrates its effectiveness through experiments on real-world datasets. The authors propose that their method enhances explainability in selective rationalization tasks by applying causal interventions to identify significant features impacting predictions.

### Strengths and Weaknesses
Strengths:
- The paper achieves state-of-the-art (SOTA) results across various datasets.
- It is well-written, making it accessible and easy to follow, with clear problem definitions and method proposals.
- The experimental results align with human evaluations, indicating the method's practical relevance.

Weaknesses:
- The causal graph presented is similar to existing works, raising concerns about originality and the effectiveness of using text categories as confounders.
- There is a lack of clarity regarding the implementation of interventions, particularly in how changes in conditional probabilities are observed.
- The paper does not adequately address the Entity Bias problem or provide sufficient statistical validation for claims regarding shortcuts in rationalization.

### Suggestions for Improvement
We recommend that the authors improve the clarity and detail of Figure 2, ensuring it accurately reflects the structural causal model (SCM) and includes independent noise terms. Additionally, the authors should provide a more thorough explanation of the intervention process, particularly how P(R|do(X)) is implemented across different scenarios. It is crucial to address the Entity Bias issue more comprehensively and include recent literature in the related works section to strengthen the paper's foundation. Finally, we suggest incorporating more recent models, such as those from 2022 onwards, into the baseline comparisons to enhance the study's relevance.