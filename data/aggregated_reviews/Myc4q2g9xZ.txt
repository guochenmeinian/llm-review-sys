ID: Myc4q2g9xZ
Title: Multi-modal Situated Reasoning in 3D Scenes
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the MSQA dataset, a large-scale multi-modal situated reasoning dataset collected using 3D scene graphs and vision-language models (VLMs) across diverse real-world 3D scenes. The authors propose an interleaved multi-modal input setting to enhance the 3D visual question answering (VQA) domain. They introduce benchmarks, Multi-modal Situated Question Answer (MSQA) and Multi-modal Next-step Navigation (MSNN), to evaluate embodied reasoning and navigation capabilities. Experiments demonstrate that pre-training on MSQA improves model performance in understanding complex 3D scenes.

### Strengths and Weaknesses
Strengths:
- The paper introduces a comprehensive dataset, MSQA, addressing gaps in existing 3D scene understanding.
- It effectively integrates multimodal input settings and tasks, simulating real-world applications.
- The automated data-curated pipeline enhances data quality and expands the scope of existing tasks.

Weaknesses:
- The experimental section lacks comprehensiveness, particularly in demonstrating the effectiveness of the "T+I" mode for enhancing spatial perception.
- Baseline improvements on the LEO model are lower than expected in basic metrics.
- The reliance on language prompts in situational understanding may disconnect from actual embodied scenarios.
- The dataset's scene variety is limited, and the omission of relevant literature on 3D situational awareness is notable.

### Suggestions for Improvement
We recommend that the authors improve the experimental section to better highlight the effectiveness of the MSQA dataset's "T+I" mode in enhancing spatial perception abilities. Additionally, consider comparing the accuracy of the fine-tuned model without the situation component to validate the effectiveness of the situation input. To address the reliance on language prompts, we suggest incorporating scenarios that emphasize visual inputs for situational understanding. Furthermore, we encourage the authors to explore scaling the dataset to include more scenes, potentially leveraging large-scale synthetic datasets, and to include a discussion of the missing relevant work on 3D situational awareness to strengthen the literature review. Lastly, please address the identified typos and formatting issues for clarity and coherence.