ID: IdF7VT6eEs
Title: Online Performative Gradient Descent for Learning Nash Equilibria in Decision-Dependent Games
Conference: NeurIPS
Year: 2023
Number of Reviews: 25
Original Ratings: 6, 7, 5, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies multi-agent performative games, generalizing the assumptions from Narang et al. (2022) to encompass generalized linear cases. The authors propose algorithms based on online stochastic approximation of the link function and individual gradient updates, proving convergence rates for both linear and kernel function classes. Additionally, the authors present a novel approach to online game optimization by connecting it with RKHS estimation, framing the learning of Nash equilibria in decision-dependent games as a bilevel problem. They analyze the challenges of bounding estimation errors under the power norm and introduce innovative proof techniques that decouple power norm bounds from RKHS norms. The results demonstrate that their methods can achieve convergence to Nash equilibria in both linear and non-linear parametric functions, outperforming traditional methods like AGM and RSGM. Numerical experiments validate their theoretical findings.

### Strengths and Weaknesses
Strengths:
- The problem statement is clear, and the solution is intuitive.
- The authors are the first to estimate the unknown structure of the distribution in multi-agent performative games, a significant contribution to machine learning literature.
- The paper introduces significant technical innovations that enhance the understanding of power norm bounds in online algorithms.
- The analysis effectively addresses major difficulties in the estimation error under the power norm, providing a clear framework for future research.
- The experimental results are promising and may have practical implications, demonstrating convergence in complex settings.

Weaknesses:
- The technical contributions are not sufficiently elaborated, as the proofs largely follow Narang et al. (2022) with minor modifications.
- The linearity assumption is restrictive, and the experiments are limited to simple instances that resemble known online regression problems.
- The theoretical novelty compared to Narang et al. appears inadequate, with concerns about the estimation of the distribution map and the use of similar techniques for proof.
- The authors initially mischaracterized their feedback model, leading to confusion regarding the use of bandit feedback versus first-order feedback.
- Some technical aspects, such as the reliance on specific gradient oracles, may limit the applicability of the proposed methods in broader contexts.

### Suggestions for Improvement
We recommend that the authors improve the elaboration of their technical contributions, clearly distinguishing their work from Narang et al. (2022) and emphasizing the unique insights that led to their results. Additionally, we suggest conducting experiments with more complex, multi-agent scenarios to demonstrate the advantages of their approach over existing algorithms. Addressing the linearity assumption through empirical verification in non-linear settings would also strengthen the paper. Furthermore, we recommend improving the clarity of their assumptions regarding feedback models by explicitly stating the use of first-order oracles instead of bandit feedback. We encourage the authors to explore the extension of their methods into the bandit feedback setting, potentially leveraging noisy observations to approximate gradients in decision-dependent games. Lastly, we advise careful proofreading to correct any typos and clarify confusing notations.