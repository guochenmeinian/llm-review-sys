ID: VFRyS7Wx08
Title: Rethinking Inverse Reinforcement Learning: from Data Alignment to Task Alignment
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 5, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper addresses the reward misalignment issue in inverse reinforcement learning (IRL) by proposing a novel approach called PAGAR, which utilizes both protagonist and antagonist policies to derive a set of task-aligned reward functions from expert demonstrations. The authors aim to reorient IRL-based imitation learning (IL) from data alignment to task alignment, emphasizing that achieving high utility under these task-aligned reward functions is crucial for task fulfillment. The approach is theoretically grounded and validated through experiments on discrete grid-world tasks, as well as continuous control tasks, demonstrating improved performance over existing methods like GAIL and VAIL.

### Strengths and Weaknesses
Strengths:  
1. The introduction of task-related objectives to mitigate reward ambiguity is a novel and interesting concept.  
2. The paper has a solid theoretical foundation, which is its primary strength.  
3. Empirical results indicate superior performance compared to GAIL and VAIL.  
4. The use of the MiniGrid environment as a benchmark is justified, highlighting its complexity and relevance.  
5. Expanded experimental evaluation includes continuous control tasks and additional baselines, demonstrating the method's robustness across various environments.

Weaknesses:  
1. The empirical evaluation is limited to simple discrete environments, lacking continuous settings such as MuJoCo, which diminishes the generalizability of the findings.  
2. The experimental campaign is not extensive, with only two comparison baselines that do not convincingly demonstrate the proposed algorithm's superiority.  
3. The complexity of the MiniGrid environment may not be sufficiently clarified in the manuscript.  
4. The paper's narrative is cluttered, making it difficult to follow, and the complexity of the algorithm raises questions about its scalability.  
5. The theoretical sections could benefit from improved clarity to enhance communication of the main ideas.

### Suggestions for Improvement
We recommend that the authors improve the empirical evaluation by including more complex environments, such as the LocoMuJoCo locomotion benchmark, to better demonstrate the scalability of their approach. Additionally, incorporating more IRL algorithms as baselines, such as MaxEnt IRL, ML-IRL, and f-IRL, would strengthen the comparative analysis. The authors should also clarify the theoretical framework and streamline the presentation to enhance readability, focusing on the motivation and implications of their findings. Furthermore, addressing the limitations of their method in more detail, particularly regarding the time efficiency and the nature of task-relatedness, would provide valuable insights for future research.