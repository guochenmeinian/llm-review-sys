ID: u6XxyuD3Ro
Title: Online Convex Optimisation: The Optimal Switching Regret for all Segmentations Simultaneously
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two significant contributions to the field of online convex optimization. First, it introduces the RESET algorithm, which addresses the problem of switching regret by eliminating the logarithmic penalty typically associated with efficient algorithms, achieving optimal switching regret without the $\sqrt{\ln T}$ factor. Additionally, the authors explore dynamic regret, demonstrating that RESET can achieve a bound of $\mathcal{O}(\sum_{k} \sqrt{\Lambda_k})$ for segmentations with $\mathcal{O}(1)$ path length. Second, the paper studies a metric that interpolates between adaptive regret and dynamic regret, proposing a method for sequentially aggregating base algorithms to achieve these types of regret, showing significant improvements over existing methods, particularly in the context of segmentation and dynamic regret.

### Strengths and Weaknesses
Strengths:  
- The solution is clear and intuitive, with a correct proof. The novel combination method of $z_t^i = \mu_t^i w_t^i + (1 - \mu_t^i) z_t^{i-1}$ is particularly impressive and crucial for achieving optimal switching regret.  
- The theoretical contributions are significant, potentially marking a breakthrough in removing the logarithmic penalty from switching regret guarantees.  
- The results are interesting and relevant to the community, demonstrating a clear advancement over the state of the art.  
- The authors have committed to improving the paper's readability and addressing reviewer suggestions.

Weaknesses:  
- The presentation lacks clarity and rigor, making it difficult to follow. Key discussions, such as the reasoning behind the weighted combination method and its comparison to traditional methods, are missing.  
- The writing requires substantial revision for clarity and impact.  
- The algorithm's memory requirements may still be $O(T)$, which contrasts with existing approaches that achieve $O(\log(T))$ memory.  
- The dynamic regret results are not convincing, as they only apply to comparator sequences with limited movement, raising questions about their general applicability.  
- There is a lack of a dedicated section discussing the connections and distinctions between adaptive and dynamic regret.  
- The analysis sections could benefit from summarizing into specific lemmas and providing more explanation regarding the analysis of $\mathcal{Q}(v)$.

### Suggestions for Improvement
We recommend that the authors improve the clarity and rigor of the presentation by providing more precise descriptions of theorems and clearly stating the assumptions under which they hold. Additionally, the authors should include discussions on the similarities or differences between their approach and existing methods, particularly the geometric covering intervals approach. We suggest moving some technical details to the appendix to enhance readability and addressing minor issues such as inconsistent citations and punctuation. It is crucial to include a dedicated section discussing the connections and distinctions between adaptive regret and dynamic regret. The authors should emphasize that the aggregation method used differs from that in SAOL [1] and consider comparing their methods with those in [2]. In Section 3, providing a formal algorithmic description would improve clarity. Summarizing the analysis into specific lemmas and explaining their significance would also be beneficial. Additionally, more explanation regarding the analysis of $\mathcal{Q}(v)$ is needed, as its current form is challenging to understand. Lastly, we encourage the authors to explore the adaptation of their techniques to derive the switching regret bound for exp-concave functions as discussed in recent work [3].