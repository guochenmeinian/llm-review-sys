ID: h5zYGF68KH
Title: PaGoDA: Progressive Growing of a One-Step Generator from a Low-Resolution Diffusion Teacher
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents PaGoDA, an adversarial distillation method designed for single-step generation of higher image resolutions than those produced by the teacher diffusion model. The authors propose a training process that utilizes forward PF-ODE to collect noise-data pairs, progressively adds upsampling layers, and employs multiple loss functions, including reconstruction loss, GAN loss, and additional losses for text-to-image tasks. PaGoDA demonstrates strong performance on ImageNet and COCO benchmarks, supported by various ablation studies. The authors also emphasize the potential of PaGoDA to enhance inference speed and reduce computational costs compared to traditional LDM frameworks, while acknowledging the need for additional experiments to address identified biases in the FID metric and to validate the effectiveness of reconstruction loss in prompt alignment.

### Strengths and Weaknesses
Strengths:
- This is the first study to enable higher resolution generation than the teacher diffusion model during distillation.
- The use of forward PF-ODE for noise collection enhances sample diversity and stability in training.
- The incorporation of reconstruction loss demonstrates improved performance in prompt alignment.
- PaGoDA shows promise in reducing computational costs for high-resolution generation and offers a lighter training load compared to LDM.
- The paper is well-written, with comprehensive implementation details and thorough theoretical analysis.

Weaknesses:
- The claimed faster sampling speed of PaGoDA compared to LCM is questionable, as a smaller VAE might yield better efficiency.
- The method's reliance on a large dataset for training increases costs compared to sampling-free alternatives.
- The use of a pre-trained discriminator may bias evaluation metrics, necessitating alternative comparisons.
- The paper lacks qualitative results demonstrating the diversity of generated images in text-to-image tasks.
- There is a need for further experiments to validate findings, particularly regarding the reconstruction loss and the effectiveness of PaGoDA in practical scenarios.
- The paper lacks a comprehensive discussion of related literature.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the training pipeline by addressing the efficiency of PaGoDA compared to smaller VAEs. Additionally, the authors should provide a detailed comparison of data collection versus training costs and clarify the inference CFG scale used in experiments. It would be beneficial to conduct human evaluations to assess the quality and diversity of generated images, as automated metrics may not correlate well with human preferences. Furthermore, we suggest that the authors explore the implications of using a pretrained discriminator and consider evaluating the model without the adversarial objective to better understand its performance. Lastly, we encourage the authors to conduct the additional experiments suggested, particularly rerunning the experiments presented in Figure 5-(c) to validate the findings on reconstruction loss, and to include a thorough discussion of related literature to enhance the context and relevance of their work.