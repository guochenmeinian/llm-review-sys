ID: ZsS0megTsh
Title: SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 6, 4, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 5, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an audio-visual face forgery detection method that utilizes only real data for training. The authors propose a framework where audio and visual features are fused through concatenation and processed by an encoder to predict the cluster for each time frame. During testing, discrepancies between audio and visual signals indicate video fakeness, with both fixed and dynamic time offsets employed to measure these discrepancies. The work aims to address generalization issues in previous detectors by leveraging unsupervised learning.

### Strengths and Weaknesses
Strengths:
1. The approach considers misalignment in real data through dynamic time offsets.
2. Training solely with real data is an innovative direction that enhances generalization to unseen deepfakes.
3. The framework includes a detailed description of the alignment model and thorough experimental analysis.

Weaknesses:
1. The paper lacks a discussion of how it differentiates from closely related work, particularly [22], and does not adequately explain its advantages over existing methods.
2. Comparisons with state-of-the-art detectors primarily focus on vision-only modalities, which may not provide a fair assessment.
3. There are numerous missing references to other audio-visual deepfake detectors, limiting the contextual understanding of the work.
4. Insufficient details regarding training code and hyperparameters hinder reproducibility.
5. The novelty of the method is questioned, as the architecture is not significantly refined beyond existing models.

### Suggestions for Improvement
We recommend that the authors improve the discussion on how their work differs from related studies, particularly [22], and clarify the advantages of their approach. Additionally, the authors should include comparisons with other audio-visual deepfake detectors to provide a more comprehensive evaluation. It is essential to enhance the description of the proposed method, particularly the local representation alignment, to clarify its implementation and significance. Furthermore, providing training code and detailed hyperparameter information would greatly aid reproducibility. Lastly, addressing the limitations regarding audio quality and the method's applicability to various forgery types would strengthen the paper's impact.