ID: KLL70pTQ17
Title: Oracle-Efficient Reinforcement Learning for Max Value Ensembles
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 6, 5, 5, -1, -1
Original Confidences: 3, 3, 2, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for combining multiple policies in a Markov Decision Process (MDP) to enhance performance beyond that of individual policies. The authors propose an algorithm that learns per-policy value functions for an increasing horizon \( h \), selecting actions based on the policy with the highest value at each state and horizon. The approach is theoretically sound and demonstrates promising empirical results in complex robotic tasks, suggesting applicability in multi-task reinforcement learning (RL).

### Strengths and Weaknesses
Strengths:  
- The proposed method is sound and practical, leading to impressive empirical results.  
- The assumptions and limitations are well-discussed, aiding in the applicability assessment of the method.  
- The algorithm is computationally efficient and scales well with large state spaces, validated through experiments.  

Weaknesses:  
- The paper lacks clarity and intuition, potentially hindering reproducibility.  
- The definition of approximate max-following policies is dry and lacks illustrative examples, making it difficult for readers to grasp its significance.  
- The reliance on heuristics for generating constituent policies may limit the method's broader applicability.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper, particularly in explaining the oracle for value functions, including examples to indicate its potential sources. Additionally, the authors should explicitly detail how to produce the \( \mu_h \) distributions, noting the implications of environment reset requirements. Enhancing the definition of approximate max-following policies with intuitive explanations or examples would greatly benefit reader comprehension. Finally, clarifying the role of heuristics in the generation of constituent policies in the abstract would help prevent misunderstandings.