ID: vVrwnY76W1
Title: Remember what you did so you know what to do next
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the effectiveness of a pre-trained LLM (GPT-J 6B) applied to ScienceWorld, a simulated game benchmark for elementary science experiments. The authors demonstrate that an LLM-based transfer learning scheme can outperform previously built RL-based agents by a factor of 1.4x to 3.3x, depending on the method used. The findings suggest that factors such as diverse pre-training, task formulation, and input context length are crucial for successful LLM transfer learning, contrasting with earlier studies that indicated poor performance of LLMs on this task.

### Strengths and Weaknesses
Strengths:
- The paper conveys a simple yet important message about LLM transfer learning's effectiveness in text-based science games, providing guidance for future research.
- The empirical results show a significant performance improvement over RL-based methods, supported by straightforward experiments and good analysis.
- Detailed evaluations of how training data volume and context length affect performance offer valuable insights.

Weaknesses:
- The differences in training data between GPT-J and T5 complicate the interpretation of results, necessitating experiments that control for this variable.
- The paper's writing and structure are poor, with informal language that detracts from its academic rigor.
- Some analyses, particularly regarding the model's tendency to hallucinate invalid objects, lack depth and clarity, leaving key questions unaddressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity and structure of the paper by avoiding informal language and ensuring that all terms are appropriately academic. Additionally, we suggest including experiments that control for the differences in training data to better understand their impact on performance. It would also be beneficial to provide a more thorough analysis of the model's errors, particularly regarding hallucinations, and to clarify the implications of the various conditions tested. Finally, we advise revising the Limitations and Ethics sections to focus on relevant concerns and removing trivial statements.