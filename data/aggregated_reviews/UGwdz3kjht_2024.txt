ID: UGwdz3kjht
Title: Prioritize Alignment in Dataset Distillation
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Prioritize Alignment in Dataset Distillation (PAD) method, which addresses the issue of misaligned information in existing dataset distillation techniques. The authors propose a two-step approach: (1) pruning the target dataset based on sample difficulty relative to the compression ratio, and (2) utilizing only deep layers of the agent model during distillation to avoid low-level, redundant information. The method aims to enhance performance across various dataset settings.

### Strengths and Weaknesses
Strengths:
1. The paper synthesizes a universal framework for existing data distillation methods by categorizing them into information extraction and embedding, highlighting the common theme of information misalignment.
2. The proposed method effectively integrates insights from data selection and representation learning, improving the understanding of data distillation mechanisms.

Weaknesses:
1. The introduction of two hyperparameters—initial ratio and data addition epoch—complicates the method's practical application, as their sensitivity could hinder adoption in larger datasets.
2. The performance improvements over the existing DATM framework are marginal, raising concerns about the practical value of the proposed changes.

### Suggestions for Improvement
We recommend that the authors improve the experimental validation of their method by conducting tests on a wider range of datasets and varying pruning ratios to substantiate their claims. Additionally, we suggest including a comparison of the effects of discarding deeper-layer parameters. Clarifying the generation of the EL2N score in Figure 2 and addressing the discrepancies in baseline results across figures and tables would enhance the paper's clarity. Finally, we encourage the authors to discuss the applicability of their method to more recent state-of-the-art methods like SRe2L.