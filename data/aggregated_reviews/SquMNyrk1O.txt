ID: SquMNyrk1O
Title: Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 5, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the WTA-CRS (Winner-Take-All Column Row Sampling) algorithm, aimed at addressing the memory bottleneck in neural network training caused by storing feature maps. The authors claim that WTA-CRS achieves up to 2.7× peak memory reduction with minimal accuracy loss and allows for a 6.4× increase in batch size. The algorithm utilizes unbiased estimators for matrix multiplication with reduced variance, requiring only sub-sampled activations for gradient calculation. The experimental results indicate that WTA-CRS outperforms existing methods and is compatible with current parameter-efficient fine-tuning (PEFT) techniques, although it incurs some computational overhead.

### Strengths and Weaknesses
Strengths:
- The proposed WTA-CRS algorithm effectively addresses a significant issue in PEFT, making training more accessible for researchers with limited resources.
- The paper includes a theoretical analysis of WTA-CRS.
- WTA-CRS demonstrates superior performance compared to existing algorithms.
- An anonymized code repository is provided for reproducibility.

Weaknesses:
- The paper lacks a comparative analysis with popular memory-efficient techniques such as tensor rematerialization and ZeRO, which are only partially discussed in the appendix.
- Experiments are limited to a single architecture (T5), despite the method's broader applicability.
- The use of quantization (16-bit) is unclear, raising concerns about the compatibility of WTA-CRS with quantization noise.

### Suggestions for Improvement
We recommend that the authors improve the comparative analysis by including popular memory-efficient training techniques like tensor rematerialization and ZeRO in the main paper. Additionally, conducting experiments across various neural network architectures would strengthen the findings. The authors should clarify whether quantization was employed and provide theoretical or experimental insights on its interaction with WTA-CRS. Lastly, an analysis of the overhead associated with the WTA-CRS method, particularly regarding computational costs, would enhance the clarity and completeness of the research.