ID: ybc9V6Cbq2
Title: Better Quality Pre-training Data and T5 Models for African Languages
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new multilingual dataset comprising 16 African languages and four high-resourced languages spoken in Africa, created by auditing and cleaning mC4, crawling various websites, and using this dataset to pretrain a multilingual T5 model. The authors demonstrate that their model outperforms existing multilingual models across 17 languages and four downstream tasks, showcasing the dataset's effectiveness.

### Strengths and Weaknesses
Strengths:
- The introduction of a new pretraining corpus for 16 low-resourced African languages is beneficial for the NLP community.
- The data creation process is thorough, highlighting careful consideration in its development.
- The new T5 model shows superior performance compared to existing models, with convincing experimental results.

Weaknesses:
- Some sections are confusing, particularly regarding the number of languages and performance claims.
- Lack of hyperparameter discussion makes reproducing downstream task results challenging.
- Insufficient evidence supports claims about the quality of the dataset, and comparisons with other corpora are limited.

### Suggestions for Improvement
We recommend that the authors improve clarity in the introduction by accurately stating the number of languages in the dataset. Additionally, we suggest providing a detailed discussion of hyperparameters to facilitate reproducibility. To strengthen claims about dataset quality, we encourage the authors to conduct ablation tests and provide more comprehensive comparisons of corpus statistics. Finally, we advise including comparisons against additional baselines for QA and summarization tasks to enhance the robustness of the findings.