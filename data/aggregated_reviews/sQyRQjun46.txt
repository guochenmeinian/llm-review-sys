ID: sQyRQjun46
Title: Understanding and Addressing the Pitfalls of Bisimulation-based Representations in Offline Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 28
Original Ratings: 6, 7, 6, 6, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 3, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the shortcomings of bisimulation-based methods in offline reinforcement learning (RL), particularly focusing on issues like missing transitions and reward scaling. The authors propose a $\tau$-expectile-based bisimulation operator (EBS) to address these challenges, achieving improved in-sample learning as $\tau\to 1$. They also introduce a reward scaling (RS) technique that tunes hyperparameters based on reward differences. The authors emphasize the importance of reward scaling in bounded state-space metrics and demonstrate through experiments that both the expectile-based operator and reward scaling significantly enhance the quality of learned representations across various offline RL tasks, including D4RL datasets. However, the empirical evidence supporting these claims is questioned.

### Strengths and Weaknesses
Strengths:  
- The authors provide a thorough mathematical analysis of both the original and expectile-based bisimulation operators, which is beneficial for future research in offline RL.  
- The paper effectively connects bisimulation metrics to policy evaluation, offering practical insights.  
- The initial motivation regarding the role of bisimulation metrics in offline settings is presented clearly, emphasizing their importance for representation learning and transfer.  
- Empirical results convincingly demonstrate the proposed methods' effectiveness, particularly in the D4RL benchmarks, highlighting the critical issue of missing transitions in bisimulation-based representations.  
- The authors offer a theoretical guarantee for their proposed reward scaling, contributing to the understanding of hyperparameter selection in offline RL.

Weaknesses:  
- The claim that learning a bisimulation operator in the offline setting generally fails is questionable, as certain environments may allow for successful learning.  
- The motivation for using the expectile operator needs clearer justification, particularly regarding its relevance to Proposition 4 and overfitting prevention.  
- The motivation appears misguided, as existing literature does not convincingly demonstrate that bisimulation performs poorly in offline settings; rather, it shows that various methods underperform in both offline and online contexts.  
- The proposed methods do not uniquely address the issues of bisimulation, as missing transitions are a general offline RL problem, and reward scaling is not exclusive to offline scenarios.  
- The experiments lack variance measures and qualitative analyses, which could provide deeper insights into the performance variations across different datasets.  
- The D4RL empirical results lack clarity, particularly regarding the exclusion of MICo+RS, and the pixel-based experiments are difficult to assess due to high variance and insufficient runs.  
- The complexity of measuring dataset incompleteness is acknowledged but not sufficiently addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind the expectile operator and its role in addressing overfitting in offline RL. Additionally, conducting an ablation study comparing the proposed reward scaling strategy against simpler methods would strengthen the analysis. It would also be beneficial to include more diverse environments in the experimental evaluation to assess the generalizability of the proposed methods. Furthermore, providing learning curves and variance measures for the V-D4RL experiments would enhance the robustness of the findings. We suggest providing confidence intervals for the D4RL results to improve their interpretability. Clarifying the relationship between the proposed methods and the existing literature on bisimulation metrics would strengthen the paper's argument. Finally, addressing the concerns regarding the relevance of the reward scaling and the nature of missing transitions in offline RL would enhance the paper's rigor.