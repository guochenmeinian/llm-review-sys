ID: rI7oZj1WMc
Title: Learning Successor Features the Simple Way
Conference: NeurIPS
Year: 2024
Number of Reviews: 22
Original Ratings: 5, 5, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for learning Successor Features (SFs) from pixel-level observations in reinforcement learning (RL) by combining a Temporal-Difference (TD) loss with a reward prediction loss. The authors propose a simpler approach that avoids representational collapse by decoupling the loss function to learn successor features and task encoding separately. They eliminate the need for a separate SF loss, which distinguishes their work from previous methods. The authors conducted experiments demonstrating that their method is more efficient and practical for continual learning, particularly in complex environments like Mujoco, and they clarify that their approach does not require prior knowledge of task specifics and can learn from pixel inputs. Their results indicate that the proposed model consistently outperforms DQN/DDPG, showcasing improved performance and faster learning compared to existing methods.

### Strengths and Weaknesses
Strengths:
- The proposed method is simple, easy to implement, and well-motivated, effectively addressing representational collapse.
- The integration of SFs into the Q-learning framework enhances learning efficiency and reduces the number of hyperparameters required.
- The authors provide robust validation of their approach using complex environments and demonstrate superior transfer capabilities.
- Experimental results indicate consistent outperformance of DQN/DDPG across various settings, particularly in continual learning scenarios.

Weaknesses:
- The writing quality could be improved; the Introduction resembles a review of related work rather than contextualizing the proposed method.
- Figures, particularly Figure 1, lack clarity and context, making them difficult to understand, and some figures are pixelated, hindering readability.
- The experiments are limited to simple environments, and the effectiveness of the method with other RL algorithms remains untested. The claim of avoiding drawbacks is presented too absolutely and should be moderated.
- Several design choices lack sufficient explanation, and the analysis of experimental results is minimal, often concluding with "our method is better" without addressing why. Additionally, the connection between Proposition 1 and representation collapse may cause confusion for readers.

### Suggestions for Improvement
We recommend that the authors improve the writing quality, particularly in the Introduction, to better set the context for their method. We suggest incorporating a proof sketch regarding representation collapse near line 100 and emphasizing the necessity of avoiding constant vectors for basis features in Section 4. Consider moving complex figures, such as the first three subplots in Figure 1, to the experimental section for better clarity, and provide larger figures or clearer visualizations to better illustrate performance trends. Additionally, the authors should test the proposed method in more complex environments, such as Atari games, and with various RL algorithms to validate its effectiveness comprehensively. We suggest including ablation studies to clarify the importance of design choices and enhancing the analysis of experimental results to explain the observed performance improvements. Lastly, we advise moderating absolute claims regarding the method's advantages to reflect a more nuanced perspective and clarifying the specific results related to Reviewer Tczf's concerns regarding DQN's performance.