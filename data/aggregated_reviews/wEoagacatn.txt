ID: wEoagacatn
Title: Interaction-aware Dynamic 3D Gaze Estimation in Videos
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: 7, 9, 9, -1
Original Confidences: 4, 4, 3, 4

Aggregated Review:
### Key Points
This paper presents a dynamic 3D gaze estimation framework that addresses the complexities of human gaze interactions in real-world video scenarios. The authors propose leveraging human interaction labels to enhance gaze estimation, utilizing a Human Interaction Graph and a Transformer-based spatiotemporal module to capture spatial and temporal relationships in gaze movements. The work demonstrates improvements in gaze estimation accuracy on the Gaze360 benchmark, emphasizing the practical significance of the proposed method. However, certain terms in the equations, such as P(a_t |g_t) and P(a_(t+1)|g_t, a_t, I_(t+1)), lack clear definitions, and the significance of the interaction score C_ij when greater than one is not discussed.

### Strengths and Weaknesses
Strengths:
- The paper provides a comprehensive background and literature review on gaze dynamics, effectively contextualizing the research.
- The proposed framework shows notable improvements in gaze estimation accuracy and introduces innovative methodologies, including a multi-stage training algorithm.
- The presentation is clear, with well-explained figures and a concise formulation of the problem.

Weaknesses:
- The statistical significance of performance improvements is not measured.
- The availability of the code for reproduction and objective evaluation is unclear.
- Certain technical components, such as ResNet and AutoRegressive Transformers, are not adequately described.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the terms used in their equations, specifically defining P(a_t |g_t) and P(a_(t+1)|g_t, a_t, I_(t+1)), and discussing the significance of the interaction score C_ij when it exceeds one. Additionally, we suggest that the authors provide information regarding the availability of their code to facilitate reproduction and evaluation by other researchers. Finally, we encourage the authors to include a more detailed description of the ResNet and AutoRegressive Transformers used in their framework.