ID: eMR57voMz1
Title: Diversify \& Conquer: Outcome-directed Curriculum RL via Out-of-Distribution Disagreement
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 4, 7, 6, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 1, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel curriculum learning algorithm called Diversify for Disagreement & Conquer (D2C) within the reinforcement learning framework. D2C distinguishes itself by not requiring prior knowledge of the environment's structure, instead utilizing a goal-conditioned binary classifier to differentiate between visited and desired states. The algorithm incorporates a diversification objective to encourage disagreement on unvisited states, claiming to automatically generate diverse curriculum goals that provide intrinsic rewards to guide the agent towards the desired goal state.

### Strengths and Weaknesses
Strengths:
1. The technique of identifying similarities between explored states and desired outcomes with a diversification objective is novel in the domain of curriculum reinforcement learning.
2. Empirical evaluations demonstrate significant improvements in the proposed algorithm across various environments, including maze and robotics manipulation tasks.
3. The approach is geometrically flexible and well-articulated, with an in-depth ablation study of relevant hyperparameters.

Weaknesses:
1. The reliance on a dataset of unlabeled target data for modeling unvisited states poses a significant limitation, particularly in environments with complex state spaces, such as those involving pixel observations.
2. The algorithm may be prone to over-exploration, and the role of the diversification process in the overall curriculum proposal remains unclear.
3. The writing requires polishing for improved readability, and the experimental settings lack sufficient challenge to demonstrate the model's significance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the interpolation process referenced in Equation 4 and address how the diversification objective integrates into the overall algorithm. Additionally, it is crucial to clarify the implications of over-exploration in scenarios where the goal distribution is limited. We suggest enhancing the discussion of related work, particularly in comparison to existing exploration methods like random network distillation. Finally, we encourage the authors to emphasize the necessity of their approach in addressing the "without external reward" framework and to present compelling examples where their method outperforms traditional reward-based approaches.