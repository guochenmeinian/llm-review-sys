ID: kOp0kiXZ3a
Title: Nearly Lossless Adaptive Bit Switching
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel quantization method for deep neural networks (DNNs) that focuses on optimizing quantization-aware training (QAT) across multiple bit-widths with weight-sharing. Key contributions include: (1) Adaptive Learning Rate Scaling (ALRS), which dynamically adjusts learning rates for different precisions to mitigate competitive interference during one-shot joint training; (2) Double Rounding, an extension for fixed-precision quantization that enhances accuracy. Experimental results on the ImageNet-1K dataset demonstrate that the proposed methods outperform state-of-the-art approaches in both multi-precision and mixed-precision scenarios.

### Strengths and Weaknesses
Strengths:
- The submission is well-written and includes good figures.
- Extensive experiments are conducted across multiple datasets and networks.
- The ablation study is robust, evaluating various aspects of the proposed scheme.

Weaknesses:
- Some analysis is lacking, particularly regarding the potential for increased quantization errors due to double rounding.
- The rationale for applying ALRS only to scaling factors needs clarification, as smaller bit-width weights could also benefit from smaller learning rates.
- Figure 1 is somewhat confusing, with poorly explained colored arrows.
- The paper should include comparisons with more mixed-precision quantization methods and recent literature on multi-bit-width quantization.

### Suggestions for Improvement
We recommend that the authors improve the analysis of double rounding to address potential quantization errors. Additionally, clarify the application of ALRS to scaling factors and consider extending its use to small bit-width weights. We suggest revising Figure 1 for better clarity and including comparisons with more mixed-precision quantization research in Section 4, as well as incorporating recent papers on multi-bit-width quantization into the Related Work section.