ID: Yx8Sw2H5Q7
Title: Compositional Policy Learning in Stochastic Control Systems with Formal Guarantees
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 7, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CLAPS (Compositional Learning for Probabilistic Specifications), a novel framework for learning and verifying neural network policies in stochastic control systems. It decomposes complex control tasks into simpler subtasks represented as a directed acyclic graph (DAG) and utilizes reach-avoid supermartingales to provide formal guarantees on the probability of satisfying reach-avoid specifications. The authors evaluate their approach in a Stochastic Nine Rooms environment, demonstrating its efficacy in deriving guarantees for global compositional policies.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and presents a clear and easy-to-follow methodology.  
- The compositional approach allows for the analysis of complex tasks through simpler subtasks, enhancing the framework's applicability to safety-critical systems.  
- The literature review is thorough, providing a comprehensive overview of current methods in safe reinforcement learning.  
- The proposed algorithm offers significant contributions to control policy learning with formal probability guarantees.

Weaknesses:  
- The scalability of the proposed framework is a major concern, particularly due to the potential complexity of the DAG representation in more intricate tasks.  
- The empirical evaluation is limited to a single, relatively simple environment, raising questions about the approach's performance in higher-dimensional settings.  
- The paper may overlook recent works addressing similar safe reinforcement learning problems, which could provide additional context and comparison.

### Suggestions for Improvement
We recommend that the authors improve the scalability of their framework by addressing the limitations associated with the DAG representation. Additionally, conducting experiments in more complex environments would strengthen the empirical evaluation and demonstrate the framework's robustness. We suggest including comparisons with recent literature on safe reinforcement learning to contextualize the contributions of CLAPS better. Furthermore, clarifying the relationship between RASM and barrier certificates, as well as providing a complexity analysis of the proposed algorithm, would enhance the technical depth of the paper.