ID: X9Vjq9Fuhq
Title: Non-Convex Bilevel Optimization with Time-Varying Objective Functions
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 7, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on bilevel optimization in an online setting, where objectives at both levels vary over time. The authors propose a practical single-loop algorithm, SOBOW, which updates the lower-level variable once per upper-level update. The lower-level variable is updated using vanilla SGD, while the hypergradient for the upper-level update is computed via Conjugate Gradient steps and averaged over a specified window. This approach reduces computational and memory overhead compared to existing baselines, which require access to all past objectives and gradients. The paper demonstrates that SOBOW achieves sublinear regret under certain assumptions and shows strong empirical performance against baselines in online bilevel applications.

### Strengths and Weaknesses
Strengths:
- **Practical online solution:** The single-loop algorithm is well-suited for the online bilevel setting, where objectives are available sequentially, marking a significant contribution to the field.
- **Intuitive presentation of theoretical analyses:** The authors clearly outline the necessary assumptions and steps leading to their results, making the theoretical framework accessible.
- **Strong empirical performance against baseline:** The proposed SOBOW matches the performance of the OAGD baseline while significantly reducing computational and memory requirements, achieving up to a $20\times$ speedup.

Weaknesses:
- **Hyperparameters in the definition of regret:** The novel notion of bilevel regret depends on window size $K$ and decay rate $\eta$, which is unusual as regret should not rely on hyperparameters. A more standard definition is suggested for clarity.
- **No dependence on lower-level suboptimality in the regret:** The current definition does not account for the suboptimality of the lower-level decision variable, which is typically included in static bilevel optimization analyses.
- **Lack of comparison with existing single-loop algorithms:** The authors do not address how SOBOW compares with established single-loop static algorithms, which could provide context for its performance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definition of bilevel regret by removing its dependence on hyperparameters like $K$ and $\eta$. Consider adopting a more standard definition that reflects per-time-step suboptimality. Additionally, we suggest incorporating a discussion on the significance of lower-level suboptimality in the regret calculation. It would also be beneficial to compare SOBOW with existing single-loop algorithms to contextualize its performance and address potential challenges related to hypergradient estimation errors. Finally, we encourage the authors to explore the possibility of reducing the conjugate gradient loop to enhance computational efficiency further.