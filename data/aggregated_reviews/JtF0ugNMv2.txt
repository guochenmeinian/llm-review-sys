ID: JtF0ugNMv2
Title: T2T: From Distribution Learning in Training to Gradient Search in Testing for Combinatorial Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework, T2TCO, for solving combinatorial optimization problems by training a generative model (discrete diffusion solver) to estimate the distribution of high-quality solutions. During testing, it employs an effective local search (iterative objective-aware denoising) guided by gradient feedback within the estimated solution space. The authors demonstrate the framework's superiority over previous state-of-the-art methods through experiments on TSP and MIS.

### Strengths and Weaknesses
Strengths:
- The writing is clear, and the motivation for the proposed method is well-articulated, addressing the limitations of previous tailored search methods and the constructive diffusion solver.
- The method is sound and novel, particularly due to the objective-aware denoising process with gradient feedback, supported by theoretical analyses.
- Empirical results show strong performance, achieving state-of-the-art results on TSP and MIS.

Weaknesses:
- The method is limited to diffusion-based approaches, which may restrict its applicability and is considered incremental relative to DIFUSCO.
- High computational complexity, requiring four A100 GPUs for training, raises concerns about efficiency. Further details on training time and GPU memory requirements are needed.
- The empirical evaluation lacks depth, particularly regarding generalization performance and comparisons with other methods like Monte-Carlo Tree Search.
- The paper does not clearly state the assumptions regarding the types of problems the proposed approach can solve or the necessary post-processing procedures.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the applicability of the proposed method, specifying the types of problems it can effectively address, including the nature of decision variables and constraints. Additionally, we suggest providing a clearer distinction between novel contributions and elements adapted from existing works, particularly in Section 3.2. 

To enhance empirical evaluation, we encourage the authors to include experiments on generalization across different problem sizes and distributions, as well as detailed results on classical benchmark datasets like TSPLIB. Addressing the computational complexity by exploring efficiency improvements for diffusion-based models would also be beneficial. Lastly, clarifying the design choices, such as the application of 2OPT and the rationale behind the number of iterations in the gradient search, would strengthen the paper's contributions.