ID: tCGyM6CpRI
Title: Optimizing Retrieval-augmented Reader Models via Token Elimination
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for optimizing retrieval-augmented language models, specifically the Fusion-in-Decoder (FiD) model, through token filtering to reduce decoding time in open domain question answering tasks. The authors propose a token-level elimination method that removes irrelevant information based on average attention scores, which significantly reduces runtime by up to 62.2% with minimal performance loss. The empirical analysis indicates that the decoder's initial layers contain the majority of tokens from the gold passage, highlighting the method's relevance.

### Strengths and Weaknesses
Strengths:  
- The proposed token filtering method is well-motivated and supported by thorough empirical analysis, showing significant improvements in latency and comparable results to state-of-the-art methods.  
- The paper is well-written and provides insights into the decoder's information usage, which could benefit future decoding algorithm designs.

Weaknesses:  
- The presentation lacks clarity, particularly regarding the token elimination method and its implementation details.  
- The contribution is somewhat incremental, as similar filtering mechanisms have been previously utilized in other tasks, and the paper could benefit from a more extensive discussion of prior work.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation, especially in Section 4, by providing full details on the token filtering method. Additionally, we suggest including comparisons with other baseline methods for saving latency, such as top-k passage selection and FiD-light. The authors should also address the questions raised regarding the detailed formula for the attention-based score, the results of token filtering alone, and the impact of filtering on subsequent decoder layers. Furthermore, we encourage the authors to conduct qualitative analyses of the generation output to assess the impact of their approach.