ID: cBY66CKEbq
Title: A Unified Principle of Pessimism for Offline Reinforcement Learning under Model Mismatch
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an algorithm addressing offline reinforcement learning (RL) challenges, specifically the mismatch between environment dynamics used for dataset generation and evaluation, and data scarcity for certain (s, a) pairs. The authors propose a unified solution within the distributionally robust Markov decision processes (MDPs) framework, which aims to learn a policy/value function that maximizes expected return under the worst-case environment dynamics. The formulation adapts the bound on data scarcity, incorporating visit counts of (s, a) pairs, and derives high-probability lower bounds of the learned value functions under various divergences, demonstrating tighter bounds than prior work.

### Strengths and Weaknesses
Strengths:  
- The paper offers a theoretically sound offline RL algorithm that integrates model misspecification and data scarcity challenges, simplifying analysis and inspiring future research.  
- It includes a thorough introduction and comparison with prior work, enhancing clarity regarding its contributions.  
- The proposed framework is versatile, well-structured, and provides detailed theoretical analysis and guarantees for near-optimal performance.

Weaknesses:  
- The practicality of the algorithm is questionable, as it relies on tabular representations and solving robust Bellman equations, which may be difficult in real-world applications.  
- Empirical validation is limited to simple tasks, lacking comprehensive results on more complex benchmarks.  
- Claims regarding lower computational costs compared to existing methods are not empirically supported, and a detailed comparison of computational costs is needed.

### Suggestions for Improvement
We recommend that the authors improve the practical applicability of the algorithm by exploring its use with function approximations. Additionally, we suggest providing empirical results on more complex tasks to validate the effectiveness of the method and including a comparison of computational costs with other existing methods to substantiate claims of efficiency. Furthermore, elaborating on the advantages of this work compared to related frameworks, particularly regarding sample complexity, would enhance the paper's contributions.