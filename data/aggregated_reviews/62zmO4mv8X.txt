ID: 62zmO4mv8X
Title: Counterfactual Conservative Q Learning for Offline Multi-agent Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel offline multi-agent reinforcement learning algorithm called CounterFactual Conservative Q-Learning (CFCQL) to address the overestimation issue and achieve team coordination. CFCQL computes conservative regularization for each agent separately in a counterfactual manner and linearly combines them for overall conservative value estimation. The authors demonstrate that CFCQL maintains the underestimation property and performance guarantees similar to single-agent methods while improving regularization and safe policy improvement bounds, particularly advantageous with a large number of agents. Theoretical comparisons and extensive experiments show that CFCQL outperforms existing methods on various datasets. Additionally, the paper provides a detailed analysis of dataset composition and training procedures for the CFCQL algorithm, including a comprehensive breakdown of the datasets used, such as Equal_Line, Medium, Medium-Replay, Expert, and Mixed datasets, along with their respective trajectories, transition samples, and training steps. The authors also address an underfitting issue, indicating that longer training schedules may be necessary for CFCQL on larger datasets.

### Strengths and Weaknesses
Strengths:
- The method is well-motivated and theoretically validated, showing substantial improvements over state-of-the-art baselines.
- Empirical results are promising and extensive, demonstrating the effectiveness of CFCQL across multiple environments.
- The paper provides a detailed explanation of the proposed algorithm and its counterfactual approach.
- Strong offline RL baselines are provided in the multi-agent setting, enhancing the overall quality of the paper.
- Detailed descriptions of the experimental setup and dataset composition are included, improving clarity.

Weaknesses:
- The writing and presentation are unorganized, with vague technical claims and a high frequency of grammatical errors.
- The motivation for the algorithm is unclear, and the theoretical analysis lacks elaboration and context.
- Dataset ablations and the interpretation of results, particularly in Figure 3(b), are not well explained, raising concerns about scalability.
- The composition of the medium datasets raises questions about their relationship with larger dataset sizes, leading to uncertainty regarding the training of CFCQL on these datasets.
- The paper does not include sufficient offline RL baselines for comparison, and the claims regarding robustness and generalization are not well supported.
- The structure and design of the datasets may not align with established literature and training protocols, which affects the empirical evaluation of CFCQL.

### Suggestions for Improvement
We recommend that the authors improve the clarity and organization of the writing, ensuring that technical claims are well-supported by intuition and insights. The introduction should be revised for better language and motivation. We suggest adding straightforward independent learning baselines to the StarCraft results to enhance empirical comparisons. Additionally, please clarify the optimal Q-values in Section 4.1, define $\beta$ clearly, and provide intuitive explanations for theorems and the term "counterfactual." Addressing the dataset ablation results and providing a detailed dataset composition for experiments would also strengthen the paper. Furthermore, we recommend that the authors improve the dataset design to ensure alignment with literature and training protocols, particularly concerning the medium datasets and their scalability. Lastly, we encourage the authors to clarify how CFCQL should be trained on larger datasets to strengthen the empirical evaluation of their work.