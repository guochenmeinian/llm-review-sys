ID: R46HGlIjcG
Title: Localizing Memorization in SSL Vision Encoders
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into memorization within self-supervised learning (SSL) models, introducing two metrics, LayerMem and UnitMem, to identify where memorization occurs in neural networks. The authors find that memorization increases with layer depth, primarily occurring in fully connected layers of vision transformers (ViTs). They extend previous work by analyzing intermediate layers and propose that localizing memorization can enhance fine-tuning and inform pruning strategies.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and logically organized, with extensive empirical evaluations across various architectures and SSL methods.
- It introduces novel metrics for measuring memorization in SSL contexts, providing valuable insights into the distribution of memorization within networks.

Weaknesses:
- The contribution may be perceived as lacking novelty, primarily extending previous work without sufficient empirical comparison to existing literature.
- There is a lack of in-depth analysis regarding how memorized examples differ across random seeds and hyper-parameters, which limits understanding of memorization dynamics.
- The definitions of LayerMem and UnitMem raise concerns about their appropriateness for measuring memorization across layers, and the paper lacks a thorough discussion on the impact of normalization layers and weight decay on results.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contribution by providing a more comprehensive empirical comparison with existing literature, particularly with respect to Meehan et al.'s findings. Additionally, the authors should analyze how memorized examples vary across different random seeds and hyper-parameters, and discuss the implications of normalization layers and weight decay on memorization. Clarifying the definitions and justifications for LayerMem and UnitMem as metrics would also strengthen the paper. Finally, enhancing the presentation of results and including insightful explanations for observed phenomena would improve the overall clarity and impact of the findings.