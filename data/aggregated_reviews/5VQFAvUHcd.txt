ID: 5VQFAvUHcd
Title: Replicable Clustering
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 7, 6, 6, 6, 7, -1, -1
Original Confidences: 2, 4, 3, 2, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on replicable clustering algorithms, focusing on the replicability concept introduced by Impagliazzo et al. (2022). The authors propose algorithms for statistical clustering problems, including k-medians, k-means, and k-centers, emphasizing the importance of producing consistent results across different executions on samples from the same distribution. The paper introduces a novel framework that achieves an O(1)-approximation with polynomial complexity for k-medians and k-centers, although the latter has exponential sample complexity. The authors also develop a Replicable Quad Tree for replicable estimation, which is distinct from traditional methods.

### Strengths and Weaknesses
Strengths:  
- The paper is the first to explore replicable clustering, addressing a significant issue in machine learning.  
- It introduces a new framework and establishes sampling complexities for various clustering norms.  
- The Replicable Quad Tree method is innovative and has potential applications beyond this work.  
- The writing is clear, making complex concepts accessible to readers unfamiliar with the field.  

Weaknesses:  
- The sample complexity exhibits exponential dependence on the dimension \(d\), which poses challenges for high-dimensional data.  
- The main proofs are difficult to follow; intuitive explanations prior to proofs would enhance comprehension.  
- There is insufficient experimental validation using real-world datasets, limiting the practical applicability of the findings.  
- The focus on k-centers is not adequately reflected in the paper's title and introduction, leading to potential confusion regarding the main contributions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the proofs by providing intuitive insights before presenting them. Additionally, addressing the exponential dependence on dimension \(d\) and exploring dimensionality reduction techniques for general norms would strengthen the paper. We suggest including more experimental results with real-world datasets to validate the proposed algorithms. Furthermore, we encourage the authors to clarify how their replicable algorithms compare to existing methods and to elaborate on the implications of their findings in relation to clustering stability and other related work.