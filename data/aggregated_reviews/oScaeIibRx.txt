ID: oScaeIibRx
Title: Softmax Output Approximation for Activation Memory-Efficient Training of Attention-based Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 8, 7, 4, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Softmax Output Approximation algorithm, which approximates softmax outputs during the forward pass and reconstructs them during the backward pass, significantly reducing memory usage in attention-based models like Transformers. The authors claim memory savings of up to 84% compared to existing softmax implementations across various tasks. Additionally, the authors propose a method for improving attention mechanisms in Transformer models through a ratio calculation to evaluate the complexity of their approach compared to baseline methods. They acknowledge the need for more detailed comparisons with pivotal works, particularly 'Long Range Arena', and plan to enhance their error bound analysis. The authors also recognize the current limitations of their implementation in terms of execution time and are working on optimizing it using CUDA Kernel.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to understand.
- The mathematical analysis of approximating softmax outputs is reasonable, particularly the novel approach of using sorting without relying on pruning or quantization.
- Experimental results show effective memory reduction during pretraining or fine-tuning with comparable accuracy to baselines.
- The authors demonstrate a clear understanding of their method's strengths and weaknesses and show willingness to improve their work based on constructive feedback.

Weaknesses:
- Low accuracy on the IMDb dataset raises concerns about the method's applicability to other challenging tasks.
- The claimed negligible approximation time overhead is questionable, especially considering sorting large softmax outputs.
- The initial ratio calculation lacks specificity and does not account for the complexity of attention modules.
- The current implementation is not optimized for execution time, which may affect performance.
- Insufficient comparisons with existing algorithms that approximate activations, such as ActNN, GACT, and Mesa, may limit the paper's impact.

### Suggestions for Improvement
We recommend that the authors improve the accuracy on the IMDb tasks, potentially through hyperparameter tuning, and provide a qualitative explanation if challenges persist. Additionally, it would be beneficial to report the increase in training time compared to the baseline to validate the approximation time overhead claim. We suggest improving the specificity of the ratio calculation by detailing how it accounts for the complexity of attention modules. Furthermore, including thorough comparisons with existing algorithms in the Related Work section, particularly focusing on GACT and Mesa, would strengthen the discussion. It is also advisable to enhance the error bound analysis and ensure that the correct error bounds are presented. Lastly, we encourage the authors to optimize their implementation using CUDA Kernel to reduce time overhead and conduct additional experiments on hyperparameter tuning for the IMDb task.