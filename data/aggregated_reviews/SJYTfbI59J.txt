ID: SJYTfbI59J
Title: Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the zero-shot ranking effectiveness of large language models (LLMs) in information retrieval, specifically focusing on LLMs as query language models (QLMs). The authors find that LLMs exhibit robust zero-shot ranking abilities without fine-tuning and propose a ranking system that integrates LLM-based QLMs with a hybrid zero-shot retriever, achieving state-of-the-art results. The study highlights the competitive effectiveness of zero-shot QLMs compared to fine-tuned QLMs, suggesting that pre-trained models like LLaMA and Falcon possess strong zero-shot ranking capabilities. However, the paper raises concerns regarding the authenticity of zero-shot settings in pre-training data and the limitations of focusing solely on open-source LLMs.

### Strengths and Weaknesses
Strengths:
- The paper provides a comprehensive study on the zero-shot ranking ability of LLMs, revealing significant insights into their effectiveness without additional fine-tuning.
- It demonstrates that integrating LLM-based QLMs with a hybrid zero-shot retriever results in a novel ranking pipeline that excels in both zero-shot and few-shot scenarios.
- The experiments are detailed, and the code is publicly available for reproducibility.

Weaknesses:
- The limited focus on open-source LLMs restricts the applicability of the proposed method, overlooking the potential of powerful closed-source models.
- There is a lack of complete statistical significance analysis, hindering comprehensive evaluation and comparison with previous methods.
- The paper does not adequately address the authenticity of the zero-shot setting in the pre-training data and lacks a thorough discussion on why instruction fine-tuning may degrade ranking performance.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the authenticity of the zero-shot setting in the pre-training data of LLaMA and Falcon, providing a thorough analysis to identify any potential task leakage. Additionally, we suggest conducting a complete statistical significance analysis of the results to enhance the evaluation of the proposed approach. The authors should also consider exploring the performance of their method with powerful closed-source models and include a broader range of BEIR benchmarks in their experiments. Finally, we encourage the authors to clarify the rationale behind the choice of the interpolation parameter value and to investigate its optimal settings across different LLMs.