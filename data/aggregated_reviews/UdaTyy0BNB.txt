ID: UdaTyy0BNB
Title: Double Gumbel Q-Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 6, 4, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel off-policy algorithm, DoubleGum, which utilizes a Gumbel distribution to model temporal difference (TD) errors instead of the traditional homoscedastic normal distribution. The authors argue that this approach provides a more accurate representation of the noise in Q-learning, leading to improved performance in both discrete and continuous control tasks. The paper includes extensive empirical evaluations across various environments, demonstrating the algorithm's potential advantages. Additionally, the authors conduct a detailed analysis of the Mean-Squared Bellman Error in reinforcement learning, specifically focusing on the semi-gradient update rule and its implications for loss function convergence. They derive the Taylor expansion of both the online and target Q-values, leading to a comprehensive expression for the change in loss, \(\delta L\), which is critical for understanding the convergence properties of the Q-learning algorithm.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and organized, providing clear empirical evidence that TD errors are better modeled by a Gumbel distribution.
- The proposed algorithm, DoubleGum, shows promising results in various tasks, including complex environments.
- Detailed reporting of hyperparameters and computational resources enhances reproducibility.
- The derivation of the Taylor expansion for the Q-values is thorough and mathematically rigorous, providing clear insights into the loss function's behavior.
- The paper effectively connects the theoretical aspects of reinforcement learning with practical implications for convergence.

Weaknesses:
- The theoretical foundation of the algorithm is weak, as it lacks a solid convergence guarantee and relies on assumptions that may not hold in practice.
- The empirical results do not convincingly demonstrate that the advantages stem from the Gumbel distribution, as the practical implementation diverges from the theoretical model.
- The choice of baselines for comparison, particularly the absence of SAC, raises questions about the robustness of the results.
- The review indicates a lack of familiarity with the reference DR3, suggesting that the context or background may not be sufficiently established for all readers.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their figures to better illustrate the stability of DoubleGum compared to existing algorithms. Additionally, we suggest including SAC in the experimental comparisons to provide a more comprehensive evaluation of DoubleGum's performance. It would also be beneficial to provide a rigorous theoretical analysis of the convergence properties of the proposed algorithm. Furthermore, we encourage the authors to clarify the motivation behind using Gumbel noise in the new function approximator and to address the empirical evidence supporting the connection between DoubleGum and pessimistic value estimation more thoroughly. Lastly, we recommend that the authors improve the accessibility of the paper by providing additional context or explanations regarding the reference DR3 to enhance reader comprehension.