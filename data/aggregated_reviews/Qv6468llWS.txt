ID: Qv6468llWS
Title: PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 8, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called PDE-Refiner aimed at predicting future states of partial differential equations (PDEs) from initial conditions using deep-learning surrogate simulators over long rollouts. The authors propose a combination of autoregressive solution rollouts, direct next-step predictions through MSE Training, and a refinement process inspired by diffusion models to enhance prediction accuracy. The authors clarify that the same neural network generates both $\hat{u}^1$ and $\hat{\epsilon}^k$, asserting that all prediction targets share a standard deviation of around 1, thus maintaining scale consistency. The methodology is evaluated on the 1D Kuramoto-Sivashinsky equation and a 2D Kolmogorov Flow, with extensive ablations and supplementary materials provided for reproducibility. The paper also discusses the relationship between PDE-Refiner and diffusion models, particularly in Section 3.1, and explains the inclusion of Equation (5) to illustrate this connection.

### Strengths and Weaknesses
Strengths:
- The paper addresses the significant challenge of predicting PDE solutions, relevant across various fields such as medicine and climate science.
- It explores the novel application of diffusion-like architectures, which have potential to enhance traditional numerical solvers.
- The clarity of presentation and detailed methodology allows readers to follow the arguments easily, even if they are not experts in the field.
- Comprehensive evaluations and thorough discussions of results, including error bars and reproducibility details, strengthen the findings.
- The paper demonstrates significant improvements in model performance across various architectures, validating the applicability of PDE-Refiner.
- The authors effectively address reviewer concerns regarding model generality and clarify the relationship between PDE-Refiner and diffusion models.
- The experimental results show that PDE-Refiner outperforms MSE baselines, even with models having a higher parameter count.

Weaknesses:
- The comparison of results for the 2D Kolmogorov Flow appears problematic due to differing computation methods for ML surrogates and classical solvers, raising concerns about comparability.
- Generalization tests are limited as they do not explore interpolation or extrapolation abilities beyond the training domain, which is critical for PDE simulations.
- The model's efficiency is questioned, particularly regarding the time consumption and GPU memory usage, which could affect practical applicability.
- Some reviewers expressed a need for further elaboration on the rationale behind specific equations, such as Equation (5).
- The potential benefits of separating neural operators were not fully explored, which could leave some questions about the model's flexibility.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the correlation thresholds used in their evaluations and discuss how varying these thresholds might impact findings on rollout stability. Additionally, further evaluations should be conducted to clarify the performance differences observed between the proposed method and LI-CNN, particularly regarding the contributions of the U-Net architecture versus the refinement process.

To enhance generalization testing, we suggest that the authors explore holding out certain parameter ranges during training to assess interpolation and extrapolation capabilities. Furthermore, addressing the limitations related to the diffusion model's data requirements and efficiency is crucial; comparisons with advanced neural operators beyond U-Net should be included to validate the method's effectiveness. 

We also recommend that the authors improve the explanation of Equation (5) to provide clearer insights into its formulation and relevance. Additionally, consider discussing the implications of separating the neural operators in more detail, even if the current setup is preferred, to address any lingering concerns about model flexibility. Lastly, we encourage a more detailed discussion on the relationship between the denoising process and the various frequency amplitudes to improve reader comprehension.