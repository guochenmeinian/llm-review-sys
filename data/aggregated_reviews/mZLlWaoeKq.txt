ID: mZLlWaoeKq
Title: JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 6, 7, 6, -1, -1, -1, -1, -1
Original Confidences: 5, 2, 3, 5, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents JourneyBench, a Visual Language Understanding (VLU) benchmark designed to evaluate models on their ability to understand and reason about generated images across five diverse tasks. The authors propose a novel adversarial Human-Machine-in-the-Loop (HMIL) framework to enhance the generation of high-quality data. The benchmark aims to rigorously assess models' advanced reasoning capabilities, particularly in scenarios involving imaginary images, thereby exposing biases and hallucination tendencies.

### Strengths and Weaknesses
Strengths:
- JourneyBench covers five challenging tasks, including multi-image arithmetic VQA and VQA with hallucination triggers, addressing critical issues often overlooked in existing benchmarks.
- The HMIL framework effectively generates high-quality evaluation data, ensuring robust and difficult tasks for current models.
- The paper provides a comprehensive analysis of model performance across various dimensions, contributing to a clearer understanding of current limitations in multimodal reasoning.

Weaknesses:
- The benchmark primarily presents tasks qualitatively; the authors should include quantitative data statistics to better understand the benchmark's composition.
- The connection between the five tasks is not clearly justified, raising questions about the rationale for their selection.
- The total number of samples (~13.5K) may be insufficient for comprehensive evaluation across all edge cases and model capabilities.
- There is a lack of detailed error analysis linking model performance to training data, which could enhance understanding of specific weaknesses in VLMs.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by providing quantitative data statistics to clarify its composition. Additionally, a clearer justification for the selection of the five tasks is necessary to establish their relevance. To enhance the robustness of the evaluation, consider increasing the sample size for smaller subsets, particularly for the multi-image VQA task. We also suggest including comparisons with human preference-learned models to provide a more comprehensive evaluation of model performance. Furthermore, a detailed analysis of VLM errors should be compiled, linking model training data and performance to the evaluation dataset. Finally, it is crucial to address ethical concerns regarding the depiction of real individuals in generated images, ensuring that privacy policies are adhered to rigorously.