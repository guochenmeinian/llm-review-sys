ID: vscmppXqXE
Title: GEMINI: Controlling The Sentence-Level Summary Style in Abstractive Text Summarization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GEMINI, a model for text summarization that enables switching between "extractive" and "abstractive" summarization styles. The authors propose a method to automatically detect the summary style for each sentence, integrating this capability into a pre-trained BART model. GEMINI is evaluated on three benchmarks—CNN/DM, XSum, and WikiHow—showing improvements over BART and other baselines. The contributions primarily focus on NLP engineering experiments.

### Strengths and Weaknesses
Strengths:
- The model's ability to switch between summarization styles is novel, combining the strengths of both extractive and abstractive approaches.
- Thorough experimental setup with well-supported claims and a clear presentation of results.
- The paper is well-written, particularly the Analysis Section, which discusses the model's performance in relation to task abstractiveness.

Weaknesses:
- The sample size of human evaluation (30 documents) is too small.
- There is a contradiction regarding the performance on WikiHow, where GEMINI generates fewer extractive sentences than human labels suggest.
- The paper lacks a limitations section, which is necessary.
- The human evaluation lacks depth, with unclear details about the evaluators and scoring methods.
- Limited baseline comparisons raise questions about the model's relative performance.

### Suggestions for Improvement
We recommend that the authors improve the human evaluation by providing more details about the evaluators and the scoring process, including rater agreement levels. Additionally, clarifying the distinction between GEMINI and BART-Rewriter would help mitigate confusion regarding the model's unique contributions. The authors should also consider including comparisons with additional BART-based baselines to strengthen their claims. Finally, we suggest adding a limitations section to address potential shortcomings of the model and its evaluation.