ID: PHh1s8dNlY
Title: DIVE: Towards Descriptive and Diverse Visual Commonsense Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 6
Original Ratings: -1, -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework named DIVE aimed at enhancing the descriptiveness and diversity of visual commonsense generation through generic inference filtering and contrastive retrieval learning. The authors propose a method to filter out generic descriptions from the training dataset, thereby improving the quality of generated outputs. Experimental results demonstrate that DIVE achieves state-of-the-art performance in terms of descriptiveness and diversity, supported by both quantitative metrics and human evaluations.

### Strengths and Weaknesses
Strengths:
- The focus on improving descriptiveness and diversity in visual commonsense generation is significant and relevant.
- The paper is well-written, easy to understand, and includes thorough experimental evaluations.
- DIVE shows substantial improvements over existing models, as evidenced by various metrics and human assessments.

Weaknesses:
- The novelty of the approach is limited, as it relies on established methods like contrastive learning without a clear explanation of how it enhances generation diversity.
- The metrics used to assess descriptiveness and diversity may not adequately capture the specific visual commonsense knowledge, raising questions about the validity of the claims.
- The methodology lacks detail in certain areas, such as the definition of "similar" images and the specific vision-language models used for feature extraction.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology by explicitly defining how "similar" images are determined and specifying which vision-language models are employed for feature extraction. Additionally, we suggest that the authors provide more comprehensive comparisons with other methods, such as unlikelihood training and contrastive decoding, to strengthen their experimental results. It would also be beneficial to address the limitations of the adopted metrics in capturing detailed visual commonsense knowledge. Lastly, including a random sample of outputs and clarifying the adaptation of models like GPT-2 and BART for vision input would enhance the paper's rigor.