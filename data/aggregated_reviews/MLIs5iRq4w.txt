ID: MLIs5iRq4w
Title: Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 5, 6, 6, 4, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for distilling knowledge from pre-trained diffusion models (DMs) into generative models, specifically GANs, using a framework called Diff-Instruct. The authors propose a novel Integral Kullback-Leibler (IKL) divergence for this purpose, which is robust for comparing distributions with misaligned supports. The effectiveness of Diff-Instruct is demonstrated through experiments on datasets like CIFAR and ImageNet64, achieving state-of-the-art performance in single-step diffusion-based models and enhancing existing GANs. Additionally, the paper examines the training-from-scratch setting for Diff-Instruct, addressing the mode-seeking issue associated with reverse KL divergence. The authors identify specific training techniques, such as noise level truncation and using a larger learning rate for \(s_\phi\), to mitigate challenges encountered during training. The results indicate that while training from scratch yields an FID of 27.5 and an Inception Score of 9.88, these metrics are inferior to those achieved with an optimally initialized generator. The authors emphasize the importance of initializing the generator using Tweedie's formula from the teacher model to enhance performance and stability, and they suggest that exploring new techniques, including a likelihood-ratio estimator, could further improve training outcomes.

### Strengths and Weaknesses
Strengths:
- The paper addresses the significant challenge of distilling diffusion models, contributing to faster inference.
- The IKL divergence formulation is innovative and adds depth to the method.
- Experimental results indicate strong performance, particularly in one-step distillation and GAN refinement.
- The paper provides a clear and detailed analysis of the training-from-scratch approach and its associated challenges.
- The proposed initialization technique using Tweedie's formula demonstrates significant potential for improving generator performance.
- The authors acknowledge the mode-seeking issue and present empirical evidence of its occurrence.

Weaknesses:
- The practical utility of the distillation results in larger domains remains uncertain, as the method's effectiveness with larger images is unclear.
- The requirement for two additional models (student and auxiliary) may hinder scalability in larger domains.
- Notation inconsistencies and unclear objectives regarding knowledge transfer versus distillation are present.
- The performance metrics from training from scratch are notably lower than those from pre-trained models, indicating room for improvement.
- The discussion on potential new techniques for enhancing training-from-scratch could be expanded.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their notation and ensure consistency with existing literature. Additionally, the paper should explicitly define the primary objective of knowledge transfer. We suggest including a baseline method that uses the same teacher model and generator architecture for comparison. Furthermore, the authors should elaborate on the convergence of the proposed algorithm and clarify the role of the auxiliary model post-training. Lastly, addressing the time required for distillation and providing a detailed explanation of the robustness of IKL divergence would enhance the paper's clarity and impact. We also recommend that the authors improve the discussion on potential new techniques for training-from-scratch, particularly drawing inspiration from adversarial training and text-to-3D generation. Exploring regularization methods for \(s_\phi\) and techniques from GANs, such as non-saturating divergences, could provide valuable insights into stabilizing Diff-Instruct. Additionally, further investigation into noise schedule annealing from text-to-3D generation algorithms may yield beneficial strategies for diffusion distillation.