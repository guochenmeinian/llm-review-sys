ID: fC2SV2sQ8J
Title: LaKD: Length-agnostic Knowledge Distillation for Trajectory Prediction with Any Length Observations
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the LaKD method aimed at enhancing trajectory predictions for variable input observation lengths. LaKD employs dynamic length-agnostic knowledge distillation, where training samples are augmented by randomly masking input observations at various lengths. The prediction with the lowest error serves as the teacher prediction, while other predictions distill information from it using KL divergence loss on feature embeddings. Additionally, the authors introduce gradient weighting based on neuron importance to mitigate the impact of knowledge distillation on teacher predictions. The method is applicable to various trajectory prediction models, demonstrated through evaluations on the HiVT and QCNet models across Argoverse 1, Argoverse 2, and nuScenes datasets, showing improved performance over standard baselines.

### Strengths and Weaknesses
Strengths:
- The evaluation is thorough, applying the method to two popular trajectory prediction models and three public datasets.
- The proposed method outperforms baselines, including a recent CVPR 2024 work.
- The paper is well-written and easy to follow.

Weaknesses:
- The performance improvement over the naive Random Masking baseline is not significant, raising doubts about the practical complexity of the method.
- The qualitative analysis lacks clarity, particularly in demonstrating the advantages of the proposed method over others.

### Suggestions for Improvement
We recommend that the authors improve the qualitative results to clearly illustrate the scenarios where their method excels compared to others. Additionally, incorporating a broader range of backbone models beyond HiVT and QCNet would strengthen claims of generality. Clarifying the framework's validity in situations where trajectory information differs significantly would also enhance understanding. Finally, addressing the performance degradation observed in Ablation Table 3 when M exceeds 3 would provide valuable insights into the method's behavior.