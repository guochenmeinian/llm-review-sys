ID: C66DBl9At8
Title: Exploring Memorization and Copyright Violation in Frontier LLMs: A Study of the New York Times v. OpenAI 2023 Lawsuit
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 7, 6, 5
Original Confidences: 4, 4, 2

Aggregated Review:
### Key Points
This paper presents an extensive study on the memorization of LLMs in the context of the New York Times lawsuit, comparing four models across three attacks using five metrics. The analysis indicates that while LLMs do memorize articles, the New York Times's claims may exaggerate this phenomenon, with OpenAI's model showing less memorization than its competitors. The authors explore the implications of memorization capabilities concerning copyright concerns in training data.

### Strengths and Weaknesses
Strengths:
- Extensive analysis of a particularly relevant subject, given the ongoing lawsuit.
- Exhaustive experiments that substantiate the validity of the claims.
- Very well-written, making the paper clear and easily understandable.

Weaknesses:
- Some legal background details may be overly detailed for the main text and could be better placed in the appendix.
- The evaluation of OpenAI's output filtering in relation to the lawsuit may affect the comparison of memorization capabilities with other LLMs that do not employ similar filtering.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the impact of OpenAI's output filtering on memorization comparisons by specifying whether other LLMs have similar features. Additionally, consider moving some legal background information, such as "legal aspects of memorization," to the appendix for better flow. It would also be beneficial to review findings from the paper titled "Spotting LLMs with Binoculars: Zero-Shot Detection of Machine-Generated Text" for insights on mitigating memorization of sensitive data. Lastly, clarify the implications of the absence of exact matches for legal risks faced by LLM providers like ChatGPT.