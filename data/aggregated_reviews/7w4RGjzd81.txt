ID: 7w4RGjzd81
Title: Unbiased Watermark for Large Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 3, 5, 5, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on watermarking large language models (LLMs) without compromising output quality, introducing "unbiased watermarking" that avoids trade-offs seen in prior work. The authors propose two novel techniques, $\delta$-reweight and $\gamma$-reweight, and an improved likelihood ratio test for detection. The discussion includes the risks associated with LLMs and how unbiased watermarking can facilitate responsible AI.

### Strengths and Weaknesses
Strengths:
1. The introduction of unbiased watermarking that maintains output quality, addressing trade-offs noted in previous research.
2. The proposal of novel $\delta$-reweight and $\gamma$-reweight techniques that effectively preserve output quality in experiments.
3. Development of an improved detection method with a proven upper error bound, enhancing detection reliability.
4. Concrete demonstration of the efficacy of watermarking techniques in maintaining the utility of LLMs for downstream tasks.

Weaknesses:
1. The paper lacks a thorough examination of the watermark detection method's efficacy, particularly regarding detection accuracy, robustness to interference, and computational efficiency.
2. Experiments are limited to BART language models, with no evaluation of other popular LLMs like GPT models.
3. Additional experiments using LLMs for a broader range of natural language tasks beyond text summarization and machine translation would strengthen claims about output quality preservation.
4. There is no discussion on the resilience of the proposed watermarking methods against potential adversarial attacks or interference attempts.

### Suggestions for Improvement
We recommend that the authors improve the examination of the watermark detection method's efficacy by providing detailed insights into detection accuracy, robustness, and computational efficiency. Expanding the experiments to include other popular LLMs, such as GPT, and additional natural language tasks would enhance the generalizability of the findings. Furthermore, discussing the resilience of the watermarking methods against adversarial attacks would provide a more comprehensive understanding of their effectiveness. Lastly, clarifying the purpose of Table 1 and Figure 3, as well as simplifying complex sections, would improve the paper's clarity and presentation.