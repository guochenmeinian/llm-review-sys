ID: M7zNXntzsp
Title: Building a stable classifier with the inflated argmax
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 7, 6, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new theoretical framework for studying algorithmic stability in multiclass classification, emphasizing stable class selection based on predicted class probabilities. The authors propose a selection criterion called "inflated argmax," which is theoretically shown to be stable and effective in generating a tight candidate prediction set. The paper includes simple experiments using Fashion-MNIST to demonstrate the stability of the inflated argmax. Additionally, the authors introduce several notions on classifier stability and discuss classification with abstention, highlighting its potential for improving classifier reliability. The inclusion of additional algorithms in the experiments enhances the practical applicability of the proposed method.

### Strengths and Weaknesses
Strengths:
1. The paper provides a robust theoretical framework for stable selection criteria in multiclass classification.
2. The notion of $\epsilon$-compatibility is a clear and useful extension of classical algorithmic stability, aiding further analysis.
3. The authors present a simple rule that, while potentially looser than inflated argmax, enhances understanding of the problem.
4. Efficient computation of the selection rule post-bagging is well-articulated in Proposition 10.
5. The inflated argmax method is theoretically justified and performs competitively across multiple evaluation criteria.
6. The inclusion of additional algorithms in the experimental section enhances the practical relevance of the research.

Weaknesses:
1. The experimental validation is limited and does not sufficiently highlight the effectiveness of inflated argmax, lacking reasonable baselines that allow for multiple class predictions.
2. The reliance on bagging introduces computational challenges, particularly for large datasets or resource-constrained scenarios.
3. The presentation of the inflated argmax and its relation to bagging could be clearer, with some theoretical aspects not directly tied to the main contributions.
4. Limited experiments and high computational costs associated with bagging are concerns.
5. Some notational clarity issues were noted, particularly regarding feature representation in definitions.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including additional datasets and classifiers to better demonstrate the advantages of the proposed framework. Specifically, providing performance metrics for inflated argmax without bagging could clarify its necessity. Visualizing the number of bags required for reasonable performance would also be beneficial. Furthermore, addressing the impact of overconfidence in probability scores on the inflated argmax in practice is crucial. We suggest exploring alternative baselines, such as top-2 class prediction or a threshold-based approach for class selection, to enhance comparative analysis. Lastly, we recommend improving the clarity of notation in definition 1 by explicitly including the notation of feature $x$ in $C$, and conducting further analysis of the problem setting to address the computational costs associated with bagging. A sensitivity analysis on the choice of the tolerance $\epsilon$ would also strengthen the paper's robustness.