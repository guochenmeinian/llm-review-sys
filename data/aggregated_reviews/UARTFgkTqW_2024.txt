ID: UARTFgkTqW
Title: MagR: Weight Magnitude Reduction for Enhancing Post-Training Quantization
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 8, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an optimization-based preprocessing technique called Weight Magnitude Reduction (MagR) aimed at enhancing the performance of post-training quantization (PTQ) for large language models (LLMs). The authors motivate their method by demonstrating that a non-linear transformation of weights can facilitate sub-4bit quantization without requiring additional post-processing during inference. MagR minimizes the $\ell_{\infty}$ norm of weights while preserving layer outputs, allowing for effective quantization with minimal accuracy loss. The optimization is framed as an $\ell_{\infty}$-regularization problem, solved via proximal gradient descent.

### Strengths and Weaknesses
Strengths:
- MagR achieves state-of-the-art weight quantization without introducing additional computational overhead, a significant advancement over competing methods that require linear transformations.
- The authors conduct extensive evaluations across various datasets and benchmarks, demonstrating superior performance compared to strong competitors like OmniQuant.
- The novel approach of regularizing the weight's $\ell_{\infty}$ norm is both innovative and effective.

Weaknesses:
- The paper lacks comprehensive ablation studies, particularly regarding the penalty factor $\alpha$ and its impact on accuracy, as well as runtime comparisons with methods like QuIP.
- The theoretical justification connecting the rank-deficient feature matrix and $\ell_{\infty}$ regularization is insufficient, and a more general formulation as an $\ell_p$ constraint would strengthen the method.
- Presentation issues include unclear relevance of statistics in Table 1, missing activation quantization bitwidth in Table 3, and a misleading section title regarding runtime.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper's presentation, particularly in the title and abstract, to better reflect the contribution to LLM compression. Additionally, providing a thorough discussion on the generalization ability, robustness, and potential applications of MagR would enhance the paper's impact. We suggest including ablation studies on the penalty factor $\alpha$ to assess its sensitivity and exploring the performance of MagR with other quantization methods beyond MagR+OPTQ. Furthermore, a detailed theoretical analysis of the optimality of the $\ell_{\infty}$ norm and its implications for quantization should be included. Lastly, we urge the authors to provide runtime comparisons with QuIP and other methods to substantiate claims regarding efficiency.