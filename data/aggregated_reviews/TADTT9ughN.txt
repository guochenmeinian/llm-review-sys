ID: TADTT9ughN
Title: Deep Bayesian Active Learning for Preference Modeling in Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents BAL-PM, a Bayesian active learning framework designed to efficiently learn a preference model by selecting informative prompt-response pairs. The authors address the issue of redundant sampling in previous methods by incorporating both task-dependent and task-agnostic uncertainty to enhance sample diversity. Experiments conducted on Reddit and CNN/DM datasets demonstrate that BAL-PM significantly reduces the amount of training data required compared to random sampling and the BALD baseline, achieving reductions of 33% and 68%, respectively.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and easy to follow.
- The motivation for the proposed method is clearly articulated, and the empirical results support its effectiveness.
- The method shows strong performance across various model sizes, indicating scalability.

Weaknesses:
- The experimental evaluation lacks clarity, particularly regarding the log-likelihood metric and its calculation, raising concerns about its sufficiency for assessing preference modeling.
- The necessity of using the base LLM's feature space for entropy estimation is not well justified, potentially leading to high computational overhead.
- The reliance on an ensemble of adapters for batch collection may complicate large-scale implementation.
- The discussion of related works is insufficient, particularly regarding other active preference optimization techniques.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental section, particularly by providing detailed explanations of the log-likelihood metric and considering additional evaluation metrics across diverse datasets. We also suggest that the authors clarify the rationale for using the feature space of the base LLM for entropy estimation and address potential computational costs. Furthermore, enhancing the discussion of prior works in active preference optimization would strengthen the paper. Lastly, we encourage the authors to explore the inclusion of a preference simulator for non-pool-based settings to validate the method's effectiveness in a broader context.