ID: xrk9g5vcXR
Title: QuIP: 2-Bit Quantization of Large Language Models With Guarantees
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents QuIP, an algorithm for weight quantization in large language models (LLMs), achieving nearly lossless performance with 3-bit quantization for models larger than 3B and good performance with 2-bit quantization for models larger than 7B. The authors propose the LDLQ rounding method, which is optimal within a family of adaptive rounding methods, and introduce incoherence preprocessing, leading to significant accuracy improvements for low bitwidth compression. The paper includes theoretical guarantees and extensive experimental results, demonstrating the effectiveness of QuIP compared to existing methods like OPTQ. While QuIP outperforms competitive methods at the 2-bit quantization level, its practicality is limited, particularly when considering smaller models with higher bit-widths that may be more effective within a given memory budget. The review emphasizes the need for transparency in presenting results, especially in comparing different model options under fixed memory budgets.

### Strengths and Weaknesses
Strengths:
- QuIP is the first post-training quantization algorithm to compress weights to 2 bits with reasonable performance, enhancing LLM deployment.
- The LDLQ method is both worst-case and average-case optimal, with solid theoretical proof and a detailed analysis of its performance.
- The paper provides a comprehensive theoretical framework and empirical validation, including code for reproducibility.
- The authors effectively address reviewer feedback and demonstrate improvements in their responses.
- The paper shows that QuIP achieves better performance than competitors at 2-bit quantization.

Weaknesses:
- The evaluation is primarily conducted on the OPT model, which may not represent current standards; testing on more popular models like LLaMA or Falcon would strengthen the findings.
- The performance of 2-bit quantization, while improved, still lags behind that of 4-bit quantization methods, raising concerns about practical usability.
- The practicality of 2-bit quantization is questioned, as smaller models with higher bit-widths may be more effective within a given memory budget.
- The concept of incoherence is not clearly defined until later in the paper, which could confuse readers.
- The inference speed measurements lack precision and fair comparison with OPTQ.

### Suggestions for Improvement
We recommend that the authors expand the evaluation to include additional LLM families, such as LLaMA or Falcon, to validate the generalizability of QuIP. We suggest providing a clearer definition and intuitive explanation of incoherence in the introduction to enhance reader understanding. It would be beneficial to include runtime measurements for inference speed to assess the practical implications of the proposed methods, along with more precise measurements and a fair comparison of inference speed with OPTQ. Additionally, we recommend improving the transparency of results by including tables that compare different model options under fixed memory budgets. Finally, we recommend correcting minor typographical errors, such as "OBC" to "OBQ" in line 64 and "OTPQ" to "OPTQ" in line 217, and to include citations for LDL decomposition and the results of applying equation (4) in equation (3) for clarity.