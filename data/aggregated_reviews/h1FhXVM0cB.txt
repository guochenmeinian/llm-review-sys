ID: h1FhXVM0cB
Title: Improved Convergence in High Probability of Clipped Gradient Methods with Heavy Tailed Noise
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 6, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an improved convergence analysis of clipped gradient descent methods under heavy-tailed noise, focusing on high-probability complexity bounds for both convex and non-convex optimization problems. The authors propose a novel analysis framework that enhances the dependency on failure probability $\delta$ compared to prior works. They analyze Clipped-SGD, Clipped-SMD, and Clipped-ASMD, achieving time-optimal rates that align with existing lower bounds.

### Strengths and Weaknesses
Strengths:
- The authors provide a new analysis framework for high-probability error bounds that is more flexible than classical approaches, allowing for unknown time horizons and improved dependency on $T$.
- The paper is well-structured and clearly written, making the results accessible.
- The analysis includes significant contributions, such as high-probability convergence for constrained problems without bounded variance assumptions and results independent of the time horizon $T$.

Weaknesses:
- The deterministic term in the non-convex case is not optimal, particularly as $p \to 1$, where it should be $O(T^{-1})$ instead of $O(T^{\frac{2-2p}{3p-2}})$.
- Some proofs, such as those for Lemma 3.4 and Theorem B.2, are incomplete and require more detailed sketches.
- The assumptions made, particularly regarding the bounded moments of the gradient noise, may be too strong for certain settings like mini-batch scenarios.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the comparisons with prior works by including a table that outlines convergence rates and assumptions. Additionally, the authors should discuss the implications of the deterministic term and logarithmic factors in more detail, particularly in relation to existing results. We also suggest providing complete proofs for all key results and clarifying the assumptions regarding the gradient noise to enhance the robustness of the analysis.