ID: VN298kRz91
Title: Romanization-based Large-scale Adaptation of Multilingual Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of universal romanization using UROMAN to enhance the cross-lingual transfer capabilities of pre-trained language models (PLMs) for low-resource and unseen languages. The authors demonstrate that UROMAN can effectively transliterate non-Latin scripts, yielding competitive performance in named entity recognition (NER) and dependency parsing (DP). The findings indicate that UROMAN performs comparably to language-specific transliteration systems and shows promise in leveraging existing model information through overlapping vocabulary items.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant issue in multilingual NLP and provides robust analysis demonstrating the sample efficiency of the proposed method.
- UROMAN shows competitive performance against language-specific transliteration and vocabulary adaptation, with results averaged over multiple random seeds enhancing robustness.
- The experimental setup is diverse, and the paper is well-written with clear descriptions of the experiments.

Weaknesses:
- The evaluation is limited to only two tasks, which may not fully represent the effectiveness of UROMAN across different NLP applications.
- The novelty of the work is somewhat constrained, as some findings reiterate established knowledge in the literature regarding transliteration and vocabulary adaptation.
- The paper lacks comparisons with other methods, such as byte-based tokenization and out-of-the-box mBERT models, which would strengthen the results.

### Suggestions for Improvement
We recommend that the authors improve the literature review by including references to byte-based and pixel-based tokenization methods, along with justifications for the preference of romanization. Additionally, the authors should present baseline results from finetuning the entire mBERT model, as this is a reasonable practice and would enhance the robustness of their findings. Including results for out-of-the-box mBERT and details on training data sizes for language adapters would also be beneficial. Finally, we suggest conducting significance tests to address the variance in performance across individual languages and clarifying whether results are averages over multiple runs.