ID: lzfzjYuWgY
Title: Do LLMs Build World Representations? Probing Through the Lens of State Abstraction
Conference: NeurIPS
Year: 2024
Number of Reviews: 27
Original Ratings: 7, 4, 6, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the types of abstractions that large language models (LLMs) use to encode the world, distinguishing between goal-oriented abstractions and world-general abstractions. The authors propose a framework based on state abstraction theory to assess different levels of abstraction, including world-irrelevant, $Q^*$-irrelevant, and $\pi^*$-irrelevant abstractions. They demonstrate this framework's application through a synthetic planning task called REPLACE, finding that LLMs, particularly when fine-tuned, favor goal-oriented abstractions while general world abstractions are less successfully recovered. The study highlights that existing conflicts in literature arise from varying degrees of alignment between raw states and goal-oriented abstractions across tasks. The authors also emphasize that certain predicates can be effectively probed from LLMs adapted to specific tasks, indicating that supervised fine-tuning (SFT) primarily enhances goal-oriented abstractions. Furthermore, the authors address concerns regarding the lack of internal world models in LLMs, suggesting that fine-tuning does not yield an internal world model for the task at hand and that the performance of LLMs in in-context learning (ICL) is uninformative due to poor results.

### Strengths and Weaknesses
Strengths:
- The paper presents a novel framework for probing different types of state abstractions in LLMs, contributing to a deeper understanding of emergent world representations.
- It is well-written and clearly structured, making the methodology and findings accessible.
- The REPLACE task is thoughtfully designed, providing a useful testbed for future research on LLM world representations.
- The authors effectively address reviewer concerns regarding the probing of pre-trained LLMs and the presentation of results.
- The engagement during the rebuttal process demonstrates a commitment to improving the paper's quality.

Weaknesses:
- The experimental results are limited to the REPLACE task, which may not generalize to other contexts, such as Othello or more complex datasets.
- The findings primarily reflect the behavior of fine-tuned LLMs, raising questions about the generalizability of the results to models that perform well without task-specific adaptation.
- The lack of baseline comparisons with non-LLM models makes it difficult to interpret the significance of the results.
- The presentation of results requires substantial rewrites for clarity.
- The findings on LLMs not encoding world-general abstractions need to be validated across a broader range of realistic tasks to support general conclusions.

### Suggestions for Improvement
We recommend that the authors improve the robustness of their findings by including additional evaluation settings and datasets beyond the REPLACE task, such as Othello or other game datasets. We suggest conducting experiments with a broader range of LLM families and sizes to enhance the generalizability of the conclusions drawn from the study. Additionally, we advise clarifying the claims regarding goal-oriented abstractions in the abstract and results sections, specifying that these findings pertain to fine-tuned LLMs and may differ for models capable of performing tasks out-of-the-box. We encourage the authors to explore the probing of earlier layers of the LLMs to assess the presence of world-general abstractions, as this could provide further insights into the model's representation capabilities. Finally, we recommend improving the clarity of result presentations through substantial rewrites and demonstrating findings across a wider variety of tasks, including both realistic and synthetic scenarios, to strengthen claims regarding LLMs not encoding world-general abstractions.