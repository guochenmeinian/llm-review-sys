ID: tj86aGVNb3
Title: Feature learning via mean-field Langevin dynamics: classifying sparse parities and beyond
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper characterizes the generalization of neural networks and proves efficient sample complexity guarantees in the mean-field region with feature learning. It presents a framework to establish sample complexity of MFLD for binary classification, yielding improved convergence rates and dimension dependence when applied to the k-sparse parity problem. The authors propose an annealing method to enhance convergence rates and compute classification error using local Rademacher complexity.

### Strengths and Weaknesses
Strengths:
- The technique used in the analysis is unique, focusing on optimizing the distribution of parameters rather than norm control, leading to improved sample complexity and convergence rate.
- Proposition 4 provides a novel characterization of the margin at the stationary distribution.
- The paper is well-written, clearly stating theoretical results, and includes empirical results that align with theoretical findings.

Weaknesses:
- The focus on the binary classification problem limits the significance of the results.
- The emphasis on the k-sparse parity problem lacks clarity regarding its motivation and significance.
- The width and number of iterations required are $O(e^d)$, which may be impractical.
- The paper does not address label noise, which could affect the exponential convergence rate.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the significance and motivation behind studying the k-sparse parity problem. Additionally, we suggest addressing the limitations related to the width and number of iterations required, as well as exploring the implications of label noise on convergence rates. We also advise defining $d$ clearly in the introduction and correcting the typo in Assumption 2 regarding $log(0)$. Furthermore, we encourage the authors to consider generalizing results to bounded smooth activation functions and to clarify the differences in Table 1.