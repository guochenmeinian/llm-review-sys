ID: nblJSwtdrJ
Title: Text-to-Model: Text-Conditioned Neural Network Diffusion for Train-Once-for-All Personalization
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 4, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Tina, a text-conditioned neural network diffusion model aimed at generating personalized models from text prompts. Tina employs a diffusion transformer model with CLIP-based task description embeddings, showcasing significant generalization capabilities even with small datasets (~1000 samples). The model is evaluated under zero-shot/few-shot image prompts and demonstrates robust performance across various tasks, including unseen categories.

### Strengths and Weaknesses
Strengths:
- The paper provides a comprehensive explanation of Tina's design and framework.
- It conducts a detailed ablation study and experiments across multiple datasets, demonstrating excellent generalization.
- The topic is engaging, and the presentation is clear and accessible.
- The experimental process is reliable, with robust comparisons against existing models.

Weaknesses:
- The model parameter size in the experiments is too small; larger models are necessary for a thorough evaluation.
- More comprehensive and competitive baselines should be included to better demonstrate the model's effectiveness.
- Methodological details, such as specific configurations and hyperparameters, are sparse, hindering reproducibility.
- The justification for using DiT as the weight generation model is insufficient.

### Suggestions for Improvement
We recommend that the authors improve the model parameter size in the experiments to enhance evaluation effectiveness. Additionally, please include the results of direct fine-tuning in Table 1 and provide an ablation study on the impact of text prompts. Clarifying the inheritance of parameters from G.pt for initialization and justifying the choice of DiT as the weight generation model would also strengthen the manuscript. Furthermore, we suggest revising Figure 2 for better clarity and including necessary explanations in the captions of the model framework overview.