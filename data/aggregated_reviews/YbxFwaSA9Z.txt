ID: YbxFwaSA9Z
Title: Can Learned Optimization Make Reinforcement Learning Less Difficult?
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 8, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents OPEN, a meta-learning approach that introduces a learned optimizer for reinforcement learning (RL) to tackle challenges such as non-stationarity, plasticity loss, and exploration. The authors propose a three-stage update rule for OPEN, trained via evolutionary strategies, and demonstrate its superior performance compared to traditional optimizers like Adam and other learned optimization techniques across both single- and multi-task environments. Additionally, the paper explores various optimizers, including Adam and OPEN, providing detailed results on their performance across different environments. The authors also propose the implementation of Noisy Networks for Exploration with PPO, highlighting the differences in noise application compared to OPEN. Comprehensive results and analysis are provided, demonstrating the contributions of their method and the novelty of their approach in training optimizers in reinforcement learning.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with a clear presentation and comprehensive background.
- It effectively addresses significant challenges in RL, providing a scalable solution.
- The ablation studies are extensive, and the results are easy to interpret, supporting the claims of improved performance.
- The authors provide clear and detailed results, including specific values for mean and standard error across different optimizers.
- The implementation of Noisy Networks for Exploration is well-explained, with important distinctions from OPEN highlighted.
- The novelty of the contributions is articulated, particularly in the context of reinforcement learning and optimizer training.

Weaknesses:
- The empirical comparison techniques lack sufficient detail.
- The review of related techniques is limited, particularly regarding parameter noise approaches.
- The contribution of individual components within the method is unclear, as it appears to be a combination of existing techniques rather than a novel approach.
- The evaluation is primarily focused on gridworld tasks, raising questions about generalization to more complex environments.
- The performance of Noisy Networks is notably lower than that of the more canonical PPO implementation, which may raise questions about its efficacy.
- The initial clarity regarding the contributions of the method could have been improved, although revisions have been made in the introduction.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the novelty of their work in the introduction, specifically highlighting how their approach differs from existing methods like Optim4RL. Additionally, it would be beneficial to include a detailed comparison of their method with parameter noise techniques, as well as to evaluate OPEN on more complex environments, such as Atari, to assess its scalability. Furthermore, we suggest enhancing the explanation of the computational cost of OPEN in the main text to address concerns regarding its efficiency compared to Adam. Lastly, we encourage the authors to clarify the notation used in equations for better readability and to further explore the reasons behind the performance differences between Noisy Networks and traditional PPO to enhance the discussion and provide deeper insights into the implications of their findings.