ID: M5JW7O9vc7
Title: $\texttt{Model-GLUE}$: Democratized LLM Scaling for A Large Model Zoo in the Wild
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 6, 7, -1, -1, -1, -1
Original Confidences: 2, 4, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical evaluation of strategies for combining multiple large language models (LLMs) to enhance performance across various tasks. The authors benchmark existing model merging and mixture-of-experts (MoE) methods using the Llama-2 model family and propose "Model-GLUE" as a comprehensive guideline for scaling LLMs. The study includes a thorough comparison of different strategies and summarizes results in an accessible Q&A format.

### Strengths and Weaknesses
Strengths:
* The paper conducts a thorough study on numerous combinations of strategies.
* Results are presented in an easily accessible Q&A format, which is valuable.
* The proposed Model-GLUE represents a current best-practice strategy.

Weaknesses:
* The overall message lacks clarity and could be articulated more effectively.
* The paper does not adequately address energy consumption of the resulting models, which is a significant environmental and financial concern.
* The provided code repository is empty, and many technical procedures are not properly documented.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the overall message and provide a more detailed explanation of the methods, particularly regarding the 'optimal merging strategy.' Additionally, the authors should discuss the energy consumption of the models and rank current heuristics accordingly. It would also be beneficial to include a discussion of findings related to the Mistral model family for a more comprehensive analysis. We suggest documenting the technical implementations more thoroughly and ensuring the code repository is populated with usable software. Finally, we recommend revising the title for clarity and addressing minor typographical errors throughout the paper.