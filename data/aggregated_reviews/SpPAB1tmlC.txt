ID: SpPAB1tmlC
Title: Unveiling Encoder-Free Vision-Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents EVE, a novel paradigm for Vision-Language Models (VLMs) that eliminates the need for a preceding visual encoder in the LLM decoder, aiming for a more flexible and lightweight framework. EVE integrates a Patch Embedding Layer and a Patch Aligning Layer to align image tokens with the language model, proposing a new training recipe for robust encoder-free learning. The authors assert that EVE's architecture allows for maximum freedom for the model to discover patterns independently and emphasizes the effectiveness of the LLM-guided Prealigning Stage in preventing model collapse. The results indicate that EVE achieves performance somewhat comparable to existing encoder-based VLMs across several benchmarks, challenging conventional inductive biases and warranting a reconsideration of terminology in the field.

### Strengths and Weaknesses
Strengths:
- The paper exhibits exceptional clarity and coherence, with effective visual aids enhancing comprehension.
- The motivation for removing off-the-shelf encoders is compelling and innovative, contributing valuable insights to the research community.
- The model's architecture allows for arbitrary resolution and aspect ratio, which is typically not supported by pre-trained vision encoders.
- Comprehensive details on dataset curation and implementation specifics are provided, along with significant ablation studies that demonstrate the model's capabilities.
- The integration of multimodal data and text-only FineWeb data in various ratios provides valuable insights.

Weaknesses:
- The classification of the model as "encoder-free" is misleading due to the presence of the Patch Embedding Layer, which resembles an image encoder.
- Claims of "almost lossless" conversion by the Patch Embedding Layer are undermined by the use of average pooling, a lossy operation.
- The terminology "encoder-free" and "lossless" may be misleading, as some reviewers suggest a more accurate description would be joint training of LLM and a lightweight vision encoder.
- EVE's performance still lags behind encoder-based models like LVIS-4V and ShareGPT4V, despite using more training data, particularly in early training stages.
- The practical significance of the claimed flexibility in Image Resolution and Aspect Ratio is not adequately demonstrated.
- The lack of exploration into image augmentations could limit performance enhancements.
- Structural issues exist, such as figures not being referenced correctly in the text.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the term "encoder-free" by addressing the role of the Patch Embedding Layer in the model. Additionally, the authors should provide a more rigorous justification for the claim of "almost lossless" conversion, considering the implications of average pooling. To address the performance gap, we suggest exploring strategies that could enhance EVE's capabilities relative to encoder-based models. We also encourage the authors to quantify and demonstrate the practical benefits of flexible Image Resolution and Aspect Ratio support. Furthermore, we recommend conducting ablation studies to explore the model's performance without the Patch Aligning Layer (PAL) and incorporating experiments on image augmentations, such as resizing, to provide insights into further performance improvements. Lastly, we advise correcting the structural issues in figure references to enhance the logical flow of the manuscript and addressing discrepancies in performance claims compared to the PaliGemma paper to strengthen the paper's credibility.