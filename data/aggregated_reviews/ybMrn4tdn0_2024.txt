ID: ybMrn4tdn0
Title: Auditing Local Explanations is Hard
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 4, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an auditing framework aimed at verifying the accuracy of local explanations for machine learning models, particularly in scenarios where trust is lacking. The authors propose a formal definition of auditing local explanations, emphasizing the significance of the explanation region's size. They establish bounds on the sample complexity required for effective auditing, which are functions of this locality property. The framework allows auditors to query model predictions and local explanations, contributing to a theoretical analysis of auditing sample complexity.

### Strengths and Weaknesses
Strengths:  
**S1. Theoretical Framework**: The paper proposes a significant theoretical framework for auditing local explanations, advancing the rigor in verifying the trustworthiness of machine learning explanations.  
**S2. Key Metrics Identification**: The introduction of the explainability loss function offers a quantifiable measure for evaluating local explanations, enhancing the systematic assessment of explanation quality.  
**S3. Importance of Locality**: The analysis highlights the critical role of locality in explanations, addressing an underexplored aspect in the explainability literature.

Weaknesses:  
**W1. Lack of Empirical Evaluation**: The absence of evaluations on real-world datasets undermines the practical implications of the findings. Initial evaluations should be conducted to validate the theoretical results.  
**W2. Limited Discussion of Previous Research**: The paper lacks a thorough discussion on how its findings relate to existing research, particularly regarding the alignment with metrics from works like [Dasgupta 2022] and [Bassan 2023].  
**W3. Misleading Presentation of Explanations**: The focus on a specific type of explanation—those approximating the true model in a local region—may mislead readers into thinking the approach is more general than it is.

### Suggestions for Improvement
We recommend that the authors improve the paper by conducting empirical evaluations on real-world datasets to validate their theoretical findings. Additionally, a more comprehensive discussion on how their work relates to previous research, particularly regarding the consistency and sufficiency metrics, would strengthen the paper. Furthermore, clarifying the scope of the explanations discussed and ensuring that the presentation accurately reflects the limitations of the proposed framework would enhance the overall clarity and impact of the work.