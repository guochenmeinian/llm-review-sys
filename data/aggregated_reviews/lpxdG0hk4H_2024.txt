ID: lpxdG0hk4H
Title: ShowMaker: Creating High-Fidelity 2D Human Video via Fine-Grained Diffusion Modeling
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 7, 5, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework for 2D human video generation called ShowMaker, which utilizes fine-grained diffusion modeling to create half-body conversational videos based on 2D keypoints. The framework includes a Key Point-based Fine-grained Hand Modeling module and a Face Recapture module for enhanced facial texture and identity consistency. The authors claim that their approach demonstrates superior performance through sufficient experiments, showcasing high-quality generated videos.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written, clearly articulating the motivations and technical details of the proposed framework.
2. ShowMaker exhibits significant practical value for conversational scenarios, particularly in TV shows.
3. The Face Recapture module effectively generates accurate facial regions.
4. The experiments substantiate the superiority of the proposed model, yielding highly satisfactory video outputs.

Weaknesses:
1. The experimental settings lack clarity, particularly regarding the training duration balance for each ID in the testing examples.
2. The novelty of the proposed method appears limited, with many components derived from prior works, raising concerns about its originality.
3. The paper lacks recent baselines in experimental comparisons, such as MagicPose and TPS.
4. There are several writing errors that hinder comprehension.
5. The visualization of long video generation and motion retargeting under varying pose conditions is insufficient.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental settings, particularly regarding the training data distribution across IDs. Additionally, we suggest including comparisons with recent baselines like MagicPose and TPS in the main text. The authors should provide longer video generation examples, ideally from the TikTok dataset, and include motion retargeting visualizations where the poses differ significantly. Furthermore, we encourage a more detailed explanation of the novelty of the proposed method and the rationale behind specific design choices, such as the use of a discrete codebook for hand information. Lastly, addressing the writing errors throughout the paper will enhance overall comprehension.