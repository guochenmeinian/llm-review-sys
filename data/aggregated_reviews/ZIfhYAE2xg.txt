ID: ZIfhYAE2xg
Title: Sparse Parameterization for Epitomic Dataset Distillation
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 8, 6, 7, 5, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework for dataset distillation called SPEED, which incorporates Spatial-Agnostic Epitomic Tokens (SAETs), Sparse Coding Matrices (SCMs), and a Feature-Recurrent Network (FReeNet). The authors propose that this framework effectively addresses spatial redundancy and achieves state-of-the-art results across various datasets, including CIFAR-10/100 and TinyImageNet. The method is designed to be compatible with multiple dataset matching techniques, enhancing their performance.

### Strengths and Weaknesses
Strengths:
- The introduction of sparse coding into dataset distillation is both interesting and effective, as evidenced by experimental results.
- The paper is well-written, with clear explanations and visualizations that enhance understanding.
- Extensive experiments, ablation studies, and insightful analyses contribute to the paper's robustness.
- The proposed method achieves state-of-the-art performance and offers a new approach to distilling images at the patch level.

Weaknesses:
- There is a lack of discussion regarding the relationships between dataset distillation, sparse coding, and corset selection.
- Evaluation results in Table 2 are incomplete, particularly for TinyImageNet, raising questions about the consistency of the evaluation metrics.
- The presentation of Tables 1 and 2 could be improved to clarify the differences between methods and the implications of IPC values.
- The potential performance of a heavier model following the same idea as the proposed method remains unexplored.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the relationships between dataset distillation, sparse coding, and corset selection. Additionally, please provide a more comprehensive evaluation for TinyImageNet in Table 2, including IPC values for all configurations. We suggest clarifying the presentation of Tables 1 and 2 to differentiate between the problems addressed by various methods. Finally, consider exploring the performance implications of adopting a heavier model and the potential addition of convolutional layers after patch stitching to enhance image quality.