ID: LUsx0chTsL
Title: Talking Heads: Understanding Inter-Layer Communication in Transformer Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 7, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the interaction between attention heads across layers in transformer models, focusing on the "inhibition-mover subcircuit." The authors propose using Singular Value Decomposition (SVD) to identify low-rank communication channels, which they demonstrate can enhance model performance on tasks like item recall. They also address the noise in existing composition metrics and validate their findings through various experiments, including the Laundry List Task and Indirect Object Identification (IOI). Additionally, the authors explore communication channels in transformer models related to compositional operations for object identification and recall tasks, suggesting that increasing the residual stream dimension may help address a larger number of items. They acknowledge the need to clarify how their findings relate to the concept of a "capacity limit" and have received feedback regarding the generality of their findings and the specificity of their title.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a fundamental question in transformer interpretability, providing a novel in-depth study of inter-layer communication.  
- The use of weight-based decompositions allows for static analysis without input data, enhancing the study's rigor.  
- The findings connect to existing literature on language model robustness, explaining sensitivity to item order in lists.  
- The authors have effectively addressed several key concerns raised in the reviews, particularly regarding the results from larger models and intervention details.  
- The proposed changes and clarifications are expected to strengthen the paper significantly, making it an interesting and valuable case study with potential broader applicability.  

Weaknesses:  
- The composition score is not applicable to models with relative positional embeddings, limiting its generalizability.  
- The study is confined to smaller models, raising concerns about the scalability of results to larger architectures.  
- Key concepts and related work are presented too briefly, hindering comprehension, and there are numerous minor typographical errors throughout the paper.  
- There is a limited discussion of related work, which may affect the perceived generality of the findings.  
- The title and overall tone of the paper may mislead readers regarding the scope and implications of the results, particularly in relation to inter-layer communication in transformers.  

### Suggestions for Improvement
We recommend that the authors improve the contextualization of their research by expanding the related work section to include more comprehensive discussions on prior studies regarding information passage in transformers. Additionally, the title should specify that the communication channels concern compositional operations or mention inhibition directly. The authors should clarify the methodology for the composition score and provide more experimental details to support their claims. It would also be beneficial to test their findings on larger models to assess generalizability, either in the main text or appendix. Furthermore, we encourage the authors to evaluate the performance of edited models in text generation, as this could provide valuable insights into the role of inhibition channels. Lastly, we suggest refining the writing for clarity and addressing typographical errors to enhance overall readability.