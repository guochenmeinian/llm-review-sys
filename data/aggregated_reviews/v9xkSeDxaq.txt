ID: v9xkSeDxaq
Title: ReffAKD: Resource-efficient Autoencoder-based Knowledge Distillation
Conference: NeurIPS
Year: 2023
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: 4, 4, 4

Aggregated Review:
### Key Points
This paper presents an approach for knowledge distillation that eliminates the need for a large teacher model by utilizing a compact autoencoder to extract essential features and compute similarity scores between classes. The authors evaluate their method on benchmark datasets, demonstrating performance comparable to or exceeding traditional knowledge distillation techniques.

### Strengths and Weaknesses
Strengths:  
- The authors provide a detailed description of their approach, which is significant in the field.  
- The use of autoencoders as a proxy for knowledge distillation is an innovative concept, and the paper includes comparisons with multiple existing methods.  
- Extensive ablation studies on the impact of alpha and temperature parameters yield valuable insights.  

Weaknesses:  
- The results in Figures 5 and Tables 2 and 3 rely on a single random seed, necessitating the inclusion of mean and standard deviation results across multiple seeds.  
- The claim of employing an unsupervised method for generating soft labels is questionable, as it still requires random selection of samples from different classes.  
- The writing quality needs significant improvement, with several typos and formatting issues present.  
- Figure 5 lacks clarity, and the term ‘TinySLG’ is not referenced elsewhere in the paper.  
- There is a lack of comparison regarding wall-clock training time between the proposed autoencoder and a large teacher model, which is essential for understanding efficiency.  
- The evaluation is limited to smaller datasets, raising concerns about scalability to larger datasets and tasks, and the inclusion of Transformer-based networks like ViT could strengthen the claims.

### Suggestions for Improvement
We recommend that the authors improve the writing quality and address the formatting issues throughout the paper. Additionally, please provide results based on multiple seeds to enhance the reliability of the findings. Clarification on the unsupervised nature of the method is needed, particularly regarding the sample selection process. It is also crucial to include a comparison of wall-clock training times between the proposed method and a large teacher model. Finally, we suggest conducting experiments on larger datasets and incorporating other architectures, such as Transformer-based networks, to validate the approach further.