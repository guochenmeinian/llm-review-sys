ID: 1SF2tiopYJ
Title: CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graph Diffusion
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 6, 6, 4, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 5, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework called CommonScenes for generating 3D indoor scenes from scene graphs. The model employs a dual-branch architecture where one branch generates the scene layout using a variational autoencoder (VAE), while the second generates compatible 3D shapes using a latent diffusion model. The authors enhance the scene graph dataset, termed SG-FRONT, by annotating it with scene graph labels derived from the 3D-FRONT dataset. They differentiate their approach from previous works by focusing on modeling a continuous latent manifold that allows for sampling multiple plausible scenes based on scene graph conditions. The evaluation metrics include FID and KID scores for fidelity and diversity, and the authors report qualitative and quantitative results demonstrating the model's potential. They also acknowledge the labor-intensive nature of obtaining textual annotations for scene graphs as a contribution to the community.

### Strengths and Weaknesses
Strengths:
- The proposed model is novel and consistently produces plausible scenes, alleviating the need for large libraries of assets.
- The development of the SG-FRONT dataset is a significant contribution that can facilitate further research.
- The authors provide a clear differentiation of their work from existing methods, emphasizing the unique contributions of their approach.
- Comprehensive responses to reviewer concerns demonstrate a thorough understanding of the topic and methodology.
- The paper shows improved results in scene generation quality compared to prior works, supported by quantitative evaluations.
- The supplementary video provided offers an intuitive explanation of the method and additional results.

Weaknesses:
- The complexity of the model, comprising multiple sub-modules, may obscure the contributions of individual components.
- The paper lacks clarity on the training dynamics between the layout and shape branches, particularly regarding whether a two-stage training process is employed.
- There is insufficient discussion on the quality of generated shapes and the limitations of the approach, particularly concerning the reliance on annotated scene graphs.
- The reliance on annotated scene graphs may limit accessibility for some users.
- The potential for interpenetration in generated scenes indicates a limitation in achieving fully collision-free outputs.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the training process, particularly whether the BCG is produced first before jointly training the shape and layout branches. Additionally, it would be beneficial to clarify the use of class conditioning in the Shape Decoding module. We suggest including an evaluation of the generated shapes' quality, potentially using COV and MMD metrics, rather than relying solely on Chamfer Distance. Furthermore, we encourage the authors to discuss the limitations of their approach more thoroughly in the main paper, addressing the challenges associated with obtaining annotated scene graphs and the implications for practical applications. We also recommend improving the clarity of the limitations associated with the interpenetrating phenomena in 3D-FRONT and consider introducing an additional IoU loss to mitigate these issues. Leveraging the texture renderer from CC3D could enhance the quality of the generated scenes. Incorporating the additional experiments discussed during the rebuttal period into the final version of the paper would also strengthen the overall contribution.