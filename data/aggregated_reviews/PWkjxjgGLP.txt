ID: PWkjxjgGLP
Title: Hierarchical Visual Feature Aggregation for OCR-Free Document Understanding
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 4, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach to enhance document understanding capabilities through hierarchical visual feature aggregation, utilizing a feature pyramid hierarchy with cross-attentive pooling to effectively manage multi-scale visual information. The authors propose a novel relative text-position prediction task to improve model performance and address text truncation issues. They clarify that their primary objective is to enhance existing Multimodal Large Language Models (MLLMs) using document data, rather than developing a new foundation model. The authors focus on OCR-free methods that utilize comparable MLLMs and demonstrate the effectiveness of their approach by enhancing two foundation models, BLIP-2 and mPLUG-Owl. Experimental results indicate the method's effectiveness on standard benchmarks, showing superiority in advancing document understanding. Additionally, the authors include comparisons with recent concurrent works, TextMonkey and mPLUG-DocOwl 1.5, noting that their method outperforms TextMonkey despite mPLUG-DocOwl 1.5's superior performance due to its advanced backbone and larger dataset.

### Strengths and Weaknesses
Strengths:
- The paper conducts extensive ablation studies that validate the design choices and interactions within the framework, providing a comprehensive understanding of each module's contributions.
- It introduces a novel relative text-position prediction task that significantly enhances the model's ability to interpret spatial relationships between text elements.
- The hierarchical visual feature aggregation method allows for the fusion of multi-scale visual features without increasing computational complexity.
- Comprehensive comparison with state-of-the-art methods addresses reviewer concerns and demonstrates the effectiveness of enhancements to BLIP-2 and mPLUG-Owl.
- Inclusion of comparisons with recent concurrent works adds depth to the evaluation.

Weaknesses:
- The baseline for the ablation studies is less robust, as indicated by the performance on DocVQA, where the baseline achieves only 35.17% accuracy compared to the proposed model's 72.7%, raising concerns about the persuasiveness of the ablation study's conclusions.
- The discussion surrounding the multi-scale feature pyramid hierarchy lacks depth; alternative configurations, such as using two versus three or four local scales, should be explored to provide insights into the balance between computational efficiency and performance.
- The performance of the authors' model is overshadowed by mPLUG-DocOwl 1.5, which may raise questions about the competitiveness of their approach.
- The reliance on OCR-free methods may limit applicability in certain contexts.
- The comparison in Table 3 is not comprehensive enough, as it omits models like mPLUG-DocOwl 1.5 and TextMonkey, which should be included for a more thorough evaluation.

### Suggestions for Improvement
We recommend that the authors improve the robustness of the baseline used in the ablation studies to strengthen the conclusions drawn. Additionally, we suggest a more thorough exploration of alternative configurations for the multi-scale feature pyramid hierarchy to enhance understanding of its role. We also recommend that the authors improve their discussion on the limitations of their model in comparison to mPLUG-DocOwl 1.5, particularly regarding the impact of using a more advanced backbone and larger dataset. Providing more insights into the general-purpose applicability of their approach could strengthen their argument for its versatility. Finally, clarifying the description of operations in the paper and ensuring that all relevant results are highlighted in tables would enhance clarity and presentation.