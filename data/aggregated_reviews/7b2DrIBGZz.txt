ID: 7b2DrIBGZz
Title: Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 4, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the integration of Large Language Models (LLMs) into text-to-image diffusion models, addressing challenges such as misalignment between LLM training objectives and the requirements for discriminative prompt features in diffusion models, as well as positional bias from the decoder-only architecture. The authors propose a novel framework called LLM-infused Diffuser, introducing the LLM-Infused Diffusion Transformer (LI-DiT) to enhance text representation capabilities. The paper demonstrates superior performance over state-of-the-art models in both open-source and commercial systems.

### Strengths and Weaknesses
Strengths:
- The introduction of the LLM-infused Diffuser effectively facilitates the integration of LLMs into diffusion models, enhancing generation performance.
- The experiments validate the proposed framework's effectiveness against state-of-the-art open- and closed-source baselines.
- The paper includes a solid analysis of the reasons for degraded prompt-following ability and presents impressive qualitative and quantitative results.

Weaknesses:
- Scalability of the LI-DiT approach to other diffusion models is not fully discussed or validated.
- The paper lacks detailed information on training and inference costs, such as GPU memory consumption and training time.
- There are concerns regarding the novelty of the contributions, particularly in the prompt engineering and the technical aspects of the Linguistic Token Refiner and Collaborative Refiner.
- The visual results may exhibit cherry-picking, and the authors need to clarify the comparisons with existing works that also integrate LLMs with diffusion models.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the scalability of LI-DiT to other diffusion models and provide detailed information on training and inference costs. Additionally, we encourage the authors to expand on related work, particularly focusing on the alignment between language model types and non-autoregressive image generation. Clarifying the visual results and addressing the potential cherry-picking issue would also enhance the paper's credibility. Finally, a more thorough exploration of the novelty of the proposed methods and clearer comparisons with existing literature would strengthen the contribution of this work.