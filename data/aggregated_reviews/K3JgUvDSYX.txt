ID: K3JgUvDSYX
Title: Fast Optimal Locally Private Mean Estimation via Random Projections
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 6, 6, 7, 7, -1, -1, -1
Original Confidences: 3, 5, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an efficient distributed mean estimation framework that reduces local data dimensions using random projections and applies an existing private processor (Asi et al [7]) on the projected data before transmission to the server. The authors propose further speedups through a more efficient projection matrix and correlated sampling. Utility bounds and speedup analyses are derived, supported by experimental results on both synthetic and real-world datasets.

### Strengths and Weaknesses
Strengths:
- Accelerating private distributed mean estimation is a valuable topic.
- The paper is beautifully written, with a clear narrative.
- Utility bounds are derived, and the theoretical analysis is solid.

Weaknesses:
- The novelty of the proposed framework is somewhat limited, as it applies random projection in a straightforward manner to speed up PrivUnitG (Asi et al [7]).
- A comparison of efficiency to the naive PrivUnitG in Table 2 is missing and should be added; the proposed framework may not significantly improve over PrivUnitG, as evidenced by experimental results where PrivUnitG appears more efficient.
- There is a potential error in the proof of Theorem 1 that may invalidate the result, particularly regarding the argument in line 486 about the expectation.

### Suggestions for Improvement
We recommend that the authors improve the comparison of efficiency by including a direct comparison with the naive PrivUnitG in Table 2. Additionally, please clarify the proof of Theorem 1, especially the argument regarding the expectation in line 486. It would also be beneficial to expand Section 3 to provide more intuition on how correlated sampling is advantageous. Lastly, consider exploring a unified framework that encompasses both the proposed scheme and SQKR, as both utilize Kashin's representation.