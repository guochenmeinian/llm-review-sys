ID: rIc17Kziiq
Title: Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an expansion of two underpowered datasets from Ettinger (2019) to better evaluate language models (LLMs) on negation and role reversal tasks. The authors introduce new datasets generated through templates and GPT-3, aiming to address the limitations of previous studies that may have drawn skewed conclusions due to insufficient statistical power. The evaluation of 22 LLMs reveals a general decrease in performance, with some models showing increased sensitivity to negation.

### Strengths and Weaknesses
Strengths:  
The paper provides valuable resources for probing LLMs, demonstrating the importance of dataset size in linguistic tasks. The execution is clean, and the findings suggest that prior studies may have underestimated model capabilities due to underpowered datasets.

Weaknesses:  
Concerns remain regarding the validity and reliability of the new datasets, particularly their potential biases and category imbalances. The lack of external validation raises questions about whether the datasets accurately reflect human language abilities. Additionally, the paper does not sufficiently detail the logic behind claims of skewed results or perform necessary statistical tests to support findings about model rankings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims regarding skewed results by explicitly detailing the logic and evidence behind this assertion. Conducting a Card-style power estimate or bootstrapped replications of the studies would enhance the robustness of their findings. Furthermore, it would be beneficial to clarify the rationale for using two different dataset expansion methods and to provide direct comparisons between them. Addressing the reliability of differences in model performance through appropriate statistical tests, such as per-item permutation tests, is also crucial. Lastly, we suggest a thorough revision of the limitations section to critically connect identified issues with their implications for the datasets.