ID: 7FXgefa9lU
Title: This Reads Like That: Deep Learning for Interpretable Natural Language Processing
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an adaptation of prototype-based learning from computer vision to NLP, introducing a learned weighted similarity measure and a post-hoc explainability method that highlights important words in inference and matching prototypes. The similarity measure model performs disentanglement by separating and aligning embedding factors relevant to each prototype. The authors evaluate their method on two classification datasets, demonstrating its ability to extract interpretable word sequences for classification decisions.

### Strengths and Weaknesses
Strengths:  
- The method is simple, reasonable, and novel, with straightforward applicability to other models and tasks.  
- The paper is well written and easy to follow, contributing to the interpretability discourse in NLP.  
- Results indicate qualitatively high interpretability and improved performance on the evaluated datasets.  

Weaknesses:  
- The method is evaluated on a limited range of models and datasets, which raises questions about the generalizability of the claims.  
- There is a lack of analysis regarding the "interpretability tax" associated with the method.  
- The chosen baseline model is outdated, making it difficult to contextualize the improvements over more recent methods.  
- The experimental setup lacks clarity, with insufficient reporting on model specifics and hyper-parameters.

### Suggestions for Improvement
We recommend that the authors improve the experimental evaluation by including a broader range of models and datasets to strengthen their claims. Additionally, the authors should analyze the magnitude of the interpretability tax associated with their method. It would be beneficial to include more recent and relevant baselines to contextualize their contributions effectively. We also suggest clarifying the experimental setup by providing detailed information on the model used, hyper-parameters, and whether results were averaged across multiple runs. Finally, we advise the authors to incorporate hedging in their claims and explicitly mention the extension of prototypical networks to NLP in the conclusion.