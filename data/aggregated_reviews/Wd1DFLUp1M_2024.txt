ID: Wd1DFLUp1M
Title: Aligning Diffusion Behaviors with Q-functions for Efficient Continuous Control
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Efficient Diffusion Alignment (EDA), which integrates preference alignment theories with reinforcement learning to enhance offline continuous control. EDA employs a two-stage training process, involving pretraining a behavior cloning model without reward information and subsequently fine-tuning with rewards using Direct Preference Optimization (DPO). The experimental results demonstrate EDA's strong performance and sample efficiency on the D4RL benchmark.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and clearly explains the proposed method.
- EDA effectively connects preference-based optimization with diffusion, allowing efficient training of diffusion-based RL policies.
- The introduction of Bottleneck Diffusion Models (BDM) is notable, as it modifies the score function to predict scalar values, enhancing flexibility in action generation.

Weaknesses:
- EDA shows only marginal performance improvements over baselines, with limited clarity on the necessity of alignment versus direct policy optimization.
- The framework's complexity, due to multiple training stages, may hinder its practicality, and it lacks sufficient discussion on training time compared to baselines.
- The method is evaluated solely on the D4RL benchmark, limiting its generalizability, and there is a lack of experiments comparing EDA with other energy-based generative models.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the introduction to explain the rationale behind alignment versus direct optimization. Additionally, providing insights on the training time of EDA compared to baselines would enhance understanding. The authors should also consider including experiments on more diverse real-world benchmarks beyond D4RL to strengthen the paper's impact. Lastly, we suggest discussing the computational efficiency and robustness of the method, particularly regarding the potential need for higher-order gradients in backpropagation.