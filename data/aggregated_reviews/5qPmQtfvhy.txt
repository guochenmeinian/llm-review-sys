ID: 5qPmQtfvhy
Title: Algorithmic progress in language models
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 4, 7, 6, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the contributions of algorithmic progress and compute scaling to language model performance from 2012 to 2023. The authors utilize a dataset of over 200 evaluations, demonstrating that models require half the compute every eight months, a rate surpassing Moore's law. They conclude that recent performance gains are largely due to compute scaling, with the transformer architecture contributing significantly to overall progress.

### Strengths and Weaknesses
Strengths:
- The paper provides a novel and empirical approach to quantifying algorithmic progress in language modeling, supported by extensive data.
- The methodology is clearly articulated, and the limitations of the analysis are thoroughly discussed, enhancing the paper's credibility.

Weaknesses:
- A critical weakness is the lack of clear insights or suggestions for future directions in the field, which limits the paper's relevance to the community.
- The analysis relies on several assumptions that may undermine the robustness of the conclusions, raising concerns about the applicability of the statistical model for future advancements.
- Minor presentation issues include overlapping captions in Figures 1a and 1b and the need for clearer labeling and readability in Figure 2.

### Suggestions for Improvement
We recommend that the authors improve the discussion on future directions for the field, addressing how insights can be extrapolated beyond current trends. Additionally, clarifying the statistical models used in the main text rather than the appendix would enhance understanding. Finally, addressing the minor presentation issues in the figures will improve overall clarity.