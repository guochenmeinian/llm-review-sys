ID: 6s77hjBNfS
Title: Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3

Aggregated Review:
### Key Points
This paper presents a methodology for training smaller language models from larger pretrained architectures through targeted structured pruning and dynamic batch loading. The authors propose a constrained optimization approach for structured pruning, focusing on specific dimensions, layers, and attention heads. Dynamic batch loading aims to align the validation loss of the pruned model with reference values predicted by scaling laws. The results indicate that the pruned Sheared-LLaMA models achieve state-of-the-art performance on various benchmarks compared to similarly sized models.

### Strengths and Weaknesses
Strengths:
- The problem is well-formulated, and the algorithm description is clear and comprehensive.
- The paper includes a substantial amount of experimental data, demonstrating fine-tuning on downstream tasks and comparisons with other models.
- The proposed pipeline effectively outperforms existing models of the same size, showcasing practical significance.

Weaknesses:
- The scaling law for dynamic batch loading requires multiple checkpoints, which may not always be available.
- The pruning stage is costly, and further analysis is needed to optimize the balance between pruning and continued pre-training.
- There are issues with bibliography references and typos, and Figure 1 is misleading regarding the pruned model's training context.
- The overall time for the Sheared LLaMA pipeline and throughput reports in Appendix B are absent.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the minimax problem solution in section 2.1, specifying whether a standard optimizer like AdamW is used. Additionally, we suggest enhancing the comparison with LLM-Pruner to clarify whether performance gains stem from the compression method or architectural differences. Finally, we advise re-evaluating the token budget allocation in Table 5, potentially conducting experiments with a constant time budget to identify optimal allocations for future work.