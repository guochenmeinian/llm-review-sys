ID: 1SmXUGzrH8
Title: FineStyle: Fine-grained Controllable Style Personalization for Text-to-image Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FineStyle, a few-shot fine-tuning paradigm for text-to-image models that enhances style personalization using a single reference image. It addresses the challenges of image-text alignment in few-shot fine-tuning and introduces a novel data augmentation technique to synthetically increase image-text pairs, along with concept-oriented masking during the fine-tuning phase. The authors claim that FineStyle achieves better fine-grained control in visual results compared to existing methods. Additionally, the authors critique reviewer VFWr's feedback, asserting that the reviewer overlooked their contributions and misinterpreted key figures. They emphasize that their style transfer method is not inferior and that evaluations are subjective, arguing against the necessity of attention visualization for demonstrating performance improvements.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated and organized, effectively identifying issues in current few-shot fine-tuning frameworks.
- The proposed method is simple and well-demonstrated, showcasing good controllability.
- The derivation of segmentation maps from cross-attention maps for concept-oriented masking is commendable.
- The authors demonstrate a strong commitment to open-source practices, which could benefit the AI/art community.
- They provide detailed explanations of their methodology and the rationale behind their choices, such as using the MUSE backbone for comparability with StyleDrop.

Weaknesses:
- There are minimal details on the pre-trained models used, specifically the MUSE variant and VQGAN, which raises concerns about their applicability.
- The paper lacks comparisons with several state-of-the-art methods, such as DreamStyler and ControlStyle, limiting the evaluation of FineStyle's effectiveness.
- The requirement for detailed text descriptions for style reference images is inconvenient and may hinder usability.
- The human evaluation lacks transparency regarding the number of participants and image-text pairs, and the reported user preference proportions do not sum to one.
- FineStyle's performance appears inferior to StyleDrop in style learning, and the paper does not provide sufficient experimental analysis to substantiate claims regarding the KV adapter's effectiveness.
- The authors' rebuttal indicates a significant disagreement with reviewer VFWr, which may reflect a lack of consensus on the paper's contributions.
- The authors acknowledge that some clarifications regarding the MUSE variant and comparisons with latent diffusion models were initially lacking.

### Suggestions for Improvement
We recommend that the authors improve the transparency of the pre-trained models by specifying the MUSE variant and providing details on the VQGAN used. Additionally, we suggest including comparisons with relevant state-of-the-art methods, such as DreamStyler, ControlStyle, and latent diffusion models, to strengthen the evaluation of FineStyle. The authors should also consider simplifying the text description requirements for style reference images to enhance usability. Furthermore, providing detailed information about the human evaluation process, including participant numbers and image-text pairs, would improve clarity. Lastly, we encourage the authors to conduct additional experimental analyses, such as visualizations of attention maps and incorporating quantitative results like CLIP scores and human evaluations, to substantiate claims regarding the KV adapter's effectiveness and to explore the potential of incorporating iterative human feedback for better style transfer outcomes.