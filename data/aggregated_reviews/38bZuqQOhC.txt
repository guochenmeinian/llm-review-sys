ID: 38bZuqQOhC
Title: Does Continual Learning Meet Compositionality? New Benchmarks and An Evaluation Framework
Conference: NeurIPS
Year: 2023
Number of Reviews: 24
Original Ratings: 7, 5, 8, 8, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents benchmarks CGQA and COBJ to evaluate compositionality in continual learning settings, introducing five few-shot testing schemes to assess systematicity, productivity, and substitutivity. The authors propose a novel approach to continual learning through RPSnet, employing modularity-based methods to prevent catastrophic forgetting while learning new tasks. They emphasize that RPSnet maintains a comparable number of trainable parameters to other methods, despite having more overall parameters. The evaluation protocol is designed to assess compositionality at various checkpoints, clarifying that their benchmarks align with standard continual learning settings. Additionally, the authors explore compositional learning in relation to the stability-plasticity dilemma, proposing a novel evaluation protocol that assesses compositionality across three dimensions: system-level accuracy (A_sys), prototype-level accuracy (A_pro), and subcategory-level accuracy (A_sub). They validate their claims using the two benchmarks, demonstrating that pre-trained models exhibit better accuracy but may not improve compositionality.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and presents an important problem in continual learning.
2. The benchmarks are well-designed, addressing a notable gap in existing literature.
3. The evaluation procedure is comprehensive, covering various aspects of compositionality.
4. The authors provide a clear rationale for their methodology, particularly regarding the use of modularity-based methods to mitigate forgetting.
5. The evaluation protocol effectively assesses compositionality, offering insights into the model's understanding of previously learned concepts.
6. The paper introduces a novel evaluation protocol for compositionality that encompasses multiple dimensions, enhancing the understanding of continual learning metrics.
7. The authors effectively address reviewer concerns and clarify key points, improving the overall clarity and depth of the paper.

Weaknesses:
1. The datasets are limited to grid-like images, which may not reflect naturalistic compositionality and could limit the generalizability of the findings.
2. The related work section is inadequate, lacking recent literature on continual learning, and discussions on the construction process of benchmarks are limited due to page constraints.
3. The justification for freezing feature extractors is insufficient, and the evaluation setting may not represent true continual learning.
4. Some concepts selected for evaluation may limit flexibility in systematicity tests, as they can only be combined with specific attributes.
5. The paper lacks a thorough discussion on quantitative metrics that evaluate the continual learning ability of feature extractors, which is critical for validating the main claims.
6. The authors acknowledge grammar errors and typos that may lead to misunderstandings.

### Suggestions for Improvement
We recommend that the authors improve the datasets by incorporating naturalistic images, such as MSCOCO, and consider augmentations like changing grid locations and introducing noise. Additionally, the authors should justify the claim regarding "compositionality addressing the stability-plasticity dilemma" with experimental results and clarify the motivations behind evaluating the three compositional capabilities. The literature review should include recent works, such as those from NeurIPS 2022 and CVPR 2023, to enhance the context of the benchmarks. We also suggest moving the discussion of related work from the appendix to the main text to improve reader comprehension and providing a more detailed description of the construction process for benchmarks. Furthermore, the authors should expand their explanation of the limitations regarding the selected concepts to better address flexibility in systematicity tests. Lastly, we encourage the authors to thoroughly proofread the manuscript to correct grammar errors and typos that could obscure key points, and consider simplifying Figure 1 to enhance clarity and alignment with the proposed problem settings.