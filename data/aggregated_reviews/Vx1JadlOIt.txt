ID: Vx1JadlOIt
Title: VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 7, 5, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents VisionLLM, a unified framework for vision-centric and vision-language tasks, utilizing natural language task prompts and integrating visual tasks with large language models (LLMs) through the use of additional tokens. The model connects a pre-trained visual backbone with a language decoder, Alpaca, and employs a language-aware image-tokenizer. VisionLLM adopts various language instruction formats and proposes an “output-format-as-query” framework for efficient parallel decoding. The authors demonstrate strong empirical results, particularly in object detection, grounding, and captioning, while also addressing the flexibility of using language instructions for task specification. However, concerns arise regarding the impact of LLMs on performance in specific tasks like instance segmentation and captioning, with some skepticism about their effectiveness for visual tasks.

### Strengths and Weaknesses
Strengths:
- The paper provides strong empirical results, particularly in object detection, achieving scores close to state-of-the-art despite being essentially zero-shot.
- The model is relatively easy to train, leveraging LORA and pre-trained models, which enhances accessibility for other researchers.
- The approach of decoding outputs in natural text is a promising direction, offering more flexibility than traditional visual prompt tuning.
- The authors provide a comprehensive rebuttal that addresses many concerns raised by reviewers, demonstrating a willingness to engage in further discussion.
- The framework is designed to unify various vision tasks with LLMs, highlighting its potential for open-ended applications.

Weaknesses:
- The method is complex, with many components whose interactions are not clearly explained, making implementation challenging for readers.
- The paper currently lacks coverage of additional vision-language tasks, such as Visual Question Answering (VQA), which should be included.
- The motivation for using large language models (LLMs) in this context is not sufficiently justified, raising questions about the advantages over task-specific models.
- Some reviewers express skepticism regarding the effectiveness of using LLMs for visual tasks, questioning whether their impact is positive or negative.
- The use of additional tokens raises concerns about potential limitations and ambiguities, particularly regarding their interaction with existing vocabulary tokens.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the model architecture and interactions between components, possibly by including pseudo-code for implementations and detailed descriptions of the “output-format-as-decoding” method. Additionally, we suggest expanding the scope of tasks covered in the paper to include VQA and providing a clearer rationale for the choice of LLMs, including comparisons to task-specific models. We also recommend that the authors improve the clarity of their explanations regarding the impact of LLMs on vision tasks, specifically addressing whether their effects are beneficial or detrimental. Furthermore, the authors should provide more detailed justifications for the use of additional tokens, particularly in relation to their open-ended purpose and how they avoid ambiguity with original tokens. Finally, we encourage the authors to incorporate the discussions from the rebuttal into the revised version to enhance the paper's comprehensiveness.