ID: yE62KM4qsO
Title: Advancing Bayesian Optimization via Learning Correlated Latent Space
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 4, 3, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to Bayesian optimization by leveraging a low-dimensional latent space learned through a variational autoencoder (VAE) augmented with new regularization terms. The authors propose several heuristic regularization constraints, including Lipschitz regularization and a specific loss term, $\mathcal{L}_\mathbf{z}$, to align the latent space with black-box function values and enhance the representation of promising samples. They emphasize the importance of smoothness in the latent space around optima and argue that the standard normal distribution prior is too restrictive, allowing their approach to offer more flexibility. The paper includes a theoretical justification for the proposed losses, particularly Theorem 1, and an experimental study demonstrating the method's advantages across various benchmarks.

### Strengths and Weaknesses
Strengths:  
- The paper addresses an important research area and provides a well-described background and motivation for the derived algorithm.  
- Theoretical contributions, particularly Theorem 1, offer significant insights into the proposed method.  
- The rationale for the necessity of $\mathcal{L}_\mathbf{z}$ is clear, emphasizing its flexibility compared to the prior.  
- The experimental results indicate improvements over reasonable baselines, showcasing the method's effectiveness, supported by preliminary analyses and figures illustrating smoother landscapes.  
- The paper is clearly written and easy to follow, with a thorough description of the algorithm.

Weaknesses:  
- The contributions appear limited due to reliance on numerous heuristics, which may complicate practical application.  
- The method's reliance on many hyperparameters raises concerns about its usability in real-world scenarios, especially for expensive-to-evaluate functions.  
- The central problem of the paper is not well articulated, leading to confusion about the motivation behind optimizing average distance towards a specific constant $c$.  
- The technical challenges and gaps addressed by the proposed method are insufficiently described, which may hinder reader understanding.  
- The paper lacks a principled modeling perspective, with some loss terms appearing ad-hoc and requiring further justification.  
- Technical details, such as the choice of hyperparameters and the assumptions regarding noise in the objective function, are insufficiently addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the problem description, specifically detailing the gaps between the latent space and input space, as well as the motivation behind optimizing average distance towards the constant $c$. Additionally, we suggest providing a principled analysis of the interaction among the various loss terms and justifying the choice of hyperparameters, including how they can be selected in practice. It would be beneficial to include a discussion on the computational cost of different Bayesian optimization approaches and provide runtime comparisons. Furthermore, we encourage the authors to expand the limitations section and clarify the assumptions regarding noise in the objective function. Lastly, including a simple synthetic example and more comprehensive experimental results would enhance the paper's impact.