ID: nvYDPF4LJK
Title: VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a model that supports a wide range of multimodal tasks, leveraging an LLM, modality-specific encoders, and task-specific decoders. The approach utilizes a "super link" mechanism, which employs special tokens and soft prompts for effective task handling. The model is evaluated across numerous benchmarks, demonstrating competitive performance with specialized models.

### Strengths and Weaknesses
Strengths:
- The work is extensively evaluated on many benchmarks.
- The model competes well with more specialized approaches.
- The paper is well-written and easy to follow.
- It addresses an important problem in building efficient generalist models.

Weaknesses:
1. The title is misleading, suggesting a single model for many tasks, while it is an aggregation of powerful pretrained models.
2. The contribution is limited; the super link is essentially soft prompts and special tokens, which are common in other approaches. The paper should discuss similar works.
3. There is a lack of detail regarding model training, specifically the training objectives and loss weighting.
4. The authors did not experiment with one-stage training, and no evidence supports the chosen three-stage design.
5. The claim of "Multimodal In-Context Ability" lacks quantitative comparison to support superiority in a few-shot ICL setting.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper's title to accurately reflect the model's nature. Additionally, the authors should enhance the discussion of related works to contextualize their contributions better. It is crucial to provide more details on the training process, including objectives and loss weighting. We suggest conducting experiments with one-stage training to validate the design choice of three-stage training. Finally, we encourage the authors to include quantitative comparisons to substantiate claims of superiority in multimodal in-context abilities.