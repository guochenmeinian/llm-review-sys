ID: 59E19c6yrN
Title: Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 4, 7, 7, 7, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark for evaluating the negotiation skills of AI agents, particularly large language models (LLMs), through complex multi-agent negotiation games. The authors formalize the negotiation game structure, which includes agents, issues, scoring functions, and a solution space, allowing for various game types such as compromising, greedy, and adversarial games. Empirical investigations reveal that even advanced models like GPT-4 struggle with the benchmark, suggesting its challenging nature. Additionally, the authors explore negotiation dynamics using GPT-4, initially setting the number of players to six based on established negotiation literature and later expanding to include a seventh player and an additional issue, resulting in a significant increase in the number of deals analyzed. They also examine the effects of increasing the temperature parameter to 1, observing changes in performance metrics across different configurations. However, the connection to LLMs is not well justified, as the benchmark could stand alone without them.

### Strengths and Weaknesses
Strengths:
- The benchmark addresses real-world motivational use cases and is well-structured.
- The formalization of negotiation games and interaction protocols is clear.
- Extensive empirical investigations and ablation studies enhance the findings.
- The authors effectively expanded their experimental framework to include more players and issues, enhancing the scalability of their model.
- The detailed results from multiple experiments provide a robust analysis of negotiation outcomes under varying conditions.
- The updated code and documentation facilitate easier setup and experimentation for future research.

Weaknesses:
- The focus on LLMs detracts from the benchmark's core contribution.
- The benchmark oversimplifies negotiation scenarios and lacks scalability.
- Limited qualitative analysis of negotiation strategies and absence of baseline comparisons weaken the evaluation.
- The performance drop at temperature 1 raises questions about the stability of the model under increased randomness.
- The rationale for the initial choice of six players could be more thoroughly justified in the context of the study's objectives.

### Suggestions for Improvement
We recommend that the authors improve the focus on the benchmark itself by structuring the empirical investigation around its usability, maintainability, and ease of extension. Expanding the set of metrics and elaborating on their interpretations is crucial, as these metrics are integral to the benchmark. Additionally, including simpler agents as baselines would provide meaningful comparisons against LLM-based agents. Clarifying the rationale behind the minimum thresholds for successful deals and exploring the impact of different agent behaviors would enhance the benchmark's robustness. Furthermore, we recommend that the authors improve the justification for the initial selection of six players to clarify its relevance to the study's goals. Lastly, consider exploring further the implications of the performance drop at temperature 1, particularly regarding the robustness of the model under conditions of irrationality among agents. A more elaborate GitHub repository with examples for running the benchmarks would also improve user accessibility.