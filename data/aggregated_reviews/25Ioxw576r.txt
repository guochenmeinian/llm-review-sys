ID: 25Ioxw576r
Title: You Only Cache Once: Decoder-Decoder Architectures for Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 6, 7, 8, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents YOCO, a novel decoder-decoder architecture that integrates gated linear attention with standard attention (SA) to optimize GPU memory usage and improve inference efficiency. The architecture employs efficient self-attention (ESA) in the first half of the layers, followed by cross-attention (CA) layers, sharing the output of the last ESA across subsequent CA layers. YOCO achieves significant parameter reduction and enhances key-value (KV) cache compression, demonstrating superior performance in language modeling tasks, particularly in long-context scenarios. The authors report successful scaling to a 3-billion-parameter model trained on 1 trillion tokens, achieving competitive results against Llama-like architectures and excelling in needle-in-haystack tests.

### Strengths and Weaknesses
Strengths:
1. YOCO's hybrid structure delivers remarkable results in retrieval-centric tasks and demonstrates robust performance in needle-in-haystack scenarios.
2. The data-dependent gated-retention method significantly improves retention capabilities.
3. YOCO's substantial KV cache compression enhances retrieval efficiency compared to existing linear attention models.

Weaknesses:
1. The paper lacks comparisons with existing linear-time/hybrid models trained on trillions of tokens, such as RWKV6 and TransNormer, whose checkpoints are publicly available.
2. There is insufficient discussion surrounding the design decisions, particularly regarding how efficient self-attention and the decoder-decoder structure affect model performance.
3. The novelty of the proposed architecture is limited, as it does not introduce new techniques or insights, and the authors do not provide explanations for its effectiveness.
4. The evaluation primarily focuses on memory and latency improvements without addressing training efficiency gains or the impact of fixed-size sliding windows on performance.

### Suggestions for Improvement
We recommend that the authors improve the manuscript by adding more comparisons with existing linear-time/hybrid models, such as RWKV6 and TransNormer. Additionally, we suggest including discussions on concurrent works like Samba and Mamba2 to enhance the context of their contributions. The authors should provide more thorough argumentation surrounding their design decisions and conduct ablation studies on the sliding-window attention size. Lastly, addressing the in-context learning ability of the architecture and discussing potential applications in multimodal scenarios would strengthen the paper.