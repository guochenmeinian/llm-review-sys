ID: JVtwC9RzlI
Title: TextGraphBART: Unifying Graph and Text with Structure Token
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 4, 4, 3, 3, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TextGraphBART, a method that integrates the processing and generation of text and graph data using a single transformer-based model. The authors propose a Structure Token that encodes graphs with text labels into a sequence of tokens, facilitating the handling of both data types interchangeably. The model is evaluated on text-to-graph (T2G) and graph-to-text (G2T) tasks, demonstrating comparable results to baseline models while utilizing fewer parameters.

### Strengths and Weaknesses
Strengths:
1. The introduction of Structure Tokens is a significant advancement, providing a unified method for processing and generating both text and graph data.
2. The method integrates seamlessly with existing Transformer architectures, requiring only minor modifications and avoiding the need for specialized loss functions or additional modules.
3. Empirical results show that TextGraphBART achieves comparable performance on both T2G and G2T tasks with baselines but with fewer parameters.

Weaknesses:
1. The baseline settings are weak, lacking comparisons with strong baselines that utilize advanced graph-structured-aware methods.
2. The model settings are limited, as the paper only tests one model size, necessitating further experiments with larger models.
3. The ablation study, while useful, could be more detailed in exploring the interactions and contributions of various components.
4. The evaluation is restricted to one-directional modal assessments, which may not adequately demonstrate the model's capabilities.
5. The proposed Structure Token's claim of being lossless in preserving graph structure is questionable, as information may be lost after embedding.
6. The experiments lack comprehensiveness, with outdated baselines and insufficient evidence for claims regarding scaling and the necessity of the domain token.

### Suggestions for Improvement
We recommend that the authors improve the baseline comparisons by including stronger graph-structured-aware methods. Additionally, we suggest conducting experiments with larger model sizes to substantiate claims regarding scaling. The authors should enhance the ablation study to provide a more thorough analysis of component interactions. Furthermore, we encourage the inclusion of more recent datasets and baselines, such as WebNLG (2020), to better demonstrate the contributions of their method. Lastly, we advise addressing the concerns regarding the Structure Token's lossless preservation of graph structure and providing clearer evidence for the necessity of the domain token.