ID: RiSMijlsLT
Title: SimMMDG: A Simple and Effective Framework for Multi-modal Domain Generalization
Conference: NeurIPS
Year: 2023
Number of Reviews: 24
Original Ratings: 5, 6, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for multi-modal domain generalization (DG) that addresses the challenge of learning from multi-modal data while ensuring generalization to unseen distributions. The authors propose to separate features within each modality into modality-specific and modality-shared components, aligning only the modality-specific features through supervised contrastive learning and employing a cross-modal translation module to enhance the modality-shared features. The methodology aims to ensure that modality-shared features, which reflect common information across modalities, are closely aligned in the embedding space, while modality-specific features, which contain unique information, are distanced from these shared features. The work introduces a new dataset, Human-Animal-Cartoon (HAC), and demonstrates the effectiveness of the proposed method through extensive experiments on both HAC and EPIC-Kitchens datasets. Experimental results validate the assumptions through Pearson correlation coefficients and Cosine similarity, demonstrating strong positive correlations for modality-shared features and weak correlations for modality-specific features. Additionally, cross-modal retrieval experiments indicate that modality-shared features are meaningful and shareable, significantly outperforming modality-specific features in retrieval tasks.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel approach to multi-modal DG, which is a relatively underexplored area with practical relevance.
- The methodology of splitting features into modality-specific and modality-shared components is innovative and contrasts with existing models like CLIP.
- The comprehensive experimental evaluation across various combinations of modalities provides a robust understanding of the method's capabilities.
- The experimental validation of modality-shared features through correlation metrics and cross-modal retrieval tasks is compelling and supports the authors' claims.
- The framework effectively distinguishes between modality-shared and modality-specific features, contributing to the understanding of feature representation across modalities.
- The paper is well-written and easy to follow, with effective visual aids.

Weaknesses:
- The appropriateness of the setting is questionable, as the testing domain/modality exists in training data, which may not align with traditional definitions of domain generalization.
- There is insufficient proof that the network effectively learns modality-specific and modality-shared features.
- The rationale for the effectiveness of the cross-modal translation module is unclear, particularly regarding how it maintains feature disentanglement.
- The technical contributions, such as the use of supervised contrastive loss, are not sufficiently novel, as they are established techniques in the literature.
- The reliance on numerical validation does not guarantee that modality-shared features represent meaningful concepts like shapes or actions, raising questions about their interpretability.
- The authors do not disentangle modality-shared features into specific attributes, which may limit the granularity of the analysis.
- The paper lacks detailed information on the HAC dataset, including its development process and quality assurance mechanisms.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the setting by explicitly defining how it aligns with traditional domain generalization concepts. Additionally, we suggest providing empirical evidence to demonstrate that the network successfully learns modality-specific and modality-shared features. The authors should clarify the role of the cross-modal translation module in maintaining feature disentanglement and consider discussing the novelty of their technical contributions in greater depth. We also recommend improving the interpretability of modality-shared features by providing clearer definitions or examples of what constitutes meaningful shared information. Furthermore, consider incorporating a more detailed analysis of how modality-shared features relate to specific attributes, similar to approaches used in StyleGAN-like works. Including comprehensive details about the HAC dataset, including its development process and quality assurance measures, could strengthen its contribution to the field. Lastly, we encourage the authors to include all added experiments and intuitive reasoning in the main paper for better clarity.