ID: izNfcaHJk0
Title: Privacy Amplification via Compression: Achieving the Optimal Privacy-Accuracy-Communication Trade-off in Distributed Mean Estimation
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 7, 6, 6, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the distributed mean estimation problem under communication constraints and central differential privacy (DP). The authors propose a method utilizing a Kashin representation and a Coordinate Subsampled Gaussian Mechanism to achieve optimal trade-offs among privacy, utility, and communication costs. The framework allows local clients to transmit a random subsample of parameters, significantly reducing communication overhead while enhancing privacy guarantees. Additionally, the paper analyzes optimal $\ell_2$ error for $\ell_2$ mean estimation under $(\epsilon,\delta)$-differential privacy in central and shuffle models, establishing that the lower bound on error is $\frac{\sqrt{d}}{\epsilon n}$. The authors argue that replacing input vectors with outputs from unbiased estimators does not asymptotically change the final error, provided the mean squared error (MSE) of the estimator is at most $\frac{d}{\epsilon^2 n}$. They contend that their approach, which accounts for the randomness in local compression, significantly reduces the noise required despite increased sensitivity, particularly in cross-device federated learning (FL) settings. The results are supported by simulations, demonstrating the effectiveness of the proposed approach.

### Strengths and Weaknesses
Strengths:
- The work is the first to explore the communication-privacy-accuracy tradeoff for the distributed mean estimation problem with central DP.
- It introduces a simple scheme that achieves order-optimal error bounds.
- The paper provides optimal rates for distributed mean estimation under specific privacy and communication constraints, contributing to the field of distributed learning.
- The paper offers a rigorous analysis of the trade-offs between privacy, communication, and accuracy in mean estimation.
- The authors clarify the implications of local compression on differential privacy, contributing to the understanding of sensitivity in this context.
- The relevance of the proposed methods in practical scenarios, particularly in cross-device FL, is well articulated.

Weaknesses:
- The solution, while novel in its application, is based on well-known techniques, limiting its originality.
- Theoretical results for histogram estimation are presented without corresponding simulation results, which may mislead readers regarding contributions.
- The frequent use of the term 'federated learning' is inappropriate as the problem addressed is more aligned with distributed learning under a central server.
- The results are perceived as following from established facts, raising questions about their novelty.
- There is skepticism regarding the practical significance of the results, particularly in atypical settings where $d \gg \epsilon^2 n$.
- The sensitivity increase is noted to be $\sqrt{d/b}$ rather than $d/b$, indicating a potential misunderstanding in the initial analysis.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the Kashin representation in the paper, as its introduction is currently insufficient. Expanding on this topic in the appendix would enhance understanding. Additionally, we suggest that the authors address the discrepancies in Theorem 5.3 and the appendix, ensuring consistency in the rates presented. It would also be beneficial to provide simulation results for the histogram estimation claims made in the abstract and introduction. Furthermore, we recommend that the authors improve the clarity of their contributions by explicitly addressing the novelty of their approach compared to existing methods. We also suggest that the authors provide more empirical evidence to support the practical significance of their results, particularly in the context of typical use cases in federated learning. Finally, we encourage the authors to ensure that the sensitivity analysis is accurately represented throughout the paper to avoid any potential misunderstandings and to reconsider the terminology used, specifically the term 'federated learning,' to accurately reflect the scope of their work.