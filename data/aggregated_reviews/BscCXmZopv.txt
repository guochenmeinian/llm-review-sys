ID: BscCXmZopv
Title: SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 6
Original Ratings: -1, -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SODA, a large-scale social dialogue dataset generated using a framework called CO3, which utilizes commonsense knowledge triplets from a knowledge graph. The authors employ GPT-3.5 to create narratives from these triplets, subsequently generating multi-turn dialogues. The resulting dataset, SODA, contains approximately 1.5 million dialogues and is used to train the COSMO conversational model, which demonstrates superior performance compared to existing models like GPT-3.5 and ChatGPT in various evaluation scenarios.

### Strengths and Weaknesses
Strengths:
- The integration of large language models with commonsense knowledge for data synthesis is novel and useful, addressing the scarcity of high-quality open-domain dialogue data.
- The data synthesis pipeline is well-structured, considering quality, safety, and naturalness, with comprehensive evaluations of the dataset.
- COSMO outperforms state-of-the-art models while being smaller in size, indicating effective training on the SODA dataset.
- The authors' commitment to open-sourcing both the dataset and the model is commendable.

Weaknesses:
- The paper lacks automatic evaluation results, which are crucial for validating the model's performance; the authors should consider using ChatGPT or GPT-4 for this purpose.
- Human evaluation details are insufficient, including the number of annotators and inter-annotator agreement, and some evaluation dimensions are poorly defined.
- The training setting for COSMO is unclear, particularly regarding the integration of ProsocialDialogue and the evaluation of safety.
- The evaluation methodology comparing SODA with previous datasets is questionable, as they do not share the same context or topics.

### Suggestions for Improvement
We recommend that the authors improve the automatic evaluation by incorporating metrics from ChatGPT or GPT-4 to validate the model's performance. Additionally, please provide more detailed information on human evaluations, including the number of annotators and clearer definitions for evaluation dimensions. It would be beneficial to clarify the training setting for COSMO, particularly regarding the use of ProsocialDialogue, and to conduct evaluations on models trained solely on SODA. Lastly, we suggest independently evaluating SODA against previous datasets to ensure a fair comparison.