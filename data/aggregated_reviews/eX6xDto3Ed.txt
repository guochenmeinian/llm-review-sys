ID: eX6xDto3Ed
Title: Flat Seeking Bayesian Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 6, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to posterior inference for Bayesian neural networks (BNNs) by introducing the Sharpness-Aware Posterior (SA-Posterior), which considers the sharpness/flatness of the loss landscape to enhance generalization performance. The authors develop a theoretical framework based on sharpness-aware minimization (SAM) and provide empirical evidence demonstrating that their method outperforms existing BNN techniques across various datasets, including CIFAR-10, CIFAR-100, and ImageNet.

### Strengths and Weaknesses
Strengths:
- The paper effectively combines sharpness-aware methods with BNNs, providing a theoretical basis and applying it to multiple BNN techniques, such as stochastic weight averaging Gaussian (SWAG) and stochastic gradient Langevin dynamics (SGLD).
- Clear explanations and a well-structured presentation enhance the paper's accessibility.
- Empirical results indicate improvements in accuracy, negative log-likelihood (NLL), and expected calibration error (ECE).

Weaknesses:
- The distinction between optimization problems for different Bayesian methods is insufficiently clarified, particularly regarding SWAG and SGLD.
- The ImageNet experiments are limited to SWAG and FSWAG, lacking a broader evaluation of other methods.
- The empirical improvements are often minor, raising questions about the adequacy of the experimental design to demonstrate generalization performance.
- The paper does not address potential computational costs associated with the proposed method or compare it with recent approaches.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the optimization problems used for the various Bayesian methods in Section 3. Additionally, it would be beneficial to expand the ImageNet experiments to include more methods. We suggest including the number of repetitions used to create confidence intervals in the table captions and discussing the computational costs associated with the proposed method compared to baselines. Furthermore, incorporating an algorithm section could enhance comprehension of the method's workings. Lastly, addressing limitations, such as the use of diagonal covariance in the Gaussian variational approach, would provide insights into potential areas for improvement.