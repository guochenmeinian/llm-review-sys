ID: V75gAxpW40
Title: Gradient-Variation Online Learning under Generalized Smoothness
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents gradient-variation extensions of online learning guarantees under a generalized smoothness assumption. The authors propose an algorithm achieving a $O(\sqrt{V_T})$ guarantee, where $V_T=\sum_{t} \sup_{x} \|\nabla f_t(x)-\nabla f_{t-1}(x)\|^2$, and a $O(\log{V_T})$ guarantee for strongly-convex functions. The results extend to a universal guarantee applicable without prior knowledge of loss convexity. Additionally, the authors introduce a strongly-adaptive guarantee and a $O(\sqrt{V_TP_T})$ dynamic regret guarantee. 

### Strengths and Weaknesses
Strengths:  
- The generalized smoothness assumption is a significant contribution, allowing for a broader application than previous global smoothness conditions.  
- The paper is well-structured, with clear technical development and novel results, particularly in adaptive algorithms and regret measures.  
- The function-to-gradient variation technique is effective, facilitating the decoupling of meta-algorithms from base learners.

Weaknesses:  
- The requirement for prior knowledge of $\min_x f_t(x)$ and multiple-query gradient access is a strong assumption that simplifies the problem excessively.  
- Section 4 lacks depth and clarity, presenting information too rapidly without sufficient discussion of challenges.  
- Some key ideas appear borrowed from existing works without adequate acknowledgment of the associated technical challenges.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Section 4 by providing a more thorough discussion of the algorithm and the challenges in deriving non-stationary regret guarantees. Additionally, we suggest that the authors address the strong assumptions regarding prior knowledge of $\min_x f_t(x)$ and multiple-query gradient access, as these may limit the applicability of their results. It would also be beneficial to clarify the potential technical difficulties in analyzing the proposed algorithm in relation to existing meta-algorithms. Finally, we encourage the authors to condense discussions on less relevant topics, such as exp-concave functions, to focus on the main contributions.