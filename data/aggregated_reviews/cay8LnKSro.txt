ID: cay8LnKSro
Title: Empowering Convolutional Neural Nets with MetaSin Activation
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MetaSin, a novel activation function for deep learning that combines a ReLU function with a sum of parameterized sinusoidal functions. The authors argue that MetaSin is particularly effective for image prediction tasks, supported by multiple experiments demonstrating its state-of-the-art performance compared to traditional activation functions like ReLU. The paper also discusses a distillation approach for stabilizing training, where activation parameters are initialized based on a teacher model using ReLU.

### Strengths and Weaknesses
Strengths:
- The work is original and introduces a new activation function that may significantly impact the deep learning community.
- The paper is well-structured, clearly written, and provides a solid theoretical foundation for the proposed function, backed by empirical results.
- The implementation of MetaSin is optimized, reportedly achieving three times the speed of native PyTorch implementations without additional memory overhead.

Weaknesses:
- The comparison to other activation functions, particularly Swish, is lacking, especially in the context of image classification tasks.
- The absence of standard deviations in experimental results raises concerns about the significance of the findings.
- The requirement of knowledge distillation for effective training is a limitation that the authors acknowledge but does not simplify the implementation as claimed.

### Suggestions for Improvement
We recommend that the authors improve the comparison by including additional baselines, particularly Swish, in section 5.3 to provide a more comprehensive evaluation. Additionally, we suggest including standard deviations for all experimental results to substantiate claims of state-of-the-art performance. More details on the CUDA implementation, particularly regarding kernel fusion and the gradient of the MetaSin operator, should be included to enhance clarity. Furthermore, we encourage the authors to clarify the role of distillation in their methodology to avoid misleading implications about the simplicity of replacing activation functions.