ID: Cb8RP9KLyh
Title: Regress, Don’t Guess – A Regression-like Loss on Number Tokens for Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 6, 7, 6, 7
Original Confidences: 4, 5, 3, 3

Aggregated Review:
### Key Points
This paper presents Number Token Loss (NTL), a novel method designed to enhance numerical reasoning in language models by addressing the limitations of cross-entropy loss for numerical tokens. The authors demonstrate that NTL significantly improves performance, especially in interpolation tasks, and is easily integrable into existing models, making it practical for applications in finance and scientific data processing. However, the paper could benefit from improved formatting and a more thorough exploration of result variance to substantiate performance claims.

### Strengths and Weaknesses
Strengths:
1. The introduction of **Number Token Loss (NTL)** provides an elegant solution to effectively handle numerical tokens in language models.
2. The paper has a clear objective focused on enhancing numerical reasoning in LMs, which is well-defined and directly addressed.
3. Experimental results indicate that NTL improves arithmetic reasoning, particularly in numerical interpolation tasks, showcasing practical benefits.
4. The method is straightforward to implement and integrates well into existing architectures, making it suitable for real-world applications where numerical accuracy is critical.

Weaknesses:
1. Formatting issues disrupt the flow, particularly the abrupt placement of **Figure 1** and the excessive space taken by **Figures 2 and 3**, which reiterate concepts already explained in the text.
2. The authors' assertion regarding the rarity of NTL's **non-injectiveness** may not hold true in larger datasets, and the potential benefits of combining NTL with cross-entropy for early convergence need more explicit discussion.
3. The exploration of edge cases where token-level accuracy is close but numeric differences are significant is lacking, which could provide a fuller understanding of NTL's limitations.
4. Detailed explanations of baseline methods like T5 and xVal occupy too much space, detracting from the focus on novel contributions.

### Suggestions for Improvement
We recommend that the authors improve the formatting by repositioning **Figure 1** to enhance the introduction's flow and reducing the size of **Figures 2 and 3** to avoid redundancy. Additionally, the authors should discuss the potential for combining NTL with cross-entropy loss more explicitly, particularly regarding its impact on early convergence. Expanding the exploration of edge cases where token-level predictions are close but numerically different would provide deeper insights into NTL's limitations. Finally, we suggest condensing the descriptions of baseline methods to allow for a more thorough analysis of the novel contributions and implications of NTL.