ID: 8YN62t19AW
Title: A Unified Discretization Framework for Differential Equation Approach with Lyapunov Arguments for Convex Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 6, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a unified discretization framework for the differential equation (DE) approach to convex optimization, addressing the transition back to discrete optimization methods. The authors introduce the concept of "weak discrete gradient" (wDG), which consolidates conditions for discrete gradients, allowing for the derivation of existing optimization methods and their convergence rates as special cases. The framework facilitates the development of new optimization methods by integrating known DEs with wDG.

### Strengths and Weaknesses
Strengths:  
- Comprehensive Approach: The paper fills a significant gap in the DE approach to convex optimization by providing a unified discretization framework, simplifying the analysis of discrete gradients.  
- Clarity: The pedagogical style is commendable, making complex theories accessible.  
- Abstract Optimization Methods: The introduction of wDG offers a systematic framework for analyzing existing methods and deriving convergence rate estimates.  
- Potential for New Methods: The framework encourages exploration of novel optimization techniques by combining known DEs with wDG.

Weaknesses:  
- Knowledge Requirement: The approach necessitates prior knowledge of accelerated gradient flows, limiting its practicality.  
- Case-Specific Discrete Arguments: While the framework consolidates discrete arguments, finding practical wDGs for various methods may still be complex.  
- Non-Universal Applicability: Certain methods, like the Douglas-Rachford splitting method, may not fit within the framework, raising questions about the applicability of wDG.  
- Estimated Rates: The convergence rates derived are not always optimal, indicating further work is needed for specific algorithms.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the necessity of knowing the accelerated gradient flow prior to applying their framework, as this limits its practicality. Additionally, we suggest providing a more detailed comparison with existing works, particularly regarding assumptions and results. Clarifying the use of the "free" iterate z in the discrete accelerated schemes would enhance understanding. Addressing the limitations of the framework, especially concerning non-optimal convergence rates and applicability to certain methods, would strengthen the paper. Finally, we encourage the authors to explore the extension of their framework to constrained optimizations and to discuss the potential for finding wDGs for non-convex smooth functions.