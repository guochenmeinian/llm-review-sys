ID: V6w7keoTqn
Title: EMVP: Embracing Visual Foundation Model for Visual Place Recognition with Centroid-Free Probing
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 8, 5, 5, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 5, 5, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for fine-tuning a visual foundation model for visual place recognition (VPR), focusing on a novel probing stage. The authors propose a Centroid-Free Probing (CFP) method and a Dynamic Power Normalization (DPN) module, which enhance feature extraction and fine-tuning performance. The paper claims to achieve state-of-the-art (SOTA) results on mainstream VPR datasets through theoretical and experimental validation of these methods.

### Strengths and Weaknesses
Strengths:
1. The paper is well-organized and easy to follow, with clear motivations and contributions.
2. The CFP method is innovative, eliminating the need for explicit semantic centroid calculations while introducing a Constant Normalization (CN) operation.
3. The DPN module is designed to improve parameter efficiency and can be utilized for post-processing during the probing stage.
4. Extensive experiments support the effectiveness of the proposed methods, demonstrating superior performance compared to existing SOTA techniques.

Weaknesses:
1. Some details require clearer expression, particularly regarding the relationship between VPR and image classification tasks and the applicability of LP and MP methods.
2. Inaccuracies exist, such as mischaracterizing NetVLAD as a second-order feature and failing to clarify the differences between DPN and classic adapters.
3. The training details, including the loss function and sampling strategy, are incomplete, and the metrics used are not adequately explained.
4. The paper lacks comprehensive results in Table 1, with missing comparisons to other methods and datasets.

### Suggestions for Improvement
We recommend that the authors improve clarity by providing more detailed explanations of the relationship between VPR and image classification, as well as the applicability of LP and MP methods. Additionally, please address the inaccuracies regarding NetVLAD and clarify the distinctions between DPN and classic adapters. It is essential to include comprehensive training details, such as the loss function and sampling strategy, and to explain the metrics used in the experiments. Finally, we urge the authors to complete the missing results in Table 1 to ensure a thorough comparison with existing methods.