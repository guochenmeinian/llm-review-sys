ID: CHffPbQXjX
Title: Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new architecture for mixture-of-experts (MoE) models called Stratified Mixture of Experts (SMoE), aimed at addressing parameter inefficiency in MoE systems. The authors hypothesize that the inefficiency arises from equal capacity among experts, which fails to meet the varying capacity needs of different tokens. SMoE introduces a stratified structure allowing dynamic capacity assignment, enabling tokens to utilize more experts as needed. Evaluations on multilingual machine translation across three datasets demonstrate that SMoE significantly outperforms established MoE baselines while using the same or fewer parameters. The analysis reveals that capacity assignment is influenced by factors such as target language, source language, token frequency, and the position of the SMoE block.

### Strengths and Weaknesses
Strengths:
- The paper tackles a significant issue in improving the parameter efficiency of MoE models, which is crucial for developing more efficient large-scale models.
- The novel stratified architecture of SMoE allows for adaptive capacity assignment, showing promising potential for enhancing model performance.
- Empirical results indicate substantial improvements over several state-of-the-art MoE models, particularly on the largest multilingual dataset.

Weaknesses:
- The performance improvement of SMoE over existing models is marginal, raising questions about the validity of the authors' hypothesis regarding parameter inefficiency.
- The focus on machine translation limits the applicability of SMoE, and the benefits for other NLP tasks remain unclear.
- The analysis lacks a thorough examination of the increased computational costs associated with SMoE compared to standard MoE models.
- The paper does not provide sufficient guidance on tuning hyperparameters like the number of strata and experts per strata for optimal performance.

### Suggestions for Improvement
We recommend that the authors improve the robustness of their claims by conducting experiments on a broader range of NLP tasks beyond machine translation. Additionally, comparing SMoE against state-of-the-art non-MoE models would clarify whether the performance gains are due to the stratified architecture or simply a result of increased capacity. A more detailed analysis of the computational costs and the trade-offs involved in using SMoE should be included. Finally, providing clearer guidance on hyperparameter tuning for the number of strata and experts per strata would enhance the practical applicability of the model.