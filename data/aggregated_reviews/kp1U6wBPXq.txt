ID: kp1U6wBPXq
Title: Adapting Language Models to Compress Contexts
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for processing long documents by adapting pre-trained language models into AutoCompressors, which condense lengthy contexts into summary vectors. The authors propose a summary accumulation technique that combines vectors from segments to create a comprehensive summary. Their approach demonstrates improved performance in various tasks, including retrieval-augmented language modeling and zero-shot passage re-ranking, with AutoCompressors outperforming few-shot in-context learning in 8 of 11 tasks evaluated.

### Strengths and Weaknesses
Strengths:
- The results indicate that all modules are beneficial.
- The method is efficient, requiring less memory for training.
- The paper is clearly written and easy to follow.
- The experimental setting is solid, and the methodology is well-argued.

Weaknesses:
- The innovativeness of the work is not high.
- The background of the Recurrent Memory Transformer (RMT) is not adequately discussed.
- A relevant benchmark, Long Range Arena, is missing from the evaluation.

### Suggestions for Improvement
We recommend that the authors improve the discussion of the RMT background to provide context for their contributions. Additionally, addressing the missing Long Range Arena benchmark would strengthen the evaluation. We also suggest exploring the impact of positional embeddings on summary tokens, as well as testing the model on other architectures beyond causal decoder-only settings.