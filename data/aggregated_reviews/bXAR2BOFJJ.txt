ID: bXAR2BOFJJ
Title: Bayesian Nonparametric Learning using the Maximum Mean Discrepancy Measure for Synthetic Data Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 6, 6
Original Confidences: 3, 4

Aggregated Review:
### Key Points
This paper presents a novel Bayesian estimator for maximum mean discrepancy (MMD) integrated into generative adversarial networks (GANs), addressing challenges such as mode collapse and optimization difficulties. The authors propose that their Bayesian nonparametric (BNP) GAN enhances sample diversity and inferential accuracy, outperforming traditional methods. The paper includes original theoretical guarantees regarding the estimator's consistency, generalization, and robustness, although practical applicability may be limited due to GAN training complexities.

### Strengths and Weaknesses
Strengths:
- The integration of Bayesian uncertainty into MMD for GANs is a novel and interesting approach in Bayesian statistics.
- The paper effectively identifies the limitations of frequentist MMD GANs in handling varying data uncertainties.
- The writing is clear and conveys the main ideas effectively.
- The acknowledgment of preliminary empirical results is commendable.

Weaknesses:
- The empirical evaluation is limited and does not convincingly support the proposed method's advantages over frequentist MMD GANs or standard GANs.
- Comparisons with state-of-the-art neural network-based discriminator GANs are lacking, raising questions about the claims of superiority.
- Details regarding frequentist MMD results are unclear, and confidence intervals for FID and KID measurements are absent.
- The discussion on optimization dynamics lacks convincing evidence and generalizability.
- The theoretical proofs in the appendix are poorly connected to the main text, with insufficient explanations and references.

### Suggestions for Improvement
We recommend that the authors improve the introduction by adding relevant citations to better contextualize the background. Additionally, a more thorough analysis of dataset selection is necessary, particularly regarding the relationship between results from larger datasets like ImageNet and the proposed Bayesian lemmas. To strengthen empirical support, the authors should include comparisons with state-of-the-art neural network-based discriminator GANs and clarify the details of frequentist MMD results. Including confidence intervals for FID and KID measurements would enhance the robustness of the findings. Furthermore, showcasing the method on the CIFAR dataset could demonstrate its effectiveness on more complex data. Lastly, the authors should clarify the discussion on optimization dynamics and ensure that the theoretical proofs are well-integrated and clearly explained within the main text.