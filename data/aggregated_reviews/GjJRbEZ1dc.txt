ID: GjJRbEZ1dc
Title: Multitask Learning with No Regret: from Improved Confidence Bounds to Active Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 3, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 3, 5, 2, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents refined confidence intervals for multitask kernel regression, specifically applied to multitask online and active learning settings. The authors propose a tighter confidence bound that is smaller than existing bounds, particularly for extreme cases of task independence and identity. They explore both known and unknown task discrepancies, utilizing a minibatch learning approach to estimate the upper bound of task discrepancies. Empirical validations are provided to support their theoretical contributions. Additionally, the authors investigate the validity of confidence bounds in the context of multitask learning using the MHC-I dataset, arguing that reporting regret achieved by baselines is crucial for demonstrating the effectiveness of their approach. They justify their choice of the MHC-I dataset due to its alignment with performance maximization goals, unlike other datasets such as "School Exams Data" and "Retail."

### Strengths and Weaknesses
Strengths:
- The introduction of a new refined confidence bound that is tighter than naive alternatives by leveraging the multitask kernel structure.
- A comprehensive study with empirical results demonstrating the effectiveness of the proposed methods across various learning settings.
- A clear rationale for selecting the MHC-I dataset, highlighting its relevance to active learning and performance maximization.
- Well-articulated focus on theoretical contributions, with experiments designed to validate these theories.

Weaknesses:
- Some writing lacks clarity, making it difficult to follow the arguments.
- The concept of regret in Section 4 is presented in a confusing manner, potentially misrepresenting the learning objectives.
- The paper lacks visualizations of generated tasks and confidence intervals, which could enhance understanding and validation of the results.
- The justification for dataset choice may not be sufficiently clear to all readers, potentially leading to confusion.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their writing, particularly in sections with dense mathematical notation, by providing a separate paragraph to define all relevant terms. Additionally, we suggest that Algorithm 3 be included in the main paper rather than the appendix to enhance visibility. The experimental section should be expanded to include more results and discussions, particularly on the implications of the findings. We also encourage the authors to clarify the scope of their work in the abstract and title, emphasizing that it focuses on multitask kernel regression rather than general multitask learning. Furthermore, we advise providing a more detailed explanation of the regret notion in Section 4 to accurately reflect the learning goals for each task. Lastly, we recommend that the authors improve the clarity of their justification for using the MHC-I dataset by explicitly stating the differences in goals between it and other datasets, and include visualizations of the generated tasks and associated confidence intervals to provide a more comprehensive understanding of their findings.