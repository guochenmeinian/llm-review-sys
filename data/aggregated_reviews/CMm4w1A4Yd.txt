ID: CMm4w1A4Yd
Title: A Deeper (Autoregressive) Approach to Non-Convergent Discourse Parsing
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a unified model, N-CoDiP, for Non-Convergent Discourse Parsing, utilizing a fine-tuning of ROBERTa. The authors claim to achieve state-of-the-art results on the Change My View (CMV) dataset, framing the task as an utterance-level multi-label classification. The model employs Gated Residual Networks (GRN) and Asymmetric Loss to address class imbalance, while also exploring one-hot speaker representation and auxiliary tasks.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow, with comprehensive experimental results.
- The proposed model offers a practical solution for discourse parsing, achieving comparable results with fewer resources and less complexity than existing methods.

Weaknesses:
- The contributions are incremental, with many techniques adapted from prior research without sufficient novelty.
- The model's reliance on a single ROBERTa instance is seen as primitive; comparisons with more advanced models like T5 or BART are lacking.
- The dataset is small, and the lack of statistical significance tests on results raises concerns about robustness.

### Suggestions for Improvement
We recommend that the authors improve the discussion of the annotation system and clarify the model's specific relation to Non-Convergent Discourse Parsing. Additionally, including a case study in the experimental section could enhance the paper's depth. It would also be beneficial to evaluate the model against more advanced baselines that utilize context from previous utterances. Finally, we suggest conducting statistical significance tests on the results to strengthen the findings.