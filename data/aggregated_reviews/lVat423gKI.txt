ID: lVat423gKI
Title: Language Representation Projection: Can We Transfer Factual Knowledge across Languages in Multilingual Language Models?
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a lightweight method for transferring factual knowledge from high-resource languages to low-resource languages using Language Representation Projection (LRP2) modules. The authors propose a parameter-free framework that enhances the transferability of factual knowledge across multilingual pre-trained language models, demonstrating improved results on the mLAMA dataset. The method relies on the assumption that sentence representations in one language can be projected to another by adding fixed language-specific vectors.

### Strengths and Weaknesses
Strengths:
- The paper introduces a parameter-free framework that shows promising results in enhancing factual knowledge transfer in low-resource languages.
- The method is simple, intuitive, and computationally efficient, addressing an important and timely problem.
- The findings suggest the existence of cross-lingual knowledge neurons in multilingual models.

Weaknesses:
- The presentation is suboptimal, with critical content relegated to appendices, making core claims difficult to assess without additional context.
- The foundational assumption of the method—using fixed vector addition to project representations—is not sufficiently motivated or empirically verified.
- The reported improvements in performance are modest, particularly for mBERT, and the baseline scores for the BLOOM model are inadequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the paper by restructuring it into a long format to include essential explanations in the main body, particularly regarding evaluation metrics and knowledge neuron analysis. Additionally, the authors should empirically verify the effect of vector addition on query representations to demonstrate their closeness to equivalent representations in the target language. It is also crucial to ensure a proper baseline English score for the BLOOM model, potentially by refining the probing method used. Finally, we suggest providing clearer guidance on selecting the number of layers for LIRP and LSRP, as this hyperparameter significantly impacts performance across languages and tasks.