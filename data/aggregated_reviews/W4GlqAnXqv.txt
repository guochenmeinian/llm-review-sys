ID: W4GlqAnXqv
Title: Frequency Balanced Datasets Lead to Better Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for selecting data for language modeling by removing sentences with high-frequency tokens, hypothesizing that this will create a more balanced corpus. The authors demonstrate that their approach achieves competitive language model perplexity with 10x less data on English Wikipedia and shows state-of-the-art results across various domains, including non-English languages. The proposed data filtering method also improves performance on downstream tasks, although models trained on larger corpora still outperform those trained on the filtered data.

### Strengths and Weaknesses
Strengths:
- The method is simple and effectively reduces corpus size while maintaining competitive language model perplexity.
- The approach leads to better results on downstream tasks, particularly in testing grammatical integrity.

Weaknesses:
- The paper lacks comparisons with other filtering methods, such as simple perplexity filtering.
- Downstream tasks are primarily English-centric, making it difficult to assess the method's efficacy in non-English languages.
- The explored design space of filtering algorithms is limited, and the baseline comparisons may not be valid.

### Suggestions for Improvement
- We recommend that the authors include comparisons with other filtering methods, such as simple perplexity filtering, to strengthen their claims.
- To enhance the evaluation, we suggest providing results from the full datasets used to obtain the 10M-freq corpus for a more accurate baseline comparison.
- We recommend that the authors clarify the experimental setup for perplexity calculations, particularly regarding vocabulary consistency across models.
- To improve the paper's appeal, we suggest incorporating downstream tasks related to agglutinative languages and machine translation, which could demonstrate the practical implications of the proposed method.
- We recommend using bold in tables and avoiding red colors to enhance readability for color-blind readers.