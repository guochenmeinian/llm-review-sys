ID: gjEzL0bamb
Title: MimicTalk: Mimicking a personalized and expressive 3D talking face in minutes
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 5, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MimicTalk, a novel approach to personalized talking face generation that utilizes a person-agnostic model instead of individual neural radiance fields (NeRFs) for each identity. The authors propose a static-dynamic-hybrid adaptation pipeline and an in-context stylized audio-to-motion model to enhance expressiveness and efficiency while significantly reducing training time. Experimental results indicate that MimicTalk outperforms previous methods in video quality and expressiveness. Additionally, the authors express gratitude for the reviewers' time and effort, indicating a commitment to addressing concerns and clarifying any questions.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a hybrid approach that effectively combines static and dynamic features for personalized talking face generation, enhancing expressiveness through the in-context stylized audio-to-motion model.
2. The method significantly reduces training time, demonstrating improved robustness and efficiency in handling various identities.
3. The design motivation is clear, and the paper is well-organized, with effective illustrations and implementation details.
4. The authors demonstrate a proactive approach in seeking feedback and are open to further clarification, reflecting a collaborative spirit in the review process.

Weaknesses:
1. The experimental results lack comprehensiveness and detail, failing to convincingly substantiate claims of superior performance, particularly in style-control results.
2. The evaluation is limited in scope, lacking diverse conditions such as varying lighting and occlusions, which are crucial for real-world applicability.
3. The head pose generation is not adequately addressed, and the representation of facial motion through PNCC limits expressiveness.
4. The responses to the reviewers do not provide specific details or insights into the content of the paper, which may leave some concerns unaddressed.

### Suggestions for Improvement
We recommend that the authors improve the comprehensiveness of their experimental results by including a wider range of scenarios and datasets, particularly those that test varying lighting and occlusions. Additionally, we suggest that the authors provide more examples for comparison with state-of-the-art approaches to substantiate their claims. It would also be beneficial to jointly model audio-to-motion and audio-to-pose to enhance synchronization and expressiveness. Furthermore, we encourage the authors to clarify the choice of loss weights in their pipeline and discuss how these choices affect performance. Lastly, we recommend that the authors improve their responses by including specific references to how they have addressed the reviewers' concerns in their rebuttal and providing a summary of key findings or contributions in their communication to enhance clarity and engagement with the reviewers.