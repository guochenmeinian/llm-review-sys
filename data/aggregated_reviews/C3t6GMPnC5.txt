ID: C3t6GMPnC5
Title: Mamba State-Space Models Can Be Strong Downstream Learners
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of Mamba state-space models (SSMs) in comparison to Transformer large language models (LLMs) across various downstream learning tasks. The authors propose that while Mamba models demonstrate robust recurrent dynamics and can achieve significant speed and memory efficiency gains through fine-tuning, they still face challenges in achieving performance parity with Transformers, particularly in in-context learning (ICL), mixed-precision fine-tuning (MPFT), and parameter-efficient fine-tuning (PEFT). The study includes a theoretical analysis of Mamba's stability and empirical validation of its limitations on real datasets.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to read.
- The theoretical analysis provides valuable insights into Mamba's stable dynamics, supported by empirical evidence.
- Mamba's recurrent dynamics show robustness to small input changes, ensuring stability during training and fine-tuning.

Weaknesses:
- The theoretical analysis focuses on the hidden state stability but does not extend to the output, raising questions about the stability of the output $y_t$.
- The implementation of LoRA on Mamba blocks neglects the transition matrix $\bar{A_t}$, which is crucial for fine-tuning.
- The study lacks depth in addressing ICL limitations, which appear to be general training recipe issues rather than Mamba-specific problems.
- Comparisons with additional backbone models are limited, with Pythia being the sole baseline.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis by extending the stability discussion to the output $y_t$. Additionally, consider including the transition matrix $\bar{A_t}$ in the LoRA implementation during fine-tuning. To enhance the depth of the ICL analysis, we suggest providing more detailed explanations of the limitations and comparing Mamba with other models, such as GLA and linear attention. Finally, including more backbone models for comparison would strengthen the empirical findings of the study.