ID: faj2EBhdHC
Title: Graph Neural Networks Need Cluster-Normalize-Activate Modules
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel plug-and-play module named Cluster → Normalize → Activate (CNA) aimed at enhancing the performance of Graph Neural Networks (GNNs) by addressing the oversmoothing issue prevalent in deep GNN architectures. The CNA module clusters nodes into super nodes, normalizes them, and applies individual activation functions, demonstrating significant improvements in accuracy and reduced mean squared error across various tasks, including node classification and regression.

### Strengths and Weaknesses
Strengths:
- The paper introduces a creative solution to the oversmoothing problem in GNNs through the CNA module, marking a significant advancement in deep learning for graph-structured data.
- The authors provide thorough empirical evaluations across multiple tasks and datasets, substantiating the effectiveness of the proposed method.

Weaknesses:
- The paper lacks a theoretical analysis to support the empirical findings, which could strengthen claims about the CNA module's effectiveness. The authors should provide a theoretical analysis or proof supporting the empirical results.
- The computational complexity introduced by the clustering step in the CNA module is not adequately addressed, raising concerns for large graphs or real-time applications. The authors should consider optimizations or alternative clustering methods to tackle potential scalability issues.
- There are inconsistencies in parameter counts in Table 4, specifically why SageConv+CNA has fewer parameters than GraphSage while GCNConv+CNA has more than GCN.

### Suggestions for Improvement
We recommend that the authors improve the theoretical foundation of the CNA module by providing a rigorous analysis of its effectiveness and the expressive power compared to non-CNA modules. Additionally, the authors should explore the implications of dataset properties on the effectiveness of CNA and conduct more extensive experiments across different types of graphs, such as homophilic versus heterophilic and inductive versus transductive settings. Furthermore, the authors should clarify the role of the clustering method, particularly the use of k-means, and discuss its stability and sensitivity to parameters. Lastly, including performance metrics for SAGEConv and GCNConv baselines in Table 4 would enhance clarity regarding the parameter introduction by the CNA module.