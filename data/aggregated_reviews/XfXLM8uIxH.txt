ID: XfXLM8uIxH
Title: Counting Reward Automata: Sample Efficient Reinforcement Learning Through The Exploitation of Reward Function Structure
Conference: AAAI
Year: 2023
Number of Reviews: 3
Original Ratings: 6, 6, 7
Original Confidences: 3, 3, 3

Aggregated Review:
### Key Points
This paper presents Counting Reward Automata (CRA), a novel framework for modeling reward functions in reinforcement learning (RL) as formal languages, capable of expressing any recursively enumerable reward function, unlike previous state machine-based approaches limited to regular languages. The authors propose Counterfactual Q Learning, which leverages the structure of CRA to enhance sample efficiency and convergence guarantees. Empirical results indicate that their method outperforms existing approaches in sample efficiency, automaton complexity, and task completion.

### Strengths and Weaknesses
Strengths:
- The CRA framework is general and expressive, handling a wide range of problems, including context-free and context-sensitive languages.
- The paper provides comprehensive theoretical and empirical evidence supporting the performance and scalability improvements of CRA over existing methods.
- It effectively utilizes natural language and large language models to facilitate intuitive and human-readable specifications from task descriptions.

Weaknesses:
- The paper lacks a clear comparison or discussion of trade-offs between CRA and other neuro-symbolic or hierarchical RL methods targeting long-horizon tasks.
- There is insufficient analysis or evaluation of the robustness, generalization, or interpretability of CRA-based agents.
- The empirical studies are limited, and details regarding the use of ChatGPT for generating formal language specifications from natural language are minimal.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the version of ChatGPT used in evaluations and provide explicit comparisons of sample efficiency with baseline methods. Additionally, the authors should include detailed descriptions of the dataset, including train/dev/test splits and examples for counterfactual reasoning. A confidence interval for the results in Figure 6 would enhance the robustness of the findings. Furthermore, a more thorough analysis of the trade-offs between CRA and other methods, as well as an evaluation of the robustness and interpretability of CRA-based agents, would strengthen the paper.