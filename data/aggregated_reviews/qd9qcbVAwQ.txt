ID: qd9qcbVAwQ
Title: Parselüêç: Algorithmic Reasoning with Language Models by Composing Decompositions
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 6, 7, 7, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Parsel, a framework that decomposes algorithmic reasoning problems into subparts, samples programs for each subpart, and verifies them. Parsel transforms problems into an intermediate pseudocode-like language to describe functionality and dependencies, utilizes existing language models for sampling, and employs test generation techniques for verification. The authors claim that Parsel improves performance in solving code problems and virtual robot control, demonstrating performance gains on standard code generation benchmarks and a robotic planning task. The evaluation on HumanEval and APPS shows that Parsel effectively generates longer and more complex programs while managing computational costs.

### Strengths and Weaknesses
Strengths:
1. The overall idea of divide-and-conquer and verifying subparts is reasonable and intuitive.
2. Using an intermediate language simplifies reasoning and parsing.
3. The design of the Parsel language is intuitive, effectively combining language model capabilities with deterministic specifications.
4. The paper is well-written and motivated, with thorough experiments validating performance gains and analyzing inference costs.
5. The results indicate outstanding performance with lower budget requirements compared to existing methods.

Weaknesses:
1. The comparison between Parsel and Codex is unfair due to differing sample and evaluation budgets, undermining the claim of substantial improvement.
2. There is a lack of detail and analysis on how verification of decomposed functions aids performance, leaving several critical questions unanswered regarding unit tests, constraints, and the effectiveness of component verification.
3. Minor reproducibility concerns arise due to the recent discontinuation of OpenAI's public API for Codex, which may hinder experimental replication.
4. Questions remain regarding the expressiveness of the Parsel language, particularly its ability to handle data structure specifications and mutual recursion.
5. Clarity issues exist in sections discussing human expert comparisons and the evaluation budget versus generation budget trade-offs.

### Suggestions for Improvement
We recommend that the authors improve the fairness of their comparisons by addressing the sample and evaluation budget discrepancies. Additionally, we urge the authors to provide more detailed analyses on how verification of decomposed functions contributes to overall performance, specifically addressing questions about unit test generation, the correctness of unit tests, and the impact of partitioning the function reference graph into strongly connected components (SCCs). We also suggest improving clarity in the "Functions without constraints" section and providing more detailed explanations of how constraints are enforced. Furthermore, please clarify the setup of the human expert comparison experiment and the conclusions drawn from it. It would be beneficial to include an example of mutual recursion and discuss the sharing of child functions between parents in real-world programs. To address reproducibility concerns, we encourage the authors to use publicly available models like GPT-3.5 for additional results. Lastly, we recommend that the authors explicitly discuss the evaluation time and its implications for usability, as well as consider the proposed experiment of replacing the Parsel program with a Codex prediction for further insights.