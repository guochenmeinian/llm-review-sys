ID: KHfQKygNSc
Title: Robustness of Named-Entity Replacements for In-Context Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper explores the robustness of the in-context learning (ICL) paradigm by focusing on named-entity replacements, revealing that LLMs show a consistent lack of robustness to such changes. The authors conduct experiments on three datasets using both an encoder-decoder LLM (Flan-T5) and a decoder-only LLM (GPT-J), analyzing factors like frequency, token length, and perplexity of replaced entities. The study highlights the variance in downstream performance due to entity substitutions and suggests that this area warrants further exploration for enhancing LLM performance.

### Strengths and Weaknesses
Strengths:
- The paper presents a clear aim and is well-structured, making it easy to follow.
- Extensive experiments across multiple datasets lend credibility to the findings.
- The authors provide detailed methodologies and results, facilitating reproducibility.

Weaknesses:
- The analysis is limited to first names, which restricts the applicability of the findings.
- The premise that name changes should not alter answers is questioned, as certain names carry inherent contextual meanings that could influence outcomes.
- The authors do not adequately discuss the implications of their findings or provide suggestions for improving model robustness.

### Suggestions for Improvement
We recommend that the authors improve the discussion in Section 3 by addressing the reasons behind the performance differences observed in entity replacement strategies, particularly why strategy D shows lower accuracy. Additionally, the authors should consider testing the robustness of entity replacements across a broader range of entities beyond first names. It would also be beneficial to explore contextual associations of names and their impact on performance, potentially grouping similar names to assess intra-cluster differences. Finally, we encourage the authors to clarify their plans for sharing datasets and code to enhance reproducibility.