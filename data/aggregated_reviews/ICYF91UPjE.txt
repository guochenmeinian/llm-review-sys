ID: ICYF91UPjE
Title: MathCAMPS: Fine-grained Synthesis of Mathematical Problems From Human Curricula
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 3, 6, 8, 6
Original Confidences: 5, 3, 4, 3

Aggregated Review:
### Key Points
This paper presents a new dataset aimed at evaluating grade-school math skills of Large Language Models (LLMs) through fine-grained categories. The authors propose a synthetic benchmark based on K-8 Common Core math questions and introduce a methodology that incorporates follow-up questions to enhance evaluation consistency. The study reveals that many LLMs perform poorly under this new methodology, indicating challenges in their mathematical reasoning capabilities.

### Strengths and Weaknesses
Strengths:
- The authors propose a framework for generating high-quality mathematical problems aligned with Common Core standards.
- The introduction of a fine-grained synthetic benchmark fills a significant gap in evaluating LLMs' mathematical reasoning.
- The methodology using follow-up questions is innovative and enhances the evaluation process.

Weaknesses:
- The focus on grade-school math may be irrelevant, as current LLMs demonstrate near-perfect accuracy on such questions.
- The dataset quality is questioned due to observed inaccuracies in LLM responses.
- The connection between follow-up questions and the human curriculum lacks clarity, and the main contributions are not well highlighted.

### Suggestions for Improvement
We recommend that the authors improve the dataset quality by rigorously validating the questions, particularly for higher grades. Additionally, focusing on more challenging problems, such as MATH level or higher, would enhance the relevance of the benchmark. A discussion addressing the unexpected performance of LLMs on grade-5 versus grade-6 problems would also strengthen the paper. Lastly, clarifying the connection between follow-up questions and the human curriculum could improve the overall coherence of the contributions.