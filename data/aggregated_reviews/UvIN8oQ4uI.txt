ID: UvIN8oQ4uI
Title: Guiding Large Language Models via Directional Stimulus Prompting
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 4, 6, 6, 7, 6, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 2, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Directional Stimulus Prompting (DSP) method aimed at providing fine-grained guidance for large language models (LLMs) through a small trainable policy model. This model generates hints for queries, facilitating improved outputs via supervised fine-tuning and reinforcement learning (RL). The effectiveness of DSP is demonstrated through experiments on summarization and dialogue generation tasks, yielding significant performance improvements.

### Strengths and Weaknesses
Strengths:  
1. The paper proposes a direct and feasible solution for guiding black-box LLMs, which is applicable given the rapid development of LLMs.  
2. The writing is clear and well-structured, making the content accessible.  
3. Experimental results effectively demonstrate the method's efficacy, with notable improvements in performance metrics.  
4. The authors have released their code, enhancing reproducibility and facilitating further research.  

Weaknesses:  
1. The technical novelty is limited, as the training methods for the policy model are standard and largely borrowed from existing works.  
2. The selection of "pseudo-stimulus" during supervised fine-tuning lacks a general principle, particularly for open-ended text generation tasks.  
3. The experiments are conducted on a narrow set of datasets (CNN/DM and MultiWOZ), which may not convincingly evaluate the method's performance across diverse tasks.  
4. The reliance on automatic evaluation metrics like ROUGE is insufficient; human evaluation should be incorporated for a more robust assessment.  
5. The discussion of limitations regarding the potential for generating harmful or biased content is inadequate.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their approach by exploring unique techniques tailored to the specific tasks at hand. Additionally, we suggest providing a general principle for selecting pseudo-stimulus, especially in open-ended text generation contexts. Expanding the experimental evaluation to include a broader range of datasets and tasks would strengthen the findings. Incorporating human evaluation alongside automatic metrics would enhance the rigor of the results. Finally, we encourage the authors to discuss the risks associated with guiding LLMs towards generating unethical content and propose strategies to mitigate these risks.