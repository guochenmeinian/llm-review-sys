ID: nC8WUrpWjG
Title: Answer-state Recurrent Relational Network (AsRRN) for Constructed Response Assessment and Feedback Grouping
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an answer-state recurrent neural network (AsRNN) aimed at enhancing the assessment quality of constructed responses in STEM contexts. The authors propose encoding various input types (context, question, student answer, etc.) using a pre-trained language model, with the encoded results processed through a recurrent network that incorporates contrastive learning loss alongside standard cross-entropy loss. A new dataset, Col-STAT, is introduced, comprising five input types for evaluation. The proposed method shows improved performance on Col-STAT and competitive results on other datasets compared to existing models, including GPT-3.5.

### Strengths and Weaknesses
Strengths:
- The research motivation and connection between background and proposed method are well-articulated.
- The paper addresses a novel aspect of automated assessment, particularly in providing formative feedback.
- The implementation of AsRNN is clear, and the design choices are justified with explanations and ablation studies.

Weaknesses:
- The usefulness of the proposed network is questionable, as performance improvements largely stem from contrastive learning, with unclear benefits when it is not used.
- The architectural decisions lack sufficient discussion, particularly the choice of the Recurrent Relational Network over other graph neural networks.
- The dataset size raises concerns about generalizability, and the paper does not adequately address the complexity of training and inference times.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the mapping between symbols in Figure 2 and their corresponding equations, ensuring that terms like C_i, Q_i, R_i, and A_i are clearly defined. Additionally, the authors should provide a detailed explanation of how hyper-parameters, particularly the time step t, are determined and clarify the differences between "student response" and "student response items." To enhance the paper's impact, we suggest including results from simpler settings to illustrate the effectiveness of contrastive learning. Finally, the authors should consider expanding the discussion on architectural choices and the implications of their findings on incorrect answers in the context of formative feedback.