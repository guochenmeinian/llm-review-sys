ID: VVDewLcVkx
Title: GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 3, 4, 6, -1
Original Confidences: 4, 4, 4, -1

Aggregated Review:
### Key Points
This paper presents GameBench, a collection of 9 out-of-distribution game benchmarks designed to evaluate the strategic reasoning abilities of language model agents. The authors employ GPT-3 and GPT-4 models, utilizing Chain-of-Thought (CoT) prompting and Reasoning Via Planning (RAP) scaffolding, revealing that these models underperform compared to humans, even in optimal conditions. However, the paper lacks comprehensive insights and comparisons with related works, making it difficult to interpret the results effectively.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a unique benchmark for evaluating strategic reasoning in LLMs, addressing a significant gap in the field.
2. It provides a detailed analysis of model performances and explores the sensitivity of models to various factors.
3. The benchmark covers a wide range of strategic reasoning tasks across diverse game environments.

Weaknesses:
1. Important details about the games and their implementation are missing, making the paper hard to follow.
2. The analysis of results is insufficient, with unclear reasons for LLM underperformance and a lack of concrete evidence.
3. Many related works are not discussed, limiting the contextual understanding of the proposed benchmarks.

### Suggestions for Improvement
We recommend that the authors improve the introduction of the games by including a brief summary of each game and how they fit into the "reasoning categories" outlined in Table 1. Additionally, a thorough comparison with related works should be included to highlight similarities and differences. The authors should also enhance the analysis of results by discussing potential reasons for LLM underperformance and proposing solutions. Furthermore, we suggest that the authors clarify whether the games are implemented in Python code or if existing APIs are used, as this impacts the evaluation of their contribution. Lastly, it would be beneficial to include more examples of game play to help readers understand the context of the findings.