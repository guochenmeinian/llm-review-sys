ID: iajxrSgOSX
Title: DELIFFAS: Deformable Light Fields for Fast Avatar Synthesis
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 5, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for real-time rendering of animatable full-body avatars using a hybrid mesh-lightfield representation. The authors propose a two-surface formulation that integrates with a lightfield MLP, achieving near real-time performance at 30FPS for 1K resolution. The method demonstrates competitive quantitative and qualitative results against state-of-the-art techniques. Additionally, the paper introduces a novel approach for human avatar reconstruction from multiview video data, leveraging deformable light fields for improved geometry and texture modeling. The two-surface light field approach maps points on a base mesh and viewing directions to the radiance of real geometry, enabling the encoding of detailed surface textures and view-dependent effects, thus achieving photorealistic novel view synthesis results. Unlike previous methods limited to a single surface and static scenes, this approach can recover real geometry between inner and outer surfaces and represent dynamic scenes, such as human motion, through additional conditioning on motion history.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow, with a technically sound proposed method that effectively integrates two-layer surface formulation with lightfield-based neural rendering.
- The method runs at 31FPS for 1K resolution and requires only one MLP evaluation per ray, showcasing efficiency.
- Thorough quantitative and qualitative evaluations are provided, with reasonable baseline choices and an ablation study indicating reduced sensitivity to mesh quality.
- The two-surface parameterization effectively recovers real geometry and supports dynamic scene representation.
- The paper demonstrates superiority over the single-surface method in various ablation studies, confirming the effectiveness of the proposed design.

Weaknesses:
- Insufficient details on how the deformed mesh is obtained raise questions about the adequacy of normals for full image reconstruction, particularly regarding clothing dynamics.
- The impact of the underlying mesh quality on performance is not adequately addressed, and the authors do not clarify the training conditions for the models compared in qualitative assessments.
- The claim regarding perceptual supervision compared to previous approaches is misleading, as prior works also support full-image supervision with perceptual losses.
- The reliance on L1-supervision in ablation studies may limit the understanding of the full impact of perceptual supervision.
- The novelty of the framework is questioned due to prior explorations of differentiable rasterization for animatable avatars, and the risk of overfitting is noted due to reliance on a single dataset.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how the deformed mesh is generated and its implications for clothing dynamics. It would be beneficial to provide more details on the underlying motion-to-mesh model and its influence on performance. Additionally, we suggest including comparisons with other datasets to mitigate overfitting concerns and to validate the method's generalizability. Clarifying the differences between this work and prior studies on differentiable rasterization is essential. We also recommend improving the clarity of the ablation study results by including a more detailed discussion on the implications of using L1 versus L1+perceptual supervision. Lastly, we encourage the authors to address the limitations of the proposed method, particularly regarding the handling of complex clothing and the potential for artifacts in rendering, and to ensure that the final revision includes proper citations for relevant works to enhance the paper's academic rigor.