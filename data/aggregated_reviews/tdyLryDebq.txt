ID: tdyLryDebq
Title: FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 3, 6, 5, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new evaluation metric for natural language generation (NLG) quality based on the similarity between the frequency spectra of cross-entropy in human-written and model-generated texts. The authors propose using Fourier Analysis of the Cross-Entropy of language (FACE) to quantify how closely the surprisal frequency patterns of generated text align with those of natural language. The method involves computing a discrete Fourier transform of the token-level cross-entropy sequence and evaluating the similarity using various metrics. The authors argue that their approach, which applies the raw Fourier Transform (FFT) to cross-entropy sequences, contrasts with previous methods that utilized simple n-gram models and the periodogram method. They suggest that significant spectral differences between human and model outputs lie in low-frequency components. Experimental results indicate that FACE correlates with human judgments of text quality, although not as strongly as the existing metric MAUVE.

### Strengths and Weaknesses
Strengths:
1. The metric is well-motivated, aligning with psycholinguistic theories about periodicity in surprisal statistics.
2. The algorithm is clearly described and offers a computationally efficient approach to NLG evaluation.
3. The authors provide a unique application of FFT to NLG evaluation, highlighting the importance of avoiding smoothing techniques in analyzing cross-entropy.
4. The proposed metrics of FACE offer a comprehensive analysis of frequency domain information.
5. The paper is well-written and easy to follow, with a comprehensive experimental section.

Weaknesses:
1. The motivation behind the periodicity of cross-entropy in human language is unclear, which undermines the paper's soundness.
2. The method lacks novelty, as applying FFT to cross-entropy sequences is perceived as not significantly different from Xu et al.'s periodogram method and does not explore deeper analyses or alternative uses of frequency domain information.
3. The work is criticized for potentially lacking depth and broader relevance, focusing more on feature engineering rather than addressing profound problems in NLG evaluation.
4. The experimental section does not compare against sufficient baselines, raising questions about the effectiveness of the proposed metric in distinguishing between human and model-generated texts.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their motivation regarding the periodicity of cross-entropy in human language. Additionally, consider incorporating deeper analyses of the frequency domain information and exploring alternative methods that utilize this information more effectively. We suggest that the authors provide a more in-depth discussion of the implications of their findings to demonstrate the broader relevance and potential impact of their work, moving beyond a straightforward application of existing methods. Expanding the experimental section to include comparisons with more baselines would strengthen the validation of FACE. Finally, providing insights into the advantages of FACE over MAUVE in other aspects would enhance the paper's contribution.