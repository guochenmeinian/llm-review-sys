ID: SLTQluG80x
Title: Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 7, 5, 7, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on learning models sufficient for risk-sensitive reinforcement learning (RL). The authors demonstrate that value-equivalent models are inadequate in risk-sensitive contexts and introduce the distribution equivalence principle, which identifies models that yield the same return distribution as the actual environment. To address the limitations of matching the entire return distribution, the authors propose statistical function equivalence, focusing on specific statistics of the return, and define relevant loss functions. The paper also integrates the proposed framework with existing model-free risk-sensitive algorithms and validates its performance through empirical evaluations.

### Strengths and Weaknesses
Strengths:
- The paper addresses a novel topic regarding the learning of models for risk-sensitive policies.
- The writing is clear, particularly in sections 2-5, where ideas are well-formalized.
- The theoretical contributions are significant, with comprehensive proofs supporting the proposed methods.

Weaknesses:
- The motivation for using distribution equivalence over maximum likelihood estimation (MLE) is insufficiently explained, and the paper lacks demonstrations of scenarios where the proposed approach outperforms MLE.
- The experimental design is unclear, with no comparison against MLE, limiting the persuasiveness of the results.
- The empirical evaluations are limited, lacking detailed results and clarity on parameter settings in experiments.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the advantages of their proposed approach compared to MLE, particularly in scenarios with limited model capacity or data. Additionally, the authors should include comparisons against MLE model estimation in their empirical evaluations to substantiate claims of improved performance. Clarifying the experimental details, including parameter settings and the representation of models, will enhance the paper's rigor. Lastly, addressing the limitations of the proposed framework, particularly regarding its applicability to various risk measures, would strengthen the overall contribution.