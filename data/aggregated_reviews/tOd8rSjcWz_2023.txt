ID: tOd8rSjcWz
Title: Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 8, 7, 10, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the MultiModal C4 (mmc4) dataset, a large-scale image-text interleaved dataset designed to enhance multimodal in-context learning and few-shot learning. The dataset, built upon the text-only C4 corpus, incorporates 571 million images aligned with 43 billion English tokens using a linear assignment algorithm based on CLIP features. Initial experiments with OpenFlamingo on MSCOCO image captioning demonstrate the dataset's effectiveness for few-shot learning.

### Strengths and Weaknesses
**Strengths:**
1. The scale of the mmc4 dataset significantly exceeds that of previous datasets, providing a valuable resource for multimodal research.
2. The paper details comprehensive data collection, filtering, and alignment strategies, which serve as a guide for future dataset construction.
3. The authors have created various subsets of the dataset and established a baseline method, enhancing its versatility.

**Weaknesses:**
1. The paper lacks a discussion on the implications of using the full dataset for training, particularly regarding the impact of incorporating lower-quality data.
2. The results presented focus solely on captioning, with no additional examples of in-context learning provided.
3. The validity of the dataset is questioned due to the reliance on the CLIP model, and the manual verification process for image relevance is limited to a small sample size.

### Suggestions for Improvement
1. We recommend that the authors investigate the effects of training on the full dataset, including the potential impact of lower-quality data on model performance.
2. The authors should include additional experimental results showcasing in-context learning examples to highlight the dataset's broader applicability.
3. We suggest that the authors release the respective CLIP scores for each image-text pair and provide more detailed information regarding the positional relationships of images and text in the HTML DOM.
4. The authors should address the social impact of the dataset, particularly concerning privacy and potential biases, and consider conducting further manual screening for face detection.
5. We recommend improving the clarity of the manuscript by addressing grammatical errors, ensuring proper acronym usage, and providing detailed explanations for key information that is currently missing.