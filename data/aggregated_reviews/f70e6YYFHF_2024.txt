ID: f70e6YYFHF
Title: The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 7, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of the "reversal curse," where models trained on a relation in one order fail to generalize to the reverse order. The authors propose reframing this issue as the "factorization curse," which highlights the models' inability to learn the same joint distribution under different factorizations. They introduce the new dataset WikiReversal for evaluation and suggest using factorization-agnostic training objectives, specifically MLM-U, to mitigate these issues. The paper provides experimental evidence supporting these claims.

### Strengths and Weaknesses
Strengths:
1. The introduction of the "factorization curse" broadens the understanding of model failures in information retrieval.
2. The WikiReversal dataset is a valuable resource for evaluating model performance in realistic scenarios.
3. Extensive experiments demonstrate the existence of the "factorization curse" in both toy examples and real cases.
4. The proposed factorization-agnostic training objective shows promise in improving model performance while maintaining quality in forward queries.

Weaknesses:
1. The experiments are conducted with small model sizes (100M parameters), leaving scalability to larger models unclear.
2. There is insufficient analysis on whether the proposed training objective negatively impacts performance on standard benchmarks, particularly in open generation tasks.
3. The architecture details for MLM-U are sparsely explored, and the paper does not compare against XLNet as a baseline, which could strengthen the argument for MLM-U.

### Suggestions for Improvement
We recommend that the authors improve the scalability analysis by testing their approach on larger models, such as those exceeding 70B parameters. Additionally, the authors should evaluate the side effects of fine-tuning on unrelated tasks using LM Eval Harness to understand potential trade-offs. It would also be beneficial to empirically confirm whether MLM-U mitigates the two-token factorization curse and to provide a comparison against XLNet to clarify the advantages of their proposed method. Lastly, addressing the minor typographical errors and clarifying notation inconsistencies would enhance the overall presentation.