ID: HvWfTrjwWa
Title: Generalized Balancing Weights via Deep Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 2, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for estimating balancing weights in causal effect estimation, utilizing the density ratio of counterfactual and observed distributions. The authors propose estimating this density ratio through the optimization of the variational expression of the f-divergence, specifically advocating the use of $\alpha$-divergence to mitigate the vanishing-gradient problem in neural networks. The study also explores the properties of the alpha divergence, particularly in the context of causal effect estimation, and claims that their method allows for the application of $f'$ instead of the sub-differential $\partial f$, assuming $f$ is twice differentiable. The paper adopts Theorem 3.2.2 of Pearl (2009) as a definition, although the notation used for joint distributions has been criticized for lacking clarity. The authors address concerns regarding the sample complexity of the alpha divergence estimator and the unbiasedness of mini-batch gradients. While the paper includes theoretical results, it notably lacks experimental validation.

### Strengths and Weaknesses
Strengths:
- The expression of $\phi$ as $\exp(T)$ simplifies the optimization variable's domain.
- The discussion on the vanishing gradient and the proposed method for setting $\alpha$ is insightful and potentially beneficial.
- The authors effectively address major concerns regarding definitions and clarify the assumptions made in their methodology.
- The study presents interesting theoretical results related to density ratio estimation.

Weaknesses:
- Several technical aspects lack precision, and the writing is disjointed, making the overall message unclear.
- The abstract's claims regarding the advantages of $\alpha$-divergence are not sufficiently substantiated, particularly concerning sample complexity and unbiasedness.
- The reliance on external references for key definitions detracts from the paper's self-containment.
- The novelty of the approach is questionable, as it primarily revisits existing methods for density ratio estimation without introducing significant innovations.
- The presentation of the paper, particularly in the main manuscript, is considered unclear and overly complicated, especially regarding causal effect estimation.
- The notation used for joint distributions is confusing, and the scope of the paper may limit its broader applicability.
- The absence of experimental results in the main text is a notable limitation.

### Suggestions for Improvement
We recommend that the authors improve the clarity and coherence of the writing to enhance the overall message of the paper. It would be beneficial to provide a more rigorous explanation of why $\alpha$-divergence is advantageous, particularly in relation to sample complexity and the vanishing-gradient issue. Additionally, the authors should ensure that the paper is more self-contained by including essential definitions and concepts rather than relying on external references. We suggest incorporating experimental results in the main body of the paper to validate the proposed method and comparing it with existing baselines to demonstrate its effectiveness. Furthermore, we recommend improving the clarity of the notation used in Definition 3.2 to avoid confusion regarding joint distributions. We suggest reorganizing the paper to highlight the main contributions before delving into specific applications, as the current structure complicates the main messages. Lastly, we encourage the authors to justify the choice of $f$-divergence over other divergences by discussing the unbiasedness property more explicitly.