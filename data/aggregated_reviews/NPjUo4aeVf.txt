ID: NPjUo4aeVf
Title: MDEval: Evaluating and Enhancing Markdown Awareness in Large Language Models
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MDEval, a novel benchmark for evaluating Markdown Awareness in large language models (LLMs), which is crucial for the readability and structure of chatbot responses. The authors construct a dataset with 20,000 instances in English and Chinese across ten subjects and propose metrics to assess Markdown Awareness. The evaluation approach is reference-free, relying on a capable LLM to correct markdown mistakes and score based on the edits required. The study demonstrates that fine-tuning underperforming models on this dataset can enhance their Markdown Awareness to levels comparable to advanced models like GPT-4.

### Strengths and Weaknesses
Strengths:
- The introduction of MDEval addresses an important yet overlooked metric—Markdown Awareness—contributing originality to the field.
- The paper is well-structured, with clear methodology and extensive experiments validating the benchmark's effectiveness.
- It provides high-quality results, evaluating the performance of nine mainstream LLMs and demonstrating the potential for improvement through fine-tuning.

Weaknesses:
- There is a lack of clarity regarding the relevance of this work to the Search and retrieval-augmented AI track, raising questions about its connection to the broader Web community.
- The evaluation method, which uses a capable model to generate references, may introduce biases, as this model could also produce incorrect outputs affecting the scoring.
- The paper does not sufficiently analyze why certain models perform better or worse in Markdown Awareness, limiting insights into the benchmark's implications.

### Suggestions for Improvement
We recommend that the authors improve the connection to the Search and retrieval-augmented AI track by clearly articulating its relevance. Additionally, we suggest revisiting the evaluation approach to ensure that the use of a capable model for reference generation does not compromise the validity of the results. It would be beneficial to include a more in-depth analysis of model performance variations to enhance the understanding of the benchmark's implications. Lastly, we encourage the authors to justify the choice of Markdown as the optimal format for evaluating readability and to explore alternative metrics that consider semantic or visual impacts.