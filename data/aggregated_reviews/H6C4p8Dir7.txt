ID: H6C4p8Dir7
Title: OmniTokenizer: A Joint Image-Video Tokenizer for Visual Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 6, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents OmniTokenizer, a transformer-based tokenizer designed for both image and video tokenization within a unified framework. The authors propose a decoupled spatial-temporal architecture and a progressive training strategy, starting with image data to develop spatial encoding before incorporating video data for temporal dynamics. Extensive evaluations demonstrate that OmniTokenizer achieves state-of-the-art performance in reconstruction tasks and enhances generative models across various datasets.

### Strengths and Weaknesses
Strengths:
- The joint tokenizer for image and video modalities represents a significant advancement, with the potential to benefit the community.
- The progressive training strategy is intuitive and effectively improves performance, as evidenced by the ablation results.
- The paper is well-written and presents the concepts clearly.

Weaknesses:
- There is a lack of thorough comparisons to relevant baselines, particularly MagVITv2, which is crucial for evaluating the method's effectiveness. The absence of qualitative comparisons, especially for video results, is notable.
- The architecture's novelty is limited, as it closely resembles previous methods, and the rationale behind design choices, such as the decoupled attention mechanisms, is insufficiently explored.
- Important details regarding the training process, such as the order of VQ and KL training, and the specifics of the generative models are not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the paper by including a comprehensive comparison with relevant baselines, particularly MagVITv2, to provide a clearer understanding of the method's performance. Additionally, incorporating qualitative results for video generation in the supplementary materials would enhance the evaluation. We suggest providing more detailed explanations of the architecture, particularly regarding the decoupled attention mechanisms and the training process. Finally, including insights into the scalability of the model, such as computational complexity and resource requirements, would be valuable for future research.