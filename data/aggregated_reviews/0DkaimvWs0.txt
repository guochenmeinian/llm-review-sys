ID: 0DkaimvWs0
Title: Contrastive Pre-training for Personalized Expert Finding
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework for expert finding in Community Question Answering (CQA) by utilizing contrastive learning techniques. The authors propose a question title-body contrastive pre-training task to enhance question modeling and a personalized tuning network to learn expert representations. The experimental results indicate that the proposed method outperforms several conventional baseline methods across multiple datasets.

### Strengths and Weaknesses
Strengths:
- The proposed title-body contrastive pre-training task and personalized tuning network are innovative and reasonable.
- The performance comparisons across six datasets with multiple baselines are well-executed and convincing.
- Extensive experiments and an ablation study demonstrate the effectiveness of the proposed methods.

Weaknesses:
- The explanation of "expert ID embedding" is unclear, lacking specifics on its representation and ability to capture personalized characteristics.
- Insufficient experimental analysis on parameters such as question body and title lengths, and the number of negative samples.
- The comparison with PMEF lacks fairness, as it does not involve another pre-trained model like ExpertBert.
- The necessity for personalized fine-tuning is inadequately justified, and comparisons with existing data augmentation methods are missing.
- The technical novelty is limited, and the writing quality requires improvement.

### Suggestions for Improvement
We recommend that the authors clarify what "expert ID" refers to and its capability to represent personal characteristics. Additionally, the authors should justify the chosen lengths for question body and title, considering potential information loss, and ensure consistency across datasets. A fair comparison with another pre-trained model, such as ExpertBert, is essential to validate the advantages of the proposed method. Furthermore, we suggest including comparisons with existing data augmentation methods and other unsupervised contrastive learning techniques like SimCSE. Lastly, improving the overall presentation and writing quality of the paper is necessary.