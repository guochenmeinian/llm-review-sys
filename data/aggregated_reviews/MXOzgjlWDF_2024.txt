ID: MXOzgjlWDF
Title: Structured Unrestricted-Rank Matrices for Parameter Efficient Finetuning
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 5, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach for parameter-efficient fine-tuning (PEFT) using structured unrestricted-rank matrices (SURMs) to replace low-rank adaptations like LoRA. The authors propose three variants of SURMs—Kronecker, Toeplitz, and Circulant matrices—demonstrating that these structured matrices can achieve comparable performance to existing methods while significantly reducing the number of trainable parameters. The experiments cover various tasks in both vision and language domains, showing that structured matrices can effectively approximate different classes of matrices, leading to improved performance at a lower parameter cost. Additionally, the authors investigate the performance of SURM in both large and small data regimes using the iNat2021 dataset, which includes over 2.7 million training images across 10,000 species. They demonstrate that SURM achieves comparable results to full fine-tuning while utilizing significantly fewer parameters (55K vs. 86M). The authors also explore the limitations of low-rank methods like LoRA and MoRA in low data regimes, showing that SURM's circulant and skew-circulant matrices outperform these methods in terms of loss metrics.

### Strengths and Weaknesses
Strengths:  
- The use of structured matrices for finetuning yields performance on par with or better than previous works while reducing parameter costs.  
- The comprehensive experiments demonstrate SURM's efficiency in parameter usage and performance across different data regimes.  
- The toy experiments convincingly show that structured matrices can better approximate matrices than low-rank matrices, validated through multiple experimental setups.  
- The authors effectively address reviewer comments and incorporate additional experiments, enhancing the manuscript's depth.  
- The clarity of figures and comprehensive supplementary materials enhance the paper's presentation.  

Weaknesses:  
- The paper lacks studies on more complex tasks and larger models, limiting the generalizability of the findings.  
- Insufficient details regarding hyperparameter selection and data splitting raise concerns about the robustness of the experimental results.  
- The theoretical justification for the advantages of structured matrices over low-rank adaptations is underdeveloped, and empirical evidence needs strengthening.  
- There are concerns regarding the experimental settings, particularly the discrepancy in results compared to the original MoRA paper, which reported high accuracy in memorization tasks.  
- The choice of cross-entropy loss as a metric instead of accuracy raises questions about the validity of the comparisons made.  

### Suggestions for Improvement
We recommend that the authors improve the exploration of the proposed method on larger models and more challenging tasks, such as language generation and commonsense reasoning, to better assess its effectiveness. Additionally, providing detailed explanations of hyperparameter choices and data splitting methods would enhance the reproducibility of the results. Strengthening the theoretical justification for the use of structured matrices, alongside empirical validation through ablation studies, would significantly bolster the paper's contribution. We also suggest including comparisons with higher-rank adapters and conducting experiments in regimes where full fine-tuning typically performs best to demonstrate the advantages of SURMs more convincingly. Furthermore, we recommend improving the clarity of how the number of parameters is adjusted in the skew-circulant formulation and discussing as a hypothesis why SURM performs well in both low and high data regimes. Addressing the limitations of this technique in greater detail, specifically in scenarios where it may underperform compared to other methods, would also be beneficial. Finally, we encourage the authors to conduct further experiments to validate their findings regarding UUID memorization and consider reporting accuracy alongside cross-entropy loss for a more comprehensive evaluation.