ID: xbbknN9QFs
Title: On Evaluating Adversarial Robustness of Large Vision-Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 7, 6, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an evaluation of the adversarial robustness of large vision-language models (VLMs) through extensive experiments on models such as UniDiffuser, BLIP, MiniGPT-4, and LLaVA. The authors propose two adversarial strategies: transfer-based and query-based, which leverage surrogate models like CLIP and BLIP to enhance the effectiveness of attacks. The study highlights the vulnerabilities of these models to adversarial attacks, particularly in black-box settings.

### Strengths and Weaknesses
Strengths:
- The paper conducts comprehensive experiments across multiple prominent VLMs, providing a representative analysis of adversarial robustness.
- The introduction of effective and realistic attack strategies tailored for VLMs demonstrates significant contributions to understanding their vulnerabilities.
- The presentation is clear, with good motivation and visual demonstrations that validate the proposed methods.

Weaknesses:
- The evaluation lacks exploration of baseline defenses, such as adversarial training on CLIP, which could mitigate the vulnerabilities identified.
- The focus on the vision component raises questions about the potential for language-based attacks, which are not addressed.
- The black-box setting's validity is questionable due to the reliance on white-box access to surrogate models, necessitating further elaboration on transferability.
- The cost-effectiveness of the query-based strategy is not sufficiently discussed, particularly regarding the inference cost per query.

### Suggestions for Improvement
We recommend that the authors improve the discussion on baseline defenses, specifically how adversarial training on CLIP could mitigate the vulnerabilities of their proposed attacks. Additionally, the authors should explore the potential for language-based attacks and provide a more detailed analysis of the black-box setting's assumptions regarding transferability. It would also be beneficial to include information on the computational resources required for reproducing the experiments and to demonstrate the sole use of the query-based strategy without a strong prior for a more lightweight approach. Finally, we suggest that the authors clarify the cost-effectiveness of the query-based attacks, including the forward inference cost and the number of queries needed per attack.