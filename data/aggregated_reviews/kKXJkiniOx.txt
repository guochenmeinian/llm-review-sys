ID: kKXJkiniOx
Title: ConDaFormer: Disassembled Transformer with Local Structure Enhancement for 3D Point Cloud Understanding
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 6, 5, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ConDaFormer, a novel Transformer architecture for 3D point cloud understanding. The authors propose a method that disassembles the cubic window into three orthogonal 2D planes, significantly reducing computational costs while maintaining performance. Additionally, the architecture incorporates depth-wise sparse convolutions to enhance local structure capture. The effectiveness of ConDaFormer is validated through experiments on point cloud detection and segmentation datasets, demonstrating substantial improvements over existing methods.

### Strengths and Weaknesses
Strengths:
1. The disassembly of the cubic window into tri-planes is well-founded and effectively reduces computational costs.
2. The method shows remarkable performance improvements in semantic segmentation and achieves satisfactory results on widely-used datasets.
3. Comprehensive ablation studies support the effectiveness of the proposed design and hyper-parameter choices.
4. The paper is well-structured and clearly presented.

Weaknesses:
1. The interaction between different planes relies solely on additive operations, which may compromise 3D structural information.
2. The proposed model has similarities with existing works, such as FlatFormer, and the authors need to clarify these differences.
3. Some experiments, particularly in object-level classification and segmentation, are missing.
4. The performance in 3D object detection does not significantly surpass the baseline FCAF3D, and further validation on outdoor perception tasks is recommended.

### Suggestions for Improvement
We recommend that the authors improve the interaction between the planes beyond additive operations to enhance task performance. Additionally, we suggest conducting further experiments on object-level classification and segmentation datasets, such as ShapeNet and ModelNet, to strengthen the empirical results. Clarifying the distinctions between ConDaFormer and similar models like FlatFormer will also enhance the paper's contribution. Lastly, addressing the notation of the attention matrix for the three planes could improve clarity.