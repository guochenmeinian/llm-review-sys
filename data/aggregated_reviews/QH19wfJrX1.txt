ID: QH19wfJrX1
Title: mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents mLongT5, a multilingual transformer model designed to handle long input sequences, with extensive evaluations across various languages. The authors propose a publicly available pre-trained model, demonstrating its capabilities in multilingual summarization and question-answering tasks, outperforming existing models like mBERT and mT5.

### Strengths and Weaknesses
Strengths:
- The paper introduces a practically useful multilingual language model capable of processing long input sequences.
- It includes extensive evaluations across multiple languages and scenarios, contributing valuable insights to the field.
- The open-sourcing of the model and datasets is beneficial for the research community.

Weaknesses:
- The evaluation method using ROUGE is criticized for its reliance on exact lexical matches, with suggestions to use BertScore for more reliable assessments.
- There is a perceived lack of innovation in the model's development, and some experimental results lack sufficient explanation, particularly regarding performance discrepancies with mBERT.
- The paper does not adequately discuss computational costs associated with pre-training the models.

### Suggestions for Improvement
We recommend that the authors improve the evaluation methodology by incorporating at least one model-based evaluation metric alongside ROUGE scores. Additionally, the authors should provide a detailed analysis of the scenarios where mLongT5 underperforms compared to mT5 or multilingual BERT, particularly in relation to the Russian summarization results. An ablation study using the same datasets as mBERT for pre-training mLongT5 could clarify performance issues. Finally, we suggest adding a discussion on the computational costs of pre-training these models to provide a more comprehensive understanding of their resource requirements.