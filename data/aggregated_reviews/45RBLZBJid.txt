ID: 45RBLZBJid
Title: Accelerated On-Device Forward Neural Network Training with Module-Wise Descending Asynchronism
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AsyncFGD, a forward-only training method that integrates asynchronous updates with Forward Gradient Descent (FGD) to enhance training efficiency on memory-constrained platforms. The authors propose that their method can alleviate resource underutilization by employing multiple parallel workers to process different parts of the network. The paper includes both theoretical analysis and empirical evaluations, demonstrating the method's performance on small-scale datasets.

### Strengths and Weaknesses
Strengths:
- The proposed proof of convergence for AsyncFGD-SGD is novel.
- The method addresses an important challenge in efficient on-device training and is well-motivated.
- The authors provide a clear explanation of their approach and conduct empirical evaluations on actual hardware.

Weaknesses:
- The novelty of AsyncFGD is not clearly articulated, as it appears similar to existing forward-only training methods.
- The experimental results are limited to small-scale datasets, raising questions about scalability.
- The paper lacks a thorough discussion on memory consumption and does not compare the memory footprint of AsyncFGD with that of FGD and BP methods.
- There are significant concerns regarding the assumptions made about resource utilization and the conditions under which AsyncFGD achieves speedup.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the novelty of AsyncFGD by explicitly differentiating it from existing methods. Additionally, we suggest expanding the experimental evaluation to include large-scale datasets to better assess the method's scalability. A detailed analysis of memory consumption for all methods, including how the selection of *K* affects memory usage, should be included to enhance the discussion on memory efficiency. Furthermore, we advise addressing the limitations of the proposed method, particularly regarding its performance compared to BP-based methods. Lastly, we encourage the authors to proofread the paper for typos and ensure that all sections are complete and coherent.