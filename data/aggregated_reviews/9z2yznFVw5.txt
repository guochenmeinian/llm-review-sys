ID: 9z2yznFVw5
Title: Decomposed Prompt Tuning via Low-Rank Reparameterization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method called "Decomposed Prompt Tuning via Low-Rank Reparameterization," which aims to enhance the efficiency of prompt tuning for large pre-trained language models by reducing the number of trainable parameters while maintaining effectiveness. The authors propose a low-rank reparameterization technique and validate their approach through experiments on the SuperGLUE benchmark, demonstrating its superiority over existing methods in terms of both effectiveness and efficiency.

### Strengths and Weaknesses
Strengths:
- The proposed method significantly reduces the number of trainable parameters while maintaining performance.
- The low-rank reparameterization technique is innovative and effective.
- Experimental results on the SuperGLUE benchmark are clear and comprehensive, showcasing the method's effectiveness in both high-resource and low-resource scenarios.
- The paper is well-written and presents its findings clearly.

Weaknesses:
- The evaluation is limited to the SuperGLUE benchmark, raising questions about performance on other datasets.
- There is insufficient analysis of the computational cost and memory requirements, which may affect practical applicability.
- The paper lacks a thorough comparison with other related methods, particularly adapter-based approaches like LoRA, which could provide a better understanding of the proposed method's strengths and weaknesses.
- The interpretability of the derived soft prompts is not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the relationship between the rank and the soft prompt, providing further empirical or theoretical analyses to explain the effectiveness of the decomposed prompt. Additionally, we suggest elaborating on how the proposed approach addresses the challenges of prompt tuning optimization. It would be beneficial to include a detailed analysis of the computational cost and memory requirements, as well as a thorough comparison with existing methods, particularly adapter-based approaches, to contextualize the advantages of their method. Finally, we encourage the authors to explore the interpretability of the derived soft prompts to enhance understanding of the model's decision-making process.