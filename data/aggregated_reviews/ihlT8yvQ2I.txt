ID: ihlT8yvQ2I
Title: GNNEvaluator: Evaluating GNN Performance On Unseen Graphs Without Labels
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 6, 6, 7, 7, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel problem of evaluating Graph Neural Networks (GNNs) on unseen test graphs without ground-truth labels. The authors propose a two-stage evaluation framework comprising DiscGraph set construction and GNNEvaluator training and inference. This framework captures distribution discrepancies between training and test graphs, modeling structural discrepancies and estimating node classification accuracy without labeled data. Experimental results demonstrate the method's effectiveness on real-world unlabeled test graphs.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a practical evaluation method for GNNs, addressing the challenge of model assessment in real-world scenarios where evaluation often takes considerable time.
2. The framework effectively constructs a set of meta-graphs to simulate unseen test graphs, capturing data distribution discrepancies and modeling structural differences using latent node embeddings.
3. The experimental validation on real-world unlabeled test graphs showcases the proposed method's capability in estimating node classification accuracy.

Weaknesses:
1. Section 3.3 lacks sufficient detail on how to extract the seed sub-graph Sseed from the observed training graph S.
2. The representation of discrepancy is overly simplistic, raising concerns about its generalizability to larger datasets with diverse patterns, such as ogbn-arxiv and ogbn-products.
3. The graph augmentation methods only introduce minimal disturbances, questioning their applicability in scenarios with larger gaps. Additionally, it remains unclear how these methods could predict performance for models trained with graph augmentation techniques like dropedge.
4. The transferability of the GNN evaluator is uncertain, particularly regarding its ability to predict performance across different GNN architectures, such as from SAGE to GCN.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the seed sub-graph extraction process in Section 3.3. Additionally, consider enhancing the complexity of the discrepancy estimation to ensure better generalization to larger datasets. We suggest exploring more robust graph augmentation techniques that can create significant disturbances and evaluating their impact on model performance. Furthermore, it would be beneficial to clarify the transferability of the GNN evaluator across different architectures and provide a discussion on the practical applications of the proposed method, especially in light of the observed prediction errors. Lastly, including ablation studies for the GNNEvaluator and expanding the experimental scope to include more datasets and settings would strengthen the paper.