ID: POxV67j3L9
Title: Do Metadata and Appearance of the Retrieved Webpages Affect LLM's Reasoning in Retrieval-Augmented Generation?
Conference: EMNLP/2024/Workshop/BlackBoxNLP
Year: 2024
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: 4, 4, 4

Aggregated Review:
### Key Points
This paper presents an investigation into the sensitivity of retrieval-augmented LLMs to context perturbations, specifically focusing on how variations in metadata—publication date, news source, and webpage aesthetics—affect model outputs. The authors utilize the ConflictingQA dataset and find that LLMs exhibit varying degrees of sensitivity to these alterations, with notable model-dependent behaviors, particularly in Claude models regarding CSS formatting. The experimental design includes paired comparisons and a new synthetic dataset to mitigate confounding effects from LLM training.

### Strengths and Weaknesses
Strengths:
- The investigation addresses a relevant and timely research question regarding input properties influencing RAG model outputs.
- The experimental method is robust, employing both real and synthetic datasets and utilizing paired comparisons to draw causal conclusions.
- The findings reveal interesting model-specific effects and the authors thoughtfully consider the ethical implications of their work.

Weaknesses:
- The paper lacks exploration of the "blackbox" nature of LLMs, which is a key focus of the conference. It does not sufficiently enhance understanding of LLM functioning.
- There is no inclusion of human performance as a baseline for comparison, which would provide valuable context for interpreting LLM results.
- The rationale for selecting the three specific attributes studied is not adequately justified, leaving the paper feeling incomplete.
- The investigation of non-textual information is conducted in isolation, without examining potential interaction effects between different types of metadata.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the binary classification of answers, explicitly stating the proportion of non-binary responses and justifying the binarization process earlier in the paper. Additionally, simplifying the notation for tested settings by using terms like original/flipped would enhance comprehension. We suggest including human evaluations on a subset of the data to provide context for LLM results. The authors should clarify their choice of metadata attributes and consider including discussions on potential interaction effects and strategies to mitigate undesired influences in RAG systems. Lastly, we advise revising the limitations section to focus on specific drawbacks of the technical approach rather than general statements about the paper's length.