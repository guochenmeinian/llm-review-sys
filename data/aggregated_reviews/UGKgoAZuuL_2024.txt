ID: UGKgoAZuuL
Title: Bridging Inter-task Gap of Continual Self-supervised Learning with External Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 4, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach, BGE, to enhance inter-task data comparison in Continual Contrastive Self-Supervised Learning (CCSSL) by incorporating external data. The authors argue that existing methods overlook inter-task discrimination, leading to suboptimal performance. BGE utilizes a One-Propose-One (OPO) sampling algorithm to select relevant external data, facilitating implicit comparisons and improving feature discriminability. Additionally, the authors address privacy concerns in training machine learning models by leveraging external datasets while avoiding the retention of past training images. They propose scenarios, such as remote sensing and surveillance, where storing past images poses security risks, and argue that their method can improve performance by sampling relevant external data from publicly available datasets after the training stage. Experiments demonstrate BGE's effectiveness in enhancing classification results across various datasets.

### Strengths and Weaknesses
Strengths:
1. The finding that existing regularization-based CCSSL methods overlook inter-task discrimination and the proposed method leveraging external data are novel.
2. The paper provides extensive experimental results validating BGE's effectiveness in improving classification performance across different datasets.
3. The authors provide a clear rationale for their method's applicability in privacy-sensitive contexts.
4. Preliminary experiments indicate the effectiveness of the proposed method, particularly in comparison to random baselines.

Weaknesses:
1. The proposed method is a straightforward extension of contrastive learning with external data, limiting its novelty and technical contributions.
2. The SSL method primarily employs BarlowTwins, which is not typically considered a contrastive learning method, raising concerns about the appropriateness of terminology and claims made in the paper.
3. The introduction of external data may increase computational cost and training time, which could limit applicability in resource-constrained environments.
4. There is insufficient justification for the experimental paradigm, particularly regarding privacy concerns and the applicability of the method in real-world scenarios.
5. The paper lacks concrete examples of tasks and data domains where the method can be effectively deployed.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their method by exploring more innovative approaches beyond straightforward extensions of existing techniques. Additionally, we suggest providing a clearer justification for the use of BarlowTwins as a contrastive method and including insights on how non-contrastive SSL methods can effectively distinguish inter-task data. Furthermore, the authors should analyze the computational cost associated with using external data and discuss the trade-off between performance improvement and time consumption. We also recommend improving the justification for the experimental paradigm by providing concrete scenarios where their method is applicable, particularly in privacy-sensitive contexts. The authors should explore the scenario of using a public model with a private dataset in future submissions. It is crucial to include a replay baseline throughout all experiments, as well as the rotating external data baseline and external-data-only baseline. The authors should investigate the effect of external data budget size (K) and clarify the fraction of DomainNet domains selected. Finally, we suggest incorporating a discussion on the limitations of using generative models for representation learning and considering more recent visualization techniques like PaCMAP to address concerns regarding t-SNE's sensitivity to parameter choice.