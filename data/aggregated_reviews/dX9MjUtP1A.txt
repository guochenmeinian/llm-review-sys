ID: dX9MjUtP1A
Title: Self-Correcting Bayesian Optimization through Bayesian Active Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 23
Original Ratings: 5, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel function, SAL, for active learning based on statistical distance, which is integrated into Bayesian optimization (BO) to enhance performance. The authors propose a framework called self-correcting BO (SCoreBO) that simultaneously learns Gaussian process (GP) hyperparameters while optimizing a black-box function. They clarify the distinction between marginalizing the acquisition function and the model over hyperparameter uncertainty, addressing potential misunderstandings in the BO community. The authors emphasize that while PES and other methods utilize a fully Bayesian treatment, SCoreBO's main contribution lies in its joint uncertainty reduction approach, which enhances optimization performance. The paper also explores Bayesian Active Learning by Disagreement (BALD) and its equivalence to the SAL-KL method, emphasizing the role of different distance metrics in optimizing hyperparameter tuning tasks. Experimental results indicate improved performance of SCoreBO over traditional benchmarks, although most experiments are conducted on synthetic benchmark functions.

### Strengths and Weaknesses
Strengths:
- The paper introduces new active learning algorithms leveraging the Query-by-Committee strategy and two statistical distances.
- It addresses the significant challenge of learning hyperparameters while optimizing a black-box function, which is crucial for real-world applications.
- The work is generally well-written and clear, with a thorough evaluation against a suitable set of benchmarks.
- The paper provides a comprehensive mathematical foundation for the SAL-KL/BALD equivalence, enhancing the theoretical understanding of the methods.
- The authors demonstrate strong experimental results, showing that SCoreBO outperforms existing methods in various tasks.
- The clarification of terminology and the relationship to existing methods like BALD and PES enhances the paper's rigor.
- The authors' commitment to addressing reviewer feedback and improving the paper's clarity is commendable.

Weaknesses:
- The literature review is inadequate, raising concerns about the novelty of the proposed problem and solution.
- The narrative and coherence of the paper require improvement to clearly convey the contributions and methodologies.
- The paper lacks sufficient real-world experiments, primarily focusing on synthetic benchmarks.
- Some reviewers express concerns about the novelty of the approach and the need for a more comprehensive theoretical discussion.
- The complexity of the proposed methods may limit their applicability in high-throughput scenarios.
- The reliance on fully Bayesian hyperparameter treatment introduces significant computational demands, which could hinder practical implementation.
- There are inconsistencies in symbol usage and technical details, which hinder understanding.
- The performance of SCoreBO on certain tasks, such as the Rosenbrock function, raises concerns about its efficiency under specific conditions.

### Suggestions for Improvement
We recommend that the authors improve clarity by precisely defining their symbols, particularly distinguishing among the true underlying function \( f(x) \), the observed noisy function value \( f(x)+\epsilon \), and the GP model \( GP(x) \). Additionally, we suggest including more real-world experiments, such as tuning network parameters in AutoML, to demonstrate the practical applicability of their method. We encourage the authors to justify their choice of distance metrics, particularly why they prefer Hellinger or Wasserstein distances over KL divergence, and to clarify the relationship between their proposed SAL and existing methods like BALD. Furthermore, it would be beneficial to provide more details on the fully Bayesian treatment mentioned in the paper and to include a comparison with baselines that average over samples of GP hyperparameters, such as PES. We also recommend that the authors improve the introduction to clearly articulate the unique contributions of SCoreBO and differentiate it from existing methods. The authors should consider expanding the theoretical discussion surrounding the properties of SCoreBO and its relationship to other methods, particularly in the context of active learning and hyperparameter optimization. Lastly, we advise correcting minor typographical errors, ensuring consistent formatting throughout the paper, and improving the clarity of the limitations section by explicitly stating the conditions under which SCoreBO may underperform.