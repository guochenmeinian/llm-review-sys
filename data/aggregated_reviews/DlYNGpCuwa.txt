ID: DlYNGpCuwa
Title: Aligning LLM Agents by Learning Latent Preference from User Edits
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 4, 7, 7, 7, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel task focused on user editing of agent responses to enhance personalization. The authors propose the PRELUDE framework to learn user preferences from historical edits in an interactive manner and introduce the CIPHER algorithm to consolidate these preferences with retrieval from past edits. The effectiveness of these methods is validated through experiments in summarization and email writing contexts.

### Strengths and Weaknesses
Strengths:
1. The proposed task of user editing during interactions is innovative and significant.
2. The methods are intuitive and straightforward to understand.
3. Comprehensive experiments are conducted to demonstrate the effectiveness of the proposed methods.

Weaknesses:
1. Users may be reluctant to invest time in providing feedback, suggesting a need for implicit preference acquisition methods.
2. The use of Levenshtein edit distance as a cost function may not adequately capture user preferences, which reside in the semantic space rather than token sequences.
3. The embedding methods lack a fine-tuning process, potentially leading to retrieval gaps as they primarily index texts based on content rather than user preferences.
4. The user simulator based on GPT-4 requires evaluation to ensure alignment with human responses.
5. The accuracy matrix necessitates a classification model, which is not mentioned in the paper.
6. The authors do not provide open access to the source code and data, despite indicating "YES" in checklist 5.

### Suggestions for Improvement
We recommend that the authors improve the framework by exploring implicit methods for gathering user preferences to address potential reluctance in providing feedback. Additionally, consider revising the cost function to better reflect semantic differences in user preferences. We suggest incorporating a fine-tuning process for the embedding methods to enhance retrieval accuracy based on user styles. Furthermore, the authors should evaluate the user simulator against real-world human responses to validate its effectiveness. Clarifying the accuracy metric and providing open access to the source code and data would also strengthen the paper.