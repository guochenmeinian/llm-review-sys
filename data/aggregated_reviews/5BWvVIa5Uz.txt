ID: 5BWvVIa5Uz
Title: Emergent Inabilities? Inverse Scaling Over the Course of Pretraining
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the phenomenon of inverse scaling, where performance on certain tasks decreases as model size or training data size increases. The authors evaluate the Pythia model family across four tasks previously shown to exhibit inverse scaling with respect to model size, discovering that two of these tasks also demonstrate inverse scaling related to training data size. The findings suggest that smaller models exhibit less inverse scaling behavior than larger models.

### Strengths and Weaknesses
Strengths:
- The findings are novel and contribute a new perspective on inverse scaling, particularly regarding training data size.
- The paper is well-written and accessible, making it easy to understand the presented concepts.
- The experiments cover a diverse range of model sizes, providing a valuable foundation for future research.

Weaknesses:
- The contribution is limited, focusing on only four existing datasets without in-depth analysis or broader experimentation.
- The paper lacks a thorough exploration of the reasons behind the observed inverse scaling, particularly regarding training dynamics.
- The largest model evaluated is relatively small compared to contemporary models, limiting the relevance of the findings.

### Suggestions for Improvement
We recommend that the authors improve the depth of their analysis by including a broader range of tasks, particularly emergent tasks, to strengthen their conclusions about data-related inverse scaling. Additionally, the authors should provide a more detailed discussion on why only tasks with existing model size-related inverse scaling were examined, and consider including tasks that do not exhibit such scaling. A qualitative analysis of model outputs, especially for larger models, would also enhance the understanding of performance discrepancies. Finally, clarifying the linear regression experiments and addressing the missing references would improve the paper's rigor and completeness.