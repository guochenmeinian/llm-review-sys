ID: UQnQDLml8D
Title: Epistemic Uncertainty and Observation Noise with the Neural Tangent Kernel
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 1, 4
Original Confidences: 4, 3

Aggregated Review:
### Key Points
This paper presents a study on NTK approximations of neural networks trained with a ridge penalty, aiming to estimate the posterior variance corresponding to the NTK mean. The authors propose a method for computing the mean and covariance of the posterior distribution of a Gaussian Process with the neural tangent kernel, incorporating aleatoric noise. However, the authors' claims regarding the convergence of regularized neural networks to a regularized NTK and their variance estimation are challenged as incorrect or incomplete.

### Strengths and Weaknesses
Strengths:  
- The proposed method for addressing the mean and covariance of the posterior distribution is sensible.  
- The paper attempts to tackle an important aspect of neural network training related to regularization and noise.

Weaknesses:  
- The proof of convergence to a regularized NTK is incomplete, relying on assumptions that are not substantiated.  
- The estimate of variance is criticized for being merely an empirical estimate rather than a true posterior variance.  
- The presentation lacks rigor, with missing assumptions and unclear notation.  
- The experiments section is underdeveloped, lacking detailed explanations and justifications for hyperparameter choices.

### Suggestions for Improvement
We recommend that the authors improve the rigor of their proof by clearly establishing that the first-order approximation holds under regularization. Additionally, the authors should clarify the assumptions on the model \( f \), including differentiability, and explicitly state the relevant spaces. It would be beneficial to address the discretization error in the proof, as well as clarify the notation used throughout the paper. 

We suggest enhancing the experiments section by providing clearer labels for plots, a more convincing discussion of results, and justifications for hyperparameter choices. The authors should also make the code for reproducing experiments publicly available and ensure that the bibliography is consistent and properly formatted. Lastly, we recommend elaborating on the introduction's claim about "shedding light on regularization in deep learning" for better clarity.