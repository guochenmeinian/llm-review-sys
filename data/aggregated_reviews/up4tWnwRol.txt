ID: up4tWnwRol
Title: The Fine-Grained Complexity of Gradient Computation for Training Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 5, 4, 6, 8, -1
Original Confidences: 2, 3, 2, 3, -1

Aggregated Review:
### Key Points
This paper presents a study on the fine-grained complexity of gradient computation for training attention networks, particularly under the Strong Exponential Time Hypothesis (SETH). The authors demonstrate that when the norm of certain matrices is bounded by \( B \), the gradient computation lower bound approaches linearity. They provide an almost optimal algorithm that aligns closely with this lower bound. The analysis extends previous work on forward computations to backward computations, establishing a threshold for gradient computation and proving the impossibility of a subquadratic algorithm when the bound exceeds this threshold.

### Strengths and Weaknesses
Strengths:
- The paper is theoretically robust and among the first to analyze the computational complexity lower bound for gradient computations in attention networks.
- The complexity analysis is sound and offers valuable insights for future algorithm design, particularly in the context of LLM training.
- The organization and clarity of the presentation are commendable, making the results accessible.

Weaknesses:
- The work is purely theoretical and lacks empirical validation, particularly regarding its applicability to multi-layer attention networks and large language models.
- The clarity of the writing could be improved, especially in defining the problem and presenting contributions. The significance of the results is unclear, particularly concerning practical gradient computation.
- The discussion on the softmax approximation is insufficiently detailed, leading to potential confusion regarding its application.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the introduction by clearly defining the problem and outlining the contributions in a structured manner. Additionally, the authors should address the implications of their algorithm for real applications, particularly how to maintain the norm below a certain threshold and whether to adapt their algorithm for practical gradient computation. We suggest including numerical evidence to demonstrate the advantages of maintaining a small norm or using the proposed algorithm in practice. Furthermore, a dedicated subsection detailing the precise softmax approximation used would enhance understanding. Lastly, the authors should clarify any ambiguous notation, particularly regarding the parameters in Definition 1.4, to avoid confusion.