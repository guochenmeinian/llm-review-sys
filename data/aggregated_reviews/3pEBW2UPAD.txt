ID: 3pEBW2UPAD
Title: ReHLine: Regularized Composite ReLU-ReHU Loss Minimization with Linear Computation and Linear Convergence
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 7, 6, 5, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 2, 3, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new optimization algorithm, ReHLine, for minimizing convex piecewise linear-quadratic (PLQ) loss functions with linear constraints. The authors claim that their method achieves the best known iteration complexity and computational cost, resulting in significant speed improvements over existing solvers. The proposed algorithm is based on a decomposition of PLQ functions into ReLU and smoothed ReLU components, demonstrating a linear convergence rate. 

### Strengths and Weaknesses
Strengths:
- The paper is well-written and clearly presents the theoretical foundations and empirical results supporting the proposed method.
- The ReHLine algorithm shows significant performance gains in various tasks, especially on large-scale datasets.
- The approach is flexible, handling a broad range of linear models and constraints.

Weaknesses:
- The paper lacks a thorough discussion of existing works related to dual-coordinate ascent methods and general optimization solvers, which could provide better context for its contributions.
- Experimental evaluations are missing details, such as the implementation specifics of baseline solvers and the significance of the large-scale condition mentioned.
- Claims of substantial speed improvements may be misleading without proper comparisons to state-of-the-art algorithms, as the paper primarily contrasts against all-purpose solvers.

### Suggestions for Improvement
We recommend that the authors improve the discussion of related work, particularly regarding dual-coordinate ascent methods and general optimization solvers, to clarify the novelty of their contributions. Additionally, please provide clearer details on the experimental setup, including the implementation of baseline solvers and the significance of the large-scale condition. It would also be beneficial to include comparisons against state-of-the-art algorithms for empirical risk minimization to substantiate claims of performance improvements. Lastly, consider addressing the limitations regarding the handling of non-strongly convex penalties and providing an ablation study to demonstrate the performance of different solvers.