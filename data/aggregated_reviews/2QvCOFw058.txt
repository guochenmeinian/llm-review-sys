ID: 2QvCOFw058
Title: Globally Q-linear Gauss-Newton Method for Overparameterized Non-convex Matrix Sensing
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a custom Gauss-Newton method, termed AGN, aimed at solving general over-parameterized matrix sensing problems. The authors demonstrate that AGN is a descent method under benign assumptions and achieves Q-linear convergence under restrictive RIP assumptions. The method incurs a computational cost similar to gradient descent per iteration while converging significantly faster, avoiding saddle points. The paper also claims that AGN can achieve super-linear convergence under specific conditions on the sensing operator.

### Strengths and Weaknesses
Strengths:
1. The introduction of a new algorithm, AGN, provides theoretical guarantees beyond gradient descent methods for a challenging non-convex problem.
2. AGN is theoretically shown to avoid being trapped in Hessian points.
3. The authors prove rapid convergence of AGN under small RIP constants.
4. The paper is well-structured, addresses relevant prior work, and is generally well-written.

Weaknesses:
1. The computational cost and the complexity of obtaining a good AGN update are not clearly explained, leaving readers uncertain about the tradeoff between performance and computation.
2. The analysis in Section 5 relies on very restrictive settings with small RIP constants, which are rarely encountered in practice, raising questions about AGN's performance in more realistic scenarios.
3. Theorem 2 discusses convergence in terms of function value, but it is unclear whether the distance between $X_t$ and the ground truth decreases at a linear rate, which is crucial for understanding convergence.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the computational cost and the process of obtaining AGN updates to better inform readers about the tradeoffs involved. Additionally, we suggest that the authors provide a more comprehensive analysis of AGN's performance under less restrictive RIP constants to illustrate its advantages in practical scenarios. Furthermore, we encourage the authors to clarify the rationale behind the choice of convergence metrics in Theorem 2 and to include a theorem or proof regarding the claimed super-linear convergence rate of the AGN algorithm. Lastly, consolidating all empirical results into a single section with comparisons to state-of-the-art algorithms would enhance the paper's structure and readability.