ID: JPtobPtxKT
Title: Visual Perception by Large Language Modelâ€™s Weights
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 6, 5, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents VloRA, a paradigm for building MLLMs that aligns visual features with the parameter space of LLMs. By representing visual information as model weights, it eliminates the need for visual tokens in the input, thereby reducing input sequence length and enhancing efficiency. The authors propose a novel method for integrating visual understanding into LLMs, achieving competitive performance on various benchmarks while significantly lowering computational costs.

### Strengths and Weaknesses
Strengths:
- The motivation is compelling, addressing the significant computational costs that limit MLLM training and inference.
- VLoRA notably reduces the FLOPs of MLLMs during both training and inference, potentially decreasing GPU RAM consumption.
- Experimental results indicate that VLoRA maintains competitive performance across benchmarks.

Weaknesses:
- The paper emphasizes FLOPs advantages, but this metric may not accurately reflect real latency, particularly during long sentence generation, where decoding time is primarily output-length dependent.
- The experiments lack comprehensiveness; comparisons against LLaVA-1.5 should include tasks like VQAv2, GQA, TextVQA, and others.
- The paper should discuss related works in detail, including HyperPELT, LLaMA-Adapter, and Memory-Space Visual Prompting.
- Practical questions remain regarding the model's performance with multiple images and the impact of modern optimization techniques like FlashAttention and KVCaching.

### Suggestions for Improvement
We recommend that the authors improve the experimental data on real training speed and GPU RAM requirements for both VLoRA and LLaVA. Additionally, please provide insights into the inference efficiency of VLoRA, particularly for long-sequence generation. It is crucial to discuss similar recent works in detail and include results on benchmarks such as VQAv2, GQA, and TextVQA. Furthermore, clarify the choice of CapsFus-30m over blip-558k for pretraining and assess VLoRA's competitiveness with a smaller model. Finally, a more comprehensive discussion of limitations, particularly regarding practical benefits and the handling of multiple images, would enhance the paper's depth.