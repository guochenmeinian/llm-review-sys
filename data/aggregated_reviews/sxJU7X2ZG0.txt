ID: sxJU7X2ZG0
Title: Generative Calibration for In-context Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for calibrating in-context learning (ICL) classifications by estimating generation probabilities, specifically focusing on the marginal probability \( p(y) \) through Monte-Carlo estimation. The authors empirically demonstrate that \( p(x|y) \) effectively ranks examples in terms of AUROC, outperforming existing calibration methods across various settings. The paper also theoretically analyzes the distribution shift and decomposes the posterior distribution based on Bayesian principles, leading to insights about label marginal distributions.

### Strengths and Weaknesses
Strengths:
- The method is straightforward and lightweight, reducing instability in ICL.
- The Bayesian interpretation provides a novel perspective on ICL.
- Extensive empirical validation across 12 text classification tasks and 12 LLMs demonstrates consistent improvements.
- The paper is well-written, and the analysis is clear and supported by empirical results.

Weaknesses:
- Some statements lack scientific rigor, particularly regarding Schwartz's theorem and the connection between assumptions and conclusions.
- The proposed calibration method, especially the "multiple-generation" solution, is not well explained.
- Key assumptions about label marginals are justified only by empirical evidence without theoretical backing.
- Certain equations are not very readable, and the topic prior \( p(\theta) \) is insufficiently discussed.

### Suggestions for Improvement
We recommend that the authors improve the clarity and rigor of their claims, particularly regarding Schwartz's theorem and the logical connections in their arguments. Additionally, more details about the "multiple-generation" solution should be provided, including its efficiency and time costs compared to ICL across different sequence lengths. The authors should also enhance the readability of Equations (1) and (2) and provide a clearer discussion of the topic prior \( p(\theta) \) in relation to LLMs. Finally, addressing the limitations of their method and providing theoretical justification for key assumptions would strengthen the paper.