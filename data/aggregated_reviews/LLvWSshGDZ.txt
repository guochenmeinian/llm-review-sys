ID: LLvWSshGDZ
Title: DiffTED: One-shot Audio-driven TED Talk Video Generation with Diffusion-based Co-speech Gestures
Conference: thecvf
Year: 2024
Number of Reviews: 3
Original Ratings: 3, 4, -1
Original Confidences: 3, 5, -1

Aggregated Review:
### Key Points
We recognize that the paper presents a unique approach for conditional gesture generation using key points, facilitating straightforward video rendering. It includes comprehensive evaluations and an interesting ablation study, particularly noting that predicting noise outperformed predicting key-point positions. However, the paper is not self-contained, relying heavily on the TPS Motion Model without adequate explanation. Additionally, it lacks clarity in detailing the proposed solution and misses citations for several statements.

### Strengths and Weaknesses
Strengths include the innovative one-shot setting that allows generalization to unseen speakers with a single source image, and the exploration of a diffusion model that outperforms traditional LSTM and CNN models. The comprehensive evaluation and ablation studies are also commendable. Conversely, weaknesses involve missing details about the proposed method, unclear incorporation of conditions, and insufficient related work citations. The final video quality suffers from blurry artifacts and misaligned lip movements, and the paper lacks novelty and clarity.

### Suggestions for Improvement
We suggest enhancing the clarity of the paper by including detailed information about the condition representation and its incorporation into the framework. Additionally, we recommend adding citations for diffusion-based gesture generation solutions and explaining the inability to compare with these works. Addressing the identified issues, particularly regarding writing quality and the inclusion of results on facial gestures, will strengthen the paper significantly.