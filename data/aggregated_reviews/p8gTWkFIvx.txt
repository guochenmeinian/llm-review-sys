ID: p8gTWkFIvx
Title: Modality-Independent Teachers Meet Weakly-Supervised Audio-Visual Event Parser
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 7, 6, 8, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called VALOR for weakly supervised audio-visual video parsing, utilizing pseudo-labels from pre-trained models CLIP and CLAP to generate fine-grained temporal labels. The authors demonstrate that their approach significantly outperforms previous methods on the LLP dataset for both audio-visual video parsing and event localization tasks. The model is trained using a combination of audio-visual, audio-only, and video-only losses, leveraging a hybrid attention network (HAN) structure. The paper includes ablation studies that validate the design choices made.

### Strengths and Weaknesses
Strengths:
- The experimental results show a clear improvement over prior approaches, establishing new state-of-the-art performance on the LLP dataset.
- The integration of unimodal guidance with dense labeling effectively addresses the AV video parsing problem.
- The paper is well-structured, with clear explanations and a consistent narrative.

Weaknesses:
- The flow of the paper could be improved, particularly regarding the placement of the CLIP and CLAP introductions, which could be moved to the Preliminaries section.
- The motivation for Section 4.2 is unclear, as similar content is already presented in the Introduction.
- Some experimental settings lack clarity, such as the number of labels and their order, which could be better described.
- The paper does not sufficiently compare the VALOR method against stronger baselines, which could provide a more comprehensive evaluation of its performance.

### Suggestions for Improvement
We recommend that the authors improve the flow of the paper by relocating the introduction of CLIP and CLAP to the Preliminaries section. Clarifying the motivation for Section 4.2 and providing a more detailed description of the dataset, including the number of labels and their order, would enhance understanding. Additionally, we suggest including comparisons with stronger baselines to better contextualize the improvements achieved by VALOR. Addressing the clarity of the automatic label harvesting process and the choice of class-dependent thresholds would also strengthen the paper. Finally, exploring the use of soft labels directly in loss computation and providing a broader analysis of the impact of class imbalance would be beneficial.