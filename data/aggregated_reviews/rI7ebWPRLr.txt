ID: rI7ebWPRLr
Title: Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the MASFormer (Mixed Attention Span Transformer), which aims to reduce the quadratic complexity of self-attention by utilizing full attention in the bottom layers and sparse attention in the remaining layers. The authors argue that capturing long-range dependencies early in the computation is crucial for long-range language modeling and summarization tasks. They demonstrate that their approach saves computational resources while maintaining performance, particularly when continuing the training of a pre-trained model like GPT-2 with larger sequence lengths.

### Strengths and Weaknesses
Strengths:
- The problem is well-motivated, and the proposed method is simple and practical, allowing for easy integration with existing language models.
- The experiments are well-conducted, showing that MASFormer outperforms other attention mechanisms while being more efficient.
- The paper is clear and well-written, providing a detailed analysis of the model's performance.

Weaknesses:
- The paper lacks novelty, as it does not introduce new techniques and does not adequately compare time and space efficiency with other models.
- The theoretical computational complexity remains quadratic due to the use of full attention in the initial layers, which is not sufficiently addressed in the experiments or analysis.

### Suggestions for Improvement
We recommend that the authors improve the paper by including a comprehensive analysis of the time and space efficiency required for each model. Additionally, we suggest that the authors compare their approach with existing efficient transformers to better contextualize their contributions. It is also essential to demonstrate the computational time and memory requirements rather than relying solely on "attention cost" metrics, as this will provide a clearer understanding of the trade-offs involved.