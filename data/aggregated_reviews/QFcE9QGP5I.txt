ID: QFcE9QGP5I
Title: Adaptive Quasi-Newton and Anderson Acceleration Framework with Explicit Global Convergence Rates
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 4, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for developing first-order iterative methods aimed at solving unconstrained minimization problems, drawing connections between quasi-Newton and Anderson acceleration methods. The authors establish explicit global non-asymptotic convergence rates under specific assumptions, utilizing a backtracking strategy for step size determination. The efficiency of the proposed methods is compared against a fine-tuned BFGS algorithm through numerical experiments. The authors also clarify the use of double-precision floating points in their computations, addressing concerns about gradient sensitivity and the choice of step size \( h = 10^{-9} \). Although the implementation of a backtracking strategy for determining \( h \) did not enhance convergence speed, the algorithm demonstrates theoretical guarantees and scales comparably to current quasi-Newton methods.

### Strengths and Weaknesses
Strengths:
- The problem is well-motivated, and the proposed methods achieve explicit global convergence rates that integrate aspects of gradient descent and cubic regularized Newton's method.
- The inputs for the algorithms are clearly discussed, facilitating implementation.
- The complexity of the algorithms is analyzed and compared in numerical experiments.
- The algorithm provides theoretical guarantees and scales comparably to existing quasi-Newton methods.
- The authors have effectively addressed previous concerns and clarified technical aspects.

Weaknesses:
- The theoretical framework relies on numerous assumptions that may be difficult to verify, raising concerns about their necessity.
- The algorithm settings in the numerical experiments are somewhat confusing, particularly regarding the online techniques used.
- The performance of the accelerated algorithm is reported as suboptimal and unstable, with some numerical results not supporting the theoretical claims.
- Some reviewers maintained their initial scores, indicating unresolved concerns regarding the paper's evaluation and potential limitations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the theoretical framework by reducing the number of assumptions and providing verification methods. Additionally, we suggest clarifying the algorithm settings in the numerical experiments, particularly the online techniques used for different methods. It would be beneficial to address the performance issues of the accelerated algorithm and provide a more detailed comparison with classical methods like BFGS. Furthermore, we encourage the authors to enhance the presentation by correcting typos and ensuring consistent notation throughout the paper. Finally, we recommend that the authors improve the clarity of their algorithm's evaluation metrics and provide more detailed comparisons with existing methods to address the remaining concerns raised by reviewers. Elaborating on the specific reasons for any negative evaluations could further enhance the paper's reception.