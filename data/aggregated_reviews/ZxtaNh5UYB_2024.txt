ID: ZxtaNh5UYB
Title: Learn more, but bother less: parameter efficient continual learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LB-CL, a continual learning algorithm for large language models (LLMs) that addresses catastrophic forgetting and forward knowledge transfer through orthogonal low-rank SVD decomposition and sensitivity-based parameter initialization. The authors propose that the orthogonal subspace learning component prevents forgetting by maintaining orthogonal SVD decompositions for different tasks, while the sensitivity-based initialization enhances forward transfer. The method is evaluated on multiple benchmarks, demonstrating superior performance compared to existing state-of-the-art methods.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel combination of sensitivity-based knowledge transfer and orthogonal subspace learning, contributing uniquely to continual learning for LLMs.
- Comprehensive experimental evaluations robustly support the effectiveness of LB-CL against existing methods.
- The methodology is well-developed and includes extensive ablation studies that validate the role of each component.

Weaknesses:
- The advantages of the proposed SVD decomposition over existing methods like LoRA are not sufficiently clarified.
- The paper lacks experiments to assess the sensitivity of the method to task order and does not clarify whether task-ID is required during inference.
- The computational complexity of maintaining orthogonal subspaces as the number of tasks increases is not addressed.
- Some related works are missing, and certain design choices and observations are unclear.
- The paper does not provide open access to the code and datasets, hindering reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the advantages of SVD decomposition over LoRA and provide experiments demonstrating the sensitivity of LB-CL to task order. Additionally, the authors should clarify whether the classification head requires task-ID during inference and address the computational complexity of maintaining orthogonal subspaces. We also suggest including missing related works and providing clearer explanations for certain design choices. Lastly, we encourage the authors to make the code and datasets publicly available to enhance reproducibility.