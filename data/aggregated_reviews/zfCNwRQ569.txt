ID: zfCNwRQ569
Title: Interpreting Unsupervised Anomaly Detection in Security via Rule Extraction
Conference: NeurIPS
Year: 2023
Number of Reviews: 31
Original Ratings: 5, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a rule-set extraction method for a black-box anomaly detection model trained solely on normal or unlabeled data, addressing the need for interpretable anomaly detection, particularly in security contexts. The authors propose using an Interior Clustering Tree (IC-tree) algorithm to derive distribution decomposition rules and a Compositional Boundary Exploration (CBE) algorithm to identify minimal hypercubes enclosing normal data. The method simplifies the complex distribution of normal data into simpler distributions, estimating decision rules to approximate the decision boundary of the pre-trained model. The evaluation against five existing techniques across four black-box models and two datasets shows that the proposed method generally outperforms these methods in terms of fidelity and robustness. The authors also acknowledge the challenges of assessing human understanding in model explanations and propose a step-by-step methodology for deriving explanations from anomaly detection results.

### Strengths and Weaknesses
Strengths:
- The problem addressed is significant, with a clear demand for interpretable anomaly detection in various industries, including security.
- The proposed method is model-agnostic, making it applicable across different applications.
- The algorithms yield interpretable rules with high fidelity to the black-box models, and empirical results indicate that the proposed algorithms generally outperform existing techniques.
- The paper is well-written and presents a useful approach for explainable anomaly detection.
- The authors demonstrate transparency in their methodology and show a commitment to improving their work by reaching out to practitioners for future evaluations.

Weaknesses:
- There is a lack of discussion regarding computational costs associated with the proposed method.
- Several methodological aspects remain unclear, potentially due to the authors' assumptions about the audience's expertise in rule-based methods.
- The motivation for focusing on unsupervised ML methods is unconvincing, as practitioners recognize the inherent inaccuracies.
- The experimental setup is flawed, particularly regarding the use of the CIC-IDS17 dataset, which is known to be problematic.
- Evaluation metrics do not convincingly measure the human-understandability of the explanations provided by the method.
- The experiments are currently limited to security domains, which may restrict the algorithm's perceived applicability.
- The distinction between the IC-Tree and OC-Tree is not clearly articulated, leading to potential misunderstandings regarding their functionalities.
- The 'Understanding Model Decisions' section lacks sufficient detail, which could hinder the reader's comprehension of the algorithm's workings.

### Suggestions for Improvement
We recommend that the authors improve the discussion on computational costs associated with the proposed method. Clarifying the methodology, especially for non-expert readers, would enhance understanding. Additionally, we suggest providing a more compelling argument for the relevance of unsupervised ML in this context and addressing the limitations of the datasets used in the experiments. It would be beneficial to explore alternative datasets and ensure that the experimental setup is robust and statistically sound. We also recommend that the authors improve clarity by addressing specific questions regarding the reasoning behind initial versus auxiliary explorers, the choice of candidates based on minimal probability of being normal, and the derivation of hypercubes. Furthermore, we encourage the authors to strengthen the explanation section and acknowledge the limitations of their approach, emphasizing the need for incorporating expert opinions in future work. Lastly, expanding the empirical validation to include more challenging datasets would help assess the complexity and interpretability of the extracted rules.