ID: GuErIOGLie
Title: Unified Segment-to-Segment Framework for Simultaneous Sequence Generation
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 6, 8, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a unified Seg2Seg framework for simultaneous sequence generation tasks, utilizing a latent segment as a pivot between source and target sequences. The framework employs expectation training to explore potential mappings, achieving high-quality generation with low latency across tasks such as streaming ASR, SimulMT, and SimulST. The authors demonstrate favorable latency-quality trade-offs compared to existing methods.

### Strengths and Weaknesses
Strengths:
- The introduction of latent segments effectively models the mapping between source and target in simultaneous generation tasks.
- The Seg2Seg framework shows improved performance across various tasks compared to existing approaches.
- The model learns optimal segment boundaries without relying on pre-defined heuristics, enhancing its applicability across different modalities.

Weaknesses:
- The framework lacks clarity on how target token predictions are learned, with insufficient details provided.
- The reliance on wav2vec 2.0 raises questions about the classification of streaming ASR and SimulST as special cases of SimulMT, necessitating further experiments to validate performance against previous SOTA methods.
- Some experimental results are confusing, particularly regarding the offline model's performance compared to the HMT paper, raising questions about the contributions of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the learning process for target token predictions within the framework. Additionally, conducting experiments such as GSiMT and HMT would help verify the performance of previous SOTA methods on the proposed tasks. It is crucial to address the discrepancies in experimental results, particularly in Figure 4, to clarify the contributions of the proposed method versus existing models. Furthermore, we suggest including a comparison of training time and costs between the proposed approach and fixed strategies like Wait-k, as well as exploring the use of the gumbel-max trick in expectation training. Lastly, we encourage the authors to provide more insight into the similarities and differences between their approach and the MMA framework.