ID: 0A9f2jZDGW
Title: Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 8, 9, 8, 10, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive analysis of task arithmetic in pre-trained CLIP models, challenging the notion that it arises solely from linear fine-tuning in the NTK regime. The authors introduce weight disentanglement as a crucial factor enabling task arithmetic, demonstrating that linearized fine-tuning enhances weight disentanglement and task arithmetic performance. The investigation includes a theoretical framework and extensive empirical validation, revealing that weight disentanglement is a learned property during pre-training.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written and well-organized, providing a novel perspective on task arithmetic through the lens of weight disentanglement.
- The analysis is supported by extensive empirical findings, and the proposed method for improving task arithmetic is both simple and effective.
- The work strikes a good balance between theoretical insights and practical applications, making it accessible to a broad audience.

Weaknesses:
- The empirical results are limited to CLIP models, raising questions about the generalizability of the findings to other architectures or modalities.
- There is a lack of experiments to falsify the hypothesis, particularly regarding tasks sharing input data.
- The paper does not include a limitations section, which would be beneficial for transparency.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by conducting experiments with additional pre-trained models, such as ResNet or BERT, to establish the broader applicability of their conclusions. Additionally, we suggest including standard deviations in performance metrics to provide a clearer understanding of variability. Furthermore, addressing potential scenarios where linearization may not be applicable, such as with certain activation functions or architectures, would enhance the robustness of the analysis. Lastly, clarifying the paper's focus on CLIP models in the abstract would prevent any misleading implications regarding its applicability to other vision-language models.