ID: evP9mxNNxJ
Title: Are We on the Right Way for Evaluating Large Vision-Language Models?
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 7, 7, 6, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the evaluation of large vision-language models (LVLMs) and identifies two main issues with current benchmarks: the lack of necessity for visual information and data leakage. To address these problems, the authors propose a new benchmark called MMStar, which includes 1,500 multi-modal samples validated by human reviewers. The authors also introduce two metrics, Multi-modal Gain and Multi-modal Leakage, to assess performance and data leakage in LVLMs.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and clearly articulated, facilitating comprehension.
- The evaluation process is meticulously designed, yielding convincing conclusions.
- The authors provide a benchmark that effectively evaluates visual dependency, addressing flaws in existing benchmarks.
- The dataset is divided into six subtasks, ensuring a balanced evaluation across different aspects.

Weaknesses:
- The proposed metrics, Multi-modal Gain and Multi-modal Leakage, depend on the base LLM, complicating comparisons across different LVLMs.
- The manual review process significantly reduces the benchmark size from 11,607 to 1,500 samples, with vague criteria for this reduction.
- The integration of choices directly into the prompt limits usability; separate columns for original questions and options would enhance evaluation.
- The clarity of the manual review process and the rationale behind the criteria used for filtering are insufficiently detailed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the manual review process by providing detailed descriptions of the criteria used and the agreement rates among reviewers. Additionally, we suggest including a wider variety of question types in the MMStar benchmark beyond multiple-choice questions. It would also be beneficial to add a column for original questions and another for possible options in the dataset. Furthermore, we encourage the authors to clarify the rationale behind the significant reduction in sample size and the dependency of the proposed metrics on the base LLM. Lastly, addressing the issue of random guessing and data biases in the context of the findings would strengthen the paper.