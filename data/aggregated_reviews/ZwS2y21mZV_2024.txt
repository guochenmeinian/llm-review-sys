ID: ZwS2y21mZV
Title: Approximation Rate of the Transformer Architecture for Sequence Modeling
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 5, 6, 8, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on Jackson-type approximation rates for single-layer Transformers with one head, comparing their performance with RNNs. The authors introduce novel complexity measures to construct approximation spaces for Transformers, establishing a representation theorem that leads to Jackson-type results for target spaces with a representation theorem.

### Strengths and Weaknesses
Strengths:  
- The literature overview is concise.  
- The Jackson-type approximation rate for the Transformer is derived for the first time.  
- Results are presented within a general framework using rigorous mathematical tools, providing a solid theoretical foundation.  
- The hypothesis of singular value decay pattern can be validated through experiments, emphasizing the importance of pairwise coupling and low-rank structure.  

Weaknesses:  
- The main theorem (Theorem 4.2) appears trivial as it combines definitions of complexities $C^\alpha$ and $C^\beta$.  
- The results are limited to 2-layer single-head Transformers, restricting applicability to more common models like multi-layer multi-head Transformers.  
- Much relevant literature on the approximation power of Transformers has been omitted.  
- The paper deviates from standard Transformer architecture by requiring a neural network layer before the attention mechanism, potentially inheriting limitations from the Kolmogorov Representation Theorem.  

### Suggestions for Improvement
We recommend that the authors improve the presentation of Theorem 4.2 to clarify its independence from the Kolmogorov theorem, as its current placement may be misleading. Additionally, we encourage the authors to provide a similar bound in a constructive manner without relying on the Kolmogorov theorem. It would be beneficial to address how the results can be generalized to analyze multi-layer multi-head Transformers and whether this generalization would yield new insights. Furthermore, we suggest including a more comprehensive literature review to encompass relevant works on the approximation power of Transformers. Lastly, we advise the authors to clarify the construction of the F1 function realizing equation (25) and the flow from equations (22) to (25) to enhance understanding.