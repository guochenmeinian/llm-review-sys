ID: Rvk1wdwz1L
Title: Similarity, Compression and Local Steps: Three Pillars of Efficient Communications for Distributed Variational Inequalities
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates distributed variational inequalities (VI) and saddle point problems (SPP) using a server-workers architecture. The authors propose a method that integrates three communication-saving techniques: compression, local steps, and data similarity across workers. The method demonstrates superior communication complexity for strongly monotone VI and strongly-convex strongly-concave SPP compared to existing methods, while also extending to a partial participation setting. The paper establishes theoretical guarantees on communication complexity and convergence rates.

### Strengths and Weaknesses
Strengths:  
The paper effectively combines three established techniques—similarity, compression, and local updates—to enhance communication efficiency in distributed learning. The authors provide a thorough analysis of convergence and complexity, achieving the best known theoretical results for deterministic algorithms addressing variational inequalities.

Weaknesses:  
The writing and organization are subpar, with numerous typos and unclear statements. The contribution appears incremental, building on prior works, and the assumption of data similarity may not hold in practice. Additionally, the centralized optimization setting may limit applicability, and synchronization issues are not adequately addressed. The analysis does not consider stochastic scenarios, which could be more relevant for real-world applications.

### Suggestions for Improvement
We recommend that the authors improve the clarity and organization of the paper by addressing typos and unclear statements. Specifically, the initial value of \(m^k\) should be defined in Algorithm 1, and the description of the algorithm should be more concise. We suggest providing a lower bound that accounts for compression to better assess the method's optimality. Additionally, a discussion on the implications of synchronization delays and the potential for a stochastic version of the algorithm would enhance the paper's relevance. Lastly, we encourage the authors to clarify the assumptions made regarding data similarity and to explore the implications of these assumptions on the proposed methods.