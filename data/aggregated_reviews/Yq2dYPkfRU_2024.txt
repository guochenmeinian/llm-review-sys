ID: Yq2dYPkfRU
Title: Stability and Sharper Risk Bounds with Convergence Rate $O(1/n^2)$
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on generalization measured by gradients through uniform gradient stability, providing generalization bounds of order $O(1/n+\beta+\sqrt{E_Z[\|\nabla f(A(S);Z)\|_2^2]/n})$ for $\beta$-uniformly stable algorithms. It derives generalization error bounds under a Polyak-Lojasiewicz (PL) condition and applies these findings to empirical risk minimization, gradient descent, and stochastic gradient descent for strongly convex, smooth, and Lipschitz problems. The work demonstrates high probability excess risk bounds of $O(1/n^2)$, indicating fast rates under specific conditions. Additionally, the authors introduce a framework where stability implies generalization, showing that ERM, PGD, and SGD satisfy stability conditions. The complexity of the presented bounds may overwhelm readers, necessitating clearer organization, and the authors clarify their assumptions compared to those in [Klochkov and Zhivotovskiy, 2021].

### Strengths and Weaknesses
Strengths:  
- The paper provides high-probability bounds for the generalization gap on gradients, including the gradient norm at the minimizer, which can imply fast rates in interpolation settings.
- It offers comprehensive applications to various algorithms, such as empirical risk minimization and stochastic gradient descent.
- The paper introduces a novel framework that connects algorithmic stability and generalization, potentially leading to faster convergence rates.
- The authors effectively clarify their assumptions and results in response to reviewer queries.

Weaknesses:  
- The high-probability analysis largely derives from existing work, lacking sufficient novelty in the analysis. The authors should clarify the challenges and their contributions.
- Theorems 1 and 2 only improve existing results by a constant factor, which is not significant.
- The focus on strongly convex problems in Section 4, despite the generalization by gradients being more relevant for nonconvex problems, raises concerns about the assumptions of smoothness and Lipschitz continuity being overly strong.
- The complexity of the bounds and assumptions may confuse readers.
- The computational cost for SGD is high, with Theorem 6 requiring $T=n^4$ and Theorem 13 requiring $T=n^2$, which may not be practical for large-scale problems.
- Several typos and inconsistencies are present throughout the paper, and there is a lack of explicit comparisons with relevant literature, particularly K&Z'21.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their analysis by clearly summarizing the challenges faced and their contributions to the existing literature. Additionally, it would be beneficial to clarify the assumptions made in the problem setup and to provide a more detailed comparison of their results with previous work, particularly K&Z'21. We also suggest that the authors maintain a summary table in the contribution section that highlights relevant bounds to enhance clarity. Furthermore, the authors should address the computational cost concerns for SGD and consider revising the assumptions of smoothness and Lipschitz continuity to broaden the applicability of their results. Lastly, we urge the authors to thoroughly proofread the manuscript to correct the identified typos and inconsistencies.