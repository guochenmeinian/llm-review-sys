ID: 38dQv3OwN3
Title: Fairness Aware Counterfactuals for Subgroups
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 6, 7, 6, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework called FACTS (Fairness Aware Counterfactuals for Subgroups) aimed at auditing subgroup fairness through counterfactual explanations. The authors explore the complexities of recourse bias at both micro (individuals) and macro (subgroups) levels, introducing new notions of subgroup fairness that are model-agnostic and efficient. The experimental evaluation demonstrates the framework's applicability using benchmark datasets.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant and timely issue in machine learning, focusing on fairness in decision-making processes.
- FACTS is a novel, model-agnostic framework that is highly parameterizable, providing a thorough explanation of subgroup fairness notions.
- The experimental evaluation showcases the effectiveness and efficiency of the proposed approach.
- The writing is clear and well-organized, making the paper accessible.

Weaknesses:
- The methodology in the experimental evaluation lacks detail, particularly regarding data collection and statistical analysis, which could enhance clarity and reproducibility.
- The framework may struggle with poorly defined protected attributes and the challenges of defining a cost function for recourse.
- The algorithm's limitations include its reliance on categorical features and the potential for excessive subgroup and action combinations.
- The experiments section is limited in scope, lacking diverse datasets and comparisons with other fairness metrics.

### Suggestions for Improvement
We recommend that the authors improve the description of the experimental methodology, including specific criteria for data selection and preprocessing steps, to enhance clarity and reproducibility. Additionally, providing more detailed statistical analyses of the experimental results would bolster the credibility of the findings. The authors should also address how FACTS handles poorly defined protected attributes and explore the implications of defining a cost function for recourse. Expanding the experiments to include more datasets and comparisons with other fairness metrics would strengthen the paper. Finally, we suggest clarifying the algorithm's limitations regarding the handling of continuous features and ensuring that the definitions and terms used are consistent with existing literature.