ID: 9m87e9Keq1
Title: RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 8, 6, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the impact of synthetic data on improving the reasoning capabilities of large language models (LLMs), specifically focusing on the GSM8K and MATH datasets. The authors propose a per-step DPO approach that leverages negative synthetic data to enhance sample efficiency, addressing limitations observed in previous methods that primarily utilized positive examples. Key findings indicate that while finetuning on positive examples yields modest gains, incorporating negative responses alongside positives significantly improves model performance and sample efficiency.

### Strengths and Weaknesses
Strengths:
- The work reveals important nuances in using synthetic data, particularly the value of leveraging negative examples and the importance of credit assignment for intermediate steps.
- The authors conduct extensive experiments, including scaling studies and comparisons with various baselines, strengthening the validity of their claims.
- The conceptual model of negative sample construction is an interesting contribution, formally expressing the synthetic data problem in the context of reinforcement learning.

Weaknesses:
- The paper lacks a discussion on computational costs, which is crucial for assessing the scalability of the proposed approaches.
- There is insufficient clarity regarding the per-step DPO definition and its implementation, particularly in relation to the algorithmic outline and experimental setup.
- The findings primarily focus on mathematics, raising questions about generalizability to other language tasks or domains.
- The paper does not adequately address potential long-term consequences of training on synthetic data, such as error accumulation over generations.

### Suggestions for Improvement
We recommend that the authors improve the discussion on computational costs by providing comparisons of the overall FLOP required for finetuning, including synthetic data generation. Additionally, clarification on the per-step DPO definition should be included, possibly with an algorithmic outline in the appendix. We suggest that the authors expand their analysis to consider the generalizability of their findings beyond mathematics and include discussions on the long-term implications of synthetic data training. Finally, we encourage the inclusion of more quantitative results and a dedicated conclusion section to enhance the paper's clarity and presentation.