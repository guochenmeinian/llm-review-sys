ID: Li2rpRZWjy
Title: Rule Extrapolation in Language Modeling: A Study of Compositional Generalization on OOD Prompts
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 8, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on rule extrapolation, an out-of-distribution (OOD) behavior of autoregressive language models (LLMs), focusing on how different model architectures affect this ability. The authors propose a normative theory for OOD prompt completion, which effectively explains empirical observations regarding training dynamics that facilitate rule extrapolation. The research evaluates various models, including linear, recurrent (LSTM), transformers, and state space models, in the context of formal languages to systematically analyze their extrapolation capabilities.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a significant question regarding the extrapolation capabilities of language models, providing valuable insights through extensive simulations.  
- It introduces a normative theory that clarifies the training dynamics related to rule extrapolation, serving as a foundational work for future investigations into OOD problems in language models.  
- The clarity of presentation and structured methodology enhances the paper's accessibility and impact.

Weaknesses:  
- The investigation is limited to four models of fixed sizes, neglecting the impact of model capacity on performance.  
- The paper lacks a comprehensive exploration of different architectures and hyper-parameters, which raises questions about the observed performance differences.  
- Some sections, such as the normative theory, do not significantly contribute to the overall impact of the paper, and certain technical terms are introduced without adequate definitions.

### Suggestions for Improvement
We recommend that the authors improve the investigation by including a broader range of model sizes and architectures to better assess the impact of model capacity on performance. Additionally, we suggest that the authors clarify the definitions of key terms introduced in the paper to enhance readability for a wider audience. Expanding the discussion in section 2.1 to include recursively enumerable languages could provide a more complete context for the current work. Furthermore, we encourage the authors to consider reproducing the parity experiment with current architectures and to include results for the recently proposed xLSTM to increase the paper's significance. Lastly, we advise reordering sections 4.1 and 5 for improved logical flow and impact.