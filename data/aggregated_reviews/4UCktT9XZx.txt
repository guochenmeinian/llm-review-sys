ID: 4UCktT9XZx
Title: MuSe-GNN: Learning Unified Gene Representation From Multimodal Biological Graph Data
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 8, 6, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MuSe-GNN, a model designed for learning gene embeddings from single-cell sequencing and spatial transcriptomic data through multimodal machine learning and deep graph neural networks. The authors propose a method that incorporates regularization via weighted similarity learning and contrastive learning to establish cross-data gene-gene relationships, resulting in informative graph structures for model training. Empirical results demonstrate that MuSe-GNN effectively learns functional gene similarities across various tissues and outperforms existing gene embedding models across multiple metrics, potentially aiding in the investigation of diseases like COVID-19 and lung cancer.

### Strengths and Weaknesses
Strengths:
1. The model effectively learns gene expression, utilizing regularization techniques to analyze large-scale multimodal biological datasets.
2. It is applied to COVID and cancer datasets, revealing potential disease resistance mechanisms based on differentially co-expressed genes.
3. MuSe-GNN shows significant performance improvements, with enhancements ranging from 24.2% to 100.4%.
4. The overall methodology and mechanisms are well-detailed, and the writing is clear and accessible.

Weaknesses:
1. The novelty of the proposed method is limited, and the differences from existing models like Geneformer are not adequately addressed.
2. There is a lack of comparison with related works and insufficient ablation studies to validate the model's components.
3. The methodology section, particularly the cross-graph Transformer, is not clearly described, making it difficult to follow.
4. Competitors' evaluations may be unfair due to the tuning of MuSe-GNN's hyper-parameters without similar adjustments for comparison models.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology section, particularly regarding the cross-graph Transformer. Additionally, it would be beneficial to include comparisons with other deep learning-based multimodal representation learning models and to conduct further ablation studies to assess the impact of individual components on performance. We also suggest that the authors address the hyper-parameter tuning discrepancies between MuSe-GNN and its competitors to ensure fair comparisons. Lastly, clarifying the differences between MuSe-GNN and Geneformer would strengthen the paper's contribution.