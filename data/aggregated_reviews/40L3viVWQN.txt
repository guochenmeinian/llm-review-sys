ID: 40L3viVWQN
Title: The Pick-to-Learn Algorithm: Empowering Compression for Tight Generalization Bounds and Improved Post-training Performance
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a meta-learning algorithm called Pick-To-Learn (P2L), which compresses a training set \(D\) into a smaller set \(T\) while ensuring that the performance of a base learner trained on \(T\) meets a predetermined threshold. The authors leverage recent results from compression theory to establish tight generalization bounds based on the size ratio \(|T|/|D|\). The algorithm iteratively selects the least well-explained data point from \(D\) to include in \(T\) until all remaining points meet the loss threshold. The paper also compares the P2L+GD algorithm against PAC-Bayes, emphasizing that P2L is a meta-algorithm that, when combined with gradient descent (GD), yields a learning algorithm suitable for performance evaluation. The authors clarify that while P2L+GD generates both a hypothesis and a risk bound, PAC-Bayes algorithms have advanced the state of the art in hypothesis learning and certification. Experimental results on the MNIST dataset and synthetic regression demonstrate that P2L outperforms the PAC-Bayesian baseline and performs competitively with test-set approaches. The authors acknowledge the need to assess the compatibility of P2L with PAC-Bayes through additional simulations.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel application of compression theory to derive generalization bounds, providing a fresh perspective in the field.
- The algorithm is well-structured, and the proofs are clearly presented, making the paper accessible and enjoyable to read.
- The results indicate that P2L achieves tighter bounds than PAC-Bayes, showcasing its potential for practical applications.
- The manuscript demonstrates a clear understanding of the trade-offs between retraining from scratch and incremental updates, highlighting computational advantages and sensitivity to data points.
- The authors provide a straightforward modification to the original P2L algorithm that allows for the incorporation of multiple points at each iteration without altering the proofs.
- The paper includes a detailed explanation of the proof for Theorem A.4, showcasing the rigorous approach taken in deriving results.

Weaknesses:
- The results primarily rely on existing theoretical frameworks, lacking significant original contributions or complex proofs.
- The experimental evaluation is limited, with a need for broader dataset comparisons and deeper exploration of interpretability.
- The algorithm's practical applicability is questionable, as it requires pretraining on a substantial portion of the dataset and may not perform well with random initialization.
- The claim that "P2L dominates PAC-Bayes" is deemed imprecise, as it implies incompatibility between the two approaches.
- The manuscript lacks a comparative analysis of P2L+PAC-Bayes, which could enhance the understanding of their relationship and performance.

### Suggestions for Improvement
We recommend that the authors improve the experimental evaluation by comparing P2L against more baselines on diverse datasets, including CIFAR and other PAC-Bayesian bounds. Additionally, the authors should investigate the interpretability of the results, particularly by evaluating P2L on synthetic binary classification datasets to see if the selected set \(T\) corresponds to support vectors. 

We also suggest that the authors clarify the role of \(h_0\) and the implications of using gradient descent versus stochastic gradient descent in their methodology. Furthermore, the paper would benefit from a more extensive discussion of existing literature on training set compression schemes and how they relate to P2L. 

Lastly, we encourage the authors to address the algorithm's efficiency, particularly regarding the potential quadratic complexity when retraining from scratch at each iteration, and consider exploring the extension of adding multiple points to \(T\) simultaneously to mitigate initialization reliance. We also recommend improving the clarity of their claims regarding the relationship between P2L and PAC-Bayes, ensuring that it accurately reflects the comparative performance of P2L+GD. Incorporating the proposed simulations of P2L+PAC-Bayes into the supplementary material could provide a clearer assessment of their compatibility. Finally, we encourage the authors to expand the discussion on the proof of Theorem A.4 to enhance its comprehensibility.