ID: 2YtdxqvdjX
Title: Implicit Convolutional Kernels for Steerable CNNs
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to achieving G-equivariant neural networks by implicitly parameterizing steerable filters using G-equivariant MLPs, rather than relying on analytical derivation of steerable basis functions. The authors provide theoretical justification for their method and demonstrate its effectiveness through empirical studies on various datasets, including point cloud classification, N-body simulations, and molecular data (QM9). The proposed framework allows for flexibility by conditioning the kernel on additional problem-related parameters.

### Strengths and Weaknesses
Strengths:
- The method offers a novel and flexible approach to parameterizing G-steerable kernels, which is conceptually simpler than previous analytical methods.
- The empirical validation across diverse datasets shows the method's effectiveness and relevance in real-world applications.
- The paper is well-written and clearly presents the proposed framework.

Weaknesses:
- The reliance on analytically solving Eq. 2 for specific groups remains a limitation, as implicit kernels do not alleviate this requirement.
- The empirical evaluations could benefit from more extensive analysis, particularly regarding the extent of learned equivariance and the impact of model depth on performance.
- The paper assumes a high level of familiarity with advanced concepts, which may hinder accessibility for a broader audience.

### Suggestions for Improvement
We recommend that the authors improve the empirical analysis to assess the extent of learned equivariance, potentially exploring the relationship between MLP size and equivariance learning. Additionally, enhancing clarity and detail in certain sections would aid comprehension, particularly for readers less familiar with concepts like natural action and Clebsch-Gordan coefficients. We suggest making the paper more accessible by defining specialized terms and providing context for equations and concepts that may not be widely understood. Furthermore, addressing the limitations and rationale for choosing specific datasets in the evaluation would strengthen the paper's impact. Lastly, we encourage the authors to consider alternative implicit functions beyond multi-layer perceptrons, as this could provide valuable insights into the flexibility of implicit kernels.