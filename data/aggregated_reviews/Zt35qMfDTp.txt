ID: Zt35qMfDTp
Title: AstroCLIP: Cross-Modal Pre-Training for Astronomical Foundation Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 2
Original Ratings: 7, -1
Original Confidences: 3, 3

Aggregated Review:
### Key Points
This paper presents a CLIP-inspired foundational model tailored for astronomy, trained on multi-band images and DESI spectrograms using InfoNCE loss. The authors demonstrate that models pretrained on large astrophysics databases can outperform conventional ML approaches for astrophysics ML tasks. The study observes a synergistic effect with the inclusion of more modalities, likely offering valuable insights to the research community.

### Strengths and Weaknesses
Strengths:  
- The paper effectively demonstrates the superiority of pretrained models over traditional ML methods in astrophysics.  
- The introduction of a CLIP-inspired model and the use of InfoNCE loss are innovative contributions.  
- The observation of synergistic effects with multiple modalities enhances the study's relevance.

Weaknesses:  
- The paper may lack detailed comparisons with specific conventional ML approaches, which could strengthen the argument.  
- Further clarification on the implications of the synergistic effects could enhance understanding.

### Suggestions for Improvement
We recommend that the authors improve the paper by providing more detailed comparisons with specific conventional ML approaches to strengthen their argument. Additionally, clarifying the implications of the observed synergistic effects would enhance the overall understanding of the study's contributions.