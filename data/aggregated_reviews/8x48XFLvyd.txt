ID: 8x48XFLvyd
Title: Globally Convergent Variational Inference
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 6, 5, 7, -1, -1, -1
Original Confidences: 1, 2, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of an alternative objective for variational inference, specifically the expected forward KL divergence. The authors demonstrate that under certain technical assumptions, convexity is achieved, facilitating global optimization. They introduce a tractable surrogate objective, showing that the approximation error can be minimized. Experimental results indicate potential global convergence even when assumptions are not met. The work also addresses non-convexity in posterior approximations and establishes that the FKL objective is strictly convex when the variational family belongs to the exponential family, particularly when parameterized by a neural network.

### Strengths and Weaknesses
Strengths:  
- The paper tackles an intriguing problem with a clever methodology, showcasing careful technical development.  
- The use of expected forward KL with the exponential family is both simple and interesting, particularly in addressing ELBO's non-convexity.  
- The connection between NTK and gradient dynamics is novel and adds depth to the analysis.  
- The writing is clear and accessible despite the technical nature of the content.  

Weaknesses:  
- The paper is dense and technical, lacking sufficient intuitive explanations.  
- The findings in Lemma 1 regarding convexity may be underestimated; demonstrating efficacy on a simpler problem could strengthen the argument.  
- The claim that the FKL objective with NTK finds global optima is misleading, as it only suggests proximity to the global solution under certain conditions.  
- The novelty of the results is questioned, particularly in relation to existing analyses in supervised learning settings.  
- Experimental comparisons to ELBO/IWBO optimization are lacking, making it difficult to assess performance gaps.

### Suggestions for Improvement
We recommend that the authors improve the paper by providing more intuitive explanations of their approach and its implications. Clarifying the restrictiveness of the assumptions would enhance understanding. Demonstrating the efficacy of their findings on a toy problem without neural networks could further validate their claims. Additionally, we suggest including direct comparisons of the FKL objective with ELBO/IWBO optimization in the experiments, along with plots reporting symmetric-KL over training for both objectives to illustrate performance differences.