ID: SLwy8UVS8Y
Title: PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 6, 5, 7, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a two-stage latent text diffusion model that utilizes an autoencoder to condense lengthy texts into a limited number of paragraph embeddings, followed by a continuous time diffusion model that learns the distribution of these embeddings. The authors propose PLANNER, which combines latent semantic diffusion with autoregressive generation to enhance text fluency and control over paragraph generation. The model is evaluated across various tasks, demonstrating improved generation quality and diversity compared to existing baselines.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated and addresses the issue of repetitive outputs in autoregressive models.
- The proposed method is novel, employing a diffusion process on latent embeddings, and shows effective results in generating high-quality text.
- Extensive experiments are conducted, providing valuable insights into the model's performance across different tasks.

Weaknesses:
- There is a lack of detailed experimental analysis on the learned paragraph embeddings, particularly regarding their quality and the impact of sentence structure.
- The choice of using "first k hidden state vectors" from the VAE encoder appears arbitrary, with insufficient justification provided.
- Evaluation metrics and baseline model quality raise concerns, particularly in relation to the CNN-DM summarization task.
- The training objective lacks clarity, as the VAE and diffusion components are trained separately, complicating the interpretation of the training process.
- The evaluation is based on a subsample of the test set, which may hinder comparability with other works.

### Suggestions for Improvement
We recommend that the authors improve the experimental analysis on the learned paragraph embeddings to demonstrate their quality and effectiveness in avoiding repetition. Additionally, we suggest providing a clearer justification for the choice of "first k hidden state vectors" and exploring the potential benefits of combining PLANNER with stochastic decoding methods. It would also be beneficial to enhance the evaluation of baseline models and consider using the full test set for a more robust comparison. Finally, we encourage the authors to clarify the training objective and provide ablation studies to justify design choices and their impacts on results.