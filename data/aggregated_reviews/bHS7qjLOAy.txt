ID: bHS7qjLOAy
Title: Riemannian Laplace approximations for Bayesian neural networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 6, 6, 5, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Riemannian Laplace approximation that incorporates the Riemannian geometry of the loss surface to enhance posterior estimation in Bayesian neural networks. The authors demonstrate that this loss-aware approximation captures the true posterior more effectively, provide a framework for the Laplace approximation using Hessians in both normal and tangential spaces, and propose a practical algorithm for integrating the necessary ordinary differential equations (ODEs). Experimental results on standard datasets support the method's efficacy compared to traditional Laplace approximations.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and structured, making the core ideas accessible, even to those with limited knowledge of Riemannian geometry.
- The methodological contribution is original, with the potential to inspire further research in Bayesian deep learning.
- The experimental section includes both intuitive toy examples and quantitative evaluations against standard benchmarks.

Weaknesses:
- The empirical evaluation is limited to small-scale datasets, which may not reflect the method's performance in more complex scenarios.
- There is insufficient analysis of the computational complexity, particularly regarding scalability to larger models and datasets.
- The explanation of the method's advantages over traditional approaches lacks clarity, particularly in the context of mini-batch processing.

### Suggestions for Improvement
We recommend that the authors improve the background section on Riemannian geometry to aid readers less familiar with the topic. Additionally, the authors should consider including a comparison with non-neural network models to demonstrate the method's versatility. A more detailed analysis of computational complexity, including runtime benchmarks for various model sizes and dataset scales, would strengthen the paper. Finally, incorporating comparisons with baseline methods such as deep ensembles and MC-dropout would provide a clearer context for the proposed approach's performance.