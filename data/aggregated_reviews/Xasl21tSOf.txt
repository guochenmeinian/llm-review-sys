ID: Xasl21tSOf
Title: Provable Training for Graph Contrastive Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 8, 7, 7, 7, -1, -1
Original Confidences: 4, 5, 4, 5, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the properties of different nodes in graph contrastive learning (GCL) with various graph augmentations. The authors discover the imbalanced training issue of GCL methods and introduce the concept of “node compactness” to measure adherence to GCL principles. They propose a node compactness regularization and demonstrate its effectiveness across eight benchmarks.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated, supported by both theoretical and empirical analysis.
- It provides interesting insights into node properties in GCL.
- The effectiveness of the proposed model is validated through thorough experiments.

Weaknesses:
- The motivation to distinguish nodes is not adequately addressed in the paper.
- The training process is unclear, particularly regarding the computation of the POT and the training of the InfoNCE loss.
- The discussion section is concise and does not satisfactorily address the main points.
- Some experimental results and techniques lack clear explanations, and certain notations are not defined.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the training process, specifically detailing whether the POT should be computed first or if the model should be trained iteratively with the InfoNCE loss. Additionally, we suggest expanding the discussion section to provide a more comprehensive analysis of the limitations and challenges of the proposed method. It would also be beneficial to include experiments that analyze nodes' sensitivity to different graph augmentations and clarify the reasoning behind the results in figures, particularly regarding node compactness and its relationship with node properties.