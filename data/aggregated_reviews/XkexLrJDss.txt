ID: XkexLrJDss
Title: Exploring the Potential of Large Language Models in Generating Code-Tracing Questions for Introductory Programming Courses
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the potential of using large language models (LLMs), specifically GPT-4, to automatically generate code-tracing questions for introductory programming courses. It addresses the challenges of time-consuming manual question creation and limited scalability, exploring whether LLMs can provide a viable solution. The authors prompt GPT-4 to generate questions based on code snippets and descriptions, comparing them to human-authored questions using various evaluation metrics. The findings indicate that GPT-4 can produce questions of comparable quality to human-authored ones, with over 50% being indistinguishable from human questions. The contributions include a high-quality dataset of human and LLM-generated questions, a rigorous evaluation methodology, and insights into LLMs' potential for educational content generation.

### Strengths and Weaknesses
Strengths:
- Novel application of LLMs in code-tracing question generation, distinct from prior NLP work.
- High-quality dataset tailored to the code-tracing scenario, valuable for future research.
- Rigorous evaluation methodology, including human ratings and textual similarity metrics, setting a standard for assessing LLM-generated content.
- Insightful analysis of LLMs' promise and limitations in generating code-tracing questions.

Weaknesses:
- The study focuses solely on GPT-4, limiting exploration of other LLMs and comparative analysis.
- The dataset comprises only 176 question pairs from limited sources, raising concerns about statistical robustness and generalizability.
- The evaluation metrics are not comprehensive enough and are somewhat subjective, lacking an automatic evaluation component.

### Suggestions for Improvement
We recommend that the authors expand the dataset to include a larger and more diverse set of question pairs to enhance the robustness and generalizability of their findings. Additionally, we suggest incorporating other LLMs into the study to allow for comparative analysis and a broader exploration of potential alternatives. Furthermore, we encourage the authors to develop more comprehensive and objective evaluation metrics, possibly including automatic evaluation methods, to strengthen the assessment of generated questions. Lastly, we recommend considering personalized question recommendations based on different submissions to enhance the practical applicability of the approach.