ID: wWguwYhpAY
Title: Neural Experts: Mixture of Experts for Implicit Neural Representations
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a mixture of experts (MoE) approach for implicit neural representations (INRs), enabling the learning of local piece-wise continuous functions through domain subdivision and local fitting. The authors propose a novel manager architecture and initialization method that enhance speed, accuracy, and memory efficiency without requiring ground truth. The method is evaluated across various tasks, including image, audio, and 3D shape reconstruction, demonstrating improved performance compared to traditional architectures like SIREN.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- The proposed MoE INR shows good performance relative to baselines.
- The randomized initialization significantly enhances performance, as evidenced by the ablation study.
- The architecture design and pre-training strategy for the manager are innovative and contribute to the field.

Weaknesses:
- The paper lacks a detailed discussion of closely related works, particularly previous MoE INRs and decomposition-based INRs.
- There are insufficient comparisons with other INR architectures, particularly those that utilize locality bias.
- The experimental evaluation is limited, using small datasets and lacking robustness in comparisons with more complex tasks.
- A comprehensive ablation study on hyper-parameters of the MoE INRs is missing, leaving the allocation of parameters among the modules unclear.

### Suggestions for Improvement
We recommend that the authors improve the discussion of related works, specifically addressing previous MoE INRs and decomposition-based methods. Additionally, we encourage the authors to conduct key comparison experiments with these methods to illustrate the necessity of learnable partition regions. A detailed ablation study on hyper-parameters, including the layer configurations of the encoder, manager, and experts, should be included to clarify parameter allocation. Finally, expanding the experimental evaluation to include larger datasets and more complex tasks would strengthen the paper's contributions.