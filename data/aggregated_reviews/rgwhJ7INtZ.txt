ID: rgwhJ7INtZ
Title: Super Consistency of Neural Network Landscapes and Learning Rate Transfer
Conference: NeurIPS
Year: 2024
Number of Reviews: 21
Original Ratings: 8, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the loss landscape's stability during training, introducing the concept of "super consistency," which describes how certain spectral properties of the loss Hessian remain stable under width and depth scaling (muP). The authors empirically demonstrate that the top eigenvalues of the loss Hessian stabilize, supporting their claims with theoretical convergence guarantees. They also highlight that the dynamics of the weights are governed by specific equations, revealing that the NTK framework exhibits finite-size effects, contrasting with the super consistent behavior observed in the Hessian. Additionally, the paper presents an analysis of the learning rate in muP for gradient descent, where the learning rate is set to η₀γ² with γ = ℴ(N^{0.5}). The authors discuss the scaling of learning rates across different layers, noting discrepancies in weight updates between input and hidden layers, and clarify the definition of super consistency while providing insights into the eigenvalue distribution of weight updates and the Hessian matrix.

### Strengths and Weaknesses
Strengths:
- The observation of super consistency in the loss landscape and its correlation with learning rate transfer under muP scaling is a significant contribution.
- The paper is well-structured and presents its findings clearly, making it accessible to readers.
- The authors provide empirical validation across various models and datasets, including ResNets and GPT-2.
- The authors effectively clarify the definition of super consistency, addressing prior concerns.
- The paper provides a thorough analysis of learning rate scaling and its implications for weight updates across layers.
- Confidence in the implementation is supported by empirical results and alignment with existing codebases.

Weaknesses:
- Empirical evaluations are limited, particularly regarding larger-scale environments and the applicability of super consistency across different models and modalities.
- The theoretical analysis primarily focuses on two-layer linear networks, lacking comprehensive verification for more complex architectures.
- The discussion on the eigenvalue distribution of weight updates is deemed slightly beyond the paper's scope.
- There is a recommendation for a muP coordinate check, which remains unaddressed in the current version.

### Suggestions for Improvement
We recommend that the authors improve the empirical evaluations by exploring larger-scale environments and different model architectures to validate the generalizability of super consistency. Additionally, we suggest enhancing the theoretical analysis to extend beyond two-layer linear networks, potentially incorporating nonlinear networks or deep architectures. We also recommend improving the clarity of the eigenvalue distribution discussion, ensuring it is appropriately contextualized within the paper's scope. Furthermore, we urge the authors to conduct a muP coordinate check on their implementation before finalizing the camera-ready version, as this would enhance the robustness of their empirical claims. Finally, clarifying the definition of "super consistency" in the introduction and abstract would strengthen the paper's presentation and rigor.