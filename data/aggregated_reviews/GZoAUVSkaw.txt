ID: GZoAUVSkaw
Title: First-Order Minimax Bilevel Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 5, 6, 6, -1, -1, -1
Original Confidences: 4, 2, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two novel algorithms, FOSL and MemCS, for multi-block minimax bilevel optimization problems, effectively avoiding the high complexity associated with second-order gradient computations. The authors reformulate the lower-level problem as a value-function-based constraint, transforming the minimax bilevel optimization into a surrogate minimax problem. Theoretical convergence analysis is solid, and extensive experiments demonstrate the algorithms' superior performance in applications such as deep AUC maximization and robust meta-learning.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and structured, with strong motivation and clear logic.  
- The reformulation of minimax bilevel optimization problems is well-justified, and the convergence analysis is thorough.  
- Experimental validation shows that FOSL and MemCS outperform baselines on various datasets.

Weaknesses:  
- The paper lacks discussion on the scalability of the proposed algorithms and their suitability for different problem sizes.  
- The gradient calculation procedure is not detailed, which is crucial for optimization in neural networks.  
- There are strong assumptions in the theoretical framework that may not hold in practical scenarios, and memory consumption comparisons with baselines are missing.  
- A typo in the title of algorithm 2 ("Cold-star") needs correction.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the scalability of FOSL and MemCS, specifically addressing the types of problems they are suitable for. Additionally, including a comparison of runtime differences between FOSL, MemCS, and second-order methods would enhance the understanding of their effectiveness. The authors should clarify how the gradient of the optimization problem is calculated, whether through implicit differentiation, unrolling methods, or direct analytical solutions. Lastly, addressing the strong assumptions in the theoretical analysis and providing memory consumption data for baselines would strengthen the paper.