ID: 52r4XJYzjg
Title: Improving Context-Aware Preference Modeling for Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 5, 4, 4, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a two-step approach to preference modeling in language models (LMs), focusing on estimating user intent before evaluating generated text within the context of that intent. The authors introduce the Reasonable Preference Reversal (RPR) datasets, designed to isolate context-specific preferences from general ones. Extensive experiments demonstrate that models can achieve improved performance when intent contexts are provided during evaluation, surpassing traditional methods.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a significant issue in language modeling by enhancing LLMs' ability to process user preferences in context-dependent scenarios.
2. The introduction of the RPR datasets is a valuable contribution, specifically aimed at disentangling context-specific preferences.
3. Comprehensive experimental results validate the proposed method, showing improvements over existing models like GPT-4 and Llama 3.

Weaknesses:
1. The paper lacks testing across a broader range of general benchmarks, such as MMLU and AGI-Eval, which could assess the impact of fine-tuning on the LLM's original capabilities.
2. Applicability to other LMs, particularly those of different sizes or series, remains unclear, which could affect the method's validation.
3. The experiments do not convincingly demonstrate the advantages of two-step preference modeling over traditional reward modeling on general preference datasets, raising concerns about the robustness of the findings.

### Suggestions for Improvement
We recommend that the authors improve the breadth of their benchmarking by including general benchmarks like MMLU and AGI-Eval to evaluate the impact of their method on LLM capabilities. Additionally, clarifying the applicability of their approach to various LMs would enhance the robustness of their claims. To strengthen their argument, we suggest providing empirical evidence supporting the necessity of two-step preference modeling and addressing the ambiguity in intent estimation. Finally, we encourage the authors to elaborate on the dataset curation process, including specific details about prompts and validation methods, to enhance transparency and reproducibility.