ID: FBNyccPfAu
Title: Variational Monte Carlo on a Budget — Fine-tuning pre-trained Neural Wavefunctions
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 5, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a pre-trained deep learning wavefunction ansatz aimed at solving the Schrödinger equation via variational Monte Carlo (VMC) methods. The authors propose a novel architecture that enhances zero-shot accuracy over previous methods, although fine-tuning is still necessary for accurate relative energies. The model is trained on a diverse dataset of 98 molecules, demonstrating improved performance in various metrics compared to traditional quantum chemistry methods. Additionally, the paper includes contributions such as improved architecture, a larger dataset, and enhanced MCMC burn-in, which are valuable despite not showing a significant leap in methodology. However, the achievable accuracy is limited compared to more complex architectures like PsiFormer, which require training from scratch.

### Strengths and Weaknesses
Strengths:
- The paper addresses the significant challenge of reducing computational overhead in VMC methods through a clear and well-articulated deep learning wavefunction ansatz.
- The architecture is improved, eliminating reliance on the Hartree-Fock method and enhancing absolute energy performance.
- The authors provide extensive empirical studies and reproducible code, showcasing the method's effectiveness across multiple systems.
- Valuable contributions in architecture, dataset size, and MCMC burn-in improvements enhance the paper's credibility.

Weaknesses:
- Comparisons with previous methods focus on optimization steps rather than raw computational costs, making it difficult to assess practical applicability.
- The relative energy results may not be convincing without additional fine-tuning, and the method's performance in terms of energy variance is not adequately discussed.
- The presentation lacks clarity regarding the training and evaluation steps, particularly in distinguishing pre-training from fine-tuning.
- There is a lack of significant methodological advancement, and the extensive use of multipliers like 300x or 5x for performance improvements may obscure meaningfulness due to the absence of a standardized comparison target.

### Suggestions for Improvement
We recommend that the authors improve the discussion of computational costs, particularly in comparison to conventional quantum chemistry methods, to enhance the practical relevance of their findings. Additionally, we suggest providing relative energy results with increased fine-tuning steps (e.g., 8k and 16k) to demonstrate convergence. It would also be beneficial to include a detailed analysis of energy variances across different methods and clarify the distinction between equivariance and symmetry-breaking in the context of their architecture. Lastly, we encourage the authors to enhance the clarity of their presentation regarding the training and evaluation processes and improve the clarity of performance metrics by providing standardized comparison targets to enhance the interpretability of the multipliers used in the results.