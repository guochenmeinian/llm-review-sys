ID: GtbwJ6mruI
Title: Skill-aware Mutual Information Optimisation for Zero-shot Generalisation in Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 6, 6, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new contrastive learning objective, Skill-aware Mutual Information (SaMI), defined as \(I_{SaMI}(c;\pi_c;\tau_c) = I(c;\tau_c) - I(c;\tau_c|\pi_c)\), and a K-sample estimator, Skill-aware Noise Contrastive Estimation (SaNCE), aimed at enhancing zero-shot generalization in reinforcement learning (RL) tasks. The authors emphasize the importance of maximizing \(I_{SaMI}\) through a skill-aware trajectory sampling strategy, addressing the \(\log K\) curse and ensuring effective compression of information from limited samples. Theoretical and empirical evidence supports the superiority of the proposed methods, which are validated on modified MuJoCo and Panda-gym benchmarks. The authors clarify that the SaMI formula has not changed, with the only modification being a correction of a typo in the log probability expression.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel and relevant problem formulation that addresses the need for distinguishing context embeddings according to skills in Meta-Reinforcement Learning (Meta-RL).
- The authors provide a clear definition of SaMI and its optimization goal, enhancing the understanding of the framework.
- The proposed methods, SaMI and SaNCE, are empirically validated, demonstrating significant improvements in zero-shot generalization and robustness to sample size reductions.
- The paper effectively addresses the \(\log K\) curse, proposing a solution that is relevant for sample-limited tasks in RL.
- The motivation for the study is well-articulated, linking theoretical insights to practical applications in zero-shot generalization.
- The paper is well-structured, with clear explanations and effective use of figures to support the presentation of results.

Weaknesses:
- The scalability of the proposed methods to more complex environments is not thoroughly discussed, raising concerns about their practical utility.
- The success of the methods is heavily dependent on accurate skill definitions, which may vary across environments and tasks, potentially impacting robustness and generalizability.
- There is confusion regarding the definitions of total correlation and interaction information, which may hinder clarity for readers.
- The reliance on approximating \(p(c,\pi_c,\tau_c)\) through Monte Carlo sampling without extensive discussion may leave gaps in understanding the underlying distribution.
- The paper lacks a comprehensive discussion of the full scope of Meta-RL, particularly regarding algorithms that do not utilize explicit context representations.
- The presentation of results, particularly in terms of statistical significance and clarity in figures, requires improvement.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the scalability of the proposed methods to complex environments and address potential challenges related to skill definitions. Additionally, we suggest that the authors provide a more comprehensive overview of Meta-RL to accurately contextualize their contributions. To enhance clarity, we recommend explicitly distinguishing between total correlation and interaction information in the manuscript. Furthermore, providing a more detailed discussion on the approximation of \(p(c,\pi_c,\tau_c)\) and its implications would enhance the reader's understanding of the framework's foundations. The authors should also enhance the clarity and statistical rigor of their results, particularly regarding the interpretation of performance improvements and the use of visualizations such as t-SNE. Finally, including an algorithm box could aid in understanding the learning procedure.