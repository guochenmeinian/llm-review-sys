ID: 3G1ZDXOI4f
Title: LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 5, 8, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LONGVIDEOBENCH, a benchmark aimed at evaluating Large Multimodal Models (LMMs) on their understanding of long video and subtitle inputs, featuring 3,763 videos and 6,678 questions across 17 categories. The evaluations reveal that even advanced proprietary models struggle with long-context understanding, while open-source models exhibit a more pronounced performance gap. The authors propose a new referring reasoning task to enhance evaluation and note that model performance improves with increased frame processing, underscoring the benchmark's significance for future LMM assessments.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel benchmark specifically for long video understanding, addressing a notable gap in existing benchmarks.
- It encompasses a wide range of video lengths and themes, with diverse question types.
- The dataset is meticulously curated, ensuring high-quality annotations and extensive evaluations of various LMMs.

Weaknesses:
- The benchmark lacks long temporal dynamic context, primarily focusing on static objects or scene changes, which does not fully capture the essence of long video understanding.
- The novelty of the work appears limited, as it does not significantly differentiate itself from existing long video benchmarks.
- There is a lack of qualitative examples and human performance metrics to better understand model limitations and potential biases.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by incorporating more temporal dynamic context in the questions to better reflect long video understanding. Additionally, evaluating the Certificate Length concept could enhance the assessment of long-term understanding. Providing qualitative examples and failure cases would help elucidate the performance bottlenecks of existing LMMs. Finally, we suggest that the authors clarify the unique contributions of LONGVIDEOBENCH compared to existing benchmarks to strengthen its significance in the field.