ID: HKvyBizboA
Title: InfoRank: Unbiased Learning-to-Rank via Conditional Mutual Information Minimization
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel unbiased learning-to-rank framework called InfoRank, which addresses position and popularity biases by consolidating them into a single observation factor. The approach minimizes mutual information between observation and relevance estimations, utilizing two separate MLP modules to estimate these factors. The framework incorporates an attention mechanism and a regularization term based on conditional mutual information. Experimental evaluations on three datasets demonstrate that InfoRank yields more precise and unbiased ranking strategies compared to existing methods.

### Strengths and Weaknesses
Strengths:
1. The paper tackles an important problem in ranking systems and provides a clear explanation of the proposed method.
2. The experimental results are statistically significant and demonstrate the effectiveness of InfoRank across diverse datasets.
3. The method's potential to enhance other ranking models indicates broader applicability.

Weaknesses:
1. The novelty of InfoRank is questioned, as the decomposition of relevance and observation has been extensively studied, and the conditional independence assumption has been debated in recent literature.
2. The paper lacks comparisons with state-of-the-art methods from the last four years, which limits the evaluation of its effectiveness.
3. The complexity of the method may hinder practical adoption, and there is a risk of overfitting due to the reliance on an attention mechanism.

### Suggestions for Improvement
We recommend that the authors improve the novelty discussion by clearly differentiating InfoRank from existing works, particularly addressing the concerns regarding the conditional independence assumption. Additionally, we suggest including more recent state-of-the-art baselines in the experiments to strengthen the evaluation. To enhance practical applicability, consider simplifying the model or providing insights on how to mitigate potential overfitting issues.