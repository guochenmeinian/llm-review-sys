ID: 8ohsbxw7q8
Title: Graph Diffusion Policy Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Graph Diffusion Policy Optimization (GDPO), a policy gradient method designed to optimize graph diffusion probabilistic models with non-differentiable reward signals. The authors establish a connection between the T-step denoising process and a T-step Markov Decision Process (MDP), addressing convergence issues observed with the REINFORCE algorithm in graph diffusion models. GDPO demonstrates significant performance improvements over baseline methods, including adaptations from image diffusion models.

### Strengths and Weaknesses
Strengths:
- GDPO is a well-motivated and innovative approach to optimizing graph diffusion models.
- The paper is clearly written and easy to follow, with effective visualizations and thorough empirical studies demonstrating the method's effectiveness.
- GDPO shows substantial performance gains across various graph generation tasks, achieving notable improvements in molecular graph generation.

Weaknesses:
- The paper lacks a rigorous theoretical analysis of the eager policy gradient, particularly regarding the bias-variance trade-off.
- There is insufficient exploration of modern reinforcement learning techniques, such as actor-critic or PPO/TRPO, which could enhance the proposed method.
- The experimental settings are not fully detailed, and the paper does not adequately discuss the implications of using classifier-based or classifier-free guidance in comparison to GDPO.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of the eager policy gradient, particularly focusing on the bias-variance trade-off. Additionally, we suggest incorporating a comparison with other reinforcement learning methods, such as MolGAN, to contextualize GDPO's performance. Clarifying the experimental settings and discussing the implications of different guidance approaches would also strengthen the paper. Finally, conducting an ablation study on highly ambiguous graphs, as proposed, could provide valuable insights into GDPO's limitations and robustness.