ID: JL2eMCfDW8
Title: Federated Learning over Connected Modes
Conference: NeurIPS
Year: 2024
Number of Reviews: 22
Original Ratings: 5, 6, 6, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for federated learning (FL) called FLOCO, which utilizes linear mode connectivity to establish a solution simplex in neural network weight space. The authors propose that clients are represented as points in this simplex, allowing for personalized model training while facilitating efficient updates to both global and local models. The method incorporates Riesz s-Energy regularization and Euclidean projection to maintain client similarity and improve performance. Additionally, the authors introduce FLOCO++, which builds upon FLOCO and Baseline methods to enhance local test accuracy in federated learning settings. They report average local test accuracy for various configurations, including Baseline 1, Baseline 2, FLOCO+, and FLOCO++, demonstrating improvements in accuracy with the proposed methods. The authors also plan to conduct further experiments with a larger simplex configuration (M=6) to capture client characteristics more effectively.

### Strengths and Weaknesses
Strengths:  
- The paper is well-structured and introduces a novel application of linear mode connectivity in personalized FL, which is interesting and relevant.  
- The authors provide detailed experimental results that illustrate the performance of their methods compared to baselines.  
- The method is evaluated across various experimental setups, providing accessible illustrations and helpful hyperparameter details for reproducibility.  
- The introduction of FLOCO++ shows a thoughtful progression in their research, indicating responsiveness to prior feedback.  

Weaknesses:  
- Some algorithmic details require clearer explanations, particularly regarding the implementation of simplex learning and the generation of subregions.  
- The experimental baselines are outdated, lacking comparisons with more recent methods in clustered federated learning.  
- Claims regarding variance reduction and convergence speed need stronger empirical support, particularly in relation to global accuracy.  
- The scoring misunderstanding regarding the NeurIPS review system raises concerns about the clarity of the review process.  
- There is a lack of clarity regarding the baseline comparison with standard FedAvg and personalization, which could hinder the understanding of the proposed method's effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the algorithm description, particularly in the "Simplex Learning" section, to elucidate how loss and gradients are computed. Additionally, consider including more recent baselines and ablation studies, such as FedAvg with simplex learning and local fine-tuning, to validate the benefits of the proposed method. It would also be beneficial to address the limitations of the method more explicitly, especially concerning its applicability in generic FL settings with many clients. We suggest refining the scope of the paper to focus on cross-silo FL, as this aligns better with the method's requirements for stateful clients. Furthermore, we recommend that the authors improve clarity in their submission by explicitly addressing the baseline comparisons, particularly with standard FedAvg and personalization. We also suggest providing a more detailed explanation of the scoring system to avoid misunderstandings. Finally, we encourage the authors to include the results from the upcoming experiments with M=6 in the final paper to strengthen their findings.