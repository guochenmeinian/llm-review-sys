ID: nMjktU5AiP
Title: IndiSocialFT: Multilingual Word Representation for Indian languages in code-mixed environment
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents IndiSocialFT, a word embedding model developed using FastText for 20 Indian languages, incorporating diverse textual characteristics from both structured and unstructured sources. The dataset includes 0.6 billion location-filtered tweets, Facebook posts from notable Indian personalities, news media articles, and comments on popular video channels. The authors demonstrate that IndiSocialFT outperforms baseline models, including IndicFT and other FastText and TF-IDF based models, through intrinsic and extrinsic evaluations.

### Strengths and Weaknesses
Strengths:  
- IndiSocialFT covers more languages than similar previous studies, enhancing its applicability for various downstream tasks.  
- The evaluation methodology is robust and well-structured, with promising results showing superior performance over other pre-trained embeddings for Indian languages.  

Weaknesses:  
- There is a lack of comprehensive evaluation across all languages covered, and some datasets, like the word similarity task, are relatively small.  
- Performance differences between IndiSocialFT and other models are marginal, requiring significant statistical tests for validation.  
- The paper lacks detailed language-wise statistics, clarity on the amount of code-mixed data used, and comparisons with other significant models like IndicBERT and IndicBART.  
- There is insufficient explanation regarding why the model performs well, leading to concerns about the novelty and philosophical contribution of the work.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the dataset composition, including the number of posts and tokens from each source, and provide language-wise statistics. Additionally, including comparisons with other language models trained on Indian language corpora, such as IndicBERT and IndicBART, would strengthen the paper. We also suggest that the authors consider using macro-F1 score instead of accuracy for evaluation, as it is crucial in multi-class classification setups. Finally, significant statistical tests should be conducted to validate any claims of performance improvement.