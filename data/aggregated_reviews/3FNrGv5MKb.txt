ID: 3FNrGv5MKb
Title: $k$NN-LM Does Not Improve Open-ended Text Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the impact of interpolation-based retrieval-augmented language models (LMs) on open-ended text generation, specifically analyzing two kNN-LM models across different datasets. The authors conclude that these interpolated LMs do not enhance the quality of open-ended text generation, despite showing improvements in perplexity and classification-related tasks in prior studies. The work provides valuable insights into the limitations of retrieval-augmented LMs for this application.

### Strengths and Weaknesses
Strengths:  
- The paper presents thorough evaluations of interpolation baselines and offers insights into how models like kNN-LM improve perplexity primarily by enhancing prediction accuracy on a limited token subset, while their quality deteriorates over longer sequences.  
- The authors conduct comprehensive experiments and analyses, including various automatic metrics and human evaluations, which contribute to a deeper understanding of the phenomenon.  
- The findings are novel and beneficial for the field of natural language generation.

Weaknesses:  
- The scope is limited to interpolation-based methods like kNN-LM, which may not generalize to other retrieval-augmented LMs.  
- Human evaluation is constrained by a small sample size, with only three raters assessing 45 examples, which could weaken the conclusions.  
- The paper does not explore more advanced models based on kNN-LM, missing opportunities to discuss recent improvements in this area.

### Suggestions for Improvement
We recommend that the authors improve the human evaluation by increasing the number of raters and examples to strengthen their conclusions. Additionally, we suggest that the authors analyze more advanced models based on kNN-LM to provide a broader context for their findings. Finally, we encourage the authors to address the questions raised regarding the future improvements of retrieval-based LMs as indicated in the reviews.