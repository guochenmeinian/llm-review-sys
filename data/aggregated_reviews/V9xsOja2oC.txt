ID: V9xsOja2oC
Title: Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel study on how speech models learn syntactic dependencies and manage homophony, specifically in the French language. The authors propose the use of context mixing methods adapted from the text domain to analyze the influence of syntactic cues in disambiguating spoken words. The findings reveal that encoder-only models utilize spoken cue-word representations, while encoder-decoder models rely on text cue-word representations in the decoder prefix. The analysis confirms the significant role of syntactic dependencies in both model types.

### Strengths and Weaknesses
Strengths:  
- The paper provides a compelling investigation into the phenomenon of homophony in French, demonstrating the applicability of context-mixing methods from the text domain to speech models.  
- It offers clear technical insights and visualizations of layer-wise context-mixing scores, enhancing interpretability.  
- The study is well-written and presents sound contributions to the field of explainable AI.

Weaknesses:  
- The focus on a single linguistic phenomenon (homophony) and language (French) raises questions about the generalizability of the findings to other languages or phenomena, such as polysemy.  
- The analysis is limited to the encoder block of the Transformer architecture, neglecting the decoder block and other components.  
- There are minor errors and omissions in the presentation, such as unclear legends in figures and missing descriptions of certain variables.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by exploring additional languages and linguistic phenomena beyond homophony. Additionally, the authors should consider extending their analysis to include the decoder block and other components of the Transformer architecture. To enhance clarity, please address the minor errors noted, such as the example on lines 058-069 and the omission of d_h in Equation (4). Furthermore, we suggest providing more detailed explanations for the context-mixing score maps and ensuring that all model variants are included in the analysis for a more comprehensive comparison. Lastly, including English translations in Table 1 would aid understanding for non-French speakers.