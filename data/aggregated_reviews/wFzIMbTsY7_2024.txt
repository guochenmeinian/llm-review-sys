ID: wFzIMbTsY7
Title: Decision Mamba: Reinforcement Learning via Hybrid Selective Sequence Modeling
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 10, 4, 7, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Hybrid Mamba (HM), a method that combines the Mamba model and Transformer to enhance reinforcement learning (RL) performance. The authors find that in-context RL methods with Mamba as the backbone are generally more efficient than Transformer, though effectiveness improvements are not significant. HM leverages Mamba to generate high-value sub-goals, which condition the Transformer, leading to substantial gains in online testing efficiency and task-solving capabilities. Experiments on benchmarks such as D4RL and Grid World demonstrate HM's improved performance. Additionally, the authors explore long-term memory capabilities in RL through in-context learning, asserting that their experiments validate these capabilities. However, concerns remain regarding the sufficiency of the empirical results, which are limited to simple RL tasks, primarily focusing on gridworld tasks.

### Strengths and Weaknesses
Strengths:  
1. The paper is well-written and coherent, effectively explaining complex ideas.  
2. HM significantly accelerates testing speed, achieving up to 28 times faster results than baseline methods.  
3. The empirical analysis includes diverse tasks and ablation studies, convincingly demonstrating the algorithm's strengths.  
4. The authors provided a detailed response and additional results that addressed some concerns, with promising results on gridworld tasks.

Weaknesses:  
1. The lack of explanation for the baselines AD (Mamba) and DM in the experimental setup may confuse readers.  
2. Some experimental settings, such as the GPU device used, are not clearly explained in the main text.  
3. The claim of presenting an in-context RL approach contradicts the use of a global update method, which is closer to gradient-based RL.  
4. The experiments on long-term dependencies are unconvincing; achieving good results in tasks with arbitrary horizons does not necessarily prove effective long-term memory embedding.  
5. The conclusion regarding long-term memory capabilities is questioned, as the experiments may not adequately validate this claim.  
6. The empirical results are limited to simple tasks, lacking the complexity necessary for a robust evaluation of in-context learning.  
7. There is a significant absence of in-context evaluation, with no demonstration of learning new tasks in the context of the model.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental setup by providing detailed explanations of the baselines AD (Mamba) and DM. Additionally, it is crucial to clarify the GPU device used in the main text. We suggest that the authors further explain the unique contributions of combining Mamba and the Transformer, as the performance improvements seem expected due to the advantages of conditional-based RL algorithms. To strengthen the evidence for HM's long-term memory capabilities, we recommend testing its stability and performance with varying horizon lengths. Furthermore, we encourage the authors to enhance the evaluation by incorporating more complex tasks, such as Procgen, and assessing performance on unseen seeds and tasks, as this addition is crucial to demonstrate the effectiveness of HM in in-context learning and to advance the field beyond gridworld environments. Finally, addressing the inconsistencies in tense usage and clarifying the construction of valuable sub-goals would enhance the paper's overall presentation.