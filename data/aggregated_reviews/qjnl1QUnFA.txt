ID: qjnl1QUnFA
Title: High-Fidelity Audio Compression with Improved RVQGAN
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents RVQGAN, a neural audio codec that employs a convolutional encoder/decoder and Residual Vector Quantization as a bottleneck, achieving state-of-the-art performance at bitrates from 3 to 8 kbps compared to the EnCodec model. Key innovations include the retrieval of the nearest codebook entry using cosine similarity, dropping the exponential moving average rule for codebook learning, and selecting all quantizers 50% of the time to enhance bandwidth performance. The authors conduct extensive ablation studies and provide a subjective comparison with EnCodec across various bitrates.

### Strengths and Weaknesses
Strengths:
- The execution and illustration of the tackled issues and proposed solutions are commendable.
- The final model's quality surpasses existing state-of-the-art methods.
- The detailed ablation study with objective metrics adds robustness to the findings.
- The model's capability to handle fullband audio across multiple domains is a significant advantage.

Weaknesses:
- The method represents an incremental improvement over prior work, with many components derived from existing studies, raising questions about its novelty.
- Some details regarding the codebook updates and low-dimensional projections are unclear.
- The lack of subjective evaluations in ablation studies limits insights into subjective performance gains.
- The comparison of a 24 kHz baseline model with a 44.1 kHz model could skew metrics, necessitating careful consideration of the impact on results.

### Suggestions for Improvement
We recommend that the authors improve clarity on the algorithm used for VQ, specifically regarding the computation and update frequency of PCA and codebooks. Additionally, the architecture for the multi-scale discriminator should be explicitly detailed. We suggest including a breakdown of results by audio category to enhance understanding. Finally, providing insights into the distribution of the learned snake activation parameters would further clarify model utilization.