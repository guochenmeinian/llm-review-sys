ID: iSd8g75QvP
Title: A Trichotomy for Transductive Online Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 27
Original Ratings: 7, 5, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on realizable transductive online learning with a fixed and known sequence of instances, proposing a trichotomy of error rates based on the VC and Littlestone dimensions of the hypothesis space. The authors establish that if the VC dimension is infinite, the number of mistakes is linear ($n$); if the VC dimension is finite and the Littlestone dimension is infinite, mistakes are logarithmic ($\Theta(\log n)$); and if both dimensions are finite, mistakes are constant ($\Theta(1)$). They improve the lower bound on mistakes from $\Omega(\sqrt{D_L})$ to $\Omega(D_L)$ when the Littlestone dimension is bounded and extend their results to the multi-class case. Additionally, the paper provides a comprehensive analysis of the agnostic case in transductive learning, identifying a trichotomy that contributes significantly to the field. The qualitative characterization of learnability is complete, establishing that a binary hypothesis class $\mathcal{H}$ is learnable if and only if $\text{VC}(\mathcal{H}) < \infty$. The authors explore the implications of unbounded label sets $|\mathcal{Y}|$, revealing that the DS dimension does not characterize learnability in this context, which poses an intriguing open problem. They also address previous concerns regarding the necessity of threshold dimension and the proof of the lower bound of $\log{n}$.

### Strengths and Weaknesses
Strengths:  
- The paper provides a clear overview of the transductive learning model, which is well-motivated and potentially more realistic than standard online settings.  
- The identification of the trichotomy is a significant contribution, and the qualitative characterization of learnability is thorough and insightful.  
- The proofs, particularly for Theorem 3.1, are well-explained and clear, enhancing the paper's overall strength.  
- The extension of results to the agnostic case strengthens the paper, and the authors provide a compelling example demonstrating the limitations of the DS dimension.  

Weaknesses:  
- Many results are either known or implicitly exist in prior work, with the main novelty being the improvement of the lower bound.  
- The proofs, aside from Theorem 3.1, are relatively straightforward, leading to limited technical originality.  
- The informal version of Theorem 4.1 is somewhat misleading, particularly regarding the mistake rates when the VC dimension equals $n$.  
- Some skepticism exists regarding the introduction of new results post-submission, and the clarity of certain technical aspects, such as the relationship between threshold dimension and Ldim, could be improved.  
- There are minor presentation issues, including typos and unclear definitions that could benefit from visual aids.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the informal version of Theorem 4.1 by specifying the dependence on VC and Littlestone dimensions. Additionally, we suggest writing up the bounds for a finite instance space as corollaries to enhance reader intuition. It may also be beneficial to discuss the implications of the term "transductive" in the context of known sequences versus known sets, as this could clarify potential ambiguities. Furthermore, we encourage the authors to consider a more comprehensive treatment of the proofs, particularly for Theorem 3.1, and to address the agnostic setting more thoroughly, as it could provide valuable insights into the learnability of the proposed models. We also recommend that the authors improve the clarity of the relationship between threshold dimension and Ldim in the context of their proofs and include the example regarding the DS dimension in the final version, as it significantly enhances the paper's narrative. Lastly, we encourage the authors to explicitly state the implications of extending the agnostic arguments to the case of finite $|\mathcal{Y}|$ in the final version.