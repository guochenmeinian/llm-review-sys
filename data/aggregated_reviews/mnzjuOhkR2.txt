ID: mnzjuOhkR2
Title: Struct-XLM: A Structure Discovery Multilingual Language Model for Enhancing Cross-lingual Transfer through Reinforcement Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a re-training reinforcement learning (RL) procedure for multilingual large language models (LLMs) that enhances their understanding of syntactic structures. The proposed method focuses on determining constituency-like structures to address a cross-lingual translation ranking task, offering a data-efficient re-training approach that requires only an 8MB corpus, contrasting sharply with the 40GB+ requirements of existing methods. The authors claim significant novelty by combining RL, a translation ranking task, and a Siamese network to improve cross-lingual capabilities, supported by extensive experiments.

### Strengths and Weaknesses
Strengths:
- The proposed re-training approach is notably data-efficient, requiring minimal corpus size.
- The integration of RL to discover structure based on translation ranking loss is innovative and effective.
- The paper is well-written, with a thorough literature review and competitive empirical evaluations.

Weaknesses:
- The main advantage of low data requirements may be less relevant given the data-intensive nature of LLMs; a focus on low-resource scenarios would enhance coherence.
- Some claims, such as the model "learning universal syntactic structures," lack substantiation, particularly when qualitative analyses do not align with expert annotations.
- The reported improvements are marginal, necessitating statistical significance testing to validate results and avoid potential overfitting.
- The limitations section resembles future work suggestions rather than a critical self-assessment.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology by elaborating on the intuition behind each experimental step, particularly in sections 3.2, 3.3, and 3.4. Additionally, the authors should provide statistical significance scores for performance comparisons to reinforce their claims. A more detailed analysis of the discovered structures is essential, potentially through quantitative comparisons with human-extracted structures. To address the limitation of using a 13.7k parallel corpus for warm-start training, we suggest conducting experiments to explore broader applicability to low-resource languages. Lastly, we encourage the authors to update the draft with discussions and additional details shared during the rebuttal phase.