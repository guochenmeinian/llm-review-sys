ID: BI031mw7iS
Title: Flow: Per-instance Personalized Federated Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 5
Original Ratings: 5, 6, 6, 5, -1
Original Confidences: 4, 5, 3, 4, -1

Aggregated Review:
### Key Points
This paper presents Flow, a per-instance and per-client personalized federated learning algorithm that utilizes dynamic routing to create adaptive personalized models tailored to individual client data instances. The authors provide convergence analysis for both global and personalized models and demonstrate through empirical evaluation that Flow outperforms existing state-of-the-art personalized federated learning methods in terms of generalization and personalized accuracy across various vision and language tasks.

### Strengths and Weaknesses
Strengths:  
- The paper introduces a novel approach to per-instance personalized federated learning, addressing a significant challenge in the field.  
- The problem of balancing local and global model predictions is well-articulated and relevant.  
- The experiments are comprehensive, utilizing both computer vision and natural language processing datasets, and the writing is clear and easy to follow.  
- The theoretical analysis supports the proposed method's credibility and effectiveness.

Weaknesses:  
- The routing module's design as a fully connected network lacks clarity; the authors should elaborate on why this choice was made instead of using a sequential model like RNN.  
- The paper does not analyze the additional computational and communication overheads associated with Flow.  
- Some hyperparameters, such as K1 and K2, are not introduced, and there are no convergence curves compared to baseline algorithms.  
- The motivation for per-instance federated learning is not convincingly articulated, and the claims regarding the performance of personalized models compared to global models are unclear.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the routing module's design choices, particularly the decision to use a fully connected network over a sequential model. Additionally, the authors should validate the effectiveness of the alternative training for global parameters and provide more evaluations to support their claims. It would be beneficial to include a detailed analysis of the computational and communication costs associated with Flow. Furthermore, we suggest introducing the values of hyperparameters K1 and K2 and including convergence curves for comparison with baseline algorithms. Finally, the authors should clarify the motivation behind per-instance federated learning and address the potential overhead introduced by their approach.