ID: EdIGMCHk4l
Title: RRHF: Rank Responses to Align Language Models with Human Feedback
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 4, 6, 6, 7, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new learning paradigm called RRHF (Ranking-based Reinforcement Learning from Human Feedback) aimed at aligning large language models (LLMs) with human preferences. The authors propose a ranking-based approach that utilizes sampled responses from multiple sources to align the probabilities of generated responses with human feedback. They demonstrate that RRHF achieves performance comparable to the state-of-the-art Proximal Policy Optimization (PPO) method on the Helpful and Harmless dataset, while being more memory efficient, requiring only one model at a time compared to multiple models for PPO. The authors also clarify the integration of fine-tuning loss in RRHF, enhancing the understanding of its training stability. However, the paper does not adequately address prior work on instruction-tuning, which has developed similar methods for boosting LLM performance.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel learning paradigm for aligning language models with human preferences, providing an alternative to existing methods like PPO.
- RRHF is significantly more memory efficient, requiring only one model during training.
- The experiments on the Helpful and Harmless dataset show that RRHF achieves alignment performance comparable to PPO.
- The authors successfully demonstrate the application of RRHF, showing its potential with less hyperparameter tuning compared to PPO.
- The additional results and evaluations, including human assessments, strengthen the paper's findings and provide a clearer picture of RRHF's performance.

Weaknesses:
- The paper does not adequately address prior work on instruction-tuning, which has developed similar methods for boosting LLM performance.
- The online sampling approach raises concerns, as updated models sometimes generate less meaningful responses, potentially due to a weak reward model or low sample diversity.
- The reward model appears weak, leading to overfitting and a lack of diversity in generated responses, as evidenced by repetitive outputs.
- There is a lack of comprehensive evaluation against best-of-N results, which could provide a more robust comparison.
- Concerns about potential overfitting to rewards remain, as evidenced by outputs that may not align with human preferences, suggesting that reliance on reward model scores could be misleading.
- The title is misleading, as "without tears" is not clearly defined in the context of the paper.

### Suggestions for Improvement
We recommend that the authors improve the discussion of related work by referencing prior developments in instruction-tuning to provide a more comprehensive context. Additionally, we suggest investigating the performance of RRHF with a more robust reward model, such as ChatGPT, to address the observed issues with sample quality. To mitigate overfitting, the authors should consider implementing a method to determine when to halt training. Furthermore, we encourage the authors to explore dividing the online sampling-RRHF into stages to avoid loading multiple models simultaneously. We also recommend improving the evaluation by including comparisons with other published results rather than solely their own implementation, and incorporating best-of-N results to enhance the comprehensiveness of the findings. Lastly, we advise clarifying the title to eliminate confusion regarding its meaning.