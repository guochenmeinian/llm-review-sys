ID: QRAS5wSgEy
Title: CADet: Fully Self-Supervised Out-Of-Distribution Detection With Contrastive Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 5, 5, 5, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 2, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for Out-of-Distribution (OOD) detection called CADet, which utilizes self-supervised contrastive learning in conjunction with the Maximum Mean Discrepancy (MMD) two-sample test. The authors aim to detect whether two sets of samples are drawn from the same distribution and propose MMD-CC as an improvement to MMD for scenarios with limited samples. The paper is divided into two main parts: the first focuses on detecting distribution shifts between datasets, while the second introduces CADet for identifying whether a single sample belongs to a set of inliers. However, critical details about CADet, such as the similarity function, are inadequately explained, hindering a comprehensive evaluation. The authors also claim that the MMD-CC test outputs valid p-values, but the paper currently lacks a formal proof of this validity. The description of the similarity function as a cosine may be misleading since it is not fixed but rather a composition with learned representations. Furthermore, the omission of comparisons to the CSI method, which is claimed to be the closest approach in the literature, is noted as a significant weakness.

### Strengths and Weaknesses
Strengths:
- The combination of Sutherland's MMD with self-supervised contrastive learning yields good empirical performance in detecting distributional shifts.
- The paper is well-written and organized, providing a clear summary of preliminary ideas and experimental settings.
- The authors have clarified several points in their rebuttal, indicating a willingness to improve the paper.
- The proposed MMD-CC test is a novel contribution to the field.

Weaknesses:
- Claims regarding the novelty of the approach are overstated, with significant omissions in the literature review, including key citations related to OOD detection.
- The validity of the computed p-values is questionable, as they do not hold for fixed validation sets and are based on flawed assumptions in the MMD-CC method.
- The paper lacks a formal proof for the validity of the MMD-CC test's p-values.
- The description of the similarity function is potentially confusing and requires rewording.
- The experimental design raises concerns, particularly regarding the use of external datasets for training feature generators, which undermines the paper's claims about OOD detection.
- The authors fail to adequately compare their methods against state-of-the-art approaches, such as the CSI method, and do not provide sufficient justification for their methodological choices.

### Suggestions for Improvement
We recommend that the authors improve the literature review by including relevant citations, particularly those related to OOD detection methods that do not require access to OOD samples. Additionally, the authors should clarify the computation of p-values and ensure that they are valid under the assumptions made. It is crucial to provide a detailed explanation of the CADet method, including the similarity function, to facilitate a thorough evaluation. We suggest rewording the description of the similarity function to accurately reflect that it is a composition of the cosine with learned representations. Furthermore, we urge the authors to include comparisons with state-of-the-art methods, such as the CSI approach, providing justification for its omission and detailing specific implementations and hyperparameter tuning in an appendix. Finally, we recommend including error bars in reported results to reflect variability and enhance the robustness of their findings.