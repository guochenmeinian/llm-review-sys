ID: TUGoUNkccV
Title: Correlative Information Maximization: A Biologically Plausible Approach to Supervised Deep Neural Networks without Weight Symmetry
Conference: NeurIPS
Year: 2023
Number of Reviews: 25
Original Ratings: 4, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a biologically plausible learning approach for supervised learning in deep neural networks, which does not require symmetric weights in the forward and backward directions. The authors propose an information theoretic framework that maximizes mutual information between layers, demonstrated on datasets like MNIST and models of pyramidal neurons. Additionally, the paper explores the Correlative Mutual Information (CMI) framework, particularly focusing on the role of the $\epsilon_k$ parameter as a correction factor for rank-deficient correlation matrices. The authors clarify that each choice of $\epsilon_k$ defines a valid alternative CMI measure and discuss its implications on prediction error covariance and sensitivity to estimation errors. Furthermore, the paper compares CMI with Shannon Mutual Information and provides a detailed comparison between Canonical Correlation Analysis (CCA) and Correlative Information Maximization (CorInfoMax), highlighting their distinct optimization variables and objectives.

### Strengths and Weaknesses
Strengths:  
- The manuscript tackles significant issues regarding the implementation of supervised learning in biological neural networks and offers a principled solution using information theoretic concepts.  
- The derivation of update rules appears mathematically sound and creative, contributing to the understanding of credit assignment in neural networks.  
- The paper provides a thorough mathematical framework for understanding the role of the $\epsilon_k$ parameter in CMI and effectively clarifies the distinction between CMI and Shannon Mutual Information.  
- The authors effectively clarify the differences between CCA and CorInfoMax, providing precise mathematical formulations.  
- The inclusion of recent experimental results demonstrates the practical implications of the proposed frameworks.

Weaknesses:  
- The paper suffers from poor writing, with dense notation and equations that obscure rather than clarify the proposed approach.  
- There is a lack of clarity regarding the learning algorithm, particularly its locality in space and time, which is crucial for biological plausibility.  
- The experiments primarily report test accuracy without exploring additional metrics, such as the symmetry of final weights or comparisons to feedback alignment.  
- The performance of the model does not seem impressive compared to standard backpropagation, especially on more challenging datasets.  
- The choice of large $\epsilon_k$ values may detract from the original concept of mutual information, necessitating clearer explanations in the text.  
- The paper lacks a clear demonstration of how the proposed methods can be implemented in practice and insufficient exploration of potential limitations or challenges associated with the proposed CorInfoMax extension.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the manuscript by reducing the number of equations and enhancing interpretative discussions around the model's predictions and comparisons to biological systems. It would be beneficial to include a limitations section that discusses the sensitivity to hyperparameter choices and the necessity of the feedback matrix being the identity at the last layer. Additionally, we suggest that the authors explicitly compare their approach to feedback alignment and address how their method differs from existing solutions, such as those proposed by Akrout et al. (2019). We also recommend that the authors improve the clarity of the relationship between CMI and mutual information, particularly regarding how larger values of $\epsilon_k$ influence the optimization process. Furthermore, we encourage the authors to enhance the presentation of their experimental results to better illustrate the contributions of their framework and to provide concrete examples or case studies for practical implementation. Finally, we suggest that the authors address potential limitations or challenges of the CorInfoMax extension to provide a more balanced perspective.