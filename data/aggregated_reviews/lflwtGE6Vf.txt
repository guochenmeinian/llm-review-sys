ID: lflwtGE6Vf
Title: (FL)$^2$: Overcoming Few Labels in Federated Semi-Supervised Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 5, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to federated semi-supervised learning (FSSL) that addresses the challenges posed by confirmation bias and the limited availability of labeled data at the server. The authors propose a method that incorporates client-specific adaptive thresholds, sharpness-aware consistency regularization, and learning status-aware aggregation to improve model performance. Experimental results demonstrate that the proposed method outperforms existing FSSL algorithms on benchmark datasets, highlighting the inadequacy of centralized semi-supervised methods in federated settings.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written with a clear logical flow.
2. The problem addressed is significant, and the proposed solution is elegant, effectively highlighting the limitations of existing centralized methods.
3. Strong experimental results support the claims made.

Weaknesses:
1. Some methodological aspects are unclear, such as the necessity of Eq 10 during adaptive thresholding and the implications of using a fixed threshold alongside client-specific thresholds.
2. The experiments are limited to two datasets, raising questions about the method's applicability to more complex classification tasks and other data types.
3. The paper lacks a limitations section, and the discussion of confirmation bias and its mitigation is not sufficiently persuasive.

### Suggestions for Improvement
We recommend that the authors improve clarity in the methodology by explicitly justifying the use of Eq 10 during adaptive thresholding and addressing the potential issues with fixed thresholds. Additionally, we suggest expanding the experimental evaluation to include more diverse datasets, such as CIFAR-100 and FMNIST, to demonstrate the robustness of the proposed method. Furthermore, we encourage the authors to provide a more thorough discussion of the limitations of their work and to include theoretical justifications for their approach to strengthen the overall argument regarding confirmation bias.