ID: j2YZDuHNDf
Title: Faithful Temporal Question Answering over Heterogeneous Sources
Conference: ACM
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FAITH, a temporal question answering (QA) system designed for faithful responses to implicit temporal questions using heterogeneous sources such as text, knowledge graphs (KG), and infoboxes. The system comprises three modules: a temporal question understanding module that structures question intent, a faithful evidence retrieval module that identifies relevant evidence with temporal filtering, and an explainable heterogeneous answering module that generates entity-level answers with supporting evidence. Additionally, the authors introduce a new benchmark dataset for evaluating implicit constraints.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and expands the scope of temporal QA to multiple heterogeneous sources.
- It successfully creates a temporal QA model and a benchmarking dataset.
- The model provides evidence alongside answers to enhance answer faithfulness.

Weaknesses:
- The novelty of the work is insufficiently explained, particularly regarding the reliance on existing methods like entity-centric retrieval and ExplaiGNN.
- The lack of formal definitions for task formats and "faithful" question answering is a significant gap.
- Methodological clarity is lacking, with many details remaining ambiguous, resembling a black box.
- The paper does not adequately reference recent literature on temporal question answering.

### Suggestions for Improvement
We recommend that the authors improve the formal definition of "faithful" in the context of their work and clarify how it relates to existing metrics in the NLP community. Additionally, the authors should provide a clearer explanation of the novelty of their approach compared to ExplaiGNN. We suggest including more detailed descriptions of the methods used, particularly regarding the temporal question understanding and evidence retrieval processes. Furthermore, the authors should enhance the dataset description by providing statistical distributions of question types and clarifying the generation of large-scale data from the initial 8 questions. Finally, we encourage the authors to incorporate human evaluation in their experiments to quantitatively assess the scope of faithfulness and address the limitations of their model, particularly concerning the performance of Un-Faith compared to FAITH.