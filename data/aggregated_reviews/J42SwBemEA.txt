ID: J42SwBemEA
Title: State Chrono Representation for Enhancing Generalization in Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 30
Original Ratings: 5, 6, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the State Chrono Representation (SCR), an innovative approach to enhance generalization in reinforcement learning (RL) by integrating temporal information into bisimulation metric learning. SCR aims to calculate state distances by considering future dynamics and cumulative rewards, thereby fostering long-term behavioral representations. The methodology employs two distinct encoders for state representation and chronological embeddings, introducing a novel distance metric that diverges from the standard \(L_p\) norm. The framework is evaluated across various challenging environments, demonstrating state-of-the-art performance in tasks requiring advanced generalization capabilities. Additionally, the authors study SCR's performance in high-variance environments, specifically within the Distracting Control Suite (DCS), and propose that increasing the number of seeds, rather than episodes, enhances robustness. They report metrics including median, IQM, mean, and optimality gap, while addressing concerns about high variance by including standard deviation calculated over all evaluation episodes. The authors also acknowledge the need for additional empirical evidence to support claims of state-of-the-art performance across all tasks and plan to provide hyperparameter settings for baseline methods in the appendix.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a significant challenge in RL: capturing long-term temporal state representations within a well-defined metric space.
2. SCR's incorporation of future behavioral information into the representation space is a logical strategy for improving generalization.
3. The experimental results are compelling, showing SCR's superiority over existing methods, particularly in challenging generalization tasks.
4. The authors provide a comprehensive analysis of SCR's performance, demonstrating significant improvements over baseline methods in distraction settings.
5. The inclusion of standard deviation over all evaluation episodes addresses concerns regarding high variance and the exclusion of extreme values.
6. The authors demonstrate adaptability of SCR with data augmentation through preliminary experiments and are open to addressing reviewer concerns.

Weaknesses:
1. The main results are based on only 5 seeds, raising concerns about the reliability of performance conclusions.
2. High variance in reported results makes it difficult to determine which algorithm performs better.
3. The paper lacks a comprehensive discussion on the relationship between SCR and established practices in RL representation learning, particularly regarding future information prediction.
4. The limitations section lacks depth, particularly concerning scenarios where SCR might underperform.
5. The justification for using only 5 seeds, citing common practice, does not adequately address the need for robustness in results.
6. The paper lacks learning curves for some experiments, which are crucial for demonstrating stability and performance over time.
7. The criteria for selecting baselines lack clarity and consistency, particularly regarding the treatment of DrQ and DrQ-v2 + CBM.
8. Current results do not convincingly support claims of state-of-the-art performance across all tasks, with notable gaps in some areas.
9. The absence of shared hyperparameter settings for baseline methods limits reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the robustness of their results by increasing the number of seeds used in experiments to provide more reliable performance conclusions. Additionally, addressing the high variance in results, particularly in Table 1, 2, and Figure 3, would strengthen the findings. The authors should also include learning curves for all experiments, particularly in the Minigrid and Meta-World tasks, to better illustrate the performance differences between SCR and the baselines. Furthermore, we suggest including a thorough discussion of the relationship between SCR and related works, such as self-predictive representation algorithms, to clarify its contributions to the field. A more insightful analysis of the limitations of SCR, especially in scenarios where it may not perform optimally, should be included. We also recommend improving the clarity of baseline selection criteria by explicitly outlining the rationale for including or excluding specific methods. Additionally, we encourage the authors to extend their experiments to provide empirical evidence of state-of-the-art performance across all tasks, particularly for those with significant gaps. Finally, we suggest including the tuned hyperparameter settings of baseline methods in the appendix to enhance transparency and reproducibility in their research.