ID: Cs74qIBfiq
Title: Zero-Shot Robustification of Zero-Shot Models with Auxiliary Foundation Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 6, 4, 4, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents RoboShot, a method designed to enhance the robustness of pretrained model embeddings in a fully zero-shot manner by leveraging insights from language models based on task descriptions. The approach modifies embeddings to remove harmful components and enhance useful ones, ensuring invariance to spurious features through projection onto an orthogonal subspace. The authors demonstrate that RoboShot improves performance in multi-modal and language model zero-shot tasks. Additionally, the paper introduces a method for partitioning input embeddings into harmful, helpful, and benign components, enhancing zero-shot classification without fine-tuning.

### Strengths and Weaknesses
Strengths:
1. **Novel and useful setting:** The method's focus on improving robustness through task descriptions is innovative, preserving the usability of pretrained models.
2. **Extensive experiments and analyses:** The authors provide thorough evaluations across various datasets and settings, demonstrating the method's efficacy.
3. The paper is well-written and structured, making it accessible and easy to follow.

Weaknesses:
1. **Limitation of method:** The reliance on language model insights means that if the model fails to identify potential issues, the method cannot address them.
2. **Gender bias:** The evaluation of gender bias lacks clarity regarding the scope, including which genders are considered and unresolved biases.
3. The proposed method is primarily applicable to embedding-based zero-shot classification, potentially limiting its relevance given the rise of prompt-based methods like ChatGPT.
4. The method's performance improvement is not consistent across all datasets, raising questions about its overall effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the discussion of the method's limitations, particularly regarding when it may fail. Additionally, we suggest including a comparison with prompt-based methods, such as querying ChatGPT for predictions, to provide a more comprehensive baseline. It would also be beneficial to ensure that the overall average performance demonstrates improvement alongside the worst group performance. Finally, we encourage the authors to clarify the design of the queries used to extract insights and explore the impact of different prompts on performance.