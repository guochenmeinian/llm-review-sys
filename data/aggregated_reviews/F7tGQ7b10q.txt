ID: F7tGQ7b10q
Title: HonestLLM: Toward an Honest and Helpful Large Language Model
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 6, 4, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach to enhance the honesty and helpfulness of large language models (LLMs) through the construction of the HONESET dataset and the introduction of two methods: a training-free approach and a fine-tuning-based method. The evaluation demonstrates that the proposed methods can significantly improve LLMs' performance, with increases of 65% for Llama3-8b and 124% for Mistral-7b.

### Strengths and Weaknesses
Strengths:
- The paper addresses a critical issue affecting LLMs and provides the HoneSET dataset for future research.
- The evaluation of the proposed methods is thorough, showcasing their effectiveness across multiple LLMs.
- The clarity of writing and structure aids in understanding the proposed methods.

Weaknesses:
- The inclusion of a 3D pie chart detracts from readability; a different plot type is recommended for Figure 2.
- The term "human experts" lacks clarity regarding their qualifications in the dataset creation process.
- The paper does not adequately explore the implications of the proposed approach in Retrieval Augmented Generation (RAG) settings, nor does it compare its effectiveness against RAG in terms of honesty.
- The dataset construction lacks statistical indicators of agreement among the seven human experts, and the rationale for category proportions is insufficiently justified.
- The potential negative impact of the fine-tuning process on LLM safety standards is not addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the term "human experts" by providing details on their qualifications. Additionally, the authors should include a comparison of their approach with RAG settings and explore its potential to enhance LLM honesty in those contexts. We suggest replacing the 3D pie chart in Figure 2 with a more readable plot type. Furthermore, the authors should justify the construction of the dataset categories and provide statistical indicators of expert agreement. Lastly, an ablation study on the necessity of the two-stage training process in DPO would strengthen the paper.