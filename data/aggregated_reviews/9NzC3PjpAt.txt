ID: 9NzC3PjpAt
Title: Make You Better: Reinforcement Learning from Human Gain
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 3, 4, 6, 6, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new reinforcement learning objective called Reinforcement Learning from Human Gain (RLHG), which aims to enhance human goal-achievement abilities in collaborative tasks by incorporating human performance into the objective function. The authors evaluate the RLHG agent in the MOBA game, Honor of Kings, through experiments in both simulated and real-world settings. They propose a two-step approach: first, estimating the primitive human performance using a value network, and second, training a gain network to assess the enhancement in human returns from interactions with the agent.

### Strengths and Weaknesses
Strengths:
- The problem setting is relevant and addresses real-world applications, particularly in assistive agents for MOBA games.
- The authors conduct experiments with human participants, demonstrating the practical implications of their work.
- The proposed solution of optimizing for human gain is intuitive and aligns with enhancing cooperative agent behavior.

Weaknesses:
- The main claim lacks robust comparison; the authors do not evaluate their method against a model that directly optimizes for human objectives, raising questions about the superiority of their approach.
- The writing is often unclear, making it difficult to grasp the contributions and methodologies, particularly regarding the two-step training process and the role of the gain network.
- The method's reliance on predefined human goals limits its applicability, and the sensitivity of the method to the choice of partner during training is not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing to better articulate the contributions and methodologies, particularly in defining key terms and notations. Additionally, we suggest that the authors conduct comparisons against models that optimize directly for human objectives to substantiate their claims. It would also be beneficial to explore scenarios where human goals are not predefined and to analyze the method's performance in such cases. Finally, consider simplifying the experimental setup to facilitate reproducibility and validation of the algorithm's effectiveness.