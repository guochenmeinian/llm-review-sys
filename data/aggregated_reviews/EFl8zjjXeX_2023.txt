ID: EFl8zjjXeX
Title: OV-PARTS: Towards Open-Vocabulary Part Segmentation
Conference: NeurIPS
Year: 2023
Number of Reviews: 23
Original Ratings: 6, 6, 7, 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 3, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark for the open vocabulary part segmentation task, introducing two new datasets: Pascal-Part-116 and ADE20K-Part-234, along with three specific tasks: Generalized Zero-Shot Part Segmentation, Cross-Dataset Part Segmentation, and Few-Shot Part Segmentation. The authors conduct extensive experiments to evaluate various methods and analyze their performance, contributing significantly to the field. Additionally, the paper explores open-vocabulary part segmentation (OV-PARTS) through methodologies such as the use of CLIP for pixel-level understanding and addresses the challenges of part segmentation, particularly in the context of unseen classes. The authors justify their choice of datasets, highlighting the limitations of existing benchmarks like PartImageNet and PACO-LVIS, and discuss baseline models, including MaskFormer and ZSseg.

### Strengths and Weaknesses
Strengths:
- The authors conducted numerous comparative experiments, aiding researchers in understanding the performance gaps among different methods.
- The proposed benchmark and systematic experiments contribute to the development of the community.
- The writing is clear, and the paper is well-organized, with a clear rationale for methodological choices and the design of experiments.
- The inclusion of supplementary material with detailed explanations and visualizations aids in understanding complex concepts.
- The authors provide clear explanations of dataset modifications and the rationale behind dataset selection, enhancing reproducibility.

Weaknesses:
- The construction method of the datasets is only briefly described, lacking clarity on the differences between the constructed and original datasets.
- There is no statistical comparison with similar datasets, limiting the novelty of the new datasets.
- Some terminology and definitions, particularly regarding benchmark settings and evaluation metrics, are unclear.
- The paper's organization could be improved to enhance clarity, given the breadth of topics covered.
- Some results, particularly in the one-shot setting, are not fully represented in the main tables, which may limit the reader's understanding of the model's capabilities.
- Some figures, particularly in the appendix, suffer from low quality and readability issues.

### Suggestions for Improvement
We recommend that the authors improve the description of the dataset construction methods to clarify the differences from the original datasets. Detailed explanations on how merged class labels were chosen and the rationale behind selecting Pascal-Part and ADE20K over other benchmarks should be included. Additionally, we suggest providing a computation complexity analysis and ensuring that all models are evaluated using the same backbone architecture for fair comparison. Clarifying the definitions of benchmark types and metrics, as well as addressing ambiguities in the evaluation process, will enhance the paper's clarity. We also recommend improving the organization of the paper to ensure clarity, especially in sections covering the various methodologies and results. Consider including the "One Shot" results in Table 3 to provide a more comprehensive view of the model's performance. Lastly, we encourage the authors to improve the quality of figures in the appendix to enhance readability, provide clearer explanations for the omissions in Tables 1 and 2, and compare ZSseg+ with other models in a one-shot learning context to substantiate claims regarding its performance.