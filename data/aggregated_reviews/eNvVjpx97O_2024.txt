ID: eNvVjpx97O
Title: StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to enhance the handling of long-context dialogues by Large Language Models (LLMs) through the use of special tokens, specifically end-of-utterance (EOU) tokens referred to as conversational attention sinks (conv-att sinks). The authors propose two auxiliary tasks, SMR and LMR, to improve the information aggregation capabilities of these tokens. The proposed method, StreamingDialogue, demonstrates significant improvements in computational efficiency and performance across multiple dialogue datasets, achieving a 4× speedup and reducing memory usage by 18× compared to dense attention strategies.

### Strengths and Weaknesses
Strengths:
1. The approach addresses a critical challenge in dialogue systems, and the proposed method is intuitive and well-structured.
2. StreamingDialogue shows promising results, outperforming memory-efficient baselines and achieving comparable performance to dense attention.
3. The paper is well-written, and the experiments substantiate the authors' claims regarding the effectiveness of the proposed method.

Weaknesses:
1. The training method is overly complex, leading to excessive training costs due to multiple forward passes on the same sample.
2. Comparisons with alternative methods, such as infinity former and compressive transformer, are lacking, which limits the evaluation of the proposed approach's effectiveness.
3. The metrics used for evaluation, particularly in Table 1, are not well-suited for open-domain dialogue tasks, and there is inconsistency in the use of BLEU metrics across different tables.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their training method to reduce complexity and associated costs. Additionally, we suggest including comparisons with alternative approaches like infinity former and compressive transformer to strengthen the evaluation. The authors should also consider using more appropriate metrics for open-domain dialogue tasks and ensure consistency in the presentation of BLEU metrics across all tables. Finally, providing further analysis on the performance of StreamingDialogue on longer sequences and additional datasets would enhance the robustness of their findings.