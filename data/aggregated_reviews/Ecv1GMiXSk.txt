ID: Ecv1GMiXSk
Title: Better Correlation and Robustness: A Distribution-Balanced Self-Supervised Learning Framework for Automatic Dialogue Evaluation
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 5, 6, 4, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework called Better Correlation and Robustness (BCR) aimed at enhancing turn-level dialogue evaluation models (TDEMs) by addressing the challenges of low correlation with human judgments on medium coherence samples and nonuniform score distribution. The authors propose two main techniques: a training set reconstructing method (TSR) to balance coherence distribution and a dynamic penalty loss function (DP loss) to ensure uniform score distribution. TSR involves reconstructing datasets with uneven distributions in self-supervised regression tasks to achieve balanced supervision signals, which includes constructing data in underrepresented domains and denoising labels using model predictions. The authors assert that TSR can be adapted to various tasks, including Factuality, Data to Text, and Text Summarization. Experimental results indicate that the BCR framework significantly improves performance over state-of-the-art methods when applied to a vanilla BERT-based model. The paper also discusses the computational costs associated with human annotation for dialogue evaluation, emphasizing the high inter-annotator consistency required, and clarifies the use of Spearman's r for statistical testing while recommending human evaluation for dialogue models.

### Strengths and Weaknesses
Strengths:
- The motivation is well-founded, addressing a significant gap in dialogue evaluation research.
- The proposed methods, BCR and TSR, demonstrate novelty and provide a clear methodology with robust experimental results supporting their efficacy.
- The paper is generally well-organized and easy to comprehend.
- The authors address important concerns regarding computational costs and the need for human evaluation in dialogue models.
- The inclusion of detailed statistical testing procedures enhances the rigor of the research.

Weaknesses:
- The main assumption regarding the test set's coherence distribution lacks sufficient justification, and the terms "medium" and "coherence" are ambiguous.
- Theorem 1's universality is questioned, as it appears limited to the SSL framework, potentially hindering its applicability to other evaluation metrics.
- The paper's focus on a narrow topic limits its broader applicability, and there is insufficient discussion on computational costs and the impact of the proposed method.
- Many experimental details, including hyperparameter tuning and statistical significance testing, are inadequately explained.
- The need for comparisons with up-to-date baselines is emphasized but not fully addressed in the initial submission.

### Suggestions for Improvement
We recommend that the authors improve the justification for the assumption regarding the coherence distribution and clarify the definitions of "medium" and "coherence." Additionally, we suggest improving the clarity of Theorem 1's applicability by emphasizing its independence from the SSL framework in the main text. Providing more detailed explanations of the experimental setup, including hyperparameter tuning criteria and statistical testing procedures, would enhance the paper's robustness. It would also be beneficial to discuss the computational costs associated with implementing the BCR method and explore its transferability to other evaluation tasks. Finally, we encourage the authors to include comparisons with contemporary baselines and detailed responses regarding transferability, computational costs, statistical tests, and human annotation costs in the paper or Appendix to reinforce their findings. Consider including a section on alternative strategies for creating augmented samples and soft pseudo labels.