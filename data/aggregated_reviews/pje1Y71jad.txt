ID: pje1Y71jad
Title: Cost-efficient Knowledge-based Question Answering with Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 6, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a cost-efficient strategy named "Coke" for automatically assigning the most suitable model for specific Knowledge-based Question Answering (KBQA) tasks. The authors introduce a method that determines whether to utilize a Large Language Model (LLM) or a Knowledge Graph-based Model (KGM) based on historical data, formulating the problem as a multi-armed bandit problem. The proposed approach aims to enhance performance while minimizing costs, demonstrated through experiments on three domain-specific datasets.

### Strengths and Weaknesses
Strengths:
1. The problem definition and mathematical explanation are clear, with a novel approach to model selection based on historical success, question semantics, and cost regret.
2. The experimental results convincingly demonstrate the advantages of the proposed method, showing potential for cost reduction while improving performance.
3. The writing is clear, and the theoretical analysis, including expectation bounds and confidence bounds, supports the design of the automatic selection process.

Weaknesses:
1. The clarity of cost metrics is questionable; it is unclear if the experimental results consider KGM costs or if they are treated as cost-free.
2. The datasets used are similar in reasoning, limiting generalizability, and the experimental setup lacks detail regarding model training and cost function implementation.
3. The results indicate only modest cost savings, suggesting room for improvement in the decision-making model, particularly in real-world applications.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the cost metrics in the experiments section, specifically addressing whether KGM costs are included. Additionally, providing a pseudo-algorithm to illustrate the selection process would enhance understanding. We suggest including results from various combinations of base models as additional ablation studies to strengthen the findings. Furthermore, expanding the discussion on the practical applicability of the method in real-world scenarios, such as chat-bots, would be beneficial. Lastly, incorporating more heuristic baselines and addressing the implementation details of the model training would improve the overall robustness of the paper.