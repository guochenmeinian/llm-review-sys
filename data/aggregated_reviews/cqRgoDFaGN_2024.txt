ID: cqRgoDFaGN
Title: FasterDiT: Towards Faster Diffusion Transformers Training without Architecture Modification
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 6, 6, 6, 6, -1, -1, -1, -1
Original Confidences: 5, 3, 3, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FasterDiT, a method designed to accelerate the training of Diffusion Transformers (DiT) without altering their architecture. By formulating the Probability Density Function (PDF) of Signal-to-Noise Ratio (SNR), the authors analyze the relationship between training performance and robustness, ultimately proposing FasterDiT to enhance training efficiency. The paper includes extensive empirical experiments demonstrating significant improvements in training speed and performance.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel approach by reformulating the SNR, contributing to improved training strategies.
- It provides robust empirical evidence through extensive experiments, showing FasterDiT achieves significant acceleration in training time while maintaining competitive performance metrics.
- The motivation for addressing slow convergence in DiT is clearly articulated, making the contribution relevant and practical.

Weaknesses:
- The generalizability of FasterDiT is not sufficiently demonstrated, with a lack of experiments on larger datasets or more complex tasks.
- A comprehensive comparison with existing methods for accelerating DiT training is missing, limiting the contextual evaluation of FasterDiT's effectiveness.
- The writing quality is poor, with unclear explanations and several typographical errors that hinder understanding.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, addressing typos and ensuring that technical terms, such as "std," are clearly defined. Additionally, we suggest including experiments on higher resolutions (e.g., 512x512 or 1024x1024) to evaluate the scalability of FasterDiT. A more thorough comparison with both classifier-free guidance and class-conditional results should be included to enhance the robustness of the findings. Finally, discussing the method's applicability to other diffusion models would strengthen the paper's contributions.