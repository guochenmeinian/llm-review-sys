ID: 4RoD1o7yq6
Title: Binary Classification with Confidence Difference
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 9, 6, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to binary classification problems through a new data type based on "confidence difference," which measures the difference in conditional probabilities between two input instances. The authors introduce new loss functions and derive an excess risk bound with probability. They also explore the impact of noise on confidence difference and propose a method to rectify empirical loss functions. Empirical tests demonstrate that the proposed algorithm consistently outperforms competitors on benchmark datasets.

### Strengths and Weaknesses
Strengths:
- The proposed data type based on confidence difference is original and contributes significantly to the field.
- The research is supported by solid theoretical analysis, enhancing its quality.
- The paper is clearly written and easy to follow, with broad application scenarios that underscore its significance.

Weaknesses:
- The justification for the novel problem setting is weak, particularly regarding claims about bias in confidence difference estimates.
- There is a lack of comparison with traditional label-based performance, which could highlight the advantages of the proposed method.
- Some minor issues need clarification, such as the differences in experimental design compared to prior works and the use of certain notations.

### Suggestions for Improvement
We recommend that the authors improve the justification for the confidence difference claims, particularly regarding its lower bias compared to individual estimates. Additionally, including performance comparisons with traditional label-based methods would strengthen the experimental section. We also suggest clarifying the experimental design differences noted in the questions and addressing the use of any notations that are not utilized elsewhere in the paper.