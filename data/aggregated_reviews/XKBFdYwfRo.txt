ID: XKBFdYwfRo
Title: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 5, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for addressing linear inverse problems using pre-trained latent diffusion models (LDM) as a prior, extending the original diffusion posterior sampling (DPS) to LDM by approximating the gradient term of the intractable likelihood. The authors propose two approximations, GLM-DPS and PSLD, with PSLD demonstrating superior performance. Theoretical analyses are provided in a toy linear setting, and experimental results indicate improved performance over the original DPS across various tasks, including inpainting, denoising, and super-resolution. The authors also acknowledge that their results are valid only under restrictive assumptions in the linear noiseless case and introduce the gluing objective to address issues with (latent) DPS not recovering the ground truth. Metrics like MSE/PSNR and LPIPS are discussed for validating reconstruction, although the theoretical justification for asserting that DPS or PSLD sample from the true posterior is lacking. The authors clarify their tuning of the hyper-parameter $\eta$ and the uniformity of noise standard deviation across experiments.

### Strengths and Weaknesses
Strengths:
- This work is the first to utilize LDM for linear inverse problems.
- PSLD outperforms the original DPS in most scenarios.
- Theoretical analysis, although based on a toy model, adds value to the methodology.
- The authors provide a clear rationale for the introduction of the gluing objective.
- The paper effectively addresses the limitations of DPS in noisy settings and emphasizes the importance of the latent space analysis.
- The inclusion of comparisons with other methods demonstrates the proposed method's superiority.

Weaknesses:
- There is insufficient analysis of the complexity and running time of the PSLD method; details on running time comparisons are needed.
- The theoretical results, particularly Theorems 3.4, 3.7, and 3.8, require clarification regarding their generalizability to real diffusion models.
- The experiments lack comparisons with other recent diffusion-model-based algorithms, which were available prior to the NeurIPS submission.
- The theoretical justification for the metrics used in validation is lacking.
- The discussion on the tuning of the hyper-parameter $\eta$ is insufficiently detailed in the main text.
- The uniformity of noise standard deviation across experiments is not adequately discussed.
- The paper does not address colorization tasks, and quantitative comparisons are limited to inpainting; additional comparisons for super-resolution and denoising are suggested.

### Suggestions for Improvement
We recommend that the authors improve the analysis of the complexity and running time of the PSLD method, including comparisons with other methods. Clarifying the generalizability of the theoretical results to real diffusion models is essential. Additionally, we suggest including comparisons with other recent diffusion-model-based algorithms, such as those by Wang et al., Meng and Kabashima, and Song et al. Furthermore, the authors should improve the clarity of the discussion regarding the theoretical justification for the metrics used, particularly in relation to posterior sampling. Please provide a more detailed explanation of the tuning process for the hyper-parameter $\eta$ in the main paper. It is also advisable to include a discussion on the implications of using a constant noise standard deviation across experiments in the revised version. Lastly, the authors should consider adding results for colorization tasks and expanding quantitative comparisons across all tasks, including super-resolution and denoising.