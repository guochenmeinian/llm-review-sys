ID: aVSxwicpAk
Title: 4+3 Phases of Compute-Optimal Neural Scaling Laws
Conference: NeurIPS
Year: 2024
Number of Reviews: 20
Original Ratings: 7, 6, 6, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of the Power Law Random Feature (PLRF) model and an analysis of random features using random matrix theory, focusing on the dynamics of one-pass stochastic gradient descent (SGD) in a high-dimensional setting. The authors derive scaling laws that characterize the compute-optimal frontier based on parameters such as data complexity, target complexity, and model parameter count. They utilize a Volterra equation approach to analyze the generalization error dynamics and support their theoretical claims with extensive numerical simulations. The work highlights the effects of SGD noise and finite width on scaling behaviors, revealing insights into the optimal allocation of compute resources. The authors propose to move the definitions of $K_{pp}$ and $F_{ac}$ earlier in the text and to rename them for clarity. They discuss the philosophical divide regarding kernel regression and the implications of eigenvalue decay rates in the context of large language model (LLM) scaling. Additionally, the paper addresses the optimization trajectory in SGD, suggesting a change in terminology from "SGD frustrated" to "SGD limited," and acknowledges the need for a related work section to contextualize their contributions within existing literature.

### Strengths and Weaknesses
Strengths:
- The paper provides a thorough theoretical and numerical analysis, contributing significantly to the understanding of scaling laws in machine learning.
- The theoretical results are compelling and contribute novel insights into the analysis of SGD and random features.
- The derivations are technically sound, and the experiments are well-executed, enhancing the overall presentation of the findings.
- The inclusion of a table comparing their work with previous studies enhances clarity and utility for the community.

Weaknesses:
- The presentation is challenging, particularly for non-expert readers, due to a sparse citation of related literature and insufficient discussion of prior work.
- The related work section is currently too brief and requires improvement.
- The mathematical techniques employed lack clarity regarding their novelty and general applicability, and the connection to more complex models is not adequately addressed.
- The discussion around the deterministic equivalent and its justification lacks rigor and clarity, particularly regarding the technical challenges involved.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the novelty and general applicability of the mathematical techniques used, providing clearer context and connections to existing literature. Additionally, we suggest that the authors expand the related work section to provide a more comprehensive overview of existing literature and clearly highlight their contributions. It is crucial to enhance the accessibility of the appendix by restructuring the order of theorems and propositions, and including an "overview of proofs" to aid navigation. Furthermore, we encourage the authors to include a detailed discussion of the technical challenges associated with justifying the deterministic equivalent, as outlined in footnote 7, to enhance the rigor of their analysis. Finally, we urge the authors to explore the implications of their findings for practical applications, such as large-scale language models, to increase the impact of their research.