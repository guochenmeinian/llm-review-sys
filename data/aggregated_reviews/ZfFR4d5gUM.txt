ID: ZfFR4d5gUM
Title: Leveraging the two-timescale regime to demonstrate convergence of neural networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 5, 6, 6, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on training 2-layer neural networks using a two-timescale regime, where the learning rate of the first layer is significantly smaller than that of the second layer. This approach simplifies the training process by allowing the first layer to be trained while performing linear regression on its outputs. The authors demonstrate this strategy through a toy model, showing that a 2-layer network can fit a specific family of 1D piecewise constant functions and that SGD may fail outside the two-timescale regime.

### Strengths and Weaknesses
Strengths:
* The presentation is clear and the results are well-explained.
* The paper includes both asymptotic and non-asymptotic results, with a clean derivation in the $\eta \to 0, \varepsilon \to 0$ limit (Sec. 4.2).
* The two-timescale training strategy is a novel approach that may address limitations in existing techniques, allowing for multiple training steps of the first layer.

Weaknesses:
* The focus is primarily on 1D piecewise constant functions, raising concerns about generalization to higher dimensions and other function classes.
* The dynamics appear to be relatively local, necessitating specific neuron initialization that may not be feasible in high dimensions.
* The paper lacks a discussion on the training-for-one-large-step technique and does not address the limitations of its assumptions.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by exploring the application of their strategy to higher-dimensional problems and more standard 2-layer networks. Additionally, we suggest that the authors relax the initialization requirements, allowing for a more global feature learning process. Finally, we encourage the authors to include a discussion on the training-for-one-large-step technique and to provide numerical experiments in higher dimensions to strengthen their claims.