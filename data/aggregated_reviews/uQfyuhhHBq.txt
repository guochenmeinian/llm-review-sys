ID: uQfyuhhHBq
Title: Penalty Decoding: Well Suppress the Self-Reinforcement Effect in Open-Ended Text Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the self-reinforcement effect in open-ended text generation and proposes two new metrics for measuring this effect at the N-gram and Nucleus levels. The authors also introduce a simple penalty decoding method and three strategies—repetition penalty, forgetting mechanism, and length penalty—to mitigate the self-reinforcement effect. The study presents a comprehensive comparison with existing decoding methods and reports state-of-the-art results in automatic evaluations.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to understand.
2. It provides a good analysis of the self-reinforcement effect and presents clever evaluation metrics.
3. The proposed methods show potential effectiveness in automatic evaluations.

Weaknesses:
1. The work lacks human evaluation, which is crucial for validating results in open-ended text generation.
2. The technical novelty is limited, and the proposed methods are somewhat incremental, with similarities to existing approaches.
3. The writing and organization could be improved for clarity, and the analyses do not fully align with the insights presented.

### Suggestions for Improvement
We recommend that the authors improve the clarity and organization of the paper to enhance reader comprehension. Additionally, the authors should include human evaluation to substantiate the effectiveness of their methods, as automatic metrics may not be reliable. We also suggest that the authors provide a more in-depth analysis of their forgetting mechanism and clarify the differences between their approach and existing methods, particularly in relation to *Keskar et al. (2019)*. Finally, addressing the notation and experimental settings in comparison to previous works would strengthen the paper's contributions.