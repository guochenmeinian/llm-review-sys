ID: hLtbmYoW5w
Title: Universal Trojan Signatures in Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 2
Original Ratings: 6, 8
Original Confidences: 4, 4

Aggregated Review:
### Key Points
This paper presents a method for detecting trojans in reinforcement learning models by leveraging the distinct behavior of trojaned models with clean inputs. The authors propose calculating the gradients of both the policy and value networks with respect to the input, using these gradients as features to train a binary classifier for trojan detection. The experiments utilize the TrojAI challenge datasets, demonstrating the effectiveness of the proposed approach, which outperforms a baseline weight analysis method. Additionally, the paper introduces a defense method for auditing backdoors in RL agents, highlighting that small input perturbations significantly affect the advantage predictions of attacked models, leading to simple detection methods based on Jacobians.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a timely issue in detecting trojaned reinforcement learning models.
2. The example illustrated in Figure 2 is both intuitive and interesting.
3. The proposed detection method is reasonable and exhibits good performance.
4. The method is simple and well-motivated, contributing to an underexplored topic in backdoor attacks.

Weaknesses:
1. The proposed approach requires training on hundreds of models, and it would be beneficial to observe how detection performance varies with different numbers of training models.
2. The potential for adaptive attacks that match the attribution between trojaned and clean models on clean inputs raises concerns about the performance of the detection approach under such circumstances.
3. Future work should investigate the sensitivity and generalization of the proposed detection method when transferred to new environments.

### Suggestions for Improvement
We recommend that the authors improve the analysis of detection performance by varying the number of training models to understand its impact. Additionally, the authors should address the potential for adaptive attacks and clarify the robustness of their detection method under such scenarios. Finally, we suggest that the authors explore the sensitivity and generalization of their detection method in new environments to enhance its applicability.