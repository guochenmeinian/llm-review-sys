ID: nC47EZVfAw
Title: Low-Resource Comparative Opinion Quintuple Extraction by Data Augmentation with Prompting
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel Data Augmentation with Prompting (DAP) approach for comparative opinion quintuple extraction (COQE), addressing performance issues associated with applying Large Language Models (LLMs) directly to COQE due to data complexity and scarcity. The authors propose an end-to-end model architecture and a two-stage data augmentation strategy that utilizes ChatGPT for generating triplet datasets followed by transfer learning. Experimental results demonstrate the effectiveness and robustness of the proposed method across three benchmark datasets.

### Strengths and Weaknesses
Strengths:  
- The paper introduces a novel two-stage data augmentation method leveraging ChatGPT and transfer learning, which enhances COQE performance.  
- It is well-organized and easy to follow, with positive experimental results validating the proposed approach.  

Weaknesses:  
- The motivation for using ChatGPT for data augmentation is unclear, and comparisons with other augmentation methods are lacking.  
- The paper considers only one baseline for COQE, and ablation studies are absent, which limits the robustness of the findings.  
- Some technical aspects, such as the definition of K in formula (5) and the use of transfer learning, require clarification.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind using ChatGPT for data augmentation and include comparisons with other data augmentation methods. Additionally, the authors should incorporate more baselines in their experiments and conduct ablation studies to strengthen their claims. Clarifying the definition of K in formula (5) and providing a more detailed explanation of the transfer learning process would enhance the paper's rigor. Furthermore, we suggest adding the generated data to the "pipeline" and reporting its performance to demonstrate the improvement achieved.