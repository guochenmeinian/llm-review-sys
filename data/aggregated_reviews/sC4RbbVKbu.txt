ID: sC4RbbVKbu
Title: SutraNets: Sub-series Autoregressive Networks for Long-Sequence, Probabilistic Forecasting
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 6, 6, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "SutraNets: Sub-series Autoregressive Networks for Long-Sequence, Probabilistic Forecasting," which models univariate time-series using two interleaved networks for 'fine-grained' and 'coarse-grained' time-steps. This approach reduces signal paths, mitigates inference error accumulation, enables parallel training, and allows for more efficient parameter sharing. The authors conduct experiments on six standard datasets, demonstrating superior performance compared to baselines. They compare SutraNets to various methods, including C2FAR and its variations, as well as Low2HighFreq, while noting that comparisons with recent methods like TimesNet are complicated due to differences in historical data context. The authors advocate for including a seasonal-naive baseline to account for strong weekly seasonality in datasets like `traffic` and `electricity` and mention a recent comparison with Scaleformer for a more valid evaluation.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized and clearly written, featuring a comprehensive literature review and effective figures.
- The core idea is intuitive, proposing several extensions to standard autoregressive architectures and investigating their impact on performance.
- The method shows improved performance over baseline models and provides extensive experimental results across multiple datasets, demonstrating the model's capabilities.
- The authors have implemented a rigorous comparison framework, ensuring fair evaluation across methods.

Weaknesses:
- Some figures lack clarity; for instance, Figure 4's term 'more confident' could be better defined, and quantifying findings would enhance understanding.
- The paper does not adequately address related work on multivariate forecasting, missing references to significant prior studies.
- There are inaccuracies in equations and descriptions that could mislead readers regarding the model's conditioning and performance implications.
- There is a lack of comparison with recent long-time series prediction methods, raising questions about the model's effectiveness in that context.
- The evaluation does not include baselines for long-time series prediction methods, leading to uncertainty about SutraNet's performance.

### Suggestions for Improvement
We recommend that the authors improve clarity in Figure 4 by explicitly defining 'more confident' and quantifying findings, possibly through likelihoods on hold-out test data. Additionally, we suggest revising Figure 6 to include "inference time per forecast" for better unit representation. The authors should also incorporate relevant prior work on multivariate forecasting in the literature review and ensure accurate conditioning in equations, particularly in equation (2) and the inline formula in line 131. Furthermore, we encourage the authors to explore the implications of different recurrence orders and hidden state dynamics on performance. To strengthen the validation of SutraNets, we recommend including comparisons with recent long-time series prediction methods and incorporating baselines for long-time series prediction methods to provide a clearer context for assessing the model's effectiveness. It would also be beneficial to adapt long-time prediction methods for probabilistic forecasting to illustrate SutraNet's prowess in long-sequence prediction.