ID: NapL36HSBT
Title: Provably Robust Temporal Difference Learning for Heavy-Tailed Rewards
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 4, 6, 7, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 2, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies reinforcement learning under heavy-tailed rewards, proposing two algorithms: robust TD learning and robust Natural Actor-Critic (NAC). The authors provide theoretical results and numerical verification, demonstrating polynomial sample complexity in the generative setting. The work addresses the failure of classical TD learning under heavy-tailed rewards and introduces a dynamic gradient clipping mechanism to ensure convergence. Additionally, the authors present a method for managing heavy-tailed noise in reinforcement learning through explicit regularization techniques, such as $\ell_2$-projection and early stopping, to control $\|\Theta(t)\|_2$. They differentiate their approach from previous work by addressing both iid and Markovian sampling, particularly in the context of gradient bias and heavy-tailed semi-gradients. The importance of a richer function approximation scheme to achieve smaller approximation errors is also discussed, emphasizing the trade-off between approximation capability and sample complexity.

### Strengths and Weaknesses
Strengths:
- The problem of heavy-tailed rewards is significant and underexplored in reinforcement learning.
- The paper is well-written, with clear motivation and theoretical contributions, including convergence rates for both TD(0) and NAC.
- Numerical results support the theoretical claims, showcasing the advantages of the proposed robust algorithms.
- The paper provides a clear distinction between their approach and prior work, particularly in handling gradient bias.
- The discussion on the bias-complexity tradeoff is insightful and adds depth to the understanding of approximation errors in RL.

Weaknesses:
- The technical contribution is questioned, as it appears to be a straightforward combination of existing robust mean estimation techniques and previous analyses of TD learning.
- The assumptions, particularly regarding realizability and the knowledge of the parameter \( p \), are considered strong and potentially limiting.
- Some reviewers expressed confusion regarding the realizability assumption and approximation errors, indicating a need for clearer explanations.
- The complexity of the proposed methods may pose challenges in practical applications, particularly regarding sample efficiency.
- The presentation could be improved, with some notations undefined and minor details missing in proofs.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the technical contributions by explicitly detailing how their results extend beyond existing work. Additionally, the authors should provide more background on the assumptions, particularly regarding heavy-tailed distributions, to clarify potential limitations. It would be beneficial to address the practical estimation of the parameter \( p \) and discuss the implications of the realizability assumption on convergence. We also suggest that the authors improve clarity by expanding the discussion on the realizability assumption and including results without this assumption to address reviewer concerns. Furthermore, providing more explicit examples of how to achieve smaller approximation errors using richer function classes could enhance the practical applicability of their findings. Finally, enhancing the presentation by defining all notations and ensuring completeness in proofs will strengthen the paper.