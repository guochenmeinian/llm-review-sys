ID: sXErPfdA7Q
Title: Document-Level Machine Translation with Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the capabilities of large language models (LLMs), specifically GPT-3.5-turbo (ChatGPT) and GPT-4, in document-level machine translation. The authors compare these models against commercial systems (GoogleTranslate, DeepL, Tencent's MT) and publicly available models across three translation directions: Chinese to English, English to German, and English to Russian. They employ document-level BLEU (d-BLEU), TER, and COMET scores for evaluation, supplemented by a simplified human evaluation based on "discourse awareness" and "overall quality." The study includes experiments on discourse-aware prompts, comparisons with commercial and document-level NMT models, and an analysis of LLMs' ability to process discourse-level information.

### Strengths and Weaknesses
Strengths:
- The authors explore a timely and significant topic in document-level translation.
- The inclusion of human evaluation adds depth, despite some methodological concerns.
- The experimentation covers diverse text types (news, fiction, Q&A).

Weaknesses:
- The impact of different training methods is confounded by variations in training data and potential data contamination.
- Lack of significance testing undermines claims of performance differences.
- The human evaluation lacks detail, making it difficult to assess its reliability and validity.
- The automatic evaluation primarily relies on d-BLEU, which may not be optimal without multiple references.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the datasets used, particularly their publication dates and potential overlap with training data. It is essential to perform significance testing to substantiate claims about performance differences among models. We also suggest enhancing the human evaluation by providing more detailed methodology, including the number of evaluators, their compensation, and the evaluation process. Additionally, we encourage the authors to clarify the objectives of the "discourse-aware prompts" experiment and consider alternative evaluation methods, such as using sliding windows for COMET. Lastly, we advise including relevant citations to previous works that explore LLM capabilities in machine translation.