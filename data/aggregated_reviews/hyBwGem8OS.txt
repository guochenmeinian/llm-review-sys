ID: hyBwGem8OS
Title: InteMATs: Integrating Granularity-Specific Multilingual Adapters for Cross-Lingual Transfer
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to enhance cross-lingual transfer in pretrained multilingual models by integrating multilingual adapters trained at different granularities—sentence-level and document-level—using an AdapterFusion mechanism. The authors propose training these adapters with contrastive learning on a newly created multilingual entity-aligned dataset derived from Wikipedia, covering 43 languages. The main contributions include the creation of this dataset, the fusion of adapters, and the application of contrastive learning, which reportedly leads to improved performance in various NLP tasks, particularly for low-resource languages and longer contexts.

### Strengths and Weaknesses
Strengths:
- The proposed contributions, including the multilingual dataset and the integration of different granularities, are well-motivated and relevant.
- The experimental results demonstrate performance improvements across various tasks, indicating the method's efficacy.
- The paper includes extensive experiments and a comprehensive analysis of the relevance of document-level adapters.

Weaknesses:
- The specific contributions of each component—contrastive learning, the dataset, and the adapter fusion—are not clearly delineated, making it difficult to ascertain their individual impacts on performance.
- The comparison with existing models lacks clarity, as many baselines were trained on different datasets and objectives, complicating the interpretation of results.
- The paper does not adequately address the need for a comprehensive benchmarking analysis or cite relevant literature on multilingual adapters.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contributions by explicitly detailing the impact of each component on the overall results. Additional experiments should be conducted to isolate the effects of contrastive learning and the dataset on cross-lingual transfer performance. Furthermore, we suggest including a brief introduction to each baseline method used in the experiments, outlining their key features and training methodologies, and ensuring proper citations for all relevant literature on multilingual adapters. Lastly, addressing the readability of tables and figures, such as increasing font sizes, would enhance user-friendliness.