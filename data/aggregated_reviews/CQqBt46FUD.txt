ID: CQqBt46FUD
Title: Unbiased learning of deep generative models with structured discrete representations
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 4, 5, 7, 7, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel implicit optimization approach for Structured Variational Autoencoders (SVAEs), utilizing the implicit function theorem to compute implicit gradients, thereby enhancing stability and memory efficiency. The authors revisit natural gradients and propose an unbiased version, demonstrating improvements through experiments on human pose capture and audio spectrogram datasets. Evaluation metrics include modeling performance (likelihood), visual quality (FID score), and missing data imputation.

### Strengths and Weaknesses
Strengths:
1. The paper exhibits impressive theoretical depth, particularly regarding implicit gradients and unbiased natural gradients.
2. A comprehensive appendix details all derivations, enhancing the paper's rigor.
3. Experimental results indicate a clear improvement of `SVAE-SLDS` over the baseline `SVAE-SLDS-Bias` in terms of likelihood and FID scores.

Weaknesses:
1. The paper lacks structure and clarity, necessitating better synthesis and presentation of its main contributions.
2. It attempts to cover too many contributions, leading to insufficient support for each.
3. The experimental section is limited, utilizing only two small datasets.
4. Missing is an experiment demonstrating the algorithm's applicability to larger-scale datasets, despite claims of improved memory efficiency.
5. Section 3.2 incorrectly suggests that discrete latent variable models can only be optimized by reparameterizing discrete distributions, neglecting viable alternatives like score function estimators.

### Suggestions for Improvement
We recommend that the authors improve the paper's structure and clarity to better highlight the main contributions. Additionally, we suggest focusing on fewer contributions to allow for more thorough support of each. The experimental section should be expanded to include larger datasets to validate claims of memory efficiency. We also advise addressing the inaccuracies in Section 3.2 by discussing alternative optimization methods for discrete latent variables, such as score function estimators, and including relevant literature in the related work section. Furthermore, we encourage the authors to clarify how FID scores are computed and to provide comparisons with contemporary methods like the Gumbel-Softmax and NES strategies.