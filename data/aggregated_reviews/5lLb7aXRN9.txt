ID: 5lLb7aXRN9
Title: Unconditional stability of a recurrent neural circuit implementing divisive normalization
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 7, 5, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the stability properties of the ORGaNICs model, a neurobiologically plausible recurrent neural network (RNN) that implements divisive normalization (DN) through recurrent excitation. The authors demonstrate the local stability of ORGaNICs under specific conditions, asserting that it is unconditionally stable under mild constraints. This significant finding is supported by extensive numerical tests, allowing the model to be trained using na√Øve backpropagation through time (BPTT) without gradient clipping, and performing comparably to long short-term memory (LSTM) networks. The authors provide empirical evidence of ORGaNICs' performance on standard machine learning tasks, suggesting that it can effectively learn long-term dependencies while mitigating issues related to vanishing and exploding gradients. Additionally, the dynamic implementation of normalization is highlighted as a novel contribution in machine learning.

### Strengths and Weaknesses
Strengths:
- The paper contributes to understanding the relationship between biological neural networks and learning properties, particularly emphasizing the role of divisive normalization in stabilizing neural dynamics.
- The theoretical stability analysis is sophisticated and offers significant insights, while the empirical results are robust and well-presented.
- The demonstration of unconditional stability in ORGaNICs is crucial for effective training and represents a noteworthy advancement in the field.
- The authors have addressed many reviewer concerns and improved the manuscript based on feedback.

Weaknesses:
- The introduction of the dynamics lacks sufficient intuitive explanations, and key terms related to dynamical systems may not be adequately introduced for the NeurIPS audience.
- The tone of the paper has been criticized as overly promotional, detracting from its academic rigor.
- Some claims regarding the model's stability and its implications for gradient issues are imprecise, and comparisons to existing architectures could be better articulated.
- Limitations regarding the proof of stability for general recurrent weight matrices and the reliance on empirical evidence have been highlighted.
- The exposition is considered dense and could benefit from clearer synthesis and organization.
- The scope of the experiments is limited, and the paper does not sufficiently address potential limitations or generalizability of the findings.

### Suggestions for Improvement
We recommend that the authors improve the introduction by providing more intuitive explanations of the dynamics and connecting them to existing RNN architectures. Introducing the Lyapunov energy earlier could enhance understanding. Additionally, we suggest that the authors clarify the definitions of terms like "Lyapunov diagonally stable," "Z-matrix," and "M-matrix" to make their results more accessible.

To strengthen the claims regarding stability and gradient issues, we advise the authors to refine their statements and ensure that comparisons to other models, such as SSNs, are precise and well-supported. We also recommend improving the tone of the paper to foster a more collaborative narrative, avoiding overly critical comparisons with other models. Specifically, consider rephrasing sections in the Related Work to acknowledge contributions from other research without undermining their significance.

Enhancing clarity in the main text by providing more intuitive explanations of technical terms and ensuring that the exposition is more straightforward would be beneficial. Finally, we encourage the authors to conduct hyperparameter tuning for comparisons with other models to validate their results more robustly and to engage in a more honest discussion of the limitations of their work, including potential computational complexities and the implications of the model's assumptions on biological plausibility.