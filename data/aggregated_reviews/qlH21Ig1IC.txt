ID: qlH21Ig1IC
Title: Adaptive Proximal Gradient Method for Convex Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents refined algorithms for convex optimization, specifically enhancing the analysis of Adaptive Gradient Descent (AdGD) to accommodate larger step sizes by leveraging local curvature rather than global smoothness. The authors propose multiple variants, including ProxAdGD, which extends the method to non-differentiable convex functions. The paper includes theoretical guarantees and numerical experiments demonstrating the advantages of these algorithms over existing methods, particularly Armijo's line search.

### Strengths and Weaknesses
Strengths:  
1. The paper effectively provides intuition and examples, enhancing reader understanding beyond technical results.  
2. Improved guarantees that exploit local properties of the objective are beneficial in both deterministic and stochastic settings.  
3. The writing is clear and accessible, with a focus on explaining the rationale behind the methods.  

Weaknesses:  
1. A more detailed comparison with the results of [MM20] is necessary to clarify whether the improvements are merely in constants or indicate fundamentally different behavior.  
2. The absence of experiments comparing step size choices with [MM20] limits the quantification of improvements.  
3. The focus is restricted to convex problems, which may overlook potential applications in non-convex settings.  
4. Some theorem statements and numerical experiment discussions lack precision and completeness.

### Suggestions for Improvement
We recommend that the authors improve the comparison with [MM20] by including a detailed analysis of the step sizes and their implications. Additionally, we suggest conducting experiments that directly compare the step sizes of their algorithms with those of [MM20] to substantiate claims of improvement. It would also be beneficial to clarify the implications of their results in non-convex contexts and to enhance the precision of theorem statements and numerical experiment discussions for better clarity.