ID: zEoP4vzFKy
Title: Automated Classification of Model Errors on ImageNet
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 7, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a pipeline to automatically categorize ImageNet classification misclassifications, building on previous work that defined four types of misclassifications. The authors report an accuracy of 60% based on human annotations of model misclassifications and assert that their pipeline achieves a 73% agreement with human annotations on the remaining samples, abstaining from classifying 18% of samples. The analysis categorizes errors into six types, revealing critical model failures and contextualizing results with prior literature. The authors evaluate over 100 models, providing insights into error distributions and trends, and report similar results for a different model, Greedy Soups, indicating a consistent trend in error categorization. While the authors acknowledge the limitations of manual labeling, they position their work as a scalable alternative for analyzing model errors.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and easy to follow, largely relying on previous work.
- The quality and presentation of the analysis are excellent, with full code provided for reproducibility.
- The sequential categorization of errors is a valuable approach, and the analysis builds upon prior research effectively.
- The pipeline is consistent and repeatable, providing a structured approach to error categorization.
- High agreement rates with human annotations (73% and 75% for different models) demonstrate the effectiveness of the classification method.
- The authors acknowledge the limitations of manual labeling, positioning their work as a scalable alternative.

Weaknesses:
- The novelty of the paper is minimal, as the definitions and categorizations of misclassifications have been established in prior work. The motivation for this categorization is not clearly articulated.
- The analysis is limited to one model and one dataset, which restricts the generalizability of the findings. Only 378 samples were tested, raising concerns about the robustness of the conclusions.
- The authors primarily compare their work with one previous study, lacking broader comparisons with other datasets and classifiers.
- The claim of a fully automatic error classification pipeline is overstated, as many categorizations rely on manual annotations from prior studies.
- Concerns remain regarding the reliance on manually obtained multi-label annotations, which may limit scalability.
- The analysis appears limited, particularly in its applicability to datasets beyond ImageNet, and some reviewers express doubts about the overall practicability of the approach, especially in addressing fine-grained failures.

### Suggestions for Improvement
We recommend that the authors improve the motivation section to clearly articulate the need for their categorization of misclassifications and how it advances the field. Expanding the analysis to include multiple models and datasets would enhance the generalizability of their findings. Additionally, we suggest that the authors provide a more detailed discussion of the limitations of their pipeline, particularly regarding its reliance on manual annotations. Clarifying the design choices for detecting fine-grained out-of-vocabulary mistakes and providing more illustrative examples would strengthen the paper. We also recommend improving clarity regarding the reliance on manually obtained multi-label annotations in their error categorization. Applying the proposed pipeline to datasets beyond ImageNet, potentially utilizing the ReLabel approach, could enhance the novelty and impact of the work. Addressing the scalability concerns more thoroughly and demonstrating the pipeline's effectiveness across diverse datasets would further strengthen the paper's contributions. Finally, we encourage the authors to extend their comparisons to other relevant works and datasets to better contextualize their contributions.