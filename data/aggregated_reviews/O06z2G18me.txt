ID: O06z2G18me
Title: Evaluating the Moral Beliefs Encoded in LLMs
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 7, 8, 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the moral beliefs encoded in large language models (LLMs) through the creation of the MoralChoice dataset, which includes approximately 1300 moral scenarios with varying ambiguity. The authors propose a systematic survey-based framework utilizing three novel metrics: semantic likelihood, consistency, and uncertainty, to evaluate 24 mainstream LLMs. The findings indicate that while LLMs can engage in commonsense moral reasoning, their consistency and certainty vary significantly, underscoring the necessity for diversifying human alignment data or providing explicit moral guidance during training. Additionally, the paper explores how pre-trained models like ada, babbage, curie, and davinci respond to in-context examples, with plans for an ablation study on a low ambiguity dataset. The authors propose conducting experiments in zero-shot, five-shot with annotator labels, and ten-shot with random labels scenarios. They also address the importance of preserving annotator well-being and plan to pursue IRB approvals. Findings from additional decoding ablation studies reveal changes in semantic consistency and average conditional semantic entropy (A-CSE) across various decoding techniques, while acknowledging the limitations of their data creation process and the implications of overriding model refusals to answer.

### Strengths and Weaknesses
Strengths:
- The paper addresses the critical topic of moral beliefs in LLMs and introduces a novel dataset.
- It provides an extensive comparison of various open-source and commercial LLMs, contributing valuable insights to the field.
- The metrics for semantic consistency and average conditional semantic uncertainty (A-CSU) are well-defined and relevant.
- The authors' commitment to incorporating additional experiments and addressing limitations enhances the paper's depth.
- The detailed analysis of decoding methods and their impact on semantic consistency provides valuable insights.
- The acknowledgment of ethical considerations regarding sensitive data and annotator well-being is commendable.

Weaknesses:
- The annotation process for the dataset lacks clarity, including details about the annotators' demographics and the justification for their selection.
- The ethical implications of the scenarios presented require a dedicated discussion section.
- The authors' choice of a temperature of 1 for sampling leads to non-deterministic behavior, which is not adequately justified and may impact the results significantly.
- The discussion of results is limited, necessitating a more extensive analysis to enhance the study's impact.
- The need for a more thorough discussion on the implications of overriding model refusals to answer is highlighted.
- The dataset creation process lacks comprehensiveness, and the representativeness of annotators is questioned.
- The findings raise concerns about the generalizability of the models' responses, particularly in high-ambiguity scenarios.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the annotation process by providing detailed information about the annotators and justifying their demographic representation. Additionally, we suggest including a dedicated section discussing the ethical implications of the scenarios used. The authors should justify their choice of sampling temperature and consider exploring different sampling configurations to strengthen their findings. We also recommend that the authors improve the discussion surrounding the implications of overriding model refusals to answer, as this significantly influences audience interpretation. Furthermore, we suggest that the authors extensively report and analyze the refusal rate for each model when presenting vanilla questions. It would also be beneficial to clarify what behaviors from the models may be concerning from a utility perspective and what further insights can be drawn from the work. Finally, we encourage the authors to include all additional experiment results and analyses conducted during the rebuttal phase in the final version.