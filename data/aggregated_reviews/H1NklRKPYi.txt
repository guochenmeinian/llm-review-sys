ID: H1NklRKPYi
Title: LCM: Locally Constrained Compact Point Cloud Model for Masked Point Modeling
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 8, 6, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a locally constrained compact point cloud model (LCM) that includes a locally constrained compact encoder and a locally constrained Mamba-based decoder. The encoder substitutes the self-attention layer with a local aggregation layer, achieving a balance between performance and efficiency. The decoder addresses varying information densities in masked and unmasked patches. Extensive experiments demonstrate that LCM significantly outperforms existing Transformer-based models in both performance and efficiency.

### Strengths and Weaknesses
Strengths:  
- The novelty of the LCM model is evident, addressing shortcomings in Transformer-based point mask modeling and significantly enhancing self-supervised learning techniques.
- The theoretical analysis and experimental results are detailed, allowing for substantial improvements in existing self-supervised methods.
- The paper is well-organized, with clear tables, figures, and notations, and offers a thorough explanation of the methodology.

Weaknesses:  
- Some figure illustrations lack clarity, particularly the small fonts in Figures 4 and 5.
- Sections 3.3 and 3.4 require clearer interpretation, and Equation 4-7 needs a different explanation.
- The experimental comparisons are insufficient, particularly regarding the performance of PointGPT-L and the limited dataset size of ShapeNet.
- The reliance on static local constraints may limit the model's ability to dynamically identify important regions in point clouds.

### Suggestions for Improvement
We recommend that the authors improve the clarity of figure illustrations, particularly by adjusting the font sizes in Figures 4 and 5. Additionally, we suggest providing a more detailed interpretation of Sections 3.3 and 3.4, and clarifying the analysis of Transformer and Mamba network parameters. To strengthen the experimental results, we encourage the authors to compare their method with PointGPT-L and to test the model on larger datasets to demonstrate its generalization ability. Lastly, addressing the limitations regarding dynamic importance and long-range dependency modeling would enhance the model's applicability in various tasks.