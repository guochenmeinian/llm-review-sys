ID: BOP5McdqGy
Title: Uncovering and Quantifying Social Biases in Code Generation
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates social biases in code generation models, marking a pioneering effort in this area. The authors propose a method for constructing biased prompts, develop evaluation metrics, and introduce a classifier to quantify biases in generated code. They conduct extensive experiments and human annotations to validate their findings, revealing significant biases in the models tested.

### Strengths and Weaknesses
Strengths:
- This is among the first systematic studies addressing social biases in code generation.
- The authors provide a comprehensive framework, including prompt construction, dataset creation, evaluation metrics, and analysis, establishing a foundation for future research.
- The annotated dataset is a valuable resource for subsequent studies.

Weaknesses:
- The methodology relies on biased prompts, which may yield predictable results, reflecting input bias rather than inherent model bias. The focus on 'direct bias' overlooks the more critical issue of 'unintentional bias' in real-world applications.
- The analysis oversimplifies demographic categories, limiting the study's comprehensiveness. For instance, the UnFairness Score is restricted to binary demographics, which may not adequately represent multi-class scenarios.
- The artificial nature of the dataset raises concerns about its applicability to real-world code generation contexts, potentially leading to inconclusive findings.

### Suggestions for Improvement
We recommend that the authors improve the exploration of unintentional and context-dependent biases to provide a more nuanced understanding of bias in code generation. Additionally, consider expanding the demographic categories beyond binary classifications to enhance the study's applicability. We suggest including a discussion of the limitations, particularly regarding the dataset and the practical implications of the findings. Finally, we encourage the authors to conduct experiments with GPT-3.5 and GPT-4 to assess the impact of RLHF on social biases in code generation models.