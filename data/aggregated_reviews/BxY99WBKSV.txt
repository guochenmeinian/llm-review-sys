ID: BxY99WBKSV
Title: Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, 4

Aggregated Review:
### Key Points
This paper addresses the issue of ties in the meta-evaluation of machine translation (MT) metrics, specifically examining the shortcomings of Kendall’s τ. The authors propose a new version of pairwise accuracy that accounts for ties and introduce a tie calibration procedure to facilitate fair comparisons between metrics that do and do not predict ties. Experiments conducted on the Multidimensional Quality Metrics (MQM) ratings dataset from the WMT22 metrics shared task for three language pairs (English-German, Chinese-English, and English-Russian) demonstrate that the proposed method leads to fairer ranking-based assessments of metric performance.

### Strengths and Weaknesses
Strengths:
- The paper is clear, well-written, and addresses an important problem in MT meta-evaluation.
- It provides convincing motivating examples that highlight the flaws of existing metrics, particularly Kendall’s τ.
- The proposed methods are easy to adopt within the MT community and offer improvements over baseline metrics.

Weaknesses:
- The evidence for a "fair" comparison between different metrics is difficult to grasp.
- The significance of the tie problem may be questionable, raising concerns about its impact on metrics comparison.
- Limited analysis is conducted across only three language pairs, and the calibration method does not generalize across dissimilar datasets.

### Suggestions for Improvement
We recommend that the authors improve the introduction of the tie problem by including real examples earlier in the paper to aid understanding for readers unfamiliar with the concept. Additionally, we suggest exploring the use of relative scores in the tie calibration algorithm, as this could provide further insights and results. Finally, consider expanding the empirical analysis to include more language pairs and annotation methods beyond the MQM framework to enhance the robustness of the findings.