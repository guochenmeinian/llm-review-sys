ID: tVO3b68Oyp
Title: SyllableLM: Learning Coarse Semantic Units for Speech Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 3, 8, 6, 3, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a two-stage speech language model utilizing semantic and acoustic tokens, inspired by AudioLM. The authors propose a speech tokenizer that generates semantic tokens through an initial segmentation based on syllable-like structures from HuBERT, followed by an iterative refinement process. Acoustic tokens are derived from HuBERT-based tokens, termed "mHuBERT." Experimental results indicate improvements in unsupervised syllable segmentation, lower speech reconstruction WER, competitive accuracy in speech language modeling tasks, and enhanced speech continuation quality.

### Strengths and Weaknesses
Strengths:
- Originality: The paper introduces a novel method for unsupervised syllable-like segmentation using conditional probabilities from a masked language model.
- Quality: The experimental design is robust, with ablation studies providing valuable insights into modeling choices.
- Clarity: Results are presented in an interpretable manner.
- Significance: The model demonstrates competitive performance with reduced computational costs.

Weaknesses:
- Readability: The paper requires substantial revisions for clarity, with issues such as references to unintroduced items and confusing terminology.
- Confusing equation: The equation defining the similarity matrix lacks clarity, making it difficult to compute certain values.
- Writing style: The overall writing is wordy and disorganized, hindering comprehension.

### Suggestions for Improvement
We recommend that the authors improve the paper's readability by ensuring all terms and concepts are clearly defined before use. Specifically, clarify the references to "our loss" and the "similarity matrix" to avoid confusion. Additionally, we suggest refining the terminology around "pretraining" and "agglomeration" to prevent ambiguity. The authors should also simplify the writing style, using concise sentences and formal notations where appropriate. Finally, we encourage the authors to provide a more comprehensive demonstration of efficiency, including visualizations of GPU training time versus performance and bitrate versus unit quality.