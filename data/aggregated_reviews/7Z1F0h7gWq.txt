ID: 7Z1F0h7gWq
Title: Learn and Consolidate: Continual Adaptation for Zero-Shot and Multilingual Neural Machine Translation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a two-stage method for continual learning in multilingual neural machine translation (MNMT), aimed at efficiently updating models with limited new data. The authors propose a learning stage to encode new knowledge into a module, followed by a consolidation stage to integrate this knowledge into the original model. The experiments demonstrate improved performance in both supervised and zero-shot translation tasks compared to strong baselines.

### Strengths and Weaknesses
Strengths:
- The approach is novel within the continual learning domain for MNMT, enhancing both zero-shot and supervised translations without increasing model size.
- The results are promising, showing significant improvements over existing methods.
- The experimental design is thoughtful, with a comprehensive literature review and robust comparisons of various baselines.

Weaknesses:
- The paper lacks clarity in several areas, particularly regarding technical details and the necessity of the proposed two-stage strategy.
- Some claims are over-stated, such as the assertion of no additional parameters being introduced during the learning stage.
- The selection of baselines is questionable, with some being unrelated to continual learning.
- The ablation study is insufficient, lacking comparisons with other tuning methods and further exploration of the model's weight changes.

### Suggestions for Improvement
We recommend that the authors improve clarity in Section 3.3 by explicitly explaining how to derive equation 9 from equation 8, defining $\theta_f^*$, clarifying the meaning of ‘i’, and specifying which parameters are used during inference. Additionally, the authors should reconsider the baseline selections, incorporating methods from [1] and [2] that are relevant to continual learning. More experiments should be included in the ablation study to justify the choice of FFN as the learned module and to compare it with adapter-based, LoRA-based, and prefix tuning methods. Finally, we suggest examining more language pairs beyond de-zh to strengthen the findings.