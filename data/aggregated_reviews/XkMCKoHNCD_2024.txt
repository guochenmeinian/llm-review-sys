ID: XkMCKoHNCD
Title: A test of stochastic parroting in a generalisation task: predicting the characters in TV series
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 3, 2, 1, 3
Original Confidences: 4, 5, 5, 4

Aggregated Review:
### Key Points
This paper presents an analysis of logistic regression on sentence embeddings to predict the speaker of dialogue lines in *The Big Bang Theory*. The authors fit a PCA model to embeddings from a sentence transformer, using PCA dimensions as linear features, and compare classification accuracy with GPT-4 and a limited user study. The study aims to explore whether LLMs exhibit true generalization or merely stochastic parroting.

### Strengths and Weaknesses
Strengths:  
The authors tackle an important debate in the AI community regarding LLM capabilities, providing thorough descriptions of their methods and extensive qualitative analysis of PCA features. The writing is clear, and the background for the methods is well-established.

Weaknesses:  
The research question lacks clarity and focus, with significant portions of the analysis appearing specific to *The Big Bang Theory* rather than addressing the broader implications for LLM evaluation. The experimental design is exploratory and does not clearly define "stochastic parrots" versus "general intelligence." The methods are well-known but lack proper literature references, and the user study is limited in scope, involving only two participants related to the authors.

### Suggestions for Improvement
We recommend that the authors improve the clarity and focus of their research question, ensuring a stronger connection between their findings and the initial framing of LLM evaluation. Justifying the selection of datasets and characters, as well as addressing potential biases, would strengthen the analysis. Expanding the user study to include a larger and more diverse participant pool, ideally without prior exposure to the show, would enhance the reliability of results. Additionally, we suggest that the authors refine their methodology by reviewing relevant literature on linear probes and causal probes, and by defining key concepts more explicitly. Finally, the authors should consider revising the writing to adhere to common academic conventions, omitting excessive detail on well-known methods.