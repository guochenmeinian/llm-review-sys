ID: jS34QpqdWs
Title: Nonstationary Sparse Spectral Permanental Process
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 7, 6, -1, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to modeling permanental processes through a Nonstationary Sparse Spectral Permanental Process (NSSPP) and its deep kernel variant (DNSSPP). The authors aim to address key challenges in existing models, such as the cubic complexity of exact GP inference, the need for specific kernel types, and the limitations of shallow kernels. The proposed methods utilize a sparse spectral representation to enhance computational efficiency and allow for nonstationary kernels, while the deep kernel variant incorporates hierarchically stacked spectral feature mappings to improve expressiveness. Experimental results demonstrate competitive performance on both stationary and nonstationary datasets.

### Strengths and Weaknesses
Strengths:
- The integration of sparse spectral representation with nonstationary kernels is a novel contribution to the literature.
- The introduction of the deep kernel variant (DNSSPP) significantly enhances model expressiveness.
- The paper is well-structured, clearly articulating motivations, methodology, and findings.
- Experimental results effectively showcase the advantages of the proposed methods compared to baselines.

Weaknesses:
- The contribution may not be substantial enough compared to prior work, particularly the GSSPP model.
- The approximation method based on Bochner's Theorem appears to be a minor change from GSSPP, raising questions about the novelty.
- The rationale behind baseline selections in experiments lacks clarity, particularly regarding the number of inducing points used.
- The deep kernel formulation complicates the analytic computation of the intensity integral, necessitating numerical integration.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the rationale behind baseline selections, particularly regarding the number of inducing points used in comparisons. Additionally, we suggest including a performance comparison with a baseline that employs stacked mappings of stationary kernels to better ascertain the source of performance improvements. Addressing the limitations of the deep kernel formulation in terms of overfitting and discussing relevant literature on this topic would also enhance the paper's depth. Finally, we encourage the authors to clarify the computation of the marginal likelihood for DNSSPP, as details are currently lacking.