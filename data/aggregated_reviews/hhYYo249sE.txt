ID: hhYYo249sE
Title: Subgraph Federated Unlearning
Conference: ACM
Year: 2024
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework for subgraph federated unlearning, termed ReGenUnlearn, which integrates a Reinforced Federated Policy Sampler (RFPS) and Parameter-free Graph Prompt Knowledge Distillation (PGPKD). The RFPS employs reinforcement learning to sample subgraphs effectively, mitigating cross-client interference while maintaining model utility. The PGPKD enhances the unlearning process by encoding essential graph knowledge into prompts. Experimental results across four datasets demonstrate the framework's superior performance compared to existing methods.

### Strengths and Weaknesses
Strengths:
- The authors address the important problem of unlearning in subgraph federated learning.
- The paper is well-structured and clearly articulates its ideas.
- Extensive experiments validate the proposed framework's effectiveness and robustness.

Weaknesses:
- The problem definition lacks clarity, particularly regarding overlapping subgraphs and the membership status of shared nodes post-unlearning.
- Current experiments do not sufficiently demonstrate unlearning efficacy on cross-client subgraphs; empirical evidence, such as a membership inference attack, is needed.
- The efficiency analysis appears unfair, as it does not account for the pre-training time of the RFPS in reported runtimes.
- The experiments are limited to one GNN model, necessitating results from additional models for generalizability.
- The manuscript's articulation requires improvement for better readability, including correcting notation inconsistencies.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the problem definition, specifically addressing the membership status of overlapping nodes after unlearning. Additionally, provide empirical evidence demonstrating the unlearning efficacy on cross-client subgraphs, such as through a membership inference attack. We suggest including the pre-training time of the RFPS in the efficiency analysis and conducting experiments with multiple GNN models to enhance generalizability. Furthermore, we encourage the authors to refine the manuscript's articulation for improved coherence and readability.