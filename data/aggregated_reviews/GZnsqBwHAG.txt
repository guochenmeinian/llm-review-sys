ID: GZnsqBwHAG
Title: Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 7, 8, 4, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel concept called the “safety landscape,” which evaluates the safety of generative language models through the introduction of the “safety basin” and a new metric named “Visage.” The authors demonstrate how Visage can assess model robustness against malicious fine-tuning using experimental results in both 1D and 2D safety landscapes across various open-source language models. The findings suggest that the safety landscape can help in constructing more secure models. Furthermore, the paper analyzes the differing landscapes of LLM capability and safety, emphasizing that their trends are not aligned. The authors propose that the safety basin has a sharper curvature than the capability basin, indicating that safety decreases more rapidly under perturbations. The comparison between the Base model and the Chat model illustrates the changes in safety due to different training methods, highlighting that the Chat model, trained with multiple safety alignment methods, is safer than the Base model, which is generally unsafe.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and introduces a novel approach to model safety, particularly through the concepts of safety landscape and safety basin.  
- The Visage metric is valuable for developing models resistant to malicious fine-tuning, especially for vulnerable open-source models.  
- Experimental validation in 1D and 2D safety landscapes effectively demonstrates the utility of Visage.  
- The paper identifies a unique characteristic of the LLM safety landscape, providing strong evidence of its distinct behavior compared to capability.  
- The comparison between the Base and Chat models effectively demonstrates the impact of training methods on safety.

Weaknesses:  
- The term “safety” is broadly used, primarily focusing on a model's refusal to answer harmful queries, which oversimplifies the concept. The evaluation method using refusal keyword detection has limitations, as safe responses can vary by context.  
- The discussion on limitations lacks depth, failing to address context-dependent unsafe responses and the implications of the refusal keyword detection mechanism.  
- The paper has conceptual clarity issues, particularly in distinguishing between loss landscapes and the safety landscape, which could confuse readers.  
- The analysis may benefit from further clarification on how different “templates” affect safety across models.  
- Additional experiments could strengthen the findings and support the conclusions drawn.

### Suggestions for Improvement
We recommend that the authors improve the discussion on limitations by elaborating on the context-dependent nature of safety responses and the shortcomings of the refusal keyword detection mechanism. Additionally, clarifying the distinction between loss landscapes and the safety landscape throughout the paper is essential. We also suggest improving the clarity regarding the influence of different “templates” on safety outcomes. To enhance the experimental rigor, we recommend measuring additional capabilities beyond harmful question answering, such as toxic content generation and bias, and using alternative evaluation metrics like LLamaGuard for a more comprehensive analysis of safety. Finally, incorporating more experiments could further validate the findings and enhance the robustness of the conclusions presented.