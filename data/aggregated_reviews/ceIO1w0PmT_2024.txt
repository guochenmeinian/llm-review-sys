ID: ceIO1w0PmT
Title: OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents OmniJARVIS, an instruction-following agent for open-world Minecraft, which utilizes a behavior encoder to generate behavior tokens based on textual, visual, and action inputs through self-supervised learning. The authors propose a novel behavior tokenizer that encodes behavior trajectories into compact representations, allowing for effective modeling with other modality tokens via autoregressive transformers. The experiments demonstrate that OmniJARVIS outperforms baselines in atomic, programmatic, and open-ended tasks, showcasing its potential for complex long-horizon tasks.

### Strengths and Weaknesses
Strengths:
- The introduction of a quantized action codebook and the use of Finite Scalar Quantization (FSQ) for behavior tokenization represent a novel approach to representing sub-goals.
- OmniJARVIS shows significant performance gains over strong baselines like DEPS and GROOT, particularly in long-horizon tasks and open-ended instruction following.
- The paper is generally clear and well-organized, with informative illustrations that enhance readability.
- The proposed approach demonstrates scalability and potential applications beyond Minecraft.

Weaknesses:
- The impact of the new dataset compared to the proposed architecture is unclear, necessitating further analysis to isolate their effects.
- The paper lacks comprehensive ablations to validate the contributions of individual components, such as FSQ and the curated dataset.
- Important details regarding the encoder and policy decoder architectures are insufficiently described.
- The paper does not adequately discuss limitations or analyze agent behavior and failure modes.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the individual contributions of the dataset and architecture, as well as their combined impact. More comprehensive ablations should be conducted to demonstrate the utility of the proposed architecture and the new dataset, including comparisons against multimodal LLM training methods like QFormer or Perceiver. Additionally, the authors should provide more detail in the ablations presented in Table 6, clarify the training settings, and include discussions on the failure modes of OmniJARVIS. Lastly, a separate "Limitations" section should be added to address the generalizability of the approach beyond Minecraft.