ID: IdXpzsTWRs
Title: StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents STORYANALOGY, a novel dataset comprising approximately 24,000 annotated story pairs aimed at advancing story-level analogical reasoning in AI systems. The authors argue that previous research has primarily focused on word-level analogies, leaving a gap in understanding story-level analogies, which involve complex narrative comparisons. The dataset is generated using large language models (LLMs) and human annotations, and it includes an evaluation framework based on Structure Mapping Theory, assessing entity and relational similarity. Empirical results reveal that current models significantly underperform compared to human capabilities in identifying and generating story analogies.

### Strengths and Weaknesses
Strengths:
- The paper introduces a substantial resource for story-level analogies, enhancing research in this under-explored area.
- It provides a rigorous evaluation methodology and comprehensive empirical results, demonstrating the limitations of existing models.
- The dataset creation process is well-documented, and the annotation guidelines are clear.

Weaknesses:
- The reliance on pre-trained off-the-shelf encoder models as baselines is not convincingly justified, raising concerns about their adequacy.
- Important implementation details are relegated to the appendix, which detracts from the paper's clarity.
- The use of a proprietary tool for analogy generation limits replicability and transparency in the methodology.
- The evaluation metric α lacks sufficient justification, and the paper does not adequately address potential societal biases in the dataset.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by incorporating essential implementation details directly into the main text rather than the appendix. Additionally, addressing the justification for using α as a metric for evaluation is crucial. The authors should also consider expanding the dataset to include more diverse domains to enhance its applicability. Furthermore, we suggest providing a more detailed ethics statement regarding potential biases in the dataset. Finally, improving the presentation of figures and tables, including font size and explanations, would enhance readability and comprehension.