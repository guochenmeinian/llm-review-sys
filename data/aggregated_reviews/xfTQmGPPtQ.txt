ID: xfTQmGPPtQ
Title: Parameter-efficient Tuning for Large Language Model without Calculating Its Gradients
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel parameter-efficient fine-tuning (PEFT) method for large language models (LLMs) that significantly reduces memory usage while maintaining competitive performance. The authors propose first training the PEFT modules on a smaller language model (SLM) and then adapting these modules to the LLM using a linear projection and a bridge model. The method demonstrates up to 5.7x memory reduction on T5 and GPT-2 models, supported by extensive experiments on SuperGLUE tasks.

### Strengths and Weaknesses
Strengths:
- The proposed method effectively addresses excessive memory usage during fine-tuning, achieving comparable performance while significantly reducing memory footprint.
- Extensive experiments provide solid support for the main claims, demonstrating the method's effectiveness across different model families.

Weaknesses:
- Important baselines, such as LST, BBT, and BBTv2, are missing from the comparisons, raising concerns about the robustness of the findings.
- The use of '0% #params' in tables is misleading; the authors should clarify the total trainable parameters instead.
- The writing lacks clarity, and the experimental settings are underspecified, with no error bars presented for results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing and ensure that the description of the parameter updates accurately reflects the methodology. Specifically, please clarify the use of '0% #params' in the tables and provide the total number of trainable parameters. Additionally, we suggest including error bars in the experimental results to account for variance, and refining the presentation of figures and tables for better readability. Finally, consider adding comparisons with missing important baselines to strengthen the paper's claims.