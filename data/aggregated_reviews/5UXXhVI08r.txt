ID: 5UXXhVI08r
Title: Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 5, 6, 7, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a dynamic prompt learning approach for image editing that modifies self-attentions to better focus on relevant nouns in text prompts. The authors utilize null text inversion alongside dynamic tokens updated with background leakage loss, disjoint object attention loss, and attention balancing loss. The approach is particularly effective for editing images with multiple semantically related objects, as demonstrated through a test set of 60 multi-object images from LAION. The authors report improvements in qualitative and some quantitative evaluations.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written, clearly explaining the method and related works.
2. A novel dynamic prompt learning method is proposed, addressing cross-attention leakage in images with multiple semantically related objects.
3. The authors provide a test set for evaluations, showcasing the method's superiority through examples.

Weaknesses:
1. The scope of the proposed method is narrow, primarily benefiting cases with two or more semantically related objects and specific word-swapping prompts.
2. Limited examples raise doubts about the method's general applicability to broader image editing problems or other prompt types.
3. The small size of the test set and the marginal improvements in quantitative evaluations weaken the conclusions drawn.
4. The manuscript lacks comprehensive comparisons with other baseline models and does not adequately address the limitations or potential societal impacts of the work.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their evaluation datasets and editing prompts, specifically distinguishing between the URS-Set and MO-Set. Expanding the test set to include a wider variety of editing prompts and examples would enhance the robustness of the findings. Additionally, conducting evaluations on scenarios with single or unrelated objects, and comparing results with other baselines such as instructpix2pix and plug-and-play, would provide a more comprehensive assessment of the method's effectiveness. We also suggest including a quantitative evaluation of object masking performance and clarifying the contributions of each component in the proposed approach through ablation studies. Lastly, addressing the limitations and potential negative impacts of the method would strengthen the overall presentation.