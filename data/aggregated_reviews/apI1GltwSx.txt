ID: apI1GltwSx
Title: DELT: A Simple Diversity-driven EarlyLate Training for Dataset Distillation
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 5, 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an EarlyLate training scheme aimed at enhancing image diversity in dataset distillation by partitioning predefined IPC samples into smaller subtasks and optimizing them locally. The authors propose a curriculum learning approach that distills easier samples first and gradually incorporates harder ones, leading to improved distillation performance. The experiments demonstrate significant advancements over existing methods, particularly in addressing the diversity challenges faced by batch-to-global matching algorithms.

### Strengths and Weaknesses
Strengths:
1. The EarlyLate training scheme effectively enhances the diversity of synthetic images, providing a novel approach that can inspire future work.
2. The method reduces computational load compared to batch-to-global matching methods.
3. Comprehensive experiments validate the method's superiority across various performance metrics and architectures.

Weaknesses:
1. The contribution of the work is incremental compared to previous methods.
2. The motivation and advantages of the "Selection Criteria" in the initialization approach are unclear, particularly regarding the ranking presented in Fig 5.
3. The paper involves numerous hyperparameters, but lacks guidance on their tuning and principled approaches.
4. The impact of the initialization method is inadequately explored, with only CDA+init shown in the ablation study.
5. Performance comparisons with other state-of-the-art methods on MobileNet-v2 are missing in Table 1.
6. The reliance on training tricks like random crop in methods such as SRe2L is not sufficiently addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation and advantages of the "Selection Criteria" in the initialization approach, particularly in relation to Fig 5. Additionally, the authors should provide a detailed discussion on hyperparameter tuning and consider including more advanced initialization methods in the ablation study. It is crucial to present performance comparisons with other state-of-the-art methods on MobileNet-v2 in Table 1. Furthermore, the authors should clarify the extent to which their method relies on training tricks like random crop. Lastly, we suggest polishing the writing in Section 3 to enhance clarity and understanding of the proposed methods.