ID: fn0pQWkFsP
Title: SBI-RAG: Enhancing Math Word Problem Solving for Students through Schema-Based Instruction and Retrieval-Augmented Generation.
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 7, 6, 4
Original Confidences: 4, 2, 4

Aggregated Review:
### Key Points
This paper presents a novel framework, Schema-Based Instruction Retrieval-Augmented Generation (SBI-RAG), designed to enhance students' abilities to solve math word problems (MWPs) by integrating schema-based instruction with large language models (LLMs). The authors propose a structured methodology that includes schema classification, context retrieval, and LLM-based solution generation, evaluated using the GSM8K dataset. A new "reasoning score" metric is introduced to assess the quality of solutions, emphasizing stepwise reasoning.

### Strengths and Weaknesses
Strengths:
- The integration of schema-based instruction with LLMs and retrieval-augmented generation is a novel and promising approach.
- The framework promotes structured reasoning, evaluated by the introduced reasoning score metric, which assesses the quality of stepwise reasoning.
- The paper is well-structured, with a clear methodology and comprehensive evaluation that supports claims of improved reasoning quality.

Weaknesses:
- The reliance on LLM-as-a-Judge for evaluation lacks direct human feedback, which could provide more nuanced insights into the framework's effectiveness.
- The evaluation is limited to the GSM8K dataset, focusing primarily on arithmetic word problems, raising questions about the framework's generalizability to more complex problems.
- The quality of retrieved documents significantly influences the generated solutions, and any limitations in the document set can affect reasoning quality.

### Suggestions for Improvement
We recommend that the authors improve the evaluation methodology by incorporating direct human feedback from educators and students to provide a more comprehensive assessment of the framework's effectiveness. Additionally, the authors should consider expanding the evaluation to include more complex problem datasets to assess the generalizability of the framework. It would also be beneficial to clarify the prompt templates used in both the proposed approach and the baselines, as this could significantly impact the evaluation outcomes. Finally, addressing the potential biases in the reasoning score metric, particularly regarding schema-related terms, would enhance the fairness of the evaluation.