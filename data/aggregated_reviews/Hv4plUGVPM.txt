ID: Hv4plUGVPM
Title: Using Expert Gaze for Self-Supervised and Supervised Contrastive Learning of Glaucoma from OCT Data
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: 6, 8, 4, -1
Original Confidences: 5, 4, 2, 4

Aggregated Review:
### Key Points
This paper presents a method that utilizes clinicians' gaze data to enhance glaucoma diagnosis from OCT images, addressing the challenge of limited data availability in medical computer vision. The authors propose the GazeFormer, a transformer-based model that generates embeddings from gaze data, which are then used for glaucoma detection through a self-supervised pretraining approach. The paper discusses the integration of gaze data with OCT images and evaluates the proposed method's performance.

### Strengths and Weaknesses
Strengths:  
- The paper tackles an important issue in medical imaging by addressing the scarcity of labeled data.  
- It provides a well-organized structure with sufficient experimental details and evaluations of each pipeline stage.  
- The GazeFormer model is an innovative approach to representation learning, inspired by BERT.

Weaknesses:  
- The paper is overly lengthy and complex, making it difficult to follow the relationship between its contributions.  
- There are significant gaps in clarity regarding the method's differentiation from existing approaches, generalizability, and robustness to gaze data variations.  
- Technical details about the GazeFormer model, such as architecture specifics and gaze data encoding, are insufficiently addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity and organization of the paper by separating it into two distinct papers to better articulate the contributions. Additionally, the authors should provide a more detailed overview of the limited data challenge, including typical data availability and how their method addresses this issue. It is crucial to include information on the generalizability of the proposed method, the statistical significance of accuracy improvements, and the robustness of the method to variations in gaze data quality. Furthermore, we suggest that the authors elaborate on the GazeFormer model's architecture, training process, and how gaze data's spatiotemporal relationships are retained. Lastly, insights into the performance of pseudo-labels compared to human annotations and the impact of varying gaze data amounts on classification accuracy should be included.