ID: xKYtTmtyI2
Title: Validated Image Caption Rating Dataset
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
The authors present the Validated Image Caption Rating (VICR) dataset, which comprises 68,217 ratings from 113 participants for 15,646 image-caption pairs. Utilizing a gamified approach for data collection, the dataset achieves higher inter-rater agreement compared to existing datasets like Flickr8K. A well-reasoned 5-point rating scale is introduced, considering accuracy, completeness, and context. The dataset demonstrates superior performance against previous metrics when evaluated with trained models such as ViLBERT. The authors also emphasize the importance of modeling the distribution of judgments for image-caption pairs rather than relying solely on rounded averages. They acknowledge the context-free nature of their dataset and express intent to incorporate contextual factors in future data collections. Additionally, the authors plan to enhance dataset documentation in line with the guidelines from *Datasheets for Datasets* (Gebru et al., 2021) and clarify the ground-truth captions and selection criteria for captions from various sources.

### Strengths and Weaknesses
**Strengths:**
- The dataset significantly improves upon Flickr8K in terms of inter-rater agreement and cardinality.
- The gamified data collection method is innovative and ethically sound, involving paid participants under institutional review.
- The rating scale is thoughtfully designed, taking multiple factors into account, and the entire rating process is well-documented.
- The dataset development process is well-structured, focusing on consistency among raters.
- Significant improvements to documentation and clarity have been made in response to feedback.
- The inclusion of qualitative visualizations enhances the dataset's usability.

**Weaknesses:**
- The dataset is context-free, which may limit its applicability in scenarios where context is crucial for evaluating caption quality, particularly for users with visual impairments.
- The reliance on rounded averages may obscure the nuances of individual judgments.
- There is a lack of detailed analysis regarding the usability of the dataset and the impact of the gamified approach on rating consistency.
- Insufficient details about the selection criteria for captions and dataset documentation were initially present.

### Suggestions for Improvement
The authors should consider incorporating contextual factors into the dataset to enhance its applicability, particularly for accessibility purposes. They are encouraged to improve the modeling of the distribution of judgments for each image-caption pair to capture the nuances of individual ratings. Additionally, the authors should engage more deeply with HCI literature related to low vision and blind users to enhance the accessibility of their image descriptions. They should elaborate on the selection criteria for captions and include a graph showing the number of captions per image in the dataset documentation. Furthermore, it would be beneficial to provide qualitative examples of image/caption pairs and analyze how length and linguistic diversity affect ratings. Lastly, exploring the potential for the automated rating predictor to filter out low-quality captions from existing datasets and conducting preliminary experiments demonstrating this capability would strengthen the findings. Expanding dataset comparisons to include other caption quality datasets would also enhance the overall contributions of the work.