ID: 9tUjsRLjf2
Title: Dynamic Regret of Adversarial Linear Mixture MDPs
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 3, 8, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of adversarial reinforcement learning (RL) within episodic linear mixture Markov Decision Processes (MDPs) featuring adversarial full-information rewards and stationary unknown transition kernels. The authors propose a novel algorithm that addresses adversarial rewards and derive both upper and lower bounds of dynamic regret, claiming optimality concerning the number of episodes \( K \) and the non-stationary measure \( P_T \). The work aims to fill gaps in previous literature by providing a theoretical foundation for RL in non-homogeneous MDPs.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel algorithm for adversarial RL without prior knowledge, with reasonable steps and detailed insights into each phase.
2. It provides an upper bound on dynamic regret that achieves optimal dependence on \( K \).
3. The analysis successfully derives both upper and lower bounds, demonstrating the algorithm's effectiveness in recovering static settings and improving adversarial reward regret for tabular MDPs.

Weaknesses:
1. The algorithm's applicability is limited to finite state cases, reducing its significance.
2. The proof of the lower bound lacks rigor, particularly in its division into two cases, which does not align with lower bound definitions.
3. The paper does not sufficiently engage with existing literature on non-stationary RL, particularly regarding the relationship with works like [1], which achieves optimal regret under different conditions.

### Suggestions for Improvement
We recommend that the authors improve the rigor of the lower bound proof by constructing a single hard instance that encompasses both cases. Additionally, the authors should provide a clearer analysis of the relationship between their findings and those of [1], particularly regarding the applicability of non-stationary RL in adversarial settings. It would also be beneficial to discuss the implications of the additional dependency on \( S_T \) and how it affects the regret bounds compared to existing works. Lastly, we suggest including a more comprehensive discussion of related works, particularly the omission of "Optimistic Policy Optimization is Provably Efficient in Non-stationary MDPs," to enhance the paper's context and contributions.