ID: kEzI6OYXV4
Title: Are All Steps Equally Important? Benchmarking Essentiality Detection in Event Processes
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents work on commonsense events, specifically focusing on essential step detection (ESD) within goal-oriented processes. The authors introduce a new dataset based on WikiHow, where steps are annotated for their essentiality, and evaluate various models to demonstrate the challenges in identifying essential steps. The study connects to cognitive science and highlights the limitations of language models (LMs) in capturing common sense knowledge.

### Strengths and Weaknesses
Strengths:
- The paper is clear, with a solid methodology and a well-motivated task of detecting essential events.
- It contributes a useful dataset for benchmarking and deepens understanding of event-related reasoning in LMs.
- The results indicate significant performance gaps between models and human capabilities, emphasizing the need for new evaluation datasets.

Weaknesses:
- The paper lacks references to previous work on commonsense knowledge and essentiality, which could provide context and depth.
- The downstream applications of the dataset and task remain unclear, and the definition of essentiality needs further discussion.
- Additional experiments with large-scale models like LLAMA and a more detailed analysis of results are suggested.

### Suggestions for Improvement
We recommend that the authors improve the literature review by referencing previous works on crowdsourcing event steps, such as "A Crowdsourced Database of Event Sequence Descriptions for the Acquisition of High-quality Script Knowledge" (Wanzare et al., LREC 2016) and "MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge" (Ostermann et al., LREC 2016). Additionally, we suggest that the authors clarify the downstream uses of the dataset and provide a more robust discussion on what constitutes essentiality. Finally, we encourage the authors to conduct further experiments with open-source models like LLAMA and provide a detailed analysis of their results.