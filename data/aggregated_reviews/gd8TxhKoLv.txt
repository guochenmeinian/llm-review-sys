ID: gd8TxhKoLv
Title: PROTEGE: Prompt-based Diverse Question Generation from Web Articles
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method named Prompt-based diverse question generation (PROTEGE), which enhances question generation through a decoder modification and a greedy hill-climbing algorithm. The method utilizes different cross-attention layers in the decoder and combines their representations. Experimental results indicate that PROTEGE generates questions with improved fidelity and diversity compared to previous methods. The study also explores the balance between diversity and fidelity using a sub-modular objective function.

### Strengths and Weaknesses
Strengths:
- The experimental results are promising, with thorough analysis regarding diversity and fidelity.
- The inclusion of human evaluations enhances the reliability of the findings.
- The paper is well-written and presents a novel contribution by employing separate cross-attention architecture for conditional generation.

Weaknesses:
- The proposed method performs poorly against baselines when evaluated using NLG metrics (METEOR, BLEU, ROUGE), and additional examples in the Appendix are needed to explain these low scores.
- Important details regarding the experimental setup are missing, such as the number of examples and participants in human evaluations.
- The framework is relatively simple, with the key difference being an extra layer for prompt embedding, which is a common approach in NLP.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental setup by specifying the number of examples and participants involved in human evaluations. Additionally, the authors should provide more detailed explanations of the heuristic method in Section 2.2 and clarify the terms "pre-greedy" and "post-greedy" in Table 4. To enhance the practical applicability of the research, we suggest exploring the method's performance on other controllable generation tasks beyond question generation. Finally, including more empirical diversity, such as additional languages, would strengthen the study.