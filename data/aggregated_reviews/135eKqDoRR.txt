ID: 135eKqDoRR
Title: Bayesian-guided Label Mapping for Visual Reprogramming
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 8, 7, 7, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a technique called label mapping (LM) used in visual reprogramming (VR), which establishes the relationship between pre-trained labels and downstream labels. Unlike conventional fine-tuning, LM aims to find weights in the last layer without gradients, utilizing Bayes' theorem for theoretical justification. The authors demonstrate the effectiveness of their approach through extensive experiments, revealing significant improvements in performance.

### Strengths and Weaknesses
Strengths:
1. The paper exhibits good flow and clarity, particularly in analyzing loss using Bayes' rule.
2. Extensive experiments provide convincing results, especially in visualizing the relationship between pre-trained and downstream labels.
3. The technique maintains the integrity of pre-trained models, which is advantageous for generalization.
4. Theoretical contributions are solid, detailing how to embed a pre-trained model into p(y^T|x^T) and demonstrating higher accuracy with BLM.

Weaknesses:
1. Benchmark datasets, while consistent with previous works, may not be the most relevant; scenarios where fine-tuning the last layer is unfeasible should be explored.
2. Certain sections, particularly between lines 253-266, are confusing and could be relocated to the appendix.
3. The presentation of algorithm tables is inadequate; they should be included in Section 4 for better clarity.
4. Practical guidance on choosing Padding or Watermarking is lacking.
5. The necessity of experiments to increase n is unclear; the authors should clarify its significance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the confusing paragraph between lines 253-266 by relocating it to the appendix. Additionally, the main algorithm table should be moved to Section 4 to enhance implementation clarity. The authors should also provide practical guidance on selecting Padding or Watermarking and clarify the significance of increasing n in experiments. Lastly, addressing the gaps in the transfer learning literature and providing standards for model selection for specific downstream tasks would strengthen the paper's positioning.