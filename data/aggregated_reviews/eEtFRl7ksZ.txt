ID: eEtFRl7ksZ
Title: Emergence of Text Semantics in CLIP Image Encoders
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 6, 7, 6
Original Confidences: 4, 5, 4

Aggregated Review:
### Key Points
This paper presents an investigation into how CLIP image encoders capture the semantics of rendered text in images, proposing that a semantic subspace exists within the image representation for text-based tasks like sentiment classification. The authors conduct experiments using a linear probe for each font to evaluate CLIP's understanding of text in images, demonstrating that CLIPâ€™s image representations can classify sentiment comparably to its text encoder. The study employs controls, such as ROT-K cipher and jumbled text, to support the hypothesis that the model's success stems from genuine semantic understanding rather than superficial associations.

### Strengths and Weaknesses
Strengths:
- The topic is novel and intriguing, contributing to the field of multi-modality representation.
- The experiments are well-executed, utilizing controls like ROT-K and Jumble tests to validate the model's semantic understanding across multiple CLIP-based models.

Weaknesses:
- The paper lacks practical images, leaving the robustness of the conclusions unclear.
- The reliance on synthetic training data and linear probing raises questions about the performance of the CLIP image encoder itself on this task.
- The introduction lacks fluency and organization, particularly regarding the mention of tokenization methods.
- The similarity between CLIP image and text representations appears low, necessitating either baseline comparisons or an explanation for this discrepancy.

### Suggestions for Improvement
We recommend that the authors improve the paper by including practical images to substantiate their conclusions. Additionally, the authors should evaluate the performance of the CLIP image encoder independently on the sentiment classification task. To enhance clarity, we suggest reorganizing the introduction for better flow and removing unnecessary mentions of tokenization methods. Finally, the authors should address the low similarity between CLIP image and text representations by providing baseline comparisons or explanations to clarify the encoder's validity.