ID: NUQeYgg8x4
Title: Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations
Conference: EMNLP/2024/Workshop/BlackBoxNLP
Year: 2024
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: 3, 4, 3

Aggregated Review:
### Key Points
This paper presents a counterexample to the strong version of the Linear Representation Hypothesis (LRH), which asserts that meaningful features are encoded linearly in neural network representations. The authors investigate a sequence copying task using GRUs of varying hidden sizes and demonstrate that smaller models utilize a non-linear "onion-based representation," where identical tokens at different time positions are encoded in the same direction but distinguished by vector magnitudes. The authors verify their findings through causal interventions, revealing that larger models adopt linear representations while smaller models exhibit non-linear encoding.

### Strengths and Weaknesses
Strengths:
- The study is well-motivated and has significant implications for methods relying on linear encoding assumptions.
- The paper is clearly written, guiding the reader through the authors' reasoning and findings.
- The experimental setup effectively supports the main claims, and the authors thoroughly discuss alternative explanations.

Weaknesses:
- The onion unigram intervention achieves high accuracy in larger models, raising concerns about its potential to overshadow other interventions that could explain the smallest models' behavior.
- The paper lacks clarity regarding the definitions of key terms and the motivation behind the chosen concepts, particularly the distinction between token identity and time position.
- The authors' interpretation of the LRH may be overly strict, as encoding in the same direction with varying magnitudes could still align with linear encoding.

### Suggestions for Improvement
We recommend that the authors improve the clarity of key terms and concepts from the outset, particularly defining "concepts" in the context of LRH and their specific task. Additionally, it would be beneficial to explore the trigram hypothesis to further investigate the behavior of the smallest models. We suggest revising the title to reflect the nuanced findings more accurately, such as “*Can* Learn to Store and Generate Sequences using Non-Linear Representations.” Furthermore, addressing the potential implications of their findings on other architectures, such as transformers, could enhance the paper's impact.