ID: 2YSHEBRRol
Title: Aligning Individual and Collective Objectives in Multi-Agent Cooperation
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 5, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel optimization method called Altruistic Gradient Adjustment (AgA), which employs gradient adjustments to align individual and collective objectives in mixed-motive cooperative games. The authors prove that AgA effectively attracts gradients to stable fixed points of the collective objective while considering individual interests. The methodology includes an "alignment" parameter to balance individual and collective maximization, and the algorithm demonstrates convergence to equilibrium points. Empirical validation is provided through experiments in various environments, including sequential social dilemmas and StarCraft II.

### Strengths and Weaknesses
Strengths:
1. The algorithm is simple, intuitive, and easily implementable in existing multi-agent algorithms.
2. The paper is well-written and organized, providing clarity and rigor in its presentation.
3. The experiments are conducted in well-selected benchmark environments, demonstrating the algorithm's superiority.
4. The inclusion of ablation studies for relevant parameters, particularly the "lambda" term, enhances the analysis.

Weaknesses:
1. AgA introduces an additional adjustment term that requires case-by-case tuning.
2. The method adds computational complexity due to the need for Hessian-vector products.
3. There is uncertainty regarding AgA's ability to consistently optimize individual objectives, as evidenced by comparisons with other methods like Simul-Co.
4. Some sections contain typos and incomplete sentences, affecting overall presentation quality.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the reward contour graphs in Figure 1 by clearly defining the axes and the action space. Additionally, a more in-depth discussion on the computational complexity of AgA compared to other algorithms is necessary, including empirical results that illustrate individual metrics achieved by agents using AgA versus other methods. Furthermore, we suggest that the authors address the tuning process for the adjustment term and provide more comprehensive results on how AgA satisfies individual incentives. Lastly, enhancing the overall presentation to eliminate typos and improve coherence in the Related Work section would strengthen the manuscript.