ID: 8xyd9i1XLb
Title: MoPe: Model Perturbation based Privacy Attacks on Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a membership inference attack called Model Perturbations Attack (MoPe) that identifies with high confidence whether a given text is in the training data of a pre-trained language model, assuming white-box access to the model's parameters. The authors propose that MoPe improves accuracy over existing loss-based attacks and explores the influence of training point order and model size on attack success. The method involves adding noise to the model in parameter space and measuring the drop in log-likelihood, approximating the trace of the Hessian matrix for model weights. The paper evaluates MoPe across various language models, demonstrating its effectiveness.

### Strengths and Weaknesses
Strengths:
- The development of a novel membership inference attack with improved accuracy over existing methods.
- Comprehensive examination of the role of training point order and model size in the success of the attack.
- Effective demonstration of MoPe's ability to approximate the Hessian trace in practice.

Weaknesses:
- The focus on a single attack type without discussing potential solutions or mitigation strategies for the identified security risks.
- The limitation of only considering white-box access, which may not reflect real-world scenarios.
- Lack of experimental comparison with shadow-training approaches and unexplained restrictions on target models.

### Suggestions for Improvement
We recommend that the authors improve the discussion of potential solutions or mitigation strategies to address the security risks identified in this paper. Additionally, the authors should explore how the results would generalize to black-box or gray-box settings. We suggest clarifying the implications of the finding that the loss of a point alone is insufficient to determine extractability and discussing the relevance of this for future research on memorization or "unlearning." Furthermore, we encourage the authors to compare MoPe with membership methods based on "with point" vs. "without point" models and to include a deeper discussion on the connection between model perturbations and Hessian traces. Lastly, we suggest revisiting the related work section to consider direct experimental comparisons with other methods that could be easily adopted.