ID: OeLInnFKUK
Title: Practical Differentially Private Hyperparameter Tuning with Subsampling
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 6, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for differentially private (DP) hyperparameter tuning that utilizes subsampling of sensitive data to enhance computational efficiency and reduce privacy loss. The authors provide a Renyi DP (RDP) analysis, demonstrating that their approach lowers the DP bounds and computational complexity while achieving superior privacy-utility trade-offs compared to existing state-of-the-art methods. Empirical results support the effectiveness of the proposed method across various datasets.

### Strengths and Weaknesses
Strengths:
1. The paper effectively addresses the challenge of efficient differentially private hyperparameter optimization, employing subsampling to enhance computational efficiency and leverage privacy amplification.
2. The theoretical analysis and experimental results indicate improved performance in terms of privacy and utility compared to baseline methods.
3. The presentation is clear and well-structured, making the paper mostly easy to follow.

Weaknesses:
1. The extrapolation heuristics are primarily designed for DP-SGD, limiting the general applicability of the method to other adaptive optimizers.
2. Clarity issues exist, particularly regarding the introduction of concepts such as hyperparameter extrapolation and justifications for certain methodological choices. The mathematical notation also requires improvement.

### Suggestions for Improvement
We recommend that the authors improve clarity by introducing key concepts, such as the subsampling ratio $\gamma$, earlier in the paper. Additionally, we suggest providing clearer justifications for methodological choices, particularly regarding training on subsets versus the entire dataset. The authors should also consider addressing the applicability of their method to other adaptive optimizers beyond DP-SGD. Furthermore, revising the language and mathematical notation for consistency and clarity will enhance the overall readability of the paper.