ID: EFkw0OgZOr
Title: Training an Open-Vocabulary Monocular 3D Detection Model without 3D Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 5, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents OVM3D-Det, an open-vocabulary monocular 3D object detection framework that utilizes only RGB images for training. The authors propose innovative designs such as adaptive pseudo-LiDAR erosion and bounding box refinement, addressing challenges from the absence of LiDAR point clouds. The framework leverages large language models to generate pseudo 3D labels, demonstrating effectiveness in both indoor and outdoor scenarios.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and introduces an innovative approach to open-vocabulary monocular 3D object detection using only RGB images.
2. The proposed method shows superior performance compared to existing state-of-the-art approaches.
3. The adaptive pseudo-LiDAR erosion and bounding box refinement effectively address challenges associated with noisy point clouds and occluded objects.
4. Extensive experimental results validate the effectiveness of the approach across various datasets.

Weaknesses:
1. The overall average precision (AP) remains low, heavily relying on the performance of the depth estimator, particularly for distant objects.
2. The evaluation on the KITTI dataset is limited to five classes; results on the NuScenes dataset would better demonstrate open-vocabulary performance.
3. The adaptive pseudo-LiDAR erosion module lacks sufficient detail and visualizations, making it difficult to assess its effectiveness.
4. The related work section is outdated and should include recent papers accepted by CVPR 2024.
5. The ablation studies are confusing, lacking clarity on the improvements of each component compared to the baseline.

### Suggestions for Improvement
We recommend that the authors improve the clarity and detail of the adaptive pseudo-LiDAR erosion module in the methods section, including more visualizations to demonstrate its effectiveness. Additionally, providing results on the NuScenes dataset would enhance the demonstration of open-vocabulary capabilities. We also suggest updating the related work section to include recent relevant literature and clarifying the ablation studies to specify the contributions of each component. Finally, addressing the dependence on depth estimation quality and providing insights into the computational requirements and efficiency of the proposed framework would strengthen the paper.