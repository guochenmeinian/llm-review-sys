ID: FOkKndty5B
Title: SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 4, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to fine-grained video understanding using video large language models (Vid-LLMs) through the introduction of the SlowFocus mechanism. The authors propose a two-stage inference strategy that combines high-frequency sampling of relevant video segments with low-frequency global sampling, aiming to enhance temporal understanding. Additionally, they introduce the FineAction-CGR benchmark to evaluate the capabilities of Vid-LLMs in fine-grained temporal tasks. The method also enhances valid frames per second (fps) while avoiding increased computational costs, achieving competitive results on long video benchmarks such as EgoSchema and MovieChat-1K. The authors propose incorporating LLaVA-Next as a baseline, which demonstrates improved performance on ActivityNet-QA. They address concerns regarding ablation studies and clarify the impracticality of feeding all frames at high frequency due to computational constraints.

### Strengths and Weaknesses
Strengths:
1. The introduction of a two-stage inference strategy offers a new perspective for improving video understanding.
2. The FineAction-CGR benchmark provides a valuable tool for assessing Vid-LLMs' performance in fine-grained temporal understanding tasks.
3. The methodology is well-explained, and the experiments demonstrate the validity of the proposed designs.
4. The method shows competitive performance on long video benchmarks despite not being specifically trained on them.
5. Incorporation of LLaVA-Next as a baseline strengthens the comparative results.
6. The authors provide detailed responses to reviewer concerns, enhancing the clarity of their approach.

Weaknesses:
1. The practicality of the coarse-to-fine reasoning approach is questionable due to increased inference costs and reliance on accurate event localization, which is not sufficiently validated against strong baselines.
2. Comparisons with stronger baselines, such as MovieChat and VideoChat2, are lacking, raising concerns about the claimed advancements.
3. The effectiveness of the two-stage model on long video benchmarks remains unvalidated, necessitating further evaluation on datasets like EgoSchema and MovieChat-1K.
4. The current writing requires major modifications to reflect the new information provided in the rebuttal.
5. The explanation of why baselines are weak on the CGR benchmark lacks clarity.
6. The ablation studies requested by reviewers were not fully addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their motivation by providing detailed evaluations of inference speed and comparing their method against uniform sampling at higher resolutions. Additionally, we suggest including stronger baselines in their comparisons to substantiate claims of superiority. It is crucial to validate the effectiveness of the two-stage model on long video benchmarks, and we encourage the authors to incorporate evaluations on datasets such as EgoSchema and MovieChat-1K. Furthermore, addressing the missing details in the architecture of the temporal encoder and clarifying the confusion in Table 5 would enhance the paper's clarity. We also recommend that the authors improve the writing to ensure all new information and updates from the rebuttal are reflected in the final version. Highlight compute and inference costs to support the motivation, including a comparison of naive baselines versus the authors' method. Clearly explain the weaknesses of the baselines on the CGR benchmark and detail the Stage 3 finetuning to avoid unfair comparisons. Update tables with results from the new LLaVA-Next baseline and include the new ActivityNet results. Additionally, include the results on long video benchmarks in the main paper and consider providing visualizations of selected frames from EgoSchema QnA examples in the appendix to validate the model's generality.