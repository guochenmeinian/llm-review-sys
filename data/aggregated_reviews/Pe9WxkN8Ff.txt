ID: Pe9WxkN8Ff
Title: Learning Transformer Programs
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 6, 7, 7, 7, -1, -1, -1
Original Confidences: 2, 3, 5, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for constraining the parameter space of Transformers to enable the direct translation of learned weights into human-readable programs, specifically in RASP. The authors propose a discretized architecture that modifies attention mechanisms to ensure that the model representation consists of distinct variables. The approach focuses on attention-only models, with extensions to include MLP layers. The authors demonstrate their method through various tasks, including named entity recognition and sentiment classification.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel architecture that facilitates direct code translation, advancing mechanistic interpretability.
- The methodology is well-motivated, and the writing is clear, providing a solid foundation for future research in interpretable neural networks.

Weaknesses:
- The interpretability of the learned programs is questionable, particularly outside simple tasks. More detailed analysis of the programs is needed.
- The scalability of the technique to complex problems or longer sequences remains unclear, and the optimization challenges for larger Transformers are not adequately discussed.

### Suggestions for Improvement
We recommend that the authors improve the analysis of the learned programs, particularly in terms of their interpretability and stability across training runs. Additionally, a more thorough investigation into the scalability of the technique and the challenges faced when applying it to larger networks or more complex tasks should be included. Clarifying the limitations of the target language compared to general RASP and addressing the optimization challenges in detail would also strengthen the paper.