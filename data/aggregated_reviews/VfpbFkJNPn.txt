ID: VfpbFkJNPn
Title: Could Small Language Models Serve as Recommenders? Towards Data-centric Cold-start Recommendation
Conference: ACM
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel solution to the system cold-start recommendation problem using small language models, addressing the lack of historical user-item interactions and the high inference costs of larger models. The authors propose a data-centric method that includes a high-quality data selection strategy and a transferable prompt pretraining stage. Experiments conducted on three small datasets aim to demonstrate the effectiveness of these techniques, although the results are met with skepticism due to insufficient baseline comparisons.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a significant and relevant problem in recommendation systems, particularly the feasibility of using small language models for cold-start scenarios.
2. The writing is clear and accessible, making the concepts easy to follow.
3. The innovative data-centric method leverages high-quality sample construction and prompt tuning, providing a straightforward approach to a challenging issue.

Weaknesses:
1. The experiments lack convincing evidence, as the selected baselines and datasets do not adequately demonstrate the proposed method's effectiveness.
2. Reported results show limited improvements over random strategies, raising concerns about the method's overall performance.

### Suggestions for Improvement
We recommend that the authors improve the baseline comparisons by including stronger alternatives, such as few-shot learning with randomly generated examples that could not exist in \(R_{tgt}\). Additionally, we suggest clarifying the design process of the function \(g_{dom}(\dot)\) in the transferable pre-trained task prompt and addressing how the in-context learning (ICL) approach compares with few-shot learning methods. Furthermore, the authors should analyze the "two-stage greedy algorithm" for prompt decomposition and provide insights into the availability of the self-constructed corpus in real-world scenarios. Lastly, we encourage the authors to elaborate on the dataset splits to ensure no leakage between training, validation, and test datasets.