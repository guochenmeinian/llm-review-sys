ID: NKdtztladR
Title: Latent Diffusion for Language Generation
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 4, 3, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel latent diffusion model for text generation that integrates pre-trained encoder-decoder models, such as BART and FLAN-T5, with a continuous diffusion framework. The authors propose using a Perceiver Resampler to handle variable-length inputs, allowing the model to learn the diffusion process effectively. Additionally, the paper provides a comprehensive experimental comparison of various text diffusion methods, including Diffusion-LM and DiffuSeq, highlighting their publication dates and favorable outcomes. The authors develop a hybrid approach that leverages the strengths of both diffusion and autoregressive generation, showcasing innovation in adapting pre-trained language models into effective language autoencoders.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and clear, making the proposed method easy to understand.
- The architectural design is novel, and the extensive experiments convincingly evaluate the method's effectiveness.
- Extensive experimental comparisons with recent text diffusion methods demonstrate the robustness of the proposed approach.
- The introduction of a perceiver-based network effectively adapts existing pre-trained language models, showcasing innovation in addressing challenges specific to language diffusion.
- The hybrid approach is a notable contribution, combining the benefits of diffusion and autoregressive techniques.

Weaknesses:
- The long-term impact of this work on the research community is questionable, as fine-tuning existing language models still yields superior results. The performance of diffusion-based approaches remains hard to assess, especially at scale.
- A critical comparison with standard autoregressive models is missing, particularly regarding inference latency, which is discussed but not systematically analyzed.
- The contribution is limited, as the diffusion model primarily serves to generate conditions for the language decoder, raising questions about its necessity and efficiency.
- The experiments lack new baselines and a comprehensive literature review, limiting the evaluation's depth and relevance.
- The paper acknowledges the lack of established benchmarks in the text diffusion community, which may affect the reliability of comparisons.
- Some datasets used in the comparisons are noted to have quality issues, potentially impacting the validity of the results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the research motivation, specifically addressing the advantages of using a latent diffusion model over autoregressive and non-autoregressive models. Additionally, we suggest including a systematic comparison of inference latency between the proposed method and standard autoregressive models. The authors should also consider incorporating more recent diffusion-based text generation models into their experiments and provide a more comprehensive literature review to contextualize their contributions. Furthermore, we recommend that the authors address the quality issues of the datasets used in their comparisons and explore the establishment of standardized benchmarks for text diffusion methods to enhance the reliability of future evaluations. Finally, addressing the questions regarding the training of the autoencoder and the influence of different VAE models on performance would enhance the paper's rigor.