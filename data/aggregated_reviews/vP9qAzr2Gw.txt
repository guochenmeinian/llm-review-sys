ID: vP9qAzr2Gw
Title: Supra-Laplacian Encoding for Transformer on Dynamic Graphs
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 4, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method called Supra-Laplacian Encoding for spatio-temporal Transformers (SLATE) aimed at addressing dynamic graph challenges. The authors propose transforming discrete-time dynamic graphs into multi-layer networks and leveraging the spectral properties of their supra-Laplacian matrices. SLATE incorporates a cross-attention mechanism to model pairwise relationships between nodes effectively, enhancing dynamic link prediction. The method reportedly outperforms existing state-of-the-art approaches across multiple datasets.

### Strengths and Weaknesses
Strengths:
1. SLATE innovatively applies spectral graph theory to dynamic graphs.
2. The rigorous experimental setup demonstrates superior performance compared to state-of-the-art methods on nine datasets.
3. The authors provide a detailed explanation of the method and theoretical concepts, along with open-source code for reproducibility.

Weaknesses:
1. The experimental results for CanParl in Table 2 are subpar.
2. The permutation setting may restrict SLATE's generalization to unseen nodes and large-scale graphs.
3. The explanation for adding temporal connections in the supra-Laplacian construction is insufficient, and the necessity of each step in Figure 2 lacks convincing arguments.
4. Scalability remains a concern, particularly for larger datasets, due to high computational complexity and precomputation needs.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the explanation regarding the addition of temporal connections in the supra-Laplacian construction. Additionally, a more comprehensive description of how to construct the supra-Laplacian is necessary. We suggest that the authors clearly define the characteristics of the SLATE model and provide stronger justifications for each step in Figure 2. To address scalability, consider exploring the impact of using sparse attention mechanisms instead of fully connected transformers. Lastly, we encourage the authors to conduct extensive experiments to determine the optimal window size for various datasets and clarify the theoretical advantages of Supra-Laplacian encoding compared to existing methods.