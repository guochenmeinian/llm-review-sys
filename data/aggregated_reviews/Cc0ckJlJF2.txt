ID: Cc0ckJlJF2
Title: Reward Machines for Deep RL in Noisy and Uncertain Environments
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 8, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an extension of the Reward Machine framework to partially observable reinforcement learning environments, specifically addressing scenarios where agents lack direct access to a labeling function for state transitions. The authors propose a model that learns to predict the Reward Machine state from the agent's trajectory, supported by theoretical and experimental results. The work is particularly relevant for real-world applications, especially in the context of large language models, where defining reward functions can be challenging. Additionally, the authors discuss the broader contextualization concerning foundation models and clarify the role of abstraction models, proposing to incorporate these key points into the final paper to enhance its clarity and relevance.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated, reasoned, and communicated, with a concrete running example that elucidates theoretical points.
- The empirical experiments are thorough, covering multiple runs, baselines, and a variety of environments, demonstrating significant improvements in performance and sample efficiency under noisy conditions.
- The introduction of a novel approach to handle noisy environments by modeling them as Partially Observable Markov Decision Processes (POMDPs) is innovative.
- The authors have effectively addressed most concerns raised in the reviews and have shown a willingness to improve the paper based on constructive feedback. The proposed clarifications are expected to provide valuable insights to readers.

Weaknesses:
- The analysis of differences between environments is somewhat lacking; particularly, the reasons behind the TDM approach's varying performance across environments are not fully explored.
- The assumption that the form of the abstraction model is known may limit applicability in complex environments, and the paper primarily compares methods proposed by the authors, raising questions about the effectiveness of existing RL methods.
- Clarity issues exist regarding the relationship between the abstraction model and the problem setting, as well as the definitions of optimal behavior and belief states.
- There may still be outstanding concerns that need to be addressed, although these have not been specified in the reviews.

### Suggestions for Improvement
We recommend that the authors improve the analysis of environmental differences, particularly addressing why TDM performance varies across environments. Additionally, a broader comparison with existing RL methods would strengthen the claims regarding the proposed approach's effectiveness. Clarifying the role of the abstraction model in the problem setting and providing precise definitions for terms like "optimal behavior" and "optimal belief" would enhance the paper's clarity. We also encourage the authors to identify and address any remaining concerns to ensure a comprehensive revision. Finally, including examples of learned beliefs or outputs from the abstraction model could aid reader understanding.