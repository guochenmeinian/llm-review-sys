ID: 83m634EuTW
Title: Re-Examining Summarization Evaluation across Multiple Quality Criteria
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an evaluation of summarization metrics across multiple Quality Criteria (QS), including relevance, consistency, fluency, and coherence. The authors argue that conventional methods relying on high correlations between metric and human scores are inadequate in a multiple QS context. They demonstrate that even the best metrics fail to detect significant summary corruptions and reveal spurious correlations among QS. Additionally, the authors propose a method to identify unreliable metric-human correlation scores influenced by confounding variables.

### Strengths and Weaknesses
Strengths:  
- The paper offers a novel perspective on summarization evaluation metrics in a multiple QS setting.  
- It raises important concerns regarding the reliability of automatic evaluation metrics and provides experimental support for its claims.  
- The structure, title, and abstract are well-crafted, and the references are relevant.  

Weaknesses:  
- Experiments are limited to the SummEval benchmark, raising questions about the generalizability of results to other benchmarks and languages.  
- Some main claims lack sufficient support, and there are major technical and methodological issues.  
- The paper could benefit from clearer parameter settings and examples to enhance reproducibility and comprehension.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by conducting experiments on additional benchmarks beyond SummEval. Additionally, we suggest including worked-out examples to facilitate understanding and illustrating how ratings change with summary corruptions. Incorporating results from partial correlations could further strengthen the paper's arguments regarding confounding factors.