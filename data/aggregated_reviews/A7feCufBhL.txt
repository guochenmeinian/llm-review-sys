ID: A7feCufBhL
Title: Image Captioners Are Scalable Vision Learners Too
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 8, 6, 9, 8, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 5, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comparative analysis of image captioning and contrastive learning (CLIP) as pre-training objectives for visual representation learning. The authors demonstrate that training a Vision Transformer (ViT) with an image-to-text generation objective yields competitive results against CLIP, particularly in tasks requiring fine-grained language understanding. The introduction of parallel decoding as an additional pre-training objective complements conventional autoregressive decoding, enhancing the model's performance.

### Strengths and Weaknesses
Strengths:
- The paper provides a systematic and refreshing observation that image-to-text generative learning can produce vision encoders comparable to contrastive models, opening new avenues for research.
- Pre-training details are well-controlled, ensuring a fair comparison between the models.
- Comprehensive evaluations across various tasks, including image classification and vision-language tasks, are well-presented, with notable insights from the results.
- The proposed parallel prediction method intuitively strengthens supervision for the ViT.

Weaknesses:
- A minor limitation exists regarding the suitability of Cap-ViT representations for encoder-decoder tasks compared to CLIP-ViT, suggesting that frozen adaptation may not fully leverage Cap-ViT's advantages. Fine-tuning performance should also be reported.
- The layout of tables and figures is confusing, impacting readability and comprehension.
- The reliance on a proprietary dataset (WebLi) for pretraining and the absence of code hinder reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the layout of tables and figures for better clarity and flow. Additionally, including a discussion of potential negative societal impacts would enhance the paper's comprehensiveness. To bolster reproducibility, we encourage the authors to release the code used in their experiments. Furthermore, we suggest evaluating the performance of the exact autoregressive language model used in the CLIP paper, considering auxiliary objectives like backward captioning or masked language modeling, and exploring the use of multiple parallel decoders. Lastly, including evaluations on dense prediction tasks such as object detection and segmentation could enrich the study.