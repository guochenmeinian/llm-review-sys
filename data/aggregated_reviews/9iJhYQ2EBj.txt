ID: 9iJhYQ2EBj
Title: Comparing Representations in Static and Dynamic Vision Models to the Human Brain
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 8, 7, 7, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into how well embeddings from deep neural network models trained on static and dynamic inputs predict neural responses to naturalistic movie stimuli, specifically examining the movie "Forrest Gump." The study finds that video models and temporal dynamics are crucial for matching visual cortex responses, with even simple optic flow models accounting for unique variance in neural responses. The authors compare self-supervised and supervised models, including Masked Autoencoders (MAEs), and highlight the importance of benchmarking neuroAI in dynamic contexts.

### Strengths and Weaknesses
Strengths:
- The study addresses an important area by extending research from static images to dynamic stimuli, providing insights into neural processing in more naturalistic settings.
- The methodology for comparing brain and model responses is well-motivated and rigorous, with comprehensive analyses that reveal unique contributions of different models.
- The experimental design effectively explores overlapping and unique variance in neural responses, moving beyond simple predictivity scores.

Weaknesses:
- The differences in predictivity between static and dynamic models are minimal (r = 0.05), making claims of superiority for dynamic models seem premature.
- The limited number of models compared restricts the ability to draw strong conclusions about the causes of observed differences, as variations in datasets, architectures, and loss functions are not fully accounted for.
- The study does not adequately address the potential correlations between perceptual and higher-level features in complex stimuli, which complicates interpretation of results.

### Suggestions for Improvement
We recommend that the authors improve the discussion of the inherent correlations in complex movie stimuli, as highlighted by Grall and Finn (2022), to clarify how these may affect model predictions. Additionally, we suggest providing more motivation for the selection of models, particularly for the Masked Autoencoders, to strengthen the rationale for their inclusion. To enhance the robustness of the findings, we encourage the authors to compare optic flow models to pymoten to explore specific dynamic visual representations. Finally, expanding the model pool in future research to include a broader variety of datasets and architectures will help disentangle the contributions of different factors to brain predictability.