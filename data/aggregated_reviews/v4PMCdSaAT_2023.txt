ID: v4PMCdSaAT
Title: A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
The authors propose a benchmark for evaluating feature selection methods in tabular data learning, particularly focusing on robustness against corrupted, uninformative, and redundant features. They introduce Deep Lasso, a method that generalizes Lasso for deep neural networks, demonstrating its superior performance in challenging scenarios. The evaluation includes a variety of feature selection methods and datasets, assessing the downstream performance of MLP and FT-Transformer models.

### Strengths and Weaknesses
**Strengths:**
1. The benchmark accounts for various extraneous feature types and diverse datasets, enhancing its applicability.
2. A wide range of feature selection methods is evaluated, including statistical tests and regularization techniques.
3. Deep Lasso consistently improves performance in more challenging scenarios, despite its higher computational demands.

**Weaknesses:**
1. The datasets used are limited to numerical features, which may not reflect the heterogeneous nature of real-world tabular data.
2. The benchmark comprises only 12 datasets, potentially limiting the generalizability of the findings.
3. The paper lacks a limitations subsection and does not report the number of repetitions per experiment, which could affect the reliability of results.

### Suggestions for Improvement
1. The authors should extend the benchmark to include datasets with a mixture of numerical and categorical features to better represent real-world scenarios.
2. An analysis of Deep Lasso's performance across different dataset sizes is recommended to explore its potential in bridging the gap between neural networks and GBDT.
3. The authors should include a limitations subsection to clarify the constraints of their study.
4. Reporting standard errors for the results is essential to understand the variability and reliability of the findings.
5. The authors are encouraged to investigate additional types of extraneous features, such as spurious features, and explore various forms of feature corruption beyond Gaussian noise.