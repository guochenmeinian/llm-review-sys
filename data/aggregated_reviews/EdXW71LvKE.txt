ID: EdXW71LvKE
Title: CRT-Fusion: Camera, Radar, Temporal Fusion Using  Motion Information  for 3D Object Detection
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 5, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CRT-Fusion, a method for 3D object detection that integrates temporal information with radar-camera features through three modules: Multi-View Fusion (MVF), Motion Feature Estimator (MFE), and Motion Guided Temporal Fusion (MGTF). MVF enhances depth information using Radar-Camera Azimuth Attention (RCA) and combines radar features with camera features. MFE estimates pixel-wise velocity and occupancy, guiding the temporal feature alignment in MGTF. The evaluation on the nuScenes dataset demonstrates that CRT-Fusion achieves state-of-the-art performance in 3D object detection.

### Strengths and Weaknesses
Strengths:
1. The MGTF module effectively decouples and fuses multi-frame object features on dense BEV features.
2. The framework balances performance and computational efficiency, making it suitable for real-time radar processing in autonomous driving.
3. The paper achieves state-of-the-art results for radar-camera 3D object detection.

Weaknesses:
1. The method requires velocity supervision, which is not available in some datasets like Waymo.
2. The inference time for each part of the model is not provided, despite showing speed-accuracy trade-offs in Table 1.
3. Hyperparameters significantly affect model accuracy, as indicated in Table 9.
4. The model's generalization is not demonstrated beyond BEVDepth.
5. The term "BEV segmentation" in MFE is misleading; terms like object occupancy or foreground segmentation would be more appropriate.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the training process, including whether the multi-frame multi-modal model is trained in one step. Additionally, the authors should specify the batch size used during inference and discuss the trade-off between performance gains and increased latency due to longer temporal cues. It would be beneficial to include qualitative results to illustrate where the proposed approach excels or fails compared to existing methods. Furthermore, we suggest that the authors provide a detailed description of the loss function and clarify how velocity estimation is supervised. Lastly, we encourage the authors to summarize limitations in the main paper rather than relegating them to the supplementary section.