ID: co4p15OMoc
Title: Implicit Manifold Gaussian Process Regression
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 7, 5, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel methodology for Gaussian process (GP) regression that learns the implicit structure of data residing on low-dimensional manifolds, addressing the challenges posed by the curse of dimensionality. The authors propose a framework utilizing Matern kernels on K-nearest neighbor (KNN) graphs, leveraging random walk normalized graph Laplacians to approximate eigenvalues and facilitate scalability to large datasets. The methodology is validated through experiments on synthetic and rotated MNIST datasets.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel approach to GP regression, effectively combining existing theoretical results with practical applications.
- It demonstrates end-to-end differentiability, allowing simultaneous learning of kernel hyperparameters and geometric parameters.
- The scalability of the model is enhanced by the sparse structure of precision matrices and the use of a weighted nearest neighbor graph.

Weaknesses:
- The distinction between original contributions and prior work is unclear, particularly in Section 3.1, which primarily discusses existing literature.
- The experimental validation is limited, focusing on synthetic datasets and variations of rotated MNIST, lacking real-world datasets with inherent manifold structures.
- The paper does not adequately address the limitations of graph construction and its impact on performance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the manuscript by clearly separating original contributions from prior work, particularly in Section 3.1. Additionally, we suggest enhancing the experimental section by including results from more complex real-world datasets to better demonstrate the methodology's applicability. It would be beneficial to illustrate the implicit manifold learned from the rotated MNIST data, such as displaying the kernel values matrix along the rotation trajectory. Furthermore, we advise restructuring Section 3.2 to clarify the training algorithm and ensure that all parameters are defined before their use in equations.