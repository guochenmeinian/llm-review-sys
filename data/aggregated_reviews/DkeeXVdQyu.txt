ID: DkeeXVdQyu
Title: How to Scale Your EMA
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 6, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 2, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel scaling strategy for the momentum parameter in teacher-student settings, focusing on adapting momentum during changes in batch size, particularly in large batch-size scenarios. The authors validate their scaling rule through theoretical analysis and extensive experiments, demonstrating that a slight modification of the scaling preserves performance at larger batch sizes. Additionally, the paper proposes a scaling rule for the exponential moving average (EMA) in optimization, showing its effectiveness across various tasks and optimizers, enabling large batch training with significant wall-clock time reductions.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow, addressing the relevant problem of large batch-size training, which enhances computational efficiency.
- The experimental setup is broad, encompassing various student-teacher settings, including Polyak-Ruppert averaging, pseudo-labeling, and self-supervised learning.
- The technique is well-motivated through theoretical analysis, reinforcing the choice of scaling.

Weaknesses:
- Key aspects of the experimental setup are inadequately explained, particularly regarding the "No EMA Scaling Rule" in real data experiments and the specifics of the warm-up phase for progressive scaling.
- The role of the baseline momentum in Polyak-Ruppert averaging is unclear, necessitating further elaboration from the authors.
- The applicability of the scaling rule to modern optimizers, especially Adam, requires deeper discussion, as its necessity and sensitivity to different architectures remain ambiguous.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental setup by explicitly defining what "No EMA Scaling Rule" entails and how it interacts with the learning rate scaling. Additionally, the authors should provide detailed information on the warm-up period and the scaling process post-warm-up. Clarifying the impact of the baseline momentum in Polyak-Ruppert averaging is essential, particularly regarding its influence on performance. Furthermore, we suggest including an in-depth discussion on the applicability of the scaling rule to various optimizers, particularly Adam, to address the concerns about its generality and effectiveness across different model types.