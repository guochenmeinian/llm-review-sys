ID: O1fp9nVraj
Title: On scalable oversight with weak LLMs judging strong LLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 5, 8, 4, -1, -1, -1
Original Confidences: 4, 3, 2, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive study of scalable oversight methods, specifically examining how weaker LLMs can supervise stronger LLMs through various protocols, including debate and consultancy. The authors focus on inference-time scalable oversight, analyzing the effectiveness of debates judged by less capable LLMs across multiple tasks. The findings suggest that debates generally yield better performance than consultancy, although both methods often underperform compared to direct question answering. The study includes a large-scale analysis involving 9 tasks and 128 questions, highlighting the potential of debate as a scalable oversight protocol.

### Strengths and Weaknesses
Strengths:
1. The study is carefully designed, varying judge strength, tasks, and oversight protocols to assess their effects.
2. It comprehensively covers various judge models and tasks, providing a balanced presentation that does not overstate results.
3. The paper is well-written and contextualizes its findings within existing literature, enhancing understanding of the subject matter.
4. The results indicating debate's superiority over consultancy are intriguing and suggest promise for debate as a scalable oversight method.

Weaknesses:
1. The models used for debating and judging are not trained to be convincing or accurate, which raises concerns about the validity of the results.
2. Overall results are mixed, with direct QA often outperforming both debate and consultancy, particularly in closed and multimodal tasks.
3. The paper lacks novelty, as the comparison between debate and consultancy has been explored in prior works, and it does not significantly advance understanding of scalable oversight.

### Suggestions for Improvement
We recommend that the authors improve the training of the debater and judge models to enhance their convincingness and accuracy. Additionally, we suggest providing clearer discussions on why debate and consultancy perform poorly in closed QA and multimodal tasks. The authors should also address the implications of using weak judges in the debate protocol and clarify whether improvements in judge accuracy are due to the debate format or the judges' capabilities. Finally, we encourage the authors to highlight the novel contributions of their work more explicitly to distinguish it from previous studies.