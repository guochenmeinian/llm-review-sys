ID: dhJ8VbcEtX
Title: AQuA: A Benchmarking Tool for Label Quality Assessment
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 6, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AQuA, a benchmarking tool designed for label quality assessment in machine learning, specifically focusing on evaluating methods that detect noisy labels in datasets. AQuA offers a standardized evaluation framework, benchmark datasets, and evaluation metrics to assess label error detection methods. The authors propose a design space and benchmark design to facilitate the development and evaluation of these methods, highlighting the impact of labeling errors on model generalization. The benchmark includes 16 datasets across four modalities (image, text, tabular, and time series) and employs four noise injection strategies (asymmetric, class-dependent, instance-dependent, and uniform) at two rates (10%, 40%). The paper includes a large-scale experiment to demonstrate AQuA's efficacy, assessing the performance of four existing label noise detection methods and training models from scratch with identified noisy samples removed. The authors also address hyper-parameter tuning, evaluation metrics, and the handling of class-dependent noise, while providing publicly available data cards to enhance replicability.

### Strengths and Weaknesses
Strengths of the submission include its significant contribution to the research community through a unified evaluation platform, practical relevance for ML practitioners, and a well-defined design space for label error detection models. The inclusion of diverse downstream classifiers, including state-of-the-art transformer models, enhances the robustness of the evaluation. Clear documentation and data cards improve replicability and transparency, while the paper effectively addresses the prevalent issue of erroneous labels with a wide range of datasets and state-of-the-art baselines.

However, weaknesses include the limited scope of benchmark datasets, which only cover four modalities, and the lack of detailed analysis regarding the performance of different label error detection models. The justification for using support weighting over macro-weighting in evaluations lacks clarity, and the paper does not adequately define the four synthetic noise injection methods. Additionally, the choice of datasets may include easier classification problems, potentially limiting the challenge presented. Current hyper-parameter tuning practices may favor baseline methods, raising concerns about generalization, and the paper does not adequately address potential biases in the datasets or provide comprehensive recommendations for mitigating societal risks associated with label errors.

### Suggestions for Improvement
We recommend that the authors expand the benchmark datasets to include additional modalities, such as audio data, to enhance AQuA's applicability. A more detailed analysis of model performance across various datasets and conditions would also be beneficial. Furthermore, we suggest that the authors improve the justification for using support weighting instead of macro-weighting in their evaluations and provide definitions of the four synthetic noise injection methods in the main text. To enhance the diversity of architectures, we recommend incorporating ViT-small for image tasks and exploring RNN-only and transformer-based architectures for time-series data. 

Additionally, we suggest adding a 2% noise rate to the benchmark to better reflect realistic noise levels in practice, and explicitly stating whether data augmentation was used or not, including details on the hyperparameters used in the experiments for replicability. We recommend discussing the number of seeds used for both model initialization and noise injection methods to ensure consistency across experiments. If noise injection is uniform across experiments, consider providing a static copy of the noised labels for reproducibility. Lastly, we suggest refining the experimental paradigm for text models to better understand the effects of added noise on performance and discussing the potential overfitting of models due to limited regularization and the implications of their hyper-parameter tuning approach.