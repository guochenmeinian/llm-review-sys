ID: SS3CK3yx5Z
Title: Does progress on ImageNet transfer to real-world datasets?
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 4, 7, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the transferability of models pretrained on ImageNet to six diverse real-world datasets, emphasizing the distinction between datasets collected for machine learning and those web-scraped from various sources. The authors evaluate 19 models, including CNNs and Transformers, such as ConvNext, SWIN, and CAFormer, and find that while initial correlations exist, modern architectures show diminishing returns in performance on these datasets. They benchmark augmentation techniques and analyze the impact of CLIP pre-training, concluding that augmentations beneficial for ImageNet do not necessarily enhance fine-tuning performance on downstream tasks. The authors also discuss the implications of dataset size and task saturation on transfer performance, providing a clear definition of "real-world datasets" and justifying their dataset selection, particularly emphasizing the practical relevance of Kaggle datasets.

### Strengths and Weaknesses
Strengths:
- The study addresses a significant question regarding the transferability of ImageNet progress to real-world applications.
- The paper is well-written and easy to follow, with a thorough review of prior work.
- Extensive experiments across multiple models and datasets provide robust support for the findings.
- The authors include a diverse range of models and present detailed benchmarking results, enhancing the paper's empirical foundation.
- The commitment to reproducibility is evident through the provision of open-source code and training hyperparameters.

Weaknesses:
- The reliance on pretrained checkpoints may introduce biases related to hyperparameters, complicating the evaluation of architectural impacts.
- The differentiation between web-scraped and curated datasets may not adequately account for semantic similarities, potentially skewing results.
- The focus on end-to-end fine-tuning overlooks other techniques that might yield different insights.
- Some findings, while valuable, may not be surprising to the community, raising concerns about overshadowing previous research.
- The paper does not sufficiently address the implications of training from scratch versus using pre-trained weights, which could affect the generalizability of the findings.
- Concerns regarding dataset size and the potential saturation of tasks are not fully resolved, particularly for datasets like EuroSAT.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental setup by moving key details from the appendix to the main text, particularly regarding end-to-end fine-tuning. Additionally, consider including a broader range of models, especially modern architectures and self-supervised learning approaches, to enhance the analysis. It would also be beneficial to explore the impact of training from scratch and to address dataset size more thoroughly. We suggest that the authors explicitly acknowledge that the claims in the paper are valid only for RGB images and small datasets (â‰¤50K training examples) in the final version. Clarifying the rationale behind the differentiation between web-scraped and real-world datasets would also enhance the paper's coherence. Finally, providing detailed documentation alongside the open-source code would facilitate reproducibility and further research in this area.