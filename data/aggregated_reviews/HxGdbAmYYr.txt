ID: HxGdbAmYYr
Title: Mixture of Adversarial LoRAs: Boosting Robust Generalization in Meta-Tuning
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 6, 5, 6, -1, -1, -1
Original Confidences: 3, 5, 4, 2, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Adversarial Meta-Tuning (AMT) aimed at enhancing the robust generalization of pre-trained models for out-of-domain few-shot learning by constructing a robust LoRAPool through meta-tuning Low-rank Adapters (LoRAs). The authors propose a method that adversarially generates query samples while using support samples from the source domain, computing inner gradients on the source and meta-gradients on adversarial examples. They also introduce a test-time merging strategy to optimize performance and demonstrate significant improvements over existing methods across various benchmarks.

### Strengths and Weaknesses
Strengths:
- The study is novel and significant, achieving superior performance with the proposed AMT method.
- The writing and presentation are generally clear and easy to follow, with comprehensive experimental results.
- The idea of perturbing singular vectors during LoRA training is innovative and shows promising results.

Weaknesses:
- There is a lack of evaluation on unseen adversarial attacks, which is necessary to validate the claimed improvements in adversarial generalization.
- The evaluation based on PGD-10 is insufficient, as it is considered a weak attack; robustness should also be presented under AA attack.
- The paper does not compare its approach with other popular methods for fine-tuning adaptation of foundation models, such as adapters, which could provide valuable context.
- Some technical details, such as the operation of top_k in Eq. (7) and the differences between tuning-free and test-time fine-tuning settings, are unclear.
- Missing key ablations and a hyper-parameter study limit the understanding of the framework's distinctiveness and effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by testing the robustness of AMT against unseen adversarial attacks and presenting results under AA attack. Additionally, a comparison with adapter methods should be included to contextualize the contributions of the proposed approach. Clarifying technical details, such as the meaning of top_k and the differences between tuning-free and test-time fine-tuning settings, would enhance the paper's clarity. We also suggest including key ablation studies to demonstrate the impact of initialization choices and conducting a hyper-parameter study to better understand the framework's performance.