ID: UJ9o8wbB5U
Title: Provably Safe Reinforcement Learning with Step-wise Violation Constraints
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 7, 5, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on safe reinforcement learning (RL) with step-wise violation constraints, distinguishing it from the commonly used CMDP with additive expectation cost constraints. The authors propose an algorithm that offers bounds on violation and regret, and further develop a method to learn a near-optimal safe policy, demonstrating its effectiveness through experiments. The paper formulates a strict step-wise violation constraint RL problem, providing theoretical guarantees on regret and safety violations, and introduces a model-based algorithm that achieves optimal performance.

### Strengths and Weaknesses
Strengths:
1. The paper addresses an important problem in safe RL, presenting a novel and more general formulation than CMDP.
2. The proposed approach appears sound, supported by theoretical analyses of violation and regret bounds.
3. The safe RL algorithm outperforms existing baselines in empirical evaluations.
4. The paper is well-written and organized, making it accessible to readers.

Weaknesses:
1. The paper lacks references to relevant works on safe RL with step-wise violations, such as the one by Wang et al. (2022), which should be discussed.
2. Clarity is needed regarding whether the MDP model addresses continuous/discrete deterministic/stochastic systems, as the transition set could be infinite in continuous state spaces.
3. The application of the unsafe set concept seems limited to state-dependent safety problems, raising questions about its applicability to (state, action)-dependent safety issues.
4. The definition of the reward-free setting is confusing, particularly regarding cost feedback.
5. The comparison with existing works, especially Amani et al. (2021) and Shi et al. (2023), is insufficient, lacking detailed differentiation and analysis.

### Suggestions for Improvement
We recommend that the authors improve the literature review by including and discussing relevant works on safe RL with step-wise violations. Clarifying whether the MDP model encompasses continuous/discrete systems is essential, as is addressing the implications of state-dependent versus (state, action)-dependent safety constraints. We suggest revising the definition of the reward-free setting to eliminate confusion regarding cost feedback. Additionally, we encourage the authors to provide a thorough comparison with related works, particularly focusing on how their formulations differ and the significance of their contributions.