ID: hJ5DREWdjs
Title: GazeSAM: Interactive Image Segmentation with Eye Gaze and Segment Anything Model
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: 5, 6, 7, -1
Original Confidences: 5, 3, 4, 4

Aggregated Review:
### Key Points
This paper presents GazeSAM, an interactive tool for 2D/3D image segmentation that utilizes 2D eye gaze as an input prompt, integrating it with the pre-trained Segment Anything Model (SAM). The authors demonstrate that GazeSAM achieves comparable segmentation quality to mouse clicks while reducing annotation time by 50%. The paper is well-written and includes qualitative evaluations, with a promise to make the demo code publicly available. However, the authors claim that GazeSAM is the first to combine eye gaze and SAM for interactive segmentation, which contradicts prior work showing eye gaze as a prompt for SAM in AR/VR applications.

### Strengths and Weaknesses
Strengths:
- The tool provides near real-time segmentation feedback, enhancing the user experience.
- It is a working tool that will be available on GitHub, potentially aiding segmentation dataset creation.
- The use of eye tracking significantly reduces annotation time.

Weaknesses:
- Insufficient details regarding IRB exemption and data collection from human subjects.
- Claims about reduced fatigue lack supporting evidence and should be removed.
- No quantitative results for medical data are provided, making it difficult for potential users to assess the tool's applicability.
- The impact of calibration on results is unclear, including time requirements and precision.
- There are minor quality differences in segmentation results between eye tracking and mouse use, with no confidence intervals provided.
- Limited user experience analysis and feedback on the tool's usability compared to mouse use.
- Experiments appear to be conducted with a single user, raising concerns about generalizability.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of “One point” versus “All point” prompts for GazeSAM to compare time and mask quality. Additionally, the authors should assess how mask quality is affected by user-Tobii tracker calibration errors, as recalibration may be necessary during annotation. It is crucial to investigate the tool's effectiveness for segmenting smaller regions of interest, considering potential issues with microsaccades and user fatigue. We also suggest exploring the possibility of automating interactions with the side panel through gaze to further reduce annotation time. Lastly, providing more comprehensive quantitative results, particularly for medical data, and a deeper analysis of user experience would enhance the paper's contribution.