ID: JVlWseddak
Title: EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 8, 8, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 5, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents EgoSchema, a benchmark dataset for evaluating long video understanding capabilities of vision and language systems. It includes over 5000 curated multiple-choice question-answer pairs derived from 250 hours of real video data, utilizing temporal certificate sets to assess video understanding tasks. The authors evaluate state-of-the-art video and language models on this dataset, highlighting the challenges posed by long-form video question answering, where existing models achieve less than 33% accuracy in zero-shot settings. Additionally, the paper provides a comprehensive analysis of temporal certificate sets in video question answering (QA), noting an inter-annotator agreement of 54.3% based on a human experiment with 86 clips. The authors propose exploring few-shot learning for video QA, categorizing temporal certificates by length and benchmarking the accuracy of the mPLUG-OWL model, revealing that longer certificates present significant challenges. They also plan to release a few-shot prompting set to facilitate future research.

### Strengths and Weaknesses
Strengths:
- The introduction of temporal certificate sets offers valuable insights into the challenges of long-form video tasks.
- The dataset curation process is meticulously detailed, ensuring high quality through a combination of automated generation and human curation.
- The dataset's design effectively assesses multimodal systems' capabilities in understanding long-form videos, with a clear motivation for its creation.
- The paper provides valuable insights into the subjectivity of temporal certificates and their impact on video QA.
- The categorization of temporal certificates and benchmarking of model performance across different lengths is well-executed.
- The intention to release a few-shot prompting set is a promising step for advancing research in this area.

Weaknesses:
- The paper does not explore additional evaluation metrics beyond multiple-choice question answering.
- Limitations regarding domain generalization and potential biases from the underlying Ego4D dataset are not fully discussed.
- There are concerns about redundancy in descriptions and the long-tail distribution of actions in the dataset.
- The paper lacks solid examples to illustrate the effects of redundant descriptions on LLMs.
- There is a need for ablation studies to investigate the long-tailed problem more thoroughly.

### Suggestions for Improvement
We recommend that the authors explore additional evaluation metrics beyond multiple-choice question answering to enhance the dataset's applicability. We suggest discussing the limitations of the dataset and the models evaluated, particularly regarding domain generalization. We encourage the authors to filter redundant descriptions in the dataset to improve clarity and understanding of actions. We recommend ensuring diversity in the generated questions and answers to avoid duplication across different clips. We advise clarifying the calculation of temporal certificate lengths and the verification process involved in their determination. Additionally, we recommend that the authors include solid examples that illustrate the effect of redundant descriptions on LLMs and how these descriptions serve as a crucial bridge for describing video events. Finally, we suggest conducting ablation studies to further investigate the impact of the long-tailed problem on model performance.