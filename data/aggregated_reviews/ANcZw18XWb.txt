ID: ANcZw18XWb
Title: Spectral Heterogeneous Graph Convolutions via Positive Noncommutative Polynomials
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to heterogeneous graph neural networks (HGNNs) through the Positive Spectral Heterogeneous Graph Convolutional Network (PSHGCN), which utilizes positive semidefinite polynomials to enhance the expressiveness of spectral graph filters. The authors demonstrate that PSHGCN can learn diverse heterogeneous graph filters while ensuring positive semidefiniteness, supported by experimental results that show its superiority over baseline methods on open benchmarks.

### Strengths and Weaknesses
Strengths:
1. The paper thoroughly explores the design philosophy of PSHGCN from multiple perspectives, including detailed comparisons with existing spectral HGNNs.
2. It is well-written and easy to follow, with clear experimental settings and provided code.
3. The theoretical analysis and explicit time complexity of the proposed method are valuable contributions.

Weaknesses:
1. The performance improvement is marginal, particularly outside the IMDB dataset, and lacks significant testing between the best and second-best performances.
2. Concerns about scalability arise due to the exponential growth of trainable weights relative to the number of relations in heterogeneous graphs.
3. The paper does not provide total running times for large datasets, and the implementation details, such as learned coefficients, are unclear.
4. Some theoretical claims, including expressiveness guarantees and the simplification of the Sum of Squares form, lack sufficient proof or clarity.

### Suggestions for Improvement
We recommend that the authors improve the statistical analyses by including significant tests of repeated trials to confirm that the superior performance of PSHGCN is not accidental. Additionally, we suggest providing total running times for the tested large datasets to enhance transparency. Clarifying the learned coefficients and their importance, as well as providing visualizations, would strengthen the paper. Finally, addressing the theoretical concerns regarding expressiveness guarantees and the simplification of the Sum of Squares form with appropriate proofs would enhance the paper's rigor.