ID: XPhSbybD73
Title: Probabilistic Decomposed Linear Dynamical Systems for Robust Discovery of Latent Neural Dynamics
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 5, 7, 7, -1, -1
Original Confidences: 5, 2, 3, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel model of latent neural dynamics, the probabilistic decomposed linear dynamical systems (p-dLDS), which extends the existing decomposed linear dynamical systems (dLDS) model. The authors propose a full probabilistic formulation and a variational EM algorithm for inference, demonstrating improved performance on synthetic and real neural datasets compared to existing time-varying linear alternatives. The model aims to enhance robustness to noise and capture systems with multiple fixed points.

### Strengths and Weaknesses
Strengths:  
- The proposed model and method are original and significantly contribute to neural dynamics modeling.  
- The probabilistic formulation of dLDS enhances performance, and the flexibility of time-varying linear combinations of degrees of freedom (DOs) allows for better adaptability in various scenarios.  
- The paper is well-motivated, clearly presented, and includes high-quality evaluations across multiple tasks.  
- The inclusion of code and hyperparameter settings supports reproducibility.

Weaknesses:  
- The model's complexities may hinder reliable parameter fitting and data inference, particularly due to the multi-step approximate procedure for sparsity coefficients and the dependence on a hyperparameter for the time-varying offset term.  
- The experimental results lack variance reporting across independent runs, and there is no ablation study on the importance of the slow-fast decomposition of the latent state.  
- The interpretation of discrete states and segmentation in experiments raises questions about their validity and clarity.

### Suggestions for Improvement
We recommend that the authors improve the approach for determining `b_t` by considering alternatives to the moving average that accommodate varying timescales more effectively. Additionally, please provide more details on the hyperparameter selection scheme for the window size, including the relationship between the optimal offset values and the timescales of p-dLDS coefficient switching.

We encourage the authors to explore simplifications in the model that allow for simpler inference in the update step for sparsity coefficients, such as modeling each $\gamma_t$ independently. Furthermore, we suggest justifying the classification method used in the reaching experiment, comparing it to classifications based on continuous states.

Lastly, we advise addressing the technical confusions regarding the coefficient prior and the Gaussian form of the variational posterior, as well as clarifying the implications of covariance matrix structures on SGD learning.