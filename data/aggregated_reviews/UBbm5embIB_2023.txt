ID: UBbm5embIB
Title: Learning Human Action Recognition Representations Without Real Humans
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 7, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a methodology for pre-training action recognition models using a combination of synthetic videos and human-removed real videos, addressing privacy concerns. The authors introduce a new pre-training strategy, Privacy-Preserving MAE-Align (PPMA), which leverages the strengths of both synthetic data and real-world context without compromising privacy. Additionally, the authors propose a method aimed at making humans unidentifiable in videos through segmentation and inpainting techniques. They acknowledge the importance of analyzing segmentation and inpainting errors and are in the process of including this analysis in the final version of the paper. The proposed method is evaluated against established benchmarks, demonstrating improved performance in downstream tasks compared to models trained solely on human data. The authors emphasize that precise reconstruction of inpainted objects is not necessary as long as human identities remain obscured.

### Strengths and Weaknesses
Strengths:
- The integration of synthetic data with processed real videos for privacy protection is innovative.
- The results indicate a significant advantage of the proposed method over traditional approaches.
- The literature survey is comprehensive, and the technical approach is sound, with a well-constructed dataset for evaluation.
- The authors are responsive to reviewer feedback and have made significant revisions to the paper.
- The main goal of ensuring human unidentifiability is clearly articulated and supported by manual verification.
- The inclusion of detailed analyses and discussions in the final version is promising.

Weaknesses:
- The analysis lacks depth, particularly regarding the quality of segmentation and inpainting.
- The paper currently lacks a comprehensive analysis of segmentation and inpainting errors.
- The contribution of the data is marginal, with insufficient manual verification.
- The paper primarily utilizes the ViT-B model, limiting the exploration of other potential backbones.
- Some reviewers noted that the inclusion of additional backbones and visual examples would enhance the paper, though not deemed mandatory.
- There are concerns regarding the clarity of certain methodological choices.

### Suggestions for Improvement
We recommend that the authors improve the analysis by providing more detailed insights into the segmentation and inpainting quality, specifically clarifying the influence of artificial inpaintings on the results. Additionally, we suggest including a broader range of model backbones in the experiments to enhance the robustness of the findings. We encourage the authors to provide more visual examples and supplementary materials to enhance understanding. Clarifying the rationale for using the MAE compared to other self-supervised methods would also strengthen the paper. Finally, addressing the limitations regarding the scalability of the proposed methods with larger datasets and elaborating on the details of the models and pre-training processes would provide valuable insights for future research.