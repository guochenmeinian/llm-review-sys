ID: AG9A7Ae9r3
Title: DIFFER:Decomposing Individual Reward for Fair Experience Replay in Multi-Agent Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 5, 7, 6, 6, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for training a mixing network and agent networks separately within the centralized training and decentralized execution (CTDE) paradigm for multi-agent reinforcement learning (MARL). The authors derive an individual reward that preserves the invariance between the gradients of the mixing network's loss function and the sum of the agent networks' loss functions. They implement Prioritized Experience Replay to select samples for training the agent networks. The proposed method, DIFFER, is evaluated against QMIX, COMIX, QPLEX, and MASER across various tasks, including SMAC, GRF, and MAMujoco, demonstrating superior performance and the effectiveness of a fair experience replay mechanism.

### Strengths and Weaknesses
Strengths:
- Originality: The concept of fair experience replay based on the gradient constraints between the mixing and agent networks is novel.
- Quality: Experimental results robustly support the claims made regarding the proposed method.
- Clarity: The manuscript is well-written and easy to follow.
- Significance: The study highlights the importance of individual experiences in the CTDE approach, which has been overlooked in prior research.

Weaknesses:
- The paper lacks theoretical analysis, particularly regarding the independence of the agent network's loss function from the target networks; a detailed discussion of the loss function is needed.
- The off-policyness of the proposed method requires further discussion.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of the loss function, particularly addressing the independence of the agent network's gradient from the target networks. Additionally, a discussion on the off-policyness of the proposed method should be included. Clarifying how the TD error is calculated and whether the target network is necessary would enhance understanding. We also suggest including a comparison with policy-based MARL methods to strengthen the evaluation and discussing the implications of individual reward decomposition in mixed cooperative-competitive settings.