ID: pTSrqU2zpF
Title: Posterior Sampling via Autoregressive Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 6, 7
Original Confidences: 3, 5

Aggregated Review:
### Key Points
This paper presents a novel framework for learning bandit algorithms from historical data, specifically addressing a cold-start recommendation problem. The authors propose using an autoregressive model to predict sequences of feedback/rewards, which implicitly learns an informed prior, framed as an implementation of Thompson Sampling with a learned prior. The paper includes theoretical guarantees and demonstrates the framework on a news recommendation task.

### Strengths and Weaknesses
Strengths:  
- The integration of autoregressive models for posterior sampling in bandit problems is innovative.  
- Theoretical results include a novel regret bound that connects pre-training loss with online decision-making performance.  
- The framework is applied to a real-world problem, showcasing its practical impact and scalability.  

Weaknesses:  
- The comparison to state-of-the-art methods is limited, lacking important citations and broader empirical evaluation.  
- There is insufficient discussion on the computational complexity of training and deploying autoregressive models for large-scale applications.  
- The assumption of exchangeability may restrict the method's applicability in dynamic real-world scenarios.  

### Suggestions for Improvement
We recommend that the authors improve the empirical evaluation by including comparisons with a wider range of baseline methods and conducting experiments on diverse datasets. Additionally, a more detailed discussion of the computational complexity and scalability of the approach is necessary. The authors should address the limitations posed by the exchangeability assumption and explore potential extensions to non-exchangeable settings. Finally, enhancing the comparison with recent work on approximate Thompson sampling and hypermodels would strengthen the paper's positioning in the current literature.