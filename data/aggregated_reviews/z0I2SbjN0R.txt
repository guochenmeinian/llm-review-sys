ID: z0I2SbjN0R
Title: DiffusionPDE: Generative PDE-Solving under Partial Observation
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 6, 4, 8, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DiffusionPDE, a method utilizing diffusion techniques to address partially observed PDEs by learning the joint distribution of solution and coefficient space. The authors argue that their model excels in sampling likely full states from partial data, outperforming previous methods, including a simple baseline using an RBF kernel. They demonstrate the model's effectiveness through various experiments on forward and inverse problems, showing improvements over standard baselines like PINNs and FNOs. The authors also discuss the challenges of inferring unobserved data, noting that while PINNs can handle partial observations, they typically exhibit a test error between 10% and 30%. Furthermore, they clarify that neural operators, while resolution invariant, require complete continuous grids as input, which their sparse observation points do not satisfy, limiting their effectiveness.

### Strengths and Weaknesses
Strengths:  
- The paper effectively applies diffusion methods to solve PDEs, addressing both forward and inverse problems.  
- The diffusion model demonstrates superior performance in sampling full states from partial observations compared to existing methods.  
- The diverse experimental settings in the main text and supplementary materials support the model's effectiveness under partial observations.  
- The authors provide a clear rationale for their approach, supported by relevant literature on statistical inference in various domains.  
- The presentation is clear, with concise algorithms and equations, making the paper well-written overall.  

Weaknesses:  
1. The technical contribution is limited, primarily representing an application of existing diffusion models in PDE solving without significant novelty.  
2. Important baselines, such as U-Net, OFormer, and Transolver, are missing from comparisons, which could provide a more robust evaluation.  
3. The paper lacks efficiency comparisons, including GPU memory usage and running time.  
4. The reliance on sparse observation points presents challenges for neural operators, which may limit the applicability of their findings.  
5. The high test error associated with PINNs raises questions about their reliability in practical applications.  
6. The current framework does not support predicting future evolution of time-dependent PDEs, focusing instead on reconstruction or imputation.  

### Suggestions for Improvement
We recommend that the authors improve the technical novelty by clearly distinguishing their contributions from existing methods that also utilize diffusion models. Additionally, include comparisons with powerful baselines like U-Net, OFormer, and Transolver to strengthen the evaluation. It is crucial to provide model efficiency comparisons, detailing GPU memory and running times for all methods. Furthermore, we suggest addressing the limitations regarding temporal modeling and discussing the implications of the weights used in loss functions to clarify their impact on the model's performance. We also recommend improving the discussion on the limitations of neural operators in handling sparse inputs, potentially providing more empirical evidence or examples. Lastly, clarifying the implications of the high test error in PINNs could strengthen the argument for the advantages of their diffusion model. Consider expanding the scope of examples beyond standard PDEs to enhance the paper's applicability and relevance.