ID: MI8Z9gutIn
Title: Memory-Efficient Gradient Unrolling for Large-Scale Bi-level Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 5, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called Forward Gradient Unrolling with Forward Gradient, abbreviated as (FG)²U, aimed at addressing the high memory requirements of traditional gradient unrolling methods in bi-level optimization for large-scale machine learning models. The authors propose (FG)²U to reduce space consumption from $\mathcal{O}(MN)$ to $\mathcal{O}(M)$ while providing convergence analysis and extensive experiments demonstrating the algorithm's benefits.

### Strengths and Weaknesses
Strengths:
1. The presentation is clear and easy to follow, making the paper accessible.
2. The method significantly reduces memory overhead, making it suitable for large-scale applications.
3. Extensive experiments cover both small and large-scale settings, showcasing the method's versatility.

Weaknesses:
1. The proposed method increases computational complexity, with costs scaling as $\mathcal{O}(KTN)$, which may be unrealistic for large $N$.
2. The convergence analysis pertains to problem (2) rather than the original problem (1).
3. The theoretical dependence on the parameter dimension $N$ raises concerns about convergence rates in large-scale applications.
4. The algorithm is deterministic and does not consider stochastic settings; insights on unbiasedness of the hypergradient estimator are needed.
5. There is a lack of ablation studies on the choice of parameter $b$ and real space consumption comparisons in Tables 1 and 2.

### Suggestions for Improvement
We recommend that the authors improve the computational complexity analysis of the proposed method. Additionally, we suggest including more large-scale datasets in the experiments and addressing the theoretical guarantees concerning the parameter dimension $N$. It would be beneficial to provide insights into stochastic settings and the unbiasedness of the hypergradient estimator. Furthermore, we encourage the authors to conduct an ablation study on the parameter $b$ and to include real space consumption data in the comparisons presented in Tables 1 and 2.