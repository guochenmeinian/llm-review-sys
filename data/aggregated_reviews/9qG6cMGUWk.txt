ID: 9qG6cMGUWk
Title: Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing Label Bias in Foundation Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 4, 7, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 2, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on biases in foundation models, particularly CLIP, due to imbalanced training datasets. The authors propose a Generalized Logit Adjustment (GLA) method, an optimization-based approach for debiasing these models. The GLA method shows significant improvements in accuracy across multiple tasks and datasets, achieving gains of 1.5 percentage points on ImageNet and 1.4-4.6 percentage points on 11 few-shot datasets. Importantly, GLA does not require access to the pre-training dataset, making it practical for fine-tuning scenarios. The paper includes both theoretical justification and extensive empirical evidence supporting the proposed method. The authors also indicate that the estimation of $\pi_p$ through $\log \mathbf{q}$ is accurate only when the downstream data closely resembles the pre-training dataset, and they plan to include additional experimental evidence, specifically the LAION experiment, to support these claims.

### Strengths and Weaknesses
Strengths:
1. **Solid Theoretical Analysis**: The paper provides a robust theoretical foundation for the GLA method, formalizing label bias estimation as a constrained optimization problem and proving GLA as a Bayes optimal classifier.
2. **Practical Solution**: GLA's independence from the pre-training dataset enhances its applicability in fine-tuning contexts, addressing privacy and copyright concerns.
3. **Comprehensive Evaluation**: The evaluation spans three real-world settings and fine-tuning paradigms, lending credibility to the findings.
4. **Significant Performance Improvement**: The reported accuracy gains across various tasks underscore the method's effectiveness.
5. **Incorporation of Feedback**: The authors demonstrate a willingness to incorporate feedback and clarify key points in the paper.

Weaknesses:
1. **Limited Validation**: Results are derived from a narrow set of tasks and models, with no validation of GLA's effectiveness on other models like BERT or GPT.
2. **Estimating Pre-training Bias**: The reliance on downstream data for estimating pre-training label bias may lead to inaccuracies if the distributions differ significantly.
3. **Narrative Integration**: The narrative needs improvement to effectively integrate new results and perspectives.
4. **Discussion Clarity**: The discussion on the estimation of $\pi_p$ requires subtle adjustments to accurately reflect its limitations.

### Suggestions for Improvement
We recommend that the authors improve the validation of the GLA method by testing it on a broader range of models, including BERT and GPT, to establish its generalizability. Additionally, we suggest discussing the differences between GLA and existing label bias estimation methods, as well as clarifying the assumptions underlying the proposed method. It would also be beneficial to provide more detailed algorithmic steps for estimating class margins and to address the potential impact of noisy labels and imbalanced test distributions on performance. We recommend that the authors improve the narrative by clearly integrating the new results and perspectives into the main text. Specifically, adjust the discussion regarding the estimation of $\pi_p$ to emphasize that $\log \mathbf{q}$ approximates $\pi_p$ accurately only when the downstream data is similar to the pre-training dataset and that approximating $\pi_p$ may not be optimal for all downstream tasks. Furthermore, including the LAION experiment in the main text to substantiate these points would strengthen the paper's claims. Lastly, ensure that the accuracies on the photo domain are clearly presented to facilitate comparison with the sketch domain results.