ID: JRMSC08gSF
Title: Hints-In-Browser: Benchmarking Language Models for Programming Feedback Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 9, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark of multiple large language models (LLMs) for generating programming feedback directly in the browser, enhancing privacy and reducing costs. The authors evaluate various models, including fine-tuned and compressed versions, and rigorously assess their performance across multiple criteria such as quality, cost, time, and privacy. The tool is open-sourced, allowing for in-browser inference, which is particularly beneficial for educational settings. The authors also introduce new benchmark datasets collected from real-world applications to improve hint generation.

### Strengths and Weaknesses
Strengths:
- The paper provides a comprehensive benchmark of multiple LLMs, rigorously evaluating them across various metrics.
- The use of in-browser inference enhances privacy and reduces costs, making it a practical solution for educational technology.
- The development of a web application demonstrates the feasibility of the proposed approach.
- Open-sourcing datasets and code contributes positively to the research community.

Weaknesses:
- The evaluation metrics lack clarity, particularly regarding their scale.
- The paper overlooks the potential of using a server with a GPU for class-wide inference, which could be more cost-efficient.
- The rationale for using only 25 buggy programs for evaluation is unclear, and there may be issues with near-duplicates in the dataset.
- The discussion on privacy implications and the cost model lacks sufficient depth.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the evaluation metrics, ensuring that their scales are explicitly defined. Additionally, consider discussing the feasibility of using a server-based GPU for inference to enhance cost efficiency. It would be beneficial to expand the evaluation set beyond 25 buggy programs and ensure that there are no near-duplicates. We suggest providing a rationale for the use of "real-world" buggy programs and reporting results across multiple settings. Furthermore, incorporating a detailed explanation of the human annotation process and addressing the potential privacy concerns regarding the Karel visual programming environment would strengthen the paper. Lastly, we encourage the authors to explore the energy consumption of running models locally and to provide detailed documentation on integrating customized datasets into the pipeline.