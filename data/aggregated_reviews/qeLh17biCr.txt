ID: qeLh17biCr
Title: Task Me Anything
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 8, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark generation engine, Task-Me-Anything, designed for evaluating multimodal language models (MLMs) across various scenarios. The engine includes a taxonomy, programmatic task generators, and evaluation algorithms aimed at reducing computational overhead. The authors analyze existing MLMs, highlighting their limitations in counting, spatial, and temporal understanding, while providing a dataset generation scheme that allows for targeted evaluation of specific model capabilities.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and generally clear, making it easy to follow.
- The practical task of automatically creating benchmarks for MLMs is innovative and relevant.
- The proposed benchmark generation process is novel and insightful, offering a flexible and scalable approach to task generation.
- The analysis of MLMs using the proposed engine yields useful conclusions that can guide future developments.

Weaknesses:
- There is uncertainty regarding the domain gap between synthetic images/videos and real-world applications, particularly in critical fields like finance and medicine.
- The generalization ability of the benchmark engine is questionable, as it relies on predefined taxonomies and task templates, suggesting a close-set benchmark.
- The paper lacks experimental comparisons with existing benchmarks, which could validate the effectiveness of the proposed engine.
- Many conclusions drawn are somewhat obvious, such as larger models outperforming smaller ones, raising the need for more novel insights.

### Suggestions for Improvement
We recommend that the authors clarify how natural language queries are translated into actionable queries within the dataset generation engine, explicitly mentioning the use of a semantic parser. Additionally, the authors should bring discussions about the limitations of the framework due to generated data from the supplementary material to the main text and expand on this discussion. It is also crucial to address the domain gap between synthetic and real-world data, and to include experimental comparisons with existing benchmarks to validate the proposed engine's effectiveness. Lastly, the authors should strive to provide more interesting insights beyond common conclusions.