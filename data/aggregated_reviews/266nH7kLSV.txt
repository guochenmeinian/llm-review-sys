ID: 266nH7kLSV
Title: Temporal Graph Neural Tangent Kernel with Graphon-Guaranteed
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 5, 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical exploration of graph neural tangent kernels (GNTK) applied to temporal graphs, introducing a temporal GNTK model with rigorous error bounds. The authors establish a connection between the convergence of the temporal GNTK and the graphon GNTK. Theoretical analyses demonstrate the model's transferability and robustness, while experiments validate its effectiveness on graph-level and node-level tasks. Additionally, the paper introduces a novel approach using the Neural Tangent Kernel (NTK) for temporal graph classification, simplifying the architecture while maintaining or enhancing performance compared to existing methods like GraphMixer. Empirical results show that the proposed method outperforms GraphMixer and other baselines across various datasets, and the authors discuss trade-offs involved in simplifying neural architectures, proposing future research directions, including the potential integration of prototype learning to enhance efficiency.

### Strengths and Weaknesses
Strengths:
- The motivation is strong, addressing the timely topic of temporal graphs and extending GNTK theory to this domain.
- The theoretical analysis is comprehensive, covering generalization bounds, convergence, and time complexity.
- The proposed NTK approach shows lower time complexity and competitive performance compared to recurrent models.
- Empirical results indicate that the method outperforms several existing baselines in temporal graph classification tasks.
- The paper addresses the evolving trend of simplifying neural architectures in the temporal graph learning community.

Weaknesses:
- The theory appears to be an incremental modification of existing work, lacking novelty in its approach.
- Comparisons with traditional temporal graph learning methods remain unclear, particularly regarding how the proposed method captures temporal relationships.
- Strong assumptions underpinning the model raise questions about the validity of some claims, and the time complexity is notably high, limiting scalability.
- There is a lack of theoretical or empirical comparisons regarding performance trade-offs between the proposed method and other architectures.
- The authors acknowledge that their method still requires a substantial amount of training data, which may limit its applicability in certain scenarios.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Equation (1) and explicitly address how node features are incorporated. Additionally, please clarify the initialization of weights as i.i.d. standard normals in the derivation of (6). It would be beneficial to discuss how the proposed kernel compares with typical temporal graph learning methods, such as LSTM, RNN, or transformers, to capture temporal interactions. We also suggest including a discussion on limitations and potential societal impacts of the proposed method. Furthermore, we recommend that the authors enhance the theoretical and empirical comparisons of their method against other architectures to clarify any performance trade-offs. Exploring the integration of prototype learning to reduce the dependency on extensive training data could also enhance the efficiency and applicability of their approach. Finally, consider expanding the related work section to enhance the paper's persuasiveness and include more diverse graph neural architecture-based methods in the experiments.