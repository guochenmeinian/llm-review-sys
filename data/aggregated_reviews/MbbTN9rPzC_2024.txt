ID: MbbTN9rPzC
Title: Quantile Activation: Departing from single point estimation for better generalization across distortions
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 4, 4, 5, -1
Original Confidences: 2, 3, 3, -1

Aggregated Review:
### Key Points
This paper presents a novel activation function called Quantile Activation (QACT), designed to enhance the robustness of neural networks against data distortions. The authors propose an end-to-end framework integrating QACT with modified loss functions and quantile classifiers, validated on benchmark datasets like CIFAR10C and MNISTC. The results indicate that QACT improves generalization and robustness, outperforming state-of-the-art models such as DINOv2 under significant distortions.

### Strengths and Weaknesses
Strengths:  
1. The paper is well organized and clearly written, providing useful empirical and theoretical insights.  
2. The originality of proposing a context-aware activation function demonstrates high quality through extensive validation, showcasing significant potential for enhancing classification model generalization.  
3. The experimental results indicate the superiority of the proposed method over conventional techniques.

Weaknesses:  
1. The proposed method's complexity may lead to overfitting in scenarios with limited training data.  
2. The evaluations are limited, as only a few methods are compared, lacking the most recent state-of-the-art approaches, which constrains understanding of the method's true contribution.  
3. The concept of context dependency is not clearly explained until the conclusion, and the motivation in the introduction is inadequately articulated.  
4. The paper lacks a comprehensive review of related work on robustness, and the comparative analysis with other robustness methods is insufficient.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the context dependency concept by introducing and elaborating on it earlier in the paper. Additionally, the motivation and fundamental problem should be articulated more clearly in the introduction. We suggest including a more comprehensive review of related work on robustness to input distortions and expanding the comparative analysis to include other state-of-the-art methods specifically designed for robustness. Furthermore, we encourage the authors to clarify confusing notations and terminology throughout the paper to enhance readability. Finally, we recommend demonstrating the computational speed and memory requirements of QACT compared to classical activation functions in experiments.