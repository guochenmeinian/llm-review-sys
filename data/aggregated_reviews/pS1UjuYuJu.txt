ID: pS1UjuYuJu
Title: Vision and language representations in multimodal AI models and human social brain regions during natural movie viewing
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 8, 7, 7, 6, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 5, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the effectiveness of multimodal Deep Neural Networks (DNNs) in predicting fMRI activity in social regions during naturalistic movie viewing. The authors investigate the alignment of vision and language embeddings and find that visual-linguistic alignment does not enhance the model's ability to match neural responses in social contexts. Key claims include that vision and language embeddings are often unaligned, that language-aligned vision models perform worse than language-only models, and that language alignment does not improve STS prediction performance for vision encoders.

### Strengths and Weaknesses
Strengths:
1. The paper extends prior work on multimodal brain encoding models.
2. It utilizes specific models like SLIP, providing controlled comparisons.
3. The dataset focuses on social interactions, an unexplored area.
4. The use of GLM analysis to identify regions of interest (ROIs) is commendable.

Weaknesses:
1. The article lacks clarity on technical terms, such as TR (repetition time), which hinders understanding of the data fed to the models.
2. The performance of the SLIPtext model is constrained by its training methodology, limiting its effectiveness in capturing rich language information.
3. Claims regarding non-overlapping information between vision and language models may be overstated due to SLIPtext's limitations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of technical terms, particularly by defining TR and discussing the selection of image-text pairs. Additionally, we suggest exploring different frame-combining strategies or citing studies that address their effects on results. To strengthen claims about model performance, consider testing other models with more integrated vision and language encoders. Lastly, we encourage the authors to investigate the implications of using video models, as they may enhance the alignment of predictions with fMRI data.