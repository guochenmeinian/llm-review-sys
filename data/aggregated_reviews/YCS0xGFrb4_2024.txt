ID: YCS0xGFrb4
Title: Regularized Conditional Diffusion Model for Multi-Task Preference Alignment
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach using a regularized conditional diffusion model to align with preferences in multi-task reinforcement learning (RL) scenarios. The proposed method generates preference representations that effectively distinguish between various task trajectories and returns. By maximizing mutual information between conditions and generated trajectories, the model regularizes existing diffusion frameworks, enabling it to produce optimal task-specific trajectories aligned with given preferences. Extensive experiments demonstrate the method's performance and generalization ability across different tasks.

### Strengths and Weaknesses
Strengths:
- The motivation for generating preference representations to differentiate tasks is clear and well-articulated.
- The method and its designs are reasonable and effectively capture versatile preference representations, enhancing alignment with human intents.
- The introduction of a mutual information regularization objective improves consistency between conditions and generated trajectories.

Weaknesses:
- The results indicate that CAMP does not consistently outperform existing methods, raising concerns about its effectiveness in learning good task representations.
- The method exhibits high computational complexity, necessitating a discussion on space and time complexity for comprehensive comparisons.
- There is a lack of emphasis on distinguishing the proposed method from similar approaches like OPPO and MTDiff, and the contribution list needs updating to reflect unique contributions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the distinctions between their method and existing approaches, particularly OPPO and MTDiff, in the main text. Additionally, we suggest providing a detailed discussion on the computational costs associated with the model. An ablation study on the influence of the number of tasks should be included to assess how task representations vary with different task combinations. Furthermore, the authors should address the observed performance variations across tasks and discuss the impact of the quality of preference data on results.