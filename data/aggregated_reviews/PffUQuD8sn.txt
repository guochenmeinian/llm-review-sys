ID: PffUQuD8sn
Title: Statistical Depth for Ranking and Characterizing Transformer-Based Text Embeddings
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for computing distributions of a corpus using transformer embeddings, extending the concept of statistical depth into the embedding space. The authors employ distance functions, such as cosine similarity, and demonstrate that their sampling method outperforms baseline samplings. They propose transformer-based text embedding (TTE) depth, which is applied to various NLP tasks, including prompt creation and measuring differences between human-written and machine-generated text. However, the performance improvements over baselines are relatively small.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and addresses an interesting research topic.
- The proposed method and its associated Python package could be valuable contributions to the community.
- Thorough ablation experiments are conducted to determine the optimal number of samples for effectiveness.

Weaknesses:
- The claimed performance gains are minor and not statistically significant enough to be considered superior to random selection.
- Evaluation on the prompt creation task is limited, and the approach may not generalize well to real-world datasets with noisy data.
- The organization of the paper could be improved, particularly by merging Sections 4 and 5, and refining Section 3 for clarity.

### Suggestions for Improvement
We recommend that the authors improve the evaluation on the prompt creation task by obtaining a larger evaluation set using the ChatGPT API. Additionally, the authors should clarify the meaning of "Z" in Eq (1) and address how their approach can prevent the selection of noisy data from the corpus. Furthermore, we suggest refining the organization of the paper for better clarity and coherence, particularly in Sections 3, 4, and 5.