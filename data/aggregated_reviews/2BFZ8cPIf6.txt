ID: 2BFZ8cPIf6
Title: Learning Functional Transduction
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 5, 7, 8, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel deep learning approach for meta-learning, leveraging the theory of reproducing kernel Banach spaces (RKBS). The authors propose a method that jointly trains a deep transformation and a parametrized kernel function during the meta-training stage. The model demonstrates the ability to learn in-context, inferring functional relationships for new datasets. Experimental results validate the effectiveness of the proposed method across various tasks, including PDE modeling and MNIST-like datasets.

### Strengths and Weaknesses
Strengths:  
- The motivation for addressing meta-learning is strong, and the connection to RKBS may inspire new ideas in the field.  
- The framework for transductive learning through reproducing kernels is novel and well-articulated.  
- The experiments effectively showcase the benefits of the approach, particularly in PDE modeling.  
- The paper is generally clear, despite some complexity in the underlying ideas.  

Weaknesses:  
- The proposed method requires a meta-training procedure, raising concerns about fairness in experimental comparisons.  
- There is a lack of thorough discussion on the method's superiority over existing meta-learning approaches.  
- The initial discussion on transduction versus inference is misleading, conflating terms that could confuse readers.  
- The computational complexity of the method, particularly regarding FFT execution, is not adequately addressed.  

### Suggestions for Improvement
We recommend that the authors improve the comparison between their method and other meta-learning approaches, providing specific points of superiority. Additionally, including an experiment on in-context learning of language patterns would strengthen the paper. Clarifying the distinction between transduction and inference in the context of few-shot learning would enhance understanding. Discussing the computational complexity of the method, especially regarding FFTs, is crucial. Finally, addressing the potential for releasing code would benefit the community.