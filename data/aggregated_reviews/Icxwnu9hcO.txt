ID: Icxwnu9hcO
Title: NIS3D: A Completely Annotated Benchmark for Dense 3D Nuclei Image Segmentation
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 8, 6, 8, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 5, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the NIS3D dataset, a resource for 3D embryonic cell image segmentation, containing annotations for 22,000 cells across five volumes from multiple species. The dataset features high-density images with varying signal-to-noise ratios and shapes, and it includes instance segmentation labels created by skilled annotators, supplemented by confidence scores to reflect annotator agreement. The authors benchmark existing 3D segmentation methods, which have shown unsatisfactory results on this dataset, highlighting the challenges of 3D instance segmentation compared to 2D. Additionally, the paper introduces PrinCut, a semi-automatic annotation tool designed to enhance the efficiency and accuracy of 3D cell boundary annotation. PrinCut utilizes both morphological and intensity information through a max-flow min-cut algorithm, reportedly improving annotation speed by approximately tenfold compared to manual methods and addressing human bias by detecting subtle intensity changes.

### Strengths and Weaknesses
Strengths:  
- The NIS3D dataset is the first benchmark providing densely annotated 3D cell volumes, valuable for future model development.  
- It includes a confidence score to measure inter-annotator variability, enhancing the reliability of evaluations.  
- The authors conducted a thorough experimental comparison of multiple 3D segmentation methods, demonstrating the dataset's utility.  
- The use of PrinCut significantly increases annotation efficiency and accuracy.  
- PrinCut effectively addresses human bias by detecting subtle features in the data.  
- The proposed training/test split settings will enhance the robustness of future evaluations.

Weaknesses:  
- The dataset's size, with only five images, is relatively small compared to other datasets, limiting its effectiveness for comprehensive evaluations.  
- The current labeling approach may not ensure continuity of cell surfaces in 3D, as it primarily relies on 2D slice annotations.  
- There is a lack of discussion on inter-annotator variability and existing related datasets, which could provide context for the benchmark.  
- The dual utility of the segmentation algorithm in the STAPLE evaluation may introduce bias.  
- The iterative nature of the STAPLE algorithm can be time-consuming for large datasets.

### Suggestions for Improvement
We recommend that the authors increase the dataset size to at least ten images to enhance its credibility and evaluation capabilities. Additionally, we suggest conducting experiments to assess how the dataset can refine pre-trained models, potentially splitting it into subsets for training and testing. The authors should also ensure that the 3D labeling maintains continuity of cell surfaces and clarify their methodology for resolving ties in annotations. Furthermore, including a default train/test split and discussing existing datasets, such as the C. elegans nuclei dataset, would provide a more comprehensive context for their work. Lastly, we encourage the authors to elaborate on the proposed evaluation metric by situating it within the broader landscape of related work on incorporating annotator agreement into evaluation metrics. We also recommend improving the discussion of the STAPLE algorithm's limitations, particularly regarding its dual utility and potential biases in evaluation, and including a detailed comparison of the STAPLE algorithm with the proposed method in the revised manuscript to clarify the advantages of the approach.