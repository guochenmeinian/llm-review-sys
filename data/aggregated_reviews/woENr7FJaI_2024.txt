ID: woENr7FJaI
Title: Automated Multi-level Preference for MLLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 5, 6, -1, -1, -1
Original Confidences: 4, 5, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Automated Multi-level Preference (AMP) framework aimed at improving Multimodal Large Language Models (MLLMs) by addressing hallucination issues. The framework includes an automated dataset generation pipeline for creating multi-level preference datasets without human annotators and introduces the Multi-level Direct Preference Optimization (MDPO) algorithm. The authors also propose a novel multi-round dialogues hallucination benchmark, MRHal-Bench, and conduct extensive experiments demonstrating the effectiveness of their method across multiple benchmarks.

### Strengths and Weaknesses
Strengths:
- The introduction of multi-level preferences enhances the model's ability to discern subtle differences and integrate cross-level comparisons.
- The automated pipeline for generating high-quality multi-level preference datasets is a significant contribution, potentially reducing bias and saving resources.
- Comprehensive experiments across various benchmarks validate the proposed method's effectiveness.

Weaknesses:
- The paper's contribution largely hinges on the preference fine-tuning algorithm, showing limited innovation beyond this aspect.
- The method does not exhibit significant improvements on the LLaVA-Bench benchmark.
- Performance on adversarial tasks in the POPE benchmark is moderate, indicating a need to reassess the impact of MDPO on model robustness.
- There is a lack of intrinsic evaluation of the automated multi-level preference dataset, making it unclear what artifacts may be introduced.
- Missing comparisons with rank-based preference alignment approaches and results for FGAIF on MRHal-Bench limit the thoroughness of the analysis.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of the automated dataset generation pipeline by providing quantitative information on the generated preference datasets, possibly including a comparison with human annotations. Additionally, conducting more experiments on general benchmarks such as TextVQA, GQA, and IconQA would enhance the general applicability of the method. The authors should also consider updating the baseline model comparisons to include more current models like LLaVA-v1.6, DeepSeek-VL, or MiniCPM-V. Furthermore, addressing the lack of intrinsic evaluation for the automated dataset and engaging with prior literature on non-binary preference alignment would strengthen the paper. Lastly, including the missing results for FGAIF on MRHal-Bench would provide a more comprehensive comparison.