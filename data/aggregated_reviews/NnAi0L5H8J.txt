ID: NnAi0L5H8J
Title: Multi-Instance Partial-Label Learning with Margin Adjustment
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 7, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach to address the multi-instance partial-label learning (MIPL) problem, where each training sample is a bag of instances linked to candidate labels. The authors propose the MIPLMA algorithm, which introduces a margin-aware attention mechanism and a margin-compliant loss function to mitigate issues with high prediction probabilities on non-candidate labels. Experimental results demonstrate MIPLMA's superior performance compared to existing MIPL and other multi-instance algorithms.

### Strengths and Weaknesses
Strengths:
1. The paper is well-organized and comprehensible.
2. The experiments are comprehensive and validate the proposed method.
3. The theoretical analysis supports the effectiveness of the model.

Weaknesses:
1. The margin-aware attention mechanism, which utilizes a changing temperature parameter, appears trivial, and the impact of attention score gaps on reducing non-candidate label predictions is not adequately explained.
2. The margin-compliant loss function may be challenging to optimize within a neural network framework, requiring more detailed exposition.
3. The novelty of the margin-aware attention and loss mechanisms is questioned, as they have been previously utilized in other contexts.
4. The algorithm does not consistently outperform existing methods across all datasets and settings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the margin-aware attention mechanism by providing a more detailed explanation of how attention score gaps influence predictions on non-candidate labels. Additionally, the authors should elaborate on the optimization process of the margin-compliant loss function to enhance its applicability within neural networks. We suggest including comparisons with recent partial-label learning algorithms like PICO to strengthen the paper's contributions. Furthermore, the authors should clarify the definition of negative instances to avoid confusion and consider conducting additional experiments to support claims regarding the superiority of features learned by ResNet on various benchmarks. Lastly, we recommend discussing the parameter \(\lambda\) in the margin regularization scheme more thoroughly, particularly its initial value and adjustment strategy throughout training.