ID: hSkEcIFi3o
Title: Adversarial Examples Are Not Real Features
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 5, 8, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper builds upon the work of Ilyas et al. by challenging the definitions of robust and non-robust features. The authors demonstrate that non-robust features are task-specific and that robust features do not guarantee robustness across different tasks. They employ linear probing and self-supervised learning on CIFAR10 datasets to investigate these claims, revealing that non-robust features are not universally useful and that models trained solely on robust features exhibit vulnerabilities.

### Strengths and Weaknesses
Strengths:
- The paper extends the definitions of robust and non-robust features, introducing concepts like "Absolute Useful" and "Relative Useful" features, which enhance understanding of feature importance.
- The authors pose compelling research questions and empirically investigate them, leading to significant findings about the nature of adversarial examples and feature utility.

Weaknesses:
- The presentation suffers from typographical errors and unclear references to figures, which detracts from readability and clarity.
- The paper lacks a comprehensive overview of self-supervised learning methods and their dataset construction, which would aid understanding.
- Some claims, particularly regarding the robustness of datasets, are not sufficiently supported by experimental evidence.

### Suggestions for Improvement
We recommend that the authors improve the manuscript's presentation by thoroughly proofreading for typographical errors and ensuring all figures are correctly referenced. Additionally, we suggest including a brief overview of the self-supervised learning methods used and clarifying the distinction between "tasks" in the context of self-supervised learning versus multi-task learning. It would also be beneficial to explicitly state the research questions in the introduction and discuss their relevance. Lastly, we encourage the authors to explore the robustness of their models under different attack methods, such as PGD and CW, to validate their findings further.