ID: nK6OnCpd3n
Title: Text-Aware Diffusion for Policy Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 4, 6, 4, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Text-Aware Diffusion for Policy Learning (TADPoLe), a method that utilizes pretrained text-conditioned diffusion models to generate dense reward signals for reinforcement learning tasks. The approach enables agents to learn text-aligned behaviors without requiring expert demonstrations or handcrafted reward functions. TADPoLe demonstrates effectiveness across various environments, achieving zero-shot policy learning with natural language inputs. The framework shows capabilities in robotic environments, particularly on Adroit tasks (door, pen, hammer), while acknowledging limited performance on the FrankaKitchen suite, solving only one of five tasks. The authors emphasize the inherent generalization capabilities of their approach and propose that future improvements could include fine-tuning on in-domain demonstrations.

### Strengths and Weaknesses
Strengths:
- The authors introduce a novel approach to reward generation using pretrained diffusion models, alleviating the need for manually crafted reward functions.
- TADPoLe supports zero-shot policy learning, allowing agents to learn from natural language descriptions.
- The framework demonstrates promising results on complex dexterous manipulation tasks without fine-tuning and highlights generalization across various robotic environments.
- The authors provide transparency regarding performance limitations on the FrankaKitchen suite.

Weaknesses:
- The paper suffers from significant presentation issues, with unclear explanations of components like the 'symlog' transformation and 'noise level range'.
- The motivation for using well-rendered videos does not align with the goal of enhancing the practicality of reinforcement learning.
- Experimental evaluations are limited, lacking comprehensive comparisons with other methods, particularly the Diffusion-Reward method.
- The ablation study is insufficient, failing to justify the design choices of the 'symlog' component and weight selections.
- Limited success on the FrankaKitchen tasks raises concerns about the robustness of the approach in diverse environments.
- The lack of fine-tuning may hinder performance compared to models that utilize in-domain demonstrations.
- The writing lacks clarity, making it difficult for readers to fully grasp the methodology and its implications.

### Suggestions for Improvement
We recommend that the authors improve the clarity and coherence of the writing to enhance readability. Additionally, the authors should expand the range of tasks in their experiments and include comparisons with the Diffusion-Reward method to better contextualize TADPoLe's performance. A more detailed ablation study is necessary to isolate the effects of the 'symlog' transformation and the specific weightings used. Furthermore, the authors should address the scalability issue regarding the reliance on well-rendered videos for environments lacking such resources. We also suggest exploring fine-tuning strategies for language-grounding on the FrankaKitchen tasks, as indicated in the literature, and incorporating more detailed comparisons with state-of-the-art models that leverage in-domain demonstrations to strengthen the argument for TADPoLe's generalization capabilities.