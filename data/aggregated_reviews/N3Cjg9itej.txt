ID: N3Cjg9itej
Title: GAUSS: GrAph-customized Universal Self-Supervised Learning
Conference: ACM
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GAUSS, a self-supervised learning framework for graph neural networks (GNNs) aimed at achieving universality and generalization. The authors propose a method that replaces global parameters with locally learnable propagation matrices tailored for each node's ego-network. Theoretical analyses and comprehensive experiments demonstrate the effectiveness of GAUSS across various benchmarks, including homophilic and heterophilic graphs.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant open problem in universal self-supervised learning for graphs.
- It proposes a principled approach to locally learning graph structure in a self-supervised manner.
- GAUSS achieves state-of-the-art results on both homophilic and heterophilic datasets and shows robustness against graph perturbations.

Weaknesses:
- The experimental evaluations are weak, focusing on small graphs with the largest having fewer than 20k nodes, which is inadequate for real-world applications.
- The organization of the paper could be improved, with excessive emphasis on preliminaries over methodology, making it difficult to follow.
- Some claims lack clarity, particularly regarding how GAUSS better utilizes flexible GNN encoders compared to existing methods.
- The proposed method appears to be a trivial extension of existing self-expressive models, with limited novelty.
- Missing optimization details for Problem (10) and unclear notation usage detract from the clarity of the writing.

### Suggestions for Improvement
We recommend that the authors improve the experimental evaluations by testing GAUSS on larger, real-world web-scale graphs to better demonstrate its applicability. Additionally, we suggest enhancing the organization of the paper to provide a clearer methodology section. The authors should clarify how their approach leverages flexible GNN encoders and provide a detailed analysis of the computational complexity compared to baselines. Finally, we encourage the authors to address the missing optimization details and ensure consistent notation throughout the paper.