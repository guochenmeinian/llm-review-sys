ID: lSbbC2VyCu
Title: Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 7, 4, 6, 4, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "rewarded soup" (RS), a technique for combining policies trained on different rewards into a single policy that performs well on a convex combination of those rewards. The method involves linearly interpolating weights of individual policies, leveraging their shared initialization and linear connectivity during finetuning. The authors propose a strategy for evaluating the performance of RS against a linearized multi-objective reinforcement learning (MORL) approach, using a fixed preference $\hat{\mu}$ to benchmark RS against an oracle policy that maximizes a linear combination of two rewards. The authors argue that RS can mitigate reward misspecification and enhance alignment with human preferences, while also discussing the benefits of weight interpolation in RS, which may lead to improved performance due to implicit regularization and variance reduction.

### Strengths and Weaknesses
Strengths:
- The paper addresses significant challenges in aligning generative models with diverse human preferences and adapting to changing reward specifications.
- RS is theoretically well-founded as a Pareto coverage set of policies, with empirical evidence supporting its effectiveness.
- The finding that linear mode connectivity applies to RL policies trained on different rewards is noteworthy, and the paper is well-structured and clear.
- The authors provide a clear framework for comparing RS with linearized MORL, emphasizing the importance of benchmarking against an oracle policy.
- The discussion of weight interpolation benefits offers valuable insights into why RS may outperform linearized MORL in certain scenarios.
- The experiments are comprehensive and compelling, demonstrating the method's applicability across various tasks.

Weaknesses:
- The discussion on reward misspecification lacks nuance; claims regarding RS's ability to mitigate this issue should be more cautiously framed, as they pertain to specific types of misspecification.
- Comparisons to existing multi-objective RL approaches are limited, leaving gaps in understanding the potential for achieving Pareto optimal trade-offs.
- The novelty of the paper is questioned, as it primarily applies existing weight interpolation techniques to a new domain without sufficient theoretical analysis or quantitative comparisons of computational costs.
- There is a lack of comprehensive comparisons to existing multi-objective RL literature, leaving some concerns unresolved.
- The explanation regarding the performance of RL on scalarized rewards compared to RS is not entirely convincing, raising questions about the robustness of the RL experiments.

### Suggestions for Improvement
We recommend that the authors improve the discussion on reward misspecification by clarifying the specific contexts in which RS is effective. Additionally, we suggest including more comparisons to multi-objective RL methods to better contextualize the contributions of RS. The authors should also provide theoretical analysis to support their hypotheses and include ablation studies to explore how variations in reward differences and the number of networks affect performance. Furthermore, we recommend that the authors improve their comparisons to multi-objective RL literature by including more results for review. Lastly, we suggest closely examining the potential issues with their RL-related experiments, particularly regarding the competitiveness of scalarized rewards against RS, and providing further empirical evidence or clarifications on these points to strengthen the paper's arguments.