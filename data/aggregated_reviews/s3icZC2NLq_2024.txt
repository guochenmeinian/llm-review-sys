ID: s3icZC2NLq
Title: A Nearly Optimal and Low-Switching Algorithm for Reinforcement Learning with General Function Approximation
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 6, 8, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on reinforcement learning with low switching cost and general function approximation, proposing the MQL-UCB algorithm. The algorithm features a deterministic policy-switching strategy that minimizes switching costs, a monotonic value function structure with controlled complexity, and a variance-weighted regression scheme for high data efficiency. MQL-UCB achieves minimax optimal regret and a near-optimal switching cost, matching the lower bound for both metrics.

### Strengths and Weaknesses
Strengths:
1. The problem addressed is interesting and relevant within the context of reinforcement learning.
2. The paper is solid, with correct proofs and strong bounds, particularly the optimal switching cost bound.
3. The presentation is generally clear, and the algorithm is intuitive and well-structured.

Weaknesses:
1. Some assumptions, such as the completeness of all functions and the existence of a bonus oracle, appear overly strong and may limit the applicability of the results.
2. The algorithm's complexity raises concerns about practical implementation, and comparisons with existing methods, such as LSVI-UCB, are lacking.
3. The paper does not sufficiently clarify how it improves upon related work, particularly regarding the counting of switches and the assumptions made.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their assumptions, particularly the completeness and Eluder dimension, and provide justification for their necessity. Additionally, including empirical experiments, even in simplified settings, would help demonstrate the algorithm's practical performance compared to standard approaches. A more detailed discussion on the counting of switches in relation to prior work would enhance understanding. Finally, insights into the algorithm's performance in scenarios with a smaller number of episodes compared to the length of episodes would be valuable.