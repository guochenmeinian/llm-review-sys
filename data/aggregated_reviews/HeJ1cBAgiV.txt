ID: HeJ1cBAgiV
Title: SCAFFLSA: Taming Heterogeneity in Federated Linear Stochastic Approximation and TD Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 6, 7, 6, -1, -1
Original Confidences: 2, 2, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the convergence of Scaffnew in the quadratic setup, achieving a linear speedup in the number of clients, which was not attained by the original Scaffnew paper. The authors also introduce a new algorithmic variant, SCAFFLSA, which reduces communication between agents while maintaining linear speedup. The paper provides a non-asymptotic analysis of Federated Linear Stochastic Approximation, offering finite-time MSE bounds for general LSA and TD learning under i.i.d. noise, with a separate unbiased bound for Markovian noise.

### Strengths and Weaknesses
Strengths:  
- The paper proposes a new analytical framework for analyzing the sample and communication complexity of FedLSA and extends this framework to Federated TD learning.  
- The introduction of SCAFFLSA significantly improves the sample and communication complexity compared to FedLSA and Scaffnew.  

Weaknesses:  
- The achieved speedup is limited to the quadratic setup, and the application of the quadratic loss function is quite restricted.  
- The organization of the paper is poor, making it difficult to follow due to the abundance of symbols and equations. Contributions related to i.i.d. and Markovian noise appear scrambled, and the experiments are relatively weak. Additionally, there is a missing "c" superscript above equation (1) in Algorithm 1.

### Suggestions for Improvement
We recommend that the authors improve the organization of the paper by separating the results into distinct sections for i.i.d. noise, Markovian noise, and TD learning to enhance readability. We also suggest providing more robust experimental results and applications of SCAFFLSA, particularly in the context of federated TD learning. Furthermore, clarifying the functions $\boldsymbol{A}^c()$ and $\boldsymbol{b}^c()$ with practical examples would be beneficial. Lastly, addressing the missing "c" superscript in Algorithm 1 and revising the paper for clarity and typos is essential.