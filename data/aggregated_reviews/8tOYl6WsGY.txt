ID: 8tOYl6WsGY
Title: BoostAdapter: Improving Vision-Language Test-Time Adaptation via Regional Bootstrapping
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a training-free test-time adaptation approach for vision-language models (VLMs) that integrates entropy minimization with a training-free adaptor to enhance adaptation performance. The authors maintain a key-value memory for feature retrieval from both historical and boosting samples, with experimental results indicating the method's effectiveness across various benchmarks.

### Strengths and Weaknesses
Strengths:
1. The combination of entropy minimization and a training-free adaptor is effective for test-time adaptation on VLMs.
2. The figures are clear, particularly the performance figure, which is intuitive.
3. The method is theoretically supported, enhancing its credibility.
4. The experiments are comprehensive and well-executed.

Weaknesses:
1. The contributions may be viewed as incremental, lacking significant novel insights.
2. **Figure 2 (b):** The double-arrow directions should be colored in green and orange for clarity.
3. **Figure 3:** The term "Boosting Cache" is not adequately explained, leading to confusion.
4. Several typographical errors need correction.
5. The paper lacks a parameter study, such as the threshold $\tau$.
6. In Figure 4b, details on how the model performs entropy minimization and updates learnable prompts are missing.
7. The term "Hand-crafted Prompt" is unclear, and the stability of performance across different prompts needs investigation.
8. The method's categorization into TTA is questionable, as it maintains low-entropy samples per class in the cache.

### Suggestions for Improvement
We recommend that the authors improve the clarity of **Figure 2 (b)** by using distinct colors for the double-arrow directions. Additionally, they should provide an explanation for the term "Boosting Cache" in **Figure 3** and correct the identified typographical errors. A parameter study, particularly regarding the threshold $\tau$, should be included to enhance the analysis. The authors should also clarify the setup for entropy minimization in **Figure 4b** and define what constitutes a "Hand-crafted Prompt," while investigating the method's performance stability across different prompts. Finally, we suggest that the authors elaborate on the unique insights their approach offers compared to existing methods and address the questions regarding the method's performance relative to TDA and its application to various datasets.