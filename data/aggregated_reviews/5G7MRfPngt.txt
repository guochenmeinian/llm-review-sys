ID: 5G7MRfPngt
Title: VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Programs of Thought
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 6, 5, 9, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ICAL, which aims to enhance decision-making in large language and vision-language models by generating optimized trajectories and language annotations from noisy demonstrations and human feedback. The authors propose a method that abstracts noisy trajectories into optimized sequences with language comments, refined through human feedback during execution. Experiments demonstrate that ICAL significantly improves performance on benchmarks like TEACh, VisualWebArena, and Ego4D, surpassing state-of-the-art methods.

### Strengths and Weaknesses
Strengths:
- ICAL significantly enhances decision-making and task success rates across various benchmarks.
- The method reduces reliance on expert examples by generating useful abstractions from sub-optimal demonstrations and human feedback.
- ICAL is versatile, effectively applied in dialogue-based instruction following, multimodal web tasks, and video action anticipation.
- The approach integrates human feedback to refine abstractions, improving agent performance over time.

Weaknesses:
- The method still depends on human feedback for refining abstractions, which may not always be feasible or scalable.
- The process heavily relies on GPT-4V, with no ablation study provided for replacing it.
- The effectiveness of ICAL is constrained by the capabilities of underlying Vision-Language Models (VLMs).
- Performance may be affected by the quality of initial noisy demonstrations and the accuracy of human feedback.
- The scaling capability of the proposed method is unclear, as it requires each trajectory to undergo a human-in-the-loop fine-tuning process, which may be inefficient.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the human-in-the-loop fine-tuning phase and provide more details on how feedback was obtained, particularly for the TEACh experiment. Additionally, we suggest including an ablation study to explore the impact of replacing GPT-4V. It would also be beneficial to discuss the efficiency of human feedback compared to direct editing of sub-optimal trajectories. Finally, we encourage the authors to address the limitations of their approach more thoroughly, including common failure modes and potential scalable solutions.