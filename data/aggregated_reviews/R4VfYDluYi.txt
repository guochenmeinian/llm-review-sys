ID: R4VfYDluYi
Title: Learning Co-Speech Gesture for Multimodal Aphasia Type Detection
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, 4

Aggregated Review:
### Key Points
This paper presents a novel multimodal graph neural network (GNN) framework for detecting aphasia types by integrating language and gesture information. The authors conduct an ablation study to evaluate the effectiveness of linguistic, acoustic, and gesture features, revealing that gesture features outperform acoustic features. Additionally, the authors analyze the impact of gender on aphasia type detection, contributing to a more nuanced understanding of the condition.

### Strengths and Weaknesses
Strengths:  
- The paper introduces a novel approach to detecting aphasia types, enhancing diagnostic accuracy and treatment strategies.  
- The GNN model demonstrates superior performance compared to existing state-of-the-art models, supported by thorough experimental and qualitative analyses.  
- The authors provide detailed explanations of the model and promise to share the code with the community, fostering reproducibility.

Weaknesses:  
- The dataset of 3651 samples may be insufficient for training a complex model, and the authors do not report the number of parameters in the final model.  
- There is a lack of clarity regarding the comparison between text and gesture features, as Table 4 suggests that text features may outperform gesture features, which requires further explanation.  
- The authors do not report the ASR word error rate performance for the Whisper transcription, raising concerns about the impact of disfluencies on detection accuracy.

### Suggestions for Improvement
We recommend that the authors improve the dataset size or provide a detailed justification for its adequacy given the model's complexity. Additionally, the authors should clarify the comparison between text and gesture features, particularly in light of Table 4. It would be beneficial to report the ASR WER performance to substantiate claims regarding Whisper's capabilities. Furthermore, we suggest correcting the statement about Whisper's ability to capture disfluencies, ensuring it accurately reflects the capabilities of traditional ASR systems. Lastly, enhancing the clarity and size of Figure 1 would improve the readability of gesture depictions.