ID: 8AKBcTXEd3
Title: Unifying Discrete and Continuous Representations for Unsupervised Paraphrase Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach for unsupervised paraphrase generation that unifies discrete and continuous representations, addressing limitations of existing methods such as back-translation and denoising. The authors propose a self-supervised pseudo-data construction method trained on non-parallel corpora, utilizing a VQ-VAE architecture to maintain entity faithfulness while generating diverse paraphrases. The evaluation is conducted on WikiAnswers and Quora datasets, introducing a new metric, Entity-Score, to assess entity precision and recall. The results indicate that the proposed method outperforms existing state-of-the-art approaches.

### Strengths and Weaknesses
Strengths:
- The novel approach to unsupervised paraphrase generation is promising and shows strong experimental results.
- The introduction of the Entity-Score metric is a significant advancement for evaluating entity mention faithfulness.
- The paper is well-organized and presents relevant baselines, enhancing the clarity of the methodology.

Weaknesses:
- Some aspects of the methodology, particularly the training of the pseudo-data constructor and the handling of entities, lack clarity and require further elaboration.
- The paper does not evaluate its approach against popular datasets like ParaNMT or models such as ChatGPT, which could provide a more comprehensive assessment of its performance.
- There are concerns regarding the grammatical correctness of generated sentences due to word shuffling.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the pseudo-data constructor's training process and provide more details on how entities are handled during inference. Additionally, we suggest evaluating the proposed method against established datasets like ParaNMT and comparing it with models such as ChatGPT to validate its effectiveness. Lastly, addressing the grammatical correctness of generated sentences would strengthen the overall presentation of the results.