ID: CoCiPVtNSu
Title: GNNShap: Scalable and Accurate GNN Explanation using Shapley Values
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GNNShap, a Shapley value-based explanation method for graph neural networks (GNNs) that aims to enhance computational efficiency. The authors propose several techniques, including pruning the computational graph, parallelized fast sampling, and efficient matrix multiplication for Shapley value computation. Experimental results indicate that GNNShap achieves higher fidelity scores in shorter times compared to existing methods.

### Strengths and Weaknesses
Strengths:
1. The paper is well-motivated, with a clear summary of current Shapley value-based interpretability algorithms.
2. The proposed method demonstrates improved efficiency and higher fidelity scores in GNN explanations.
3. The architecture design is clear and accessible, and the experiments are comprehensive.

Weaknesses:
1. The novelty of the proposed method is unclear, as many techniques appear to be simple modifications of existing methods.
2. There is a lack of ablation studies to clarify the effects of different components in the proposed method.
3. The experimental results are inconsistent, with some datasets showing only marginal improvements over baseline methods.
4. Baselines for comparison are outdated, and more recent methods should be included.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the technical contributions by elaborating on the differences between their sampling method and existing methods. Additionally, we suggest including an ablation study to assess the impact of various components in GNNShap. The authors should also provide more comprehensive comparisons with recent interpretability methods and clarify the reported running times for all algorithms. Finally, addressing the inconsistencies in experimental results and providing insights into the complexity of generating explanations would strengthen the paper.