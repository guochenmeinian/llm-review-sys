ID: 2W4LxJbgec
Title: Scaling laws for language encoding models in fMRI
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 6, 7, 8, 5, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 3, 5, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an evaluation of the impact of language model (LM) scale (in terms of N parameters) and fMRI dataset size on the performance of downstream encoding models predicting individual voxel activity in the brain. The authors explore both text-based models (OPT and LLaMa) and acoustic models (HuBERT, WavLM, and Whisper), finding that fMRI encoding performance improves logarithmically with model scale, reaching an asymptote around 30B parameters for text models, while acoustic models show ongoing improvement. Additionally, increasing the fMRI data per subject significantly enhances performance. The study suggests a need for deeper neuroscience data collection and increased computational resources for encoding analyses.

### Strengths and Weaknesses
Strengths:
- The work explores a novel and relevant axis for brain encoding analyses, contextualized with related work.
- The submission is technically sound, with well-supported claims, clear writing, and organization.
- Results are significant and may encourage scaling up neural encoding analyses.

Weaknesses:
- The evaluation of LM scale solely based on N parameters is reductive and requires addressing.
- There is a lack of uncertainty quantification in the results, which needs to be specified.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of LM performance by considering the size of the pretraining dataset, as differences between LLaMa and OPT models may be significant. Additionally, we encourage the authors to visualize the mean perplexity across the evaluated dataset for a more functional view of model performance. Clarification on what the error bars represent in figures 1b and 1e is necessary, and uncertainty quantification should be included for other figures. We suggest increasing the resolution of figures 1a, 1b, 1d, 1e, 3b, and 3c. Lastly, addressing the potential ill-conditioning of regression problems as model size increases would strengthen the paper.