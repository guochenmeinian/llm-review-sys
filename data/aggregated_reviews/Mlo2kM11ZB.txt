ID: Mlo2kM11ZB
Title: Birder: Communication-Efficient 1-bit Adaptive Optimizer for Practical Distributed DNN Training
Conference: NeurIPS
Year: 2023
Number of Reviews: 23
Original Ratings: 5, 5, 4, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents BRAM, a one-bit adaptive optimizer designed for distributed machine learning, which compresses model updates to enhance communication efficiency. The authors propose a Hierarchical-1-bit-All-Reduce algorithm that leverages high intra-node bandwidth while maintaining a lightweight computational footprint. BRAM employs element-wise quantization for the entire update of an adaptive optimizer, contrasting with existing methods like SSDM that utilize vector-wise quantization. Theoretical analysis supports BRAM's convergence properties, asserting that it achieves a convergence rate comparable to uncompressed methods like Adam. Experiments demonstrate BRAM's performance across various neural network architectures, including ResNet-50 and BERT-Base, showing comparable results to uncompressed methods like SGDM and Adam.

### Strengths and Weaknesses
Strengths:
- The paper introduces a compute-efficient optimizer that effectively reduces communication overhead while maintaining fast convergence speed and high inference performance.
- BRAM's element-wise quantization offers improved communication efficiency and rapid training speed.
- A theoretical convergence analysis is provided, showing that BRAM achieves a convergence rate comparable to uncompressed Adam.
- The proposed Hierarchical-1-bit-All-Reduce algorithm is reasonable and enhances communication efficiency.
- Extensive empirical validation across diverse neural network architectures showcases BRAM's broad applicability.

Weaknesses:
- The literature review is inadequate, lacking comprehensive coverage of existing one-bit compression methods.
- The experimental setup raises concerns regarding fairness, as it does not compare BRAM with several state-of-the-art methods.
- Some reviewers express concerns regarding the novelty of the algorithm design and the lack of significant improvements in convergence rates.
- The theoretical analysis is brief and lacks depth, and the sensitivity of BRAM's performance to its assumptions is not adequately discussed.
- There are mentions of missing relevant citations in the initial draft, which could enhance the paper's credibility.

### Suggestions for Improvement
We recommend that the authors improve the literature review by including a more comprehensive survey of one-bit compression techniques, particularly SSDM and Marsit. Additionally, we suggest conducting experiments that compare BRAM with other state-of-the-art methods like HiTopKComm and PowerSGD to validate its effectiveness. The authors should also expand the theoretical analysis to provide deeper insights into the convergence properties and discuss the sensitivity of BRAM to hyperparameters and noisy gradients. Furthermore, we recommend that the authors clarify the novelty of BRAM by explicitly contrasting it with existing adaptive optimizers and addressing concerns regarding its algorithmic design. Incorporating the missing relevant citations would strengthen the paper's foundation and provide a more comprehensive context for their contributions. Lastly, a more detailed analysis comparing BRAM with SSDM could further elucidate the differences in quantization approaches.