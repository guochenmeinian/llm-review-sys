ID: 3qa4YLkcEw
Title: TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a systematic benchmark, TRACE, for evaluating continual learning abilities in large language models (LLMs) across general ability, instruction-following, and safety. The authors conduct extensive experiments on eight challenge tasks, highlighting the need for a comprehensive benchmark due to the limitations of existing ones. The benchmark includes complex tasks such as domain-specific tasks, multilingual capabilities, code generation, and mathematical reasoning, and introduces metrics like General Ability Delta, Instruction Following Delta, and Safety Delta.

### Strengths and Weaknesses
Strengths:
- The proposed benchmark is straightforward and easy to follow.
- Extensive experiments validate the effectiveness of the benchmark.
- The tasks and datasets chosen in TRACE are potentially significant if properly justified.

Weaknesses:
- The experimental results are limited to dense LLM models; experiments with sparse MOE LLMs (e.g., Mixtral, DeepSeek-V2) are necessary for a comprehensive assessment.
- Multi-stage continual learning across tasks and domains has not been explored.
- The clarity of the proposed metrics and some results is lacking, particularly regarding the performance of certain continual learning algorithms.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the proposed metrics and provide a better explanation of the continual learning baselines. Additionally, conducting experiments with sparse MOE LLMs and performing multi-stage continual learning across tasks and domains would enhance the comprehensiveness of the study. It would also be beneficial to include a running time comparison among different LLM models to assess computational efficiency. Finally, we suggest strengthening the justification for the chosen datasets and addressing the originality claims in light of recent surveys on continual learning for LLMs.