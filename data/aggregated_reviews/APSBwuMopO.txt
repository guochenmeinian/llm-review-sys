ID: APSBwuMopO
Title: Optimized Feature Generation for Tabular Data via LLMs with Decision Tree Reasoning
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 4, 3, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents OCTree, a framework that utilizes large language models (LLMs) to generate and refine feature generation rules for tabular data. By integrating decision tree reasoning, OCTree iteratively enhances feature generation based on feedback from prior experiments. The framework shows improved performance across various prediction models, including decision trees and multilayer perceptron (MLP) models, and demonstrates effectiveness on real-world datasets, regardless of the presence of language descriptions. The authors highlight their method's superiority over competing approaches, such as AutoFeat and OpenFE, particularly in generating useful features for MLP models, while also addressing the importance of training time and the impact of dataset size on performance comparisons.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel approach by combining LLMs with decision tree reasoning for feature generation, showcasing significant performance improvements in experiments.
- It demonstrates consistent improvement across multiple datasets and baseline models, particularly in generating relevant features for MLPs.
- The authors are responsive to feedback and committed to including additional evaluations in the final revision.
- It is well-structured and clearly written, making the research question of automatic feature engineering accessible.

Weaknesses:
- The framework is costly and time-consuming due to the need for training new models for validation in each iteration, limiting full automation.
- The improvement over baseline methods is not substantial in many datasets, with relative error rate reductions being less than 4% in several cases.
- There is a lack of comparative analysis regarding training times, making it difficult to evaluate efficiency against other methods.
- The experimental design includes datasets sampled down to fewer than 50,000 instances, which may lead to underperformance of baseline methods and raises concerns about effectiveness in high-dimensional scenarios.
- Datasets are confined to a maximum of 54 features, which may affect scalability and effectiveness on larger datasets.
- The notation used in the paper is confusing, hindering understanding of the algorithm's workings.
- There is a lack of comprehensive comparisons with all relevant methods across all evaluation parts of the paper.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their notation to enhance understanding of the algorithm. Additionally, we suggest including a comparison with CAAFE, the most relevant algorithm, to establish a solid baseline for evaluating OCTree's performance. It would be beneficial to provide evidence of how decision tree reasoning enhances efficiency and to address the potential risk of LLM memorization in feature generation. We also recommend improving the evaluation of training time by including quantitative results in the final revision. Furthermore, we suggest avoiding sampling datasets down to fewer than 50,000 instances to ensure fair baseline performance. Finally, we encourage the authors to include comparisons with all relevant methods in all evaluation sections, particularly in Table 2, to enhance the comprehensiveness of their analysis.