ID: jUgBvYwc50
Title: ZARA: Improving Few-Shot Self-Rationalization for Small Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework, ZARA, aimed at enhancing self-rationalization in small language models. The framework incorporates an approximator that transforms samples into NLI-type examples and employs an ensemble of NLI models to predict unlabeled instances. Highly confident samples are utilized to retrain the language model, leading to improved self-rationalization and higher accuracy on the FEB dataset. The authors focus on smaller, more accessible models, proposing a straightforward methodology for self-training that leverages plausible rationales to boost performance.

### Strengths and Weaknesses
Strengths:
- The paper introduces a clever application of off-the-shelf NLI models to assess the plausibility of model predictions, demonstrating improvements over prior methods.
- The methodology is simple yet effective, and the paper is well-written with thoughtful experiments and analyses.

Weaknesses:
- The framework is not fully automatic, requiring specific mappings for each subtask.
- There are concerns regarding the efficiency of training 60 models for each episode, as well as the clarity of data splits and the potential for test set leakage.
- The distinction between truly plausible rationales and those that may leak labels is unclear, and the paper lacks sufficient baseline comparisons for plausibility.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the framework by providing more details on the differences between episodes and the rationale for training multiple models. Additionally, it would be beneficial to address the concerns regarding the correlation versus causation of performance improvements and to clarify the data split methodology to prevent potential leakage. We suggest including more insights on the performance of different off-the-shelf NLI models and ensuring that the paper cites relevant works, such as STaR, more prominently. Lastly, we encourage the authors to conduct significance tests on the results presented in Table 3 to validate the improvements.