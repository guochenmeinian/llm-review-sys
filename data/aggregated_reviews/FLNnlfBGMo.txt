ID: FLNnlfBGMo
Title: Efficient Prompt Optimization Through the Lens of Best Arm Identification
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on prompt optimization under budget constraints, proposing a framework called TRIPLE that connects prompt optimization to the fixed-budget best arm identification (BAI-FB) problem. The authors leverage multi-armed bandit (MAB) literature to enhance prompt design methods, demonstrating that TRIPLE significantly improves prompt evaluation and selection across various tasks and settings. The study includes extensive experiments validating the effectiveness of TRIPLE and its integration with existing prompt generation schemes.

### Strengths and Weaknesses
Strengths:
- The formulation of prompt optimization as a BAI-FB problem is well-justified and clearly presented.
- The empirical results indicate that TRIPLE effectively identifies good prompts, showing significant gains over baselines.
- The experiments cover a wide range of baselines, LLMs, and datasets, providing a comprehensive evaluation.

Weaknesses:
- The novelty of the work is limited, as it primarily utilizes existing FB-BAI methods without introducing substantial new methodologies.
- There is insufficient discussion on how the size of the prompt pool affects optimization performance, and empirical results on this aspect are lacking.
- The presentation could be improved, particularly by reducing redundancy in Section 3 and enhancing explanations in Section 4.
- The evaluation settings lack detail, particularly regarding the consistency of evaluation budgets across baselines.
- The scope of prompt candidates is limited, with a maximum of 150, which may bias results compared to studies considering larger prompt domains.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the impact of prompt pool size on optimization performance and include empirical results similar to Figure 3 to illustrate these effects. Additionally, we suggest clarifying the evaluation settings, ensuring that the evaluation budgets per prompt are consistent across all baselines. The authors should also consider expanding the range of prompt candidates to better reflect the literature, potentially including comparisons with recent baselines like ZOPO and OPRO. Furthermore, we encourage the authors to enhance the clarity of their presentation by reducing redundancy in Section 3 and elaborating on critical concepts in Section 4. Lastly, providing a discussion on how TRIPLE can be applied to multi-objective prompt optimization would strengthen the paper.