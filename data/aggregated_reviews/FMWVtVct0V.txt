ID: FMWVtVct0V
Title: Towards Anytime Fine-tuning: Continually Pre-trained Language Models with Hypernetwork Prompts
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a prompt-guided continual pre-training method (HPrompt-CPT) that utilizes a hypernetwork to generate domain-specific prompts. The authors propose an evaluation protocol called anytime fine-tuning, which assesses model performance across past, current, and unseen future domains. The method incorporates agreement and disagreement losses to enhance adaptability and generalization while mitigating forgetting. Experiments demonstrate that HPrompt-CPT outperforms existing fine-tuning methods in adaptability, generalization, and forgetting across two datasets.

### Strengths and Weaknesses
Strengths:
- The paper addresses crucial capabilities of continual pre-trained models: adaptability, generalizability, and forgetting.
- The proposed anytime fine-tuning evaluation protocol offers a comprehensive assessment of continual pre-training models.
- The hypernetwork prompt module and associated losses are innovative and aim to balance key aspects of continual learning.
- The experiments provide strong empirical support for the effectiveness of HPrompt-CPT.

Weaknesses:
- The evaluation protocol, anytime fine-tuning, is not novel as it resembles common practices in testing on seen and unseen datasets.
- The experiments are limited to only two datasets and the RoBERTa model, raising concerns about generalizability and representativity.
- The complexity introduced by the hypernetwork may not be justified, and simpler methods could be explored.
- The design of downstream task evaluations lacks clarity, particularly regarding prompt fine-tuning and domain knowledge.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental design, particularly how prompts are fine-tuned for downstream tasks and whether the domain is known beforehand. Additionally, we suggest expanding the empirical analysis to include more datasets and models beyond RoBERTa to strengthen the claims of generalizability. The authors should also provide a more thorough justification for the complexity of the hypernetwork prompt module and consider exploring simpler alternatives. Finally, addressing the questions regarding prompt generation, the rationale for using KL-Divergence, and the significance of the t-SNE plot would enhance the paper's depth and clarity.