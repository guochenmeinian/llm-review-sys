ID: RGSYhsjK60
Title: OODREB: Benchmarking State-of-the-Art methods for Out-Of-Distribution Generalization on Relation Extraction
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark dataset, OODREB, constructed from various state-of-the-art (SOTA) datasets to evaluate the performance of SOTA approaches in out-of-distribution (OOD) settings. The authors propose a method to generate OOD benchmarks that reflect real-world data distributions, moving away from traditional independent and identically distributed (IID) datasets. The study reveals that models underperform in OOD contexts, highlighting significant gaps in current relation extraction methods, particularly those based on language models.

### Strengths and Weaknesses
Strengths:
- The paper addresses a critical issue in relation extraction evaluation, emphasizing the need for realistic OOD benchmarks.
- It introduces a novel method for generating comprehensive OOD benchmarks, integrating multiple datasets.
- The findings provide valuable insights into the limitations of existing SOTA models in OOD settings.
- The paper is well-written and accessible, making it suitable for a diverse audience.

Weaknesses:
- The definition of out-of-distribution is not clearly articulated, leading to ambiguity regarding the distributions of the datasets used.
- The complexity of the proposed method for generating OOD benchmarks may pose practical implementation challenges.
- The focus on language model-based extraction limits the exploration of other potential models for relation extraction.
- The paper lacks a robust theoretical framework, particularly concerning the role of causality in relation extraction.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definition of out-of-distribution and provide a more detailed discussion on the relevance of the proposed approach to the Web context in the introduction. Additionally, the authors should clarify the objectives of the experimental evaluation and justify critical choices, such as the attack strategies used. We suggest enhancing the theoretical framework by elaborating on the concept of causality and its implications for relation extraction. Furthermore, the authors should provide motivating examples regarding causality and consider refining the method of random shuffling or replacement to better align with linguistic patterns. Lastly, including a link to access the dataset would greatly benefit the community.