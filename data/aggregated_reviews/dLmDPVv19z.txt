ID: dLmDPVv19z
Title: Constrained Policy Optimization with Explicit Behavior Density For Offline Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel algorithm for offline model-free reinforcement learning called Constrained Policy optimization with Explicit Behavior density (CPED). The authors propose using a Flow-GAN model to explicitly estimate behavior policy density, which helps mitigate issues related to overly conservative policies and out-of-distribution (OOD) actions. The algorithm is evaluated across various D4RL datasets, demonstrating competitive performance. Additionally, the paper compares the Flow-GAN and VAE models for estimating behavior policy density, emphasizing the superior performance of Flow-GAN. The authors conducted toy example experiments to compare the mean of log-likelihood of generated samples from both models, showing that Flow-GAN significantly outperforms VAE in approximating the original data distribution across two settings.

### Strengths and Weaknesses
Strengths:
- The algorithm is clearly presented and well-motivated.
- Strong empirical evaluation shows competitive results on D4RL MuJoCo tasks.
- Theoretical justification for the convergence of the Flow-GAN and RL training is solid.
- The authors provide compelling evidence through new experiments that highlight the advantages of Flow-GAN over VAE.
- The use of multiple settings in the toy examples enhances the robustness of the findings.

Weaknesses:
- The contribution appears limited as it borrows the Flow-GAN concept directly into offline RL without significant innovation.
- The hand-crafted per-environment piecewise linear scheme for alpha reduces the method's generality.
- Algorithm 1 contains inaccuracies regarding the interleaving of Flow-GAN optimization with RL training.
- The definition of the ‘offline MDP’ lacks clarity and does not conform to standard MDP definitions.
- The optimal Q-function in Theorem 4.2 requires clearer definition and explanation.
- The VAE model struggles with multi-modal data, which may limit its applicability in certain scenarios.
- The inability to attach additional figures on the OpenReview website restricts the presentation of results.

### Suggestions for Improvement
We recommend that the authors improve the novelty of the contribution by exploring alternative methods for defining the safe area, such as using a state-action-based uncertainty penalty similar to MOReL. Additionally, consider replacing the hand-crafted alpha tuning scheme with a more generalizable approach to enhance applicability. Clarifying the implementation details of CPED and addressing the inaccuracies in Algorithm 1 will strengthen the paper. Furthermore, we suggest providing more empirical comparisons with SPOT and other relevant methods to substantiate claims of superiority. Revising the definition of the ‘offline MDP’ to align with standard conventions will enhance clarity. To improve the clarity of findings, we recommend including visual representations of the generated samples in the revised manuscript and providing more compelling evidence for the superiority of the Flow-GAN model through further detailed comparisons or additional settings.