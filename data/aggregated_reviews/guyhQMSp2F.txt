ID: guyhQMSp2F
Title: Use perturbations when learning from explanations
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to Machine Learning from Explanations (MLX), framing it as a robustness problem where human-annotated explanations define a lower-dimensional manifold for perturbations. The authors critique previous regularization-based MLX methods for requiring strong model smoothing, which negatively impacts performance. They propose a combination of robustness-based methods with existing MLX techniques, demonstrating state-of-the-art results across multiple benchmarks. The theoretical and empirical analyses support the efficacy of this combined approach.

### Strengths and Weaknesses
Strengths:  
- The paper effectively improves upon existing MLX methods by applying robustness techniques in a novel context, demonstrating their effectiveness through both theoretical insights and empirical results.  
- The structure and clarity of the paper facilitate understanding, and the combination of robustness and regularization is well-justified and shows improved performance.

Weaknesses:  
- The contribution appears limited to combining established robust training techniques with a single MLX approach, raising questions about its generalizability to other MLX methods.  
- The evaluation is restricted to three datasets, and the absence of results from more recent MLX approaches discussed in the introduction diminishes the comprehensiveness of the analysis.  
- Obtaining human-specified masks for training is labor-intensive, which may limit the applicability of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the contextualization of their work by discussing how explanations are treated in the literature on transformer-based image models, particularly regarding attention mechanisms. Addressing the relevance of their setup in light of current state-of-the-art architectures could enhance the paper's impact. Additionally, we suggest expanding the limitations section to provide more detail on when the robustness methods may not be feasible, and including a sensitivity analysis on hyper-parameters to better understand the method's performance across different settings. Finally, it would be beneficial to explore the applicability of their approach to other MLX methods to demonstrate its generalizability.