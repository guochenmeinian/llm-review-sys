ID: Y7kK2HcxDK
Title: GDA: Grammar-based Data Augmentation for Text Classification using Slot Information
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new data augmentation method called Grammar-based Data Augmentation (GDA), which utilizes automatically generated generation patterns based on slot variables. The approach consists of three phases: (1) inference of "Rules of Grammar" (RoGs) from a slot-tagged training dataset, potentially using Named Entity Recognition (NER) to identify slots; (2) manipulation of RoGs to create more general rules through lexical operations; and (3) generation of new sentences using these rules. The authors claim that GDA enhances the diversity of generated sentences at both syntactic and semantic levels, particularly in low-resource settings, and evaluate their method against state-of-the-art data augmentation techniques and LLMs, including an ablation study. However, the choice of context-free grammar (CFG) as the underlying structure raises concerns about its ability to model context-dependent language effectively.

### Strengths and Weaknesses
Strengths:
- The development of a CFG-based approach for generating sentence templates is innovative and has low computational requirements.
- GDA yields diverse and natural samples, addressing both syntactic and semantic fidelity.
- The evaluation setup demonstrates the benefits of GDA in low-resource settings and acknowledges its limitations, particularly regarding tasks with NER-taggable tokens.

Weaknesses:
- The paper lacks a convincing demonstration of GDA's superiority over existing methods due to inconsistencies in experimental settings and reporting.
- Claims regarding the reduction of semantic errors and the nature of the generated rules are not adequately supported by evidence.
- The reliance on slot information limits applicability to datasets without such information, and the approach may not perform well in one-shot or zero-shot learning scenarios.
- The reliance on CFG may restrict the modeling of context-dependent language, leading to potential semantic inaccuracies.
- The methodology does not adequately address error propagation issues, particularly when NER slot predictions are incorrect.

### Suggestions for Improvement
We recommend that the authors improve the clarity and consistency of their experimental settings, ensuring that comparisons are made across similar datasets. Additionally, we suggest providing a more robust justification for the omission of comparisons to other data augmentation methods in the second set of experiments. The authors should also clarify the claims regarding semantic error reduction and provide quantitative evidence to support these claims, possibly through significance testing of the reported figures. Furthermore, we advise revising the abstract to accurately reflect the nature of the approach and ensuring that the terminology used aligns with traditional CFG definitions. We encourage the authors to explore alternative grammatical frameworks that better capture context and elaborate on how to mitigate error propagation in their pipeline, particularly in scenarios where NER tagging is not feasible. Lastly, a more detailed discussion of the datasets and their respective tasks would enhance the paper's comprehensiveness.