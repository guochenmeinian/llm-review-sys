ID: zRYSJbcRcV
Title: Stanford-ORB: A Real-World 3D Object Inverse Rendering Benchmark
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 6, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
The paper introduces a new benchmark for real-world 3D object-centric inverse rendering, featuring a dataset with ground truth 3D scans, environment maps, and multi-viewpoint images for 12 objects across 7 scenes. This benchmark aims to address the limitations of existing datasets by providing comprehensive metrics for evaluating inverse rendering algorithms in terms of geometry, illumination, and appearance. Key findings include that the performance on geometry and view synthesis benchmarks is highly correlated, with IDR outperforming NeRF in these areas, while NeRF excels in scene reconstruction despite lower surface reconstruction fidelity compared to SDF-based methods like IDR. Additionally, NVDiffRecMC achieves the best performance in the relighting benchmark, significantly surpassing NVDiffRec, demonstrating the effectiveness of its differentiable Monte-Carlo-based renderer. We have expanded our dataset based on reviewer suggestions, adding new objects to enhance diversity.

### Strengths and Weaknesses
**Strengths:**
- The dataset is significant for advancing the field by overcoming limitations of existing datasets that either lack detailed geometry or are synthetically generated.
- The data acquisition process is thorough, capturing dense 3D scans and high-dynamic range images, which is well-documented.
- Our object selection covers a diverse range of geometries and materials, enhancing the benchmarking analysis.
- The inclusion of real-world capturing noises and reflection effects in our dataset provides challenges that synthetic benchmarks may not replicate.
- We have evaluated multiple state-of-the-art methods, providing a comprehensive analysis of their performance.

**Weaknesses:**
- The selection of only 12 objects raises questions about diversity and representation, as many objects share similar geometries and aspect ratios.
- Some reviewers noted limitations in the variety of objects and potential systemic errors introduced by the chrome ball used for capturing environment maps.
- The evaluation lacks a detailed discussion of results and their implications, with insufficient visual examples to illustrate method performance.
- The computational expense of existing inverse rendering methods raises concerns about the practicality of the benchmark.
- The paper does not adequately address the potential societal impacts of high-fidelity inverse rendering.

### Suggestions for Improvement
We suggest enhancing the rationale behind object selection to ensure a broader range of geometries and materials. A discussion on the significance of results should be included, along with visual examples of method outputs. Additionally, we recommend comparing the dataset with synthetic data to highlight the sim-to-real gap. The paper should also address the societal implications of inverse rendering more thoroughly, considering potential negative impacts. We plan to create detailed instructions for capturing data to facilitate contributions from other researchers and continue expanding the dataset while maintaining reasonable size for computational efficiency. Furthermore, we recommend refining the text in the final version, particularly by replacing the term 'significantly' in lines 306 and 308 with more appropriate language, as it is technically loaded and should be used with caution. Lastly, we recommend providing clearer documentation on the dataset usage and output expectations to improve reproducibility.