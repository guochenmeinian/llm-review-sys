ID: gvtCR7dHJ3
Title: Dual Cone Gradient Descent for Training Physics-Informed Neural Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 6, 6, 8, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to optimizing physics-informed neural networks (PINNs) by framing the problem as a multi-objective optimization challenge. The authors propose the Dual Cone Gradient Descent (DCGD) method, which identifies non-conflicting update directions within a dual cone formed by the gradients of boundary and residual losses. The empirical results demonstrate that DCGD outperforms existing optimization techniques for PINNs, enhancing predictive accuracy and stability during training. Furthermore, the authors provide empirical results showing that DCGD achieves a $L^2$ error of **1.20e-2** without fine-tuning and propose two methods for combining DCGD with other optimizers: sequential application and simultaneous gradient updates. The combination of DCGD with Adam, L-BFGS, and NNCG yields further reductions in error, with the best performance recorded at **5.60e-3** using Combination 1.

### Strengths and Weaknesses
Strengths:
1. The concept of utilizing the dual cone of gradients is a clear and sound idea, contributing to the clarity of the presentation.
2. The proposed DCGD framework effectively reduces gradient conflicts, enhancing training stability.
3. Empirical results effectively showcase the advantages of the proposed method, demonstrating significant improvements over traditional optimization methods.
4. Clear presentation of two combination strategies for integrating DCGD with existing optimizers.

Weaknesses:
1. The theoretical contributions, particularly Theorem 4.5, lack clarity and relevance, raising questions about their significance.
2. The paper does not adequately address recent literature on optimization strategies for PINNs, which could provide a more comprehensive context for the proposed method.
3. The practical success of PINNs is overstated, potentially misleading readers regarding their effectiveness in solving PDEs.
4. The experiments were conducted under default parameter settings, which may limit the generalizability of the results.
5. There is a lack of detailed exploration regarding hyperparameter tuning, which could further optimize performance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Theorem 4.5 and provide a more detailed explanation of its implications. Additionally, the authors should incorporate a discussion of recent optimization strategies for PINNs to strengthen the related work section. It would also be beneficial to moderate claims regarding the practical success of PINNs to avoid misleading representations. Furthermore, we suggest including visualizations of PINN predictions compared to exact solutions, particularly for failure modes, and reporting training timing to enhance the empirical evaluation. Lastly, we recommend that the authors improve the exploration of hyperparameter tuning to assess its impact on the performance of DCGD and provide a more comprehensive analysis of the results obtained from both combination strategies to enhance the clarity and depth of the findings.