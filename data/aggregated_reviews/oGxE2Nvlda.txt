ID: oGxE2Nvlda
Title: UniT: A Unified Look at Certified Robust Training against Text Adversarial Perturbation
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 5, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents UniT, a unified framework for certified robust training against text adversarial perturbations. The authors propose two frameworks of robust training: Type I, which employs random data augmentation over text inputs, and Type II, which adds Gaussian noise to the latent feature space. UniT merges these approaches by incorporating data augmentation and Gaussian noise, alongside a novel decoupled regularization (DR) loss that modularizes feature extraction and robust regularization. The evaluation shows that UniT consistently outperforms existing methods like SAFER and CISS, with strong ablation studies on various loss function designs and hyper-parameter settings.

### Strengths and Weaknesses
Strengths:
1. The modular loss function design is innovative, and the ablation studies are robust.
2. UniT demonstrates superior performance compared to existing certified approaches.

Weaknesses:
1. The paper only addresses synonym substitutions, neglecting other text adversarial perturbations such as insertion and deletion, which makes the title "text adversarial perturbation" an overstatement.
2. The comparison between UniT and CISS lacks clarity and requires better presentation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the comparison between UniT and CISS, addressing specific questions raised in the reviews. Additionally, it would be beneficial to include natural accuracy in all hyper-parameter tables and provide a more justified motivation for the use of IBP in the context of certification. The authors should also consider discussing how synonyms are selected based on embeddings and the implications of their framework's applicability to other tasks and models. Furthermore, including comparisons with advanced perturbation methods based on large language models could enhance the paper's contribution. Lastly, guidance on efficiently selecting hyperparameters for practical applications would make the proposed approach more accessible.