ID: o7DOGbZeyP
Title: LookHere: Vision Transformers with Directed Attention Generalize and Extrapolate
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 7, 8, 6, -1, -1, -1, -1
Original Confidences: 5, 5, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LookHere, a novel positional encoding method for Vision Transformers aimed at improving performance with high-resolution images. LookHere utilizes 2D masks to constrain attention heads to specific directions, enhancing the model's ability to handle resolution changes. Comprehensive experiments demonstrate that LookHere outperforms existing positional encoding methods in terms of resolution scalability.

### Strengths and Weaknesses
Strengths:
1. The concept is clearly articulated, addressing the challenges posed by high-resolution images through the use of 2D attention masks.
2. The experiments are extensive and robust, effectively validating the proposed method's performance.
3. The paper is well-written, with clear illustrations and a thorough discussion of prior work.

Weaknesses:
1. The authors acknowledge limitations, such as the reliance on hand-designed masks and the single scale of ViT. Additionally, a discussion of ConViT, which introduces convolutional bias, is missing.
2. The generalization of LookHere to smaller-scale images remains unverified, and the ablation study on design choices is lacking.
3. The proposed high-resolution ImageNet dataset variant is limited in size, potentially introducing bias.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of hand-designed masks and consider including a comparison with ConViT. Additionally, the authors should explore the generalization of LookHere to smaller images and provide an ablation study to verify the efficacy of specific design choices. Finally, we suggest increasing the size of the ImageNet-HR dataset to ensure a more representative sample.