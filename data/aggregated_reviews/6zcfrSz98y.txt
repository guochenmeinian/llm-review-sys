ID: 6zcfrSz98y
Title: $\mathcal{M}^4$: A Unified XAI Benchmark for Faithfulness Evaluation of Feature Attribution Methods across Metrics, Modalities and Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 4, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the $M^4$ framework for evaluating the faithfulness of explanations in Explainable Artificial Intelligence (XAI) across various modalities (text and images), models (ResNets, Transformers, MobileNets), methods (IG, SG, GradCam), and metrics (no ground truth, pseudo ground truth, synthetic). The authors propose a unified benchmark pipeline that categorizes existing evaluation metrics and systematically analyzes feature attribution methods, conducting extensive experiments using datasets like ImageNet and Movie Review. They provide insights into the performance of different attribution methods and acknowledge the computational complexity of Shapley values based methods, justifying the exclusion of white-box models from their benchmark. The implementation is open-sourced, promoting accessibility for the research community.

### Strengths and Weaknesses
Strengths:
- Addresses a crucial challenge in evaluating faithfulness in XAI, particularly relevant for high-stakes applications.
- Proposes a taxonomy for XAI evaluation metrics, enhancing systematic understanding.
- The framework is modular, allowing easy integration of various deep learning models and attribution algorithms.
- Utilizes state-of-the-art models and methods, contributing to the robustness of the research.
- Includes both classic and state-of-the-art feature attribution methods, demonstrating broad applicability.
- Open accessibility fosters community engagement and further innovation.
- The authors provide a user-case scenario for practical application, enhancing the framework's utility.

Weaknesses:
- The explanation techniques are outdated; newer methods like Guided-IG and AGI are not included.
- The benchmark's focus on post-hoc feature attributions limits its applicability to other XAI aspects like model transparency.
- Many models and methods are readily available, potentially reducing the framework's unique value.
- The pseudo ground truth explanations may encourage new algorithms to replicate the mistakes of older methods.
- The framework currently lacks cross-modal explanations, which could enhance its value in multi-modal data contexts.
- The computational cost of including additional advanced algorithms is a concern.
- The benchmark may not comprehensively cover emerging or niche models and data types.

### Suggestions for Improvement
We recommend that the authors improve the framework by integrating newer explanation techniques such as Guided-IG and AGI to enhance its value. Additionally, consider including well-established attribution methods like CAM variants, LIME variants, and attention-based methods to broaden the scope of evaluation. To address the limitations of focusing solely on sentiment analysis, we suggest expanding the benchmark to include a wider range of NLP tasks. Furthermore, we recommend improving the clarity and justification of the pseudo ground truth metric, ensuring that its assumptions and limitations are well-documented. We also suggest incorporating cross-modal explanations to unify the framework and enhance its perceived value. Finally, we encourage the authors to explore the inclusion of more advanced attribution algorithms, despite the computational costs, to broaden the framework's applicability and engage with the ethical dimensions of the work, particularly regarding bias and fairness in AI, to strengthen the submission's impact.