ID: haUnEiXgQ7
Title: Vision-Language Models are Strong Noisy Label Detectors
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 6, 5, 5, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 2, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel Denoising Fine-Tuning (DEFT) framework aimed at addressing noisy labels in vision-language models, particularly CLIP. The authors propose a two-stage method: first, a noise detector is learned through prompt learning of the text encoder in CLIP, followed by full fine-tuning of pretrained models using the filtered clean data. The framework utilizes class-specific positive and negative textual prompts to enhance noise detection and employs parameter-efficient fine-tuning (PEFT) to adapt the visual encoder. Extensive experiments demonstrate the method's effectiveness across various synthetic and real-world datasets.

### Strengths and Weaknesses
Strengths:
1. The proposed method is well-motivated, effectively adapting CLIP for noisy label detection.
2. The analysis of the relationship between noisy data and fine-tuning methods is insightful.
3. The two-stage method shows good performance compared to existing baselines.
4. The experimental validation across multiple datasets supports the method's robustness.

Weaknesses:
1. The rationale behind the effectiveness of positive and negative prompts in detecting noisy labels is unclear; visual examples are suggested for clarification.
2. The optimization process, particularly regarding positive prompts, lacks clarity.
3. The proposed noisy detector is not reusable across different datasets, limiting its applicability.
4. The focus on fine-tuning rather than pretraining narrows the research scope.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the optimization process for positive prompts, specifically whether they are learned together with negative prompts and Visual PEFT using $L_{dp} + L_{sim}$. Additionally, providing visual examples to illustrate how positive and negative prompts aid in detecting noisy labels would enhance understanding. The authors should also consider discussing the implications of their method's reliance on pretrained models and explore potential adaptations for scenarios with high noise ratios or imbalanced datasets. Lastly, addressing the computational overhead associated with maintaining dual prompts in resource-constrained environments would be beneficial.