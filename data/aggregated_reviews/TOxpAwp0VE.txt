ID: TOxpAwp0VE
Title: Multi-task Graph Neural Architecture Search with Task-aware Collaboration and Curriculum
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 4, 7, 7, 8, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to multi-task graph neural architecture search (GNAS) through the introduction of MTGC, which includes a structurally diverse supernet and a soft task-collaborative module. The methodology comprises three stages: forward propagation through the supernet, updating architecture parameters and the collaborative module, and modifying model weights via task-wise curriculum learning. The authors emphasize the importance of designing task-specific GNNs and provide empirical results demonstrating the effectiveness of their method across various datasets.

### Strengths and Weaknesses
Strengths:
- The paper articulates a well-defined research problem and introduces a novel angle by applying GNAS in a multi-task context.
- The proposed methods are plausible and generalizable, with a comprehensive experimental stage that validates their effectiveness.
- The introduction of task-wise curriculum learning is a significant contribution to balancing task gradients during the search phase.

Weaknesses:
- The relationship between the proposed method and existing GNN architectures is unclear, and comparisons with general multi-task NAS methods are insufficient.
- Some designs lack empirical support, particularly regarding the assumption of graph structure differences for different tasks.
- The presentation of Figure 2 is confusing, and the definition of the search space is vague, requiring further clarification.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Figure 2 and provide explicit details regarding the search space definition. Additionally, we suggest including more discussions on the knowledge shared among different GNN architectures in the multi-task setting, along with relevant examples. It would also be beneficial to enhance the experimental section by incorporating performance comparisons with existing multi-task methods and addressing the implementation details regarding the use of edge weights in architecture performance.