ID: nXXwYsARXB
Title: A hierarchical decomposition for explaining ML performance discrepancies
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 8, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a hierarchical decomposition model for binary machine learning classifiers, enabling detailed analysis of distribution shifts at various levels. The nonparametric approach allows for the incorporation of causal knowledge and provides confidence intervals for estimators. The authors also propose a method to detect root causes of performance variability across domains, distinguishing between covariate and outcome shifts, and identifying significant input features contributing to these shifts.

### Strengths and Weaknesses
Strengths:
1. The introduction of a new concept for decomposing distribution shifts has broad applicability, particularly with the use of $R^2$-measures for enhanced interpretability.
2. The framework allows for the integration of causal knowledge without requiring a causal graph.
3. The paper is well-organized, effectively motivating complex definitions and providing significant real-world case studies.

Weaknesses:
1. The proposed method is limited to binary classification, which should be explicitly noted in the abstract or title.
2. The framework's reliance on baseline variables W restricts its applicability for partial shifts, a limitation that is acknowledged but requires further discussion.
3. The conditions for fast convergence, particularly the strong assumption in equation (8), are not adequately addressed, raising concerns about their feasibility in the presented setting.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the distinction between baseline variables W and conditional covariates Z, providing more explicit explanations and justifications for their segmentation in real-world examples. Additionally, the authors should include more naive baselines in their experiments to validate the effectiveness of their method. A detailed discussion on the conditions for fast convergence rates, including examples that satisfy these conditions, would enhance the paper's rigor. Furthermore, we suggest moving extensive derivations to a Technical Supplement to improve accessibility for a broader audience. Lastly, addressing minor typographical errors and improving terminology for clarity would benefit the overall presentation.