ID: ntF7D8tAlQ
Title: Estimating Generalization Performance Along the Trajectory of Proximal SGD in Robust Regression
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 6, 4, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on linear regression with Gaussian design affected by noise with bounded first moments, particularly in the high-dimensional regime where feature dimensionality $p$ scales with the number of training data $n$. The authors propose two estimators that approximate the out-of-sample error achieved by various iterative solvers, including GD, SGD, proximal GD, and proximal SGD, under a convex, differentiable, and smooth loss function with Lipschitz-continuous gradients. The framework also addresses robust linear regression using the Huber loss function and provides non-asymptotic guarantees for finite $n$ and $p$, demonstrating the asymptotic consistency of the estimators. The paper includes examples and numerical experiments, utilizing a generalized probabilistic approximation of the generalization error.

### Strengths and Weaknesses
Strengths:  
The authors have developed a reliable procedure for determining the best stopping time for solvers in contexts lacking consistent estimators, particularly in robust regression with heavy-tailed noise. This methodology serves as an alternative to inconsistent or computationally expensive cross-validation techniques. The paper is well-written and includes clear proofs.

Weaknesses:  
The main results extend findings from prior work, applying them to a broader context but offering limited novel contributions. While the methodology tracks generalization performance over training time, it does not provide a clear approach for optimally tuning parameters beyond stopping time. The analysis of additional tuning parameters, such as mini-batch size, is insufficiently developed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how their methodology can be used to tune parameters other than stopping time, providing specific examples. Additionally, a detailed comparison of their methodology with classical methods, such as ridge and lasso regression, would enhance the paper's impact. We also suggest that the authors elaborate on the assumptions regarding the penalty term and provide more insights into the computational complexity of their estimators, especially concerning scalability with larger datasets. Finally, addressing the disconnect between theoretical results and empirical findings, particularly regarding the constants in Theorems 3.6 and 3.7, would strengthen the manuscript.