ID: WC1jbtEwRS
Title: Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents PFLAT (prompt flatness), a new metric designed to quantify the "expected utility" of language prompts, inspired by flatness regularization in statistical learning. The authors propose a formal optimization framework that unifies existing prompt selection metrics, demonstrating that combining PFLAT with these metrics enhances performance and sample efficiency in classification tasks.

### Strengths and Weaknesses
Strengths:  
1. The introduction of a new framework for prompt selection that merges prompt loss and flatness, clarifying distinctions and efficacy among previous studies.  
2. Strong empirical results showing improved performance in classification NLP tasks.  
3. Comprehensive understanding of existing methods and their relationship with PFLAT, supported by theoretical foundations.  

Weaknesses:  
1. Experiments are limited to classification tasks, with no exploration of more challenging generation tasks.  
2. Lack of clarity regarding the relationship between prompts listed in tables 9 to 14 and the classification task examples, hindering interpretability of the results.  
3. The rationale for using correlation with accuracy as an evaluation metric instead of directly utilizing accuracy is not explained.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the relationship between the prompts in tables 9 to 14 and the classification tasks, possibly by providing comparisons of prompts selected before and after using PFLAT. Additionally, we encourage the authors to extend their experiments to include generation tasks, as this would enhance the applicability of their findings. Lastly, please clarify the rationale behind using correlation with accuracy as an evaluation metric.