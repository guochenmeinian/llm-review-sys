ID: BoKg2pcF0H
Title: DiffusionSL: Sequence Labeling via Tag Diffusion Process
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework called DiffusionSL that employs diffusion models for sequence labeling tasks. The authors introduce a Bit-Tag converter for quantization and a Bit Diffusion Transformer for noise removal. The approach is evaluated on three tasks, yielding promising results. The paper formulates the sequence labeling task as a conditional generation problem, demonstrating the validity of the proposed method through sufficient experiments and ablation studies.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and clear, making it easy to follow.
- The proposed method shows performance gains over existing methods in the domain.
- Strong experimental setup with good results and comparisons, including ablation studies.

Weaknesses:
- The experimental setup is described too briefly, lacking details on performance metrics and computational performance.
- The proposed approach is similar to existing work (e.g., "Analog Bits") without proper citation or discussion, raising concerns about novelty.
- The framework does not address span-level information, which may limit its applicability to nested NER tasks.

### Suggestions for Improvement
We recommend that the authors improve the experimental setup description by including performance metrics and a discussion of computational performance. Additionally, consider including results from a simple vanilla baseline like BERT and presenting the algorithms in pseudocode. Address the novelty issue by citing relevant works, particularly "Analog Bits," and discussing how the proposed method differs. Finally, explore the potential for handling nested NER tasks and provide insights into the impact of initial sampling of random tags and the number of possible tags on model effectiveness.