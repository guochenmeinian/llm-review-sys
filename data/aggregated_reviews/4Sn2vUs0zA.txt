ID: 4Sn2vUs0zA
Title: Reference-Based POMDPs
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 6, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for solving Partially Observable Markov Decision Processes (POMDPs) by utilizing a reference policy. The authors demonstrate the existence and uniqueness of solutions for reference-based POMDPs and establish connections to standard POMDPs. Additionally, they develop an online planning algorithm, RefSolver, which leverages the reference policy to improve decision-making in partially observable environments.

### Strengths and Weaknesses
Strengths:
- The exploration of the POMDP problem is valuable, particularly starting from an available reference policy.
- The general concept is straightforward and easy to follow.
- Theorems related to reference-based POMDPs are well-developed, with a complete logical analysis despite some reliance on approximations.
- The paper systematically incorporates prior knowledge to enhance online POMDP planning.

Weaknesses:
- Mathematical details are challenging to comprehend, and the purpose of Theorem 3.1 is unclear.
- The method's applicability is hindered by the necessity of an initial reference policy, which may not always be available or optimal.
- The empirical evaluation is limited to two synthetic grid domains, lacking comparisons with other established methods like DESPOT.
- The relationship of the proposed approach to existing literature on policy priors and regularizers is inadequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of mathematical details and explicitly state the assumptions regarding the reference policy. Additionally, the authors should clarify the purpose of Theorem 3.1 and provide a more comprehensive discussion on related works to contextualize their contributions. The empirical evaluation should be expanded to include comparisons with other algorithms, such as DESPOT, to strengthen the findings. Finally, the authors should explore the performance of their method with various initial policies, especially those that are not fully observable.