ID: 3ivnixHy16
Title: Boosting Text-to-Video Generative Model with MLLMs Feedback
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 8, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel dataset, VideoPrefer, which contains 135,000 annotations of human preferences for videos based on specific language prompts. The authors propose a reward model, VideoRM, developed using Reinforcement Learning from Human Feedback (RLHF) to enhance the alignment of text-to-video models with human preferences. The study demonstrates the effectiveness of both VideoPrefer and VideoRM through comprehensive experiments, addressing the challenges of poor video quality and misalignment with text prompts prevalent in existing models. Additionally, the authors analyze bias in generated data and trained models, particularly focusing on preference bias for specific videos, revealing a significant preference for real videos (93% versus 70%). They also plan to provide insights into the backgrounds of human annotators, combining both experts and non-experts to ensure unbiased labeling, and conduct further analyses on potential biases related to gender, race, and ethnicity.

### Strengths and Weaknesses
Strengths:
1. The research innovatively utilizes multimodal large models for dataset annotation, significantly enhancing the accuracy and efficiency of labeling, which has substantial real-world applications.
2. The introduction of VideoRM represents a significant advancement in evaluating video quality, providing a robust method for assessing video content and improving user experience.
3. The quantitative analysis effectively showcases the strong semantic alignment capabilities of the proposed VideoRM model, highlighting its potential impact in the field.
4. The authors demonstrate a commitment to addressing bias in their dataset and models, enhancing the evaluation's comprehensiveness by including both expert and non-expert annotators.
5. The authors are open to feedback and willing to incorporate suggestions to improve the paper.

Weaknesses:
1. The reliance on GPT-4 for data annotation necessitates thorough sample checks to ensure quality, as the current approach lacks systematic verification, undermining the dataset's integrity.
2. The evaluation of generated videos should incorporate diverse perspectives through role-playing prompts, as a single prompt template limits the representation of varied opinions.
3. The pseudocode in Algorithm 1 lacks task-specific algorithmic innovation, following standard practices without introducing novel techniques tailored to the task's challenges.
4. The absence of interpretability experiments in the paper limits understanding of the model's decision-making process and effectiveness.
5. The initial version lacked detailed statistical analysis and clarity on bias, which may affect the paper's completeness, and there may be insufficient exploration of ethical biases.

### Suggestions for Improvement
We recommend that the authors improve the quality of the annotated data by conducting systematic spot checks and corrections. Additionally, incorporating role-playing prompts in the evaluation process would provide a more comprehensive representation of diverse opinions. We suggest exploring novel techniques in the reinforcement learning application for fine-tuning to enhance algorithmic innovation. Furthermore, including interpretability experiments would deepen insights into the model's decision-making process. We also recommend improving the clarity of the paper by addressing all identified issues, including typos and the addition of statistical analysis on curated video prompts. Enhancing the discussion on bias by including detailed analyses of potential biases related to video length and ethical considerations such as gender and race would strengthen the paper. Lastly, incorporating the backgrounds of annotators in the paper will provide a clearer context for the evaluation process.