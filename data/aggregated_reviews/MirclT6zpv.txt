ID: MirclT6zpv
Title: Delayed Algorithms for Distributed Stochastic Weakly Convex Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 4, 8, 7, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 2, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates delayed stochastic optimization problems with composite objective functions, demonstrating that the convergence rate of the delayed stochastic subgradient method can be improved from $\mathcal{O}(\tau_{\max}/\sqrt{K})$ to $\mathcal{O}(\bar{\tau}/\sqrt{K})$ under weakly convex conditions. The authors propose a delayed stochastic prox-linear method to address the impact of delayed information when the stochastic part $f$ follows the structure $f = h\circ c$. Additionally, they utilize a hard-thresholding technique to manage delay magnitude, thereby enhancing the robustness of the original algorithm. The study also explores distributed nonsmooth optimization with delayed updates, improving results by replacing maximum delay with expected delay and proposing a prox-linear algorithm that mitigates delay in convergence rates.

### Strengths and Weaknesses
Strengths:
1. The paper is clearly written and well-organized, making it accessible to readers.
2. Most proofs are accurate, showcasing the authors' mathematical proficiency.
3. Valuable numerical experiments illustrate the strengths and weaknesses of the DSGD and DSPL methods, particularly regarding step size selection.

Weaknesses:
1. The research direction is narrow, focusing solely on convergence rate improvements without substantial evidence of their significance. A broader exploration of implications and applications is needed.
2. The theoretical results lack depth; for example, the authors do not clarify why DSGD improves the maximum delay $\tau_{\max}$ to $\bar{\tau}$ or the implications for objective functions lacking the $h\circ c$ structure.
3. The assumptions about the objective function's randomness are overly uniform, raising concerns about the validity of the convergence rate improvements. The assumption $\sup_{g\in \partial f(x,\xi)}\|g\| \le L_f, \forall \xi$ may not hold in practical scenarios.

### Suggestions for Improvement
We recommend that the authors improve the exploration of the broader implications and potential applications of their methods to enhance the paper's significance. Additionally, we suggest providing more in-depth explanations of the theoretical results, particularly clarifying the improvements in convergence rates and the assumptions made about the objective functions. Acknowledging the limitations of these assumptions would also strengthen the analysis. Lastly, we encourage the authors to conduct experiments demonstrating the convergence of DSPL under varying magnitudes of delay and to address the clarity of the experimental results, including labeling axes and defining terms in figures.