ID: k73M4XEvFX
Title: SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 7, 6, 6, 6, -1
Original Confidences: 3, 4, 3, 5, -1

Aggregated Review:
### Key Points
This paper presents SORRY-Bench, a benchmark designed to evaluate aligned large language models (LLMs) on their ability to recognize and reject unsafe user requests. It utilizes a fine-grained taxonomy of 45 potentially unsafe topics and 450 class-balanced unsafe instructions, addressing the over-representation of certain topics in existing benchmarks. SORRY-Bench also incorporates 20 linguistic augmentations to examine the effects of various languages and prompt formats. The authors demonstrate that fine-tuned 7B LLMs can achieve accuracy comparable to larger models like GPT-4, with lower computational costs. The evaluation includes over 40 proprietary and open-source LLMs, analyzing their refusal behaviors systematically.

### Strengths and Weaknesses
Strengths:
- The paper addresses a critical issue in LLM safety and is well-written, effectively conveying its motivation.
- The extensive experiment section is comprehensive and well-designed, demonstrating the value of SORRY-Bench.
- The discussion is engaging and thought-provoking, contributing to the field.

Weaknesses:
- The details of the human annotation process in Section 2.2 require further elaboration, particularly regarding annotator agreement on the 45-class safety taxonomy.
- The dataset lacks a balanced set of safe yet harmful-looking prompts, which could enhance the evaluation of LLMs.
- The evaluation of language combinations in translation mutation methods is not addressed, limiting insights into LLM performance with multilingual inputs.

### Suggestions for Improvement
We recommend that the authors improve the elaboration on the human annotation process, specifically detailing annotator agreement on labeling data to the 45-class safety taxonomy. Additionally, including a balanced set of safe yet harmful-looking prompts in SORRY-Bench would provide a more comprehensive evaluation of LLMs. Evaluating LLM performance with inputs containing multiple languages could yield valuable insights. Furthermore, clarifying the dataset sample creation process, including the selection criteria for the 10 examples per category and the specific LLMs used, would enhance the study's reliability.