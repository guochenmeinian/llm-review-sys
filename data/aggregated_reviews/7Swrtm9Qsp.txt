ID: 7Swrtm9Qsp
Title: Stable Minima Cannot Overfit in Univariate ReLU Networks: Generalization by Large Step Sizes
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 8, 8, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the implicit regularization of large learning rates in gradient descent for univariate linear regression using two-layer ReLU neural networks. The authors demonstrate that if gradient descent converges to a local minimum, the function represented by the neural network at this minimum has a bounded first-order total variation, which helps prevent overfitting, as supported by a generalization bound. The theoretical findings are validated through numerical experiments.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a crucial topic in understanding the implicit regularization of optimization algorithms in deep learning, focusing on a realistic setting that avoids interpolation.  
- The results are novel, extending beyond previous work on large learning rates and providing an interesting interpretation in function space.  
- The writing is clear, and the mathematical presentation is precise.

Weaknesses:  
- The analysis is limited to univariate cases, with only brief comments on potential extensions to multivariate scenarios.  
- There is insufficient focus on the optimization aspect, lacking rigorous evidence that gradient descent will find solutions that meet the assumptions.  
- The "optimized" assumption may be unrealistic without constraints on the ground truth function, and the interplay between learning rate and noise levels is not adequately discussed.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the role of overparameterization in their setting, as it may influence optimization. Additionally, the authors should clarify the "optimized" assumption in Corollary 4.2, particularly regarding the necessity of constraints on the ground truth function. It would be beneficial to include discussions and experiments on the interplay between the learning rate and noise levels, as well as the implications of underparameterization on the results. Finally, we suggest strengthening the empirical arguments by considering noise levels that do not overshadow the signal.