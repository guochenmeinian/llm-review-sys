ID: rMLnxh4oT5
Title: CASE: Commonsense-Augmented Score with an Expanded Answer Space
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CASE, a Commonsense-Augmented Score with an Expanded Answer Space, aimed at enhancing language model performance on multiple-choice QA tasks. CASE reduces noise from less important words and incorporates implicit commonsense knowledge, generating lexically divergent yet conceptually similar answers to expand the answer space. The authors demonstrate that CASE outperforms strong baselines across five commonsense benchmarks, contributing to NLP Engineering experiments.

### Strengths and Weaknesses
Strengths:  
- The proposed framework is well-conceived, with a compelling motivation and effective integration of answer scoring using pretrained language models, including GPT-3.  
- Extensive experiments validate the efficacy of CASE, showing superior performance against various zero-shot QA systems.  
- The analyses are comprehensive and insightful, providing valuable reflections on the proposed framework's performance.  

Weaknesses:  
- The proposed token-level weighting method may be less impactful compared to other zero-shot QA approaches, such as fine-tuning on knowledge bases.  
- There are confusing notations in Tables 1 and 2, particularly regarding the baseline LM, which is not explicitly identified.  
- The paper lacks a thorough analysis of the quality of generated answers and does not sufficiently explore the role of sentence similarity and keyword connections.  
- The scope of the method is limited to answer selection tasks, and its applicability in few-shot settings is not addressed.  

### Suggestions for Improvement
We recommend that the authors improve clarity in Tables 1 and 2 by explicitly stating the baseline LM. Additionally, conducting a detailed analysis of the quality of generated answers through expert annotation would enhance the paper's contributions. It would also be beneficial to explore the use of generated embeddings from GPT-3 to strengthen the scoring step. Furthermore, including experiments in few-shot scenarios could provide practical advantages, and a comprehensive evaluation of GPT-3's performance should be reported. Lastly, we suggest clarifying the optimization criteria for automatic keyword extraction in Section 3.2.1 and ensuring consistent terminology throughout the paper.