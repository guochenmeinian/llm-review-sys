ID: hiwHaqFXGi
Title: Disentangled Generative Graph Representation Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 4, 4, 6, 4, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DiGGR (Disentangled Generative Graph Representation Learning), a self-supervised learning framework designed to enhance the disentanglement of learned graph representations through disentangled latent factors. The authors argue that existing generative graph models often overlook entangled representations, leading to reduced robustness and explainability. Extensive experiments across 11 public datasets for node and graph classification tasks demonstrate that DiGGR significantly outperforms many existing self-supervised methods.

### Strengths and Weaknesses
Strengths:
- Innovative Approach: DiGGR utilizes disentangled latent factors to guide graph mask modeling, enhancing explainability and robustness.
- Comprehensive Experiments: The framework is validated through extensive experiments, showing significant performance improvements over existing methods.

Weaknesses:
- Complexity and Scalability: The framework's computational complexity may limit scalability to large graphs or real-time applications, a concern that is not thoroughly discussed.
- Lack of Theoretical Analysis: The paper lacks a detailed theoretical analysis explaining why the disentanglement process improves performance, which could provide deeper insights into the methodâ€™s efficacy and limitations.
- Novelty Concerns: The task of graph disentangled learning is not new, and the paper does not sufficiently discuss existing methods or clarify the motivation behind using masks and specific GNN types.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of the disentanglement process to elucidate its impact on performance. Additionally, addressing the computational complexity and scalability of the framework is crucial, possibly by providing training time comparisons to baseline methods. The authors should also clarify the motivation for using masks and justify the choice of GAE over other GNN types. Furthermore, a more comprehensive discussion of related works in disentangled representation learning is necessary to highlight the novelty of their approach. Lastly, including experimental settings for reproducibility would enhance the paper's clarity and rigor.