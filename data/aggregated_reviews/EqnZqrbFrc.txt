ID: EqnZqrbFrc
Title: Sample Complexity of Goal-Conditioned Hierarchical Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of the sample complexity in goal-conditioned hierarchical reinforcement learning (HRL) and establishes a lower bound using hierarchical decomposition to quantify it. The authors propose a novel hierarchical Q-learning algorithm that exploits goal-based hierarchical decomposition of an MDP into high-level and low-level sub-MDPs, jointly learning their policies. The empirical validation involves examining the sample complexity of the proposed algorithm on various toy grid-world tasks.

### Strengths and Weaknesses
Strengths:  
- The authors address an important theoretical problem in HRL, deriving a lower bound to quantify sample complexity.  
- The paper provides strong theoretical guarantees on the lower bound of episodes needed to learn an epsilon-accurate hierarchical policy, which also serves as a lower bound for an optimal policy.  
- The theoretical findings are backed by empirical results on maze environments, offering convincing insights.  

Weaknesses:  
- The paper lacks a thorough theoretical analysis regarding the selection of sub-goal spaces in continuous or discrete environments, which is crucial for the efficiency of the HRL algorithm.  
- There is no comparison between the proposed method and existing HRL algorithms in the experimental section, limiting the evaluation of the proposed approach.  
- The assumptions regarding the derived bounds being restricted to a tabular setting need clarification in the Introduction.  
- The empirical results are limited to maze environments, and the benefits of decomposition should be explored in more diverse domains and tasks.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis by including a discussion on the selection of sub-goal spaces and their impact on sample efficiency. Additionally, we suggest incorporating a comparison of the proposed method with existing HRL algorithms in the experimental section to provide a more comprehensive evaluation. Clarifying the scope of the derived bounds in the Introduction and expanding empirical evaluations to include diverse tasks beyond navigation would enhance the paper's contributions.