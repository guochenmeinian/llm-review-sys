ID: ubp5s2tgXq
Title: Uncovering Meanings of Embeddings via Partial Orthogonality
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 4, 2, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the semantic meaning of embedding vectors by generalizing the concept of the Markov boundary through a relaxation of conditional independence. The authors propose an algorithm to approximate this boundary by identifying embeddings with high cosine similarity to a target vector after projecting onto orthogonal complement subspaces. Additionally, the paper introduces the concept of independence preserving embeddings, which underpins the study of linear algebraic independence in embeddings.

### Strengths and Weaknesses
Strengths:
- The connection between token semantics and linear algebraic independence is formally discussed, providing a novel perspective on the Markov boundary.
- The concept of independence preserving embeddings is well-defined and demonstrates the maintenance of independence structure in embeddings.
- Empirical evaluations using CLIP embeddings reveal intriguing patterns, suggesting that the proposed method effectively captures semantic meanings.

Weaknesses:
- The experimental section is limited, focusing primarily on CLIP embeddings without robust evaluation metrics to substantiate the proposed method's effectiveness.
- The definitions in section 3 regarding dimensions and independence may cause confusion, and the paper lacks a thorough exploration of related work in knowledge graph embeddings.
- The method only evaluates a single target embedding, and no experiments are conducted on independence preserving embeddings or contextualized embeddings.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by incorporating a more extensive evaluation with diverse embedding models beyond CLIP, such as GloVe, word2vec, and BERT. Additionally, providing precise numerical results using semantic evaluation metrics would strengthen the claims made regarding the advantages of orthogonality projection. Clarifying the definitions of dimensions in section 3 and including toy examples or visual figures to elucidate the concept of independence preserving embeddings would enhance comprehension. Lastly, exploring the implications of contextualized embeddings processed by Transformers could broaden the scope of the research.