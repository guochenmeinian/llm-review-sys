ID: gIG8LvTLuc
Title: How Does Adaptive Optimization Impact Local Neural Network Geometry?
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 4, 6, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into why Adam outperforms SGD in language model training by examining the local geometry of the training loss around algorithm iterates. The authors introduce the metric $R^{OPT}_{med}$ to analyze the uniformity of the Hessian diagonal, demonstrating that Adam tends to converge faster due to its bias towards regions with higher uniformity. Theoretical results are provided for a two-layer linear network, showing that Adam achieves a more uniform Hessian diagonal compared to SGD.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant question regarding the performance of optimization algorithms, providing valuable insights and intuitions.
- The theoretical analysis is novel and contributes to understanding the implicit biases of optimization methods.
- Extensive experiments support the findings, and the writing is clear and well-structured.

Weaknesses:
- Many plots suffer from small font sizes and readability issues, including legends and axes.
- The empirical investigation lacks depth in smaller models, limiting the generalizability of the findings.
- The relationship between the proposed statistic and the optimization algorithms remains unclear, particularly regarding the reasons for Adam's favorable behavior.
- The paper does not adequately explore scenarios where SGD outperforms Adam, raising questions about the universality of the observed uniformity measure.

### Suggestions for Improvement
We recommend that the authors improve the readability of plots by adjusting font sizes and enhancing the clarity of legends and axes. Additionally, we suggest conducting empirical investigations on smaller or toy models to assess the universality of the observed phenomenon. It would be beneficial to clarify the motivations behind the choice of $R^{OPT}_{med}$ and its implications for optimization performance. Furthermore, we encourage the authors to explore the behavior of SGD in settings where it outperforms Adam to provide a more comprehensive understanding of the uniformity measure's relevance. Lastly, including the proof sketch of Theorem 1 in the main text could enhance the clarity of the theoretical contributions.