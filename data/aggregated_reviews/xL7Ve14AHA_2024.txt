ID: xL7Ve14AHA
Title: Regularized Adaptive Momentum Dual Averaging with an Efficient Inexact Subproblem Solver for Training Structured Neural Network
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Regularized Adaptive Momentum Dual Averaging (RAMDA) algorithm for training structured neural networks. RAMDA integrates various optimization techniques, including a preconditioning matrix inspired by AdaGrad, to enhance the convergence of the regularized momentum dual averaging (RMDA) method. The authors provide theoretical analyses that support the convergence properties of RAMDA and develop an efficient inexact subproblem solver. Extensive empirical results demonstrate RAMDA's superior performance across multiple learning applications.

### Strengths and Weaknesses
Strengths:
1. Theoretical results indicate convergence towards the solution of the subproblem when proximal gradient methods are employed as local solvers, ensuring robustness under standard $L$-smoothness assumptions.
2. Empirical validation shows RAMDA outperforming RMDA and other gradient-based methods in various neural network tasks, with clear criteria for subproblem resolution.

Weaknesses:
1. There is a potential error in Eq. (3) regarding the diagonal operator for $P^t$, which should involve the square root $\sqrt{\cdot}$. Clarification on whether $P^t$ is inspired by AdaGrad stepsizes would enhance the distinction of RAMDA from RMDA.
2. The authors should address the impact of varying $\epsilon_t$ on RAMDA's training performance, as a higher value than $10^{-8}$ may yield lower training times while maintaining comparable perplexity in specific tasks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the theoretical results, particularly regarding the definition of the proximal operator when $\psi$ is nonconvex, and ensure that the proofs are thoroughly examined for correctness. Additionally, providing a detailed discussion on the computational complexity of the proposed subproblem solver and its scalability with data size and model complexity would enhance the manuscript. We also suggest including practical guidelines for implementing RAMDA, such as parameter tuning and handling different data distributions, to aid practitioners.