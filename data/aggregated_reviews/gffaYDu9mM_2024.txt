ID: gffaYDu9mM
Title: In-N-Out: Lifting 2D Diffusion Prior for 3D Object Removal via Tuning-Free Latents Alignment
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 6, 7, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach termed "In-N-Out" for enhancing 3D object removal tasks by leveraging tuning-free latents alignment. The authors propose a method that combines 2D diffusion models with NeRF to achieve multiview consistency through an "inpaint-outstretch" strategy. They address geometric mismatches and color inconsistencies, and their method shows improved performance compared to recent works like Spin-NeRF and NeRFiller.

### Strengths and Weaknesses
Strengths:  
- The authors provide an extensive review of related work, clearly delineating their contributions.  
- The explicit and implicit latents alignment proposed is an intriguing concept that could inspire further research.  
- The method shows significant performance gains both qualitatively and quantitatively, supported by an ablation study demonstrating the importance of ELA and ILA.

Weaknesses:  
- The paper exhibits signs of being rushed, affecting the clarity and quality of academic writing.  
- Key definitions and terms, such as Omega(Â·) and D_hat, are not adequately defined.  
- The explanation of ILA lacks clarity, particularly regarding the rationale for replacing key and value pairs.  
- The method's reliance on a pre-defined base frame raises questions about its sensitivity and choice.  
- The paper does not adequately address the sensitivity analysis of hyperparameters, particularly lambda_a, which is crucial for ILA.

### Suggestions for Improvement
We recommend that the authors improve the clarity and quality of academic writing throughout the paper, addressing issues such as undefined terms and grammatical errors. We suggest providing a detailed sensitivity analysis of hyperparameters, especially lambda_a, to enhance the robustness of the method. Additionally, we encourage the authors to clarify the rationale behind the ILA mechanism and the selection process for the base frame. Finally, including comparisons with a broader range of baseline methods would strengthen the paper's contributions.