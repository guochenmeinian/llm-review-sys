ID: ubgdInLSF9
Title: SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 5, 7, 7, 8, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a system of model compression methods for text-to-image models, specifically targeting stable diffusion. The authors propose robust training through stochastic depth training to enhance network resilience to architectural changes and introduce CFG-free knowledge distillation for compressing the U-Net. Additionally, the authors conduct an in-depth analysis of the computational costs associated with different layers. The work aims to reduce inference time to under two seconds on mobile devices while maintaining performance.

### Strengths and Weaknesses
Strengths:
- The paper addresses a timely and relevant topic in the compression of large generative models, with good evaluations and experimental results.
- The proposed architectural compression and step reduction methods are impressive and well-motivated, potentially applicable to other diffusion-based models.
- The writing is clear and accessible, supported by effective visual aids like Figure 2 and Table 1.

Weaknesses:
- Many compression methods are derived from existing techniques, lacking novelty, particularly in the knowledge distillation aspect where CFG guidance is simply added.
- Insufficient comparisons with existing KD methods for stable diffusion and a lack of qualitative results hinder the evaluation of the proposed methods.
- The reported training costs appear excessively high, contradicting claims of reduced computational requirements, and more detailed information on training hours and datasets is needed.
- Clarifications are required regarding the use of additional datasets, the applicability of the distillation scheme to other models, and the specifics of the image decoder compression method.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contributions by clearly distinguishing their methods from existing ones, particularly in the knowledge distillation section. Additionally, providing more comprehensive comparisons with other KD methods for stable diffusion would strengthen the evaluation. We suggest including detailed information about training costs and the datasets used, as well as clarifying the overlap between datasets for architecture evolution and evaluation. Lastly, enhancing the clarity of Algorithm 1 and the explanation of the impact of hyperparameters, such as gamma, would improve the paper's overall readability and technical depth.