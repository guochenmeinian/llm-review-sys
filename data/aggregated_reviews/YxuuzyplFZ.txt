ID: YxuuzyplFZ
Title: EyeGraph: Modularity-aware Spatio Temporal Graph Clustering for Continuous Event-based Eye Tracking
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 8, 6, -1, -1, -1, -1
Original Confidences: 4, 5, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to eye tracking using event-based cameras, introducing "EyeGraph," a multi-modal near-eye tracking dataset collected with a wearable event camera. The authors propose a dynamic graph-based method for high-fidelity tracking of pupillary movement through an unsupervised topology-aware spatio-temporal graph clustering approach. The main contributions include the EyeGraph dataset, benchmarking against existing supervised state-of-the-art algorithms, and extensive experimental validation.

### Strengths and Weaknesses
Strengths:  
- The paper introduces an innovative dynamic graph-based method for event-based eye tracking, addressing issues like label sparsity and RGB-based inference.  
- The EyeGraph dataset is a significant contribution, offering data under varied luminance and user movement conditions.  
- The unsupervised clustering approach demonstrates promising results, comparable to supervised methods.  
- Comprehensive experimental validation supports the effectiveness of the proposed methods.

Weaknesses:  
- The achieved temporal resolution of the event-based cameras is unclear.  
- The claim that the EyeGraph dataset mimics in-the-wild settings lacks strong support due to controlled experimental conditions.  
- Missing details on ground truth data and its usage, along with unclear eye-tracking ground truth.  
- The dataset is limited to single-eye tracking, which may not fully capture binocular movements.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the achieved temporal resolution and provide a more robust justification for the in-the-wild claim regarding the EyeGraph dataset. Additionally, we suggest including other clustering methods in the benchmarking to evaluate overall performance for pupil coordinates. Clearer details on ground truth data and its application for gaze estimation are necessary. The authors should also consider expanding the dataset to include both eyes and diverse lighting conditions, as well as discussing the limitations of single-eye tracking and potential solutions. Lastly, we encourage the authors to present experimental results for each condition separately to enhance the overview of their findings.