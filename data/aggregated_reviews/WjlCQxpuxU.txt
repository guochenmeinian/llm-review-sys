ID: WjlCQxpuxU
Title: Action Inference by Maximising Evidence: Zero-Shot Imitation from Observation with World Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 6, 5, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an imitation learning approach that first trains a world model to predict next observations based on given actions, followed by a second phase where a policy is trained to maximize the likelihood of observations from expert demonstrations. The authors compare their method, AIME, to BCO(0) in the DMC walker and cheetah environments. The empirical results indicate that AIME outperforms previous methods in these tasks, although the experimental results raise some concerns regarding their clarity and anomalies.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant issue in deep reinforcement learning (DRL) related to sample inefficiency and proposes a novel method, AIME, which utilizes observational data effectively.
- AIME's two-phase learning process is unique, allowing agents to understand their own actions and behaviors, which is crucial for learning.
- Empirical validation shows that AIME outperforms state-of-the-art baselines, enhancing the credibility of the approach.

Weaknesses:
- The experimental results are difficult to interpret and contain anomalies, particularly in the Cheetah dataset, which raises questions about the robustness of the findings.
- The problem setting of AIME is limited, relying on action-labeled data in the first stage, which may not generalize well to scenarios with diverse policies or low-quality data.
- The paper does not adequately discuss the computational complexity of AIME or its scalability to more complex tasks, which could limit its practical application.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by including more ambitious and varied scenarios, such as using larger-scale datasets like those from VPT or open-source benchmarks like VD4RL, to enhance the generalizability of their findings. Additionally, consider addressing potential biases in the datasets used for comparison and explore the implications of using a mixture of policies in the training data. To strengthen the paper, clarify the significance of the problem settings studied and provide insights into the performance measurement methodology. Finally, discussing the computational complexity and scalability of AIME would provide a more comprehensive understanding of its applicability in real-world scenarios.