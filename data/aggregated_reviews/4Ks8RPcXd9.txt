ID: 4Ks8RPcXd9
Title: Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms
Conference: NeurIPS
Year: 2023
Number of Reviews: 35
Original Ratings: 2, 6, 5, 4, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a gradient manipulation method named SDMGrad for multi-task learning (MTL), which enhances the previous MGDA method by introducing two constraints: one constraining the common descent direction near a specific preference and another adding a regularization term for computed weights. The authors propose a stochastic multi-objective optimization (MOO) framework aimed at improving convergence properties and provide a stochastic non-convex convergence analysis. They conduct empirical experiments on multi-task supervised learning and reinforcement learning, although some important baselines, particularly Nash-MTL, are missing, raising concerns about the practical implementation of the proposed method.

### Strengths and Weaknesses
Strengths:
1. The paper proposes a method for MTL that contributes significantly to theoretical understanding.
2. A stochastic non-convex convergence analysis for the proposed methods is provided.
3. The code is available for reproducibility.
4. The authors engage with reviewer feedback, indicating a willingness to improve the manuscript.

Weaknesses:
1. The novelty of the proposed methods is limited, as they closely resemble existing work like CAGrad and MoCo.
2. Important baselines, particularly Nash-MTL, are not included, which outperforms SDMGrad in all benchmark datasets.
3. There are inconsistencies between the proposed method and its implementation, particularly regarding gradient normalization and hyperparameter settings.
4. The experimental results lack rigor and clarity, leading to potential misunderstandings regarding the performance of the proposed method compared to baselines.
5. The paper lacks clarity on how to set the regularization coefficient $\rho$ in Eq. (9), and the implementation appears to use $\rho=0$.

### Suggestions for Improvement
We recommend that the authors improve the novelty discussion by clearly articulating the advantages of the proposed SDMGrad over existing methods like CAGrad and MoCo. Additionally, include missing important baselines, especially Nash-MTL, in the experiments to provide a more comprehensive evaluation. Clarify the setting of the regularization coefficient $\rho$ and ensure that it is appropriately implemented in the code. It is essential to conduct a comprehensive ablation study to assess the contributions of the various components of the proposed method. Furthermore, we suggest providing clearer experimental results, including comparisons with relevant baselines, to avoid misleading interpretations of performance. Finally, consider moving the experimental section to the Appendix if significant revisions cannot be made to enhance clarity and rigor.