ID: 7FaWK7HpKK
Title: Interpreting Answers to Yes-No Questions in User-Generated Content
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new yes/no QA corpus aimed at interpreting answers to yes/no questions in social media, specifically Twitter. The dataset comprises 4,442 question-answer pairs, with a linguistic profile provided. The authors compare various methods, including baselines, RoBERTa, and GPT-3, and explore fine-tuning with different corpora. The study addresses a significant challenge in understanding informal responses that may not explicitly state "yes" or "no."

### Strengths and Weaknesses
Strengths:
- The corpus is well-constructed and follows standard data gathering and annotation practices.
- The linguistic analysis offers valuable insights into the nuances of interpreting user-generated content.
- The evaluation of multiple models provides a comprehensive comparison, highlighting the challenges of the task.

Weaknesses:
- The paper lacks clarity and polish, reading more like a draft than a final submission, with unclear sentences and repetitions.
- The significance of the task is not effectively conveyed, and the introduction fails to outline the research gaps adequately.
- Some claims lack supporting evidence, and the analysis of the dataset is shallow, with essential details misplaced in the appendix.
- The proposed dataset is relatively small, and the methods may require reconsideration due to potential issues with dataset splitting.

### Suggestions for Improvement
We recommend that the authors improve the clarity and structure of the paper, ensuring it reads as a polished submission. A more comprehensive introduction should be included to better outline the context and significance of the research gap. We suggest providing concrete evidence for claims made, particularly regarding precision improvements and criticisms of keyword-based methods. The authors should enhance the linguistic analysis to yield clearer takeaways or consider moving it to the appendix if it does not add value. Additionally, we advise a reconsideration of the methods used, especially concerning dataset splitting, and suggest expanding the dataset to enhance its robustness.