ID: LJCQH6U0pl
Title: Towards Principled Graph Transformers
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 1, 6, 5, 7, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Edge Transformer (ET), a novel architecture for graph learning tasks that utilizes global attention on node pairs rather than individual nodes. The authors demonstrate that ET possesses the expressive power of 3-WL and achieves competitive results in algorithmic reasoning and molecular regression tasks without relying on positional or structural encodings. The empirical evidence indicates that ET outperforms existing theoretically motivated models.

### Strengths and Weaknesses
Strengths:
1. ET bridges the gap between theoretical expressivity and practical performance.
2. The paper provides comprehensive empirical evidence showing that ET outperforms existing models and competes well with state-of-the-art methods in various graph learning tasks.
3. The proposed triangular attention mechanism is novel and the paper is well-written, making technical steps easy to follow.

Weaknesses:
1. The total number of parameters for the model is not provided, raising questions about the source of ET's performance.
2. ET's efficiency is still inferior to most graph transformers with O(n^2) complexity, limiting its applicability.
3. The authors' contribution primarily involves applying the ET model to graphs without proposing a new method.
4. The empirical evaluation lacks diversity in benchmark datasets, making it challenging to assess ET's practical performance comprehensively.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the parameter count for ET on each dataset to address concerns about performance sources. Additionally, the authors should explore ways to reduce the complexity of ET and provide a more detailed explanation of the differences in tokenization compared to existing models. Including a broader range of benchmark datasets, such as OGB and Zinc-full, would enhance the empirical evaluation. Furthermore, it would be beneficial to discuss the implications of using positional encodings in relation to ET's performance and expressivity.