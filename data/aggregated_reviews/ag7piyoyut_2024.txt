ID: ag7piyoyut
Title: Incorporating Surrogate Gradient Norm to Improve Offline Optimization Techniques
Conference: NeurIPS
Year: 2024
Number of Reviews: 23
Original Ratings: 5, 7, 5, 7, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a model-agnostic method to enhance offline optimization techniques by incorporating sharpness-aware minimization into the training loss of surrogate models. The authors introduce a new regularizer based on surrogate sharpness, which is characterized by the maximum output change under low-energy parameter perturbation. The method is theoretically analyzed and empirically validated, showing significant performance improvements across various tasks.

### Strengths and Weaknesses
Strengths:
1. The approach is model-agnostic, applicable across diverse models and optimization tasks.
2. The theoretical analysis provides a robust framework for understanding the regularization effect on offline optimization.
3. Extensive empirical testing demonstrates significant performance improvements, validating theoretical predictions.

Weaknesses:
1. The effectiveness relies heavily on the correct setting of hyperparameters, such as the sharpness threshold, which can be challenging to optimize.
2. The computational complexity is increased; a complexity analysis is needed.
3. The novelty of the method in relation to existing sharpness-aware minimization techniques could be better articulated.
4. Assumption 2 regarding the positive minimum eigenvalue of the parameter Hessian may be too restrictive, requiring more intuitive explanations or empirical evidence.
5. The paper lacks a comprehensive benchmarking against baseline methods and detailed descriptions of tasks and datasets.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the distinction between surrogate sharpness and loss sharpness, potentially repositioning Figure 1 for better understanding. The authors should enhance the Theoretical Analysis section by providing more insightful discussions based on Theorems 1 and 2, and improve the formatting of Equation 16. Additionally, we suggest conducting a thorough comparison with baseline methods across a wider range of tasks and providing a detailed complexity analysis of the proposed method. Finally, addressing the stability of performance gains and including sharpness visualizations on unseen data would strengthen the paper.