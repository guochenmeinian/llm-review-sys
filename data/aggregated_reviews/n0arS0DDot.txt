ID: n0arS0DDot
Title: BLAST: Block-Level Adaptive Structured Matrices for Efficient Deep Neural Network Inference
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 4, 5, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for compressing linear layers in deep learning models using a block low-rank structure, referred to as the BLAST matrix. The authors propose that this structure allows for lower complexity matrix multiplications by decomposing weight matrices into blocks of diagonal matrices with shared low-rank factors. The optimization process minimizes the Frobenius norm error between the original and compressed matrices through gradient descent, with preconditioning employed to enhance convergence speed. The technique is evaluated across various models and tasks, demonstrating improvements in validation accuracy for ViT-s on ImageNet-1k compared to existing methods. Additionally, the paper discusses the distinction between BLR^2 and BLAST matrices, emphasizing the need to decouple matrix structure terminology from compression methods and addressing the exclusion of Gaudi-GBLR and BSP+LR from comparisons.

### Strengths and Weaknesses
Strengths:
- The approach is original and builds on existing low-rank and block sparse techniques.
- The evaluation includes diverse models and tasks, showcasing the generalizability of the method.
- The writing is clear and well-organized, making the distinctions of BLAST matrices understandable.
- The authors provide a clear rationale for maintaining distinct terminology for matrix structures, which aids in clarity.
- They acknowledge and appreciate constructive feedback, indicating a willingness to improve the work.

Weaknesses:
- The proposed BLAST matrix structure is essentially an existing BLR^2 structure, which should be cited, and all references to BLAST should be replaced with BLR^2.
- The performance of Gaudi-GBLR surpasses the proposed method on CIFAR-10 and CIFAR-100, raising questions about the method's effectiveness.
- The significance of the proposed method is unclear, as it does not consistently outperform prior methods, and the compression rates achieved are limited.
- The logic behind excluding certain methods from comparisons lacks clarity.
- The authors have not identified suitable compression algorithms for BSP+LR or GBLR matrices, which limits the comprehensiveness of their experiments.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by addressing the following points:
- Clearly cite the original BLR^2 paper and replace all mentions of BLAST with BLR^2.
- Provide a more detailed comparison with Gaudi-GBLR and other methods excluded from the benchmarks, explaining their relevance to the task.
- Include configuration details such as the rank *r*, batch size, and input sequence length used in experiments for better reproducibility.
- Clarify the importance of shared factors U and V along rows and columns, and consider discussing the implications of allowing each block to have its own factors.
- Improve the quality of generated images in diffusion tasks by refining the layer selection for compression.
- Avoid attributing performance benefits solely to FLOPs savings without sufficient evidence; instead, focus on sharing throughput and latency behavior directly.
- Improve the justification for excluding Gaudi-GBLR and BSP+LR from comparisons by considering the best compression algorithms for these methods.
- Explore potential compression methods for BSP+LR and GBLR matrices that may have been overlooked.