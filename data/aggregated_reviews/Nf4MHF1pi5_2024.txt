ID: Nf4MHF1pi5
Title: Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on backdoor attacks targeting LLM-based agents through the poisoning of fine-tuning data. The authors categorize attacks into three types—Query-Attack, Observation-Attack, and Thought-Attack—based on the trigger's location and its effect on the agent's output. Experimental evaluations demonstrate the effectiveness of these attacks on tasks from AgentInstruct and ToolBench using LLaMA2-7B models. The paper also discusses preliminary countermeasures against these attacks, indicating that current defenses are inadequate.

### Strengths and Weaknesses
Strengths:
1. The authors provide a comprehensive taxonomy of backdoor attacks, which is crucial for understanding vulnerabilities in LLM agents.
2. Experimental results show that even a small percentage of poisoned samples can lead to significant attack success rates.
3. The paper is well-structured and presents its findings clearly, making it accessible to readers.

Weaknesses:
1. The novelty of the proposed attacks is limited, as they closely resemble existing backdoor attack formulations in reinforcement learning literature.
2. The experimental settings, particularly for the Thought-Attack, utilize unrealistic poisoning ratios, which may not reflect real-world scenarios.
3. The mathematical formalism is overly complex and could benefit from simplification for better clarity.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by exploring lower poisoning ratios, such as 1%, to assess the attack success rate more realistically. Additionally, incorporating a baseline where all tools are equally represented in the training data would strengthen the findings. We suggest streamlining the mathematical formalism to enhance clarity, particularly by clarifying the necessity of expectations in Equation (2) and addressing the LaTeX errors noted in Equations (3) and (4). Lastly, we encourage the authors to provide a more detailed discussion on how their work differs from existing literature on backdoor attacks in reinforcement learning.