ID: lArwl3y9x6
Title: Normalization Layers Are All That Sharpness-Aware Minimization Needs
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 3, 9, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an adversarial perturbation method linked to Sharpness-Aware Minimization (SAM) that focuses solely on the affine normalization parameters, termed SAM-ON (SAM-OnlyNorm). The authors demonstrate that this approach yields improvements over standard SAM and various SAM variants, particularly in terms of computational efficiency. The investigation into the effectiveness of SAM-ON shows consistent performance improvements over traditional optimizers and SAM-all across various architectures and datasets, including Vision Transformers (ViTs) trained from scratch on ImageNet. The authors evaluate out-of-distribution robustness on ImageNet-R and ImageNet-C and test multiple architectures, including VGG and DenseNet, on CIFAR10 and CIFAR100. They also explore the relationship between SAM-ON and adversarial robustness, indicating that perturbing only normalization layers can yield significant benefits, although the relationship between sharpness and generalization remains ambiguous.

### Strengths and Weaknesses
Strengths:
1. The proposed SAM-ON method improves upon existing SAM variants by only perturbing normalization layers, which constitute a small fraction of total parameters.
2. The findings provide new insights into the role of normalization layers in SAM, suggesting they may be critical for its effectiveness.
3. The authors provide comprehensive experimental results, demonstrating SAM-ON's superiority across multiple architectures and datasets.
4. The exploration of the relationship between SAM-ON and adversarial robustness adds valuable insights to the understanding of normalization layers in deep learning.

Weaknesses:
1. The performance gains reported, particularly in Table 1, are marginal, raising questions about the originality of the approach.
2. The justification for SAM-ON is insufficient, as the authors do not adequately explain the observed increase in sharpness or its implications for other metrics of flatness.
3. Some results, particularly for ViTs, are still pending completion, which may limit the robustness of the findings.
4. The reduction in test accuracy when applying SAM to only part of the normalization layers raises questions about the trade-offs involved.
5. The results are limited to CIFAR and ImageNet, with a lack of exploration into other datasets or benchmarks, particularly regarding out-of-distribution robustness.
6. The paper does not sufficiently address the effectiveness of SAM-ON across various architectures, especially for larger datasets.

### Suggestions for Improvement
We recommend that the authors improve the justification for SAM-ON by providing a more thorough theoretical analysis of why perturbing only the normalization layers leads to better performance. Additionally, the authors should conduct experiments on a wider range of datasets and architectures, including VGG and DenseNet, to strengthen the generalizability of their findings. It would also be beneficial to explore and discuss the implications of sharpness in relation to other metrics of flatness, as well as to clarify the relationship between normalization layers and adversarial robustness. Furthermore, we suggest that the authors provide final results for ViTs in the revised version to strengthen the overall conclusions. Finally, a more comprehensive discussion of the limitations of the study is warranted.