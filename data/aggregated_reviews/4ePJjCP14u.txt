ID: 4ePJjCP14u
Title: Rethinking Bayesian Optimization with Gaussian Processes: Insights from Hyperspectral Trait Search
Conference: NeurIPS
Year: 2023
Number of Reviews: 2
Original Ratings: 5, 5
Original Confidences: 4, 4

Aggregated Review:
### Key Points
This paper presents an analysis of the limitations of standard Gaussian processes (GP) regression for Bayesian optimization (BO) in the context of co-heritability search from hyperspectral reflectance data, particularly addressing sharp and aperiodic hypothesis spaces. The authors utilize toy data and complex mathematical functions to support their findings. The originality lies in demonstrating a scientific optimization problem where conventional GP assumptions fail, providing practitioners insight into when not to rely on standard BO-GP. The paper proposes a new benchmark for developing better solutions, although it lacks a comprehensive discussion on novel approaches to address these limitations.

### Strengths and Weaknesses
Strengths:
- Demonstrates negative results and issues with applying plain GP for BO.
- Addresses a real-world scientific problem with empirical results and comparisons to random search.
- Provides useful insights into model selection for misspecified models.

Weaknesses:
- Lacks alternative solutions beyond random search; future work is only briefly mentioned.
- Limited novelty in the algorithms presented.
- Insufficient information regarding the publication of the new benchmark dataset as an open resource.
- Incorrect conclusions regarding the fundamental limitations of Gaussian processes, specifically not acknowledging the role of kernel choice and hyperparameters.

### Suggestions for Improvement
We recommend that the authors improve the accuracy of their conclusions by restricting claims about "Gaussian processes" to "Gaussian processes with the Matern kernel" or "Gaussian processes with standard kernels." It is crucial to engage with previous literature on the effects of hyperparameters, as these significantly influence the performance of GP-BO compared to random search. We suggest that the authors study the impact of hyperparameters on their results and revise their main conclusion to reflect that the choice of kernel and hyperparameters is critical. Additionally, we advise correcting the computational complexity statement regarding fitting the GP, as it should be $O(N^2)$ with proper implementation.