ID: 9SpWvX9ykp
Title: Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 6, 8, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to model-based reinforcement learning (RL) by proposing the Generate, Improve, and Fix with Monte Carlo Tree Search (GIF-MCTS) framework, which synthesizes code world models (CWMs) from offline datasets of environment interactions. The authors demonstrate that these CWMs, represented as Python programs, can effectively model environments and facilitate policy learning. The paper also introduces a benchmark for evaluating CWMs, showing that GIF-MCTS achieves reasonable performance across various environments. However, concerns arise regarding the assumptions about offline datasets, comparisons to offline RL methods, and the comprehensiveness of the related work section.

### Strengths and Weaknesses
Strengths:
- The motivation for using code to model environments is compelling, and the framework effectively addresses this.
- The exploration of offline setups provides a unique perspective on code world models, distinguishing it from previous work.
- The writing is clear, and figures are utilized effectively to illustrate concepts.

Weaknesses:
- The dataset collection method raises questions about how to ensure diverse scoring behaviors without true reward functions.
- The impact of dataset quality on CWM performance is unclear, particularly regarding the use of random policy trajectories.
- The paper lacks a thorough comparison to offline RL methods and does not adequately address the limitations of relying on pre-trained models.
- The related work section is not comprehensive, missing significant contributions in programmatic RL.
- Clarity issues exist, particularly regarding the term "rollout" and the specifics of LLM-generated code.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the introduction and abstract to align expectations regarding the offline setup. Additionally, addressing the dataset collection methodology and its implications on performance is crucial. The authors should include comparisons to offline RL methods to contextualize their contributions better. Expanding the related work section to include significant prior research in programmatic RL is necessary. Finally, clarifying the use of "rollout" and providing detailed specifications on LLM actions would enhance understanding.