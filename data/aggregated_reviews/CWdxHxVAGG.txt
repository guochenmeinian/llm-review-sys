ID: CWdxHxVAGG
Title: Reject option models comprising out-of-distribution detection
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 4, 4, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a formal analysis of three distinct reject option models for classifiers dealing with out-of-distribution (OOD) inputs: cost-based rejection, bounded TPR-FPR, and bounded precision-recall. The authors propose double-score OOD methods that leverage uncertainty scores from both OOD detection and misclassification detection, aiming to improve evaluation metrics for OOD methods. The paper argues that existing metrics provide incomplete or inconsistent evaluations when used separately or in combination, and introduces novel metrics that better capture the performance of selective classifiers in OOD settings. Additionally, the authors propose a double-score rule aimed at enhancing decision-making in the context of ID and OOD sample rejections, suggesting that distinct costs for rejecting ID and OOD samples can improve the decision framework.

### Strengths and Weaknesses
Strengths:
- The authors extend the concept of rejection in non-OOD settings to OOD detection, providing a systematic and original approach.
- The proposed models share a common class of optimal strategies, demonstrating that they outperform existing baselines.
- The theoretical merits of the double-score concept are acknowledged, indicating a solid foundation for the proposed methodology.
- The authors provide a structured approach to parameter tuning, referencing equations (13) and (14) for guidance.

Weaknesses:
- The assumption that the OOD distribution is known significantly limits the practical applicability of the proposed methods.
- The experimental section lacks sufficient detail, with results based on a single run and limited datasets, raising concerns about generalizability.
- The proposed method does not adequately differentiate between ID and OOD rejections, limiting its practical application.
- The reliance on OOD data is a significant barrier, as the method cannot be confidently applied without clear OOD samples.
- The discussion surrounding the hyper-parameter $\mu$ is deemed insufficient, raising concerns about its determination and the overall trustworthiness of the decision-making process. Additionally, the notation is inconsistently used, which may confuse readers.
- There are concerns regarding the simultaneous fulfillment of constraints in the proposed formulations, questioning the validity of achieving desired false positive and true positive rates.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by providing more detailed documentation of models, training methods, and hyper-parameters, as well as conducting multiple runs to assess statistical significance. Additionally, we suggest that the authors clarify the implications of their assumption regarding the known OOD distribution and discuss potential strategies for practical applications where this may not hold. Enhancing the motivation for Section 3 and including visual aids, such as figures, could also improve clarity and engagement. Furthermore, we recommend that the authors improve the clarity of the discussion surrounding the hyper-parameter $\mu$, providing explicit guidance on its determination to enhance trustworthiness. Addressing the limitations related to the access and estimation of OOD data is crucial for practical applicability. We suggest clarifying the simultaneous adjustment of $\lambda$ and $\mu$ in the context of equations (13) and (14) to alleviate confusion regarding the constraints. Lastly, correcting the notation issue regarding $\mu$ will enhance the paper's precision and readability.