ID: ZAMQmPeKLR
Title: Hallucination Detox: Sensitive Neuron Dropout (SeND) for Large Language Model Training
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 5, 8, 8
Original Confidences: 4, 4, 5

Aggregated Review:
### Key Points
This paper presents a novel training protocol called Sensitive Neuron Dropout (SeND) aimed at mitigating hallucinations in large language models (LLMs) by systematically dropping neurons with significant variability, termed Sensitive Neurons. The authors also introduce an Efficient EigenScore (EES) for hallucination detection, which approximates traditional EigenScore calculations at twice the speed. The empirical basis for their method is grounded in the observation of oscillatory behavior in hallucinations during training. Experiments conducted on Eleuther AIâ€™s Pythia 1B model using the HELM and MedHALT datasets demonstrate a 40% improvement in FactScore performance when SeND is applied alongside Retrieval Augmented Generation.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel method to mitigate hallucinations during LLM training, enhancing factual confidence.
- It clearly defines the problem of fluctuating hallucinations, providing a solid foundation for the proposed method.
- The method shows strong performance, improving LLM reliability by up to 40% compared to standard training.

Weaknesses:
- The evaluation of SeND is limited to the Pythia 1B model, lacking exploration across a broader range of LLM architectures.
- The choice of datasets (HELM and MedHALT) raises concerns; the authors should address the robustness of their method across diverse datasets.
- Targeting hallucinatory neurons is an established technique, and the authors need to demonstrate the scalability of their approach with larger datasets and models, as the experiments lack diversity in model size and type.

### Suggestions for Improvement
We recommend that the authors improve the writing style for clarity and efficiency, particularly in section "3.2 PERFORMANCE OF SEND ON PYTHIA 1B," where important results are relegated to the Appendix. Additionally, the rationale for including the last token matrix in Equation 1 should be clarified. The authors should also provide a more comprehensive analysis of the comparison between SEND and random dropout, as well as address the sensitivity of neurons over epochs, particularly the lack of observations regarding activation changes during initial versus later epochs. Finally, expanding the evaluation to include a wider range of LLM architectures and datasets would strengthen the paper's contributions.