ID: Cc5yhA1PrC
Title: A Joint Matrix Factorization Analysis of Multilingual Representations
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis tool based on PARAFAC2, a joint matrix factorization method, to compare latent representations of multilingual and monolingual language models. The authors aim to analyze how multilingual models encode morphosyntactic features across layers and the correlation between factorization outputs and downstream task performance. They conduct an empirical study using multilingual and monolingual models pre-trained on 33 languages, revealing variations in morphosyntactic encoding influenced by language properties. The findings indicate a strong correlation between factorization outputs and performance on cross-lingual tasks in the XTREME benchmark.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel analysis tool that enables comprehensive comparisons of multilingual representations.
- It provides clear answers to research questions regarding morphosyntactic feature encoding and its variations across languages and layers.
- The large-scale empirical study enhances the generalizability of the findings.
- The correlation analysis between factorization outputs and task performance offers practical insights for downstream task selection.

Weaknesses:
- The paper lacks clarity in explaining technical details of the joint matrix factorization analysis, such as enforcing the matrix V to be identical for all decompositions.
- There is insufficient quantitative discussion comparing the proposed method with existing techniques like SVCCA and CKA, limiting the understanding of its advantages and disadvantages.
- The research scope is narrow, focusing primarily on morphosyntactic features without considering other linguistic aspects, and the models used are limited.

### Suggestions for Improvement
We recommend that the authors improve the clarity of technical details, particularly regarding how to "enforce the matrix V to be identical for all decompositions" and the method for obtaining token embeddings. Additionally, we suggest providing a comprehensive quantitative comparison with existing methods like SVCCA and CKA to highlight the advantages of the proposed approach. Expanding the analysis to include other linguistic features, such as semantics or pragmatics, would enhance the paper's applicability. Finally, addressing the limitations of the data used and ensuring the usability of the software and pre-trained models for a broader audience would strengthen the paper's contributions.