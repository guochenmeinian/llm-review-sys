ID: uiiVSVADDc
Title: Annotator: A Generic Active Learning Baseline for LiDAR Semantic Segmentation
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 6, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Annotator, a voxel-centric online active learning baseline for label-efficient LiDAR point cloud semantic segmentation. The authors propose a novel label acquisition strategy called voxel confusion degree (VCD), which enables significant reductions in annotation requirements while maintaining performance comparable to fully supervised methods. The study evaluates Annotator across various active learning setups, including active learning (AL), active source-free domain adaptation (ASFDA), and active domain adaptation (ADA), demonstrating its effectiveness through extensive experiments on multiple datasets.

### Strengths and Weaknesses
Strengths:
- The proposed VCD criterion is sensible and leads to improved results with fewer annotations.
- The experiments are comprehensive, covering different setups and demonstrating the method's effectiveness.
- The paper addresses a critical problem in active learning for industrial applications, which is underexplored.

Weaknesses:
- The contribution is primarily centered on the VCD selection criterion, while the broader field of active learning is presented as a contribution without sufficient depth.
- The writing quality is low, with misused technical terms and vague statements.
- The authors only compare their approach to two selection criteria, neglecting other relevant methods without justification.
- Results appear questionable, particularly regarding the performance achieved with minimal annotations in a multi-class setting.

### Suggestions for Improvement
We recommend that the authors improve the clarity and precision of their writing, ensuring that technical terms are used correctly and vague statements are clarified. Additionally, we suggest expanding the comparison to include other selection criteria and providing justifications for any omitted methods. The authors should also address the concerns regarding the validity of their results, particularly the implications of training large networks with minimal annotations. Finally, we encourage the inclusion of a dedicated section discussing the limitations of the work to enhance transparency.