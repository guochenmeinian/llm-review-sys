ID: Xq2s5yxzd2
Title: Multi-Prompt Alignment for Multi-Source Unsupervised Domain Adaptation
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to multi-source unsupervised domain adaptation (UDA) by introducing prompt learning. The authors propose a method that learns individual prompts for each source-target pair using a contrastive loss, followed by aligning these prompts through an autoencoder-based step with an L_1 constraint. The Latent Subspace Tuning (LST) strategy facilitates efficient adaptation to subsequent target domains. The effectiveness of the proposed method is validated through experiments on datasets such as ImageCLEF, Office-Home, and DomainNet.

### Strengths and Weaknesses
Strengths:
- The application of prompt learning to multi-source UDA is groundbreaking.
- The paper is well-written and easy to understand.
- The motivation is meaningful, and the proposed method aligns well with it.
- Empirical results demonstrate good performance across various benchmarks.

Weaknesses:
- The reliance on CLIP as the backbone may restrict the method's applicability to classification tasks, raising questions about its extension to other tasks like segmentation.
- The prompt design lacks novelty, primarily following Ge et al. (2022) without original contributions.
- The dimensionality reduction step in prompt design appears limited in its effectiveness, and the necessity of the autoencoder for this purpose is questionable.
- The LST's independence as an innovation point is insufficiently established, and comparisons to stronger baselines are missing.

### Suggestions for Improvement
We recommend that the authors improve the novelty of the prompt design by incorporating original elements beyond Ge et al. (2022). Additionally, the authors should clarify the effectiveness of the dimensionality reduction methods used and consider removing the autoencoder step if it does not provide substantial benefits. It is crucial to include comparisons with stronger CLIP-based baselines in the multi-source UDA experiments and to provide more detailed discussions on the prompt's class-specific and domain-specific attributes. Furthermore, exploring alternative prompt engineering strategies could enhance the framework's robustness and applicability.