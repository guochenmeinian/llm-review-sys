ID: rJhk7Fpnvh
Title: Sources of Hallucination by Large Language Models on Inference Tasks
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into hallucinations in LLMs during inference tasks, particularly focusing on incorrect entailment predictions. The authors propose that these hallucinations arise from two biases: the attestation bias, linked to the presence of hypotheses in pretraining data, and the relative frequency bias, associated with statistical patterns in that data. The study aims to enhance the understanding of these biases to improve LLM training and deployment.

### Strengths and Weaknesses
Strengths:  
- The paper identifies significant limitations in current LLMs, emphasizing the need to differentiate performance on specific datasets from general task performance.  
- It features an elegant experimental design that effectively demonstrates the two heuristics and the memorization mechanisms involved.  
- The methodology creatively illustrates how biases can lead to hallucinations in LLMs.

Weaknesses:  
- The experimental validation of the authors' theory is unclear, particularly regarding the random premise task and the assurance of NO-ENTAIL outcomes.  
- The dataset transformation and experimental setup are perceived as cumbersome and lacking clarity.  
- The contribution may be limited due to the focus on only NLI tasks and three LLMs, and the paper is somewhat difficult to follow without sufficient examples.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their experimental validation, particularly in ensuring that the random premise task consistently results in NO-ENTAIL outcomes. Additionally, we suggest that the authors provide a more detailed explanation of the relative frequency bias and enhance the overall presentation of their methodology to facilitate understanding. Finally, we encourage the authors to clarify the main contributions in the abstract to better convey the significance of their work.