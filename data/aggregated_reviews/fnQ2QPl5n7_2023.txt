ID: fnQ2QPl5n7
Title: GUARD: A Safe Reinforcement Learning Benchmark
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 4, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GUARD (Generalized Unified SAfe Reinforcement Learning Development Benchmark), a significant contribution to safe reinforcement learning (RL). It addresses the challenges of applying RL in safety-critical applications by providing a comprehensive benchmark that includes a diverse array of RL agents, tasks, and safety constraints. The authors propose a self-contained implementation of state-of-the-art safe RL algorithms, facilitating reproducibility and experimentation. Additionally, the paper analyzes various reinforcement learning algorithms, including Unconstrained Methods (TRPO), Constrained Methods (TRPO-IPO, TRPO-Lagrangian, PCPO, CPO, TRPO-FAC), and Hierarchical Methods (USL and Safelayer). The authors provide insights into their performance, noting that while TRPO-based methods excel in high reward performance, they lack cost considerations. Constrained methods account for costs but exhibit slower reward increases, with TRPO-IPO showing the lowest reward performance. Hierarchical methods enhance safety but require a warm-up period, and their performance varies across different robots. The paper also acknowledges limitations in evaluation metrics, reliance on outdated libraries, and the focus on on-policy algorithms.

### Strengths and Weaknesses
**Strengths:**
1. GUARD serves as a comprehensive benchmark, covering a wide range of RL agents, tasks, and safety constraints, enhancing its applicability to real-world scenarios.
2. The implementation of state-of-the-art safe RL algorithms is self-contained, promoting reproducibility and ease of use for researchers.
3. The paper provides a thorough examination of various reinforcement learning algorithms and their respective performances, identifying key limitations in the GUARD benchmark.

**Weaknesses:**
1. The analysis of constraint violations lacks clarity, as there are no visualizations to indicate which algorithms violated constraints or achieved high rewards without violations.
2. The paper does not sufficiently compare GUARD with existing benchmarks, failing to highlight its unique advantages.
3. The codebase has been reported as non-functional by some users, raising concerns about its usability.
4. The benchmark experiments offer limited insights into the nuances of algorithm performance, lacking thorough analysis and ablation studies.
5. There is insufficient coverage of algorithm diversity, and the complexity of real-world robot tasks exceeds current capabilities.
6. The evaluation metrics are limited, and the reliance on outdated libraries detracts from the overall effectiveness of the analysis.

### Suggestions for Improvement
We recommend that the authors improve the clarity of constraint violation analysis by including visualizations or detailed explanations. Additionally, the authors should provide a more comprehensive comparison with previous benchmarks to emphasize GUARD's unique contributions. Addressing the challenges and limitations of current safe RL algorithms explicitly in the paper would enhance its depth. Furthermore, we suggest the authors consider enhancing the modularity of the codebase to facilitate user modifications for tasks and constraints, ensuring the codebase is functional and well-documented to support the research community effectively. We also recommend improving the performance analysis in section 6 by including more discussions and visualizations. Expanding the evaluation metrics to provide a more holistic view of agent capabilities is essential. Updating the Mujoco interface for compatibility with the latest library versions will enhance usability. Additionally, extending the algorithms library to include both on-policy and off-policy strategies will increase the benchmark's versatility. Finally, fine-tuning parameters for robots and environments is crucial for ensuring simulation stability, and documenting these adjustments will aid in transparency and reproducibility.