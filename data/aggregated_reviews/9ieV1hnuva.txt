ID: 9ieV1hnuva
Title: Hypervolume Maximization: A Geometric View of Pareto Set Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 6, 7, 6, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 3, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to multi-objective optimization that enables the direct modeling of the entire Pareto set using neural networks. The authors establish an equivalence between learning the complete Pareto set and hypervolume maximization, providing a theoretical analysis of the gap between estimated and true hypervolumes. The proposed method is evaluated across various benchmark and real-world problems, demonstrating promising results compared to existing algorithms.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a crucial equivalence between Pareto set learning and hypervolume maximization, facilitating convergence analysis.
2. The theoretical foundation is well-established, particularly regarding the generalization gap between estimated and true hypervolumes.
3. The experimental results validate the proposed method's superiority over baseline approaches.

Weaknesses:
1. The innovations of the proposed method are not sufficiently emphasized, particularly regarding efficiency improvements over other Pareto learning methods.
2. The details of the proposed method are inadequate, making it vulnerable to misinterpretation.
3. The empirical studies lack diversity in problem complexity, primarily focusing on 2 or 3 objective optimization problems, which raises questions about generalizability.

### Suggestions for Improvement
We recommend that the authors improve the emphasis on the innovations of their method, particularly its efficiency compared to existing techniques. Additionally, we suggest providing more detailed descriptions of the proposed method to strengthen its interpretability. To enhance the empirical analysis, we encourage the authors to include a broader range of benchmark problems, such as those from DTLZ and WFG, and to explore the scalability of their approach to larger problem instances. Furthermore, it would be beneficial to elaborate on the design of the neural network structure and the construction of the training data to clarify its relationship with benchmark problems.