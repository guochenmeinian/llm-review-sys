ID: GDYuzX0rwj
Title: Facing Off World Model Backbones: RNNs, Transformers, and S4
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 5, 5, 6, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents S4WM, a novel world model architecture for model-based reinforcement learning (MBRL) that utilizes Structured State Space Sequence (S4) models to enhance long-term memory capabilities. The authors evaluate S4WM against traditional RNN and Transformer-based models, demonstrating its superior performance in memory-demanding tasks. Additionally, the paper discusses the evaluation of generation consistency in non-stochastic environments, emphasizing the challenges posed by stochastic dynamics. The authors propose that action-dependent modeling simplifies the evaluation of memorization capabilities by focusing on non-stochastic transitions and explore the long-term memory capacity of MBRL agents through benchmarks assessing various abilities, particularly with sequences of up to 2000 steps. They also address the limitations of existing models like RSSM and TSSM in reconstructing color information due to their memory constraints and computational complexities.

### Strengths and Weaknesses
Strengths:
- The integration of S4 into world models is a significant advancement, showcasing improved long-term memory.
- The experimental design is thorough, addressing various aspects of the proposed methods, including reward prediction and memory efficiency.
- The paper effectively addresses the challenges of evaluating generation consistency in non-stochastic environments.
- It provides a thorough investigation into the memory capacity of different models, particularly S4WM, in complex environments.
- The authors demonstrate significant improvements over existing models in specific benchmarks.

Weaknesses:
- The contribution appears incremental, heavily relying on S4 without sufficiently distinguishing itself from existing benchmarks.
- The evaluation lacks depth, particularly in complex environments, and does not adequately explore the impact of the proposed model on advanced MBRL algorithms.
- Some sections, such as the explanation of the architecture's differences from Dreamer, are insufficiently detailed.
- The paper lacks a clear set of synthetic tasks that adequately represent the MBRL problem.
- There are limitations in the performance of S4WM and TSSM compared to RSSM, which are noted but not sufficiently addressed in the results.

### Suggestions for Improvement
We recommend that the authors improve the discussion of existing benchmarks, particularly regarding the reference to Yan et al. (2022), to clarify how their work differentiates itself. Additionally, we suggest enhancing the depth of the benchmark evaluation by including trajectory reconstruction insights and exploring the model's performance in more complex environments. It would also be beneficial to evaluate S4WM in online learning scenarios and consider multi-step prediction during training to assess its potential advantages. Furthermore, we recommend improving the clarity of the evaluation metrics for generation consistency in stochastic environments and providing a more comprehensive set of synthetic tasks that can serve as indicators for MBRL capabilities. Lastly, we suggest including a discussion of the results in the context of limitations, particularly regarding the performance of TSSM and S4WM relative to RSSM.