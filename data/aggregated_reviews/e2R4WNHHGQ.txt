ID: e2R4WNHHGQ
Title: Fair Bilevel Neural Network (FairBiNN): On Balancing fairness and accuracy via Stackelberg Equilibrium
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a bi-level optimization approach aimed at optimizing performance and fairness in machine learning models. The authors argue that their method is theoretically superior to traditional regularization techniques that incorporate fairness objectives. They demonstrate the effectiveness of their approach through experiments on various datasets, including tabular, graph, and vision data, showcasing the Pareto frontiers achieved compared to multiple baselines.

### Strengths and Weaknesses
Strengths:
- The authors provide a novel perspective on fairness optimization through bi-level optimization, which is not commonly applied in this context.
- Theoretical analysis and proofs are rigorously presented, with Theorem 3.9 illustrating the advantages of their approach.
- Extensive experiments yield promising results across multiple domains, demonstrating competitive performance against state-of-the-art methods.

Weaknesses:
- The paper lacks clarity in several areas, particularly regarding the assumptions made and their implications.
- The connection between theoretical results and practical implementation is insufficiently detailed.
- The methodology section is dense and challenging to follow, with assumptions simply listed rather than discussed.
- Missing experimental details, such as compute time and parameter counts for baselines, hinder a comprehensive understanding of the proposed method's practicality.

### Suggestions for Improvement
We recommend that the authors improve clarity by discussing the implications of the assumptions made, particularly regarding smoothness and convergence. It would be beneficial to provide a more detailed exploration of the relationship between theory and implementation. Additionally, we suggest that the authors analyze related works on constrained or multi-objective optimization of hyper-parameters to contextualize their approach better. The methodology section should be streamlined for better readability, and the experimental baselines need clearer descriptions to facilitate comparison. Finally, we encourage the authors to explicitly address the limitations of their approach, including the complexity of hyper-parameter selection and the challenges of reusing published models.