ID: 3W2RQMdVXF
Title: MMLU-Pro+: Evaluating Higher-Order Reasoning and Shortcut Learning in LLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 7, 7, 7
Original Confidences: 4, 3, 4

Aggregated Review:
### Key Points
This paper presents MMLU-Pro+, a novel benchmark designed to assess higher-order reasoning and shortcut learning in large language models (LLMs). The authors propose significant modifications to the existing MMLU-Pro dataset, including the introduction of questions with multiple correct answers, which challenges LLMs to evaluate the validity of various statements and engage in complex reasoning. The experimental design is robust, utilizing multiple cutting-edge LLMs to highlight performance variations. The paper also introduces new evaluation metrics, such as the shortcut selection ratio and correct pair identification ratio, which provide deeper insights into model behavior.

### Strengths and Weaknesses
Strengths:  
- The paper contributes to the community by automating dataset augmentation, expanding the dataset space significantly.  
- It aligns well with the workshop's focus on the implications of misleading information in LLMs.  
- The experimental design effectively demonstrates the benchmark's ability to discern model performance variations.

Weaknesses:  
- The manuscript lacks comprehensive details regarding the experimental setup, including model configurations and hyperparameters, which hinders transparency and reproducibility.  
- There is insufficient elaboration on the practical implications of the newly proposed evaluation metrics, limiting their relevance beyond the research context.

### Suggestions for Improvement
We recommend that the authors improve the manuscript by providing detailed information about model configurations and hyperparameters to enhance transparency and facilitate reproducibility. Additionally, we suggest that the authors elaborate on the practical implications of the new evaluation metrics, discussing how they can be utilized in real-world applications to solidify the benchmark's significance in advancing LLM evaluation and development.