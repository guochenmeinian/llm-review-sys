ID: WxxYSpsv97
Title: Generating Extractive Answers: Gated Recurrent Memory Reader for Conversational Question Answering
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Gated Recurrent Memory Reader (GRMR), which integrates traditional extractive Machine Reading models into a generalized sequence-to-sequence framework for conversational question answering (CQA). The authors demonstrate improved space efficiency on the CoQA dataset compared to traditional models, but report a significant performance gap when compared to state-of-the-art models like FlowQA and SDNet.

### Strengths and Weaknesses
Strengths:  
- The GRMR method offers a space-efficient architecture for CQA tasks.  
- The study provides sufficient support for its claims and arguments, demonstrating a new approach to memory efficiency.

Weaknesses:  
- The F1 score is considerably lower than that of models using contextualized embeddings, with a notable gap when compared to recent architectures.  
- The baselines in the experiments are outdated, primarily consisting of non-LLM models from before 2020, which diminishes the relevance of the comparisons.  
- There is a lack of detailed memory overhead comparisons and insufficient parameter specifications, making reproducibility challenging.

### Suggestions for Improvement
We recommend that the authors improve the relevance of their baseline comparisons by including more recent models, such as RoBERTa or GPTs. Additionally, we suggest updating Table 3 to provide more granular data, including separate rows for FlowQA and SDNet, and incorporating a plot that compares the trade-off between space efficiency and F1 scores of GRMR against the best-performing models. A deeper discussion on the accuracy versus space trade-offs, particularly in the 0-ctx case, would also enhance the paper's contribution.