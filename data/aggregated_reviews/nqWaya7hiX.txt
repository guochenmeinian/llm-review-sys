ID: nqWaya7hiX
Title: Wings: Learning Multimodal LLMs without Text-only Forgetting
Conference: NeurIPS
Year: 2024
Number of Reviews: 24
Original Ratings: 6, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the text-only forgetting phenomenon in multimodal large language models (MLLMs), where performance declines on text-only evaluations after training with visual inputs. The authors attribute this issue to attention shifts in MLLM-LAWS (Layer-level Attention Weights) before and after processing images. To mitigate this, the authors propose WINGS, a model designed to balance the influence of visual and textual inputs by processing visual features through an independent module alongside the LLM attention block. Experimental results across text-only, visual question answering (VQA), and a newly constructed Interleaved Image-Text (IIT) benchmark demonstrate the effectiveness of WINGS. The authors acknowledge limitations, such as the scarcity of spatial relationship instructions in their training set, which may lead to an over-reliance on rare spatial information. They also explore the concept of modality expansion, suggesting that addressing audio features could complement the existing visual and textual modalities.

### Strengths and Weaknesses
Strengths:
- The observation linking text-only forgetting to MLLM-LAWS is intriguing.
- WINGS shows significant improvements in both text-only and multimodal QA tasks.
- The focus on the important issue of text-only forgetting is commendable, and the performance metrics presented are impressive.
- The attention shift analysis demonstrates a thoughtful approach to understanding multimodal interactions.
- The paper is well-organized and presents clear writing.

Weaknesses:
- The paper lacks a deep discussion on the underlying reasons for the correlation between text-only forgetting and MLLM-LAWS, particularly since no images are involved in text-only evaluations.
- There is insufficient explanation regarding the LoRa's impact on multimodal performance, despite its efficiency in mitigating text-only forgetting.
- The ablation study is limited, only comparing results on the IIT benchmark without broader evaluations across other benchmarks.
- The generalizability of the attention shift findings is questionable, as they rely on models trained with similar data distributions and optimization strategies.
- The lack of explicit regularization to suppress attention shifts raises concerns about the effectiveness of WINGS in mitigating reliance on visual features.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the underlying mechanisms of the correlation between text-only forgetting and MLLM-LAWS, particularly addressing why this relationship exists without images. Additionally, we suggest providing a detailed explanation of the LoRa's performance trade-offs and including ablation studies across various benchmarks to strengthen the findings. We also recommend improving the clarity of the router operation in the paper, ensuring that the output weights are accurately described. Furthermore, we suggest that the authors provide a clearer explanation of how WINGS achieves balance between visual and textual learners, potentially incorporating explicit regularization techniques to address attention shifts more effectively. Finally, we encourage the authors to conduct further experiments with diverse training data and optimization strategies to enhance the generalizability of their findings.