ID: UmW9BYj761
Title: No Filter: Cultural and Socioeconomic Diversity in Contrastive Vision-Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper discusses the negative impact of filtering for English-only data on the performance of vision-language models (VLMs), particularly in tasks involving diverse cultural regions. The authors propose eliminating this filtering step and suggest a short English-only fine-tuning stage to balance standard task performance with cultural diversity. The findings indicate that training solely on English data leads to a bias towards Western-centric benchmarks, while training on unfiltered datasets can enhance cultural understanding without sacrificing performance on established benchmarks.

### Strengths and Weaknesses
Strengths:  
- The paper addresses the important issue of cultural bias in VLMs and proposes a straightforward solution that could influence future training practices.  
- The experiments are thorough and provide solid empirical support for the claims made.  
- The proposed training pipeline could help mitigate cultural bias while maintaining performance on popular benchmarks.

Weaknesses:  
- The contribution appears limited, as it is expected that training on global data would improve performance on benchmarks measuring global understanding.  
- The findings lack contextualization with prior works on training CLIP-style models and multilingual understanding.  
- The necessity of the proposed geo-localization task is unclear, and the writing lacks clarity in structure and relation between sections.  
- There is insufficient discussion on resource and computational limitations, which may affect the feasibility of training on larger datasets.

### Suggestions for Improvement
We recommend that the authors improve the contextualization of their findings by referencing prior works on the effects of training CLIP-style models on large, weakly-curated datasets. Additionally, the authors should clarify the necessity of the geo-localization task and enhance the clarity of the writing throughout the paper. It would be beneficial to include a discussion on resource limitations and the implications of training on larger datasets, particularly for underrepresented communities. Finally, we suggest that the authors provide specific evidence to support claims regarding the exacerbation of disparities and ensure that the claims in the "Summary of Findings" section are more grounded in data.