ID: qP0Drg2HuH
Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Read and Reward (R&R) framework, which enhances reinforcement learning (RL) agents' efficiency by utilizing instruction manuals. The framework consists of a QA Extraction module that summarizes relevant information and a Reasoning module that evaluates in-game interactions, providing auxiliary rewards to accelerate learning. The authors demonstrate that R&R significantly improves sample efficiency in several challenging Atari games, including Skiing.

### Strengths and Weaknesses
Strengths:
- **Interesting Approach**: This is the first work demonstrating improved RL performance through reading instruction manuals, which is encouraging given the challenges faced by pure RL methods in games like Skiing.
- **Contribution to NLP + RL**: The exploration of using NLP to enhance RL agents reduces the need for extensive training from scratch and provides useful priors.
- **Minimal Human Labor**: The framework requires minimal human input, only necessitating generic questions about the game's objectives.

Weaknesses:
- **Ad Hoc Setting**: The framework appears to be a patchwork of various modules, and a multi-modality model like GPT-4 could potentially replace these modules, enabling learning from diverse sources such as image/video tutorials.
- **Insufficient Evaluation**: The evaluation is limited to four games; testing on additional challenging games, such as Montezuma Revenge, would be beneficial.
- **Lack of Robust Evaluation**: The absence of statistical analysis and averaging over random seeds raises concerns about the reliability of the results.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including more games, particularly those known to challenge RL algorithms, and providing statistical analyses with error bars in the results. Additionally, exploring the use of vision-language models for reasoning could expand the framework's capabilities. Addressing the limitations of bounding-box detectors and discussing the implications of using noisier textual data for reward shaping would enhance the robustness of the findings. Finally, clarifying how the R&R method can be applied in environments with delayed rewards would strengthen the paper's contributions.