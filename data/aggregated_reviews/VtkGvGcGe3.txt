ID: VtkGvGcGe3
Title: Evaluating Cognitive Maps and Planning in Large Language Models with CogEval
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a test battery designed to evaluate the emergence of cognitive maps and planning abilities in large language models (LLMs) through tasks adapted from cognitive science experiments. The authors find that LLMs generally perform poorly on these tasks, which include goal-oriented planning and the use of shortcuts, with performance influenced by factors such as graph sparsity. The authors propose a framework called CogEval to standardize behavioral evaluations for LLMs, demonstrating that LLMs lack evidence of emergent planning capabilities. Additionally, the study emphasizes the need for clear baselines to interpret results effectively, suggesting that comparisons to known quantities, such as no-exposure conditions or well-understood computational models, would enhance the clarity of their findings. The review highlights that current results are challenging to interpret without such comparisons, as they do not provide a clear understanding of the models' performance.

### Strengths and Weaknesses
Strengths:
- The adaptation of cognitive science experiments into text prompts is innovative and necessary for mapping LLM capabilities.
- The thorough evaluation of multiple prompts and statistical analysis of LLM responses contrasts with anecdotal claims of emergent intelligence.
- The experiments conducted are deemed useful and have led to a positive reevaluation of the paper.
- The authors acknowledge the importance of comparing their results to other models, indicating a willingness to improve clarity.
- The paper is well-structured, with clear motivation and a comprehensive experimental setup.

Weaknesses:
- The presentation suffers from unedited writing, with long sentences and unclear grammar that hinder comprehension.
- Figures and tables lack clarity and context, with some appearing disorganized or poorly formatted.
- The experimental design is inadequately described, making it difficult to assess the robustness of the findings.
- The lack of clear baseline comparisons makes it difficult to interpret the performance of the LLMs.
- The authors' assertion that results are not hard to interpret is contested, as the absence of known benchmarks limits understanding.

### Suggestions for Improvement
We recommend that the authors improve the clarity and readability of the paper by editing for grammar and condensing long sentences. Additionally, the authors should ensure that figures and tables are clearly labeled and placed near their references in the text. It is crucial to provide a detailed description of the experimental design, including the number of trials and conditions evaluated, to allow for reproducibility. We suggest reframing the conclusions to avoid overly strong claims, such as "no evidence for understanding cognitive maps or planning," and instead emphasize the limitations of LLMs in these tasks. Explicit comparisons to chance-level performance and human benchmarks would further contextualize the results and strengthen the paper's claims. Furthermore, we recommend that the authors improve the interpretability of their results by incorporating comparisons to well-understood baselines, such as no-exposure conditions or established computational models. This could involve explicitly comparing their findings to human performance or other models like model-based planners. Additionally, clarifying the comparisons presented in the paper would help readers better understand the performance of the system, particularly in relation to model-based reinforcement learning (MBRL) or successor representation (SR) agents.