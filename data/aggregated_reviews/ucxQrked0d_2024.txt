ID: ucxQrked0d
Title: Making Offline RL Online: Collaborative World Models for Offline Visual Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Collaborative World Models (CoWorld), a novel approach to offline reinforcement learning (RL) with visual inputs. CoWorld conceptualizes offline RL as an online-to-offline transfer learning problem, utilizing an auxiliary online simulator to mitigate overfitting in representation learning and value function overestimation. The method involves training separate source and target world models and actor-critic agents, aligning their latent spaces, and employing a three-stage learning procedure to enhance performance on the target task.

### Strengths and Weaknesses
Strengths:  
- The idea is novel, framing offline RL as an online-to-offline transfer learning problem.  
- The method addresses significant issues, including overfitting in representation learning and the trade-off between value function overestimation and over-conservatism.  
- Comprehensive ablation studies validate the contributions of each component in the CoWorld framework.  
- The proposed setting offers a new perspective on reinforcement learning cross-task transfer, which is challenging yet valuable.  
- The paper is generally well-written, with clear figures and tables.

Weaknesses:  
- CoWorld's heavy reliance on an auxiliary online environment may limit its applicability in real-world scenarios.  
- The rationale for maintaining separate world models is unclear and inadequately supported.  
- Performance improvements over simpler baselines are sometimes marginal and sensitive to hyperparameters.  
- The computational complexity of alternating between online and offline agents may be high and unstable.  
- The requirement for a high-quality simulator for a similar source domain contradicts the motivation for offline RL.  
- The results exhibit significant standard deviations, raising concerns about the reliability of the evaluation.  
- The conditions for mitigating value overestimation and the success of reward alignment lack formal analysis and discussion.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the necessity and function of individual components, such as world model learning and state alignment. Additionally, the authors should provide a more detailed discussion on the practical scenarios where CoWorld is applicable and how it operates when the two world model architectures differ significantly. To strengthen the evaluation, we suggest conducting more experimental runs and providing statistical analysis to substantiate claims of significant performance improvements over baselines. Finally, addressing the unclear rationale for separate world models and the conditions for mitigating value overestimation would enhance the paper's overall rigor.