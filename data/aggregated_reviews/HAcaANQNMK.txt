ID: HAcaANQNMK
Title: ESPACE: Dimensionality Reduction of Activations for Model Compression
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 5, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ESPACE (Eigen Static Principal Activation Component Estimation), a technique for compressing large language models (LLMs) by focusing on the dimensionality reduction of activation tensors rather than traditional weight-centric tensor decomposition. The method projects activation tensors onto a pre-calibrated set of principal components, enabling weight compression during inference through matrix multiplication associativity. The authors claim that ESPACE can achieve up to 50% compression with minimal increase in perplexity and a practical reduction in inference latency.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel approach to LLM compression by emphasizing activation tensor dimensionality reduction, which adds originality to the research.
- A solid theoretical foundation is provided, including optimal constructions for projection matrices to minimize mean squared error and forward-propagated noise metrics.
- The paper is well-organized and accessible, effectively communicating complex ideas.

Weaknesses:
- The method is perceived as a PCA projection of activation tensors, and the authors need to defend its novelty.
- Empirical results for the combination of ESPACE with other compression techniques are lacking, making claims about its orthogonality unconvincing.
- More direct comparisons with state-of-the-art methods, particularly in terms of perplexity across different compression techniques, would strengthen the paper.
- The necessity for a calibration phase may limit scalability, and the impact of the calibration set on performance requires deeper analysis.
- The paper primarily focuses on GPT-3 and Llama models, potentially limiting generalizability to other architectures.

### Suggestions for Improvement
We recommend that the authors improve the empirical comparisons with related baselines, such as pruning and quantization, to demonstrate the significance of ESPACE more convincingly. Additionally, including a robust error analysis and a discussion on the trade-offs in selecting compression settings and projection matrices would enhance the paper's depth. It would also be beneficial to evaluate ESPACE's performance on diverse inference tasks and provide a sensitivity profile versus layers in the Transformer model. Finally, addressing the practical implications of the calibration phase and exploring the method's robustness under various conditions would strengthen the overall contribution of the work.