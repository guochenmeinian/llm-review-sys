ID: 0UfULsElsz
Title: Generalisable Agents for Neural Network Optimisation
Conference: NeurIPS
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 4

Aggregated Review:
### Key Points
This paper presents a novel multi-agent reinforcement learning (MARL) approach, GANNO, for optimizing neural network hyperparameters by dynamically scheduling learning rates at each layer during training. The authors effectively articulate the motivation to reduce human intervention in training processes, highlighting the significance of their method in enhancing computational efficiency and convergence speed. The use of separate datasets for training, testing, and validation is commendable, promoting generalization. While the method shows promising results in standard cases, it does not outperform existing models in more complex environments, particularly with ResNet architectures.

### Strengths and Weaknesses
Strengths:
- The paper addresses the critical issue of hyperparameter tuning in neural networks, proposing an automated and broadly applicable solution.
- The writing is clear, with a logical flow and effective articulation of motivation and purpose.
- The method demonstrates faster convergence and computational efficiency, making it relevant in the context of environmental concerns.

Weaknesses:
- The performance claims are based on a limited experimental setup, making it challenging to assess the significance of the findings.
- Some reasoning lacks cohesion, such as the interpretation of results in Figure 3 and the implications of initial learning rates shown in Table 1.
- The generalization claims regarding the agents' adaptability to different architectures and datasets require clarification, as the definition of generalization appears context-dependent.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the notation for the summation operator to avoid confusion. Additionally, a more detailed discussion of computational resource improvements, convergence times, and hyperparameter search comparisons with alternatives is necessary. The authors should provide further details on the experimental setup, including how the hyperparameters of the RL are selected and the robustness of agents to these choices. Lastly, exploring the scalability of the method to larger and more complex networks would enhance the paper's contributions.