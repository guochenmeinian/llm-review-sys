ID: 6Jbzz0L9z0
Title: Adversarial Vulnerabilities in Large Language Models for Time Series Forecasting
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 5, 7, 7
Original Confidences: 4, 3, 4

Aggregated Review:
### Key Points
This paper presents a gradient-free black-box attack that targets the output of LLM-based forecasting models, revealing their susceptibility to adversarial manipulations. The authors evaluate this vulnerability across multiple models, including TimeGPT, GPT-3.5, GPT-4, and Mistral, using Gaussian White Noise (GWN) and Directional Gradient Approximation (DGA) techniques. The findings underscore the critical need for robust LLMs to enhance reliability in real-world applications, particularly in high-stakes environments like finance and healthcare.

### Strengths and Weaknesses
Strengths:
1. The writing is clear and effectively introduces the analysis of adversarial attacks on LLM-based time series forecasting.
2. The proposed gradient-free black-box attack significantly degrades prediction accuracy, opening new research avenues.
3. The empirical evaluation across multiple datasets and models provides a solid foundation for the findings.

Weaknesses:
1. The paper lacks actionable solutions or defense strategies against the demonstrated vulnerabilities, making the contribution seem incomplete.
2. Certain technical aspects, particularly the mathematical formulation of DGA, could be clearer for readers unfamiliar with gradient approximation techniques.

### Suggestions for Improvement
We recommend that the authors improve the discussion on potential defense strategies against the attacks presented, which would enhance the paper's practical relevance. Additionally, clarifying the explanation of the DGA method and providing further details on the hyperparameter study would improve comprehension. Finally, a more extensive comparison with lighter models, as suggested in the conclusion, would provide a broader view of model vulnerabilities.