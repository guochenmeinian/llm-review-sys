ID: ZmuLcqwzkl
Title: DAREL: Data Reduction with Losses for Training Acceleration of Real and Hypercomplex Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: 4, 2, 4, 2

Aggregated Review:
### Key Points
This paper presents a method aimed at reducing computational time and memory usage during the training of deep neural network models. The authors propose a two-stage approach that selects diverse and challenging samples prior to training and employs an importance sampling method during training, introducing a concept of training budget. The experiments conducted include tasks on CIFAR-100 and natural language generation datasets, with a detailed report on results.

### Strengths and Weaknesses
Strengths:
- The proposed method significantly contributes to the field of data selection for training acceleration, incorporating budget considerations and improvements in both pre-training and training phases.
- A comprehensive related work section outlines key research directions in training acceleration.
- The methodology is clearly detailed, and the experiments are reproducible to some extent.

Weaknesses:
- The explanation of the proposed method lacks clarity, with critical components not sufficiently detailed, leading to confusion about its implementation and purpose.
- Results lack significance testing, making it difficult to assess the quality improvements over baseline methods.
- Presentation issues include improper citation formatting, overly long paragraphs, and numerous typos and grammatical errors.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the proposed method by providing detailed explanations of key components, such as the differences between online and offline methods, the use of the LinearModel, and the specifics of Algorithms 1 and 2. Additionally, we suggest including significance testing for the results to substantiate claims of quality improvements. The authors should also address formatting issues, such as using parentheses for citations and breaking up long paragraphs for better readability. Lastly, a thorough proofreading is necessary to correct typos and grammatical errors.