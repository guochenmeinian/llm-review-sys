ID: 0Rdp7a3y2H
Title: Adversarial Text Generation by Search and Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a search and learning framework for Adversarial Text Generation by Search and Learning (ATGSL), introducing three novel black-box adversarial attack methods: ATGSL-SA, ATGSL-BM, and ATGSL-FUSION. The authors propose treating adversarial example generation as an unsupervised text generation problem, demonstrating that ATGSL outperforms several existing black-box attack baselines across various classification datasets. However, inconsistencies in methodology and implementation raise concerns about the validity of comparisons and results.

### Strengths and Weaknesses
Strengths:
- The proposed ATGSL framework is novel and effective, as evidenced by experimental results.
- The paper includes a thorough literature review, highlighting the uniqueness of the approach.
- The evaluation is comprehensive, covering multiple datasets and baselines.

Weaknesses:
- There are multiple inconsistencies regarding the methodology, particularly concerning the use of hard versus soft constraints.
- The paper does not adequately address the potential for applying ATGSL at more granular levels, such as character-level or sentence-level attacks.
- The human evaluation section lacks comparisons to previous baselines and does not assess label-preserving capabilities.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the constraints used in their methodology, ensuring consistency throughout the paper. Additionally, we suggest that the authors explore the applicability of ATGSL at more granular levels, such as character-level and sentence-level attacks. To enhance the human evaluation section, we recommend including comparisons with previous baselines and assessing the label-preserving ability of ATGSL. Furthermore, we advise the authors to clarify the calculation of grammar scores based on GPT-2 outputs and provide more details on the selection of hyper-parameter values (alpha/beta) for ATGSL-SA. Lastly, addressing typographical errors and ensuring accurate terminology throughout the paper would enhance overall presentation.