ID: YllS5zEzVq
Title: Probing LLMs for Joint Encoding of Linguistic Categories
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the joint encoding of linguistic categories in Large Language Models (LLMs), particularly focusing on Part-of-Speech (POS) tagging and dependency labeling. The authors propose a framework for testing this joint encoding, revealing evidence of such encoding at both the same and different levels across languages in multilingual models. The findings suggest that LLMs can learn diverse linguistic properties simultaneously, contributing to the understanding of their capabilities.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents an interesting approach to understanding joint encoding in LLMs.
- It provides empirical evidence and insightful analysis regarding the encoding of linguistic properties, enhancing the reader's understanding of the implications.

Weaknesses:
- The contribution appears limited, and the research could have been condensed into a shorter paper.
- The authors only tested specific layers and did not explore how joint encoding varies across all layers.
- The use of a two-layer MLP as a probing model raises questions about its validity, and the paper lacks a discussion on the implications of its findings for NLP applications.

### Suggestions for Improvement
We recommend that the authors improve the presentation of their research by including more experiments, such as testing different model sizes and configurations across all layers to assess joint encoding variability. Additionally, addressing the validity of the probing model choice and discussing the implications of their findings for NLP systems would enhance the paper's relevance. Finally, providing a more thorough comparison with alternative attribute-removal methods and including missing references would strengthen the analysis.