ID: uWGH6jDTVv
Title: Complementary Benefits of Contrastive Learning and Self-Training Under Distribution Shift
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 6, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach that combines self-training and contrastive learning for unsupervised domain adaptation (UDA), proposing Self-Training Over Contrastive learning (STOC). The authors demonstrate that STOC significantly improves target domain accuracy compared to using either method alone. Theoretical analysis indicates that while contrastive learning enhances representations by focusing on domain-invariant features, self-training effectively mitigates reliance on spurious correlations. Empirical results across multiple benchmarks support these findings.

### Strengths and Weaknesses
Strengths:
- The paper provides robust theoretical and empirical evidence supporting the complementary benefits of self-training and contrastive learning.
- It explores an under-researched area, yielding significant insights into the synergy between these methods.
- The extensive empirical study across eight benchmarks demonstrates the effectiveness of the proposed approach.

Weaknesses:
- The description of the proposed method STOC is vague and lacks formal detail.
- Contributions are not clearly articulated, particularly distinguishing STOC from other methods.
- The benchmarks used are not competitive, missing popular datasets like Office-31 and Office-Home, and the paper does not adequately position itself against concurrent work.
- The assumptions regarding infinite unlabeled data and the choice of models are restrictive, and the paper contains minor typographical errors.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the STOC method by providing a more formal and detailed description. Additionally, please clearly state the contributions of the paper, particularly in distinguishing STOC from other methods. We suggest including evaluations on popular UDA benchmarks such as Office-31 and Office-Home, as well as positioning the work against concurrent studies. Furthermore, consider addressing the restrictive assumptions made in the theoretical framework and expanding the empirical analysis to include a broader range of contrastive learning and self-training methods.