ID: ASqdVeifn7
Title: 4-bit Shampoo for Memory-Efficient Network Training
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to reduce memory usage in second-order optimizers by implementing 4-bit quantization on the eigenvector matrix of the preconditioner instead of the preconditioner itself. The authors demonstrate that this approach maintains performance while significantly decreasing memory requirements, achieving up to 40% memory savings with a minor runtime overhead. The theoretical justification for this method is provided, alongside empirical results across various neural network architectures and datasets.

### Strengths and Weaknesses
Strengths:
- The paper introduces an innovative memory reduction technique for second-order optimizers, thoroughly analyzing its effectiveness.
- The theoretical justification supports the superiority of quantizing the eigenvector matrix over naive quantization methods.
- Extensive experiments validate the method's performance across multiple datasets and models.

Weaknesses:
- The evaluation is limited to image classification tasks, lacking experiments on large networks and other domains like natural language processing.
- There is insufficient analysis of the computational overhead introduced by the quantization process.
- The paper does not adequately address the impact of quantization errors on training dynamics or provide a comprehensive theoretical framework for convergence guarantees.

### Suggestions for Improvement
We recommend that the authors improve the evaluation scope by including experiments on large language models (LLMs) and other tasks beyond image classification. Additionally, a detailed analysis of the computational overhead associated with quantization and dequantization processes should be included. We suggest that the authors provide a breakdown of memory usage to clarify how the 4-bit Shampoo achieves memory efficiency. Furthermore, including comparisons with more state-of-the-art optimizers like Adagrad, M-FAC, and EVA would strengthen the paper's claims. Lastly, we encourage the authors to explore the use of schedule-free optimization methods and consider the implications of quantization errors on long training periods.