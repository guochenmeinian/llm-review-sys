ID: 4M9f8VMt2C
Title: Long-form factuality in large language models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 8, 7, 6, 8, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LongFact, a dataset of 2,280 questions across 38 topics aimed at evaluating the factuality of long-form answers generated by large language models (LLMs). The authors propose the Search-Augmented Factuality Evaluator (SAFE), which employs LLMs and Google search to verify the accuracy of individual facts in long responses. Additionally, the authors introduce the F1@K metric to measure the precision and recall of factual accuracy in model responses. The study finds that larger models generally produce more factual responses.

### Strengths and Weaknesses
Strengths:
1. The substantial workload includes dataset construction, evaluation methods, and assessment metrics.
2. LongFact provides broad domain coverage, enhancing evaluation depth.
3. SAFE reduces the cost and time of manual evaluations, demonstrating satisfactory factual error detection ability.

Weaknesses:
1. The paper's layout could be improved, as significant information is relegated to the appendix, potentially causing key details to be overlooked.
2. The frequent citation of the FactScore method lacks specificity regarding which aspects were utilized.
3. Inconsistencies in Table 16 regarding repeated queries contradict claims made in the "Rating Individual Facts" section.
4. The claim that SAFE corrected 76% of discrepancies raises questions about the reliability of annotations from Min et al. (2023).

### Suggestions for Improvement
We recommend that the authors improve the paper's layout by integrating significant contributions, such as the SAEE method, into the main text for better visibility. Additionally, the authors should clarify which aspects of the FactScore method were adopted and summarize these elements in the main body. Addressing the inconsistencies in Table 16 is crucial for maintaining credibility. Finally, the authors should consider discussing the implications of SAFE's performance relative to the annotations from Min et al. (2023) to clarify the significance of their findings.