ID: RJpAz15D0S
Title: Cheaply Estimating Inference Efficiency Metrics for Autoregressive Transformer Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 2, 8, 6, 7, 4, 2, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 3, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a metric called `idealized runtime` to evaluate the inference efficiency of various large language models (LLMs) as if they were executed on a standardized hardware/software platform. The authors propose that the `idealized runtime` can be derived from known architectures of LLMs, modeled as a function of output length `o` and prompt length `p`, under specific conditions. They argue that their method provides a more accurate estimation of runtime compared to traditional approaches, which often rely on assumptions about hardware performance. Additionally, the authors introduce an alternative `denoised runtime` for closed models accessed via APIs and acknowledge the increasing context lengths in current models, suggesting that their methodology can be adapted to account for this complexity. The goal is to provide a fair comparison of LLMs across different implementations, highlighting the tradeoffs between inference efficiency and capability while discussing the limitations of closed models regarding transparency in hyperparameters and architecture details.

### Strengths and Weaknesses
Strengths:
- The authors emphasize that scaling laws for LLMs should consider accuracy against inference latency and cost, which is a valuable perspective.
- The proposed method for estimating `idealized runtime` is straightforward and applicable to standard hardware/software systems.
- The metric "Idealized Runtime" offers a novel approach to estimating inference latency.
- The authors demonstrate a willingness to adapt their methodology to accommodate larger context lengths.
- The paper engages with current trends in model offerings and acknowledges the limitations of closed models.

Weaknesses:
- The paper relies on assumptions that limit its applicability, such as the context length being much smaller than the embedding size, which may not hold for newer LLMs with larger context windows.
- The significance of approximating latencies through fitting rather than analytical calculations is questionable, as the authors provide insufficient evidence to support this approach.
- The analysis may be misleading when considering batch size, particularly in the case of batch size = 1.
- The significance of the "Idealized Runtime" metric in practical applications is questioned, as it may not provide new insights for ML engineers.
- The findings in Figure 3 do not convincingly support the claim of consistency with raw runtime measurements.
- The presentation lacks clarity and contains redundancies, with some conclusions drawn without adequate evidence.
- The paper does not adequately address the variance in runtime across different tasks and models, nor does it explore the implications of batch size on runtime comparisons.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation of their assumptions, particularly regarding the context length and embedding size, by incorporating recent developments in LLMs. Additionally, we suggest providing a quantitative analysis comparing the `idealized runtime` with simple FLOP-based latency estimates to justify the necessity of their fitting approach. Clarifying the impact of batch size on runtime and detailing the hardware/software specifications used in their experiments would enhance the paper's rigor. We also recommend improving the clarity of the analysis regarding the implications of batch size on computation and providing a more detailed justification for the significance of the "Idealized Runtime" metric in practical applications. Enhancing the discussion in Section 5 to clarify the insights gained from their findings and considering incorporating a quadratic regression model to better account for larger context lengths would be beneficial. Lastly, addressing the transparency issues surrounding hyperparameters in closed models could strengthen the paper's contributions.