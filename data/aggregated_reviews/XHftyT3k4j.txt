ID: XHftyT3k4j
Title: Enhancing Argument Structure Extraction with Efficient Leverage of Contextual Information
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an Efficient Context-aware ASE model (ECASE) aimed at enhancing data- and compute efficiency in argument structure extraction. The authors propose several techniques, including a sequence-attention module, a distance-weighted similarity loss, and discourse marker masking. Experimental results across five datasets demonstrate that ECASE achieves state-of-the-art performance compared to context-aware baselines.

### Strengths and Weaknesses
Strengths:
- The core idea is well-motivated, aiming to improve context-aware ASE by efficiently learning from informative distant contexts.
- The method is clearly designed and explained, allowing for future modifications.
- The introduction of a weighted regularization term effectively leverages similarities in argumentative relations.
- Thorough experiments provide sufficient comparisons with benchmark models, confirming the effectiveness of ECASE.

Weaknesses:
- The model design is perceived as simple and outdated, particularly the sequence-attention and discourse marker techniques.
- The ablation studies lack clarity, with marginal improvements noted in some comparisons.
- There are inconsistencies with results from related works, necessitating further explanation.
- Some errors, such as the mislabeling of "CACE" in Figure 4, detract from the presentation quality.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the ablation study results by plotting more horizontal lines or displaying numerical values. Additionally, the authors should address the marginal relevance of the techniques used and better connect the concatenation of [CLS] tokens and data augmentation in the narrative. It is crucial to provide explanations for the discrepancies in performance compared to previous works, particularly regarding the AbstRCT dataset. Finally, we suggest correcting the labeling errors in figures and ensuring that all terms are clearly defined upon first use.