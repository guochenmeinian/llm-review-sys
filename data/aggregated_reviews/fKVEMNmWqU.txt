ID: fKVEMNmWqU
Title: Reduced Policy Optimization for Continuous Control with Hard Constraints
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 6, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Reduced Policy Optimization (RPO) algorithm designed to address hard equality and inequality constraints in reinforcement learning (RL). The framework integrates a two-phase approach: the first phase trains a policy network for equality constraints, while the second phase employs the Generalized Reduced Gradient (GRG) method to manage inequality constraints. The authors validate their approach through experiments on three benchmarks, demonstrating its effectiveness compared to existing CMDP-based methods. Additionally, the authors formulate instantaneous constraints \(f(s_t, a_t)=0\) and \(g(s_t, a_t)\leq 0\) without using Expectation \(E[]\) or Probability \(Pr()\) operators, asserting that both \(s_t\) and \(a_t\) are deterministic at the current time step \(t\). They clarify that the randomness associated with \(s_t\) and \(a_t\) arises from the transition probability function, which is conditioned on previous states and actions, differentiating their approach from standard constrained MDPs (CMDP).

### Strengths and Weaknesses
Strengths:
- The paper tackles the significant issue of safe RL with hard constraints, presenting a well-motivated and organized approach.
- The experimental results indicate that RPO outperforms CMDP baselines, showcasing its novelty as the first application of GRG in RL for hard constraints.
- The manuscript is well-written, with thorough background and related work.
- The authors provide clear clarifications that address concerns regarding the deterministic nature of their constraints.
- The algorithm demonstrates low overhead due to the solver, making it a viable option for hard constraints.
- The performance in the OPF with battery energy storage task is noted as a positive aspect.

Weaknesses:
- The paper overlooks recent literature on RL with hard safety constraints, such as Wang et al. (2022), which could provide valuable context.
- The two-stage approach resembles shielding methods, yet the authors do not adequately compare their work with existing literature on RL + shielding.
- The formulation of the MDP assumes a general stochastic environment, raising concerns about the treatment of random variables in the context of hard constraints.
- The experimental evaluation relies on relatively simple problems, which may not fully demonstrate the algorithm's capabilities.
- There is a need for clearer delineation of random variables and their realizations in the paper.
- The manuscript could benefit from a discussion on various solvers and optimization tools used.

### Suggestions for Improvement
We recommend that the authors improve the literature review by incorporating recent works addressing RL with hard safety constraints, particularly Wang et al. (2022). Additionally, the authors should clarify how their approach compares to RL + shielding methods, such as Bastani et al. (2021). We suggest revisiting the MDP formulation to ensure it accurately reflects the treatment of random variables, potentially incorporating probability formats. Furthermore, we encourage the authors to conduct evaluations on more complex benchmark problems to better illustrate the advantages of RPO. We also recommend that the authors improve clarity by explicitly defining the random variables and their realizations in the revised manuscript. Including a discussion about possible solvers and optimization tools would enhance the paper's comprehensiveness. Lastly, clearer illustrations of the problem formulation should be presented to further address reviewer concerns.