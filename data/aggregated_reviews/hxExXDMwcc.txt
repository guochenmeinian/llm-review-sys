ID: hxExXDMwcc
Title: PlugMed: Improving Specificity in Patient-Centered Medical Dialogue Generation using In-Context Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a patient-centered medical dialogue model called PlugMed, which utilizes prompt learning from large language models (LLMs) through a prompt generation (PG) module and a response ranking (RR) module to enhance dialogue strategies and improve response specificity. The authors propose two evaluation metrics: intent accuracy and high-frequency medical term accuracy, which assess the reasonableness of dialogue actions and the inclusion of essential medical information, respectively. Experimental results indicate that PlugMed generates responses with greater inclusion of comprehensive medical terms and achieves more accurate dialogue intents compared to other LLM-based methods.

### Strengths and Weaknesses
Strengths:
- The model effectively incorporates both global and local contexts for generating medically appropriate responses.
- The proposed evaluation metrics are relevant and contribute to assessing the quality of medical dialogue generation.
- The experimental validation on multiple datasets enhances the credibility of the proposed method.

Weaknesses:
- The paper lacks technical novelty, primarily combining established components without addressing relevant technical challenges.
- There is insufficient detail on the internal structure of the prompt generator and the models used.
- Standard evaluation metrics for dialogue generation, such as ROUGE, METEOR, and PPL, are not reported, limiting the assessment of response quality.
- The paper does not adequately discuss the overhead computation introduced by the plug-and-play model and its implications for generalizability.

### Suggestions for Improvement
We recommend that the authors improve the technical novelty by addressing relevant challenges and providing a more detailed description of the internal structures of the prompt generation and response ranking modules. Additionally, including standard evaluation metrics like ROUGE and BLEU would strengthen the empirical findings. We also suggest discussing the overhead computation and potential biases in the retrieval process, as well as exploring the model's performance across multiple backbone LLMs to demonstrate generalizability. Lastly, an ethical discussion regarding the use of medical terminology and data is essential and should be included.