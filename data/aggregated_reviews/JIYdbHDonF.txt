ID: JIYdbHDonF
Title: Simple, Scalable and Effective Clustering via One-Dimensional Projections
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 7, 6, 6, 4, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new clustering algorithm that improves the runtime of the k-means++ algorithm, achieving a time complexity of \(O(nd + n \log n)\) independent of the number of clusters \(k\). The authors propose projecting data points onto a one-dimensional space using a random Gaussian vector, which allows for efficient k-means++ seeding. The theoretical analysis indicates an approximation ratio of \(\widetilde{O}(k^4)\), which is worse than the \(O(\log k)\) guarantee of k-means++. Experimental results demonstrate the algorithm's efficiency for coreset construction and k-means++ implementation.

### Strengths and Weaknesses
Strengths:
1. The proposed method is simple and intuitive, making it easy to understand and implement.
2. The one-dimensional projection approach is novel and provides a nontrivial approximation guarantee.
3. The paper includes rigorous proofs for running time and approximation ratio.
4. Experimental results validate the method's efficiency in practical applications.

Weaknesses:
1. The approximation ratio of \(\widetilde{O}(k^4)\) is significantly worse than \(O(\log k)\), and the experimental results do not clearly demonstrate a strong advantage over existing methods like the Lightweight coreset approach.
2. The clarity of figures, particularly Figure 2 and Figure 3, is compromised due to indistinguishable colors and lack of formal problem definition.
3. The organization and presentation of the paper could be improved, particularly in naming different algorithm variants and clarifying experimental results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of figures by using more distinguishable colors and providing formal definitions of the problems and metrics used. Additionally, we suggest explicitly naming the different algorithm variants in the text and figures to enhance understanding. The authors should also consider discussing related fast clustering algorithms beyond k-means to contextualize their work better. Lastly, addressing the limitations of the approximation ratio and providing clearer experimental comparisons with existing methods would strengthen the paper's contributions.