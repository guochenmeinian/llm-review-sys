ID: ldtjC7TSJ5
Title: Non-Autoregressive Sentence Ordering
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel non-autoregressive approach for the sentence ordering task, utilizing an order-invariant encoder for sentence representations and a non-autoregressive decoder to predict sentence order using pointer networks. The authors introduce an exclusive loss and a greedy selective and removing strategy to mitigate repetition issues. The experiments show that this method outperforms previous generation and most ranking-based methods while being slightly faster than autoregressive methods.

### Strengths and Weaknesses
Strengths:
- The proposed non-autoregressive model surprisingly outperforms autoregressive models in sentence ordering tasks, contributing valuable insights to non-autoregressive model research.
- The experiments are robust, particularly the comparison against chatGPT, and the paper is well-written and easy to follow.
- The approach effectively exploits bilateral dependencies and includes detailed analyses, such as quantifying repetition reduction.

Weaknesses:
- The non-autoregressive decoding approach lacks advanced techniques like iterative refinement, which may limit its innovation.
- The ChatGPT experiments appear preliminary, and the paper does not sufficiently address the current state-of-the-art, particularly Re-BART.
- Some methodological details are underspecified, and the paper does not fully report competing baselines, which affects completeness.

### Suggestions for Improvement
We recommend that the authors improve the introduction by providing more background on non-autoregressive decoding to enhance reader comprehension. Additionally, we suggest incorporating advanced NAT techniques like iterative refinement to strengthen the model's capabilities. The authors should also analyze the efficiency and repetition impact of integrating NAON with BART beyond just accuracy results. Finally, we encourage the authors to include a more comprehensive comparison with existing models, particularly those mentioned in the Re-BART paper, to ensure completeness in reporting results.