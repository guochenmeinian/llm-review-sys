ID: XrKhwfPmyI
Title: EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using Discharge Summaries
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 8, 7, 9, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the EHRNoteQA, a clinician-validated question-answering benchmark designed for evaluating LLMs on clinical discharge summaries. It features two tasks: open-ended and multiple-choice question answering, covering eight distinct topics and requiring integration of information from multiple discharge summaries. The authors propose a robust evaluation method and assess 27 different LLMs, highlighting various challenges in the clinical context.

### Strengths and Weaknesses
Strengths:
- The benchmark addresses the need for assessing LLMs' applicability in clinical settings, particularly their long-context understanding.
- It encompasses a diverse range of topics, modeling data closer to real-world scenarios.
- The comprehensive evaluation of 27 LLMs is well-executed, and the paper is clearly written.

Weaknesses:
- The dataset is limited by generating only one question per patient per topic, which could restrict its size and diversity.
- The absence of additional task instructions during model evaluation may overlook insights into models' reasoning capabilities.
- The evaluation metrics, particularly for open-ended questions, lack objective measures to support results.
- The dataset is skewed towards "Treatment" style questions, which may limit its applicability.

### Suggestions for Improvement
We recommend that the authors improve the dataset by generating one question per topic for each patient, resulting in a more comprehensive dataset. Additionally, we suggest incorporating Chain-of-Thought (CoT) methodologies in model evaluations to enhance understanding of model reasoning. The authors should also present objective metrics, such as exact-match or similarity-based metrics, to support the evaluation of open-ended questions. Furthermore, clarifying the rationale for using GPT-4 in the multiple-choice question answering setup would be beneficial. Lastly, providing clear documentation and hosting the dataset on standard platforms like HuggingFace Hub would facilitate easier access and usage.