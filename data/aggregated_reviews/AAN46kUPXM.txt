ID: AAN46kUPXM
Title: Neural expressiveness for beyond importance model compression
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "Expressiveness," a metric for evaluating the dissimilarity of feature maps from different filters, and introduces NEXP, a technique for pruning filters based on this metric. The authors apply NEXP to tasks such as image classification and object detection, demonstrating its effectiveness in various scenarios. However, the performance improvements over existing methods are not consistently significant.

### Strengths and Weaknesses
Strengths:
1. The paper is well-structured and easy to follow.
2. The application of expressiveness in structured pruning appears novel, contributing to the field.
3. Comprehensive experiments are conducted, including various pruning scenarios and tasks.

Weaknesses:
1. The notation used in the paper is confusing, particularly regarding layer and filter indices.
2. The concept of expressiveness is not new in the context of pruning and neural architecture search.
3. Performance improvements from NEXP are not prominent, often showing higher compression ratios but lower accuracy compared to other methods.
4. The method's practicality is limited to CNNs, with no evidence of effectiveness on more recent architectures like transformers.

### Suggestions for Improvement
We recommend that the authors improve the clarity of notation to avoid confusion. Additionally, the authors should elaborate on the significance of expressiveness in pruning and provide more robust comparisons with state-of-the-art methods. To enhance the paper's applicability, we suggest including evaluations of NEXP on transformer architectures and discussing its computational scalability. Finally, a more detailed conclusion that characterizes the main contributions and implications of the findings would improve readability.