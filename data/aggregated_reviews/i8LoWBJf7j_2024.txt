ID: i8LoWBJf7j
Title: Interpretable Lightweight Transformer via Unrolling of Learned Graph Smoothness Priors
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 2, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a transformer-like neural network by unrolling iterative optimization algorithms that minimize graph smoothness for imaging tasks. The authors propose using graph Laplacian regularizer (GLR) and graph total variation (GTV) as smoothness priors, demonstrating superior performance in signal restoration with fewer parameters compared to conventional methods. The experimental results indicate robustness to covariate shifts.

### Strengths and Weaknesses
Strengths:  
1. The method is well-illustrated, with convincing theoretical details.  
2. The mathematical formulation for both GLR and GTV is clearly detailed.  
3. The proposed methods achieve good restoration performance with significantly fewer parameters than traditional transformer-based methods.  

Weaknesses:  
1. More discussions on the proposed method are needed, particularly regarding interpretability and the mechanisms supporting this claim.  
2. The connection between the proposed graph learning module and the self-attention mechanism in transformers lacks clarity.  
3. A runtime analysis compared to existing methods would strengthen the evaluation.  
4. The paper does not discuss limitations or provide a convergence guarantee for the proposed model.  
5. The experimental results only include "iGTV," with no mention of "iGLR."

### Suggestions for Improvement
We recommend that the authors improve the discussion on related works in GNNs, particularly addressing the interpretability of the proposed network with additional details or visualizations. Clarifying the purpose of the comparison between the graph learning module and self-attention mechanisms would enhance understanding. Including a runtime analysis and commonly used metrics like cPSNR and SSIM would strengthen the evaluation. Additionally, discussing the advantages and disadvantages of GLR and GTV, along with guidance on their selection, would be beneficial. Lastly, we suggest providing a convergence guarantee for the proposed model and including experimental results for "iGLR."