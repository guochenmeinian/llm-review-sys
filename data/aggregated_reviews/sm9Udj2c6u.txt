ID: sm9Udj2c6u
Title: Feasibility of Automatically Detecting Practice of Race-Based Medicine by Large Language Models
Conference: AAAI
Year: 2024
Number of Reviews: 4
Original Ratings: 6, 6, 7, 5
Original Confidences: 3, 4, 4, 4

Aggregated Review:
### Key Points
This paper presents a study on the use of Large Language Models (LLMs) for identifying and evaluating race-based content in medical contexts. The authors propose a structured comparison of various LLMs, addressing the critical issue of racial stereotypes in medical advice. The findings contribute to understanding how LLMs handle sensitive topics, which is increasingly relevant in healthcare.

### Strengths and Weaknesses
Strengths:
1. The study addresses a significant gap in evaluating LLMs concerning sensitive topics like racial stereotypes in medical advice.
2. The methodological approach, including a structured comparison across different LLMs, provides a solid foundation for the findings.

Weaknesses:
1. The paper inaccurately states "nine unique LLM-prompt combinations" instead of "twelve unique combinations," which affects accuracy.
2. The limited skin tone classification used does not adequately represent the diversity of race; a more comprehensive classification, such as the Monk Skin Tone Scale, is recommended.
3. The claim of a lack of methods to evaluate harmful race-related content is misleading, as major AI players have released relevant benchmarks and scorecards.
4. The background of the physicians involved in the study is not detailed, which is essential for understanding potential biases.
5. The statistical measures used (Sensitivity, Specificity, NPV, PPV, F1) require clearer explanations for accessibility.
6. The methodology could benefit from exploring few-shot prompting or fine-tuning, given the limitations of zero-shot prompting in niche domains.

### Suggestions for Improvement
We recommend that the authors improve the accuracy of the paper by correcting the number of LLM-prompt combinations. Additionally, consider incorporating a more diverse set of prompts that reflect a broader range of racial stereotypes. The authors should also explore existing benchmarks and datasets to contextualize their findings better. Providing details on the physicians' backgrounds involved in the research is crucial for understanding biases. A clearer explanation of the statistical measures would enhance accessibility for a wider audience. Finally, we suggest exploring few-shot prompting or fine-tuning the models to improve accuracy in race-related medical data.