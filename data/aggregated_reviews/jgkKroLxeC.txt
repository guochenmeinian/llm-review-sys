ID: jgkKroLxeC
Title: Unified Graph Augmentations for Generalized Contrastive Learning on Graphs
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 8, 7, 7, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the generality of graph augmentations across various graph types and tasks, presenting a new interpretation that unifies graph augmentations as local attribute modifications from a message-passing perspective. The authors propose a novel graph augmentation module that alters node attributes to simulate augmentation effects, leading to a framework for graph contrastive learning that balances consistency and diversity across augmented views. The theoretical understanding of the framework's generality is provided, and extensive evaluations demonstrate its superior performance on diverse datasets and tasks.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized and well-written.
- The motivation for unifying graph augmentations from a message-passing view is interesting.
- The proposed framework is simple, efficient, and backed by solid theoretical guarantees.
- Experiments across various graph types and tasks show the proposed method's superior efficacy and efficiency compared to existing works.

Weaknesses:
- The paper contains several typos and grammatical errors that need correction.
- The theoretical analysis lacks intuitive explanations; the authors should clarify how the theorem relates to superior performance.
- The concepts of consistency and diversity require more comprehensive explanations, particularly regarding their definitions in self-supervised versus semi-supervised learning scenarios.
- The relationship between the number of AC vectors and model performance is unclear.
- Some equations use the symbol $\gets$, which should be changed to $\to$ for convenience.
- Certain experiments need further explanation, particularly regarding the framework's robustness to topology and attribute noises.
- Some hyper-parameters remain unverified, and the impact of the parameter $\varepsilon$ in Eq. (7) on performance is uncertain.
- The HSIC term's computational intensity may affect efficiency.
- Descriptions from Lines 143 to 153 are difficult to understand.
- Reference [33] incorrectly cites adversarial training instead of adversarial attack.
- Missing commas after equations, such as in Eq. (9), detract from clarity.

### Suggestions for Improvement
We recommend that the authors improve the paper by correcting typos and grammatical errors. Additionally, the authors should provide more intuitive explanations for the theoretical analysis and clarify the relationship between the theorem and performance. A more comprehensive discussion on consistency and diversity, particularly in different learning scenarios, is necessary. The authors should also elucidate the connection between AC vectors and model performance. Furthermore, we suggest changing the symbol $\gets$ to $\to$ in equations for clarity, providing further explanations for experiments, verifying hyper-parameters, and addressing the computational efficiency of the HSIC term. Lastly, the authors should enhance the clarity of descriptions in the specified lines and ensure proper citation in reference [33].