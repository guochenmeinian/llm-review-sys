ID: o4coDIby7e
Title: Measuring Goal-Directedness
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 8, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a measure of evidence for goal-directedness based on the maximum causal entropy principle, termed MEG. It operationalizes this measure as the ability to predict variable D, representing an agent's decisions, based on the hypothesis that it is optimizing a utility function U, which signifies the goal. The authors propose algorithms for estimating MEG and demonstrate its application through small-scale experiments. The framework is grounded in Dennett's intentional stance and Jaynes' maximum entropy inference, resulting in a formulation similar to Ziebart's Maximum Causal Entropy IRL framework. Additionally, the authors propose a method for quantitatively measuring goal-directedness in observed behaviors, addressing both mathematical and philosophical aspects. This method is based on a causal model with explicit utility, state, and decision variables. For a known utility function, goal-directedness is measured by finding a policy that maximizes the utility function while being regularized by entropy, with comparisons made to uniform prediction. The approach is extended to unknown utilities through parameterized utility function sets, with practical demonstration on CliffWorld. MEG is shown to possess desirable properties, including invariance to scaling and translation of utility functions.

### Strengths and Weaknesses
Strengths:
- The work is well-motivated and addresses a significant problem in AI safety, particularly in quantifying goal-directedness.
- The significance of measuring goal-directedness is emphasized as a core requirement for agency.
- The proposed solution is novel, extending Maximum Causal Entropy IRL in a new application.
- The writing is clear, and the examples enhance understanding.
- MEG exhibits useful properties, such as being applicable even with limited trajectory data and extending to unknown utility functions.

Weaknesses:
- The utility of MEG for assessing generalist AI agents is questionable due to its computational expense and reliance on causal models.
- The framing of MEG as a measure of goal-directedness is misleading; it is more accurately described as a measure of evidence for goal-directedness.
- The scalability of MEG is uncertain, particularly in complex environments where the internal motivations of agents are not well understood.
- Insufficient exploration of the relationship between MEG and existing frameworks, particularly inverse reinforcement learning (IRL).
- Societal implications, particularly regarding potential misuse by bad actors, could be discussed in greater detail.
- A more comprehensive empirical demonstration of practicality would enhance the paper.

### Suggestions for Improvement
We recommend that the authors improve the clarity of MEG's definition to emphasize that it measures evidence for goal-directedness rather than goal-directedness itself. Additionally, addressing the computational demands of MEG in large-scale settings is crucial, as is providing a more detailed discussion of its limitations, particularly regarding the complexity of utility functions in the unknown-utility case. We suggest that the authors clarify the relationship between MEG and IRL frameworks, potentially drawing parallels to Bayesian approaches. Furthermore, we recommend improving the discussion of societal implications by including a detailed analysis of how their findings could enable dangerous systems. A more extensive empirical demonstration of the method's practicality should also be included. We encourage the authors to clarify Proposition 3.2 more explicitly in the statement and ensure that the terminology regarding goal-directedness is consistent throughout the paper, specifically reserving "towards" for cases where the policy is positively directed with respect to the utility function. Finally, emphasizing the point about black box access to environments or simulators would strengthen the paper's contributions.