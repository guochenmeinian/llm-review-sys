ID: e9D2STGwLJ
Title: Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: 4, 5, 3, 4

Aggregated Review:
### Key Points
This paper presents a model-adaptive key-value (KV) cache compression method for autoregressive Transformer models, focusing on reducing memory footprint during inference. The authors propose an original strategy that identifies five common attention structures to save the K and V of specific tokens rather than the entire sequence. Their approach is based on two assumptions: different layers and heads require distinct attention structures, and the identified structure during prompt encoding is maintained throughout token generation. The evaluation shows that their method surpasses previous non-adaptive methods and matches the performance of full KV caching.

### Strengths and Weaknesses
Strengths:
- The method is based on empirical evidence and improves upon existing KV cache compression techniques in both memory efficiency and quality.
- It introduces an adaptive KV caching scheme that dynamically selects compression strategies based on input prompts.
- The paper is well-structured, with clear communication of methods, experiments, and results.

Weaknesses:
- The justification for the assumptions made in the analysis lacks detail, particularly regarding the identification of attention structures and the limited dataset comparisons.
- The analysis of the consistency of profiles across layers is insufficient, with only a few layers and heads examined.
- There are inconsistencies in reported memory reduction figures and a lack of clarity regarding sequence length in experiments.
- Implementation details and latency effects are not adequately addressed, and the ablation study appears arbitrary and limited to one benchmark.

### Suggestions for Improvement
We recommend that the authors improve the justification for their assumptions by providing more detail on how attention structures are identified and including comparisons with additional datasets beyond GSM8K. Additionally, we suggest expanding the analysis of layer and head profiles to include a summarized view of all layers and heads, possibly using metrics like "average derivative" for visualization. It would also be beneficial to clarify the sequence length used in experiments and ensure consistent reporting of memory reduction figures. Furthermore, we encourage the authors to include implementation details, particularly regarding any special CUDA kernels required, and to discuss potential effects on throughput. Lastly, we recommend including comparisons with other KV cache compression methods such as quantization and distillation for a more comprehensive evaluation.