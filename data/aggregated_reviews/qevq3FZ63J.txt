ID: qevq3FZ63J
Title: MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 4, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MAGIS, a multi-agent framework designed to resolve GitHub issues in software development. It comprises four specialized agents: Manager, Repository Custodian, Developer, and Quality Assurance Engineer, which collaboratively address task decomposition, code generation, and review. Experimental results indicate that MAGIS achieves a resolved ratio of 13.94% on the SWE-bench dataset, significantly outperforming baseline models like GPT-4. The authors conduct an empirical study to identify factors affecting LLM performance, providing insights into the challenges of applying AI in software engineering.

### Strengths and Weaknesses
Strengths:
- The paper effectively addresses the complex problem of LLMs resolving GitHub issues, demonstrating an eight-fold performance improvement over GPT-4.
- It employs a rigorous empirical methodology, analyzing multiple factors influencing LLM performance and providing valuable insights for the research community.
- The framework's design is intuitive, leveraging the collaborative roles of agents to enhance code generation and review processes.

Weaknesses:
- The reproducibility of experiments is questionable due to the unavailability of code and data for baseline models like GPT-4 and Claude.
- The paper lacks clarity on the specific prompt content used, raising concerns about the dependency of results on prompt quality.
- The framework's complexity and resource requirements are not adequately analyzed, leaving questions about the necessity of additional computing resources.
- The reliance on the SWE-bench dataset, limited to 12 Python repositories, may not represent the diversity of real-world software projects.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the prompt design by including specific details on the prompts used in experiments. Additionally, a more thorough analysis of the computational requirements and efficiency of MAGIS compared to simpler approaches should be provided. To enhance reproducibility, the authors should make the code and data available for baseline models. We also suggest conducting ablation studies to explore the interactions between agents and validate the framework's generalizability beyond the SWE-bench dataset. Finally, addressing the potential overfitting to SWE-bench and including feedback from actual software developers could strengthen the paper's contributions.