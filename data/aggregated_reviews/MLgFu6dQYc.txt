ID: MLgFu6dQYc
Title: How to Boost Any Loss Function
Conference: NeurIPS
Year: 2024
Number of Reviews: 43
Original Ratings: 6, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 1, 3, 2, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for boosting any bounded loss function using a weak learner, introducing a new algorithm called SECBOOST that employs zeroth-order optimization. The authors highlight that traditional boosting methods rely on assumptions about the loss function, such as convexity and differentiability, whereas SECBOOST operates without these constraints. They claim that their method converges for essentially any loss function, achieving in-sample error guarantees that approach the minimum of the loss function as the number of iterations increases. The authors also discuss the implications of weak learning assumptions, particularly Assumption 5.5, and the necessity of dividing by \(M_t\) to ensure the weak learner's effectiveness. Additionally, they introduce new tools related to higher-order \(v\)-derivative information, which are crucial for their analysis. However, the paper has been criticized for its classification within learning theory rather than optimization, as it primarily addresses optimization issues without investigating generalization.

### Strengths and Weaknesses
Strengths:  
- The originality of the work is notable, as the authors could not find prior research on combining zeroth-order optimization with boosting.  
- The writing style is generally clear, allowing for comprehension of the local meaning of sentences.  
- The paper effectively situates itself within the broader context of boosting and optimization, addressing historical developments and current gaps in the literature.  
- The incorporation of quantum calculus and higher-order \(v\)-derivative information adds an interesting dimension to the work.  
- The authors are transparent about their decision to disregard generalization aspects, which clarifies their focus.  

Weaknesses:  
- The lack of examples comparing SECBOOST to established algorithms for specific loss functions limits the demonstration of its applicability.  
- Some assumptions, particularly Assumptions 5.4 and 5.5, are convoluted and may confuse readers regarding their implications and verifiability.  
- The overall presentation is poor, lacking clarity and organization, making it laborious to review.  
- The claimed contributions are not sufficiently enumerated or emphasized, leading to confusion about their significance.  
- Theorem 5.3 lacks a self-contained statement, requiring multiple logical steps to grasp its importance.  
- The authors do not provide a precise formulation of the optimization problem they are addressing, which is essential for understanding their approach.  
- The motivation for zeroth-order methods is inadequately justified, particularly in relation to existing literature on boosting.  
- Some references and comments were not visible to reviewers, leading to confusion and wasted time.  

### Suggestions for Improvement
We recommend that the authors improve the paper by providing concrete examples of loss functions where SECBOOST can be compared to the best-known algorithms in the boosting setting. Additionally, clarifying the conditions and implications of Assumptions 5.4 and 5.5 would enhance reader understanding. We suggest enhancing the presentation by providing a clearer roadmap and explicitly enumerating contributions to aid comprehension. The authors should ensure that Theorem 5.3 is presented in a self-contained manner and provide a precise statement of the optimization problem early in the paper. Strengthening the motivation for their approach by discussing how it compares to existing methods in the literature, particularly addressing the existence of an optimal boosting algorithm that does not utilize gradients, would also be beneficial. Finally, addressing the inconsistent notation and definitions, as well as correcting typos and grammatical issues, will improve the overall clarity of the paper.