ID: FisyQfoJCm
Title: MoGenTS: Motion Generation based on Spatial-Temporal Joint Modeling
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for text-conditioned motion generation by quantizing each joint into individual vectors, rather than encoding the entire body pose into a single code. The authors propose a new quantization strategy that associates a token with each joint, organizing motion sequences into a 2D token map. This allows for the application of 2D operations such as convolution, positional encoding, and attention mechanisms. The architecture includes a spatial-temporal 2D joint VQVAE and employs a masking strategy to predict masked tokens. The method demonstrates superior performance compared to state-of-the-art approaches.

### Strengths and Weaknesses
Strengths:
- The paper introduces a joint-level quantization approach that effectively preserves spatial relationships and simplifies encoding.
- The integration of a 2D joint VQVAE, temporal-spatial 2D masking, and spatial-temporal 2D attention forms a robust framework for capturing dynamics in human motion.
- Extensive experiments validate the method's efficacy, showing improvements over previous methods on key datasets.
- The paper is well-written and clear, with a user study included.

Weaknesses:
- The quantization process is difficult to understand, particularly regarding the representation of joints and the implications of residual quantization.
- There are ambiguities in the presentation of certain equations and the overall method until later sections.
- The computational overhead of using 2D code maps and spatial-temporal attention is a concern, as it may exceed that of previous methods.
- Experiments are limited to smaller datasets, and the paper lacks a detailed discussion on the extraction of FID features and classifier-free guidance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the quantization process, particularly how each joint is represented and the implications of residual quantization. It would be beneficial to provide a clearer explanation of the equations and the overall method earlier in the paper. Additionally, we suggest that the authors compare the inference speed of their method with mainstream approaches to address concerns about computational overhead. Expanding experiments to include larger-scale datasets would strengthen the validation of the proposed method. Finally, we recommend that the authors include a detailed discussion on the extraction of FID features and classifier-free guidance in the main text rather than relegating it to the appendix.