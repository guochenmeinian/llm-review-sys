ID: KE5QunlXcr
Title: Incorporating Syntactic Knowledge into Pre-trained Language Model using Optimization for Overcoming Catastrophic Forgetting
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the incorporation of syntactic knowledge into language models through additional training. The authors propose four pretraining tasks aimed at enhancing syntactic understanding and explore gradient surgery and elastic weight consolidation methods to prevent catastrophic forgetting. The main contributions include the introduction of these tasks, the examination of optimization methods, and the demonstration of improved performance on tasks such as CoLA, RTE, MRPC, and key phrase extraction.

### Strengths and Weaknesses
Strengths:
- The paper provides a thorough investigation of integrating syntactic knowledge into language models, with clear presentation and commendable clarity.
- The ablation study on optimizers and syntactic tasks offers a comprehensive analysis of their impacts.
- The experimental rigor is notable, with multiple comparisons across various tasks and hyperparameter settings.

Weaknesses:
- The novelty of the work is limited, as similar syntactic training methods have been previously explored, and the optimization functions are not new.
- The focus is solely on encoder-only models, neglecting the potential benefits of evaluating decoder-only models like GPT.
- Improvements are only demonstrated on a subset of GLUE tasks, with no advancements noted in the remaining tasks.
- There is a lack of comparison with existing methods, such as adapters, which could provide context for the proposed approach.

### Suggestions for Improvement
We recommend that the authors improve the discussion of existing literature on syntactic knowledge integration to clarify the novelty and relevance of their work. Additionally, we suggest including comparisons with larger and more contemporary models to assess the practical applicability of the proposed methods. The authors should also consider evaluating their approach on all GLUE tasks to provide a more comprehensive performance overview. Furthermore, we encourage the authors to clarify the choice between gradient surgery and elastic weight consolidation, potentially including only one method to streamline the focus of the paper. Lastly, addressing the potential domain drift and the relevance of perplexity as a measure of semantic retention would strengthen the study's arguments.