ID: EEwb201bnO
Title: Infer Induced Sentiment of Comment Response to Video: A New Task, Dataset and Baseline
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 8, 6, 6, 6, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel research task called Multimodal Sentiment Analysis for Comment Response of Video Induced (MSA-CRVI), which aims to infer viewer opinions and emotions from comments on micro-videos. The authors introduce the Comment Sentiment toward Micro Video (CSMV) dataset, the largest of its kind, comprising 107,267 comments and 8,210 micro videos totaling 68.83 hours. They propose a baseline method, Video Content-aware Comment Sentiment Analysis (VC-CSA), which includes three modules: Multiscale Temporal Representation, Consensus Semantic Learning, and Golden Feature Grounding, demonstrating significant improvements over existing methods.

### Strengths and Weaknesses
Strengths:
1. The introduction of the MSA-CRVI task is innovative and addresses a gap in sentiment analysis.
2. The CSMV dataset is large-scale, diverse, and manually annotated, providing a valuable resource for research.
3. The VC-CSA method is well-structured to tackle the complexities of the new task.

Weaknesses:
1. The dataset only provides visual features, limiting further exploration and performance improvements.
2. Some baseline methods lack novelty, and more recent baseline experiments are needed.
3. The dataset's annotation may introduce biases, and the consistency of these annotations requires demonstration.

### Suggestions for Improvement
1. We recommend that the authors improve the dataset by including audio features to enhance sentiment understanding, given the multimodal nature of the task.
2. The authors should consider diversifying the video sources beyond TikTok to assess the robustness of their findings across different platforms.
3. We suggest that the authors clarify the selection criteria for baseline methods and conduct additional experiments with more recent approaches to strengthen their claims.
4. The authors should discuss the limitations of the dataset more thoroughly, including potential biases and the implications of using a limited number of emotion labels.