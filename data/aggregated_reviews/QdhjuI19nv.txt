ID: QdhjuI19nv
Title: Compositional Generalization for Data-to-Text Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an assessment of compositional generalization in the Data-to-Text generation task, proposing a benchmark based on WebNLG 2017. The authors introduce a novel framework called predicate decomposition, which clusters unseen predicate combinations into smaller, seen groups for improved text generation. The evaluation method emphasizes faithfulness through metrics like PARENT and OK-percent, revealing that state-of-the-art pre-trained models struggle with compositional generalization.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and clearly articulates the development of the cluster-based compositional generalization method.
- The proposed approach demonstrates improved performance over T5, requiring smaller training sets without additional labeled data.
- The evaluations are grounded in predicate configurations, establishing a solid benchmark for future research.

Weaknesses:
- The study is limited to a single model size, raising concerns about its effectiveness with larger models.
- The method appears prone to repetition issues, particularly in generating longer texts, without effective solutions provided.
- The CG-Random performs well in many scenarios, questioning the necessity of the complex predicate composition process.

### Suggestions for Improvement
We recommend that the authors evaluate their method using larger models to address the efficacy of bridging compositional generalization gaps. Additionally, we suggest that the authors elaborate on strategies to mitigate repetition issues in generated texts, especially when entities recur across sentences. Finally, we encourage the inclusion of the 4-way OK-percent metric results to strengthen the evaluation of their approach.