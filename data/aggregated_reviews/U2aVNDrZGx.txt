ID: U2aVNDrZGx
Title: Benchmarking Complex Instruction-Following with Multiple Constraints Composition
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 7, 6, 6, 6, -1
Original Confidences: 3, 4, 3, 4, -1

Aggregated Review:
### Key Points
This paper presents ComplexBench, a new benchmark for evaluating large language models' (LLMs) ability to follow complex instructions with multiple constraints and compositional structures. The authors propose a hierarchical taxonomy of complex instructions, which includes four constraint types, 19 constraint dimensions, and four composition types. They introduce a manually constructed dataset of 1,150 instructions and a novel evaluation method called Rule-Augmented LLM-based (RAL) evaluation, which combines rule-based and LLM-based approaches. The experimental results reveal the strengths and weaknesses of 15 LLMs in handling complex instructions.

### Strengths and Weaknesses
Strengths:
- Addresses a significant gap in LLM evaluation benchmarks with a comprehensive taxonomy of instruction complexity.
- High-quality, manually constructed dataset.
- Extensive experiments with both closed and open-source LLMs.
- The RAL evaluation method effectively bridges traditional linguistics with modern AI techniques, demonstrating superior agreement with human evaluators.

Weaknesses:
- The dataset is limited to Chinese, which restricts its generalizability.
- Some samples in the dataset may deviate from realistic patterns, raising concerns about the artificiality of rule-based data generation.
- Clarity issues exist regarding the dataset construction process and the dependency graph of constraints.

### Suggestions for Improvement
We recommend that the authors improve the dataset's accessibility by expanding it to include multiple languages, enhancing its generalizability. Additionally, we suggest providing a detailed example of the dataset construction procedure in the appendix to clarify the main steps and mitigate potential biases. The authors should also include more information about the human annotators involved in the dataset creation and evaluation process. Furthermore, addressing the correlation between performance on existing datasets and results from ComplexBench would be beneficial. Lastly, we recommend clarifying the licensing and distribution details of the dataset within the paper.