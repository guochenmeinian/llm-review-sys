ID: kCCD8d2aEu
Title: Coherent Soft Imitation Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 6, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel imitation learning (IL) method called Coherent Soft Imitation Learning (CSIL), which integrates Behavioral Cloning (BC) and Inverse Reinforcement Learning (IRL). CSIL learns a "coherent reward" function that ensures the BC policy is optimal, followed by soft policy iteration on this reward. The authors evaluate CSIL across various tasks, including high-dimensional continuous control and image-based scenarios. They propose that the divergence view provides useful intuition, although it is complicated by the presence of two KL terms and the dependency of state distribution on the policy variable. The coherent reward shapes the reward function in a way that allows the BC policy to be optimal under certain conditions, and the authors discuss how the soft value function can help the policy improve by overcoming compounding errors.

### Strengths and Weaknesses
Strengths:
- The proposed algorithm CSIL is novel and demonstrates the ability to solve complex control tasks with minimal demonstrations.
- Extensive evaluation across diverse settings enhances the robustness of the findings.
- The authors provide a detailed explanation of the coherent reward shaping and its implications for the BC policy.
- Empirical results and examples are presented that clarify the mechanism of the proposed method, enhancing understanding.

Weaknesses:
- CSIL does not effectively combine the advantages of BC and IRL, as it primarily yields the BC policy as the best outcome, failing to address compounding errors inherent in BC.
- The theoretical foundation regarding the coherent reward lacks novelty, as similar results have been previously established.
- The argument regarding how the $Q$ function aids in overcoming compounding errors lacks clarity and rigorous analysis.
- The complexity of the divergence objective makes it challenging to derive formal characterizations of optimal solutions.
- The writing quality is subpar, leading to confusion, and the complexity of the method may hinder reproducibility.
- Experimental results show CSIL underperforming compared to existing methods like DAC in certain tasks.

### Suggestions for Improvement
We recommend that the authors improve the theoretical discussion by addressing the existing results related to the coherent reward function. Additionally, clarifying the derivation and initialization processes in Algorithm 1 would enhance understanding. We suggest improving the clarity of the explanation regarding how the $Q$ function helps the policy overcome compounding errors, along with a more rigorous analysis of the $Q$ functions during the training process. Simplifying the writing and reducing jargon could make the paper more accessible. Finally, we suggest including a summary of experimental results to better illustrate the method's complexity and performance across different tasks.