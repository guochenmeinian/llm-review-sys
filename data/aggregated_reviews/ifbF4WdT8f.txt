ID: ifbF4WdT8f
Title: EvoPrompting: Language Models for Code-Level Neural Architecture Search
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 4, 4, 7, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to neural architecture search (NAS) by utilizing large language models (LLMs) as mutation and crossover operators within an evolutionary computation framework. The authors propose EvoPrompting, which leverages in-context learning to enhance the evolutionary search process. Experiments are conducted on two benchmarks: MNIST-1D for convolutional networks and CLRS for graph neural networks, demonstrating that EvoPrompting achieves state-of-the-art results on CLRS and performs comparably on MNIST-1D.

### Strengths and Weaknesses
Strengths:  
- The exploration of LLMs in NAS is innovative, providing a flexible search space and enabling the generation of diverse architectures.  
- The empirical results indicate that EvoPrompting produces effective neural networks with fewer parameters compared to existing baselines, particularly on the CLRS benchmark.

Weaknesses:  
- The paper lacks a comprehensive ablation study to justify the advantages of using LLMs over traditional crossover and mutation operators.  
- The experiments primarily serve as demonstrations, lacking comparisons with state-of-the-art NAS methods and larger, realistic datasets.  
- The rationale for using LLMs in NAS is not clearly articulated, and the fitness function's derivation raises questions about its appropriateness.

### Suggestions for Improvement
We recommend that the authors improve the paper by including a detailed ablation study to validate the effectiveness of LLMs compared to traditional methods. Additionally, conducting experiments on larger and more diverse datasets would strengthen the claims made. It would also be beneficial to clarify the rationale behind using LLMs for NAS and to provide comparisons with existing NAS techniques. Furthermore, we suggest including a thorough analysis of the language model's performance across different sizes and exploring alternative prompt-tuning methods such as adapters and LoRA.