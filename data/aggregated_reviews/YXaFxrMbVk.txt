ID: YXaFxrMbVk
Title: The effect of fine-tuning on language model toxicity
Conference: NeurIPS
Year: 2024
Number of Reviews: 1
Original Ratings: 8
Original Confidences: 4

Aggregated Review:
### Key Points
This paper presents an exploration of the impact of fine-tuning on the toxicity of large language models (LLMs), focusing on parameter-efficient methods like Low-Rank Adaptation (LoRA). The authors compare base models with instruction-tuned variants, evaluate the effects of non-adversarial datasets on toxicity, and investigate community-contributed model variants. The study assesses models from Google, Meta, and Microsoft, analyzing their toxicity rates across various prompts, including severe and non-toxic inputs. Overall, the paper significantly contributes to understanding fine-tuning's effects on LLM toxicity, particularly regarding open models and community contributions.

### Strengths and Weaknesses
Strengths:  
The paper provides a comprehensive analysis of multiple models, including those from major labs, which enhances the empirical robustness of the findings. 

Weaknesses:  
The analysis could benefit from including more model variations, such as the new Llama3.2 small family and potentially OpenAI’s close-source models. Additionally, the absence of adversarial testing limits the evaluation of model robustness against deliberate toxic inputs. The impact of LoRA parameters on final performance is also not addressed, which is crucial for practitioners in fine-tuning.

### Suggestions for Improvement
We recommend that the authors improve the analysis by including the new Llama3.2 small family (1B & 3B) to verify the impacts of fine-tuning on model size and consider incorporating OpenAI’s close-source models. We also suggest adding adversarial testing to evaluate model robustness against toxic inputs. Lastly, we encourage the authors to test the impact of LoRA parameters on final performance, providing recommendations for optimal parameters based on model size, fine-tuning dataset, and task nature.