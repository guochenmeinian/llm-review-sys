ID: uccHPGDlao
Title: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 7, 6, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the feasibility of using Large Language Models (LLMs) as evaluators for fine-grained NLP tasks, introducing two new benchmarks to assess LLM judges against human evaluators. The authors evaluate several LLMs, notably finding that GPT-4 aligns closely with human judgments. Additionally, the paper presents a fine-tuned vicuna judge model, which is a significant contribution due to its potential for enhancing reproducibility in future works, unlike proprietary models such as GPT-4. The model's free and open-source nature adds to its appeal, and the authors have indicated that they have released the weights of the fine-tuned judge model, responding to feedback and interest from reviewers. The paper also discusses LLM biases and proposes mitigation techniques while introducing Chatbot Arena as a platform for generating quality human evaluations.

### Strengths and Weaknesses
**Strengths:**
- The potential for LLMs to evaluate language models at a reduced cost is highlighted, suggesting a future where LLMs can replace human evaluators.
- The authors provide a thorough examination of LLM biases and present methods to mitigate these, indicating high agreement rates with human evaluators.
- The introduction of Chatbot Arena enhances the availability of high-quality evaluation data.
- The development of a fine-tuned model addresses reproducibility issues and its open-source availability is a significant advantage.

**Weaknesses:**
- The paper lacks detailed statistical analysis of LLM-human agreement, particularly regarding the nature of disagreements.
- The evaluation question set is limited, with only 160 questions, which may not be sufficient for comprehensive assessment.
- Clarity issues arise in the scoring methods and the presentation of results, particularly in Table 2 and Table 5.
- The model's explanations for its judgments may exhibit biases and lack objectivity, as evidenced by the model's tendency to favor certain options without clear reasoning.

### Suggestions for Improvement
We recommend that the authors improve the statistical analysis of LLM-human agreement, providing more fine-grained data on disagreements, especially in areas where human evaluators typically diverge. Additionally, expanding the evaluation question set beyond 160 questions would strengthen the dataset's robustness. We suggest enhancing clarity in the scoring methods by providing concrete examples and clearer explanations, particularly for Table 2. Furthermore, we recommend that the authors improve the transparency of the model's explanations by addressing the biases in the judgment process and highlighting these limitations in the final version of the paper to provide a more balanced view of the model's capabilities. Lastly, we encourage the authors to reconsider the release of the model to enhance the overall quality and impact of their work.