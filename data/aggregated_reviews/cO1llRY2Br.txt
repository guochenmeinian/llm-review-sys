ID: cO1llRY2Br
Title: Initializing and Retrofitting Key-Value Adaptors for Traceable Model Editing
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 7, 5, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents iReVa, a method for model editing that retrofits key-value pairs into MLP blocks of transformer models to perform CRUD operations. iReVa aims to update knowledge in language models without damaging existing information, enhancing interpretability and traceability. The authors validate their approach through experiments on GPT models, demonstrating improvements in edit success and generalization while maintaining specificity.

### Strengths and Weaknesses
Strengths:
- The results indicate strong performance compared to relevant literature, particularly in batch editing scenarios.
- The approach allows for the separability of each edit, enabling operations like edit updation and deletion, as shown in the unique Edit Withdrawal Test.
- The paper provides a comprehensive analysis of iReVa's performance, including knowledge withdrawal tests.

Weaknesses:
- The novelty of the approach is questionable, as it closely resembles T-Patcher, with both methods relying on inserting neurons for editing.
- Missing comparisons with T-Patcher in the existing methods section weaken the analysis, especially given their similarities.
- The Edit Withdrawal Test lacks clarity regarding its execution and the extent of edits removed.
- The paper does not discuss the computational efficiency of iReVa, which is crucial for practical applications.
- The reliance on the assumption that factual knowledge is stored in MLP blocks may limit the method's applicability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the Edit Withdrawal Test section by detailing the experimental setup and the scope of edits removed. Additionally, the authors should include comparisons with T-Patcher and other relevant methods to strengthen the analysis. A discussion on the computational efficiency of iReVa, as well as its applicability to tasks like erasing hallucinations, would enhance the paper's robustness. Finally, addressing the limitations and broader societal impacts of iReVa would provide a more comprehensive understanding of its implications.