ID: XRJXKBeeTD
Title: Fine-Tuning is Fine, if Calibrated
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 5, 5, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a simple post-training calibration technique aimed at improving the classification of absent classes after fine-tuning a pre-trained model. The authors argue that fine-tuning does not entirely degrade the features of absent classes but rather downscales their logits, leading to overconfidence in the fine-tuning classes. The proposed method introduces a calibration hyper-parameter to enhance the prediction probabilities of absent classes, supported by analyses using the Nearest Class Mean classifier across multiple datasets. The findings indicate that the performance of absent classes can be improved through calibration, although the extent of this improvement is influenced by the fine-tuning procedure and the distribution of absent classes.

### Strengths and Weaknesses
Strengths:
- The proposed method is straightforward to implement and yields significant performance improvements.
- The use of the NCM classifier effectively isolates the influence of the linear classifier post-fine-tuning, providing a solid rationale for the method.
- The finding that fine-tuning does not completely erase the features of absent classes is intriguing and opens avenues for further research.
- A comprehensive ablation study highlights the method's strengths and limitations.

Weaknesses:
- **Statements are not precise**: Claims regarding the retention of relationships among absent classes lack clarity, as the extent of forgetting is contingent on the fine-tuning process.
- **Limited fine-tuning setting**: The method's applicability is constrained to specific configurations, assuming that the pre-trained model can classify all fine-tuning classes, including absent ones.
- The analysis of the NCM classifier's accuracy does not convincingly demonstrate improved discriminative ability, requiring more robust evidence.
- The rationale for accuracy drops in fine-tuning classes post-calibration is unclear, suggesting that claims of feature enhancement for absent classes may not hold.
- The proposed method's practical advantages over direct fine-tuning are not sufficiently addressed, necessitating a comparison of resource requirements.

### Suggestions for Improvement
We recommend that the authors improve the precision of their statements regarding the relationships among absent classes and the effects of fine-tuning procedures. Additionally, the authors should expand the fine-tuning settings to include a broader range of scenarios. To strengthen their claims, we suggest providing more rigorous evidence for the NCM classifier's accuracy improvements and clarifying the reasons behind accuracy drops in fine-tuning classes. Furthermore, it would be beneficial to compare the computational costs of the proposed calibration method against fine-tuning on absent classes to substantiate its practical value. Lastly, we encourage the authors to refine the analysis presented in figures and consider removing less impactful discussions from the main text to enhance clarity.