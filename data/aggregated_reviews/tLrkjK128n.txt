ID: tLrkjK128n
Title: Optimistic Active Exploration of Dynamical Systems
Conference: NeurIPS
Year: 2023
Number of Reviews: 33
Original Ratings: 5, 5, 5, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an active exploration method, OpAx, for learning dynamics models in Markovian systems with continuous states and actions. The authors propose maximizing one-step information gain without a cost function, alternating between optimistic planning and data collection to enhance model estimates. OpAx utilizes model epistemic uncertainty as a reward to facilitate global exploration, thereby enhancing zero-shot generalization capabilities. The authors claim their method provides theoretical guarantees for exploring the entire state-action space, a first in this context. The paper includes a convergence analysis demonstrating that OpAx asymptotically converges to the true model under certain assumptions and evaluates the algorithm against relevant baselines in continuous control tasks.

### Strengths and Weaknesses
Strengths:
- The paper addresses an active exploration problem relevant to both reinforcement learning and optimal control communities.
- The algorithm is simple and intuitively sound, combining theoretical justification with empirical validation.
- The authors provide a practical algorithm with theoretical guarantees applicable to a wide range of dynamical systems.
- The approach effectively addresses the challenge of zero-shot generalization by exploring dynamics globally rather than focusing on high-reward regions.
- The revised presentation clarifies the motivation behind the work and its distinction from reward-free RL approaches.

Weaknesses:
- The motivation for the algorithm as a tool for system identification lacks robustness from a reinforcement learning perspective.
- The theoretical justifications for the proposed method's superior zero-shot generalization remain unclear, lacking insights beyond experimental results.
- Experimental results show limited improvement over simpler baselines, such as planning with the average model.
- The paper overlooks several related works in reinforcement learning, particularly in reward-free exploration and active model estimation.
- The paper lacks explicit guarantees on zero-shot performance without structural assumptions, raising concerns about its applicability in certain scenarios.
- The connection between model accuracy and multi-task performance is not sufficiently articulated, leaving some reviewers unconvinced of its necessity.

### Suggestions for Improvement
We recommend that the authors improve the motivation for their approach by explicitly relating their model to standard Markov Decision Processes (MDPs) and clarifying the expressive power of their framework. Additionally, the authors should enhance the experimental section by demonstrating scenarios where OpAx significantly outperforms baseline methods, particularly in terms of model performance. We suggest including a discussion of the limitations of their work, especially regarding empirical results and theoretical guarantees, and addressing the absence of recent literature on reward-free RL and active model estimation. Furthermore, we recommend improving the clarity of their theoretical justifications for why OpAx achieves better zero-shot generalization, providing insights beyond experimental results. The authors should clarify the concept of "undirected exploration" in OpAx and ensure that the pseudocode reflects the multi-task feature they assert. It would also be beneficial to formally state the generalization objective and its implications in the problem setting and analysis sections. Lastly, we encourage the authors to clearly state the zero-shot performance objective in the problem statement section, linking it directly to the model's accuracy and multi-task performance, and to refine the language around the claim of solving multiple downstream tasks in a zero-shot manner to avoid confusion with meta-RL and multi-task RL frameworks.