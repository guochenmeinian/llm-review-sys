ID: sbusw6LD41
Title: Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a thorough analysis of the activation outlier problem in transformer models during quantization, highlighting limitations of existing approaches. The authors propose two components, clipped softmax and gated attention, aimed at regularizing activation outliers during training. Comprehensive experiments demonstrate the effectiveness and efficiency of these methods across various transformer architectures and tasks.

### Strengths and Weaknesses
Strengths:  
- The proposed components, clipped softmax and gated attention, are simple yet effective, supported by experimental results.  
- The analysis of outliers and their relation to attention heads provides solid insights into the problem.  
- The paper is well-organized, with detailed information on related methods, enhancing reader comprehension.  
- Experiments include diverse transformer structures, showcasing the methods' applicability.

Weaknesses:  
- The quantization performance at lower bit widths (e.g., 4W4A) remains unexplored.  
- The evaluation primarily compares the proposed methods to vanilla PTQ, lacking comparisons with other outlier suppression techniques.  
- The paper does not provide results for larger-scale models, raising concerns about generalization.  
- Detailed data distribution post-training with the proposed methods is missing, which is crucial for validating their effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including comparisons with existing outlier suppression methods to better demonstrate the practical value of their approach. Additionally, conducting experiments on larger-scale models would provide insights into the generalization capabilities of the proposed methods. We also encourage the authors to release the code for reproducibility and to include detailed data distributions after training to validate the reduction of outliers effectively.