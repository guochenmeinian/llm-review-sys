ID: cOw65A9FGf
Title: Text-Guided Attention is All You Need for Zero-Shot Robustness in Vision-Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach to enhance the robustness of CLIP models against adversarial perturbations by utilizing saliency maps derived from inner products between image and text embeddings. The authors propose two regularization terms: one aligns the saliency maps of original and adversarial examples, while the other matches the saliency maps between the pre-trained CLIP model and the target model. Experimental results indicate that fine-tuning CLIP with these terms improves its robustness to attacks. The proposed framework, Text-Guided Attention for Zero-Shot Robustness (TGA-ZSR), aims to maintain performance on clean images while enhancing zero-shot adversarial robustness across multiple datasets.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and organized, with clear descriptions of motivation and methods.
2. The approach of enhancing CLIP robustness through aligning saliency maps is novel and intuitive.
3. Extensive experiments demonstrate the effectiveness of the proposed method across 16 datasets.
4. The method improves robustness without significantly sacrificing performance on clean data.

Weaknesses:
1. The experimental results do not fully support claims of enhanced generalization; specifically, there is a notable decrease in clean accuracy compared to the original CLIP model.
2. The method is only demonstrated on CLIP, raising questions about its applicability to other vision-language models.
3. The introduction of additional hyperparameters lacks a clear strategy for tuning across different datasets or tasks.
4. No error bars or statistical significance tests are reported for the experimental results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims regarding generalization and adversarial robustness, ensuring they align closely with the experimental data presented. Additionally, the authors should extend the experimental section to include evaluations against a broader range of adversarial attacks beyond PGD and provide explanations for the marginal advantages observed with AutoAttack. It would also be beneficial to include comparisons with other types of attention mechanisms, such as vision-only self-attention, to clarify the contributions of the text guidance. Finally, addressing the limitations of the method, particularly regarding hyperparameter sensitivity and applicability to other tasks, would strengthen the paper.