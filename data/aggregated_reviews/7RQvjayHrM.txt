ID: 7RQvjayHrM
Title: RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents RouterDC, a novel LLM query router that utilizes contrastive learning losses to train an encoder and LLM embeddings for efficient query routing. The motivation stems from the variability in LLM performance across tasks and the computational efficiency of routing compared to ensemble methods. The methodology employs a small LLM encoder, `mDeBERTaV3-base`, and learnable LLM embeddings, using a supervised learning approach with contrastive losses to optimize routing. Experimental results indicate that RouterDC outperforms existing routing methods on both in-distribution and out-of-distribution tasks.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant challenge in LLM utilization by focusing on efficient routing to optimize performance across various tasks and domains.
- The innovative contrastive learning approach for query routing enhances training stability and performance through both *sample-LLM contrastive loss* and *sample-sample contrastive loss*.
- Experimental results demonstrate RouterDC's superiority over existing routing methods, indicating the effectiveness of the proposed approach.

Weaknesses:
- The paper does not address the parameter efficiency of RouterDC compared to existing methods, which is crucial for understanding scalability, especially as the number of LLMs increases.
- LLM costs are not incorporated into the loss function, which could enhance cost efficiency by selecting the cheapest performing LLM.
- The inclusion of incapable LLMs, as shown in Table 3, leads to unnecessary computational overhead and performance degradation, yet this issue lacks analysis.
- The performance improvements from the sample-sample contrastive loss appear marginal and require further analysis.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the parameter efficiency of RouterDC compared to existing methods, particularly regarding training costs as the number of LLMs scales up. Additionally, incorporating LLM costs into the loss function could enhance cost efficiency. We suggest analyzing the impact of removing incapable LLMs on overall performance and considering an LLM-dropping mechanism to improve routing effectiveness. Furthermore, a clearer analysis of the marginal performance improvements from the sample-sample contrastive loss is necessary. Lastly, we encourage the authors to include a comparison with cascade-based approaches to provide a more comprehensive understanding of RouterDC's position within existing research.