ID: HavKlV22xJ
Title: Model-free Low-Rank Reinforcement Learning via Leveraged Entry-wise Matrix Estimation
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 4, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a leveraged matrix estimation method for low-rank policy evaluation, extending it to policy iteration as a model-free learning algorithm. The authors propose a two-phase approach to estimate the Q matrix, first using half the sample budget to determine leverage scores, then sampling entries based on these scores. The method guarantees entry-wise estimation error while relaxing the incoherence assumption commonly used in low-rank matrix estimation. The effectiveness of the method is supported by simulations included in the appendix.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and easy to follow, with rigorous theoretical proofs.  
- The relaxation of the incoherence assumption is a significant advantage of the proposed method.  
- The algorithm is parameter-free, not relying on spikiness/rank/coherence bounds, which enhances its applicability.

Weaknesses:  
- There is insufficient discussion on the existing literature regarding leveraged matrix estimation methods in matrix completion and low-rank matrix estimation.  
- The paper lacks real-world experiments, limiting its practical significance.  
- The transition from coherence to spikiness needs clearer justification, and the improvement over existing methods is not adequately demonstrated.

### Suggestions for Improvement
We recommend that the authors improve the discussion on how their method compares with other regularized policy evaluation approaches, such as nuclear-norm/max-norm regularization. Including experiments on real-world datasets would enhance the significance of this work. Additionally, we suggest providing a table comparing the performance and assumptions of previous works in low-rank reinforcement learning to clarify the contributions of this paper. It would also be beneficial to replace shadows in the figures with error bars for better readability. Lastly, we encourage the authors to elaborate on the implications of the low-rank assumption of the Q function under deterministic policies and explore the extension of their estimation idea to model-based methods and non-tabular settings.