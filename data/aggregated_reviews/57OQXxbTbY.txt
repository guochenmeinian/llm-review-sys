ID: 57OQXxbTbY
Title: Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 6, 7, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Cal-DPO, a variation of DPO designed to address the issue of decreasing rewards for chosen answers. Cal-DPO incorporates calibration terms alongside DPO loss to align the rewards from the language model with absolute ground truth values. Theoretical analysis indicates that Cal-DPO increases the likelihood of chosen responses while decreasing that of rejected ones. Empirical results demonstrate that Cal-DPO outperforms DPO and other baselines across various benchmarks. The authors assert that their reported DPO results are optimal and provide a comparison with Cal-DPO, emphasizing the sensitivity of Cal-DPO to hyperparameters without cherry-picking parameters. They also discuss the implications of over-optimization and the motivation behind their approach, which seeks to maintain reward calibration to improve downstream task performance.

### Strengths and Weaknesses
Strengths:
1. The focus on the decreasing reward issue in DPO is significant, and the proposed Cal-DPO effectively addresses this concern.
2. The theoretical analysis of Cal-DPO appears sound and is well-articulated.
3. The paper is well-structured, with clear motivations, methodology, and analyses.
4. The authors effectively address reviewer concerns, demonstrating clarity in their rebuttal and willingness to improve the manuscript.
5. The inclusion of source code enhances reproducibility, which is crucial for validation of results.
6. The paper presents a clear motivation for the proposed method, linking it to existing literature on preference optimization.

Weaknesses:
1. The presentation of the motivation behind the method lacks clarity, particularly regarding the use of the term "calibration."
2. Some misunderstandings from reviewers indicate a need for clearer explanations in the manuscript.
3. The theoretical results could be stated more precisely, especially regarding the implications of Theorem 2.
4. The challenge of over-optimization is acknowledged but not fully explored within the scope of the paper.
5. There are grammatical errors and unclear terminology that need to be addressed.
6. The experiments are limited to the Zephyr-7B model, raising concerns about generalizability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind the use of "calibration" in their method. Additionally, we suggest that the authors state the theoretical results more precisely, particularly the implications following Theorem 2. The authors should also address grammatical errors and clarify any unclear terminology. Furthermore, we encourage the authors to conduct experiments with other models beyond Zephyr-7B to enhance the generalizability of their findings. Lastly, we recommend including a more detailed discussion on the challenge of over-optimization in the final version, as well as a comparison to IPO to provide a more comprehensive evaluation of the proposed method.