ID: LdvVd0bNyO
Title: ODE-based Recurrent Model-free Reinforcement Learning for POMDPs
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 5, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GRU-ODE, a novel ODE-based recurrent model aimed at addressing POMDP control tasks. By leveraging a numerical ODE solver, GRU-ODE effectively integrates past observations, actions, and rewards into its context. Compared to traditional RNN-based methods, GRU-ODE excels in managing irregular observations and inferring unfamiliar physical environments. Experimental results indicate that GRU-ODE outperforms baseline models across various POMDP tasks, meta RL tasks, and scenarios with temporally irregular observations. Additionally, the authors propose a method utilizing KL loss with a prior based on the previous state in SAC-LSTM to minimize significant changes between hidden states of adjacent time steps. This approach aims to address issues observed with GRU-ODE, where hidden states can change significantly, particularly in reinforcement learning contexts. The use of KL divergence is proposed to regularize adjacent outputs, enhancing the representational capacity of the model.

### Strengths and Weaknesses
Strengths:
- The experimental results validate the GRU-ODE method, showcasing competitive performance.
- The paper is well-organized and accessible.
- The use of ODEs allows for handling continuous time dynamics, enhancing model accuracy.
- GRU-ODE effectively extracts unobservable information from partially observable environments.
- The model demonstrates robustness against irregular observations, a significant advantage in real-world applications.
- The authors demonstrate a clear understanding of the limitations of their approach and provide a rationale for using KL divergence to regularize outputs.
- The inclusion of additional experiments has been positively received, indicating merit in the proposed method.

Weaknesses:
- The applicability of GRU-ODE is limited by assumptions regarding the reward function and task physics, which may not hold in all scenarios.
- The evaluation lacks sufficiently recent baselines, weakening the persuasiveness of the results.
- ODE-based methods incur higher training and inference times; comparisons with baseline methods should be included.
- The evaluation environments may not adequately represent POMDP challenges, as simply removing parts of the state does not simulate true partial observability.
- There is a concern regarding the fairness of applying the previous-state-prior KL constraint solely to their method without similar applications to other methods.
- Some reviewers noted computational constraints that may affect the overall evaluation of the approach.

### Suggestions for Improvement
We recommend that the authors improve the applicability of GRU-ODE by addressing the assumptions regarding the reward function and task physics. Additionally, including results from more recent baseline studies, such as those from the past two years, would strengthen the evaluation. We suggest that the authors provide comparisons of training and inference times against standard methods to clarify the computational costs involved. Furthermore, we encourage the authors to explore more complex evaluation environments that better reflect the challenges of POMDPs, rather than relying on basic state removal techniques. To enhance the robustness of their findings, we recommend conducting ablation studies for the previous-state-prior KL constraint applied to other methods, ensuring a fair comparison. Addressing concerns about computational constraints in the revised version would also strengthen the paper further.