ID: N7R2emgl67
Title: Learning to Rank Context for Named Entity Recognition Using a Synthetic Dataset
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to document-level Named Entity Recognition (NER) by utilizing instruction-following large language models (LLMs) like Alpaca for generating synthetic training data. The authors propose a neural retriever trained on this synthetic dataset to enhance the NER task by retrieving relevant context from novels. Experimental results indicate that this method achieves a 1-point increase in F1 score on the Dekker et al. (2019) dataset compared to traditional methods.

### Strengths and Weaknesses
Strengths:
- The use of LLMs for data augmentation in NER is a promising and innovative direction.
- The paper is well-written and organized, facilitating reproducibility.
- Experimental results demonstrate that the proposed method outperforms unsupervised retrievers.

Weaknesses:
- The dataset generation is limited to specific entity types, raising concerns about generalizability to new types.
- The paper lacks a thorough analysis of the quality of the generated samples and does not include strong baselines for comparison.
- There is insufficient exploration of the computational costs and the necessity of generating new datasets for different NER tasks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the dataset generation process and its applicability to various entity types. Including a random baseline for comparison and assessing the relevance of the dataset against a neural reranker trained on existing similarity datasets would strengthen the evaluation. Additionally, we suggest conducting qualitative analyses to elucidate the source of the F1 score improvement and exploring the implications of discrepancies between training and test distributions. Finally, the authors should consider including comparisons with other re-ranking models and provide more details on the training of the neural retriever.