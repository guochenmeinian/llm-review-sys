ID: oSOVME9kl2
Title: Implicit Regularization of Sharpness-Aware Minimization for Scale-Invariant Problems
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 8, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the implicit regularization effect of Sharpness-Aware Minimization (SAM) on scale-invariant problems, introducing a new concept called Balancedness. The authors demonstrate both theoretically and empirically that SAM promotes Balancedness, leading to the proposal of Balancedness-Aware Regularization (BAR), which significantly enhances computational efficiency while improving performance in fine-tuning language models using LoRA. The study reveals that SAM's regularization effect is particularly strong on noisy data and provides insights into the interplay between SAM and scale-invariance in optimization.

### Strengths and Weaknesses
Strengths:
- The introduction of the concept of Balancedness offers a novel perspective on SAM's implicit regularization effects.
- The theoretical analysis is robust, with empirical validation that convincingly supports the claims made.
- The paper is well-structured, making it accessible for readers.

Weaknesses:
- The paper lacks a rigorous argument regarding the effects of SGD on Balancedness, particularly in the context of learning rates.
- Certain sections, such as the discussion of sharpness limitations, are confusing and could benefit from clearer explanations.
- The presentation is at times unclear, making it difficult for readers to follow the arguments and results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by simplifying explanations and providing more context for complex ideas, particularly in sections discussing Theorem 1 and the limitations of sharpness. Additionally, we suggest including the code for `eval.py` in the supplementary materials to enhance reproducibility. It would also be beneficial to discuss the BAR algorithm more thoroughly in the main body of the paper, rather than relegating it to the appendix, and to clarify the role of hyperparameters in the proposed algorithms. Finally, addressing the relationship between Balancedness and low-rank features, as well as the impact of optimization methods like AdamW on Balancedness, would strengthen the paper's contributions.