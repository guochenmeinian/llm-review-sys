ID: JG0Qa4LY3s
Title: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a solution to the challenges faced by multi-modal recommenders through the Multi-Modal Knowledge Distillation (MMKD) framework, which incorporates prompt-tuning. The authors address overfitting from high-dimensional features and inaccuracies in media content affecting user preference modeling. MMKD simplifies recommenders by compressing models via distilling edge relationships and multi-modal node content, while soft prompt-tuning bridges the semantic gap between multi-modal contexts and collaborative signals. A disentangled multi-modal list-wise distillation is also introduced to mitigate inaccuracies in multimedia data. Experimental results indicate that MMKD outperforms existing methods, with significant contributions including the development of the MMKD framework and its evaluation on real-world datasets.

### Strengths and Weaknesses
Strengths:
1. The MMKD framework innovatively addresses overfitting and inaccuracies in multi-modal recommenders.
2. It effectively compresses complex models, reducing resource consumption without sacrificing accuracy.
3. The paper is well-written, with clear illustrations and thorough experimentation against multiple baseline approaches.
4. The proposed methods are novel and technically sound, providing convincing empirical evidence for MMKD's efficacy.

Weaknesses:
1. The division information of the benchmark datasets used in experiments is unclear.
2. Some spelling and grammar issues require attention.
3. The methods may be overly complex, and the rationale for using overfitted teacher models needs clarification.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the division information for the benchmark datasets utilized in the experiments. Additionally, addressing the spelling and grammar issues throughout the manuscript is essential. We suggest simplifying the methods to enhance comprehensibility and conducting multiple experiments with different teacher models to validate the approach. Lastly, it would be beneficial to clarify how the MMKD model mitigates overfitting when using potentially overfitted teacher models.