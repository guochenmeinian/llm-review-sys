ID: jR2FkqW6GB
Title: Is Learning in Games Good for the Learners?
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 7, 7, 6, 7, 8, -1, -1
Original Confidences: 2, 4, 5, 2, 5, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of trade-offs between reward and regret in repeated gameplay between two agents. The authors introduce a generalized notion of equilibrium, termed (Phi-A, Phi-B)-equilibrium, which allows for different regret constraints and demonstrates that such equilibria can be achieved through algorithms maintaining regret guarantees against any opponent. The study investigates the efficiency of no-swap-regret learning algorithms compared to the Stackelberg strategy and characterizes the maximal reward achievable against a no-regret learner. Additionally, it addresses how to learn the Stackelberg strategy through repeated play against a no-regret learner.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and organized, addressing a wide range of important questions in the context of learning in games.  
- The theoretical analysis is solid, providing valuable insights into the nuanced problem of playing against no-regret learners.  
- The generalized equilibrium framework is clear and likely to impact the learning in games literature significantly.  

Weaknesses:  
- The presentation could be improved, particularly in section 1.1, which is lengthy and may confuse readers due to the introduction of concepts not yet defined.  
- The breadth of results may obscure the main takeaway, making the submission read like a list of findings rather than a cohesive argument.  
- Some minor clarifications are needed regarding specific theorems and notations, as well as the interpretation of results related to mean-based algorithms.

### Suggestions for Improvement
We recommend that the authors improve the conciseness of section 1.1 by relocating some discussions to later sections to enhance clarity. Additionally, we suggest providing a longer discussion on related works, particularly references (10) and (22), to aid readers unfamiliar with the area. Clarifying the interpretation of Theorems 4 and 5 together, as well as elaborating on why exponential weights and FTPL algorithms are considered mean-based, would strengthen the paper. Finally, we encourage the authors to address the minor comments regarding notation and proofs to enhance the overall presentation.