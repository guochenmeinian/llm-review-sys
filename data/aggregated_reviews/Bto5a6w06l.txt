ID: Bto5a6w06l
Title: On Architectural Compression of Text-to-Image Diffusion Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 23
Original Ratings: 3, 7, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a stable diffusion compression pipeline that integrates architecture compression and knowledge distillation, specifically targeting text-to-image generation tasks. The authors demonstrate competitive performance compared to the original stable diffusion while achieving a reduction in parameter count and training dataset size. The study focuses on removing layers from the diffusion U-Net and fine-tuning with a limited dataset, achieving a significant reduction in parameters and inference latency. Additionally, the authors propose various compressed architectures for Stable Diffusion, including the **BK-SDM-Tiny**, which achieves a 51% reduction in parameters and a 43% improvement in latency while maintaining acceptable generation quality. The study includes a comprehensive sensitivity analysis and various empirical contributions that aim to support practitioners working with resource-constrained settings.

### Strengths and Weaknesses
Strengths:
1. The approach is straightforward and easy to understand.
2. The paper addresses a timely topic, with potential broad impacts on usability and environmental considerations.
3. It demonstrates the effectiveness of conventional compression techniques applied to diffusion models.
4. The introduction of multiple architecture variants and a block-wise sensitivity analysis enhances the practical applicability of the findings.
5. The results demonstrate significant potential for reducing model size while maintaining performance, which is crucial for users with limited computational resources.

Weaknesses:
1. The paper offers only marginal improvements over existing stable diffusion methods, limiting its generalizability to other frameworks.
2. The reduction in parameters is not substantial enough to indicate significant advancements for industrial applications.
3. The methodology lacks novelty, as pruning and distillation are well-established techniques, and the rationale for specific layer removals is not adequately justified.
4. There is insufficient theoretical analysis or systematic exploration of the pruning choices made.
5. The main contribution regarding architecture compression lacks in-depth analysis, particularly concerning the scalability of the UNet structure.
6. There is insufficient exploration of extreme compression rates and their impact on generation quality, which raises questions about the trade-offs between speed and output quality.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their work by providing a more systematic approach to block removal, possibly incorporating importance scores for each block. Additionally, we suggest conducting a thorough analysis of the trade-offs between speed and quality, exploring various compression levels beyond the current model. Clarifying the rationale behind layer selection and including comparisons with other diffusion methods would enhance the paper's contribution. Furthermore, we encourage the authors to elaborate on the effects of more excessive compression rates, as this could provide valuable insights into the balance between inference speed and generation quality. Finally, addressing the potential societal impacts of model compression could provide a more comprehensive discussion.