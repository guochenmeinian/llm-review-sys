ID: ktYjrgOENR
Title: DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Duty-Distinct Chain-of-Thought (DDCoT) Prompting for multimodal reasoning tasks, particularly in visual question answering (VQA). The authors analyze challenges such as hallucination in language models (LMs) and propose a method that integrates reasoning and visual recognition capabilities. The experimental results indicate that DDCoT improves performance in both zero-shot and fine-tuning settings on the ScienceQA benchmark.

### Strengths and Weaknesses
Strengths:
- The authors provide a clear motivation and comprehensive analysis of the hallucination problem in multimodal reasoning.
- The proposed DDCoT method effectively combines LLMs and visual models, demonstrating superior performance over common baselines.
- The paper is well-structured, with clear explanations and illustrative examples that enhance understanding.

Weaknesses:
- The method appears to be a straightforward combination of existing approaches, lacking significant technical novelty compared to prior works like Self-asking COT and Visual ChatGPT.
- The claims regarding zero-shot multimodal reasoning are insufficiently supported, with a lack of comparison to pretrained visual language models (VLMs) and other relevant methods.
- The rationale-compressed visual embedding (RCVE) module's calculations are unclear, particularly in how it integrates visual features.
- The presentation suffers from redundancy and confusion in terminology, such as "negative space prompting," and the overall clarity could be improved.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their method by clearly distinguishing it from existing approaches and providing a more detailed analysis of its unique contributions. Additionally, we suggest including comparisons with relevant pretrained VLMs and other similar methods to substantiate claims of originality. Clarifying the calculations involved in the RCVE module and enhancing the presentation of figures and terminology will also benefit the paper. Finally, exploring additional datasets beyond ScienceQA could strengthen the generalizability of the findings.