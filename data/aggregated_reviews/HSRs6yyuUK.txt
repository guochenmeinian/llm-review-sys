ID: HSRs6yyuUK
Title: Preventing Model Collapse in Deep Canonical Correlation Analysis by Noise Regularization
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 5, 5, 5, 4, -1, -1, -1
Original Confidences: 5, 3, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents NR-DCCA, a novel approach to Deep Canonical Correlation Analysis (DCCA) that incorporates noise regularization to mitigate model collapse in multi-view representation learning (MVRL). The authors analyze the differences between Linear CCA and DCCA, attributing model collapse to the rank of weight matrices, and propose NR-DCCA as a solution. Experimental results demonstrate the effectiveness of NR-DCCA.

### Strengths and Weaknesses
Strengths:
1. The perspective of observation regarding model collapse is novel.
2. The proposed method is simple and effective, showing clear performance improvements in both synthetic and practical settings.
3. Theoretical analyses are sound, providing meaningful insights into the differences between Linear CCA and DCCA.

Weaknesses:
1. The term "model collapse" may be misleading; the performance degradation resembles overfitting rather than an extreme collapse.
2. The method, while effective, is overly simplistic and lacks depth in theoretical support for its claims regarding weight matrix rank and overfitting.
3. The presentation suffers from readability issues, particularly in the organization of the appendix and the clarity of figures.
4. The authors do not adequately address the universality of NR loss across other DCCA methods or provide sufficient proof for the effectiveness of noise regularization.

### Suggestions for Improvement
We recommend that the authors improve the precision of terminology, particularly regarding "model collapse," to avoid confusion with overfitting. Additionally, the authors should enhance the theoretical support for their claims about the relationship between weight matrix rank and overfitting. We suggest reorganizing the appendix for better readability and clarity, ensuring that all experimental settings and protocols are clearly defined. Finally, the authors should consider including more recent comparison methods and larger datasets to strengthen the contribution of their work.