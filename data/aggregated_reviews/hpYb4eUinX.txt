ID: hpYb4eUinX
Title: Boosting Verification of Deep Reinforcement Learning via Piece-Wise Linear Decision Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 7, 5, 6, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an inverse transform-then-train approach for verifying deep reinforcement learning (DRL) systems by encoding a deep neural network (DNN) into efficiently verifiable linear control policies, which are optimized through reinforcement learning. The method is compatible with existing DRL training algorithms and demonstrates that piece-wise linear decision neural networks (PLDNNs) can achieve up to 438 times speedup in verification and significantly reduce overestimation. The approach involves partitioning the input state space and training linear policies for each partition, allowing for tighter and more efficient verification.

### Strengths and Weaknesses
Strengths:
- The paper explores an important direction in the verification of DRL systems.
- It introduces an innovative methodology that is practical and shows significant improvements in verification efficiency.
- The empirical evaluation indicates that PLDNNs maintain performance comparable to standard policy networks while being more amenable to verification.

Weaknesses:
- Some technical aspects are unclear, particularly regarding the extraction of piecewise linear functions and the concept of dual overestimation.
- The presentation of contributions may be misleading, and the limitations of using PLDNN are not adequately discussed.
- There is a lack of analysis on the impact of partitioning on performance improvement, and the scalability of verification in high-dimensional state spaces is a concern.

### Suggestions for Improvement
We recommend that the authors improve the clarity of technical sections, particularly in explaining the extraction of piecewise linear functions and the concept of dual overestimation. Additionally, we suggest providing a more detailed discussion on the limitations of PLDNN, including its applicability in high-dimensional settings and discrete action spaces. An empirical comparison between the transform-then-train and train-then-transform processes would also enhance the paper's contribution. Finally, including more motivations for the design of each component and addressing the scalability of verification in complex environments would strengthen the overall presentation.