ID: gxMfNArldP
Title: Q-VLM: Post-training Quantization for Large Vision-Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 2, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Q-VLM, a post-training quantization framework for Large Vision-Language Models (LVLMs) that aims to reduce model complexity by replacing floating-point numbers with quantized values and utilizing integer arithmetic for operations. The authors innovate by mining cross-layer dependencies to optimize rounding functions, thereby minimizing quantization noise. Experimental results demonstrate significant memory compression and speed improvements on various models, including LLaVA, without severe performance degradation.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel approach to post-training quantization that considers cross-layer dependencies, enhancing efficiency and accuracy.
2. It is well-organized and clearly written, facilitating understanding of the authors' contributions.
3. Extensive evaluations and ablation studies provide insights into the importance of each component of Q-VLM.

Weaknesses:
1. The evaluation is primarily limited to the ScienceQA dataset, which may not represent the diverse challenges faced by LVLMs in real-world applications. Broader dataset evaluation is necessary.
2. The benefits of using entropy as a proxy for quantization are unclear, and the method's marginal performance improvements raise questions about its consistency across different tasks.
3. Important baseline methods are missing from the comparisons, and the rationale for the proposed methods being superior to existing solutions needs further clarification.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including a wider range of datasets to assess the generalizability of their method beyond ScienceQA. Additionally, clarifying the motivation for using entropy as a proxy for quantization and providing comparisons with state-of-the-art methods like SmoothQuant and ZeroQuant variants would strengthen the paper. It would also be beneficial to discuss the specific advantages of their approach over traditional quantization methods and to clarify the differences in quantization strategies applied to the visual encoder and language model.