ID: M2UzLRoqic
Title: Reducing Transformer Key-Value Cache Size with Cross-Layer Attention
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 7, 8, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 5, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Cross-Layer Attention (CLA), a method designed to reduce the memory consumption of Key-Value (KV) caches in transformer-based large language models (LLMs) by sharing the KV cache across contiguous layers. The authors conducted pre-training experiments demonstrating the effectiveness of CLA compared to multi-query attention (MQA) and grouped-query attention (GQA). The approach is straightforward and integrates well with existing KV cache compression techniques.

### Strengths and Weaknesses
Strengths:
1. The problem addressed is significant for real-world applications of LLMs.
2. The proposed CLA approach is simple, easy to implement, and the paper is well-structured.
3. Experimental results indicate that CLA achieves better memory/accuracy trade-offs, allowing for longer sequences and larger batch sizes.

Weaknesses:
1. Experimental results are not fully presented; key metrics for various models are missing, limiting a comprehensive understanding of CLA's effectiveness.
2. The authors should consider using a fully open-source pre-trained model as a baseline for more convincing results.
3. Training loss curves are absent, which are crucial for assessing model convergence.
4. Citation formats are inconsistent, with missing publication venues in several references.
5. Minor typographical errors are present in the text.

### Suggestions for Improvement
We recommend that the authors improve the comprehensiveness of their experimental results by including all relevant metrics and training loss curves. Additionally, consider using a fully open-source pre-trained model, such as OpenLLaMA, as a baseline for testing CLA. Addressing the citation format inconsistencies and correcting typographical errors will enhance the paper's professionalism. Furthermore, providing insights on why cross-layer sharing of KV cache may yield better models than GQA would strengthen the argument for CLA. Lastly, exploring the adaptability of pre-trained models to CLA through fine-tuning could significantly increase the impact of this work.