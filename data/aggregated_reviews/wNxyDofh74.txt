ID: wNxyDofh74
Title: Progressive Ensemble Distillation: Building Ensembles for Efficient Inference
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 6, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents B-DISTILL, a progressive distillation algorithm aimed at approximating a large teacher model using an ensemble of smaller student models. The authors propose a two-player zero-sum game formulation to derive a weak learning condition, supported by theoretical guarantees and empirical results demonstrating its effectiveness in anytime inference and early prediction tasks. The method allows for a flexible trade-off between accuracy and inference time.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, clear, and organized, making the argumentation mostly easy to follow.
- The proposed technique is underpinned by both theoretical and empirical support, showcasing its practical applicability.
- The approach of connecting ensemble distillation to boosting methods is novel and intriguing.

Weaknesses:
- The computational requirements during training are unclear, particularly regarding the potential expense of fitting multiple models for each student.
- The empirical results lack robust comparisons to baseline methods, particularly in challenging tasks like TinyImageNet and ImageNet-1k.
- The presentation is confusing, with unclear relationships between concepts, inconsistent notation, and insufficient clarity on model configurations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of computational requirements during training, specifically addressing the overhead on memory and training speed when storing and loading activations and $K$-matrices. Additionally, include common distillation techniques and other methods for early-exit or anytime inference as baselines to provide a clearer context for the proposed method's performance. It would also be beneficial to clarify the relationship between two-player games and boosting in the related work section, and to ensure consistent notation throughout the paper. Lastly, consider modifying the title to better reflect the focus on efficient inference rather than solely on knowledge distillation.