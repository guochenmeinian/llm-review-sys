ID: JFVTItwiZZ
Title: Harnessing Large Language Models for Text-Rich Sequential Recommendation
Conference: ACM
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework, LLM-TRSR (Large Language Models for Text-Rich Sequential Recommendation), to address the challenges faced by Large Language Models (LLMs) in text-rich sequential recommendation scenarios. The authors highlight limitations such as input length, computational overhead, and the ability to capture shifts in user preferences. The framework incorporates two summarization techniques—hierarchical summarization and recurrent summarization—along with Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) methods. Experimental results on the Amazon and MIND datasets demonstrate the effectiveness of the proposed approach.

### Strengths and Weaknesses
Strengths:
- The proposal effectively harnesses LLMs for text-rich sequential recommendation.
- Introduction of hierarchical and recurrent summarization techniques enhances the framework's capability.
- The paper is well-written and provides extensive examples, making it accessible.

Weaknesses:
- The authors do not provide ablation study results to validate the effectiveness of the proposed modules.
- Comparative experimental results on parameter count and time efficiency are lacking.
- Some relevant literature is not cited, and the baseline models used for comparison are inadequate.

### Suggestions for Improvement
We recommend that the authors improve the manuscript by providing ablation study results to demonstrate the effectiveness of the proposed modules. Additionally, comparative experimental results on parameter count and time efficiency should be included. The authors should also consider incorporating more relevant papers in the related work section and conducting deeper ablation studies to verify the effectiveness of each stage of their framework. Furthermore, it would be beneficial to explain the choice of baseline models and include more text-based models for a fair comparison. Lastly, elaborating on the reasons why the "recurrent" summary may outperform the "hierarchical" summary could provide valuable insights.