ID: j5AoleAIru
Title: What You See is What You Read? Improving Text-Image Alignment Evaluation
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 7, 6, 6, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for evaluating semantic alignment between text and images through the benchmark SeeTRUE and two alignment metrics: VQ^2 and end-to-end VNLI. SeeTRUE includes a diverse set of real and synthetic image-caption pairs, with human annotations for alignment. The VQ^2 method generates question-answer pairs from captions to assess alignment, while VNLI fine-tunes a visual NLI model. Experimental results indicate that these methods outperform existing multimodal models, particularly in challenging scenarios.

### Strengths and Weaknesses
Strengths:
- The collection of human judgments on text-image alignment is a significant contribution, aiding future research.
- The annotation pipeline is well-designed and thoroughly described.
- SeeTRUE addresses limitations of existing benchmarks by including challenging negative captions and diverse image types.
- The proposed metrics are well-structured and demonstrate promising results in evaluations.

Weaknesses:
- The VQ^2 method's reliance on question-answer sampling may introduce variability, which is not adequately discussed.
- The potential biases from language models used to generate contradicting captions could affect dataset integrity.
- The manuscript lacks clarity on certain design choices, such as the motivation behind filtering question-answer pairs.
- There are concerns regarding the computational costs associated with VQ^2 compared to baseline methods.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the effects of sampling in the VQ^2 method to clarify potential variability. Additionally, providing more details about the PaLM instruction used for generating contradicting captions would enhance understanding. It would also be beneficial to include a comparison of computational costs in the main results of Table 2. Lastly, the authors should clarify the unique contributions of their methods in the context of existing literature on generating contradicting captions and discuss how SeeTRUE can be utilized to evaluate compositionality beyond current benchmarks.