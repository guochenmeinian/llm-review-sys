ID: 2XT3UpOv48
Title: Structured Federated Learning through Clustered Additive Modeling
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 8, 3, 8, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 5, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on heterogeneous federated learning, focusing on clustered federated learning and introducing a novel Clustered Additive Modeling (CAM) framework. The authors propose a method that combines a global model with cluster-specific models to address issues such as clustering collapse and model dynamics. Empirical results demonstrate the effectiveness of the proposed method across various datasets and non-IID settings.

### Strengths and Weaknesses
Strengths:
1. The writing is clear and accessible.
2. The empirical performance of the proposed method appears promising.
3. The CAM framework effectively addresses fundamental issues in clustered federated learning, such as clustering collapse and sensitivity to initialization.

Weaknesses:
1. The comparison with existing methods under non-IID data settings requires enhancement.
2. The ablation study on structural clustering is insufficient.
3. The rationale behind the CAM framework lacks sufficient justification, particularly regarding how it mitigates clustering collapse and handles outlier clients.
4. The optimization process for IFCA-CAM and FeSEM-CAM could lead to sub-optimal solutions due to separate updates of model parameters.

### Suggestions for Improvement
We recommend that the authors improve the comparison with other methods under non-IID settings, including more distribution-based label imbalance and feature skew cases. Additionally, the authors should conduct a more thorough ablation study on structural clustering and provide clearer discussions on the communication and computational costs associated with their method. It would also be beneficial to elaborate on how the proposed method can solve the challenges identified in the introduction and to include a convergence analysis that addresses the stability of clustering during training. Finally, we suggest providing more examples of cluster-wise non-IID settings and clarifying the impact of hyper-parameters on performance.