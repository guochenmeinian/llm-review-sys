ID: 4D7hnJ9oM6
Title: WATT: Weight Average Test Time Adaptation of CLIP
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 8, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Test-Time Adaptation technique named Weight Average Test-Time Adaptation (WATT) for Vision-Language Models (VLMs) like CLIP. WATT enhances test-time adaptation by utilizing various text prompt templates to create pseudo-labels for model updates and employing weight averaging to consolidate learned information. The method is evaluated across multiple datasets, demonstrating its ability to improve performance without necessitating further modifications to the model or additional trainable modules.

### Strengths and Weaknesses
Strengths:
- The novelty of WATT lies in its innovative approach to test-time adaptation, incorporating weight averaging with diverse text prompts, marking a significant advancement over traditional TTA techniques.
- The rigorous experimental setup showcases thorough evaluations across various datasets, yielding performance improvements over leading methods.
- The clarity of presentation, supported by visual aids and detailed explanations, enhances understanding of both methodology and results.
- The method's adaptability using a single image without model alterations is crucial for real-world applications.

Weaknesses:
- The evaluation lacks comparisons with recent state-of-the-art methods published in CVPR/ICCV, which are absent in both the Experimental and Related Work sections.
- While the paper claims efficiency, it does not provide a detailed comparison of computational costs with other TTA methods, which is vital for practical deployment.
- The focus on image classification limits the generalizability of the approach; extending it to other tasks like segmentation or detection could yield a more comprehensive understanding of its applicability.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including comparisons with recent state-of-the-art methods such as those mentioned in the reviews. Additionally, a detailed analysis of the computational costs associated with WATT compared to other TTA methods would be beneficial. We also suggest exploring the extension of WATT to other vision tasks, such as segmentation or object detection, to assess its broader applicability. Furthermore, clarifying the evaluation process—whether it adapts with a single batch or continuously during inference—would enhance understanding. Lastly, addressing the potential inaccuracies in text templates and measuring their similarity scores in the CLIP text encoder space could strengthen the methodology.