ID: OoOCoZFVK3
Title: Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CORY, a novel reinforcement learning (RL) technique for fine-tuning language models using a multi-agent framework. The approach involves two agents: a "pioneer" that generates initial outputs and an "observer" that refines these outputs, both trained cooperatively with a PPO algorithm. The authors claim that CORY enhances downstream performance, training stability, and robustness against distribution collapse compared to single-agent PPO. Additionally, the paper discusses multi-objective reinforcement learning (MORL) and its application to the fine-tuning of large language models (LLMs), categorizing MORL algorithms into multi-policy and single-policy methods. The authors conducted experiments using GGF-PPO, a state-of-the-art algorithm in MORL, which showed only marginal improvements over PPO. They argue that single-policy methods cannot leverage the emergent intelligence of multiple LLMs as effectively as their proposed method, CORY, and justify their choice of baselines, asserting that PPO and ER are sufficient for evaluating their method.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and accessible for readers familiar with RL and LLMs.
- The introduction of a multi-agent framework is innovative and may influence future research.
- The empirical results support the proposed method effectively.
- The paper provides a clear categorization of MORL methods and their relevance to LLM fine-tuning.
- Experimental results demonstrate the performance of GGF-PPO relative to other methods, supporting the authors' claims.
- The authors address reviewer concerns regarding MORL and baseline selection effectively.

Weaknesses:
- The paper lacks sufficient contextualization and comparison with relevant baselines, particularly in language modeling and multi-objective RL.
- An insufficient number of benchmarks is tested, with only one dataset per category.
- The choice of hyperparameters is not justified, limiting the claims about the method's superiority.
- The theoretical analysis of the framework is absent, and the presentation of claims in the abstract does not align with experimental evidence.
- The marginal performance improvement of GGF-PPO over PPO raises questions about its overall effectiveness.
- There is a need for more detailed information on the tuning of GGF-PPO to ensure fairness in comparisons with other methods.

### Suggestions for Improvement
We recommend that the authors improve the contextualization of their work by comparing CORY to established multi-objective RL methods and relevant language modeling techniques. Additionally, testing across multiple datasets per category would strengthen the evaluation of their method. Justifying the choice of hyperparameters and providing a theoretical analysis of the framework would enhance the robustness of their claims. We also suggest clarifying the tuning process for GGF-PPO, providing detailed information on the adjustments made to ensure fair comparisons with other methods. Lastly, incorporating a more explicit discussion of the limitations of GGF-PPO in the context of its marginal improvements would enhance the manuscript's depth and overall coherence.