ID: iHb4MOMyOd
Title: Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to pre-training sentence embeddings using a graph-based auto-encoder, effectively bridging pre-training and sentence-ranking tasks in unsupervised summarization. The authors propose leveraging intra-sentential and inter-sentential features from a sentence-word bipartite graph, resulting in embeddings that outperform traditional methods like BERT and RoBERTa in extractive summarization tasks. The experimental results demonstrate the embeddings' adaptability across various sentence-ranking methods, including TextRank, PacSum, and FAR.

### Strengths and Weaknesses
Strengths:  
- The paper provides an in-depth exploration of the graph-based pre-training auto-encoder, contributing valuable pre-trained embeddings for extractive summarization.  
- It showcases the versatility and effectiveness of the proposed embeddings across different unsupervised summarization techniques, supported by strong experimental results.

Weaknesses:  
- Some sections lack clarity, particularly regarding the fine-tuning process for sentence embeddings, which may confuse readers.  
- The experimentation is limited to the news domain, necessitating broader evaluation across diverse datasets, such as scientific articles and dialogue summarization.  
- The novelty of the approach is questioned, as the topic of extractive summarization with graph ranking has been extensively explored, and the claimed improvements over existing methods are not convincingly demonstrated.

### Suggestions for Improvement
We recommend that the authors improve the clarity of ambiguous descriptions, particularly regarding the fine-tuning process for sentence embeddings. Additionally, we suggest expanding the experimentation to include various domains beyond the news dataset to enhance the generalizability of the proposed method. It would also be beneficial to compare the proposed method with other summarization-focused pre-trained models, such as BART or PEGASUS, and to include more evaluation metrics for summary redundancy and coherence, as well as human evaluations.