ID: lSZXSDwvGv
Title: Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the capability of multiple large language models (LLMs) to enhance their negotiation strategies through a game involving self-play and AI feedback. The authors find that only certain LLMs can improve from AI feedback, with performance varying based on their roles in the negotiation. The study reveals insights into the models' abilities, the trade-offs between deal price and success rate, and improvements in language complexity and strategy through iterative feedback. Additionally, the authors propose a method for determining deal prices using an LLM to classify whether two players have reached an agreement, achieving over 90% accuracy through prompt engineering. They extract the deal price by identifying the last number in a sentence, a technique considered straightforward and replicable by researchers with basic prompt engineering skills. The authors instruct the model to negotiate by emphasizing the goal of achieving a low purchase price, positing that human instructions serve as the primary motivation for the model's bargaining behavior.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to understand, with a sensible experimental methodology that effectively narrows down model selection based on feedback responsiveness.
- The exploration of LLM interactions and their potential for autonomous improvement in negotiation is an interesting and relevant topic for AI research.
- The classification accuracy of over 90% demonstrates the effectiveness of the prompt engineering employed.
- The extraction method for deal prices is simple and replicable, making it accessible for other researchers.
- The results provide valuable insights into the role of LLMs as agents in negotiation scenarios.

Weaknesses:
- The incorporation of feedback appears limited to providing context rather than utilizing fine-tuning APIs, which could strengthen the findings.
- The choice of GPT-3.5 as a fixed agent raises questions about the robustness of results, particularly when compared to stronger models like GPT-4 or in negotiations with humans.
- The negotiation context lacks depth, as there is no intrinsic motivation for the buyer or seller, making the scenario feel contrived and less applicable to real-world situations.
- The AI feedback technique lacks clarity and depth, requiring more detailed explanation and comparison with other methods, such as Chain of Thought (CoT).
- The authors acknowledge that exploring the model's behavior when negotiating for its own interests is beyond the current scope, which may limit the understanding of the model's capabilities.

### Suggestions for Improvement
We recommend that the authors improve the incorporation of feedback by utilizing fine-tuning APIs for the models to enhance the robustness of their findings. Additionally, consider using stronger agents like GPT-4 in the experiments and explore the effects of negotiating with human participants. We suggest providing a more compelling context for the negotiation to better reflect real-world scenarios and enhance the intrinsic motivation for the agents involved. Furthermore, we encourage the authors to elaborate on the AI feedback technique, including its development and its relationship to other techniques like CoT, to clarify its novelty and effectiveness. Lastly, we recommend that the authors improve the exploration of the model's behavior when bargaining for its own interests to provide a more comprehensive understanding of its capabilities and clarify the limitations of the current approach in relation to broader applications.