ID: 5UW6Mivj9M
Title: Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach, CEMAL, for distilling math word problem-solving capabilities from large language models (LLMs) into smaller, more efficient student models. The authors utilize LLMs as math tutors to generate customized exercises based on students' learning statuses and feedback, drawing on principles from educational science such as knowledge tracing and personalized learning. The evaluation across multiple math word problem datasets demonstrates that CEMAL outperforms fine-tuned and knowledge distillation baselines, achieving competitive results with LLMs, particularly in out-of-distribution settings.

### Strengths and Weaknesses
Strengths:  
- The proposed CEMAL method shows significant performance improvements over existing techniques in both in-distribution and out-of-distribution settings.  
- The incorporation of educational principles like knowledge tracing and personalized learning is innovative and enhances the knowledge distillation process.  
- The experimental evaluation is comprehensive, covering various datasets and modeling approaches.

Weaknesses:  
- The clarity of the manuscript is lacking, making it difficult to follow the experimental procedures and discern the novelty of the contributions.  
- The results' statistical stability is questionable, with a need for standard deviation reporting.  
- The paper mixes methods and results, leading to confusion regarding the descriptions of datasets and their processing.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the manuscript by revising Figure 2 to enhance readability and better convey the workflow. It would be beneficial to clarify the various models used throughout the paper, providing more descriptive names for each approach to aid reader comprehension. Additionally, we suggest that the authors cleanly separate the descriptions of datasets, processing methods, and model evaluations to avoid confusion. Lastly, addressing the statistical significance of the results and discussing potential strategies to enhance the quality of generated problems would strengthen the paper.