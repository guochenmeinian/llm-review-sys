ID: HU2uyDjAcy
Title: Local and Adaptive Mirror Descents in Extensive-Form Games
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 7, 6, -1, -1, -1
Original Confidences: 3, 2, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into learning epsilon-Nash equilibria from trajectory feedback in zero-sum extensive-form games, focusing on a fixed sampling policy framework. The authors propose an algorithm based on online mirror descent, providing near-optimal regret bounds under various learning rate settings. The paper aims to address the high variance issues associated with importance sampling over action sequences, as highlighted by previous works.

### Strengths and Weaknesses
Strengths:  
1. The paper is well-written, with a detailed introduction and literature review that clearly outlines the problem setting and framework.  
2. The proposed algorithm demonstrates optimal dependency on $T$ and near-optimal dependency on other game-related parameters, effectively removing the importance sampling term in the fixed-rate setting.  
3. Empirical results convincingly justify the theoretical claims, with comparisons to benchmark methods.

Weaknesses:  
1. The framework remains confusing, as the theoretical results do not clearly reflect the advantages of the fixed sampling policy, and the regret bounds do not improve upon existing simultaneous regret minimization procedures.  
2. The paper lacks comparisons to additional benchmarks, limiting the understanding of its results within the fixed sampling policy framework.  
3. The motivation for variance reduction is unclear, and the empirical evidence does not sufficiently demonstrate performance gains beyond variance reduction.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the framework and explicitly highlight the advantages of the fixed sampling policy in relation to the theoretical results. Additionally, the authors should include comparisons to more benchmarks to contextualize their findings better. It would also be beneficial to clarify the motivation for variance reduction and enhance the empirical evidence to demonstrate any performance improvements. Lastly, we suggest that the authors address the confusion surrounding the citation of McAleer et al. (2022) and provide grounding for the results of [2] to strengthen the paper's contributions.