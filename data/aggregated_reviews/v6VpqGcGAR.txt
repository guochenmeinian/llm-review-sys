ID: v6VpqGcGAR
Title: Winner Takes It All: Training Performant RL Populations for Combinatorial Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 5, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a multi-decoder neural network structure and training method aimed at solving combinatorial optimization problems. The authors propose a model update method where multiple decoders specialize in different problem instances, yielding promising results in experiments on TSP, CVRP, KP, and JSSP. Additionally, the paper introduces a construction method that learns a population of agents to enhance exploration in the solution space, demonstrating improved solving efficiency for four NP-hard problems.

### Strengths and Weaknesses
Strengths:
- The novel idea of agent populations and the training method for building them is a significant contribution.
- The proposed method shows remarkable accuracy improvements, particularly in fast inference scenarios, outperforming existing methods like POMO.
- The applicability of the method to various combinatorial optimization problems is a notable strength.
- The simplicity and clarity of the paper's presentation enhance its accessibility.

Weaknesses:
- The motivation for the research is unconvincing, lacking a compelling example of combinatorial optimization problems.
- Important recent studies are omitted as baselines in the experimental results, particularly in TSP and CVRP contexts.
- The claim that the method produces complementary policies lacks theoretical or empirical proof.
- The experiments are insufficient, with missing evaluations on larger problem sizes and key baselines.

### Suggestions for Improvement
We recommend that the authors improve the motivation section by providing a more compelling example of combinatorial optimization problems. Additionally, consider including recent studies such as EAS, DPDP, and HGS as baselines in the experimental results to strengthen the comparisons. We also suggest providing theoretical or empirical evidence to support the claim of producing complementary policies. Lastly, expanding the experiments to include larger problem sizes and a time analysis of training costs would enhance the robustness of the findings.