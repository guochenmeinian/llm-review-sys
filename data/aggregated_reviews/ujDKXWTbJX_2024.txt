ID: ujDKXWTbJX
Title: JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 31
Original Ratings: 8, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for synthesizing high-quality mathematical reasoning data using a smaller language model (LLM), specifically DeepSeekMath-7B, instead of relying on larger models like GPT-4. The authors propose a multi-step pipeline that includes distilling GPT-4's capabilities, utilizing gradient-based value estimation for data selection, and synthesizing approximately 6 million QA pairs. The resulting model, JiuZhang3.0, demonstrates state-of-the-art performance across various benchmarks, effectively reducing costs associated with large-scale data synthesis. Additionally, the authors emphasize the importance of high-quality data in the synthesis process and detail their methods for fine-tuning and data selection, including the use of a reward model to evaluate the quality of solutions. The authors also propose scaling the synthetic pre-training data from 4.6 billion to 10 billion tokens, resulting in significant performance improvements for both DeepSeekCoder-v1.5 and Mistral-7B, achieving state-of-the-art results with a fraction of the data typically required by other models.

### Strengths and Weaknesses
Strengths:
- The paper effectively demonstrates the feasibility of training a smaller LLM to replace GPT-4 for synthesizing QA pairs, significantly lowering costs.
- JiuZhang3.0 achieves superior performance compared to previous state-of-the-art methods in both natural language reasoning and tool manipulation settings.
- The extensive ablation studies validate the effectiveness of the proposed pipeline and provide insights into the importance of prompt diversity and data synthesis strategies.
- The authors provide a comprehensive explanation of their data sources and synthesis methods, enhancing the reproducibility of their work.
- The method is cost-effective, utilizing only 4.6 billion tokens compared to 120 billion used by other models, while still achieving competitive performance.
- The public release of synthetic data allows for broader accessibility and potential improvements in LLM training.

Weaknesses:
- The paper does not incorporate continual pre-training, which is crucial for enhancing reasoning abilities, and lacks clarity on its necessity within the proposed framework.
- There is insufficient analysis regarding the performance impacts of different training stages and data selection strategies, particularly concerning the validity of synthesized questions and solutions.
- The complexity of the pipeline, especially the gradient-based value estimation, may lead to confusion regarding the contributions of various components.
- The iterative approach to data synthesis does not yield performance improvements, which raises questions about the necessity of such methods.
- Concerns remain regarding the generalization capabilities of the models, particularly with in-domain samples, and improvements in certain benchmarks were not as significant as expected.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the necessity of continual pre-training on large-scale synthetic instruction data and its compatibility with existing training pipelines. Additionally, conducting ablation studies to analyze the impact of continual pre-training on domain-specific corpora and the scaling behavior of the knowledge distillation dataset size would enhance the robustness of the findings. It would also be beneficial to provide a more detailed explanation of the data synthesis model's capabilities and the loss calculation method used during continual pre-training. Furthermore, we suggest improving the title to better reflect the framework's focus on data generation rather than the model itself. We encourage the authors to clarify the masking application in the SFT process to ensure consistency across their experiments and to evaluate their models on out-of-domain tasks to improve generalization testing. Finally, addressing the concerns regarding the effectiveness of the method on specific datasets could enhance the overall impact of the work.