ID: wbbTqsiKzl
Title: High-dimensional Asymptotics of Denoising Autoencoders
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 7, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the performance of denoising autoencoders (DAEs) in high-dimensional settings, specifically trained on data from a Gaussian mixture perturbed by isotropic Gaussian noise. The authors propose one-layer networks with arbitrary activation functions, tied weights, and a skip connection, trained with an $L_2$-regularized loss function. The main results include formulas for the denoising test mean squared error (MSE) for the full DAE and two simpler architectures. The authors analyze the role of each component in DAE performance, finding that the skip connection is crucial and that DAEs can outperform other methods like PCA. The paper also derives the test error for reconstruction autoencoders in the noiseless limit.

### Strengths and Weaknesses
Strengths:
- The authors derive complex asymptotic formulas using the replica method, demonstrating its powerful application in this context.
- Empirical experiments validate the theoretical predictions, highlighting the successful performance of DAEs.
- The paper fills a literature gap, complementing previous results focused on reconstruction autoencoders and infinite data regimes.

Weaknesses:
- The analytical formulas are somewhat obscure, and many observations could be inferred from empirical results without relying on them.
- The optimal MSE error grows with data dimension, yet the gap between DAE and PCA remains constant, raising questions about the essential differences between the two methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the analytical formulas and emphasize the unique insights derived from them. Additionally, please clarify if empirical results align with replica predictions when the number of neurons $p > 1$. We also suggest providing a rigorous derivation of the convergence to the oracle denoiser as indicated in Figure 3, and addressing the implications when $K = 1$. Lastly, consider dedicating more space to make theoretical concepts more accessible to a broader audience.