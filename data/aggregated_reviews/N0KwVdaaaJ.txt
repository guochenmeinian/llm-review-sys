ID: N0KwVdaaaJ
Title: Theoretical Analysis of the Inductive Biases in Deep Convolutional Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 7, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of the sample complexity of Convolutional Neural Networks (CNNs) compared to Fully Connected Networks (FCNs) and Locally Connected Networks (LCNs). The authors demonstrate that CNNs reduce sample complexity from quadratic to linear due to local connections, and further to logarithmic through weight sharing. They explore the inductive biases in CNNs, showing that a depth of \(O(\log d)\) is sufficient for universality and that CNNs can learn sparse functions efficiently. However, this claim regarding the necessity of depth for universality is contested by reviewers who reference prior works, suggesting that universality may be achieved under different conditions. The authors clarify their training methodology for FCNs and CNNs, addressing concerns about overfitting and generalization.

### Strengths and Weaknesses
Strengths:
- The paper provides a solid theoretical framework, revealing the roles of multichanneling and downsampling in CNNs.
- The results, particularly regarding the sample complexity of learning sparse functions, are novel and insightful.
- The clarity of presentation and comprehensive appendix enhance the understanding of the concepts.
- The authors offer a clear rebuttal to concerns about their methodology and the universality claim, providing valuable insights into the differences in sample complexity between CNNs, LCNs, and FCNs.

Weaknesses:
- The significance of the theoretical results is questioned, as empirical analyses of sample complexity benefits from weight sharing and locality already exist.
- The paper lacks empirical validation of its theoretical findings, which diminishes its practical relevance.
- There are concerns regarding the clarity of certain technical aspects, such as the assumptions in Theorem 4.6 and the implications of using exponentially increasing channels.
- The claim regarding the necessity of depth for universality is seen as overstated and not sufficiently supported by existing literature.
- The response lacks clarity on several questions raised in the original reviews, particularly regarding the novelty of their findings.

### Suggestions for Improvement
We recommend that the authors improve the paper by validating their theoretical results with experimental evidence, particularly using real data to demonstrate the claimed sample complexities. Additionally, it would be beneficial to clarify the assumptions made in Theorem 4.6 and provide a more intuitive explanation of the symmetry-based lower bounds discussed. The authors should also consider addressing the relevance of recent advancements in Transformer models in relation to CNNs, as this could enhance the paper's significance in the current landscape of deep learning. Furthermore, we suggest that the authors improve the clarity of their claims regarding universality by downplaying the assertion that their work is the first to establish this requirement. It would also be beneficial to address all outstanding questions from the original reviews, particularly those related to the novelty of their results. Finally, the authors should ensure that their discussion includes relevant prior works to provide a more comprehensive context for their findings and highlight the technical challenges and methods for overcoming them in the main text to strengthen the generalization analysis.