ID: EEtJTfvNZx
Title: Harnessing the power of choices in decision tree learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 8, 7, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel algorithm called Top-K for decision tree learning, which balances traditional greedy methods and optimal decision tree construction. The authors propose a parameter \( k \) that allows consideration of the top \( k \) features for splits at each node, enabling the exploration of exponentially many trees. Theoretical results, including the greediness hierarchy theorem, demonstrate that increasing \( k \) can improve accuracy in certain data distributions. Additionally, the paper explores optimal decision tree (ODT) search methodologies, highlighting the efficiency of modern solvers and various formulations of the optimal decision problem. The authors emphasize the scalability and theoretical motivations of the Top-K algorithm compared to existing methods like Optimal Sparse Decision Trees (OSDT) and greedy algorithms, while addressing the limitations of lookahead approaches in decision tree construction.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel contribution to decision tree learning, with a theoretically sound algorithm that addresses major issues in existing approaches.
- It provides a comprehensive overview of existing decision tree methodologies and situates the proposed Top-K algorithm within this context.
- The authors present solid theoretical motivations for their approach, which is backed by empirical evidence.
- The focus on scalability and efficiency in high-dimensional datasets is a significant contribution.
- The paper is well-written, offering a thorough literature review and comprehensive experimental evaluations that support the proposed method.

Weaknesses:
- The novelty of the idea is somewhat limited, as it does not present a strikingly new concept.
- The paper primarily focuses on binary classification with binary features, raising questions about its applicability to non-binary data and regression settings.
- The related work section is brief and lacks depth, particularly in accurately representing the history and context of sparsity penalties in decision trees.
- The experimental design raises concerns regarding the representativeness of the datasets used, as conclusions drawn from only 20% of the datasets may not be robust.
- Comparisons with existing algorithms, particularly DL8.5, lack clarity and may mislead readers regarding the generalization capabilities of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the related work section by providing a more in-depth discussion of how their approach relates to existing algorithms, particularly in the context of parameterized complexity and the historical context of sparsity penalties. Additionally, addressing the limitations regarding the applicability of the algorithm to non-binary and regression settings would enhance the paper's impact. We suggest conducting more thorough experiments across a more comprehensive dataset rather than a cherry-picked subset to substantiate claims about the algorithm's performance. Furthermore, clarifying the implications of the binarization process on accuracy and exploring potential optimizations for the Top-K algorithm would strengthen the overall contribution. Finally, it would be beneficial to clarify the comparison with DL8.5, particularly regarding out-of-sample accuracy, and to refine the theoretical contributions to explicitly indicate how they inform practical applications of regularization in decision trees.