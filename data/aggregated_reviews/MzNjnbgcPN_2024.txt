ID: MzNjnbgcPN
Title: OptEx: Expediting First-Order Optimization with Approximately Parallelized Iterations
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 6, 3, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework, OptEx, aimed at accelerating first-order optimization methods by leveraging approximately parallelized iterations. The authors propose using kernel methods to estimate gradients from previous iterations, thereby breaking the serial dependencies typical of stochastic optimization. The paper establishes theoretical guarantees indicating an acceleration rate of $\sqrt{N}$ with $N$ parallel computations and supports these claims with extensive empirical studies across various domains.

### Strengths and Weaknesses
Strengths:
1. The framework's introduction is novel and addresses significant inefficiencies in traditional optimization methods.
2. Robust theoretical guarantees are provided, including bounds on estimation error and iteration complexity.
3. Extensive empirical results demonstrate the practical applicability and efficiency gains of OptEx.

Weaknesses:
1. The complexity of kernelized gradient estimation may hinder practical implementation and understanding for a broader audience.
2. The theoretical guarantees depend on assumptions that may not hold in all scenarios, potentially limiting the generality of the results.
3. The empirical studies do not sufficiently explore large values of $N$, which limits the demonstration of the framework's full acceleration potential.

### Suggestions for Improvement
We recommend that the authors improve clarity in the presentation of the kernelized gradient estimation and its practical implications to facilitate understanding. Additionally, the authors should address the limitations of their theoretical guarantees by discussing scenarios where the assumptions may not hold. It would be beneficial to include comparisons with existing parallelization methods, such as mini-batching and model averaging, to contextualize the advantages of OptEx. Finally, we suggest that the authors clarify the definition of the next iterate in their algorithm and ensure that the convergence theory aligns with standard results in the literature.