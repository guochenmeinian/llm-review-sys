ID: dxyNVEBQMp
Title: Introducing Spectral Attention for Long-Range Dependency in Time Series Forecasting
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Spectral Attention mechanism aimed at addressing long-term dependencies in time series forecasting. By leveraging frequency domain information and preserving temporal correlations, the authors propose a method that enhances the forecasting performance of various baseline models. The paper claims to achieve state-of-the-art results across multiple real-world datasets, while also facilitating parallel training across multiple timesteps.

### Strengths and Weaknesses
Strengths:
- The proposed Spectral Attention mechanism significantly improves forecasting performance across multiple datasets.
- The method is versatile and can be integrated into various baseline models, demonstrating broad applicability.
- Extensive experimental validation shows consistent performance improvements.

Weaknesses:
- The writing and logical presentation require strengthening, making some sections difficult to understand, particularly regarding the formulations and figures.
- The novelty of the work is questionable, as similar concepts exist in prior literature that are not adequately referenced.
- The computational complexity analysis is superficial; a detailed comparison of training and inference times, as well as memory usage, is needed.
- The integration process of Spectral Attention with different models lacks specific guidelines, which could hinder practical application.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing and logical flow, particularly in the presentation of formulas and figures. Additionally, it is crucial to include a thorough discussion of related works, particularly the existing paper on "Spectral Attention," to clarify the novelty and contributions of their method. A detailed analysis of computational complexity and memory requirements should be provided to substantiate claims of minimal overhead. Furthermore, we suggest including specific guidelines for integrating Spectral Attention with various architectures to facilitate its application in practice.