ID: pJlFURyTG5
Title: Scalable Constrained Policy Optimization for Safe Multi-agent Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on constrained multi-agent reinforcement learning (MARL) in a cooperative setting, focusing on decentralized learning without global observability. The authors propose a constrained policy optimization method and its practical version, Scal-MAPPO-L. Theoretical results are established regarding dynamics/policy truncation and trust-region subproblems, validated through numerical experiments.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized and complete in structure.
- It derives a monotone improvement property in the exact setting without parameterization.
- The empirical algorithm, Scal-MAPPO-L, is reasonably compared with other PPO family algorithms, demonstrating decent performance.

Weaknesses:
- The novelty of the paper is questionable due to similarities with existing literature, particularly [10], raising concerns about its technical contributions.
- The introduction of the spatial decay of correlation is abrupt, lacking prior mention of the agents' graph structure.
- The empirical algorithm Scal-MAPPO-L lacks theoretical guarantees, and the experimental corroboration does not clarify the computational/communication burden compared to MAPPO-L.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the technical contributions of the paper relative to [10], specifically addressing the challenges faced when extending results to the current decentralized setting. Additionally, the authors should provide more robust justifications for why their methods outperform existing literature [11] and [12]. Clarifying the empirical algorithm's performance guarantees under additional assumptions would also enhance the paper's rigor. Furthermore, we suggest introducing the notion of the agent graph earlier in the paper to aid comprehension. Lastly, addressing the clarity of the assumptions and their relation to previous works, as well as providing a code repository for reproducibility, would significantly strengthen the paper.