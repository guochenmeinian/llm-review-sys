ID: oNuam8eFz2
Title: Particle-based Variational Inference with Generalized Wasserstein Gradient Flow
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 6, 5, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 5, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to compute gradient flows of the KL divergence using a generalized Wasserstein distance, termed Generalized Wasserstein Gradient Flows (GWGF). The authors provide a forward Euler scheme for this gradient flow, noting that the choice of Young function affects convergence rates. They propose an algorithm utilizing neural networks to solve GWGF and adapt the Young function for improved convergence. The paper includes various applications and comparisons with methods like SVGD, demonstrating consistently superior results.

### Strengths and Weaknesses
Strengths:
- The paper is clear and presents an appealing idea of gradient flows in optimal transport problems with general costs.
- It includes a good theoretical analysis and numerous applications with comparisons to other methods.
- The proposed algorithm shows strong convergence guarantees and improved performance over existing methods.

Weaknesses:
- The class of Young functions explored is limited, primarily focusing on $c'(x,y)=\|x-y\|_p^p$ for $p\in [1.1,4]$, leaving out potentially interesting costs.
- The assumptions regarding the neural network's ability to learn vector fields are significant, yet their realism in practical scenarios is questionable.
- The title is too similar to existing works, and the implementation details are lacking.

### Suggestions for Improvement
We recommend that the authors improve the exploration of Young functions by testing with less common costs to enhance the generality of their findings. Additionally, clarifying the assumptions regarding the neural network's learning capabilities and providing empirical evidence to support these assumptions would strengthen the theoretical foundation. The authors should also consider revising the title to better reflect the contributions and include more intuitive illustrations of the method. Finally, providing implementation details and addressing the clarity of main results, particularly regarding how the vector-valued function is learned, would enhance the paper's overall quality.