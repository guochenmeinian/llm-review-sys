ID: muFvu66v7u
Title: DP-SGD Without Clipping: The Lipschitz Neural Network Way
Conference: NeurIPS
Year: 2023
Number of Reviews: 22
Original Ratings: 5, 4, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the use of Lipschitz constrained networks as a replacement for clipping functions to limit gradient sensitivity in DP-SGD. The authors propose that Lipschitz constrained networks can mitigate the detrimental effects of clipping on convergence and performance. They introduce a method to replace the Vector-Jacobian product with a Scalar-Scalar product, significantly improving computational efficiency. Additionally, the paper presents an analysis of gradient norms in the context of differential privacy, focusing on their behavior during training and implications for model performance. The authors propose a method that aims to improve the signal-to-noise ratio by eliminating per-sample gradient computations, allowing for larger batch sizes. However, concerns are raised regarding the noise introduced by their method relative to gradient norms, particularly in comparison to existing techniques like adaptive clipping.

### Strengths and Weaknesses
Strengths:
- The concept of eliminating clipping is promising, addressing known issues with DP-SGD's convergence and performance.
- The proposed method demonstrates a significant speed advantage over existing SGD approaches, which is crucial given the inefficiencies of DP-SGD.
- The writing is clear, and the idea is interesting.
- The experiments are extensive, and the publication of the library is commendable.
- The authors acknowledge the need for additional figures to illustrate gradient norm distributions during training.

Weaknesses:
- The effectiveness of "Clipless DP-SGD" compared to "Clip DP-SGD" in terms of training stability and performance remains uncertain.
- The paper lacks clarity on the upper bounds of gradient norms, which should have been included initially.
- The method requires adding significant noise relative to typical gradient norms, which may hinder practical utility.
- The comparison with existing methods, particularly regarding performance metrics, raises concerns about the claimed improvements.
- The paper contains several minor typographical errors and inconsistencies in figure references.
- The experimental results do not convincingly demonstrate that the proposed method outperforms existing approaches in validation accuracy.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their presentation by correcting typographical errors and ensuring consistency in figure references. Additionally, we urge the authors to provide mathematical bounds to substantiate the effectiveness of their methods and to include further experimental results beyond Figure 5 to validate their claims. We also suggest that the authors improve the clarity of the upper bounds by including them in the main text, as this is crucial for readers to assess the method's effectiveness. Furthermore, we encourage the authors to provide a comparison of the performance of DP-SGD with clipping versus Clipless DP-SGD on the same plot for the GNP networks. To address concerns about noise degrading performance, we recommend that the authors explore the integration of adaptive clipping within their framework, as it could enhance the signal-to-noise ratio while maintaining compatibility with their approach. Lastly, a more detailed explanation of the implementation of GNP networks in DP-SGD is necessary to clarify how the proposed methods address potential issues.