ID: PFOVYrD6fj
Title: GraphSHINE: Training Shift-Robust Graph Neural Networks with Environment Inference
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GraphSHINE, a novel approach for training shift-robust graph neural networks (GNNs) to address performance degradation caused by out-of-distribution (OOD) testing nodes. The authors propose a method that includes an environment estimator and a mixture-of-expert GNN predictor to mitigate confounding biases from the environment. The experimental results demonstrate significant performance improvements over state-of-the-art methods, validating the effectiveness of the proposed approach.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, clear, and easy to follow.
- It addresses a timely topic with strong motivation and presents a novel method.
- Extensive experiments validate the proposed method across various types of graph data.

Weaknesses:
- The paper lacks experiments illustrating the types of environments the model can infer.
- Important related works on OOD problems in node-level tasks are missing.
- The introduction could benefit from a real-world example from a graph-structured data perspective.
- The reliance on inferred pseudo environment labels may affect the method's effectiveness.
- The complexity of the Mixture-of-Expert model may lead to higher computational costs without comparative analysis.

### Suggestions for Improvement
We recommend that the authors improve the introduction by incorporating a real-world example relevant to graph-structured data. Additionally, conducting experiments to illustrate the types of environments the model can infer would enhance the paper's clarity. The authors should also include comparisons of various prior distributions to elucidate the impact on performance. Furthermore, addressing the potential computational costs associated with the Mixture-of-Expert model and providing results on time and memory costs would strengthen the analysis. Lastly, correcting citation errors and including missing important baselines would improve the overall quality of the paper.