ID: Eewh7sl0Xj
Title: SKI to go Faster: Accelerating Toeplitz Neural Networks via Asymmetric Kernels
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 6, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Toeplitz matrix architecture designed for sequence modeling, introducing two variations: a fast version optimized for bi-directional tasks and a Fourier-based model advantageous for causal tasks. The architecture aims to enhance training speed and marginally improve performance. The authors propose modifications to the Toeplitz Neural Operator (TNO) to reduce computational complexity, specifically addressing the inefficiencies of the relative positional encoder (RPE) and the FFT in the context of sequence modeling. Additionally, the authors introduce a theoretical framework for improving computational efficiency in neural networks by utilizing fixed learned kernels in the frequency domain, which allows for significant speed-ups in processing time, particularly in Toeplitz matrix calculations. They acknowledge that while their theoretical results are not tight, they provide a foundational understanding for the practical application of their method and discuss the nuances of their theoretical statements in relation to discrete and continuous signal representations.

### Strengths and Weaknesses
Strengths:
- The paper effectively speeds up an alternative architecture to transformers, which is a relevant topic in NLP.
- The approximations to the Toeplitz Neural Network (TNN) demonstrate effectiveness in experiments, leading to faster networks without performance deterioration.
- The theoretical framework is grounded in established concepts, and the presentation is mathematically rigorous.
- The authors provide rigorous proofs for their theoretical statements and contextualize them within their method.
- They acknowledge the limitations of their theoretical results and propose clarifications to enhance understanding.
- The approach of using fixed learned kernels in the frequency domain is innovative and has potential practical applications.

Weaknesses:
- Significant concerns exist regarding the theoretical claims, with some theorems lacking clarity or being potentially incorrect.
- The experimental results, particularly in Table 1, do not convincingly demonstrate improvement, and the choice of tasks for comparison is questionable.
- The paper lacks detailed speed benchmarks and does not adequately motivate the need for speed improvements in TNNs.
- Some theoretical statements, such as Proposition 1 and Theorem 1, lack clarity and may not require formal proof.
- Theorems 2 to 4 are criticized for being couched in confusing language, obscuring the main message.
- The assumption of sending sequence lengths to infinity during inference is deemed unrealistic and may not reflect practical scenarios.

### Suggestions for Improvement
We recommend that the authors improve the clarity and rigor of the theoretical claims, particularly by addressing the concerns raised about the theorems and propositions. Specifically, we suggest removing or revising the formal statements that may not be rigorously proven and replacing them with heuristic justifications. We also recommend improving the clarity of Proposition 1 by moving it to the appendix and mentioning it briefly in the main text. Additionally, we encourage the authors to provide a more detailed explanation of the causal training proposal in section 3.1, including the implications of the Hilbert transform and the role of the $\lambda$ parameter. It would also be beneficial to include a comprehensive speed benchmark across various sequence lengths and devices to substantiate the claimed improvements. Furthermore, we suggest that the authors fully acknowledge the limitations of their theoretical statements, emphasizing that they serve primarily as a justification for the method rather than providing tight bounds. We advise simplifying the language surrounding Theorems 2 to 4 to enhance comprehension and reconsidering the assumptions regarding sequence lengths during inference to align more closely with practical applications. Lastly, we encourage exploring more efficient implementations to reduce CPU overhead in their computations.