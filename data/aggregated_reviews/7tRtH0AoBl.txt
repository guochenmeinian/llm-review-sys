ID: 7tRtH0AoBl
Title: Randomized Exploration for Reinforcement Learning with Multinomial Logistic Function Approximation
Conference: NeurIPS
Year: 2024
Number of Reviews: 21
Original Ratings: 6, 4, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two randomized reinforcement learning algorithms, RRL-MNL and ORRL-MNL, for the multinomial logistic (MNL) transition model, achieving both computational and statistical efficiency without stochastic optimism. The authors propose the first frequentist regret analysis for reinforcement learning with non-linear function approximation, addressing limitations of linear MDPs. The complexity of ORRL-MNL shows improved dependence on the problem-related constant $\kappa^{-1}$ compared to RRL-MNL. The work provides several regret bounds and demonstrates the algorithms' performance through extensive numerical simulations, validating their superiority in MNL-MDPs, particularly as the number of states increases.

### Strengths and Weaknesses
Strengths:
1. The MNL parameterization is significant and convenient, ensuring the transition kernel is a probability distribution.
2. The proposed algorithms are computationally more efficient than existing methods and do not require prior knowledge of $\kappa$.
3. The algorithms are statistically efficient, supported by experimental results that showcase superior performance.
4. The paper provides a novel frequentist regret analysis without additional assumptions.

Weaknesses:
1. Some algorithmic details are unclear and would benefit from more intuitive explanations, particularly regarding Assumption 4 and the selection of $L_{\theta}$.
2. The regrets of both proposed algorithms exceed $\widetilde{O}(dH^{\frac{3}{2}}\sqrt{T})$ of previous work.
3. The assumption of known reachable states is seen as a significant limitation, and concerns regarding the computational efficiency of the algorithms, particularly in relation to the size of the state space, remain unresolved for some reviewers.
4. The paper lacks sufficient discussion on the MNL-MDP setting, limiting its perceived usefulness.
5. The theoretical analysis, particularly in the appendix, is difficult to read due to long equations and missing explanations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of algorithmic details by providing more intuitive explanations, especially regarding Assumption 4 and the selection of $L_{\theta}$. Additionally, it would be beneficial to enhance the discussion on the MNL-MDP setting to demonstrate its relevance and address the implications of the known reachable states assumption. We suggest incorporating proof sketches to clarify the contributions and addressing the computational efficiency concerns more thoroughly, particularly in relation to the planning process and comparisons with model-free algorithms. Lastly, we recommend improving the formatting of equations in the appendix for better readability and ensuring that all hyperparameters necessary for reproducibility are clearly listed in the main text or appendix.