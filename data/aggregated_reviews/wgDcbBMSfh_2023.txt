ID: wgDcbBMSfh
Title: CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 7, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
The paper presents CROSSCODEEVAL, a benchmark dataset for code completion that emphasizes cross-file contextual understanding, addressing limitations of existing datasets that focus solely on single-file contexts. The dataset, derived from real-world Python and Java repositories, includes code prompts that necessitate local API usage defined within the repository. The authors acknowledge the challenge of defining logical similarity across code fragments and focus on custom method invocations for identifying dependencies. Experimental results indicate that state-of-the-art models, such as CodeGen and StarCoder, struggle without cross-file context but perform significantly better when it is included. The authors have clarified the terms "RG" and "Oracle RG" to "Cross-file Context" and "Oracle Context," and updated relevant sections and figures for better understanding. They expanded Section 4.1 to discuss existing benchmarks and added a limitation section in the Supplementary Materials. The authors agree on the importance of incorporating more programming languages and have evaluated recent state-of-the-art (SOTA) models, including GPT-3.5, StarCoder, and CodeGen2.5, yielding valuable insights into their performance.

### Strengths and Weaknesses
**Strengths:**
1. **Novelty**: CROSSCODEEVAL fills a critical gap by providing a benchmark for multi-file code completion, enhancing the realism of evaluations.
2. **Comprehensive Evaluation**: The paper evaluates popular models across varying levels of cross-file context, yielding valuable insights into their performance.
3. **Quality Control**: The dataset is carefully curated to avoid data leakage and ensure quality through static analysis and manual verification.
4. **Clarity in Revisions**: The revisions, including updated figures and clearer terminology, enhance the clarity and understanding of the methodology.
5. **Proactive Response to Feedback**: The authors' proactive approach to addressing reviewer feedback demonstrates a commitment to improving the manuscript.

**Weaknesses:**
1. **Limited Scope**: The dataset currently focuses on line/statement completion, restricting its applicability to more complex tasks like entire method completion.
2. **Outdated Model Evaluation**: The evaluation lacks assessments of newer models, such as OpenAI GPT-3.5, which could provide a more comprehensive performance overview.
3. **Lack of Execution-Based Metrics**: The evaluation relies on Exact Match and Edit similarity metrics, which do not capture execution-based correctness, limiting the assessment of model performance.
4. **Unresolved Challenges**: The challenge of defining logical similarity across code fragments remains unresolved, and the reliance on random code snippets for context was shown to be ineffective, indicating a potential gap in methodology.

### Suggestions for Improvement
The authors should expand the dataset to include more challenging tasks, such as entire method completion, to enhance its versatility. They are encouraged to include evaluations with newer models like OpenAI GPT-3.5 to provide a more current assessment of state-of-the-art performance. Additionally, incorporating execution-based metrics would strengthen the evaluation by assessing functional correctness and performance. The authors should explore more robust methods for defining logical similarity across code fragments to enhance dependency identification. They are also recommended to consider alternative approaches to utilizing cross-file context beyond random selection to improve the effectiveness of their methodology. Lastly, the authors should continue to refine the clarity of their explanations, particularly in Sections 2.2 and 3.3, to ensure that all technical aspects are easily understood by the audience. Expanding language support beyond Python and Java would further broaden the benchmark's applicability.