ID: JNd6XPdaXj
Title: Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge in Foundation Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper evaluates Encyclopedic Knowledge Retrieval from LLMs using a manually curated dataset across 20 languages. The authors conclude that LLMs exhibit a decline in performance on non-English datasets, particularly those in non-Latin scripts. The study builds on previous work and includes a multilingual dataset that is valuable for further research.

### Strengths and Weaknesses
Strengths:  
- The study is well-conducted, providing clear findings of interest.  
- It offers a multilingual exploration of LLM performance, contributing a new dataset for future experimentation.  
- The analysis includes a section on subject entity error analysis and employs contrastive learning methods.  
- The results are reproducible, and the code and data are publicly available.  

Weaknesses:  
- The paper lacks depth in explaining the reasons behind the experimental results.  
- Some aspects, such as the prior/baseline ratio of true/false questions, could have been explored further.  
- There are broken hyperlink references and formatting issues, such as the need for commas in large numbers for readability.  

### Suggestions for Improvement
We recommend that the authors improve the discussion on the reasons behind the results obtained, particularly regarding cultural and language-specific knowledge. Additionally, consider including cross-lingual experiments to assess model predictions across different languages. Clarification is needed on the assessment of removed stem/fact pairs in the dataset. Lastly, please address the formatting issues, including fixing broken hyperlinks and enhancing the readability of numerical data.