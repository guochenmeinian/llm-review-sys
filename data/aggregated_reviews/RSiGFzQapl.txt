ID: RSiGFzQapl
Title: Bridging the Divide: Reconsidering Softmax and Linear Attention
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper addresses the computational inefficiency of Softmax attention in Vision Transformers, particularly with high-resolution inputs. The authors provide a theoretical analysis demonstrating that the injectivity and local modeling capabilities of attention mechanisms significantly affect performance. They show that linear attention, which has linear complexity, is not injective and thus performs poorly compared to Softmax attention. To remedy this, the authors propose modifications to make linear attention injective, resulting in InLine Attention, which improves performance in vision tasks while maintaining computational efficiency. Experiments indicate that InLine attention achieves comparable performance to Softmax attention.

### Strengths and Weaknesses
Strengths:
- The paper presents a solid theoretical analysis explaining the performance gap between linear and Softmax attention, focusing on injectivity and local modeling capabilities.
- The proposed modifications to linear attention are simple yet enhance the performance and computational efficiency of Vision Transformers.
- The findings regarding linear attention's non-injectivity and local modeling issues are well-supported by empirical evidence.

Weaknesses:
- The novelty of the paper is limited, primarily focusing on the injective property of Softmax and linear attention without sufficient acknowledgment of prior work.
- There is no ablation study on different embedding functions for InLine attention, which could strengthen the method's justification.
- The lack of experiments on language models raises concerns about the method's adaptability beyond vision tasks.
- The performance of the proposed method is not significantly superior to existing models, and comparisons to closely related works are insufficient.

### Suggestions for Improvement
We recommend that the authors improve the acknowledgment of prior work on local modeling capabilities and provide a more comprehensive analysis of how InLine compares to methods like FLatten and VVT. Additionally, including an ablation study on different embedding functions could enhance the robustness of their claims. We also suggest conducting experiments on language models to demonstrate the adaptability of InLine attention beyond vision tasks. Finally, clarifying the implications of negative attention scores resulting from the normalization process would strengthen the analysis.