ID: OZ7aImD4uQ
Title: Scale Alone Does not Improve Mechanistic Interpretability in Vision Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 7, 7, 7, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 5, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical evaluation of the interpretability of individual neurons across different model scales, focusing on whether larger models yield greater interpretability. Through large-scale human experiments on Amazon Mechanical Turk, the authors find no evidence that larger models are more interpretable; instead, there is weak evidence suggesting they may be less interpretable. The authors also release their evaluation data to facilitate future automatic interpretability assessments.

### Strengths and Weaknesses
Strengths:
- Sound experimental setup and statistical analysis.
- Addresses an important problem with a novel approach.
- Larger scale than previous studies, reinforcing findings that feature visualizations are less informative than natural images.
- Well-written and organized, with a good discussion of related work.

Weaknesses:
- The sample size of 84 neurons per model may be insufficient for meaningful comparisons, particularly across layers in models like ResNet-50.
- The experimental focus could be narrowed to fewer models or layers to enhance clarity.
- The choice of top-5% and top-15% activations may be too broad; results in the top-1% range would be more insightful.
- The hypothesis that scaling improves interpretability lacks substantial new methods or findings.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by increasing the number of neurons sampled per model to allow for more robust statistical significance. Additionally, consider narrowing the focus to fewer models or layers to enhance the clarity of results. The authors should also explore using top-1% activation thresholds for a more nuanced analysis. Furthermore, we suggest elaborating on the implications of mechanistic interpretability in the conclusion, clarifying whether it pertains to neural network design or training objectives.