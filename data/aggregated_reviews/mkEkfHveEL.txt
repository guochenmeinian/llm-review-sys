ID: mkEkfHveEL
Title: Interview Evaluation: A Novel Approach for Automatic Evaluation of Conversational Question Answering Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel evaluation method for Conversational Question Answering (CQA) models, addressing the limitations of existing evaluation techniques that rely on pre-collected human-human conversations. The authors propose an interview evaluation method that generates questions based on model predictions, simulating an interactive dialogue. They introduce three new metrics: Questions Per Round (QPR), Persistent Failure Rate (PFR), and Answer Conversion Rate (ACR), which provide a multi-dimensional analysis of model performance. The experimental results indicate that this approach offers a more comprehensive understanding of CQA model strengths and weaknesses.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, effectively communicating complex ideas and contributing to the accessibility of CQA evaluations.
- The proposed method is novel and practical, reducing the need for costly data collection while considering the dynamics of conversational interactions.
- The introduction of multiple evaluation metrics allows for a nuanced assessment of model performance.

Weaknesses:
- The reliance on large language models (LLMs) for evaluations may introduce biases and overfitting, with insufficient discussion of these challenges.
- The evaluation of diversity in responses is limited to 'exact match' criteria, neglecting other important factors such as question types and context relevance.
- The validity of the question generator's output and the proposed metrics has not been adequately examined.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding potential biases and overfitting associated with using LLMs for CQA evaluations. Additionally, we suggest incorporating a more comprehensive evaluation of diversity that includes various factors beyond exact matches. Furthermore, the authors should provide detailed insights into the human evaluation process to ensure it aligns with the proposed metrics. Finally, including the specific prompt template used in the evaluation would enhance reproducibility and clarity.