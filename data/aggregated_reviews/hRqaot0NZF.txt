ID: hRqaot0NZF
Title: LESS: Label-Efficient and Single-Stage Referring 3D Segmentation
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 7, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Label-Efficient and Single-Stage method for referring 3D instance segmentation, utilizing only binary mask supervision. The authors introduce several modules, including Point-Word Cross-Modal Alignment, Query Mask Predictor, and Query Alignment, to enhance cross-modal feature alignment. Additionally, they implement area regularization loss and point-to-point contrastive loss to mitigate interference from multiple objects and backgrounds. The authors propose a straightforward and effective single-stage coarse-to-fine cross-modal alignment network and a coarse-to-fine background point filtering loss function, aiming to achieve state-of-the-art performance under efficient binary mask supervision. Experimental results demonstrate that the proposed method surpasses existing state-of-the-art techniques on the ScanRefer dataset with binary supervision, supported by extensive ablation studies.

### Strengths and Weaknesses
Strengths:
1. The paper pioneers the single-stage referring 3D instance segmentation task, bridging detection and matching with binary mask supervision, providing valuable insights for multi-modal 3D scene understanding.
2. The design of alignment modules for fine-grained and coarse-grained feature alignment shows a certain level of novelty.
3. The proposed architecture is straightforward and effective, contributing to the field of open vocabulary 3D segmentation.
4. Strong experimental results on the ScanRefer dataset validate the proposed method's effectiveness.
5. The introduction of a coarse-to-fine cross-modal alignment network and filtering loss function is innovative and well-supported by ablation studies.
6. The paper is well-written and easy to follow.

Weaknesses:
1. The methods TGNN and X-RefSeg are not evaluated with the RoBERTa backbone.
2. The rationale behind the lack of performance gains with more queries and layers is not adequately explained.
3. The experiments are limited to the ScanRefer dataset, lacking validation on additional datasets like Nr3D/Sr3D.
4. The final mask prediction in the Query-Sentence Alignment module relies on a weighted sum, which lacks intuitive justification.
5. The paper does not adequately cite related literature, such as Mask3D, which shares similarities with the Query Mask Predictor module.
6. The paper could benefit from a clearer distinction of its contributions compared to existing works, particularly in relation to OpenScene and other referenced studies.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including the methods TGNN and X-RefSeg with the RoBERTa backbone to provide a more comprehensive comparison. Additionally, the authors should clarify the reasoning behind the performance of additional queries and layers, as well as justify the use of a weighted sum in the Query-Sentence Alignment module. It would also be beneficial to validate the method across multiple datasets beyond ScanRefer to enhance the robustness of the findings. Furthermore, we suggest that the authors improve the clarity of their contributions by explicitly highlighting the differences between their work and previous studies, particularly OpenScene, to better illustrate the significance of their single-stage pipeline in 3D referring segmentation. Finally, we suggest that the authors include relevant citations to related works, particularly Mask3D, to strengthen the literature context of their contributions.