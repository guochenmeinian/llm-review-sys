ID: IZzZnp7IUs
Title: Crystal: Introspective Reasoners Reinforced with Self-Feedback
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CRYSTAL, a method for commonsense reasoning that first introspects knowledge relevant to a question and then generates an answer based on this knowledge. The authors propose a two-step approach that utilizes reinforcement learning to refine both the introspection and reasoning processes. The evaluation shows that CRYSTAL outperforms standard supervised fine-tuning methods and chain-of-thought distilled approaches across various commonsense QA tasks. The generated knowledge is found to significantly support the reasoning process, with expert annotations validating its effectiveness.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and thorough, providing extensive technical details.
- The mutual adaptation of knowledge generation and reasoning is a relevant and important research problem.
- The experimental evaluation is comprehensive, demonstrating the method's generalization across diverse tasks.

Weaknesses:
- The evaluation lacks clarity regarding the baseline model used for DirectQA, which is crucial for meaningful comparisons.
- The reinforcement learning component appears to be heavily based on prior work (Rainier), raising questions about the novelty of the approach.
- Comparisons with more fundamental baselines, such as retrieval augmented generators and standard chain-of-thought approaches, are missing.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the evaluation section by explicitly specifying the model used as DirectQA, ideally selecting a state-of-the-art model for comparison. Additionally, we suggest conducting comparisons with retrieval augmented generators and standard chain-of-thought approaches to provide a more comprehensive understanding of CRYSTAL's performance. Furthermore, clarifying the version of the GPT-3 model used for generating silver knowledge and its relation to Rainier's work would enhance the reliability of the results presented.