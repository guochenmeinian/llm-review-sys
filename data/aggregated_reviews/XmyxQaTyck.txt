ID: XmyxQaTyck
Title: Benchmarking the Attribution Quality of Vision Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 6, 6, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an attribution quality metric that advances the state of the art (SOTA) in evaluating attribution methods for deep neural networks (DNNs) in computer vision. The authors propose an evaluation protocol called the In-Domain Single-Deletion Score (IDSDS), which addresses long-standing issues such as alignment of train/test domains and inter-model comparison. Their method effectively evaluates deletion-based metrics, yielding new findings and insights into the accuracy-explainability tradeoff across various attribution methods and model architectures.

### Strengths and Weaknesses
Strengths:
1. The authors present a novel solution to the challenges of out-of-domain (OOD) data generation and the lack of inter-model comparison in existing attribution metrics.
2. The extensive evaluation of 23 attribution methods and the analysis of model architecture effects contribute valuable insights to the field.
3. The empirical findings challenge previous assumptions, demonstrating that intrinsically explainable models outperform standard models in attribution quality.

Weaknesses:
1. The IDSDS protocol sacrifices granularity by evaluating at the patch level rather than the pixel level, which may limit precision.
2. Fine-tuning the model for evaluation may alter its behavior, potentially affecting results, particularly in `Accuracy corrupted`.
3. Some descriptions, such as those for Figure 2 (c) and the definition of `correlation`, lack clarity, which could hinder understanding.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the descriptions and explanations, particularly for Figure 2 (c) and the definition of `correlation`, specifying common metrics like Pearson and Spearman correlation. Additionally, we suggest that the authors include more formal definitions throughout the paper, such as a line describing the Spearman rank order coefficient. Addressing related work that relies on ground truth and exploring other methods beyond ROAR would also enhance the paper's comprehensiveness. Finally, we encourage the authors to provide additional experimental results to demonstrate the similarity between fine-tuned and original models, as this is crucial for validating their assumptions.