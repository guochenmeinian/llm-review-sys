ID: ZdxGmJGKOo
Title: SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 8, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to bilevel optimization in a federated learning environment, introducing SimFBO, which reformulates hypergradient computation as a least squares problem, allowing for an unbiased estimate with only a single communication round per server-side update. The authors also propose ShroFBO, a variant robust to system-level heterogeneity. Theoretical analyses indicate that both algorithms converge with sample complexity comparable to existing federated bilevel schemes while significantly reducing communication overhead. Preliminary empirical results demonstrate the communication efficiency of the proposed methods compared to existing baselines.

### Strengths and Weaknesses
Strengths:
- The paper features a well-motivated and intuitive presentation, clearly discussing the main problem and challenges with existing solutions.
- A critical reformulation of hypergradient estimation using a standard quadratic program is presented, effectively removing bias and communication overhead.
- The generality of the proposed algorithmic framework is highlighted, accommodating various client-side optimizers and system-level heterogeneity.

Weaknesses:
- The proposed framework introduces an increased hyperparameter space, which complicates practical implementations due to the difficulty of estimating optimal hyperparameters efficiently.
- The requirement for second-order derivatives may pose scalability issues, raising questions about the feasibility of developing fully first-order methods within the current framework.
- The manuscript suffers from unclear mathematical notations and numerous typographical errors, impacting overall presentation quality.

### Suggestions for Improvement
We recommend that the authors improve the clarity of mathematical notations and definitions throughout the manuscript to enhance readability. Specifically, ensure that all variables and parameters are clearly defined upon their first use. Additionally, we suggest that the authors address the scalability concerns by exploring the possibility of developing fully first-order methods based on the current SimFBO framework. Furthermore, providing a more detailed explanation of the experimental settings and the impact of hyperparameter tuning on performance would strengthen the empirical results. Lastly, we encourage the authors to include a discussion on the limitations of their work, particularly regarding the assumption of lower-level strong convexity.