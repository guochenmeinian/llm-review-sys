ID: 78yDLKi95p
Title: Language Model Tokenizers Introduce Unfairness Between Languages
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 7, 4, 5, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the disparities in tokenization across different languages in large language models (LLMs). The authors introduce the concept of "tokenizer parity" to measure fairness in tokenization, highlighting significant real-world implications such as increased cost, latency, and reduced content capacity due to varying tokenization lengths. The analysis reveals that the premium for certain low-resource languages can exceed ten times that of English, advocating for the development of multilingually fair tokenizers to mitigate these issues.

### Strengths and Weaknesses
Strengths:
- The paper addresses an important and underappreciated issue in NLP, providing insightful analysis on the effects of tokenizer selection across languages.
- It introduces the novel concept of "tokenizer parity," offering a systematic approach to assess fairness in tokenization.
- The writing is clear and well-structured, with effective examples illustrating the significance of the problem.

Weaknesses:
- The paper lacks a concrete roadmap for achieving multilingually fair tokenizers and does not provide experimental results to support its claims.
- There is insufficient discussion on the potential negative effects of implementing fair tokenizers and the impact of tokenizer disparity on downstream task accuracies.
- The authors do not adequately explore the inherent differences between languages, such as morphological richness, that may contribute to tokenization disparities.

### Suggestions for Improvement
We recommend that the authors improve the discussion on how to achieve multilingually fair tokenizers, possibly by providing detailed recommendations for integrating tokenizer parity into existing NLP pipelines. Additionally, conducting preliminary experiments to evaluate the proposed tokenizer's performance and its impact on high-resource languages would strengthen the paper. Addressing the potential negative effects of fair tokenizers and discussing the implications of tokenizer disparity on downstream tasks would enhance the overall depth of the analysis.