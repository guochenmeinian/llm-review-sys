ID: Mrs9a1XQAp
Title: Beyond Slow Signs in High-fidelity Model Extraction
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 7, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a unified approach to model parameter extraction by integrating two prior works and optimizing efficiency. The authors identify that the neuron wiggle sign extraction method addresses the bottleneck highlighted in Calini's paper. They also propose methods for identifying hard-to-extract neurons and parallelizing the extraction process. Empirical evaluations demonstrate that the unified model extraction method achieves significant speed improvements over previous methods.

### Strengths and Weaknesses
Strengths:
- The authors make a crucial observation that the neural wiggle method alleviates the sign extraction bottleneck.
- They present a unified model extraction method that combines sign extraction and model signature extraction.
- Additional optimization methods are proposed to enhance the speed of the extraction process.

Weaknesses:
- The paper lacks an explicit description of the attack model/threat model.
- The benchmarks utilized are small-scale, raising questions about the intellectual property value of the evaluated models.
- The scalability of the approach to larger models is not fully explored, and potential ethical implications are not thoroughly discussed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the attack model and provide a more detailed discussion on the ethical implications of their work. Additionally, the authors should consider exploring the scalability of their approach to larger, more complex models and include a more robust discussion on potential defenses against their improved extraction techniques. Furthermore, we suggest restructuring the paper to focus more on technical contributions, moving background information to a dedicated section or appendix.