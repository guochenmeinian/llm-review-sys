ID: 9Ihu0VBOTq
Title: Provable Advantage of Curriculum Learning on Parity Targets with Mixed Inputs
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 1, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of curriculum learning, demonstrating that a neural network trained first on "easier" examples can learn a target more efficiently than when trained on "complex" examples. The authors focus on learning $k$-sparse parities from a mixture of uniform and biased distributions, showing that a two-layer neural network trained via a layer-wise curriculum version of SGD can learn these parities in $\tilde O(d)$ steps, while standard SGD requires $\Omega(d^{1+\delta})$ steps. The paper's results indicate that curriculum learning can lead to significant efficiency improvements, particularly in the context of the sparse parity problem.

### Strengths and Weaknesses
Strengths:
- The paper is well written and presents sound proofs, with experimental results supporting the theoretical claims.
- It addresses a common practical procedure—curriculum learning—while providing a theoretical foundation that is currently limited in the literature.
- The results effectively demonstrate the efficiency gains from training on easier samples first.

Weaknesses:
- The novelty compared to prior work, particularly [CM23], is unclear, as the main differences appear to be in the training method and proof techniques.
- The algorithm's restrictions and nonstandard modifications, such as large biases and $\ell_\infty$ projections, require further justification.
- The analysis is limited to a specific learning problem, and assumptions diverge from standard deep learning practices, which may affect the applicability of the results.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the novelty of their work in comparison to [CM23], clarifying how their approach differs significantly. Additionally, we suggest providing more justification for the algorithm's modifications, particularly the choice of large biases and the necessity of $\ell_\infty$ projections. It would also be beneficial to broaden the analysis beyond the specific case of learning $k$-parity and to address the mismatch between theoretical assumptions and experimental setups. Finally, we encourage the authors to clarify the notation used in theorems and ensure that all relevant parameters and initialization methods are clearly defined.