ID: ZbmS3MU25p
Title: CSMeD: Bridging the Dataset Gap in Automated Citation Screening for Systematic Literature Reviews
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 4, 7, 7, 8, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 2, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark dataset for systematic literature reviews (SLR) by aggregating existing datasets, creating a consistent experimental setup, and developing software tools for unified access. The authors identify issues with previous datasets and propose solutions, while also conducting baseline experiments to evaluate various classification methods for citation screening. They further evaluate classification methods across various domains, acknowledging challenges in categorizing SLRs due to overlapping definitions. The authors address the data leakage issue by ensuring that their curated dataset, LivSB, is free from contamination through deduplication and a time-wise split for the new LivSB-FT dataset. They clarify Figure 1 by updating example abstracts and simplifying the figure's content. The authors also discuss the speed of screening algorithms, focusing on accuracy while noting the importance of computational efficiency, and tackle dataset accessibility by opting for open-access publications, which enhances reproducibility.

### Strengths and Weaknesses
Strengths:
- The unification of datasets is a significant contribution, facilitating future research efforts.
- Comprehensive evaluation of classification methods across domains.
- Effective resolution of data leakage issues in the LivSB dataset.
- Improved clarity in Figure 1 with updated examples and simplified content.
- Acknowledgment of the importance of speed in screening algorithms.
- Commitment to using open-access publications for reproducibility.

Weaknesses:
- The contribution is considered incremental and may not meet the publication standards for NeurIPS.
- The writing is dense and lacks clarity, with insufficient formal definitions and task formulations.
- The experimental analysis is superficial, lacking detailed evaluations and addressing data leakage issues.
- Limited categorization of SLRs due to the predominance of DTA and Intervention reviews.
- Potential accessibility challenges stemming from the reliance on open-access publications.

### Suggestions for Improvement
We recommend that the authors improve the overall clarity of the writing by reducing density and providing more formal definitions and task formulations. A separate "task formulation" section should be included to clarify input and output structures. Additionally, the authors should conduct a more thorough analysis of the experimental results, including evaluations across different domains and addressing potential data leakage. It is also crucial to ensure consistency in reported numbers of systematic reviews throughout the paper and to fix any broken links, such as the streamlit app. Furthermore, we recommend that the authors improve the categorization of SLRs by exploring additional classification criteria beyond DTA and Intervention reviews, if possible. Lastly, consider discussing strategies to mitigate the limitations posed by the reduced number of accessible documents in open-access publications.