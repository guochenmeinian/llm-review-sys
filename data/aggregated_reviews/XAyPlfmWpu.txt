ID: XAyPlfmWpu
Title: Binarized Neural Machine Translation
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 8, 6, 5, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel binarization technique for Transformers applied to machine translation, termed Binarized Neural Machine Translation (BMT). The authors address the inflated dot-product variance associated with one-bit weights and activations by incorporating additional LayerNorms and residual connections. Experiments on the WMT dataset demonstrate that a one-bit weight-only Transformer can match the quality of a floating-point model while being 16Ã— smaller. The authors also conduct a scaling law study indicating that one-bit weight Transformers effectively scale and generalize in both in-domain and out-of-domain contexts.

### Strengths and Weaknesses
Strengths:
- A novel binarized NMT model is proposed, potentially beneficial for production servers.
- The scaling factor to mitigate activation variance is simple and effective, and the overall model architecture is convincing.
- The experimental results are robust, showing promising outcomes, particularly in fully binarized settings.

Weaknesses:
- The paper lacks discussion on how training and inference efficiency compares to floating-point models.
- The proposed code should include a README to assist reviewers in running and verifying the main results.
- Binarized activation results remain poor, and the main results are tested only on one benchmark (WMT17 en-de).
- There is insufficient comparison with other quantization methods and a lack of discussion on limitations and variance in experimental results.

### Suggestions for Improvement
We recommend that the authors improve the discussion on training and inference efficiency relative to floating-point models. Including a README in the proposed code would facilitate easier verification of results. Additionally, the authors should address the poor performance of binarized activations and expand the evaluation to include comparisons with other quantization methods. A more thorough discussion of the limitations of the proposed method is also necessary. Finally, we suggest presenting alternative evaluation metrics alongside BLEU to provide a more comprehensive assessment of translation quality.