ID: jTyjwRpLZ5
Title: Stochastic Zeroth-Order Optimization under Strongly Convexity and Lipschitz Hessian: Minimax Sample Complexity
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 6, 5, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on optimizing second-order smooth and strongly convex functions using noisy evaluations of the objective function. The authors provide a tight characterization of the minimax simple regret through matching upper and lower bounds and propose an algorithm that combines bootstrapping and mirror-descent stages. The work is significant in the context of zero-order stochastic optimization, particularly in machine learning.

### Strengths and Weaknesses
Strengths:
- The authors address a relevant problem where higher-order derivatives are unavailable, which is of great interest in machine learning.
- A tight result for suboptimality is achieved, improving upon previous work by Novitskii & Gasnikov (2021) by a factor of \(d^{1/3}\).
- The algorithm employs a novel gradient evaluation procedure that enhances the estimation of gradients and Hessians.

Weaknesses:
- The paper lacks sufficient motivation for the work, particularly regarding the necessity of using two different gradient estimators.
- The references are outdated, primarily citing works from 2021 and earlier, missing recent advancements in zero-order optimization.
- The presentation is unclear, with major results not rigorously stated as theorems, and the necessity of the two stages in the algorithm is not well justified.
- The paper does not include any experimental validation, which is crucial for a NeurIPS submission.

### Suggestions for Improvement
We recommend that the authors improve the motivation section to clarify the importance of their work and the rationale behind using two different gradient estimators. Additionally, the authors should update the references to include recent literature, such as Akhavan et al. (2023) and Gasnikov et al. (2024), to reflect the current state of research in zero-order optimization. We suggest that the authors explicitly state their main results as theorems and provide a clearer explanation of the necessity of the two stages in Algorithm 4. Furthermore, including experimental results to demonstrate the effectiveness of the proposed algorithm would significantly enhance the paper's impact. Lastly, we encourage the authors to consider extending their analysis to higher-order smoothness and other problem settings, such as non-convex functions.