ID: gP4aAi7q8S
Title: CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 8, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new benchmark dataset, CausalChaos!, for causal video question answering (QA) based on the "Tom and Jerry" cartoon series. The dataset emphasizes complex causal reasoning through multi-level answers that include both direct responses and detailed explanations of causal relationships. The authors aim to address limitations in existing datasets, such as surface-level understanding and lack of complexity, by formulating thought-provoking questions that require longer causal chains and incorporating scene and shot changes.

### Strengths and Weaknesses
Strengths:
- The dataset leverages the "Tom and Jerry" cartoon series, providing a unique and challenging resource for studying causal video QA.
- It includes multi-level answers and specifically created hard-negative answers, enhancing its utility.
- The authors ensure high-quality annotations through human involvement, which is particularly valuable in the context of large models.

Weaknesses:
- The quality of explanations in the dataset can be inconsistent and subjective, lacking a clear annotation protocol.
- The improvements in model performance on real-world applications are limited and not well articulated.
- Evaluations do not include larger language models (LLMs) or vision-language models (VLMs), restricting the understanding of the dataset's potential impact.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the evaluation of reasoning (E), specifying whether it is based on free text or multiple-choice answers. Additionally, we suggest providing more detailed interpretations of the results in Table 3 and addressing potential licensing issues related to practical usage of the dataset. To enhance the dataset's reliability, we advise the authors to standardize the quality control of explanations and clarify the annotation protocol. Incorporating evaluations with larger LLMs and VLMs, as well as reporting human performance, would provide a more comprehensive understanding of the dataset's capabilities. Finally, we encourage the authors to analyze the dataset's applicability to real-world scenarios, particularly addressing any biases between cartoon and real-world contexts.