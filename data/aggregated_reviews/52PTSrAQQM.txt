ID: 52PTSrAQQM
Title: Bootstrapping Top-down Information for Self-modulating Slot Attention
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method that enhances object-centric learning by integrating top-down information into the Slot Attention mechanism. The authors propose a "top-down pathway" where slots are quantized into a learnable codebook after an initial Slot Attention iteration, and this information is used to modulate the attention mechanism in a subsequent iteration. The method is evaluated within the DINASOUR framework across several datasets, demonstrating improved segmentation performance compared to previous models.

### Strengths and Weaknesses
Strengths:
- The proposed method effectively incorporates top-down information, which is a significant advancement in object-centric learning.
- The model consistently outperforms DINASOUR and shows insightful analysis of the learned codebook, confirming the motivation behind the approach.

Weaknesses:
- The performance is highly sensitive to the codebook size, and the criteria for selecting the optimal size are not clearly articulated.
- The implementation is limited to DINASOUR, and the method would benefit from being tested with other object-centric learning frameworks.
- The design of the codebook, which includes spatial information in semantic modulation, could be refined to focus solely on semantic data.
- Inconsistencies in experimental data and the classification of spatial modulation as top-down are concerning.

### Suggestions for Improvement
We recommend that the authors improve the clarity around how to choose the best size of the codebook and consider implementing the proposed method with additional object-centric learning frameworks beyond DINASOUR. Additionally, we suggest refining the codebook design to contain only semantic information, enhancing its utility. Addressing the inconsistencies in experimental results and providing a clearer justification for the use of vector quantization would also strengthen the paper. Lastly, including comparisons with related works and discussing the impact of the proposed method on training and testing times would provide a more comprehensive evaluation.