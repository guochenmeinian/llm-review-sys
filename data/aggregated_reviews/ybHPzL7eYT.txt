ID: ybHPzL7eYT
Title: Large Spatial Model: End-to-end Unposed Images to Semantic 3D
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Large Scene Model (LSM), a unified 3D scene understanding framework that integrates multiple vision tasks into a single model. LSM employs pixel-aligned point maps to represent scenes, combining geometric, appearance, and semantic information. Utilizing a Transformer architecture with cross-view and cross-modal attention, the model achieves efficient scene-level 3D semantic reconstruction and rendering in real-time on a single GPU. The integration of a 2D semantic model facilitates open-vocabulary understanding, enhancing its applicability across various real-world scenarios. The authors claim that multitask training allows LSM to outperform state-of-the-art baselines, although this claim lacks sufficient ablation studies.

### Strengths and Weaknesses
Strengths:
- The technical approach is sound, with clear presentation and well-ablated contributions from various modules.
- The model demonstrates high inference efficiency, achieving reconstruction in 0.1 seconds and rendering at 270 FPS.
- LSM reconstructs the underlying 3D representation in a single feedforward pass, improving speed and efficiency for inference.

Weaknesses:
- The paper does not adequately ablate the multi-task learning design choice, leaving questions about the impact of removing certain tasks on overall performance.
- The title "Large Scene Model" may mislead regarding model parameter scaling, as the paper does not focus on scaling model size; a more fitting title could be "Multitask" or "Unified Scene Model."
- The writing quality, particularly in the abstract and methods sections, requires improvement for clarity and coherence.

### Suggestions for Improvement
We recommend that the authors improve the ablation studies to clarify the impact of multitask learning on performance. Additionally, for Figure 5, it would be beneficial to include the source view supplied as input to the model to clarify pose divergence. The authors should also consider revising the title to better reflect the model's focus and capabilities. Furthermore, aligning the training and testing phases with previous methods and providing results on the Replica Dataset would strengthen the evaluation. Lastly, enhancing the presentation of the methods section to introduce modules and their interconnections more cohesively would improve clarity.