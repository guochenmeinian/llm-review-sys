ID: DjwSceRw7B
Title: Macedon: Minimizing Representation Coding Rate Reduction for Cross-Lingual Natural Language Understanding
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach called Macedon for cross-lingual natural language understanding (NLU) tasks, addressing the challenge of training models with limited low-resource language data. The authors propose a method that minimizes representation coding rate reduction to learn language-agnostic representations, facilitating cross-lingual transfer learning. The evaluation of Macedon on three public benchmark datasets—PAWS-X, QADSM, and XNLI—demonstrates its superiority over existing state-of-the-art methods.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel approach, Macedon, which effectively minimizes information loss during representation learning.
- It evaluates the proposed method on three benchmark datasets, providing a comparative analysis with existing methods.
- The writing is clear, making the proposed method accessible to readers.

Weaknesses:
- The paper lacks clarity regarding the base model architecture of Macedon, specifically whether it follows mBERT or another model, and why alternatives like XLM-R were not considered.
- The selection of baseline models is insufficient, as it does not include stronger models such as XLM-R, which limits the comprehensiveness of the evaluation.
- The experimental section is underdeveloped, with a limited number of tasks evaluated.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the base model architecture of Macedon, specifically addressing the choice of mBERT over XLM-R. Additionally, the authors should expand the selection of baseline models to include XLM-R and evaluate their approach on a wider range of multilingual benchmark datasets. Finally, enhancing the experimental section with more comprehensive comparisons and additional tasks would significantly strengthen the paper's contributions.