ID: 4ImZxqmT1K
Title: Learning to Receive Help: Intervention-Aware Concept Embedding Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 9, 9, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents IntCEMs, an extension of Concept Embedding Models (CEMs) designed to enhance the model's ability to respond to external interventions during test time. The authors propose an intervention prediction module that learns to mimic an optimal intervention policy, incorporating a penalty to ensure high accuracy post-intervention. Experiments demonstrate that IntCEMs outperform traditional CEMs and Concept Bottleneck Models (CBMs) across multiple datasets, including MNIST and CUB. Additionally, the authors explore the effectiveness of interventions in models like Hybrid-CBMs and GlanceNets, emphasizing the importance of how information is processed through bottlenecks. They propose that models allowing more information retention post-intervention may perform better, although this is not a sufficient condition, as seen in Hybrid-CBMs. The authors acknowledge that GlanceNets do not utilize unsupervised concepts for prediction, which could lead to challenges during interventions, especially in real-world tasks with incomplete training concepts. They also address concerns about typicality in concept selection and the limitations of their evaluation regarding large datasets and user studies.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and organized, making it easy to follow.
- The proposed method effectively addresses a significant limitation in existing CBMs and CEMs by optimizing for interventions during training.
- Extensive empirical results support the effectiveness of the proposed method, with clear objectives guiding the experiments.
- The authors provide a thorough analysis of the relationship between model architecture and intervention effectiveness.
- They acknowledge and plan to address the issue of information leakage in their revised manuscript, and the empirical observations regarding concept selection are insightful.

Weaknesses:
- A major limitation is the absence of user studies to validate the claims regarding the model's readiness for human intervention.
- The requirement for concept annotations at training time limits the usability of the method, similar to vanilla CBMs and CEMs.
- The experiments are primarily conducted on simpler datasets (MNIST, CUB), lacking real-world complexity and larger datasets that could better demonstrate the method's applicability.
- The paper's focus on a relatively minor problem may limit its broader impact.
- The evaluation lacks additional baselines, datasets, and well-crafted user studies due to space constraints.
- The writing can be confusing at times, particularly regarding the integration of the "learning to receive help" concept and the use of Greek letters, which may hinder readability.

### Suggestions for Improvement
We recommend that the authors improve the paper by conducting user studies to validate the effectiveness of IntCEMs in real-world scenarios, demonstrating how users interact with the model and whether it outperforms baselines in practical applications. Additionally, we suggest exploring the usability of the method in settings without concept annotations, potentially incorporating post-hoc or label-free CBMs. Expanding experiments to include larger and more complex datasets, such as ImageNet or COCO, would strengthen the findings and allow for impactful user studies. We also recommend improving the discussion of information leakage by explicitly stating its implications in the context of IntCEMs. Furthermore, including a short discussion in the limitations subsection about the need for larger-scale studies and user studies would enhance the robustness of their findings. Lastly, we encourage the authors to clarify the narrative around "learning to receive help" and simplify the notation to enhance readability.