ID: HsuDlFYL82
Title: Maestro: Uncovering Low-Rank Structures via Trainable Decomposition
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: 3, 3, 5, 4

Aggregated Review:
### Key Points
This paper presents a novel approach for low-rank layer learning, proposing to embed low-rank structures directly into the training process through a generalized variant of ordered dropout. The authors claim that this method enhances computational efficiency and provides effective rank extraction when combined with Hierarchical Group Lasso. The theoretical support and experimental results lend credibility to the proposed technique, although the evaluation lacks comparisons with multiple approaches, particularly transformer models.

### Strengths and Weaknesses
Strengths:
- The paper is well-written with clear explanations.
- The proposed approach is interesting and novel, showing promising empirical results.
- The integration of low-rank approximation during training is beneficial for in-domain data.

Weaknesses:
- The evaluation is limited, particularly regarding transformer models, and only one approach is compared.
- Marginal gains on CIFAR-10 and potential performance issues with out-of-domain data.
- Notation and mathematical clarity require improvement, with several specific issues noted in the equations.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including multiple comparison approaches, particularly with transformer models, to strengthen the assessment of their technique. Additionally, addressing the concerns related to out-of-domain performance through regularization could enhance the robustness of the approach. We also suggest clarifying the notation and mathematical expressions, particularly in equations 1, 2, and 3, to improve overall clarity. Finally, discussing the benefits of independent rank decomposition for each layer in more detail would provide valuable insights.