ID: iTUlYblV0K
Title: MQuAKE-Remastered: Multi-Hop Knowledge Editing Can Only Be Advanced With Reliable Evaluations
Conference: NeurIPS
Year: 2024
Number of Reviews: 31
Original Ratings: 6, 3, 8, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive audit and correction of the MQUAKE dataset, which is essential for evaluating multi-hop knowledge editing methods in large language models. The authors identify and rectify significant errors, revealing that up to 76% of cases in MQUAKE-T may be contaminated, and propose MQUAKE-REMASTERED, a revised dataset that addresses these issues while maintaining its evaluative capacity. The authors benchmark nearly all existing multi-hop knowledge editing methods on this new dataset, demonstrating that their method, GWalk, significantly outperforms existing methods such as DeepEdit and RAE. They argue that their work provides a reliable evaluation framework for future research and insights into designing better editing models.

### Strengths and Weaknesses
Strengths:
- The authors provide a detailed analysis of errors in the original dataset, quantifying their impact and enhancing the reliability of evaluations with MQUAKE-REMASTERED.
- GWalk demonstrates significant performance improvements over existing methods, contributing valuable insights into model design.
- The benchmarking of several open-source knowledge editing methods on the new dataset yields reliable performance metrics, enhancing understanding within the research community.
- The paper includes a thorough error analysis and quantification, highlighting the authors' contributions to addressing flaws in the MQUAKE dataset.
- Detailed information about the experimental setup aids reproducibility.

Weaknesses:
- The focus on the MQUAKE dataset limits the audience, as other datasets like RippleEval exist that address similar issues.
- The presentation is cluttered, with excessive use of bold text and section titles, making it difficult to follow. The experimental results in Tables 3 and 4 are also hard to interpret.
- The discussion of related works is incomplete, with insufficient references to highly relevant benchmarks, limiting its contextual relevance.
- The contribution appears limited, primarily focusing on fixing one dataset without significant new insights.
- The guidance for future multi-hop editing model design is insufficient, and important technical details are relegated to the appendix.
- Clarity issues persist regarding the verification of the "correctness" of the audited dataset and the determination of "conflicting edits."
- Some reviewers question the novelty of the GWalk method, suggesting that its technical contributions may not be substantial enough for a dedicated paper.
- Concerns about the limited number of methods benchmarked in the initial submission, with many results being out of memory (OOM).

### Suggestions for Improvement
We recommend that the authors improve the paper's structure and clarity by reducing the use of bold formatting and enhancing the presentation of experimental results. A comprehensive review of existing multi-hop editing benchmarks should be included to better contextualize their work and strengthen the motivation for their study. The authors should also expand the discussion of their contributions beyond fixing the dataset to highlight any new insights gained. Additionally, we suggest moving critical technical content from the appendix to the main paper for better accessibility. Lastly, the authors should clarify how they ensure the correctness of the audited dataset, address the concerns raised regarding the evaluation of existing methods, and consider including more recent methods as they become available to strengthen their findings.