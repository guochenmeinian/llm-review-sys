ID: 2ZtGWNn37W
Title: Multi-Fidelity Active Learning with GFlowNets
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 5, 6, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a multi-fidelity active learning scheme utilizing GFlowNets to address scientific discovery challenges, particularly in exploring high-dimensional spaces for novel solutions. The authors aim to efficiently identify high-quality candidates within a budget by leveraging lower-fidelity surrogate models for cost-effective evaluations. The proposed method, MF-GFN, is evaluated against single-fidelity and other multi-fidelity active learning schemes, demonstrating potential advantages in sample efficiency and diversity of high-scoring candidates. The authors propose adaptations to the GFlowNet algorithm, surrogate models, acquisition functions, and oracles, distinguishing their work from Jain et al. (2022). They commit to releasing a complete codebase, including the GFlowNet implementation, which was inadvertently omitted in the initial submission. The authors also plan to enhance their literature review to include relevant multi-fidelity methods.

### Strengths and Weaknesses
Strengths:
- The investigation of multi-fidelity active learning is relevant to various scientific discovery scenarios.
- The application of GFlowNet as an amortized sampler for high-reward samples is innovative.
- The methodology is well-articulated, with a clear algorithmic outline (Algorithm 1) demonstrating the potential for lower acquisition costs.
- The authors demonstrate a clear commitment to reproducibility by promising to release a complete implementation of their code.
- They provide a thorough empirical evaluation of their proposed algorithm, showcasing its contributions to the field.
- The willingness to enhance the literature review indicates responsiveness to reviewer feedback.

Weaknesses:
- The evaluation results are not consistently convincing; MF-GFN does not consistently outperform alternatives, and gains in performance are often minimal.
- The reliance on the highest fidelity oracle for single-fidelity active learning quickly depletes the budget, and a more balanced approach using low-fidelity samples is recommended.
- Comparisons across different examples lack consistency, with varying methods and K values used arbitrarily.
- The computational cost of fitting models and retraining GFlowNet in each iteration is not adequately discussed.
- The impact of batch size on MF-GFN performance remains unexplored.
- The justification for the cost-adjusted utility function lacks empirical evaluation.
- The initial omission of the GFlowNet code may raise concerns about the completeness of the submission.
- The literature review may have lacked sufficient references to existing multi-fidelity active learning research prior to the authors' commitment to expand it.

### Suggestions for Improvement
We recommend that the authors improve the evaluation metrics by incorporating existing multi-fidelity active learning baselines and ensuring consistent comparisons across examples. Further, the authors should consider evaluating single-fidelity active learning at various fidelities to provide a comprehensive performance picture. A more detailed discussion on the computational costs associated with training GFlowNet and the implications of batch size on performance is necessary. Additionally, we suggest providing a clearer justification for the cost-adjusted utility function and conducting empirical evaluations to support its reliability. We also recommend improving the literature review by accurately referencing pertinent existing multi-fidelity active learning research, including BMFAL (Li et al., 2022), D-MFDBAL (Wu et al., 2023), and Gal et al. (2017). Finally, the updated manuscript should clearly articulate the differences between their work and existing literature to strengthen the paper's contribution.