ID: Xyj46OxEhK
Title: Look Ma, No Hands! Agent-Environment Factorization of Egocentric Videos
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 5, 6, 7, 5, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for addressing challenges in using egocentric videos for robotics tasks, specifically focusing on occlusion and visual mismatch between human hands and robot end effectors. The authors propose a factored representation of the scene, separating the agent (human hand) from the environment through a Video Inpainting via Diffusion Model (VIDM). Experiments demonstrate VIDM's effectiveness in enhancing inpainting quality and its utility for various downstream robotics tasks.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and structured, with helpful descriptions of experimental protocols.
- Comprehensive experiments evaluate both inpainting quality and the utility of the proposed representations across multiple downstream tasks.

Weaknesses:
- The current downstream applications are not as convincing; the inpainted environment $I_{t}^{\text{env}}$ appears more critical than the complete factorized representation.
- The technical contribution of VIDM is limited, resembling a basic segment-then-inpaint pipeline without novel modules.
- Lack of comparative experiments with other video-inpainting methods raises concerns about VIDM's inference speed and generalizability.
- The experiments in reward learning and real-world policy learning tasks are limited in scope, making it difficult to assess their generalizability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how the agent representation $I_{t}^{\text{agent}}$ is utilized in downstream tasks, particularly in applications 3, 4, and 5. It would be beneficial to provide more concrete use cases where $I_{t}^{\text{agent}}$ interacts with $I_{t}^{\text{env}}$. Additionally, we suggest conducting an ablation study to identify which components of VIDM contribute most to performance improvements. The authors should also consider including comparisons with established video-inpainting methods to better demonstrate VIDM's advantages. Lastly, expanding the scope of experiments in reward learning and real-world policy tasks could enhance the assessment of the proposed method's effectiveness and generalizability.