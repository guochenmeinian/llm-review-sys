ID: nsupkM0ppH
Title: Watermarking PLMs on Classification Tasks by Combining Contrastive Learning with Weight Perturbation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel technique for watermarking pre-trained language models (PLMs) on classification tasks by combining contrastive learning with weight perturbation. The authors propose a backdoor-based watermarking method that embeds watermarks in PLMs to protect intellectual property, enhancing their value and security. The effectiveness of the method is demonstrated through experiments on various classification tasks, showing a high success rate in ownership verification and robustness against certain attacks.

### Strengths and Weaknesses
Strengths:
- The motivation and research questions are clearly articulated, with a well-described working principle of the proposed method.
- The paper provides a detailed evaluation of both accuracy and ownership verification success rates, supported by multiple baselines and evaluation metrics.
- The structure is clear and logically organized, facilitating reader comprehension.

Weaknesses:
- Experiments are limited to the BERT family of models and text classification tasks, raising concerns about generalizability to other models or tasks.
- The use of rare words as watermark triggers may lack stealth, making it easier for attackers to remove the watermark.
- Certain key points, such as the two loss functions in Equation 3 and notation inconsistencies in Equation 1, require clarification.
- The novelty of the approach is questioned, as similar concepts have been explored in prior research, and the evaluation is restricted to basic datasets.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their method by evaluating it on a broader range of models and tasks, including more diverse BERT-like PLMs such as DeBERTa and ELECTRA. Additionally, we suggest providing a comprehensive evaluation of the method's robustness against various attacks, including those involving distilled models. Clarifying the two loss functions in Equation 3 and ensuring consistent notation in Equation 1 would enhance the paper's clarity. Finally, we encourage the authors to explicitly delineate the unique contributions of their work in relation to existing literature.