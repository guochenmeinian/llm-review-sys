ID: eHF4pDRWjT
Title: Efficient Retrieval with Learned Similarities
Conference: ACM
Year: 2024
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Mixture-of-Logits (MoL) method aimed at enhancing retrieval tasks in recommendation systems and natural language processing (NLP). The authors demonstrate that MoL serves as a universal similarity approximator and introduce a mutual information-based load-balancing loss to improve accuracy. They provide both exact and approximate retrieval algorithms, showing that MoL achieves state-of-the-art performance across various datasets for recommendation and question-answering tasks.

### Strengths and Weaknesses
Strengths:
- The authors prove that MoL is a universal similarity approximator.
- They introduce a new regularization loss to enhance MoL.
- The paper presents new approximations for top-K retrieval using MoL.
- It addresses a significant problem by moving beyond dual encoder and dot product retrieval setups while maintaining efficiency.

Weaknesses:
- The impact of the regularization loss appears minimal based on the ablation study, and there is a lack of reported confidence intervals or statistical testing.
- The datasets used in experiments are relatively small.
- Latency analysis focuses solely on the MoL setup without including baselines.
- The paper lacks examples, making it harder to follow.
- The work is somewhat incremental relative to previous research, necessitating clearer articulation of its innovations and contributions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the theoretical proofs and provide additional context and intuition to aid understanding. It would be beneficial to implement the baselines themselves rather than relying on results from other papers and to check for statistical significance in performance differences. Additionally, we suggest including theoretical latency bounds for the approximate top-K approach in comparison to brute force methods. Finally, the authors should harmonize the number of significant digits in their tables for consistency.