ID: ABYdKpDb8p
Title: Learning Transferable Features for Implicit Neural Representations
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents STRAINER, a novel training framework for Implicit Neural Representations (INRs) that focuses on learning transferable features across similar signals. STRAINER employs shared initial encoder layers across multiple INRs with independent decoder layers, leading to faster convergence and improved reconstruction quality. The framework is evaluated on various tasks, including image fitting, super-resolution, and denoising, demonstrating effectiveness in both in-domain and out-of-domain scenarios. The paper provides detailed analyses of feature transferability and training dynamics.

### Strengths and Weaknesses
Strengths:
- The originality of sharing initial encoder layers marks a significant advancement in enhancing feature transferability in INRs.
- Empirical evaluations are comprehensive, covering multiple datasets and tasks, convincingly showcasing STRAINER's advantages in reconstruction quality and convergence speed.
- The paper is well-structured, with clear methodology and results, supported by visualizations that enhance readability.
- STRAINER addresses a critical limitation in the INR landscape, making it significant for applications like medical imaging.

Weaknesses:
- The paper lacks theoretical insights into why shared encoder layers improve feature transferability, which could strengthen its contributions.
- Occasional stability issues, such as PSNR drops during test signal fitting, are noted but not thoroughly addressed.
- The comparison with other advanced models is limited, missing a broader exploration of related literature on generalizable INRs.

### Suggestions for Improvement
We recommend that the authors improve the theoretical framework by providing insights into the mechanisms behind the shared encoder layers' effectiveness in enhancing feature transferability. Additionally, addressing the stability issues more comprehensively would enhance the robustness of STRAINER. We suggest including comparisons with a wider range of models, such as those utilizing CNNs and Transformers, to better contextualize STRAINER's performance. Furthermore, conducting ablation studies on key hyperparameters, such as the number of shared encoder layers, would provide valuable insights into the model's behavior.