ID: IAAPhOLhcX
Title: How Sparse Can We Prune A Deep Network: A Fundamental Limit Perspective
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 6, 5, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical framework to estimate the fundamental limits of pruning in neural networks, utilizing convex geometry to derive bounds on the pruning ratio. The authors identify weight magnitude, network flatness, and the sum of the squares of the smallest weights as critical factors influencing the pruning ratio. They argue that while different initializations may yield similar empirical distributions of weights, the pruning limits are intrinsic properties determined by the dataset and task. The authors provide computational methods for estimating these bounds and validate them empirically through experiments across image classification tasks. They acknowledge that L1 regularization can promote sparsity but may also lower performance, necessitating comparisons between networks trained with and without this regularization. The authors assert that the bounds provided apply universally, regardless of the loss function used.

### Strengths and Weaknesses
Strengths:
1. The proposed bounds demonstrate that flatter networks allow for more parameters to be pruned, supporting the effectiveness of magnitude-based pruning.
2. The paper makes a significant theoretical contribution to understanding pruning limits in deep neural networks (DNNs).
3. The authors offer algorithms to compute these bounds, which are empirically validated.
4. The theoretical analysis aligns well with empirical results, providing insights into pruning behaviors.
5. The clarification regarding the use of either the proportion of remaining or removed weights as the pruning ratio enhances the theoretical framework.

Weaknesses:
1. The analysis of Figure 4 is unclear; the interpretation of the plots does not convincingly support the benefits of iterative pruning, particularly for partially sparse networks.
2. The dependence of pruning limits on weight initializations is questioned, as Table 2 does not convincingly support this claim.
3. The authors have not experimentally confirmed the impact of L1 regularization on iterative versus one-shot pruning, necessitating a more detailed explanation in Section 7.
4. The impact of L1 regularization on pruning limits remains unclear, and comparisons of performance with and without regularization are necessary.
5. The interchange of terms like pruning ratio and sparsity without clear definitions complicates understanding; specific definitions should be provided.
6. The implications of choosing different values for epsilon in relation to pruning limits and network density require further exploration.
7. The derived bounds depend on the new weights of pruned networks, which raises concerns about their applicability.
8. The paper lacks a comparison with state-of-the-art pruning methods, limiting the context of the proposed bounds.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the analysis in Figure 4 and provide a detailed explanation of how iterative pruning impacts partially sparse networks. Additionally, the authors should conduct experiments to confirm the insights regarding L1 regularization and its effects on pruning strategies, including comparisons of pruning limits and performance for networks trained with and without L1 regularization. We urge the authors to define key terms such as pruning ratio and sparsity more concretely to enhance comprehension. Furthermore, addressing the dependency of the bounds on new weights and including comparisons with existing pruning methods would strengthen the paper's contributions. Lastly, we suggest elaborating on the implications of epsilon values on pruning limits and network density to provide a more comprehensive understanding of their theoretical framework.