ID: YJMUVwLcEi
Title: Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of Lexical Overlap in Train and Test Reference Summaries
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the effect of lexical overlap in training and test reference summaries on summarization model performance and factual errors. The authors demonstrate that increased overlap correlates with higher ROUGE-2 scores, indicating potential memorization by models, which may lead to factual inaccuracies. The experiments, conducted across four summarization datasets and with three state-of-the-art systems, reveal that models trained on overlapping data tend to remember facts rather than infer them. The findings suggest a significant influence of training-test overlap on model performance, with implications for future dataset preparation.

### Strengths and Weaknesses
Strengths:
- The paper provides valuable insights into model behavior influenced by training and testing data, contributing to more informative evaluation methods for summarization tasks.
- The experimental setup is robust, with sound observations supported by multiple analyses across diverse datasets.
- The study addresses a critical issue in generative models regarding hallucinations and factuality, proposing practical solutions through fine-tuning and calibration.

Weaknesses:
- The paper lacks clarity in presenting its findings, with complex variables and confusing graphs that hinder readability.
- The proposed evaluation protocol is not explicitly prioritized, and specific guidelines for practical application are missing.
- Some technical details, such as the detection and classification of entities, are insufficiently explained, and results for two datasets are relegated to the appendix.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by simplifying the presentation of complex variables and ensuring that graphs are more intuitive, possibly by enhancing label significance. It would be beneficial to provide explicit, prioritized guidelines for the proposed evaluation protocol. Additionally, the authors should include more detailed explanations of technical aspects, such as entity detection methods and types, within the main text rather than the appendix. Finally, we suggest that the user study methodology be elaborated to clarify the criteria for relevance and consistency in summary comparisons.