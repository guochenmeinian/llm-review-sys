ID: v091fzXTu0
Title: Controllable Generation via Locally Constrained Resampling
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 8, 6
Original Confidences: 2, 3

Aggregated Review:
### Key Points
This paper presents a probabilistic method for constrained language model generation, addressing issues with greedy constrained generation for autoregressive models. The authors propose a distribution to use with the prior autoregressive distribution and introduce a pseudo-likelihood based sample distribution corrected with importance sampling. Their method is validated through experiments on LLM detoxification and Sudoku, demonstrating its effectiveness.

### Strengths and Weaknesses
Strengths:  
- The solution is well motivated.  
- The writing is clear overall.  
- The proposed methods are effective as shown in the experiments.  
- The authors provide a clear workflow and sufficient background, making the overall framework convincing.

Weaknesses:  
- Minor typos are present, such as on line 197: "fast."  
- The paper lacks a discussion of its limitations and should include a cautionary note regarding potentially misleading information or offensive language.  
- The runtime performance of the sampling process needs to be addressed, particularly whether locally constrained resampling could significantly slow down the process under certain conditions.

### Suggestions for Improvement
We recommend that the authors improve the discussion of the paper's limitations and include a cautionary note for readers. Additionally, we suggest that the authors conduct a detailed experiment on the wall-clock time cost of their method and address the runtime performance of the sampling process.