ID: TjgG4UT62W
Title: Kernel Stein Discrepancy thinning: a theoretical perspective of pathologies and a practical fix with regularization
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 7, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a regularized version of Stein thinning aimed at enhancing the post-processing of outputs from Markov Chain Monte Carlo (MCMC) methods by addressing two significant pathologies associated with kernelized Stein discrepancy (KSD): mode proportion blindness and mode collapse. The authors provide formalizations of these issues (Theorems 2.3 and 2.4) and propose a regularization technique that incorporates entropy and Laplacian terms to mitigate these challenges. The effectiveness of this approach is supported by both theoretical analysis and numerical experiments.

### Strengths and Weaknesses
Strengths:
- **Novelty**: The paper offers formalizations and solutions to acknowledged pathologies in KSD, particularly addressing Pathology II, which has not been previously formalized.
- **Applicability**: It provides thorough discussions and empirical evidence on the impact of the regularization parameter, offering practical guidelines for its selection.
- **Writing**: The paper is well-motivated, clearly written, and logically structured, making it accessible.

Weaknesses:
- **Limitations**: There is a lack of in-depth discussion regarding the limitations of the L-KSD approach, particularly concerning the bias introduced by regularization for finite sample sizes.
- **Clarity**: Some sections require further elaboration, such as the intuition behind the entropic regularization's effectiveness and the interpretation of certain theoretical results.
- **Hyperparameter Tuning**: The introduction of an additional hyperparameter, lambda, lacks a systematic tuning approach, which could hinder practical application.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of the L-KSD approach, particularly regarding the bias for finite sample sizes. Additionally, we suggest providing more clarity on the intuition behind the entropic regularization's role in alleviating Pathology I and elaborating on the interpretation of Theorem 3.3. Furthermore, we encourage the authors to explore additional experimental scenarios, such as sampling from energy-based models and Gaussian mixtures with multiple distant modes, to enhance the robustness of their findings. Lastly, we recommend developing a systematic method for tuning the hyperparameter lambda to facilitate its application in various contexts.