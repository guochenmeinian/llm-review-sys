ID: 3XnBVK9sD6
Title: InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic Reward Modeling
Conference: NeurIPS
Year: 2024
Number of Reviews: 23
Original Ratings: 5, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to training reward models for human preferences using a variational information bottleneck (IB) objective to address reward misgeneralization and overoptimization in reinforcement learning from human feedback (RLHF). The authors derive a method based on a Gaussian assumption for the latent representation and propose using a variational lower bound of their IB objective, approximated through a sample estimate from a preference dataset. The derivation incorporates the reparameterization trick to express the latent representation as a deterministic function of the input and Gaussian noise. The final objective integrates the Bradley-Terry model for human preference distribution, leading to a comprehensive formulation that includes KL divergence terms for both accepted and rejected samples. The authors provide empirical evidence of improved performance compared to several baselines, including a better reward-KL frontier, and introduce the Cluster Separation Index (CSI) as a tool for detecting overoptimization in the latent space.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a critical issue in RLHF, making it relevant to the community.  
- The utilization of the IB approach for reward modeling is novel and shows promising performance improvements.  
- The derivation provides significant clarity and depth, enhancing the understanding of the optimization objective.  
- The incorporation of the reparameterization trick is a critical addition that impacts the implementation of the method.  
- The method demonstrates empirical effectiveness against various baselines, and the dual functionality of enhancing RLHF performance while providing an interpretable latent space is beneficial.  
- The authors demonstrate responsiveness to reviewer feedback, indicating a commitment to improving the clarity of their work.  

Weaknesses:  
- The comparison to baselines is insufficient, lacking evaluations against contemporary methods such as WCO, UWO, and WARM, which undermines claims of superiority.  
- There is a lack of clarity regarding hyperparameter selection, which raises concerns about fairness in comparisons.  
- The evaluation is limited to one dataset and a small range of reward model sizes, necessitating broader testing for generality.  
- The language used to describe the method is overly enthusiastic, detracting from clarity.  
- The motivation for the method lacks sufficient justification, particularly regarding the concept of misgeneralization.  
- Some notational inconsistencies regarding the representation of latent variables and distributions may confuse readers.  
- The final objective's presentation could benefit from clearer distinctions between different types of latent representations and their corresponding distributions.  
- The CSI method's effectiveness is not quantitatively evaluated, leaving its practical utility uncertain.  

### Suggestions for Improvement
We recommend that the authors improve the comparison to baselines by including evaluations against WCO, UWO, and WARM to substantiate claims of performance superiority. Additionally, a systematic approach to hyperparameter selection for all methods should be detailed to ensure fair comparisons. Expanding the evaluation to include multiple datasets and running experiments with multiple seeds would enhance the robustness of the findings. We suggest toning down the language throughout the paper to maintain neutrality and clarity. Furthermore, a clearer definition of misgeneralization and its relation to existing methods should be provided. We recommend improving clarity by explicitly defining the notation for $\boldsymbol S$, ensuring it consistently represents the tuple $(\boldsymbol S^w, \boldsymbol S^l)$. Additionally, we suggest clarifying the relationship between $p_{\phi}(\boldsymbol s|\boldsymbol x)$ and its components, distinguishing between product distributions and single Gaussian distributions. We advise eliminating abbreviations in the final objective to enhance readability and understanding. Lastly, including a detailed explanation of the early stopping algorithm based on the CSI metric would provide valuable insight into the training process.