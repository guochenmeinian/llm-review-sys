ID: znY173SCxu
Title: Time-Reversed Dissipation Induces Duality Between Minimizing Gradient Norm and Function Value
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 9, 7, 5, 8, 4, -1, -1, -1, -1
Original Confidences: 5, 2, 2, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the novel concept of H-duality, establishing a one-to-one correspondence between methods that efficiently minimize function values and those that minimize gradient magnitudes, under the assumption of convex and $L$-smooth objective functions. The authors prove that if one method meets a convergence condition, its H-duality counterpart will also converge. They derive a new class of methods from H-duality, demonstrating its utility in establishing convergence and symmetry properties between algorithms.

### Strengths and Weaknesses
Strengths:  
- The concept of H-duality is novel and provides a fresh perspective on the relationship between different optimization methods.  
- The paper is well-written, with clear explanations of complex proofs, making the main theorems accessible.  
- The results are interesting and could enhance understanding of algorithms like OGM-G, potentially leading to better practical methods.

Weaknesses:  
- The applicability of the work is limited to $L$-smooth and convex functions, with the value of $L$ needing to be known a priori.  
- The requirement for a fixed number of iterations $N$ may restrict practical use, as methods are often terminated based on stopping criteria rather than a predetermined iteration count.  
- The relevance of the work to NeurIPS is questioned, as it primarily addresses optimization theory rather than specific machine learning techniques.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the term "efficiently" in the context of function value and gradient magnitude reduction, potentially by formalizing its connection to different measures of convergence. Additionally, we suggest discussing the implications of requiring a fixed $N$ for practical applications and exploring the generalization of the H-duality to accommodate a broader class of algorithms and loss functions. Finally, including experimental results comparing the proposed SFG method with FISTA-G would enhance the persuasiveness of the claims regarding performance improvements.