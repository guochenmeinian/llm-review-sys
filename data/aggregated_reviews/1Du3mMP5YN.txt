ID: 1Du3mMP5YN
Title: Learning to Shape In-distribution Feature Space for Out-of-distribution Detection
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 7, 4, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach called Distributional Representation Learning (DRL) for out-of-distribution (OOD) detection, which optimizes the in-distribution (ID) space during pre-training. The authors propose an Expectation-Maximization (EM) algorithm that includes unnormalized class prototypes and an online approximation of normalization constants, enhancing both theoretical and practical aspects of OOD detection. Empirical results demonstrate DRL's strong performance across various benchmarks, including CIFAR-10 and CIFAR-100.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel method for ID space shaping, addressing a common oversight in OOD detection.
- DRL shows robust empirical performance across high-resolution and traditional benchmarks.
- The theoretical framework provides a solid foundation for understanding the DRL method.

Weaknesses:
- The paper contains contradictions regarding the reliance on pre-defined mixture distributions while criticizing existing methods for similar assumptions.
- Improvements on standard benchmarks are not consistently significant, raising concerns about the validity of reported results.
- The ablation study and analysis could be enhanced, particularly regarding convergence and the impact of hyperparameters.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper's structure and formatting to better introduce the DRL method. Additionally, providing a detailed analysis of the impact of the total number of classes (k) on computation and memory requirements would enhance understanding of the model's scalability. The authors should also include a rigorous theoretical justification for the sequential sampling technique and address the discrepancies in reported performance metrics compared to previous works. Finally, a more comprehensive discussion on the hyperparameter $\beta$ and its influence on the model would be beneficial.