ID: sp8wHIsnu9
Title: Synthesize, Partition, then Adapt: Eliciting Diverse Samples from Foundation Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 4, 6, 4, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework called Synthesize-Partition-Adapt (SPA) aimed at generating diverse and high-quality responses from language models. The authors propose partitioning a synthetic dataset using methods like influence functions, training multiple model adaptations on these partitions, and sampling from the resulting models during inference. Empirical evaluations demonstrate that this method improves output diversity compared to random partitioning. The authors also contrast their approach with temperature-based sampling methods, highlighting the importance of the partitioning strategy.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant issue in LLMs regarding the lack of diversity in generated outputs.
- It effectively demonstrates that principled data partitioning leads to greater diversity at lower temperatures compared to random partitioning.
- The writing is clear and the results are well-presented.

Weaknesses:
- The motivation behind the three phases (synthesize, partition, adapt) is not clearly articulated, leaving questions about their necessity and effectiveness.
- Evaluation on natural language tasks lacks standard quality metrics, making it unclear if quality is maintained alongside increased diversity.
- The method's reliance on a predefined number of partitions raises concerns about its flexibility and adaptability in practice.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation for the three phases of their framework. Additionally, including standard evaluation metrics for natural language tasks would strengthen the paper's claims regarding quality maintenance. It would also be beneficial to explore the sensitivity of results to the diversity of queries used for partitioning and to provide more detailed comparisons with existing baselines in the literature. Finally, discussing the implications of using a predefined number of partitions and how this affects the quality-diversity trade-off during inference would enhance the paper's robustness.