ID: 6EDbuqER4p
Title: Conic Activation Functions
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 7, 7, 6, 4
Original Confidences: 4, 4, 3, 3

Aggregated Review:
### Key Points
This paper presents Conic Activation Functions (CoLU), a novel activation function for neural networks that generalizes symmetry principles from component-wise activations like ReLU to continuous orthogonal groups, inspired by the Lorentz cone. The authors propose that CoLU enhances performance in various architectures, including MLPs, ResNets, Transformers, and UNets, particularly in image and text classification tasks. They demonstrate that CoLU outperforms traditional activations, exploring enhancements such as multi-head structures, soft scaling, and axis sharing to further improve performance.

### Strengths and Weaknesses
Strengths:  
- The introduction of CoLU as a non-component-wise activation function is a fresh and innovative approach.
- CoLU shows significant performance improvements over ReLU across multiple architectures, indicating its potential utility in deep learning.
- The theoretical motivation is strong, with connections to geometric and algebraic symmetry enhancing the interpretability of the method.

Weaknesses:  
- The paper lacks organization and clarity, particularly in the Introduction, where the purpose of the first paragraph is unclear.
- There is insufficient discussion on the computational cost of CoLU, especially in large-scale networks with complex configurations.
- The empirical results do not convincingly demonstrate substantial improvements over existing activation functions, as comparisons are limited to ReLU without addressing other recent alternatives.

### Suggestions for Improvement
We recommend that the authors improve the organization and clarity of the paper, particularly in the Introduction. A detailed analysis of the computational cost of CoLU in large-scale networks should be included. Additionally, we suggest that the authors benchmark CoLU against a broader range of activation functions, particularly learned activation functions, to provide a more comprehensive evaluation of its performance. It would also be beneficial to simplify the presentation of CoLU to enhance accessibility and include geometric visualizations to aid understanding. Finally, addressing the empirical results more robustly and providing clearer comparisons with established activation functions would strengthen the paper's contributions.