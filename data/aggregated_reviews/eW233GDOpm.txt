ID: eW233GDOpm
Title: Response Length Perception and  Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to enhance the efficiency of LLM inference by predicting response lengths and grouping queries with similar lengths into micro-batches for parallel processing. The authors demonstrate empirical gains in throughput, achieving an 86% improvement without compromising response quality. The approach is compatible with existing inference acceleration techniques and addresses the significant issue of wasteful computations in LLM usage.

### Strengths and Weaknesses
Strengths:
1. The proposed method is straightforward and effective, yielding significant improvements in inference speed.
2. The technique addresses a critical problem in LLM usage, providing a solution that is easy to implement.
3. The authors provide reproduction code, enhancing the paper's transparency and reproducibility.

Weaknesses:
1. The assumption that requests can be reordered may not hold in real-world applications, and the authors did not present a fairness metric to assess the impact on individual requests.
2. Key aspects of the methodology, such as the justification for target length predictions and the FCR mechanism, lack empirical support.
3. The paper does not adequately compare the proposed technique to existing LLM inference methods, which raises concerns about its novelty and applicability.

### Suggestions for Improvement
We recommend that the authors improve the paper by providing more thorough ablation studies to examine the effects of request reordering on inference latency. Additionally, justifications for the target length prediction and the FCR mechanism should be included, supported by experimental data. It would also be beneficial to include comparisons with existing LLM inference techniques to clarify the contributions of the proposed method. Finally, addressing the trade-off between speed and response quality when using the Perception in Advance (PiA) method would enhance the paper's depth.