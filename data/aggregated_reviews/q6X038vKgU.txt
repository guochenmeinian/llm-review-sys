ID: q6X038vKgU
Title: Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 4, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TSDiff, an unconditional diffusion model for time series generation, enhanced by self-guidance and prediction refinement mechanisms. The authors argue that TSDiff achieves comparable forecasting accuracy to conditional models while demonstrating superior performance in generating synthetic samples. The methodology is evaluated through extensive experiments across various tasks and datasets, showcasing its effectiveness.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and methodology is clearly articulated.
- Extensive empirical results convincingly demonstrate the proposed model's effectiveness.
- Introduction of a new metric for better comparison with baselines is commendable.

Weaknesses:
- The conditional generation aspect raises questions about its similarity to inpainting problems, suggesting the need for discussion of relevant baselines like DDRM.
- Several equations lack labels, complicating comprehension.
- The paper does not sufficiently elaborate on the model's details, such as input-output dimensions and handling of missing inputs.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the model by labeling all equations and providing detailed explanations of the methodology, particularly regarding the handling of missing data and the dimensions of input and output variables. Additionally, it would be beneficial to discuss the implications of self-guidance in practical scenarios and explore its applicability in other domains, such as image processing. Addressing the computational cost concerns with comparative experiments against conditional diffusion models would also enhance the paper's impact.