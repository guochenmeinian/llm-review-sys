ID: qO9VagA7kF
Title: From Comprehensive Study to Low-Rank Compensation: Exploring Post-Training Quantization in LLMs
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 7, 5, 4, 6, -1, -1
Original Confidences: 4, 2, 3, 5, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive analysis of various post-training quantization (PTQ) techniques for large language models (LLMs), including an evaluation of different schemes, model families, and bit precision. The authors propose an optimized method called Low-Rank Compensation (LoRC) to enhance model quality recovery with minimal size increase. Notably, the paper highlights that activation quantization is generally more susceptible to weight quantization, and none of the current quantization methods can achieve the original model quality.

### Strengths and Weaknesses
Strengths:  
- The motivation for studying quantization methods is solid, given the rapid development of LLMs and the necessity for deploying them on accessible hardware.  
- The insights provided, particularly regarding the susceptibility of activation quantization, are valuable for the community.  
- The paper is well-written, logically structured, and presents a reasonable improvement based on the observations made.  

Weaknesses:  
- The proposed method relies on low-rank approximation, which may be considered a sparsification-based approach, potentially straying from the core focus of the paper.  
- The novelty of the work is limited, as post-training quantization with finer-grained and zero-shift techniques is not new.  
- The sensitivity analysis is conducted with only one quantization method, limiting the generalizability of the conclusions.  
- Figure 1 is too dense, making it difficult to recognize key points, and the accuracy improvement shown is marginal compared to the na√Øve baseline.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Figure 1 by reducing density to enhance recognition. Additionally, we suggest comparing the proposed LoRC method with traditional quantization methods, such as those applied to ResNet-series, to provide a broader context. It would also be beneficial to conduct sensitivity analyses across different quantization functions, such as minmax and percentile quantization, to deliver more comprehensive conclusions. Furthermore, please clarify the results related to the layer norm in the OPT family and whether the compensation technique requires fine-tuning. Lastly, addressing how the compensation technique compares to existing methods like LAPQ and ensuring that error does not accumulate across layers would strengthen the paper.