ID: Poj71ASubN
Title: What Knowledge Gets Distilled in Knowledge Distillation?
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical study analyzing the implicit knowledge transferred from teacher to student networks through knowledge distillation (KD). The authors investigate various properties, including localization information, adversarial vulnerability, invariance to data transformations, and domain adaptation, using multiple distillation methods and architectures on ImageNet. The findings suggest that the distilled student learns the teacher's decision boundary, which has implications for generalization to new domains and the potential transfer of harmful biases.

### Strengths and Weaknesses
Strengths:
1. The paper explores novel questions regarding the types of knowledge transferred in KD, providing a fresh perspective on the phenomenon.
2. It covers various knowledge types, demonstrating clear evidence that students non-trivially imitate teachers across different distillation methods.
3. The experiments are carefully designed, using independent students as baselines to assess the effectiveness of distillation.

Weaknesses:
1. The main experiments are limited to the ImageNet dataset, raising concerns about the generalizability of the findings to other datasets or tasks.
2. The argument explaining why KD transfers implicit knowledge is somewhat circular and lacks depth, particularly regarding the transfer of decision boundaries in out-of-distribution and adversarial contexts.
3. The empirical nature of the study limits its contributions, as it lacks theoretical analysis to characterize the distilled knowledge.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by including experiments on additional datasets, such as CIFAR100 or language datasets. Furthermore, we suggest that the authors clarify their rationale for selecting the studied properties and consider including a broader range of recent KD methods to enhance the comprehensiveness of their evaluation. Additionally, we encourage the authors to address the limitations of their study explicitly, particularly regarding the types of knowledge that may not be transferred and the theoretical underpinnings of their observations.