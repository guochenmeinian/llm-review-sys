ID: jfE7XCE89y
Title: FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 6, 7, 5, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion," a framework aimed at addressing challenges in multimodal data, particularly in healthcare, where data may be incomplete or irregularly sampled. The authors propose a novel Laplace gating function that enhances predictive performance and convergence rates compared to traditional Softmax functions. Key innovations include: 1. Sparse MoE Layers for optimal modality partitioning; 2. A theoretically proven Laplace Gating Function; 3. Efficient handling of missing modalities and irregular data trajectories. The framework's effectiveness is validated through empirical evaluations across diverse prediction tasks.

### Strengths and Weaknesses
Strengths:
1. The demand for a flexible gating function in multimodal MoE makes the concept promising, with potential applications in medical tools.
2. The theoretical proof provided is rigorous and sufficient.
3. The experimental design is comprehensive, addressing various scenarios and data types, supported by extensive supplementary information.

Weaknesses:
1. The comparative works cited are outdated, with many predating 2020; more recent comparisons are needed.
2. Figure 1 lacks a clear description, failing to adequately explain the data processing workflow.
3. The similarity between Gaussian gating and the proposed function raises concerns about the fairness of theoretical comparisons with Softmax, potentially diminishing the importance of the results.

### Suggestions for Improvement
We recommend that the authors improve the comparison section by including more recent works to enhance the relevance of their findings. Additionally, we suggest providing a clearer description of Figure 1 to better explain the data processing workflow. Further, the authors should clarify the theoretical distinctions between their gating function and Gaussian gating to address potential misunderstandings and enhance the perceived significance of their theoretical contributions. Lastly, a discussion on the necessity of linking the gating function to methods related to mTAND and imputation for improved feature quality could provide valuable insights.