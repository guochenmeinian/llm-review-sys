ID: CzAFnfwbGd
Title: Coneheads: Hierarchy Aware Attention
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to attention mechanisms by introducing hyperbolic shadow cone similarity, leading to the development of Hierarchy Aware Attention. The authors propose utilizing hyperbolic entailment cones to define pairwise similarity scores in hyperbolic space, replacing the traditional Euclidean inner product in Transformers. The method demonstrates competitive performance across various tasks, including graph prediction, natural language processing, and vision, while significantly reducing the number of parameters required.

### Strengths and Weaknesses
Strengths:
- The concept of using hyperbolic attention cones to define hierarchical relationships is innovative and contributes meaningfully to the field.
- The proposed method outperforms standard dot-product attention across all tested settings and maintains better performance at lower dimensions.
- Extensive experiments validate the effectiveness of the approach across multiple domains.

Weaknesses:
- The paper lacks explicit discussion on the specific hierarchy that the model is expected to learn, leaving the nature of the learned hierarchy ambiguous.
- The title's use of "Attention" may overstate the contribution, as the primary focus is on defining pairwise similarity rather than addressing broader aspects of attention mechanisms.
- The speed of calculating similarity is only briefly mentioned, which is crucial for understanding the attention mechanism's efficiency.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the hierarchy the model is intended to learn, possibly by including qualitative results or attention heatmaps that demonstrate hierarchical behavior. Additionally, it would be beneficial to compare the proposed method with hyperbolic distance attention in a results table to clarify the contributions of entailment cones. We also suggest conducting experiments to explore the sensitivity of the model to initialization by running multiple seeds on a subset of tasks. Finally, providing open-sourced code would enhance accessibility for practitioners unfamiliar with hyperbolic embeddings.