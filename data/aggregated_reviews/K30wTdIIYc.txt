ID: K30wTdIIYc
Title: Controlling Text-to-Image Diffusion by Orthogonal Finetuning
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 6, 6, 7, 6, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Orthogonal Finetuning (OFT), a novel method for fine-tuning text-to-image diffusion models that maintains the "hyperspherical energy" to preserve semantic structures during adaptation. The authors also introduce Constrained Orthogonal Finetuning (COFT), which limits angular deviation from the pretrained model to enhance generative performance. The experiments conducted on subject-driven and controllable generation tasks demonstrate the effectiveness of the proposed methods compared to state-of-the-art techniques.

### Strengths and Weaknesses
Strengths:
1. The paper clearly articulates the orthogonal finetuning concept and supports it with theoretical references on "hyperspherical energy."
2. The exploration of neuron angles is well-motivated and presents a new perspective in the parameter-efficient fine-tuning (PEFT) landscape.
3. Experimental results show solid performance improvements in both subject-driven and controllable generation tasks.

Weaknesses:
1. The related work on "subject-driven generation" is insufficient, lacking recent advancements such as P+, DisenBooth, and Cones.
2. The statistical differences between COFT and OFT in Tables 1 and 2 are marginal, raising questions about the conclusion that COFT is superior.
3. There are no specific-domain finetuning comparisons with LoRA, which is essential given its prevalence in downstream applications.
4. The paper lacks a conclusion and limitations section, which are critical for contextualizing the findings.

### Suggestions for Improvement
We recommend that the authors improve the related work section by including discussions on recent advancements in subject-driven finetuning, such as P+, DisenBooth, and Cones. Additionally, we suggest providing clearer statistical analysis to support claims of COFT's superiority over OFT. It would be beneficial to include comparisons with LoRA in specific-domain finetuning contexts. Furthermore, we encourage the authors to add a conclusion and limitations section to better frame their contributions and acknowledge potential shortcomings.