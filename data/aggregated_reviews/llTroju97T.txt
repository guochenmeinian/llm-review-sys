ID: llTroju97T
Title: Personalized Adapter for Large Meteorology Model on Devices: Towards Weather Foundation Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 7, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LM-WEATHER, an approach that utilizes pre-trained language models (PLMs) for on-device modeling of heterogeneous meteorological variables. The authors propose a lightweight personalized adapter to enhance local weather knowledge and employ low-rank based transmission for efficient inter-device communication. The framework demonstrates effectiveness in modeling real-world weather data while maintaining low resource demands. Extensive experiments validate the approach using two real-world meteorological datasets, ODW1 and ODW2, supporting regional weather forecasting.

### Strengths and Weaknesses
Strengths:
- The paper validates the superiority of PLMs in modeling meteorological variables and provides a comprehensive comparison of various time series models.
- It introduces a novel integration of personalized adapters into PLMs, facilitating efficient on-device meteorological modeling.
- The extensive appendix and open-sourced codes enhance reproducibility and provide valuable experimental details.

Weaknesses:
- The rationale for using averaging operations during inter-device communication with the personalized adapter is unclear and requires further explanation.
- The organization of symbols and equations is disorganized, and there are instances of incorrect singular and plural usage, necessitating a rewrite for clarity.
- The paper lacks detailed explanations of the communication mechanism, the architecture of the feedforward network (FFN), and the theoretical background of the proposed theorems.

### Suggestions for Improvement
We recommend that the authors improve the clarity and organization of the definitions and formulations in Sections 3.1 and 4. Additionally, please provide a strong justification for the choice of GPT-2 as the PLM and consider exploring more advanced models like GPT-4, Llama, or Vicuna. It would be beneficial to include more detailed information on the communication strategy, the architecture of the FFN, and the theoretical basis for the proposed theorems. Furthermore, integrating essential experimental details into the main body of the paper would enhance readability and comprehension.