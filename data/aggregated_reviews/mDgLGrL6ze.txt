ID: mDgLGrL6ze
Title: ECHo: A Visio-Linguistic Dataset for Event Causality Inference via Human-Centric Reasoning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ECHo, a diagnostic dataset focused on event causality inference within visio-linguistic social scenarios, specifically derived from the crime drama CSI. The dataset includes three annotation tasks: Characteristics Identification, Keyframe-Grounded Emotion Interpretation, and Event Causality Inference. The authors evaluate the performance of large language models (LLMs) and multimodal models, demonstrating that a Theory of Mind (ToM)-enhanced model exhibits improved reasoning capabilities.

### Strengths and Weaknesses
Strengths:  
- The dataset is well-constructed and could significantly aid future research in human-centric video and narrative comprehension.  
- The paper is clearly written, with effective figures illustrating the annotation pipeline and dataset statistics.  
- The integration of ToM is crucial for narrative understanding and enhances the evaluation of artificial agents' reasoning capabilities.

Weaknesses:  
- The dataset's relatively small size limits its applicability primarily to evaluation purposes.  
- Its reliance on CSI content restricts generalizability to broader daily life scenarios.  
- The annotation process lacks quantitative evaluation for quality assurance, such as inter-rater agreement, which raises concerns about the reliability of the experimental results.  
- The experiments do not include systematic human evaluation of model performance, leaving the significance of small differences in automatic evaluation measures unclear.

### Suggestions for Improvement
We recommend that the authors improve the dataset's generalizability by considering a wider range of scenarios beyond crime dramas. Additionally, it is essential to justify the quality of the data through quantitative evaluations, such as inter-rater agreement. We suggest incorporating systematic human evaluations to substantiate the findings and clarify the relationships between the annotation tasks. Furthermore, addressing the questions raised regarding the annotation process, such as the role consistency of characters and the selection criteria for keyframes, would enhance the clarity of the methodology. Lastly, we encourage the authors to explain the rationale for using instructGPT over more advanced LLMs like ChatGPT and GPT-4.