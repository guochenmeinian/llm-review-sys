ID: fyLvHzEssH
Title: Neural (Tangent Kernel) Collapse
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 5, 7, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical connection between Neural Tangent Kernel (NTK) alignment and the Neural Collapse (NC) phenomenon in deep neural networks (DNNs). The authors derive the dynamics of DNNs trained with mean squared error (MSE) loss and a block-structured NTK, identifying three distinct convergence rates. They argue that the emergence of NC can be explained through the dynamics of squared error minimization on the kernel, supported by large-scale numerical experiments. Additionally, the authors establish a novel connection between neural connectivity (NC) and local elasticity in neural networks, proposing a framework where local elasticity is derived from the NTK structure, with dynamics explicitly derived from gradient flow equations. This contrasts with prior models that utilized a block-structured effect matrix with only two distinct values, which lacked a clear link to actual gradient flow dynamics.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written, organized, and presents a thorough analysis of the dynamics of DNNs with block-structured NTK.  
- It offers a new perspective on NC, identifying conditions under which it occurs, and provides substantial empirical support for its claims.  
- The clarity of presentation makes complex concepts accessible to readers.  
- The paper advances previous models by establishing a direct connection between local elasticity and the NTK structure, enhancing the theoretical foundation of the work.

Weaknesses:  
- The assumption of a perfect block structure in NTK is overly strong and unlikely to hold in realistic settings, as it requires gradients to be perfectly orthogonal across classes.  
- The paper does not adequately justify its fundamental assumptions, particularly regarding the independence of gradients and the implications of the block structure.  
- There is insufficient exploration of the effects of non-balanced datasets and the introduction of stochasticity in the dynamics considered.  
- The prior work's assumptions regarding local elasticity's impact on dynamics are not sufficiently critiqued, which may weaken the argument for the authors' approach.

### Suggestions for Improvement
We recommend that the authors improve the justification for their assumptions, particularly regarding the block structure of the NTK and the independence of gradients. Clarifying that the perfect block structure is an extreme case and providing results in more general scenarios would enhance the paper's applicability. Additionally, exploring the effects of non-balanced datasets and stochasticity on the dynamics of DNNs would strengthen the analysis. Finally, we suggest that the authors improve the discussion on the limitations of the previous work, particularly regarding the assumptions made about local elasticity and its effects on dynamics, to better justify their proposed framework.