ID: elPtHcfjpH
Title: LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unlabeled Image Collections
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for enhancing zero-shot classification in vision-language (VL) models through an unsupervised approach that utilizes additional unlabeled examples. The authors propose generating a dataset of text descriptions for target classes using large language models (LLMs) and training a text classifier on top of the VL textual encoder. Subsequently, they finetune the classifier and visual prompts using relevant unsupervised images and designed augmentations. Extensive evaluations demonstrate significant performance improvements over existing methods, particularly CLIP.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written and easy to follow.
- The performance enhancement shown by LaFTer is significant and consistent across most test cases.
- The idea of tuning the visual encoder of VL models using text supervision is highly interesting and presents a new direction for zero-shot learning research.

Weaknesses:
- The performance of the CLIPPR baseline is unexpectedly lower than the original CLIP, which requires clarification from the authors.
- The claim in the abstract that LaFTer is the first to reduce the gap from the supervised baseline is misleading, as it does not close the gap in about 50% of cases.
- The method's reliance on the quality of the visual representation of the VL model and the necessity of access to unlabeled images from the same distribution are limitations that need addressing.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the performance gap observed with the CLIPPR baseline. Additionally, please clarify the misleading statement in the abstract regarding LaFTerâ€™s novelty in reducing the gap from the supervised baseline. It would also be beneficial to provide more details on the experiments denoted by LaFTer* and consider including ablations on the diversity and quality of generated texts. Lastly, we suggest imposing constraints on the LLM to ensure it generates visually meaningful texts for classification.