ID: ddKCg3OhGw
Title: Functional Equivalence and Path Connectivity of Reducible Hyperbolic Tangent Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 5, 7, 10, 5, -1, -1, -1, -1
Original Confidences: 2, 4, 3, 5, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the functional equivalence class for reducible neural network parameters, focusing on single-hidden-layer hyperbolic tangent networks. The authors propose a canonicalization algorithm to determine canonical parameters and characterize the connectivity properties of these classes. The findings suggest that the parameter network's diameter can be small when a majority of units are blank, which is relevant for understanding modern architectures and deep learning.

### Strengths and Weaknesses
Strengths:  
The paper provides a comprehensive understanding of functional equivalence classes, offering rich insights into the parameter space and loss landscape. It includes a novel canonicalization algorithm and visualizations that clarify the path structure of functional equivalence classes. The writing is clear, and the theoretical results are well-motivated and connected to existing literature.

Weaknesses:  
The theoretical nature of the paper may limit its immediate practical applications, particularly as it focuses solely on single-hidden-layer networks with hyperbolic tangent activation, which may not generalize to more complex architectures. The relevance of reducible parameters to practical applications remains unclear, and the evaluation of the proposed algorithm is limited.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the applicability of their findings to multi-layer networks and different activation functions, such as ReLU. Additionally, the authors should clarify the practical implications of their canonicalization algorithm and provide examples of its application in loss minimization. Addressing the adaptability of the algorithm for networks trained through backpropagation would enhance the paper's relevance. Finally, we suggest incorporating a review of related literature on pruning and sparsity to contextualize the findings within contemporary neural network research.