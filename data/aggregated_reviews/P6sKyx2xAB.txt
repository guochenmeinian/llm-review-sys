ID: P6sKyx2xAB
Title: UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents UniTime, a novel approach for effective cross-domain time series learning, addressing challenges such as data characteristic disparities and domain confusion. The authors propose a unified model paradigm that employs human-crafted instructions for domain identification and a Language-TS Transformer to align different input spaces. Experimental results indicate that UniTime consistently outperforms state-of-the-art baselines, demonstrating advancements in forecasting performance and zero-shot transferability.

### Strengths and Weaknesses
Strengths:
- The topic is both interesting and important, with a clear presentation of the proposed methods.
- The experimental design is comprehensive, and the proposed method shows consistent performance improvements over baselines.
- The utilization of a language model for cross-domain forecasting is innovative and effectively addresses key challenges.

Weaknesses:
- The motivation for the approach requires further elaboration, particularly regarding the choice of model architecture, as GPT-2 may not be optimal.
- The relevance of the study to web mining is questionable, as it primarily addresses general MTS tasks without focusing on web-specific issues.
- The paper potentially overstates its contribution, as similar approaches have been explored in prior works.
- The reliance on hand-crafted domain instructions raises concerns about scalability and fairness in comparisons with other baselines.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the motivation behind their approach, particularly addressing the choice of model architecture and exploring alternatives like T5. Additionally, providing more insights into the ablation study regarding the performance impact of masking would be beneficial. The authors should clarify the challenges that prevent other baseline models from being trained across multiple datasets. We also suggest incorporating quantitative analysis to strengthen claims about the benefits of knowledge transfer across time series datasets. Finally, addressing the potential issues related to the quality and standardization of domain instructions would enhance the robustness of the proposed method.