ID: E3HDagVPNG
Title: A Siamese Transformer with Hierarchical Refinement for Lane Detection
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 3, 7, 4, 8, -1, -1, -1
Original Confidences: 4, 5, 5, 5, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the LAne TRansformer (LATR), a Siamese Transformer model designed for lane detection that employs a high-to-low hierarchical refinement structure to enhance accuracy in complex environments. It integrates global semantic information with finer-scale features and introduces a Curve-IoU loss to improve lane line fitting. The evaluation across multiple datasets demonstrates consistent but marginal accuracy improvements alongside increased speed.

### Strengths and Weaknesses
Strengths:
1. The proposed method achieves relatively high performance on the OpenLane dataset.
2. The Curve-IoU loss effectively enhances performance.
3. The hierarchical design for lane detection is commendable, and the extensive evaluation across three datasets is thorough.

Weaknesses:
1. The motivation for the paper is unclear, and the hierarchical architecture lacks novelty, being similar to existing works like CLRNet and Mask2Former.
2. The use of a stronger image backbone (Swin vs. ResNet) raises concerns about the fairness of comparisons with previous methods.
3. The writing quality is poor, with a disorganized methodology section that complicates understanding.
4. The paper's focus on 2D lane detection is less valuable compared to 3D lane detection, and the proposed loss function lacks originality.

### Suggestions for Improvement
We recommend that the authors clarify the motivation and novelty of the proposed method. To enhance the method's innovativeness, consider using common public backbones for ablation studies to ensure fair comparisons. Additionally, we suggest improving the writing quality and organization of the methodology section. To address the marginal improvements observed, tuning hyperparameters or adding more decoder layers could be beneficial. Finally, further testing in diverse real-world environments and exploring self-supervised learning techniques may strengthen the robustness claims of the model.