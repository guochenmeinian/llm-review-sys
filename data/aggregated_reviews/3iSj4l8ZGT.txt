ID: 3iSj4l8ZGT
Title: Learning Interpretable Low-dimensional Representation via Physical Symmetry
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 4, 5, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach to interpretable representation learning called Self-Supervised learning with Physical Symmetry (SPS), which leverages physical symmetry as a self-consistency constraint within an autoencoder framework. The authors propose a model that predicts the temporal evolution of latent variables while ensuring equivariance under transformations such as translation and rotation. SPS is applied in music to learn a linear pitch factor from unlabeled audio and in computer vision to derive a 3D Cartesian space from unlabeled video data. The study introduces the concept of representation augmentation, which enhances sample efficiency by generating additional training samples through symmetry-informed transformations. Experiments conducted on synthetic datasets in audio (predicting pitches from spectrograms) and vision (predicting coordinates of a moving ball) demonstrate the model's ability to learn interpretable representations and achieve style-content disentanglement. The authors also explore the implications of physical symmetry as a cognitive bias and a counterfactual inductive bias, suggesting that it can guide the model's learning process.

### Strengths and Weaknesses
Strengths:
- The core idea of using physical symmetry is original and may inspire future research.
- The paper is well-written, with clear methodology and well-executed experiments that support the hypothesis.
- The representation augmentation technique is novel and shows empirical effectiveness, particularly in style-content disentanglement.
- Leveraging physical symmetry helps uncover meaningful low-dimensional representations, aiding in feature abstraction while filtering irrelevant information.
- The symmetry-based representation augmentation improves sample efficiency and reduces overfitting, enhancing model robustness.
- The diversified total loss function optimizes various aspects of learning, contributing to overall model performance.
- The method adapts to different problems through various group transformations, demonstrating versatility.
- The existence of different model versions like SPSVAE and SPSAE provides flexibility for task-specific needs.

Weaknesses:
- The writing in the initial sections is convoluted, making it difficult to grasp the main idea early on.
- Experiments are limited to synthetic datasets, raising concerns about generalizability to real-world applications.
- The improvements observed are only slight or moderate compared to the baseline, with much of the effect attributed to sequence prediction and reconstruction objectives rather than the proposed symmetry constraints.
- Optimal results hinge on the correct choice of symmetry transformation, which can be challenging in real-world data.
- The model's efficiency is significantly influenced by the augmentation factor K, necessitating extensive experimentation for optimal selection.
- The effectiveness of the method in higher-dimensional latent spaces remains unexplored, posing challenges for real-world applications.
- Excessive application of representation augmentation may lead to overfitting, particularly with limited data.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the initial sections, including the title and abstract, to better convey the application domains and the reliance on time-series data. The authors should also discuss the limitations of the synthetic datasets more openly in the text and provide more challenging real-world experiments to demonstrate the generalizability of their method. Additionally, we suggest that the authors clarify the relationship between the proposed physical symmetry and the observed improvements in representation learning, as well as provide a more detailed explanation of terms like "prior model" and "global invariant style code" earlier in the manuscript. We recommend that the authors improve the clarity of the distinction between physical symmetry as a concept and representation augmentation as an implementation. Further exploration of the scalability of the SPS approach to other tasks and strategies to prevent overfitting would enhance the paper. We suggest that the authors emphasize the application examples and the autoencoder structure in the introduction to provide clearer context. Lastly, addressing the theoretical underpinnings of the method and its applicability to high-dimensional latent spaces, along with a formalized theory quantifying the impact of representation augmentation, could strengthen the theoretical foundation of the work.