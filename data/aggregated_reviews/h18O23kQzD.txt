ID: h18O23kQzD
Title: BLURD: Benchmarking and Learning using a Unified Rendering and Diffusion Model
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 8, 7, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method, BLURD, for generating cost-effective, precisely controlled, and scalable datasets by integrating 3D rendering with generative diffusion models. The authors propose a new dataset (BLURD 3D and BLURD SD) aimed at evaluating and comparing machine learning models, particularly CLIP architectures. Key contributions include the combination of 3D rendering and Stable Diffusion for realistic image generation, the creation of a dataset for benchmarking CLIP models, and an evaluation of CLIP models' performance in recognizing race. The paper also discusses limitations in evaluation methods based on class name annotations and demonstrates the dataset's application in real-world face datasets.

### Strengths and Weaknesses
Strengths:
1. The dataset is sufficiently large for training effective models.
2. The concept of paired data between 3D Blender models and SD-generated images is valuable, with a clear and detailed methodology.
3. The figures are well-designed and visually appealing.
4. The integration of rendering engines and diffusion models is novel and provides true annotations.
5. The dataset facilitates the study of fairness issues like race and gender bias.

Weaknesses:
1. The dataset quality does not consistently meet expectations for photorealism.
2. The paper lacks exploration of applications in inverse graphics, such as reverse engineering meshes from SD-generated images.
3. The segmentation experiment in Figure 6 requires more comparison methods and training data for comprehensive conclusions.
4. Table 1 needs aesthetic improvement, and the organization of figures and tables is suboptimal.

### Suggestions for Improvement
We recommend that the authors improve the dataset's photorealism to meet higher expectations. Additionally, the authors should explore more applications in inverse graphics, such as reverse engineering meshes from SD-generated images. For the segmentation experiment in Figure 6, we suggest including more comparison methods and training data. We also advise correcting the caption for Figure 2 and clarifying the use of the IP2Adapter control net shown in Figure 1. Furthermore, the authors should justify and minimize inherent bias in the selection of 3D assets and consider adding face pose as a control feature. Lastly, we recommend reorganizing figures and tables for better clarity and ensuring that the table numbers and their order of appearance are consistent throughout the paper.