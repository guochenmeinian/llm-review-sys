ID: g1dMYenhe4
Title: MIMEx: Intrinsic Rewards from Masked Input Modeling
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 8, 6, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for exploration in reinforcement learning (RL) called Masked Input Modeling for Exploration (MIMEx). MIMEx utilizes a masked autoencoding objective to derive intrinsic rewards for exploration, particularly in sparse-reward environments. The authors argue that existing intrinsic reward methods can be unified under pseudo-likelihood estimation, and MIMEx computes intrinsic rewards based on masked predictions over input sequences. The effectiveness of MIMEx is demonstrated through evaluations on the PixMC-Sparse benchmark suite and the DeepMind Control Suite, showing superior performance compared to baseline methods like ICM and RND.

### Strengths and Weaknesses
Strengths:  
- The paper is clearly presented and well-structured, effectively arguing for a coherent narrative.  
- It introduces an original approach that generalizes existing concepts in RL, particularly the application of pseudo-likelihood estimation and masked sequence modeling.  
- The experimental results are thorough, including extensive ablation studies and evaluations across various benchmarks, which enhance the significance of the findings.  

Weaknesses:  
- The computational cost of integrating a masked autoencoder for intrinsic rewards is not adequately addressed, raising concerns about runtime and resource utilization.  
- The formulation of MIMEx may not sufficiently capture the dynamics of existing methods like ICM and RND, as it lacks the incorporation of actions in its predictions.  
- The paper could benefit from a more diverse set of experimental tasks to better demonstrate the generalizability of MIMEx beyond the specific environments tested.

### Suggestions for Improvement
We recommend that the authors improve the analysis of the computational cost associated with MIMEx, including overall runtime and resource utilization compared to other baselines. Additionally, we suggest that the authors clarify the performance drop observed with a sequence length of 6 in Figure 4 and address the inclusion of actions in their framework to enhance the expressiveness of MIMEx. Furthermore, we encourage the authors to evaluate MIMEx on a broader range of environments to substantiate its generalizability and to provide a more nuanced discussion regarding the claims of unification in their framework.