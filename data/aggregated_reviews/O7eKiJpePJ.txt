ID: O7eKiJpePJ
Title: Instruct and Extract: Instruction Tuning for On-Demand Information Extraction
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel task called On-Demand Information Extraction (ODIE), which aims to extract content based on user instructions and present it in a structured tabular format. The authors introduce a new dataset, INSTRUCTIE, comprising 14,000 synthesized training examples and 150 human-annotated test samples. The LLaMA-7B model fine-tuned on this dataset outperforms existing open-source models but still falls short compared to proprietary models like ChatGPT and GPT-4. The paper emphasizes the importance of user-driven extraction schemas, contrasting with traditional information extraction methods.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant research question by prioritizing user demand in information extraction.
- It introduces a large instruction-tuning dataset that supports and benchmarks the ODIE task.
- The training procedure effectively tackles challenges associated with synthetic text as training data.

Weaknesses:
- The task formulation lacks clarity, particularly regarding the output format and its limitations in covering multi-event extraction scenarios.
- The evaluation metrics, such as cosine similarity and ROUGE-L, are deemed inappropriate for assessing relational tables, which are permutation-invariant.
- The human evaluation criteria are vague, and the sample size for testing is insufficient compared to the training set.

### Suggestions for Improvement
We recommend that the authors improve the task definition by clearly delineating the scope of extraction and considering a more flexible output format to accommodate various extraction scenarios. Additionally, we suggest using more appropriate evaluation metrics that account for the permutation-invariance of relational tables and providing exact match results to clarify ambiguities. The authors should also enhance the clarity of the human evaluation criteria and consider expanding the test sample size. Finally, it would be beneficial to explicitly state whether the INSTRUCTIE dataset will be publicly available.