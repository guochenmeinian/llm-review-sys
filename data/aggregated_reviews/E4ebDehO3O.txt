ID: E4ebDehO3O
Title: Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on enhancing the multilingual capabilities of Large Language Models (LLMs) through a method called Cross-Lingual-Thought Prompting (XLT). The authors propose that XLT, which involves using a generic template prompt to stimulate cross-lingual and logical reasoning, can significantly improve LLM performance across various languages, particularly low-resource languages. The methodology and presentation are deemed correct, and the paper reports substantial improvements in multilingual tasks.

### Strengths and Weaknesses
Strengths:
- The introduction of XLT is a novel and straightforward method that shows impressive improvements in multilingual NLP tasks.
- The exploration of various prompt variants confirms the effectiveness of the proposed approach.

Weaknesses:
- The study relies on only two models, "text-davinci-003" and "gpt-3.5-turbo," which limits the generalizability of the findings.
- Key information is missing regarding how model outputs are extracted and how the proposed method handles formatting issues.
- The baselines used are considered unnatural, with a lack of a natural baseline that prompts in the target language.
- The concept of "Language Democratization" is introduced but lacks clarity and justification.

### Suggestions for Improvement
We recommend that the authors improve the comparative analysis by including experiments on a broader range of multilingual models, such as BLOOM or mT5, to better assess the generalizability of XLT. Additionally, we suggest providing deeper insights into the design factors that contribute to the success of the model in multilingual tasks, rather than relying on a trial-and-error approach. Clarifying the definition and implications of "Language Democratization" is essential, as well as ensuring that the baselines used are more representative of natural language use. Finally, we advise addressing the inconsistencies in presentation and correcting typographical errors to enhance clarity.