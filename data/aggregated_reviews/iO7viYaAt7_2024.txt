ID: iO7viYaAt7
Title: Expectation Alignment: Handling Reward Misspecification in the Presence of Expectation Mismatch
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on reward misspecification, focusing on how human designers may have incorrect beliefs about a robot's operating domain. The authors introduce the concept of "expectation alignment" over occupancy frequency, formalizing it through a framework that includes linear programs to evaluate states against forbidden and goal sets. The proposed method is compared to inverse reward design (IRD), highlighting its iterative nature in querying human feedback. Additionally, the authors propose utilizing expectation sets as an alternative to traditional reward functions to better capture human preferences in robotic systems. They discuss Theorem 1, which emphasizes the importance of identifying situations where no reward function can satisfy the expectation set, even if a policy exists.

### Strengths and Weaknesses
Strengths:
- **Originality & significance**: The work is original, offering a novel approach to reward design through occupancy measures, which could extend to practical applications in dual reinforcement learning.
- **Quality & clarity**: The paper is well-written, effectively guiding readers through a complex problem formulation.
- The ability to flag unsatisfiable expectations is a significant advantage of the proposed method.
- The authors acknowledge the potential disconnect between expectation sets and true human preferences, enhancing the paper's credibility.
- The intention to include a running example that ties directly to Theorem 1 is a positive step for clarity.

Weaknesses:
- **Clarity**: More context on inverse reward design (IRD) is needed for readers to interpret evaluation results without referring to external sources. The claim that the proposed method guarantees no expectation violations compared to IRD lacks appropriate context, as IRD operates under a one-shot learning paradigm, while the proposed method is iterative.
- The assumption that expectation sets universally capture true human preferences may not hold in all cases.
- The proof of Theorem 1 relies on an artificial scenario where no actions are available in final states, which may limit its applicability.
- The distinction between expectation sets and reward functions could be more prominently presented.
- The experimental results are insufficiently robust, and further experiments in varied environments are necessary to strengthen the findings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing additional context on inverse reward design within the main text or appendix. It would be beneficial to clarify the comparison metrics between the proposed method and IRD, ensuring that the claims regarding expectation violations are appropriately contextualized. Additionally, we suggest enhancing the experimental evaluation by including more diverse environments beyond the current gridworld settings. We also recommend improving the presentation of the assumption regarding expectation sets to clarify that it may not universally apply. Furthermore, finding a more realistic example to demonstrate Theorem 1 that does not rely on the absence of actions in final states would enhance the paper's applicability. Finally, including formal proofs for theorems and propositions in the appendix would significantly increase the submission's quality and strength.