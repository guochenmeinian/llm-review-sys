ID: uMPEhA2LZI
Title: Unified and Generalizable Reinforcement Learning for Facility Location Problems on Graphs
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach for solving facility location problems (FLPs) on graphs using a deep reinforcement learning (DRL) framework. The authors propose a swap-based method that effectively addresses the p-median problem and facility relocation problem, leveraging graph neural networks to enhance scalability and solution quality. Extensive experiments demonstrate significant speedups, achieving over 2000 times faster solutions compared to traditional solvers like Gurobi on large graphs, particularly in the context of Shanghai road networks.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a unified framework that effectively tackles both the p-median problem and facility relocation problem.
2. The integration of DRL with graph neural networks enhances the model's generalization and representation capabilities.
3. The proposed method shows impressive scalability and computational efficiency, validated through extensive experiments.

Weaknesses:
1. The iterative selection process for facility removal and insertion may lead to local optimality.
2. The paper lacks detailed explanations of the model architecture, particularly regarding the diagram and the significance of specific components.
3. The reward structure defined in equation (6) requires further justification, particularly regarding its formulation as an improvement ratio.
4. While the method excels in time efficiency, there is room for improvement in overall performance efficacy.
5. A comparison with other deep learning-based methods is missing, limiting the contextual understanding of the proposed approach.
6. The writing needs standardization, including the explanation of abbreviations upon first use.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the model architecture diagram, clarifying the meaning of the arrow pointing to “filter.” Additionally, the authors should justify the reward definition in equation (6) and consider defining it as the improvement ratio between states before and after step i. To enhance overall performance, we suggest conducting a broader range of experiments beyond Shanghai road networks and including comparisons with other deep learning methods. Furthermore, the authors should standardize the writing, ensuring that all abbreviations are defined upon first use. Lastly, addressing the potential local optimality of the iterative approach would strengthen the paper's contributions.