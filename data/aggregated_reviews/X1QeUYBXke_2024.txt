ID: X1QeUYBXke
Title: Gradient Guidance for Diffusion Models: An Optimization Perspective
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 4, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates gradient guidance for adapting or fine-tuning pre-trained diffusion models from an optimization perspective. The authors propose a look-ahead loss-based gradient guidance and two variants of diffusion-based generative optimization algorithms. They provide theoretical guarantees for adapting and fine-tuning diffusion models and explore the convergence of generated samples under simplified linear models. The authors clarify that their results are applicable to arbitrary distributions and not limited to linear score models, emphasizing that the guidance term, G_loss, is applicable to any pre-trained score network. Their experiments validate the method on both nonlinear and nonconcave objectives, and they assert that their work is the first to provide optimization convergence theory for adapting pretrained diffusion models to solve optimization problems with guarantees.

### Strengths and Weaknesses
Strengths:
- Theoretical approaches effectively address issues in diffusion with guidance, demonstrating that naive guidance fails on latent data structures and proposing adaptive gradient-guided diffusion.
- The idea of deriving gradient guidance using a low-dimensional linear sample space is novel and interesting.
- The authors effectively clarify misconceptions regarding the reliance on simplified models, emphasizing the broader applicability of their methodology.
- The theoretical contributions, particularly the establishment of optimization convergence theory, are significant and well-articulated.
- The paper is well-written, clear, and presents strong theoretical results.
- The experiments demonstrate the method's robustness across various model types and objectives.

Weaknesses:
- The method's reliance on linear assumptions raises questions about its practical applicability, as it requires gradient backpropagation, making it slow compared to existing methods.
- The theoretical analysis is limited to linear models, and the paper lacks a comprehensive comparison with existing direct optimization methods.
- The paper could benefit from a more comprehensive discussion of related works, particularly regarding direct optimization methods.
- There is a need for clearer differentiation between their approach and existing methodologies in the literature.
- The presentation could be improved, particularly in explaining transitions between sections and clarifying assumptions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their proofs by including the reasoning behind each step, such as stating how each step follows from previous results. Additionally, the authors should conduct experiments on highly nonlinear synthetic data distributions to validate the method's applicability beyond linear assumptions. A more thorough literature review and comparison with existing works on direct optimization should be included to clarify the advantages and limitations of their approach. We also suggest that the authors improve the related work section by incorporating discussions on direct optimization methods, specifically those involving self-play and online data collection for finding global optima. Finally, the authors should enhance the presentation by providing clearer explanations during transitions between sections and addressing the strong assumptions made in their theoretical analyses.