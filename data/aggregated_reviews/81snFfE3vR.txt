ID: 81snFfE3vR
Title: One-step differentiation of iterative algorithms
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 6, 7, 8, 5, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 5, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical study on one-step differentiation, also known as Jacobian-free Backpropagation, aimed at addressing the challenges of computing derivatives in bilevel optimization problems with equilibrium constraints. The authors demonstrate that one-step differentiation is effective in many scenarios, supported by theoretical and numerical results. They provide comparisons with existing Automatic and Implicit Differentiation techniques and suggest a K-step approach to enhance accuracy for linear algorithms. The paper includes various examples and corollaries beneficial for practitioners.

### Strengths and Weaknesses
Strengths:
- The paper conveys a clear and accessible message about the effectiveness of one-step differentiation, making it approachable for non-experts.
- It includes valuable insights, such as applying one-step differentiation to $F^K$ and using different operators for forward and backward passes.
- The bound on hypergradient approximation and the comparisons of gradient estimation complexities in Table 2 are commendable.

Weaknesses:
- The authors should consider discussing Inexact Automatic Differentiation (IAD) as a third benchmark approach and include it in numerical experiments.
- The notation in Lemma 3 is confusing and should be revised for clarity.
- The paper lacks a thorough discussion on the relationship between their work and that of [3], as well as potential limitations of the one-step differentiation method.

### Suggestions for Improvement
We recommend that the authors improve the discussion of Inexact Automatic Differentiation (IAD) and its relevance to their work, potentially incorporating it into their numerical experiments. We suggest clarifying the notation in Lemma 3 by using $\theta_i$ instead of $x_k$ and $g$ instead of $f$. Additionally, we encourage the authors to elaborate on the generation of QP instances and ensure that the time displayed in Figure 3 is clearly indicated in the caption. Lastly, we advise expanding the discussion on the limitations of the one-step differentiation method and the specific classes of algorithms that meet Assumption 1.