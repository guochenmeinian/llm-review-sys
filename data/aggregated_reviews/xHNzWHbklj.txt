ID: xHNzWHbklj
Title: Towards Better Dynamic Graph Learning: New Architecture and Unified Library
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 8, 5, 4, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel Transformer-based architecture, DyGFormer, for continuous-time dynamic graph learning, incorporating a neighbor co-occurrence encoding scheme and a patching technique to enhance dynamic edge representation. The authors also introduce DyGLib, a unified library designed for reproducible and scalable dynamic graph learning research, which includes various baselines and datasets. Extensive experiments on 13 benchmark datasets validate the effectiveness of the proposed method.

### Strengths and Weaknesses
Strengths:
1. The authors propose original designs, such as the neighbor co-occurrence encoding scheme and patching technique, which effectively capture correlations and long-term dependencies in dynamic graphs.
2. The paper includes comprehensive experiments across multiple datasets, providing valuable insights into the performance of both DyGFormer and re-implemented baselines.
3. DyGLib serves as a high-quality tool for researchers, promoting a standardized evaluation framework for dynamic graph learning.

Weaknesses:
1. The formalization of the dynamic graph learning problem inaccurately defines the target as learning time-aware node representations, while DyGFormer focuses on dynamic link representations. A discussion on this distinction is needed.
2. The authors mention differing baseline performances without specifying which baselines improved, necessitating explicit identification.
3. The paper's presentation suffers from excessive citations that hinder clarity, unclear problem statements regarding model optimization, and a lack of comprehensive technical discussions motivating the model's design.

### Suggestions for Improvement
We recommend that the authors improve the problem formalization in section 3 to clarify the focus on dynamic link representations rather than node representations. Additionally, explicitly listing which baselines showed performance improvements would enhance clarity. The authors should also reduce the number of citations per statement to a maximum of five and consider replacing some with concrete examples from their work. Furthermore, providing a formal definition of the training loss and clarifying whether the proposed method is task-dependent or task-independent would strengthen the model's presentation. Summarizing the encoding and patching procedures in pseudo-code, even in supplementary material, would aid in understanding. Lastly, addressing the scalability of the proposed method for larger datasets and discussing the implications of the inductive setting in the context of dynamic graph learning would be beneficial.