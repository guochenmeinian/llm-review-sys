ID: PomhVDrvco
Title: EpiK-Eval: Evaluation for Language Models as Epistemic Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the ability of language models (LMs) to learn from fragmented information, specifically focusing on their capacity to identify and combine related information for consistent inference. The authors propose a benchmark called Epik-Eval, which evaluates LMs' knowledge consolidation through a dataset of fragmented narratives transformed into question-answer pairs. The study finds that LMs, particularly T5 and Flan-T5, perform poorly on QA tasks when trained on fragmented data, leading to the conclusion that future work should adapt LMs' training objectives to better learn context beyond single documents.

### Strengths and Weaknesses
Strengths:
- The proposed dataset is novel and provides insights into LMs' epistemic capabilities and knowledge consolidation.
- The study is the first to effectively investigate LMs' ability to combine information from fragmented sources, which is crucial for various applications.
- The experimental analysis of T5 and Flan-T5 models is thorough, and the final recommendations are interesting and relevant.

Weaknesses:
- The fragmentation of the dataset (EpiK-Eval) is extreme, making it challenging even for humans to identify related fragments, which may limit the realism of the findings.
- The paper lacks clarity in several aspects, including the construction of prompts for segmented narratives and the methodology for dataset creation and validation.
- The evaluation does not sufficiently address how the results relate to existing multi-hop question answering frameworks, such as HotpotQA.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the dataset's construction and validation methodology to enhance understanding of the empirical study. Additionally, we suggest that the authors explore a range of fragmentation ratios to provide a more realistic evaluation scenario. It would also be beneficial to include a broader range of LLMs in the evaluation to strengthen the findings. Lastly, we encourage the authors to clarify the relationship between their work and existing literature on multi-hop QA and epistemic reasoning.