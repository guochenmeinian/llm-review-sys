ID: kxBsNEWB42
Title: Acceleration Exists! Optimization Problems When Oracle Can Only Compare Objective Function Values
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 5, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the problem of $\min_x f(x)$ using an order oracle that returns $sign(f(x)-f(y)+\delta)$, where $\delta$ is bounded noise. The authors propose a method integrating line search with randomized coordinate update algorithms, providing convergence rates for non-convex, convex, and strongly convex objectives, as well as an accelerated variant for strongly convex cases. The work extends to a stochastic setup, drawing connections to normalized SGD and deriving asymptotic convergence. The authors also introduce four new algorithms, including both unaccelerated and accelerated methods utilizing a deterministic Order Oracle, and analyze asymptotic convergence with a stochastic Order Oracle. They emphasize that their results are significant and open avenues for future research, while also acknowledging the potential for practical applications, which they plan to address in future experiments. Numerical experiments illustrate the performance of the proposed algorithms.

### Strengths and Weaknesses
Strengths:  
- The paper has clear exposition and adequately surveys related literature.  
- The problem is of significant interest to the machine learning community, with many applications.  
- The technical claims are sound, and the presented ideas are conceptually simple, demonstrating good performance across various settings.  
- The order oracle setting is novel, and the theoretical analysis appears solid.  
- The paper introduces four novel algorithms that contribute meaningfully to the field.  
- The theoretical framework is robust, with clear implications for future research.  
- The authors demonstrate that their approach can reduce computational resources and improve convergence.

Weaknesses:  
- The analysis claiming that "the convergence rate of random coordinate descent with the order oracle is equal to first-order method random coordinate descent" lacks surprise, as the algorithm searches for the best (1D) stepsize.  
- The motivation for using coordinate descent with the order oracle could be elaborated, as it seems somewhat ad-hoc.  
- The numerical experiments are limited, relying on simple functions and lacking comprehensive validation in real-world scenarios.  
- The accelerated algorithm's double-loop structure raises concerns about its ability to handle noise effectively, particularly regarding the additional log factor in oracle calls.  
- The paper does not adequately address the limitations of the proposed methods in high-dimensional settings.  
- The experimental section is perceived as lacking practical results, which may require further review.

### Suggestions for Improvement
We recommend that the authors improve the motivation section to provide a clearer justification for using coordinate descent with the order oracle. Additionally, expanding the experimental validation to include more complex functions and real-world problems would strengthen the paper's claims. It would also be beneficial to discuss the performance of the proposed methods in high-dimensional spaces and address any inherent limitations. Furthermore, we suggest improving the design of the double-loop algorithm to potentially reduce the log factor in oracle calls, as this remains a significant concern. Clarifying the necessity of the second line search in the accelerated algorithm and its implications for noise handling would enhance the paper's rigor. Lastly, we recommend revising the notation and ensuring all parameters are defined before their use to improve clarity.