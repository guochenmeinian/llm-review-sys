ID: Z3DS4Pcxct
Title: Addressing Uncertainty in LLMs to Enhance Reliability in Generative AI
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 7, 6
Original Confidences: 5, 4

Aggregated Review:
### Key Points
This paper presents a novel approach to uncertainty quantification (UQ) in large language models (LLMs) for generative tasks, particularly open-world question answering. The authors propose leveraging embedding-based semantic similarity to cluster outputs using a dynamic semantic clustering algorithm based on the Chinese Restaurant Process (DDCRP) and integrate conformal prediction to generate reliable prediction sets. Empirical results indicate that this method yields smaller, more precise prediction sets while maintaining high accuracy.

### Strengths and Weaknesses
Strengths:
1. The writing is clear and readable, with ample visualizations supporting the text.
2. The main contribution—utilizing semantic similarity for uncertainty assessment—is well-defined, with a clear explanation of its motivation.
3. Empirical results are compelling, demonstrating the effectiveness of the dynamic semantic clustering and conformal prediction methods through comparisons with various baselines and datasets.

Weaknesses:
1. The approach lacks a theoretical guarantee such as marginal coverage, which is standard in conformal prediction.
2. The focus is solely on question-answering tasks, neglecting broader applications like natural language generation and text classification.

### Suggestions for Improvement
We recommend that the authors improve the theoretical framework by providing guarantees such as marginal coverage for their approach. Additionally, we suggest expanding the scope of the paper to explore potential applications beyond question answering, including natural language generation and text classification. Furthermore, we encourage the authors to include more detailed explanations on the tuning and optimization of the parameters in the Chinese Restaurant Process and to provide comprehensive comparisons with traditional uncertainty estimation methods like Bayesian neural networks or Monte Carlo Dropout.