ID: oxj8mx4Sv2
Title: Deep Discriminative to Kernel Generative Networks for Calibrated Inference
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an algorithm aimed at improving out-of-distribution (OOD) calibration performance while retaining in-distribution performance of a base discriminative model, either a random forest or a multi-layer perceptron. The proposed method constructs an adjacency matrix based on "rule/activation patterns," performs clustering to segment the input space, and utilizes a Gaussian kernel for regression. The algorithm theoretically converges to the true posterior distribution with infinite training data. Experiments demonstrate superior out-of-distribution calibration compared to base models while maintaining comparable accuracy and in-distribution calibration.

### Strengths and Weaknesses
Strengths:
1. The algorithm's originality in identifying classifier-induced partitions enhances understanding of calibration properties (**Originality**).
2. The method guarantees asymptotic convergence to the true posterior in the infinite sample regime (**Soundness**).
3. The paper is generally clear, though it could benefit from more algorithmic detail (**Clarity**).
4. Code availability supports reproducibility, although it has not been tested by all reviewers (**Reproducibility**).

Weaknesses:
1. The performance of the algorithm in finite sample regimes and high dimensions is unclear, limiting the analysis to infinite data scenarios (**Significance**). The authors should provide experimental analysis on higher-dimensional datasets like CIFAR10 or CIFAR100.
2. Previous work indicates that neural networks are often over-confident on OOD data; an analysis in this context could strengthen the paper (**Quality**).
3. The relationship between the partitioning technique and spline-based approximation literature is not adequately explored (**Quality**).

### Suggestions for Improvement
We recommend that the authors improve the analysis of the algorithm's performance in finite sample regimes and high dimensions, potentially through experiments on datasets like CIFAR10 or CIFAR100. Additionally, providing an analysis of OOD classification phenomena, as highlighted in previous works, would enhance the paper's quality. The authors should also clarify the connection between their partitioning technique and existing literature on spline approximations. Furthermore, addressing the clarity of figures and the writing style, particularly in complex sentences, would improve overall readability. Lastly, including comparisons with recent methods, such as those discussed in reference [4], would provide necessary context and strengthen the experimental results.