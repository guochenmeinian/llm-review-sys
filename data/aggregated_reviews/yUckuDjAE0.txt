ID: yUckuDjAE0
Title: Learning Bregman Divergences with Application to Robustness
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 4, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to learn Bregman divergences using input-convex neural networks (ICNNs) to differentiate semantically meaningful image corruptions from random noise. The authors propose a procedure that mimics mirror descent over the learned Bregman divergence to enhance classifier robustness against image corruptions. Experimental results demonstrate that the method outperforms previous learned similarity metrics and generalizes well to unseen corruptions.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written, easy to follow, and includes effective visualizations.  
- The proposed method is novel and shows promising performance, attracting interest from the machine learning and robustness communities.  
- The authors provide clear explanations for their algorithmic choices, particularly regarding the use of Bregman divergences.

Weaknesses:  
- The technical presentation and results lack convincing depth, particularly regarding the claims of performing mirror descent due to limitations in the approximation of the inverse map and absence of a projection operator.  
- The reliance on a dataset describing corruptions raises questions about its reasonableness for benchmarks like CIFAR-10-C.  
- The comparisons against existing methods may not be fair, as they focus on specific perturbations while other methods operate under broader threat models.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims regarding mirror descent, potentially rephrasing it as an approach "inspired by mirror descent." Additionally, the authors should address the concerns about the dataset used for training the Bregman divergence and provide a more comprehensive discussion on the scalability of their approach to datasets like ImageNet-C. It would be beneficial to include more experiments across various datasets and perturbation types to strengthen the empirical validation of their method. Furthermore, the authors should clarify their architectural choices, particularly regarding the use of Hadamard square activations in ICNNs, and provide a detailed explanation of their implications. Lastly, we suggest revising the mathematical presentation for clarity, ensuring that all footnotes are correctly placed and that technical terms are defined appropriately for readers unfamiliar with the concepts.