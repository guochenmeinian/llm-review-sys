ID: vtC3sLXjDY
Title: How Reliable Are AI-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a reinforcement learning-driven technique for directing language models (LLMs) to generate text that can deceive AI-generated text detectors. The primary objective is to evaluate the reliability and trustworthiness of these detection systems. The authors propose a novel approach that utilizes "evasive soft prompts" learned through reinforcement learning, which can be transferred to other models with further fine-tuning, demonstrating that existing detectors are unreliable under adversarial conditions.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant and timely issue regarding the reliability of AI-generated text detectors.
- The proposed method effectively reveals the unreliability of current detection systems.
- The transferability of the evasive soft prompts enhances applicability to both current and future LLMs.
- The writing is clear, and the experimentation is extensive across various AI text detectors, baselines, and datasets.

Weaknesses:
- The paper does not address the scenario where human-written text is misclassified as AI-generated, impacting the overall reliability assessment of detectors.
- There is a lack of evaluation for the effectiveness of the evasive soft prompts, making it difficult to ascertain their quality.
- The method's applications are limited, requiring access to input embeddings of LLMs and being sensitive to the specific detectors used.
- No human evaluation is conducted to determine if the generated text resembles human-written text.

### Suggestions for Improvement
We recommend that the authors improve the paper by addressing the scenario where human-written text is incorrectly classified as AI-generated to provide a more comprehensive evaluation of detector reliability. Additionally, conducting an evaluation of the evasive soft prompts would help establish their effectiveness. Including human evaluation of the generated text would clarify its resemblance to human writing. Lastly, we suggest providing more details on the F1 analysis, including false positive and false negative rates, to enhance the robustness of the findings.