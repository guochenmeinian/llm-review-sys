ID: glfYOAzh2f
Title: Selective Generation for Controllable Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 8, 3, 7, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents Neuro-Selective Entailing-Generation (NSEGen), a novel approach aimed at enhancing the trustworthiness of generative language models (GLMs). The method extends selective classification to language generation tasks, utilizing textual entailment to assess semantic correctness between generated and true answers. NSEGen employs a semi-supervised approach that leverages both labeled and unlabeled data through an entailment set, addressing the challenge of costly entailment labeling. The authors introduce neuro-selection functions to optimize feature selection, aiming to minimize the false discovery rate (FDR) while providing theoretical guarantees on FDR control.

### Strengths and Weaknesses
Strengths:  
- The paper introduces a framework that effectively addresses the metric misalignment issue in GLMs, providing a more nuanced evaluation of language model outputs through selective generation and textual entailment.  
- The approach is underpinned by thorough theoretical analysis and guarantees, including a detailed proof for the correctness of the algorithm (Theorem 1) and conditions for achieving precision monotonicity (Lemma 3).  

Weaknesses:  
- The introduction of several new components and parameters may complicate implementation and tuning, potentially hindering practitioner adoption.  
- Experiments are limited to the Natural Question dataset with two models (GPT-3.5 and Alpaca-7B), raising questions about the method's generalizability across diverse language generation tasks.  
- There is repetitiveness in the Experiment section regarding GLMs and datasets, and clarity is needed in distinguishing the authors' approach from baseline comparisons.  
- The reliance on single-directional textual entailment may lead to accepting incorrect generations, and the baselines lack clear descriptions and alignment with current effective methods for selective generation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental section by providing a more detailed explanation of the baselines and their implementations. Additionally, it would be beneficial to include ablation studies to determine the amount of data required for training a robust selective algorithm, particularly in low-resource scenarios. We also suggest that the authors consider incorporating bi-directional entailment to enhance the robustness of their correctness determinations. Lastly, a concise algorithm box outlining the proposed approach would aid reader comprehension, and a thorough proofreading could address minor typographical errors.