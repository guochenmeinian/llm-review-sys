ID: vyraA7xt4c
Title: Mercury: A Code Efficiency Benchmark for Code Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 8, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Mercury, a benchmark designed to evaluate both the functional correctness and computational efficiency of LLM code generation. It includes a dataset of 1,889 Python tasks, each with efficiency baselines, and introduces a novel metric called Beyond, which assesses both correctness and efficiency. The empirical results indicate that Direct Preference Optimization (DPO) significantly enhances code efficiency compared to Supervised Fine-Tuning (SFT).

### Strengths and Weaknesses
Strengths:
- The paper proposes the first benchmark that comprehensively evaluates code efficiency alongside functional correctness for LLMs.
- Mercury includes an extensive dataset of 1,889 Python tasks with real-world efficiency baselines, allowing for thorough evaluation and comparison of different code generation models.
- The experimental setup is robust, featuring detailed comparisons between DPO and SFT, providing valuable insights into the performance of various models.

Weaknesses:
- The Beyond metric is highly dependent on the quality of the collected solutions, raising concerns about fairness if the benchmark solutions are biased.
- The coverage of test cases generated by GPT-4 may not ensure all edge cases are considered, potentially affecting the validity of comparisons.
- The paper lacks a verification process for the generated test cases, which could lead to untested scenarios being included in the evaluation.

### Suggestions for Improvement
We recommend that the authors improve the verification process for the test case generator to ensure that all generated test cases are adequately validated. Additionally, the authors should consider implementing an upper bound for the Beyond metric based on the fastest solution to mitigate potential biases. Furthermore, it would be beneficial to explore the performance of commercial LLMs on this benchmark and provide a more detailed explanation of the dataset construction process, particularly regarding the rationale for filtering tasks based on data structures.