ID: BrPZMOQiSN
Title: SequentialAttention++ for Block Sparsification: Differentiable Pruning Meets Combinatorial Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 6, 6, -1, -1, -1
Original Confidences: 2, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical understanding of differentiable pruning for neural networks, revealing connections to group lasso. The authors analyze sparse training with group sparsity and propose a general theoretical framework that encompasses both hard thresholding and scoring pruning methods. They introduce the algorithm Sequential Attention++, which combines AC/DC with block sparsification and score-based pruning, demonstrating state-of-the-art results through empirical evaluation.

### Strengths and Weaknesses
Strengths:  
1. The paper provides a robust theoretical framework for analyzing score-based sparsification methods, establishing the existence of a unique global minimizer.  
2. The unification of differential pruning with combinatorial optimizations represents a significant theoretical contribution, supported by strong empirical performance of the proposed method.  

Weaknesses:  
1. The authors claim that magnitude is not necessarily the best importance scoring metric, yet it has been observed to perform well in hard thresholding methods. Clarification on the performance of magnitude-based scoring in structured pruning and score-based sparsification is needed.  
2. The use of an unnormalized softmax in the proposed algorithm raises questions about its impact on results.  
3. The potential benefits of overparametrization when using a separate scoring parameter need further exploration.  
4. Comparisons with structured DST on ImageNet would enhance the evaluation of the proposed method.  
5. The necessity of the dense phase in the algorithm should be justified, particularly if the algorithm is tested without the AC/DC approach.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind the combination of Sequential Attention and AC/DC, providing stronger theoretical support for this choice. Additionally, the authors should address the questions regarding the performance of magnitude-based scoring in different pruning contexts and clarify the implications of using an unnormalized softmax. It would also be beneficial to include comparisons with structured DST for a more comprehensive evaluation and to justify the dense phase's necessity in the algorithm.