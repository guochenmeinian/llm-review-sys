ID: TDS3kqRteY
Title: REx: Data-Free Residual Quantization Error Expansion
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents REx, a novel quantization method that utilizes residual error expansion to enhance quantization accuracy. The authors propose a data-free approach to find the post-training quantization (PTQ) recipe based on a speed/accuracy trade-off by computing the residual expansion of weights and activations. The method minimizes the error between FP16 and INT quantizations by adding quantized residual errors and introduces selective error computations based on parameter magnitude. The authors provide a theoretical upper bound for quantization error and demonstrate its effectiveness through empirical comparisons with existing methods.

### Strengths and Weaknesses
Strengths:
- The proposed PTQ method is data-free and presents an interesting approach to quantization.
- The authors provide a theoretical basis for the error upper bound, which is greatly appreciated.
- The paper includes comparisons with a fair number of previous approaches, demonstrating improvements in accuracy.

Weaknesses:
- Figure 2 is difficult to read due to the color scheme, particularly for colorblind individuals; the authors should improve the color scheme.
- The claim regarding the sparsity of residual expansions leading to unstructured sparsity raises concerns about inference latencies, especially for devices that do not support such sparsity.
- The BOPS metric is unclear, and comparing methods at equivalent BOPS may not be fair due to hardware limitations.
- The assertion that the residue with W1/A16 incurs virtually no cost lacks clarity, particularly regarding computational overhead in compute-bound scenarios.
- The paper does not adequately discuss the impact of inference latencies, which significantly diminishes the work's impact.
- Several writing errors were noted, such as incorrect word usage in specific lines.

### Suggestions for Improvement
We recommend that the authors improve the color scheme of Figure 2 for better accessibility. Additionally, the authors should clarify the implications of unstructured sparsity on inference latencies and provide a more detailed discussion on the BOPS metric and its relevance to hardware capabilities. It would be beneficial to include results on latency when applying the proposed strategy, as latency is a critical real-world metric. Furthermore, we suggest that the authors elaborate on how they plan to address the computational overhead associated with the proposed method and clarify the budget $\gamma$ distinction between weight and computational overhead. Lastly, we encourage the authors to review the writing for grammatical accuracy and clarity.