ID: 04EC4ZnZJj
Title: MemoryFormer : Minimize Transformer Computation by Removing Fully-Connected Layers
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 5, 6, 6, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new transformer model named MemoryFormer, which replaces the fully-connected layer with a Memory Layer utilizing locality-sensitive hashing (LSH) to reduce computational complexity. The authors claim that this approach minimizes computations by focusing solely on self-attention, storing pre-computed features in hash tables for retrieval during inference. The MemoryFormer is evaluated on six tasks, demonstrating superior performance and reduced FLOPs compared to other efficient transformer models.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and easy to follow, addressing the computational efficiency of transformers by focusing on the fully-connected layer, which is a novel approach.  
- The use of LSH for linear projections and the gradient descent mechanism for learning hash tables enhances the model's feasibility for CPU deployment.  
- Extensive experiments show that MemoryFormer outperforms baseline models while consuming significantly fewer FLOPs.  

Weaknesses:  
- The authors do not provide the exact number of parameters for the different model sizes, which is essential for comparison.  
- There is a lack of an ablation study regarding the non-linearity in the feedforward network (FFN) module.  
- The paper does not discuss the training cost associated with frequent hashing and retrieval, which could impact VRAM I/O on GPUs.  
- The inference latency of MemoryFormer on CPU and GPU is not reported, which is critical for evaluating its practical application.  
- The formulation of some equations may be confusing without careful reading, and the paper lacks a comparison of the number of parameters between MemoryFormer and baseline models.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the parameter counts for different model sizes to facilitate better comparisons. Additionally, conducting an ablation study on the non-linearity in the FFN module would strengthen the analysis. It would also be beneficial to include a discussion on the training costs associated with the hashing and retrieval processes, as well as the inference latency on different hardware setups. Finally, enhancing the readability of the equations and providing a more detailed experimental setup would improve the overall evaluation of the MemoryFormer model.