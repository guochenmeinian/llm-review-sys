ID: mLJOMUwQyz
Title: INFORM : Information eNtropy based multi-step reasoning FOR large language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for selecting demonstrations and Chain-of-Thought (CoT) elements based on unigram-based entropy, favoring more surprising elements for their higher information content. The method dynamically selects the number of demonstrations and filters CoT elements that yield correct answers. The authors propose a comprehensive framework, INFORM, which employs information entropy for CoT prompt selection and inference, demonstrating superior performance across seven reasoning benchmarks using models like GPT-3.5-Turbo and text-davinci-003.

### Strengths and Weaknesses
Strengths:
- The approach of using information entropy for prompt selection is innovative and contributes to understanding in-context learning.
- The empirical validation is thorough, and the results are strong across various reasoning tasks.
- The authors conducted an ablation study to assess the effectiveness of each component of the proposed framework.

Weaknesses:
- The paper lacks clarity on specific benchmarks used for evaluating certain models and does not adequately specify the derivation of key parameters like H_min, H_max, N_min, and N_max.
- There is insufficient detail regarding the evaluation of open-source models, with a notable absence of larger models like OPT and BLOOM.
- The reliance on English language reasoning without consideration of linguistic characteristics may limit the generalizability of the results.

### Suggestions for Improvement
We recommend that the authors improve clarity by specifying the benchmarks used in the "Robustness of INFORM" section and detailing the derivation of values for H_min, H_max, N_min, and N_max. Additionally, it would be beneficial to include comparisons with larger open-source models and to address the environmental impact of their research by reporting on budget and CO2 emissions. Finally, we suggest that the authors discuss the implications of linguistic characteristics on their findings to enhance the paper's depth and applicability.