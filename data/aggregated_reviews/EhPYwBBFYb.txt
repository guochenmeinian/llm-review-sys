ID: EhPYwBBFYb
Title: UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents UPRISE (Universal Prompt Retrieval for Improving Zero-Shot Evaluation), a method that utilizes a lightweight retriever to automatically retrieve prompts for zero-shot evaluation of language models (LLMs). The authors propose a two-step process involving prompt retrieval and response generation, demonstrating superior performance compared to traditional prompt engineering methods. A comprehensive evaluation shows UPRISE's effectiveness across various tasks, emphasizing its cross-task generalization capability.

### Strengths and Weaknesses
Strengths:  
- UPRISE introduces a novel method addressing the challenge of task-specific prompt engineering, potentially improving LLM efficiency.  
- The paper provides a thorough evaluation, demonstrating the method's effectiveness against traditional approaches.  
- A detailed analysis of UPRISE's mechanisms enhances understanding and could inform future research.  
- The method shows consistent performance across multiple large LLMs and includes a clean implementation with code provided.  

Weaknesses:  
- The effectiveness of UPRISE may depend heavily on the quality and diversity of the training data, which could limit generalization.  
- The focus on zero-shot evaluation may not align with broader learning settings, potentially limiting its applicability.  
- The analysis of UPRISE's limitations in specific tasks, such as commonsense reasoning, is insufficient and requires more depth.  
- The paper lacks sufficient intuition about the types of prompts retrieved, which undermines claims of interpretability.

### Suggestions for Improvement
We recommend that the authors improve the analysis of the training data used for UPRISE, addressing its diversity and quality to clarify its impact on performance. Additionally, the authors should explore how UPRISE might generalize to few-shot learning or fine-tuning settings. We suggest incorporating real-world scenarios or complex tasks in testing to better demonstrate practical applicability. Furthermore, providing a visual representation of the prompt retrieval process could enhance understanding of the retriever's interpretability. Lastly, we encourage the authors to clarify the computational requirements and efficiency of the retriever compared to traditional methods.