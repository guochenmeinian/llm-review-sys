ID: LHKmzWP7RN
Title: Local LoRA: Memory-Efficient Fine-Tuning of Large Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, 4

Aggregated Review:
### Key Points
This paper presents Local LoRA, a method designed to fine-tune large language models on hardware with limited memory by dividing the model into chunks and training each sequentially. The authors propose a simple proxy model to compute gradients for each chunk without needing subsequent chunk gradients, enabling the fine-tuning of arbitrarily large models on a single GPU. The approach is evaluated through experiments on LLaMA2 7B and 13B parameter models, demonstrating its potential to address memory constraints in fine-tuning.

### Strengths and Weaknesses
Strengths:
- The proposal offers a straightforward and effective solution for fine-tuning large models under memory limitations, supported by experimental results.
- The method is novel, addressing the challenge of handling large datasets through chunking.
- The paper is well-written, with a clear structure and good technical quality, including a sound experimental setup and ablation studies.

Weaknesses:
- Performance deteriorates with an increased number of chunks, and the Local LoRA 13B model often underperforms compared to the E2E LoRA 7B solution.
- The comparison is limited to E2E LoRA, lacking evaluations against other methods like checkpointing and offloading.
- There are typesetting issues and a broken structure, including a missing conclusion and related works in the appendix.
- Implementation details are unclear, particularly regarding optimizer state management, and limitations of the proposed method are not well articulated.

### Suggestions for Improvement
We recommend that the authors improve the paper's structure by including a conclusion section and ensuring all related works are properly referenced. Additionally, providing comparisons with alternative fine-tuning methods beyond E2E LoRA would enhance the analysis. Clarifying implementation details, especially regarding optimizer states, is crucial for reproducibility. Finally, addressing the limitations of the Local LoRA method and its scalability to larger models would strengthen the paper's contributions.