ID: uhki1rE2NZ
Title: Parameter Symmetry and Noise Equilibrium of Stochastic Gradient Descent
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the effect of exponential symmetries on the learning dynamics of Stochastic Gradient Descent (SGD). The authors establish a theorem indicating that every exponential symmetry leads to a unique and attractive fixed point in SGD dynamics, which may elucidate various machine learning phenomena, including matrix factorization and the concepts of progressive sharpening or flattening. The paper also explores the relationship between SGD behavior and exponential symmetry, providing explicit solutions for noise equilibria in certain applications.

### Strengths and Weaknesses
Strengths:  
- The authors prove a novel theorem linking continuous symmetry to attractive fixed points in SGD, which is general and potentially impactful.  
- The application of this theorem to matrix factorization is supported by strong analytical and experimental evidence.  
- The paper addresses the relationship between SGD and exponential symmetry, contributing to the understanding of SGD dynamics.

Weaknesses:  
- The existence of the conserved quantity \( C \) is not guaranteed.  
- The paper relies on the trace of the Hessian for stability analysis, which may not accurately reflect the largest eigenvalue.  
- There is a lack of empirical verification for the theorem regarding noise leading to balanced norms in deep linear networks.  
- Clarity issues exist in the analyses, particularly in Section 5.4 regarding the relationship between Equation 24 and Figure 4.  
- The mathematical preliminaries and assumptions are considered insufficient, and the explanations of contributions are lacking.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their analyses, particularly in Section 5.4, to better relate Equation 24 to Figure 4. Additionally, empirical verification of the theorem regarding noise and balanced norms in deep linear networks should be included to strengthen the paper. The authors should also address the concerns regarding the assumptions made, especially the existence of \( C \) and the reliance on the trace of the Hessian. Clarifying the definitions and relationships in the mathematical preliminaries, such as the explicit definition of \( \Sigma_v(\theta) \) and the conditions under which the covariances are isotropic, would enhance the paper's rigor. Finally, we encourage the authors to differentiate their work from similar studies and to provide practical examples of empirical risk minimization that satisfy their assumptions.