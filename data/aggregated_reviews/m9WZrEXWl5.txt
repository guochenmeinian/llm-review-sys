ID: m9WZrEXWl5
Title: Directional Smoothness and Gradient Methods: Convergence and Adaptivity
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel concept of directional smoothness, which replaces the traditional smoothness constant \(L\) with a function \(M(x,y)\) that characterizes smoothness along the line between points \(x\) and \(y\). The authors derive sub-optimality bounds for gradient descent (GD) in the deterministic convex setting, demonstrating that these bounds can adapt to local smoothness properties. The paper empirically evaluates the theoretical claims through logistic regression problems and shows that Polyak and normalized GD can achieve convergence guarantees based on directional smoothness without altering the algorithms. Additionally, the authors claim to improve upon existing results by relaxing the L-smoothness assumption, although this relaxation only weakens the assumption to a uniform bound on smoothness between consecutive points.

### Strengths and Weaknesses
Strengths:
1. The introduction of directional smoothness generalizes existing smoothness notions while allowing for meaningful algorithm analysis.
2. The convergence analysis provides insights into how algorithms can adapt to local smoothness properties.
3. The authors present a comprehensive range of properties for directional smoothness and methods for computing upper bounds.
4. The reconstruction of prior theorems represents an exciting contribution to the field and engages with significant technical questions effectively.

Weaknesses:
1. Many results prior to Section 4.2 are direct consequences of the descent inequality, raising questions about their novelty.
2. The relaxation of the L-smoothness assumption does not fully meet the expectations set by the authors' claims, as it only establishes a uniform bound on smoothness.
3. The paper is limited to deterministic optimization, neglecting the stochastic setting, which is prevalent in modern optimization.
4. The novelty of some results, particularly Theorem 4.4, is unclear, as it appears to replace \( \eta_t \) with a potentially inferior \( M(x_{t+1},x_t) \).
5. The proof technique for GD convergence does not adequately address local/path-wise or directional smoothness, lacking clarity on the definitions and convergence proofs.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the applicability of their framework to stochastic optimization, as this would enhance the credibility of their approach. Additionally, we suggest that the authors clarify the technical novelty of their results, particularly regarding the transition from global to adaptive smoothness. It would be beneficial to include examples or applications that demonstrate the advantages of their adaptive bounds over classical methods. Furthermore, we encourage the authors to enhance their proof techniques to address the limitations regarding local/path-wise and directional smoothness, providing detailed convergence proofs. Addressing potential inconsistencies in notation and assumptions throughout the paper would improve clarity and precision. Lastly, we suggest exploring the implications of their findings in non-convex settings, as this could broaden the impact of their work.