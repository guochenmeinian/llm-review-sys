ID: lvibangnAs
Title: Unifying Generation and Prediction on Graphs with Latent Graph Diffusion
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Latent Graph Diffusion (LGD) model that unifies various tasks such as classification, regression, and generation across node-level, edge-level, and graph-level features. The authors propose treating regression and classification as conditional generation tasks, employing a latent diffusion model to generate features. Extensive experiments demonstrate competitive performance across different graph-based tasks.

### Strengths and Weaknesses
Strengths:
1. The reformulation of regression and classification problems as diffusion generation tasks is innovative.
2. The technical foundation of the paper is sound, with a well-motivated introduction of latent diffusion.
3. The unified framework is theoretically supported and covers a comprehensive range of tasks and datasets.

Weaknesses:
1. The evaluations are insufficient, lacking comparisons with classical diffusion-based models and larger datasets, which diminishes the robustness of the findings.
2. The approach to treating classification and regression tasks as generation tasks requires a simple baseline to validate the complexity introduced.
3. The model's architecture and training details are unclear, and the paper lacks an open-source implementation.
4. Some claims regarding the model's capabilities are overstated or inaccurate, particularly concerning the ease of 3D information generation and the novelty of the framework.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including comparisons with classical models such as DiGress, HGGT, and DruM, particularly on larger datasets like MOSES. Additionally, providing a simple baseline for the classification and regression tasks would clarify the necessity of the proposed complexity. The authors should also include a detailed table outlining the training dataset sources, sizes, and model architecture parameters. Furthermore, clarifying the decoder's role in reconstructing structural information and ensuring that all relevant figures are included in the main text would enhance the paper's clarity. Lastly, addressing the overclaims regarding the model's capabilities and including more effective evaluation metrics like QED and TPSA would strengthen the paper's contributions.