ID: UXSqUOMwbE
Title: QA-NatVer: Question Answering for Natural Logic-based Fact Verification
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents QA-NatVer, a natural logic inference system for fact verification that utilizes a dynamic programming approach for proof selection, employing a multi-granular merging mechanism to construct proofs from non-overlapping claim chunks. The authors propose a scoring function that integrates NatOp probability and verdict scores to optimize assignments. The system demonstrates improved accuracy and explanation capabilities in fact verification, outperforming existing baselines in a few-shot setting on the FEVER dataset.

### Strengths and Weaknesses
Strengths:  
- The paper introduces a novel method combining natural logic with question answering, significantly contributing to the field.  
- It is well-written and straightforward, with thorough testing across multiple datasets and languages.  
- The QA-NatVer system shows strong performance, particularly in low-resource settings, and enhances the explainability of neural fact verification systems.  

Weaknesses:  
- The limited number of proof annotations on the test set (only 32) undermines the robustness of the findings.  
- The experimental comparisons with multiple baselines using different pre-trained models lack fairness.  
- The method's applicability may be restricted to simpler claims, raising concerns about its effectiveness on complex, naturally-occurring claims.  
- There is insufficient discussion on integrating the approach with large language models (LLMs), and the pipeline's efficiency requires clarification.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the proof selection process by providing state transition equations or figures. Additionally, addressing the integration of their method with current LLMs, such as InstructGPT and ChatGPT, would enhance the paper's relevance. Including discussions on the method's performance with complex claims and providing an efficiency analysis of the pipeline would also be beneficial. Lastly, the authors should consider including more references to related works, particularly regarding logic-regularized reasoning, to strengthen their arguments.