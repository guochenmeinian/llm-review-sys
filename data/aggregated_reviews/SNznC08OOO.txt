ID: SNznC08OOO
Title: RoboDepth: Robust Out-of-Distribution Depth Estimation under Corruptions
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 6, 9, 7, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents RoboDepth, a benchmark designed to evaluate the robustness of depth estimation models against common distortions encountered in real-world scenarios. The authors introduce a dataset comprising 18 types of disturbances categorized into four groups: weather/lighting conditions, sensor failures or movements, and data processing issues. They assess 42 existing depth estimation models, revealing many lack robustness due to inadequate testing methods. The authors emphasize the importance of evaluating model performance under diverse operating conditions and propose strategies for enhancing robustness. Additionally, the paper provides performance metrics for training and testing on both clean and corrupted data, highlighting the impact of training distribution shifts on model performance. The work is well-documented and includes a GitHub repository for further access.

### Strengths and Weaknesses
Strengths:
- The RoboDepth benchmark addresses a significant gap in current testing procedures by evaluating depth estimation models under realistic corruptions.
- The dataset is comprehensive, covering 18 corruptions across various categories, facilitating thorough evaluations of models.
- The paper effectively benchmarks depth estimation models against a variety of corruption types, providing valuable insights into their robustness.
- The dataset specifications and experimental validation are thorough and solid, with comprehensive experimental results and discussions enhancing the paper's contributions.
- The authors have made significant improvements based on reviewer feedback, including clarifying the tone and structure of the paper and enhancing the discussion of benchmarking methodologies.

Weaknesses:
- The reliance on synthetic corruptions raises concerns about their realism and applicability to real-world scenarios, potentially limiting the generalizability of the findings.
- The paper primarily focuses on benchmarking existing models without sufficiently addressing how this work can inform future improvements in monocular depth estimation.
- Some sections, such as the explanation of out-of-distribution scenarios, could benefit from further clarity.
- While major concerns were addressed, there may still be areas for further improvement.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the dataset details, including the quantity of samples for each corruption type and the quantitative measurement of severity levels. Additionally, we suggest providing a more precise explanation of what qualifies as an out-of-distribution (OoD) situation and how it contrasts with in-distribution scenarios. It would also be beneficial to discuss the societal impact of using synthetic data and to include performance metrics comparing training and testing on clean versus corrupted data to clarify the reasons for performance degradation. Finally, while the simulated corruptions are useful, we encourage the authors to explore the integration of real-world datasets to enhance the robustness evaluation further and expand the discussion on the implications of the findings for future research in depth estimation robustness.