ID: sLhXMkI0kx
Title: On skip connections and normalisation layers in deep optimisation
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 7, 4, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical framework for analyzing optimization in deep architectures, particularly focusing on the roles of architectural components such as skip connections and normalization layers. The authors claim to provide the first proof that certain deep networks can converge to global optima at infinity using gradient descent, even when such optima only exist at infinity, as with cross-entropy loss. They argue that the optimization benefits of skip connections may stem from improved loss landscape conditioning and support this with empirical results demonstrating slight performance improvements through modified skip connections.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated, addressing the critical question of bridging theory and practice in deep learning optimization.
- It is clearly written and contextualizes itself within existing literature effectively.
- The contributions include a global analysis of convergence far from initialization and practical insights into architectural design considerations.

Weaknesses:
- The theoretical bounds, particularly regarding the benefits of skip connections, may not translate well into practice due to unrealistic assumptions and worst-case scenarios.
- The claim that skip connections aid optimization by improving loss regularity lacks novelty, as similar assertions have been made in prior works.
- Experimental results are limited, particularly concerning hyperparameter tuning, and further validation across different optimizers and architectures is needed.
- Important related works on skip connections and singular values are not adequately cited or discussed.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation of their claims by conducting additional experiments with tuned hyperparameters across various optimizers and architectures. It would also be beneficial to clarify the implications of their theoretical results, particularly regarding the assumptions made in their analysis. Furthermore, we suggest that the authors include discussions of relevant literature on skip connections and singular values to strengthen the paper's context and impact. Lastly, providing a more accessible presentation of their findings could enhance the paper's clarity and reach within the machine learning community.