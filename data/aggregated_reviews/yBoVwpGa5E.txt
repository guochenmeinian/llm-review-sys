ID: yBoVwpGa5E
Title: Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 4, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into robust overfitting (RO) in adversarial training (AT) through a minimax game perspective. The authors argue that robust overfitting arises from an imbalance between the model trainer and the attacker, particularly after learning rate (LR) decay, which disrupts the balance between fitting and attacking abilities. This imbalance leads the trainer to capture more non-robust features, resulting in robustness degradation. The authors propose the ReBalanced Adversarial Training (ReBAT) method to mitigate this issue and improve robustness. They also discuss how adding training data can help prevent RO by increasing the difficulty for the model to memorize non-robust features. Empirical results support their claims, demonstrating the effectiveness of their approach, including a detailed explanation of how the inner maximization objective of AT involves untargeted attacks, where misclassified adversarial examples indicate the presence of non-robust features.

### Strengths and Weaknesses
Strengths:
1. The study of AT from a minimax game perspective is intriguing and provides a holistic understanding of robust overfitting.
2. The experiments conducted are solid and provide comprehensive empirical validation across various datasets and network architectures, reinforcing the proposed explanations.
3. The paper offers a detailed explanation of how additional training data can help prevent RO.

Weaknesses:
1. The motivation for the study is unclear, with the authors' explanations of robust overfitting relying on observation-driven analyses that lack clarity. Specific terms like "false non-robust mapping" need intuitive definitions.
2. The explanation regarding the relative strength of the attacker and trainer as the trigger for RO lacks convincing rigor and does not adequately address various empirical behaviors, such as the impact of additional training data.
3. The novelty of the proposed method is limited, as existing research has addressed similar issues, such as the work by Yu et al. on strength-adaptive adversarial training, and similar viewpoints have been discussed in prior literature without substantial new insights.
4. Experimental results are incomplete; individual performances of the proposed techniques should be reported, and comparisons with existing methods like AWP need clarification.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their motivation and provide more intuitive explanations for key concepts related to robust overfitting. Additionally, we suggest that the authors enhance the theoretical foundation of their claims by providing rigorous proof of the relationship between non-robust features and robust overfitting. The authors should also enhance the novelty section by acknowledging and discussing existing research that overlaps with their contributions, particularly in relation to the minimax game perspective. To strengthen the experimental section, we suggest reporting the individual performances of each technique used in ReBAT and clarifying the robustness comparisons with AWP. Finally, we encourage the authors to include theoretical analyses that complement their empirical findings, potentially using game theory to deepen the understanding of the proposed method's implications.