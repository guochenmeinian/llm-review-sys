ID: 2jUKhUrBxP
Title: Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 5, 5, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called Parameter-Efficient and Quantization-aware Adaptation (PEQA) aimed at efficiently fine-tuning Large Language Models (LLMs). PEQA combines quantization and parameter-efficient fine-tuning, updating only a small fraction of model weights while significantly reducing memory usage. The authors demonstrate PEQA's effectiveness through comprehensive experiments, showing it outperforms Post Training Quantization (PTQ) and has acceptable degradation compared to LoRA, although the accuracy loss is notable. The primary experiments utilize the Alpaca datasets, comparing PEQA with other efficient tuning methods.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow.
2. The idea and method are clearly articulated.
3. The evaluation is thorough, encompassing various benchmark tasks and model sizes.
4. The approach significantly reduces memory usage, enhancing accessibility for fine-tuning LLMs.

Weaknesses:
1. The degradation of accuracy compared to LoRA is non-negligible.
2. The paper primarily combines existing techniques rather than introducing a novel method.
3. The clarity of certain statements and the explanation of memory usage during fine-tuning require improvement.
4. The evaluation setup's baseline settings are unusual and could mislead readers regarding PEQA's performance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology section, particularly regarding the gradient computation for scaling factors. Additionally, the authors should provide a more thorough discussion on the limitations of PEQA compared to similar methods like AlphaTuning. It would also be beneficial to include a clearer explanation of memory usage during gradient calculation and to address the unexpected evaluation results observed in the experiments. Finally, we suggest that the authors enhance the baseline comparisons by including post-training quantization methods applied before or after fine-tuning, such as LoRA and Adapter, to provide a more comprehensive assessment of PEQA's effectiveness.