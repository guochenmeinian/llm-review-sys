ID: blC2kbzvNC
Title: Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence and Variance Reduction
Conference: NeurIPS
Year: 2023
Number of Reviews: 31
Original Ratings: 2, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two adaptive methods, AdaSPS and AdaSLS, along with theoretical analyses of PSGD utilizing these methods. The authors claim that AdaSPS and AdaSLS achieve optimal convergence rates for convex and strongly convex functions, supported by numerical results. Additionally, the authors introduce adaptive methods AdaSVRPS and AdaSVRLS, asserting optimal asymptotic rates in both strongly-convex or convex and interpolation or non-interpolation settings, although they acknowledge that the rates for strongly-convex non-interpolation settings are not optimal. The authors also address concerns regarding the proof of Theorem 1, particularly the treatment of parameters \(c_l\) and \(c_p\) as constants rather than random variables, which could affect the validity of their claims. The contribution primarily lies in the introduction of these step-sizes and their application to variance reduction techniques.

### Strengths and Weaknesses
Strengths:
1. The paper is well-structured and clearly written, facilitating comprehension.
2. AdaSPS and AdaSLS demonstrate competitive convergence rates across various settings, particularly in strongly convex scenarios without requiring problem-dependent parameters.
3. The integration of variance reduction techniques with the proposed methods is innovative and could inspire further algorithm development.
4. The proposed methods show competitive performance in practice and offer flexibility in updating the full gradient based on computational power.
5. The authors provide a novel approach to integrating line-search with variance-reduction techniques, potentially overcoming previous limitations.

Weaknesses:
1. The focus on convex optimization limits the applicability of the findings, as deep learning problems are often nonconvex.
2. The theoretical soundness is questioned due to doubts regarding Lemmas and Theorems, particularly concerning the existence of lower bounds for step-sizes.
3. Numerical experiments are insufficient, lacking evaluations on deep neural networks and relying solely on synthetic datasets.
4. The claimed optimal rates are not achieved in all settings, particularly for strongly-convex non-interpolation cases.
5. The treatment of parameters in the proof raises concerns about the validity of the results, as they should depend on stochastic information rather than being treated as constants.

### Suggestions for Improvement
We recommend that the authors improve the theoretical framework by addressing the doubts raised regarding Lemma 17 and the existence of lower bounds for step-sizes. Additionally, the authors should provide examples of satisfying the interpolation condition and clarify the practical application of \(\ell_{i_t}^\star\) and \(\eta_t\) in deep learning contexts. We also suggest that the authors improve the clarity of their claims regarding the optimality of rates, explicitly stating the exceptions for strongly-convex non-interpolation settings. Furthermore, we encourage the authors to revise the proof of Theorem 1 to ensure that the treatment of parameters \(c_l\) and \(c_p\) aligns with their stochastic nature, thereby reinforcing the validity of their results. Expanding the numerical results to include training on benchmark datasets such as CIFAR-100 and ImageNet would enhance the paper's relevance and demonstrate the proposed algorithms' effectiveness in real-world scenarios. Lastly, we recommend clarifying the separation of variance-reduction and interpolation methods in their manuscript to avoid confusion.