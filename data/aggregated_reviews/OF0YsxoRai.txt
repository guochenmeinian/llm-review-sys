ID: OF0YsxoRai
Title: Scalable Bayesian Optimization via Focalized Sparse Gaussian Processes
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FocalBO, a novel method for scaling Bayesian Optimization (BO) to large datasets by employing a sparse Gaussian Process (GP) model, termed focalized GP. The authors propose fitting a sparse GP at various scales, optimizing the acquisition function for each scale, and selecting the next query point based on a probability proportional to its acquisition function value. FocalBO is shown to maintain the same $\mathcal{O}(m^3)$ computational complexity as sparse variational GPs (SVGP), contrasting with TuRBO's $\mathcal{O}(n^3)$ complexity when using exact GPs. Empirical results indicate that FocalBO significantly reduces GPU memory usage compared to TuRBO, demonstrating superior scalability and performance in both large offline and online settings. The algorithm is evaluated against several benchmarks with a high number of data points, addressing the critical and underexplored issue of scaling BO to large datasets.

### Strengths and Weaknesses
Strengths:  
- The paper addresses the critical issue of scaling BO to large datasets, filling a significant gap in existing methods.  
- FocalBO introduces a novel sparse GP model that enhances acquisition function optimization and effectively utilizes large offline datasets.  
- The algorithm shows significant performance improvements over existing baselines, particularly in terms of GPU memory usage and scalability.  
- The reasoning and motivation behind the approach are conceptually clear, and the empirical studies, especially in Sections 5.2 and 5.3, are compelling, featuring challenging high-dimensional problems.  

Weaknesses:  
- The empirical nature of the paper necessitates more baseline comparisons, particularly with a naive "keep closest N TuRBO" approach.  
- The claim that "FocalBO with TuRBO is the first GP-based method to achieve top-tier performance" lacks sufficient justification; direct comparisons with MBO baselines or specific performance metrics from prior work are needed.  
- The theoretical implications section is underdeveloped, lacking new mathematical insights or computational experiments to support claims.  
- The direct comparison between FocalBO and TuRBO may not fully account for the differences in computational complexity and resource requirements.  
- The paper does not adequately discuss its limitations, particularly regarding the implications of the Lreg term in Equation 8 and the choice of search region center.  

### Suggestions for Improvement
We recommend that the authors improve the empirical evaluation by including more baseline comparisons, specifically with the naive "keep closest N TuRBO" method. Additionally, please provide direct comparisons with MBO baselines or specific performance metrics from prior work to substantiate the claim regarding top-tier performance. 

We suggest enhancing the theoretical implications section by including new mathematical insights and empirical evidence to support the claims made. Furthermore, we encourage the authors to improve the clarity of the comparison between FocalBO and TuRBO by explicitly detailing the implications of their differing computational complexities. Lastly, we recommend discussing the limitations of their approach, particularly regarding the choice of the search region center and the implications of the Lreg term in Equation 8.