ID: B6HSIgvyJ3
Title: A Batch-to-Online Transformation under Random-Order Model
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 6, 8, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for converting offline approximation algorithms into online algorithms suitable for the random-order model, where samples are presented randomly rather than adversarially. The authors emphasize the significance of average sensitivity in this transformation, demonstrating that offline algorithms with low average sensitivity can effectively solve corresponding online problems. The framework is applied to various problems, including online clustering, low-rank matrix approximation, and regression, yielding low (epsilon-approximate) regret.

### Strengths and Weaknesses
Strengths:
- The framework is general and applicable to various online learning problems, providing a unified approach.
- The connection between regret and average sensitivity is well-articulated and insightful.
- The paper is coherent, well-written, and presents non-trivial results that may interest the NeurIPS community.

Weaknesses:
- The technical novelty is limited, as the method combines known results in a straightforward manner without introducing new algorithmic techniques.
- There is a lack of experimental validation, particularly in comparison to existing works, such as those by Cohen-Addad et al. (2021).
- The results are restricted to approximate regret bounds, which may be seen as a significant limitation.

### Suggestions for Improvement
We recommend that the authors improve the paper by including numerical experiments to validate the performance of their framework against existing methods, particularly those by Cohen-Addad et al. (2021). Additionally, clarifying whether the method can handle the case with epsilon=0 would enhance the paper's contribution. Providing a more detailed context regarding the relationship between their approach and prior works, such as Garber et al. (2020) and Sherman et al. (2021), would also be beneficial. Finally, addressing the limitations of approximate regret bounds and exploring potential guarantees in adversarial settings could strengthen the paper's impact.