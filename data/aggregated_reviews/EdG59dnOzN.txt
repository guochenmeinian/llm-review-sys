ID: EdG59dnOzN
Title: Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 4, 7, 6, 7, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel algorithm, KATE, which achieves scale-invariance for Generalized Linear Models and demonstrates optimal convergence guarantees in smooth nonconvex settings. The authors provide a thorough theoretical analysis and empirical results, showing KATE's effectiveness compared to existing algorithms like AdaGrad and Adam, particularly in terms of convergence rate and efficiency.

### Strengths and Weaknesses
Strengths:  
- KATE exhibits impressive scale-invariance properties and matches the convergence rates of AdaGrad and Adam, which is a significant algorithmic finding.  
- The paper is well-written and easy to follow, with comprehensive numerical experiments that validate KATE's performance across various machine learning tasks.  

Weaknesses:  
- The paper lacks a detailed discussion on the limitations of KATE, particularly in scenarios where its assumptions may not hold.  
- The experiments do not convincingly demonstrate the practical impact of scale-invariance, and there is insufficient analysis of KATE's performance in real-world applications compared to traditional adaptive algorithms.  

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of KATE, particularly regarding scenarios where its assumptions may not hold. Additionally, providing insights into KATE's computational efficiency and scalability with larger datasets would enhance its practical applicability. We suggest conducting experiments that clearly demonstrate the scale-invariance property of KATE compared to AdaGrad and Adam, particularly using simulated data in logistic regression. Finally, we encourage the authors to explore the implications of poor data scaling in training large AI models, as this could significantly enhance the paper's impact.