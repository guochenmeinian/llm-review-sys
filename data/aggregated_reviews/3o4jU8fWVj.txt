ID: 3o4jU8fWVj
Title: EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 5, 5, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents EquiformerV2, an enhanced version of the original Equiformer architecture aimed at improving performance in 3D molecular modeling. The authors propose several architectural modifications, including the use of SO(2) linear layers to replace SO(3) tensor products, attention re-normalization, separable S² activation, and separable layer normalization. These changes are intended to improve computational efficiency and model performance on benchmarks such as OC20 and AdsorbML.

### Strengths and Weaknesses
Strengths:
1. The paper is well-organized and clearly written, with a coherent structure.
2. Empirical results demonstrate that EquiformerV2 achieves state-of-the-art performance on the OC20 benchmark, with significant improvements in test energy MAE.
3. The introduction of efficient parameterizations and architectural modifications is a valuable contribution to the field.

Weaknesses:
1. There is insufficient comparison with the original Equiformer architecture, particularly regarding the impact of the new SO(2) linear layers.
2. The novelty of integrating eSCN convolutions and S² activation is limited, as these concepts are primarily derived from previous works.
3. The motivations for certain architectural changes are unclear, and further quantitative evidence is needed to support their necessity.

### Suggestions for Improvement
We recommend that the authors improve the comparison against the original Equiformer architecture by including performance metrics for EquiformerV1 in all relevant tables. Specifically, a side-by-side comparison of EquiformerV1 and V2 while keeping other hyperparameters fixed would be beneficial. Additionally, we suggest providing error bars in the ablation studies to assess statistical significance and including a comprehensive analysis of the computational resources used for training and inference.

Clarifying the description of the separable S² activation and justifying the necessity of higher-order representations compared to increasing layers or channels would enhance the paper's clarity. Furthermore, we encourage the authors to benchmark EquiformerV2 on the QM9 dataset, as it would provide valuable insights into its performance across different datasets. Lastly, addressing the unclear motivations for architectural modifications with quantitative evidence would strengthen the paper's contributions.