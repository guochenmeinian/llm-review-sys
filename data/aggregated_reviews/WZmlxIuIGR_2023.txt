ID: WZmlxIuIGR
Title: Safety Gymnasium: A Unified Safe Reinforcement Learning Benchmark
Conference: NeurIPS
Year: 2023
Number of Reviews: 41
Original Ratings: 6, 6, 8, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Safety-Gymnasium, a benchmark for safe reinforcement learning (SafeRL) research, designed to enhance usability and scalability over previous environments like Safety Gym. The authors propose a comprehensive suite that includes diverse environments, safety constraints, and a collection of algorithms known as Safe Policy Optimization (SafePO). Key features include an upgraded physics engine, multi-agent scenarios, and the integration of complex tasks such as DexterousHands. The authors also address challenges related to transferring learned policies to real-world applications and the difficulties inherent in sim-to-real transitions.

### Strengths and Weaknesses
Strengths:
- The availability of Safety-Gymnasium and the SafePO library promotes collaborative research and supports safer AI development.
- The benchmark includes a diverse range of safety-critical tasks and constraints, facilitating rigorous evaluation.
- The scalability and flexibility of Safety-Gymnasium are well-recognized, making it a promising platform for long-term research.
- Significant technical improvements, such as the upgrade to MuJoCo 2.3, simplify installation and enhance performance.
- The authors demonstrate a commitment to addressing real-world applicability through ongoing efforts to transition virtual experiments to physical robots.
- The framework offers a user-friendly design suitable for beginners, with a focus on flexibility and adaptability for various research needs.

Weaknesses:
- The organization and dependencies of the SafePO library are complex, which may hinder usability and maintainability.
- The paper lacks sufficient justification for the selection of environments, and the analysis of algorithm results is insufficient.
- There is a lack of thorough analysis regarding the performance consistency of baseline algorithms, particularly in relation to constraint satisfaction.
- The originality of the work is questioned, as it appears to draw heavily from prior research without introducing substantial new ideas.
- Limited discussion of the platform's limitations, particularly regarding the realism of tasks and sensor fidelity, could hinder understanding of the benchmark's weaknesses.
- Concerns remain regarding oscillations in reward and constraint curves of the SafePO algorithm, which may affect its viability for future research.

### Suggestions for Improvement
We recommend that the authors improve the organization of the SafePO library by minimizing dependencies and ensuring a more modular design to enhance usability. Additionally, the authors should provide a detailed description of the visual tasks in Appendix A.5 or indicate them as future work. A thorough analysis of baseline algorithm performance, particularly addressing inconsistencies in constraint satisfaction and oscillation issues, is essential for ensuring reliability. Furthermore, we suggest including discussions on failure cases to enhance the benchmark's value for researchers. A clearer justification for the choice of environments, along with a comprehensive evaluation of performance metrics, would strengthen the paper's contributions. Finally, we encourage the authors to clarify the submission history to alleviate any concerns regarding dual submissions.