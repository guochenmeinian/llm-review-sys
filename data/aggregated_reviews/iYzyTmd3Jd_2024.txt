ID: iYzyTmd3Jd
Title: CooHOI: Learning Cooperative Human-Object Interaction with Manipulated Object Dynamics
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for learning multi-human cooperative object manipulation, specifically focusing on collaborative carrying tasks. The authors propose a two-stage method where, in the first stage, a single agent learns object carrying from motion capture data using the Adversarial Motion Priors (AMP) framework. In the second stage, multi-agent reinforcement learning fine-tunes the policy for collaborative carrying. The experiments demonstrate that the trained policy effectively manages collaborative tasks with objects of varying shapes and weights.

### Strengths and Weaknesses
Strengths:
1. The cooperative interaction learning framework effectively learns multi-human collaborative behaviors using only single-human motion capture data, achieving better generalization than tracking-based methods.
2. Implicit communication through object dynamics bridges the single-human and multi-human stages, enhancing collaborative learning.
3. The paper includes extensive ablation studies and boundary analyses, providing valuable insights into the method's performance.

Weaknesses:
1. The simulation employs an oversimplified humanoid model without hand representation, limiting interaction capacity, as acknowledged by the authors.
2. The agent's observation features may be inadequate for complex manipulations, as they primarily consist of self-motion and cropped object bounding box features, lacking a global overview necessary for intricate tasks.
3. The need for separate policies for object categories with similar shapes indicates a generalization issue.

### Suggestions for Improvement
We recommend that the authors improve the humanoid model by incorporating dexterous hand representation to enhance interaction capabilities. Additionally, the authors should expand the agent observation features to include a global overview of objects and collaborators to facilitate more complex manipulations. To address the generalization problem, we suggest exploring methods that allow for a single policy to handle a broader range of object shapes and weights. Furthermore, clarifying the definitions of terms like "held point" and providing explanations for new concepts introduced in the main text would enhance comprehension. Lastly, we encourage the authors to conduct experiments with a more diverse set of object shapes and motion styles to assess the robustness of their framework.