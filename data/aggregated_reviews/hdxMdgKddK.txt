ID: hdxMdgKddK
Title: DocTrack: A Visually-Rich Document Dataset Really Aligned with Human Eye Movement for Machine Reading
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a significant advancement in understanding Visually-Rich Documents (VRDs) by introducing DOC TRACK, a unique dataset that aligns human eye movement data with document comprehension tasks. The authors propose a preordering pipeline that integrates human-like reading orders into AI models, aiming to enhance their performance in understanding VRDs. The paper also evaluates the impact of these human-like reading orders on AI models, revealing that while improvements have been made, AI still struggles to comprehend VRDs as effectively as humans.

### Strengths and Weaknesses
Strengths:
- The introduction of DOC TRACK as a benchmark dataset is a novel contribution that can facilitate future research in VRD reading order generation.
- The proposed preordering pipeline offers a practical solution to improve AI models' understanding of VRDs.
- Comprehensive evaluations, both intrinsic and extrinsic, provide valuable insights into the efficacy of human-like reading orders.

Weaknesses:
- The dataset lacks a valid/dev set, complicating system tuning.
- The small sample size of human participants (five) raises concerns about the dataset's robustness.
- Some experimental results contradict the dataset's setup and assumptions, with insufficient analysis provided to explain these discrepancies.

### Suggestions for Improvement
We recommend that the authors improve the dataset by including a valid/dev set to facilitate better tuning of the system. Additionally, it would be beneficial to open-source the baseline systems to support community engagement. The authors should also address the discrepancies in experimental results by providing further analyses and clarifications. Furthermore, we suggest incorporating additional experiments to explore the agreement among annotators and to detail the OCR engine used, as well as to clarify the reading order's phrase-level application. Lastly, acknowledging the time-intensive nature of the experimental procedure as a limitation and offering insights for future research would enhance the paper's contributions.