ID: ZgDNrpS46k
Title: Analyzing & Reducing the Need for Learning Rate Warmup in GPT Training
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 6, 6, 7, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of learning rate warmup in neural network training, particularly focusing on the GPT-2 model. The authors identify the necessity of warmup and propose modifications to the Lion optimizer to address issues related to large initial updates. They analyze the impact of warmup on training dynamics and suggest methods for controlling parameter updates and representation changes. The study aims to provide insights into the mechanistic underpinnings of warmup and potential optimizations to eliminate its necessity.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant topic, providing a systematic understanding of the learning rate warmup heuristic.
- The analysis of normalized Gradient Descent is insightful and reproduces known scaling laws.
- The writing is clear, and the experiments are well-motivated.

Weaknesses:
- The authors do not adequately discuss the relevant paper, "On the Variance of the Adaptive Learning Rate and Beyond," which raises questions about their contributions.
- Conclusions are primarily based on intuitive explanations and a simple GPT-2 experiment, lacking convincing evidence.
- The analysis of representation changes using linear transitions appears overly simplistic.
- The experiments are limited to a fixed setup, raising concerns about generalizability across different architectures and datasets.
- The organization of the paper is somewhat messy, making it difficult to follow.

### Suggestions for Improvement
We recommend that the authors improve the discussion of the paper "On the Variance of the Adaptive Learning Rate and Beyond" to clarify their unique contributions. Additionally, the authors should provide more persuasive evidence to support their claims and consider narrowing the scope of the title to better reflect the content. We suggest that the authors explore the relationship between gradient clipping and adaptive learning rates. Improving clarity and structure by including detailed explanations and transitions between sections would enhance readability. Furthermore, the authors should specify the lowest learning rate used in their experiments and consider using clearer terminology, such as "update step" or "effective learning rate," instead of "update size." Lastly, moving the limitations discussion from the appendix to the main text would improve visibility.