ID: SqW42eR2wC
Title: Offline Inverse Constrained Reinforcement Learning for Safe-Critical Decision Making in Healthcare
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 3, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 2, 4, 2, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on inverse reinforcement learning (IRL) with learnable constraints in the offline setting, specifically targeting healthcare applications such as sepsis and mechanical ventilation. The authors propose the Constraint Transformer (CT) framework, which integrates historical patient data into constraint modeling and employs a generative world model to enhance safety in decision-making. The approach combines decision transformer architecture with inverse constrained reinforcement learning (ICRL) to address the limitations of Markovian assumptions in healthcare contexts. Additionally, the paper explores the action space of drug dosages in a medical context, focusing on the agent's decision-making process. The authors intend to include naive baselines in their experiments to better observe differences in actions and acknowledge the need for further experiments, including LSTM ablation studies and testing for other types of dangerous actions in continuous action space environments.

### Strengths and Weaknesses
Strengths:
- The paper tackles a significant gap in ICRL applications by incorporating historical data into decision-making.
- It effectively models constraints using a generative world model, demonstrating improved safety in healthcare applications.
- Extensive evaluations on healthcare benchmarks show that the proposed CT framework reduces unsafe behaviors and approximates lower mortality rates.
- The authors demonstrate a clear understanding of the complexities involved in modeling drug dosages and decision-making in a medical environment.
- The intention to add naive baselines and conduct additional experiments reflects a commitment to improving the robustness of their findings.

Weaknesses:
- The writing is incoherent, which significantly hinders understanding, with many notations undefined and critical concepts poorly explained.
- The dependence on generated violating data raises concerns about the sensitivity of the estimated policy to the generative world model.
- The definition of metrics and optimal policies is flawed, particularly in relation to mortality rates and the implications of the behavior policy on evaluation results.
- The paper lacks completed experiments, particularly regarding LSTM ablation and the testing of various dangerous actions.
- The definition of "too high" dosages may be unclear, and the authors have not fully explored the implications of their action space design.

### Suggestions for Improvement
We recommend that the authors improve the clarity and coherence of the writing, ensuring that all notations are defined and critical concepts are clearly articulated. Specifically, clarify the definition of the metric $\omega$ and its relation to mortality rates, as well as the implications of the behavior policy on evaluation results. Additionally, provide more details on how the DIFF between the estimated policy and the physicians' policy is calculated, and consider including naive baselines in the evaluation to strengthen the results. We also suggest expediting the completion of LSTM ablation experiments and other planned tests to enhance the depth of their study. Furthermore, improve clarity regarding the definition of "too high" dosages and ensure that the categories of dangerous actions are thoroughly tested. Lastly, consider conducting ablation studies to justify the use of the transformer architecture over simpler models like RNNs, particularly given the short-term nature of the datasets used.