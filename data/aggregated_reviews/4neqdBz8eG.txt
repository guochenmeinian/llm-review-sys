ID: 4neqdBz8eG
Title: Rethinking Weight Decay for Robust Fine-Tuning of Foundation Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 6, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Selective Projection Decay (SPD), a novel weight decay technique that selectively imposes penalties on certain layers of deep networks to enhance both in-distribution (ID) and out-of-distribution (OOD) performance. The authors clarify that SPD's performance gains stem solely from this selectivity, as discussed in Section 3.3. They demonstrate the effectiveness of SPD through extensive experiments across various image classification, segmentation, and reasoning tasks, showing improved performance compared to baseline methods, including L2 regularization and AdamW. The authors also address the interplay between pre-trained models and datasets, noting that fine-tuning can lead to performance improvements. Additionally, they provide insights into hyper-parameter selection and the correlation between aggressive regularization and OOD accuracy.

### Strengths and Weaknesses
Strengths:
- The method is well-motivated and simple to implement, making it accessible for users.
- The exposition of SPD is clear, with strong empirical results across multiple benchmarks.
- The authors effectively address reviewer concerns and clarify the mechanics of SPD.
- The approach is compatible with parameter-efficient fine-tuning methods, enhancing its practical applicability.
- Empirical results are robust, demonstrating the significance of the proposed method.
- The paper is well-written, with clear intuition and motivation, particularly in sections 3.3 and 3.4.

Weaknesses:
- The presentation has minor issues, such as the lack of a preliminaries section to define notations and dimensions.
- The selective nature of SPD is not clearly articulated in Algorithm 2, leading to confusion regarding layer-specific penalties.
- The theoretical analysis of the method is insufficient, as noted by multiple reviewers.
- Some experimental comparisons are lacking, particularly against Elastic Weight Consolidation (EWC) and other exploration-based optimizers.
- Some aspects of hyper-parameter selection and its implications remain unclear.
- The results on ImageNet are not entirely clear, particularly regarding the performance trends across different domains.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Algorithm 2 by explicitly indicating how layer-specific penalties are applied. Additionally, including a short preliminaries section to define all notations and their dimensions would enhance understanding. We suggest improving the theoretical analysis of the proposed method to provide a deeper understanding of its mechanisms and implications. Furthermore, clarifying the hyper-parameter selection process, particularly for ImageNet, would enhance the paper's comprehensibility. Conducting ablation studies to isolate the contributions of selective projection and adaptive regularization would also be beneficial. Lastly, we recommend comparing SPD against EWC and other relevant methods to provide a more comprehensive evaluation, and consider converting Tables 2a and 2b into figures for improved readability.