ID: Ur9f4hNIpN
Title: Predictor-Corrector Enhanced Transformers with Exponential Moving Average Coefficient Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a transformer architecture inspired by predictor-corrector methods for solving ordinary differential equations (ODEs). The authors propose a framework that utilizes the Adams-Bashforth-Moulton method and an exponential moving average (EMA) for coefficient learning, enhancing the stability and accuracy of transformer models in tasks such as translation and summarization. The proposed architecture shows improvements over standard and previous ODE-based transformers, with empirical results indicating better performance across various benchmarks. Additionally, the authors conduct experiments on time-series forecasting and image classification tasks, providing an analysis of the practical costs associated with training and inference for both encoder-decoder and decoder-only paradigms. Results from a 3B PCformer using 16B and 50B tokens indicate no significant challenges in scaling up.

### Strengths and Weaknesses
Strengths:  
- The integration of high-order ODE solutions into transformer architecture is innovative, providing a new perspective on cross-layer connections.  
- The use of EMA for coefficient learning enhances training stability and flexibility.  
- The paper is well-structured and clearly written, with comprehensive experimental results demonstrating the efficacy of the proposed model.  
- Comprehensive experiments on diverse tasks (time-series forecasting and image classification).  
- Detailed analysis of training and inference costs for different paradigms.  
- Clear results demonstrating the scalability of the 3B PCformer.

Weaknesses:  
- The experiments are insufficient and lack up-to-date benchmarks, particularly in scaling law style experiments.  
- There is a lack of emphasis on the computational overhead introduced by the high-order predictor and multi-step corrector methods.  
- The theoretical justification for the predictor-corrector framework could be more detailed, and comparisons with state-of-the-art models are missing.  
- The review does not specify any weaknesses regarding the clarity of results and explanations, but the authors may need to ensure clarity in their findings.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including formal experiments on modern LLM benchmarks and scaling law style evaluations. Additionally, quantifying the computational overhead and providing detailed analyses of training and inference costs compared to standard transformers would enhance the practical applicability of the proposed methods. Clarifying the theoretical aspects of the predictor-corrector framework and including comparisons with state-of-the-art models would strengthen the paper's contributions. Furthermore, we recommend that the authors improve the clarity of their results and explanations to better address reviewer concerns. Finally, discussing the generalizability of the proposed methods to other domains beyond natural language processing could broaden the impact of the work.