ID: DFb1gwnhQS
Title: FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 8, -1, 7, -1, -1
Original Confidences: 3, -1, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents the FIRE dataset, a feedback-refinement resource comprising 1.1M multi-turn conversations derived from 27 source datasets. It enables Vision Language Models (VLMs) to refine responses based on feedback across diverse tasks. The dataset is collected in two stages: FIRE-100K, generated by GPT-4V, and FIRE-1M, produced by a model trained on FIRE-100K. The authors propose the Fire-Bench benchmark, consisting of 11K feedback-refinement conversations, to evaluate VLMs' feedback-refining capabilities. Fine-tuning LLaVA on FIRE-100K and FIRE-1M results in the FIRE-LLaVA model, which outperforms untrained VLMs by 50%.

### Strengths and Weaknesses
Strengths:
- The paper introduces a significant feedback-refinement dataset that integrates data from 27 diverse sources, providing a valuable resource for instruction tuning and evaluating VLMs.
- The authors propose FIRE-Bench, an evaluation benchmark that spans both seen and unseen datasets, enabling comprehensive assessment of feedback-refinement capabilities in VLMs.
- The clarity of the paper is commendable, presenting necessary information regarding data generation and the proposed benchmark in an accessible manner.

Weaknesses:
- The authors mention the possible range for scores late in the paper, complicating the understanding of score choices.
- There is limited human curation due to the dataset's large size.
- The performance drop in 5 benchmarks raises concerns, as 3 out of 5 show significant declines without discussion on the reasons.
- The choice of a single baseline model (LLaVA-Next-8B) limits the evaluation's robustness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of score ranges by presenting this information earlier in the paper. Additionally, discussing the reasons behind the performance drops in benchmarks would enhance the analysis. Including multiple baseline models could strengthen the evaluation. The authors should clarify the differing criteria for filtering dialogues in FIRE-100K and FIRE-1M. Providing the value of T_baseline and the proportion of n_k = 1 per model would also be beneficial. Lastly, enhancing the figures for clarity, such as consistent coloring and reframing instructions as questions, would improve readability.