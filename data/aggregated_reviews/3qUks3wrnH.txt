ID: 3qUks3wrnH
Title: Efficient Multi-task Reinforcement Learning with Cross-Task Policy Guidance
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Cross-Task Policy Guidance (CTPG), a framework aimed at enhancing multi-task reinforcement learning (MTRL) by utilizing cross-task policy similarities. CTPG trains a guide policy for each task to select the most appropriate behavior policy from a pool of control policies across tasks, thereby improving training trajectories and learning efficiency. The authors introduce two gating mechanisms: a policy-filter gate to eliminate non-beneficial control policies and a guide-block gate to prevent unnecessary guidance for tasks that have been mastered. Empirical evaluations on benchmarks such as MetaWorld and HalfCheetah indicate that CTPG, when integrated with existing parameter sharing methods, significantly boosts performance.

### Strengths and Weaknesses
Strengths:
- The paper introduces an innovative method that leverages cross-task similarities in MTRL, providing an efficient way to exploit shared skills.
- Extensive empirical validation demonstrates significant improvements in learning efficiency and performance across various benchmarks.
- The paper is well-structured, clearly explaining the CTPG framework and effectively using figures and tables to illustrate its benefits.

Weaknesses:
- Scalability of CTPG to more complex environments is not adequately discussed, limiting the understanding of its practical utility.
- The computational complexity of training and deploying CTPG, particularly regarding the guide policy and gating mechanisms, is not explicitly addressed, which raises concerns about resource requirements and execution time.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the scalability of CTPG to more complex environments, addressing potential challenges and limitations for practical deployment. Additionally, we suggest that the authors provide a detailed analysis of the computational requirements for training and deploying CTPG, including execution time and resource consumption compared to existing MTRL methods. Furthermore, clarifying how the guide step K is determined for new MTRL domains and exploring the applicability of other RL algorithms beyond SAC would enhance the paper's robustness. Lastly, we encourage the authors to improve the clarity of Section 4 by providing sufficient intuitions before introducing each component of the proposed method.