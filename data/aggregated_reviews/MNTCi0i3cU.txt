ID: MNTCi0i3cU
Title: InstructSafety: A Unified Framework for Building Multidimensional and Explainable Safety Detector through Instruction Tuning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents INSTRUCTSAFETY, a unified framework integrating seven common subtasks of safety detection based on different instructions. The authors propose an instruction collection comprising 39 datasets and a method for augmenting adversarial samples to enhance model robustness. Extensive experiments validate the proposed method, demonstrating promising results.

### Strengths and Weaknesses
Strengths:
- The paper introduces a unified framework for safety detection tasks, which is well-structured and relevant.
- The instruction collection with 39 datasets supports future research and integration with Language Models (LLMs).
- The proposed method for generating augmented data is reasonable and shows state-of-the-art performance.

Weaknesses:
- The framework's ability to extend to new safety evaluation tasks is unclear.
- The discussion on the quality and diversity of the instruction collection is insufficient.
- The evaluation lacks performance metrics for adversarial examples and does not compare Flan-T5 with other models across multiple datasets.
- There is a need for clarity regarding the definition of "safety detection" and its exclusion of adversarial robustness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how the framework can incorporate new safety evaluation tasks in the future. Additionally, the authors should provide a detailed discussion on the diversity of the instruction collection and its impact on instruction tuning. It is essential to evaluate the performance of the generated adversarial examples and include baseline comparisons for the subtasks of Target Detection and Safety Explanation. Finally, the authors should clarify the scope of "safety detection" to encompass various aspects of model robustness, including adversarial attacks.