ID: argZAtDMMF
Title: FLOP: Tasks for Fitness Landscapes Of Protein wildtypes
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 7, 5, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark for predicting the function of wildtype proteins, focusing on train/test set construction amidst non-iid data. The authors benchmark traditional machine learning methods, including random forests and zero-shot learners, across three datasets, revealing mixed results that suggest previous model performance comparisons may be overly optimistic. The study investigates the generalization ability of representation learning models based on protein sequence, structure, and evolution for predicting mutation effects, employing phylogenetic information to prevent information leakage during training. Additionally, the authors propose new methodologies for evaluating protein embeddings, emphasizing the importance of avoiding pooling operations that may filter out critical information. They discuss the differences between hold-out validation and random partitioning in their experimental design and clarify the motivation behind their ablation studies. The paper also addresses the limitations of their datasets, particularly regarding the inclusion of model-generated sequences and the challenges of dataset size.

### Strengths and Weaknesses
Strengths:
- The paper is well written and clearly differentiates its contributions from prior benchmarks.
- The authors provide a well-motivated use of specialized cross-validation splits and benchmark several algorithms.
- The inclusion of ablation studies and careful empirical analyses enhances the understanding of dataset construction importance.
- The authors provide a clear distinction between hold-out validation and random partitioning, enhancing the understanding of their methodology.
- They acknowledge and address the limitations of their datasets, particularly the implications of using model-generated data.
- The incorporation of ProteinMPNN as a zero-shot predictor adds value to their findings.

Weaknesses:
- The authors do not adequately discuss the limitations of their benchmark design, focusing instead on model results.
- The dataset sizes are small, raising concerns about their representativeness for mutation effect prediction tasks.
- Certain sections, particularly those discussing the datasets and their relevance, lack sufficient detail and logical connections.
- The writing quality still requires further editorial refinement to correct grammatical errors and enhance clarity.
- The paper does not adequately address the potential societal impacts of their work.
- There is a lack of detailed exploration of the limitations of the benchmark design itself, beyond model results.

### Suggestions for Improvement
We recommend that the authors improve the discussion of the limitations of their benchmark design, specifically addressing the challenges associated with using model-generated sequences and justifying the representativeness of their small datasets. Additionally, the authors should provide a clearer rationale for including artificial sequences from Monte Carlo simulations and demonstrate their validity. To enhance dataset representativeness, we suggest constructing a more comprehensive dataset, potentially referencing ProteinGym for additional datasets. The authors should also consider incorporating various partitioning methods to address the complex epistasis landscape in mutation effect prediction. Furthermore, we encourage the authors to explore alternative sequence representation methods, such as convolutional approaches, and to verify pre-trained model capabilities in a zero-shot context. Lastly, improving the clarity and coherence of the writing, particularly in sections 4 through 8, is essential for better reader comprehension, and it is crucial for the authors to explicitly address the limitations and potential negative societal impacts of their work, particularly concerning the use of model-generated data.