ID: i28zCSsQIc
Title: GloptiNets: Scalable Non-Convex Optimization with Certificates
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 7, 6, 8, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 4, 1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GloptiNets, a method for non-convex global optimization on the hypercube, utilizing non-negativity certificates applicable to any function with computable Fourier coefficients. The methodology allows for flexibility, enabling the enhancement of certificate accuracy through the k-SOS method. The algorithm is compatible with GPU computation and is benchmarked against the TSSOS library on six low-dimensional problems, demonstrating similar or inferior performance when the number of coefficients is small.

### Strengths and Weaknesses
Strengths:
- The methodology is flexible, not limited to polynomial functions, and does not impose structural assumptions beyond the hypercube constraint.
- The paper is well-written, with clear theoretical derivations and a comprehensive exploration of the framework's implementation.

Weaknesses:
- Some technical explanations are difficult to follow, and the title may mislead regarding the optimization context.
- The experiments are limited to low dimensions, with benchmarks that do not reflect real-world optimization problems, raising concerns about practical applicability.
- The accuracy of the certificates is low compared to interior-point methods, and the computational time required to achieve higher accuracy remains unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity of technical explanations and consider revising the title to better reflect the optimization context. To enhance the practical applicability of GloptiNets, we suggest conducting more extensive experiments on real-world problems where optimality certificates are critical. Additionally, the authors should address the low accuracy of the certificates by exploring ways to increase the number of samples, the size of the neural network, and the training duration. It would also be beneficial to provide estimates regarding the computational time required to achieve accuracy comparable to TSSOS. Lastly, we encourage the authors to clarify the role of the R matrix and the choice of kernel functions in the optimization process.