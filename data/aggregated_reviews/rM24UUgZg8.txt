ID: rM24UUgZg8
Title: Activating Self-Attention for Multi-Scene Absolute Pose Regression
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 4, 5, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into multi-scene absolute pose regression (APR) using transformers, focusing on the distortion of query-key embeddings in self-attention. The authors propose a Q-K alignment loss and fixed sinusoidal positional encoding to mitigate this distortion, demonstrating improved performance on indoor and outdoor datasets. However, the contributions are questioned due to the reliance on prior work regarding query-key distortion.

### Strengths and Weaknesses
Strengths:
- The originality of applying Q-K distortion analysis to multi-scene APR is notable, with the proposed fixed-position encoding showing better performance than learnable encodings.
- The algorithm is straightforward and the paper is well-written, making it accessible.

Weaknesses:
- The contribution regarding Q-K distortion is not sufficiently novel, as it draws heavily from existing literature, raising doubts about its significance.
- Experimental results show only marginal improvements over baseline methods, which diminishes the perceived contribution of the work.
- The motivation for multi-scene APR lacks validation against single-scene methods, and comparisons with state-of-the-art single-scene techniques are missing.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contribution by providing a deeper discussion on the implications of query-key distortion specifically for the APR task. Additionally, we suggest including comparisons with single-scene state-of-the-art methods to validate the claimed advantages of multi-scene APR in terms of speed and memory efficiency. A more comprehensive ablation study is also advised to clarify the contributions of the proposed auxiliary loss versus fixed positional encoding. Lastly, we encourage the authors to enhance the literature discussion to contextualize their findings within the broader APR landscape.