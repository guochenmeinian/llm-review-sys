ID: cw6v58yo6s
Title: Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a pipeline that automatically generates a multi-turn chat corpus using ChatGPT, introducing a chatting model named Baize, which is trained with parameter-efficient tuning based on LLaMA. The authors apply reinforcement learning with self-feedback to enhance Baize's performance. The framework also proposes an efficient method for instruction-tuning LLMs, utilizing self-chat for dialogue collection and ChatGPT's feedback as an alternative to RLHF. The evaluation of Baize is conducted using the LM Evaluation Harness library and GPT-4 evaluator, highlighting its efficiency.

### Strengths and Weaknesses
Strengths:
- The paper proposes valuable resources, including a dataset and model, for the research community.
- It introduces novel techniques such as self-chat and self-distill, which avoid human annotation and feedback, respectively.
- The framework is considered efficient and reproducible compared to existing methods like Vicuna.

Weaknesses:
- The novelty of the proposed pipeline is limited, as similar approaches have been explored in prior works.
- There is a lack of quality control and human evaluation in the data collection process.
- The evaluation is primarily automated, lacking detailed performance analysis and comparisons with other models.
- Baize's results are not competitive on leaderboards, raising concerns about the effectiveness of the proposed pipeline.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their unique contributions and how they distinguish their work from existing research. Additionally, we suggest incorporating human evaluations to assess the quality of the generated data and addressing the potential for hallucinations in the model's responses. It would also be beneficial for the authors to provide a detailed analysis of Baize's performance compared to other models and explore methods to enhance its competitiveness. Finally, we encourage the authors to clarify their future plans for improving the pipeline and consider the implications of using full-parameter fine-tuning versus LoRA.