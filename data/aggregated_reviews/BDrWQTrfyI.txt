ID: BDrWQTrfyI
Title: BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents BAM (Branch-Attend-Mix), a method for enhancing Mixture of Experts (MoE) models by leveraging parameters from pre-trained dense models. The authors propose initializing both feed-forward network (FFN) and attention layers from specialized dense models, which improves MoE training efficiency and performance. Evaluations on language models demonstrate BAM's superiority over the baseline BTX in perplexity and downstream tasks.

### Strengths and Weaknesses
Strengths:
- The results indicate that BAM outperforms BTX under the same data and compute budget in both perplexity and downstream task evaluations.
- The paper is well-written, with clear figures and a logical flow that enhances comprehension.
- The approach of using Mixture of Attention (MoA) addresses limitations of parameter averaging in previous work.

Weaknesses:
- The technical novelty is low, as the paper primarily combines existing methods (BTX and Mixture of Attention) without significant new contributions.
- The specialized models do not appear sufficiently specialized for their tasks, undermining the conclusions drawn from Table 3.
- The parallel-attention transformer architecture is not widely used; more ablation studies are needed to assess its impact on performance and efficiency.
- The MoA requires "additional engineering tricks for optimization," necessitating further ablation studies comparing performance with BTX under the same inference throughput budget.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the specialized models' performance in Table 3 to strengthen their conclusions. Additionally, conducting more ablation experiments on the parallel-attention transformer architecture would provide insights into its scalability and efficiency. We suggest comparing FLOP and latency at inference time for both BAM and BTX to better understand the cost implications. Finally, exploring the role of soft routing in MoA layers from both empirical and theoretical perspectives would enhance the paper's depth.