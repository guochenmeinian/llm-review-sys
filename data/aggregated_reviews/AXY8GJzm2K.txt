ID: AXY8GJzm2K
Title: Learn From One Specialized Sub-Teacher: One-to-One Mapping for Feature-Based Knowledge Distillation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel feature distillation loss function that aligns the student model’s last hidden layer representation with the teacher’s neuron by neuron, differing from existing MSE or cosine distance loss functions. The authors propose a one-to-one mapping approach, optimizing a subset of neurons for a per-feature distillation loss function, claiming this variation yields additional gains. The empirical results demonstrate promising performance improvements over existing methods across various natural language processing tasks.

### Strengths and Weaknesses
Strengths:
- The paper effectively illustrates the proposed idea with clear visualizations and precise mathematical formulations.
- Empirical results indicate that the proposed loss function enhances student model performance, supported by statistically significant repeated runs.
- The implementation is straightforward and practical for real-world applications.

Weaknesses:
- Some claims lack sufficient justification, such as the stability of the proposed loss function and its computational efficiency.
- The paper does not provide adequate details on the datasets used, assuming reader familiarity with tasks like SQUAD-V1 or IMDB.
- The analysis of the proposed loss function is insufficient, particularly regarding the necessity of its components and their contributions to effectiveness.
- The paper does not compare its approach with more advanced knowledge distillation methods or conduct ablation studies.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for the proposed One-to-One Mapping approach, addressing its advantages over global metrics like MSE or cosine distance. Additionally, the authors should compare their method with advanced knowledge distillation techniques such as Contrastive Representation Distillation and Decoupled KD. We suggest providing more detailed explanations of the datasets and evaluation metrics used in the experiments. Furthermore, conducting ablation studies to assess the impact of different components of the proposed method would strengthen the analysis. Lastly, addressing the typos in the cosine distance formula and ensuring clarity in the loss functions presented in figures would enhance the paper's presentation.