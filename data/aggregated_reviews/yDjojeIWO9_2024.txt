ID: yDjojeIWO9
Title: Transferable Adversarial Attacks on SAM and Its Downstream Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 8, 4, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel adversarial attack method called Universal Meta-Initialized and Gradient Robust Adversarial Attack (UMI-GRAT) targeting fine-tuned derivatives of the Segment Anything Model (SAM). The authors address the challenge of executing attacks without prior knowledge of downstream tasks or data distributions. They provide theoretical insights into adversarial update deviations and introduce a gradient robust loss to enhance attack transferability. The effectiveness of UMI-GRAT is validated through extensive experiments across various downstream tasks.

### Strengths and Weaknesses
Strengths:
1. The work addresses a significant security issue in deploying large foundation models in real-world applications, exploring a challenging scenario where adversarial attacks occur without prior knowledge of the task or data.
2. The authors provide a detailed theoretical analysis and a robust experimental framework, demonstrating superior performance of UMI-GRAT compared to previous methods.
3. The proposed method is unique, motivated by real-world safety concerns, and backed by comprehensive experiments.

Weaknesses:
1. The novelty of the approach is questioned, as components like downstream agnostic adversarial examples and meta-learning-based fast initialization have been previously explored in the literature.
2. The methodology section lacks clarity, and the randomness of experimental results is not adequately addressed.
3. The authors should provide a more comprehensive analysis of UMI noise and include additional evaluation metrics for a richer analysis.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology section by summarizing formulas as Theorems and placing proofs in supplementary material. Additionally, we suggest providing a comprehensive analysis of UMI noise, including the size of the natural image dataset and the effects of various hyperparameters. It would also be beneficial to include further data on additional evaluation metrics, such as $E_\phi$ and $F_\beta^\omega$, to enrich the analysis. Furthermore, the authors should clarify how their work differs from existing literature on downstream agnostic adversarial examples and meta-learning-based methods. Lastly, expanding the experimental evaluation to include other large foundation models would provide valuable insights into the generalizability of the proposed attack.