ID: qInb7EUmxz
Title: Persistence Homology Distillation for Semi-supervised Continual Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 5, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new method, PsHD, aimed at preserving intrinsic structural information in semi-supervised continual learning through the application of persistent homology and knowledge distillation. The authors propose using a memory buffer to calculate PsHD loss between previous and new network variants. The experiments indicate some improvements over baseline methods, supported by comprehensive ablation studies.

### Strengths and Weaknesses
Strengths:
1. The paper is well organized, making the ideas easily comprehensible, and the motivation is clearly articulated.
2. The application of persistent homology to continual learning is novel, and the experimental validation is extensive, covering various setups.
3. The visualization of attention maps effectively illustrates how the method enhances continual learning.

Weaknesses:
1. The novelty of the paper is limited, as the concept of knowledge distillation in continual learning is not new. The authors should provide comparisons with traditional knowledge distillation methods and relevant literature.
2. The effectiveness of the proposed method appears constrained, with improvements of less than 1% in many cases, particularly on simpler datasets, raising concerns about the robustness of the claims.
3. Some essential details are missing, such as an explicit equation for $L_{CL}$ and a relevant reference that could contextualize the work.
4. The writing and presentation require refinement for clarity, particularly in figures and language that may overstate the results.

### Suggestions for Improvement
We recommend that the authors improve the clarity and presentation of the paper, particularly in figures like Figure 4, which could benefit from enhancements. Additionally, we suggest including comparisons with traditional knowledge distillation methods to substantiate the novelty of PsHD. It would be beneficial to address the limited effectiveness of the proposed method by providing a more thorough analysis of performance improvements across different datasets. Finally, we encourage the authors to explicitly define $L_{CL}$ and include any missing references to strengthen the manuscript.