ID: FeCWZviCeP
Title: Hierarchical Programmatic Option Framework
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 8, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents HIPO, a hierarchical programmatic option framework that utilizes deep reinforcement learning to generate programmatic policies for long-horizon, repetitive tasks. HIPO retrieves historical programs through neural embeddings, employing these as reusable options to address recurring tasks. The authors evaluate HIPO on the Karel problem sets, demonstrating its effectiveness and zero-shot generalizability.

### Strengths and Weaknesses
Strengths:
1. Effective approach: HIPO addresses a challenging problem with a practical solution, particularly evident in long-horizon tasks, as shown by results in Karel-long.
2. Thorough ablation studies: The paper includes comprehensive ablation studies that highlight the optimal performance of HIPO compared to other variants.
3. Novel reward functions: The authors introduce reward functions that enhance the diversity and compatibility of the program skill set obtained by the cross-entropy method (CEM).

Weaknesses:
1. Lack of retrieval evaluation: There is insufficient empirical evidence demonstrating that the proposed techniques improve the effectiveness, diversity, and compatibility of retrieved programs.
2. Execution time considerations: HIPO does not account for variance in execution time when incorporating a discount factor of 0.99, which could lead to inconsistencies in the theoretical objective and actual loss function.
3. Clarity issues: The difference between CEM+diversity top k and CEM+diversity x |M| is unclear, and further explanation is needed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by including examples of low-level policies to enhance interpretability. Additionally, the authors should justify the design choices regarding the discount factor and consider incorporating an SMDP approach to address execution time variance. It would also be beneficial to conduct experiments on the CEM + compatibility setting and clarify the differences between the two CEM settings. Lastly, we suggest adding a dedicated limitations section to situate HIPO within the broader literature and compare it with other methods.