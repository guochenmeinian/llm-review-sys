ID: GN2GXjPyN8
Title: Antigen-Specific Antibody Design via Direct Energy-based Preference Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 5, 6, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach for antibody design using a pre-trained conditional diffusion model fine-tuned with a direct energy-based preference optimization method. The focus is on optimizing residue-level energy preferences to enhance antibody generation with desirable structures and high binding affinities. The authors demonstrate the effectiveness of their method by comparing it against state-of-the-art baselines on 55 cases from RAbD benchmarks, achieving promising results.

### Strengths and Weaknesses
Strengths:
1. The algorithm is innovative, combining diffusion models and direct preference optimization to address antibody design.
2. The definitions of the diffusion process and direct preference optimization are clear, and the figures illustrating protein structures are informative.
3. The results are compelling, with AbDPO achieving favorable outcomes in CDR total energy compared to other methods.
4. The experimental section is thorough, covering 55 of 60 antibodies with detailed comparisons to baseline methods.

Weaknesses:
1. There is a lack of explanation regarding the SE(3)-equivariant neural network, which may hinder understanding of the methodology.
2. The paper does not adequately address the 10% lower AAR compared to dyMEAN, leaving questions about the validity of the results.
3. The derivation of the final loss for fine-tuning could be simplified.
4. The adaptation of reinforcement learning from human feedback (RLHF) requires further clarification.
5. The evaluation of binding energy using Rosetta is computationally expensive and introduces variability due to its stochastic nature, which is not accounted for in the reported results.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the SE(3)-equivariant neural network to enhance comprehension. Additionally, providing a solid rationale for the observed AAR discrepancy would strengthen the paper. Simplifying the derivation of the final loss for fine-tuning and clarifying the adaptation of RLHF would also be beneficial. Furthermore, we suggest that the authors report the standard deviation of binding energy by running Rosetta relaxation multiple times with different random seeds to ensure statistical significance in their results.