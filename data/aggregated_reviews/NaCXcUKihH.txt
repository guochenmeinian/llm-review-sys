ID: NaCXcUKihH
Title: Towards a theory of how the structure of language is acquired by deep neural networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 7, 6, 6, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a conceptual characterization of the data requirements for learning the latent hierarchical structure of input languages, focusing on token correlations qualified by token distances. The authors propose that language models can leverage these correlations to form complex hierarchical representations. Empirical analyses, including those on artificial PCFGs/RHMs and Shakespeare plays, verify the theory, revealing that more data aids in resolving longer-distance dependencies and allows for deeper representations of token relationships.

### Strengths and Weaknesses
Strengths:
- Thorough characterization of how latent hierarchical structures can be induced by neural language models from token correlations.
- Deep definition of “token correlation,” exploring the relationship between token distance and correlations, tracing back to hierarchical dependencies.
- Comprehensive analysis of two architectures, Transformers and CNNs, with thorough and qualified comparisons.
- Quantification of previously unexamined variables, notably (effective) context window size.

Weaknesses:
- Linguistic foundations appear shaky, with mischaracterization of the poverty of the stimulus and reliance on PCFGs, which do not adequately represent context-sensitive language.
- The proposed RHM includes non-language-like features, such as unambiguous production rules, which may not reflect natural language's inherent ambiguity.
- Use of Shakespearean English, which differs significantly from modern language, raises questions about the choice of dataset over more naturalistic alternatives.
- Analyses resemble past empirical work on language models, particularly regarding LSTMs, which should be cited.
- Uncertainty about the practical impact on LM research or interpretability methods, potentially limiting the work's reach.

### Suggestions for Improvement
We recommend that the authors improve the linguistic foundations by accurately characterizing the poverty of the stimulus and reconsidering the use of PCFGs in favor of models that better reflect context-sensitive language. Additionally, we suggest testing the analysis on more contemporary and naturalistic datasets, such as The Pile or FineWeb, to enhance the relevance of findings. It would also be beneficial to explore the implications of the conjecture on the maximum context length for training real LLMs and to include a broader range of empirical experiments to strengthen the paper's contributions.