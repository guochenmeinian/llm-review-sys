ID: 8ElstW3DUT
Title: DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents **DADA**, a flexible framework for adapting language models to non-standard dialects of English by training individual adapters for specific linguistic features and using AdapterFusion for improved performance. The authors demonstrate that DADA outperforms baseline methods on various GLUE tasks, including MNLI and SST2, across dialects such as African-American Vernacular English and Appalachian English. While performance improvements are modest and DADA generally underperforms compared to training on dialectal datasets, it avoids the challenges of dialect identification and offers high interpretability. The framework's potential for future research in multilingual NLP is also noted.

### Strengths and Weaknesses
Strengths:
- The paper proposes a novel methodology that utilizes linguistic information for dialect adaptation.
- It effectively argues the ethical and societal need for dialect adaptation.
- The extensive literature review on dialect adaptation and parameter-efficient learning is commendable.
- The framework's flexibility allows for adaptation across various tasks and dialects.
- The interpretability of the model is a significant advantage.

Weaknesses:
- The comparison of DADA with other setups is limited to a single task (MNLI), which may restrict the generalizability of results.
- The absence of publicly available feature adapters is a notable drawback.
- The relative efficiency of DADA compared to maintaining individual dialectal adapters is questionable.
- Concerns regarding reproducibility arise from unspecified training objectives and dataset sizes, as well as limited experimental runs.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their results by comparing DADA across multiple tasks rather than just MNLI. Additionally, releasing the feature adapters to the public would enhance the paper's impact. Clarifying the efficiency of DADA in comparison to individual dialectal adapters is essential, as is providing detailed information on the training objectives and dataset sizes for the feature adapters. Finally, addressing the concerns about the validation mismatched split and ensuring the clarity of figures would strengthen the paper's reproducibility.