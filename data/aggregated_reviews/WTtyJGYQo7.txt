ID: WTtyJGYQo7
Title: Improved Self-Explanatory Graph Learning Method Based on Controlled Information Compression and Branch Optimization
Conference: AAAI
Year: 2024
Number of Reviews: 1
Original Ratings: 7
Original Confidences: 4

Aggregated Review:
### Key Points
This paper presents SAGE, a self-explainable graph learning framework aimed at mitigating distribution shift by compressing irrelevant structures, thereby ensuring stable explanations. By enforcing a lower bound on attention weights and refining parameters through branch optimization, SAGE demonstrates strong predictive performance and interpretable explanations across multiple benchmark datasets.

### Strengths and Weaknesses
Strengths:  
SAGE introduces a novel approach to controlling information compression with a probabilistic lower bound on edge attention scores, which helps avoid abrupt distributional shifts. The branch optimization technique effectively refines model parameters without disrupting the main training loop. The technical derivation is robust, employing a GIN backbone, Gumbel-softmax reparameterization for edge selection, and a KL divergence penalty that aligns attention weights with a predefined distribution. The contribution of improving self-explainable GNN performance addresses the recognized challenge of distribution shift, potentially offering valuable insights for practitioners seeking interpretable GNNs with minimal performance loss.

Weaknesses:  
The method's novelty may be seen as somewhat incremental compared to other IB-inspired techniques, and the reliance on a manually chosen hyperparameter (r) could limit its applicability. The theoretical justification for why partial compression enhances performance is not deeply explored, and the paper lacks a rigorous analysis of how the distribution boundary (r) interacts with distribution shift. Additionally, while performance gains are noted, they are not consistently dramatic across all datasets, and the comparison with a broader range of explainability techniques is insufficient.

### Suggestions for Improvement
We recommend that the authors improve the theoretical insights into why partial compression of noisy edges leads to enhanced performance and reduced distribution shift. Specifically, can they characterize how r affects the divergence between compressed and original distributions mathematically? Additionally, the authors should consider providing a heuristic or automated method for selecting r to alleviate the need for heavy tuning. Expanding the experiments to include comparisons with a broader range of self-explanatory or post-hoc methods would strengthen claims about general efficacy. Finally, the authors should address the computational complexity and runtime implications of the branch optimization and Gumbel-softmax sampling processes, particularly concerning large graphs.