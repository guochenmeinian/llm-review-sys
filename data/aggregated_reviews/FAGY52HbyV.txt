ID: FAGY52HbyV
Title: Debiased and Denoised Entity Recognition from Distant Supervision
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 5, 5, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DesERT, a novel self-training framework for distant-supervised named-entity recognition (NER) that addresses two types of biases: structural noise in distant labels and inherent bias from self-training. The authors propose a dual-form self-training approach and a debiased module to enhance sample selection and token representations. Experimental results indicate significant performance improvements, achieving a new state-of-the-art average F1 score across five benchmark datasets.

### Strengths and Weaknesses
Strengths:
- The problem of open-domain distantly supervised NER is crucial and well-motivated.
- The paper is clearly written and presents a comprehensive analysis of the challenges in the NER task.
- The proposed method demonstrates substantial performance gains over existing methods on multiple datasets.

Weaknesses:
- Important related works are missing, particularly those that share similar motivations for debiasing distant supervision. The authors need to discuss key differences with these baselines.
- The novelty of the proposed method is questionable, as it appears to be a combination of existing techniques without sufficient task-specific adaptation.
- The writing lacks clarity in presenting the framework, making it difficult to follow the relationships between components.
- The empirical advantages of the proposed method over baselines are not significantly substantial, raising concerns about the tuning process.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by including a high-level algorithm that illustrates the workflow of the proposed approach. Additionally, the authors should justify the novelty of their method in relation to existing works and provide a detailed discussion on the tuning of hyperparameters for both their method and the baselines. It would also be beneficial to include a discussion on the implications of recent advancements in LLMs for the NER task and to consider generalizing the method to broader contexts beyond distant supervision. Lastly, we suggest moving the experiments involving ChatGPT to the main content for better visibility.