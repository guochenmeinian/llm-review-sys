ID: LCHmP68Gtj
Title: SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic Understanding
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 4, 5, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a self-supervised learning approach to generate 2D neural maps by fusing features from street-view and aerial images. The authors propose a contrastive map alignment objective to align query maps with reference maps based on known 3D poses. The method demonstrates improved performance in visual positioning tasks compared to traditional Structure from Motion (SfM) and other learning-based methods. The approach includes innovative techniques such as depth-weighted splatting and hard negative sampling via RANSAC, and it leverages large-scale pretraining for enhanced semantic mapping capabilities. The authors argue that their method is designed to improve generalization across different geographic areas, addressing the limitations of existing datasets like NuScenes, KITTI, and Waymo. They clarify that while powerful GPUs are needed for training, inference can be performed on less powerful hardware.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel and effective method for learning neural scene representations from multimodal images, which is well-motivated and clearly articulated.
- The implementation details, including monocular inference and multimodal fusion, are well-executed, contributing to the overall performance.
- The experimental design shows strong improvements over baselines, validating the effectiveness of the proposed approach.
- The authors provide a clear motivation for their pre-training experiments and the use of multimodal data, emphasizing the importance of generalization across diverse datasets.
- The paper discusses the computational efficiency of their model during inference.

Weaknesses:
- The paper lacks sufficient experimental evidence to support the effectiveness of the fusion of multi-modal features and the proposed supervision method.
- The influence of pose accuracy on the model's performance is not adequately discussed, and the authors should clarify how reliable pose information can be obtained.
- Comparisons are limited, particularly in Experiment 1, where only OrienterNet is used, and additional comparisons with other satellite-based localization methods are necessary.
- The rebuttal fails to benchmark the pre-training experiment against existing approaches, which reviewers find crucial for validation.
- There are concerns regarding the clarity of certain claims, particularly about the ortho-rectification process and the representation of fine-grained structures in 3D models.
- The overall clarity of the paper could be improved, particularly in explaining complex concepts such as the interpolation operator and the depth model's handling of image distortions.
- The paper does not adequately address the implications of computational costs for real-world applications.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing a pseudo-algorithm to illustrate the training and inference procedures of SNAP. Additionally, it is essential to supplement the experimental evidence for the effectiveness of the fusion and contrastive learning supervision in the generation process of neural maps. The authors should conduct ablation studies to investigate the influence of different components of the model, such as Equation (1) and the max-pooling used for multimodal fusion. Furthermore, we suggest that the authors include a discussion on the computational requirements, including training and inference time, particularly for practical applications in mobile robotics. We recommend improving the robustness of the pre-training experiment by benchmarking it against existing methods, such as MAE or other self-supervised approaches, to provide a clearer validation of their claims. Clarifying the statements regarding ortho-rectification and the representation of fine-grained structures in the final version would also be beneficial. Additionally, including more quantitative results comparing the performance of their method against those using only aerial images would strengthen the argument for the effectiveness of multimodality. Finally, addressing the computational cost implications for real-world applications more thoroughly would enhance the paper's relevance.