ID: 8Ofbg2KYMu
Title: Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two CUDA kernel optimization techniques: batched GEMM and fusion for neighborhood attention. The batched GEMM optimization achieves average improvements of 895% (full precision) and 272% (half precision) latency for 1-D and 2-D neighborhood attention, respectively. Fusion optimization yields average improvements of 1759% and 958% in 1-D and 2-D problems. These optimizations result in up to 104% improvement in inference and 39% in training existing models based on neighborhood attention. The fused kernels can match or outperform the authors' self-attention baseline in nearly all benchmarked problem sizes.

### Strengths and Weaknesses
Strengths:
1. The paper effectively explains the overhead associated with transitioning from standard self-attention kernels to neighborhood attention.
2. The CUDA kernel optimization techniques are clearly illustrated, despite the complexity of the kernels.

Weaknesses:
1. The paper lacks a quantitative comparison between the neighborhood attention kernel and the state-of-the-art fused dot-product attention kernel.
2. Evaluation is limited to a single GPU model, the NVIDIA A100, raising concerns about the generalizability of the proposed optimizations.

### Suggestions for Improvement
We recommend that the authors improve the quantitative explanation of performance benefits by showing the arithmetic intensity of the naive, GEMM, and fused kernels under various inputs and configurations. Additionally, providing measurements on memory, cache, and occupancy would clarify the performance improvements. It would also be beneficial to include the best performance achieved by torch.compile in PyTorch 2 to motivate the necessity of developing CUDA kernels. Furthermore, the authors should discuss how the optimized kernels impact the algorithm, such as enabling larger models or longer contexts, and whether the new tensor memory accelerator introduced in H100 can assist with the neighborhood attention kernel. Lastly, consider extending the methodology to support other attention mechanisms beyond N-Atten, as recent research has proposed novel approaches for linear or sub-quadratic attention mechanisms.