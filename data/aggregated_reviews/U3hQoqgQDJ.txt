ID: U3hQoqgQDJ
Title: Interfacing Foundation Models' Embeddings
Conference: NeurIPS
Year: 2024
Number of Reviews: 22
Original Ratings: 3, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FIND, a generalized interface for aligning embeddings from foundation models using a lightweight transformer without tuning pretrained model weights. The authors propose a benchmark, FIND-Bench, derived from COCO, to evaluate interleaved tasks such as segmentation and retrieval. FIND demonstrates state-of-the-art performance and adaptability across various tasks, showcasing its potential for creating a shared embedding space through multi-task training. Additionally, the authors introduce a novel approach to aligning embeddings across different granularities and modalities within foundation models like X-Decoder and LLaMA. They incorporate queries and prompts to unify granularity and facilitate seamless communication between embeddings, validating their model through various experiments, including grounded segmentation and image-text retrieval.

### Strengths and Weaknesses
Strengths:
- The idea of a universal benchmark for grounding, retrieval, and segmentation is novel and significant.
- The paper offers a significant contribution by unifying foundation model embeddings across granularity and modality.
- The experimental results are robust, demonstrating the effectiveness of the proposed method across multiple tasks and validating the design choices.
- The authors provide a clear motivation for their research, aligning with the Platonic representation hypothesis.

Weaknesses:
- The writing quality needs significant improvement, making it challenging for readers to grasp the key contributions.
- The paper lacks clarity in defining key terms such as "embedding" and "interleaved," which complicates understanding.
- Certain figures, particularly Figure 1, may lead to misunderstandings regarding the necessity of multiple language encoders.
- The method in section 3.2.2 is simplistic and lacks innovation, requiring more analysis on the challenges of the interleaved approach.
- There is insufficient discussion of related works, particularly regarding models like LLaVA and BLIP-v2, which also address modality unification.

### Suggestions for Improvement
We recommend that the authors improve the clarity of key terms and concepts at the beginning of the paper to enhance reader comprehension. Additionally, we suggest that the authors improve the writing quality in future versions, particularly focusing on clarifying the motivation and contributions. Revising Figure 1 to prevent misunderstandings about the use of multiple language encoders would be beneficial, ensuring that the representation accurately reflects the model's functionality. We also encourage the authors to provide a more detailed discussion of the method in section 3.2.2, including the challenges of the interleaved approach, and to include comparisons with recent multimodal models to highlight the unique contributions of their work. Finally, we recommend clarifying the specifics of the embedding sampler and addressing the missing baselines related to instruction-tuned vision-language models.