ID: xWCp0uLcpG
Title: Robust Data Pruning under Label Noise via Maximizing Re-labeling Accuracy
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on data pruning in the context of noisy labels, proposing the method Prune4ReL, which aims to maximize the neighborhood confidence of training examples to enhance re-labeling accuracy. The authors provide a theoretical analysis bounding the error of models trained on selected subsets, demonstrating that this error is inversely proportional to neighborhood confidence. Empirical evaluations indicate that Prune4ReL shows promise, outperforming some baselines, although results are mixed against others.

### Strengths and Weaknesses
Strengths:
- The target setting is well-motivated and the proposed method is theoretically sound with guarantees.
- The writing is clear and the methodology is reasonable, supported by comprehensive evaluations.
- The authors present a theorem linking model error to neighborhood confidence, and ablation studies provide insights into the method's parameters.

Weaknesses:
- The comparison against only the SmallLoss baseline, which is outdated, limits the evaluation; other sample selection methods should be included as baselines.
- Some related works in robust learning leveraging neighborhood information are not discussed.
- The empirical results show that while Prune4ReL outperforms some methods, it often matches or shows only slight improvements over existing baselines, raising questions about its effectiveness.

### Suggestions for Improvement
We recommend that the authors improve their baseline comparisons by including more recent sample selection methods in robust learning. Additionally, the authors should address the absence of related works that utilize neighborhood information. Clarifying the results in Tables 1 and 2, particularly regarding the performance of the Forget baseline, would enhance the paper's rigor. Finally, we suggest that the authors provide more detailed justifications for the performance gap between Prune4ReL and uniform sampling, as well as elaborate on the implications of their findings in the context of more complex tasks beyond image classification.