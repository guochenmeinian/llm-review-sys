ID: oXHyYHp4Zb
Title: SparseLLM: Towards Global Pruning of Pre-trained Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 5, 5, 5, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SparseLLM, a novel pruning technique aimed at enhancing the computational and memory efficiency of large language models (LLMs). The authors propose decomposing the global pruning objective into manageable subproblems, utilizing auxiliary variables to maintain dependencies while pruning. The evaluation indicates that SparseLLM outperforms previous methods in terms of perplexity and accuracy across various sparsity settings.

### Strengths and Weaknesses
Strengths:
1. The proposed method is innovative and addresses a significant problem in LLMs.
2. The paper is well-written and easy to follow, with comprehensive experiments and detailed settings.
3. The formulation allows for systematic exploration of global and local sparsity, which is powerful.

Weaknesses:
1. The pruning procedure's adaptability to different model architectures raises concerns about its generality.
2. The evaluation lacks clarity on the costs associated with pruning and does not adequately compare performance against dense baselines.
3. The experiments focus on older models, and there is a lack of exploration at lower sparsity levels and for structured pruning.
4. Performance at high sparsity levels drops significantly, questioning the practical utility of the method.

### Suggestions for Improvement
We recommend that the authors improve the generality of the pruning method to accommodate various model architectures. Additionally, we suggest providing a clearer evaluation of pruning costs and including comparisons with dense baselines. The authors should consider experimenting with newer models and exploring lower sparsity levels, as well as structured pruning methods. Finally, we encourage the inclusion of ablation studies on the hyperparameters $\alpha$ and $\beta$ to enhance the understanding of their impact on performance.