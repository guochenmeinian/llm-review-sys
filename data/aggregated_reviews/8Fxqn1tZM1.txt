ID: 8Fxqn1tZM1
Title: Scale Equivariant Graph Metanetworks
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 6, 8, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to learning over weight spaces through GNN-based metanetworks, specifically ScaleGMN and ScaleGMNB, which are designed to be equivariant to both permutation and scale symmetries. The authors extend the metanetwork design to account for scaling symmetries induced by activation functions in input neural networks. The paper includes theoretical analysis of the expressive power of the proposed architectures and evaluates their performance on various datasets, demonstrating significant improvements over existing methods.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a crucial and timely problem in deep-weight space learning, presenting a new architecture that incorporates both scale and permutation equivariance.
2. The theoretical analysis provides insights into the expressive power of the proposed methods, confirming their capability to simulate forward and backward passes.
3. Empirical results indicate substantial improvements compared to baseline methods in the field.

Weaknesses:
1. The empirical evaluation is limited and lacks diverse, challenging baselines; a more comprehensive assessment is necessary.
2. Writing and formatting need refinement, particularly regarding long sentences and the late introduction of the proposed method.
3. The experimental section focuses solely on invariant tasks, neglecting equivariant tasks that could enhance the study's robustness.

### Suggestions for Improvement
We recommend that the authors improve the empirical evaluation by including a wider range of natural baselines, such as permutation equivariant baselines like DWS/NFN or NG-GNN with scaling data augmentations. Additionally, conducting experiments on equivariant tasks, such as INR editing and domain adaptation, would significantly strengthen the empirical study. We also suggest enhancing the writing clarity by breaking down long sentences and improving the overall structure, particularly by introducing the proposed methods earlier in the paper. Furthermore, including evaluations of runtime and memory consumption relative to baselines, as well as experiments with larger and more diverse input architectures, would be beneficial. Lastly, an ablation study regarding design choices and addressing the missing results for DWS and INR2Vec in Table 1 would enhance the paper's thoroughness.