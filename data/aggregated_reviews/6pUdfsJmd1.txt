ID: 6pUdfsJmd1
Title: AI-Assisted Generation of Difficult Math Questions
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 7, 7, 7
Original Confidences: 3, 4, 3

Aggregated Review:
### Key Points
This paper presents a framework that integrates LLMs with a human-in-the-loop approach to generate a diverse set of challenging math questions. The authors propose a technique for extracting skills from two mathematical questions to create a more difficult question that combines both skills. The paper provides analytical evidence indicating that their benchmark, MATH^2, is significantly more challenging than existing benchmarks, supported by a dataset of 180 high-quality, LLM-generated but human-verified questions. Additionally, the authors include a detailed analysis of failure modes and alternative question generation setups in the appendix.

### Strengths and Weaknesses
Strengths:  
1. The concept of combining two skills to generate more challenging questions is innovative and applicable to other fields.  
2. The high-quality, human-verified benchmark is expected to remain challenging for LLMs for an extended period.  
3. The relationships between model accuracies on MATH and MATH^2 are intriguing.  
4. The detailed appendix provides essential information for replicating the proposed pipeline for other benchmarks.  

Weaknesses:  
1. The paper lacks a comprehensive definition and examples of "skill," which may confuse readers.  
2. A skill-wise breakdown of model performances would enhance understanding.  
3. The human validation process is highly intensive and time-consuming, relying on expert evaluation.  
4. The potential for expert bias in question generation is not adequately addressed, and a framework for identifying experts could be beneficial.  
5. The paper does not explore the implications of incorporating more than two skills in the question generation phase.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the concept of "skill" by providing a comprehensive definition and concrete examples, such as adding an example in Figure 1. Additionally, including a skill-wise breakdown of model performances would provide valuable insights. To address the human validation process, the authors should consider discussing the time implications and potential biases of expert evaluations. Finally, exploring the effects of adding more skills in the question generation phase would enrich the discussion on complexity.