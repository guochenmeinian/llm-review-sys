ID: nU7Se8oIPJ
Title: Concept Unlearning for Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 7, 7, 5
Original Confidences: 3, 3, 4

Aggregated Review:
### Key Points
This paper presents a novel approach called Concept Unlearning (CU) for machine unlearning in large language models (LLMs) by removing specific concepts using knowledge graphs. The authors propose that CU is superior to traditional unlearning methods, which focus on deleting specific data points and may lead to hallucinations. Experimental results indicate promise in addressing data privacy concerns while maintaining general knowledge. However, the authors acknowledge limitations such as hallucination, incomplete forgetting, and dependence on a reference LLM.

### Strengths and Weaknesses
Strengths:
1. **Innovative Method:** The introduction of Concept Unlearning through knowledge graphs addresses privacy concerns effectively.
2. **Algorithm Design:** The gradient ascent (GA) approach for Node Unlearning (NU) and Edge Unlearning (EU) is well-defined.
3. **Comprehensive Evaluation:** The method demonstrates effectiveness in concept forgetting across various datasets.

Weaknesses:
1. **Lack of Robustness:** Performance varies significantly depending on the target concept.
2. **Hallucination Issues:** The method does not adequately address hallucinations, which are inherent to current LLMs.
3. **Dependency on Reference LLM:** The method's effectiveness is heavily reliant on the quality of the reference LLM.
4. **Incomplete Unlearning:** The method does not fully meet Concept Unlearning requirements, with significant variance in results indicating incomplete unlearning.
5. **Simplistic Methodology:** The reliance on masking the target entity and re-querying lacks sophistication for a concept-unlearning technique.
6. **Unexplored Impact on Remaining Entities:** The effects of unlearning one entity on others are not analyzed.
7. **Limited Evaluation Scope:** The evaluation is restricted to entities, limiting broader applicability.

### Suggestions for Improvement
We recommend that the authors improve the robustness of the method by addressing the variability in performance across different target concepts. Additionally, the authors should provide a clearer explanation of why only two types of $**x**_{prompt}$ are sufficient for unlearning and clarify how to compute the loss, $p_{\theta}$ for $L_1$ and $L_2$. Addressing hallucination issues and exploring the impact of unlearning on other entities could enhance the method's effectiveness. Finally, broadening the evaluation to include other data types or concepts would strengthen the conclusions drawn from the study. We also suggest modifying the layout of Table 4 at the end of page 8 for better clarity.