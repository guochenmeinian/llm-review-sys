ID: 7rWTS2wuYX
Title: Revisiting Ensembling in One-Shot Federated Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 6, 7, 3, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FENS, a one-shot federated learning (FL) method that enhances global model accuracy while minimizing communication costs. Unlike traditional FL methods, FENS employs an aggregator model that iteratively trains on top of a local model ensemble after a single round of model uploading. The experiments demonstrate that FENS outperforms existing one-shot methods and approaches the performance of full federated averaging, particularly in cross-silo settings.

### Strengths and Weaknesses
Strengths:
- The method is efficient in heterogeneous cross-silo settings and easy to reproduce without additional server training data.
- The paper is well-written, with extensive experiments covering various datasets, including realistic health data.
- The authors provide clear code for reproducibility, enhancing the submission's quality.
- The use of the FLamby benchmark allows for evaluation on datasets with realistic heterogeneity.

Weaknesses:
- The contribution lacks significant technical novelty, with some methods being straightforward adaptations of existing concepts, such as Stacked Generalization.
- The paper does not include strong baselines for comparison, particularly neglecting recent methods like Fed-ET and FedGKD.
- There is a lack of theoretical analysis to support the design of FENS, and privacy concerns arise from the need to dispatch all local models to clients.
- The communication cost analysis does not clarify whether quantization is assumed, and the method's compatibility with privacy-preserving technologies is insufficiently addressed.

### Suggestions for Improvement
We recommend that the authors improve the theoretical grounding of FENS by providing a more robust analysis of its design. Additionally, including comparisons with stronger baselines, such as Fed-ET and FedGKD, would enhance the evaluation. We suggest expanding the ablation study to discuss the impact of server training and the use of public data, as well as exploring the implications of privacy-preserving techniques. Furthermore, we encourage the authors to trim Sections 1 and 2 for conciseness while expanding discussions on the aggregator model architecture and other system metrics, such as total client computation and server memory usage. Lastly, addressing the potential limitations regarding data minimization and privacy in FENS would strengthen the paper.