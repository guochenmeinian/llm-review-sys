ID: 2ePf1sBgLU
Title: Learning Invariant Representations with a Nonparametric Nadaraya-Watson Head
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel algorithm for learning invariant representations using a fixed head that predicts class alignment based on similarities between query and support features. The method optimizes for a representation through a constrained maximum likelihood estimation problem. The authors evaluate their technique on three benchmark datasets, demonstrating competitive performance against various baselines.

### Strengths and Weaknesses
Strengths:
1. The formulation of the problem and the proposed approach are well-articulated, with generally clear writing.
2. The application to medical image classification is relevant and significant.
3. The approach intriguingly simplifies the IRM problem by fixing a head instead of learning one.
4. Empirical results show state-of-the-art or competitive performance on benchmark datasets.
5. The authors acknowledge the limitations of their method.

Weaknesses:
1. The method is computationally expensive due to the need for pairwise comparisons during training and inference. While the use of the Cluster method mitigates this, the empirical results do not significantly outperform the baseline [50]. A comparison of computational costs with [50] would be beneficial.
2. The necessity of the NW head for satisfying certain conditions is unclear; a simpler estimator might suffice without requiring a support set for every query.
3. The Introduction and Related Works sections would benefit from a more thorough discussion of IRM criticisms and how the proposed method addresses these concerns.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the statement "the objective is equivalent to unconstrained maximum likelihood under the assumption in Eq. 1," as it is currently unclear. Additionally, we suggest that the authors clarify the random inference mode, particularly how uniform sampling across the dataset is achieved with class representation. It would also be beneficial to provide details on how the features for the NW Probe are trained. Lastly, we encourage the authors to include a discussion on the selection of the kernel bandwidth and the impact of the number of clusters on performance.