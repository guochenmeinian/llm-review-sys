ID: mSNfjOcDUv
Title: InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 4, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents InfoPrompt, an information-theoretic framework for soft prompt tuning that maximizes mutual information between prompts and model parameters. The authors develop two novel loss functions to enhance prompt initialization and task-relevant information extraction. Their experiments on classification, relation extraction, and named entity recognition tasks demonstrate that InfoPrompt outperforms several baselines, including WARP, IDPG, and Adapters, as well as LPT, across multiple tasks such as RTE, SST2, MRPC, and CoLA. The authors clarify that BBT is excluded from comparison due to its focus on scenarios where back-propagation is unavailable, which does not align with the local model training context considered in their work. Additionally, they acknowledge the need for a theoretical analysis and plan to include a proof sketch in the updated version.

### Strengths and Weaknesses
Strengths:
- The approach is innovative, with a clear motivation and well-structured presentation.
- The experimental results show that InfoPrompt outperforms baseline models, indicating its effectiveness.
- The experiments are comprehensive and show significant improvements over baseline methods.
- The authors provide a clear rationale for excluding BBT from their comparisons, enhancing the clarity of their methodology.
- The theoretical underpinnings and mathematical formulations are sound and contribute to the understanding of the method.
- Commitment to releasing code and incorporating additional experiments demonstrates transparency and responsiveness to reviewer feedback.

Weaknesses:
- The paper lacks comparisons with other popular parameter-efficient fine-tuning methods, such as LoRA or MaM adapters, which could provide a more robust evaluation.
- There is insufficient evidence to support claims regarding initialization sensitivity, as results over multiple seeds or initialization types are not presented.
- Limited space in the paper restricts the inclusion of comprehensive theoretical analysis, which may affect the depth of understanding for readers.
- The scope of tasks explored could be expanded to further validate the proposed approach.
- The writing contains minor typos and could benefit from polishing for clarity.

### Suggestions for Improvement
We recommend that the authors improve their comparisons by including popular parameter-efficient fine-tuning methods like LoRA or HyperFormer to strengthen the evaluation of their approach. Additionally, providing results over multiple seeds or different initialization types would substantiate claims about reduced sensitivity to initialization. We also suggest expanding the range of tasks evaluated, particularly by including sequence generation tasks and ensuring a complete suite of GLUE evaluations to avoid cherry-picking datasets. Furthermore, we encourage the authors to enhance the theoretical analysis by including a proof sketch in the updated version to better present their theoretical results. Finally, addressing minor typos and enhancing the clarity of the writing would improve the overall presentation of the paper.