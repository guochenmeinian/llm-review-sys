ID: b7gtyaaM2y
Title: Towards General Error Diagnosis via Behavioral Testing in Machine Translation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for generating black box test cases for machine translation (MT) systems by modifying specific features of segments in an existing test set, specifically the WMT21/22 news translation test sets. The authors propose using open-source tools and ChatGPT to create these test cases, which are evaluated based on an automatic MT quality metric (COMET). A test case is deemed successful if the original translation is rated 'good' and the score difference between the original and modified segments is 'small', defined by hyperparameters alpha and beta. The paper claims that the method can generate an infinite number of test cases and demonstrates its effectiveness through human evaluation.

### Strengths and Weaknesses
Strengths:
- The authors convincingly demonstrate that their method generates valid test cases, supported by human evaluations of fluency and translation adequacy.
- The proposed method shows scalability, allowing for the generation of diverse test cases by varying linguistic features.

Weaknesses:
- The experiments are limited to a single language direction (English to Chinese), raising concerns about the method's applicability to other languages.
- The hyperparameters alpha and beta are critical for determining test case success, yet the authors do not explain how their values (0.8 and 0.05) were derived or optimized.
- The reliance on ChatGPT complicates reproducibility, as the results may not be easily replicated with different language pairs or MT evaluation metrics.
- The paper lacks clarity on how multiple test cases can be generated from a single parallel sentence and does not provide sufficient details on the algorithm's implementation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their methodology by providing detailed explanations of how hyperparameters alpha and beta were chosen and optimized for reproducibility. Additionally, the authors should include the specific ChatGPT model used and the prompts employed in their experiments. To enhance the paper's robustness, we suggest expanding the evaluation to include multiple language pairs and addressing the concerns regarding the interpretation of results, particularly in relation to errors occurring outside modified spans. Lastly, we encourage the authors to clarify the implications of their findings regarding the behavioral tests and the limitations of using ChatGPT in their evaluation framework.