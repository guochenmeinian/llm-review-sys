ID: 2myGfVgfva
Title: MiraData: A Large-Scale Video Dataset with Long Durations and Structured Captions
Conference: NeurIPS
Year: 2024
Number of Reviews: 23
Original Ratings: 9, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents three main contributions: a large-scale dataset, MiraData, for long-duration videos with detailed structured captions; a benchmark, MiraBench, for evaluating video generation models across multiple metrics; and the MiraDiT model, which utilizes a Diffusion Transformer architecture to generate high-quality long videos. The dataset is notable for its rigorous collection and annotation process, ensuring high visual quality and significant motion strength. The evaluation framework emphasizes temporal consistency and motion intensity, providing a comprehensive assessment of generated videos. Additionally, the paper evaluates MiraDiT's performance on the T2V-CompBench benchmark, indicating that MiraDiT trained on MiraData outperforms the version trained on WebVid-10M across all metrics, particularly in Dynamic Attribute Binding. However, the comparison is complicated by variations in computational resources and model architectures. The authors also address the measurement of visual quality and text-video alignment, noting that while structural and dense captions enhance overall alignment, they do not necessarily improve visual quality. The paper acknowledges limitations related to captioning variations, platform-specific biases, and ethical considerations regarding data usage.

### Strengths and Weaknesses
Strengths:
* The paper addresses a significant challenge in the video generation community by focusing on long-duration video generation.
* MiraData is a substantial contribution, allowing for the training of high-quality video models with detailed annotations.
* The empirical results demonstrate that MiraDiT outperforms existing models, showcasing the effectiveness of structured captions.
* The evaluation framework, MiraBench, introduces innovative metrics for assessing video quality.
* Comprehensive evaluation using multiple metrics in T2V-CompBench.
* Inclusion of human evaluation results that align with automatic metrics, validating the effectiveness of the evaluation framework.

Weaknesses:
* The paper lacks a comparison between automatic metrics and human judgment, which diminishes the credibility of the evaluation results.
* There is insufficient assessment of the quality of captions in MiraData and the performance of the stitching operation.
* The evaluation is limited to the proposed MiraBench, without reporting results on existing benchmarks.
* Some related works on video generation benchmarks are not discussed.
* The comparison of models is hindered by differences in training resources and methodologies.
* Limited ability of evaluation prompts to showcase the strengths of MiraData due to their simplicity.
* Notable discrepancies exist between automatic evaluations and human judgments, particularly with the VideoCrafter2 model.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including a comparison between the automatic metrics in MiraBench and human judgment to enhance the credibility of the results. Additionally, the authors should assess the quality of captions in MiraData to determine their accuracy in describing video content and evaluate the stitching operation's performance. It would be beneficial to report results on existing T2V benchmarks to provide a broader context for the findings. We also suggest incorporating more complex evaluation prompts to better demonstrate MiraData's capabilities. Expanding the ethics section to address data copyright, ownership, and the mechanisms for content removal requests will strengthen the paper's integrity. Finally, we encourage the authors to clarify the relationship between automatic and human evaluation results by openly acknowledging discrepancies and providing a direct comparison of model rankings.