ID: ZC0PSk6Mc6
Title: Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 7, 4, 8, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Successive Concept Bottleneck Agents (SCoBots), which utilize concept bottleneck models to enhance interpretability and decision-making in reinforcement learning (RL). SCoBots integrate relational and object-based concepts for action selection, allowing for inspection and revision of decision policies. The authors demonstrate that SCoBots perform comparably to deep RL agents while addressing issues like reward sparsity and goal misalignment. Experimental results indicate SCoBots' competitive performance in Atari games, highlighting their potential for improving human-machine interactions and understanding.

### Strengths and Weaknesses
Strengths:
- The method is novel, applying concept bottlenecks to RL tasks and transforming them into understandable functions via large language models (LLMs).
- The expert reward signal effectively addresses sparse reward issues.
- The paper is well-organized, with clear definitions and effective visualizations of experimental results.

Weaknesses:
- Concerns exist regarding the adaptability of Object Representations and Relational Concepts to changing environments.
- The paper lacks clarity on the learning process of concepts, particularly in the absence of labels.
- The relation extractor's definition and its model structure are inadequately detailed.
- Missing comparisons with standard object detectors that SCoBots assume access to.
- The fixed set of relations may not generalize well to new or partially observable environments.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the relation extractor's definition, including its model structure and objective function. Additionally, the authors should provide a baseline comparison with deep RL agents using standard object detectors. To enhance the paper's applicability, we suggest exploring more realistic tasks beyond Atari games, such as vision-based manipulation and autonomous driving. Furthermore, the authors should clarify the learning process for the object extractor and ensure a comprehensive definition of the Markov Decision Process (MDP) is included. Lastly, addressing how the introduction of additional reward signals affects policy fidelity would strengthen the paper's contributions.