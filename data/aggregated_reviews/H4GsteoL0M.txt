ID: H4GsteoL0M
Title: On the Overlooked Structure of Stochastic Gradients
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 6, 5, 7, 6, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of stochastic gradients in deep learning, focusing on their statistical properties and covariance structures. The authors conduct formal statistical tests revealing that dimension-wise stochastic gradients exhibit power-law heavy tails, while iteration-wise stochastic gradient noise typically shows Gaussian-like light tails. Additionally, they discover that the covariance spectra of stochastic gradients follow a power-law structure, which has significant implications for understanding stochastic optimization in deep learning.

### Strengths and Weaknesses
Strengths:  
- The paper provides comprehensive empirical results that clarify the distribution of stochastic gradients and their covariance matrix.
- It distinguishes between two types of stochastic gradient noise (SGN), enhancing the understanding of SGN distributions.
- The findings regarding power-law behavior in covariance spectra contribute valuable insights into the low-dimensional learning space of deep neural networks.

Weaknesses:  
- The experiments are primarily limited to simpler models like LeNet and FCN, which may not generalize to more complex architectures such as ResNet or Transformers.
- The theoretical framework lacks depth, particularly in explaining the emergence of power-law gradients and addressing discrepancies between the Hessian and Fisher Information Matrix (FIM).
- Some figures, such as Figure 1, lack clarity in what is being plotted, and the Gaussianity of stochastic gradient noise needs further justification.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Figure 1 by providing a precise description of the plotted quantities, possibly in equation form. Additionally, the authors should address the theoretical underpinnings of the observed power-law covariance in deep learning, clarifying when this observation holds true. Expanding the experimental scope to include more complex architectures like LSTM and Transformers would also strengthen the findings. Finally, we suggest revising the analysis of eigengaps to ensure it contributes meaningfully to the paper's core arguments.