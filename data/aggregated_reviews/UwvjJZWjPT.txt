ID: UwvjJZWjPT
Title: Inductive biases of multi-task learning and finetuning: multiple regimes of feature reuse
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 7, 5, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the implicit bias of multi-task learning (MTL) and fine-tuning pre-trained models (PT+FT) within diagonal linear networks and shallow ReLU neural networks. The authors demonstrate that both MTL and PT+FT bias networks towards *reuse features*, confirmed through theoretical proofs (Corollary 1, Corollary 2, Proposition 3). They clarify that during fine-tuning, both layers are trained, while linear probing only trains the readout weights. The authors show that during fine-tuning, networks interpolate between a "lazy" and "rich" regime (Proposition 4) and empirically find that the chosen features during PT+FT are a sparse subset of those learned during pretraining. The paper also provides evidence that only PT+FT benefits from features correlated between auxiliary and primary tasks in ReLU networks and introduces a weight rescaling technique to enhance PT+FT performance. Additionally, the authors address notation issues, suggesting clearer terms for dimensions and features, and they plan to enhance figure readability.

### Strengths and Weaknesses
Strengths:
- The setting is relevant to contemporary deep learning, with a focus on the inductive bias of multi-task networks.
- The paper includes numerous experimental results on toy models, contributing to the understanding of implicit regularization.
- The finding that MTL and PT+FT induce a regularizer that interpolates between $\ell_1$ and $\ell_2$ regularization is novel.
- The practical application of a weight rescaling technique enhances the theoretical findings.
- The authors provide clear responses to reviewer queries, enhancing the clarity of their theoretical contributions.
- The results are deemed interesting, with the potential for further exploration in future work.

Weaknesses:
- Corollary 1 and its extension for ReLU networks are already established in the literature and should be referenced.
- The focus on linear diagonal networks, which are not commonly used in practice, detracts from the paper's applicability.
- The motivation for the regularizer used during fine-tuning in Proposition 3 is unclear and requires further discussion.
- The notation is overly complex, making the paper difficult to read and parse.
- Figures are too small and lack clarity, which detracts from the presentation of results.
- Some reviewers express concern that the section on diagonal linear networks may distract from more significant findings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the notation throughout the paper to enhance accessibility for a broader audience. Specifically, simplifying the notation in Corollary 1 and ensuring consistent definitions for terms like "large initialization" versus "small initialization" would be beneficial. We suggest denoting hidden weights as $w$ and readout weights as $v$ for clarity. Additionally, we recommend increasing the size of figures and replacing color bars with legends for better readability. The authors should provide a clearer motivation for the regularizer used in Proposition 3 and ensure that the assumptions in this proposition are well-justified. Consideration should also be given to the placement of the diagonal linear network results; while some reviewers advocate for their inclusion in the main text, others suggest moving them to the appendix. The authors should reflect on their target audience when making this decision. Lastly, addressing the identified typos will enhance the overall quality of the manuscript.