ID: f2U4HCY8bg
Title: Iterative Reachability Estimation for Safe Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 8, 7, 7, 5, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an iterative reachability estimation method for safe reinforcement learning (RL), addressing the limitations of previous approaches by handling stochastic dynamics and improving performance with deterministic dynamics. The authors propose a safe RL problem formulation using REF and develop an adapted AC algorithm, providing theoretical convergence results and extensive experimental comparisons. The connection between HJ reachability and CMDP is established, enhancing the understanding of constrained RL.

### Strengths and Weaknesses
Strengths:
- Originality is notable, as the paper addresses stochastic reachability within a constrained RL framework, previously unexplored.
- The quality of the paper is high, with clear explanations of improvements over prior work and comprehensive experimental comparisons.
- The significance of safety in stochastic systems is well-articulated, emphasizing the challenges and importance of reachability methods.

Weaknesses:
- The problem formulation in equation (4) requires better emphasis for clarity.
- The notation system is somewhat confusing, potentially leading to misunderstandings for readers unfamiliar with prior work.
- Algorithm 1 lacks sufficient detail to highlight differences from previous algorithms, such as the REF update.
- Writing quality needs improvement, particularly in clarifying comparisons with RCRL and addressing the balance between reward and cost in experiments.

### Suggestions for Improvement
We recommend that the authors improve the emphasis on the problem formulation in equation (4) to enhance clarity. Additionally, the notation system should be streamlined to avoid confusion among readers. The authors should revise Algorithm 1 to clearly differentiate their approach from previous algorithms, particularly regarding the REF update. Furthermore, we suggest enhancing the writing quality to clarify the distinctions between their method and RCRL, and to better address the balance between reward and cost in the experimental results.