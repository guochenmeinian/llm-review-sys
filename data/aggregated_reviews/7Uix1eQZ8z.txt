ID: 7Uix1eQZ8z
Title: A State Representation for Diminishing Rewards
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 5, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on diminishing marginal utility (DMU) in reinforcement learning (RL), focusing on how rewards decay as an agent revisits states. The authors introduce a novel state representation, termed the λ representation (λR), which allows for convergence results and extends to continuous domains. Additionally, the paper explores a specific subclass of problems within submodular RL, emphasizing the exploitation of diminishing rewards. The authors propose a family of value-based approaches that enhance policy-based methods and are compatible with off-policy learning. They argue that their contributions are complementary to concurrent work by Prajapat et al., unifying various state representations into the λR framework. The paper discusses the challenges of defining a Bellman equation in the λR setting and proposes a recursive relationship instead.

### Strengths and Weaknesses
Strengths:
- The problem setting of DMU in RL is intriguing and relevant.
- The writing is clear and well-structured, making it easy to follow.
- The paper provides a solid theoretical analysis of the λ representations and their implications.
- The authors clearly distinguish their work from concurrent research, highlighting the complementary nature of their contributions.
- The inclusion of proofs and detailed discussions in the appendix enhances the clarity and rigor of the results.

Weaknesses:
- The assumption of decay with λ is overly restrictive, limiting practical applicability. The authors should consider alternative decay models, such as using λ as a lower or upper bound on rewards.
- The experimental evaluation lacks diversity in reward functions and does not compare against state-of-the-art exploration baselines, making it difficult to assess the proposed methods' effectiveness.
- The motivation for studying DMU in this context is not strongly articulated, and the paper could benefit from clearer distinctions between objective and subjective diminishing rewards.
- There is a lack of clarity regarding the convergence to an optimal policy, with concerns raised about local versus global convergence.
- The notation in Eq. 2.4 may be confusing, potentially hindering reader comprehension.
- The implications of Theorem 5.1 regarding global governance and additive error require further elucidation.

### Suggestions for Improvement
We recommend that the authors improve the motivation for their study by explaining the practical implications of DMU in RL beyond the general concept. Additionally, consider exploring alternative decay models for λ that could broaden the applicability of the findings. We suggest including a comparison with various state-of-the-art exploration baselines in the experiments, as well as conducting experiments on more complex tasks beyond the Halfcheetah example. Furthermore, we advise clarifying the assumptions behind Proposition 4.1 and providing a high-level algorithm block for better understanding. We also recommend improving the clarity surrounding the convergence properties of their methods, specifically addressing the distinction between local and global convergence. Additionally, revising the notation in Eq. 2.4 to enhance reader understanding and providing a more detailed explanation of the implications of Theorem 5.1 to clarify its significance will strengthen the paper. Lastly, addressing the identified typos and grammatical errors will enhance the overall presentation.