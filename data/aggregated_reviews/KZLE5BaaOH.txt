ID: KZLE5BaaOH
Title: A StrongREJECT for Empty Jailbreaks
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 9, 7, 5, -1
Original Confidences: 4, 3, 4, -1

Aggregated Review:
### Key Points
This paper presents StrongREJECT, a high-quality jailbreak benchmark that includes a dataset of rigorously selected forbidden prompts and an advanced automated evaluator. StrongREJECT aims to accurately assess the effectiveness of jailbreak techniques for language models, achieving state-of-the-art agreement with human evaluators. The authors highlight that existing jailbreak research often overestimates the success of jailbreak attacks and that current methods can lead to low-quality responses.

### Strengths and Weaknesses
Strengths:
1. The work is significant and contributes meaningfully to LLM safety research.
2. It reports many interesting findings, including the limitations of existing jailbreak methods.
3. The paper is well-written and easy to understand.

Weaknesses:
1. The dataset of forbidden prompts could be more extensive and time-evolving.
2. The scoring function in Section 2.2 may benefit from differentiated weights for "specific" and "convincing" scores to reduce potential bias.
3. The dataset currently comprises only 313 prompts, which is relatively small.

### Suggestions for Improvement
We recommend that the authors improve the dataset by including a wider variety of forbidden prompts to ensure comprehensive coverage. Additionally, we suggest providing more accessible tools or scripts for using the benchmark to enhance usability. Analyzing mismatch cases between human evaluations and the proposed metric scores could also clarify the metric's effectiveness. Finally, establishing a protocol for regular updates to the dataset would help keep pace with evolving standards and policies.