ID: 4jRNkAH15k
Title: DETAIL: Task DEmonsTration Attribution for Interpretable In-context Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 5, 6, 8, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DETAIL, a novel technique for attributing and interpreting in-context learning (ICL) demonstrations in transformer-based language models. The authors propose an adaptation of the influence function to address the unique characteristics of ICL, treating transformers as implementing an internal kernelized ridge regression. DETAIL is evaluated across various tasks, including demonstration perturbation, noisy demonstration detection, and real-world applications like demonstration reordering and curation, demonstrating its superiority over existing methods in performance and computational efficiency.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel approach, DETAIL, that leverages the internal optimizer perspective of transformers to address ICL attribution challenges.
2. The method incorporates random matrix projection for dimensionality reduction, achieving significant speedups (up to 10x) while maintaining effectiveness.
3. DETAIL shows effectiveness across multiple tasks, including noisy demonstration detection and curation, indicating broad applicability.
4. Extensive experiments validate the method's effectiveness on both custom transformers and large language models (LLMs) like Vicuna-7b and Llama-2-13b.
5. The transferability of DETAIL scores from white-box to black-box models enhances its practical applicability.

Weaknesses:
1. The evaluation on the MNIST dataset in Section 5.1 lacks justification, as insights from this simplified setting may not clearly transfer to more complex LLMs, making it feel disconnected from the main contributions.
2. The Llama-2-13b model achieves only 60-70% accuracy on the AG News dataset without perturbation, which is significantly lower than the typical performance range of 85-96% reported in the literature. More insight into ICL performance on this dataset is needed.

### Suggestions for Improvement
We recommend that the authors improve the justification for the MNIST evaluation in Section 5.1, clarifying its relevance to the main contributions of the paper. Additionally, we suggest providing more insight into the performance of ICL on the AG News dataset to address the observed discrepancies. Furthermore, we encourage the authors to conduct more comprehensive experiments across a wider range of tasks and models, including results on Llama-2-13b and exploring the use of BERT embeddings for better initialization. Lastly, addressing the computational cost of DETAIL in demonstration curation and providing commentary on its potential for identifying adversarial attacks and aiding LLM training would enhance the paper's depth.