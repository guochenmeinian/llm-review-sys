ID: ptcFuwr4YD
Title: Can you Summarize my learnings? Towards Perspective-based Educational Dialogue Summarization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Multi-Modal Perspective-based Educational Dialogue Summarization (MM-PerSumm) task, which focuses on summarizing student-virtual tutor dialogues from three perspectives: student, tutor, and generic. The authors introduce the CIMA-Summ dataset, which extends an existing dataset by providing summaries from these perspectives. They propose the Image and Perspective-guided Dialogue Summarization (IP-Summ) model, a SeqSeq language model that utilizes multi-modal learning and a perspective-based encoder to enhance dialogue summarization.

### Strengths and Weaknesses
Strengths:
- The task of summarizing dialogues from multiple perspectives is innovative and beneficial for AI-based educational systems.
- The CIMA-Summ dataset, augmented with diverse summaries, is promising and potentially useful.
- The IP-Summ model appears to contribute positively to summarization quality, supported by comprehensive analysis and ablation studies.

Weaknesses:
- The small dataset size of over 1,000 articles raises concerns about the training and evaluation splits, which are inadequately addressed.
- The student's responses seem template-based, questioning the necessity of a complex graph-based approach.
- The paper does not consider more recent baselines from 2022, limiting the comparison of results.
- The analysis of model performance and results requires further depth, as highlighted by reviewers.

### Suggestions for Improvement
We recommend that the authors improve the technical rigor by providing a detailed analysis of the training and evaluation splits, possibly employing cross-validation due to the small dataset size. Additionally, consider simplifying the model's approach to the student's responses, as the current complexity may be unwarranted. We suggest including comparisons with more recent baselines to strengthen the results. Lastly, we encourage the authors to enhance the analysis of the model's performance and provide more comprehensive details on the training of baseline models, as well as addressing the reviewers' questions regarding similarity among summaries and inter-annotator agreement.