ID: b1J3WplfgM
Title: SKD-NER: Continual Named Entity Recognition via Span-based Knowledge Distillation with Reinforcement Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel Continual Named Entity Recognition (CNER) model enhanced by a reinforcement learning (RL) framework for knowledge distillation. The authors propose a span-based approach that addresses catastrophic forgetting and label noise, achieving state-of-the-art performance across single and multiple entity types on two datasets. The method is designed to be model-agnostic, allowing for easy integration into existing CNER systems.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and organized, with a clear presentation of the proposed method.
- The use of RL to dynamically adjust distillation parameters is innovative and contributes to improved performance.
- Experimental results indicate that the approach achieves state-of-the-art performance in CNER.

Weaknesses:
- The effectiveness of the RL strategies is unstable, and the absence of code submission limits reproducibility.
- Experimental rigor is lacking, with no multiple runs for averaging or significance tests conducted.
- Comparisons in the experimental results are unfair due to differences in encoder models used.
- The analysis of limitations is overly simplistic, and the motivation for the model design requires better articulation.

### Suggestions for Improvement
We recommend that the authors improve the organization and clarity of the motivation behind their model design, particularly regarding label noise and the necessity of dynamic hyper-parameter adjustment. Additionally, we suggest conducting multiple runs and significance tests to enhance the rigor of the experimental results. The authors should also ensure fair comparisons by using consistent encoder models across experiments. Finally, providing deeper analysis or case studies on performance drops and addressing the stability of RL in their results would strengthen the paper.