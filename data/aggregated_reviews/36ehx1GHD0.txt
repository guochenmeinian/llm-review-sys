ID: 36ehx1GHD0
Title: CLImage: Human-Annotated Datasets for Complementary-Label Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 6, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on Complementary-Label Learning (CLL), addressing the challenge of training classifiers using only negative labels. The authors identify a gap in CLL research due to the reliance on synthetic datasets and introduce four real-world datasets collected through human annotations. Benchmark experiments reveal a significant performance drop when transitioning from synthetic to real-world datasets, primarily due to annotation noise and biases. The research emphasizes the need for robust CLL algorithms and validation methods.

### Strengths and Weaknesses
Strengths:
- The paper introduces the first real-world datasets for CLL, enhancing the understanding of CLL algorithms' practical applicability.
- A comprehensive protocol for collecting complementary labels is well-explained and appears robust, ensuring high-quality data.
- Thorough benchmarking and a detailed ablation study provide valuable insights into the challenges of real-world CLL.

Weaknesses:
- The rationale for collecting a CLL dataset lacks strong motivation and clarity, raising questions about its necessity and relevance.
- The logical structure is convoluted, making it difficult to follow, particularly with the placement of the "results analysis" before the "experiments" section.
- Limited discussion on potential biases among human annotators and the generalizability of findings across different domains.

### Suggestions for Improvement
We recommend that the authors improve the clarity and logical flow of the paper, particularly by restructuring sections for better comprehension. A stronger motivation for the necessity of a real-world CLL dataset should be articulated, including examples of real-world scenarios that necessitate CLL. Additionally, conducting experiments on larger datasets with more classes could validate claims regarding the limitations of current algorithms. Finally, exploring the generalizability of findings across various domains and employing state-of-the-art models could further enhance the research's impact.