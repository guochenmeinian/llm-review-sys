ID: 7X5zu6GIuW
Title: Do's and Don'ts: Learning Desirable Skills with Instruction Videos
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 7, 5, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "DoDont," an instruction-based skill discovery algorithm aimed at learning desirable behaviors while avoiding undesirable ones through unsupervised skill discovery (USD). The method utilizes instruction videos to train an instruction network that differentiates between desirable (Do’s) and undesirable (Don’ts) behaviors, subsequently adjusting the reward function of the skill discovery algorithm. The authors validate their approach through experiments in complex continuous control tasks, demonstrating that DoDont effectively learns desirable behaviors with minimal instruction videos.

### Strengths and Weaknesses
Strengths:
- The integration of instructional videos into the USD framework is innovative, addressing the challenge of learning desirable behaviors without predefined reward signals.
- The paper offers practical value by providing a USD algorithm that learns meaningful and complex behaviors rather than merely generating variations of simple actions.
- Thorough experimental validation on three tasks shows that DoDont outperforms state-of-the-art methods in learning complex and desirable behaviors.
- The presentation, writing, and clarity of the paper are commendable.

Weaknesses:
- The instruction network relies on in-domain video data, which may not always be readily available in real-world scenarios, although it can be relatively easy to obtain.
- The idea of combining unsupervised RL with task specification is not novel, as earlier works have already explored intrinsic and extrinsic rewards.
- The paper lacks comparisons with existing methods for automatic reward design, such as those using large language models (LLMs) or vision-language models.
- The collection of "Don't" videos from random action rollouts may not adequately cover hazardous behaviors, potentially limiting the effectiveness of the learned reward function.

### Suggestions for Improvement
We recommend that the authors improve the justification of their novelty in combining unsupervised RL with task specification, as this concept has been previously explored. Additionally, we suggest including comparisons with existing methods for automatic reward design, particularly those utilizing LLMs, to highlight the advantages of DoDont. Clarifying the specific characteristics required for effective training of instruction videos, as well as addressing how the quality and clarity of these videos impact the algorithm's performance, would enhance the paper. Furthermore, we encourage the authors to provide more insights into the state space used in their experiments, particularly regarding the combination of visual inputs and proprioceptive data.