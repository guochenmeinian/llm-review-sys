ID: yAKuSbIwR7
Title: Neural Synaptic Balance
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 5, 3, 5, 5, -1, -1, -1
Original Confidences: 3, 5, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical framework for understanding neural synaptic balance, defined as the condition where the total norm of input weights equals that of output weights in neurons. The authors explore the conditions under which randomly initialized balanced models maintain this balance throughout training, incorporating various components of neural networks such as activations and regularizers. The study includes mathematical proofs demonstrating that balancing a network is a convex optimization process.

### Strengths and Weaknesses
Strengths:  
The study is comprehensive and provides detailed mathematical derivations, contributing to a clearer understanding of deep neural networks. The paper is well-structured, making it easy to follow the progression from basic claims to more complex concepts.

Weaknesses:  
The paper lacks sufficient motivation for why neural synaptic balance is significant. The authors do not convincingly explain the advantages of understanding this phenomenon, especially since simple regularizers like L2 can achieve balance naturally. Additionally, the empirical results suggest that unbalanced networks can sometimes outperform balanced ones, raising questions about the necessity of the proposed methods. The discussion on biological relevance is limited, and the implications of energy consumption in balanced networks are not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the motivation for their study by discussing potential application domains and the advantages of understanding neural synaptic balance. It would be beneficial to clarify the relationship between network balance and learning assessment, specifically addressing which gradient is referenced in their claims. We suggest revising Theorem 5.1 for clarity and conciseness, potentially breaking it down into smaller lemmas. Additionally, including an analysis of how classification performance relates to balancing could provide valuable insights. Finally, the authors should discuss the practical consequences of their findings and consider providing specific recommendations regarding learning rates and network structures for optimal training.