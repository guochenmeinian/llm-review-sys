ID: rmhSMGjWPp
Title: Multi-label and Multi-target Sampling of Machine Annotation for Computational Stance Detection
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of using language models (LLMs) for stance detection data annotation, comparing various zero-shot labeling techniques and evaluating a classifier trained on weakly labeled data. The authors demonstrate that LLMs like Alpaca, Vicuna, and GPT-3.5 turbo can achieve reasonable performance, but their predictions are sensitive to variations in prompts and labels. Additionally, the authors finetune the Roberta model, showing improvements in performance with machine-labeled data for out-of-domain tasks.

### Strengths and Weaknesses
Strengths:  
- The paper provides a solid experimental setup and demonstrates sound reproducibility by utilizing openly available language models.  
- It shows how LLMs can improve annotation tasks and highlights the factors affecting their performance.

Weaknesses:  
- The paper lacks focus, with several techniques explored superficially and insufficient analysis of the results.  
- Sections of the paper, particularly the experiments and the finetuning of Roberta, are unclear and disconnected from the main narrative.  
- There are concerns regarding potential test data leakage in the pre-trained models used for annotation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their experimental design by explicitly defining independent variables and their values in Figures 2-4. Additionally, we suggest that the authors enhance the presentation of results in Tables 1 and 4 to facilitate easier comparisons, possibly by providing averages or clearer layouts. To address the lack of focus, we encourage the authors to provide more motivation for the finetuning of the Roberta model and its evaluation across various datasets. Finally, to strengthen the argument against data leakage, we recommend repeating the analysis in Section 4 using a leave-one-out approach for training on out-of-domain datasets.