ID: qieeNlO3C7
Title: Transformers learn through gradual rank increase
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 6, 4, 6, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents theoretical justifications and empirical evidence that transformers exhibit incremental learning dynamics in the low-initialization regime. The authors analyze a restricted diagonal attention model and establish features of learning dynamics in single-layer transformers through stringent assumptions, including nondegeneracy of dynamics, existence of strict local minima, and robustness of gradient flow. These assumptions enable the authors to demonstrate that learning progresses through discrete, incremental stages of rank increase, which are empirically validated using a toy learning scenario and a full multi-layer vision transformer.

### Strengths and Weaknesses
Strengths:  
1. **Convincing Theory**: The theoretical framework is accessible and provides valuable tools for understanding learning dynamics in complex models.  
2. **Empirical Confirmation**: The empirical validation of incremental dynamics in the single attention layer model is intriguing, showing predicted incremental "activation" of directions.

Weaknesses:  
1. **Organization**: Important observations are relegated to Supplementary Material, and the main paper could benefit from better integration of experimental evidence related to the assumptions.  
2. **ViT Training Regime**: The reliance on Adam for empirical results may introduce dynamics that obscure the learning process; results using vanilla SGD would be beneficial for verification.

### Suggestions for Improvement
We recommend that the authors improve the organization of the paper by integrating key experimental evidence related to assumptions 4.2, 4.3, and 4.4 into the main text, potentially including plots from the Supplementary Material that illustrate stepped behavior and dependence on $\alpha$. Additionally, we suggest conducting experiments using vanilla SGD to verify that the observed learning dynamics are indeed due to the low-initialization regime. Furthermore, addressing the strong assumptions in the theoretical framework and discussing their implications for practical applications would enhance the paper's relevance.