ID: hInB4JIQ5P
Title: CoEdIT: Text Editing by Task-Specific Instruction Tuning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a text editing system utilizing human-readable task instructions, such as “make the sentence simpler,” rather than fixed actions. The authors constructed the CoEdit dataset from the public text editing dataset ITERATER, converting editing intentions into instruction triplets <instruction: source, target>. The dataset is employed to fine-tune various FlanT5 models, which are then compared against major editing systems and LLM models through quantitative and qualitative evaluations, including human assessments for composite editing tasks.

### Strengths and Weaknesses
Strengths:  
- The paper includes extensive benchmark comparisons with major editing systems, demonstrating competitive results with a smaller model size.  
- It explores composite text editing through human evaluation, a less discussed area in prior works.  
- Implementation details, including prompts, are clearly described, aiding reproducibility.  

Weaknesses:  
- The novelty of the approach is limited, as fine-tuning LLMs is a common technique, and the contributions of the CoEdit dataset construction are less significant.  
- Key baseline comparisons are missing, particularly with single-task fine-tuned Flan-T5 models, which could clarify the effectiveness of multi-task instruction tuning.  
- The argument regarding generalization performance is weak, with concerns about the similarity of unseen tasks to training tasks.

### Suggestions for Improvement
We recommend that the authors improve Section 3 by providing more details about the model training based on previous work with FlanT5 to clarify performance variations. Additionally, we suggest including comparisons with single-task fine-tuned Flan-T5 models to strengthen claims about the effectiveness of multi-task instruction tuning. To address concerns about generalization, we encourage the authors to select more distinct unseen tasks and provide clearer explanations regarding the similarity and differences between tasks. Lastly, we advise refining the clarity of the CoEdit terminology throughout the paper to avoid confusion.