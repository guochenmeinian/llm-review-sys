ID: RcPHbofiCN
Title: Mixture of In-Context Experts Enhance LLMs' Long Context Awareness
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 6, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Mixture of In-Context Experts (MoICE) method aimed at enhancing long-context awareness in large language models (LLMs) using Rotary Position Embedding (RoPE). The authors propose a router that dynamically selects RoPE angles for each attention head and token, employing a lightweight training strategy that freezes LLM parameters while updating only the routers. Empirical evaluations indicate that MoICE outperforms existing methods on long-context tasks, demonstrating its effectiveness.

### Strengths and Weaknesses
Strengths:  
- The MoICE approach innovatively addresses the limited context awareness in LLMs by dynamically selecting RoPE angles.  
- Extensive experiments across multiple tasks and datasets show MoICE's competitive performance while maintaining efficiency.  
- The paper includes detailed ablation studies and analyses on hyperparameters, demonstrating robustness.  
- The writing is clear, and the claims are well-supported by experimentation.

Weaknesses:  
- The evaluation section shows minimal performance gains with MoICE, raising questions about the significance of the approach. Reporting mean and standard deviation of results would enhance clarity.  
- The experiments lack sufficient open-ended tasks, relying on a small dataset that does not adequately demonstrate generalizability.  
- The router is trained only for context lengths of 8k, and exploring larger input lengths would be beneficial.  
- The auxiliary loss definition appears ad hoc, and the method's adaptability to non-RoPE models is limited.

### Suggestions for Improvement
We recommend that the authors improve the evaluation section by reporting mean and standard deviation of results to clarify the significance of performance gains. Additionally, conducting more experiments on open-ended tasks, such as TriviaQA, would strengthen the generalizability of MoICE. Exploring the application of MoICE during the pre-training stage on larger context lengths (e.g., 16k) would also be valuable. Finally, a discussion on the auxiliary loss definition and its implications for non-RoPE models should be included.