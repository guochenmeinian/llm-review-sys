ID: pGlnFVmI4x
Title: Boosting Summarization with Normalizing Flows and Aggressive Training
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "FlowSUM," a novel variational encoder-decoder framework for Transformer-based summarization that addresses two significant challenges: insufficient semantic information in latent representations and posterior collapse during training. The authors propose a controlled alternate aggressive training (CAAT) strategy with an improved gate mechanism and utilize normalizing flows for flexible latent posterior modeling. Extensive experiments demonstrate that FlowSUM significantly enhances summary quality and facilitates knowledge distillation without a substantial increase in inference time.

### Strengths and Weaknesses
Strengths:
1. The use of normalizing flows for variational summarization is a unique and promising idea.
2. The introduction of a new training strategy with an improved gate mechanism effectively addresses posterior collapse.
3. The paper is well-written and well-motivated, with extensive experiments and good ablation studies.

Weaknesses:
1. Figure 1 lacks clarity regarding the NF latent module.
2. The paper requires clearer definitions of variational components in the proposed framework, particularly in Figure 1.
3. The performance of the VED framework, based on PLMs, needs further exploration.
4. The differences between the proposed VED and existing frameworks should be more explicitly stated.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Figure 1 by clearly denoting the variational posterior, variational prior, and encoder-decoder structure. Additionally, we suggest providing references to support the backbone of the proposed framework and addressing the performance of the VED framework in summarization tasks. The authors should also clarify the distinctions between their proposed VED and other frameworks, particularly regarding the introduction of y into the variational posterior. Lastly, we advise correcting typos and ensuring that abbreviations are defined upon first use.