ID: LPtO1evrGa
Title: Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the first comprehensive analysis of Large Language Model (LLM)-based conversational systems, focusing on proactive dialogues, including clarification, target-guided, and non-collaborative dialogues. The authors propose the proactive chain-of-thought prompting scheme (ProCoT) to enhance dialogue systems' planning capabilities. Experiments with ChatGPT and Vicuna reveal that these systems rarely ask clarification questions, often make abrupt topic transitions, and struggle with strategic decision-making. The ProCoT scheme is shown to mitigate some of these limitations.

### Strengths and Weaknesses
Strengths:
- The paper provides a thorough evaluation of proactive dialogue capabilities using various prompting strategies.
- It offers a sound evaluation protocol and is well-written, making it accessible to readers.
- The work contributes valuable insights into the challenges of LLM-based proactive dialogue systems.

Weaknesses:
- The novelty of the paper is limited, as the concepts of intermediate steps and chain-of-thought reasoning are not new.
- The evaluation results are mixed, with ProCoT showing effectiveness in some areas but not others, leaving a significant gap to state-of-the-art performance.
- The paper's density makes it challenging to reconstruct findings, and some methodological details are insufficiently supported.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by streamlining the content and providing clearer guidance on the evaluation metrics and results. Specifically, clarify the sources of baseline and state-of-the-art figures in Table 1, and ensure consistency in terminology throughout the paper. Additionally, we suggest that the authors include crucial evaluation metrics in the main text rather than relegating them to the appendix, as this information is vital for understanding the paper's conclusions. Lastly, we encourage the authors to address the reproducibility concerns by providing more detailed descriptions of the prompting techniques and evaluation conditions.