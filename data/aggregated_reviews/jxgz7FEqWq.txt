ID: jxgz7FEqWq
Title: Sparse Low-rank Adaptation of Pre-trained Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called sparse low-rank adaptation (SoRA) for parameter-efficient fine-tuning of large pre-trained language models. SoRA introduces a sparse gating unit that dynamically adjusts the intrinsic rank of low-rank decomposition matrices during training, utilizing a proximal gradient method with sparsity regularization. The authors demonstrate that SoRA enhances the representation power and parameter efficiency of the existing low-rank adaptation (LoRA) method, outperforming various baselines on natural language understanding tasks. The method also explores the relationship between non-zero parameters and the model's memorization and generalization capabilities through a sparsifying scheduler.

### Strengths and Weaknesses
Strengths:
- The proposed SoRA method offers a novel approach to enhance the representational strength of LoRA by allowing dynamic rank adjustments.
- The paper provides a clear theoretical explanation of SoRA's principles and demonstrates its effectiveness across multiple tasks, achieving significant improvements in parameter efficiency.
- Reviewers commend the clarity of writing and the overall soundness of the study.

Weaknesses:
- The study lacks sufficient ablation analysis and hyperparameter optimization, particularly regarding the parameter $\xi$ for controlling model sparsity.
- The problem definition could be clearer, as the extensive use of symbols may lead to confusion.
- Comparisons with relevant baseline methods, such as MPOP and LoRA, are insufficient, and standard deviations from experiments are not reported.

### Suggestions for Improvement
We recommend that the authors improve the ablation analysis and provide a thorough exploration of hyperparameter optimization, particularly for the parameter $\xi$. Additionally, we suggest clarifying the problem definition to reduce confusion caused by symbol usage. The authors should include comparisons with the MPOP method and other relevant baselines, as well as report standard deviations in experimental results to substantiate claims of improvement. Lastly, a detailed analysis of the computational complexity of SoRA would enhance its applicability in resource-constrained environments.