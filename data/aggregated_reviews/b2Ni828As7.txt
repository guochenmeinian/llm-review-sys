ID: b2Ni828As7
Title: Transformers to Predict the Applicability of Symbolic Integration Routines
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 3, 7, 6, 7
Original Confidences: 4, 4, 2, 4

Aggregated Review:
### Key Points
This paper presents a machine learning approach to optimize symbolic integration in Computer Algebra Systems (CAS) by training a transformer model to predict the success of various integration methods. The authors compare the performance of the transformer against traditional human-made heuristics in Maple, demonstrating that the transformer outperforms these established methods. The study provides a large dataset of integrable expressions, which is beneficial for machine learning applications in symbolic integration.

### Strengths and Weaknesses
Strengths:
- The method effectively helps software decide which integration technique to use.
- The transformer model shows strong performance compared to human-engineered heuristics.
- The paper is well-structured and addresses an interesting research question.

Weaknesses:
- A significant portion of the work relies on data collection from prior studies, lacking novelty.
- There is insufficient explanation regarding how the model makes decisions, particularly the role of absolute values, which affects interpretability.
- The implications of the findings are not adequately highlighted, and the evaluation lacks completeness, particularly regarding the costs associated with using the transformer guard versus traditional methods.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contributions by emphasizing unique aspects of their work. Additionally, we suggest conducting more extensive experiments to further investigate the advantages of the transformer model, as mentioned in the future work section. It would be beneficial to clarify the implications of the transformer outperforming existing heuristics for the field of symbolic integration. Furthermore, we advise including a comparison with a "trivial" baseline guard in Table 1 for methods without guards to strengthen the argument for the transformer guard's utility. Reporting recall alongside precision would provide a more comprehensive view of the model's performance. Lastly, we encourage the authors to enhance the writing in Section 4 by correcting typos, grammatical errors, and citation formatting, and to provide background on LIGs either in the section or through an appendix.