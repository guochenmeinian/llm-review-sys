ID: lWlBAJTFOm
Title: Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for improving complex reasoning in language models (LMs) by training a separate decomposition model that guides a solver model through reinforcement learning (RL). The authors propose that this approach, which contrasts with traditional prompt-based methods, enhances performance and generalization across various benchmarks. The results indicate significant improvements over baseline prompting methods, demonstrating the potential of fine-tuning smaller models to guide larger black-box models effectively.

### Strengths and Weaknesses
Strengths:
- The proposed method addresses a critical issue in complex reasoning and shows strong performance improvements over prompt-based baselines.
- The approach effectively utilizes RL to fine-tune a decomposition model, which can generalize well to other solver models and benchmarks.
- The paper is well-presented, with a clear motivation for the research.

Weaknesses:
- The motivation for separating decomposition and solution generation lacks sufficient support, particularly the analogy to neuroscience.
- Comparisons are unfair as they only include prompting methods as baselines, neglecting the potential benefits of fine-tuning existing models.
- The case study is limited to one sample, suggesting potential cherry-picking, and a more comprehensive qualitative analysis is needed.
- Some descriptions, particularly regarding cosine similarity and the mathematical expressions, are unclear and could confuse readers.

### Suggestions for Improvement
We recommend that the authors improve the motivation for using separate models by providing stronger empirical support or discussing concurrent work that demonstrates the effectiveness of monolithic models. Additionally, including a more diverse set of baselines that incorporate elements of fine-tuning would strengthen the experimental design. A broader qualitative analysis with multiple samples, highlighting both successes and failures, would enhance the understanding of the method's strengths and weaknesses. Furthermore, we encourage the authors to clarify ambiguous terminology and improve the overall writing quality for better readability.