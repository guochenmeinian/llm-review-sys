ID: VaLAWrLHJv
Title: LoRA-GA: Low-Rank Adaptation with Gradient Approximation
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 6, 6, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LoRA-GA, a novel initialization method for Low-Rank Adaptation (LoRA) that approximates the gradient update of weights. The authors propose aligning the gradients of the low-rank matrix product with those of full fine-tuning from the first step, achieving a 2-4 times improvement in convergence speed and better accuracy compared to vanilla LoRA and its variants across several benchmark datasets.

### Strengths and Weaknesses
Strengths:
1. The paper offers a novel perspective on LoRA initialization from the gradient update aspect.
2. It provides a comprehensive theoretical analysis that is well-written and easy to follow.
3. The combination of gradient approximation and stable scale for initialization is a unique contribution.

Weaknesses:
1. The difference between LoRA-GA and LoRA reparameterization is not illustrated in Figure 1.
2. The paper lacks a discussion on the influence of sampled batch size.
3. The experimental results primarily focus on GULU and metamath-100k, which may not represent a fair comparison with other methods.
4. The application of eigenvectors for initialization, while original in this context, may not be entirely new.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Figure 1 to better illustrate the differences between LoRA-GA and LoRA reparameterization. Additionally, a discussion on the influence of sampled batch size should be included. To enhance the robustness of the experimental results, we suggest conducting experiments on the full metamath dataset and exploring more complex tasks. Furthermore, we encourage the authors to ensure that hyperparameters for baseline methods are adequately tuned to provide a fair comparison, as discrepancies in performance may arise from suboptimal settings.