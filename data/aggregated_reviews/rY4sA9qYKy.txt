ID: rY4sA9qYKy
Title: On the impact of activation  and normalization in obtaining  isometric embeddings at initialization
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 7, 5, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of layer normalization and its interaction with non-linear activation functions, focusing on the isometry of the Gram matrix in neural networks. The authors propose that layer normalization, when applied correctly, biases the input-output mapping towards isometry at initialization, which can mitigate issues like rank collapse in multi-layer perceptrons (MLPs). The paper includes both theoretical results and empirical evidence, highlighting the importance of higher-order Hermite coefficients in achieving isometric properties.

### Strengths and Weaknesses
Strengths:
- The paper provides a novel theoretical contribution that enhances understanding of layer normalization and activation functions in relation to isometry.
- It includes a substantial number of empirical results that support the theoretical claims, particularly regarding the effects of normalization and activation on the Gram matrix.

Weaknesses:
- The presentation of the paper is criticized for being rushed and lacking coherence, with many sections feeling disconnected.
- There are numerous typographical errors and unclear statements that detract from the overall clarity.
- The empirical analysis is limited, focusing on a narrow range of activation functions and datasets, which may hinder the generalizability of the findings.

### Suggestions for Improvement
We recommend that the authors improve the overall presentation of the paper by enhancing coherence between sections and addressing typographical errors. Specifically, clarify statements such as "fluctuations in norms" and the implications of "orthogonalization of these results." Additionally, we suggest expanding the discussion on the properties of Definition 3 and providing more context for the empirical results, including the datasets used. Incorporating larger-scale data, such as MNIST, could strengthen the empirical analysis. Lastly, we encourage the authors to better justify the motivation for studying isometry and its relevance to neural networks, potentially by referencing past literature.