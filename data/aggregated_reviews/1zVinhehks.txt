ID: 1zVinhehks
Title: Graph Classification via Reference Distribution Learning: Theory and Practice
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 4, 7, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel algorithm called GRDL for graph classification, which treats latent node embeddings from GNN layers as discrete distributions, allowing for direct classification without global pooling. The authors provide theoretical analysis of generalization error bounds and demonstrate the algorithm's superiority through experiments on various graph datasets, claiming GRDL is ten times faster than leading competitors.

### Strengths and Weaknesses
Strengths:
- The proposed algorithm GRDL and its theoretical results are original and novel.
- The paper is well-organized, featuring extensive theoretical results and numerical comparisons.
- The motivation, assumptions, optimization, implications of theorems, and experimental settings are clearly articulated.
- The method effectively addresses the challenges of graph classification by eliminating readout operations and directly classifying discrete distributions.

Weaknesses:
- The adaptation of ideas from domain transfer learning to graph classification is insufficient, as graphs in each dataset are drawn i.i.d., contradicting the hypothesis of different reference distributions for different classes.
- The paper does not sufficiently avoid information compression compared to existing pooling methods, as the mapping from the embedding matrix to a vertex can be seen as information compression.
- High time complexity ($O(N^2)$) limits scalability to larger graphs, and the datasets used for large-scale experiments are not truly large-scale in terms of node count.
- The theoretical contribution regarding generalization error bounds lacks clarity and practical guidance for hyperparameter selection, with trial-and-error still required for hyperparameter tuning.
- Important baselines are missing from experiments, including recent graph pooling methods and other GNN models like GCN and GAT.

### Suggestions for Improvement
We recommend that the authors improve the adaptation of their method to graph classification by addressing the contradiction in their hypothesis regarding reference distributions. Additionally, clarify the information compression aspect of their approach compared to existing methods. To enhance scalability, consider revising the algorithm to reduce time complexity and include experiments on synthetic large-scale datasets. We suggest adding comparisons with state-of-the-art efficiency competitors and incorporating recent graph pooling methods and GNN models in the experimental results. Finally, provide clearer definitions and practical guidance regarding hyperparameter choices in the theoretical analysis.