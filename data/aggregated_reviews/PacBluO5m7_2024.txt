ID: PacBluO5m7
Title: KnowGPT: Knowledge Graph based Prompting for Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents KnowGPT, a novel Knowledge Graph (KG)-based prompting framework designed to effectively integrate domain knowledge into large language models (LLMs) and ground their responses in established facts to mitigate hallucinations. KnowGPT addresses challenges such as large search spaces, high API costs, and labor-intensive prompt engineering. It features a knowledge extraction module that identifies relevant information from KGs and a context-aware prompt construction module that generates effective prompts from the extracted knowledge. The authors introduce a reinforcement learning (RL) method with a comprehensive reward scheme and an auto-selection algorithm for prompt formats. Experiments demonstrate that KnowGPT achieves a notable 92.6% accuracy on the OpenbookQA leaderboard, nearing human-level performance, and extensive validation includes comparisons with 22 baseline models and detailed ablation studies.

### Strengths and Weaknesses
Strengths:
1. The topic of LLMs combined with KGs is highly relevant, as KGs are essential for correcting LLM outputs.
2. The proposed method employs a search approach to determine the optimal graph structure and prompt format for LLMs.
3. The authors provide a clear distinction between their work and existing methods like DeepPath and Active Ensemble Policy Learning (AEKE), highlighting differences in motivation, reward functions, and training strategies.
4. The introduction of a penalty term in the Multi-Armed Bandit (MAB) approach enhances exploration in prompt format selection, improving the balance between exploration and exploitation.
5. The results indicate that the method outperforms most existing approaches, demonstrating strong performance.

Weaknesses:
1. The paper lacks critical details about KnowGPT, including: (a) training of the reinforcement learning component on the knowledge graph; (b) optimization of the Knowledge Graph Agent with the Prompt Construction using Multi-Armed Bandit; (c) implementation specifics for the two-hop subgraph extraction; and (d) clarity on prompt examples provided to LLMs, leading to confusion regarding classification types.
2. The paper lacks a thorough discussion on how the action space in reinforcement learning on Knowledge Graphs is managed.
3. Clarifications on the differences between "Graph Description" and "Sentences" in prompts could be more explicit, particularly regarding the noise in "Graph Description."
4. The contribution primarily revolves around a search algorithm for subgraph structures and prompt forms, which the authors should distinguish more clearly from existing methods like Deeppath and AEKE.

### Suggestions for Improvement
We recommend that the authors improve the clarity and detail regarding the training of the reinforcement learning component, including the overall objective function and how to address the large action space in reinforcement learning on Knowledge Graphs. Additionally, the authors should provide specific examples of prompts used with LLMs to clarify the classification problem. We also suggest elaborating on the differences between the proposed search algorithm and those in existing literature, particularly in relation to Deeppath and AEKE. Furthermore, we recommend providing a more detailed explanation of the differences between "Graph Description" and "Sentences" in the prompt to clarify the impact of noise on the extracted subgraph structure. Lastly, a more concise introduction would enhance the paper's readability.