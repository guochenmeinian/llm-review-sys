ID: 9XDYEEBRV6
Title: Coded Computing for Resilient Distributed Computing: A Learning-Theoretic Framework
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 4, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework for coded computing aimed at enhancing the reliability of distributed machine learning applications. The authors derive loss-minimizing encoding and decoding functions, utilizing learning theory to establish a regularized objective function for the decoder and a loss function for the overall system. The proposed method, which sends mixtures of input samples to worker nodes, is theoretically analyzed and compared with the existing Berrut coded computing (BACC) approach, demonstrating improved estimation accuracy in empirical evaluations.

### Strengths and Weaknesses
Strengths:
- The introduction of a learning theory-based framework for coded computing is novel and interesting.
- The paper provides rigorous theoretical analysis, including convergence rate derivation and recoverability analysis.
- Experimental results validate that the proposed method outperforms BACC in terms of reconstruction accuracy.

Weaknesses:
- The practical use case for the proposed method remains unclear, particularly in the context of inference, where existing models already demonstrate low latency.
- The experimental section lacks comprehensiveness, considering only one baseline and not reporting the computational efficiency of the encoder and decoder.
- The relationship between the optimal encoder and decoder functions and the neural networks used in experiments is not adequately analyzed.

### Suggestions for Improvement
We recommend that the authors clarify the practical applications of their method, particularly in scenarios where inference latency is a concern. Additionally, we suggest expanding the experimental section to include comparisons with multiple baselines and reporting the computational resources required for fitting and evaluating the encoder and decoder. Furthermore, we encourage the authors to analyze the performance of the proposed method under optimal encoder and decoder conditions and to provide a discussion on the implications of using mixtures of input samples that may not align with the training data distribution.