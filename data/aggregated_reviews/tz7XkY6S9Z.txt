ID: tz7XkY6S9Z
Title: Mr. HiSum: A Large-scale Dataset for Video Highlight Detection and Summarization
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 5, 6, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a large-scale video summarization dataset named "Mr. Sum," comprising 31,892 videos with labels derived from the viewing behaviors of over 50,000 users per video. The dataset aims to address the scarcity of large-scale video summarization datasets, providing a benchmark for video highlight detection and summarization tasks. The authors propose a novel approach to video summarization by leveraging natural watching behavior as labels, primarily focusing on highlight detection rather than comprehensive summarization. The dataset is based on YouTube's "most replayed" statistics, which are aggregated from user interactions, and the authors emphasize the importance of video category information during model training. However, the paper lacks a detailed discussion on the potential biases and fairness of these statistics.

### Strengths and Weaknesses
**Strengths:**
- The dataset is significantly larger than existing ones, enhancing its utility for machine learning training and testing.
- The use of natural watching behavior data from YouTube is innovative and adds value to the research.
- The dataset's scale (50,000+ users) enhances the reliability of the annotations for highlight detection.
- The authors have made substantial revisions to clarify the dataset's purpose and improve the manuscript's coherence.
- Empirical validation of the labels' effectiveness is conducted, and the paper is well-structured and clear.

**Weaknesses:**
- The paper lacks a detailed explanation of the annotation process and the quality assurance of the labels.
- There is ambiguity in the distinction between video summarization and highlight detection, which may oversimplify the summarization problem.
- The dataset's reliance on YouTube-8M for categorization raises questions about its unique contribution.
- The current benchmark includes a limited number of methods, with only three evaluated, which may not suffice for a robust comparison.
- The paper does not adequately address the potential biases in the dataset, relying solely on YouTube statistics without verification.
- There is insufficient discussion on the rationale for category selection in the analysis.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the annotation process, including how the quality of the labels was ensured and the methodology for collecting user data. A discussion on the human cost and recruitment process for obtaining the annotations should be included. Additionally, we suggest conducting human evaluations on a subset of videos to validate the alignment of replay statistics with human annotations. The authors should also clarify the rationale behind the selection of specific video categories for analysis and address potential biases in the dataset. To strengthen the benchmark, we encourage the authors to include more methods and run these baselines on the entire dataset to provide a more comprehensive evaluation. Finally, we advise the authors to conduct thorough checks to identify any demographic or geographic biases in the dataset, as this could offer valuable insights into its reliability.