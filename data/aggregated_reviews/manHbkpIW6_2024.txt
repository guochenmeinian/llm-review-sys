ID: manHbkpIW6
Title: Once Read is Enough: Domain-specific Pretraining-free Language Models with Cluster-guided Sparse Experts for Long-tail Domain Knowledge
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 4, 7, 7, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach called Cluster-guided Sparse Experts (CSE) aimed at enhancing language models' ability to learn long-tail domain knowledge during pretraining, potentially eliminating the need for domain-specific finetuning. The authors demonstrate that integrating CSE layers, which cluster semantically similar long-tail data, leads to superior performance on various downstream tasks compared to standard pretraining and finetuning methods. The study shows that less frequent sentences exhibit gradients dissimilar to those of majority data points, supporting the proposed method's effectiveness.

### Strengths and Weaknesses
Strengths:
1. The CSE method offers an innovative solution to the challenge of learning long-tail domain knowledge in language models, potentially reducing the need for costly finetuning.
2. The paper is well-written and concise, providing clear insights and a coherent analysis, particularly in the gradient consistency analysis.

Weaknesses:
1. The experimental setup lacks clarity, particularly regarding whether it follows the pretrain-and-fine-tune paradigm for both BERT and GPT.
2. The simplicity of the sentence frequency score calculation may undermine its robustness, and the consistency of results across figures raises questions about the effectiveness of CSE.
3. Several arguments lack substantiation, and the paper contains typographical errors and unclear statements that hinder comprehension.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental setup, explicitly stating whether the pretrain-and-fine-tune paradigm is followed for both BERT and GPT. Additionally, we suggest including a baseline MoE model pretrained with Wikipedia to strengthen comparisons. The authors should also provide more detailed analyses to support their claims, particularly regarding the "random distribution" of low-frequency sentences and the significance of increased cluster distances. Furthermore, we advise addressing typographical errors and enhancing the presentation of figures, including increasing font sizes for better readability. Lastly, we encourage the authors to discuss the rationale behind their choice of testing datasets and the implications of their findings on broader applications.