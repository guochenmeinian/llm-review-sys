ID: NrAPGwleHA
Title: Deep Learning with Physics Priors as Generalized Regularizers
Conference: NeurIPS
Year: 2023
Number of Reviews: 3
Original Ratings: 7, 7, -1
Original Confidences: 3, 4, 4

Aggregated Review:
### Key Points
This paper presents a method for regularizing neural networks with physics priors by incorporating additional collocation points for model evaluation. The approach is assessed on Hamiltonâ€™s equations, a 1D reaction equation, and a 1D convection equation, demonstrating improved performance in most cases. The authors propose structuring these priors as generalized regularizers, leveraging Vapnik's structural risk minimization (SRM) to enhance testing accuracy.

### Strengths and Weaknesses
Strengths:
- The experiments are extensive, and the results appear promising.
- The theory is well presented with good mathematical details, and the experimental section supports the theoretical arguments.

Weaknesses:
- The contributions of the paper are unclear, as incorporating physics priors is not a novel concept. 
- The abstract is vague and short; it requires improvement for the camera-ready version.
- Sections 1 and 2 are difficult to read, necessitating clearer explanations of the task and the physics prior.
- Mathematical notation needs refinement, including:
  1. Clarification on whether Equation (1) is missing the L2-norm or if observations are scalar.
  2. Proper integration of Equation (7) into the text.
  3. Enhancement of Equations (14) and (15) using the `\underbrace` command.
- Table 1 should highlight instances where the regularized model underperforms.
- A link to the code repository by Geydanus would be beneficial.
- Overall, the paper requires polishing, including addressing missing white spaces in references.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the abstract and enhance the readability of Sections 1 and 2 by providing a clearer definition of the physics prior. Additionally, we suggest refining the mathematical notation as outlined, emphasizing results in Table 1 where the regularized model does not perform better, and including a link to the code repository. Finally, we advise the authors to polish the paper by correcting formatting issues, such as missing white spaces in references.