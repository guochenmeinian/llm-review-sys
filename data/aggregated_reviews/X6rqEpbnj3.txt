ID: X6rqEpbnj3
Title: Why Transformers Need Adam: A Hessian Perspective
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 6, 7, 5, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the performance disparity between SGD and Adam in training transformers, focusing on the eigenspectrum of the Hessian across various neural network architectures. The authors identify that while the full Hessian spectrum is similar at initialization, transformers exhibit block heterogeneity, with distinct spectra across parameter blocks, unlike CNNs and MLPs which are block homogeneous. They empirically demonstrate that increasing block spectrum variation leads to a decline in SGD performance, while Adam remains stable, attributing this to Adam's adaptive learning rates. The authors propose using the Jensen-Shannon distance of Hessian blocks as a quantitative measure of the performance gap and provide a preliminary theoretical analysis of convergence rates for SGD and Adam on a quadratic function with a block-form Hessian.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant question regarding optimization challenges in transformers, which is crucial for enhancing training techniques for large language models.
- It is well-organized, clearly written, and presents interesting ideas that could benefit the community.
- The connection between Adam's performance and Hessian block-wise heterogeneity is novel, supported by comprehensive empirical observations.

Weaknesses:
- The computational expense of Hessian approximation raises concerns about practicality for large models, as experiments are limited to smaller models.
- The theoretical analysis simplifies the momentum aspect of Adam, potentially overlooking important dynamics.
- The correlation between block heterogeneity and SGD performance lacks clear causation, and the definition of "block" appears arbitrary, which may limit generalizability.
- The experimental setup lacks detail on hyperparameters, making it difficult to ascertain if performance differences stem from the claimed reasons or insufficient tuning.

### Suggestions for Improvement
We recommend that the authors improve the computational efficiency of the Hessian approximation to enhance practicality for larger models. Additionally, providing a more detailed experimental setup, including hyperparameters, would aid reproducibility and clarify the impact of tuning on SGD performance. We suggest that the authors explore the evolution of block-wise spectra during training, as this could provide deeper insights. Furthermore, conducting ablation studies on the estimation of the Hessian spectrum through the SQL method would strengthen the analysis. Lastly, addressing the validity of the claim that SGD performs worse on attention-based models with more robust empirical evidence would enhance the paper's scientific value.