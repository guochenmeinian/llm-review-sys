ID: tDMTwto6jv
Title: SEL-BALD: Deep Bayesian Active Learning with Selective Labels
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 8, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to active learning that incorporates human discretion in the labeling process, specifically addressing the issue of selective labeling where users may abstain from providing labels. The authors propose the SEL-BALD framework, which extends the BALD sampling criterion to account for the rejection of labels, thereby improving model performance. The effectiveness of the proposed algorithms is demonstrated through experiments on both synthetic and real-world datasets, highlighting the practical implications of modeling human behavior in active learning.

### Strengths and Weaknesses
Strengths:
1. The problem setting is realistic and relevant, as it acknowledges that human annotators may not always provide labels due to various constraints.
2. The paper is clearly written, with well-supported claims and a logical structure that enhances readability.
3. The proposed methods are meaningful, extending existing approaches like BALD to new contexts, which could positively impact industrial applications.
4. The integration of a rejection function into Bayesian sampling provides better guidance for active learning, enhancing data efficiency.

Weaknesses:
1. The experiments utilize the MNIST dataset, which is considered outdated; more complex datasets such as SVHN or Waterbirds would strengthen the evaluation.
2. The literature review lacks connections to related areas such as learning to defer and PU Learning, which could enhance the paper's depth.
3. The human discretion function e(x) is not sufficiently detailed, raising questions about its estimation and effectiveness in practical scenarios.
4. The computational complexity of Bayesian models may limit their applicability, and the paper does not adequately address how the human discretion model adapts to changes over time.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by incorporating more sophisticated datasets, such as Fashion MNIST or CIFAR-10, to provide a more robust evaluation. Additionally, including a brief discussion on related areas like learning to defer and PU Learning would enhance the literature context. The authors should clarify the estimation process of the human discretion function e(x) and consider using a posterior distribution in its computation. Lastly, addressing the computational demands of the Bayesian models and how they can adapt to changing human behaviors would strengthen the overall contribution of the paper.