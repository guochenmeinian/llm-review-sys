ID: ytQFU2XsBR
Title: Automatic Model Selection with Large Language Models for Reasoning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for integrating two reasoning approaches, Chain-of-Thought (CoT) and Program-Aided-Language Models (PAL), utilizing a Large Language Model (LLM) for dynamic model selection based on problem complexity and method confidence. The authors demonstrate improved performance across eight inference datasets, achieving state-of-the-art results on GSM8K and SVAMP. The paper includes a thorough theoretical analysis that supports the proposed method's effectiveness and provides insights into the performance characteristics of CoT and PAL.

### Strengths and Weaknesses
Strengths:
- The proposed method is simple, intuitive, and effective, achieving new state-of-the-art performance on GSM8K.
- The theoretical analysis substantiates the advantages of combining CoT and PAL, providing interesting insights into their performance dynamics.
- The paper is well-written and comprehensible, with robust empirical results across multiple datasets.

Weaknesses:
- There is a lack of detailed explanation regarding the experimental setup, particularly concerning the number of examples and the selection process.
- The improvements on GPT-4 are marginal, raising questions about the justification for the added complexity of the method.
- The paper relies heavily on existing models without introducing significant innovations, positioning it as offering incremental rather than transformative contributions.
- The computational overhead and sensitivity of the model selection process to prompts are not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental setup by providing detailed information on the number of examples used and the selection process for CoT and PAL. Additionally, conducting experiments on open-source models like Llama could enhance the generalizability of the findings. The authors should also address the computational overhead introduced by their method and provide a more thorough analysis of the differences between CoT and PAL, including explicit examples that highlight their respective strengths. Finally, exploring ways to mitigate the sensitivity of the model selection process to prompts would strengthen the paper's applicability.