ID: pyqPUf36D2
Title: Pseudo-Private Data Guided Model Inversion Attacks
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 7, 4, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for enhancing model inversion attacks (MIAs) by utilizing pseudo-private data to improve the reconstruction of class characteristics from trained classifiers. The authors propose a two-step approach: first, conducting existing MIAs to gather attack samples, and second, fine-tuning the GAN's generator on a selected subset of high-quality samples to increase sampling density around the pseudo-private data. Additionally, the authors introduce the use of a dynamic prior, allowing for continuous fine-tuning of the generator during the inversion process, which enhances the model's adaptability. The results demonstrate significant improvements across various attack settings and model architectures, indicating the potential effectiveness of the proposed method.

### Strengths and Weaknesses
Strengths:
- The paper introduces a unique approach by fine-tuning the GAN's generator based on previous attack results, adding a dynamic aspect to the literature on MIAs.
- The methodology is well-supported by empirical evidence, including toy examples and results on standard benchmarks, particularly in Section 3.3.
- The evaluation encompasses a wide range of model architectures and datasets, showing consistent improvements in attack results.
- The authors effectively addressed previous reviewer concerns, providing additional details and clarifications that improved the paper's quality.

Weaknesses:
- The evaluation lacks comparisons to PLG-MI, which also utilizes pseudo-labeled data for MI, and does not investigate existing defense methods like MID, BiDO, and negative label smoothing.
- The reliance on metrics computed from an evaluation model trained on the same data as the target model may yield misleading results, as it could overestimate attack success by reconstructing adversarial features.
- The rationale for using MMD and CT instead of alternatives like KL Divergence, and the choice of LPIPS as a similarity metric, is insufficiently explained.
- The initial submission lacked sufficient details and experiments to justify a higher evaluation score.
- The claim regarding the dynamic prior may lead to misunderstandings, as some prior works also utilize dynamic selection processes, albeit not in the same manner as proposed.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including comparisons to PLG-MI and investigating the effectiveness of existing defense methods such as MID and BiDO. Additionally, consider incorporating alternative metrics, such as the FaceNet distance or the information extraction score, to provide a more comprehensive assessment of attack success. Clarifying the motivations for using MMD and CT, as well as the choice of LPIPS, would enhance the paper's rigor. We also suggest improving the limitation section by including discussions on failure cases and additional limitations of their method. Furthermore, clarifying the term "dynamic prior" to avoid confusion and ensuring that it accurately reflects the continuous fine-tuning process of the generator would strengthen the paper. Finally, demonstrating the method's performance in varied settings beyond facial images could bolster the argument for its broader applicability. Addressing typographical errors and ensuring all symbols are properly defined will improve the overall presentation.