ID: Pbpk9jUzAi
Title: Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 5, 5, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the effects of architectural, training, and inference techniques on adversarial training (AT) for ImageNet, an area less explored compared to CIFAR10 due to computational costs. The authors propose ConvStem, a novel architectural modification that enhances the robustness of ConvNeXt models against various threat models. They present key techniques for enhancing robustness, including ConvStem, pre-training with heavy data augmentation, extended training duration, and increased resolution at inference. The authors provide extensive experimental validation, demonstrating significant improvements in $\ell_1$ and $\ell_2$ robustness across multiple architectures, while achieving state-of-the-art (SOTA) performance in $\ell_\infty$ robustness. They address concerns regarding the fairness of comparisons with previous works by highlighting the efficiency of their training process, achieving better results with fewer training passes. However, the paper lacks explanations for the empirical observations, particularly regarding the effectiveness of ConvStem across various architectures and AT methods.

### Strengths and Weaknesses
Strengths:
1. The investigation of adversarial training on ImageNet is crucial and underexplored.
2. Establishing effective AT techniques for ImageNet could significantly influence future benchmarking.
3. Extensive experimental validation confirming the efficacy of ConvStem across various architectures and threat models.
4. Notable robustness improvements with ConvStem on certain architectures, particularly in $\ell_1$ and $\ell_2$ robustness, with competitive performance in $\ell_\infty$.
5. Efficient training process demonstrated through fewer forward/backward passes compared to prior works.
6. The impact of test-time image resolution is an intriguing finding.

Weaknesses:
1. ConvStem's effectiveness appears limited to isotropic architectures, with minimal improvements on ConvNeXt models.
2. There is insufficient analysis explaining why ConvStem performs variably across architectures and the rationale behind the proposed techniques.
3. Marginal improvements in $\ell_\infty$ robustness in some cases raise questions about the generalizability of results.
4. Concerns regarding the applicability of ConvStem to other AT methods beyond PGD, with no evidence provided for its effectiveness in those contexts.
5. The paper lacks discussions on the robustness of ConvStem against unseen threat models and comparisons with related works.
6. The generalizability of ConvStem to state-of-the-art AT methods remains unclear.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of why ConvStem works well on isotropic architectures and less effectively on others. Additionally, we suggest providing a comprehensive discussion on the implications of pre-training and heavy data augmentation. It would be beneficial to explore the performance of ConvStem with alternative adversarial training methods, such as N-FGSM, to assess its generalizability. We also encourage the authors to conduct further experiments to train a ConvNeXt-B model under the same conditions as ConvNeXt-B+ConvStem to isolate the effects of ConvStem. Finally, we urge the authors to include comparisons against defenses focused on unseen attacks to strengthen their claims about robustness improvements and to clarify the experimental settings and results, particularly regarding the robustness improvements and the impact of different data augmentation strategies.