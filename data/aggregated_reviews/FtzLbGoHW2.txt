ID: FtzLbGoHW2
Title: Improving Gloss-free Sign Language Translation by Reducing Representation Density
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 5, 4, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on gloss-free sign language translation (SLT), focusing on the "representation density problem," where semantically distinct signs cluster closely in feature space, complicating differentiation. The authors propose SignCL, a contrastive learning strategy aimed at enhancing discriminative feature representations in a self-supervised manner. The results indicate significant improvements in BLEU scores across various translation frameworks, particularly on the CSL-Daily dataset, and demonstrate that SignCL enhances the representational capacity of visual models, achieving better performance on the PHOENIX-2014T and CSL-Daily datasets compared to existing methods like Sign2GPT and GFSLT-VLP. The authors emphasize that their systematic experiments validate the importance of improving the discriminative power of visual representations for better translation outcomes.

### Strengths and Weaknesses
Strengths:  
- The paper identifies a novel problem in gloss-free SLT and proposes an innovative solution, SignCL, which shows substantial improvements in performance without additional model parameters.  
- Thorough experiments validate the existence of the representation density problem and the effectiveness of the proposed method across multiple datasets.  
- The authors effectively demonstrate that SignCL enhances the representational capacity of visual models, contributing to improved translation performance.  
- The work is well-structured and clearly presented, facilitating comprehension.

Weaknesses:  
- The technical novelty is somewhat limited, as the contrastive loss has been widely utilized in other domains, and the proposed method is only applied to a single model.  
- Presenting the representation density problem as a core contribution lacks innovation, as it is a common goal in the field.  
- The sampling strategy for SignCL makes assumptions that may not hold in real-world scenarios, potentially leading to incorrect sample classifications.  
- The proposed contrastive loss function shows inconsistent performance improvements across datasets, raising concerns about the generalization ability of SignCL.  
- The reliance on manually defined parameters and thresholds lacks rigorous justification, and the paper does not adequately address the sensitivity of the method to these parameters.

### Suggestions for Improvement
We recommend that the authors improve the justification for the sampling strategy used in SignCL, particularly regarding the assumptions made about adjacent frames. A comprehensive sensitivity analysis of the manually defined parameters would strengthen the scientific basis of the proposed method. Additionally, we suggest that the authors conduct experiments on a broader range of SLT models to validate the effectiveness of SignCL. Clarifying the rationale behind the grouping of different methods in Section 2.2 and addressing the inconsistencies in performance metrics across datasets would enhance the paper's clarity and impact. Furthermore, providing a more distinct perspective on the representation density problem and conducting more in-depth research on the varying performance improvements of SignCL across different benchmarks would enhance the persuasiveness of their method. It may also be beneficial to clarify the reasons behind the performance discrepancies on PHOENIX-2014T and CSL-Daily, considering the linguistic differences between the datasets.