ID: PfpAQuyZCB
Title: Behavior Alignment via Reward Function Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 6, 8, 8, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework that employs a bi-level optimization procedure to combine sparse primary rewards with auxiliary rewards designed by human heuristics. The upper level optimizes a behavior alignment reward to maximize primary rewards while minimizing the discount value, ensuring long-horizon information is encoded. The lower level can utilize any policy searching algorithm. The authors provide both intuitive and theoretical analyses demonstrating the robustness of their approach against misspecified auxiliary rewards and the limitations of policy optimization algorithms. They also introduce two empirical methods to tackle the bi-level problem: Implicit Bi-level optimization and Path-wise Bi-level optimization, and conduct experiments on Mujoco tasks to showcase scalability in high-dimensional continuous control.

### Strengths and Weaknesses
Strengths:
1. The paper effectively motivates the significance of the proposed work and critiques previous methods like potential-based approaches for their higher variance and reliance on state-based potential functions.
2. A thorough analysis of the bi-level objective is provided, aiding reader comprehension of the main ideas.
3. The analysis in Section 3.1 is novel, addressing both auxiliary reward imperfections and policy optimization algorithm flaws.

Weaknesses:
1. The practical algorithms appear overly complex for application in higher-dimensional tasks, such as vision-based tasks.
2. The introduction of numerous hyperparameters, as shown in Table 2.3.4.5, complicates tuning; for example, $\alpha_\theta$ is set to 0.015625, which may require careful adjustment.
3. Experimental results lack convincing evidence; for instance, Barfi only succeeds in 1 out of 4 tasks in Figure 3 and does not outperform baselines in Figure 2.5.
4. The rationale behind Barfi's required warm-up stage and its performance without it is unclear.
5. The empirical analysis is difficult to follow; simplifying large paragraphs and focusing on in-depth analysis would enhance clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Figure 1 by visualizing the principle of how the proposed method alters the gradient direction and corrects the policy optimization process. Additionally, please specify the advantages of Barfi over other methods in the experimental results, and consider comparing it with a broader range of reward shaping methods. Including visualized results of agent behavior with the proposed method would further support its effectiveness. Lastly, we suggest representing the theoretical description of the method with a flowchart to enhance comprehension.