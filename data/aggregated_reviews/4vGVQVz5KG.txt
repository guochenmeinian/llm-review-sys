ID: 4vGVQVz5KG
Title: Unsupervised Behavior Extraction via Random Intent Priors
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 6, 7, 6, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents UBER, a method for learning behavior policies from offline experience data without reward labels, and adapting these behaviors in an online setting. UBER generates multiple reward models and trains a policy on offline data using each model. During online adaptation, it jointly learns an action-value function and policy, selecting from pre-trained policies to collect data. The authors demonstrate that UBER performs comparably or better than existing algorithms in various simulated offline-to-online scenarios.

### Strengths and Weaknesses
Strengths:
- The paper addresses the significant problem of offline-to-online reinforcement learning and proposes a straightforward yet intriguing approach for pre-training behavior policies.
- Empirical results indicate that UBER outperforms existing methods in online adaptation after pre-training on offline data.
- The experiments encompass a variety of environments, showcasing the method's applicability.

Weaknesses:
- The theoretical interpretations appear overly optimistic, particularly regarding the suboptimality bound in Eq. 8, which seems excessively loose and requires a large number of samples for modest state spaces.
- The condition in Proposition 4.3 regarding the infinity norm of reward function differences is quite strong, and there is no experimental evidence provided to support its validity in practice.
- The paper lacks sufficient baseline comparisons, such as with AWAC, to substantiate the effectiveness of UBER.
- Clarity issues arise in the presentation, particularly concerning key related work and experimental setups, with figure captions often being too brief and uninformative.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for the use of random reward functions, particularly addressing how many are necessary to adequately cover the reward function space. Additionally, we suggest including more baseline comparisons, specifically with methods like AWAC, to better demonstrate UBER's effectiveness. Clarifying the experimental setup and providing more informative figure captions would enhance the paper's readability. Lastly, discussing the limitations of the proposed method, especially regarding its performance in capturing unseen behaviors, would provide a more comprehensive understanding of its applicability.