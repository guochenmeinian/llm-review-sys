ID: 8zQ77tPTMR
Title: Consistency is Key: On Data-Efficient Modality Transfer in Speech Translation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the effectiveness of progressive training for end-to-end speech translation (E2E ST), revealing its ineffectiveness despite its common use. The authors identify the learning-forgetting trade-off as a critical issue and propose a novel method that combines knowledge distillation (KD) and consistency learning (CL) to mitigate catastrophic forgetting. They further enhance this approach with a consistency-informed KD (cKD) method that utilizes token-level consistency. The experiments conducted on the MuST-C datasets demonstrate the efficacy of their proposed methods.

### Strengths and Weaknesses
Strengths:
- The paper thoroughly evaluates the common practice of progressive training, providing important empirical findings regarding its ineffectiveness.
- The combination of CL and KD offers a valuable data-efficient training approach without requiring additional training data.
- The introduction of the cKD method represents a significant modeling contribution by leveraging intra-modal consistency for improved inter-modal transfer.

Weaknesses:
- The motivation for introducing CL is unclear, and some claims lack supporting evidence, particularly regarding the interference between MT and ST.
- The improvements from the cKD method are limited, and the necessity of KD in the context of the observed improvements is questionable.
- The influence of domain problems on the conclusions drawn is not adequately explored, and the contribution of the proposed methods appears limited compared to existing approaches.

### Suggestions for Improvement
We recommend that the authors clarify the motivation behind the introduction of CL and provide additional evidence to support their claims regarding the interference between MT and ST. It would be beneficial to include more results from various language pairs to strengthen the assertion of progressive training's ineffectiveness. We also suggest that the authors improve the presentation of their results, ensuring proper alignment in tables and correcting any typographical errors. Additionally, a comparison of training times between the baseline and proposed methods would enhance the discussion on training efficiency.