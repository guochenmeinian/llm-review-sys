ID: HbIBqn3grD
Title: Structured flexibility in recurrent neural networks via neuromodulation
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 7, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel RNN architecture, the neuromodulated RNN (NM-RNN), which incorporates synaptic modulation inspired by neuromodulatory factors in the brain. The authors propose a circuit-level mechanistic model for how modulatory signals can reconfigure the dynamics of a recurrent network to perform various tasks. They demonstrate that the NM-RNN can linearly influence the connectivity matrix of a low-rank RNN by scaling it with outputs from a smaller RNN, effectively mimicking neuromodulation. The NM-RNN outperforms classical RNNs on tasks requiring timing and history dependence, supports multitask learning, and shows flexibility compared to LSTMs on a digit recall task. The paper emphasizes the utility of RNNs in modeling neural computation, particularly in relation to working memory and prefrontal cortex dynamics, and discusses the theoretical implications of the architecture in relation to LSTMs and synaptic scaling.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with clear motivation and a well-executed concept of dynamically scaling RNN weights.
- The theoretical analysis linking NM-RNNs to LSTMs is novel and sound, providing insights into gating mechanisms.
- The authors conduct thorough numerical experiments demonstrating the NM-RNN's capabilities across multiple tasks.
- The paper effectively highlights the role of RNNs as simple and minimal models for understanding neural computations, particularly in the context of multi-task flexibility.
- It provides a clear distinction between mechanistic modeling and predictive modeling, contributing to the discourse on computational neuroscience.

Weaknesses:
- The paper's impact is primarily theoretical, lacking clear evidence that NM-RNNs outperform LSTMs in absolute performance.
- The motivation and terminology regarding neuromodulation are not sufficiently clear, which may confuse readers about the architecture's biological relevance.
- The model's scalability and computational implications are not well-studied, and comparisons with existing neuromodulation-infused RNNs are missing.
- The paper does not sufficiently address the limitations of using RNNs compared to other architectures, such as transformers, in certain contexts.
- There is a lack of engagement with recent works that utilize transformers for cognitive modeling, which may limit the scope of the discussion.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation and terminology related to neuromodulation, explicitly linking it to the proposed architecture. It would be beneficial to provide a more detailed discussion of the computational implications and trainability of the NM-RNN compared to LSTMs. Additionally, we suggest including comparisons with existing neuromodulation models and evaluating the NM-RNN on more complex tasks to better demonstrate its scalability and effectiveness. Addressing the performance plateau in relation to LSTMs could enhance the mechanistic understanding of the architecture. Furthermore, we recommend that the authors improve the discussion on the limitations of RNNs and consider the potential contributions of transformer architectures in cognitive modeling. Engaging more directly with recent literature on transformer-based models could enhance the paper's relevance and comprehensiveness in the field of computational neuroscience.