ID: q7TxGUWlhD
Title: N-agent Ad Hoc Teamwork
Conference: NeurIPS
Year: 2024
Number of Reviews: 25
Original Ratings: 5, 7, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel problem setting within cooperative multi-agent systems, termed N-agent ad hoc teamwork (NAHT), where multiple autonomous agents cooperate with uncontrolled teammates to achieve a common goal. The authors propose the Policy Optimization with Agent Modeling (POAM) algorithm, which employs a policy gradient approach alongside agent modeling to adapt to diverse teammate behaviors. The effectiveness of POAM is demonstrated through empirical evaluations in multi-agent particle environments and StarCraft II tasks, showing improved performance over baseline approaches and better generalization to unseen teammates. Additionally, the authors clarify the theoretical underpinnings of their approach, particularly regarding the relationship between action probabilities and action loss, and provide corrections to experimental figures while elaborating on the use of pre-trained agents in the MPE-PP setting.

### Strengths and Weaknesses
Strengths:  
- The introduction of the NAHT domain is a significant contribution to multi-agent reinforcement learning (MARL), particularly addressing scenarios with more than two players, which are often overlooked.  
- The methodology is well-structured, with comprehensive ablation studies justifying design decisions in the POAM algorithm.  
- The paper is well-written, with a clear flow from problem definition to solution proposal and empirical validation.  
- The authors effectively clarify the theoretical relationship between action probabilities and action loss, enhancing the understanding of their algorithm.  
- Corrections to experimental figures demonstrate responsiveness to reviewer feedback, maintaining the integrity of the results.  
- The paper includes comprehensive analyses of uncontrolled agent performance, contextualizing the proposed algorithm.  

Weaknesses:  
- The solution lacks innovation, as the encoder-decoder architecture in the Agent Modeling Network is common in opponent modeling.  
- Some experimental results contained errors, raising concerns about the overall clarity and completeness of the work.  
- The empirical results are good, but the paper would benefit from a more thorough theoretical analysis of the POAM algorithm's convergence properties and performance guarantees.  
- The performance of the naive MARL baseline appears lower than expected, which may affect the perceived effectiveness of the proposed algorithm.  
- The scalability of the POAM algorithm concerning the number of agents and task complexity is not adequately addressed.  
- The out-of-distribution generalization results are unsatisfactory, raising concerns about the representativeness of the evaluation when paired with humans or new conventions.  
- The need for substantial revisions to incorporate discussions from the rebuttal into the main paper could hinder acceptance.  

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of the POAM algorithm's convergence properties and performance guarantees. Additionally, it would be beneficial to include more extensive experiments or discussions on how the algorithm performs as the number of agents and task complexity increase. We suggest addressing the rationale for using data from uncontrolled agents in training, as this aspect is not well justified. Furthermore, we encourage the authors to explore more convention-dependent evaluation scenarios, such as Hanabi, to provide a more informative assessment of the technique's capabilities. Lastly, we advise refining the related works section to include more comprehensive references on agent/opponent modeling and to ensure that discussions from the rebuttal are incorporated into the main text to enhance overall coherence and depth.