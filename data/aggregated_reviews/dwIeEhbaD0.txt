ID: dwIeEhbaD0
Title: SmoothHess: ReLU Network Feature Interactions via Stein's Lemma
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 8, 6, 6, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 2, 2, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SmoothHess, a method to compute the Hessian of ReLU networks by convolving the network's output with a Gaussian distribution. This approach addresses the challenge of zero Hessians in piecewise linear networks, allowing for the estimation of feature interactions. The authors propose using SmoothHess as a second-order extension of SmoothGrad for estimating feature interactions while simultaneously calculating first-order feature importance values through SmoothGrad, with minimal additional computational cost. The authors demonstrate that SmoothHess can be estimated using only gradient calls and provide non-asymptotic sample complexity bounds. Experimental results indicate that SmoothHess outperforms existing methods, such as SoftPlus Hessian, in tasks involving feature interactions and adversarial attacks across several datasets. The paper includes a detailed discussion on the behavior of the Hessian under Gaussian distributions and clarifies the terminology used for second-order interactions.

### Strengths and Weaknesses
Strengths:
- The proposed method is novel and effectively addresses the limitations of existing techniques for estimating feature interactions in ReLU networks.
- The theoretical derivations are well-organized and easy to follow, enhancing the paper's clarity.
- The integration of SmoothGrad and SmoothHess allows for a comprehensive analysis of feature importance and interactions.
- The authors provide clear explanations and justifications for their methodology, particularly regarding the behavior of the Hessian and its implications for feature interactions.
- SmoothHess can be applied post-hoc, allowing for analysis without modifying the network architecture.
- The authors are responsive to feedback, indicating a willingness to improve clarity and precision in their work.

Weaknesses:
- The scope of experiments is limited to small-scale datasets and models, raising questions about the method's performance on larger, more complex datasets.
- The qualitative analysis lacks depth, particularly in the medical domain, making it difficult to assess the significance of results related to interpretability.
- The paper does not adequately explore the impact of hyperparameters, such as the covariance matrix, on the method's effectiveness.
- Some technical terms and concepts may require further clarification for readers unfamiliar with the underlying mathematics.
- The paper lacks explicit incorporation of covariance into the bounds, which is noted as a limitation for future work.

### Suggestions for Improvement
We recommend that the authors improve the comprehensiveness of their experiments by including larger datasets and models to demonstrate the scalability of SmoothHess. Additionally, providing a qualitative comparison on more familiar datasets would help clarify the method's interpretability benefits. We suggest that the authors explore the trade-offs associated with the smoothing parameters and discuss their implications on the method's performance. Furthermore, we recommend that the authors improve the clarity of technical terms and concepts to enhance accessibility for a broader audience. We also suggest that the authors consider addressing the explicit incorporation of covariance into their bounds in future work. Finally, we encourage the authors to revise the figure captions to ensure that the representation of SmoothGrad and SmoothHess attributions is clear and unambiguous.