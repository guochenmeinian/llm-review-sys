ID: 0zFVhMBZHJ
Title: Mixture of Tokens: Continuous MoE through Cross-Example Aggregation
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel architecture called Mixture of Tokens (MoT), which aims to address training instabilities and inefficiencies associated with traditional Mixture of Experts (MoE) models. MoT aggregates tokens from groups created by subsetting examples across the batch dimension, using a router to produce aggregation weights. The authors validate MoT's effectiveness through experiments on autoregressive language modeling tasks, demonstrating improved training stability and performance compared to both dense transformers and sparse MoEs.

### Strengths and Weaknesses
Strengths:  
- The paper introduces an interesting and novel extension of soft MoE to autoregressive problems, maintaining the autoregressive property while mixing tokens across examples.  
- The writing is clear and well-organized, making the concepts easy to grasp.  
- The extensive evaluation shows significant improvements in training speed and stability, with a 3x speedup compared to common MoE architectures.  

Weaknesses:  
- There is a lack of intuitive explanation regarding the modeling rationale for mixing tokens across examples, raising questions about its impact on generalization, particularly in unrelated downstream tasks.  
- The experimental evaluation may be insufficient, as it primarily focuses on autoregressive tasks without exploring other domains or providing comparisons with additional MoE approaches.  
- The necessity for batched inference could limit the method's applicability, and the relationship between examples in training batches is not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the intuitive explanation of why mixing tokens across examples is beneficial from a modeling perspective. Additionally, we suggest conducting a generalization analysis to assess the impact of MoT on downstream task adaptation. It would also be valuable to include comparisons with other MoE methods beyond those currently reported, and to clarify the implications of batched inference on the method's applicability. Finally, addressing the computational cost in relation to existing routing algorithms would strengthen the paper's contributions.