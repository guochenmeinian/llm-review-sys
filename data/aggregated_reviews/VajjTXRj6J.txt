ID: VajjTXRj6J
Title: Robust Preference Optimization through Reward Model Distillation
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a reward model distillation approach to address the limitations of DPO in language model alignment. The authors propose methods for distilling reward differences and a pessimistic variant to improve robustness and performance, particularly in biased preference datasets. They provide formal results on the limitations of DPO and demonstrate the effectiveness of their approach through empirical results.

### Strengths and Weaknesses
Strengths:  
- The analysis extends existing results in the IPO paper, providing a well-structured and clear examination of DPO's degeneration issues.  
- The proposed reward distillation approach is simple, intuitive, and shows good empirical results, potentially improving upon traditional DPO methods.  
- The theoretical contributions regarding the equivalence of pessimistic formulations are noteworthy.

Weaknesses:  
- The evaluation lacks comparisons against online methods like PPO, raising questions about the distilled DPO variants' performance and complexity trade-offs.  
- The results are limited, and the paper should demonstrate how the proposed approach mitigates issues like OOD extrapolation.  
- The gamma parameter's sensitivity during training suggests a need for further ablation studies.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including comparisons with online alignment methods such as PPO to clarify the advantages of their approach. Additionally, demonstrating how their method addresses OOD extrapolation through log-probability margins would strengthen their claims. We also suggest conducting ablation studies on the gamma parameter to discuss its impact on the results. Finally, consider expanding the applicability of the method by exploring its use in online regimes, as suggested in the reviews.