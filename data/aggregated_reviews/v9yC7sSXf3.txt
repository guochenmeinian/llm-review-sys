ID: v9yC7sSXf3
Title: Deep Neural Collapse Is Provably Optimal for the Deep Unconstrained Features Model
Conference: NeurIPS
Year: 2023
Number of Reviews: 5
Original Ratings: 7, 7, 6, 6, -1
Original Confidences: 4, 4, 4, 5, -1

Aggregated Review:
### Key Points
This paper presents an extension to the standard unconstrained features model in neural collapse theory by introducing the deep unconstrained features model (DUFM) for multi-layer deep neural networks, specifically focusing on binary classification. The authors demonstrate that under optimal conditions, the global optimum exhibits properties of deep neural collapse (DNC) across multiple layers. The theoretical framework is supported by empirical experiments that align with the predictions of the DUFM.

### Strengths and Weaknesses
Strengths:
- The paper provides a strong technical analysis and a clear theoretical framework, effectively generalizing the unconstrained features model to multiple non-linear layers.
- Extensive empirical validation shows that the optimized DUFM aligns well with theoretical predictions, reinforcing the credibility of the findings.
- The writing is clear and the logical flow is easy to follow, contributing to a better understanding of deep neural collapse in binary classification.

Weaknesses:
- The rationale for asserting that SGD converges to a solution satisfying neural collapse based solely on optimality is unclear and requires further clarification.
- The analysis is limited to binary classification without bias, which restricts its applicability to multi-class settings and real-world scenarios.
- The paper lacks extensive experiments on various datasets and network architectures, particularly in multiclass contexts, which would enhance the validation of DNC.
- There are some typos and inaccuracies in the proofs that need correction.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the rationale behind SGD convergence to neural collapse solutions, particularly regarding optimality. Expanding the discussion on the feasibility of extending the analysis to multiclass settings would be valuable. Additionally, we suggest that the authors include more extensive experiments on different datasets and network architectures to validate the findings further. Finally, addressing the identified typos and inaccuracies in the proofs will enhance the overall quality of the paper.