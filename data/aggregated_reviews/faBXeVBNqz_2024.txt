ID: faBXeVBNqz
Title: Higher-Rank Irreducible Cartesian Tensors for Equivariant Message Passing
Conference: NeurIPS
Year: 2024
Number of Reviews: 22
Original Ratings: 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the use of higher-rank irreducible Cartesian tensors as an alternative to spherical tensors for equivariant message passing in machine learning interatomic potentials. The authors lay out the formalism for constructing these tensors and their products, proving their equivariance properties, and empirically evaluating the approach on various molecular datasets. The authors aim to demonstrate that their method is competitive with state-of-the-art models in terms of accuracy and computational efficiency. Additionally, the paper provides a comprehensive evaluation of various machine learning models, including ICTP and MACE, across multiple subsystems with a focus on inference time and memory consumption. The authors report results based on independent runs, highlighting best performances in bold, with inference times measured per atom or structure, depending on the batch size.

### Strengths and Weaknesses
Strengths:  
- The mathematical foundations are clearly illustrated, particularly in constructing irreducible Cartesian tensors and computing their products.  
- The exploration of higher-rank Cartesian approaches is novel and contrasts with the conventional spherical basis.  
- The empirical evaluation, especially on out-of-domain extrapolation datasets, provides valuable insights into the generalization capabilities of the proposed method.  
- The model achieves competitive performance with state-of-the-art spherical tensor models, demonstrating satisfactory accuracy.  
- The authors provide extensive results across different configurations and subsystems, demonstrating the performance of their models.  
- They address reviewer concerns effectively, enhancing the clarity and depth of their work.

Weaknesses:  
- The motivation for choosing Cartesian tensors over existing methods, such as MACE and TensorNet, is not sufficiently justified, leaving the actual value of the work unclear.  
- Empirical evaluations are limited to relatively simple datasets, lacking experiments on more challenging or heterogeneous datasets.  
- The efficiency gains and performance improvements are not compellingly articulated, appearing as a mere substitution of mathematical approaches without strong theoretical justification.  
- The authors do not adequately discuss the disadvantages of existing Cartesian-Tensor-Product-based approaches, which is essential for supporting their motivation.  
- Some reviewers express concerns regarding the significance of performance improvements and the need for stronger baselines or more standardized datasets.  
- There is a call for additional challenging datasets to further validate the models' effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the differences and advantages of their method compared to TensorNet and MACE, including a clearer comparison of computational complexities. Additionally, we suggest conducting experiments on larger-scale and more complex datasets to validate the efficiency claims. The authors should also provide a more thorough explanation of the fundamental differences between Cartesian and spherical tensors, particularly regarding representation power and the observed performance gains. Enhancing the motivation section by addressing the limitations of existing approaches would strengthen the paper's overall argument. Furthermore, we recommend improving the robustness of their evaluation by including stronger baselines, such as Equiformer V2, and more standardized datasets like MD22 or OCP. Clarifying the training methodology for the Ta-V-Cr-W systems and addressing the handling of heterogeneity in the dataset would also strengthen the paper.