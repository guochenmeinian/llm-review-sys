ID: Q93hLxLKLB
Title: We Need to Talk About Reproducibility in NLP Model Comparison
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for dataset splitting aimed at enhancing the reproducibility of results in NLP model comparisons. The authors relate reproducibility to a 'signal-to-noise' ratio and advocate for a blocked 3x2 cross-validation (BCV) strategy, introducing additional estimators: a majority vote estimator and a mixture estimator. The effectiveness of their approach is demonstrated through experiments on three NLP tasks.

### Strengths and Weaknesses
Strengths:
- The paper addresses the critical issue of reproducibility in NLP.
- The proposed 3x2 BCV method is well-motivated and shows promising results.
- The organization and comprehensiveness of the experiments are commendable.

Weaknesses:
- Concerns exist regarding the evaluation methodology, as it appears to optimize for a specific outcome rather than objectively comparing models.
- The theoretical justification for using the signal-to-noise ratio in assessing reproducibility is unclear.
- Some observations about the 3x2 BCV have been previously noted in the literature.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the theoretical justification for the signal-to-noise ratio as a measure of reproducibility. Additionally, we suggest that the authors consider exploring alternative splitting strategies beyond the 3x2 BCV, as well as providing a comparison with mx2 BCV for various m. It would also be beneficial to include the detailed explanations regarding the experimental setup in the main text or appendix to enhance reader understanding. Lastly, we advise separating the Related Work section from the Introduction for better clarity.