ID: K8gLHZIgVW
Title: Regularization properties of adversarially-trained linear regression
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 8, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of adversarial training (AT) in linear regression, exploring its relationship with regularization methods such as minimum-norm interpolation, LASSO, and ridge regression. The authors identify three modes of behavior based on the perturbation radius: small perturbations yield minimum-norm interpolators (Theorem 1), medium perturbations relate to parameter shrinkage regression (Proposition 4), and large perturbations necessitate the zero solution (Proposition 3). Theoretical findings are supported by numerical simulations, demonstrating the advantages of adversarial training over traditional methods.

### Strengths and Weaknesses
Strengths:
- The paper extends the theory of robust optimization and regularization, characterizing the relationship between perturbation radius and modes effectively.
- It demonstrates the benefits of overparametrization, showing that robustness increases with the model's complexity.
- The writing is clear and the theoretical framework is well-structured.

Weaknesses:
- The analysis is limited to linear models, which restricts the applicability of the findings to more commonly used neural networks.
- The theoretical claims, particularly regarding the uniqueness of solutions and their implications for adversarial training, are considered weak by some reviewers.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of their analysis by addressing the applicability of their findings to neural networks, potentially extending the analysis to two-layer neural networks. Additionally, we suggest clarifying the relationship between adversarial training and parameter shrinkage methods, particularly in Proposition 4, to resolve ambiguities regarding the equivalence of these frameworks. Furthermore, we encourage the authors to provide more details on the implementation of adversarial training in their numerical experiments, including the strength of the attacks used. Lastly, we advise addressing the theoretical concerns raised about the uniqueness of solutions in Theorem 1 and Proposition 2, ensuring that the characterization of adversarial training is precise and robust.