ID: prIwYTU9PV
Title: Distributional Pareto-Optimal Multi-Objective Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 8, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to Multi-Objective Reinforcement Learning (MORL) by introducing Distributional Pareto-Optimality (DPO), which extends traditional Pareto-optimality to account for distributional preferences over returns. The authors propose the DPMORL algorithm, which learns policies based on both return distributions and their expectations, utilizing stochastic dominance to capture the optimality of multivariate distributions. Experimental results demonstrate that DPMORL effectively learns distributional Pareto-optimal policies, outperforming existing MORL methods across various benchmarks.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant gap in MORL research by emphasizing the importance of distributional properties of returns, enhancing the representation of user preferences.
- DPMORL is noted for its flexibility in accommodating a wide range of distributional preferences, supported by a robust theoretical foundation.
- The clear delineation of Distributional Pareto-Optimal policies and their relationship with utility functions adds to the theoretical rigor, complemented by an intuitive case study.

Weaknesses:
- The arrangement of figures illustrating 2D utility functions lacks coherence and could benefit from a more systematic organization.
- Theorem 1's presentation differs between the main text and the appendix, introducing additional assumptions that may mislead readers regarding the uniqueness of optimal policies.
- The limitation of the proposed algorithm to non-decreasing utility functions and objectives bounded by $[0 ~ 1]^K$ restricts its applicability.
- The absence of proof regarding the coverage of the Pareto front by the set of optimal policies is a significant oversight.

### Suggestions for Improvement
We recommend that the authors improve the organization of figures depicting utility functions to follow a more logical pattern. Additionally, clarifying the constraints referenced in Table 2 with illustrative examples would enhance understanding. It would be beneficial to delve deeper into the distribution properties of preferences and constraints, providing specific limitations and practical implications of DPMORL. Addressing the discrepancies in Theorem 1 between the main paper and appendix is crucial for clarity. Lastly, including a limitations section to discuss relevant constraints and potential applications, such as in safety-aware autonomous driving, would strengthen the paper.