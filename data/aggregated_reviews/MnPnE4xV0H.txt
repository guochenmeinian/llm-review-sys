ID: MnPnE4xV0H
Title: Pretraining Language Models with Text-Attributed Heterogeneous Graphs
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new pre-training framework for language models tailored for Text-Attributed Heterogeneous Graphs (TAHGs). The authors propose leveraging both topological relationships and text semantics during pre-training, addressing challenges related to higher-order connections and imbalanced text information. The framework, THLM, employs an auxiliary heterogeneous graph neural network and a text augmentation strategy, demonstrating its effectiveness through experiments on multiple datasets.

### Strengths and Weaknesses
Strengths:
- The integration of graph structural information with textual data is innovative and relevant, addressing the unique characteristics of TAHGs.
- The framework is applicable to various foundational GNN/LM models, showcasing versatility.
- Extensive experiments validate the proposed method, including comprehensive ablation analyses and hyperparameter studies.

Weaknesses:
- The novelty of the work is limited, as higher-order network information and imbalanced text handling have been previously explored.
- Results on the OAG-Venue dataset are notably absent, and comparisons with relevant baselines like GraphFormers and OAG-BERT are missing.
- The effectiveness of the text augmentation strategy is not convincingly demonstrated, and the learning of graph topology lacks clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how the heterogeneity of node types is addressed in the framework. Additionally, please include results from the OAG-Venue dataset and comparisons with GraphFormers and OAG-BERT to strengthen the experimental analysis. It would be beneficial to conduct significance tests and report p-values to substantiate the improvements of THLM over baselines. Furthermore, we suggest incorporating clarifications regarding the performance discrepancies observed in Tables 1, 2, and 3, as well as the differences with Heterformer, into the main text for better reader comprehension.