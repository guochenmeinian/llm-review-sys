ID: JMrIeKjTAe
Title: Generalised f-Mean Aggregation for Graph Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 24
Original Ratings: 7, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GenAgg, a generalized aggregation operator for message passing neural networks (MPNNs) and graph neural networks (GNNs) that parametrizes a function space encompassing all standard aggregators. The authors propose a novel aggregation method that allows each MPNN to learn the most suitable aggregator for various downstream tasks. GenAgg is based on an extension of the generalized f-mean operator, introducing parameters $\alpha$ and $\beta$ to relax constraints while maintaining symmetry. The authors demonstrate GenAgg's effectiveness through extensive experimentation, showing superior performance in both computer vision and graph tasks, although the performance improvements are marginal compared to traditional methods. They provide theoretical insights into its properties, including a generalized distributive property, and highlight that GenAgg can represent a larger class of aggregation functions with significantly fewer parameters than existing methods.

### Strengths and Weaknesses
Strengths:
- The extension of the generalized f-mean operator is a novel contribution, offering insights into the effectiveness of common aggregators in MPNNs.
- GenAgg serves as a framework for defining existing aggregation operators and facilitates the design of new ones.
- Theorem 3.1 allows for further analysis of common aggregators concerning their time and space complexity.
- GenAgg demonstrates the ability to represent a wide range of aggregation functions with fewer parameters, which is beneficial in scenarios where the default aggregator is not optimal.
- The authors provide a theoretical foundation for the importance of learnable aggregation functions in GNNs, particularly in preserving information during aggregation.
- Empirical results indicate that GenAgg outperforms other powerful MPNN aggregators.

Weaknesses:
- The relationship between parameters $\alpha$ and $\beta$ and their impact on downstream tasks is unclear, particularly across different domains like computer vision and graph data.
- The convergence analysis of the loss term in Equation 2 is lacking; it would be beneficial to discuss whether the observed convergence patterns in computer vision datasets are also present in GNN benchmark datasets.
- The performance improvements of GenAgg over baseline methods are minimal (less than 0.1%) in large-scale experiments, raising questions about its practical utility.
- The tradeoff regarding the additional computational cost of using GenAgg, which requires eight extra linear layers, is not addressed, potentially affecting inference speed.
- The paper does not compare GenAgg with more powerful baselines, limiting its practical application insights.
- The technical content is somewhat limited, and the figures and tables lack professionalism, with readability issues noted.
- The paper lacks a thorough analysis of the cost of efficiency, including the latency and the number of parameters required for GenAgg compared to simpler methods like sum aggregation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the relationship between parameters $\alpha$ and $\beta$ and their effects on different downstream tasks. Additionally, we suggest including a convergence analysis of the loss term defined in Equation 2 to determine if the convergence patterns observed in computer vision datasets are also applicable to GNN benchmarks. It would be beneficial to discuss the latency implications of using GenAgg due to the additional linear layers and to provide comparisons with more powerful baselines such as GraphSAGE and DeepGCNs. Furthermore, we advise enhancing the professionalism of figures and tables for better readability and including a more detailed discussion of the method's limitations, particularly regarding computational overhead. Lastly, we recommend that the authors address the marginal improvements of GenAgg more robustly, demonstrating its practical significance in various datasets and exploring the importance of aggregation functions in deeper GNN architectures.