ID: MZwFbA3DSF
Title: Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a memory-efficient method for building the attention module in Transformers, specifically through the Multiple Head Embedding (MHE) module. The authors propose a parameter-efficient approach that utilizes a single weight matrix for key, value, and query projections, achieving over 90% performance retention compared to the vanilla multi-head attention module. The results indicate strong performance in natural language understanding tasks, although the paper lacks experiments on decoder-only models and various natural language generation tasks.

### Strengths and Weaknesses
Strengths:
- The proposed MHE method significantly reduces memory usage in the attention sub-layer while maintaining high performance.
- The approach is simple, intuitive, and shows better parameter utilization compared to existing methods.

Weaknesses:
- The absence of experiments on decoder-only models and natural language generation tasks limits the applicability of the findings.
- Concerns about the baseline performance in machine translation experiments and the overall significance of memory efficiency in the context of the entire Transformer architecture.

### Suggestions for Improvement
We recommend that the authors improve the experimental scope by including decoder-only models and a broader range of natural language generation tasks, such as Text Summarization and Question-Answering, to validate the effectiveness of MHE. Additionally, conducting experiments to further verify the effectiveness of MHE in parameter utilization, particularly by reallocating saved parameters to other sub-layers, would strengthen the paper. Clarifying the memory saving rate of the proposed method and addressing the baseline performance in machine translation experiments would also enhance the paper's rigor.