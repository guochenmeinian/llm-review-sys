ID: 444LAH3MhG
Title: Boundary Matters: A Bi-Level Active Finetuning Method
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 6, 7, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Bi-Level Active Finetuning Framework (BiLAF), which optimizes sample selection in the pretraining-finetuning paradigm by combining global diversity and local decision uncertainty. The authors propose an unsupervised denoising technique to eliminate noisy samples and utilize a boundary score metric for iterative boundary sample selection. Additionally, the paper introduces a method that effectively identifies high-quality samples to enhance model training across various pre-training features and tasks, with a simple yet effective guideline for selecting boundary samples that improves flexibility and performance, particularly in long-tail problems. Extensive experiments validate that BiLAF outperforms existing methods across various datasets and tasks, demonstrating consistency between the decision boundaries of the K pseudo-classes and the true task categories, thus reinforcing the method's theoretical foundations.

### Strengths and Weaknesses
Strengths:
- The framework effectively integrates core and boundary sample selection, addressing limitations in existing active learning methods.
- The unsupervised denoising technique enhances the reliability of sample selection.
- The proposed guidelines for boundary sample selection are practical and enhance the model's flexibility.
- The paper is well-structured, with clear organization and thorough experimental validation.
- Empirical results validate the consistency of decision boundaries, reinforcing the method's theoretical foundations.

Weaknesses:
- The main diagram lacks clarity and detail, particularly in explaining the implementation of "Boundary Score Calculation" and "Iterative Selection and Removal."
- There is insufficient discussion on boundary sample selection for fine-grained categorization and the gap between active fine-tuning learning and the upper performance of fine-tuning with all samples.
- The theoretical foundation is weak, lacking a robust explanation for the effectiveness of "diversity and uncertainty" sampling in active fine-tuning methods.
- The method's reliance on pre-trained features raises concerns about its generalizability across different data sizes and tasks.
- The diminishing returns observed in the rate of return for core samples suggest that the benefits of additional samples may not be substantial beyond a certain point.
- The authors maintain default parameters across experiments, which may limit the exploration of optimal configurations for specific tasks.

### Suggestions for Improvement
We recommend that the authors improve the clarity and detail of the main diagram to better explain the implementation of key methods. Additionally, the authors should address the lack of discussion regarding boundary sample selection in fine-grained categorization and clarify the performance gap between active fine-tuning and full sample fine-tuning. Strengthening the theoretical foundation by providing a robust explanation for the proposed sampling methods would enhance the paper's credibility. Furthermore, we suggest including experiments on scenarios with limited data to investigate BiLAF's effectiveness under such conditions. We also recommend improving the exploration of parameter configurations beyond default settings to identify optimal setups for various tasks, and consider providing a more detailed analysis on the diminishing returns of core samples to better guide future research in sample selection strategies. Finally, a discussion on the implications of using pre-trained features and alternative fine-tuning methods would provide a more comprehensive understanding of the framework's applicability.