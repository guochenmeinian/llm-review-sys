ID: lCy3RwscMn
Title: Deep Natural Language Feature Learning for Interpretable Prediction
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for decomposing a main task into intermediary sub-tasks formulated as binary questions, enhancing the understanding of decision-making in deep learning models. The authors introduce a framework for interpretable classification that utilizes LLM to extract Binary Subtask Questions (BSQs) and fine-tunes a smaller BERT-like model to obtain Natural Language Learned Feature (NLLF). The proposed method is noted for its interpretability, low computational cost, and superior performance compared to state-of-the-art models on two binary classification tasks.

### Strengths and Weaknesses
Strengths:
- The idea of breaking down tasks into sub-tasks is innovative and could advance further research.
- The framework demonstrates promising experimental results, outperforming black-box models while maintaining fewer parameters and better explainability.
- Visualizations and diagrams are effectively expressive.

Weaknesses:
- The complexity of the proposed method may hinder its application in real-world scenarios, requiring high expertise and resources.
- Experiments are limited to two binary datasets, which may not adequately demonstrate the framework's effectiveness on more complex or high-stakes tasks.
- The paper lacks comparisons with well-known baseline models and does not sufficiently address the limitations regarding time complexity and computational resource demands.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by including well-known baseline models to validate their approach. Additionally, the authors should address the limitations related to the computational resources required for large datasets in the limitations section. We suggest exploring the use of filtering mechanisms in weakly labeled data to enhance performance and clarify the definitions and distinctions between interpretability and explainability in the introduction. Finally, we encourage the authors to conduct experiments on more complex tasks and provide comparisons with other explainable models to strengthen their claims.