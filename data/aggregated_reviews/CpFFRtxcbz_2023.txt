ID: CpFFRtxcbz
Title: CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: -1, 6, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: -1, 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CARE-MI, a benchmark dataset aimed at evaluating misinformation generated by large language models (LLMs) in the sensitive domain of maternity and infant care, specifically in Chinese. The dataset includes 1,612 expert-filtered questions, human-selected references, and employs a cost-effective data construction approach utilizing LLMs with human annotations. The authors propose a definition of misinformation that distinguishes it from other forms of false information, emphasizing that it arises unintentionally from LLM outputs. Extensive experiments reveal that existing Chinese LLMs perform sub-optimally in this context. The evaluation framework is designed to assess long-form text generation, allowing models to produce answers of varying lengths, which contrasts with previous work that utilized multiple-choice formats. The authors clarify their methodology regarding hyper-parameter settings, decoding methods, and the influence of retrieved knowledge on model performance.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel framework for constructing datasets tailored to knowledge-intensive topics, particularly in medical domains, which could be applicable to other datasets.
- The proposed dataset addresses Chinese misinformation about maternity and infant care, contributing significantly to low-resource languages.
- Comprehensive evaluations are conducted using both human and automated methods, ensuring thorough assessment.
- The authors provide a clear distinction between their work and existing benchmarks, highlighting the complexity of constructing datasets for specific knowledge-intensive domains.
- The evaluation framework is robust, allowing for the assessment of long-form answers, which is crucial for understanding misinformation in generated texts.
- The paper includes detailed explanations of the methodologies and hyper-parameters used, enhancing transparency.

Weaknesses:
- The dataset construction and evaluation details require further clarification, particularly regarding expert qualifications, prompt engineering, and the rationale behind the number of supporting evidences.
- The dataset may not adequately reflect the language and concerns of expectant and first-time parents, as it contains significant medical jargon.
- The paper lacks a limitations section in the main text, which is critical for evaluating the dataset's applicability.
- There is a noted mismatch between the dataset and the motivation for its creation, which remains a significant concern for some reviewers.
- The inter-annotator agreement scores for interpretability were lower, suggesting potential issues with the reliability of human evaluations in this area.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the dataset construction process by providing more information about the experts involved, including their qualifications. Additionally, the authors should elaborate on the prompt engineering process used in the question generation task. Clarifying how disagreements among annotators were resolved would enhance understanding of the annotation challenges. 

We suggest that the authors justify the choice of including at least three pieces of supporting evidence and discuss how this number may impact model performance. Providing descriptive statistics of the dataset, such as average length, would also be beneficial.

For the evaluation section, the authors should clarify the distribution of the 200 questions across the datasets mentioned and whether the prompts used for LLM evaluation were tuned for performance. Including the interpretability scores from human evaluations and explaining the selection of baseline models would strengthen the paper.

We recommend that the authors improve the clarity regarding the dataset's alignment with the stated motivations to address concerns about its relevance. Additionally, enhancing the evaluation of interpretability through more rigorous human assessments could strengthen the findings. Lastly, we recommend moving the limitations section to the main text and ensuring that all references are consistently cited. Addressing these points will enhance the overall clarity and impact of the paper.