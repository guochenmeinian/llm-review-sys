ID: WIHH0iOOUt
Title: OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models
Conference: AAAI
Year: 2024
Number of Reviews: 4
Original Ratings: 6, 6, 6, 7
Original Confidences: 5, 3, 3, 4

Aggregated Review:
### Key Points
This paper presents innovative prompting techniques that enhance the performance of the Yi-34B general language model over the clinically fine-tuned Meditron 70B across three out of four medical Q&A benchmarks: MedQA, MedMCQA, PubMedQA, and MMLU. The authors effectively demonstrate the practical utility of these techniques for open-source models, emphasizing the potential of prompt engineering as a cost-effective alternative to fine-tuning. The paper is well-written, providing clear background details and a comprehensive evaluation of the model's capabilities.

### Strengths and Weaknesses
Strengths:
- The authors effectively showcase how various prompting techniques outperform models fine-tuned on domain-specific data, benefiting the open-source community.
- The methodology, including few-shot prompting, chain-of-thought (CoT) prompting, and self-consistency, is clearly articulated, allowing for potential replication.
- The paper is well-structured and easy to follow, with informative figures that illustrate the results.

Weaknesses:
- The paper lacks an abstract and does not adhere to the AAAI-24 Author Kit formatting guidelines.
- Missing citations for papers referenced in Figure 1 and a lack of references for various prompting methods raise concerns about academic rigor.
- Figure 2 suffers from clarity issues due to color choices, and the authors should consider using different shades for better visibility.
- The evaluation of claims is limited, focusing exclusively on Meditron 70B as a baseline; including additional medical LLMs would provide a broader perspective.
- The authors do not release code for their OpenMedLM prompting platform, and the limitations of kNN FS CoT and other prompting strategies are insufficiently discussed.

### Suggestions for Improvement
We recommend that the authors improve the paper by including an abstract and adhering to the formatting guidelines specified in the AAAI-24 Author Kit. It is crucial to cite all referenced papers, particularly those related to Figure 1 and the prompting methods used. To enhance clarity, we suggest revising Figure 2 with alternative color shades. Additionally, we encourage the authors to broaden their evaluation by including more medical LLM baselines and to discuss the limitations of kNN FS CoT and other prompting strategies in greater detail. Finally, we recommend releasing the code for the OpenMedLM prompting platform to facilitate further research and replication.