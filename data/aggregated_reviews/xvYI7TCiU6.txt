ID: xvYI7TCiU6
Title: Measuring Mutual Policy Divergence for Multi-Agent Sequential Exploration
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 7, 7, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel multi-agent reinforcement learning (MARL) method called Multi-Agent Divergence Policy Optimization (MADPO), which enhances exploration and heterogeneity through a mutual policy divergence maximization framework. The authors propose using conditional Cauchy-Schwarz divergence to quantify discrepancies between episodes and agents, addressing the instability of traditional divergence measurements. Empirical validation shows that MADPO outperforms state-of-the-art sequential updating approaches in challenging multi-agent tasks.

### Strengths and Weaknesses
Strengths:
- The problem of exploration in heterogeneous agent settings is significant and underexplored in MARL.
- The introduction of MADPO and the use of conditional Cauchy-Schwarz divergence provide a theoretical foundation that addresses traditional divergence measurement issues.
- The method shows strong performance against established baselines in various benchmarks, and the framework is simple and easy to implement.
- The paper is well-written, with clear figures and a logical structure.

Weaknesses:
- The improvement over baselines in the ablation study is inconsistent, with high variance in results and only marginal gains over KL divergence in some tasks.
- The evaluation lacks statistical significance tests, making it difficult to assess the proposed method's superiority.
- The motivation for the necessity of specialized algorithms in sequential update settings is unclear.
- There is a lack of analysis comparing the proposed method with relevant literature on sequential decision-making.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind the need for specialized algorithms in sequential update settings. Additionally, we suggest incorporating statistical significance tests for the results presented in Figures 2 and 3 to validate the performance claims. It would also be beneficial to provide a detailed comparison between MADPO and existing methods, such as the one by Liu et al., to clarify how MADPO differs in handling heterogeneous sequential decision-making scenarios. Furthermore, including a table with the running times of the evaluated algorithms would help assess the computational cost of the CS divergence. Lastly, we encourage the authors to explore the adaptability of their method to homogeneous scenarios and clarify the rationale behind the weighting of intrinsic rewards.