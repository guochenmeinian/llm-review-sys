ID: 8FbuHeVU7D
Title: Differentially Private Statistical Inference through $\beta$-Divergence One Posterior Sampling
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 6, 5, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel differential privacy (DP) mechanism called \(\beta\)D-Bayes, which integrates the one-posterior sampling (OPS) technique with \(\beta\)-divergence to enhance parameter estimation across a variety of inference models. The authors argue that this approach mitigates the limitations of sensitivity bounding without constraining the feature space or statistical functionals. Empirical results demonstrate that \(\beta\)D-Bayes outperforms existing methods in DP logistic regression and neural network learning across multiple datasets.

### Strengths and Weaknesses
Strengths:  
- The paper is well-motivated, addressing significant challenges in differential privacy for Bayesian inference.  
- The theoretical analysis is meticulous, and Table 1 effectively highlights the novelty of the approach.  
- The empirical evaluation shows promising results, suggesting potential for broader applications in DP research.

Weaknesses:  
- Section 3 is overly dense and jargon-heavy, lacking intuitive explanations for key formulas, which may hinder understanding for those less familiar with Bayesian inference.  
- Figure 2 is difficult to interpret due to its complexity and color choices, making it hard to discern the proposed method's advantages.  
- The experimental design lacks clarity regarding the prediction goals, dataset information, and hyperparameter selection, which are crucial for replicability.  
- The limitations of using probabilities instead of log-probabilities, particularly regarding numerical stability and the implications of producing a single sample from the posterior, are insufficiently discussed.

### Suggestions for Improvement
We recommend that the authors improve the presentation of Section 3 by incorporating more intuitive explanations for the key formulas to enhance accessibility for readers unfamiliar with Bayesian inference. Additionally, we suggest revising Figure 2 for clarity, ensuring that colors and line styles are distinct and consistent. The authors should provide more detailed descriptions of the experiments, including the prediction goals, dataset characteristics, and hyperparameter selection processes. Furthermore, we encourage a discussion of the numerical stability implications of using probabilities in Bayesian updates and a clearer articulation of the method's computational demands and limitations.