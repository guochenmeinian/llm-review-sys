ID: V4tzn87DtN
Title: Stochastic Newton Proximal Extragradient Method
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 2, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an accelerated scheme for strongly convex problems, specifically the Stochastic Newton Proximal Extragradient (SNPE) method. The authors claim that their method achieves superlinear convergence after \(\mathcal{O}(\kappa)\) iterations, improving upon previous methods that required \(\mathcal{O}(\kappa^2)\) iterations. The approach combines Hessian averaging with a hybrid proximal extragradient framework, relying on a deterministic first-order oracle and an inexact Hessian estimator, assuming mean zero and sub-exponential Hessian noise.

### Strengths and Weaknesses
Strengths:  
- The paper is technically robust and clearly written, with impressive theoretical results that enhance the Hessian average method.  
- The incorporation of the NPE framework is innovative, and the results are easy to follow.

Weaknesses:  
- The practical relevance of the scheme is uncertain, as experiments utilize a non-accelerated version; running the accelerated version, even if it performs worse, is recommended for completeness.  
- The assumptions are overly restrictive, particularly assumptions 3 and 5, which may not be satisfied in many machine learning applications.  
- There is a lack of empirical results comparing the proposed method against existing methods like AGD, which is crucial to demonstrate the benefits of using second-order information.  
- The iteration complexity claims require clarification, as the dependence on \(\epsilon\) appears inconsistent with existing results.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation by including experiments with the accelerated version of their method. Additionally, providing a theoretical comparison with existing accelerated results using exact Hessians would enhance the paper's rigor. Clarifying the claims regarding condition number dependency and convergence guarantees when using \(x_{k+1}=\hat{x}_k\) is essential. We also suggest addressing the complexity of line-search in the algorithm and including a more detailed analysis of hyperparameter choices. Lastly, we encourage the authors to relax the requirement for exact gradients and explore the implications of using Pearlmutter's implicit Hessian-vector product for second-order updates.