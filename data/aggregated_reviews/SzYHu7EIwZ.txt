ID: SzYHu7EIwZ
Title: Precision-Recall Divergence Optimization for Generative Modeling with GANs and Normalizing Flows
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 3, 7, 6, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for training generative models that allows for a user-defined tradeoff between sample fidelity and variety by introducing a new divergence termed PR-divergence. The authors demonstrate that existing $f$-divergences can be reformulated in terms of PR-divergence, providing a theoretical basis for optimizing the precision-recall tradeoff. The authors argue that their method enhances recall compared to existing techniques, achieving state-of-the-art results on datasets such as MNIST, FashionMNIST, CelebA, FFHQ, and ImageNet. The experiments conducted include training BigGAN on CIFAR-10, CelebA64, and fine-tuning on ImageNet 128x128, showing that the proposed method can control the precision-recall tradeoff effectively. The authors also address concerns regarding batch size, noting that while larger sizes can improve FID scores, the relationship is complex and context-dependent, and they clarify their mathematical framework and the implications of using likelihood ratios in their proofs.

### Strengths and Weaknesses
Strengths:
- The paper offers a thorough analysis of $f$-divergences and establishes a solid theoretical foundation for PR-divergence.
- It contributes a unique method for explicitly controlling precision and recall in generative models.
- The authors provide insights into the optimization of precision-recall tradeoffs and propose a practical algorithm to train generative models using an auxiliary divergence.
- The clarity of presentation and soundness of the theoretical results are commendable, with no obvious errors found in the proofs.
- The authors demonstrate an understanding of the complexities involved in batch size and performance metrics through a comprehensive response to feedback.

Weaknesses:
- The empirical results are limited, with the proposed method lagging behind current state-of-the-art models in terms of precision and sample quality.
- The experimental validation lacks depth, particularly in demonstrating substantial gains in precision for high tradeoff values.
- The evaluation is limited to smaller datasets and resolutions, with calls for broader applicability to larger datasets and popular models like StyleGAN.
- The necessity of incorporating two discriminators is questioned, with suggestions that different divergences could be combined directly in the objective function.
- The practical advantages of the proposed method over post-processing techniques remain unclear, particularly regarding the efficiency of model training.
- The code for reproducibility is not provided, and the evaluation metrics used are limited to a single variant of precision-recall scores, which may not be robust in low-density regions.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation by providing FID and PR curves comparing their method with truncation and classifier-free guidance on various datasets. Additionally, clarify the discrepancies in FID results with existing literature and ensure that the experimental setup is consistent with standard practices, such as using a batch size of 64 for CIFAR-10. We suggest expanding the evaluation to include larger datasets and resolutions, as well as popular models like StyleGAN. Furthermore, we recommend improving clarity on the advantages of their method over existing techniques that combine different divergences directly and providing a more detailed explanation of the practical benefits of controlling the precision-recall tradeoff during training, particularly in comparison to post-processing methods. Finally, we encourage the authors to explore optimizing the Area Under the PR Curve (AUPRC) as an alternative to tuning the hyperparameter $\lambda$.