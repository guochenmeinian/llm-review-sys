ID: wiidCRA3at
Title: Stein $\Pi$-Importance Sampling
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 6, 6, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 2, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach for constructing an MCMC target distribution, denoted as $\Pi$, specifically designed for post-processing using Stein importance sampling and Stein thinning. The method involves tilting the original target density $p(x)$ with the square root of a Stein kernel $k_P(x)$, derived from a variational problem that minimizes the trace of the variance of a limiting Gaussian distribution. The authors justify the use of $\Pi$ for the Metropolis-Adjusted Langevin Algorithm (MALA) through an almost sure consistency guarantee (Theorem 1) and support their claims with numerical experiments on benchmark problems.

### Strengths and Weaknesses
Strengths:  
- **Originality**: The paper introduces a unique perspective on improving Stein importance sampling through the design of a target distribution distinct from the original target $P$.  
- **Quality**: The construction of $\Pi$ is clearly explained, with theoretical guarantees and extensive numerical evidence supporting its efficacy.  
- **Clarity**: The paper is well-structured, featuring a comprehensive review of related literature and methodologies.

Weaknesses:  
- **Motivation**: The rationale for employing S$\Pi$IS over running MALA without post-processing is weak. The authors should include comparisons with MALA without post-processing to address whether S$\Pi$IS is more beneficial.  
- **Practicality**: The requirement for an MCMC sampler targeting a distribution different from $P$ raises questions about the practicality of this approach compared to standard SIS, which targets $P$.  
- **Theoretical Guarantees**: The paper only provides asymptotic convergence in the $D_P$ metric, and a non-asymptotic guarantee would strengthen the contribution.

### Suggestions for Improvement
We recommend that the authors improve the justification for using S$\Pi$IS by including experiments that compare it directly with MALA without post-processing. Additionally, discussions addressing the practicality of setting up a sampler targeting $\Pi$ versus directly targeting $P$ would enhance the paper's relevance. Furthermore, providing a non-asymptotic convergence guarantee for the proposed method would significantly strengthen the theoretical foundation. Lastly, we suggest expanding the section on the construction of Stein's kernel and addressing the dimensionality effects on the performance of $\Pi$ compared to $P$.