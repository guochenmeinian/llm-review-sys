ID: fze7P9oy6l
Title: Supported Value Regularization for Offline Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the offline reinforcement learning (RL) problem and proposes the Support Value Regularization (SVR) to enhance Q-function learning, inspired by the value regularizations in Conservative Q-Learning (CQL). The authors implement SVR for out-of-distribution (OOD) samples while maintaining Bellman updates for in-distribution (ID) samples. Experimental results indicate that the SVR-regularized method outperforms competitors in D4RL tasks.

### Strengths and Weaknesses
Strengths:
1. The paper introduces Support Value Regularization (SVR) for offline RL, applying additional regularization on OOD points and utilizing importance sampling techniques for Q-value calculations in the OOD region.
2. Experimental studies demonstrate the superiority of the SVR method.
3. Theoretical proofs indicate that policy improvement can be achieved in each iteration.

Weaknesses:
1. The method's addition of regularization to OOD regions categorizes it as a Conservative offline RL method; comparisons with less/mild conservative methods are recommended.
2. As SVR is density-based, comparisons with other density-based offline RL methods should be included.
3. Some notations lack clarity, particularly the variable $\beta$ in Equation 3, which should be explained.
4. The Gaussian model used for behavior policy estimation may not provide a robust estimator due to high state dimensions; a more detailed analysis of SVR versus SVR-VAE is suggested.
5. The selection of the sampling distribution $u(a,s)$ as Gaussian warrants an ablation study to assess its impact on results.

### Suggestions for Improvement
We recommend that the authors improve clarity by providing explanations for unclear notations, particularly $\beta$ in Equation 3. Additionally, the authors should compare SVR with other less/mild conservative and density-based offline RL methods, as listed in the reviews. A more detailed analysis of the behavior policy estimation and its implications on the SVR's performance should be included, particularly regarding the Gaussian model's limitations. Furthermore, conducting an ablation study on the choice of the sampling distribution $u(a,s)$ would enhance understanding of its influence on results. Lastly, the authors should clarify the differences and advantages of SVR over existing methods, particularly in the context of importance sampling.