ID: VpkfxuVXwx
Title: PrivAuditor: Benchmarking Data Protection Vulnerabilities in LLM Adaptation Techniques
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 8, 10, 8, 6, -1
Original Confidences: 5, 5, 4, 4, -1

Aggregated Review:
### Key Points
This paper presents an experimental assessment of privacy risks during the fine-tuning of state-of-the-art large language models (LLMs), focusing on membership inference risks. It evaluates how identifiable tokens in the dataset used for finetuning can lead to the leakage of confidential information. The authors employ various likelihood metrics and conduct numerous experiments to assess the impact of adaptation techniques on inference risks across different datasets. The findings indicate that adaptation techniques significantly influence inference risk, with larger models being more susceptible to these risks. Additionally, the paper proposes a unified platform for measuring privacy risks during LLM adaptation.

### Strengths and Weaknesses
Strengths:
- Highly relevant and timely subject matter.
- Solid design and execution of experiments.
- Impressive number of structured experiments yielding significant insights.
- Clear and direct writing throughout the paper.

Weaknesses:
- Lack of details regarding dataset overlap between pre-training and fine-tuning.
- Insufficient discussion on the severity of privacy leakage and its implications.
- Some experimental results, such as in-context learning, are not adequately represented.

### Suggestions for Improvement
We recommend that the authors improve the discussion of dataset overlap, providing evidence that adaptation datasets do not overlap with pre-training datasets. Additionally, we suggest discussing the limitations of their current work and exploring how future research could extend these findings. The authors should also clarify the implications of their results regarding privacy leakage severity and ensure that all adaptation techniques, including in-context learning, are adequately represented in the results. Finally, consider streamlining Section 2.2 to focus on essential formalizations and address minor issues in figures and references.