ID: UPefaFqjNQ
Title: Brain encoding models based on multimodal transformers can transfer across language and vision
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 8, 2, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the predictive power of multimodal Transformer representations on fMRI brain measurements, utilizing the BridgeTower model to extract aligned representations of language and vision. The authors evaluate how well these representations can predict fMRI responses to narrative stories and movies, concluding that multimodal transformers yield better alignment of concepts across modalities. The study also includes a comprehensive examination of the relationship between fMRI signals and physiological processes in the brain, supported by extensive literature. The authors propose that fMRI data is validated through multiple neurophysiological measures, demonstrating reliable links to neuronal activities. Furthermore, they highlight that studies correlating deep neural network representations with fMRI signals have been replicated using other modalities, reinforcing the credibility of fMRI as a measure of brain activity.

### Strengths and Weaknesses
Strengths:
- The novelty of employing a multimodal Transformer for extracting aligned concept representations for fMRI responses is significant.
- The ability of encoding models trained on one modality to predict responses in another modality demonstrates the effectiveness of the approach.
- The paper provides clear experimental evaluations and contributes novel cognitive insights regarding the alignment of language and visual representations.
- The authors provide a robust review of literature supporting the validity of fMRI signals, establishing a strong foundation for their claims.
- The paper effectively addresses potential concerns regarding the reliability of fMRI data by citing numerous studies that corroborate the findings across different methodologies.
- The relevance of the work to the NeurIPs community is acknowledged, enhancing its impact.

Weaknesses:
- The narrative stories and movie clips used in experiments differ significantly, raising questions about the overlap of concepts across modalities.
- The study relies on a limited dataset of only five subjects, which may affect the robustness of the findings.
- The authors do not adequately address recent works on multimodal Transformers relevant to brain encoding, nor do they provide sufficient statistical rigor in presenting fMRI data.
- There are concerns regarding the potential for false negatives in identifying multimodal ROIs due to alignment issues between vision and language feature spaces, which could affect performance.

### Suggestions for Improvement
We recommend that the authors improve the overlap analysis of concepts between narrative stories and movie clips to strengthen their claims. It would be beneficial to utilize the same information for both listening and watching tasks, akin to previous studies. Additionally, incorporating captions for videos and comparing performance across modalities could provide further insights. We suggest that the authors include a more comprehensive discussion of the limitations related to their small sample size and consider expanding their dataset. Lastly, addressing the recent literature on multimodal Transformers for brain encoding would enhance the paper's relevance and depth. Furthermore, we recommend that the authors improve the discussion on the alignment between vision and language feature spaces to address potential concerns about false negatives in multimodal ROI identification, providing further clarification on how these alignment issues could impact the results.