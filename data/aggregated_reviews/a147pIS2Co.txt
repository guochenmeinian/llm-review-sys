ID: a147pIS2Co
Title: Training Chain-of-Thought via Latent-Variable Inference
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 6, 6, 7, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to enhance the training of "chain-of-thought" (CoT) prompting by treating rationales as latent variables and employing a Markov-chain Monte Carlo Expectation-Maximization (MCMC-EM) algorithm. The authors propose a control variate technique to reduce training variance while ensuring unbiasedness. Their methodology is validated on the BIG-Bench Hard dataset, demonstrating significant performance improvements over traditional prompt tuning and the STaR method.

### Strengths and Weaknesses
Strengths:
1. The paper introduces an innovative perspective on prompt-finetuning through CoT, presenting a principled and practical methodology.
2. The assumptions and derivations are clear and intuitive, with thorough exploration of connections to prior work and critical self-assessment of limitations.
3. The theoretical framework is well-supported by empirical validation on a significant benchmark.

Weaknesses:
1. The paper lacks a structured presentation, making it difficult to follow the main storyline amidst lengthy discussions.
2. Insufficient comparison with existing literature on Monte Carlo methods and classic gradient estimation techniques, leading to potential misleading statements.
3. Limited analysis of experimental results and a lack of detailed insights into the generated rationales and their correctness.

### Suggestions for Improvement
We recommend that the authors improve the structure of the paper by clearly delineating main ideas from discussions and enhancing the clarity of section titles. Additionally, we suggest including a set of equations that explicitly define the true gradients and Monte Carlo estimates to aid comprehension. Addressing the limitations of classic gradient estimation methods more thoroughly, particularly regarding posterior collapse and the use of the Importance Weighted Bound (IWB), would strengthen the paper. Finally, expanding the experimental section to include comparisons with IWB and providing insights into the variance of TRICE compared to other baselines would enhance the paper's impact.