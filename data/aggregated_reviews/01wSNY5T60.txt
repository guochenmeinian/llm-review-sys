ID: 01wSNY5T60
Title: Are Compressed Language Models Less Subgroup Robust?
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical study on how various compression techniques for Transformer-encoder models, specifically BERT and ROBERTA, influence classification robustness for minority classes and subgroups. The authors investigate 18 different compression methods across two datasets, MultiNLI and CivilComments, revealing that subgroup robustness is affected by both model size and compression method. 

### Strengths and Weaknesses
Strengths:
- The paper is well-written and relevant to the NLP community.
- It surveys a wide range of compression techniques, providing valuable insights for future research.
- The inclusion of a new dataset during the discussion period enhances the study's comprehensiveness.

Weaknesses:
- The paper lacks novel techniques or solutions, functioning primarily as a survey.
- It focuses solely on BERT-style models, missing opportunities to explore recent advancements in LLMs like LLAMA.
- The analysis of subgroup partitioning is unclear, and the definitions of key concepts and abbreviations are insufficiently detailed.
- There is a lack of deeper analysis regarding why certain models, such as TinyBERT, exhibit superior subgroup robustness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of subgroup definitions and provide detailed explanations of key concepts and abbreviations upon their first appearance. Additionally, we suggest expanding the dataset selection to include more standard classification datasets to strengthen the findings. A more thorough analysis of the performance of different compression methods, particularly regarding TinyBERT, is warranted, potentially through an ablation study. Finally, we encourage the authors to clarify the main conclusions drawn from each experiment, especially in relation to Figures 1 and 2.