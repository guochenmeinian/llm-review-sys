ID: hNSbSaD1WC
Title: The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a large experimental study on two compression techniques for large language models (LLMs): pruning and quantization. The authors focus on structured (L1) and unstructured (Lp) pruning across various modules, including attention, feed-forward, and final dense layers, and employ Post Training Dynamic Quantization (PTDQ) for the same. The study encompasses a range of LLM architectures, including encoder-only models (BERT, RoBERTa, DistilBERT, ALBERT), encoder-decoder models (Flan-T5, Lamini-Flan-T5), and decoder-only models (Vicuna-7B, WizardLM-7B). The authors report several findings, noting that pruning attention modules in encoder-decoder models often yields a stronger impact than pruning feed-forward modules, and that L1-unstructured pruning results in a smaller performance decrease for encoder-only models compared to L1-structured pruning.

### Strengths and Weaknesses
Strengths:
- The paper provides a comprehensive experimental analysis of LLM compression techniques across various methodologies and architectures.
- The presentation is clear, with main findings highlighted in textual boxes, enhancing readability.
- The focus on knowledge preservation during compression is a notable contribution that has been overlooked in prior studies.

Weaknesses:
- Many findings are intuitive and lack novelty, with some conclusions already established in existing literature.
- The paper does not sufficiently explore the importance of specific layers or neurons, nor does it analyze the impact of different compression techniques based on data domains.
- The statistical rigor is inadequate, relying on consistency rather than validating claims through appropriate statistical tests.

### Suggestions for Improvement
We recommend that the authors improve the depth of their analysis by investigating the importance of specific layers and neurons, such as comparing bottom, middle, and top layers, and examining attention heads responsible for different functions. Additionally, we suggest including experiments on a broader range of pruning and quantization techniques to enhance the generalizability of the findings. The authors should also strengthen the statistical rigor of their claims by employing tests like the Kolmogorovâ€“Smirnov or Wilcoxon tests to validate performance differences. Finally, we advise summarizing the findings into clearer "best practices" for future researchers to facilitate easier comprehension and application.