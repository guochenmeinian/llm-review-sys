ID: x816mCbWpR
Title: Recasting Continual Learning as Sequence Modeling
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 6, 4, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to continual learning by formulating it as a sequence modeling problem, specifically through the lens of meta-continual learning (MCL) using Transformers. The authors propose that the learning process can be viewed as the forward pass of a sequence model, employing meta-learning techniques to optimize the model's parameters. Efficient transformers, such as the Linear Transformer and Performer, are utilized to mitigate computational and memory costs associated with long sequences. The effectiveness of the proposed method is demonstrated through evaluations on various benchmark datasets.

### Strengths and Weaknesses
Strengths:
- The formulation of continual learning as a sequence modeling problem is innovative, with clear connections drawn between the two frameworks, particularly illustrated in Algorithms 1 and 2.
- The introduction is well-written, facilitating reader comprehension of the study's central ideas and contributions.
- The paper is generally well-presented, with good reproducibility and rich content in the Appendix.

Weaknesses:
- The distinction between continual learning and meta-continual learning is insufficiently articulated, particularly regarding the need for a large-scale offline dataset in meta-continual learning.
- Key terms related to meta-continual learning, such as episodes, meta-train, and meta-test, lack clear definitions, and important concepts like support and query sets are not mentioned.
- The paper overlooks significant baselines in meta-continual learning, such as "Wandering within a world: Online contextualized few-shot learning," and does not reference efficient meta-learning techniques.
- Experimental results do not adequately address the balance between catastrophic forgetting and rapid adaptation, and the presentation of results could be improved with visual representations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the distinction between continual learning and meta-continual learning, explicitly addressing the necessity of a large-scale offline dataset for the latter. Additionally, the authors should define and explain key terms such as episodes, meta-train, and meta-test more explicitly, and include discussions on support and query sets. It is crucial to incorporate references to overlooked baselines and efficient meta-learning techniques, as well as to add a section comparing these techniques to transformers regarding computational and memory costs. Furthermore, we suggest enhancing the presentation of experimental results by using plots to illustrate the relationship between continual sessions and average accuracy, thereby addressing the challenges of catastrophic forgetting and adaptation. Lastly, the authors should consider conducting experiments with longer meta-test episodes and varying distributions to demonstrate the practical applicability of their method.