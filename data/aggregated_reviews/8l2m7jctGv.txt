ID: 8l2m7jctGv
Title: Focus on the Core: Efficient Attention via Pruned Token Compression for Document Classification
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to enhance the efficiency of Transformer architectures by integrating token pruning and token combining. The authors propose using fuzzy logic to address uncertainty in token importance, aiming to improve performance and speed in BERT models. Extensive experiments demonstrate the effectiveness of their approach, achieving memory savings and speedup.

### Strengths and Weaknesses
Strengths:
- The experimental results convincingly show improvements in performance while reducing time and memory costs compared to the vanilla BERT model.
- The proposed method consistently enhances performance across various classification tasks.

Weaknesses:
- The contribution appears incremental, primarily combining existing methods from computer vision.
- The motivation for using fuzzy-based token pruning is unclear, raising concerns about its effectiveness in distinguishing token importance.
- The method section lacks clarity, particularly regarding the fuzzy membership functions and their implications for pruning.
- There is insufficient comparison with other token pruning methods, limiting the contextual understanding of the proposed approach.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the fuzzy-based pruning method, particularly by providing a detailed explanation of the fuzzy membership functions and their advantages over direct importance scoring. Additionally, the authors should include comparisons with other token pruning and combination baselines to strengthen their claims. Clarifying how the unimportant and important token sets are constructed, as well as addressing the initialization and scaling of combination tokens, would enhance the paper's rigor. Lastly, revising Figure 1 for accuracy and clarity is essential for better understanding.