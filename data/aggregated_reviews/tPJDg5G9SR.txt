ID: tPJDg5G9SR
Title: Attack Prompt Generation for Red Teaming and Defending Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an in-context learning-based method for constructing attack prompts to induce large language models (LLMs) to generate harmful content. The authors propose a defense framework that fine-tunes target LLMs through multi-turn interactions with the attack framework to enhance safety. The paper introduces a new red team prompt dataset generated with in-context learning and demonstrates the effectiveness of both the attack method and the defense framework through extensive experiments.

### Strengths and Weaknesses
Strengths:
- The security risk of LLMs is a crucial topic, and the proposed method leverages in-context learning to combine manual and automatic prompt construction, providing a new red team dataset that benefits the community.
- The experiments are sound, supporting the claims well, and the academic writing is clear and easy to follow.

Weaknesses:
- The paper lacks technical novelty, primarily combining existing methods without substantial innovation.
- There is no comparison with relevant baselines, which diminishes the evaluation's credibility.
- The proposed framework is sensitive to input prompts, and the impact of prompt design on performance is not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the technical novelty by incorporating more innovative elements into their framework. Additionally, including comparisons with relevant baselines would enhance the paper's credibility. Clarifying the threat model, addressing the sensitivity of the framework to input prompts, and providing more details on the prompt generation process would strengthen the overall contribution.