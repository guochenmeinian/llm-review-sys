ID: HS0faHRhWD
Title: Time-FFM: Towards LM-Empowered Federated Foundation Model for Time Series Forecasting
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 7, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TIME-FFM, a federated foundation model for time series forecasting that integrates modality alignment, prompt adaptation, and a personalized federated training strategy. The authors aim to address privacy concerns and data scarcity by transforming time series data into text tokens and leveraging pretrained language models (LMs). They argue that existing public time series datasets are insufficient for commercial applications due to their limited scale and lack of domain-specific knowledge. The authors emphasize the importance of keeping private time series data local to prevent privacy leakage and propose a personalized strategy for updating encoder parameters without extensive local optimization. Extensive experiments demonstrate the method's effectiveness across various benchmark datasets. The authors clarify that TIME-FFM aggregates only encoder parameters while maintaining personalized prediction heads for each client, distinguishing it from Federated Averaging.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to understand.
2. The innovative approach of transforming time series data into text tokens is promising for addressing data scarcity.
3. The personalized federated training strategy effectively handles data heterogeneity across domains.
4. The authors effectively address privacy concerns related to time series data and propose a relevant solution through federated learning.
5. The distinction between TIME-FFM and Federated Averaging is clearly articulated, enhancing the understanding of their approach.

Weaknesses:
1. The practical deployment of TIME-FFM is challenging due to high latency and the requirement for GPU resources, which limits applicability in resource-constrained environments.
2. The novelty of the proposed method is limited, as components like modality alignment and prompt adaptation have been explored in previous works.
3. The authors fail to adequately discuss relevant literature on time series foundation models and federated learning, which undermines the rationale and contributions of their work.
4. There is a lack of clarity regarding the mechanisms of privacy leakage in time series data, and the authors do not rigorously define privacy concepts such as differential privacy.
5. The authors' assertion about the unavailability of permissively licensed public time series data is contested, as some datasets are indeed available for commercial use.

### Suggestions for Improvement
We recommend that the authors improve the clarity and depth of the algorithm description, particularly correcting any discrepancies noted in the manuscript. Additionally, we suggest providing a more detailed explanation of the personalized federated strategy to enhance understanding. It would also be beneficial to include comparisons with recent time series forecasting models and to address the computational costs associated with their method. We encourage the authors to discuss the practical deployment challenges and provide evidence supporting the necessity of federated learning in various contexts. Furthermore, we recommend improving the discussion on the mechanisms of privacy leakage in time series data, including a rigorous definition of privacy concepts like differential privacy. Lastly, we suggest incorporating a broader discussion on the availability of permissively licensed public time series datasets and providing more details about the identification and significance of text prototypes, including examples of learned prototypes and their importance in aligning time series and LLM.