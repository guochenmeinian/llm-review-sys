ID: 0b2chPXfVG
Title: Orca: A Few-shot Benchmark for Chinese Conversational Machine Reading Comprehension
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the first Chinese Conversational Machine Reading Comprehension benchmark, Orca, which evaluates several state-of-the-art models. The authors propose a new dataset that allows for multiple passages in dialogues, addressing a gap in previous datasets that only considered single passages. The paper includes baseline performances of models like ChatGPT, highlighting areas for improvement.

### Strengths and Weaknesses
Strengths:  
- The paper introduces a valuable dataset for the Chinese Conversational Machine Reading Comprehension task.  
- It provides a comprehensive evaluation of various models, demonstrating the dataset's utility.  
- The task setting is realistic and challenging, allowing for naturalistic and free-form answers.  

Weaknesses:  
- The evaluation metrics, specifically BLEU and Distinct, may not adequately measure model performance, particularly for open-ended responses from models like ChatGPT.  
- The experiments, especially for ChatGPT, could have included few-shot cases and clearer reporting of prompts used.  
- Concerns exist regarding the quality of gold labels and the clarity of the role of retrieved documents in evaluation.

### Suggestions for Improvement
We recommend that the authors improve the evaluation metrics to better reflect the performance of models in conversational settings, possibly by considering additional metrics beyond BLEU. We suggest including few-shot performance data for ChatGPT and clearly documenting the prompts used in experiments for reproducibility. Additionally, we encourage the authors to address potential noise in the gold labels and clarify the role of retrieved documents in the evaluation process.