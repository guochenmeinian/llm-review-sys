ID: GLUIuli3Sm
Title: On the Convergence of Loss and Uncertainty-based Active Learning Algorithms
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the convergence of stochastic gradient-based learning algorithms, focusing on a stochastic step size mechanism influenced by losses or uncertainty-related quantities during training. The authors categorize their theoretical results into two main types: the first involves a Bernoulli random variable step size, while the second introduces an "Adaptive-Weight Sampling (AWS)" approach that adapts the step size based on loss/gradient values. The paper includes empirical analyses comparing AWS with traditional loss-based and uniform random sampling methods, assuming access to loss information. Additionally, the authors explore loss-based sampling in comparison to uniform sampling within the general classification setting, providing experimental evidence that sampling based on absolute error loss can achieve superior sampling complexity and convergence rates. Theorem 3.3 establishes an upper bound on expected cumulative training loss for loss-based sampling, accommodating uniform sampling as a special case, and proposes conditions under which loss-based sampling outperforms uniform sampling regarding convergence rates and sampling complexity.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and clearly written, making it easy to follow.
- It provides a flexible set of conditions for evaluating SGD procedures with a stochastic loss-dependent step size mechanism, building upon existing literature.
- The introduction of AWS offers a novel perspective in the active learning literature.
- The paper addresses a significant question regarding sampling complexity and convergence rates, contributing valuable insights to the active learning community.
- The authors' experimental results support their theoretical claims, demonstrating the effectiveness of loss-based sampling.
- The inclusion of additional discussions on the dependence of convergence rates and sampling complexities on the sampling function $\pi$ is commendable.

Weaknesses:
- The narrative is overly bloated, particularly regarding the relevance of "active learning" and "uncertainty-based" methods, which detracts from the core results.
- Certain sections exhibit rushed and unclear technical exposition, making it challenging for first-time readers to grasp the concepts.
- The scope of the work does not extend to proposing a specific loss estimator, which may limit practical applicability.
- The authors acknowledge that the choice of loss estimator is not critical, which could detract from the perceived robustness of their findings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by removing extraneous references to "active learning" and "uncertainty-based" methods, focusing instead on the core problem setting. Additionally, we suggest enhancing the technical exposition to ensure that all definitions and algorithms are clearly articulated. Specific points of confusion, such as the differences with related works and the expectations in Theorem 3.1, should be addressed more precisely. Furthermore, we encourage the authors to explore the possibility of relaxing the requirement for exact loss function access before querying and to include comparisons of theoretical sample complexity between active and passive learning settings. We also recommend improving the discussion on the dependence of convergence rates and sampling complexity on the sampling function $\pi$ in the revision. Including a more detailed analysis of the implications of using different loss estimators, as well as a clearer justification for the choice of Random Forest and neural network-based loss estimators in their experiments, would be beneficial. Finally, we encourage the authors to highlight the open research problem regarding the tightness and comparison of convergence rates and sampling complexities in the conclusion section.