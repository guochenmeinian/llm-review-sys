ID: 9jj7cMOXQo
Title: Towards Cross-Table Masked Pretraining for Web Data Mining
Conference: ACM
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new pre-trained model for tabular data called CM2, along with a large-scale dataset (OpenTabs) for cross-table predictions. The model employs a novel pretraining objective, Prompt Masked Table Modeling (pMTM), to capture structural information in tabular datasets. The authors propose that CM2 can be utilized for various downstream tasks, including regression, anomaly detection, and missing value imputation.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and easy to follow, with effective use of illustrations and emphasis on core ideas.
- It introduces a novel large-scale dataset and pretrained model that outperform previous methods on various tasks.
- The model demonstrates significant performance gains in few-shot learning settings, showcasing its adaptability across diverse applications.

Weaknesses:
- The scope of the problem addressed is unclear, particularly regarding the concept of "cross-table" pretraining and its relation to actual cross-table tasks.
- Some experimental details and dataset descriptions are vague, lacking clarity on the dataset's curation and the model's training specifics.
- The paper does not adequately justify certain design choices, such as the 128-dimensional embedding size and the tuning process for permutation invariance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the scope of "cross-table" pretraining and explicitly discuss relevant cross-table tasks in the related work section. Additionally, providing more detailed information on the dataset's curation, including sources and preprocessing steps, would enhance understanding. We suggest including statistical significance tests in the results tables to better convey the improvements. Furthermore, a detailed discussion on the tuning process for the transformer architecture and the rationale behind the embedding size choice would strengthen the paper's technical foundation. Lastly, addressing the color-blind inclusivity of figures and correcting any typographical errors would improve the overall presentation.