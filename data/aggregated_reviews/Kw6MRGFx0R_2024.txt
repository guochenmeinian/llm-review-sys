ID: Kw6MRGFx0R
Title: QBB: Quantization with Binary Bases for LLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 3, 6, 5, 6, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel quantization technique called Quantization with Binary Bases (QBB), which aims to reduce the computational complexity of large language models (LLMs) by decomposing model weights into binary matrices and scaling factors. The authors propose a three-step algorithm involving the use of gradient descent and knowledge distillation to optimize these binary matrices and scaling vectors, claiming improved performance across multiple LLM families. The paper asserts that it sets a new state-of-the-art (SOTA) for summation-only based approaches.

### Strengths and Weaknesses
Strengths:
1. The method's use of binary matrices and scaling vectors represents a significant advancement in reducing computational complexity.
2. The combination of post-training quantization with knowledge distillation enhances performance, as demonstrated by extensive experimental results across various LLMs and datasets.
3. The paper is well-organized, with clear derivations and effective illustrations.

Weaknesses:
1. The proposed binary decomposition method closely resembles previous nonlinear quantization techniques, such as LQ-Net, and requires a discussion of similarities and performance comparisons.
2. The claim of efficiency by removing multiplications is questionable, as the scaling factor still necessitates costly multiplications, particularly for large N.
3. The efficiency discussion lacks concrete estimations of memory and computation savings compared to regular linear W4A16 quantized models.
4. The comparison primarily focuses on PTQ methods with linear weight quantization, which may not provide a fair assessment given the non-linear nature of the proposed method and its additional finetuning requirements.
5. The paper does not adequately address the computational cost associated with the proposed adjustments and lacks experiments to validate these costs.

### Suggestions for Improvement
We recommend that the authors improve the discussion in Section 4.6 to provide detailed memory and computation savings compared to regular linear W4A16 quantized models. Additionally, please clarify the quantization configuration of the reported W2.3 quantized model. It would also be beneficial to compare the proposed method with non-uniform quantization methods, such as QuIP#, and to specify whether $W$ in Equation 4 refers to quantized weights. Furthermore, please provide details on the training iterations required for each layer and clarify the specification of $s_1$ and $s_2$ in Equation 7. Lastly, consider including a comparison with other mainstream language models beyond LLama to strengthen the experimental results.