ID: 8Ba7VJ7xiM
Title: Analyzing Generalization of Neural Networks through Loss Path Kernels
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 5, 8, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to generalization in neural networks through the lens of the neural tangent kernel (NTK) framework, introducing the Loss Path Kernel (LPK) as a new complexity measure. The authors derive generalization bounds that are data-and-architecture-dependent, leveraging the evolution of the NTK under gradient flow. Additionally, the paper analyzes the generalization capability of neural networks trained by gradient flow, claiming to derive upper bounds for solutions under certain smoothness assumptions. The authors assert that their results extend to stochastic gradient descent (SGD) and finite learning rates. The theoretical contributions are complemented by numerical experiments demonstrating the practical applicability of the proposed method, particularly in neural architecture search (NAS). However, concerns are raised regarding the applicability of the derived bounds to more complex scenarios like SGD, which diverges from gradient flow due to discretization errors.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant problem in understanding generalization in overparameterized neural networks, contributing meaningfully to the existing literature.
- The theoretical linkage between the proposed complexity measure and generalization bounds is well-articulated.
- Empirical results indicate that the derived bounds are non-vacuous and applicable in practice.
- The authors provide a detailed response to reviewer comments, indicating a willingness to address concerns.

Weaknesses:
- The presentation lacks clarity in certain mathematical sections, making it challenging for readers to follow the arguments.
- The empirical analysis is insufficiently detailed, particularly regarding the implementation of NAS and the clarity of figures.
- The proposed method is computationally expensive, raising concerns about its practicality.
- The claims regarding the applicability of the derived bounds to SGD and finite learning rates lack sufficient proof, as the reviewers highlight discrepancies between gradient flow and actual training scenarios.
- The use of ReLU activations violates key smoothness assumptions, raising questions about the validity of experimental results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the mathematical presentation, ensuring that complex concepts are accessible to a broader audience. Additionally, the empirical section should provide more thorough explanations of the NAS experiments, including the search space and architecture details. To enhance the practical applicability, consider discussing approximations or computational improvements for the proposed method. Furthermore, we recommend that the authors clarify their claims regarding the applicability of their results to realistic training scenarios, specifically addressing the limitations of their analysis concerning SGD and finite learning rates. It is crucial to either demonstrate that realistic training scenarios align with gradient flow solutions or retract claims of applicability beyond gradient flow. We also suggest rephrasing instances of "descent" to "flow" throughout the paper to accurately reflect the theoretical framework being discussed. Lastly, the authors should clarify the dependence of their bounds on specific batch selection regimes in the revised manuscript.