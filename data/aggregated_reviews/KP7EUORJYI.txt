ID: KP7EUORJYI
Title: Goal-Conditioned On-Policy Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Goal-Conditioned Policy Optimization (GCPO), an on-policy variant for goal-conditioned reinforcement learning (RL) that effectively addresses non-Markovian reward structures, a limitation of existing methods like Hindsight Experience Replay (HER). The authors propose a framework that combines offline pre-training from expert demonstrations with online self-curriculum learning to progressively select challenging goals. Empirical results demonstrate GCPO's superiority over other methods, particularly in a UAV task with non-Markovian rewards.

### Strengths and Weaknesses
Strengths:
1. The development of an on-policy GCRL algorithm is a significant contribution, addressing a useful problem.
2. The ability to handle non-Markovian reward domains is a notable advancement.
3. The paper includes a comprehensive set of baseline algorithms and ablation studies, enhancing the clarity and depth of the research.

Weaknesses:
1. The experiments would benefit from additional domains to illustrate the general applicability of the algorithm, particularly in Markovian reward settings.
2. There is a lack of discussion regarding relevant works in contextual/goal-conditioned RL that also utilize self-curriculum learning in non-Markovian environments.
3. The requirement for demonstrations poses a limitation, as methods like HER do not have this constraint.

### Suggestions for Improvement
We recommend that the authors improve the discussion of relevant works that consider contextual/goal-conditioned RL with self-curriculum learning, such as those by Klink et al. and Celik et al. Additionally, we suggest conducting experiments in a Markovian reward domain to demonstrate the algorithm's versatility. Clarifying how hyperparameters were tuned and providing ablations on the quality of demonstrations would strengthen the paper. Finally, addressing the limitations of requiring demonstrations and exploring potential alternatives would enhance the overall contribution of the work.