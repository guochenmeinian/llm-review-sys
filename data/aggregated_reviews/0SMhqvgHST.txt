ID: 0SMhqvgHST
Title: EEVEE and GATE: Finding the right benchmarks and how to run them seamlessly
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 3, 7, 8, -1
Original Confidences: 5, 3, 4, -1

Aggregated Review:
### Key Points
This paper presents EEVEE (Efficient Evaluation process Evolution Engine), a framework that treats evaluation process design as a learning problem. EEVEE employs a bi-level optimization approach, utilizing evolutionary algorithms and a meta-model predictor to identify optimal subsets of benchmarks for evaluating pretrained encoders in deep learning. The resulting benchmark subsets are organized into three tiers based on computational requirements and made accessible through the authors' Python framework, GATE. The authors aim to streamline the evaluation of representation learning's generalization quality, addressing the challenge of selecting relevant benchmarks from a diverse pool.

### Strengths and Weaknesses
Strengths:
- The authors' effort to unify benchmarking for representation learning is commendable and timely.
- The methodology for constructing optimal benchmarks is simple, effective, and well-structured.
- Extensive experimentation has yielded concrete recommendations for benchmark subsets optimized for various computational budgets.
- The provision of GATE facilitates efficient model benchmarking for the research community.

Weaknesses:
- The paper is poorly written and confusing in parts, with unclear terminology (e.g., "signal") and ambiguous references (e.g., checkmarks in Tab. 1).
- Limitations discussed by the authors were insufficient, and no codes are provided for correctness evaluation or reproducibility.
- The paper lacks a direct comparison with VSTAB and does not adequately address potential biases in benchmark selection.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by defining key terms such as "signal" and ensuring that all references, such as those in Tab. 1, are clearly explained. Additionally, we suggest including a direct comparison with VSTAB to strengthen the paper's positioning and addressing potential biases in benchmark selection that the MSE optimization approach might not adequately cover. Explicitly discussing the assumption that different model quality metrics can be comparable would also enhance the paper. Finally, we encourage the authors to provide code for reproducibility and to consider the implications of ranking models rather than solely focusing on MSE loss in their evaluation section.