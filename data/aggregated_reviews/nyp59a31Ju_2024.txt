ID: nyp59a31Ju
Title: Is Value Learning Really the Main Bottleneck in Offline RL?
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 6, 7, 8, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical analysis aimed at identifying the primary challenges in offline reinforcement learning (RL), focusing on value function learning, policy extraction, and policy generalization. The authors conclude that policy extraction and generalization are the main bottlenecks, rather than value function learning. The analysis is structured into two parts: evaluating the data efficiency of decoupled policy and value learning steps, and assessing policy generalization at test time across various domains.

### Strengths and Weaknesses
Strengths:
1. The systematic approach of isolating components using decoupled RL algorithms helps avoid confounding factors.
2. The paper covers results across multiple data dimensions, providing actionable insights.
3. Clear takeaways enhance understanding of the empirical results and guide future research directions.
4. The experimental methodology of comparing policy and value learning efficiency is innovative and could inform future work.

Weaknesses:
1. The main results are based on only four seeds, which may not adequately capture the variability inherent in offline RL algorithms.
2. Figures lack variance measures, making it difficult to assess the reliability of reported means.
3. The claim to “always use” certain methods is overly strong and requires moderation.
4. The focus on generalization of policies over values is unclear, as both are critical for the paper's main claims.

### Suggestions for Improvement
We recommend that the authors improve the robustness of their results by increasing the number of seeds used in experiments to better capture variability. Additionally, incorporating variance measures such as standard deviation or confidence intervals in the results would enhance the credibility of the findings. The authors should reconsider the strength of their claims, particularly the assertion to “always use” specific methods. Clarifying the rationale for focusing on policy generalization over value generalization would strengthen the paper's arguments. Finally, we suggest that the authors consider reframing the title and abstract to better reflect the dual focus on policy and value learning, or potentially splitting the paper into two distinct studies for clarity.