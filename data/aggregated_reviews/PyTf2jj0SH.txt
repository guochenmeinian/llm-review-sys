ID: PyTf2jj0SH
Title: ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Ablation Capability for Large Vision-Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 8, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two new benchmarks: LVLMs, aimed at evaluating the multi-turn conversation capabilities of multi-layer language models (MLLMs), and ConvBench, an evaluation framework for large visual language models (LVLMs). LVLMs includes 577 multi-turn conversations across 215 tasks, categorized into three ability levels: perception, reasoning, and creation. The experimental results reveal that even advanced models like GPT-4V encounter significant challenges with this benchmark. The authors introduce a hierarchical evaluation method that assesses models across these dimensions, providing insights into their performance. ConvBench proposes a multi-faceted evaluation process involving three distinct settings: evaluation without ground-truth, evaluation with perfect perception, and evaluation with both perfect perception and reasoning. Findings indicate that ConvBench poses significant challenges for current LVLMs, revealing weaknesses in reasoning and creation abilities due to limited perception. The paper also highlights the importance of high-quality dialogue history in multi-turn conversations and suggests a correlation between model size and performance improvements.

### Strengths and Weaknesses
Strengths:
1. The benchmarks are meticulously constructed, offering comprehensive evaluation frameworks with structured ability levels.
2. The papers are well-written and easy to follow.
3. The findings highlight the challenges faced by current MLLMs and LVLMs, emphasizing the critical role of perception in facilitating reasoning and creation.
4. Insights into the limitations of current models are well-articulated, particularly regarding perception and reasoning.
5. The relevance of the benchmarks in assessing real-world use cases for LVLMs is effectively demonstrated.

Weaknesses:
1. The benchmark relies on ChatGPT-3.5 as judges, raising questions about the choice of model and the costs associated with comprehensive evaluations.
2. The experimental section lacks clarity, making it difficult for readers to follow the evaluation process.
3. There are typographical inconsistencies in the formatting of GPT-4V.
4. Some areas of the paper may benefit from further elaboration to enhance understanding.
5. The authors should engage more with related works on multi-turn dialogue evaluation, such as MT-Bench-101, which proposes a hierarchical, fine-grained approach.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by employing more advanced models, such as GPT-4V, and clarify the costs of conducting a comprehensive evaluation. Additionally, addressing typographical inconsistencies in the formatting of GPT-4V is necessary. We also suggest that the authors improve the clarity of the experimental section by providing a concise introduction and addressing specific areas that may still be unclear. Enhancing the explanations of the evaluation settings and their implications could further strengthen the paper's impact. Finally, we recommend that the authors discuss more related works on multi-turn dialogue evaluation to enrich the context of their research.