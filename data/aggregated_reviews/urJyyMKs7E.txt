ID: urJyyMKs7E
Title: HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 8, 6, 9, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents HW-GPT-Bench, a hardware-aware benchmark designed to optimize configurations for large language models across various hardware metrics and devices. It utilizes surrogate models to predict performance (perplexity) and hardware metrics (latency, energy consumption, memory usage), enabling efficient exploration of multi-objective Neural Architecture Search (NAS) methods. The benchmark evaluates GPT-2-like models on 13 devices, demonstrating trade-offs between energy, latency, and perplexity, while also modeling noise in measurements.

### Strengths and Weaknesses
Strengths:
- Clear presentation and logical flow of ideas.
- Sound methodology with rigorous experiments ensuring reliable results.
- Extensive evaluation across multiple hardware metrics and devices.
- Open-source API facilitates integration of new methods into the benchmark.

Weaknesses:
- Limited novelty in methodology, primarily drawing from existing works like NAS-Bench-301.
- The benchmark's focus on GPT-2 models raises questions about scalability to larger models beyond 774M parameters.
- Insufficient diversity of hardware devices, particularly in mobile and embedded spaces.
- The relevance of the benchmark in light of scaling laws and the rationale for multiple search spaces require further clarification.

### Suggestions for Improvement
We recommend that the authors improve the diversity of hardware devices included in the benchmark, particularly by incorporating mobile, embedded devices, and NPUs. Additionally, highlighting methodological differences from existing surrogate benchmarks would enhance the paper's originality. The authors should address the scalability of the benchmark to larger models and consider evaluating it on other language model architectures beyond GPT-2. Clarifying the rationale for the specific form of the power law used in the analysis and ensuring that potential limitations are clearly discussed in the paper are also necessary. Lastly, addressing minor issues such as typos and font sizes in figures would improve clarity.