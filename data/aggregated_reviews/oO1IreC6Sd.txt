ID: oO1IreC6Sd
Title: Neural Fields with Hard Constraints of Arbitrary Differential Order
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 7, 6, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to enforce hard constraint points on neural fields by using neural networks to learn basis functions, which are combined in a linear transformation. The authors propose a novel methodology called constrained neural fields (CNF) that transforms linear equality constraints into a linear system. The method shows promise across various applications, outperforming existing techniques on the MERL BRDF dataset. The paper includes a variety of experiments and some ablation studies to validate the method.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to understand, with clear methodology and a convincing analysis of conditioning due to different kernel methods.
- The proposed method guarantees that constraint points are not violated and demonstrates high performance across diverse tasks.
- The inclusion of source code is a valuable resource for reproducibility.

Weaknesses:
- The theoretical contribution is limited; a theoretical argument for faster convergence compared to unconstrained training is lacking.
- The constraints are applicable only to single points, which is not clearly stated in the abstract, limiting the handling of initial states and boundary conditions.
- Comparisons against state-of-the-art methods are sparse, and the paper lacks learning curves and discussions on training time, which are critical for evaluating performance.

### Suggestions for Improvement
We recommend that the authors improve the theoretical foundation of the paper by providing a detailed argument for the convergence properties of the proposed method. Clarifying the applicability of constraints beyond single points in the abstract would enhance understanding. Additionally, we suggest including comparisons against soft constraint approaches and classical numerical methods to contextualize the results better. Providing learning curves and a detailed discussion on training and inference times would also be beneficial. Finally, addressing the minor issues related to citations and explanations of technical terms would improve the overall clarity of the paper.