ID: rb3wgt85in
Title: ColaCare: Enhancing Electronic Health Record Modeling through Large Language Model-Driven Multi-Agent Collaboration
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ColaCare, a multi-agent framework utilizing Large Language Models (LLMs) for clinical decision-making, specifically enhancing Electronic Health Records (EHR) modeling through collaborative analysis by DoctorAgents and a MetaAgent. ColaCare aims to bridge structured EHR data with text-based reasoning, inspired by Multidisciplinary Teams in clinical settings. The framework demonstrates improved performance in clinical mortality and readmission prediction tasks, supported by experiments on real-world datasets.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant research problem by using AI to aid clinical decision-making.
- It is well-written and clear, making it accessible to readers.
- The agentic approach with LLMs is a promising method for collaborative settings.
- A comprehensive evaluation across multiple datasets shows clear improvements over baseline models.

Weaknesses:
- ColaCare has not been evaluated by real doctors, and a human evaluation is expected in this domain.
- The reliance on a limited number of LLMs for comparison may weaken the results; incorporating additional models like LLaMA could enhance credibility.
- The experimental design lacks up-to-date baselines and broader evaluation metrics, which are necessary for validating the approach.
- The workflow's heavy reliance on manual design limits its applicability to other domains and may not represent a genuine research problem.
- The improvements in performance are not statistically significant, raising concerns about the cost-effectiveness of using LLMs.

### Suggestions for Improvement
We recommend that the authors improve the paper by including a human evaluation to validate ColaCare's effectiveness in real clinical settings. Additionally, incorporating a broader range of LLMs, such as LLaMA, and more up-to-date baselines would strengthen the comparisons and results. Expanding the evaluation metrics and including a larger, more diverse test set would provide greater clarity on the proposed method's performance. The authors should also clarify how their multi-agent collaboration differs from existing methods and explore alternative validation methods to enhance the trustworthiness of their claims. Finally, addressing the cost concerns related to LLM usage and demonstrating meaningful gains over state-of-the-art ensemble approaches would further justify the framework's implementation.