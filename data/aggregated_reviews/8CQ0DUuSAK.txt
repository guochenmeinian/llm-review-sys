ID: 8CQ0DUuSAK
Title: Clustering Pseudo Language Family in Multilingual Translation Models with Fisher Information Matrix
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for calculating translation pair similarity using the Fisher Information Matrix (FIM) to create pseudo language-families, which are employed to select auxiliary data for fine-tuning translation models. The authors claim that their approach improves upon traditional language family methods, demonstrating some positive results, albeit with less than a BLEU point improvement on average.

### Strengths and Weaknesses
Strengths:  
- The idea of using FIM to compute similarity and create pseudo families is innovative and contributes valuable insights into multilingual training.  
- The approach is straightforward and has shown reasonable improvements in experiments, making it a good asset for the research community.  
- The authors' intention to release code and data enhances reproducibility.

Weaknesses:  
- The superiority of the proposed method is not convincingly demonstrated, requiring more extensive evaluation.  
- The choice of using a Fisher Mask and the arbitrary selection of the top 30% of parameters raises questions about the methodology.  
- The paper overlooks the implications of different writing systems in traditional language families and their relevance to the generated pseudo families.  
- The use of binarized activated neurons for similarity computation appears ad hoc, suggesting a need for a more nuanced approach.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of their method by conducting more comprehensive experiments to substantiate their claims of superiority. Additionally, addressing the potential impact of different writing systems on language similarity would strengthen the paper. We suggest exploring the use of the Fisher Information itself for similarity computations instead of relying solely on masking. Furthermore, clarifying the rationale behind the choice of parameters and correcting the reference to the TED corpus in the introduction would enhance the paper's clarity and accuracy. Lastly, we encourage the authors to consider using floats of activated neurons for similarity and distance computations to provide a more robust analysis.