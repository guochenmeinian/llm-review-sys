ID: P2WRhheKMm
Title: Exploring the Practicality of Generative Retrieval on Dynamic Corpora
Conference: ACM
Year: 2024
Number of Reviews: 3
Original Ratings: 1, -1, 2
Original Confidences: 4, 4, 4

Aggregated Review:
### Key Points
This paper presents a comprehensive evaluation of Generative Retrieval (GR) compared to Dual Encoders (DE) in dynamic information retrieval scenarios. The authors propose DynamicIR, utilizing the StreamingQA benchmark to assess the adaptability, robustness, and efficiency of four models (two GR and two DE). The findings indicate that GR outperforms DE in handling evolving knowledge, demonstrating better effectiveness, lower inference costs, and improved memory efficiency.

### Strengths and Weaknesses
Strengths:
- The paper addresses various evaluation facets, including inference FLOPS, indexing time, and storage, which are often overlooked in the literature.
- By employing the StreamingQA benchmark, the study tackles practical challenges faced by IR systems with evolving corpora, enhancing the relevance of the findings.
- The comparison between GR and DE paradigms is well-structured, and the results and analysis section is comprehensive.
- The work provides meaningful insights into updating retrieval models in dynamic contexts.

Weaknesses:
- The writing quality resembles a draft, affecting clarity, with incorrect citations and formatting issues, such as a table appearing in the references.
- The paper sparsely cites relevant work by Mehta et al. 23, which explores similar questions in greater detail, including unlearning documents.
- Limiting the evaluation to a single dataset (StreamingQA) restricts the generalizability of the findings across different IR contexts.
- The formulation of SPIDER and Contriever may be insufficient to support some conclusions; pretraining with synthetic queries and including BM25 could better situate the results.
- The reliance on a single Hit@5 retrieval effectiveness metric may be limiting.

### Suggestions for Improvement
We recommend that the authors improve the clarity and quality of the writing to eliminate draft-like elements and correct citation errors. It is essential to include a broader range of related works, particularly those closely aligned with the study's focus. Expanding the evaluation to include multiple datasets and retrieval models, particularly multiple-vector search models, would enhance the generalizability of the findings. Additionally, we suggest that the authors clarify the observations regarding "training-based update" versus "indexing-based update" and consider incorporating BM25 to provide a more comprehensive situational context for their results.