ID: 7AXY27kdNH
Title: Amortized Active Causal Induction with Deep Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CAASL, an intervention design method that utilizes a transformer to predict the next intervention in causal structure learning, employing the Soft Actor-Critic (SAC) policy. The authors leverage AVICI as a reward model to estimate the adjacency matrix of the causal graph based on interventional data. CAASL demonstrates superior performance in uncovering the true adjacency matrix in synthetic environments and a gene expression simulator, particularly excelling in out-of-distribution settings. Additionally, the paper includes a thorough analysis and comparison of its approach against state-of-the-art (SOTA) baselines, addressing various criticisms raised in prior reviews. The authors propose to enhance the discussion regarding the competitiveness of a random baseline, particularly in the context of the SERGIO experiments, where improvements appear modest or unclear depending on the metric used. They acknowledge the need for clearer commentary on the 'returns' metric to avoid misinterpretation of their results.

### Strengths and Weaknesses
Strengths:
- The authors effectively combine various methodologies, including soft actor-critic, AVICI, and transformer policies, resulting in a novel approach to causal structure learning.
- The writing is clear, with well-labeled empirical analyses that explore in-distribution versus out-of-distribution generalization.
- The methodology is generally well-presented, and the experiments, especially regarding out-of-distribution performance, are encouraging.
- The authors have effectively addressed previous criticisms and clarified the limitations of their approach.
- The performance on synthetic benchmarks is positively noted, indicating the robustness of the method.
- The authors are open to incorporating additional discussions and clarifications in the final draft.

Weaknesses:
- The setup of causal structure learning is not sufficiently clear, particularly the relationship between CAASL and AVICI, leading to ambiguity regarding the novelty and efficiency of CAASL.
- The experiments appear somewhat simplistic, with random intervention policies outperforming other baselines, raising questions about the effectiveness of the proposed method.
- There is a lack of detailed background on the baselines used in experiments, and the performance of CAASL in high-dimensional settings does not meet expectations.
- The metric of 'returns' is currently described as opaque, potentially inflating the perceived effectiveness of the approach in SERGIO.
- There is a lack of clarity regarding the distinction between greedy and non-greedy strategies in the context of the algorithm, which could lead to confusion.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the causal structure learning setup, particularly how CAASL integrates with AVICI and the simulator. It would be beneficial to provide additional plots or explanations to clarify the novelty of CAASL and its advantages over AVICI. We suggest including experiments that demonstrate convergence to the true graph with sufficient interventions, as well as clarifying the performance of the random baseline compared to other methods. Additionally, conducting ablation studies on design choices, such as network architecture and activation functions, would strengthen the paper. We also recommend improving the discussion on the limitations of the random baseline in the SERGIO environment, emphasizing its competitiveness. Furthermore, please clarify the 'returns' metric in the main text to provide a more accurate representation of the results. It would also be beneficial to explicitly differentiate between greedy and non-greedy strategies in the revised manuscript, potentially by incorporating the Bandit setting and time horizon adjustments. Finally, we encourage the authors to expand the discussion on amortization and its implications for generalization, and consider summarizing the novel contributions in a dedicated section.