ID: G8aS48B9bm
Title: Byzantine Robustness and Partial Participation Can Be Achieved at Once: Just Clip Gradient Differences
Conference: NeurIPS
Year: 2024
Number of Reviews: 20
Original Ratings: 6, 4, 5, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new distributed learning method called Byz-VR-MARINA-PP, designed to achieve Byzantine robustness and partial participation simultaneously. The authors provide a theoretical analysis of its convergence and Byzantine robustness, alongside numerical results. The method combines Byz-VR-MARINA with gradient clipping to manage disturbances from Byzantine clients, demonstrating provable convergence for general smooth non-convex functions and PL functions. Additionally, the authors propose a theoretical framework for Byzantine-robust learning with partial participation, addressing a complex problem that has not been explored in such generality before. The proposed method includes an additional term in the convergence bound, which can be negligible under certain conditions. However, the practical performance of the proposed method, particularly in large-scale models, remains a concern.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and addresses an important problem in Byzantine-robust distributed learning.
- The proposed method is theoretically sound, with clear convergence guarantees and a well-structured presentation.
- The theoretical results are generally solid and tackle a challenging problem in Byzantine-robust learning.
- The authors are responsive to reviewer feedback and willing to clarify and improve their work.

Weaknesses:
- The novelty of the proposed method is limited, as it is primarily a combination of existing techniques, particularly Byz-VR-MARINA and gradient clipping.
- The requirement to compute the full gradient with probability $p$ at each iteration is computationally expensive, especially with large datasets.
- The experimental validation is conducted on relatively small datasets (a9a and MNIST), raising questions about the method's performance on larger scales.
- The practical performance of the proposed method is not satisfactory, particularly in large-scale scenarios.
- There are concerns regarding the clarity of certain claims, particularly about the uniqueness of the proposed method in the context of existing literature.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their work by providing a more detailed analysis of the proposed method's unique contributions beyond existing techniques. Additionally, we suggest that the authors explore the implications of using a robust aggregator versus a mean aggregator in their method, particularly in light of the gradient clipping technique. It would also be beneficial to include experiments on larger and more challenging datasets to better demonstrate the practical applicability of Byz-VR-MARINA-PP. Furthermore, we recommend that the authors improve the clarity of their claims regarding the uniqueness of their contributions by addressing existing works more comprehensively. Specifically, clarify the differences between their approach and the methods referenced by reviewers. We also suggest that the authors summarize the advantages and disadvantages of using full gradients periodically versus small-batch stochastic gradients, as this could enhance understanding of their method's design choices. Additionally, we encourage the authors to explore the implications of using simpler training methods, such as momentum SGD with robust aggregators, in conjunction with their proposed clipping strategy. Lastly, we advise the authors to explicitly include the condition $\hat{C} \geq \max{1, \delta_{\text{real}} n / \delta}$ in Theorems 4.1 and 4.2 to strengthen their theoretical results.