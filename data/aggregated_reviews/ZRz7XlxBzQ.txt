ID: ZRz7XlxBzQ
Title: Learning to compute Gröbner bases
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 3, 8, 8, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 2, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a machine learning algorithm for computing Gröbner bases of 0-dimensional ideals in shape position. The authors propose an algorithm that involves 1) random generation of Gröbner bases by sampling univariate polynomials related to shape position and 2) random generation of a polynomial matrix to create non-Gröbner sets. The algorithm's running time is derived in Theorem 4.8, and Transformers are employed for learning to compute Gröbner bases.

### Strengths and Weaknesses
Strengths:
- The benchmarks indicate that the proposed backward approach for computing Gröbner bases can be significantly more efficient than traditional forward algorithms.
- The article is well-written and accessible, making it comprehensible for non-experts in computational algebra.
- The authors present interesting ideas for generating training datasets and include examples of both success and failure, which can inform future improvements.

Weaknesses:
- The paper contains numerous typos, grammatical errors, and missing articles, necessitating a careful rewrite to enhance overall quality.
- The training approach using Transformers lacks theoretical guarantees regarding global/local convergence, running time, and error bounds, with insufficient explanation of its limitations.
- The focus is limited to 0-dimensional ideals in shape position, which is a strong assumption; the paper does not explore higher-dimensional ideals or provide concrete benchmarks in applications like cryptography or biological systems.
- Many important aspects, including theorem proofs and limitations, are relegated to the appendix, detracting from the main text's clarity.

### Suggestions for Improvement
We recommend that the authors improve the paper's presentation by addressing the numerous typos and grammatical mistakes. Additionally, the authors should provide theoretical guarantees for the Transformer-based training approach, including details on its convergence and error bounds. It would be beneficial to explore the applicability of their method to higher-dimensional ideals and include concrete benchmarks in relevant applications to enhance credibility. Finally, we suggest that the authors clarify the limitations of their work in the main text rather than relegating them to the appendix.