ID: svUOik2Xu1
Title: Robust Prompt Optimization for Large Language Models Against Distribution Shifts
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Generalized Prompt Optimization (GPO) framework for large language models (LLMs), focusing on enhancing generalization performance under domain shifts. The authors propose a technique that incorporates LLM-generated labels for a portion of the target data, optimizing prompts on both source and labeled target data. The GPO approach generates an ensemble of prompts, applies this ensemble to target data, and filters examples based on prompt agreement, demonstrating improved performance across various benchmark datasets.

### Strengths and Weaknesses
Strengths:
- The paper identifies a significant real-world problem and proposes a deployable solution with broad applications.
- It tackles the robustness of prompt optimization against distribution shifts, supported by extensive experiments and a detailed ablation study.
- The manuscript is well-structured, clearly written, and the experimental results substantiate the efficacy of GPO.

Weaknesses:
- The proposed solution lacks a strong machine learning foundation and may not be applicable to smaller models like BERT-large or T5-large.
- There is insufficient comparison with other gradient-free prompt optimization techniques, and certain hyperparameters influencing performance are overlooked.
- The paper does not adequately discuss past literature on using unlabeled data in NLP tasks.

### Suggestions for Improvement
We recommend that the authors improve the discussion of related work by including methods that utilize unlabeled data for performance enhancement. Additionally, we suggest conducting experiments to explore the effects of various hyperparameters, such as the number of candidate prompts $K$, on overall performance. Furthermore, we encourage the authors to provide comparisons with other gradient-free prompt optimization techniques to better contextualize their findings. Lastly, clarifying the parameters for generation, including top p, temperature, frequency penalty, and max tokens, would enhance reproducibility.