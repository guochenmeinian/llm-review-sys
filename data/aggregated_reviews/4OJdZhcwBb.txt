ID: 4OJdZhcwBb
Title: A Method for Evaluating Hyperparameter Sensitivity in Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 4, 7, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for assessing hyperparameter sensitivity in reinforcement learning (RL) algorithms, introducing two metrics: hyperparameter sensitivity, which quantifies the performance difference between the best hyperparameters for individual tasks and those that perform best on average across tasks, and effective hyperparameter dimensionality, which indicates how many hyperparameters can remain fixed while still achieving a significant portion of optimal performance. The authors apply this framework to analyze various normalization methods for PPO in continuous control environments, elucidating the trade-offs between performance and sensitivity. Additionally, the paper emphasizes the need for reduced human intervention in hyperparameter tuning methodologies, proposing a comparison with AutoRL and discussing the implications of different normalization techniques on performance and sensitivity.

### Strengths and Weaknesses
Strengths:
- The problem addressed is well-motivated, highlighting the critical issue of hyperparameter sensitivity in deep RL.
- The proposed metrics are intuitive and provide a clear framework for understanding the trade-offs involved.
- The experimental setup is thorough, utilizing 200 seeds per experiment to ensure statistical robustness.
- The authors effectively address reviewer concerns, leading to improved clarity and depth in the analysis.
- The inclusion of additional figures enhances the verification of stability and applicability of the findings.
- The discussion on hyperparameter importance and normalization techniques adds valuable insights for practitioners.

Weaknesses:
- The choice of performance metric (AUC) is inadequately justified, as it does not fully capture the final performance often prioritized in RL.
- The analysis lacks depth regarding the stability of results across different environments and normalization methods.
- The paper does not sufficiently discuss the implications of hyperparameter choices, leading to potentially misleading conclusions about sensitivity.
- The 2D plot's color segmentation may hinder analysis, as it appears arbitrary to some reviewers.
- There is a perceived lack of exploration regarding human-in-the-loop methodologies and their relevance to hyperparameter tuning.
- The paper could benefit from a more thorough examination of the AutoRL literature and its relationship to the proposed methodology.

### Suggestions for Improvement
We recommend that the authors improve the justification for the AUC metric, considering the importance of final performance in RL applications. Additionally, we suggest conducting stability analyses by varying the environments used in experiments to assess the robustness of results. The authors should also explore alternative normalization methods, such as CDF, and report their impact on the findings. Furthermore, a more critical discussion on how to leverage the proposed visualization method for practical insights would enhance the paper's contribution. We recommend improving the visualization of results by plotting them directly on the axes without arbitrary color segmentation, allowing for clearer analysis. Additionally, we suggest incorporating a more detailed discussion of human-in-the-loop approaches and their implications for hyperparameter tuning. To enhance the comparison with AutoRL, we encourage the authors to expand their literature review to include a more comprehensive analysis of how their methodology can be integrated with existing AutoRL techniques. Lastly, we advise that the authors deepen section 4.3 and consider moving section 4.2 to the appendix for better focus.