ID: YscR3LBIi7
Title: MoMu-Diffusion: On Learning Long-Term Motion-Music Synchronization and Correspondence
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 6, 7, 9, -1, -1, -1
Original Confidences: 2, 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MoMu-Diffusion, a framework for generating music from motion, motion from music, or both simultaneously while ensuring synchrony. The authors propose a bidirectional contrastive rhythmic VAE (BiCoR-VAE) that aligns latent spaces of motion and music through joint training, and a multi-modal diffusion transformer (DiT) that utilizes cross-guidance sampling. Experiments demonstrate that the proposed approach outperforms existing models in generating synchronized motion and music sequences.

### Strengths and Weaknesses
Strengths:  
- The paper introduces rhythmic contrastive learning and a method for joint diffusion, effectively aligning music and motion.  
- The user study and ablation studies are well-conducted, providing strong evidence for the model's improvements, as shown in metrics like beat matching and user preference.  
- The methodology has potential applications beyond motion-music synchronization, leveraging prior knowledge of motion.

Weaknesses:  
- The paper lacks detailed implementation specifics regarding the multi-modal VAE architecture, particularly concerning the processing of motion vectors and the normalization of autoencoder representations.  
- There is a need for qualitative comparisons of baseline models and ablation studies on hidden sizes and layers of the Diffusion Transformer to enhance understanding of synchronization claims.  
- The focus on beatness as the primary synchronization element may overlook scenarios where beat clarity is diminished.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Section 3.1 by providing specific implementation details, such as the processing of motion vectors and the definitions of $T_a$ and $T_u$. Additionally, elaborating on the importance of the kinematic amplitude indicator with a histogram would strengthen the discussion. We also suggest including qualitative samples of baseline models on the demo page to aid reader comprehension of synchronization improvements. Lastly, addressing how to handle instances of unclear beatness in motion or music would enhance the robustness of the methodology.