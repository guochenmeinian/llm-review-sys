ID: H0SoE2ch5l
Title: Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the impact of context quality and quantity on the performance of the Fusion-in-Decoder (FiD) model for retrieval-augmented generation. The authors propose a method to measure context quality based on the ratio of relevant to non-relevant passages. Key findings include that FiD training is overfit to context quality and that this overfitting is partly due to the model's increased attention to relevant passages when trained with low-quality context. The authors introduce a method to adapt cross-attention probability during inference, which outperforms FiD.

### Strengths and Weaknesses
Strengths:
- The research question and experimental designs are engaging and meaningful for future research.
- The proposed method is straightforward and easy to implement.
- The writing is clear, and figures and tables effectively support the content.

Weaknesses:
- Findings may be limited to the extractive open-domain QA tasks considered, necessitating more experiments for broader applicability.
- Some results, particularly regarding attention distributions, require further clarification and explanation.
- The novelty of introducing a temperature parameter may be limited, as existing hyper-parameters could serve similar purposes.

### Suggestions for Improvement
We recommend that the authors conduct additional experiments to validate their findings beyond the current tasks to avoid misleading conclusions. Clarifying the results in Table 2 and addressing the apparent contradictions in Figure 1 would enhance the paper's clarity. Additionally, we suggest that the authors provide a rationale for why the temperature parameter is superior to existing hyper-parameters and consider proposing a method for its automatic estimation without manual tuning.