ID: pNtG6NAmx0
Title: Statistical Knowledge Assessment for Large Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 7, 5, 4, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a statistical approach called KaRR to assess the factual knowledge contained in Generative Language Models (GLMs). The authors evaluate 14 GLMs using a large-scale assessment suite comprising 994,123 entities and 600 relations. Results indicate that KaRR correlates strongly with human assessments and exhibits lower variance across varying prompts. The paper also explores scaling laws of GLMs and the effects of tuning on instruction-following data. Additionally, an automatic evaluation metric is introduced to measure the robustness of factual knowledge in large language models (LLMs), demonstrating a higher correlation with human annotations than previous metrics.

### Strengths and Weaknesses
Strengths:
1. The proposed statistical approach is novel and effectively assesses factual knowledge in GLMs, considering various surface forms of subjects, objects, and relations.
2. The large-scale assessment suite is comprehensive, significantly larger than prior studies, enhancing the robustness of findings.
3. The automatic evaluation metric captures model performance consistency, crucial for evaluating LLMs, and shows strong invariance results compared to other baselines.

Weaknesses:
1. Some findings, such as scaling laws and tuning effects, have been previously reported, which may limit the novelty of the contributions.
2. The human correlation score of 0.43 for the automatic evaluation metric is relatively low, and the metric lacks interpretability.
3. The focus on entity-aware knowledge may overlook other significant forms of knowledge, such as numerical or temporal data, which could lead to incomplete assessments.

### Suggestions for Improvement
We recommend that the authors improve the interpretability of the automatic evaluation metric by calculating an oracle score and comparing it to the current score ratio. Additionally, exploring the impact of different threshold values on the metric's performance could enhance its generalizability. We suggest that the authors consider extending the KaRR approach to assess the generation of subjects, not just objects, to provide a more comprehensive evaluation of knowledge in GLMs. Furthermore, addressing the limitations of the current framework by discussing its applicability to more complex queries would strengthen the paper. Lastly, minor revisions to improve clarity in the presentation, such as correcting typographical errors and enhancing figure readability, would be beneficial.