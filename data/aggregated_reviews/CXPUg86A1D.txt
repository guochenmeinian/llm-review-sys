ID: CXPUg86A1D
Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Semantic Pyramid AutoEncoder (SPAE), which enables frozen Large Language Models (LLMs) to perform understanding and generation tasks involving non-linguistic modalities such as images and videos. SPAE converts raw pixels into lexical tokens from the LLM's vocabulary, facilitating multimodal tasks. The method is evaluated through in-context learning experiments with frozen PaLM 2 and GPT-3.5, showing success in image understanding and generation. Key contributions include the first successful use of a frozen language model for image content generation, a new SPAE tokenizer for interpretable representations, a progressive prompting method for long cross-modal sequences, and evaluation on visual tasks.

### Strengths and Weaknesses
Strengths:
- The method shows superior performance in image reconstruction compared to LQAE.
- It effectively utilizes in-context learning capabilities of large language models, providing an intriguing research direction.
- The approach does not require further tuning of the LLM, making it portable.
- The paper is well-written, with clear motivations and experiments.
- Impressive few-shot results on image classification.

Weaknesses:
- Experiments are limited to 128x128 resolution, lower than the mainstream 256x256.
- A larger latent space is necessary to achieve comparable performance with VQGAN.
- The necessity of the pyramid structure requires experimental validation.
- The VAE employs perceptual loss, unlike other methods; a strategy similar to ViT-VQGAN should be adopted.
- There is a lack of model variants in ablation experiments on understanding tasks.
- The 5-shot evaluation on ImageNet classification is somewhat unfair, as results for GPT-3.5 are not reported.
- Limited quantitative experiments to validate SPAE’s generalization to various tasks.

### Suggestions for Improvement
We recommend that the authors improve the resolution of experiments beyond 128x128 to align with mainstream standards. Additionally, consider exploring a larger latent space to enhance performance relative to VQGAN. We suggest providing experimental support for the necessity of the pyramid structure. The authors should adopt a training strategy similar to ViT-VQGAN that does not utilize perceptual loss. It would be beneficial to include more model variants in ablation experiments on understanding tasks. We encourage the authors to conduct further quantitative experiments to validate SPAE’s generalization across various tasks, such as captioning and visual question answering. Finally, we recommend addressing the fairness of the 5-shot evaluation by including results for GPT-3.5.