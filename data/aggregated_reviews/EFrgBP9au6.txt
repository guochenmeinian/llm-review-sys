ID: EFrgBP9au6
Title: Emergence of heavy tails in homogenized stochastic gradient descent
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the emergence of heavy tails in homogenized Stochastic Gradient Descent (SGD) applied to linear models with quadratic loss. The authors model SGD through homogenized SGD, demonstrating both theoretically and empirically that it converges to heavy-tailed distributions, despite the noise being Gaussian. The proof technique utilizes properties of Student-t-distributions and moment comparison results, allowing the authors to derive analytic upper and lower bounds for the tail index, which they validate with numerical experiments. Their findings suggest that Student-t-distributions may better describe SGD iterates than previously used stable distributions.

### Strengths and Weaknesses
Strengths:
- The original contribution of showing heavy tails arising from Gaussian noise is significant and may enhance understanding in machine learning.
- The derivation of analytical bounds on the tail index, computable from optimization hyperparameters, is a notable advancement.
- The authors confirm prior experimental findings regarding heavy tails in SGD and propose a shift towards using Student-t-distributions for modeling SGD noise.

Weaknesses:
- The absence of a conclusion limits the discussion of the paper's limitations and future work.
- The analysis is confined to linear regression with quadratic loss, which is not clearly stated in the title, abstract, or introduction, potentially overstating the paper's contributions.
- There is insufficient discussion on extending the analysis to more complex settings beyond linear regression, despite acknowledging the challenges in doing so.

### Suggestions for Improvement
We recommend that the authors improve the paper by adding a conclusion section to summarize findings and discuss limitations and future work. It is essential to clarify in the title, abstract, and introduction that the analysis is restricted to linear regression. Additionally, we suggest that the authors explore the potential for extending their analysis to non-linear models, such as a simple two-layer neural network, to enhance the paper's applicability. Furthermore, we encourage the authors to provide more explicit discussions on the conditions under which their results hold and to validate Theorem 3.4 with experimental data.