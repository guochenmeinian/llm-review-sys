ID: WBLPlszJI5
Title: Fine-Tuning Personalization in Federated Learning to Mitigate Adversarial Clients
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 5, 5, 7, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on fine-tuning personalization in federated learning (FL) to address the challenges posed by adversarial clients. The authors propose a method that leverages interpolation techniques, deriving a closed-form approximation of the interpolation parameter $\lambda$. The theoretical analysis considers data heterogeneity and adversarial client presence, aiming to characterize when full collaboration is detrimental compared to fine-tuned personalization.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a significant problem in personalized FL settings, providing a simple and applicable method to mitigate Byzantine adversaries, supported by rigorous theoretical proof.  
- The theoretical analysis is comprehensive, and the results correlating collaboration levels with tolerable adversary fractions are particularly valuable.  
- The authors derive a closed-form approximation of the interpolation parameter $\lambda$, which aids in the fine-tuning process.  

Weaknesses:  
- The practical applicability of the method is insufficiently demonstrated, with a need for more experiments on diverse datasets, Byzantine attack methods, and defense strategies.  
- The proposed fine-tuning strategy requires clients to broadcast their models and share gradients, raising efficiency and privacy concerns.  
- The experimental validations rely on simple simulated datasets and binary classification problems, limiting generalizability.  
- Some sections, such as Section 2, lack clarity, and certain assumptions are deemed strong or repetitive.  

### Suggestions for Improvement
We recommend that the authors improve the practical applicability of their method by conducting additional experiments on varied datasets and Byzantine attack methods. Furthermore, the authors should consider more complex data heterogeneity settings and models beyond binary classification to enhance generalizability. Additionally, addressing the efficiency and privacy implications of the fine-tuning strategy, particularly regarding client gradient sharing, is essential. Lastly, clarifying ambiguous sections and revisiting strong assumptions would strengthen the manuscript.