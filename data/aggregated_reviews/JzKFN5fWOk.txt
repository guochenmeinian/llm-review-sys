ID: JzKFN5fWOk
Title: D-CPT Law: Domain-specific Continual Pre-Training Scaling Law for Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a domain-specific scaling law (D-CPT Law) aimed at optimizing the mixture ratio between general and specific domain training corpora for continual pre-training of large language models. The authors conducted experiments across six downstream domains, demonstrating that D-CPT Law achieves high fitting accuracy and reduces training costs while maintaining performance. Additionally, the paper extends the D-CPT Law to cross-domain scenarios, providing practical applications for optimal mixture settings.

### Strengths and Weaknesses
Strengths:
1. The paper effectively proposes a method for determining the optimal mixture ratio between general and domain-specific corpora, supported by experimental evidence.
2. It explores three practical applications of D-CPT Law, enhancing its relevance to the community.
3. The analysis of the fitting function is comprehensive and well-justified.

Weaknesses:
1. The focus on the Qwen series models raises concerns about the generalizability of results to larger models.
2. The paper lacks a detailed design for continual pre-training, particularly regarding the final scaling function (Equation 6), which is not explicitly tailored for this context.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the rationale behind the choice of step parameters for validation loss and total training steps. Additionally, we suggest providing a theoretical justification for why L3 is the optimal parameterization and exploring the potential for other parameterizations. It would also be beneficial to include a comparison of the proposed laws with grid search over hyperparameters to quantify computational savings. Finally, we encourage the authors to clarify the specific design considerations for continual pre-training in relation to Equation 6.