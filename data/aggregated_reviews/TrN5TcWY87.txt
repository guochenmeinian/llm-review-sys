ID: TrN5TcWY87
Title: Inversion-based Latent Bayesian Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 5, 8, 7, 7, 3, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Inversion-based Latent Bayesian Optimization (InvBO), a novel approach designed to enhance latent Bayesian optimization (LBO) methods by utilizing auto-encoder-based neural networks to address the misalignment problem between latent and input spaces. The authors introduce two key components: 1) an inversion method that accurately reconstructs input samples, and 2) a potential-aware trust region anchor selection method that optimizes local search regions based on both observed function values and potential improvements. Empirical results demonstrate that InvBO significantly outperforms existing LBO methods across nine diverse tasks, including molecule design and symbolic regression, achieving a new state-of-the-art (SOTA) on the zaleplon_mpo problem. Theoretical proofs support its effectiveness in reducing surrogate model prediction errors.

### Strengths and Weaknesses
Strengths:
- The inversion method effectively resolves the misalignment issue in LBO without requiring additional function evaluations.
- The potential-aware trust region anchor selection method enhances optimization by considering both observed values and potential improvements, expanding upon previous methods.
- Theoretical justification for the inversion method is provided, reinforcing its validity.
- Extensive experiments validate InvBO's performance, showing improvements over strong LBO baselines and achieving impressive results on sample efficiency.
- The writing is clear, and the supplementary materials, including figures and tables, are well-structured.

Weaknesses:
- The paper lacks a thorough discussion on the limitations and practical considerations of LBO, such as data requirements for effective VAE training and trade-offs compared to standard BO.
- The motivation for the work is weak, lacking thorough analysis of the misalignment problem.
- Proposition 1 relies on assumptions that may not be realistic; a discussion on the implications of violating these assumptions is needed.
- The potential-aware anchor selection method lacks theoretical grounding, which could enhance its contribution.
- Experimental results are domain-specific, raising concerns about the generalizability of the proposed method to unseen domains without re-training.
- There are unresolved questions regarding the Lipschitz assumption for arbitrary black-box objectives and the appropriateness of the term "inversion."
- The improvements observed in certain tasks appear smaller than in others, warranting further explanation.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of LBO and InvBO, particularly regarding the dataset size required for effective VAE training and the implications of small datasets. Additionally, we suggest enhancing the motivation section by providing a more robust analysis of the misalignment problem. A more detailed exploration of the assumptions in Proposition 1 and their practical implications would strengthen the paper. We encourage the authors to provide theoretical insights into the potential-aware anchor selection method, possibly relating it to local optima quality or regret bounds. Furthermore, implementing and validating INV and PAS on LBOs such as TuRBO-L and LOL-BO would enhance the robustness of the findings. Clarifying the dimensionality of the latent space and its impact on performance would also strengthen the paper. Lastly, we suggest that the authors elaborate on the inversion mechanism to resolve the confusion surrounding its terminology and its relationship to existing methods like GAN inversion, and including a comparison with the Genetic Expert-Guided Learning (GEGL) method could provide valuable context for the performance of InvBO in molecular design tasks.