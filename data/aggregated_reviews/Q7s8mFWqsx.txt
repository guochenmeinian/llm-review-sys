ID: Q7s8mFWqsx
Title: Learning an Actionable Discrete Diffusion Policy via Large-Scale Actionless Video Pre-Training
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach that leverages human-object interaction videos to enhance robotic policy learning through a three-step process: 1) learning a common video tokenizer for both human and robot videos, 2) employing a discrete-diffusion model for video token denoising across both datasets, and 3) applying a discrete-diffusion model for action denoising specifically on robot data. The authors demonstrate that this technique yields superior performance compared to various baselines.

### Strengths and Weaknesses
Strengths:
1. The proposed idea is compelling, integrating concepts from self-supervised pre-training and diffusion policies.
2. The authors achieve strong performance on manipulation benchmarks, surpassing many existing baselines.
3. The methods are clearly articulated and easy to understand.

Weaknesses:
1. The paper asserts that pre-training on human videos aids robot action learning, yet the experiments do not clarify the contribution of human videos to policy performance, especially given the data disparity between models trained with and without them.
2. The visual differences between Ego4D videos and the target tasks raise questions about the effectiveness of the video tokenizer's shared codebook between human and robot datasets.
3. The challenge of generating egocentric videos is acknowledged, but the paper lacks insights into the quality of generated videos and their correlation with downstream policy performance.
4. The absence of a baseline comparison with methods like VideoMAE limits the evaluation of the proposed tokenizer-based diffusion model.

### Suggestions for Improvement
We recommend that the authors improve clarity on the contribution of human videos to policy performance by conducting additional experiments with varying amounts of robot trajectories. Additionally, we suggest including a comparison against generalist imitation learning methods such as Octo and RT-1 to contextualize the results better. It would also be beneficial to explore the performance of the model on unseen tasks and to evaluate off-the-shelf pre-trained video generation models like Stable Video Diffusion for future exploration. Lastly, providing a scaling curve that illustrates the relationship between the amount of human data and performance would enhance the understanding of the model's capabilities.