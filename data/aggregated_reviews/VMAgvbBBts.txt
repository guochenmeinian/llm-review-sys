ID: VMAgvbBBts
Title: UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 5, 6, 5, 5, 5, -1, -1, -1
Original Confidences: 5, 5, 4, 2, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an unsupervised approach for data pre-selection, leveraging the joint feature space of vision and text through the BLIP-2 model. The authors propose using learnable prompts to enhance representation and achieve a diverse cluster structure for effective instance selection. The method employs instance-level and cluster-level contrastive losses, demonstrating its efficacy on seven benchmark datasets.

### Strengths and Weaknesses
Strengths:
- The integration of vision-language models for data pre-selection is innovative and leverages multimodal features effectively.
- The approach promotes diverse and clustered representations, addressing limitations in existing models through prompt learning.
- The experimental results show superior performance compared to state-of-the-art methods, indicating the method's generalizability.

Weaknesses:
- The lack of a CLIP baseline limits the comparative analysis, as including it would provide a fundamental reference point.
- The scope of experiments is narrow, focusing solely on image classification without exploring other downstream tasks like detection or segmentation.
- The rationale for certain experimental choices, such as the arbitrary annotation of 200 images per benchmark, is unclear and needs justification.
- The paper lacks comprehensive ablation studies, particularly regarding the impact of instance-level and cluster-level contrastive losses.
- There are several grammatical issues and unclear statements that detract from the overall clarity of the paper.

### Suggestions for Improvement
We recommend that the authors improve the comparative analysis by including a CLIP baseline to enhance the evaluation of their approach. Additionally, conducting experiments on a broader range of downstream tasks, such as detection and segmentation, would strengthen the paper's claims about generalizability. The authors should clarify the rationale behind the annotation of 200 images per benchmark and consider varying this number to analyze its impact on model performance. Furthermore, we suggest conducting more comprehensive ablation studies to analyze the effects of different components of the proposed method. Lastly, revising the paper for grammatical accuracy and clarity would improve its presentation.