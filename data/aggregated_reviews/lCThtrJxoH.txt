ID: lCThtrJxoH
Title: Team-PSRO for Learning Approximate TMECor in Large Team Games via Cooperative Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 5, 6, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents algorithms “Team PSRO” and “Team PSRO Mix-and-Match” for solving zero-sum two-team games, extending the PSRO framework. Team-PSRO is guaranteed to converge to a TMECor, while Team-PSRO Mix-and-Match enhances population policies. Additionally, the authors introduce a novel algorithm, Team PSRO-MM Top-K, which selects the top k strongest opponents to improve policy performance in cooperative reinforcement learning (RL). The authors argue that their methods outperform existing baselines, including fictitious team play and PFSP, and they acknowledge the need for a more detailed discussion on TME and TMECor to cater to a diverse audience. They assert that their approach effectively reduces the problem of finding TMECor in large games to a cooperative RL problem, emphasizing that the choice of cooperative RL algorithm (e.g., MAPPO vs. QMIX) does not fundamentally alter the method's applicability. The authors evaluate their methods on Kuhn Poker, Liar's Dice, and Google Research Football, demonstrating improved performance over self-play.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written and well-organized, with effective use of notation that is colorblind-friendly.
- The theoretical claims regarding convergence to TMECor are supported, and the experimental results show promising performance.
- The introduction of Team PSRO-MM Top-K demonstrates empirical superiority over established baselines.
- The authors provide a thorough discussion of their results, including algorithmic iterations and walltime, and are receptive to feedback, showing commitment to enhancing the manuscript.

Weaknesses:
- The novelty of the proposed algorithms is marginal, with Team-PSRO being a straightforward application of PSRO to two-team games, and there are concerns regarding the novelty of Team PSRO-MM Top-K in relation to existing work by Liu et al.
- The related work section includes tangential references, and more relevant comparisons are needed in the experiments.
- The authors' claims about independent RL performing worse than MAPPO lack clarity and may require further justification.
- The presentation of results, particularly in Figure 2, lacks clarity and sufficient statistical analysis, and there is insufficient discussion of baseline methods, particularly for small games and larger environments like Google Research Football.
- The paper does not sufficiently address how Team PSRO compares to game abstraction methods, which could impact the perceived significance of the findings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the related work section by focusing on more relevant studies and explicitly differentiating Team PSRO from existing methods, particularly in relation to Liu et al.'s work. Additionally, we suggest providing clearer descriptions of the algorithms, particularly the function P in Team DO-MM and the derivation of NE in TEAM-PESO. To enhance the novelty of the work, consider developing a more sophisticated method for mixing policies in Team-PSRO-MM. Including more baseline comparisons, such as NFSP and CFR for smaller games, and PSRO with behavioral diversity for larger games, would strengthen the evaluation. Furthermore, we advise addressing the clarity of the results in Figure 2 by providing detailed statistical analyses and explanations for the error bars and comparisons made. Finally, including comprehensive details on the statistical tests in the final manuscript will strengthen the validation of their claims and enhance the understanding of Team PSRO's impact compared to game abstraction methods.