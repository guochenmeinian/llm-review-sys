ID: uv3ge0goPa
Title: Training Your Image Restoration Network Better with  Random Weight Network as Optimization Function
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 8, 8, 4, 7, -1, -1, -1
Original Confidences: 5, 5, 5, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach by utilizing random weights networks as loss functions for image restoration tasks, inspired by mathematical manifolds. The authors propose several techniques, including Taylorâ€™s Unfolding Network and Invertible Neural Network, and validate their effectiveness through extensive experiments across various image restoration tasks. The results indicate that the proposed loss functions can enhance performance while maintaining the original model configuration.

### Strengths and Weaknesses
Strengths:
1. The authors propose an innovative concept of using random weights networks as loss functions, offering a plug-and-play solution that leads to significant performance improvements without complex network designs.
2. The design is grounded in rigorous mathematical principles, enhancing its theoretical foundation and enriching the manifold representation.
3. Comprehensive experiments, including ablation studies and motivation analysis, provide convincing evidence of the method's effectiveness.

Weaknesses:
1. The paper contains a major technical flaw regarding the claim that the proposed loss functions do not incur additional training computational costs, which is unreasonable as they require extra computation for gradients.
2. Quantitative improvements are limited, with minimal impact on PSNR and SSIM results, raising concerns about convergence and comparison validity.
3. The absence of visual results in the main paper is a significant oversight for an image restoration study, and supplementary materials show negligible differences.
4. The evaluation scope is narrow, lacking exploration of the proposed loss functions' applicability to more general image restoration tasks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental settings by detailing methodologies and procedures. Additionally, highlighting the best results in tables and providing more visual comparisons would enhance the presentation. The authors should also address the computational costs associated with the proposed loss functions and ensure that all experiments are conducted to convergence. Expanding the evaluation to include a wider range of image restoration tasks and providing qualitative results alongside input and ground truth images would strengthen the paper. Finally, discussing the impact of initialization distribution and the implications of using various network architectures during optimization is necessary for a more comprehensive understanding of the approach.