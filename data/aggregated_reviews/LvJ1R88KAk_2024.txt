ID: LvJ1R88KAk
Title: Demystify Mamba in Vision: A Linear Attention Perspective
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive analysis of the factors contributing to the success of the S6 module in the Mamba model and introduces a new linear attention vision network, MLLA, inspired by S6's design. The authors conclude that Mamba is a variant of linear attention, detailing six key distinctions and demonstrating that MLLA outperforms existing Mamba-based models in various tasks while maintaining efficient computation and inference speed.

### Strengths and Weaknesses
Strengths:  
- The motivation and analysis are insightful, providing a profound examination of the mathematical relationships between Mamba and linear attention.  
- The paper effectively connects linear attention and Mamba, showcasing the utility of the proposed modules through clear experimental results.  
- MLLA is positioned as a generic vision backbone network that surpasses recent Mamba-based models.  

Weaknesses:  
- The analysis of single-head attention is inaccurate and lacks clarity, as it misinterprets S6 as a single-head module without sufficient evidence.  
- The comparison on the ImageNet-1K dataset is unfair due to the use of MESA as an optimizer, which is not applied to other methods in Table 3.  
- The rationale for using Swin as a baseline is questionable, particularly regarding its window-based linear attention.  
- The setting of d_state in the SSM block design is unclear.  
- There are insufficient experiments on downstream tasks, particularly in semantic segmentation and object detection.  
- The comparison in Table 6 is unfair due to differing block settings.  
- Minor typographical errors exist, such as 'SMM' instead of 'SSM'.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the analysis regarding single-head attention by providing evidence of the number of attention matrices generated by S6. Additionally, ensure that training strategies are consistent across all methods compared, particularly regarding the use of MESA. The authors should clarify the motivation for using Swin as a baseline and provide specific configurations of the baseline model. It is essential to clarify the setting of d_state in the SSM block design. We also suggest including more comprehensive experiments on downstream tasks, ensuring that comparisons are fair and consistent across all models. Finally, rectify the typographical errors noted in the manuscript.