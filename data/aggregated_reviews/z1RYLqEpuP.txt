ID: z1RYLqEpuP
Title: Evaluating and Modeling Attribution for Cross-Lingual Question Answering
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the first study of attribution for question answering in a cross-lingual framework, defining two scenarios: in-language and in-English attribution. The authors annotate approximately 10,000 examples across five languages and evaluate the attribution of CORA, a state-of-the-art cross-lingual QA system, revealing that a significant portion (7%-47%) of answers are not attributable to either in-language or cross-language passages. They demonstrate that PaLM 2 and NLI models can accurately detect attribution in this setting, achieving over 90% accuracy across all languages. Additionally, they show that using their attribution detection model as a reranker improves attribution by an average of +55% compared to a model without reranking.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel setup in cross-lingual question answering and presents a new dataset for evidence evaluation.
- It addresses significant issues in attribution within existing QA systems and proposes effective solutions that enhance performance.
- The study is well-written and easy to read, with a careful experimental setup.

Weaknesses:
- Attribution evaluation has only been conducted for one model, which, while popular, is not the best currently available.
- The framing of attribution detection as a binary classification problem lacks clarity compared to the ranking provided by the retriever.
- The domain is limited to Wikipedia articles, with insufficient discussion on extending the work to other domains.
- There is a lack of detail regarding compute costs and training specifics, and the limitations section is underdeveloped.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of their work, particularly regarding the narrow domain of Wikipedia articles and the implications of their findings. Additionally, we suggest including compute costs and training details to enhance reproducibility. The authors should also consider addressing the differences in data used to finetune mT5â€“QA and mT5-NLI, exploring whether combining QA and NLI data could improve performance. Furthermore, we encourage the authors to provide intuition on why in-context learning may not be effective for the attribution task, as well as to clarify why T5-NLI outperforms mT5-NLI in most cases.