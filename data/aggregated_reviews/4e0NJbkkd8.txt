ID: 4e0NJbkkd8
Title: Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents A-Crab, an offline reinforcement learning algorithm derived from ATAC, which incorporates a modified loss function for the Q-function by utilizing the importance-weighted Bellman error instead of the square Bellman error. A-Crab effectively addresses disadvantages of ATAC, such as alleviating the visitation coverage assumption, enhancing the suboptimality rate, and simplifying the minimax optimization process. Theoretical findings indicate that A-Crab demonstrates superior advantages compared to ATAC, achieving optimal statistical rates with weaker assumptions.

### Strengths and Weaknesses
Strengths:
1. The paper is well-organized, featuring a comparative table of provable offline RL algorithms that highlights A-Crab's distinctions and advantages over ATAC.
2. A-Crab's incorporation of the importance sampling ratio leads to significant improvements, including relaxation of the visitation coverage assumption, enhancement of the suboptimal rate, and simplification of the minimax problem.

Weaknesses:
1. The lack of empirical results diminishes the paper's impact, as a comparison of A-Crab's performance with ATAC would enhance its credibility.
2. The potential for instability in A-Crab due to large importance sampling ratios in offline RL scenarios raises concerns about its practical applicability.

### Suggestions for Improvement
We recommend that the authors improve the paper by including empirical results on D4RL datasets that compare the performance of A-Crab with that of ATAC. Addressing the issue of instability caused by large importance sampling ratios would also strengthen the paper's credibility. Additionally, clarifying the assumptions regarding the finiteness of the action space, policy space, and other related aspects would enhance the overall robustness of the work.