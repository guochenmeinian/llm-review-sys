ID: cA9gLXFaRo
Title: Instruction-Guided Visual Masking
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 6, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Instruction-guided Visual Masking (IVM), a versatile visual grounding method aimed at enhancing multimodal instruction following tasks. By masking instruction-irrelevant image regions, IVM improves the visual grounding ability of downstream large multimodal models (LMMs), achieving state-of-the-art results across various benchmarks. The authors also introduce a visual masking data generation pipeline and a new learning technique, Discriminator Weighted Supervised Learning (DWSL), to prioritize high-quality data samples.

### Strengths and Weaknesses
Strengths:
- The motivation behind IVM is commendable, addressing a significant problem with effective illustrations and substantial experimental validation.
- The IVM model enhances multimodal models' sensitivity to instructions and focuses on task-relevant image regions.
- The paper presents a well-developed mixture of experts (MoE) pipeline for generating reliable pixel-level labels.

Weaknesses:
- The routine of retraining models with new datasets is somewhat outdated, and recent literature on similar grounding datasets is not cited.
- The technical insights are relatively weak, lacking strong novelty in proposed methods.
- The IVM model architecture in Figure 6 is unclear, and comparisons with state-of-the-art visual grounding methods are missing.
- Some writing inconsistencies and rudimentary errors, such as inconsistent terminology, need to be addressed.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the dataset's motivation by citing recent works like Ferret and Kosmos-2. Additionally, clarify the abnormal increase in the instruction-related area in Figure 5 and consider cleaning the data. The authors should enhance the explanations of Formulas 1 and 2, drawing parallels to pseudo-label curriculum learning as seen in CLIP-VG. Modifications to Figure 6 are necessary to accurately depict the discriminator and generator processes, and a framework diagram illustrating how IVM assists in inference should be included. Finally, we suggest uniformity in terminology, such as consistently using "LoRA" and "LLM," and addressing the paper's performance on the RefCOCO/+/g dataset after incorporating IVM.