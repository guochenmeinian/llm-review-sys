ID: snxWD0Q4EI
Title: The Iterative Optimal Brain Surgeon: Faster Sparse Recovery by Leveraging Second-Order Information
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 6, 6, 7, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretically convergent iterative Optimal Brain Surgeon (I-OBS) algorithm that generalizes classic Iterative Hard Thresholding (IHT) algorithms by incorporating approximate second-order information in the sparse projection step. The authors provide practical variants for sparse linear regression and model pruning, demonstrating that the proposed algorithm achieves faster convergence and improved accuracy compared to traditional methods. The work rigorously analyzes I-OBS, highlighting its theoretical guarantees and practical applicability in post-training iterative pruning.

### Strengths and Weaknesses
Strengths:
- The proposed algorithm is simple yet effective, with theoretical guarantees absent in prior methods.
- The authors provide a rigorous analysis, including detailed derivations and proofs, and contextualize their contributions within existing literature.
- The practical formulation yields improved performance across diverse data domains, and I-OBS converges in fewer iterations compared to one-shot pruning methods.

Weaknesses:
- Theoretical analysis does not generalize well to practical problems, and the differences between theoretical and practical schemes remain unclear.
- The paper lacks grounding in practical considerations, such as overall runtime characteristics and comparisons with recent pruning methods.
- Limited experiments on fine-grained sparsity and reliance on perplexity as a metric for LLMs may mislead evaluations of compressed models.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis by clarifying the gap between the theoretical and practical schemes, potentially through experimental verification or illustrative examples. Additionally, conducting experiments with fine-grained sparsity types, such as 2:4, would enhance the practical applicability of I-OBS. It would be beneficial to evaluate LLM experiments on downstream tasks like GLUE or the LLM-KICK benchmark instead of relying solely on perplexity. We also suggest comparing I-OBS with a broader range of existing pruning methods to strengthen the significance of the work. Lastly, addressing minor typographical errors and using vector graphic formats for figures would improve the presentation quality.