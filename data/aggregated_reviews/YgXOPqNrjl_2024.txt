ID: YgXOPqNrjl
Title: Swin-LiteMedSAM: A Lightweight Box-Based Segment Anything Model for Large-Scale Medical Image Datasets
Conference: thecvf
Year: 2024
Number of Reviews: 4
Original Ratings: 10, 8, 7, 9
Original Confidences: 5, 4, 5, 4

Aggregated Review:
**Key Points**  
We are impressed by the use of the Swin transformer for accelerating inference and achieving high accuracy through appropriate training protocols. However, we note the need for more detailed information regarding the dataset sources to enhance clarity and replicability. Additionally, we identify several missing elements such as the author's ORCID, an unnumbered equation on page 6, and the absence of a complete code link.

**Strengths and Weaknesses**  
The strengths of the paper include the innovative application of the Swin transformer for efficiency and the integration of point and scribble prompts to aid model training and inference. However, weaknesses include the lack of detailed dataset information, missing author identification, and incomplete documentation of the code and equations. Furthermore, we observe that while the authors claim the Swin transformer is designed for efficiency, it performs slower than the ViT encoder for certain modalities, which requires clarification.

**Suggestions for Improvement**  
We suggest providing a comprehensive overview of the dataset sources to improve clarity and replicability. Additionally, we recommend including the author's ORCID, numbering the equation on page 6, and providing a complete code link. We also advise adding more details about the network modules in the figure captions, explaining the slower performance of the Swin transformer compared to the ViT encoder in Table 8, and considering the use of OpenVINO to enhance model inference speed. Lastly, detailing how the weights of the prompt encoder and mask decoder are managed during the second stage of model training would be beneficial.