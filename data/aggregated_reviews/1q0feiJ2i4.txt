ID: 1q0feiJ2i4
Title: Large Language Models are Visual Reasoning Coordinators
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach called Cola, which coordinates multiple vision-language models (VLMs) for visual reasoning tasks using a language model (LM). The authors propose two variants: Cola-FT, which fine-tunes the models, and Cola-Zero, which utilizes in-context learning. The method achieves state-of-the-art results across various visual reasoning benchmarks, including VQA v2, A-OKVQA, OK-VQA, e-SNLI-VE, and VSR. The analysis demonstrates how zero-shot and fine-tuned language models can effectively coordinate VLM outputs.

### Strengths and Weaknesses
Strengths:
1. The proposed method is novel, leveraging zero-shot VLMs and zero-shot/fine-tuned LLMs as coordinators.
2. The performance is significant, supported by extensive experimental results and qualitative examples.
3. The paper is well-written, with clear explanations and effective visualizations.

Weaknesses:
1. The method's performance varies across datasets, necessitating a deeper analysis of why it excels in some cases but not others, such as the comparison with 'Ensemble' on VQA v2 and OK-VQA.
2. The authors need to provide more discussion on why their method outperforms others, particularly in comparison to BLIP-2 on VQA.
3. The paper lacks results on more complex datasets like CLEVR and GQA, which could enhance the evaluation of compositional reasoning capabilities.

### Suggestions for Improvement
We recommend that the authors improve the analysis of performance variations across different datasets to clarify the strengths and weaknesses of the proposed method. Additionally, we suggest including comparisons with prior work, such as Visual Programming: Compositional visual reasoning without training, to contextualize their contributions. Furthermore, the authors should provide more details on computational resources used and the rationale behind the selection of datasets, particularly why more complex reasoning datasets were excluded.