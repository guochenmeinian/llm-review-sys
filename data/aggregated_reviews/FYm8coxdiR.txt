ID: FYm8coxdiR
Title: CLIP in Mirror: Disentangling text from visual images through reflection
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 3, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MirrorCLIP, a zero-shot framework designed to address typographic attacks by disentangling visual and textual representations. The authors leverage the observation that visual models struggle with text semantics in mirrored images, using both original and flipped images to create disentangling masks that enhance representation quality. Additionally, the paper proposes a method for image-to-image generation utilizing disentangled textual embeddings, contrasting it with a previous text-to-image generation method. The authors argue that their approach achieves semantic-level disentanglement, as evidenced by the images generated, which contain semantically relevant visual components rather than typography. They emphasize that the previous method only demonstrates format-level disentanglement, controlling visual or textual components within the same semantic context without a second semantic input. Experimental results indicate improvements in disentangling effectiveness, but raise questions about the framework's true capability in achieving semantic-level disentanglement.

### Strengths and Weaknesses
Strengths:
1. The concept behind MirrorCLIP is clear and straightforward, making the paper easy to follow.
2. The training-free methodology effectively tackles typographic attacks, demonstrating robustness without additional training.
3. The authors effectively demonstrate the semantic-level disentanglement of their method through comparative analysis with previous work.
4. The generated images using disentangled textual embeddings show improved quality over those produced by the "forget to spell" model.

Weaknesses:
1. The framework may be easily circumvented, as overlaying text with its mirrored version could compromise disentanglement.
2. The hypothesis regarding mirrored texts being difficult for visual models is overly generalized, with exceptions like palindromic words.
3. Experimental results, particularly in Figure 7, raise concerns about whether MirrorCLIP genuinely disentangles semantics or merely separates visual features.
4. Detailed results for ablation experiments involving flipped text images are lacking.
5. The authors acknowledge that their method is more susceptible to noise interference when dealing with textual embeddings of typography, affecting the quality of generated images.

### Suggestions for Improvement
We recommend that the authors improve the robustness of MirrorCLIP by addressing potential circumvention strategies, such as testing with images containing both original and mirrored text. Additionally, we suggest providing a more nuanced discussion on the generalizability of the disentangled visual semantics to other tasks beyond classification. Clarifying the mathematical notations used in the manuscript and elaborating on the practical applications of the disentanglement framework in the conclusions would enhance the paper's clarity and impact. Furthermore, we recommend improving the robustness of the model against noise interference, particularly when handling textual embeddings of typography. Finally, further clarification on the differences in optimization between their model and that of previous work could enhance the understanding of their contributions.