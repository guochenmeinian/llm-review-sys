ID: MXRy6bYBfB
Title: EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model
Conference: AAAI
Year: 2024
Number of Reviews: 3
Original Ratings: 9, 7, 7
Original Confidences: 4, 3, 4

Aggregated Review:
### Key Points
This paper presents EEGFormer, a self-supervised learning model for EEG data analysis aimed at enhancing transferability and interpretability. The authors propose a new pretraining method where EEG signals are segmented into patches, processed through a Transformer encoder, and converted into discrete indices via a vector-quantized model before being reconstructed by a Transformer decoder. The authors demonstrate EEGFormer’s application across five downstream tasks, reporting strong performance and promising interpretability of the learned representations.

### Strengths and Weaknesses
Strengths:
- The self-supervised approach has the potential to efficiently utilize vast amounts of raw EEG data.
- Encoding EEG signals into quantized vectors may facilitate the learning of interpretable representations, as evidenced by experimental results.
- Empirical evaluations indicate good performance on all tested downstream tasks, showcasing transferability.

Weaknesses:
- The concept of leveraging self-supervised learning for EEG data lacks novelty; the paper must clarify how EEGFormer improves upon existing models like BrainBERT or SeqCLR.
- The claims regarding interpretability are not substantiated by a comprehensive framework or quantitative measures, and the paper should include case studies or expert analysis comparisons.
- The finetuning paradigm is not clearly defined, and the authors do not provide sufficient discussion on data splitting and common practices.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Figure 1 by emphasizing subfigures a and b. The finetuning paradigm should be clearly defined, including how weight updates were constrained during end-to-end fine-tuning to avoid misconceptions. Additionally, the authors should present performance metrics on the Neonate dataset to illustrate differences between supervised and self-supervised training. A more thorough discussion of results, particularly regarding Figure 3 and the naïve Bayes model interpretation, is necessary to enhance understanding. Finally, the authors need to specify the datasets used for pre-training, fine-tuning, and testing to prevent any data leakage. For simplicity, we suggest presenting only the EEGFormer variant in Table 1, as the other variants show similar performance.