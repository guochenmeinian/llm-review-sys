ID: rwcTxeSsVI
Title: For Generated Text, Is NLI-Neutral Text the Best Text?
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the relationship between redundancy and non-concordant statements, focusing on different sampling strategies and decoder scores. The authors propose a rejection mechanism based on NLI predictions, demonstrating through small-scale human evaluation that it reduces errors in generated text. The study also examines the impact of varying p-values in nucleus sampling, revealing that high probabilities lead to contradictions while low probabilities result in redundancy. The findings suggest that neutral NLI relations often indicate high-quality text, while contradiction and entailment relations may signal quality under specific sampling strategies.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written and well-placed within existing literature, with a solid experimental design.
- It introduces an interesting approach by using NLI to guide language generation, showing significant correlations between NLI relations and text quality.
- The study provides sufficient support for its claims and is reproducible.

Weaknesses:
- The novelty of the work is questionable, as many ideas could be synthesized from existing literature.
- The reliability of the findings is undermined by the limited evaluation of only 50 samples.
- Concerns exist regarding the generalizability of the method and the reliability of the NLI classifier, particularly in categorizing examples into the three NLI relations.

### Suggestions for Improvement
We recommend that the authors improve the generalizability discussion by addressing the limitations of the chosen dataset (SCARECROW) and the sufficiency of the three NLI relation categories. Additionally, providing a performance comparison between using NLI and not using it in a table or figure would enhance clarity. To address reliability concerns, the authors should include details on the number of examples that cannot be categorized into any of the three NLI categories and the rate of misclassification as 'neutral.' Finally, we suggest making the title more concrete while retaining brevity.