ID: nRRJsDahEg
Title: Towards a "Universal Translator" for Neural Dynamics at Single-Cell, Single-Spike Resolution
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a multi-task-masking (MtM) approach based on a self-supervised Transformer for learning neural spiking data. The model, evaluated on the International Brain Laboratory dataset, shows improvements in tasks such as single-neuron and region-level activity prediction, forward prediction, and behavior decoding. The authors propose that this method enhances performance and generalization, contributing to a foundation model for understanding brain dynamics.

### Strengths and Weaknesses
Strengths:
1. The work demonstrates deep thinking about foundation models in neuroscience.
2. The model significantly enhances generalization by modeling across different brain areas, individuals, and sessions.
3. The MtM strategy effectively improves the capabilities of the Transformer model.

Weaknesses:
1. There is a lack of qualitative analysis regarding neural dynamics from population activity across individuals, such as visualizing features output by NDT.
2. Insufficient ablation studies are present, particularly regarding the differences between training on multi-region versus single-region data and the performance of multi-session data from a single individual versus cross-individual data.
3. The architecture of the Transformer is not described in detail, and the bit-per-spike metric is the only performance metric reported, making it difficult to evaluate real-world performance.

### Suggestions for Improvement
We recommend that the authors improve the qualitative analysis of neural dynamics by visualizing features output by NDT to identify patterns between single-neuron and population-level activities. Additionally, we suggest conducting more comprehensive ablation studies to clarify the effects of multi-region versus single-region training and the implications of using multi-session data. The authors should provide a more detailed description of the Transformer architecture, including visualizations, and consider reporting additional performance metrics alongside the bit-per-spike measure to enhance interpretability.