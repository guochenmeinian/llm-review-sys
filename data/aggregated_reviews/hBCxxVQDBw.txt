ID: hBCxxVQDBw
Title: Towards Scalable and Stable Parallelization of Nonlinear RNNs
Conference: NeurIPS
Year: 2024
Number of Reviews: 20
Original Ratings: 6, 6, 7, 5, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents improvements to the DEER method for parallelizing the evaluation of nonlinear Recurrent Neural Networks (RNNs). The authors propose using quasi-Newton methods to address the intractability of DEER, enhancing computational efficiency and stability through the introduction of damping via Kalman smoothing. The empirical results indicate superior performance in terms of speed and memory usage. Additionally, the paper discusses the performance of DEER in the context of the Lorenz task, highlighting its faster convergence on wallclock time compared to other methods. The authors clarify that DEER is a specific instance of IKE with the hyperparameter $\lambda=0$ and provide guidance on how to set this hyperparameter robustly.

### Strengths and Weaknesses
Strengths:
- The integration of quasi-Newton methods offers a novel approach to parallelizing nonlinear RNNs, effectively reducing computational complexity.
- The paper is well-structured and presents its methods clearly, making it accessible.
- The proposed techniques are empirically validated, demonstrating significant reductions in memory usage and computational time.
- The paper effectively demonstrates the advantages of DEER in terms of convergence speed and provides a clear explanation of its relationship to IKE.

Weaknesses:
- The empirical validation is limited to specific architectures and datasets, which may not fully represent the methods' applicability.
- The complexity of implementing quasi-Newton and trust-region methods could hinder widespread adoption without additional tools or simplified frameworks.
- Some contributions, while novel, rely on well-known techniques, which may weaken the perceived novelty of the work.
- The review lacks specific critiques or suggestions for further improvement, indicating a need for more detailed analysis.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation by testing a wider range of nonlinear RNN architectures and diverse datasets to better understand the methods' applicability and limitations. Additionally, we suggest providing more detailed experimental information regarding the implementation of parallelism and the types of parallel computing used. Clarifying the relationship between the proposed methods and the concept of parallelism in RNNs, particularly in relation to sequence length, would enhance the paper's clarity. Furthermore, we recommend improving the clarity of the hyperparameter setting process by providing more explicit examples or case studies. Finally, a more comprehensive discussion on the choice and impact of hyperparameters, such as the damping factor in IKE, would strengthen the manuscript and enhance the implications of using DEER in various contexts.