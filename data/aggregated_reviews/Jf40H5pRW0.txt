ID: Jf40H5pRW0
Title: Open LLMs are Necessary for Current Private Adaptations and Outperform their Closed Alternatives
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 7, 4, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper critiques the adaptation of closed-source LLMs to private data, arguing that such techniques are potentially unsafe and yield lower accuracy compared to open LLMs. The authors propose that open LLMs are preferable in sensitive fields due to better performance, lower costs, and enhanced privacy protection. Additionally, the paper explores privacy-preserving methods for text generation tasks, specifically focusing on the application of differential privacy (DP) to both model fine-tuning and prompt tuning. The authors present novel methods that demonstrate comparable performance between private prompt tuning and private fine-tuning, challenging prior assumptions about their effectiveness. A membership inference attack is included as a valuable contribution, although the authors are encouraged to expand the range of privacy attacks examined. The paper provides a thorough comparison of various adaptation techniques, highlighting the importance of considering privacy and performance holistically.

### Strengths and Weaknesses
Strengths:
- The paper provides a comprehensive overview of privacy-preserving adaptation techniques for LLMs, with extensive experiments comparing closed and open-source models.
- It clearly articulates the motivations behind the research and presents a well-structured argument for the superiority of open LLMs in terms of performance and privacy.
- The paper is well-written and demonstrates thorough experimentation, with an interesting extension of methods to the generation task.
- The inclusion of a membership inference attack adds value to the discussion of privacy levels.

Weaknesses:
- The work lacks methodological novelty, primarily extending existing methods without introducing new techniques.
- The novelty of the findings may not be fully articulated, as some conclusions align with general expectations.
- There is insufficient investigation into privacy levels beyond $\varepsilon=8$, and the justification for this choice is not clearly articulated.
- The comparison methodology between fine-tuning and prompt tuning under DP may lack fairness due to inherent differences.
- The comparison between closed and open LLMs lacks proper baselines, as zero-shot performance metrics are not provided, which could mislead readers regarding privacy guarantees.
- The exploration of additional types of attacks is limited, potentially reducing the comprehensiveness of the privacy analysis.

### Suggestions for Improvement
We recommend that the authors improve the investigation of privacy levels by exploring values beyond $\varepsilon=8$ and providing a clearer justification for their choice of this parameter. Additionally, incorporating dedicated instruction fine-tuning tasks would enhance the relevance of the findings. The authors should also address the lack of baseline comparisons by including zero-shot performance metrics for the models evaluated. Furthermore, we suggest discussing the implications of privacy against the LLM provider in more detail, including potential attacks such as membership inference and reconstruction attacks. Finally, we recommend that the authors improve the articulation of the novelty of their approach, particularly in how it contributes to the field beyond intuitive expectations, and refine the DP comparison methodology to ensure a fair assessment between model fine-tuning and prompt tuning. Expanding the range of privacy attacks examined would provide a more comprehensive view of privacy levels.