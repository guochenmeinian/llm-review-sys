ID: 9dp35y5C0p
Title: Reinforcement-Enhanced Autoregressive Feature Transformation: Gradient-steered Search in Continuous Space for Postfix Expressions
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: 6, 7, 7, 7
Original Confidences: 4, 5, 4, 5

Aggregated Review:
### Key Points
This paper presents a novel automated feature transformation framework that reformulates the feature transformation problem as a continuous optimization task, utilizing a reinforcement-enhanced autoregressive approach for gradient-steered search. The method involves four steps: reinforcement-enhanced data preparation, feature transformation operation sequence embedding, gradient-steered optimal embedding search, and transformation operation sequence reconstruction. The authors conduct extensive experiments and case studies to validate the effectiveness and robustness of the proposed method, demonstrating its potential for reuse in other fields.

### Strengths and Weaknesses
Strengths:  
1. The framework is clearly articulated, with detailed explanations of its four-step process, facilitating understanding and replication.  
2. Extensive experiments address concerns regarding the time-consuming nature of reinforcement learning in data collection, showing that the framework can achieve respectable performance with a fixed number of search steps.  
3. The authors provide thorough analyses covering performance, robustness, runtime, scalability, and memory usage, effectively substantiating their claims.  
4. The release of related code and data supports reproducibility of the experiments.

Weaknesses:  
1. The choice of LSTM as the backbone model lacks justification; alternative methods like Transformers could have been discussed.  
2. There are several typographical errors, including inconsistencies in naming conventions and incorrect references to figures and tables.  
3. The description of the optimization objective and the data collection process is insufficiently detailed, requiring further clarification.

### Suggestions for Improvement
We recommend that the authors improve the justification for using LSTM over other sequence modeling methods, such as Transformers. Additionally, the authors should address the typographical errors and inconsistencies in naming throughout the paper to enhance clarity. We suggest providing a more comprehensive description of the optimization objective in the data collection section and clarifying the rationale behind the selected datasets for the time complexity analysis. Finally, we encourage the authors to explore the feasibility of designing a more generalized operation set that views mathematical operations as analogous to sub-networks.