ID: XNnFTKCacy
Title: Coherent Entity Disambiguation via Modeling Topic and Categorical Dependency
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CoherentED, a framework for entity disambiguation (ED) that enhances topical coherence in entity predictions through two main components: a variational autoencoder (VAE) for extracting latent topic vectors from context sentences and a category memory embedding table for retrieving relevant categories for undecided mentions. The authors demonstrate that CoherentED achieves state-of-the-art results across six datasets without fine-tuning on the AIDA training set, outperforming baselines by approximately 1.3 F1 points.

### Strengths and Weaknesses
Strengths:
- The model design is neat and reasonable, with clear motivation and effective presentation.
- The experiments validate the method's effectiveness, supported by ablation studies that confirm the contribution of each component.

Weaknesses:
- The novelty of the framework is limited, as key components are borrowed from existing works, such as the VAE architecture from Optimus and training methods similar to LUKE.
- The paper lacks a thorough efficiency analysis, particularly regarding the latency introduced by the additional VAE and category memory components.
- The clarity of motivation and experimental details is insufficient, particularly concerning the AIDA dataset and the impact of hyper-parameter tuning.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation by providing more intuitive examples that compare existing methods, particularly in terms of entity interaction regarding topic and category coherence. Additionally, we suggest including a detailed efficiency analysis to address the latency introduced by the new components. The authors should also present experiments on the AIDA dataset with fine-tuning and clarify the hyper-parameter tuning process. Finally, further analysis is needed to verify whether performance improvements stem from better topic and category coherence, particularly through in-depth discussions on the context of different lengths and the impact of topic token numbers.