ID: Uafbv4rfJc
Title: Active Negative Loss Functions for Learning with Noisy Labels
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 7, 6, 5, -1, -1, -1, -1
Original Confidences: 3, 4, 5, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new class of theoretically robust passive loss functions called Normalized Negative Loss Functions (NNLFs) and proposes the Active Negative Loss (ANL) framework to enhance the Active Passive Loss (APL) by substituting the MAE with NNLFs. The authors assert that the ANL framework is noise tolerant and focuses more on well-learned classes compared to MAE, demonstrating improved performance on multiple datasets.

### Strengths and Weaknesses
Strengths:
1. The motivation and theoretical foundation of the proposed work are clearly articulated and well-explained.
2. The writing is clear, making the paper easy to read.
3. The theoretical study is solid, and the experiments are extensive and sound.

Weaknesses:
1. The paper relies heavily on previous terminology and definitions, particularly regarding active and passive loss functions, which may lead to confusion.
2. The motivation for the components of NNLF is insufficiently justified, and the theoretical analysis lacks originality, as it draws from existing literature.
3. The paper contains numerous typos and unclear mathematical notations, which detract from its overall quality.
4. The proposed method is only validated on a limited number of datasets, raising concerns about its generalizability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of mathematical notations, particularly defining terms like N in Section 2.1 and addressing the unclear definition of A in Eq. (7). Additionally, we suggest that the authors provide a more robust justification for the inclusion of the three components in NNLF and clarify the theoretical contributions to distinguish their work from existing literature. To enhance validation, we encourage the authors to conduct experiments on a broader range of real-world datasets, such as Animal-10N and Clothing-1M, and include ablation studies to assess the impact of regularization techniques on the ANL framework. Lastly, careful proofreading is necessary to correct typos and improve the overall presentation of the paper.