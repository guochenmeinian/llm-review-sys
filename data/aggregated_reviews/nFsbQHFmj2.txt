ID: nFsbQHFmj2
Title: Regret-Optimal Model-Free Reinforcement Learning for Discounted MDPs with Short Burn-In Time
Conference: NeurIPS
Year: 2023
Number of Reviews: 30
Original Ratings: 7, 6, 8, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a model-free algorithm for discounted tabular Markov decision processes (MDPs) that achieves optimal regret with low burn-in cost, space complexity, and computational cost. The authors propose innovative techniques, including a slow, adaptive execution policy-switching mechanism and variance reduction, to enhance efficiency. The algorithm reportedly has a shorter burn-in time than previous works and provides valid theoretical guarantees. Additionally, the authors discuss the choice of regret definitions in reinforcement learning (RL), arguing that their selected stationary regret metric is more practical for deriving usable policies compared to the non-stationary metric. They emphasize that both metrics measure regret during the execution of an algorithm over a finite time horizon and express a willingness to revise their presentation to clarify the implications of using different metrics.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated, addressing the need for a more sample-efficient model-free tabular MDP algorithm.
- The writing is clear, with a detailed comparison to previous algorithms that is highly appreciated.
- The integration of various techniques in the proof indicates a strong command of the subject matter.
- The authors provide a clear rationale for their choice of regret definition, emphasizing its practicality in generating usable policies.
- They acknowledge the concerns raised by reviewers and express a willingness to improve the clarity of their presentation and results.

Weaknesses:
- The proof contains numerous omissions of brackets in summation symbols, which affects clarity and correctness.
- There is a lack of empirical evidence to support theoretical claims, particularly regarding the 'low' burn-in cost.
- The paper does not provide a formal lower bound for the minimum burn-in time, which is a significant gap.
- The improvement over prior work is questioned, as the objectives considered differ, and the contribution may be seen as incremental.
- The authors' argument that prior works' comparisons of different regret definitions are invalid is contested, with reviewers pointing out that such comparisons can be formal and rigorous.
- There is a lack of consensus on the validity of the stationary regret definition, with some reviewers questioning its relevance in measuring online performance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the proof by ensuring all summation symbols include appropriate brackets. Additionally, we suggest providing empirical evidence to substantiate the theoretical claims, particularly regarding the burn-in cost. It would be beneficial to address the lack of a formal lower bound for the minimum burn-in time and to clarify the reasons behind the proposed algorithm's ability to reduce this cost. Furthermore, we encourage the authors to discuss the comparative performance of their algorithm with respect to existing methods more explicitly, particularly regarding the differences in regret definitions and their implications. Lastly, we recommend that the authors improve the clarity of their presentation by explicitly differentiating results obtained under different regret metrics and including the newly proven formal connection between stationary and non-stationary regret metrics to enhance the rigor of their comparisons with existing results.