ID: hiwF7aG1dt
Title: Iteratively Learn Diverse Strategies with State Distance Information
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 4, 6, 8, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel algorithm, State-based Intrinsic-reward Policy Optimization (SIPO), aimed at learning diverse, human-interpretable behaviors in reinforcement learning (RL). The authors analyze existing diversity measures and propose an ITR-based method that incorporates state distances to enhance policy diversity. The effectiveness of SIPO is demonstrated through empirical evaluations across various environments, including locomotion tasks and multi-agent settings.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, clearly articulating its motivation and contributions.
- It covers a substantial amount of related work, although some relevant studies are omitted.
- The evaluations regarding diversity are commendable and demonstrate promising results.

Weaknesses:
- The motivation for the necessity of state distance is weak, and the clarity of Figure 2 is insufficient.
- Several relevant works in unsupervised reinforcement learning are not adequately discussed, which could enhance the literature review.
- The experimental robustness is questionable due to the limited number of seeds used for performance metrics.

### Suggestions for Improvement
We recommend that the authors improve the literature review by including additional relevant works, such as those that utilize different similarity/dissimilarity measures. Clarifying the motivation behind Figure 2 with more detailed descriptions and numerical indicators is essential. Additionally, we suggest assessing how the proposed diversity measures compare to approximated state entropy methods, and providing a clearer comparison of performance metrics in the humanoid locomotion task. Addressing the theoretical grounding of the state distance measures and their implications for the necessity of learning diverse strategies would strengthen the paper's argument. Finally, increasing the number of seeds in experiments and including traditional total-reward metrics would enhance the robustness of the findings.