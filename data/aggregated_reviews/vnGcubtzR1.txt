ID: vnGcubtzR1
Title: On the Overlooked Pitfalls of Weight Decay and How to Mitigate Them: A Gradient-Norm Perspective
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 7, 6, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Scheduled Weight Decay (SWD), a method aimed at addressing the issue of large gradient norms associated with constant weight decay factors in deep neural networks. The authors demonstrate that SWD enhances the generalization performance of the Adam optimizer and outperforms other adaptive optimizers on CIFAR-10/100 datasets. The study highlights the pitfalls of weight decay, particularly its impact on gradient norms during training, and proposes a dynamic adjustment of weight decay strength based on gradient norms.

### Strengths and Weaknesses
Strengths:
1. The proposed method is straightforward to implement and effectively addresses the generalization gap between adaptive optimizers and SGD.
2. The authors provide a theoretical analysis of unstable stationary points and conduct extensive experiments to validate their claims.

Weaknesses:
1. The gradient analyzed in Theorem 2 appears to include L2 regularization, which is not reflected in the empirical analysis.
2. There are discrepancies between results in Table 1 of the appendix and the main text, particularly for SGD, which need clarification.
3. The theoretical results lack novelty, particularly Theorem 1, which seems trivial and relies on standard convergence rates for SGD.
4. The dependence of C_2 on the supremum norm of theta in Theorem 2 is questionable, and the authors should clarify their assumptions regarding bounded weight norms.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the theoretical results, particularly by highlighting the novelty of their findings and addressing the triviality of Theorem 1. Additionally, the authors should clarify the discrepancies in Table 1 and ensure that the gradient norms analyzed in their theorems align with their empirical results. It would also be beneficial to provide more comprehensive experiments to support the theoretical claims, especially regarding the implications of SWD on training time and computational complexity. Lastly, we suggest that the authors clarify the assumptions made in Theorem 2 regarding the boundedness of weight norms.