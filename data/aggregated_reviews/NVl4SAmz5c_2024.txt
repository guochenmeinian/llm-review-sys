ID: NVl4SAmz5c
Title: Why Warmup the Learning Rate? Underlying Mechanisms and Improvements
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 7, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the warmup technique in gradient-based training, focusing on SGD, ADAM, and their variants. The authors empirically demonstrate that warmup allows networks to handle larger learning rates and stabilizes training dynamics by moving away from divergence boundaries. Additionally, the paper proposes improvements for hyperparameter initialization to enhance training efficiency and generalization, as well as a new optimizer called GI-Adam. The authors analyze two warmup regimes, emphasizing sharpness reduction and progressive sharpening, and introduce a persistent catapult warmup schedule. They reveal that the sharpness reduction regime is suboptimal, necessitating longer warmup durations for optimal learning rates, and highlight a novel mechanism of warmup that enables networks to tolerate larger learning rates without inducing catapults.

### Strengths and Weaknesses
Strengths:  
1. The paper provides a thorough analysis of the warmup procedure, aligning with previous observations and offering practical tips for practitioners.  
2. It highlights the significance of warmup in enabling larger learning rates and discusses different stability regimes based on initialization.  
3. The experiments demonstrate the gradual self-stabilization mechanism induced by warmup across various network parameterizations and optimizers.  
4. The authors propose two practical heuristics for training, including the introduction of GI-Adam and insights on optimal warmup hyperparameters.  
5. The writing is clear and accessible, with technical questions from earlier reviews adequately addressed.  

Weaknesses:  
1. The clarity of figures is insufficient, with captions failing to adequately describe the content.  
2. The empirical nature of the observations raises questions about their theoretical validation, particularly for more complex architectures like Transformers.  
3. The conclusions drawn from experiments on ResNet models may not hold for more challenging models and datasets.  
4. The novelty of the empirical analysis is limited, heavily relying on previous works.  
5. The importance of the analysis of the two warmup regimes remains somewhat vague without a clear correlation between sharpness behavior and warmup significance.  
6. While the practical modifications are reasonable, they do not represent the strongest contributions of the paper.  
7. The effectiveness of GI-Adam compared to baseline Adam is not convincingly demonstrated, and the paper lacks experiments with Transformer architectures.

### Suggestions for Improvement
We recommend that the authors improve the clarity of figures and their captions to enhance understanding. Additionally, we suggest providing theoretical validation for the empirical findings, particularly in relation to Transformer architectures. The authors should clarify the novelty of their contributions compared to existing literature and include empirical evaluations to support claims about the effectiveness of starting warmup from the critical learning rate, $\eta_c$. Furthermore, we encourage the authors to conduct experiments with Transformer models to assess the applicability of their findings in more complex scenarios. We also recommend incorporating a discussion on the correlation between sharpness levels and their behavior during training to strengthen the conclusions regarding the importance of warmup. Lastly, a more detailed discussion of GI-Adam's performance relative to other modifications of Adam would strengthen the paper.