ID: 3H1wqEdK4z
Title: Enhancing Large Language Models through Adaptive Tokenizers
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an adaptive tokenization scheme that is learned jointly with a language model (LLM) and evaluates its performance on downstream tasks for smaller model sizes and token budgets. The authors propose a method that combines a compression loss with a language modeling loss, iteratively removing tokens that contribute the least to the combined loss. Empirical results indicate that this approach improves accuracy compared to traditional tokenization methods.

### Strengths and Weaknesses
Strengths:
- The idea of incorporating both the current and previous iteration losses via a momentum term to stabilize training is intriguing.
- The results demonstrate cross-model adaptability of vocabularies across different model sizes.
- The experiments are comprehensive, and the ablation study provides valuable insights into the impact of various design choices.

Weaknesses:
- The model size (up to 410M parameters) and corpus size (16B tokens) are relatively small, raising concerns about the generalizability of findings to larger scales typical of state-of-the-art LLMs.
- Comparisons are limited to common tokenizers (BPE, BytePiece, Unigram), lacking exploration of other adaptive tokenizers, which diminishes scientific rigor.
- The performance scores on certain tasks (ARC-C, LogiQA, PIQA, Winogrande) are lower than the Unigram baseline, questioning the significance of results.
- The loss calculation in Equation 2 may be computationally expensive, and its impact on runtime, especially with larger corpora, is unclear.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by testing on larger model sizes and token budgets. Additionally, comparisons with other adaptive tokenizers should be included to enhance the scientific contribution of the work. We suggest conducting significance tests to verify the impact of the proposed method on performance metrics. Furthermore, a detailed analysis of the computational costs associated with the adaptive tokenizer compared to standard tokenizers should be provided. Lastly, we advise clarifying vague statements in the writing and addressing the limitations of the method more thoroughly.