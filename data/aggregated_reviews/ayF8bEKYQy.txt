ID: ayF8bEKYQy
Title: OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 6, 7, 7
Original Confidences: 5, 3, 3

Aggregated Review:
### Key Points
This paper presents OlympicArena, a benchmark designed to evaluate the cognitive reasoning capabilities of LLMs and LMMs through a collection of 11,163 problems from 62 international Olympic competitions across seven scientific disciplines. The benchmark includes both text-only and interleaved text-image modalities, and features a fine-grained evaluation mechanism that assesses not only final answer accuracy but also the correctness of intermediate reasoning steps. The authors evaluate various state-of-the-art models, revealing that even advanced models struggle with the complex problems, indicating a need for further research to enhance AI cognitive reasoning abilities.

### Strengths and Weaknesses
Strengths:  
- The benchmark is comprehensive and challenging, covering a wide range of scientific disciplines with Olympic-level problems that push AI capabilities.  
- It supports multimodal and bilingual formats, enhancing accessibility and applicability.  
- The fine-grained evaluation mechanism provides insights into intermediate reasoning processes, helping to identify weaknesses in model reasoning abilities.  

Weaknesses:  
- The benchmark's static nature may not address long-term test set contamination issues.  
- There is a discrepancy between performance on simpler tasks and Olympic-level problems, raising questions about the benchmark's ability to reflect model capabilities accurately.  
- The paper lacks evaluations on certain models, such as Llama 3, and could benefit from a broader scope that includes other cognitive tasks beyond problem-solving.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of OlympicArena, particularly regarding its static nature and the discrepancies in model performance on simpler versus Olympic-level tasks. Additionally, the authors should consider expanding the benchmark to include evaluations of open-sourced models like Llama 3 and explore self-consistency experiments for mathematical reasoning. Lastly, we suggest summarizing key details currently in the appendix within the main paper to enhance clarity and accessibility.