ID: ALU676zGFE
Title: MTGS: A Novel Framework for Multi-Person Temporal Gaze Following and Social Gaze Prediction
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework for predicting human social labels and gaze heatmaps simultaneously in image sequences, enhancing gaze following accuracy through human social cues. The authors propose a model that includes a person module for calculating person tokens and an interaction module for extracting relationships between individuals and scenes. They validate the model's effectiveness across multiple datasets and introduce a new dataset suitable for both gaze prediction and social gaze classification tasks.

### Strengths and Weaknesses
Strengths:
- The experimental section is detailed, demonstrating strong results across various datasets, which supports the universality of the proposed method.
- The incorporation of human social cues into gaze prediction tasks is an innovative approach.
- Comprehensive experiments and sufficient ablation studies reinforce the contributions of different submodules.

Weaknesses:
- The introduction of the Pairwise Instance Generator module lacks clarity regarding its function and the impact of the number of Interaction Blocks on model performance.
- The use of confusing symbols and excessive superscripts/subscripts in formulas complicates understanding.
- The results show minimal differences in performance metrics, raising questions about the significance of the improvements.
- The framework's novelty is limited, as it combines existing approaches without substantial advancements.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the Pairwise Instance Generator module's function and the implications of varying the number of Interaction Blocks. Additionally, simplifying the notation and reducing the complexity of symbols in the formulas would enhance readability. To substantiate the results, we suggest conducting statistical tests to demonstrate the significance of performance differences. Furthermore, we encourage the authors to address the competitive landscape by discussing their contributions relative to existing transformer-based approaches for gaze following and social gaze estimation. Lastly, we recommend reframing the importance of temporal information in the title and abstract, as its treatment in the appendix may undermine its perceived significance.