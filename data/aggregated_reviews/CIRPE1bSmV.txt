ID: CIRPE1bSmV
Title: Mitigating Object Hallucination via Concentric Causal Attention
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of hallucinations in Large Vision-Language Models (LVLMs), attributing them to the long-term decay issue inherent in Rotary Positional Encoding (RoPE). The authors propose a novel method, Concentric Causal Attention (CCA), which aims to mitigate the effects of RoPE by reorganizing visual tokens and employing concentric causal masking. Experimental results indicate that CCA effectively reduces hallucinations and enhances perception capabilities in LVLMs.

### Strengths and Weaknesses
Strengths:
1. The exploration of positional encoding and its impact on hallucinations is innovative and avoids the latency associated with contrastive decoding methods.
2. The experimental results are robust, demonstrating the effectiveness of the proposed method.

Weaknesses:
1. The writing quality needs improvement; the abstract should provide background on RoPE and the long-term decay issue for better reader comprehension. Captions for Figures 1 and 2 require conciseness, and Figures 2 and 3 need clearer presentations.
2. The paper lacks a comparison of RoPE with other positional encodings, which would enhance the analysis. Additionally, it should include comparisons with recent training-based methods, as VCD and OPERA are training-free.
3. The method section is underdeveloped, lacking justification for the proposed scanning method. The performance of CCA on benchmark datasets is not consistently promising, and further analysis is needed to understand its effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in the abstract and figure captions, to facilitate reader understanding. Additionally, the authors should include comparisons of RoPE with other positional encodings and recent training-based methods to provide a more comprehensive analysis. We suggest expanding the method section to include a clearer justification for the concentric causal masking and to conduct further ablation studies and evaluations on additional LVLM benchmarks. Lastly, addressing the questions regarding the attention distribution and the implications of using CCA in models not employing RoPE would strengthen the paper.