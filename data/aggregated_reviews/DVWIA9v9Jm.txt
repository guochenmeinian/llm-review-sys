ID: DVWIA9v9Jm
Title: R-divergence for Estimating Model-oriented Distribution Discrepancy
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 7, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents R-divergence, a new metric for measuring the discrepancy between two distributions, defined as the difference in empirical risks computed on two datasets using a hypothesis trained on their mixture. The authors argue that R-divergence avoids the overfitting issues associated with H-divergence by calculating the minimum hypothesis and empirical risk on different datasets. The method is applied to learn with noisy labels, estimating clean and noisy samples per batch, and retraining the network accordingly. The paper demonstrates superior performance compared to H-divergence across various datasets and architectures. Additionally, the authors introduce a model-oriented approach using R-divergence to tackle challenges in two-sample testing and distance quantification, illustrated through an example involving 'Shiba Inu' and 'Corgi'. They acknowledge the need for experimental support and discuss label noise experiments, recognizing the difficulty in competing with advanced methods while committing to refining their comparisons with established baselines.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow, with a clear distinction made between R-divergence and H-divergence.
2. Strong empirical results are achieved over baseline metrics across different datasets and model architectures.
3. The comparison between ViT and CNN architectures is insightful, revealing that ViT architectures struggle with shape-biased features.
4. The application of R-divergence to improve model performance under corrupted labels is a compelling demonstration of its utility.
5. The authors effectively address reviewer concerns and demonstrate engagement with feedback.
6. The example provided is relevant and has the potential to enhance understanding of R-divergence.
7. Preliminary comparisons with baselines in label noise experiments show a proactive approach to validation.

Weaknesses:
1. The comparison with CIFAR-10 and its corrupted variant is limited to one type of corruption, unlike H-divergence, which considers multiple corruptions. More detailed experimentation with various corruptions is needed.
2. The authors should conduct experiments with ImageNet and its corrupted variants to further validate their proposed method.
3. R-divergence's reliance on the choice of loss function and hypothesis class raises concerns about its robustness as a measure of discrepancy.
4. The theoretical insights provided are limited, particularly regarding the convergence rate of R-divergence and its performance in diverse scenarios.
5. The lack of experimental support for the 'Shiba Inu' and 'Corgi' example limits its impact.
6. The current comparisons with baselines may not be comprehensive enough to attract significant attention or follow-up work.

### Suggestions for Improvement
We recommend that the authors improve their experimental design by including a broader range of corruptions in their CIFAR-10 experiments and conducting tests on ImageNet and its corrupted variants. Additionally, we suggest that the authors clarify the theoretical foundations of R-divergence, particularly addressing its dependency on loss functions and hypothesis classes. A more thorough discussion of the limitations of R-divergence, including potential overfitting issues in rich hypothesis spaces, would enhance the paper's robustness. We also recommend that the authors improve the experimental support for the 'Shiba Inu' and 'Corgi' example to strengthen their argument. Furthermore, we encourage the authors to conduct more extensive comparisons with established baselines in their label noise experiments to enhance the robustness of their findings and attract greater interest in their work. Finally, insights into the convergence behavior of R-divergence in scenarios with large L-divergence would be beneficial.