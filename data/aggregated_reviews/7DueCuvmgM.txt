ID: 7DueCuvmgM
Title: Incorporating Structured Representations into Pretrained Vision \& Language Models Using Scene Graphs
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to enhance vision-language Transformers by utilizing scene graphs (SGs) for structured information. The authors propose two approaches to obtain supervision signals from SGs: generating positive and negative captions with various compositional elements, and using object labels, relations, and coordinates to supervise "scene graph tokens." These learnable tokens interact with regular patch tokens through self-attention, aiding in predicting text embeddings for objects or relations. The model, fine-tuned from CLIP/BLIP via LoRA, demonstrates improved performance on challenging VL benchmarks that require compositional scene understanding.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with clear diagrams illustrating architectural improvements.
- It effectively shows that a small number of scene graph annotations can enhance compositional understanding in VLMs.
- Comprehensive ablation studies and straightforward loss functions contribute to the clarity and implementation of the methodology.

Weaknesses:
- The proposed method does not consistently improve performance across all benchmarks, with noted degradation in zero-shot performance and certain evaluation partitions.
- The novelty of leveraging scene graphs is questioned, as prior works exist without sufficient discussion of differences.
- The experimental section lacks comparisons with state-of-the-art methods, and visual predictions are relegated to supplementary materials.

### Suggestions for Improvement
We recommend that the authors improve the discussion of how their method differs from existing works on scene graphs, addressing the novelty of their approach. Additionally, we suggest including more state-of-the-art comparisons in the experimental section to strengthen their claims. It would also be beneficial to present some visual predictions in the main text, as these are key results of the paper. Lastly, we encourage the authors to conduct a more thorough error analysis to understand the performance degradation observed in certain tasks.