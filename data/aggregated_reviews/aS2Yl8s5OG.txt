ID: aS2Yl8s5OG
Title: On Adversarial Training without Perturbing all Examples
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 5, 5, 7, 5, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach called Subset Adversarial Training (SAT), which focuses on adversarial training using only a subset of the training data. The authors investigate two variants, Class SAT (CSAT) and Example SAT (ESAT), and find that robust training in one class can generalize to other classes not included in the adversarial training. They also introduce the concept of loss balancing to address imbalances between adversarial and non-adversarial subsets. The findings suggest that adversarial training on harder examples can enhance downstream robust performance with less data.

### Strengths and Weaknesses
Strengths:
- The experimental setting is intriguing, revealing that adversarial training on a single class can yield robustness to other classes.
- The potential to reduce adversarial training costs is demonstrated.
- The paper is clear, with thorough experiments and analyses.

Weaknesses:
- The implications of robust generalization to other classes are not sufficiently explored.
- The experiments do not address more challenging scenarios, limiting the understanding of adversarial training processes.
- The contribution to robust accuracy and training efficiency is unclear, as SAT still relies on inefficient methods like PGD-7 for adversarial example generation.
- The experiments are limited to $l_2$ bounded perturbations, neglecting other types like $l_\infty$.

### Suggestions for Improvement
We recommend that the authors improve the analysis and discussion of the implications of their findings, particularly regarding robust generalization. For instance, constructing a synthetic 11th class on CIFAR-10 of varying difficulty could provide insights into the extent of robust generalization. Additionally, we suggest including experiments with larger adversarial budgets and diverse perturbation types to strengthen the findings. Clarifying the baseline adversarial training methods used and providing more implementation details would enhance reproducibility. Lastly, addressing the cost implications of SAT in practical applications and exploring its compatibility with other robust learning methods would be beneficial.