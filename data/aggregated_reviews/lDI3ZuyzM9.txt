ID: lDI3ZuyzM9
Title: AutoGO: Automated Computation Graph Optimization for Neural Network Evolution
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 3, 7, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AutoGO, a novel NAS algorithm that builds a performance predictor for acceleration and conducts experiments for verification. The proposed framework operates directly on the Computation Graph (CG) of DNN architectures, segmenting the CG and optimizing it through a multi-objective evolutionary algorithm. The authors aim to optimize neural architectures for hardware-friendly inference, demonstrating incremental improvements in CNN architectures regarding latency, FLOPS, and power while maintaining or enhancing performance. The framework eliminates the need for users to manually define search spaces, contrasting with traditional NAS methods. The authors emphasize the importance of their segment database for CG mutations and its flexibility compared to manual designs. They claim that AutoGO improves performance across various public architecture benchmarks and demonstrates capabilities in optimizing large CNN architectures for different computer vision tasks.

### Strengths and Weaknesses
Strengths:
- The search space is constructed from segments of CGs, which is an interesting and sensible approach.
- The proposed AutoGO framework provides a novel method for optimizing CNN architectures without requiring predefined search spaces.
- The framework is technically sound, effectively addressing complex problems related to computation graphs.
- The authors commit to open-sourcing their experimental code and segment database, facilitating future research.
- The motivation and objectives of the work are clearly articulated, and the related work section is comprehensive.
- The framework demonstrates significant benefits across various neural networks and computer vision tasks.

Weaknesses:
- The novelty of the work is limited, as it does not adequately address existing performance predictors and lacks sufficient benchmarking.
- Comparability issues arise, as the results may not be easily replicated or compared with existing models due to unique hyperparameters and training recipes.
- The algorithm's reliance on a specific segment database restricts its generalizability.
- Clarity concerns exist regarding the authors' claims about novelty, particularly in mutating architectures and the implications of predefined search spaces.
- Specific questions regarding overall cost, operations used, and the amount of labeled data for training the predictor remain inadequately addressed.
- The clarity of certain technical aspects could be improved, and the paper lacks a comprehensive analysis of its limitations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims regarding the novelty of their work, particularly in relation to architecture mutation and predefined search spaces. It would be beneficial to include a detailed discussion on the limitations of using BPE for mining subgraphs and to acknowledge related works in blockwise NAS. Additionally, please provide a more detailed breakdown of the operations used in the segment database to enhance transparency. We suggest enhancing the presentation with a simplified pseudocode representation of the method and revising Figure 1 for better visual clarity. Finally, we encourage the authors to clarify the overall cost and the amount of labeled data required for training the predictor in the paper, and consider expanding their experimental validation to include more diverse datasets and architectures beyond the current benchmarks.