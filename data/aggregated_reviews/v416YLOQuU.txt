ID: v416YLOQuU
Title: Adam with model exponential moving average is effective for nonconvex optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 4, 5, 7, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the online learning framework 'discounted-to-nonconvex conversion' and proposes a bound for the expectation of the exponential average of the output gradient. The authors find that if the online learner adheres to a specific loss structure (FTRL), the output aligns with clip-Adam rules. Additionally, the authors show that with clipping and model exponential average, (Clipping) Adam can outperform SGD. They propose a variant of Adam that incorporates per-iteration clipping and EMA-based weight averaging, achieving optimal convergence rates for both smooth and non-smooth non-convex problems.

### Strengths and Weaknesses
Strengths:
- The proposed algorithm achieves optimal convergence rates for both smooth and non-smooth non-convex problems.
- The discussion on the benefits of coordinate-wise updates is insightful.
- The analysis of (Clipping) Adam with model exponential average is well-articulated.

Weaknesses:
- The paper is difficult to follow, particularly due to insufficient elaboration on key definitions and concepts, especially for readers unfamiliar with online learning.
- The main convergence results may hold even without EMA-based averaging, raising questions about its necessity.
- The clarity of certain definitions, such as $v_s$ in Definition 6 and the comparator $u_t$ in Lemma 7, is lacking.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by elaborating on key definitions and concepts, particularly for readers less familiar with online learning. Specifically, please clarify the roles of $\mathbf{x_t}$, $\mathbf{w_t}$, and $\alpha_t$ in Algorithm 1, and ensure that all notations are clearly defined. Additionally, we suggest providing empirical results to demonstrate the practical applicability of the proposed algorithm, as this would enhance its appeal to practitioners. Lastly, please address the implications of the main results more thoroughly, as the current explanations appear far-fetched and may benefit from further context.