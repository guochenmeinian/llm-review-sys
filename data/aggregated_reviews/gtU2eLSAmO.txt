ID: gtU2eLSAmO
Title: Brain-JEPA: Brain Dynamics Foundation Model with Gradient Positioning and Spatiotemporal Masking
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 8, 6, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Brain-JEPA, a self-supervised learning model that utilizes a joint-embedding predictive architecture for learning representations from brain fMRI images. The authors introduce two novel components: Brain Gradient Positioning for encoding ROI functionality into positional encoding, and a specific fMRI masking strategy. The model is pretrained on the UK Biobank dataset and evaluated on downstream tasks, achieving superior performance compared to previous models like BrainLM. The study demonstrates the potential of self-supervised learning in enhancing brain data analysis.

### Strengths and Weaknesses
Strengths:
- The proposed approach achieves strong empirical results, significantly outperforming prior methods in the field.
- A clear ablation study illustrates the importance of the brain-gradient position embedding and masking strategy.
- The model exhibits good scaling properties, indicating broader applicability.

Weaknesses:
- The empirical evaluation lacks clarity on how pretraining data influences downstream performance, particularly regarding baseline comparisons with BrainLM.
- The scaling properties of Brain-JEPA concerning dataset size remain unexplored.
- The impact of specific contributions compared to other design choices, such as predictive in latent space, is not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how pretraining data affects downstream performance, particularly in relation to baseline models like BrainLM. Additionally, exploring the scaling properties of Brain-JEPA with respect to dataset size would provide valuable insights. The authors should also investigate the impact of their contributions compared to other design choices, such as predictive in latent space, and consider whether these contributions could enhance a BrainLM baseline. Furthermore, providing more detailed information on data processing for downstream tasks, including how datasets were divided into training, validation, and test sets, would strengthen the paper.