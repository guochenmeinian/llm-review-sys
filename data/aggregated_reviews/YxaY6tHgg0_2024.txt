ID: YxaY6tHgg0
Title: DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 5, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents dimension-independent structural pruning for large language models (LLMs), introducing a novel method that allows flexible pruning across layers and dimensions. The authors propose using indexing operations to break dimension dependence, which enhances performance by permitting asymmetry across layers. The empirical results indicate that the proposed method outperforms state-of-the-art techniques across various LLM architectures, including OPT and LLaMA.

### Strengths and Weaknesses
Strengths:
- The paper highlights the impact of dimension dependence on pruning performance for the first time.
- The proposed method is simple yet effective, achieving superior results without additional parameters or weight updates.
- Comprehensive evaluation across multiple LLM architectures supports the claims of improved performance.

Weaknesses:
- While the method allows for asymmetric dimensions, it requires hidden states from different layers to match the non-pruned model, limiting its applicability in scenarios where output dimensions are critical.
- The indexing operation may introduce significant latency, potentially negating the speed benefits of pruning, and the authors do not discuss whether the pruned model is faster than the non-pruned version.
- Limited novelty and insufficient ablation studies, hyperparameter sensitivity analysis, and computational efficiency comparisons weaken the contribution.

### Suggestions for Improvement
We recommend that the authors improve the literature review to include more on model compression techniques, such as sparse mechanisms. Additionally, conducting qualitative case studies to analyze the differences between SliceGPT and the proposed method would enhance interpretability. We also suggest including experiments that compare the latency of the pruned model against the non-pruned model. Furthermore, a detailed analysis of hyperparameter sensitivity, particularly regarding the tuning of $\lambda$, and a discussion on the trade-offs between pruning ratio and performance would strengthen the paper.