ID: QnXfnQ3MFe
Title: Dynamic Low-rank Estimation for Transformer-based Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents RankDyna, a matrix decomposition method that enables dynamic rank resource allocation among matrices across different layers during the training process. The authors propose that their method achieves compression and adapts to various downstream tasks by fine-tuning the model just once. RankDyna addresses two assumptions of previous work regarding parameter importance and layer independence, achieving reasonable improvements in language model benchmarks at different compression rates.

### Strengths and Weaknesses
Strengths:
- The proposed method is simple yet well-designed, effectively capturing changes in the importance associated with singular groups.
- Extensive experiments demonstrate the method's importance and report state-of-the-art results, particularly under higher compression rates.
- The organization of sections and clarity in mathematical formulation are commendable.

Weaknesses:
- The method introduces additional computation and memory costs without a detailed efficiency analysis, such as training time and memory usage comparisons.
- Experiments are limited to the BERT model, raising concerns about the generalizability of the results to other transformer-based models and tasks, particularly in natural language generation.
- The novelty of the approach is questioned due to existing works like FWSVD and TFWSVD, with the main contribution being the introduction of dynamic estimation.

### Suggestions for Improvement
We recommend that the authors improve the efficiency analysis by providing detailed comparisons of training time and memory usage. Additionally, we suggest expanding the experimental validation to include other transformer-based models beyond BERT and verifying the effectiveness of RankDyna on natural language generation tasks. Clarifying the novelty of the approach in relation to existing methods would also strengthen the paper. Finally, addressing the questions raised regarding the consistency of results during early fine-tuning and the specifics of the importance tracking process would enhance the clarity and rigor of the study.