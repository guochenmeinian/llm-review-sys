ID: s0UNtuuqU5
Title: GeoMFormer: A General Architecture for Geometric Molecular Representation Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 3
Original Ratings: 7, 7, -1
Original Confidences: 4, 3, 4

Aggregated Review:
### Key Points
This paper presents GeoMFormer, a transformer-based model designed for molecular property prediction. The key contribution is the simultaneous learning of invariant and equivariant representations through a dual-stream architecture, utilizing cross-attention to enhance context learning between the two streams. The authors demonstrate the model's effectiveness across various benchmarks, achieving state-of-the-art or competitive performance.

### Strengths and Weaknesses
Strengths:
1. The architecture effectively separates invariant and equivariant representations, allowing for scalable parameterization.
2. Comprehensive experiments on multiple standard benchmarks with strong baselines are included.
3. GeoMFormer demonstrates state-of-the-art performance on various tasks.
4. The paper is well-written and easy to follow.

Weaknesses:
1. The approach of applying self-attention to invariant and equivariant representations has been previously explored, notably in TorchMD-Net, with this work primarily extending the interaction through cross-attention.
2. The model's performance comes at the cost of a higher number of parameters compared to most baseline models, which may hinder its applicability in smaller-scale scenarios.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contribution by clearly distinguishing how their approach advances beyond previous works like TorchMD-Net. Additionally, consider addressing the model's complexity and parameter count to enhance its applicability for smaller-scale applications.