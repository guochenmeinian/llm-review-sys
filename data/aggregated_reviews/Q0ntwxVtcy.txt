ID: Q0ntwxVtcy
Title: Near-optimal learning with average Hölder smoothness
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 7, 7, 6, -1, -1, -1
Original Confidences: 3, 3, 2, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the concept of average Hölder smoothness, extending average Lipschitz smoothness. The authors provide sample complexity bounds for learning the average Hölder smooth function class in both realizable and agnostic settings, along with corresponding algorithms. They establish lower bounds that align with the upper bounds, differing only by logarithmic factors. The paper also discusses the implications of these findings for practical applications.

### Strengths and Weaknesses
Strengths:
1. The writing is clear and the paper is easy to follow, with well-explained contributions.
2. The contributions are significant, as the sample complexity of learning average smooth Hölder functions was previously unknown, and the authors demonstrate that this class is richer than average Lipschitz functions.
3. The proposed algorithm is novel and has potential applications, particularly since classical ERM fails in the distribution-free setting.
4. The results are comprehensive, with tight upper bounds that are analogous to classical PAC learning sample complexity bounds.

Weaknesses:
1. There is insufficient evidence to support the claim that no efficient algorithm exists for the agnostic case; the reduction from agnostic to realizable settings seems generic.
2. Some typographical errors are present, such as in line 182 where "function" should be "number $N$".
3. More details about the algorithm and its modifications for the agnostic case are needed, particularly regarding the non-parametric regression approach.

### Suggestions for Improvement
We recommend that the authors improve the evidence supporting the claim about the absence of efficient algorithms for the agnostic case. Additionally, we suggest providing more concrete examples of applications where the proposed algorithm outperforms ERM, such as learning finite sums with distributions focusing on regions of low smoothness. It would also be beneficial to compare the bounds with classical PAC learning bounds for better reader comprehension. Lastly, consider including references to other works that provide sample complexity bounds for distribution-free learning in the related work section.