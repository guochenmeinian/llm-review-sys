ID: Qdf3ad5MXH
Title: MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding
Conference: NeurIPS
Year: 2024
Number of Reviews: 21
Original Ratings: 7, 5, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MMBench-Video, a benchmark designed to evaluate large vision-language models (LVLMs) on long-form video understanding tasks. It includes approximately 600 diverse web videos from YouTube, accompanied by human-annotated questions and answers. The evaluation framework employs GPT-4 for scoring, addressing limitations in existing VideoQA benchmarks, such as the predominance of short videos and limited question types. The authors propose a detailed ability taxonomy to assess models' temporal reasoning skills and emphasize that increasing the number of frames enhances model perception, although it may lead to slight increases in hallucinations. They also highlight the effectiveness of using GPT-4 for human evaluation and the cost-effectiveness of open-source LLMs like Qwen2-72B for scoring.

### Strengths and Weaknesses
Strengths:
- The proposed benchmark significantly enhances existing VideoQA evaluations by including longer videos and a broader range of question types.
- The dataset is well-collected and annotated, ensuring high-quality answers and reliability.
- The use of GPT-4 for scoring improves the accuracy and robustness of evaluations, providing valuable insights into model performance.
- The authors provide clear evidence that visual input significantly boosts performance over text-only input.
- The commitment to optimizing visual elements for better readability in figures is commendable.

Weaknesses:
- The evaluation scope is limited to models with no more than 16 frames, neglecting larger, more advanced models.
- The experimental setup lacks comprehensive analysis on the impact of video length and shot count on model performance, and there is a need for additional clarity regarding the experimental setup and the rationale behind the scoring system.
- Some reviewers expressed concerns about the temporal indispensability of models, indicating potential limitations in understanding temporal dynamics.
- The dataset's scale is relatively small, with only 609 video clips and 2000 QAs, which may limit its effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the heuristic method used for obtaining video captions, supported by empirical analysis and additional examples. The experimental section should be redesigned to incorporate a wider range of models and evaluation metrics, including results from pure LLMs. Additionally, the authors should consider increasing the number of frames used in evaluations to better assess model performance on long videos. Improving the clarity of the experimental setup and providing a more detailed explanation of the scoring system is essential. Addressing the concerns regarding temporal indispensability would strengthen the paper. Finally, including qualitative examples for each question type, conducting human evaluations, and improving the quality of chart presentations, particularly font size and color choices, would enhance the dataset's credibility and usability.