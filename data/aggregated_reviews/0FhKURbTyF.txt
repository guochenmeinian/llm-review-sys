ID: 0FhKURbTyF
Title: Efficient Potential-based Exploration in Reinforcement Learning using Inverse Dynamic Bisimulation Metric
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach that integrates bisimulation metrics with an inverse dynamics module to create potential functions for reward shaping in reinforcement learning. The authors propose the Inverse Dynamic Bisimulation Metric to facilitate meaningful exploration, supported by theoretical analyses demonstrating its convergence properties. The method claims to address typical exploration issues and improve efficiency, as evidenced by extensive experiments. The authors emphasize that the shaped reward remains unaffected by background changes once the trained metric converges, ensuring the invariance of the optimal policy despite changes in the reward structure. Experimental results indicate that the method outperforms existing reward-shaping algorithms across various tasks, including MuJoCo and Atari environments.

### Strengths and Weaknesses
Strengths:
1. The potential-based reward-shaping method effectively preserves state differences based on task-specific features.
2. The inclusion of an inverse dynamic module adds a novel perspective to the bisimulation metric.
3. The clarity of the main claim and methodology enhances comprehension.
4. The exploration bonus is independent of prior human knowledge.
5. The authors provide a thorough response to reviewer concerns, demonstrating a commitment to addressing feedback.
6. The method claims to maintain the invariance of the optimal policy, which is a significant contribution.
7. The use of bisimulation-based metrics as an exploration bonus is innovative and merits further investigation.

Weaknesses:
1. While empirical results are promising, additional experiments comparing exploration methods, particularly on DMC tasks like Humanoid, are necessary. Utilizing benchmarks such as URLB could provide further insights.
2. The manuscript contains confusing notations, particularly regarding the reward function definitions and inconsistencies in Section 3 and Theorem 4. Consistency in notation is crucial for clarity.
3. The theoretical contributions, particularly Theorems 1-3, closely resemble prior works, raising concerns about the originality and novelty of the theoretical contributions.
4. There are unresolved questions regarding the convergence of the policy in relation to the changing reward structure.
5. Some technical details, such as the choice between L1 and L2 norms, require further clarification and validation. Issues with the proof of Theorem 4 also raise concerns about the paper's soundness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of notations throughout the manuscript to avoid confusion, particularly in the reward function definitions. Additionally, conducting further experiments on more challenging exploration scenarios, such as sparse reward settings or goal-conditioned tasks, would strengthen the empirical validation of the proposed method. Addressing the resemblance of theoretical analyses to previous works and clarifying the unique contributions of the proposed method will enhance its perceived novelty. We also suggest including a more detailed discussion on the implications of using L1 versus L2 norms, supported by comprehensive ablation studies. Finally, ensuring clarity in notation and addressing any remaining technical inaccuracies will significantly enhance the quality of the paper.