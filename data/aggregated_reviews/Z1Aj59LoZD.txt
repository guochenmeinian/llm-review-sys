ID: Z1Aj59LoZD
Title: Path Regularization: A Convexity and Sparsity Inducing Regularization for Parallel ReLU Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 6, 7, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of optimization for path-regularized parallel ReLU networks, demonstrating how to represent the non-convex problem as a regularized convex problem. The authors establish that while convex problems generally allow for polynomial time solutions, the size of this convex problem is exponential in the rank \( r \) of the data matrix, though polynomial in the data dimension \( d \) and number of examples \( n \). They provide approximation guarantees for rank \( r \) approximations of full rank data matrices, allowing efficient solutions for matrices with quickly decaying singular values. Empirical results show that their method outperforms gradient descent and other optimizers in both performance and time.

### Strengths and Weaknesses
Strengths:
- Theoretical understanding of neural networks is an impactful topic.
- The paper is well-written and introduces novel analysis.
- Empirical results support the theoretical claims.

Weaknesses:
- The solution's complexity is exponential in the rank \( r \) of the data matrix, raising concerns about its practical applicability in high-dimensional scenarios.
- The experiments primarily involve networks with \( m_2=1 \), leaving questions about performance with larger \( m_2 \).
- The results may not be novel, as they closely resemble earlier works on two-layer ReLU networks, which could lead to a perception of derivativeness.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the implications of the rank \( r \) on the complexity of their solution, particularly in high-dimensional contexts. Additionally, we suggest conducting experiments with larger \( m_2 \) to assess the algorithm's scalability and performance. It would also be beneficial to consolidate repetitive statements in the text and ensure that tables and figures are placed near their discussions to enhance the flow of reading. Finally, we encourage the authors to clarify the independence of the number of subnetworks \( K \) in their convex formulation, as this aspect is crucial for understanding the practical implications of their approach.