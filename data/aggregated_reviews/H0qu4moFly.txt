ID: H0qu4moFly
Title: Embedding Dimension of Contrastive Learning and $k$-Nearest Neighbors
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 5, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 1, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the minimal embedding dimension required to satisfy triplet and k-nearest neighbor (k-NN) constraints in various \( \ell_p \) spaces. The authors derive theoretical results that highlight the significance of arboricity in determining these dimensions, particularly for \( \ell_2 \) and other \( \ell_p \) norms. The findings indicate that for contrastive learning, embeddings can be achieved in dimensions proportional to \( \sqrt{m} \), while for k-NN, dimensions can be polynomial in \( k \) for \( \ell_p \) norms and linear in \( k \) for \( \ell_2 \). Additionally, the paper provides a theoretical analysis of the k-NN settings, demonstrating that the dependence on \( n \) is at most poly-logarithmic, which contrasts with the intuitive expectation of \( n^2 \) constraints. The authors propose that the constants in their proofs can be reduced, potentially leading to a final dependency of \( 10^4 k^7 \log^3 n \), making it applicable for \( n \) on the order of \( 10^8 \) to \( 10^9 \). They acknowledge that the current k-NN construction may not be practical and suggest that future work could improve the dependence on parameters, particularly through simpler deterministic schemes.

### Strengths and Weaknesses
Strengths:
- Originality: The paper addresses the minimal dimensionality for ordinal distance constraints, a topic not previously explored despite the popularity of contrastive learning and k-NN methods.
- Theoretical Depth: It provides a rigorous mathematical framework and comprehensive analysis across multiple norms, yielding robust and versatile results.
- Significant Contribution: The demonstration of a poly-logarithmic dependence on \( n \) is a notable theoretical advancement.
- Practical Relevance: The findings are applicable to significant machine learning tasks, enhancing understanding in representation learning.
- Enhanced Understanding: Additional experiments on the \( k \)-NN graph improve the comprehension of the proposed methods.

Weaknesses:
- Missing Conclusion: The paper lacks a conclusion or future work section, ending abruptly after the experimental results.
- Experimental Validation: The experiments do not adequately demonstrate the practical applicability of the theoretical bounds, particularly regarding the embedding dimensions used.
- Complexity of Proofs: Some proofs are intricate and may be challenging for readers without a strong background in theoretical computer science.
- Large Constants: The constants in the proofs are currently large, which may limit practical applicability.
- Practicality Concerns: The complexity of the current k-NN construction raises questions about its practicality.

### Suggestions for Improvement
We recommend that the authors improve the paper by including a conclusion or future work section to summarize findings and discuss limitations. Additionally, consider refining the experimental section to better validate the theoretical claims, possibly by aligning the embedding dimensions used in experiments with those suggested by the theoretical results. We also suggest simplifying some of the more complex proofs to enhance accessibility for a broader audience. Furthermore, addressing the concerns regarding the exact behavior at \( d = \sqrt{m} \) in the experiments would provide clearer insights. We encourage the authors to improve the clarity of the \( (n,k) \) regime in which their k-NN bound is an improvement over the trivial bound of \( n \), including a specific discussion on the constants and practical applicability. Additionally, we suggest providing a table showing the required dimension for \( \ell_2 \) embeddings, as mentioned in the general comments, and clarifying the use of the size parameter \( h \) in their constructions, particularly in relation to the k-NN method.