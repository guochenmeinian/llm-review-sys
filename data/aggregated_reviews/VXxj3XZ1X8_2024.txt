ID: VXxj3XZ1X8
Title: Reproducibility of predictive networks for mouse visual cortex
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 7, 8, 7, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a systematic investigation into the reproducibility of neuronal embeddings in the mouse visual cortex using deep predictive models. The authors propose a novel approach called "adaptive regularization," where regularization parameters are learnable, and introduce iterative feature pruning to address model overparameterization. The study evaluates the stability and reproducibility of neuronal function embeddings and demonstrates that L1 regularization is crucial for achieving consistent embeddings. The work contributes significantly to the computational neuroscience community by providing a robust framework for understanding neuronal function.

### Strengths and Weaknesses
Strengths:
- The introduction of an adaptive regularization method that adjusts the strength of regularization per neuron significantly enhances consistency in clustering neuronal embeddings without compromising predictive accuracy.
- The iterative feature pruning strategy effectively reduces model dimensionality while maintaining predictive performance, improving the robustness of neuronal embeddings.
- A comprehensive evaluation of model consistency across various dimensions is provided, highlighting the reproducibility of the proposed methods.

Weaknesses:
- The study is limited to a single model type, the Rotational Equivariance CNN, which may restrict the generalizability of the findings to other architectures.
- Key terms such as "Embedding" and "Mask" are not defined in the Methods section, leading to potential confusion regarding the model parameters.
- The paper lacks sufficient context in technical sections, which may hinder understanding for readers not deeply familiar with computational neuroscience.
- The adaptive regularization introduces additional hyperparameters, complicating the model training process and potentially requiring significant computational resources.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the Methods section by defining key terms such as "Embedding" and "Mask." Additionally, we suggest providing more context in technical discussions to ensure accessibility for a broader audience. It would be beneficial to address the choice of the Rotational Equivariance CNN and consider evaluating other core architectures, such as Vision Transformers or regular CNNs, to enhance the generalizability of the findings. Furthermore, we encourage the authors to elaborate on the potential biases introduced by the adaptive regularization and pruning strategies and to discuss how these biases might be mitigated. Finally, including confidence intervals in Table 1 and improving the graphical representation of results would enhance the presentation of the findings.