ID: pKOJrwqXLh
Title: TD3: Tucker Decomposition Based Dataset Distillation Method for Sequential Recommendation
Conference: ACM
Year: 2024
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TD3, a dataset distillation method for sequential recommendation utilizing Tucker Decomposition to create synthetic sequence summaries from user-item interaction data. TD3 employs Tucker decomposition for dimensionality reduction, enhanced bi-level optimization for generalization, and Random Truncated Backpropagation Through Time (RaT-BPTT) for efficient handling of sequential dependencies. Empirical results indicate that TD3 achieves comparable or superior performance to models trained on full datasets while significantly reducing data size, training time, and memory usage.

### Strengths and Weaknesses
Strengths:
- The motivation for data distillation in sequential recommendation is both important and practical.
- The methodology is technically robust, combining Tucker decomposition with dataset distillation innovatively.
- Comprehensive experiments across multiple datasets and models demonstrate the method's effectiveness and scalability.

Weaknesses:
- The effectiveness is only verified on a single backbone (SASRec), limiting broader applicability; additional backbones like GRU4Rec and Caser should be considered.
- The initial distillation process is computationally intensive, which may be a drawback, particularly for larger datasets or complex models.
- The datasets used appear relatively small, which may limit the demonstration of effectiveness on larger datasets.

### Suggestions for Improvement
We recommend that the authors improve the experimental validation by including a broader range of comparison techniques to strengthen the evaluation of TD3's performance. Additionally, expanding the experimental scope to larger datasets would provide stronger evidence for the claims made. The authors should also address the computational cost of the distillation process more thoroughly, potentially exploring methods for incremental updates to the synthetic dataset in contexts where data is continuously updated. Finally, enhancing the accessibility of technical sections with more intuitive explanations would benefit a broader audience.