ID: exg62lfHrB
Title: Model Spider: Learning to Rank Pre-Trained Models Efficiently
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 5, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Model Spider, a method designed to efficiently and accurately rank Pre-Trained Models (PTMs) for specific tasks. The authors propose a novel approach that tokenizes both PTMs and tasks into vector representations, allowing for effective similarity measurement and ranking. The method incorporates task-specific forward results for improved accuracy when resource budgets permit. Extensive experiments demonstrate the method's effectiveness across various model zoo configurations.

### Strengths and Weaknesses
Strengths:
1. The idea of encoding PTMs and downstream tasks into vectors for ranking is well-motivated and shows significant improvements over strong baselines.
2. The methodology of tokenizing PTMs is innovative, mapping them to unsupervised trained task tokens.
3. The experiments are comprehensive, including ablation studies that emphasize the importance of the RankAgg component.

Weaknesses:
1. The generalization ability of the proposed method is not well-verified, particularly since evaluations are limited to image classification tasks.
2. The method's applicability to non-classification tasks, such as regression or generation, is not adequately addressed.
3. The evaluation is based on only 10 PTMs from a single-source model zoo, which may not sufficiently demonstrate the method's generalizability.

### Suggestions for Improvement
We recommend that the authors improve the generalization ability verification by evaluating the proposed method on a wider variety of downstream tasks beyond image classification. Additionally, the authors should elaborate on how the method can be adapted for tasks without defined classes. It would also be beneficial to clarify the role of the hyperparameter "k" in Figure 1 and ensure that important results in the appendix are referenced in the main text. Finally, we suggest discussing the limitations of the proposed method, particularly regarding its computational costs and the handling of noise in task tokenization.