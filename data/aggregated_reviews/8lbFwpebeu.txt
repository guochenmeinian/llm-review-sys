ID: 8lbFwpebeu
Title: Investigating how ReLU-networks encode symmetries
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 7, 8, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the equivariance of networks with ReLU activations, specifically examining how internal activations learn to be equivariant given an invariant data distribution. The authors provide a theoretical result for two-layer networks with non-singular weight matrices, demonstrating that equivariance requires input and output activations to be acted on by scaled permutation representations, and that the network must be layerwise equivariant. They conjecture that ReLU-CNNs trained on reflection-invariant data distributions are close to regular GCNNs. The empirical section supports this conjecture, indicating that layerwise equivariance holds in practice.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and offers original insights into equivariant networks, distinguishing it from existing literature that typically focuses on models designed to be equivariant.
- The analytical approach successfully proves layerwise equivariance for certain cases, with empirical support for the conjectures presented.
- The comprehensive nature of the paper, including supplementary material, enhances its contribution to the field.

Weaknesses:
- The strong assumptions in the analytical results may limit generalizability, particularly for networks with more than two layers or singular weight matrices.
- The connection between the conjecture and layerwise equivariance could be articulated more clearly.
- The focus on reflection-equivariant GCNNs excludes consideration of more general symmetry groups.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the argument linking Entezari's conjecture to layerwise equivariance. Additionally, we suggest addressing the necessity of the definition of the barrier in Eq. (4) at the parameter's average rather than at the maximizing parameter value. The authors should also consider expanding the discussion to include more general symmetry groups beyond reflection-equivariant GCNNs. Furthermore, we advise providing a brief explanation of the activation matching technique within the paper rather than solely referencing related work. Lastly, clarifying the implications of Proposition 2.3 and its relationship to the derived kernel constraint in Eq. (3) would enhance the paper's readability and impact.