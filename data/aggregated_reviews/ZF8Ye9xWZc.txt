ID: ZF8Ye9xWZc
Title: RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents RoMQA, a new question answering (QA) dataset designed to evaluate the robustness of QA systems through clusters of similar questions that share implicit constraints. The dataset is assessed under three experimental settings: zero-shot, few-shot, and supervised models, revealing that existing QA systems struggle with variations within question clusters. The authors propose that RoMQA addresses a critical gap in evaluating model robustness, which has been under-studied in prior datasets.

### Strengths and Weaknesses
Strengths:
- The paper introduces a high-quality dataset that significantly contributes to the field of QA.
- The clustering of questions based on shared constraints is an effective approach.
- The writing is clear, and the experiments are thorough, providing valuable insights into model performance.

Weaknesses:
- The definition of robustness is too narrow, focusing only on constraints without addressing broader aspects.
- The modeling section lacks convincing evidence regarding the performance of prompting language models compared to naive baselines.
- The evaluation primarily targets large pretrained language models, limiting the scope of QA methods considered.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the robustness definition to encompass a broader range of factors, including adversarial examples. Additionally, we suggest providing a detailed error analysis to explain model struggles with specific questions and including qualitative examples to illustrate dataset construction and evaluation. Furthermore, clarifying the role of retrieval models in the context of the provided evidence would enhance the paper's comprehensiveness. Lastly, addressing the questions raised regarding dataset construction and performance metrics would strengthen the overall argument.