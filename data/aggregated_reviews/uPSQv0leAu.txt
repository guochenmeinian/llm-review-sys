ID: uPSQv0leAu
Title: Data Selection for Language Models via Importance Resampling
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 8, 8, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method called "Data Selection with Importance Resampling (DSIR)" aimed at improving the selection of pre-training samples for language models. The authors demonstrate that their approach yields superior fine-tuning performance, achieving a 2-2.5% improvement on the GLUE dataset compared to various methods, including random sampling. They also introduce a framework for selecting a representative document subset during pre-training, emphasizing the practical implications of their work.

### Strengths and Weaknesses
Strengths:
- The paper addresses a crucial issue in making pre-training more efficient and is well written, covering relevant related work.
- The proposed method shows significant performance improvements on downstream tasks and is clearly defined, with reproducible implementation details.
- The authors provide a novel data-evaluation metric that correlates well with downstream task performance, and their results are thoroughly analyzed.

Weaknesses:
- The evaluation is limited to the GLUE benchmark, which is considered outdated, and does not explore more challenging tasks or the transferability of results to the test set.
- There is a lack of statistical treatment of the results, which undermines claims of superiority; appropriate statistical analyses should be included to validate findings.
- The method relies solely on n-gram features, and there is no exploration of other potential feature spaces or comparisons with automatic data selection methods.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including more recent and challenging benchmarks beyond GLUE to assess the robustness of their method. Additionally, incorporating statistical significance tests, such as a t-test with Bonferroni correction, would strengthen their claims. We also suggest exploring different feature extraction methods beyond n-grams to enhance the generality of their approach. Lastly, addressing the computational resources required for their method would provide a clearer understanding of its scalability.