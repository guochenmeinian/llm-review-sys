ID: Q2sDuwtutB
Title: Text-space Graph Foundation Models: Comprehensive Benchmarks and New Insights
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 6, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "Text-space Graph Foundation Models: Comprehensive Benchmarks and New Insights," introducing a benchmark for evaluating text-space Graph Foundation Models (GFMs). The authors curated over 20 novel text-space datasets across various domains, defined four GFM paradigms, and provided comprehensive evaluations. Empirical results reveal that while large language models (LLMs) show initial promise, significant performance gaps persist across datasets, with positive transfer in text-space GFMs dependent on transferable structural patterns and appropriate inductive biases. Additionally, the paper investigates two usage scenarios for GFMs: co-training and transferring, with a primary focus on co-training due to efficiency issues that limit large-scale pre-training and transferring research. The authors propose revisions to clarify the limitations of their scope and introduce existing strategies for transferring, highlighting the challenges posed by distribution gaps between pre-training and downstream data.

### Strengths and Weaknesses
Strengths:
1. The paper compiles a diverse range of text-space datasets, crucial for thorough evaluation and benchmarking of GFMs.
2. It provides experimental results across various scenarios, offering valuable insights into the performance and limitations of GFMs.
3. A complete pipeline implementation demonstrates a practical approach to evaluating and improving GFMs.
4. The authors acknowledge and address concerns raised in previous reviews, demonstrating a commitment to improving the paper.
5. The proposed revisions provide a clearer understanding of the transferring problem and its complexities, enhancing the paper's depth.
6. The study reveals valuable observations regarding the performance of GFMs in different graph scenarios, particularly the distinction between homophilous and heterophilous graphs.

Weaknesses:
1. The paper lacks evaluation of real LLMs like LLaMA on graph foundation models, which is a critical aspect.
2. There is an emphasis on co-training over pre-training, with insufficient focus on the latter, which would be more relevant.
3. Missing baseline models, such as GraphGPT and GraphText, hinder comprehensive evaluation.
4. The baselines for graph classification and link prediction are incomplete, lacking comparisons with relevant graph SSL methods.
5. The original submission lacks a comprehensive discussion of the transferring setting, which may leave readers with unanswered questions.
6. There is a need for a more detailed evaluation of existing GFM models across various datasets, which could strengthen the paper's contributions.

### Suggestions for Improvement
We recommend that the authors improve the paper structure by focusing more on the graph pre-training phase. Additionally, supplementing the study with more graph LLM models, such as GraphGPT, GraphText, and GraphAdapter, would enhance its depth. Including more graph SSL methods, like GraphCL and EdgePredGPPT, is also essential. Furthermore, clarifying the rationale behind dataset choices and ensuring the quality of generated text descriptions would strengthen the paper. We suggest that the authors improve the introduction to emphasize the importance of the transferring problem discussed throughout the paper. Additionally, including a comprehensive table evaluating existing GFM models on different datasets would provide a clearer context for their findings. Lastly, the authors should consider expanding on the limitations of their work in Section H to better articulate the challenges of transferring co-trained models to new data.