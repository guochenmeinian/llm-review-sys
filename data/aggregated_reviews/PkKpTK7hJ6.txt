ID: PkKpTK7hJ6
Title: Truncating Trajectories in Monte Carlo Policy Evaluation: an Adaptive Approach
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 5, 5, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to designing truncated trajectory lengths for policy evaluation in reinforcement learning, introducing the adaptive scheme RIDO. This method minimizes the variance of reward estimates by dividing the total interaction budget into mini-batches and iteratively selecting trajectory lengths. Theoretical results demonstrate that the variance of the reward estimate is upper bounded by the best-case variance plus error terms that decay as \(O(1/\Lambda)\). The authors argue that RIDO exploits a reset function, a common assumption in the literature, which enhances performance in various environments, particularly those with high variance in initial states. Empirical evidence shows that RIDO achieves lower variance compared to fixed-length schemes and adapts to the underlying Markov reward process, outperforming Poiani et al. (2023) in several scenarios.

### Strengths and Weaknesses
Strengths:
- The topic is timely and relevant, addressing the efficient use of limited interaction budgets in reinforcement learning.
- RIDO provides an adaptive tool for trajectory length selection, showing advantages in specific scenarios, such as delayed rewards and environments with high variance in initial states.
- The theoretical analysis is solid, with interesting problem formulations and a comprehensive foundation, including Theorem 3.1, which illustrates the necessity of adaptivity in minimizing estimation error.
- Empirical results validate RIDO's effectiveness in minimizing variance over existing methods, demonstrating its adaptability to different environments.

Weaknesses:
- The clarity and significance of the work are questionable; it is unclear why RIDO is superior or in which problem classes it should be preferred.
- A theoretical comparison with Poiani et al. is lacking, making it difficult to assess the fundamental differences between the two methods.
- The experimental results suggest that RIDO and Poiani et al.'s method often yield similar variance, raising questions about RIDO's advantages.
- The applicability of RIDO in policy optimization regimes remains unclear, with some reviewers noting comparable performance to non-adaptive approaches in certain environments.
- The writing requires improvement, particularly in defining key terms and clarifying complex notations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by explicitly stating the advantages of RIDO over Poiani et al.'s method and providing a theoretical comparison to highlight their differences. A more in-depth analysis of the experimental results could clarify the conditions under which RIDO outperforms existing methods. Additionally, we suggest including a related work section to contextualize the contributions of this paper within the broader literature on Monte Carlo methods. The authors should also address the writing issues, such as providing clear definitions for key terms and improving the organization of the text for better readability. Furthermore, we recommend that the authors provide specific examples illustrating RIDO's advantages over non-adaptive methods and address the limitations in policy optimization settings to strengthen the overall argument for RIDO's utility. Finally, we encourage the authors to explore the performance of RIDO on suboptimal policies and clarify the implications of their assumptions regarding episode termination and initial state similarity.