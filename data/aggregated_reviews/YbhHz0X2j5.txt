ID: YbhHz0X2j5
Title: VidMan: Exploiting Implicit Dynamics from Video Diffusion Model for Effective Robot Manipulation
Conference: NeurIPS
Year: 2024
Number of Reviews: 42
Original Ratings: 6, 5, 5, 7, 9, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents VidMan, a two-stage framework for robot manipulation that employs a pre-trained video diffusion model. The first stage predicts future visual trajectories using actionless video data, while the second stage adapts this model into an inverse dynamics model for action prediction. The authors demonstrate the method's efficacy through experiments on RLBench and OXE datasets, including ablation studies that highlight the impact of various components. Additionally, the authors propose a novel approach that integrates video prediction and action prediction, utilizing intrinsic dynamics from video data, which distinguishes it from existing methods like RVT. They provide extensive comparisons with state-of-the-art methods, including GR-1 and RVT, demonstrating superior performance on benchmarks such as CALVIN and RLBench-10. The paper also discusses a comparative analysis of joint frame-action prediction versus action-only prediction, revealing that their model excels in action-only prediction due to the use of cross-attention and a diffusion head. Furthermore, the authors clarify that their model encodes image observations into latent representations, which may include information about future frames due to pretraining, but does not generate explicit future predictions.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and presents a compelling idea that may inspire further research in video pre-training for decision-making in robotics.
2. The two-stage training strategy effectively enhances performance, as evidenced by comparative results against single-stage methods and strong performance across multiple tasks, particularly in RLBench.
3. The use of a layer-wise adapter for action learning is innovative within the robotics domain, and the incorporation of multi-view camera inputs significantly enhances task performance.
4. The authors effectively demonstrate the efficiency of their method compared to RVT, emphasizing adaptability to various sensor inputs.
5. The authors have made significant improvements based on reviewer feedback, including additional experiments and clarifications that enhance the submission's quality.

Weaknesses:
1. The evaluation lacks robustness, with weak results that do not allow for safe conclusions. The choice of baselines is outdated, and comparisons to state-of-the-art models like RVT and 3DDA are missing.
2. The motivation for the proposed approach is generic and does not sufficiently differentiate it from existing works like GR-1.
3. The current implementation lacks 3D perception capabilities, which may limit performance on tasks requiring spatial understanding.
4. The model's ability to interpret complex human instructions is limited, necessitating improvements in language processing.
5. The reliance on web data for pre-training is questioned, as it may not optimally align with robotic manipulation tasks.
6. Discrepancies in success rates for specific tasks between the original paper and the global PDF raise questions about the consistency of reported results.
7. The terminology surrounding "intrinsic" and "implicit dynamics" has been noted as potentially misleading, with a lack of clarity in distinguishing between these concepts.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including comparisons to state-of-the-art baselines such as RVT and 3DDA, as well as conducting a direct comparison with GR-1 on the same dataset. Additionally, the authors should clarify the motivation for their approach, emphasizing its unique contributions beyond existing models. It would also be beneficial to provide more qualitative results to illustrate the model's performance in dynamic environments. We suggest that the authors improve the model's 3D perception capabilities to enhance performance on tasks requiring spatial awareness. Incorporating state-of-the-art Large Language Models (LLMs) could significantly improve the model's understanding of complex instructions. Furthermore, we recommend improving the clarity regarding the selection of Open-Sora layers and providing a more robust justification for the use of web data in pre-training. Lastly, we encourage the authors to refine the discussion around dual-process theory to ensure it aligns more naturally with the presented work and consider modifying the terminology to consistently apply "implicit dynamics" instead of "intrinsic dynamics" to avoid confusion.