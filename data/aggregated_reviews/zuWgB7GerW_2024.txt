ID: zuWgB7GerW
Title: How DNNs break the Curse of Dimensionality: Compositionality and Symmetry Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 5, 7, 6, -1, -1
Original Confidences: 3, 3, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents Accordion Networks (AccNets), a novel neural network structure comprising multiple shallow networks. The authors propose a generalization bound for AccNets that utilizes F1-norms and Lipschitz constants of the subnetworks, demonstrating their capability to overcome the curse of dimensionality by efficiently learning compositions of Sobolev functions. The paper includes theoretical insights and empirical validation, highlighting AccNets' superior performance in complex compositional tasks compared to shallow networks and kernel methods. Additionally, the authors establish a generalization bound for deep neural networks, elucidating how depth enhances the learning of Sobolev function compositions.

### Strengths and Weaknesses
Strengths:  
The introduction of Accordion Networks (AccNets) is a creative and original contribution, supported by thorough theoretical analysis and empirical evidence. The ability of AccNets to efficiently learn compositional functions addresses a significant challenge in high-dimensional learning tasks. The paper is well-written and accessible, effectively embedding findings within prior methodologies.

Weaknesses:  
1. The practical implementation of the proposed regularization methods, particularly the first requiring infinite width, may pose challenges.  
2. Optimizing Lipschitz constants is noted as a potential limitation in practical applications.  
3. Additional experiments on diverse real-world datasets could enhance the demonstration of AccNets' robustness and generalizability.  
4. There is insufficient clarity regarding the conditions under which to use AccNets versus DNNs, necessitating clearer distinctions and discussions of each method's shortcomings.  
5. The phase plots in Figure 2 may not be compelling, and the bounds presented are generally loose, warranting a detailed appendix section on rate measurement.  
6. The linked codebase lacks the indicated notebooks, hindering reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the conditions for using AccNets versus DNNs, explicitly outlining the shortcomings of each. Additionally, we suggest including more diverse real-world datasets in experiments to validate the robustness of AccNets. It would be beneficial to provide a thorough comparison with shallow networks and generalization bounds, offering a general theoretical explanation for the differences observed. Furthermore, please ensure that the linked codebase contains all necessary materials for reproducibility. Lastly, consider defining the notions of F1 distance and Sobolev norm earlier in the paper to enhance accessibility for a broader audience.