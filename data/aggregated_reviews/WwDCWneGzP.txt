ID: WwDCWneGzP
Title: Subgraph-Aware Training of Language Models for Knowledge Graph Completion Using Structure-Aware Contrastive Learning
Conference: ACM
Year: 2024
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Subgraph-Aware Training framework for knowledge graph completion, utilizing sub-batch sampling and subgraph construction methods. The authors propose a model that fine-tunes pre-trained language models (PLMs) by incorporating the structural properties of knowledge graphs to address challenges such as long-tailed entity distributions and hard-to-distinguish negative samples. The method employs biased random walks for subgraph sampling and proximity-aware contrastive learning to prioritize low-frequency entities.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and clear, with a strong motivation for the proposed method.
- The theoretical foundation is relatively sufficient, and the experiments are comprehensive, demonstrating effectiveness across multiple benchmarks.
- The code is publicly available, ensuring reproducibility.

Weaknesses:
- The model's performance scores are not competitive with recent methods, and there is a lack of innovation in the overall framework.
- Some formulas are not labeled, and certain sections lack clarity and depth, particularly regarding formal definitions and implementation details.
- The paper does not adequately address scalability and computational trade-offs associated with subgraph sampling.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the introduction, particularly regarding the "closeness between tail and head of a false triple," to specify that this refers to the number of hops in the graph. Additionally, we suggest that the authors clarify the distinction between knowledge graph completion and link prediction, as well as include a more comprehensive state-of-the-art review that encompasses rule-based approaches.

To enhance the depth of the paper, we advise expanding Section 6.2 to discuss trade-offs between efficiency and accuracy, and to analyze how subgraph sampling impacts training times compared to simpler baselines. Furthermore, a discussion on creating a lighter version of SATKGC for resource-constrained environments would be beneficial.

Lastly, we encourage the authors to provide a detailed example of training a KGC model with SATKGC, along with a discussion of hyperparameter tuning based on various datasets and parameters, to aid in reproducibility and practical application.