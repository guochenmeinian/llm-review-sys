ID: iVINvItqhb
Title: GradSim: Gradient-Based Language Grouping for Effective Multilingual Training
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for selecting language groups for multilingual training based on gradient similarity. The authors propose a two-step approach: first, multilingual training on all available data, followed by computing average gradients from backpropagation in specific languages. The similarity between languages is defined using cosine similarity of averaged gradients, and language groups are formed by solving the set cover problem to minimize similarity. The authors demonstrate that their fine-tuning method is competitive for low-resource languages across various downstream tasks, including AfriSenti, WikiAnn, and UD. However, the reviewers express concerns that the authors may overstate the method's benefits, suggesting it may not significantly outperform baselines.

### Strengths and Weaknesses
Strengths:
- The research addresses a well-motivated problem of optimizing language grouping for multilingual training, particularly benefiting low-resource languages.
- The analysis of knowledge transferability enhances the understanding of the proposed method's effectiveness.
- The method shows small but consistent improvements over baselines in various tasks.

Weaknesses:
- The authors do not provide standard deviations or significance of differences between methods, raising concerns about the robustness of results.
- The claim that the method is data agnostic is disputed, as training data is necessary for computing language-specific gradients.
- The introduction of the methodology lacks clarity, particularly regarding its application for fine-tuning rather than pre-training.
- The improvements observed are relatively small, and the experiments do not cover a wider range of tasks beyond text categorization and sequence annotation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology section, explicitly stating that the method is applied for fine-tuning rather than pre-training. Additionally, we suggest including standard deviations and significance testing for the results to bolster the claims made. The authors should also consider exploring the application of their method in other tasks, such as reading comprehension and question answering, to provide a more comprehensive evaluation. Furthermore, addressing the questions raised regarding the assignment of languages to multiple groups and the handling of gradients during fine-tuning would enhance the paper's depth. Lastly, we encourage the authors to include missing references and correct typographical errors for better presentation.