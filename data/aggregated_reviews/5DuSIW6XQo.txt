ID: 5DuSIW6XQo
Title: Benchmarking Self-Supervised Video Representation Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 6, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark for evaluating self-supervised learning (SSL) methods for videos, addressing the lack of standard benchmarks in the field. The authors conduct extensive experiments on six pretext tasks, three network architectures, five datasets, and two downstream tasks, exploring various aspects such as dataset size, task complexity, data distribution, and noise robustness. They assess the sensitivity of SSL methods to these factors while providing insights into the performance of different pre-trained models, particularly highlighting a method for UCF101 action recognition that outperforms existing approaches with less training data. However, concerns arise regarding the completeness of the primary task—SSL video model evaluation—due to design choices and generalizations.

### Strengths and Weaknesses
Strengths:
- The authors have made a commendable effort to compile relevant video SSL literature and subject it to systematic testing.
- The paper offers a thorough evaluation of various pretext tasks for SSL in video, covering diverse settings.
- The large number of experiments enhances the generalizability of the findings across different video applications.
- The evaluation axes considered are broad, covering important aspects like dataset size and distribution shift, which are often overlooked in other benchmarks.
- The authors provide valuable insights regarding training time, pretext task selection, model capacity, and feature complementarity.
- The paper is well-structured, with detailed analyses throughout.

Weaknesses:
- The analysis relies on outdated methods and small-scale datasets, limiting the generalizability of findings.
- The chosen tasks are primarily high-level categorization, neglecting low-level tasks such as segmentation and tracking.
- The classification scheme for methods is limited, and many recent predictive models are missing from the study.
- Some sections lack motivation, particularly regarding the importance of the chosen axes for dataset size, task complexity, and noise.
- The analysis of architecture performance lacks computational complexity and memory footprint comparisons.
- Certain analyses and discussions are unclear or incomplete, such as the performance peaks in knowledge distillation and the integration of Clip Retrieval and Action Classification sections.
- The paper lacks clarity in defining key terms and acronyms, and the presentation is verbose, affecting overall readability.

### Suggestions for Improvement
We recommend that the authors improve the literature section by avoiding the clubbing of references and presenting a tabular chronological development of the field. This will help clearly define baselines, categories of methods, and provide a rationale for method choices. Additionally, a summary table of recent studies and their contributions should be included to address identified gaps. The authors should justify their choice of methods and tasks, considering the inclusion of more recent models and a broader range of tasks. Furthermore, the analysis of dataset size should extend beyond 100k samples, and the significance of results should be clearly presented. We suggest providing computational complexity and memory footprint comparisons for the architectures analyzed. Clarifying the analysis of knowledge distillation performance and ensuring coherence between the Clip Retrieval and Action Classification sections would enhance the paper's clarity. Lastly, we recommend defining terms and acronyms at their first use and ensuring uniformity in method application across experiments, while also considering using linear evaluation for downstream tasks to better assess the quality of learned representations.