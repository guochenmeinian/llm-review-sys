ID: JzQlGqBm8d
Title: Block Low-Rank Preconditioner with Shared Basis for Stochastic Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 6, 7, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, 2, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel adaptive optimizer that utilizes a block-diagonal preconditioner, approximating the second-moment matrix with a shared low-rank basis across blocks. The authors justify this approach theoretically and empirically, demonstrating its effectiveness on autoencoder and transformer benchmarks, as well as showing improved performance over existing methods like Adam and Block Adam across various tasks, including GPT-2 models. The method aims to improve memory efficiency while maintaining performance, providing optimal regret bounds and comparing favorably against existing methods. The authors also explore the impact of varying the rank parameter \( k \) on performance and discuss the implications of hyperparameter choices in their benchmarks.

### Strengths and Weaknesses
Strengths:
- The theoretical analysis is robust, providing a solid foundation for the proposed method.
- The empirical results validate the method's effectiveness, showing superior or comparable performance to existing optimizers like Adam and Block Adam across multiple tasks.
- The paper is well-structured and clearly presented, making complex concepts accessible.
- The authors effectively address previous reviewer concerns and provide additional experimental results, including those for ResNet models on CIFAR-10.
- The inclusion of detailed tables comparing performance metrics enhances the clarity of the results.

Weaknesses:
- The applicability of the method is limited, primarily focusing on fully-connected layers, with unclear extension to other architectures like convolutional networks.
- The evaluation is narrow, with experiments only conducted on MNIST for autoencoding and lacking tests on larger datasets such as CIFAR-10 and CIFAR-100.
- The choice of a fixed rank \( k=32 \) appears arbitrary, raising questions about its adequacy for larger-scale applications.
- The paper lacks comprehensive evaluation across a wider range of tasks, particularly in image classification, which limits the generalizability of the findings.
- Some reviewers express concerns regarding the adequacy of the hyperparameter tuning process and the overall completeness of the experimental results.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their method by discussing its applicability to convolutional layers and other architectures. Additionally, conducting experiments on larger datasets, such as CIFAR-10 and CIFAR-100, would strengthen the evaluation. We suggest providing a more thorough analysis of the rank \( k \) selection process and its impact on performance, potentially including ablation studies with varying ranks. Furthermore, we recommend that the authors clarify their approach to hyperparameter selection for the autoencoder benchmark and ensure that the results reflect state-of-the-art performance expectations, particularly for the WikiText-103 dataset. Lastly, including training curves for the transformer benchmark and a discussion on time complexity, supported by relevant plots or references, would enhance the understanding of the optimizer's generalization properties.