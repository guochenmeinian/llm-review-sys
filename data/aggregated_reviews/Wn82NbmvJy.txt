ID: Wn82NbmvJy
Title: Accelerating Value Iteration with Anchoring
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 7, 6, 6, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an accelerated version of the Value Iteration (VI) algorithm, termed Anchored Value Iteration (Anc-VI), which incorporates an anchoring mechanism. The authors analyze the convergence rates of Anc-VI, demonstrating that it achieves a rate of $\mathcal{O}(1/k)$ for $\gamma \approx 1$, while standard VI only achieves $\mathcal{O}(1)$. The work includes a complexity lower bound that matches the upper bound up to a constant factor of 4, and extends results to approximate value iteration and Gauss-Seidel variations.

### Strengths and Weaknesses
Strengths:
- The application of anchoring to VI is novel and yields significant results, particularly in the case of $\gamma = 1$.
- The paper provides a thorough review of existing literature, situating its contributions within the broader context of reinforcement learning (RL).
- The contributions are solid, including accelerated convergence rates and extensions to inexact and Gauss-Seidel variants.

Weaknesses:
- The terminology used, particularly regarding the definition of "rate," is somewhat unclear, leading to potential confusion.
- There is a lack of numerical comparisons with existing methods, making it difficult to assess the practical performance of the proposed algorithms.
- The paper does not adequately discuss how to derive near-optimal policies from Anc-VI, nor does it clarify the implications of the choice of the initial point $U_0$.

### Suggestions for Improvement
We recommend that the authors improve the clarity of terminology, particularly by defining "rate" and addressing the inaccuracies in statements regarding convergence rates. Additionally, we suggest including numerical comparisons with existing methods to validate the performance of Anc-VI. The authors should also elaborate on how to derive near-optimal policies from Anc-VI and clarify the implications of the choice of $U_0$ on the algorithm's performance. Furthermore, it would be beneficial to discuss the motivation for using the Bellman error as a performance measure compared to the distance to the optimal value function.