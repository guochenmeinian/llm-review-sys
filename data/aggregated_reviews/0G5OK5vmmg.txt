ID: 0G5OK5vmmg
Title: WenMind: A Comprehensive Benchmark for Evaluating Large Language Models in Chinese Classical Literature and Language Arts
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 5, 8, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 1, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents WenMind, a benchmark for evaluating large language models (LLMs) in Classical Chinese Literature and Language Arts (CCLLA), comprising 4.8K question-answer pairs across 42 tasks and three sub-domains. The authors collect data from multiple sources and design a deliberate process pipeline for dataset generation, performing evaluations on various models to demonstrate the benchmark's value. The paper addresses critical research questions regarding LLMs' performance in CCLLA, highlighting the challenges posed by linguistic distinctions between Classical and Modern Chinese. Additionally, the authors propose to enhance their methodology by providing datasets in both simplified and traditional Chinese characters and conducting separate evaluations to compare results.

### Strengths and Weaknesses
Strengths:
- The tasks for WenMind are deliberately designed and cover a broad range of aspects in Chinese literature and language arts.
- The paper conducts extensive experiments across 31 LLMs, providing valuable insights into model performance.
- It is well-written, with a clear organization that facilitates understanding of data construction and evaluation.
- The authors acknowledge the importance of using both simplified and traditional characters in their evaluations, demonstrating a commitment to thoroughness.
- The proposal to include comparative analysis of results from different character sets enhances the methodological rigor.

Weaknesses:
- The paper lacks detailed explanations of the data construction process, particularly in question crafting and data post-processing.
- Most samples are written in Simplified Chinese, which may not accurately reflect Classical Chinese, raising concerns about data quality.
- There is insufficient analysis of data contamination and a lack of quantitative comparisons with existing benchmarks.
- The current experimental results are limited to three models due to space and time constraints, which may not provide a comprehensive view of the models' performance.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the data construction process, particularly how questions are crafted from existing knowledge data. Additionally, we suggest providing a reference in sections 3 and 4 to enhance the paper's self-containment. An analysis of data contamination should be included, along with a quantitative comparison with other benchmarks to demonstrate WenMind's comprehensiveness. Furthermore, we encourage the authors to replace all data written in Simplified Chinese with the original Traditional Chinese characters to accurately assess LLM capabilities in understanding Classical Chinese. Lastly, we recommend that the authors include complete experimental results and analysis in the supplementary materials for the final version to enhance the depth of their findings. A more thorough discussion of the human verification process and error analysis would also strengthen the paper.