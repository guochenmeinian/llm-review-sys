ID: G0LfcMiRkc
Title: Linguistic Collapse: Neural Collapse in (Large) Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 5, 8, 7, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper empirically investigates the emergence of Neural Collapse (NC) properties during the training of causal language models (CLMs). The authors define NC as a phenomenon where last-layer class-mean embeddings become equinorm, have maximal angular separation, and align with their respective classifiers. However, NC typically requires conditions that do not hold for language models, such as training beyond the zero training error regime and balanced classes. The authors introduce *generalized* NC metrics to assess the geometrical properties of last-layer embeddings and classifiers, and analyze their correlation with validation loss across different training regimes using NeoGPT.

### Strengths and Weaknesses
Strengths:  
- The problem setup is intriguing as it extends previous observations in deep learning to language models, despite their distinct configurations.  
- The connection between NC and LLM training is well-articulated, and the introduction of new NC metrics is viable for LLMs and other classification regimes.  
- Extensive experiments on the TinyStories dataset establish a correlation between the proposed metrics and validation loss.

Weaknesses:  
- The suitability of the suggested NC metrics for language setups is questionable, particularly regarding the ambiguity of language next-token prediction and the imbalanced nature of language datasets.  
- The correlation coefficients in the results suggest a low relationship between NC properties and validation loss, raising concerns about the influence of model size and architecture.  
- The paper lacks clarity on the motivation for using the TinyStories dataset, and the practical value of the proposed metrics beyond standard validation methods is unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the proposed NC metrics' applicability to language models, particularly addressing the ambiguity in next-token predictions. Additionally, we suggest providing further justification for the use of the TinyStories dataset and conducting experiments on real data to confirm observed trends. It would be beneficial to clarify the relationship between NC metrics and generalization, especially in light of the negligible correlation observed for NC3. Finally, we encourage the authors to explore additional evaluation metrics beyond validation loss to validate the relationship between NC properties and LLM quality.