ID: yVu5dnPlqA
Title: MAmmoTH2: Scaling Instructions from the Web
Conference: NeurIPS
Year: 2024
Number of Reviews: 24
Original Ratings: 8, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a three-stage pipeline for harvesting large-scale instruction data from pre-training web corpora to enhance LLM reasoning. The stages include recalling relevant documents, extracting instruction-response pairs using LLMs, and refining these pairs by completing intermediate reasoning steps. The authors empirically validate the effectiveness of scaling instruction data, achieving state-of-the-art performance with their `MAmmoTH2-Plus` models on various reasoning datasets. They also introduce the `WebInstruct` dataset as a unique public resource for reasoning tasks and conduct extensive ablation studies that provide valuable insights into the synthesis of instruction data. Additionally, the paper evaluates the proposed pipeline's effectiveness and efficiency through various configurations using the DeepSeek Coder V 1.5 7B model, leading to key findings regarding cost-effectiveness, extraction efficiency, and the importance of refinement in improving answer quality.

### Strengths and Weaknesses
Strengths:
- The paper effectively demonstrates the feasibility of scaling instruction data to 10 million pairs, addressing an important empirical question.
- It synthesizes high-quality prompts and answers from web corpora, which is a crucial yet under-explored area for data augmentation.
- The `MAmmoTH2-Plus` models show superior or comparable performance to existing state-of-the-art models across diverse reasoning benchmarks.
- The pipeline achieves superior performance compared to DeepSeek Math models while using significantly fewer tokens, demonstrating cost-effectiveness across a range of reasoning tasks.
- The "Extract" step is efficient, maintaining performance with fewer tokens, and the "Refine" step significantly enhances answer quality.
- Comprehensive experiments validate the method's effectiveness, and the paper is well-written and clear.

Weaknesses:
- There is a need for further consideration regarding the compatibility of training on `WebInstruct` with existing training pipelines for optimal performance.
- The comparison in Table 7 may be unfair due to the broader domains of "Public Datasets" versus `WebInstruct`, particularly regarding the inclusion of code generation tasks.
- The marginal gains from adding `WebInstruct` for strong models suggest its limited effectiveness.
- The paper lacks detailed discussions on the necessity and impact of the extract and refine steps compared to existing methods.
- There is a slight performance decrease associated with the "Extract" step after SFT, raising questions about the initial model choice and its impact on results.
- Potential scalability limitations may arise from distilling larger models.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the compatibility of `WebInstruct` with existing training pipelines and explore the impacts of continual pre-training, training on `WebInstruct`, and final fine-tuning on additional instruction datasets. Additionally, we suggest conducting controlled experiments to demonstrate the necessity of the extract and refine steps, specifically comparing `(a) Recall + (d) SFT` and `(a) Recall + (b) Extract + (d) SFT` using the 18M recalled documents. We also recommend conducting a comparison between `Base + (a) + (b) + (c) + (d)` and `Base + (a’) + (d)` to exclude the influence of SFT datasets. Furthermore, we suggest comparing `Base + (a) + (b) + (c) + (d)` with `Base + (a’) + (a) + (b) + (c) + (d)` to evaluate the effect of WebInstruct combined with DeepSeekMath continual pre-training. Finally, we encourage the authors to explore training the DeepSeek Coder with more natural text tokens or conducting experiments on Mistral-7B to verify the impact of the initial model choice on performance, and to address potential biases introduced by the web-sourced data while discussing related works, such as "Self-alignment with Instruction Backtranslation." Including more details on the methods and prompts used in the extraction and refinement stages would enhance reproducibility and understanding.