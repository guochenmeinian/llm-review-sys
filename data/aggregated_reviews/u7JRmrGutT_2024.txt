ID: u7JRmrGutT
Title: Graph Edit Distance with General Costs Using Neural Set Divergence
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 4, 7, 7, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GRAPHEDX, a neural model designed to estimate Graph Edit Distance (GED) with customizable edit costs. It represents graphs as sets of node and edge embeddings and utilizes a Gumbel-Sinkhorn permutation generator to capture the nuances of graph structure affecting edit distances. The authors propose a method that learns alignments to compute surrogates for edit operations, addressing both equal and unequal cost scenarios. Experiments demonstrate that GRAPHEDX outperforms state-of-the-art baselines across various datasets, particularly in cases with unequal edit costs.

### Strengths and Weaknesses
Strengths:
- The handling of GED with unequal costs is a timely and under-explored topic, relevant to many real-world applications.
- The paper includes rigorous evaluations of both accuracy and efficiency, showing high performance while maintaining computational efficiency.
- The authors propose generating learnable embeddings for disconnected node pairs, which is a novel contribution.

Weaknesses:
- The clarity of the paper needs improvement, particularly regarding notation inconsistencies and the self-containment of related work.
- The technical contribution is somewhat limited, as it relies heavily on existing methods like MPNNs and the Gumbel-Sinkhorn network.
- The experiments are conducted on relatively small graphs, raising questions about scalability to larger graphs and the model's performance with suboptimal supervision signals.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by addressing notation inconsistencies, such as the differences between the pad indicator $q$ and $Î·$. Additionally, we suggest relocating the related work to the main paper and including missing references that influence the study. To enhance the technical contribution, consider exploring alternative methods beyond MPNNs and the Gumbel-Sinkhorn network. We also encourage the authors to conduct experiments on larger datasets, such as the IMDB dataset, to assess scalability and performance under various conditions. Finally, providing a detailed analysis of the model's sensitivity to the number of refinement iterations and the necessity of initial feature inclusion for baselines would strengthen the paper.