ID: dVfNPSzpnv
Title: IMDL-BenCo: A Comprehensive Benchmark and Codebase for Image Manipulation Detection & Localization
Conference: NeurIPS
Year: 2024
Number of Reviews: 20
Original Ratings: 7, 6, 8, 9, 9, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 2, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents IMDL-BenCo, a comprehensive benchmark and modular codebase for Image Manipulation Detection and Localization (IMDL). The authors propose a standardized framework that includes eight state-of-the-art models, two sets of training and evaluation protocols, 15 GPU-accelerated metrics, and three types of robustness evaluations. The work addresses critical challenges in the IMDL field, such as inconsistent evaluation protocols and the lack of open-source models, thereby facilitating rigorous experimentation and fair comparisons among IMDL models.

### Strengths and Weaknesses
Strengths:
1. The introduction of a unified benchmark and modular codebase addresses a critical need in the IMDL field, promoting standardization and fair comparisons.
2. The modularity of the codebase allows for easy customization and extension by researchers.
3. The paper provides insightful analyses and discussions on IMDL model architecture, dataset characteristics, and evaluation standards.

Weaknesses:
1. The benchmark includes only four datasets, limiting the demonstration of cross-dataset generalization capabilities.
2. The evaluation primarily relies on the F1-score, which does not fully capture model performance; additional metrics like IoU and AUC should be included.
3. Robustness evaluations are limited to three perturbation types, necessitating a broader range of evaluations to reflect real-world applications.
4. The analysis of feature extractors lacks depth, particularly regarding the finding that low-level feature extractors are not suitable for IMDL.
5. The visualization codebase could benefit from standardized modules for image forgery localization maps.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by including more datasets to enhance cross-dataset generalization. Additionally, incorporating metrics such as IoU and AUC would provide a more comprehensive evaluation of model performance. Expanding robustness evaluations to include more perturbation types is essential for real-world applicability. We suggest that the authors provide a more detailed analysis of feature extractors to better interpret their findings. Lastly, incorporating standardized visualization modules into the codebase would enhance usability for researchers.