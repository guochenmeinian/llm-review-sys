ID: AegCFewVum
Title: M$^2$-VLP: Enhancing Multilingual Vision-Language Pre-Training via Multi-Grained Alignment
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the M2-VLP model, which enhances multilingual vision-language pre-training (mVLP) through a novel multi-grained alignment approach that captures fine-grained correlations across languages and modalities. The methodology is well-structured, detailing innovative tasks such as region-token and token-level contrastive learning. The results demonstrate significant performance improvements across various vision-language tasks in multiple languages, addressing limitations of existing coarse-grained models.

### Strengths and Weaknesses
Strengths:
- The model captures deeper semantic associations through fine-grained alignment.
- It achieves high performance across diverse languages and tasks.
- The integrated contrastive learning framework provides consistent alignment across modalities.

Weaknesses:
- The computational cost is high due to the complex training structure.
- Insufficient detail on specific pretrained datasets and their roles in loss computation.
- The novelty of the individual loss functions is questionable, as they are publicly available and appear to be combined without significant innovation.
- The paper lacks a clear comparison with contemporary visual grounding methods, making it difficult to assess its advancements.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the implementation details for multi-grained contrastive learning, including mathematical derivations and intuitive explanations. Additionally, providing a thorough description of the pretrained datasets and their contributions to the model's performance would enhance reproducibility. We suggest including more comparisons with contemporary visual grounding methods to better contextualize the model's contributions. Furthermore, addressing the computational trade-offs and clarifying the interactions between different components of the model could strengthen the paper's claims.