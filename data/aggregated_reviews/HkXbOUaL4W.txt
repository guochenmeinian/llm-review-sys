ID: HkXbOUaL4W
Title: Back Transcription as a Method for Evaluating Robustness of Natural Language Understanding Models to Speech Recognition Errors
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to evaluate the robustness of Natural Language Understanding (NLU) models against speech recognition errors by repurposing NLU training data and utilizing Text-to-Speech (TTS) models to generate synthetic utterances. The generated utterances are processed by Automatic Speech Recognition (ASR) models to simulate errors, allowing for a comparison of NLU model performance on original versus TTS-generated text. The authors propose various error metrics and a logistic regression model to analyze the impact of ASR errors on NLU robustness.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel methodology for assessing NLU robustness without requiring spoken data, enhancing data efficiency.
- It provides valuable insights into the robustness of Transformer-based NLU models and opens avenues for further research.
- The findings indicate that synthetic audio does not significantly differ from real recordings in terms of robustness assessment.

Weaknesses:
- The paper lacks originality, as it does not demonstrate significant insights beyond existing ASR and NLU metrics.
- There is limited comparison with other NLU architectures, and the reliance on TTS models may overlook inherent limitations affecting robustness assessments.
- The absence of evaluation on real spoken data and comprehensive analysis of error types limits the generalizability of the findings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper's main aim, ensuring it distinguishes between dataset augmentation and robustness assessment. Additionally, we suggest incorporating WER/F1 rates in the main text to strengthen the argument for the proposed metrics. A broader comparison with various NLU architectures and a more detailed discussion on the differences between synthesized and real speech, including ASR quality metrics, would enhance the paper's impact. Finally, we encourage the authors to consider evaluating their method on real spoken data to validate the findings further.