ID: up8EYzyrKV
Title: Towards Mitigating LLM Hallucination via Self Reflection
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the existence and mitigation of hallucination in answers generated by large language models (LLMs) in the medical domain. The authors classify hallucinated answers into three types and report their distribution across five LLMs. They propose an iterative refinement method that reduces hallucination by refining both the generated background knowledge and the answers based on that knowledge. The experiments demonstrate the effectiveness of this method, particularly on biomedical datasets.

### Strengths and Weaknesses
Strengths:  
1. The paper provides a detailed analysis of hallucination in LLMs, addressing a critical weakness in generative medical question-answering systems.  
2. An innovative method is proposed that does not rely on external references, showcasing potential applicability in other domains.  
3. The study includes sufficient experiments to validate the method's effectiveness across various modern language models.

Weaknesses:  
1. The method's novelty is questioned, as it has similarities to existing work, and the authors need to clarify its uniqueness.  
2. There are concerns regarding the reliability of using LLMs as knowledge sources, particularly the lack of comparison with established knowledge bases.  
3. The sample size for analysis is considered too small, and the overall annotation process lacks detail.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by elaborating on the uniqueness of their method compared to existing literature. Additionally, the authors should justify their choice of LLMs and consider including a correlation study between the model's factuality predictions and human evaluations to enhance reliability. Furthermore, we suggest that the authors provide more comprehensive details on the annotation process and consider retrieving related knowledge from a corpus instead of solely relying on LLM-generated knowledge. Lastly, we advise revising the title to better reflect the paper's focus, such as "Towards Mitigating Hallucination in Medical Question Answering with Large Language Models via Self-Reflection."