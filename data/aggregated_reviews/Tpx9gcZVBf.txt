ID: Tpx9gcZVBf
Title: DiffAug: A Diffuse-and-Denoise Augmentation for Training Robust Classifiers
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 7, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DiffAug, a novel method for training robust image classifiers using diffusion-based data augmentation. DiffAug applies Gaussian noise to training images followed by a single diffusion denoising step, enhancing classifier robustness and performance across various metrics. The method is computationally efficient and can be integrated with other augmentation techniques, achieving notable improvements in classifier generalization, gradient quality, and image generation performance. Additionally, the authors note that augmentations can be generated using Diffusion-Transformers (DiTs) by applying the VAE decoder to the one-step denoised latent representation. They express intent to include results from experiments with DiTs in the final revision.

### Strengths and Weaknesses
Strengths:
1. The writing and presentation are clear and easy to follow.
2. The finding that training with degraded images from the diffusion process can enhance classifier performance is intriguing.
3. The experiments are comprehensive, demonstrating the effectiveness of DiffAug across diverse datasets.
4. The authors demonstrate a clear understanding of the theoretical application of DiffAug with DiTs.
5. They are responsive to reviewer feedback, committing to include additional experiments with conditional diffusion models and class-conditioning using DiTs.

Weaknesses:
1. The paper lacks an ablation study on sampling methods and processes, raising questions about the choice of a single diffusion denoising step.
2. There is insufficient comparison with existing methods that utilize diffusion models for generating training data, which could provide context for DiffAug's contributions.
3. The robustness evaluation is limited to specific datasets, leaving questions about performance under varying conditions.
4. The authors were unable to report results from experiments with DiTs during the rebuttal discussion due to time and resource constraints.
5. There is a lack of empirical evidence presented in the current submission regarding the effectiveness of DiffAug with DiTs.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the statement on prior studies related to denoised examples, particularly in relation to reference [38]. Additionally, the authors should elaborate on why Eq. 5 is considered an "additional optimization objective" and clarify its distinction from standard classification loss. We suggest investigating the performance of DiffAug with conditional diffusion models to explore potential enhancements in robustness. Furthermore, including ablation studies on the impact of different sampling methods and diffusion models would strengthen the paper. We also recommend that the authors improve the final version by including results from experiments with Diffusion-Transformer models, specifically focusing on class-conditioning. Lastly, addressing the discrepancies in reported performance metrics compared to existing literature would provide a clearer understanding of DiffAug's effectiveness.