ID: DHVqAQ9DHy
Title: Posterior Label Smoothing for Node Classification
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 2, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents PosteL, a label smoothing method that utilizes posterior distribution for node classification in graph-structured data. The authors propose a preprocessing technique that generates soft labels based on local neighborhood context and global label statistics prior to training. Extensive experiments demonstrate significant improvements in classification accuracy across various neural network models and datasets. Additionally, the authors analyze loss curves for GT and PosteL labels, clarifying discrepancies between figures in the main paper and an attached PDF due to varying experimental settings, including hyperparameters and the number of splits used for calculating mean loss. They discuss early stopping practices, noting that while GT labels can achieve comparable performance to PosteL, the evaluation is based on the lowest validation loss, with PosteL often resulting in lower test loss, indicating reduced overfitting.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow, with clear and intuitive figures.
2. PosteL is simple yet effective, allowing seamless integration with existing methods.
3. The empirical results are significant, showcasing improvements across multiple datasets.
4. The authors provide a clear explanation for the discrepancies in loss curves, addressing potential confusion regarding experimental settings.
5. The discussion on early stopping and its implications for performance comparison between GT and PosteL is insightful.

Weaknesses:
1. The authors do not explore the parameter sensitivity of PosteL, which could limit the credibility of the experiments.
2. The novelty of PosteL is unclear due to insufficient differentiation from other label smoothing methods for node classification.
3. The reliance on global label statistics may introduce bias, especially in imbalanced datasets.
4. The paper lacks a theoretical motivation or analysis, which would enhance its scientific rigor.
5. The initial response to the early stopping question may have lacked clarity, leading to miscommunication.
6. The explanation regarding the limited benefits of label smoothing for GPR-GNN or BernNet could be more explicitly included in the paper.

### Suggestions for Improvement
We recommend that the authors improve the discussion of related works that combine label smoothing with Graph Neural Networks (GNNs), such as [1] and [2], to provide a comprehensive context. Additionally, a direct comparison with the approach in [1] would clarify the novel contributions of this study. The authors should also address the iterative pseudo-labeling technique, explaining how their application differs from previous uses. Furthermore, a deeper analysis of the computational complexity and a sensitivity analysis of hyperparameters α and β are essential for validating the method's performance. Lastly, clarifying the classification task using the preprocessed smooth labels would enhance accessibility for readers. We also recommend that the authors improve clarity regarding the differences in experimental settings between the main paper and the attached PDF to prevent confusion, and explicitly include the explanation about the limited benefits of label smoothing for GPR-GNN or BernNet in the revised version to enhance understanding.