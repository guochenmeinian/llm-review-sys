ID: 4FpuOMoxsX
Title: On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities
Conference: thecvf
Year: 2024
Number of Reviews: 1
Original Ratings: 6
Original Confidences: 4

Aggregated Review:
**Key Points**  
The manuscript addresses the critical issue of safety risks associated with the deployment of language and vision-language models in robotics, particularly their vulnerability to adversarial attacks. It emphasizes the need for robust countermeasures to ensure safe integration as these models become more prevalent.

**Strengths and Weaknesses**  
We appreciate the detailed experiments conducted to assess the vulnerability of robotic systems using LLMs and VLMs, as the use of multiple types of adversarial attacks across various robotics frameworks offers a comprehensive perspective. However, we note that the perception attack methods employed are limited to traditional techniques, such as image blurring, rotation, and object addition, which detracts from the manuscript's depth. Additionally, the authors should address the limitations of their work.

**Suggestions for Improvement**  
We suggest improving the manuscript by investigating state-of-the-art attacking techniques to enhance the analysis of vulnerabilities. Furthermore, we encourage the authors to include a discussion on the limitations of their study to provide a more balanced view.