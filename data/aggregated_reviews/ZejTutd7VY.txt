ID: ZejTutd7VY
Title: TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TrojPrompt, a framework for executing backdoor attacks on large-scale language models (LLMs) through API-driven trigger discovery and progressive prompt poisoning. The method synthesizes poisoned prompts and adversarial triggers, achieving high accuracy on NLP tasks while maintaining clean test accuracy. The evaluation demonstrates TrojPrompt's effectiveness across various few-shot text classification tasks.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel approach to prompt-based attacks on LLMs, addressing a gap in existing literature focused primarily on image classification.
- The evaluation is comprehensive, covering multiple datasets and models, and includes a thorough ablation study.
- The writing is clear and accessible, making complex concepts easier to understand.

Weaknesses:
- The proposed attack resembles a universal adversarial attack more than a traditional trojan attack, as it does not modify the target model. Clarification of the attack's definition is needed.
- The threat model lacks justification; it is unclear why an attacker would want to generate a prompt that causes the PLM to misbehave while also being a user of the PLM.
- The evaluation is limited to older models, and it would be beneficial to assess the attack's effectiveness on mainstream models like ChatGPT.
- The paper does not adequately differentiate its contributions from those of RLPrompt, leading to potential confusion about originality.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the attack definition to better align it with traditional trojan attacks. Justifying the threat model is essential; the authors should explain the attacker's incentives for using a poisoned prompt. Additionally, we suggest including evaluations on mainstream LLMs such as ChatGPT to demonstrate broader applicability. The authors should explicitly state the contributions derived from RLPrompt to avoid misleading readers. Finally, providing concrete examples of poisoned prompts generated by the attack would enhance the paper's comprehensibility and impact.