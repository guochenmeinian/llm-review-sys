ID: B01gPP5YCh
Title: Parameter-Efficient Prompt Tuning Makes Generalized and Calibrated Neural Text Retrievers
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on parameter-efficient (PE) tuning for neural text retrieval, demonstrating that PE-tuned text retrievers outperform fine-tuned models across various tasks and datasets. The authors provide insights into the generalizability of PE tuning, particularly in cross-domain and cross-topic retrieval tasks. Additionally, they introduce a new fine-grained topic-specific academic retrieval dataset, OAG-QA, which contains 87 domains and 17,948 query-paper pairs.

### Strengths and Weaknesses
Strengths:
- The paper introduces PE tuning to neural text retrieval, showcasing a novel approach that enhances generalization capabilities.
- It provides empirical insights into the factors contributing to the superior performance of PE-tuned models, which may inform future research.
- The construction of the OAG-QA dataset represents a significant contribution to the NLP community.

Weaknesses:
- The paper lacks novel methodologies, primarily applying existing PE learning techniques and comparing them to established fine-tuning methods, limiting its innovation scope.
- The experiments are conducted solely on the DPR and ColBERT models, raising questions about the generalizability of the findings to other models.
- There is insufficient in-depth analysis regarding the impact of factors such as confidence calibration and query length on performance differences.
- The dataset's broader applicability and quality for other NLP tasks remain unclear.

### Suggestions for Improvement
We recommend that the authors improve the depth of their analysis regarding the performance factors of PE tuning, particularly by providing a more thorough explanation of how confidence calibration and query length influence outcomes. Additionally, we suggest conducting experiments with a wider range of models beyond DPR and ColBERT to validate the generalizability of their conclusions. Clarifying the reasons behind the low retrieval results of the Adapter method in Table 1 would also enhance the paper's clarity. Finally, an experimental analysis of the parameters of the P-Tuning v2 model should be included to strengthen the findings.