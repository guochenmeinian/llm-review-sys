ID: AQyqxXctsN
Title: Differentiable Sampling of Categorical Distributions Using the CatLog-Derivative Trick
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 3, 7, 4, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a variance-reduced statistic for $\mathbb{E}_{x \sim p(x)} [f(X)]$, utilizing Rao-Blackwellization to improve gradient estimation for tasks such as stochastic optimization. The authors propose an estimator called CateR, which is unbiased and has lower variance than existing methods like REINFORCE. A specialization for independent variables, termed IndeCateR, is also introduced. The paper includes empirical results demonstrating the effectiveness of the proposed methods across various tasks, including discrete VAEs and neurosymbolic applications. Additionally, the authors engage thoroughly with reviewer comments, indicating a willingness to improve the manuscript. They propose clarifications regarding the use of "AI accelerators," specifically GPUs, and address practical difficulties in selecting hyperparameters for GS. Revisions on DVAE experiments enhance understanding of their motivation and reproducibility.

### Strengths and Weaknesses
Strengths:
- The exposition is clear and the results are convincing, with comprehensive experiments that illustrate the effectiveness of the proposed methods.
- The paper addresses a significant problem in gradient estimation and presents an elegant solution that is interpretable as Rao-Blackwellization.
- The empirical link between gradient variance and optimization speed is well-articulated, particularly in Figure 2.
- The authors demonstrate responsiveness to reviewer feedback, indicating a commitment to refining the manuscript.
- The revisions improve clarity regarding DVAE experiments and hyperparameter selection.

Weaknesses:
- The novelty of the approach is questioned, as it closely resembles existing methods like Local Expectation Gradients (LEG) without sufficient discussion of their similarities and differences.
- Some experiments lack clarity, particularly regarding the performance of the CatLog trick, which is not demonstrated in practice.
- The computational complexity of the proposed methods is higher than REINFORCE, raising concerns about efficiency.
- There is a need for further clarification on the use of cosine distance to obtain a scalar.
- The manuscript may benefit from addressing potential alternative methods, as some readers might be distracted by these considerations.

### Suggestions for Improvement
We recommend that the authors clarify the connection between the statistic for $\mathbb{E}_{x \sim p(x)} [f(X)]$ and the gradient of a loss function in the text. Additionally, we suggest including a comparison with LEG and discussing its implications more thoroughly. It would be beneficial to provide experiments demonstrating the performance of the CatLog trick and to explore the proposed method's applicability to dependent variables. We also recommend improving the clarity of how cosine distance is utilized to derive a scalar in the revisions. Furthermore, we suggest that the authors explore the hypothesis of substituting a linear decoder to see if similar behavior occurs. Lastly, we advise revising the manuscript to explicitly state the neurosymbolic nature of the task, ensuring that readers understand the focus on boolean logical statements and the rationale for not pursuing alternative methods.