ID: wpsbUYi9nN
Title: Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LLM4CS, a framework that enhances conversational search performance by employing a Large Language Model (LLM) for rewriting queries and responses. The authors conduct extensive evaluations on CAsT-19, CAsT-20, and CAsT-21 benchmarks, demonstrating that their methodology significantly outperforms previous state-of-the-art (SOTA) methods. The framework includes three techniques: Rewriting (REW), Rewriting-Then-Response (RTR), and Rewriting And-Response (RAR), which, when combined with an ANCE-based retriever, lead to notable improvements in retrieval accuracy.

### Strengths and Weaknesses
Strengths:
- LLM4CS shows remarkable performance improvements over prior conversational query rewriting (CQR) and conversational dense retrieval (CDR) methods across multiple benchmarks.
- The framework is straightforward yet effective, providing a solid empirical basis for its claims through extensive experimentation.

Weaknesses:
- The paper lacks a discussion on the efficiency and trade-offs of using the LLM (gpt-3.5-turbo-16k) for all prompts, leaving questions about its performance relative to other models and retrieval methods.
- The novelty of the framework is limited, as it primarily combines existing techniques without introducing significant new insights or findings.
- There is insufficient comparative analysis with other methods, particularly the hybrid CRDR baseline, and the generalizability of results is questioned due to reliance on a single LLM.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the efficiency of LLM4CS, including comparisons with other retrieval methods like traditional keyword retrieval (e.g., BM25). Additionally, conducting experiments with multiple open-source LLMs of varying scales would enhance the robustness of the findings. The authors should also provide a clearer justification for the use of LLM-generated content and include insights into how it outperforms previous methods like T5QR. Furthermore, expanding the scope to include knowledge-intensive tasks and exploring alternative strategies for retrieval effectiveness could significantly strengthen the paper's contributions. Lastly, clarifying the aggregation methods and ensuring comprehensive comparisons in Table 2 would improve the overall clarity and rigor of the study.