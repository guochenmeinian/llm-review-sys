ID: sla7V80uWA
Title: Beyond MLE: Convex Learning for Text Generation
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 5, 7, 7, 3, 7, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 4, 5, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel loss function for training text generation models, extending Maximum Likelihood Estimation (MLE) by replacing the logarithm with a convex increasing function. The authors demonstrate that this approach leads to sharper model distributions, enhancing performance in both autoregressive (AR) and non-autoregressive (NAR) settings. Experimental results indicate improvements in translation quality and a reduction in the gap between greedy and beam search outputs.

### Strengths and Weaknesses
Strengths:
1. The theoretical derivation is detailed and provides a solid foundation for the proposed loss function.
2. The final loss function is simple and practical, facilitating implementation.
3. Experimental results show superiority over MLE in both AR and NAR models.
4. The approach effectively narrows the gap between greedy and beam search outputs.

Weaknesses:
1. Key experimental hyperparameters, such as the value of T, are not provided.
2. The experimental analysis is limited to comparisons with MLE, lacking evaluations against other loss functions.
3. Notation is occasionally confusing, with terms used inconsistently across different sections.
4. The focus on one-hot learning may limit applicability to decoding strategies like random sampling.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their notation to avoid confusion, particularly regarding terms that have multiple meanings. Additionally, the authors should include key experimental hyperparameters in the main text or appendix to enhance reproducibility. A broader comparison with other loss functions would strengthen the experimental analysis. Furthermore, we suggest discussing the limitations of the proposed method more explicitly in the paper. Finally, addressing the questions regarding the gradient behavior of the proposed loss and the tuning of hyperparameters would provide valuable insights into the model's performance.