ID: ES32O8mBK3
Title: H3T: Efficient Integration of Memory Optimization and Parallelism for Large-scale Transformer Training
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 4, 6, 5, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for optimizing the integration of memory and parallelism in training large language models (LLMs). The authors propose a memory-latency co-optimization training system that addresses the NP-hard problem of optimizing memory consumption and latency through three different solver searching algorithms. The evaluation shows that the proposed optimizations achieve up to 4.3x speedup and 80.5% less memory overhead compared to state-of-the-art frameworks like DeepSpeed.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized, easy to follow, and well-motivated.
- The proposed techniques are sensible and demonstrate significant reductions in memory usage and latency.
- H3T is the first attempt to automatically integrate memory optimization and parallelism strategies for large Transformer-based models, showing improved training performance.

Weaknesses:
- The contribution and technical details, particularly regarding the DP solver, require further clarification.
- A breakdown analysis of specific optimizations contributing to memory and latency reduction is lacking.
- Some figures have small font sizes, making them difficult to read.
- The paper does not adequately explain how pipeline parallelism is incorporated into H3T.
- The configuration details for DeepSpeed used in the experiments are insufficient for evaluating the significance of the results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the contribution by providing more details on the optimizations discussed in Section 3.3.2, especially regarding the DP solver. Additionally, including a breakdown analysis of the specific optimizations (e.g., re-computation, parallelism) that contribute to memory and latency reduction would enhance understanding. We also suggest increasing the font size of figures for better readability and providing a clearer explanation of the differences between ZeRO-2 and ZeRO-3. Furthermore, including a comparison with other automatic optimizers, such as Alpa or Unity, would strengthen the evaluation of H3T. Lastly, clarifying the configuration used for DeepSpeed in the experiments would allow for a more thorough assessment of the results.