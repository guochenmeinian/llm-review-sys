ID: el0ESsfbHT
Title: On Adaptive Knowledge Distillation with Generalized KL-Divergence Loss for Ranking Model Refinement
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, 1, 1, 2, -1
Original Confidences: 4, 3, 4, 4, 5

Aggregated Review:
### Key Points
This paper presents a method for knowledge distillation in training student neural document ranking models, addressing the limitations of traditional KL-divergence loss by proposing a weighted KL-Divergence (WKL) loss that incorporates ground-truth labels. The authors provide a detailed theoretical analysis of WKL, including its lower bound properties and gradient contributions. The evaluation compares WKL against state-of-the-art methods on the MS MARCO dataset, although some results lack statistical significance.

### Strengths and Weaknesses
Strengths:  
- The paper is well-motivated, clearly written, and organized, effectively identifying issues with existing BKL methods and proposing a solution with theoretical backing.  
- The evaluation setup is comprehensive, demonstrating the effectiveness of WKL over various baselines.

Weaknesses:  
- The experimental design is complex, with multiple teachers and training stages that are not adequately justified.  
- Some results in the evaluation are less pronounced and not statistically significant, raising questions about the robustness of the findings.  
- Certain aspects of the methodology, such as the introduction of a "special version of WKL" and the rationale for specific teacher-student pairings, lack clarity.

### Suggestions for Improvement
We recommend that the authors improve the organization of the experiments to clarify the advantages of the proposed approach, potentially separating comparisons with state-of-the-art methods into a different table. Additionally, we suggest that the authors provide more details on the "special version of WKL" and justify the pairing of teacher and student models. It would also be beneficial to explain the "warmup" process mentioned in Section 5.2. Finally, we encourage the authors to include more recent state-of-the-art baselines, such as SPLADE++ and ColBERTv2, to strengthen the evaluation.