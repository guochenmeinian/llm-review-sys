ID: FDfrPugkGU
Title: DoFIT: Domain-aware Federated Instruction Tuning with Alleviated Catastrophic Forgetting
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 4, 7, 8, 6, -1, -1
Original Confidences: 5, 4, 5, 4, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel domain-aware Federated Instruction Tuning (DoFIT) framework designed to address domain-aware data heterogeneity and alleviate catastrophic forgetting in collaborative training across datasets. DoFIT-base aggregates domain-specific information on the intra-domain server side and domain-agnostic information on the inter-domain server side, thereby reducing interference from other domains. The authors demonstrate that DoFIT outperforms conventional FIT methods through extensive experiments on datasets from finance, general, and medical domains.

### Strengths and Weaknesses
Strengths:  
1. The aggregation of domain-specific and domain-agnostic information in a less-conflicted parameter space is a compelling and original approach to addressing domain-aware data heterogeneity.  
2. The significant performance improvements over conventional FIT methods, supported by comprehensive analysis, pave the way for future advancements in FIT.  
3. The exploration of multi-domain considerations in the federated learning context provides valuable insights into the field.  

Weaknesses:  
1. The choice of comparison methods (FIT_{32qv}, FIT_{16qvd}, and FIT_{32d}) in a single domain is unclear; additional LoRA modules may enhance the analysis.  
2. The addition of more servers to manage varying heterogeneities increases communication burdens and security risks.  
3. The variability of parameters across datasets complicates the generalization of DoFIT to other datasets.  
4. The effectiveness of Base_{top30} versus Base_{top35} remains uncertain.  
5. It is ambiguous how LoRA is integrated with LLM for inference across different server types.  

### Suggestions for Improvement
We recommend that the authors clarify the rationale behind the selection of comparison methods and consider incorporating additional LoRA modules for a more robust analysis. Additionally, the authors should address the increased communication costs and security risks associated with the proposed server architecture. A more thorough discussion on the variability of parameters and their implications for generalization is necessary. We also suggest the authors provide clearer insights into the performance of different base configurations and elucidate the integration of LoRA with LLM during inference. Lastly, a more detailed exploration of the limitations of the proposed method would enhance the paper's depth.