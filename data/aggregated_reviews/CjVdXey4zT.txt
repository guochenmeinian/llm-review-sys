ID: CjVdXey4zT
Title: When Do Neural Nets Outperform Boosted Trees on Tabular Data?
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 7, 7, 8, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive evaluation of machine learning (ML) models for tabular data, benchmarking 19 algorithms across 176 classification datasets and training over 500K models. The authors investigate the performance of gradient boosted decision trees (GBDTs) and neural networks (NNs), concluding that no single model family dominates. GBDTs generally outperform NNs on irregular and larger datasets, while hyperparameter optimization (HPO) on well-performing learners like CatBoost is more critical than merely selecting the "right" learner. The authors introduce the TabZilla benchmark suite, comprising 36 challenging datasets, to facilitate further research and provide preliminary results on regression datasets. They also implement a "hardness metric" for evaluating use cases.

### Strengths and Weaknesses
Strengths:
- The paper addresses the nuanced question of "GBDTs or NNs?" with a robust meta-feature analysis and extensive evaluation across diverse datasets.
- It includes datasets ranging from 32 to approximately 1,000,000 instances, surpassing previous evaluations in size and enhancing the reliability of the findings.
- The open-source release of the codebase and raw results, along with the TabZilla benchmark, promotes transparency and facilitates further research.
- The writing is clear, logically structured, and supported by effective visualizations, making it accessible to readers.
- The inclusion of preliminary results on regression datasets and the addition of a "hardness metric" are valuable contributions.

Weaknesses:
- The evaluation is limited to classification datasets, omitting regression datasets, which are prevalent in real-world applications, although preliminary results are mentioned.
- Time-series datasets are also not included, which could enrich the analysis.
- Some analyses, such as the correlation between metafeatures and performance, lack depth and could benefit from more sophisticated statistical modeling.
- The authors' response regarding the intuition behind baseline performance lacks depth, leaving some questions unanswered.
- Concerns remain regarding the repeated use of the Friedman test on the same datasets, which may lead to false positives.

### Suggestions for Improvement
We recommend that the authors include additional regression datasets in the evaluation and the TabZilla benchmark suite to enhance its applicability. Incorporating time-series datasets would also be beneficial. We suggest improving the depth of analysis by conducting thorough statistical modeling to identify important metafeatures and their interactions, possibly using forward or backward selection techniques. Additionally, we encourage the authors to clarify the definition of a "winning algorithm" to avoid confusion and consider releasing the entire dataset collection alongside the benchmark. We recommend detailing the hyperparameter search process in an appendix and providing more insight into the impact of categorical features on algorithm performance. Furthermore, we suggest extending hyperparameter tuning beyond 30 iterations for NNs and addressing concerns regarding the repeated application of the Friedman test to strengthen the validity of the results presented.