ID: M12lmQKuxa
Title: Multiple Physics Pretraining for Physical Surrogate Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 3
Original Ratings: 7, 6, -1
Original Confidences: 5, 3, 4

Aggregated Review:
### Key Points
This paper presents the novel concept of multiple physics pretraining (MPP) for physical surrogate modeling, which trains large surrogate models to predict the dynamics of multiple heterogeneous physical systems simultaneously by projecting their fields into a shared embedding space. The authors validate the efficacy of MPP on both pretraining and downstream tasks, demonstrating outperformance compared to other methods.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written, with clear and easy-to-follow presentation.  
- The pretrained transformer model can match or surpass modern baselines trained on specific pretraining sub-tasks without finetuning.  
- The model shows potential for transfer to other systems with limited training samples.  

Weaknesses:  
- In section 5.2 on transfer learning, the authors only compare MPP with VideoMAE, which is not specifically designed for solving PDEs, and do not include other baseline models in this low-data setting, such as FNO, FFNO, and GFNO.

### Suggestions for Improvement
We recommend that the authors improve the comparison in section 5.2 by including additional baseline models specifically designed for PDEs, such as FNO, FFNO, and GFNO, to provide a more comprehensive evaluation of MPP's performance in low-data settings. Additionally, we suggest that the authors consider comparing the effectiveness of their pretrained model with SOTA models in PDE timestepping tasks beyond PDEbench to strengthen their findings.