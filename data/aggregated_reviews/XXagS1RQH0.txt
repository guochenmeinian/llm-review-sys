ID: XXagS1RQH0
Title: Learning-to-Rank Meets Language: Boosting Language-Driven Ordering Alignment for Ordinal Classification
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 7, 6, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 5, 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents L2RCLIP, a novel language-driven ordering alignment method for ordinal classification. The authors leverage pre-trained vision-language models to incorporate rich ordinal priors from human language, proposing RankFormer, a prompt tuning technique that enhances the ordering relation of rank prompts through token-level attention and residual-style prompt blending. Additionally, they introduce a cross-modal ordinal pairwise loss to refine the CLIP feature space, ensuring semantic and ordering alignment between texts and images. The method is evaluated on facial age estimation, historical color image classification, and aesthetic assessment tasks, demonstrating promising performance. Furthermore, the authors describe a two-stage training approach utilizing rank templates initialized by tokenized input for the model R, which remains fixed throughout the training. They explain their choice of losses, including $L_{cor}$ for ordinal alignment and a variant of contrastive loss ($L_{t2i}$ and $L_{i2t}$) for semantic alignment, and clarify the connection between these losses and the classification loss $L_{ce}$. An ablation study details various experimental settings and their corresponding losses.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel method that effectively leverages language priors to address overfitting in ordinal classification.
2. The experimental results indicate promising performance across various tasks, suggesting the method's effectiveness.
3. The authors provide a clear explanation of their initialization process and the rationale behind their choice of losses.
4. The ablation study is well-structured, showcasing the impact of different settings on performance.

Weaknesses:
1. The comparison with previous methods is potentially unfair due to the use of a stronger image backbone (CLIP) compared to VGG16 in prior works.
2. The ordinality score evaluation lacks comprehensiveness, with unexpected spikes and patterns observed in the results.
3. The claim regarding the effectiveness of rank-specific prompts lacks experimental support.
4. Some aspects of the loss functions and their interconnections lack clarity, particularly regarding the parameter $\lambda$.
5. The final loss composition is not clearly described in the main text, which could hinder understanding.
6. The manuscript may contain minor typographical errors that need addressing.

### Suggestions for Improvement
We recommend that the authors improve the fairness of comparisons by evaluating L2RCLIP using a similar backbone to previous works. Additionally, consider reducing the window of comparisons to provide a more comprehensive evaluation of the ordinality score. The authors should provide experimental evidence or a more detailed explanation to support the claim of rank-specific prompts enhancing the ordering relation. Furthermore, we suggest that the authors describe the final loss composition in the main text to clarify how the different components of the loss function are combined. We also recommend improving the clarity of the explanation regarding the parameter $\lambda$ in the context of $L_{cor}$. Including more details and code implementations in the revised version would enhance the manuscript's comprehensiveness. Finally, we encourage the authors to carefully proofread the manuscript to correct any typographical errors.