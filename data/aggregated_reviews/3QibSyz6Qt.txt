ID: 3QibSyz6Qt
Title: NarrativeXL: a Large-scale Dataset for Long-Term Memory Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel reading comprehension dataset designed for training and assessing long-term memory capabilities in language models. The dataset is derived from Project Gutenberg, featuring manually selected books, and includes scene summaries generated by the GPT-3.5 model. The authors propose two question types—read-along questions and summary correction questions—to evaluate the models' comprehension and memory retention. The paper also conducts validation experiments to highlight the dataset's benchmarks and biases.

### Strengths and Weaknesses
Strengths:  
- The dataset addresses a critical gap in training and evaluation data for long-context LLMs, providing a larger and more comprehensive resource than existing datasets.  
- The methodology is novel and could be scaled to other datasets, enhancing the community's ability to evaluate long-term memory in LLMs.  
- The experiments conducted support the dataset's utility and demonstrate the potential for improving long-term memory performance in models.

Weaknesses:  
- The paper lacks concrete statistics regarding the dataset, such as the number of scenes and question types, which are essential for evaluating its scope and quality.  
- The reliance on GPT-3.5 for generating answers raises concerns about fairness in comparisons with other models, as it may skew results in favor of GPT.  
- The presentation is scattered and difficult to read, with insufficient analysis of the dataset and its questions, and a lack of comprehensive comparisons with existing datasets.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper's presentation by organizing the content more effectively and condensing overly subjective analyses. Additionally, including a table with dataset statistics, such as the mean and standard deviation of prompt lengths, would enhance transparency. To strengthen the validation of the dataset, we suggest conducting a more extensive human evaluation to substantiate the quality of the generated summaries. Furthermore, including experiments with publicly accessible state-of-the-art models would provide a more comprehensive understanding of long-term memory capabilities in LLMs. Lastly, a thorough comparative analysis with other long-document QA datasets, particularly regarding question quality and difficulty, would significantly bolster the paper's contributions.