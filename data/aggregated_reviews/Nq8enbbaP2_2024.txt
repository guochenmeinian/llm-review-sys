ID: Nq8enbbaP2
Title: Occupancy-based Policy Gradient: Estimation, Convergence, and Optimality
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 6, 7, 3, -1, -1
Original Confidences: 2, 3, 2, -1, -1

Aggregated Review:
### Key Points
This paper presents a model-free, policy gradient method for policy optimization in finite-horizon MDPs through occupancy estimation. The authors express the gradient of the return in terms of the gradient of the log of the state-visitation probability, estimating this via squared loss regression. They provide guarantees for gradient estimation and convergence to a near-optimal policy in online RL, while addressing limitations in offline RL due to insufficient dataset coverage. The paper introduces policy gradient algorithms that extend convergence analysis to a broader class of occupancy objectives, including imitation learning and exploration.

### Strengths and Weaknesses
Strengths:  
1. The authors propose a novel perspective on policy gradients based on estimating the occupancy gradient, which has the potential to unify various research fields and generalize over objectives for sequential decision-making.  
2. The online algorithm is technically sound, with convergence claims supported by theoretical analysis. The Offline OccuPG method effectively utilizes existing methods for estimating clipped density ratios.  

Weaknesses:  
1. The paper's assumptions in the offline setting limit broader applicability and make the results difficult to verify, as seen in Theorem 16, which relies on assumptions that are not self-contained.  
2. The paper could benefit from additional clarification on motivation and comparison to existing work, as well as addressing various writing and clarity issues that hinder understanding.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind their results, particularly emphasizing the significance of optimizing a general class of functionals and the weaker assumptions for convergence in the offline algorithm. Additionally, providing a more concrete comparison between their bounds and existing results would enhance the reader's understanding. We also suggest addressing the various minor writing and clarity issues noted, such as properly defining terms in Algorithm 1 and clarifying ambiguous mathematical expressions.