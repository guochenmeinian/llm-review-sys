ID: VvnfMeC3gQ
Title: RevColV2: Exploring Disentangled Representations in Masked Image Modeling
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 5, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents RevColV2, a new backbone designed for Masked Image Modeling (MIM) that facilitates the learning of disentangled representations during pretraining. The authors maintain the entire autoencoder architecture throughout both pretraining and fine-tuning, which contrasts with previous methods that drop the decoder. Strong experimental results on datasets such as ImageNet, MS-COCO, and ADE20K demonstrate the effectiveness of the proposed architecture.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- The combination of MIM with disentangled representation learning is novel.
- The proposed method shows strong experimental results across various tasks.

Weaknesses:
- Some details are unclear, particularly regarding the input of masked versus unmasked patches.
- The complexity of the model raises concerns about pretraining and inference speed compared to vanilla ViT.
- The significance of the accuracy differences between the MAE decoder and RevColV2 decoder is questionable.
- The paper lacks a comprehensive comparison of PeCo's performance on ImageNet-1k classification.

### Suggestions for Improvement
We recommend that the authors improve clarity by resolving ambiguities regarding the input of masked and unmasked patches. Additionally, it would be beneficial to provide a detailed analysis of the training and inference speeds of RevColV2 compared to vanilla ViT. We suggest including a more significant performance comparison with PeCo on ImageNet-1k classification. Furthermore, we encourage the authors to enhance the visualizations in the paper to aid understanding and to address the factual inaccuracies noted in the review.