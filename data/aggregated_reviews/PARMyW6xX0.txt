ID: PARMyW6xX0
Title: Type-to-Track: Retrieve Any Object via Prompt-based Tracking
Conference: NeurIPS
Year: 2023
Number of Reviews: 26
Original Ratings: 5, 6, 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to multiple object tracking (MOT) using natural language descriptions, introducing a new dataset called GroOT that enhances existing MOT datasets with diverse text captions. The authors propose a new tracking paradigm termed "Type-to-Track" and develop a language-based tracking model named MENDER. The paper also introduces class-agnostic evaluation metrics, CA-MOTA and CA-IDF1, to assess the performance of the proposed methods. The authors acknowledge the high label accuracy of their annotations, claiming a 98.9% accuracy rate in the new dataset version v1.0. They clarify that the retrieval task is an extension of their main focus on prompt-based tracking.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant problem in MOT by allowing users to query videos with text prompts, which is a practical advancement.
- The GroOT dataset is well-structured, extending existing datasets with various text captions that enhance model training.
- The authors have released an updated version (v1.0) of the dataset, addressing previous concerns about annotation quality.
- The proposed evaluation protocols are diverse and clearly defined, contributing to the robustness of the research.
- MENDER demonstrates effective performance in the grounded MOT task, validated against a two-stage baseline model, serving as a baseline for grounded MOT.

Weaknesses:
- The concept of "Type-to-Track" is similar to existing works like Referring Multi-Object Tracking, necessitating a thorough comparison with related literature.
- The performance of MENDER is underwhelming compared to state-of-the-art (SOTA) trackers like TrackFormer and ByteTrack, raising questions about its generalization capabilities.
- There are inconsistencies in the dataset annotations, raising concerns about the quality and reliability of the captions, with many being repetitive and lacking informative detail.
- The evaluation metrics primarily focus on tracking performance rather than retrieval performance, which is critical for the proposed task.
- The paper lacks comprehensive experimental comparisons with state-of-the-art tracking algorithms, particularly in the `category_name` setting.
- The modeling section is overly complex, and the rationale for choosing specific architectures, such as RoBERTa, is not adequately justified.

### Suggestions for Improvement
We recommend that the authors improve the discussion and comparison of their work with related studies, particularly in the context of existing datasets like Ref-Youtube-VOS and Ref-DAVIS. Additionally, the authors should clarify the annotation process to ensure consistency and quality in the dataset. It would be beneficial to improve the diversity and informativeness of the captions by incorporating more fine-grained annotations that clearly distinguish between appearance and action. Furthermore, we suggest including retrieval-specific metrics to evaluate the effectiveness of the proposed methods in the type-to-track task. We also encourage the authors to conduct a thorough analysis of the frequency and distribution of words used in the captions to enhance clarity. Lastly, we suggest including comparisons of MENDER with state-of-the-art tracking algorithms to validate its performance comprehensively and simplifying the modeling section while providing clearer explanations for architectural choices to enhance the paper's clarity.