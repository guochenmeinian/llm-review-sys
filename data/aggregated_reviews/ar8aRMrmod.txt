ID: ar8aRMrmod
Title: Evaluating Copyright Takedown Methods for Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 7, 6, 8, -1
Original Confidences: 3, 4, 3, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive evaluation of copyright takedown methods in LLM content generation, focusing on three main goals: avoiding memorization of copyrighted text, maintaining quality in text generation, and ensuring low overhead to generation speed. The evaluation criteria encompass the risk of copyright infringement (measured by win rate), the utility of the model post-takedown (assessed using common evaluation datasets), and the efficiency of the model (evaluated by inference speed). The authors propose a framework that includes various takedown methods such as system prompts, decoding methods like MemFree, and unlearning methods like GA, providing valuable insights into the effectiveness of these strategies.

### Strengths and Weaknesses
Strengths:
- The paper addresses a critical issue in AI and copyright law, making it timely and relevant.
- It offers a robust evaluation framework that considers multiple dimensions, including infringement risk, utility, and efficiency.
- The exploration of diverse takedown methods provides a nuanced perspective on potential solutions.
- The experimental setup is well-documented, promoting reproducibility and clarity in the evaluation process.

Weaknesses:
- The evaluation is limited to LLaMA 2, which may not represent all LLM models, and the methods primarily focus on reducing memorization of copyrighted text, potentially overlooking other strategies like alignment methods.
- The BookSum dataset, used for evaluation, consists of public domain texts, which may not accurately reflect copyright infringement risks associated with copyrighted materials.
- Some relevant prior works are not included, and the evaluation could benefit from a broader dataset scope and additional model testing.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including other mainstream LLM models such as GPT-4 and Claude to enhance representativeness. Additionally, we suggest evaluating the unlearning time cost and addressing the sensitivity of unlearning methods to hyperparameters. It would be beneficial to move the experiment figures related to unlearning/RCAD to the main text for better visibility. Furthermore, we encourage the authors to explore hybrid methods or more sophisticated techniques and to discuss the scalability of these methods for larger models or real-world applications. Lastly, a deeper analysis of the societal impacts of these methods would enrich the discussion on their broader implications.