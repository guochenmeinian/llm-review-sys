ID: khB83LpPgH
Title: Which LLMs are Difficult to Detect? A Detailed Analysis of Potential Factors Contributing to Difficulties in LLM Text Detection
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 9, 6
Original Confidences: 4, 5

Aggregated Review:
### Key Points
This paper presents an investigation into the challenges of detecting AI-generated texts from various large language models (LLMs). The authors train classifiers on two datasets and analyze their performance across different writing domains, revealing that detection varies significantly, particularly with scientific writing being more difficult. Notably, OpenAI LLMs are found to be especially challenging to distinguish from human texts, particularly in student essays. The study also examines factors contributing to these detection difficulties.

### Strengths and Weaknesses
Strengths:  
1. The work includes experiments on a comprehensive set of state-of-the-art LLMs and multiple datasets.  
2. The authors provide valuable insights into the similarities between OpenAI texts and human-authored texts regarding entropy and out-of-vocabulary (OOV) ratios, which are crucial for developing defense strategies against AI-generated texts.  

Weaknesses:  
1. The paper lacks a controlled analysis of the effect of temperature, which is currently sampled randomly.  
2. There is an absence of datasets featuring shorter texts, which limits the completeness and robustness of the findings.

### Suggestions for Improvement
We recommend that the authors improve the analysis of the effect of temperature in a controlled manner rather than relying on random sampling. Additionally, incorporating datasets with shorter texts, such as social media posts or online reviews, would enhance the study's robustness. We also suggest providing more insights into the practical implications of the detection difficulties and elaborating on the factors that contribute to the challenges of detecting OpenAI-generated texts, particularly regarding entropy and OOV ratios. Clarifying the novelty of the research and detailing the hyperparameters used for training the classifiers, including the reasoning behind their selection, would strengthen the paper. Lastly, addressing potential ethical implications and discussing safeguards to mitigate risks associated with AI-generated text detection would be beneficial.