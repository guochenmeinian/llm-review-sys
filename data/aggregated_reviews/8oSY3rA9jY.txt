ID: 8oSY3rA9jY
Title: Finding Transformer Circuits With Edge Pruning
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 3, 8, 8, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Edge Pruning, a novel algorithm for circuit finding in transformer models, claiming it performs favorably compared to prior methods like ACDC and EAP, particularly in terms of circuit metrics such as faithfulness. The authors assert that Edge Pruning scales effectively to a 13B model size and provides preliminary analysis of instruction-following and in-context-learning capabilities.

### Strengths and Weaknesses
Strengths:
- Originality: The introduction of learnable parameters for edge inclusion marks a significant advancement in circuit discovery.
- Quality: The experiments are reasonable, with promising results indicating that Edge Pruning can recover ground-truth TracR circuits and outperform ACDC and EAP on key metrics.
- Clarity: The writing is generally clear and well-structured, effectively explaining the Edge Pruning method.
- Significance: The method enhances interpretability for large language models, paving the way for practical applications.

Weaknesses:
- The evaluation of circuits trained to minimize KL divergence but assessed using logit difference lacks clarity; a discussion on this discrepancy is necessary.
- The comparison with EAP and ACDC using KL divergence as the objective raises concerns about the validity of the results, as these methods originally utilized logit difference.
- The reliance on circuit metrics, which may be misleading, necessitates direct comparisons of nodes and edges found by each method, including graph metrics like node/edge IoU.
- Limited analysis of circuits found in the 13B model leaves questions about their interpretability; specific human-interpretable nodes/edges should be highlighted.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the differences between KL divergence and logit difference, clarifying their respective roles in training and evaluation. Additionally, we suggest conducting a more rigorous comparison of the nodes and edges identified by Edge Pruning against those from EAP and ACDC, including reporting graph metrics. It would also be beneficial to provide a detailed analysis of the circuits discovered in the 13B model, emphasizing human-interpretable components. Finally, we encourage the authors to explore the potential for Edge Pruning to scale to neuron or subspace-level components and to address the interpretability of the resulting computation graphs.