ID: 5uG9tp3v2q
Title: Transformers on Markov data: Constant depth suffices
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the representation capability of transformers for learning $k^{th}$ order Markov processes. The authors prove that attention-only transformers with $O(\log k)$ layers can represent conditional $k$-grams, while a 3-layer transformer enhanced with MLP and layer normalization can also effectively model these processes. The paper includes empirical validations and lower bounds on the representational power of transformers for Markov data.

### Strengths and Weaknesses
Strengths:
1. The paper provides a novel explanation of how layer normalization can be utilized in modeling $k$-grams, which is often overlooked in prior work.
2. The results demonstrate the advantage of depth over the number of heads in transformers, enhancing understanding of their predictive capabilities.
3. The writing is clear and the paper is well-structured, making it accessible to readers.

Weaknesses:
1. The evaluation is limited, lacking empirical validation for larger state spaces, which is crucial for understanding performance in practical scenarios.
2. Some theoretical claims lack clarity and are not sufficiently supported by discussions on prior work, particularly regarding contradictions in results.
3. There are minor typographical errors and unclear statements that detract from the overall quality.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation by including experiments for larger state spaces, particularly when $S$ is in the order of tens or hundreds. Additionally, we suggest clarifying the statements that contradict prior work by providing more context and discussion. It would also be beneficial to compare the performance of single-head transformers with multiple heads in the experiments. Lastly, please address the minor typographical errors and ensure that all claims are well-supported by thorough discussions.