ID: qmCxdPkNsa
Title: COOM: A Game Benchmark for Continual Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 7, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents COOM, a continual learning benchmark for embodied pixel-based reinforcement learning (RL) that evaluates critical aspects such as catastrophic forgetting, forward knowledge transfer, and sample-efficient learning. Built on VizDoom, COOM includes eight scenarios with varying visual inputs and objectives, establishing an evaluation protocol with essential metrics. The authors apply multiple well-known continual learning methods to assess baseline performance across these tasks. Additionally, the paper explores image augmentation techniques aimed at enhancing continual reinforcement learning (CRL), proposing three methods: Random Convolution, Random Shift, and Random Noise, to introduce variability in input data and mitigate issues like catastrophic forgetting. However, experiments reveal that these augmentations did not yield significant improvements, with Random Convolution adversely affecting learning. The authors also study task ordering and its impact on the assessment of different approaches in RL, proposing a fixed sequence of tasks to facilitate research within a moderately-budgeted computational setup, while acknowledging potential biases.

### Strengths and Weaknesses
**Strengths:** 
- The paper introduces the first benchmark specifically for continual reinforcement learning in complex 3D environments, featuring diverse objectives and visuals.
- It provides a comprehensive evaluation of various continual learning methods, establishing baseline performance across six unique VizDoom scenarios.
- The examination of image augmentation methods is thorough, and the authors effectively document their code and provide a comprehensive README for users.
- The addition of an RL baseline benchmarking experiment enhances the paper's robustness, and the clarification regarding single-head ablation is a positive aspect.

**Weaknesses:**
- The design choices of the benchmark closely resemble those of Continual World, inheriting some of its weaknesses, such as the exclusive use of supervised methods adapted for RL.
- The task sequences are notably short, which may not accurately reflect continual training over longer sequences.
- The authors do not sufficiently justify the choice of VizDoom over other 3D environments with richer assets, nor do they address the limitations of existing continual learning methods that achieve high performance.
- The proposed image augmentations lack substantial improvements, particularly the detrimental impact of Random Convolution, and the benchmark's perceived simplicity raises concerns about its ability to drive advancements in the field.
- The artificial nature of the fixed task sequencing may disconnect from broader findings, and the lack of randomization in task ordering could undermine the validity of comparisons.

### Suggestions for Improvement
We recommend that the authors improve the justification for selecting VizDoom over alternatives like AI2Thor or iGibson. Additionally, consider expanding the task sequences to include longer evaluations, as this would provide a more accurate representation of continual learning capabilities. It would also be beneficial to evaluate continual learning methods over various task orderings rather than a fixed sequence to account for performance variability. Furthermore, we suggest that the authors explore alternative methods or refine existing image augmentation strategies to enhance performance across all baselines. We also recommend integrating offline RL methods into their framework to provide valuable insights into the performance of their benchmark. Lastly, we encourage the authors to enhance the complexity of their benchmark tasks to better challenge future methods and stimulate further research in continual learning, while also including a discussion of the limitations related to task ordering in the final draft.