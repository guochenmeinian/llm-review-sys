ID: O1lYncfVOO
Title: S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 6, 6, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents S-CLIP, a novel semi-supervised learning method for training contrastive language-image pre-training (CLIP) models in specialized domains with limited image-text pairs. S-CLIP utilizes additional unpaired images and introduces two pseudo-labeling strategies: caption-level and keyword-level pseudo-labeling. Caption-level pseudo-labeling employs an optimal transport problem to assign pseudo-labels based on paired image captions, while keyword-level pseudo-labeling uses keywords from visually similar paired images. The experiments demonstrate S-CLIP's effectiveness across various domains, including remote sensing, achieving significant improvements over CLIP fine-tuning and other semi-supervised competitors.

### Strengths and Weaknesses
Strengths:
- Effective Pseudo-Labeling Strategies: The dual pseudo-labeling strategies enhance training robustness and improve performance in zero-shot classification and image-text retrieval tasks.
- Experimental Demonstrations: Extensive evaluations across multiple specialist domains consistently show S-CLIP outperforming existing methods, even with unlabeled images from different datasets.
- Relevance and Applicability: The method addresses a critical issue in vision-language models, making it valuable for both researchers and practitioners.
- Clarity and Presentation: The paper is well-organized and clearly communicates its methods and results, supported by helpful figures.

Weaknesses:
- Limited Baselines: The comparison with other semi-supervised approaches is outdated, lacking newer methods that could provide a more comprehensive evaluation.
- Key Content in Appendix: Important details regarding limitations and experimental setups are relegated to the appendix, which may hinder understanding.
- Novelty Concerns: While the combination of pseudo-labeling strategies is novel, it may not be sufficiently groundbreaking for a venue like NeurIPS.
- Comparison of Distance Measures: The choice of optimal transport as a distance measure is interesting, but comparisons with other image distance measures are lacking.

### Suggestions for Improvement
We recommend that the authors improve the baseline comparisons by including more recent semi-supervised learning approaches, such as those discussed in "A Survey on Deep Semi-supervised Learning" by Yang et al. Additionally, we suggest that the authors clarify the rationale for selecting only soft-PL and hard-PL as baselines, given the extensive literature in semi-supervised learning. It would be beneficial to provide more details in the main text regarding the experimental setup, including model size, training schedules, and data preprocessing strategies. We also encourage the authors to explore the generalizability of their method by testing it on more diverse datasets beyond the four specified domains. Lastly, we recommend including qualitative examples of caption-level and keyword-level selections to enhance understanding.