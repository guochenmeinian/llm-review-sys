ID: aIpGtPwXny
Title: Learning to Modulate pre-trained Models in RL
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 5, 7, 5, 6, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the efficacy of finetuning methods for Decision Transformers (DT) in reinforcement learning (RL), specifically focusing on a new method called Learning-to-Modulate (L2M), which combines Learning-to-Prompt (L2P) and Low-Rank Adaptation (LoRA). The authors construct a pretraining dataset from 50 state-based tasks across Meta-World and DMControl, evaluating various finetuning strategies on held-out tasks. The findings indicate that L2M achieves strong performance on unseen tasks while maintaining effectiveness on pretraining tasks.

### Strengths and Weaknesses
Strengths:
- The problem addressed is interesting, and the paper is well-written and easy to follow.
- The proposed method is well-motivated and demonstrates strong performance on continual learning benchmarks.
- Extensive experiments and a thorough investigation of different finetuning methods are conducted.

Weaknesses:
- There is a lack of clarity regarding the experimental setup, including details on the datasets used for pretraining and finetuning.
- The paper does not adequately discuss limitations, particularly regarding data collection and the assumptions made during experiments.
- The presentation of the methodology and experiments is relatively poor, with many technical details missing or unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental setup by explicitly stating the tasks included in the pretraining and finetuning datasets, as well as the nature of the offline replay data used for finetuning. Additionally, we suggest that the authors include a discussion of limitations related to data-driven approaches in RL, ideally supported by data showing the dependence of finetuning on task similarity. Furthermore, we encourage the authors to enhance the presentation of their methodology by providing more detailed explanations of the technical aspects, such as the loss function and the updating process for learnable keys. Lastly, we recommend that the authors explore comparisons with other recent advancements in training methods, such as weight decay and HyperDT, to contextualize the relevance of L2M.