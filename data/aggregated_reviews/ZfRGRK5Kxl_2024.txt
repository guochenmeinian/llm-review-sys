ID: ZfRGRK5Kxl
Title: TripletCLIP:  Improving Compositional Reasoning of CLIP via Synthetic Vision-Language Negatives
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TripletCLIP, a pre-training framework designed to enhance the compositional reasoning capabilities of CLIP models. The authors propose generating hard negative captions using a large language model (LLM) and synthesizing corresponding negative images with text-to-image generators. The experiments demonstrate the superior performance of TripletCLIP across various benchmarks, including compositionality, zero-shot retrieval, and classification.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel strategy for CLIP pre-training that effectively utilizes hard negative images and captions, which could be valuable for the research community.
- Comprehensive experiments show consistent improvements over baselines, validating the proposed methodology.
- The presentation is clear and well-structured, making it accessible to readers.

Weaknesses:
- The effectiveness of the TripletCLIP objective is questioned due to a lack of comparisons with hard negative (HN) baselines that jointly consider both HN text and image.
- The absolute performance level is relatively weak compared to existing models like CyCLIP and ALIP, possibly due to limited computational resources during pre-training.
- The study does not explore the impact of using multiple LLMs and text-to-image models during the synthesis process, which could enhance data diversity and model robustness.

### Suggestions for Improvement
We recommend that the authors improve the comparison of the TripletCLIP objective with hard negative baselines that incorporate both image and text in the loss calculation. Additionally, consider conducting experiments with multiple LLMs and text-to-image models to explore potential improvements in data diversity. It would also be beneficial to include additional compositional benchmarks for evaluation, such as counterfactual image-text pairs, to enhance the comprehensiveness of the study. Finally, we suggest clarifying how the total of 13M image-text pairs is calculated and addressing the potential distribution mismatch between generated negative images and real positive images.