ID: pb2U4WrAAv
Title: Gradient-free variational learning with conditional mixture networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 7, 8
Original Confidences: 2, 3

Aggregated Review:
### Key Points
This paper presents CAVI-CMN, a gradient-free variational method for training conditional mixture networks. The authors propose that CAVI-CMN outperforms other variational methods, such as BBVI, in speed while offering comparable efficiency to MLE and maintaining strong predictive accuracy. The empirical results demonstrate the advantages of CAVI-CMN over baseline methods.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized with clear motivation and significant experimental results.
- The experimental analysis effectively illustrates the strengths of CAVI-CMN compared to other inference approaches, including both synthetic and real datasets.
- Metrics chosen for experiments highlight important aspects of classification, showcasing advantages of maintaining a full posterior over model parameters.

Weaknesses:
- Section 2.1 lacks clarity regarding the conditional mixture network.
- The scalability of results on larger models or datasets is uncertain.
- The PG augmentation scheme is insufficiently described in the main text.
- A more detailed description of the neural network used would enhance understanding of the model's scale.

### Suggestions for Improvement
We recommend that the authors improve Section 2.1 to enhance clarity, possibly by rephrasing complex sentences and including a conceptual diagram. Additionally, citing the origin of WAIC is necessary, and including error bars in Table 1 would provide more insight into the experimental results. In Appendix Section C.3, it would be beneficial to report a statistic of convergence for NUTS or confirm that a minimum threshold of stationarity was reached. We also suggest removing the "steps to convergence" metric, as it varies significantly between algorithms. Finally, bolding the best statistic in each column of the results would facilitate quick comparisons, and clarifying the initialization of random parameters in the dataset descriptions would be helpful.