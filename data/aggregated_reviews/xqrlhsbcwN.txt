ID: xqrlhsbcwN
Title: Approximated Orthogonal Projection Unit: Stabilizing Regression Network Training Using Natural Gradient
Conference: NeurIPS
Year: 2024
Number of Reviews: 25
Original Ratings: 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel training framework for regression tasks called the Approximated Orthogonal Projection Unit (AOPU), optimized using truncated natural gradients. The authors utilize the Rank Rate (RR) of the augmented data covariance matrix as a metric. AOPU focuses on achieving more stable and better training in deep learning, contributing significantly to the field of soft sensing. The authors demonstrate that AOPU offers more stable training than existing architectures and optimizers, which is essential for industrial applications requiring online training. The paper includes a comprehensive theoretical analysis and experimental validation on two chemical process datasets, showcasing AOPU's effectiveness in achieving stable convergence. The revised manuscript incorporates hyperparameter experiments, comparisons of learning rate setups, and visualized comparisons, enhancing clarity and reliability. AOPU is compatible with major deep learning frameworks and introduces a unified network optimization metric, RR, which improves interpretability and suggests promising research directions.

### Strengths and Weaknesses
Strengths:
- The proposed method is novel and straightforward, with a solid theoretical foundation.
- AOPU enhances interpretability by differentiating between trackable and untrackable parameters.
- The paper provides a detailed introduction and thorough theoretical analysis.
- Experimental results demonstrate superior performance in achieving stable convergence compared to existing models.
- AOPU offers exceptional practical value and addresses stability in model training.
- The manuscript has been improved based on reviewer feedback, enhancing clarity and detail.
- AOPU's compatibility with existing frameworks and its innovative optimization metric enhance its applicability.

Weaknesses:
- The paper is poorly arranged, with core conclusions located in the appendix.
- Claims regarding improved interpretability are not adequately explained.
- Lack of thorough hyperparameter tuning and its results, which is critical for validating claims of improved training stability.
- Some reviewers noted the manuscript's structure was overly lengthy and lacked emphasis on key innovations.
- Concerns were raised regarding insufficient explanation of hyperparameter selection and the need for additional visualized experiments.
- Code is not published, raising concerns about reproducibility.
- There were requests for more disclosure of experimental details and corrections of typos.

### Suggestions for Improvement
We recommend that the authors improve the paper's organization by moving core conclusions and limitations from the appendix to the main text. Additionally, the authors should clarify the significance of interpretability in their method and provide detailed insights into hyperparameter tuning, including how learning rates were selected for all methods. It would also be beneficial to elaborate on the choice of the random Gaussian matrix for data augmentation and explain the data input process more clearly. Furthermore, we suggest that the authors include additional visualized experiments to address randomness in results and correct identified typos. Finally, we encourage the authors to publish their code to enhance reproducibility and ensure the paper meets the standards for acceptance.