ID: PT63nNpyKg
Title: Large Language Models are biased to overestimate profoundness
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an evaluation of GPT-4's ability to judge the profoundness of mundane, motivational, and pseudo-profound statements. The authors find a significant correlation between GPT-4 and human judgments, noting that GPT-4 systematically overestimates the profoundness of nonsensical statements. The study also explores the impact of different prompting techniques on profoundness ratings, revealing that while correlations exist, GPT-4 tends to assign higher profoundness ratings than humans.

### Strengths and Weaknesses
Strengths:
- The paper addresses a novel research question regarding LLMs' judgments of profoundness, contributing to the understanding of their behavior in the context of pseudo-profound bullshit.
- The authors provide reproducible prompts used in experiments, enhancing the study's transparency.

Weaknesses:
- The scope is limited as the study only evaluates GPT-4, which restricts the generalizability of the findings across different model families and sizes.
- The discussion lacks a broader context connecting human and LLM pragmatics, failing to articulate the implications of GPT-4's overestimation of profoundness.

### Suggestions for Improvement
We recommend that the authors improve the scope of their study by testing a wider range of models beyond GPT-4 to enhance the impact of their claims. Additionally, we suggest that the authors incorporate a broader discussion relating their findings to cognitive theories of bullshit detection and articulate the practical implications of GPT-4's tendency to overestimate profoundness. Furthermore, we advise focusing the related work section on causal language models only, omitting references to BERT, as MLM and CLM serve different objectives. Lastly, we encourage the authors to clarify specific terms and concepts, such as defining the midpoint-level of profoundness and providing examples of pseudo-profound bullshit in the introduction.