ID: Q8yfhrBBD8
Title: Bridge-IF: Learning Inverse Protein Folding with Markov Bridges
Conference: NeurIPS
Year: 2024
Number of Reviews: 23
Original Ratings: 6, 6, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Bridge-IF model, a novel approach to inverse protein folding that utilizes Markov bridges to learn the probabilistic dependencies between protein backbone structures and sequences. The authors claim that Bridge-IF addresses limitations of existing methods, such as error accumulation and the one-to-many mapping nature of inverse folding, achieving state-of-the-art accuracy on benchmarks like CATH. The model integrates structural conditions into protein language models, enhancing performance while maintaining parameter efficiency. Additionally, the paper includes a comparative analysis of protein design methodologies, focusing on the performance of Bridge-IF against established models like ChromaDesign and ProteinMPNN. The authors assert that while using pre-trained language models (pLMs) for sequence revision yields marginal enhancements, their approach demonstrates superior performance in de novo protein design, particularly in designability metrics.

### Strengths and Weaknesses
Strengths:
- The use of Markov bridges for inverse protein folding is innovative and well-introduced.
- Bridge-IF demonstrates superior performance in sequence recovery and de novo protein design compared to existing baselines.
- The integration of structural conditions into protein language models effectively utilizes pre-trained information, improving generation performance.
- The authors provide robust experimental results and a comparative analysis with established models, highlighting the strengths of Bridge-IF.
- The incorporation of ablation studies addresses concerns regarding data leakage and the contribution of pLMs.

Weaknesses:
- The experimental validation of Bridge-IF's ability to discover novel protein sequences or engineer existing proteins is lacking; benchmarks like GFP could be useful.
- The focus on common benchmarks raises questions about performance on highly complex protein structures.
- The background discussion on Markov bridges could be more detailed, including recent references from NeurIPS and ICML.
- Some reviewers express skepticism regarding the marginal enhancement claim of pLMs without fine-tuning, suggesting that the evidence provided does not fully support this assertion.
- There is confusion about the performance implications of using different model sizes and pre-training, with some reviewers questioning the validity of the experimental comparisons.
- The review does not specify particular areas of the paper that require improvement or detail any shortcomings.

### Suggestions for Improvement
We recommend that the authors improve the experimental validation by including tests on de novo protein designs using methods like RFdiffusion or FrameDiff, comparing results against established baselines such as ChromaDesign and ProteinMPNN. Additionally, the authors should enhance the discussion on the Markov bridge approach, clarify the role of the Potts model in addressing error accumulation, and ensure a thorough comparison with other relevant methods like SPDesign and InstructPLM. We also recommend that the authors improve the clarity of their claims regarding the marginal enhancements provided by pLMs, particularly in the context of fine-tuning, and provide additional evidence or clarifications on the performance differences between pre-trained and non-pre-trained models. Furthermore, we suggest that the authors explore and present more baseline comparisons, especially with other strong inverse folding models, to enhance the robustness of their findings. Lastly, we encourage the authors to expand the conclusion section to discuss broader implications for machine learning in protein design, including aspects such as input features, interpretability, and scaling, and to provide more specific responses to the feedback received, detailing how each comment will be addressed in the final version.