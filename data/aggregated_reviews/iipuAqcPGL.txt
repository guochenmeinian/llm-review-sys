ID: iipuAqcPGL
Title: Can Large Language Models Capture Dissenting Human Voices?
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an evaluation of several recent Large Language Models (LLMs) on Natural Language Inference (NLI) benchmarks, particularly those designed to capture human disagreements. The authors propose two methods for estimating model output distributions: Monte Carlo Estimation (MCE) and Log Probability Estimation (LPE), applicable based on whether models expose estimated probabilities of output tokens. Evaluation metrics include accuracy and distribution differences between estimated LLM outputs and human labels, utilizing Jensen-Shannon Divergence (JSD), Kullback-Leibler Divergence (KL), and Distribution Calibration Error (DCE). Results indicate that larger models without exposure to NLI data underperform and misalign with human label distributions.

### Strengths and Weaknesses
Strengths:  
- The paper addresses the important issue of capturing human disagreements in LLMs, leveraging previous work in NLI.  
- The proposed output distribution estimation methods are valuable for future research in estimating and calibrating LLMs.  
- The thorough selection of models enhances the study's contributions compared to similar prior works.  

Weaknesses:  
- Insufficient positioning relative to previous work on label distribution and human disagreements, lacking a thorough comparison with Distributed NLI.  
- The terminology around "disagreement" is unclear and lacks theoretical support, leading to confusion regarding its meaning in the context of LLMs.  
- The framing suggests that NLI tasks have an inherent "truth" that LLMs fail to capture, which overlooks the variability in human annotation processes.  
- The characterization of model exposure is overly simplistic, failing to account for the nuances of fine-tuning and few-shot learning.  
- The paper does not clearly communicate actionable insights from the findings.

### Suggestions for Improvement
We recommend that the authors improve the positioning of their work by providing a more thorough comparison with existing literature, particularly Distributed NLI. We suggest clarifying the terminology used around "disagreement" to avoid confusion and provide theoretical backing for its use. The authors should consider re-evaluating the framing of NLI tasks to acknowledge the variability in human annotation processes. Additionally, we encourage a more nuanced approach to characterizing model exposure, potentially incorporating few-shot learning comparisons. Finally, we recommend that the authors enhance the clarity of actionable insights derived from their findings to better communicate their implications.