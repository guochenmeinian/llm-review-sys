ID: uRHpgo6TMR
Title: Sampling weights of deep neural networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 6, 4, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 3, 4, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for sampling weights of neural networks that avoids iterative optimization and gradient computations. The authors propose a data-driven sampling scheme that constructs weights and biases based on pairs of training data points, demonstrating its effectiveness across various tasks, including classification benchmarks and transfer learning. The method shows potential advantages over traditional backpropagation techniques, particularly in terms of training speed and interpretability.

### Strengths and Weaknesses
Strengths:
- The method is original and offers a significant improvement in efficiency compared to iterative optimization methods.
- The theoretical framework is rigorously developed, and the empirical results support the claims of the method's effectiveness.
- The approach addresses challenges posed by random feature models and demonstrates robustness across different tasks.

Weaknesses:
- The presentation of the paper is not optimal, with some sections being dense and difficult to read.
- Comparisons are primarily made against the Adam optimizer, lacking a broader analysis involving other non-iterative methods.
- The connection between the proposed approach and gradient methods is not adequately discussed, leaving questions about its relationship to existing literature on Forward Gradient methods.
- The experiments reveal limitations in scalability, particularly with wider networks, and the method's applicability to modern architectures like convolutional networks remains unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation, particularly in the algorithm section, to enhance readability. Additionally, we suggest including comparisons with other non-iterative training methods to better showcase the advantages of the proposed approach. Addressing the connection to gradient methods and discussing potential implications for outlier sensitivity would strengthen the theoretical foundation. Finally, providing visualizations of sampled versus trained weights could clarify how the sampling mechanism operates in practice.