ID: DOdaV0Hqdy
Title: Off-Policy Evaluation for Human Feedback
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an off-policy evaluation method that leverages sparse human feedback, focusing on environments where human ratings are only available at the end of episodes. The authors propose a framework to reconstruct intermediate human rewards from total episode ratings, utilizing a variational auto-encoding approach. Additionally, the authors introduce a novel framework for Out-of-Policy Evaluation with Human Feedback (OPEHF), emphasizing the decoupling of human returns from environmental returns. They argue that existing reward decomposition methods primarily focus on delayed environmental returns, while their approach addresses the complexities of human feedback, which can be influenced by various confounding factors. The method is evaluated in two real-world scenarios, demonstrating its potential to enhance existing off-policy evaluation algorithms and asserting that their work is technically and experimentally complete.

### Strengths and Weaknesses
Strengths:
- The incorporation of human feedback into policy evaluation is significant and relevant, particularly in fields like healthcare.
- The framework is compatible with existing methods and is well-presented, with comprehensive real-world experiments that yield favorable results.
- The authors provide a comprehensive discussion on the limitations of existing reward decomposition methods and the unique challenges posed by human feedback.
- The paper addresses a realistic problem and provides a clear methodology for reconstructing intermediate rewards, presenting a significant contribution to the field of OPE.

Weaknesses:
- The novelty of the methodology is questionable, as it appears to combine several known techniques without substantial original contributions.
- The empirical evidence is not entirely convincing, and the limited number of patients in experiments raises concerns about the robustness of the validation.
- There is a lack of comparative analysis with other return decomposition methods, which could strengthen the argument for the proposed approach.
- The authors do not sufficiently address how the proposed methods in OPEHF handle the complexities arising from the decoupling of human returns from environmental dynamics.
- The proposed reward function learning may not be inherently tied to off-policy evaluation, as it could theoretically be applied in online settings as well.
- The paper does not acknowledge the limitation of using more than one human rater, which could impact the findings.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation by including more extensive experiments to strengthen their claims. Additionally, the authors should clarify the necessity of considering the total number of offline trajectories in their framework and provide insights on how this factor influences their algorithm design. It would also be beneficial to discuss how the method could handle scenarios with multiple human raters and to explore comparisons with other return decomposition methods to enhance the robustness of their contributions. Furthermore, we suggest that the authors improve the comparative analysis by including additional methods of return decomposition to provide a more robust justification for their approach. Lastly, we advise the authors to explicitly state the limitation regarding the use of multiple human raters in their work and to address the potential implications of using a bi-directional LSTM on the Markov assumption in their reward function formulation.