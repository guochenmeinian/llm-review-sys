ID: CJWQGDwa6u
Title: Differentiable Random Partition Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a probabilistic model for partitioning a non-i.i.d. set of points into a fixed number of subsets, utilizing variational inference and the reparameterization trick for gradient computation. The model comprises two processes: a multivariate hypergeometric distribution for subset sizes and a Plackett-Luce distribution for point assignment. The authors derive an evidence lower bound for the marginal probability p(X) and introduce three methods leveraging the random partitioning model, including clustering, inferring latent factors, and partitioning neurons in multi-task learning.

### Strengths and Weaknesses
Strengths:  
- The paper effectively demonstrates the applicability of random partitioning models in machine learning and outlines the advantages of this approach.  
- Numerous technical optimizations, such as lower bounds of the loss function, are developed, enhancing the method's usability.  
- The writeup is clear, and the method is sound, with comprehensive supplementary material aiding understanding.  

Weaknesses:  
- The motivation for using the multivariate hypergeometric distribution requires further elaboration.  
- Experimental results appear tailored to the method, lacking more conventional experiments that could strengthen the paper's generalizability.  
- The writing is at times confusing and notation-heavy, with insufficient intuitive explanations for the equations.  

### Suggestions for Improvement
We recommend that the authors improve the motivation for the multivariate hypergeometric distribution and clarify under what conditions it is advantageous compared to a multinomial distribution. Additionally, including a more conventional experiment, perhaps inspired by recent works, would enhance the paper's credibility. We suggest incorporating a running example to aid exposition and addressing the clarity of the differentiability aspect earlier in the paper. Furthermore, the authors should clarify the discrepancies in experimental results compared to baseline papers and consider including additional datasets beyond MNIST for clustering tasks. Lastly, we advise integrating some supplementary material into the main text for better accessibility.