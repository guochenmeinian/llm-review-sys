ID: p0GyMJugcE
Title: Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair Modeling
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MixEncoder, a lightweight cross-attention mechanism designed to enhance the efficiency of sentence pairing while maintaining performance. MixEncoder achieves over 113x speedup by encoding the query once and processing candidates in parallel. The authors conduct extensive experiments demonstrating its effectiveness, although the focus is primarily on BERT-base without testing on larger language models.

### Strengths and Weaknesses
Strengths:  
1. The proposed MixEncoder simplifies cross-attention, allowing for pre-computation and reducing the number of tokens and layers involved.  
2. The paper is clear and easy to follow, with convincing experimental results across multiple datasets indicating both time and space efficiency.  
3. The analysis of strengths and limitations is well-articulated.

Weaknesses:  
1. The performance of MixEncoder is inconsistent across datasets, notably underperforming on MS MARCO compared to Dual-BERT.  
2. The study lacks experiments on larger language models to validate its effectiveness.  
3. The methods of comparison are somewhat outdated, and the time cost with different k values is not presented.

### Suggestions for Improvement
We recommend that the authors improve the paper by conducting experiments on larger language models to verify the effectiveness of MixEncoder. Additionally, please include a detailed analysis of the time cost with varying k values, as “a larger k can lead to a better performance for the S-strategy.” Updating the comparison methods with more recent works would also enhance the paper's relevance and impact.