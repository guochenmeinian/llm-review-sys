ID: 2Bef9YxSJc
Title: Language Models Encode Collaborative Signals in Recommendation
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 4, 4, 7, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AlphaRec, a novel recommendation method that leverages user behavior knowledge encoded in large language models (LLMs) through linear mappings and graph convolutions. The authors assert that AlphaRec effectively captures collaborative filtering signals using pretrained language model embeddings, trained with a contrastive loss. Experimental analyses are conducted in both standard and zero-shot settings, demonstrating the method's potential.

### Strengths and Weaknesses
Strengths:
- The topic is significant, addressing the intersection of language models and collaborative filtering, which is of great interest to the research community.
- The proposed approach of linear mapping from textual to collaborative filtering space is relatively unexplored in existing literature.
- The paper is well-structured and presents extensive experiments validating the proposed method's effectiveness.

Weaknesses:
- The writing misrepresents prior work in collaborative filtering, particularly regarding the claim of a new paradigm introduced by AlphaRec.
- AlphaRec lacks comparisons with stronger baselines, which could alter the conclusions drawn from the experiments.
- The formulation of the recommendation task does not accurately reflect practical recommendation systems, and the novelty of the method is limited due to the use of widely adopted techniques like graph convolutions and InfoNCE loss.

### Suggestions for Improvement
We recommend that the authors improve the literature review to accurately represent prior work in collaborative filtering and clarify the novelty of their approach. Additionally, AlphaRec should be compared with stronger baselines, including state-of-the-art sequential recommenders like SASRec and BERT4Rec, to strengthen the experimental results. The authors should also refine the task formulation to better align with practical recommendation scenarios and provide clearer definitions of model terminology. Finally, we suggest elaborating on the experimental findings regarding the performance of LLMs and the implications of using additional item information, as this could enhance the paper's overall soundness and clarity.