ID: NsxthTVpqA
Title: Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new re-weighting strategy called Contrastive Alignment (CAL) aimed at enhancing visual-language model (VLM) training by dynamically assigning weights to tokens based on their visual relevance. The authors propose calculating these weights using the logit differences with and without image inputs, demonstrating improvements in vision question answering (VQA) and captioning tasks. The method is evaluated across various benchmarks, showing solid performance.

### Strengths and Weaknesses
Strengths:
- The motivation for dynamically adjusting token weights is intuitive and aligns with established practices in the language domain.
- The experimental section is thorough, providing extensive quantitative analysis and validating CAL's effectiveness across multiple tasks.
- The method is easy to implement and achieves consistent performance improvements across different benchmarks.

Weaknesses:
- Presentation issues arise from unclear descriptions, particularly in equations and experimental contexts, leading to confusion about the methodology.
- The paper lacks comprehensive evaluations on language-only tasks and hallucination scenarios, limiting the understanding of CAL's broader applicability.
- The originality of the approach is questioned, as similar methodologies have been previously explored in the literature, reducing the novelty of the contribution.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation, particularly in equations and descriptions of experimental setups, to avoid confusion. Additionally, we suggest conducting more experiments on language-only tasks and hallucination scenarios to assess the broader implications of CAL. It would also be beneficial to provide detailed comparisons with baseline models addressing noisy tokens, as well as to discuss the assumptions underlying the proposed method. Finally, we urge the authors to explore and present the weight distribution across different token sub-groups to further validate their approach.