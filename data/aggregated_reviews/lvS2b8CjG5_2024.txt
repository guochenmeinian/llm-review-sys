ID: lvS2b8CjG5
Title: EEGPT: Pretrained Transformer for Universal and Reliable Representation of EEG Signals
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 5, 6, 5, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents EEGPT, a 10-million-parameter pretrained transformer model designed for universal EEG feature extraction. The authors propose a dual self-supervised learning method that combines masked autoencoder-style reconstruction and alignment loss, demonstrating state-of-the-art performance across various downstream EEG tasks, including motor imagery classification, ERP detection, and sleep stage detection.

### Strengths and Weaknesses
Strengths:
1. The dual self-supervised learning method enhances representation quality and model robustness.
2. EEGPT achieves superior performance on multiple EEG tasks, outperforming existing models.
3. The hierarchical structure allows for efficient processing of spatial and temporal information, reducing computational complexity.
4. Comprehensive experiments validate the model's effectiveness across various EEG applications.

Weaknesses:
1. The role of the adaptive spatial filter is unclear, necessitating an ablation study to assess its impact.
2. Inconsistencies in metrics between tables hinder clarity; Cohen's Kappa should be uniformly applied.
3. The technical contribution appears incremental, primarily building on existing frameworks with minor modifications.
4. The pretraining dataset lacks diversity, potentially limiting the model's universality compared to competitors like LaBraM.
5. The ablation study is conducted on a single dataset, which may not convincingly demonstrate the proposed modules' effectiveness.
6. Different model settings for downstream datasets may restrict generalizability.

### Suggestions for Improvement
We recommend that the authors clarify the function of the adaptive spatial filter and conduct an ablation study to evaluate its effectiveness. Additionally, we suggest using consistent metrics across tables, particularly Cohen's Kappa, and reporting LaBraM results for a more comprehensive comparison. The authors should consider expanding the pretraining dataset to enhance model universality and conduct ablation studies across multiple datasets to strengthen their claims. Finally, we advise providing clearer explanations of the model design and motivations behind specific choices to improve overall clarity.