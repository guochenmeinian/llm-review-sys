ID: 2NKumsITFw
Title: Learning from Noisy Labels via Conditional Distributionally Robust Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 20
Original Ratings: 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on learning from noisy labels using conditional distributionally robust optimization (CDRO) to estimate true label posteriors. The authors formulate the problem as minimizing the worst-case risk within a distance-based ambiguity set centered around a reference distribution, deriving upper bounds for this risk. Additionally, the work addresses potential misspecification of estimated true label posteriors and employs a robust pseudo-empirical distribution as a reference probability distribution. The authors also propose a method for robust pseudo-labeling in the context of noisy annotations, specifically addressing challenges posed by sparse annotation and noise transition estimation. Empirical results demonstrate that their method maintains high accuracy, achieving around 95% accuracy at high noise rates, even with increased annotation sparsity. The authors explore various strategies, including regularization techniques and subgroup structures, to enhance robustness against small sample sizes, validating the proposed method's effectiveness across various datasets.

### Strengths and Weaknesses
Strengths:
1. The proposed methodology is grounded in solid theoretical foundations.
2. The writing is clear and comprehensible, facilitating understanding of the concepts and methodology.
3. The paper provides rigorous theoretical analyses and generalization bounds for the upper bound of the risk function.
4. The proposed algorithm effectively handles imperfect transition estimation and demonstrates robustness in generating reliable pseudo-empirical distributions.
5. Empirical results indicate high accuracy across different noise levels and annotation scenarios, showcasing the method's effectiveness.

Weaknesses:
1. The theoretical analyses are limited to binary classification, neglecting the more common multi-class classification scenario.
2. The potential misspecification of estimated true label posteriors is not clearly defined or measured, raising questions about the extent to which the proposed method addresses this issue.
3. The performance improvement of the proposed method is not significant across multiple real datasets, and the justification for chosen baselines in experiments is insufficient.
4. The frequency-counting method for noise transition estimation may struggle with small labeled instance counts, potentially impacting accuracy.
5. The manuscript could benefit from a more explicit discussion of limitations related to sparse annotation and noise transition estimation.

### Suggestions for Improvement
We recommend that the authors extend the theoretical analyses to the multi-class classification case to enhance applicability. Additionally, the authors should clearly define and measure the potential misspecification of estimated true label posteriors to clarify the impact of their method. We suggest including more state-of-the-art methods in the experiments, such as ProMix, Divide-Mix, and SOP, and providing a clearer justification for the choice of baselines. Furthermore, addressing the sparse annotation problem in noise transition estimation with more robust methods would strengthen the paper. We also recommend that the authors add a limitations section to discuss the challenges posed by sparse annotation in noise transition estimation. Incorporating regularization techniques and subgroup structures could further enhance the robustness of the method. Lastly, conducting additional experiments with larger sample sizes to validate their findings and strengthen the results would be beneficial.