ID: GYqs5Z4joA
Title: SpGesture: Source-Free Domain-adaptive sEMG-based Gesture Recognition with Jaccard Attentive Spiking Neural Network
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an innovative framework for sEMG-based gesture recognition utilizing Spiking Neural Networks (SNNs). The authors propose a novel Jaccard Attention mechanism and Source-Free Domain Adaptation (SSFDA) to enhance model robustness and accuracy, achieving a high recognition accuracy of 89.26% on a newly collected sEMG gesture dataset with various forearm postures, while maintaining system latency below 100ms on a CPU. The Jaccard Attention mechanism computes attention directly on spike sequences, preserving the low-power characteristics of SNNs, and SSFDA improves model adaptation without requiring source data.

### Strengths and Weaknesses
Strengths:
- The introduction of a Jaccard-based attention mechanism and an SNN-oriented SSFDA algorithm significantly enhances accuracy and efficiency in sEMG gesture recognition.
- The focus on real-world application and deployment adds substantial value to the work.
- The systematic comparison of SpGesture with state-of-the-art methods in terms of accuracy, inference speed, and memory consumption is well-executed.

Weaknesses:
- There is a typo in the footnote to Figure 5, where the explanations for the first and second columns are reversed.
- In section 5.5, the comparison of SJA's benefits lacks specificity; the overall time and memory usage of the entire algorithm should be broken down to isolate the attention component's impact.
- The SSFDA method currently only addresses distribution shifts from forearm posture variations, and its applicability to other factors requires further validation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the Jaccard attention mechanism by providing an intuitive explanation of how it reflects 'attention' within tokens. Additionally, we suggest that the authors include a detailed breakdown of the time and memory usage for each part of the algorithm in section 5.5 to better align with the section's intent. Furthermore, discussing the potential extension of SSFDA to address other distribution shifts and validating performance on real hardware, such as neuromorphic chips, would enhance the paper's contributions. Lastly, we encourage the authors to clarify the selection process for the probability P in pseudo-labeling and to explore how Jaccard attention could be adapted for 3D data.