ID: cjbdRN8Yxy
Title: Compressing Context to Enhance Inference Efficiency of Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a methodology for compressing input contexts to large language models (LLMs) using an information-theoretic approach termed "Selective Context." The authors utilize self-information to identify and remove low-information tokens, aiming to enhance inference efficiency while reducing memory usage. Experimental results indicate that this method can maintain reasonable performance across various tasks despite some performance drops. The authors also propose using percentiles to select important tokens, demonstrating the method's effectiveness on datasets with redundant information.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant issue in LLMs regarding context size and computational efficiency.
- It offers a novel solution grounded in information theory, supported by extensive experimental validation.
- The writing is clear and accessible, making the concepts easy to follow.

Weaknesses:
- The experimental section lacks clarity, particularly regarding the computation overhead of self-information and its impact on overall latency.
- There is no baseline comparison with simpler summarization methods, making it difficult to assess the advantages of the proposed approach.
- Presentation choices in the experimental results are inconsistent, with some figures and tables lacking clarity and depth.
- The paper does not adequately discuss related works, and the filtering methods may lead to misunderstandings in downstream tasks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental section by explicitly reporting the computational costs associated with self-information calculations. Additionally, including a baseline comparison with simpler summarization methods would provide a clearer context for evaluating the benefits of Selective Context. We also suggest a more principled approach to presenting experimental results, potentially breaking down scores by model to enhance insight. Finally, addressing the lack of discussion on related works and refining the filtering methods could strengthen the paper's contributions.