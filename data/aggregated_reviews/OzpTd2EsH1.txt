ID: OzpTd2EsH1
Title: Rethinking the Backward Propagation for Adversarial Transferability
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 6, 7, 7, 5, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Backward Propagation Attack (BPA) as a method to enhance the transferability of adversarial examples by addressing gradient truncation in transfer-based attacks. The authors empirically demonstrate that non-linear layers, such as ReLU and max-pooling, truncate the loss gradient, negatively impacting transferability. BPA modifies the gradient computation by using a non-monotonic function as the derivative of ReLU and incorporating a softmax function to smooth the derivative of max-pooling. The results show significant improvements in adversarial transferability on the ImageNet dataset.

### Strengths and Weaknesses
Strengths:  
- The paper demonstrates originality by addressing the underexplored issue of gradient truncation in transfer-based attacks.  
- The proposed BPA method is novel and effectively enhances adversarial transferability through empirical validation.  
- The paper is well-structured, clearly communicating the problem, solution, and experimental methodology.  
- Extensive experiments validate the method's effectiveness across various models and attack types.  

Weaknesses:  
- The theoretical justification for the specific modifications to the gradient computation could be elaborated to enhance understanding.  
- The limitations of the proposed techniques, such as potential performance decreases from replacing the ReLU function, require further discussion.  
- The evaluation is primarily based on the ImageNet dataset; exploring generalizability across other datasets would strengthen the findings.  
- The clarity of certain key terms and concepts could be improved, as some definitions are lacking.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for the modifications made to the gradient computation, particularly regarding the choice of the non-monotonic function and softmax. Additionally, discussing the limitations of BPA in specific scenarios would provide a more comprehensive view of its effectiveness. We suggest testing the BPA method on other datasets to assess its generalizability and including evaluations against input-transformation-based attacks like TIM, SIN, and DIM for a more thorough comparison. Lastly, clarifying key terms and providing rigorous definitions would enhance the paper's clarity.