ID: LUT4b9gOtS
Title: Learning Visual Prior via Generative Pre-Training
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents VisorGPT, a method for modeling probabilistic visual priors through generative pretraining by transforming continuous values (locations, bounding boxes) into discrete tokens. The authors propose that VisorGPT can autonomously generate spatial layouts for image synthesis, enhancing data for visual models and allowing users to control object positions through initial layouts and natural language input. Experiments demonstrate that VisorGPT effectively models visual priors and improves various downstream tasks, such as conditional image generation. The approach translates image layout information into sequences suitable for NLP architectures, facilitating the learning of dataset distributions for plausible layout sampling. The authors acknowledge the need for clearer definitions of terms like "generalized visual intelligence models" and commit to providing additional details on training processes, evaluation metrics, and specific terminology used in the paper.

### Strengths and Weaknesses
Strengths:
- The proposal of a novel setting for modeling visual priors via generative pretraining is significant, potentially combining visual prior knowledge with large language models.
- VisorGPT demonstrates good controllability, allowing modifications through natural language.
- The model's ability to generate spatial layouts can significantly enhance existing visual models.
- The paper is well-structured, easy to follow, and includes comprehensive quantitative experiments and effective visualizations.
- The authors are responsive to feedback and committed to improving clarity in the final paper.

Weaknesses:
- The rationale for using GPT to model visual priors is inadequately explained, and comparisons with baseline methods are lacking. A straightforward autoregressive model with a regression head could serve as a baseline.
- The controllability of VisorGPT is overstated; users cannot control the positions of generated objects effectively, limiting its practical impact. Existing research has achieved similar controllability without additional training.
- The motivation and significance of the work are unclear, particularly regarding the generalization to unseen layouts and the practical applications of learning layout distributions.
- The evaluation section lacks comparisons with baseline methods and does not adequately assess image quality, diversity, or controllability.
- Some relevant feedback has not yet been incorporated into the main paper or supplementary materials, and the clarity of specific terms and evaluation metrics requires further elaboration.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the rationale for using GPT in modeling visual priors and provide comparisons with baseline methods to justify their approach. Additionally, the authors should clarify the controllability aspects of VisorGPT, ensuring that users can manipulate object positions effectively. We suggest improving the clarity of the term "generalized visual intelligence models" and providing additional descriptions regarding training details, evaluation metrics such as "format accuracy" and "matching accuracy," and the definitions of special tokens (SW) and textual knowledge (TK). It would also be beneficial to enhance the evaluation section by including comparisons with existing methods and providing a clearer analysis of the advantages of their model over simply sampling from ground-truth layouts. Finally, we encourage the authors to explore potential applications beyond conditional image generation to demonstrate the broader utility of VisorGPT and ensure that all relevant feedback is integrated into the main paper rather than promised for future inclusion.