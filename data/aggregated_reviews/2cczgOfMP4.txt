ID: 2cczgOfMP4
Title: Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 5, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Chain-of-Preference Optimization (CPO), a method designed to enhance reasoning in large language models (LLMs) by utilizing preference data generated during the Tree-of-Thought (ToT) process for fine-tuning. CPO improves performance in various reasoning tasks without increasing inference complexity, achieving results comparable to Tree-of-Thought methods while being significantly faster. The authors provide a comprehensive analysis of CPO's components and demonstrate its effectiveness across multiple datasets.

### Strengths and Weaknesses
Strengths:
- The concept of CPO is innovative and effectively enhances performance without increasing inference load.
- The paper is well-written, with clear explanations and a logical structure.
- Experimental results show that CPO outperforms traditional Chain-of-Thought approaches and achieves significant improvements in tasks like question answering and arithmetic reasoning.

Weaknesses:
- CPO's performance on complex reasoning benchmarks, such as the MATH benchmark, remains unexplored.
- The reliance on reasoning paths generated by ToT may limit diversity, necessitating a systematic investigation into the changes in generated paths pre- and post-fine-tuning.
- The effectiveness of CPO is contingent on the quality of reasoning paths from ToT; suboptimal paths could diminish the utility of preference data.
- The applicability of CPO to advanced models like LLaMA3 is not sufficiently addressed.
- There is a lack of in-depth analysis regarding the interpretability of optimized reasoning paths, which could enhance understanding and trust in model decisions.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of CPO on more complex reasoning benchmarks, such as the MATH benchmark, to better assess its capabilities. Additionally, a systematic investigation into the diversity of reasoning paths generated before and after fine-tuning is necessary. The authors should also provide a deeper analysis of the interpretability of the optimized reasoning paths to enhance trust in the model's decisions. Furthermore, clarifying the applicability of CPO to advanced models like LLaMA3 would strengthen the paper's contributions. Lastly, we suggest including a more robust baseline comparison and addressing the computational costs associated with the ToT approach in dataset construction.