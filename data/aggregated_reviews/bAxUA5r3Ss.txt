ID: bAxUA5r3Ss
Title: TaskBench: Benchmarking Large Language Models for Task Automation
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 7, -1, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TaskBench, a benchmark designed to evaluate LLMs in task automation, dividing the domain into three subtasks: task decomposition, tool selection, and parameter prediction. The authors propose a novel data generation method called "Back-Instruct" and an evaluation framework named TaskEval. The methodology includes constructing a tool graph to capture dependencies and generating instructions through back-translation of sub-graphs, aiming to create complex task scenarios.

### Strengths and Weaknesses
Strengths:
- The Tool Graph effectively represents dependencies between tools.
- A diverse range of LLMs, including both closed-source and open-source models, are evaluated.
- Human evaluations are conducted to assess the quality of generated data.

Weaknesses:
- The synthetic nature of the dataset raises concerns about its general quality, particularly regarding naturalness and alignment with human-like requests.
- The "task_steps" often mirror the user requests too closely, lacking the complexity and messiness of real-life instructions.
- The dataset appears too simplistic, with many tools having only one or two parameters, potentially allowing for easy saturation of the benchmark.
- The introduction lacks a discussion of relevant API datasets and tool-augmented LLM datasets, which is misleading.

### Suggestions for Improvement
We recommend that the authors improve the dataset quality by incorporating more human involvement in its creation, beyond mere verification. Enhancing the naturalness and utility of the dataset is crucial. Additionally, a clearer description of the human verification process should be provided, including details on the effectiveness of the self-critic mechanisms. The authors should also discuss how TaskBench compares to more relevant benchmarks like API-Bench and ToolBench in the main text, rather than relegating this information to the appendix. Finally, we suggest employing metrics that better capture the structure and order of tasks in the task decomposition evaluation, moving beyond ROUGE scores.