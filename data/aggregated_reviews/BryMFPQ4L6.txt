ID: BryMFPQ4L6
Title: Augmenting Language Models with Long-Term Memory
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 5, 6, 6, 7, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework called Decoupled-Memory-Augmented LLMs (DEMA) that enhances long-context processing in language models (LMs) by utilizing a decoupled network architecture. The authors propose a side network as a memory retriever and reader, which allows the model to store and retrieve key-value pairs from a cached memory bank, thus improving performance on long-context datasets. The experimental results demonstrate that DEMA outperforms existing models in perplexity and accuracy across multiple benchmarks.

### Strengths and Weaknesses
Strengths:
- The proposed method is clear, sound, and addresses a significant challenge in long-context modeling, potentially benefiting the community.
- The experiments validate the effectiveness of the approach on various datasets, showing strong few-shot learning results and improvements in memory-augmented in-context learning.
- The paper is well-written and presents a novel idea with a strong motivation.

Weaknesses:
- The explanation of the model architecture is unclear, making it difficult to correlate the description with the presented diagram. The authors should consider rewriting this section or using pseudo-code for clarity.
- The evaluation lacks comparisons with recent memory-augmented models, limiting the contextualization of results. Additionally, the experiments primarily focus on perplexity and accuracy without including tasks like summarization or title generation.
- The ablation studies do not sufficiently discuss the contribution of the SideNet module, and the visualization of memory size impacts is not clear. Furthermore, the related work section should include discussions on other memory-augmented methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the model architecture explanation and seek feedback from a fresh reader. Presenting the architecture in pseudo-code may also enhance understanding. Additionally, we suggest including experiments on established tasks such as long document summarization and comparisons with recent memory-augmented models like Memformer, Unlimiformer, and RWKV. To strengthen the evaluation, consider including qualitative analyses and detailed comparisons with baseline models, as well as clarifying the input batching procedure. Lastly, ensure proper citations for claims made in the paper and address the ambiguity in Figure 1 regarding the representation of inputs.