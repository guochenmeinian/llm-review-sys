ID: UqYrYB3dp5
Title: Polynomially Over-Parameterized Convolutional Neural Networks Contain Structured Strong Winning Lottery Tickets
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 6, 5, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 1, 3, 3, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of the Strong Lottery Ticket Hypothesis (SLTH) in the context of structured pruning for convolutional neural networks (CNNs), extending beyond traditional neuron/filter pruning by incorporating block-sparsity. The authors propose that randomly initialized networks contain subnetworks that can perform well without training, aiming to theoretically verify that "strong lottery tickets" can be obtained through structural pruning. They assert that polynomial overparameterization is sufficient for approximating ReLU neurons through structured pruning, diverging from the conclusions of Malach et al. [2020]. To support their claims, the authors introduce a Multidimensional Random Subset Sum lemma to demonstrate the existence of structured subnetworks in random CNNs that can approximate smaller networks, thereby extending the understanding of overparameterization in deep learning. They also suggest refining the title to reflect the polynomial nature of their bounds, proposing “Polynomially overparameterized convolutional neural networks contain structured strong lottery tickets.”

### Strengths and Weaknesses
Strengths:
1. The paper addresses a significant gap in research by focusing on structured pruning within the SLTH framework, expanding the understanding of lottery ticket mechanisms.
2. The authors effectively leverage recent advancements in the multidimensional generalization of the Random Subset-Sum Problem, showcasing their ability to adapt existing mathematical tools.
3. The authors provide a novel perspective on structured pruning by integrating block-sparsity.
4. The discussion on polynomial overparameterization adds depth to the theoretical framework.
5. The authors demonstrate responsiveness to reviewer feedback, indicating a willingness to refine their work.

Weaknesses:
1. The proof ideas lack novelty, as many cited works utilize similar techniques, with Theorems 3 and 5 being the primary contributions.
2. The assumption that a smaller CNN exists with the same restrictions as discussed is questionable, raising concerns about the validity of the entire approach.
3. The practical justification for pruning is undermined by the impracticality of encoding the required network size before pruning, complicating the process.
4. The bounds necessitate a network that is polynomially larger than the target, which may not be feasible in practice.
5. There is a lack of practical evidence to support the theoretical claims, leaving some reviewers with concerns about feasibility.
6. The authors' rationale for the potential relaxation of the ReLU activation function is vague, lacking specific reasons.
7. The relationship between structured pruning in convolutional layers and fully-connected layers is not thoroughly explored.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their proof ideas by clearly distinguishing their contributions from existing literature. Additionally, we suggest that the authors provide more intuitive explanations for their proof, particularly regarding the required network size and the implications of their assumptions. We encourage the authors to elaborate on the relationship between their results and those of Malach et al. regarding structured SLTH. It would also be beneficial for the authors to clarify the reasons behind their belief that the ReLU activation function can be relaxed. Finally, we suggest that the authors investigate existing literature on structured pruning for fully-connected layers to strengthen their discussion and include empirical evaluations to provide practical insights into the structured version of SLTH.