ID: pO7YD7PADN
Title: Understanding the Effect of Model Compression on Social Bias in Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper examines the impact of model compression techniques, specifically quantization and knowledge distillation, on social biases in large language models (LLMs). The authors evaluate compressed versions of BERT, RoBERTa, and Pythia models across benchmark datasets measuring gender, race, and religion biases. Findings indicate that compression methods can reduce social bias while maintaining language modeling performance, with longer pretraining and larger models correlating with higher bias levels.

### Strengths and Weaknesses
Strengths:
- The paper explores the underexamined relationship between model compression and social bias reduction, presenting a novel direction for bias analysis.
- Experiments systematically evaluate various compression methods on multiple architectures, demonstrating that compression can mitigate biases.
- The writing is clear, effectively explaining the background, experiments, results, and implications, with visualizations summarizing key trends.

Weaknesses:
- Limited technical contributions, as the evaluated compression techniques are standard methods with little innovation.
- The analysis is constrained by a narrow focus on gender, race, and religion biases, neglecting other significant biases and datasets, including assessments on state-of-the-art generative models like GPT.
- Writing logic requires improvement, particularly in sections introducing debiasing baselines, which disrupts flow.

### Suggestions for Improvement
We recommend that the authors improve the technical depth by incorporating additional debiasing methods, such as Counterfactual Data Augmentation debiasing (CDA), dropout debiasing (DO), and Token-level debiasing (AT), and compare the results of using compression methods with these techniques. Additionally, we suggest expanding the analysis to include a broader range of biases and datasets beyond gender, race, and religion. Clarifying the results regarding the SEAT dataset and addressing the discrepancies in bias scores with recent literature would enhance the paper's robustness. Lastly, we advise revising the writing for better logical flow, particularly in sections discussing results and comparisons.