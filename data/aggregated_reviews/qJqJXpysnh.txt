ID: qJqJXpysnh
Title: Handshape-Aware Sign Language Recognition: Extended Datasets and Exploration of Handshape-Inclusive Methods
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents advancements in Sign Language Recognition (SLR) by introducing the handshape-aware PHOENIX14T-HS dataset, which augments the existing PHOENIX14T dataset with handshape labels. The authors propose two methodologies, Model I and Model II, that utilize handshape information to improve SLR performance. The study emphasizes the integration of phonological features of sign languages and explores various pretraining strategies for SLR models. However, the overall performance gains in SLR tasks are marginal compared to existing state-of-the-art methods.

### Strengths and Weaknesses
Strengths:
- The introduction of the PHOENIX14T-HS dataset provides a valuable resource for researchers in SLR, enabling the development of handshape-aware models.
- The proposed methodologies demonstrate significant advancements in SLR, particularly Model II, which achieves top performance among single-modality models.
- The paper outlines promising future research directions, including multimodal SLR models and effective pretraining strategies.

Weaknesses:
- The handshape labels in the dataset may be noisy due to reliance on a single annotator and a user-edited online dictionary, raising concerns about the reliability of the data.
- The evaluation is primarily based on a single dataset, limiting the generalizability of the proposed methods to other sign languages and contexts.
- The paper lacks a thorough comparative analysis with existing multimodal SLR methods, which could clarify the advantages of the proposed approaches.

### Suggestions for Improvement
We recommend that the authors improve the dataset's reliability by addressing potential biases in handshape annotations and conducting inter-annotator agreement analysis. Additionally, the authors should consider expanding the evaluation to include multiple sign language datasets to assess the generalizability of their methods. A deeper comparative analysis with existing multimodal approaches is essential to highlight the unique contributions of their work. Finally, exploring the integration of other modalities beyond handshapes could enhance the overall effectiveness of the proposed methodologies.