ID: FqWyzyErVT
Title: Federated Transformer: Multi-Party Vertical Federated Learning on Practical Fuzzily Linked Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 4, 5, 7, 6, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Federated Transformer (FeT), a novel framework for Vertical Federated Learning (VFL) that addresses challenges associated with fuzzy identifiers in multi-party scenarios. The authors propose three key innovations: positional encoding averaging, dynamic masking, and a party dropout strategy, which collectively enhance performance, privacy, and reduce communication overhead. Experimental results indicate that FeT significantly outperforms baseline models in scalability and privacy preservation.

### Strengths and Weaknesses
Strengths:
1. The integration of a transformer architecture for managing fuzzy identifiers in multi-party VFL is innovative, with dynamic masking and party dropout strategies enhancing scalability and reducing communication costs.
2. The proposed model is well-articulated, supported by rigorous experimental validation demonstrating substantial improvements over existing methods, and incorporates differential privacy and secure multi-party computation to strengthen privacy.
3. The paper is well-organized, providing clear explanations of problems, proposed solutions, and results, making it accessible to readers with a background in federated learning.
4. The results show significant improvements in both performance and privacy, contributing valuable insights to the field of federated learning, particularly for sensitive data applications.

Weaknesses:
1. While performance improvements are notable, the authors should clarify the dependency of these improvements on initial data alignment and fuzzy identifier distribution, and provide insights on the robustness of FeT under less ideal conditions.
2. Although scalability is discussed, there is insufficient focus on the computational resources required; the authors should comment on the computational overhead and practicality of deploying FeT in resource-constrained environments.
3. The introduction of complex mechanisms like dynamic masking and positional encoding averaging raises questions about their impact on training time and model complexity, necessitating a discussion on potential trade-offs between performance and efficiency.
4. The experiments primarily utilize synthetic datasets; the authors should address how well FeT generalizes to real-world datasets, especially those with higher noise levels and less structure.

### Suggestions for Improvement
We recommend that the authors improve the motivation of the paper by providing concrete use cases for fuzzy VFL that involve fuzzy data/linkages while addressing privacy needs. Clarifying key concepts in fuzzy VFL and the rationale behind dynamic masking and party dropout would enhance understanding. Additionally, the authors should compare the performance, privacy, and computational resource consumption of FeT against existing methods, particularly focusing on training efficiency and privacy protection capabilities. Finally, addressing the generalizability of FeT to real-world datasets and discussing the implications of the model's reliance on linkage quality would strengthen the paper.