ID: 7HFQfRjdcn
Title: Neural Characteristic Activation Analysis and Geometric Parameterization for ReLU Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 8, 3, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to addressing the instability of activation boundaries in ReLU neural networks by introducing Geometric Parameterization (GmP). The authors define a relu unit based on its activation boundary and propose using hyperspherical coordinates to stabilize the optimization process. They demonstrate that this parameterization maintains bounded changes in angles during gradient descent, thus improving optimization stability and convergence speed.

### Strengths and Weaknesses
Strengths:  
The paper identifies a significant issue with ReLU neurons that can lead to training instability and proposes an elegant solution through spherical coordinate parameterization. The computational efficiency remains comparable to traditional methods, and the size-independent nature of the parameterization is advantageous. The authors provide extensive experiments showing improved stability of activation boundaries and faster optimization.

Weaknesses:  
There are assumptions in the analysis that raise questions about their validity, particularly regarding the worst-case perturbations. The paper lacks discussions on other normalization techniques and does not empirically demonstrate scenarios where GmP may not be the best option. Additionally, some theoretical claims, particularly regarding the proof of Theorem 3.7, are flawed, and the paper overstates its theoretical contributions related to convergence speed and generalization.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their assumptions in the analysis, particularly regarding the worst-case perturbations. It would be beneficial to include empirical evidence demonstrating the performance of GmP with other activation functions and to explore scenarios where GmP may not be optimal. We also suggest revising the proof of Theorem 3.7 to correct inaccuracies and to ensure that claims about convergence speed and generalization are supported by theoretical justifications. Additionally, addressing the notation inconsistencies would enhance clarity.