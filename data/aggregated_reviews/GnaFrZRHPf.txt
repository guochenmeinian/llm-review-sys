ID: GnaFrZRHPf
Title: Adaptive Preference Scaling for Reinforcement Learning with Human Feedback
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel adaptive preference loss function inspired by distributionally robust optimization (DRO) to learn versatile rewards for downstream policy optimization. The authors propose a learnable instance-specific scaling factor that accommodates varying uncertainties in preference strength, allowing for a non-linear relationship between the preference distribution and reward difference. The proposed loss function is strictly convex, introducing negligible computational overhead, and is validated through experiments on robotic control tasks and large language models (LLMs).

### Strengths and Weaknesses
Strengths:
1. The approach of adapting optimization strength based on the characteristics of each preference pair is innovative.
2. The method is well-motivated theoretically.
3. Empirical results demonstrate practical effectiveness in both robotic control and natural language generation tasks.

Weaknesses:
1. The per-instance adaptive scaling incurs significant computational costs, hindering real-world mini-batch learning, as indicated in the algorithm box in the appendix.
2. Some derivations could be simplified for conciseness and to allow space for additional technical details.
3. The claim regarding the flexibility of the proposed reward function lacks empirical support, particularly in relation to the linear scaling limitation of the BT model.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations and potential negative societal impacts of their method. Additionally, we suggest providing a more detailed mathematical justification for the incorporation of adaptive preference scaling into DPO, particularly in the context of KL-control. The authors should also clarify the meaning of M and K in Algorithm 1, elaborate on the flexibility of the reward function, and include an ablation study of the regularization term in the proposed loss function. Furthermore, we encourage the authors to conduct multiple runs for the NLP experiments to strengthen the statistical significance of their findings and to include relevant benchmarks for a comprehensive evaluation of their approach.