ID: z9l6nHpTyT
Title: Adapter-TST: A Parameter Efficient Method for Multiple-Attribute Text Style Transfer
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Adapter-TST, an adapter-based framework for multiple-attribute text style transfer, addressing the challenge of fine-tuning large language models with limited data. The method utilizes neural adapters with parallel and stacked connection configurations to control various stylistic attributes simultaneously. The authors validate Adapter-TST using BART-Large on two datasets, showing performance comparable to several baseline methods, although improvements are not statistically significant.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, coherent, and easy to follow, effectively motivating the study of multiple-attribute textual style transfer.
- Adapter-TST is a parameter-efficient method that could interest both the TST and broader NLG communities.
- The authors have made commendable efforts to address reviewers' concerns by providing additional results and clarifying the human evaluation process.

Weaknesses:
- The evaluation is limited to a single backbone model (BART-Large), raising concerns about generalizability and evaluation bias.
- The human evaluation lacks specific details, such as the number of examples and workers involved, which are essential for transparency.
- The comparison with prior works is insufficient, with missing references and a lack of stronger baselines in the experimental setup.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by assessing Adapter-TST with multiple backbone models beyond BART-Large. Additionally, providing more transparency in the human evaluation process by detailing the number of examples and workers involved would enhance the credibility of the results. The authors should also consider including stronger baselines and relevant references in their comparisons to strengthen their argument for the effectiveness of Adapter-TST. Furthermore, a more comprehensive analysis of the results would provide deeper insights into the method's performance.