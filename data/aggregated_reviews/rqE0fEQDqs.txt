ID: rqE0fEQDqs
Title: PointGPT: Auto-regressively Generative Pre-training from Point Clouds
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 5, 6, 7, 5, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents PointGPT, a self-supervised learning strategy for 3D representation learning that adapts the autoregressive pre-training paradigm from NLP to point clouds. It utilizes an enlarged pre-training dataset and a dual masking strategy to achieve state-of-the-art (SOTA) performance on various benchmarks. The authors propose a method for arranging point patches based on spatial proximity, allowing for masked patches to be predicted in an auto-regressive manner.

### Strengths and Weaknesses
Strengths:
1. The introduction of GPT-style pre-training into 3D tasks is a novel approach, marking the first attempt to apply this concept in this domain.
2. The motivation for using GPT pre-training to avoid shape information leakage is well-articulated.
3. The enlarged training set significantly enhances performance, as evidenced by the results on object tasks.

Weaknesses:
1. The marginal performance improvement from the relative direction prompt via point order is understated, while the contribution of the enlarged pre-training dataset is more significant.
2. The dual masking strategy's similarity to attention dropout requires clarification regarding their differences.
3. The evaluation should include more practical downstream tasks, such as detection and segmentation, particularly when scene datasets are involved in pre-training.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the differences between the dual masking strategy and attention dropout. Additionally, the authors should conduct an ablation study to demonstrate the performance of PointGPT without the GPT loss and clarify the advantages of using Morton code for sorting point clouds. Incorporating comparisons of learnable parameters during training in the experimental tables is essential for assessing the contribution of 3D pre-training. Finally, we suggest that the authors cite and discuss related works from CVPR 2023 to enhance the paper's context and relevance.