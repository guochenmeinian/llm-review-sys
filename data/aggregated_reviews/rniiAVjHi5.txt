ID: rniiAVjHi5
Title: Universality of AdaGrad Stepsizes for Stochastic Optimization: Inexact Oracle, Acceleration and Variance Reduction
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 5, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents adaptive gradient methods for convex composite optimization, introducing two algorithms, UniSgd and UniFastSgd, which utilize AdaGrad step sizes. The authors establish efficiency guarantees and demonstrate both implicit and explicit variance reduction properties, extending results to include SVRG-type variance reduction for improved convergence rates. Additionally, the paper discusses the advantages of $\sigma_*$-bounds over $\sigma$-bounds in smooth stochastic optimization problems, arguing that the assumption of uniformly bounded variance is not universally applicable. The authors also present a novel version of the adaptive accelerated SVRG method (Algorithm 4), which is simpler and has stronger complexity results compared to the existing AdaVRAE method. The convergence analysis extends beyond the modified AdaGrad step size to a classical one, allowing for biased stochastic oracles and providing concise proofs that unify various methods.

### Strengths and Weaknesses
Strengths:
1. The paper extends existing results by demonstrating the universality of AdaGrad step sizes, providing efficiency guarantees and state-of-the-art complexity bounds.
2. The proposed methods are adaptive and do not require problem-specific constants, enhancing their practical applicability.
3. The extension to HÃ¶lder-smooth functions fills a gap in existing literature, and the establishment of $\sigma_*$-bounds for SGD methods with AdaGrad step sizes is presented as a novel result.
4. The convergence analysis of the adaptive accelerated SVRG method is significantly simplified, avoiding the need for complex lemmas, and the ability to handle biased stochastic oracles enhances the contributions.

Weaknesses:
1. The algorithms still require knowledge of the domain diameter, which limits their adaptability in practical scenarios.
2. The main body of the paper is difficult to comprehend; the complexity of the algorithms is not well-explained, and the novelty of the work is insufficiently emphasized.
3. Empirical evaluations are limited to synthetic problems, lacking extensive testing on real-world datasets.
4. The authors' claims regarding the applicability of lower bounds in finite-sum cases are contested, as they may not hold without the finite variance condition.
5. Some technical details might be overlooked in the simplification process, and the analysis lacks new insights, being viewed as a trivial combination of existing results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the algorithm descriptions to enhance understanding. Additionally, addressing the reliance on the domain diameter and exploring methods that do not require this knowledge would strengthen the paper's applicability. Expanding empirical evaluations to include a wider range of real-world datasets would provide a more comprehensive validation of the proposed methods. We also suggest that the authors improve the clarity of their arguments regarding the applicability of lower bounds in finite-sum cases and elaborate on the practical implications of using $P'_D$, specifically addressing how to determine the appropriate diameter $D$ in various scenarios. Finally, we recommend that the authors emphasize the significance of separating $\delta_f$ and $\delta_g$ in their analysis and ensure that all technical details are adequately addressed in their simplifications to maintain clarity and rigor.