ID: xKRg5dfWyv
Title: Bootstrapping Small \& High Performance Language Models with Unmasking-Removal Training Policy
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper empirically evaluates the impact of three factors on improving small language models for downstream tasks: pretraining data, vocabulary, and masking policy. It assesses three tasks: semantic role labeling (SRL), question answer driven semantic role labeling (QASRL), and question answer meaning representation (QAMR). The findings indicate that an unremoval masking policy generally enhances model performance, while increasing vocabulary size does not yield positive results. Additionally, continuous pretraining with more data leads to performance improvements.

### Strengths and Weaknesses
Strengths:
- The paper provides valuable insights into the pretraining and finetuning of small language models, which are easier to handle than larger models.
- It reports state-of-the-art results and thorough experimentation, supporting the authors' claims.
- The analysis of the unmasking-removal strategy offers potential implications for low-resource pretraining.

Weaknesses:
- The work is considered incremental, as it largely validates previous findings from Huebner et al. without offering significant new insights.
- The evaluation is limited to only three specific tasks and a single architecture (BabyBERTa), which restricts its broader applicability.
- Statistical significance tests are not comprehensively reported across different configurations, undermining the robustness of the results.

### Suggestions for Improvement
We recommend that the authors improve the breadth of their evaluation by including additional downstream tasks to enhance the generalizability of their findings. It would also be beneficial to conduct and report statistical significance tests for all model configurations to provide more compelling evidence. Furthermore, clarifying the high-level motivation for this work and its implications for practical NLP systems would strengthen the paper's contribution. Lastly, addressing the typos and unclear sections, such as the explanation of significance testing, would enhance clarity and readability.