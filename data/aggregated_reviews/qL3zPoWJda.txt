ID: qL3zPoWJda
Title: TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge Retention and Promotion
Conference: NeurIPS
Year: 2023
Number of Reviews: 22
Original Ratings: 8, 4, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to address catastrophic forgetting in neural networks through a three-stage process: retaining, revising, and rewinding neuron weights. The authors draw inspiration from biological mechanisms to enhance continual learning (CL) capabilities. Their experimental results demonstrate that the proposed method performs comparably or better than existing state-of-the-art (SOTA) approaches in both class- and task-incremental learning scenarios. Additionally, the paper provides a comparative analysis of the learnable parameters and memory requirements of TriRE against DER++, EWC, and PNNs. The authors demonstrate that, like DER++ and EWC, TriRE does not add learnable parameters, while PNNs exhibit an infeasible increase in parameters with longer task sequences. TriRE's memory consumption is influenced by multiple factors, including masking mechanisms, the Rewind phase, and the use of the Exponential Moving Average (EMA) model. Notably, TriRE maintains a fixed capacity model, distinguishing it from PNNs, which show exponential growth in memory consumption. The authors assert that TriRE occupies an intermediary position regarding learnable parameters and memory overhead, offering superior performance compared to PNNs.

### Strengths and Weaknesses
Strengths:
- The work creatively links biological motivations to continual learning capabilities.
- It provides a comprehensive summary of existing CL approaches, discussing their strengths and weaknesses.
- Figures are well-crafted and easily understandable.
- The performance of the proposed method is comparable to SOTA methods.
- The significance of dynamic masking, retaining, and rewinding is well-supported by experimental data.
- TriRE does not increase learnable parameters, similar to DER++ and EWC.
- Maintains fixed memory capacity, unlike PNNs, which face exponential growth.
- The paper proposes potential optimizations for the masking mechanism.

Weaknesses:
- The method is primarily applicable to CNN architectures, with additional tuning needed for hyperparameters, limiting its applicability to transformers.
- The related work section could benefit from elaboration on notable CL approaches that share strengths with TriRE and clarification on its comparative advantages.
- Limitations discussed in the appendix should be included in the main text for better visibility.
- The clarity of certain statements and technical details regarding the method's components, such as the overhead costs of rewinding and the specifics of the merging process, is lacking.
- The analysis lacks details on computational costs and hyperparameter tuning.
- Missing elements need to be addressed in the final version.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the limitations section by moving it to the main text and elaborating on the practical applicability of their work. Additionally, the authors should provide more technical details regarding the method's components, including the overhead costs associated with rewinding and the criteria for merging subnetworks. It would also be beneficial to include recent baselines for comparison and to clarify the relationship between CLS-ER and TriRE to better evaluate their contributions. Furthermore, we suggest that the authors conduct a thorough analysis of computational costs and provide experimental results to support claims of robustness to hyperparameters. Finally, we recommend including detailed explanations of hyperparameter tuning, given the large number of hyperparameters involved, and exploring techniques like spatial hashing and compression to optimize the employed masking mechanism further.