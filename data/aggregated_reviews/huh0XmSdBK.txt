ID: huh0XmSdBK
Title: NPCL: Neural Processes for Uncertainty-Aware Continual Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 4, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for Continual Learning (CL) utilizing a Neural Processes (NP) framework, inspired by MTP, which employs both global and task-specific latents for knowledge representation. The authors introduce regularization on parameter updates to prevent excessive changes and implement task-wise inference using an uncertainty quantification metric. The inference process involves deriving global and task-specific latents through shared and individual encoders, followed by decoding predictions based on these latents. The final prediction is determined using uncertainty-based inference, specifically by computing Shannon entropy scores. Their approach achieves performance comparable to state-of-the-art methods while demonstrating improved memory efficiency.

### Strengths and Weaknesses
Strengths:
- The paper introduces new regulations for employing NPs in CL.
- Prior works are well summarized, and the model is adequately discussed.
- The authors clarify the roles of global and task-specific latents in the NPCL framework, enhancing understanding of the model's operation.
- Evaluation across multiple benchmarks shows that NPCL performs comparably to state-of-the-art methods like DER.
- The method effectively integrates task IDs to improve the accuracy of predictions during inference.
- The study includes diverse ablations, highlighting the importance of task-wise inference and the utility of uncertainty in task label estimation.
- The model demonstrates efficient storage utilization compared to DER.

Weaknesses:
- The motivation for using NPs in CL lacks clarity, particularly regarding the significance of measuring predictive uncertainties.
- Some reviewers expressed concerns about the clarity and validity of the NPCL model prior to the authors' responses, indicating initial confusion regarding its formulation.
- Figures are not easily readable due to small fonts and missing labels.
- There are inconsistencies in the analysis of input correlations and limitations related to dot-product attention.
- Some equations and explanations, particularly equations (7) and (8), are confusing and require clarification.
- Comparisons in Table 1 may mislead regarding performance expectations due to differing storage usages.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their motivation for using NPs in CL, specifically addressing the importance of predictive uncertainties. Additionally, we suggest enhancing the clarity of explanations regarding the NPCL framework to preemptively address potential reviewer concerns. Updating the figures to enhance readability and ensuring all axes are labeled is also advised. Clarifying the analysis of input correlations and addressing the inconsistencies in the limitations section would strengthen the paper. We also advise revising the explanations surrounding equations (7) and (8) for better comprehension. Finally, consider presenting the comparison in Table 1 as a plot to clearly depict the relationship between storage usage and accuracy. Further experimental results could also strengthen the validation of the model and its practical implications in task-agnostic CL settings.