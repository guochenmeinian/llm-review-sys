ID: G14N38AjpU
Title: Evolutionary Neural Architecture Search for Transformer in Knowledge Tracing
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 5, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to knowledge tracing (KT) by employing Neural Architecture Search (NAS) within a modified Transformer architecture. The authors construct a search space that effectively models both global and local contexts, addressing students' forgetting behavior. They introduce a selective hierarchical input module for automated feature selection, achieving impressive results on two educational datasets, EdNet and RAIEd2020.

### Strengths and Weaknesses
Strengths:
- The paper provides a clear motivation for utilizing NAS techniques in KT, bridging a significant gap in the literature.
- It is well-structured and accessible, with convincing experimental results.
- The design of the search space is reasonable and effectively captures the nuances of KT.

Weaknesses:
- The paper lacks a detailed report on search costs, including supernet training and evolutionary search times.
- Parameters and FLOPs for all architectures in Table 2 are not provided, hindering comprehensive evaluation.
- The novelty of the approach is unclear, particularly regarding the justification for using NAS over alternative methods, such as mask-based techniques.
- The technical contribution appears limited, with insufficient discussion on how the proposed NAS generalizes to other transformer-based KT models.

### Suggestions for Improvement
We recommend that the authors improve the reporting of search costs, including supernet training and evolutionary search times, to enhance transparency. Additionally, providing parameters and FLOPs for all architectures in Table 2 would facilitate a more thorough evaluation of effectiveness. We suggest that the authors clarify the rationale for choosing NAS over other methods and consider including comparisons with alternative approaches. Furthermore, conducting additional experiments focusing on the hierarchical fusion method and its performance across various datasets would strengthen the paper's contributions.