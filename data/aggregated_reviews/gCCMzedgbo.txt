ID: gCCMzedgbo
Title: TrAct: Making First-layer Pre-Activations Trainable
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TrAct, a novel training strategy that modifies the optimization behavior of the first layer, enabling faster convergence and improved classification performance across various models. The effectiveness of TrAct is demonstrated through 50 experimental setups on multiple benchmarks, highlighting its capability in image classification tasks. The authors propose that optimizing the first layer embedding directly addresses the dependency on input pixel values, leading to a lightweight modification applicable to a range of vision models.

### Strengths and Weaknesses
Strengths:
1. The technical elaboration of the proposed method is clear and straightforward.
2. TrAct enables faster convergence and slightly better performance for the same number of epochs.
3. The evaluations across diverse benchmarks provide evidence of the method's effectiveness.

Weaknesses:
1. There is curiosity regarding the general applicability of TrAct to other visual tasks, such as detection and segmentation, which could be verified through experiments on FasterCNN.
2. A significant performance difference between SGD and Adam raises questions about the theoretical underpinnings of this observation.
3. The intuition behind first layer embedding optimization requires further elaboration, particularly how it reduces input dependency without compromising training efficiency.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the applicability of TrAct in other visual tasks by conducting experiments on FasterCNN. Additionally, we suggest providing a theoretical explanation for the performance differences observed between SGD and Adam. It would also be beneficial to elaborate on the intuition behind first layer embedding optimization to clarify how it mitigates input dependency. Furthermore, consider rephrasing the explanation about the relationship between language models and vision models in the method section for better clarity.