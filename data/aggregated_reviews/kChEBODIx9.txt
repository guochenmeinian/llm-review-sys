ID: kChEBODIx9
Title: Can Pre-Trained Text-to-Image Models Generate Visual Goals for Reinforcement Learning?
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method called LfVoid that utilizes pre-trained text-to-image models to generate consistent visual goal frames for reinforcement learning (RL) tasks. The authors propose an approach that incorporates techniques such as DreamBooth, null-text inversion, P2P, and Directed diffusion to enhance editing consistency and improve the performance of example-based RL. The experimentation demonstrates that the proposed modifications yield higher-fidelity images and improved downstream performance compared to existing methods.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, clear, and easy to understand.
- It presents an organic integration of several techniques for controlled generation.
- The proposed LfVoid effectively modifies desired objects without altering irrelevant elements, showcasing improved generative quality that enhances RL performance.
- The inclusion of real-world experiments strengthens the validity of the proposed approach.

Weaknesses:
- The evaluation benchmark is somewhat simplistic, potentially underutilizing the capabilities of the pre-trained LDM.
- There is a lack of comparison with other RL methods that leverage goal frame generation, limiting the originality and significance of the work.
- The method description lacks clarity, with vague mathematical details and insufficient context regarding the optimization approach and the necessity of inversion.
- The experimental results do not adequately demonstrate the necessity of visual goals, as no baselines in the RL experiments are not based on imagined goals.

### Suggestions for Improvement
We recommend that the authors improve Figure 1 to better communicate the problem and highlight their contributions. Additionally, it would be beneficial to compare the proposed method with notable works such as CACTI, GenAug, and ROSIE, as well as DALL-E-Bot, to provide a more comprehensive evaluation. We suggest conducting evaluations on a comprehensive image editing dataset to ensure equitable comparisons among methods. To enhance the persuasiveness of findings, consider introducing more complex tasks or additional distractors during evaluations. Furthermore, we encourage the authors to clarify the motivation, background, and problem formulation in the paper, particularly before section 3, and to provide more quantitative metrics for image editing experiments. Lastly, we recommend exploring the effects of DreamBooth and null-text inversion in the ablation study to validate their contributions.