ID: 7t6aq0Fa9D
Title: FASTopic: Pretrained Transformer is a Fast, Adaptive, Stable, and Transferable Topic Model
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FASTopic, a topic modeling paradigm that utilizes dual semantic-relation reconstruction (DSR) to model topic-document and topic-word relations, enhanced by an embedding transport plan (ETP) to mitigate relation biases. The authors claim that FASTopic outperforms existing baselines in effectiveness, efficiency, adaptability, stability, and transferability, supported by experimental results across six benchmark datasets.

### Strengths and Weaknesses
Strengths:
- The integration of DSR provides a novel approach to handling semantic relations in topic modeling.
- Comprehensive experiments demonstrate FASTopic's superiority in various metrics, including topic coherence and diversity.
- The paper is well-written and the code is accessible.

Weaknesses:
- The performance may heavily depend on the document embedding model, necessitating further discussion on how changes in embeddings affect results.
- The aggressive tone in the presentation undermines the claims made, requiring more robust experimental support.
- Certain tables and figures are misaligned, potentially distracting readers, though this does not significantly impact content quality.

### Suggestions for Improvement
We recommend that the authors improve the paper by:
1. Including comparisons with additional models such as ETM, ProdLDA, CEDC, and CTMneg to substantiate claims of superiority.
2. Incorporating a broader range of evaluation metrics to strengthen the validation of results.
3. Clarifying how training time is measured across models, including whether all steps are accounted for and if the same encoding model is used.
4. Providing insights into the selection process for the number of topics in BERTopic and comparing it with other methods.
5. Discussing the implications of the document embedding model choice and its impact on interpretability.
6. Analyzing the time complexity of the FASTopic framework and its efficiency in solving the optimal transport problem.
7. Addressing the semantic meaningfulness of the created word embeddings and exploring clustering techniques for topic generation.