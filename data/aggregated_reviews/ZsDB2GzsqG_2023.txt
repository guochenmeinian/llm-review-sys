ID: ZsDB2GzsqG
Title: MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MAGICBRUSH, an instruction-guided image editing dataset comprising 10,000 manually annotated triplets (source image, instruction, target image). The dataset supports various editing scenarios, including single-turn, multi-turn, mask-provided, and mask-free editing. The authors argue that the primary challenges in instruction-guided image editing are ensuring semantic consistency with textual instructions and achieving seamless integration of edits within images. They fine-tune InstructPix2Pix on this dataset, demonstrating improved performance compared to previous methods, and provide evidence of performance improvements when fine-tuning existing models with their dataset, asserting its value in real image editing contexts.

### Strengths and Weaknesses
Strengths:
1. This is the first large-scale, manually annotated dataset for instruction-guided real image editing.
2. The dataset offers high-quality supervision through a human-in-the-loop annotation process, which is superior to existing synthetic datasets.
3. The dataset collection details are comprehensive, and empirical results demonstrate significant improvements in edit instruction consistency and image quality when fine-tuning models with the dataset.
4. The writing is clear, professional, and easy to understand.

Weaknesses:
1. The dataset's performance upper bound is limited by DALL-E, which may not enhance image generation quality for instruction-guided editing.
2. Fine-tuning InstructPix2Pix on the dataset lacks persuasiveness due to expected performance improvements.
3. Clarity is needed on whether the test split includes out-of-domain instructions and the percentage of such instances, as the definition of out-of-distribution (OOD) instructions is somewhat vague.
4. There are minor unexpected changes in a small portion of the dataset, which the authors acknowledge but claim do not significantly impact overall quality; however, the varying OOD ratios may not fully clarify the dataset's diversity.
5. The quality of the dataset is questionable, with examples of unnatural edits noted.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the dataset's potential biases due to its reliance on DALL-E 2 and emphasize the role of DALL-E in the diagrams. Additionally, please provide more details about the crowd worker annotation process, including time, budget, and tutorial specifics. We suggest improving the clarity of the OOD instruction definitions and providing more robust evidence to support the claim that unexpected changes constitute less than 2% of the dataset. It would also be beneficial to evaluate more methods beyond InstructPix2Pix to validate the dataset's effectiveness further. Lastly, we suggest including a more thorough discussion of the limitations and failure cases of MAGICBRUSH in the main text, and consider training InstructPix2Pix from scratch on the proposed dataset to further validate its effectiveness.