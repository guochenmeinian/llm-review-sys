ID: ixcsBZw5pl
Title: Non-adversarial training of Neural SDEs with signature kernel scores
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 6, 7, 5, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 1, 4, 1, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach for training Neural Stochastic Differential Equations (SDEs) using non-adversarial methods based on signature kernel scores. The authors propose a new class of scoring rules that enhance stability and mitigate issues like mode collapse, commonly associated with GAN-based training. The method allows for memory-efficient adjoint-based back-propagation and demonstrates superior performance across various tasks, including financial data simulations.

### Strengths and Weaknesses
Strengths:
1. The proposed method outperforms existing techniques, particularly SDE-GAN, without requiring adversarial training, leading to more stable training.
2. The theoretical framework is well-articulated, with clear mathematical formulations and definitions.
3. The paper is written with high clarity, making the explanations and derivations accessible.

Weaknesses:
1. The significance of the work is unclear, particularly in relation to closely related methods discussed in the literature review, which could benefit from clearer positioning and comparisons.
2. The experimental design lacks sufficient exploration of baseline models, particularly diffusion models, which are relevant for time series generation.
3. The paper's presentation could be improved for readers with varying levels of mathematical background, as some notations and concepts are not clearly defined.

### Suggestions for Improvement
We recommend that the authors improve the motivation for using signature kernels and provide a clearer rationale for their choice among various kernels. Additionally, we suggest including sub-captions for figures, particularly Figure 1, to clarify which generator corresponds to each sub-figure. It would be beneficial to expand the experimental comparisons to include diffusion models and other relevant generative models, as well as to provide quantitative results supporting claims about performance differences. Furthermore, we advise enhancing the clarity of the presentation by minimizing assumptions about the readers' mathematical knowledge and providing more intuitive explanations of complex concepts. Lastly, addressing the minor corrections in the provided code and elaborating on the evaluation metrics would strengthen the paper.