ID: pWZ97hUQtQ
Title: Principled Weight Initialisation for Input-Convex Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 6, 3, 6, 7, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents a principled weight initialization strategy for Input-Convex Neural Networks (ICNNs) by generalizing signal propagation theory to include weights without zero mean. The authors derive an initialization strategy from this theory, demonstrating its effectiveness through empirical experiments, including applications in drug discovery.

### Strengths and Weaknesses
Strengths:  
1. The paper introduces a significant theoretical contribution by generalizing signal propagation theory for ICNNs.  
2. The derived initialization strategy enhances learning and generalization in ICNNs, supported by solid experimental results.

Weaknesses:  
1. The proposed method is not applicable to networks with skip connections, limiting its practical use.  
2. The experimental section lacks comprehensiveness, including insufficient ablation studies and unclear explanations of certain details, such as the definition and heuristic setting of $\rho_*$.  
3. The theoretical contribution is limited, primarily following the framework of previous work, and the authors do not address the backward propagation or output diversity, which are crucial for initialization.

### Suggestions for Improvement
We recommend that the authors improve the experimental design to provide more comprehensive support for their claims, including clearer explanations of the initialization's effects on training dynamics. Additionally, the authors should discuss the limitations of ICNN models compared to standard neural networks and explore the potential benefits of applying their initialization method to networks with skip connections. Clarifying the role of $\rho_*$ as a hyperparameter and its impact on training and generalization would also enhance the paper's depth. Finally, addressing the missing x-axis scale in Figure 1 and discussing the implications of activation concentration in deeper networks would strengthen the presentation.