ID: ap4x1kArGy
Title: ODRL: A Benchmark for Off-Dynamics Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 7, 6, 8, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ODRL, a benchmark for off-dynamics reinforcement learning (RL), focusing on the transfer of policies across domains with differing dynamics. The benchmark encompasses locomotion, navigation, and dexterous manipulation tasks, with variations in friction, gravity, kinematic constraints, and agent morphology. It supports four experimental settings (online-online, offline-online, online-offline, offline-offline) and includes implementations of various off-dynamics RL algorithms. The authors provide empirical evaluations revealing that no single method consistently excels, highlighting the complexity of off-dynamics RL. Enhancements to the ODRL framework include a new API for environment calls via `call_odrl_env.py`, allowing users to specify environment types, names, and shift levels directly. The authors also introduce `call_odrl_dataset` for accessing offline target domain datasets and acknowledge the need for more seeds in experiments to address statistical concerns. They have initiated a documentation site to improve clarity and accessibility of the framework and plan to support the Gymnasium API in the future, along with uploading new datasets for the humanoid robot.

### Strengths and Weaknesses
Strengths:
- ODRL's comprehensive nature and diverse task categories significantly contribute to the RL field, providing a versatile platform for evaluating algorithm adaptability.
- The empirical evaluation is thorough, offering insightful observations that guide future research directions.
- The clear definition of the off-dynamics RL setting establishes a solid theoretical foundation for the benchmark.
- The introduction of a direct API for environment calls simplifies user interaction.
- The authors are responsive to feedback, committing to incorporate suggestions and improve documentation.
- The provision of additional datasets enhances the utility of the framework.

Weaknesses:
- The benchmark requires a specific Python script for execution, limiting compatibility with other RL libraries. A separation of the benchmark from algorithm implementations is recommended.
- The paper lacks a thorough discussion of its limitations and applicability to real-world scenarios, particularly regarding the complexity of real-world dynamics.
- The experiments were conducted with only five random seeds, leading to high standard deviations that hinder significant conclusions.
- The current documentation is limited due to time constraints, which may hinder user understanding.
- The authors acknowledge the potential expense of increasing the number of seeds in experiments, which could impact the robustness of results.

### Suggestions for Improvement
We recommend that the authors improve the benchmark's accessibility by providing a more direct API and supporting established APIs (e.g., Gymnasium API) for easier integration with existing RL libraries. Additionally, a more explicit comparison with existing benchmarks should be included to highlight ODRL's unique features. The authors should also consider expanding the benchmark to include more real-world environments and discuss the relevance of the single-source/single-target setting in the context of large datasets. Furthermore, we suggest that the authors enhance the comprehensiveness of the documentation site by continuously updating it with clearer examples and more detailed explanations. We also encourage the authors to increase the number of seeds in their experiments to strengthen the statistical validity of their findings. Finally, providing a comparison of ODRL against other benchmarks in their revisions would enhance the paper's context and relevance.