ID: tUyW68cRqr
Title: Language Semantic Graph Guided Data-Efficient Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 28
Original Ratings: 6, 4, 6, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework that utilizes semantic information from labels to enhance the performance of deep neural networks in classification tasks. The authors propose constructing a Language Semantic Graph (LSG) using label embeddings generated by frozen language models, which are then used to train a Graph Convolutional Network (GCN). This framework is applicable across various modalities and incorporates two additional regularization objectives to leverage label semantics effectively. The authors also demonstrate that the GCN classifies nodes based on both initial node features and graph topology, confirming the importance of edge information through an ablation study. Experimental results indicate that the proposed framework significantly outperforms several baselines in transfer learning, semi-supervised learning, and data augmentation scenarios, while also showing that GCN accuracy is influenced by edge ratios and mitigates prompt cluster dominance.

### Strengths and Weaknesses
Strengths:
1. The paper is clearly written, facilitating reader comprehension of the proposed framework.
2. The novel approach of integrating semantic information from labels is widely applicable across different tasks and modalities.
3. Strong empirical evidence supports the necessity of edge information in GCN predictions, with comprehensive ablation studies enhancing the robustness of the findings.
4. Experimental results demonstrate substantial performance improvements over baseline models across various datasets.

Weaknesses:
1. The framework's applicability is limited to classification tasks with explicit and rich semantics; it may not perform well when classes lack clear semantic meanings.
2. The presentation quality varies, with some sections being unclear or poorly motivated, particularly regarding the GCN's role and the rationale behind the two-step approach.
3. Concerns remain regarding the closeness of GCN to a node-feature-readout network, particularly in light of Prototype Alignment's performance.
4. The GCN's training process may be perceived as primarily noise reduction rather than a substantial contribution to learning.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation, especially in sections discussing the GCN's training objectives and the role of multiple language prompts. Additionally, we suggest providing more examples of prompt templates used for generating embeddings and discussing the framework's applicability to binary classification tasks or those with fewer than ten categories. It would also be beneficial to include a quality analysis of the LSG, such as visualizations demonstrating its effectiveness in capturing label semantics. Furthermore, we recommend that the authors clarify the distinct contributions of GCN compared to Prototype Alignment, particularly addressing concerns about their performance proximity. Lastly, providing further insights into the GCN training process to emphasize its innovative aspects beyond noise reduction would enhance the paper's robustness.