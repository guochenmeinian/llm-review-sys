ID: zpVCITHknd
Title: Towards Personalized Federated Learning via Heterogeneous Model Reassembly
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 5, 6, 8, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to federated learning (FL) that utilizes a model reassembly technique to enable collaboration among clients with heterogeneous model architectures. The authors propose a method where the server stitches together parts of different deep neural networks (DNNs) to create personalized models for clients without requiring server-side data. The technique aims to mitigate the negative impact of public datasets on model performance and demonstrates effectiveness through experimental results.

### Strengths and Weaknesses
Strengths:
- The introduction of a reassembly technique for heterogeneous FL is a significant contribution that opens new avenues for model personalization.
- The paper is well-written, clearly articulating motivations and presenting convincing experimental results.
- The empirical analysis addressing the adverse effects of heterogeneous public data is a valuable aspect of the work.

Weaknesses:
- The motivation for stitching heterogeneous architectures is not well articulated, and comparisons with existing methods like FjORD and HeteroFL are lacking.
- The experimental setup primarily involves a small number of clients (N=12 to 100), which may not reflect real-world scenarios with millions of clients.
- The server's lack of historical aggregate information may hinder performance in large-scale cross-device settings.
- The requirement for the server to maintain client identities limits the applicability of privacy-preserving techniques.
- Computational overhead is a concern, as the server must train stitching layers for each client/candidate-model pair.
- The complexity of the proposed solution could benefit from an ablation study to assess the necessity of all steps in the pFedHR process.

### Suggestions for Improvement
We recommend that the authors improve the motivation for stitching heterogeneous architectures and provide a clearer comparison with existing methods. Additionally, conducting experiments with a larger number of clients would enhance the realism of the findings. The authors should acknowledge the limitations of their method in large-scale cross-device settings and consider strategies to reduce computational overhead. Clarifying the training process for stitching layers and addressing the complexity of the solution through an ablation study would also strengthen the paper. Lastly, including a readme file with the code would facilitate reproducibility.