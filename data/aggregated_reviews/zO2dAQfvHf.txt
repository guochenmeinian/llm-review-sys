ID: zO2dAQfvHf
Title: Stabilized Neural Differential Equations for Learning Dynamics with Explicit Constraints
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 3, 6, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to stabilize neural ordinary differential equations (NODEs) on Riemannian manifolds by introducing a new stabilization term derived from the pseudo-inverse of a Jacobian matrix. The authors demonstrate that this term enhances stability while allowing the model to learn dynamics constrained by conservation laws. They clarify the distinction between manifold optimization and their approach, emphasizing that their method constrains trajectories in state space rather than optimizing parameters in parameter space. The proposed method is theoretically and experimentally validated across various physical systems represented by ODEs, showing improved performance compared to vanilla NODEs and other neural ODE variants.

### Strengths and Weaknesses
Strengths:
- The proposed stabilization term is simple and theoretically sound, making it accessible even to those less familiar with the topic.
- The paper is well-written and organized, with clear notation and explanations.
- Experimental results indicate significant improvements in stability and performance across diverse dynamical systems.
- The method demonstrates effectiveness in stabilizing various neural ODE variants and offers flexibility and applicability to arbitrary coordinate systems.

Weaknesses:
- The paper lacks a comprehensive discussion of existing methods in Riemannian optimization and related fields, which could provide context for the proposed approach.
- The motivation for applying learning methods to the problem remains unclear, particularly regarding the necessity of known constraints in realistic scenarios.
- Comparisons are primarily made against vanilla NODEs, with insufficient empirical evaluation against other constrained neural ODE methods.
- The authors do not sufficiently compare their method to existing Riemannian optimization techniques, raising questions about its generality and applicability.
- The theoretical guarantees and experimental setups could be expanded, particularly regarding higher-dimensional systems and the applicability to partial differential equations (PDEs).

### Suggestions for Improvement
We recommend that the authors improve the literature review by incorporating discussions on relevant Riemannian optimization techniques and other methods that ensure constraints in neural networks. This would strengthen the context of their contribution. Additionally, we suggest including empirical comparisons with other constrained neural ODE methods to better position their work within the field. We encourage the authors to clarify the motivation behind their proposed solution, specifically addressing the necessity of learning methods for the problems discussed. Expanding the theoretical framework to include generalization error and conducting experiments on higher-dimensional systems would enhance the robustness of their findings. Finally, we recommend that the authors include careful explanations of the differences in problem setup from Hamiltonian neural networks (HNN) and Lagrangian neural networks (LNN), as well as to emphasize the advantages of their method's applicability to arbitrary coordinate systems in the final version.