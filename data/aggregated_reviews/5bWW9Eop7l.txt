ID: 5bWW9Eop7l
Title: The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 7, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a task to evaluate large language models (LLMs) on their ability to resolve conversational implicatures and presuppositions, utilizing a dataset of naturally occurring implicatures transformed into a binary classification task. The authors propose an evaluation protocol that benchmarks various state-of-the-art models, including BERT, RoBERTa, and GPT-4, under different methodologies such as fine-tuning, instruction tuning, and chain-of-thought prompting. Results indicate that while base models perform slightly above random, models like GPT-4 can reach human performance when appropriately tuned. The authors argue that their benchmark is more challenging than the BIG-bench, comprising 30% difficult additional examples that enhance the evaluation's fairness and informativeness. They emphasize that their work is the first to provide substantive insights into LLMs' performance on implicature tasks, countering claims that the BIG-bench results are sufficient for such assessments.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and provides comprehensive details for reproducibility.
- It introduces an original evaluation protocol for resolving implicatures and demonstrates that human performance can be achieved with state-of-the-art models.
- The evaluation is thorough, exploring various prompt variations and few-shot examples, yielding interesting insights into LLM performance.
- The addition of prompt sensitivity analysis demonstrates the soundness of the approach.
- The authors effectively address the learning of pragmatics by LLMs, which strengthens the paper's argument.

Weaknesses:
- The task is not novel, as it overlaps significantly with the dataset introduced in BIG-bench, which already demonstrated high performance with PaLM using k-shot prompting.
- The analysis lacks depth in discussing the implications of instruction tuning and the remaining performance gaps for particularized implicatures.
- The variance in model performance only accounts for prompt wording, and a bootstrap analysis on the test set would strengthen claims regarding model comparisons.
- There are concerns regarding the novelty of the work, particularly in relation to the BIG-bench results and the perceived impact of the additional examples included in the authors' benchmark.
- Some reviewers believe that the authors overstate the limitations of the BIG-bench, suggesting that it still provides valuable insights.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the novelty of their task in relation to existing work, particularly the BIG-bench dataset. It would be beneficial to include a more detailed analysis of instruction tuning's impact on performance and to explore the reasons behind the observed gaps in resolving particularized implicatures. Additionally, we suggest conducting a bootstrap analysis to better support claims about model performance comparisons. We also recommend that the authors improve the discussion of related works by incorporating the references provided, as they situate the current research within the broader context of implicature and presupposition modeling. Furthermore, we encourage the authors to clarify the distinction between their work and the BIG-bench results to address concerns about novelty and impact more effectively. Lastly, highlighting the insights regarding whether models are learning pragmatics more prominently in the main body of the paper, possibly expanding on this in the appendix, would enhance the overall clarity and impact of the paper.