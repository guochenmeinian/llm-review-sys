ID: aKnWIrDPiR
Title: Multimodal Clinical Benchmark for Emergency Care (MC-BEC): A Comprehensive Benchmark for Evaluating Foundation Models in Emergency Medicine
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 8, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Multimodal Clinical Benchmark for Emergency Care (MC-BEC), a dataset comprising 121,978 continuously monitored Emergency Department visits from 72,892 patients between 2020-2022. The dataset includes diverse modalities such as triage information, prior diagnoses, medications, vital signs, electrocardiogram and photoplethysmograph waveforms, and free-text reports. The authors propose three clinically relevant prediction tasks: decompensation, disposition, and ED revisit, evaluated using metrics like Area Under the Precision-Recall Curve (AUPRC). Additionally, the paper introduces an evaluation framework that includes the metrics of “Monotonicity in modalities” and “Robustness against missing modalities.” The former assesses whether adding more information during training can negatively impact model performance, while the latter addresses real-world scenarios where certain modalities may be absent during inference. The authors conducted experiments with Random Forest and XGBoost backbones, revealing performance differences, with LightGBM achieving the best results. They argue that their benchmark is better suited for generalist medical AI due to a more comprehensive dataset and expanded evaluation methods.

### Strengths and Weaknesses
Strengths:
- The dataset's size and comprehensive multimodal nature enhance its utility for emergency care research.
- Clear definitions of the predicted metrics are provided, facilitating understanding.
- The introduction of relevant metrics addresses critical research questions in multi-modal learning.
- The dataset includes raw ECG and PPG waveforms, enhancing its applicability to real clinical data.
- Expanded evaluation methods assess the impact of data modalities and robustness against missing modalities.
- Additional experiments with various backbones provide valuable insights into model performance.

Weaknesses:
- The evaluation primarily relies on AUPRC without prevalence information, limiting the interpretation of model performance.
- The reliance on AUPRC over AUC-ROC may limit the generalizability of findings due to its sensitivity to class distribution.
- The dataset lacks demographic information, which could introduce bias.
- The dataset's association with COVID-19, while minimal, may complicate its applicability for generic emergency care research.
- Important experimental details, such as hyperparameter tuning and training specifics, are insufficiently reported.
- Some ideas may be familiar to readers of existing literature, suggesting a need for clearer connections to prior work.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by providing prevalence information for all predictions or calculating the Area Under the Receiver Operating Curve (AUC) alongside AUPRC. Additionally, including demographic data and elaborating on hyperparameter tuning details would enhance the dataset's robustness. The authors should clarify the training process for the multimodal features and consider discussing additional fairness measures beyond the True Positive Rate difference. We also suggest providing a more detailed justification for the choice of evaluation metrics, particularly regarding the limitations of AUC-ROC and sensitivity/specificity in the context of class imbalance. Lastly, ensure that the dataset documentation is comprehensive and accessible in compliance with NeurIPS guidelines, and improve the clarity of how findings relate to existing literature.