ID: U7mWHBoTfb
Title: Improving Language Modelsâ€™ Meaning Understanding and Consistency by Learning Conceptual Roles from Dictionary
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a learning framework aimed at enhancing the meaning-understanding capability of pretrained language models (PLMs) by leveraging word-definition pairs from dictionaries. The authors propose a method for integrating learned parameters from multiple knowledge sources to improve performance and reduce inconsistency, demonstrating effectiveness on both English and Korean datasets. The main contributions include leveraging dictionary data for improved robustness, proposing parameter-efficient training methods, and extending applicability to new languages.

### Strengths and Weaknesses
Strengths:
- The proposed framework effectively enhances the meaning-understanding capability of PLMs.
- Experimental results show improved performance on consistency tasks, particularly on the BECEL dataset, without degrading general performance on GLUE.
- The method is generalizable and can be extended to other languages using readily available dictionary data.

Weaknesses:
- The study is limited to RoBERTa as the backbone model, raising questions about the generalizability of the method to other PLMs or larger language models.
- Missing baselines and insufficient experimental details hinder a complete understanding of the model's performance and reproducibility.
- Some conclusions drawn from the results lack sufficient backing, particularly regarding systematic improvements over GLUE.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the applicability of their method to other PLMs beyond RoBERTa, including potential experiments with models like BERT. Additionally, we suggest including more comprehensive baselines to provide a clearer performance comparison. Clarifying the experimental setup and providing detailed descriptions of parameter settings would enhance reproducibility. Finally, we encourage the authors to ensure that their conclusions are directly supported by the experimental findings, particularly in relation to the performance on GLUE.