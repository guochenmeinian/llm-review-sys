ID: Q2Wu2Cfp2x
Title: Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 6
Original Ratings: -1, -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the computational power of linear transformers (LTs) and their variants, focusing on their ability to recognize formal languages. The authors provide a theoretical analysis demonstrating that LTs inherit properties from standard transformers, and they empirically evaluate LTs and their extensions, such as delta-rule, recurrence, and self-reference, on various formal language recognition tasks.

### Strengths and Weaknesses
Strengths:
- The paper combines theoretical and empirical analyses, offering insights into the properties and limitations of linear transformers.
- It provides interesting results that suggest extensions can enhance the capabilities of LTs in recognizing specific languages.

Weaknesses:
- The theoretical contributions are seen as incremental, relying heavily on previously established proofs without sufficient novelty.
- The focus on a narrow set of formal language tasks may not adequately reflect real-world challenges or the broader capabilities of LTs.
- The paper lacks comparisons with other models, such as RNNs with external memory, and does not sufficiently analyze the computational complexity of the proposed extensions.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their theoretical contributions by providing more detailed discussions and analyses of the proofs in the Appendix to ensure the paper is self-contained. Additionally, the authors should broaden the scope of their experiments to include comparisons with other architectures that recognize formal languages. Clarifying the significance of their results, particularly regarding whether they stem from single-run experiments or hyperparameter searches, would enhance the paper's reproducibility and impact. Finally, addressing the clarity of task descriptions and ensuring consistent terminology throughout the paper would improve its overall presentation.