ID: k2hS5Rt1N0
Title: Off-Dynamics Reinforcement Learning via Domain Adaptation and Reward Augmented Imitation
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Domain Adaptation and Reward Augmented Imitation Learning (DARAIL) approach for off-dynamics reinforcement learning (RL). It aims to generate similar trajectories in the target domain as those learned via DARC in the source domain. The authors claim that DARAIL achieves state-of-the-art performance in benchmark off-dynamics environments, providing a theoretical error bound based on the dynamics shift scale. The paper also includes a comparative analysis of DARC and DARAIL in the context of varying gravity conditions in reinforcement learning environments. The authors propose that DARC's performance degrades significantly in gravity-changing scenarios, as evidenced by experiments where DARC achieves only 40% of source domain rewards in the target domain. In contrast, DARAIL effectively utilizes imitation learning to transfer trajectories without directly applying DARC, thus avoiding the assumption of similar dynamics between source and target domains. The paper discusses the varying performance of DARC across different tasks, noting that it underperforms in HalfCheetah but performs adequately in Reacher due to the latter's lesser sensitivity to gravity changes.

### Strengths and Weaknesses
Strengths:
- The paper effectively outlines the limitations of DARC and provides a clear motivation for the proposed method.
- The theoretical contributions are robust, and the experiments demonstrate that DARAIL can handle severe dynamic shifts.
- The authors provide code for reproducibility and conduct a thorough experimental evaluation.
- The paper provides clear qualitative reasoning for the performance differences between DARC and DARAIL in gravity-changing settings.
- Experimental results are presented, demonstrating the effectiveness of DARAIL over DARC in various scenarios.
- The authors address technical suggestions regarding the reward estimator and regularization parameters, indicating a willingness to refine their approach.

Weaknesses:
- The evaluation is restricted to a narrow type of dynamics change, raising concerns about the generalizability of the method.
- DARC's significant underperformance in specific environments like HalfCheetah raises concerns about its general applicability.
- The presentation quality is inconsistent, with several unclear notations and typographical errors.
- The paper lacks sufficient detail in certain sections, particularly regarding the reward estimator and its implications for performance.
- The assumption that source trajectories are optimal in the target domain is questionable and not sufficiently justified.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper's presentation, addressing typographical errors and ambiguous notations. Specifically, the authors should clarify the notation for trajectories and provide a more detailed discussion on why DARC fails to account for dynamic shifts. We also recommend improving the clarity of the explanation regarding the reward estimator \( R_{AE} \), particularly in Lines 151-166, to better articulate its role and potential biases. Additionally, we suggest including experiments on a broader range of off-dynamics scenarios beyond freezing the 0-index action to validate the general applicability of DARAIL. Conducting further experiments across a broader range of environments and dynamic shifts will strengthen their findings. Finally, ensuring precise language throughout the paper will enhance its overall presentation.