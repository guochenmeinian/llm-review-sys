ID: e2wtjx0Yqu
Title: CLadder: Assessing Causal Reasoning in Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 7, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new dataset, CLADDER, designed to evaluate the causal reasoning abilities of language models (LLMs) through 10,000 boolean questions that necessitate intensive causal reasoning. The dataset is constructed using a causal inference engine, sampling causal graphs and query types, and verbalizing these into natural language problems. The authors propose a prompting strategy, CausalCoT, which allows LLMs to explicitly state and solve causal problems, showing improved performance over baseline models. The evaluation indicates that causal reasoning remains challenging for LLMs in a zero-shot setting. Additionally, the authors utilize Pearl's framework for causal inference in NLP, emphasizing the importance of distinguishing correlation from causation. They acknowledge a performance drop in Rung 3 of their evaluation, attributing it to the complexity of query type distinctions, and express interest in exploring the integration of plugins to enhance LLM capabilities in causal reasoning.

### Strengths and Weaknesses
Strengths:
- The paper addresses an important problem in causal inference and provides a well-designed dataset with 10,000 examples.
- The data creation process is robust, covering multiple causal graphs and including anti-commonsense and nonsensical settings to mitigate memorization effects in LLMs.
- The introduction of the first formal causal inference dataset in NLP significantly contributes to the field.
- The authors provide a systematic method for generating causal inference questions, allowing for flexible dataset sizes.
- Additional error analysis is included, showcasing the LMs' capabilities in solving causal inference problems.
- The authors demonstrate awareness of the limitations of their approach and are open to further exploration of fine-tuning and plugin integration.
- The paper is well-written and easy to follow.

Weaknesses:
- The baselines tested in Table 2 require clarification and may be weak; it is unclear if they are implemented in a zero-shot setting.
- The evaluation of the proposed prompting technique lacks thoroughness.
- The language used in the dataset is somewhat synthetic due to the rule-based verbalization process, with limited evaluation of the language quality.
- Human performance metrics are absent, which would help validate the dataset's quality and the challenges posed by the templated verbalization.
- The dataset, while large, may not be effectively evaluated in practice, raising concerns about potential undiscovered shortcuts and the dataset's complexity for un-fine-tuned models.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the baselines in Table 2 and confirm whether they are evaluated in a zero-shot setting. Additionally, including results from few-shot prompting would provide a better understanding of LMs' capabilities. We suggest enhancing the dataset's language quality by evaluating the verbalization process more thoroughly. Furthermore, incorporating human performance metrics would be valuable for assessing the dataset's challenges. We also recommend improving the introduction to better motivate the problem being studied and to highlight the strengths of Pearl's framework while mentioning alternative approaches. Please discuss the performance drop on Rung 3 more explicitly, emphasizing its implications for using the technique on such problems. While detailed significance tests may not be necessary, providing a ballpark estimate of significant difference scores would be beneficial. Finally, we encourage the authors to conduct fine-tuning experiments and report preliminary results in the next version of the paper to address concerns regarding the dataset's evaluation and potential shortcuts.