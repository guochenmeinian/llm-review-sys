ID: XOpaPrb0U5
Title: QATCH: Benchmarking SQL-centric tasks with Table Representation Learning Models on Your Data
Conference: NeurIPS
Year: 2023
Number of Reviews: 23
Original Ratings: 7, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents QATCH, a benchmarking toolbox for evaluating Table Representation Learning (TRL) models on proprietary datasets. The authors validate the performance of three TRL models on question-answering (QA) tasks and one on semantic parsing (SP) across eight Kaggle datasets. They assert that existing benchmarks do not adequately capture the nuances of custom tables and propose new metrics beyond execution accuracy to assess model performance. The authors also evaluate ChatGPT and T5 models in SQL-centric tasks, proposing to extend their evaluation by incorporating datasets not available on the Web and considering open models like Llama 2. They emphasize the importance of benchmarking tools alongside new datasets, aligning their work with the NEURIPS'23 call for contributions.

### Strengths and Weaknesses
Strengths:
- The QATCH toolbox offers a novel approach for automatic generation and evaluation of test cases tailored to proprietary data.
- The paper introduces a benchmarking tool specifically for TRL models on proprietary data, filling a gap in the current literature.
- The authors demonstrate a commitment to improving clarity and presentation based on reviewer feedback, including plans to enhance metric discussions and results presentation.
- The correctness of SQL outputs is assured through rigorous construction and validation methods.
- Additional experimental results for T5 models provide valuable insights into performance metrics.

Weaknesses:
- The novelty of the work is somewhat limited as it primarily focuses on validating TRL models without substantial dataset creation.
- The evaluation is restricted to only four TRL models, and the rationale for model selection is not well articulated, raising questions about the generalizability of the findings.
- The results for QA tasks are not convincing, as none of the models perform well, raising questions about the methodology.
- There is a lack of thorough comparison with existing models and benchmarks, which diminishes the robustness of the claims made regarding proprietary data challenges.
- The paper does not adequately address the implications of ChatGPT's performance, which contradicts the assertion that proprietary datasets are inherently more challenging than SPIDER.

### Suggestions for Improvement
We recommend that the authors improve the novelty of the paper by contributing more significantly to dataset creation. Additionally, clarify how the QATCH-Generate tool ensures the correctness of generated SQL declarations and ground truth. It would be beneficial to provide a detailed discussion of the strengths and weaknesses of the cross-task performance metrics used, including their applicability to specific tasks. We suggest expanding the evaluation to include a broader range of TRL models and tasks beyond QA and SP, as well as addressing the utility of tuple cardinality and order metrics more explicitly. Furthermore, we encourage the authors to investigate and discuss the anomalous performance of ChatGPT in relation to proprietary data versus SPIDER, as this could significantly impact the perceived validity of the proposed benchmark. Lastly, consider changing the project code license to enable commercial use, and explore the possibility of producing a larger benchmark dataset to enhance the tool's impact.