ID: vO6ZdPWaHc
Title: Data Pruning via Moving-one-Sample-out
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 3, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new method called moving-on-sample-out (MoSo) aimed at removing less informative samples from training data based on the change in optimal empirical risk. Due to the computational challenges of exact calculations, the authors propose an estimator utilizing gradient information. MoSo demonstrates empirical success in data pruning, generalization across networks, and robustness to label noise. The framework also includes a first-order approximation algorithm for efficient computation. Furthermore, the paper provides a thorough examination of the topic, showcasing a clear understanding of the subject matter and proposing several innovative approaches that contribute to the existing body of knowledge.

### Strengths and Weaknesses
Strengths:
- The method is based on a simple yet effective idea.
- The proposed estimator effectively addresses the computational complexity of MoSo.
- A wide range of comparison methods are used as baselines in the experimental section.
- An ablation study details the necessity of sub-elements of the method.
- MoSo outperforms baseline methods in terms of accuracy across various tasks.
- The manuscript is well-structured and articulates complex ideas effectively.
- The authors' engagement with the review process is commendable, indicating a willingness to refine their work based on feedback.

Weaknesses:
- The justification for the gradient-based estimator relies solely on intuition; a more rigorous justification is needed.
- The introduction suggests that data pruning can reduce training time, but the experimental section only compares accuracy, lacking insights on training time.
- The term "awareness" regarding training dynamics is unclear, as is its distinction from other gradient-based methods.
- The conclusion of Proposition 1.2 is not discussed in the main text, leaving the accuracy of the estimator unaddressed.
- The computational cost of the method is not adequately analyzed, particularly regarding the surrogate model training.
- There are areas where the manuscript could benefit from further clarification and detail, particularly in the methodology section, which may leave readers with questions about the implementation of the proposed approaches.

### Suggestions for Improvement
We recommend that the authors improve the justification for the gradient-based estimator with a more rigorous analysis. Additionally, we suggest including comparisons of MoSo with baseline methods in terms of training time to substantiate claims of efficiency. Clarifying the meaning of "awareness" in relation to training dynamics and differentiating it from other methods would enhance understanding. We also encourage the authors to discuss the implications of Proposition 1.2 in the main text and provide experimental validation of the estimator's accuracy. Finally, a thorough analysis of the computational costs associated with the surrogate model training should be included to provide a clearer picture of the method's efficiency. Furthermore, enhancing the clarity of the methodology section will ensure that readers fully understand the implementation of the proposed approaches and addressing any remaining concerns raised during the review process would enhance the overall quality of the manuscript.