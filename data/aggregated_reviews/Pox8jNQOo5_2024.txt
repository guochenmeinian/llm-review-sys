ID: Pox8jNQOo5
Title: Second-order forward-mode optimization of recurrent neural networks for neuroscience
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 8, 8, 5, 8, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a second-order optimization algorithm called SOFO for training recurrent neural networks (RNNs) in neuroscience applications. SOFO utilizes a forward-only approach that computes a Gaussian-Newton curvature matrix in a random parameter subspace, allowing for memory-efficient training without backpropagation. The authors demonstrate that SOFO outperforms the Adam optimizer across various neuroscience-related tasks, including learning the trajectory of a Lorentz attractor, adaptive Kalman filtering, one-shot classification, and motor tasks. The results indicate that SOFO is faster and more memory-efficient than Adam, particularly in handling long temporal sequences.

### Strengths and Weaknesses
Strengths:
- SOFO significantly outperforms Adam in all tested tasks, maintaining a constant memory footprint due to its forward-only nature.
- The paper is well-structured, with clear mathematical notations and high-quality figures.
- The experimental setup is diverse and relevant to neuroscience, showcasing the algorithm's effectiveness.

Weaknesses:
- The models used are small, primarily limited to two-layer architectures, which may not reflect the complexities of larger networks.
- Important baselines, such as FORCE, are missing, and the paper lacks a thorough benchmarking against RTRL for more complex tasks.
- The contribution to neuroscience is questioned, as the biological plausibility of the proposed method is not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the benchmarking of SOFO against RTRL, particularly in terms of memory profiling and performance on larger networks. Additionally, please include comparisons with the FORCE algorithm to establish SOFO's advantages more clearly. It would be beneficial to provide architectural details of the MNIST experiments in the main text rather than the appendix. We also suggest clarifying the runtime complexity of SOFO in relation to the number of neurons and addressing the biological implications of using vanilla RNNs for neuroscience applications. Lastly, consider revising the language to remove terms that may detract from the paper's academic tone, such as "embarrassingly" and "anxiety."