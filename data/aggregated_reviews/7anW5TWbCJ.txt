ID: 7anW5TWbCJ
Title: Information Theoretic Lower Bounds for Information Theoretic Upper Bounds
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a lower bound on the mutual information between output weights and input training data within stochastic convex optimization, examining the tightness of mutual information-based generalization bounds. The authors demonstrate that mutual information increases with the parameter dimension, indicating that existing information-theoretic bounds inadequately capture the generalization capabilities of SGD and regularized ERM, which exhibit dimension-independent sample complexity. Additionally, the paper explores the application of Lemma 1 in the context of Theorem 1, emphasizing the need to consider \( f \) as a function of both \( z_i(t) \) and \( z_i(t') \) for \( t' \neq t \). The authors propose improvements based on reviewer feedback, including a discussion on the Gibbs algorithm and the implications of individual sample results, while acknowledging the potential for further analysis regarding the lower-bound results and their relevance to generalization.

### Strengths and Weaknesses
Strengths:  
The consideration of lower bounds is crucial for understanding the limits of information-theoretic analysis in characterizing learning algorithm generalization. The originality of the approach, particularly the use of discrete algorithms and the fingerprinting lemma, is noteworthy. The paper effectively highlights significant limitations of mutual information-based bounds, contributing valuable insights to the learning theory community. Furthermore, the authors provide a thorough response to reviewer concerns, enhancing the clarity of the submission, and the incorporation of discussions on the Gibbs algorithm and individual sample results is a positive step towards a more comprehensive understanding of the topic.

Weaknesses:  
The paper lacks clarity in several areas, particularly in the proof of Theorem 1, where the rationale for its applicability to any learning algorithm is unclear. The proof requires further refinement, especially in the application of Lemma 1. Additionally, the authors inadequately position related literature, particularly regarding the bounds discussed in Haghifam et al. (2022) and Bassiliy et al. (2018). The current version lacks a concluding section that discusses the interpretation of lower-bound results and future research directions. The presentation suffers from formatting issues and overstated claims, which may confuse readers.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the proof for Theorem 1 by explicitly detailing that \( f \) is a function of both \( z_i(t) \) and \( z_i(t') \) for \( t' \neq t \), ensuring that other variables are fixed before applying Lemma 1. It would be beneficial to accurately position the related literature, ensuring that the distinctions between different bounds are clearly articulated. We suggest revising overstated claims to reflect the nuanced nature of the results, such as rephrasing the statement regarding dimension-dependent information requirements. Additionally, we encourage the authors to add a concluding section that interprets the lower-bound results and outlines potential future directions for research. Finally, addressing formatting inconsistencies in citations and enhancing the overall readability of the paper would significantly improve its presentation.