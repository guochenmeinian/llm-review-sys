ID: VhbV56AJNt
Title: Variance-Reduced Gradient Estimation via Noise-Reuse in Online Evolution Strategies
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 4, 5, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a generalization of Persistent Evolution Strategies (PES) through the introduction of Generalized Persistent Evolution Strategies (GPES), which utilizes noise-reuse to create a class of unbiased online evolution strategies. The authors analytically characterize the variance of these estimators, identifying Noise-Reuse Evolution Strategies (NRES) as the lowest-variance estimator. Experimental results demonstrate that NRES achieves faster convergence than existing automatic differentiation (AD) and evolution strategy (ES) methods across various applications, including dynamical systems and reinforcement learning. The authors emphasize the importance of measuring performance based on the number of sequential environment steps, proposing that NRES outperforms other ES methods under practical constraints on the number of steps. They also discuss the characterization of the loss function as "chaotic" and agree to clarify its usage in the context of their findings.

### Strengths and Weaknesses
Strengths:
- The paper proposes a novel framework, GPES, which generalizes existing PES algorithms and provides a theoretical analysis of unbiasedness and variance.
- The identification of NRES as a low-variance estimator is a significant contribution, supported by rigorous mathematical proof.
- The authors provide a clear rationale for their performance measure, highlighting its significance in evaluating different approaches.
- The proposed method, NRES, is shown to outperform other ES methods regardless of the constraints on the number of steps.
- The writing quality is high, and the experiments show promising results, indicating improvements over previous methods.
- The authors are responsive to reviewer feedback, indicating a willingness to clarify and improve their manuscript.

Weaknesses:
- The novelty of GPES is limited, as it is a straightforward generalization of PES.
- Experimental setups lack detail, particularly in the reinforcement learning section, and comparisons to non-ES methods are insufficient.
- The definition of "sequential environment steps" may lead to misunderstandings regarding the performance of global random search compared to iterative methods.
- The characterization of the loss function as chaotic may require further clarification to avoid misinterpretation.
- The paper's clarity suffers in certain areas, such as the definition of "chaotic loss surfaces" and the implications of using "sequential environment steps" for performance measurement.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the manuscript by providing a precise definition of "chaotic loss surfaces" and evidence supporting their claims regarding the Lorenz system. Additionally, the authors should enhance the experimental section by including comparisons on longer sequences and higher-dimensional problems, as well as providing intuitive explanations for why GPES outperforms PES. We suggest that the authors explicitly state the constraint on the number of environment steps per update earlier in the manuscript, ensuring that it is clear how this affects the comparison of methods. The removal of "additional heuristic tricks" in the reinforcement learning section could help focus on the core mechanisms of the proposed methods. Finally, we encourage the authors to reinforce their argument that NRES consistently outperforms other methods, regardless of the number of steps allowed per update, by incorporating the relevant discussion in their revision and detailing hyperparameter settings in the experiments for better reproducibility.