ID: jK6e4DIYah
Title: Beyond Dataset Watermarking: Model-Level Copyright Protection for Code Summarization Models
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ModMark, a novel model-level watermarking method aimed at protecting the copyright of Code Summarization Models (CSMs) used in programming and software development. ModMark addresses the limitations of traditional dataset-level watermarking methods by embedding watermarks directly into the models, enhancing cross-language generalization and concealment against automated detection. The authors conduct extensive experiments across multiple programming languages, achieving a 100% watermark verification rate, which significantly outperforms existing methods.

### Strengths and Weaknesses
Strengths:
1. ModMark advances watermarking techniques by embedding watermarks at the model level, improving robustness against extraction and detection.
2. The experiments cover various programming languages, demonstrating ModMark's effectiveness and achieving a 100% verification rate without significant performance degradation.

Weaknesses:
1. The methodology section lacks clarity, particularly regarding the inspiration from TFLexAttack and the unique mapping mechanism of the tokenizer, which is only described textually. 
2. Minor formatting and expression issues exist, such as missing punctuation after equations and the need for dimensional information for symbols in formulas.
3. The complexity of trigger feature construction remains a challenge, particularly for users lacking expertise in tokenizer manipulation.
4. The paper does not sufficiently motivate the critical need for obfuscation in CSMs compared to other models, which may limit its relevance to the broader research community.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology section by providing an introduction to TFLexAttack and including equations or diagrams to describe the unique mapping mechanism of the tokenizer. Additionally, addressing minor formatting and expression issues will enhance readability. To further validate the technique, we suggest testing ModMark on recent code Large Language Models (LLMs) and providing a stronger motivation for the necessity of obfuscation in CSMs compared to other models.