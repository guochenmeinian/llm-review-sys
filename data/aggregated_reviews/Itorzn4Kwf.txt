ID: Itorzn4Kwf
Title: Accelerating Exploration with Unlabeled Prior Data
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 6, 6, 5, 6, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method that leverages unlabeled offline data to enhance online exploration in reinforcement learning. The authors propose training a reward model and random network distillation models on online data to create a UCB estimate for offline data. This approach encourages the agent to explore novel states deemed to have high rewards, contrasting with traditional methods that merely expand the exploration frontier. The method is validated across various domains, demonstrating improved exploration efficiency compared to baselines.

### Strengths and Weaknesses
Strengths:
- The method is simple and effective, facilitating online learning through unlabeled offline datasets.
- Experimental results are substantial, showing the superiority of UCB rewards over pure online exploration and other methods.
- The paper is well-structured and easy to follow, with strong coverage results and helpful baseline descriptions.

Weaknesses:
- The core assumption that offline data must have good coverage for effective learning is somewhat obscured, raising concerns about the phrasing in section 4.5 regarding exploration and reward achievement.
- The analysis of how offline data distribution affects training is shallow, lacking depth in addressing the relationship between optimism and pessimism in offline data treatment.
- The paper does not adequately explore the implications of using orthogonal offline data, which could hinder exploration.
- The number of seeds used in experiments is limited, and the representation learning section feels detached from the main topic.

### Suggestions for Improvement
We recommend that the authors improve the analysis of how offline data distribution impacts training, possibly including an ablation experiment to assess performance with limited offline data coverage. Additionally, we suggest clarifying the relationship between optimism and pessimism in the treatment of offline data. It would be beneficial to conduct experiments or provide arguments regarding the generalizability of the solution across different offline RL methods. Lastly, we encourage the authors to enhance the discussion on uncertainty modeling, including its accuracy and the optimal balance between the uncertainty bonus and the learned reward signal.