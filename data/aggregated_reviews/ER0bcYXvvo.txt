ID: ER0bcYXvvo
Title: Byzantine-Tolerant Methods for Distributed Variational Inequalities
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 6, 7, 5, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents robust optimization methods for solving distributed variational inequalities (VIs) and provides theoretical guarantees for various algorithms addressing min-max problems. The authors propose robust versions of SGDA and SEG, demonstrating convergence under large batch sizes. They also introduce momentum variants of these algorithms, enhancing convergence performance, and conduct experiments on quadratic games and robust neural network training to validate their theoretical findings.

### Strengths and Weaknesses
Strengths:
- The authors deliver theoretical guarantees for algorithms addressing VIs, particularly in min-max optimization contexts.
- The introduction of momentum versions leads to improved convergence performance, with the ability to reduce the neighborhood size with larger batch sizes.
- The paper includes a theoretical analysis of random checks of computations in a homogeneous setting and evaluates the proposed algorithms effectively.

Weaknesses:
- A significant limitation is the reliance on large batch sizes for convergence; it is unclear if this requirement can be mitigated.
- The paper's dense writing style complicates accessibility, as detailed algorithm descriptions are lacking in the main text.
- The results focus on iterate convergence, yet the authors claim implications for exact convergence in non-convex cases without sufficient clarification.
- Some discussions, particularly regarding non-convex function convergence, are missing from relevant sections.

### Suggestions for Improvement
We recommend that the authors improve the paper's accessibility by including detailed algorithm descriptions in the main text. Additionally, addressing the feasibility of circumventing the large batch size requirement would strengthen the paper. Clarifying the implications of iterate convergence for non-convex cases is essential, as is ensuring that all relevant discussions are included in their respective sections. Furthermore, we suggest providing more examples of variational inequalities to enhance the motivation for the study.