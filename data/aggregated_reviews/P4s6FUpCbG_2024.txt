ID: P4s6FUpCbG
Title: 3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel pipeline, 3DGS-Enhancer, aimed at enhancing the quality of 3D Gaussian splatting (3DGS) representations, particularly in sparse input view scenarios. The authors propose leveraging a video diffusion model to reformulate 3D-consistent image restoration tasks, generating high-quality and 3D-consistent images. They introduce a spatial-temporal decoder and a fine-tuning strategy to mitigate artifacts caused by temporal inconsistency. Extensive experiments demonstrate superior reconstruction performance across various datasets.

### Strengths and Weaknesses
Strengths:
* The paper is well-motivated, addressing the enhancement of low-quality 3DGS rendering results for distant camera viewpoints.
* The reformulation of the 3D-consistent image restoration task as temporally consistent video generation is innovative.
* Impressive results have been achieved and tested on diverse scenes, with applicability to existing 3DGS models.
* A new dataset for evaluating artifacts in 3DGS is introduced, which is beneficial for the community.
* The writing quality is clear and easy to follow.

Weaknesses:
* The related work section is thin and could benefit from more descriptive paragraphs.
* The method relies on adjacent views for continuous interpolation, which may limit its applicability.
* The choice of the spatial-temporal decoder (STD) requires quantitative ablation, including individual components like color correction.
* Training details and the specifics of the video diffusion model and fine-tuning process are insufficiently described.

### Suggestions for Improvement
We recommend that the authors improve the related work section to provide a more comprehensive overview. Additionally, please clarify the adversarial loss described in L179 for the STD and specify the data used for fine-tuning the Temporal Denoising U-Net. We suggest providing more comparisons on ablating the temporal consistency and addressing runtime discussions for scene processing. Finally, Figure 2 could be made more interpretable by ensuring a 1:1 parity between the input and output views.