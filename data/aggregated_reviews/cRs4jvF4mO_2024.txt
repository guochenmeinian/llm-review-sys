ID: cRs4jvF4mO
Title: Deep Discriminative to Kernel Density Graph for In- and Out-of-distribution Calibrated Inference
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents new methods, Kernel Density Forest (KDF) and Kernel Density Network (KDN), aimed at improving confidence calibration for traditional deep learning models and random forests, particularly addressing the high confidence exhibited by ReLU-based networks on out-of-distribution (OOD) data. The authors propose partitioning the feature space into polytopes and substituting affine functions with Gaussian kernels, enhancing calibration for both in-distribution (ID) and OOD data. Experimental results indicate that these methods outperform existing calibration techniques.

### Strengths and Weaknesses
Strengths:
- The originality of replacing affine functions with Gaussian kernels within polytopes is notable, providing a novel integrated solution for ID and OOD calibration.
- The theoretical proofs are robust, and the methods are validated through simulations and real-world datasets.
- The paper is clearly and concisely written.

Weaknesses:
- The validity of metrics is questionable; the use of Maximum Calibration Error (MCE) over Expected Calibration Error (ECE) or Adaptive Calibration Error (ACE) is not justified, and a comparison of these metrics would enhance credibility. The definition of Out-of-distribution Calibration Error (OCE) also lacks justification.
- Insufficient discussion on the research context and related work makes it difficult to evaluate the contribution. The introduction does not adequately connect OOD detection and OOD confidence calibration.
- The computational complexity of the method raises concerns about its applicability to larger networks, and the paper lacks detailed experimental results to support claims about computational limitations.

### Suggestions for Improvement
We recommend that the authors improve the justification for using MCE over ECE and ACE, including a detailed comparison of these metrics. Additionally, a clearer connection between OOD detection and OOD confidence calibration should be established in the introduction and related work sections. To address computational concerns, the authors should provide a comparison of execution times and clarify the noise represented in Table 1. Including experiments on larger and more varied datasets, as well as evaluating the methods' performance with in-training calibration techniques, would also strengthen the paper. Lastly, we suggest restructuring the presentation of toy simulations to enhance readability and flow.