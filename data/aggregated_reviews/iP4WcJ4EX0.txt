ID: iP4WcJ4EX0
Title: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: 5, 3, 3, 4

Aggregated Review:
### Key Points
This paper presents a novel method for enhancing training efficiency through sparsity-friendly replacements in models, supported by solid theoretical foundations and empirical evidence. The authors conduct experiments across various CNN and Transformer architectures, accompanied by an extensive ablation study that yields cohesive conclusions. The method explores the lottery-ticket hypothesis and demonstrates significant accuracy improvements on the ResNet family and lower perplexity on the IFT GPT-3-small model.

### Strengths and Weaknesses
Strengths:
- The writing is clear and easy to follow.
- High-quality, self-explanatory visual elements enhance understanding.
- Extensive details are provided for reproducing results.
- The paper includes a strong evaluation with detailed ablation studies.

Weaknesses:
- The reasons for latency and training time increases, even on hardware supporting unstructured sparsity, are unclear, particularly regarding the implications of FLOPs remaining constant.
- The absence of programming code complicates result reproduction, despite comprehensive descriptions of training pipelines and hardware setups.
- The text contains imperfections, including a high volume of pages and unclear table highlights.
- There are typos and inaccuracies regarding FLOPs estimations, which may mislead readers about hardware requirements.
- The technique shows weaker performance in NLP tasks compared to vision benchmarks, and comparisons to state-of-the-art distillation/pruning methods are lacking.

### Suggestions for Improvement
We recommend that the authors clarify the reasons for latency and training time increases on hardware supporting unstructured sparsity. Additionally, we suggest making the programming code publicly available to facilitate result reproduction. The authors should also address text imperfections, including reducing the volume of the references list and clarifying table highlights. Furthermore, we encourage the authors to emphasize the hardware requirements for FLOPs estimations earlier in the manuscript and to include comparisons with state-of-the-art distillation/pruning methods to strengthen their evaluation. Lastly, we recommend exploring the reasons for the observed discrepancies in performance between vision and NLP tasks.