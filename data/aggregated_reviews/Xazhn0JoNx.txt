ID: Xazhn0JoNx
Title: Making Scalable Meta Learning Practical
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 6, 5, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach called SAMA (Scalable Meta Learning with Arbitrary Optimizers) to tackle scalability issues in meta-learning. The authors combine advancements in implicit differentiation algorithms and systems to develop SAMA, which supports arbitrary optimizers while minimizing computational burden through efficient distributed training techniques. The experimental results demonstrate significant improvements in throughput and memory consumption across various tasks, particularly in data optimization and large language models.

### Strengths and Weaknesses
Strengths:
1. The paper introduces SAMA, a unique method that effectively addresses scalability challenges in meta-learning.
2. Compelling experimental results validate SAMA's performance, showing substantial throughput increases and reduced memory consumption compared to baseline algorithms.
3. The manuscript is well-written and easy to follow.

Weaknesses:
1. The application scenarios for SAMA are not sufficiently clarified, particularly regarding its effectiveness beyond data optimization tasks.
2. The methodology lacks clarity on how \(\theta^*\) is obtained and whether it requires additional training time, which could affect the evaluation.
3. There are several typos and errors, including incorrect subscripts and missing negative signs in equations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of application scenarios for SAMA, explicitly addressing its applicability to various meta-learning tasks beyond data optimization. Additionally, we suggest providing a more detailed explanation of how \(\theta^*\) is derived and its implications on training time. The authors should also correct the identified typos and errors to enhance the manuscript's overall quality. Furthermore, conducting ablation studies and comparisons with established meta-learning algorithms, such as MAML and iMAML, would strengthen the experimental section and provide deeper insights into SAMA's advantages.