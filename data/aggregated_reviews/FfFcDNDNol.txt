ID: FfFcDNDNol
Title: When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents RLbreaker, a black-box jailbreaking framework utilizing Reinforcement Learning (RL) to optimize jailbreaking prompts. The RLbreaker agent selects mutators from a limited set, enhancing prompts that are then evaluated against responses from both a victim LLM and an unaligned LLM to calculate rewards for subsequent training steps. The experiments encompass general and transfer attacks, as well as evaluations against jailbreaking defenses.

### Strengths and Weaknesses
Strengths:
- The framework reduces the search space compared to popular baselines like GCG and AutoDAN, resulting in faster training speeds.
- The paper is well-structured, making the framework easy to understand.
- The evaluation section includes reasonable assessments through general, transfer, and ablation studies, demonstrating the agent's effectiveness despite a small action base.

Weaknesses:
- Experiments are limited to a 300-sample test set with the same distribution as the training set, raising concerns about robustness and effectiveness.
- The organization of the paper could be improved for readability, particularly in discussing deep reinforcement learning and related work.
- Some technical details, such as the specific mutators used, are not clearly specified.
- The evaluation metrics, particularly GPT-Judge, show inconsistencies and raise questions about their reliability.

### Suggestions for Improvement
We recommend that the authors improve the robustness of their experiments by using a more diverse test set. Additionally, clarifying the selection process for the action space and providing more details on the mutators used would enhance the paper's clarity. Addressing the inconsistencies in evaluation metrics, particularly between GPT-Judge and other measures, is crucial. We also suggest including stronger attack and defense baselines in future evaluations to strengthen the contributions of the work.