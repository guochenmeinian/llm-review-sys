ID: MfiK69Ga6p
Title: Protein Design with Guided Discrete Diffusion
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 5, 5, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to multi-objective protein sequence design using discrete diffusion models guided by protein properties. The authors propose a method, NOS, which applies classifier guidance to the latent representation of protein sequences, addressing the challenges posed by the discrete nature of these sequences. Empirical evaluations demonstrate improved performance in real-world sequence design tasks, particularly in antibody lead optimization, which includes both in-silico and in-vitro validation. Furthermore, the paper provides a comprehensive comparison between the DiGress guidance mechanism and the NOS method, highlighting that while DiGress improves objective function values, NOS outperforms it in generating samples with higher objective values and better likelihoods. The authors detail the sampling algorithm for DiGress, which utilizes one-hot encodings and a perturbation distribution from a learned discriminative model, and explain how the parameter $\lambda$ influences the sampling distribution, contrasting it with NOS's approach of performing local updates to hidden states.

### Strengths and Weaknesses
Strengths:
- The paper is well written and easy to follow, with well-motivated technical choices.
- Empirical evaluations, especially the antibody lead optimization experiment, effectively validate the proposed method.
- The introduction clearly highlights the significance of protein sequence design.
- The paper provides a thorough experimental design and detailed hyperparameter selection, demonstrating the superiority of the proposed method.
- The inclusion of comprehensive metrics and comparisons across multiple models allows for a robust evaluation of performance.

Weaknesses:
- The technical contributions are largely incremental, with the guidance applied to latent representations rather than directly to discrete sequences, diminishing novelty.
- The experimental evaluation is limited, lacking comparisons with several relevant baselines such as DiffAb and RFDiffusion, and only focusing on a single attribute.
- The introduction lacks clarity and fails to establish a strong motivation for the research.
- Some novel methodological aspects were initially placed in the appendix due to submission constraints, potentially limiting their visibility and impact.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the introduction to better establish the significance of their work. Additionally, the authors should include comparisons with other relevant baselines, particularly DiGress, to strengthen the evaluation of their method. Expanding the experimental evaluation to include multi-objective optimization and addressing the limitations of the work in the revised version would also enhance the paper. Furthermore, providing detailed information on the computational cost and time required for training and inference, as well as clarifying the model architecture and training procedure, will improve reproducibility. We also suggest refining the presentation of technical details in Section 4.2 to highlight the contributions more effectively. Lastly, we recommend that the authors emphasize the novel aspects of their method that are not specific to guided discrete diffusion, such as automatic edit position selection via feature attribution and partial deep ensembles for uncertainty quantification, in the main text of the camera-ready version. Including density plots in the final submission would also provide a more comprehensive view of the results beyond what is captured in the tables.