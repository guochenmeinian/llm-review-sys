ID: Ks0RSFNxPO
Title: Time-Independent Information-Theoretic Generalization Bounds for SGLD
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents novel information-theoretic generalization bounds for Stochastic Gradient Langevin Dynamics (SGLD) under smooth and dissipative loss conditions. The authors derive time-independent bounds that decay to zero as the sample size increases, independent of the number of iterations and fixed step size. The approach combines information-theoretic bounds with the Fokker–Planck equation, leading to improved excess risk bounds without relying on surrogate losses. Additionally, the paper analyzes the SGLD algorithm, highlighting that the parameters obtained do not converge to local minima under a fixed temperature parameter ($\beta$) but rather to a stationary distribution known as the Gibbs posterior. The authors propose that while SGLD does not achieve convergence to minima, it effectively explores parameter space, particularly in non-convex problems, allowing for the evaluation of expected loss relative to the stationary distribution. The paper also discusses the challenges of removing dependence on parameter dimension ($d$) when analyzing excess risk and suggests exploring alternative approaches, such as conditional mutual information (CMI), to evaluate generalization performance.

### Strengths and Weaknesses
Strengths:  
1. The paper provides innovative generalization bounds for SGLD using Kullback–Leibler divergence, establishing the first information-theoretic generalization error and excess risk bounds without surrogate losses.  
2. The combination of information-theoretic analysis and the Fokker–Planck equation results in tighter bounds, which is commendable.  
3. The paper offers a thorough examination of SGLD's properties, particularly its ability to explore parameter space globally.  
4. It introduces significant insights into the relationship between gradient variance and generalization error, contributing to the understanding of algorithm-dependent generalization performance.

Weaknesses:  
1. The claim of providing the first information-theoretic generalization error bound is questionable, as the authors utilize dissipativity to address non-convexity in their proofs.  
2. The definition of dissipativity appears incorrect, which undermines the clarity and professionalism expected in a technical document.  
3. The paper lacks clarity in explaining how SGLD operates on non-differentiable loss functions, particularly in Section 4, leading to confusion about the relationship between true and surrogate losses.  
4. The dependence on parameter dimension ($d$) in the analysis of excess risk remains a challenge that the authors acknowledge but do not fully resolve.  
5. The approach's reliance on mutual information may complicate the integration of algorithm-specific statistics, such as gradient variance, into generalization evaluations.  
6. Additional analysis is needed in Theorem 4 to clarify the uniformity of the bound concerning time, particularly regarding the term $V_{\nabla}$.

### Suggestions for Improvement
We recommend that the authors improve the definition of dissipativity to ensure it is presented correctly and clearly, as this is a central concept. Additionally, we suggest that the authors clarify how SGLD can be applied to non-differentiable loss functions and provide a relationship between true and surrogate losses. To enhance the rigor of Theorem 4, we advise including a lemma or additional analysis to establish an upper bound on $V_{\nabla}$ that is uniform in $T$. Furthermore, we recommend that the authors improve the theoretical framework by exploring alternative methods to evaluate generalization that do not rely on mutual information between parameters and data. Specifically, investigating the use of conditional mutual information (CMI) involving super-samples could provide valuable insights. Lastly, we suggest that the authors clarify how the bounds introduced in Part (iii) can be integrated into the main body of the paper, ensuring that discussions on convergence properties and dimension dependence are adequately addressed in a new subsection and appendix.