ID: wWyumwEYV8
Title: A Sober Look at the Robustness of CLIPs to Spurious Features
Conference: NeurIPS
Year: 2024
Number of Reviews: 26
Original Ratings: 6, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CounterAnimal, an evaluation dataset designed to categorize animals into two groups based on their backgrounds: common and counter. The authors benchmark various CLIP and ImageNet models on this dataset, observing that CLIP models exhibit a greater accuracy drop compared to ImageNet models. The performance drop from the common to counter groups reflects the reliance on spurious background features for animal predictions. The authors emphasize that curating out-of-distribution (OOD) test data according to different models reveals varying spurious features, highlighting the necessity of a benchmark like CounterAnimal specifically for CLIP models. The revised focus of the work is on studying the robustness of CLIP models rather than directly comparing them to ImageNet models.

### Strengths and Weaknesses
Strengths:
- The paper addresses the important issue of spurious correlations in visual language models (VLMs) and introduces the CounterAnimal dataset, which is a valuable contribution for assessing the robustness of CLIP models against these correlations.
- The dataset construction is well-documented, with clear statistics and methodologies outlined, particularly in Section 2.
- The analysis of spurious correlations is comprehensive, providing insights across various dimensions, including pre-trained datasets and learning paradigms.
- The authors have made substantial revisions to the abstract and introduction to align with the new focus on robustness against spurious features, and additional experiments, including background frequency investigations, enhance the understanding of model performance.

Weaknesses:
- The methodology is flawed, as the dataset is constructed using CLIP accuracy to define common and counter groups, leading to inherent bias.
- The related work section is weak and misses important prior studies that could provide context and depth to the findings.
- The claim regarding CLIP's reliance on spurious features is perceived as somewhat obvious, as it highlights a known correlation between object captions and background features.
- The reasons for ImageNet models' lesser susceptibility to spurious bias remain unclear and warrant further discussion.
- The use of absolute performance drop as a metric may not adequately reflect model performance, as it does not account for the relative differences in initial performance levels.
- There is a lack of clarity regarding the relationship between background types and model performance, which needs to be addressed.

### Suggestions for Improvement
We recommend that the authors improve the related work section by citing and discussing relevant studies, such as “Recognition in Terra Incognita” and the waterbirds dataset, to clarify the novelty of their dataset. Additionally, we suggest clarifying the naming convention of the common and counter groups, as it currently misrepresents the underlying methodology. The authors should also address the conflation of "effective robustness" and "robustness" to avoid confusion. Furthermore, we encourage the authors to consider a model-agnostic approach for dataset construction to yield more meaningful comparisons between CLIP and ImageNet models. We recommend improving the discussion surrounding the reasons why ImageNet models are less influenced by spurious biases, possibly addressing whether this is due to the training set composition. Additionally, we suggest incorporating a discussion on the robustness of the absolute performance drop metric and its implications for model evaluation. Including results from other spurious correlation benchmarks, such as WaterBirds, would also enhance the analysis. Finally, we advise a thorough proofreading of the manuscript to enhance clarity and precision in writing, and we recommend conducting further analyses to clarify the relationship between background types and model performance, particularly regarding why certain backgrounds are easier or harder for CLIP models.