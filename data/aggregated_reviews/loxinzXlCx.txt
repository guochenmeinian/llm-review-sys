ID: loxinzXlCx
Title: A Computation and Communication Efficient Method for Distributed Nonconvex Problems in the Partial Participation Setting
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 6, 5, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to distributed optimization and federated learning, integrating variance reduction, compressed communication, and partial participation. The authors propose the DASHA-PP algorithm, extending the DASHA method to accommodate partial participation in both finite-sum and stochastic settings. The proposed method achieves state-of-the-art communication complexity and optimal oracle complexity, effectively blending variance reduction and partial participation without requiring all nodes' participation or bounded gradient assumptions.

### Strengths and Weaknesses
Strengths:  
- The integration of variance reduction, partial participation, and compressed communication into a single method is a significant contribution, addressing the multifaceted nature of distributed optimization.  
- The theoretical results provide optimal oracle complexity, indicating high efficiency in data processing.  
- The extension of the DASHA method to the partial participation scenario is a nontrivial addition, establishing state-of-the-art bounds.

Weaknesses:  
- The presentation lacks clarity and organization, particularly in how assumptions and results are presented across different settings. This could hinder reader comprehension.  
- There are minor typographical errors that need correction, such as the inconsistent representation of the set $\mathbb{R}^d$ and wording issues in the text.  
- The algorithm's presentation lacks motivation and clarity, particularly regarding the derivation of step (9) and its relation to steps (7) and (8).  
- The empirical validation is limited, with a lack of numerical comparisons to other methods, which restricts the practical applicability of the findings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by organizing the assumptions and results for each setting individually, with a particular focus on the stochastic setting. This would enhance reader understanding and allow for a more comprehensive discussion of both nonconvex and PL condition cases. 

Additionally, we suggest correcting the identified minor typographical errors and providing a clearer explanation of the algorithm, particularly for step (9). 

To strengthen the empirical validation, we recommend including numerical comparisons with other methods, such as FRECON, and expanding the scope of experiments to cover nonconvex neural network scenarios. This could be achieved by dedicating the main body to the stochastic setting and relegating the finite-sum case to the appendix, thus creating space for these comparisons.