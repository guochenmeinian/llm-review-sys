ID: QUYLbzwtTV
Title: Bias in Motion: Theoretical Insights into the Dynamics of Bias in SGD Training
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 7, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 2, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of bias dynamics in machine learning models during the training process, specifically through a high-dimensional teacher-student framework called the Teacher-Mixture (TM) model. The authors provide an analytical solution for the dynamics of key order parameters that characterize classifier performance and bias in the high-dimensional limit. They identify a three-phase learning process where bias evolves non-monotonically, influenced by sub-population characteristics at different training stages. The findings are validated through numerical experiments on synthetic and real datasets (CIFAR10, MNIST, CelebA), emphasizing the implications for fairness and robustness in machine learning.

### Strengths and Weaknesses
Strengths:  
- The paper offers a theoretical foundation for understanding the time-dependent nature of bias in machine learning models, which has not been well captured by existing studies focused on asymptotic analysis.  
- The manuscript is well-structured, with a clear logical flow and strong theoretical quality, supported by well-conducted experiments.  
- The originality of studying bias in the transient learning regime is notable, providing insights into ephemeral biases characterized by multiple timescales.

Weaknesses:  
- The practical significance of the findings remains unclear, and the connection between theoretical predictions and real-world applications needs to be more explicitly stated.  
- The analysis is limited to linear models, which may not accurately reflect the dynamics of modern over-parameterized deep learning architectures.  
- There is a lack of exploration regarding the impact of hyperparameters on the observed phenomena, and the theory's applicability to common optimizers like Adam is uncertain.

### Suggestions for Improvement
We recommend that the authors improve the connection between their theoretical findings and practical applications by explicitly discussing potential bias-mitigating mechanisms that could arise from their observations. Additionally, exploring the effects of hyperparameters on the dynamics and extending the analysis to over-parameterized models would enhance the relevance of the work. It would also be beneficial to clarify how the value of $m$ affects the insights discussed in Section 4 and to provide examples of linear models where crossing phenomena do not occur to build intuition.