ID: vq4BnrPyPb
Title: Knowledge is a Region in Weight Space for Fine-tuned Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical exploration of the relationship between fine-tuned language models in weight space, revealing that models fine-tuned on the same dataset cluster together, while those fine-tuned on similar tasks form looser clusters. The authors propose a method for selecting models based on their position in the weight space, aiming to enhance performance by starting from the centroid of these clusters. The study includes experiments across 36 datasets, providing visualizations and analyses of the weight space and loss landscape.

### Strengths and Weaknesses
Strengths:
1. The paper investigates an important problem regarding the relationships between fine-tuned models in weight spaces, offering novel insights into knowledge transfer.
2. Extensive experiments and detailed visualizations support the findings, demonstrating the existence of clustered regions in the weight space and their impact on model performance.
3. The proposed method for efficient fine-tuning based on cluster centroids is a practical contribution to the field.

Weaknesses:
1. The baseline results in Table 2 are notably poor compared to existing methods, raising questions about the effectiveness of the proposed approach.
2. The motivation for using the centroid of fine-tuned models as the starting point for efficient fine-tuning is insufficiently explained.
3. The main message regarding the exploitation of weight space clustering lacks clarity, and the paper could benefit from a deeper theoretical exploration of the observed phenomena.

### Suggestions for Improvement
We recommend that the authors improve the motivation for using the centroid of fine-tuned models as the initial point for efficient fine-tuning, providing a clearer rationale for this choice. Additionally, addressing the poor baseline results in Table 2 is crucial; a comparison with existing methods like BitFit would strengthen the argument. Furthermore, we suggest that the authors clarify how their findings can be practically applied to enhance model performance and provide a list of useful fine-tuning practices based on their observations. Lastly, a more thorough theoretical discussion on the existence of weight space clusters and their implications for knowledge transfer would enrich the paper.