ID: RQlbMrA5XL
Title: NovoBench: Benchmarking Deep Learning-based \emph{De Novo} Sequencing Methods in Proteomics
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents NovoBench, a benchmark for de novo peptide sequencing that integrates existing datasets and implements popular machine learning methods for predicting peptides from mass spectrometry data. The authors propose a unified evaluation framework that includes diverse mass spectrum data, integrated models, and comprehensive metrics, addressing the lack of consensus in evaluation datasets and the limitations of current precision and recall metrics. The work also explores the impact of various factors, such as peptide length and noise peaks, on model performance.

### Strengths and Weaknesses
Strengths:  
- The authors have made significant contributions by creating a comprehensive benchmark that improves upon prior datasets, providing clear differentiation and new metrics.  
- The manuscript is well-written, with clear methods and analyses that are timely and relevant.  
- The code repository is well-documented and accessible, facilitating reproducibility.  

Weaknesses:  
- The limitations section is insufficiently detailed, lacking a thorough discussion of the weaknesses related to feature lists, models, and data quality.  
- There is a need for more examples and discussion on how different applications might benefit from the various models presented.  
- The novelty of the work is somewhat limited, as it primarily builds upon existing benchmarks without introducing fundamentally new concepts.

### Suggestions for Improvement
We recommend that the authors improve the limitations section by providing a more comprehensive discussion of scientific limitations, including the impact of feature lists and data quality. Additionally, we suggest enhancing the discussion on how theorists and experimentalists are addressing issues related to model performance degradation. It would also be beneficial to include a greater range of post-translational modifications (PTMs) and to conduct cross-validation using different held-out test sets to better evaluate model generalization. Finally, we encourage the authors to build the automated end-to-end pipeline mentioned in the conclusion to increase the impact of their work.