ID: osox1GoFLS
Title: Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on spatial role extraction and spatial relation extraction, proposing three models: a pipeline of extraction and symbolic reasoning, an end-to-end pre-trained language model (PLM) in a question-answer format, and an end-to-end neural model with explicit layers of extraction and reasoning. The authors aim to disentangle the processes of information extraction and reasoning, demonstrating the effectiveness of their models across multiple datasets.

### Strengths and Weaknesses
Strengths:
- The paper addresses an interesting problem regarding the disentanglement of extraction and reasoning processes in spatial question answering.
- The proposed models show good performance on various datasets, indicating their potential effectiveness.
- The experimental section is comprehensive, providing extensive empirical results.

Weaknesses:
- The novelty of the proposed models is questionable, as pipeline and QA-style models are already prevalent in the information extraction field.
- The paper lacks a clear organization, making it challenging to follow, and does not adequately compare the proposed models with state-of-the-art (SOTA) frameworks.
- The motivation behind the proposed approach is unclear, particularly regarding the advantages of disentangling extraction and reasoning.
- The results section lacks insightful discussions and relies heavily on reporting outcomes without deeper analysis.

### Suggestions for Improvement
We recommend that the authors improve the clarity and organization of the paper by providing a brief introduction to all three models early on and ensuring that model names are memorable and clearly defined. Additionally, we suggest including a running example to enhance comprehension. The authors should compare their models against SOTA frameworks and clarify the logic rules used in the reasoning component. To facilitate understanding, we advise creating a table that outlines the attributes of each model alongside the baselines. Furthermore, the results section should be enriched with discussions that provide insights into the implications of the findings. Lastly, we encourage the authors to explore combining the strengths of their proposed models into a unified approach that performs well across different datasets.