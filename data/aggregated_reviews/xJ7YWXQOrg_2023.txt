ID: xJ7YWXQOrg
Title: Mathematical Capabilities of ChatGPT
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 5, 7, 7, 5, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
The paper evaluates the mathematical capabilities of ChatGPT and GPT-4 using the GHOSTS dataset, which consists of 709 prompts across six subdatasets, covering a range of mathematical skills from basic arithmetic to advanced concepts. Key findings indicate that while GPT-4 outperforms ChatGPT, the latter shows potential as a mathematical search engine but lacks consistency in producing high-quality solutions. The authors emphasize the need for further evaluation and community contributions to enhance the dataset, acknowledging that ChatGPT is not yet suitable for rigorous exam-level work. They clarify the concept of reproducibility, noting the expected variability in human-generated datasets and the non-deterministic nature of ChatGPT's outputs.

### Strengths and Weaknesses
**Strengths:**
1. The GHOSTS dataset provides a comprehensive benchmark for evaluating the mathematical capabilities of language models.
2. The methodology is rigorous, with clear rating criteria and a detailed taxonomy of error codes.
3. The authors offer nuanced insights into the performance of ChatGPT and GPT-4 across various mathematical domains and difficulty levels.
4. The dataset is meticulously designed and curated by mathematicians, establishing a robust benchmark for advanced mathematical comprehension.
5. The authors have made significant updates to the paper, including clarifications and additional explanations in response to reviewer feedback.

**Weaknesses:**
1. The evaluation is primarily based on the miniGHOST dataset, limiting generalizability; a broader dataset inclusion is necessary.
2. The dataset's size and coverage are still limited, covering only a subset of undergraduate and beginning graduate-level mathematics.
3. The reliance on human experts for evaluation introduces variability and lacks a 'ground-truth' for problems, raising concerns about reproducibility and consistency of ratings.
4. The analysis lacks insights into the underlying reasons for model performance, which could be addressed through further probing.
5. Comparisons to human performance, particularly against professional mathematicians, are insufficient.

### Suggestions for Improvement
1. The authors should expand the GHOSTS dataset to include a wider variety of mathematical topics and increase the number of prompts to strengthen the evaluation.
2. Incorporating few-shot performance and chain-of-thought approaches alongside the zero-shot prompt would provide a more comprehensive assessment of LLMs and improve practical applicability.
3. The authors are encouraged to clarify the dataset construction process, including the rationale for selecting specific chapters and the criteria for human ratings, to enhance transparency.
4. Strengthening logical reasoning training and incorporating uncertainty awareness in responses would enhance the reliability of ChatGPT's outputs.
5. The authors should consider evaluating additional models, especially newer releases, to provide a clearer picture of progress and explore the feasibility of incorporating automatic evaluation capabilities in future work.