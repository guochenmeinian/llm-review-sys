ID: Vp8WwRMWfv
Title: Recurrent Neural Language Models as Probabilistic Finite-state Automata
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a thorough investigation of the expressive power of Heaviside-activated Elman recurrent neural networks (RNNs) as language models, establishing that they define the same set of probability distributions over strings as deterministic probabilistic finite-state automata (dPFSA). The authors analyze the space complexity of simulating dPFSAs with Heaviside-activated Elman RNNs, proving linear lower bounds in both the number of states and the alphabet size. The study confirms that a Heaviside-activated Elman RNN requires at least \(\Omega (|\Sigma| |Q|)\) parameters to capture the same probability distribution as a dPFSA, aligning with Minskyâ€™s original construction.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized, clearly written, and provides valuable insights into the capabilities of RNNs.
- It effectively frames the study through weighted formal language theory, enhancing the precision of statements about RNN capabilities.
- The formal statements and proofs are mostly convincing and easy to follow, showcasing thorough research and historical context.

Weaknesses:
- The reliance on strong simplifying assumptions regarding the architecture and activations of Heaviside-activated Elman RNNs may trivialize some results, as these assumptions differ from practical language models.
- Extensive material is relegated to the appendices, which might be more suitable for a journal paper rather than a conference submission.

### Suggestions for Improvement
We recommend that the authors improve the acknowledgment of contributions from recent studies related to RNNs and formal language theory to provide a more comprehensive literature review. Additionally, we suggest that the authors clarify the term "tight" in the context of probability distributions and consider addressing the computational implications of expressing certain grammars with finite-state automata. Lastly, we advise removing Figure 1, as it does not significantly contribute to the paper's understanding, and promoting Figure 2 to the first page for better visibility.