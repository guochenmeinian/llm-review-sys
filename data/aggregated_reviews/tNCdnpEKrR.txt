ID: tNCdnpEKrR
Title: QGym: Scalable Simulation and Benchmarking of Queuing Network Controllers
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 8, 7, 7, -1, -1
Original Confidences: 4, 3, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents QGym, an open-source simulation framework and benchmark suite for queuing network control problems. It offers a flexible, event-driven environment capable of modeling various queuing systems, including non-stationary MDPs and time-varying arrival/service processes. The authors propose a comprehensive set of benchmark instances, integrating traditional queuing control policies and reinforcement learning (RL) approaches, particularly variants of Proximal Policy Optimization (PPO). Extensive empirical evaluations demonstrate that RL methods can outperform traditional policies in many scenarios, although they struggle with larger, complex networks.

### Strengths and Weaknesses
Strengths:  
The work has immense potential to accelerate and standardize research in queuing network control by providing a flexible simulation environment and a diverse set of benchmark problems. The framework's ability to model non-stationary and time-varying processes enhances its relevance, allowing for realistic evaluations of control policies. The paper is generally well-written and clearly structured, with a thorough experimental evaluation.

Weaknesses:  
The focus on a single RL algorithm (PPO) limits the exploration of a broader range of RL methods, which could yield more comprehensive insights. The scalability of the approach to very large networks remains unclear, as RL methods struggle with the largest examples, suggesting limitations for real-world deployment.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the MDP formulation by mapping it to typical notions such as state space, action space, transition function, and reward function. Additionally, the authors should specify rewards and temporal discounts, as the environment behaves as a semi-MDP. It would be beneficial to denote action $a^{(k)}$ as $a(t_k)$ for consistency and clarify the domain of actions. We also suggest including a detailed tutorial on using the package, highlighting configurations similar to existing gym environments. Finally, exploring a wider range of RL algorithms could provide more insights into the performance of queuing control policies.