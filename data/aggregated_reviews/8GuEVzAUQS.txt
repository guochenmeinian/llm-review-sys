ID: 8GuEVzAUQS
Title: Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 22
Original Ratings: 3, 7, 7, 5, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for learning world models from in-the-wild videos, introducing Contextualized World Models (ContextWM) that utilize a context encoder to separate contextual information from temporal dynamics. The authors enhance an existing framework for pre-training world models by incorporating a cross-attention mechanism and dual reward predictors to improve task-relevant representation learning. Additionally, the paper introduces a contextualized image decoder utilizing cross-attention mechanisms that resemble transformer layers for conditioning contextual information. The authors propose that a dedicated transformer-based architecture could enhance their approach, particularly in terms of pre-training on video by merely changing conditioning variables. Experiments on various benchmarks, including Meta-world and DMC Remastered, demonstrate that ContextWM outperforms DreamerV2 in several tasks.

### Strengths and Weaknesses
Strengths:  
- The motivation for disentangling context and dynamics is clear and reasonable.  
- The proposed method is simple and straightforward, with significant performance improvements noted in experiments.  
- The manuscript is well-written, structured, and includes relevant ablation studies to illustrate the role of different components.  
- The authors effectively addressed reviewer concerns regarding performance and technical contributions, leading to increased confidence in the methodology.  
- Clarifications regarding the 'IPV w/ vanilla WM' baseline have resolved prior misunderstandings.

Weaknesses:  
- The framework assumes context information is uniformly distributed across frames, neglecting potential occlusions of moving objects.  
- The performance on the Meta-world benchmark compared to the vanilla WM baseline is not particularly impressive.  
- There is a lack of comprehensive ablation studies across multiple tasks, and the paper does not adequately discuss limitations or failure cases.  
- The comparison between transformer-based architectures requires deeper investigation, as all such architectures could be considered "contextualized."  
- The current methodology may benefit from a more explicit discussion on the incorporation of pre-training on video.

### Suggestions for Improvement
We recommend that the authors improve the handling of occluded objects by incorporating specific designs to address this issue. Additionally, we suggest conducting a more comprehensive ablation study across various tasks to validate the robustness of the proposed method. The authors should also consider comparing their method against APV and DreamerV3 to provide clearer insights into the advantages of ContextWM. Furthermore, we recommend that the authors improve the depth of the comparison between transformer-based architectures in their revised paper. Lastly, we suggest that the authors explore and discuss the integration of pre-training on video more thoroughly, emphasizing the flexibility of transformer architectures in this context. A dedicated section discussing limitations and potential failure cases would also enhance the paper's depth.