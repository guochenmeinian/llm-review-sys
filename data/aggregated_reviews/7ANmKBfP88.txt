ID: 7ANmKBfP88
Title: Right this way: Can VLMs Guide Us to See More to Answer Questions?
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for estimating whether a Visual Language Model (VLM) can improve its ability to answer visual questions with better-quality images. The authors introduce a new labeled dataset for Visual Question Answering (VQA) that focuses on how to take better pictures and propose a fine-tuning scheme using synthetically generated examples. Additionally, the paper explores a recourse for unanswerable visual questions by introducing the Directional Guidance VQA task and a corresponding dataset, along with a clever data generation pipeline. Furthermore, the authors evaluate and expand the self-reflection capabilities of multiple Multi-modal Large Language Models (MLLMs) through a hierarchical cognitive process pattern theory and a manually annotated dataset.

### Strengths and Weaknesses
Strengths:
- The problem addressed is interesting, relevant, and has potential real-world impact.
- The novel dataset is a significant contribution and the performance improvement after fine-tuning on LLaVA is impressive.
- The framing of the Directional Guidance VQA task is simple and reasonable, and the data generation pipeline is innovative.

Weaknesses:
- The model's performance with different prompting techniques beyond vanilla CoT remains unexplored.
- The focus on qualitative spatial reasoning raises questions about its applicability to quantitative reasoning.
- The perturbation ranges of 0.1-0.9 and 0.3-0.7 are confusing and could benefit from evaluation over more disparate ranges.
- The narrow scope of the task and dataset may be better suited for a computer vision conference.
- The evaluation dataset for the MLLM self-reflection method is relatively small, which diminishes the robustness of the results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the perturbation ranges by evaluating over more disparate ranges to better illustrate performance drops. Additionally, we suggest exploring the model's performance with various prompting techniques beyond vanilla CoT to enhance the robustness of the findings. It would also be beneficial to validate the necessity and sufficiency of the proposed hierarchical cognitive process pattern and to clarify how it aligns with the evaluation benchmark. Finally, we encourage the authors to enhance the organization and presentation of the results section to facilitate clearer conclusions.