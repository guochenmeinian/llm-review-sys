ID: TqDt58rmpE
Title: Effective Backdoor Mitigation Depends on the Pre-training Objective
Conference: NeurIPS
Year: 2023
Number of Reviews: 3
Original Ratings: 8, 6, 7
Original Confidences: 4, 3, 4

Aggregated Review:
### Key Points
This paper presents an investigation into the effectiveness of the CleanCLIP backdoor mitigation technique, emphasizing its reliance on pre-training objectives. The authors propose that backdoored CLIP models exhibit resistance when pre-trained with multimodal contrastive loss (MMCL) and self-supervised loss (SSL). Experiments conducted on the CC3M and CC6M datasets reveal that CleanCLIP fails to mitigate backdoor attacks under these conditions, highlighting a significant challenge in safeguarding multi-modal models.

### Strengths and Weaknesses
Strengths:
- The paper identifies a critical weakness in CleanCLIP, demonstrating its ineffectiveness against backdoor attacks when utilizing a combination of MMCL and SSL.
- The experimental results are convincing and supported by comprehensive hyperparameter searches.
- The problem addressed is significant, contributing to the understanding of backdoor vulnerabilities in multi-modal models.

Weaknesses:
- The findings are limited to specific conditions, suggesting a need for investigation in more diverse settings.
- A more theoretical analysis of why CleanCLIP fails under the specified training objectives would enhance the paper's contribution.
- The readability of figures could be improved, as the font size in Figures 1, 2, and 3 may hinder data interpretation.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis to provide insights into CleanCLIP's failures against backdoor attacks with MMCL and SSL. Additionally, exploring the effectiveness of their findings in a broader range of conditions would strengthen the paper. Finally, enhancing the readability of figures by adjusting the font size would facilitate better data interpretation.