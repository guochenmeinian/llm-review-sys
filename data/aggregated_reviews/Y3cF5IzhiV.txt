ID: Y3cF5IzhiV
Title: TurtleBench: A Visual Programming Benchmark in Turtle Geometry
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 6, 7, 7
Original Confidences: 3, 4, 3

Aggregated Review:
### Key Points
This paper presents TurtleBench, a benchmark aimed at evaluating large multimodal models (LMMs) on their ability to interpret geometric patterns through visual examples and textual instructions. The authors propose two types of tasks: `scratch`, which involves *de novo* code generation, and `tweak`, which requires code editing. The evaluation of various models, including GPT-4V and Gemini, reveals generally poor performance, highlighting a significant gap between human intuitive geometrical reasoning and current AI capabilities.

### Strengths and Weaknesses
Strengths:
1. TurtleBench is a valuable contribution to the field, providing a structured approach to assess code generation and editing through scratch and tweak tasks.
2. The detailed comparisons of various multimodal language models (mLLMs) strengthen the conclusions regarding the performance gap in intuitive and visual geometrical understanding.

Weaknesses:
1. The paper lacks clarity on the number of tasks evaluated in Section 3, particularly for the `tweak` tasks.
2. The distinction between `tweak` and `edit` tasks is not well-defined, leading to potential confusion.
3. The visual instructions in Section 4.2 are not clearly described, leaving ambiguity about the nature of the visual depictions used.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the task counts in Section 3, specifically providing the number of `tweak` tasks evaluated. Additionally, we suggest that the authors either standardize the terminology by choosing a single descriptor for the `tweak` tasks or provide a clear distinction between `tweak` and `edit` tasks. Furthermore, we encourage the authors to clarify the visual instructions in Section 4.2 to specify whether a visual depiction of geometric constructions or an image of textual descriptions was generated. Lastly, we recommend that the authors explore potential directions for visual programming beyond LLMs and include comparisons with other geometric reasoning benchmarks in the field of computer vision.