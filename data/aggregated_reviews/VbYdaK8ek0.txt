ID: VbYdaK8ek0
Title: Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 3, 5, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel parameter-efficient fine-tuning method called Adapter Re-Composing (ARC), which focuses on reusing parameters across different layers of pre-trained models. The authors propose a shared adapter with learned re-scaling coefficients to maintain diversity among layers. Extensive experiments demonstrate ARC's effectiveness across 24 image classification tasks, achieving competitive performance while utilizing fewer parameters.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, organized, and easy to follow, with clear figures illustrating core ideas.
- The motivation for the weight-sharing design is compelling, particularly the observation of low-rank characteristics in learned adaptation matrices.
- Comprehensive experiments cover various datasets and attention-based architectures, providing robust evidence of ARC's effectiveness.

Weaknesses:
- The motivation for further reducing adapter parameters is weak, as existing adapters are already lightweight, making the marginal gains less significant.
- The novelty of the low-rank constraint and parameter sharing is limited, as these concepts are not new and have been previously explored in methods like LoRA.
- The performance of ARC does not consistently outperform previous methods, and additional metrics such as FLOPs would enhance the evaluation.
- The paper lacks a thorough discussion on the numerical differences among re-scaling coefficients and the potential benefits of using advanced data augmentation techniques during training.

### Suggestions for Improvement
We recommend that the authors improve the discussion of the novelty and significance of their contributions, particularly regarding the low-rank constraint and parameter sharing. Clarifying the theoretical justification for the proposed method's effectiveness, especially in Section 3.3, would strengthen the paper. Additionally, providing a comparison of GPU memory usage between ARC and other methods, as well as exploring the impact of independent downsampling and upsampling matrices, could enhance the analysis. Finally, including performance results with advanced data augmentations would provide a more comprehensive evaluation of ARC's capabilities.