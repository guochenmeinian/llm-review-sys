ID: ZgtLQQR1K7
Title: VMamba: Visual State Space Model
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 8, 7, -1, -1, -1
Original Confidences: 5, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents VMamba, a novel vision backbone model inspired by the Mamba state-space sequence model, designed for efficient visual representation learning with linear computational complexity. The core innovation is the VSS block, which integrates the 2D-Selective-Scan module (SS2D) to adapt the Mamba model from 1D NLP tasks to 2D image data. VMamba's architecture features multiple stages with hierarchical representations and offers three model sizes: Tiny, Small, and Base. Extensive experiments demonstrate VMamba's superior performance in various visual perception tasks, including image classification, object detection, and semantic segmentation, showcasing its scalability and adaptability.

### Strengths and Weaknesses
Strengths:
- The introduction of the 2D-Selective-Scan (SS2D) module effectively bridges the gap between 1D selective scanning and 2D vision data.
- Comprehensive experimental results across multiple benchmarks validate the effectiveness and robustness of VMamba.
- The paper is well-written, providing clear explanations and detailed descriptions of the architecture and experimental setups.
- Visualizations, such as activation maps and effective receptive fields, enhance understanding of the SS2D mechanism and model behavior.
- VMamba significantly reduces computational complexity from quadratic to linear, impacting the field of visual representation learning.

Weaknesses:
- Limited comparison with other state-space models (SSM) in the vision domain; more in-depth analysis with models like S4ND and Vim would strengthen the argument for VMamba's superiority.
- The Related Work section could benefit from the inclusion of additional relevant studies on neuromorphic vision and processing with SSMs.
- Uncertainty regarding VMamba's generalization to other visual tasks, such as video analysis or 3D vision; preliminary results or discussions on these aspects would enhance its versatility.
- Some mathematical derivations, particularly regarding the relationship between SS2D and self-attention, are complex and could be simplified for better accessibility.

### Suggestions for Improvement
We recommend that the authors improve the comparison with other state-space models (SSMs) used in the vision domain, such as S4ND and Vim, by providing detailed analyses of accuracy, computational efficiency, and memory usage. Additionally, we suggest expanding the Related Work section to include significant studies on neuromorphic vision and processing with SSMs. To address concerns about generalization, we encourage the authors to evaluate VMamba on tasks beyond standard benchmarks, such as video analysis or 3D vision, and discuss these findings. Lastly, we advise simplifying the mathematical derivations or providing more intuitive visual insights to enhance understanding for a broader audience.