ID: VSBBOEUcmD
Title: LLM-enhanced Self-training for Cross-domain Constituency Parsing
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to enhance constituency parsing through open-domain transfer by integrating self-training techniques with large language models (LLMs). The authors propose generating target domain examples using an LLM based on grammar rules and raw examples, iteratively training the parser over four iterations. They introduce new methods for self-training data selection based on token distribution and grammar rule similarity, alongside a confidence-based method. Results indicate competitive performance, but not superior, which undermines the overall claims. The integration with open-source LLMs like Falcon-40B is also demonstrated.

### Strengths and Weaknesses
Strengths:
- The proposed method is simple, effective, and creatively combines self-training with LLM capabilities.
- The paper provides detailed discussions and analyses, particularly in Figures 4 and 5, which highlight the method's strengths over baselines.
- The approach is interesting and shows some positive results in cross-domain parsing.

Weaknesses:
- The assumption that source and target samples share similar grammar rules limits applicability to domains with different grammar structures.
- The comparison between training sample sizes in iterations is unfair, as different amounts of new samples were added, affecting results.
- The experimental setup lacks robustness, with small improvements not statistically validated, and reliance on a single fixed amount of target text raises questions about generalizability.

### Suggestions for Improvement
We recommend that the authors improve the experimental setup by evaluating the impact of varying amounts of available target text on performance, as this could clarify the method's effectiveness in low-resource scenarios. Additionally, conducting statistical significance analyses on the results would strengthen the claims made. We suggest clarifying the details of grammar rule extraction and the hyperparameters used in LLM prompts, as well as ensuring consistent sample sizes across iterations to provide a fair comparison. Finally, we advise revising sections that appear redundant and enhancing the writing for clearer implementation details.