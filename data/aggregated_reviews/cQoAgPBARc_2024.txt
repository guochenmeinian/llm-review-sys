ID: cQoAgPBARc
Title: Improving Deep Reinforcement Learning by Reducing the Chain Effect of Value and Policy Churn
Conference: NeurIPS
Year: 2024
Number of Reviews: 28
Original Ratings: 6, 6, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the churn phenomenon in deep reinforcement learning (DRL), characterized by unexpected changes in network outputs for data not included in the training batch. The authors propose a simple regularization method, CHAIN, to mitigate value and policy churn, demonstrating its effectiveness across various DRL settings. The work explores the interplay between value churn reduction (VCR) and policy churn reduction (PCR), providing empirical results that indicate improved performance in both online and offline scenarios. Additionally, the authors introduce a method for automatic adjustment of the regularization coefficient, which significantly influences learning outcomes and aims to maintain a consistent relative loss scale across tasks. They clarify that CHAIN does not inherently promote exploration but rather mitigates action correlation.

### Strengths and Weaknesses
Strengths:
- The paper is generally well-written and easy to follow, with a clear motivation for addressing churn in DRL.
- The proposed CHAIN algorithm is simple, effective, and validated across multiple RL settings.
- Empirical evidence supports the claim that CHAIN reduces churn and enhances learning performance.
- The method for automatic adjustment of the regularization coefficient effectively addresses the need for manual tuning across various tasks.
- Clear distinctions between the roles of VCR and PCR in different learning settings enhance understanding of their impacts on performance.

Weaknesses:
- The originality of the work is limited, as the concept of churn and the proposed method bear similarities to existing literature, such as MeDQN.
- The empirical and theoretical results require improvement, particularly regarding the definitions and equations related to churn, which currently exhibit issues such as potential cancellation of values.
- The paper introduces additional hyperparameters, complicating implementation and potentially hindering performance if not set correctly.
- The authors do not propose CHAIN as an exploration method, which may limit its applicability in challenging exploration scenarios.
- Some claims regarding churn and interference may require clearer definitions and distinctions to avoid confusion.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definitions and equations related to churn, particularly by applying absolute operations to avoid cancellation issues. Additionally, consider expanding the discussion on the relationship between theoretical results and the proposed algorithm, as the current connection appears weak. It would also be beneficial to increase the number of random seeds used in experiments to at least 10 for more robust results. Furthermore, we suggest that the authors provide clearer guidance on hyperparameter settings for different tasks to enhance the usability of CHAIN. We also recommend improving clarity regarding the relationship between churn and interference, particularly in how these concepts are defined and differentiated in the context of their work. Finally, consider providing more detailed insights into the selection process for DCR/VCR/PCR in deep Actor-Critic methods to strengthen the paper's contributions and address all potential interpretations of "factors" contributing to performance.