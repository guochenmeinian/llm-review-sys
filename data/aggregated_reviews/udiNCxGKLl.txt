ID: udiNCxGKLl
Title: Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the relationship between language model (LM) surprisals and human reading times (RT), focusing on how model capacity and training set size influence predictive performance. It finds that a reversal in the positive correlation between LM quality and RT predictive capacity occurs around 1,000 training steps, consistent across two datasets with different modalities. The authors propose that very large models may not effectively model human reading times due to their excessive predictive capabilities.

### Strengths and Weaknesses
Strengths:
- The results are consistent across diverse datasets, enhancing the robustness of the findings.
- The paper contributes significantly to the literature, supporting the notion that large LMs may be "too good" at prediction.
- The experiments are well-described and soundly conducted.

Weaknesses:
- The claim regarding the necessary model capacity for capturing human-like expectations is unclear and lacks specificity.
- The analysis does not adequately address the cognitive implications of the findings, particularly the reasons behind the peak at two billion training tokens.
- The paper does not control for unigram log-probabilities, which are known to significantly predict reading times, potentially confounding the results.

### Suggestions for Improvement
We recommend that the authors clarify the claim about model capacity and its relationship to human reading times, specifying whether low capacity is necessary or if diminishing returns occur beyond a certain point. Additionally, we suggest including a discussion on why two billion training tokens represent a critical threshold. It would also be beneficial to control for unigram log-probabilities in the analysis to avoid confounding effects. Finally, we encourage the authors to provide clearer explanations of the selection and filtering of eye-tracking data in the text.