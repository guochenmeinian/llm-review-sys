ID: M7hijAPA4B
Title: Feature Dropout: Revisiting the Role of Augmentations in Contrastive Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 6, 6, 3, -1, -1, -1, -1
Original Confidences: 3, 4, 5, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a challenge to the prevailing assumption in contrastive unsupervised representation learning that data augmentations should be label-preserving. The authors propose that label-destroying augmentations can facilitate the learning of useful features for various downstream tasks. They demonstrate this through synthetic datasets and Viewmaker networks, arguing that such augmentations act as feature dropout, enhancing the feature extractor's ability to provide discriminative representations. The authors also offer a theoretical justification for the effectiveness of noise in a linear contrastive model with Gaussian-based data distribution.

### Strengths and Weaknesses
Strengths:
- Originality: The work questions a widely accepted assumption about data augmentation, presenting a novel perspective.
- Quality: Empirical and theoretical results provide valuable insights into the necessity of revisiting common assumptions in the field.
- Clarity: The paper is well-written, clearly articulating the motivation and supporting it with concrete examples.
- Significance: The findings contribute to the understanding of unsupervised representation learning, suggesting new avenues for research.

Weaknesses:
- Details of experiments and results are lacking, particularly regarding transfer accuracy and audio results.
- The theoretical analysis presents assumptions that do not align with empirical models, raising concerns about its applicability.
- The experimental setup may not accurately reflect real-world scenarios, as the independence of features in synthetic datasets may not hold in practical applications.

### Suggestions for Improvement
We recommend that the authors improve the clarity of experimental details, particularly regarding the transfer accuracy reported in Tables 1 and 2, and include audio results in the relevant figures. Additionally, we suggest addressing the gap between empirical and theoretical analyses by aligning the models used in experiments with those discussed theoretically. It would also be beneficial to clarify the definition of "multiple downstream tasks" and provide examples that reflect real-world scenarios. Furthermore, we encourage the authors to explore the sensitivity of their findings to the viewmaker perturbation budget, $\epsilon$, and to consider using more representative augmentations that preserve label information. Lastly, we advise including error bars in experimental results to enhance interpretability and ensuring that figures referenced in the main text are included in the body rather than relegated to the appendix.