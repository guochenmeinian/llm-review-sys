ID: XT9mL5vxX2
Title: Multitask Learning for Face Forgery Detection: A Joint Embedding Approach
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 4, 4, 5, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 5, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a multitask learning framework for video deepfake detection, utilizing a joint embedding architecture that combines visual and textual features. The authors propose a set of coarse-to-fine face forgery detection tasks, leveraging CLIP as the joint embedding model and employing ViT-B/32 and GPT-2 as visual and text encoders, respectively. The method achieves state-of-the-art performance on various datasets, demonstrating improved generalization capabilities.

### Strengths and Weaknesses
Strengths:
- The proposed framework effectively integrates multi-modality data, enhancing the detection of face forgeries.
- The approach to provide explanations for manipulations is valuable, contributing to the interpretability of the detection process.
- The use of language prompts to encode ground-truth labels is innovative and adds depth to the methodology.

Weaknesses:
- Many technical components are derived from existing works, limiting the novelty of the contributions.
- The description of the multitask learning method is generic and lacks specificity in relation to deepfake detection.
- The justification for using a coarse-to-fine approach and the generation of ground-truth labels via language prompts is unclear.
- The ablation study lacks clarity, and comparisons with state-of-the-art methods are insufficient.
- The experiments validating the explanations provided by the detector are limited to a single dataset, hindering the demonstration of generalization.

### Suggestions for Improvement
We recommend that the authors improve the technical description of their multitask learning framework to better relate it to deepfake detection. Additionally, the authors should provide a more detailed explanation of their dataSup scheme, as the current description is unclear and difficult to replicate. It is also advisable to expand the comparison with state-of-the-art methods, including those not currently referenced, and to conduct experiments across multiple datasets to validate the generalization of the proposed method. Furthermore, the authors should clarify the rationale behind the use of CLIP and fidelity loss in their framework. Lastly, we suggest that the authors enhance the robustness analysis by including various types of distortions commonly encountered in practical scenarios.