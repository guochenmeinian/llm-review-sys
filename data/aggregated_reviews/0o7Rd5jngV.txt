ID: 0o7Rd5jngV
Title: Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 6, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a systematic study on the expressive power and mechanisms of Transformers for sequence modeling, exploring properties such as the number of layers, attention heads, width, and dot production. The authors investigate approximation capabilities with relative positional encodings across three types of sequence modeling tasks, deriving approximation rates and commenting on the roles of various Transformer components. Theoretical results are supported by experimental evidence, although the clarity of motivation and contributions is questioned.

### Strengths and Weaknesses
Strengths:
- The paper is technically solid, providing a detailed formulation of the problem and exploring the expressive capabilities of Transformers.
- It includes rigorous proofs and well-structured theorems, making it easy to follow.
- Significant insights are offered regarding the approximation rates of Transformers and the roles of different components.

Weaknesses:
- The motivation for the study is weak, lacking clarity on why the current understanding of Transformers is insufficient.
- Technical contributions are not clearly delineated, making it difficult to distinguish between known and new techniques.
- The categorization of tasks is unclear, with weak supporting evidence for its necessity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation and contributions by explicitly stating the gaps in existing research that their work addresses. Additionally, we suggest that the authors provide a proof sketch for Theorem 4.4 to aid in verifying correctness. It would also be beneficial to clarify the relationships between the categorized tasks and to discuss the implications of layer normalization and embedding dimensions on the reported rates. Lastly, we encourage the authors to emphasize the limitations of their chosen attention mechanisms and how they compare to traditional approaches.