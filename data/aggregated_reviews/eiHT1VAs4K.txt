ID: eiHT1VAs4K
Title: Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ROBUST, a benchmark designed to evaluate the robustness of OpenIE systems against paraphrased sentences that maintain the same semantic meaning but differ in syntactic structure. The authors employ a systematic method to generate diverse paraphrases from the CaRB dataset, ensuring quality through human annotations. The evaluation of existing OpenIE methods reveals significant performance drops when faced with paraphrased inputs, highlighting a critical issue in current models. The authors propose a new metric based on the lowest F1 score among paraphrases to assess robustness, arguing that this approach provides insights into model performance variations.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and systematically details the creation of the ROBUST benchmark, making it easy to follow.  
- It addresses a significant issue in OpenIE systems, potentially guiding future research and improvements.  
- The dataset and metric offer a valuable resource for evaluating robustness in OpenIE models.

Weaknesses:  
- The proposed metric may not accurately reflect robustness, as it could penalize consistent but low-performing models.  
- Some findings in the detailed analysis conflict with the main conclusions, lacking sufficient explanation.  
- The evidence supporting the syntax-disturbance problem may be insufficient, and the methodology could benefit from comparisons with free-form paraphrasing.

### Suggestions for Improvement
We recommend that the authors improve the robustness metric by considering alternative scoring methods, such as using averaged or median F1 scores, to better capture model performance. Additionally, we suggest providing clearer explanations for the conflicting findings in the detailed analysis and addressing the potential limitations of the syntax-guided paraphrasing approach. It would also be beneficial to include a comparison of the ROBUST dataset with the CaRB dataset to clarify the differences in syntactic and expressive forms. Finally, we encourage the authors to explore the implications of using pre-trained paraphrasing models for generating paraphrases without explicit syntactic guidance.