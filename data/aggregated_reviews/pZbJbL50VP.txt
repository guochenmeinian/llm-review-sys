ID: pZbJbL50VP
Title: Cost-effective Data Labelling for Graph Neural Networks
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an innovative unsupervised active learning (AL) method for graph neural networks (GNNs) called Aggregation Involvement Maximization (AIM). The authors aim to create a scalable and flexible AL approach that does not depend on supervised information or labeled nodes. The core contribution is the formulation of the AIM problem, which seeks to select nodes that maximize the involvement of all nodes during feature aggregation. The authors demonstrate that this problem is NP-hard and provide an efficient greedy solution with theoretical guarantees. Extensive experiments show that AIM outperforms state-of-the-art AL techniques, achieving up to 19.4% higher accuracy and significant efficiency improvements.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel unsupervised AL framework that is efficient, effective, and flexible.
- The theoretical proof regarding the NP-hardness of the AIM problem and the proposed efficient solution is well-articulated.
- Extensive experiments validate the effectiveness and efficiency of the proposed method.

Weaknesses:
- The paper lacks sufficient baseline comparisons, missing methods like "ScatterSample" and Featprop.
- Some experiments require improvement, particularly in showing model performance under various conditions.
- The assumption that nodes with similar labels contribute similarly to GNN training may not hold universally.
- There is a lack of detailed discussion on time and space complexity, which is crucial for understanding the method's efficiency.

### Suggestions for Improvement
We recommend that the authors improve the baseline comparisons by including additional relevant methods such as "ScatterSample" and Featprop, and provide a rationale for the superiority of their approach. Additionally, the authors should enhance the experimental design to illustrate model performance under different considerations and provide theoretical support for the necessity of the proposed considerations. A more comprehensive discussion on the implications of label distribution in selected nodes and the potential biases in training data is essential. Finally, we suggest including a detailed analysis of time and space complexity to better convey the efficiency of the proposed method.