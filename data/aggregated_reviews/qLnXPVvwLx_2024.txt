ID: qLnXPVvwLx
Title: Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Prism, a framework designed to disentangle and evaluate the perception and reasoning capabilities of vision-language models (VLMs). The framework operates in two stages: a perception stage that extracts visual information and converts it into text using a VLM, and a reasoning stage that generates answers based on this textual information using a large language model (LLM). The evaluation of various state-of-the-art VLMs reveals that their perception ability remains consistent across different language encoder sizes, while reasoning ability is constrained by model size. The authors develop a lightweight yet effective VLM based on their findings.

### Strengths and Weaknesses
Strengths:
1. The paper proposes a novel framework for evaluating the disentangled perception and reasoning abilities of VLMs.
2. Extensive experiments validate the framework and cover various proprietary and open-source VLMs of different sizes.
3. The insights derived from the framework are valuable, and the lightweight VLM demonstrates soundness and effectiveness.

Weaknesses:
1. The evaluation of perception capability may be inaccurate as it relies on the detail of image captions generated by VLMs, which can vary based on their pre-training datasets.
2. The impact of prompt design for both perception and reasoning stages is not analyzed, which is crucial for understanding the framework's effectiveness.
3. The decoupling of inference into two stages may introduce computational overhead and latency, raising concerns about cost-effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of perception capability by considering the detail of image captions and investigating the relationship between pre-training datasets and model capabilities. Additionally, the authors should analyze the impact of different prompt designs on the performance of both stages. Discussing potential computational overhead and latency associated with the decoupling approach would also enhance the paper's clarity. Finally, we suggest providing a more in-depth discussion on the limitations of linguistic expressions in accurately describing certain visual content.