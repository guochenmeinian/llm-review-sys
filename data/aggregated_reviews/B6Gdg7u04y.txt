ID: B6Gdg7u04y
Title: LLMaAA: Making Large Language Models as Active Annotators
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LLMaAA, an active learning framework that utilizes large language models (LLMs) to generate labels for downstream models, replacing human labelers. The authors propose a method that incorporates k-NN sampling and automatic reweighting to assign learnable weights to training samples. Experimental results demonstrate that LLMaAA achieves competitive performance on named entity recognition (NER) and relation extraction (RE) tasks compared to strong baselines.

### Strengths and Weaknesses
Strengths:  
- The paper is well-organized and easy to follow.  
- LLMaAA outperforms other learning methods in low- to medium-data settings, providing valuable insights into combining active learning with LLMs.  
- The ablation study offers a comprehensive perspective on the importance of various design decisions.  

Weaknesses:  
- The novelty of the approach is limited, as it heavily borrows from existing techniques.  
- There is a lack of exploration regarding the effect of dataset sizes on performance.  
- Claims about efficiency and privacy are not well-supported, and the paper does not quantify the costs associated with different approaches.  
- The paper misuses the term "active learning" and lacks discussion on the source of unlabeled data used in experiments.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of their definitions for efficiency and reliability, as these terms are crucial to the paper's claims. Additionally, we suggest providing a thorough analysis of the effects of varying dataset sizes on LLMaAA's performance, particularly in low-resource settings. It would also be beneficial to justify the selection of the BERT student model and to clarify the motivation behind the "medium-data" setting studied. Furthermore, we encourage the authors to address the privacy concerns explicitly and to quantify the costs associated with human annotation, LLM service usage, and BERT finetuning. Lastly, we advise revising the introduction to focus more directly on the paper's contributions and to eliminate any sensational language.