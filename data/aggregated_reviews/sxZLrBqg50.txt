ID: sxZLrBqg50
Title: Is RLHF More Difficult than Standard RL? A Theoretical Perspective
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 4, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical foundation for reinforcement learning from human feedback (RLHF), analyzing utility-based preferences in various MDP settings and general preferences through the lens of von Neumann winners. The authors derive algorithms for both structured and adversarial scenarios, aiming to demonstrate that preference-based RL is not inherently more complex than reward-based RL.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written, with intuitive algorithms and a strong theoretical contribution.
- It addresses significant questions in RLHF, contributing to a deeper understanding of the subject matter.

Weaknesses:
- There are formatting issues, such as on page 8, and missing elements like a superscript in the definition of a partial trajectory.
- The lack of experimental results limits the practical applicability of the theoretical findings.
- Certain sections, particularly regarding general preferences, are vague and difficult to follow, raising concerns about the completeness of the analyses.

### Suggestions for Improvement
We recommend that the authors improve the formatting issues noted, including the line numbers on page 8 and the missing superscript in the definition of a partial trajectory. Additionally, please add space between line 343 and the subsequent equation. 

To enhance the paper's clarity, we suggest including discussions on how your approach differs from related works, specifically citing https://arxiv.org/pdf/2305.18505.pdf and https://arxiv.org/pdf/2305.14816.pdf. Furthermore, we encourage you to clarify the agnostic guarantees for Algorithm 1 in non-realizable cases and to standardize terminology regarding "oracle complexity" and "query complexity."

Consider providing examples or clarifications on how Algorithm 1 could be adapted for deterministic feedback and k-wise comparison feedback, as well as discussing the implications of using algorithms like NPG for solving adversarial MDPs. Lastly, we recommend that you address the theoretical limitations of your analyses and ensure that the title accurately reflects the scope of the paper.