ID: SHBksHKutP
Title: Contextual Stochastic Bilevel Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 5, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on contextual stochastic bilevel optimization (CSBO), where the lower-level problem involves a conditional expectation under contextual information. The authors propose a double-loop Monte-Carlo method leveraging EpochSGD to approximate the lower-level solution, providing an approximate hypergradient via the implicit function theorem. The framework is applied to meta-learning and instrumental variable regression, with theoretical complexity and convergence analyzed.

### Strengths and Weaknesses
Strengths:
- The work addresses a novel problem by incorporating contextual information, relevant to distributional robust optimization (DRO).
- The proposed algorithms are reasonable, with the use of MLMC to enhance DL-SGD performance being commendable. Theoretical guarantees and experimental results support the design principles.

Weaknesses:
- The problem appears somewhat artificial, with instances of transforming single-level problems into bilevel ones. The hypergradient form closely resembles non-contextual cases, suggesting limited novelty.
- Key components like MLMC and EpochSGD are established techniques, and challenges such as variance control could be managed with existing methods, diminishing the perceived novelty.
- Experiments lack depth, particularly in meta-learning, where comparisons to state-of-the-art bilevel optimization methods are absent. The choice of hyperparameters for MAML raises questions about the fairness of comparisons.
- The significance of bilevel optimization is not adequately demonstrated, as no standard baselines are included in the experiments.

### Suggestions for Improvement
We recommend that the authors improve the presentation to clarify the contributions and emphasize the challenges of solving the CSBO problem compared to traditional bilevel optimization. Additionally, the authors should conduct more comprehensive experiments, comparing their method against established bilevel optimization techniques and including more datasets, such as the Omniglot dataset. It would also be beneficial to briefly introduce existing bilevel optimization works in machine learning and discuss their limitations in the context of CSBO. Lastly, addressing the clarity of the gradient estimator's unbiasedness and providing intuition behind the use of EpochSGD would enhance the paper's rigor.