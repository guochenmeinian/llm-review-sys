ID: dQ9ji8e9qQ
Title: Adversarial Representation Engineering: A General Model Editing Framework for Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 5, 7, 7, -1, -1, -1
Original Confidences: 2, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an Adversarial Representation Engineering (ARE) framework aimed at editing Large Language Models (LLMs) while preserving performance. The authors introduce Representation Engineering (RepE) and enhance it with adversarial learning techniques. Key contributions include the development of an oracle discriminator for fine-tuning LLMs, a novel ARE framework for efficient editing, and extensive experiments showcasing ARE's effectiveness in various tasks, particularly in enhancing safety alignment and achieving state-of-the-art accuracy in TruthfulQA.

### Strengths and Weaknesses
Strengths:
- The ARE framework provides a practical and interpretable method for editing LLMs, with an effective iterative process between the generator and discriminator.
- The paper addresses the critical issue of understanding LLMs' internal mechanisms, offering a promising solution for safety alignment and hallucination reduction.
- The writing is clear and well-structured, with concise illustrations that effectively communicate the motivation and problem formulation.
- Experiments validate the practicality of ARE, demonstrating enhanced safety alignment and valuable insights, with code released for further research.

Weaknesses:
- The reliability of the concept discriminator requires evaluation; conducting human annotations would be beneficial for performance assessment.
- The proposed concept discriminator should be compared against a baseline that categorizes text based on concept containment.
- There is a lack of comparison with recent multi-step jailbreaking methods, which are gaining traction in the field.
- The scalability of the method for models larger than 7B is not discussed, presenting a potential limitation.
- The experimental section for Hallucination only compares one baseline, which may not sufficiently demonstrate advantages over other methods.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of the concept discriminator by incorporating human annotations to assess its reliability. Additionally, the authors should include comparisons against a baseline that categorizes text based on concept containment and consider recent multi-step jailbreaking methods as a baseline for further validation. Furthermore, we suggest discussing the scalability of the proposed method for larger models and expanding the experimental comparisons in the Hallucination section to include more baselines. Lastly, we recommend merging the limitations discussed into a specific section in the appendix for clarity.