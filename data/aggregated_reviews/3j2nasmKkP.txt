ID: 3j2nasmKkP
Title: Cluster-wise Graph Transformer with Dual-granularity Kernelized Attention
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel attention method for graphs, focusing on different resolutions of nodes through a dual granularity/resolution/hierarchy attention mechanism. It introduces Node-to-Cluster Attention (N2C-Attn), which allows clusters to interact with individual nodes, and proposes a Cluster-wise Graph Transformer (Cluster-GT). The method is evaluated on eight datasets, demonstrating superior performance compared to existing benchmarks.

### Strengths and Weaknesses
Strengths:
- The writing is clear, and the methodology is well-illustrated.
- The experimental results appear strong and comprehensive, covering both GCN and graph transformer benchmarks.
- The approach effectively resolves complexity issues associated with bi-level attention using kernelized attention.

Weaknesses:
- The novelty primarily lies in the dual granularity attention, while kernelization has been previously explored.
- The reliance on the METIS algorithm for graph partitioning may limit flexibility, and the paper's claims about avoiding graph coarsening could be misleading.
- The amount of notation complicates readability, and the paper lacks a comparison with recent graph pooling methods like MVPooL.

### Suggestions for Improvement
We recommend that the authors improve clarity by revising statements that suggest the method eliminates graph coarsening, as it still employs METIS. Additionally, consider discussing existing research on GNNs with graph coarsening to capture broader structural information. We suggest addressing the readability issue by reducing the amount of notation and providing a heuristic explanation for the performance of cluster-level attention over node-level attention. Finally, we encourage the authors to include a comparison with MVPooL in their experiments.