ID: nw6ANsC66G
Title: Probabilistic Federated Prompt-Tuning with Non-IID and Imbalanced Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 7, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to prompt-tuning within the federated learning (FL) framework, specifically addressing the challenges posed by data imbalance across diverse local data distributions. The authors propose a probabilistic model for local prompt generation and global prompt aggregation, utilizing a hierarchical generative model and a weighted bipartite matching task to optimize prompt association. Experimental results demonstrate the method's effectiveness in imbalanced data settings.

### Strengths and Weaknesses
Strengths:
1. The method is interpretable, employing Bernoulli and Gaussian distributions for local prompt generation, optimized through the EM algorithm.
2. The paper introduces an innovative probabilistic approach to prompt tuning, enhancing the understanding of local sets as independent samples of a random point process.
3. Comprehensive experiments provide robust evidence of the method's effectiveness against various federated prompt-tuning baselines.

Weaknesses:
1. The overall framework lacks clarity, with insufficient detail in the framework diagram and ambiguity regarding the consistency of DNNs across clients.
2. The local imbalance setting described is extreme and lacks real-world application support.
3. Limited discussion of existing FL methods addressing class imbalance, and the proposed prompt aggregation method does not directly tackle data imbalance.
4. The paper requires a comparative analysis with other efficient tuning methods to strengthen its arguments.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the overall framework by providing more detailed diagrams and explanations regarding the DNNs used. Additionally, the authors should discuss existing FL methods that address class imbalance to contextualize their contributions better. Expanding the experimental settings to include more variations of data imbalance would enhance the robustness of the findings. Finally, a comparative analysis with other tuning methods, such as adapter networks, would strengthen the paper's persuasiveness.