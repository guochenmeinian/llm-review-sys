ID: RsK483IRuO
Title: A Closer Look into Using Large Language Models for Automatic Evaluation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the potential of large language models (LLMs) to evaluate text quality, specifically through the comparison of two LLM-based evaluation systems: G-eval and LLM evaluation. The authors propose that generating a single numeric rating is suboptimal, and that prompting LLMs to rationalize their ratings enhances correlation with human evaluations. They also find that Auto-Chain of Thought reasoning does not improve this correlation. The study is based on real-world datasets and provides recommendations for effective prompting strategies.

### Strengths and Weaknesses
Strengths:
- The paper addresses an interesting and relevant problem regarding the use of LLMs for text quality assessment.
- It offers valuable recommendations and additional prompting strategies that enhance performance.
- The experimental procedure is clear and well-presented, facilitating understanding.

Weaknesses:
- The content is too extensive for a short paper, requiring frequent reference to the appendix, which detracts from self-containment.
- The evaluation is limited to ChatGPT, raising concerns about the generalizability of the findings to other LLMs.
- The robustness of the evaluation of Chain-of-Thought reasoning is questionable, and there is a lack of targeted conclusions and detailed reproducibility information.

### Suggestions for Improvement
We recommend that the authors improve the self-containment of the main paper by integrating essential details currently relegated to the appendix. Additionally, consider expanding the evaluation to include other open-source models such as LLaMa to enhance generalizability. We suggest providing more empirical evidence to support the guidelines, particularly regarding the sensitivity of LLM performance to various factors. Lastly, we encourage the authors to clarify the robustness of their results and to include more targeted conclusions and recommendations for further prompting.