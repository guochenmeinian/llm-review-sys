ID: Ddak3nSqQM
Title: Policy Learning from Tutorial Books via Understanding, Rehearsing and Introspecting
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 7, 6, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to reinforcement learning (RL) termed "Policy Learning from Books" (PLfB), which utilizes textual knowledge from books and tutorials to develop policy networks without extensive real-world interactions. The authors propose a three-stage framework called URI (Understanding, Rehearsing, and Introspecting) to implement this approach. The method is validated through experiments in the Google Football environment, where the trained agent achieved a 37% winning rate against built-in AI, significantly outperforming a GPT-based agent. The paper emphasizes the feasibility of extracting policies without direct environment interaction by incorporating descriptions of MDP structures, transition functions, and reward functions within the textual data.

### Strengths and Weaknesses
Strengths:
- The innovative concept of PLfB represents a significant departure from traditional RL methods, leveraging textual resources for policy network derivation.
- The URI framework mimics human learning processes, making the approach intuitive and biologically inspired.
- The empirical results demonstrate a promising performance, with a notable winning rate in the Google Football simulation.
- The methodology is well-validated through practical experiments, enhancing its credibility.

Weaknesses:
- The paper lacks detailed discussion on the influence of the prompting strategy used for generating the imaginary dataset, which could impact the robustness of the approach.
- The success of the method heavily relies on the quality of textual resources, potentially limiting its applicability in domains with sparse data.
- The generalizability of the method across different domains remains uncertain and requires further exploration.
- The complexity of the framework may pose challenges for replication and implementation by other researchers.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology by providing detailed discussions on the prompting strategies used for generating the imaginary dataset, as this could enhance the robustness of the approach. Additionally, we suggest exploring the generalizability of the PLfB method across various domains and validating its effectiveness in different challenging environments. To strengthen the empirical evaluation, we recommend including comparisons with conventional RL algorithms such as PPO, DDPG, and SAC. Furthermore, we encourage the authors to address the limitations related to the quality of textual resources in the main body of the paper and to provide pseudocode to facilitate reproducibility.