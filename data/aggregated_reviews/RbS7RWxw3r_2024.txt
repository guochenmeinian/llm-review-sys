ID: RbS7RWxw3r
Title: Iteratively Refined Behavior Regularization for Offline Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 5, 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a modified version of prior behavior-regularized offline reinforcement learning (RL) methods based on conservative policy iteration (CPI), where the current policy is regularized towards an older policy. The authors demonstrate performance benefits on the D4RL benchmark and provide theoretical guarantees in tabular settings. The algorithm is implemented as a simple modification over TD3-BC, adding a regularization term to control the distance between the current and a frozen version of the policy.

### Strengths and Weaknesses
Strengths:
- The proposed algorithm is easy to implement and can be integrated with TD3+BC or similar methods.
- Strong performance is observed across multiple offline datasets, supported by extensive experimental evaluations.
- The idea of iteratively refining the reference policy enhances robustness and performance in behavior regularization methods.

Weaknesses:
- There is skepticism regarding whether the algorithm achieves its claimed objectives, particularly if the current policy escapes the support of the behavior policy.
- The algorithm's performance is highly dependent on the value of the hyperparameter $\tau$, which varies significantly across experiments and is challenging to determine in real-world scenarios.
- Results are based on per-environment hyperparameter optimization, raising concerns about potential overfitting and the need for clearer comparisons with baselines like TD3+BC, which minimizes MSE.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their hyperparameter selection process, particularly regarding the impact of $\tau$ on performance. Including additional baselines such as %BC in their results would help elucidate the role of $\tau$. Furthermore, we suggest providing a more detailed evaluation of the algorithm's limitations and conducting ablation studies on additional offline datasets to deepen insights into its performance. Lastly, we encourage the authors to address the discrepancies in the experimental setup, particularly in Figure 1, to enhance reproducibility and understanding.