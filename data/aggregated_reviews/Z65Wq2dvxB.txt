ID: Z65Wq2dvxB
Title: Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the memorization of books by large language models (ChatGPT, GPT-4, and BERT), revealing that these models have extensively memorized many popular sci-fi/fantasy novels and copyrighted bestsellers. The study is crucial for understanding the implications of memorization on bias and validity in downstream tasks. The methodology is rigorous, evaluating hundreds of books across multiple models, and the paper is well-written and clear.

### Strengths and Weaknesses
Strengths:  
- The analysis provides significant insights into memorization in large language models, with implications for bias and validity.  
- The methodology is sound, and the paper contributes meaningfully to discussions in both the humanities and technical fields.  
- Reproducibility is enhanced by the availability of code and data.

Weaknesses:  
- The evaluation of downstream tasks is limited, necessitating further analysis to strengthen conclusions.  
- The scale of experiments raises reproducibility concerns due to resource requirements.  
- The study risks speculation, as the authors may have preconceived notions influencing their experimental design.  
- The paper does not adequately measure the generalization ability of LLMs in relation to memorization.

### Suggestions for Improvement
We recommend that the authors improve the analysis of downstream tasks by including a broader range of evaluations to strengthen their conclusions. Additionally, consider incorporating fine-tuning tasks to assess the impact of memorization on model performance. Address the generalization ability of LLMs in narrative writing, as this is a central issue related to memorization. Lastly, clarify the open-source status of LLaMA and provide more details about the human evaluation process mentioned in the paper.