ID: RYvNvCU109
Title: Energy and Carbon Considerations of Fine-Tuning BERT
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical study estimating the carbon usage in the "pre-train and then fine-tune" paradigm by reimplementing the BERT pre-training and fine-tuning stages. The authors compare fine-tuning energy use across various supervised NLP datasets and measure energy consumption on different hardware setups, providing a comprehensive analysis of the environmental impact of BERT models.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and easy to follow, with clear goals and claims supported by empirical evidence.  
- It offers valuable data points for the energy cost of fine-tuning BERT, which can serve as a reference for the community.  
- The study provides a detailed emission result on BERT pre-training and fine-tuning.

Weaknesses:  
- The work may be perceived more as a technical report than a research paper, lacking depth in discussing the significance of energy and carbon usage during fine-tuning.  
- The focus on BERT, an older model, limits the study's relevance compared to newer models like RoBERTa or T5.  
- The paper does not adequately address real-world scenarios, particularly the carbon emissions from the inference process.  
- The organization could be improved, with a missing conclusion section and tables currently in the appendix that would benefit from being included in the main content.

### Suggestions for Improvement
We recommend that the authors improve the paper's organization by including a conclusion section and moving tables from the appendix to the main content, accompanied by more in-depth discussions. Additionally, the authors should explore the differences in energy and carbon usage between various fine-tuning strategies and consider including newer models in their analysis. Addressing the concerns regarding the distinction between multi-GPU and single-GPU results will also enhance the paper's applicability and relevance.