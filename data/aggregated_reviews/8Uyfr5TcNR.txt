ID: 8Uyfr5TcNR
Title: Robust Reinforcement Learning with General Utility
Conference: NeurIPS
Year: 2024
Number of Reviews: 22
Original Ratings: 7, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to robust reinforcement learning (RL) by integrating general utility functions with robustness against environmental uncertainties. The authors propose a policy gradient algorithm that converges to a stationary point and introduce a more complex algorithm for a specific s-regular polyhedral ambiguity set, which is shown to converge globally. Additionally, the paper outlines methods for solving robust RL, including value iteration, policy iteration, and policy gradient techniques. The authors demonstrate that robust RL with convex utility encompasses significant special cases and can be applied to various scenarios such as maximum-entropy exploration and constrained RL. The theoretical foundations are supported by extensive proofs and numerical experiments, detailing the assumptions and components necessary for the theoretical results, including a softmax policy, a directly parameterized transition kernel, an s-rectangular L^1 ambiguity set, and a convex utility function.

### Strengths and Weaknesses
Strengths:
- The authors address a new problem by combining general utility and robustness, constructing algorithms with theoretical guarantees.
- The writing is technical yet understandable, effectively highlighting the authors' contributions.
- The proofs appear correct, and the results are validated through numerical experiments.
- The paper effectively addresses major concerns regarding the proofs and theorems, leading to improved clarity and understanding.
- The complexity results for robust RL with general utility outperform existing methods, providing valuable insights into the performance of robust approaches.

Weaknesses:
- The paper lacks clear motivation and comparative analysis against existing non-robust methods, which diminishes its impact.
- There remains ambiguity regarding the comparative performance of the proposed robust RL algorithms against existing robust constrained RL approaches on the same test problems.
- Numerical experiments are relegated to the appendix and should be included in the main article for better integration with theoretical results.
- The complexity of the algorithms is high, and the assumptions made prior to convergence results require further practical justification.
- The paper could benefit from a more explicit tracking of constants in the algorithmic choices related to Theorem 2.

### Suggestions for Improvement
We recommend that the authors improve the motivation and clarity of the problem definition in the introduction, providing intuitive explanations early on. Additionally, the authors should include comparative analyses with existing methods to demonstrate the advantages of their approach, particularly against existing robust constrained RL methods on identical test problems. The numerical experiments should be integrated into the main body of the paper, as they significantly enhance the theoretical findings. Furthermore, we suggest that the authors address the computational costs and scalability of their methods, proposing strategies to manage hyperparameter complexity, such as adaptive tuning or simplified parameter settings. Lastly, the authors should explicitly track all constants that appear in the algorithmic choices needed for Theorem 2 to enhance the theorem's applicability and understanding, as well as provide explicit examples of policies that satisfy their assumptions to strengthen the theoretical claims made in the paper.