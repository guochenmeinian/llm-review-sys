ID: bvl3p6JUlv
Title: Mitigating Biases in Hate Speech Detection from A Causal Perspective
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents methods to mitigate biases in hate speech detection by automatically identifying and validating spurious correlations. The authors propose two approaches: one addressing token-level biases through a masked language model (MLM) and the other employing counterfactual learning to generate non-hate speech samples. The results demonstrate consistent improvements across nine hate speech datasets. Additionally, the authors introduce a novel metric, "Relative Spuriousness," to measure the disparity between local and global model scores.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and addresses a significant issue in content moderation algorithms.
- The proposed methods are automated, facilitating future replication and extension of the work.
- The causal perspective provides a solid foundation for the proposed interventions.
- The methods show good performance and generalization across various datasets.

Weaknesses:
- The choice of evaluation metrics is not well-justified, raising concerns about the definition of an unbiased model.
- There is a lack of clarity regarding the definition of hate speech, leading to potential inconsistencies in understanding the task.
- The quality of generated counterfactual data is not addressed, and validation for non-causal confounds is missing.

### Suggestions for Improvement
We recommend that the authors improve the justification for their metric choices, particularly clarifying how an unbiased model is defined in relation to the average responses from models trained on every hate speech dataset. Additionally, the authors should provide a clearer definition of hate speech to avoid inconsistencies and validate the quality of generated counterfactual data through manual inspection or small batch annotation. Furthermore, we suggest including statistics on the correlation between detected biases and corresponding labels in the datasets, and ensuring all references are cited with their original published versions for accuracy.