ID: 8OTPepXzeh
Title: DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 5, 6, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for improving pretrained text-to-image diffusion models through reinforcement learning (RL) with human feedback in an online manner. The authors clearly differentiate between online updates and supervised fine-tuning, demonstrating that their approach enhances text-to-image generation performance. They introduce KL divergence regularization to stabilize the training process and report positive experimental results across various performance metrics.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, making it accessible and easy to follow.
- The proposed method is novel and presents a clear advantage over existing approaches, particularly in its online RL strategy.
- Empirical results indicate that the method outperforms traditional supervised human-feedback fine-tuning.

Weaknesses:
- Some claims, particularly in Section 4.3, lack strong alignment with experimental results, making it difficult to establish direct connections.
- The title does not adequately reflect the main contribution of the work, which centers on the online aspect of the method.
- The experimental evaluation is limited, focusing on only four prompts and lacking comparisons with other relevant studies, such as [17]. Additionally, the necessity of online optimization is questioned due to potential solutions for over-saturation in the supervised fine-tuning model.

### Suggestions for Improvement
We recommend that the authors improve the alignment between claims and experimental results, particularly in Section 4.3, to clarify the connections made. A more descriptive title that emphasizes the online nature of the contribution would enhance clarity. Furthermore, expanding the experimental evaluation to include a broader range of prompts and direct comparisons with existing methods, such as [17], would strengthen the paper's findings. Lastly, addressing the potential over-saturation issue in the supervised fine-tuning model and exploring additional reward models could provide deeper insights into the method's robustness and applicability.