ID: YsZTDcIQwQ
Title: Diversifying Spatial-Temporal Perception for Video Domain Generalization
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 5, 6, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Spatial-Temporal Diversification Network (STDN) aimed at video domain generalization. The authors introduce a spatial grouping method to summarize spatial cues in frames and a spatial-temporal relation module to model dependencies across multiple scales. The effectiveness of the proposed method is demonstrated through experiments on three benchmarks, claiming state-of-the-art results. Additionally, the authors respond to previous concerns, showing their commitment to addressing feedback and improving their work.

### Strengths and Weaknesses
Strengths:
- The domain generalization problem is significant for video understanding.
- The paper is well-structured and clearly written.
- Experimental results indicate the proposed method shows improvements over existing baselines.
- The authors show a willingness to engage with feedback and make improvements based on reviewer comments.

Weaknesses:
- The design lacks specificity for domain generalization; spatial grouping and temporal relation modeling could apply to traditional video classification.
- The experimental settings are not sufficiently challenging, primarily using datasets from the same domain.
- The analyses of spatial grouping and the justification of domain-invariant features are not convincing.
- Comparisons with existing methods, particularly those using different backbones, are inadequate.
- The review does not provide specific critiques or areas for further enhancement.

### Suggestions for Improvement
We recommend that the authors improve the justification for the importance of spatial grouping and temporal relation modeling specifically for domain generalization. Additionally, consider conducting experiments in cross-domain settings and include results from traditional video classification benchmarks like Kinetics400 or Something-Something V1/V2. An ablation study comparing the proposed spatial grouping module with similar methods, such as spatial attention and KNN, would provide a more thorough analysis. Finally, ensure a fair comparison with methods using ensembles and various backbones, such as I3D, to validate the proposed approach's effectiveness. Furthermore, we recommend that the authors provide more detailed responses to specific feedback points to enhance clarity and demonstrate thorough engagement with the review process.