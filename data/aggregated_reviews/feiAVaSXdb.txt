ID: feiAVaSXdb
Title: Measuring and Narrowing the Compositionality Gap in Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper explores the performance gap of large language models (LLMs) in answering compositional questions versus their sub-questions. The authors find that increasing the size of LLMs does not significantly narrow this gap, but they propose an improved elicitive prompting method, which incorporates search engines to answer sub-questions. The study evaluates this method using two existing QA datasets and a newly proposed dataset, demonstrating that the new prompting technique can yield some performance improvements in answering compositional questions.

### Strengths and Weaknesses
Strengths:  
- The paper addresses an important topic regarding LLMs' ability to handle compositional tasks, revealing that larger models do not necessarily enhance compositional ability.  
- The introduction of the self-ask prompting method and the new datasets (Compositional Celebrities and Bamboogle) provides valuable resources for future research.  
- The evaluation of compositional reasoning with 2-hop questions is a neat approach that shows potential for further exploration.

Weaknesses:  
- The contribution of the proposed prompting method is marginal, as decomposing questions and utilizing search engines are not novel concepts.  
- The experimental results do not convincingly demonstrate that the prompting method effectively narrows the compositional gap, showing only slight performance improvements.  
- The focus on GPT-3 limits the generalizability of the findings to other LLMs, and the novelty of the self-ask method is somewhat diminished by existing literature.

### Suggestions for Improvement
We recommend that the authors improve the qualitative analysis on how elicitive prompting narrows the compositionality gap, as this would enhance the understanding of the method's effectiveness. Additionally, exploring compositional reasoning involving more than 2 hops could provide deeper insights. Including results from a broader range of models beyond GPT-3 would strengthen the paper's conclusions. We also suggest that the authors present important experimental results in the main body for better visibility and clarity, and refine the writing for conciseness and flow.