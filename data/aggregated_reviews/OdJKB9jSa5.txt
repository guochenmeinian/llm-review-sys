ID: OdJKB9jSa5
Title: ST$_k$: A Scalable Module for Solving Top-k Problems
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 6, 4, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel differentiable module, $ST_k$, designed to address the Top-K problem and the challenges of imbalanced data in machine learning by approximating the hinge function with a smoothed ReLU function. The authors argue that the proposed loss function effectively prevents the model from shifting its decision boundary towards the minority class, as evidenced by the analysis of Figure 3. They demonstrate the effectiveness of their approach through extensive experiments on various datasets, including binary classification and long-tailed classification tasks, showing consistent performance improvements over baseline methods such as Average loss and $AT_k$ loss. The authors highlight the importance of continuous gradient optimization for the hyperparameter $\lambda$ and assert that $ST_k$ achieves uniform convergence, which is a strong theoretical condition.

### Strengths and Weaknesses
Strengths:
1. The paper is clearly written and easy to follow.
2. Extensive experiments validate the proposed $ST_k$ loss across both synthetic and real-world datasets, showing significant improvements in model performance on imbalanced datasets.
3. The theoretical foundation of uniform convergence is well-articulated and supports the robustness of the method.
4. The proposed method is flexible and integrates easily into existing architectures.
5. The inclusion of additional experiments enhances the understanding of the method's effectiveness and its computational efficiency compared to existing algorithms.

Weaknesses:
1. The innovation is limited, primarily involving the substitution of a ReLU-like operation with a smoothed ReLU function, which has been explored in prior works.
2. The large-scale evaluation is restricted to a long-tailed classification task using a pre-trained CLIP model, which may not convincingly demonstrate the method's effectiveness. Training from scratch on these tasks would provide a clearer assessment.
3. The absence of an ablation study on the hyperparameter $\delta$ limits understanding of its impact on performance.
4. The discontinuity in the optimization process of $\lambda$ remains a concern that could affect stability.
5. The authors' response to the critique regarding the strength of point-wise convergence may not fully address the reviewers' concerns.
6. The theoretical guarantees for the proposed method are weak, particularly regarding convergence rates and error bounds.

### Suggestions for Improvement
We recommend that the authors improve the theoretical framework by providing a detailed analysis of the convergence properties of $ST_k$ compared to $AT_k$. Additionally, conducting experiments on large-scale datasets, such as ImageNet, would strengthen the validation of their method. An ablation study focusing on the hyperparameter $\delta$ should be included to assess its influence on model performance. We also suggest that the authors improve the clarity of their explanation regarding the discontinuity affecting the optimization process of $\lambda$. Furthermore, providing further analysis on the applicability of the proposed loss function to balanced datasets, as well as a more detailed discussion on why the loss is specifically tailored for imbalanced data, would enhance the paper's contribution. Finally, clarifying how $ST_k$ complements existing long-tailed learning methods would further strengthen the paper.