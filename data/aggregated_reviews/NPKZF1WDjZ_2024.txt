ID: NPKZF1WDjZ
Title: Decompose, Analyze and Rethink: Solving Intricate Problems with Human-like Reasoning Cycle
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 7, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a reasoning framework called Decompose-Analyze-Rethink (DeAR) aimed at enhancing the reasoning capabilities of large language models (LLMs). DeAR mimics human cognitive reasoning by decomposing complex problems into simpler sub-problems using a Reasoning Tree structure, analyzing these independently, and rethinking answers based on new insights. The iterative cycle allows for dynamic adjustments and error corrections, demonstrating promising results across multiple reasoning datasets.

### Strengths and Weaknesses
Strengths:
- The paper makes solid progress in improving how LLMs solve complex problems, a crucial area in AI research.
- The organization and explanation of the proposed method are clear and align well with human reasoning.
- The experiments are robust, showing significant performance improvements over state-of-the-art methods like CoT, ToT, and GoT.

Weaknesses:
- The process for obtaining decomposition demonstrations in the logic heuristics lacks detailed explanation.
- There is insufficient discussion on how this work differs from existing paradigms of problem decomposition, potentially limiting its technical contribution.
- The experimental design could be enhanced with a more detailed analysis, particularly regarding the effectiveness of the “self-check” mechanism.
- Presentation issues include incorrect references and typographical errors.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the decomposition demonstration process in the logic heuristics. Additionally, the authors should include a comparative discussion on how their work differs from existing problem decomposition methods. To enhance the experimental design, we suggest providing a detailed evaluation of the “self-check” mechanism, including error rates of generated rationales. Furthermore, we advise correcting presentation issues, such as the incorrect reference to Table 4 and the typo in Algorithm 1 (stgt;).