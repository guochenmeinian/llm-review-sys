ID: rdnzTocXBr
Title: Rankformer: A Graph Transformer for Recommendation based on Ranking Objective
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Rankformer, a ranking-inspired recommendation model that integrates a gradient-descent-inspired architecture to align with personalized ranking objectives. The authors propose a model that efficiently combines global user-item interactions with benchmark and offset terms, demonstrating superior ranking performance across multiple datasets while significantly reducing computational complexity.

### Strengths and Weaknesses
Strengths:
- The model directly optimizes the ranking-based objective and leverages signals from negative instances during aggregation.
- Empirical evaluations on four datasets show that Rankformer consistently outperforms state-of-the-art methods.
- The architecture is novel and effectively designed based on insights from ranking objectives, and the paper is well-written.

Weaknesses:
- The model appears to modify existing graph-based methods, such as LightGCN, in a trivial manner, with ablation studies indicating minimal impact from certain components like the benchmark term.
- The motivation behind the proposed framework is unclear, particularly regarding the performance gains from stacking multiple layers of GNNs, as demonstrated in Figure 1.
- The rationale for the effectiveness of the model is insufficiently explained, especially concerning the negative sampling strategy and the role of contrastive learning.
- There are grammar mistakes present in the manuscript that need correction.

### Suggestions for Improvement
We recommend that the authors clarify the motivation behind the proposed framework, particularly regarding the claims made in Figure 1. Additionally, please provide a more detailed explanation of the negative sampling strategy and its impact on model performance. We suggest that the authors explore the potential integration of contrastive learning to enhance Rankformer's effectiveness. Furthermore, addressing the grammar mistakes noted in the review will improve the overall clarity of the paper. Lastly, a comparison with the model presented in “RankFormer: Listwise Learning-to-Rank Using Listwide Labels” would help highlight the unique contributions of Rankformer.