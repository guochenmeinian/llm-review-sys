ID: l9BsCh8ikK
Title: Visual Instruction Inversion: Image Editing via Image Prompting
Conference: NeurIPS
Year: 2023
Number of Reviews: 26
Original Ratings: 7, 5, 5, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for image editing through visual prompting, utilizing pairs of "before" and "after" images to learn a text-based editing direction. The authors optimize the text-conditioning embedding using a pretrained diffusion model (Stable Diffusion) to align with the CLIP-space direction between the images, minimizing reconstruction loss to maintain original image details. The effectiveness of this approach is demonstrated through qualitative and quantitative comparisons with existing techniques, such as InstructPix2Pix and SDEdit. The authors also analyze the influence of noise and initialization on the generation process. Furthermore, the authors claim that their method is more general than SINE, as it learns edits from before-and-after image pairs and applies them to test images, differentiating it from SINE's style transfer approach and MIDMs, which operate in latent space.

### Strengths and Weaknesses
Strengths:
- The authors introduce a straightforward yet effective technique for extracting text-based editing directions, particularly beneficial when edits are hard to articulate.
- The method requires only a single exemplar pair, enhancing its applicability across various contexts.
- The instruction concatenation technique offers additional user control, which is a notable advantage over alternative methods.
- Comprehensive comparisons with multiple techniques demonstrate superior performance across various edits and images.
- The authors provide a clear explanation of their methodology and its generality compared to SINE and MIDMs.
- The paper includes competitive performance comparisons with several relevant state-of-the-art methods.

Weaknesses:
- The method appears to be more effective for style-based edits, lacking examples or discussions on structural changes (e.g., sitting/jumping).
- Sensitivity to the choice of exemplar pair is not adequately assessed; exploring the success rate of a single input pair across test images could provide valuable insights.
- Visual results demonstrating the benefits of using multiple exemplars are missing, despite evidence that additional pairs improve outcomes.
- The reliance on pretrained models raises concerns about the quality of paired images, and the method's performance with untrained example pairs is uncertain.
- The lack of empirical evidence to substantiate claims about the generality of their method compared to SINE.
- Insufficient comparative analysis with MIDMs, which could enhance the comprehensiveness of the work.
- The reliance on Visual Prompting as a primary comparison point is questioned, with calls for more concrete justification of its relevance.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the method's effectiveness for structural changes by providing relevant examples or analyses. Additionally, assessing the sensitivity of the method to the choice of exemplar pairs through quantitative or qualitative studies would strengthen the findings. Including visual results that illustrate the impact of multiple exemplars on performance would enhance the paper's contributions. Furthermore, a more comprehensive analysis of hyper-parameter settings, particularly the values of $\lambda_{mse}$ and $\lambda_{clip}$, should be included to demonstrate robustness across different conditions. We also recommend that the authors improve their empirical validation by providing experimental results that substantiate the claim of their method's generality over SINE. A deeper exploration or comparison with MIDMs would enhance the comprehensiveness of the work. Additionally, we suggest that the authors clarify their justification for comparing Visual Prompting to their approach, particularly addressing the concerns regarding its applicability to image editing tasks. Lastly, including a comparison with Deep Image Analogies could provide valuable insights, despite the differences in methodology.