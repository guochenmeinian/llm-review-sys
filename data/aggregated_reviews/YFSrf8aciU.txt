ID: YFSrf8aciU
Title: Inverse Reinforcement Learning with the Average Reward Criterion
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 6, 3, 7, 6, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach to Inverse Reinforcement Learning (IRL) within the average-reward framework, proposing the Stochastic Policy Mirror Descent (SPMD) method for Average Reward Markov Decision Processes (MDPs) and the Inverse Policy Mirror Descent (IPMD) method for solving the IRL problem. The authors provide theoretical complexity results and validate their methods through experiments on the MuJoCo benchmark, demonstrating competitive performance against existing algorithms.

### Strengths and Weaknesses
Strengths:
- The authors develop a novel method with strong theoretical and experimental validation.
- The theoretical analysis is thorough, clearly outlining the assumptions made.
- The proposed IPMD method shows promising results across various environments.

Weaknesses:
- The paper lacks detailed experimental information, such as hyperparameters.
- An intuitive explanation for the result of Theorem 3.4 is missing, and Equation 25 is difficult to interpret.
- The authors make several assumptions in their theoretical analysis without sufficient discussion on their practical applicability or enforcement.

### Suggestions for Improvement
We recommend that the authors improve the experimental details by providing more information on hyperparameters and architecture specifics for the policy, Q network, and learned reward function in the appendix. Additionally, we suggest offering an intuitive explanation for Theorem 3.4 and clarifying the assumptions made, particularly regarding their practical implications and enforcement. Addressing the variability in performance across different environments, especially in sparse reward settings, would also enhance the paper's contributions.