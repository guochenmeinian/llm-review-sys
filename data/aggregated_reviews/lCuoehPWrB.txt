ID: lCuoehPWrB
Title: Interactive Visual Reasoning under Uncertainty
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 7, 7, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Interactive Variable Reasoning Environment (IVRE), an interactive visual reasoning environment designed to evaluate agents' reasoning abilities under uncertainty, inspired by the Blicket experiment and utilizing the CLEVR framework. The authors propose a benchmark task that requires agents to identify *blickets* in a scene, emphasizing hypothesis testing in uncertain conditions. The study investigates the performance of various agents, including a Search-Naive agent, and highlights the challenges posed by visual input models compared to symbolic ones. A comprehensive evaluation of diverse models is conducted, including human performance benchmarks, and the authors acknowledge the need for clarity in explaining the task and the performance gap between search-naïve and human agents, as well as the unexpectedly low performance of visual models.

### Strengths and Weaknesses
**Strengths:**
- The paper is well-written and clearly structured, introducing a novel interactive framework that enhances understanding of reasoning under uncertainty.
- It integrates visual reasoning into sequential decision-making and provides thorough evaluations with a diverse range of models tested, including thoughtful responses to reviewer feedback.
- The flexibility of the environment allows for both symbolic and pixel-based evaluations, and the inclusion of experiments with different models, such as slot-attention and higher resolution inputs, adds valuable insights into performance dynamics.

**Weaknesses:**
- The action space is limited to a probability simplex over objects, which does not align with traditional definitions of sensorimotor control.
- The introduction lacks clarity for new audiences, particularly regarding the task and the concept of proposing experiments, and the performance of visual models is unexpectedly low, raising questions about the effectiveness of the current approach.
- The performance of symbol-input agents is notably lower, warranting further investigation, and the paper does not sufficiently discuss the performance gap between search-naïve agents and human performance.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the performance gap between the Search-Naive method and other baselines, as its superior performance merits further explanation. Additionally, we suggest improving the introduction to provide a more intuitive understanding of the blicket setting and clarifying key concepts. Including a step-by-step tutorial for the IVRE game would facilitate better comprehension. Expanding the discussion on the performance gap between search-naïve and human agents to highlight differences in strategies and outcomes is also advisable. Addressing the surprisingly low performance of visual models with more analysis would enhance the paper's depth. Furthermore, we encourage the authors to report agents' performance with a fixed seed and consider an ablation study with larger image sizes and fewer objects to assess the impact on reasoning capabilities. Lastly, clarifying the average number of trials needed to solve an episode would enhance the understanding of the reported reward values, and addressing the societal impact of their work would be a relevant consideration for the conference.