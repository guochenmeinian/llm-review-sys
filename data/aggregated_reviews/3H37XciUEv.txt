ID: 3H37XciUEv
Title: Post Hoc Explanations of Language Models Can Improve Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AMPLIFY, a framework that utilizes post-hoc explanations from a proxy model to enhance the prompting performance of large language models (LLMs). The method involves four stages: selecting a proxy model, choosing few-shot samples, computing explanations, and formatting prompts. AMPLIFY aims to reduce the need for human annotation in few-shot in-context learning while demonstrating improvements over existing methods, particularly Chain-of-Thought (CoT) prompting, across various tasks from the Big-Bench-Hard benchmark.

### Strengths and Weaknesses
Strengths:
1. The research explores a novel application of post-hoc explanations to improve in-context learning performance, potentially opening new avenues in the field.
2. The presentation is clear, with comprehensive ablation studies and comparisons that provide sound empirical evidence for the method's effectiveness.
3. Significant improvements over CoT are noted, with gains of 10-25% in performance, particularly with a fine-tuned proxy model.

Weaknesses:
1. The performance improvement is less consistent with non-finetuned proxy models, as shown in Tables 1 and 2, where certain tasks perform worse than CoT. This inconsistency is not adequately discussed in the paper.
2. The requirement for training data for the fine-tuned proxy model deviates from the typical in-context learning assumption of using minimal annotated examples, and the performance of the fine-tuned model across different task settings (E=0/10/200) is not presented.
3. The paper lacks comparisons with other baselines, such as Auto-CoT, and omits certain datasets from the evaluation, which may limit the generalizability of the findings.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the performance of non-finetuned proxy models, particularly addressing the inconsistencies observed in the results. Additionally, it would be beneficial to provide details on the training data used for fine-tuning the proxy model and to present its performance across various task settings. We also suggest including comparisons with other relevant baselines, such as Auto-CoT, and evaluating the method on a broader range of tasks, including those requiring complex reasoning, to better assess its applicability and effectiveness. Finally, clarifying the selection process for few-shot examples and the validation set size would enhance the transparency of the methodology.