ID: FGTDe6EA0B
Title: Language Generation in the Limit
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 1, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new theoretical framework for language generation, proposing that while identifying a formal language from positive examples is generally impossible, one can learn an under-approximation of the language, which is infinite for non-finite languages. The authors constructively prove that language generation is possible in the limit, even when the list of languages is infinite. The paper also reviews classical negative results in language identification, providing a pedagogical foundation for the constructive proof. Furthermore, the authors propose a proof that could benefit from a more structured Definition/Statement/Proof approach, incorporating an algorithmic perspective using Inclusion Queries and Membership queries. They suggest defining a suitable notion of criticality, stating the main theorem in terms of these queries, outlining the protocol for language generation, and proving correctness based on criticality properties.

### Strengths and Weaknesses
Strengths:  
- The paper offers a novel theoretical perspective on a classic topic, making it approachable and relevant, especially in the context of generative AI.  
- The treatment of negative results is well-handled, serving as an effective pedagogical launching point for the constructive proof.  
- The proofs appear to be correct upon review and demonstrate thoughtful engagement with the material.  
- The proposed structural changes could enhance clarity and flow.

Weaknesses:  
- The explanations of key ideas are excessively lengthy, making the paper difficult to read.  
- The current exposition of the proof is difficult to follow and lacks organization.  
- The paper does not adequately address critical aspects beyond the proof, which are essential for its suitability in the machine learning domain.  
- The paper lacks concrete applications for its results, limiting its practical relevance.  
- The assumption that all languages in the list are infinite is seen as overly restrictive and potentially problematic.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by condensing the explanations of the proofs, particularly in sections discussing Theorem 2.1 and the membership test. A more direct presentation of the arguments would enhance readability. Additionally, we suggest that the authors adopt a Definition/Statement/Proof structure and incorporate an algorithmic approach with Inclusion Queries and Membership queries. Specifically, they should define a suitable notion of criticality, state the main theorem in relevant terms, outline the protocol for language generation, and prove correctness using criticality properties while removing the need for inclusion queries. We also urge the authors to provide concrete applications of their results, particularly in relation to language identification and large language models (LLMs). Addressing the implications of their findings for common families of languages and discussing potential complexity bounds would strengthen the paper's contribution. Finally, we advise the authors to explicitly justify the assumption that all languages are infinite, as this is a significant limitation that could affect the applicability of their results.