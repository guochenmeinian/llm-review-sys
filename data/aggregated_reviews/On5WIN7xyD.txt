ID: On5WIN7xyD
Title: Observational Scaling Laws and the Predictability of Langauge Model Performance
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 6, 7, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 2, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents observational scaling laws to align computing scaling laws from various model families trained on different recipes, enabling performance predictions without additional model training. The authors utilize PCA decomposition of model performance across benchmarks to derive a universal effective compute quantity, facilitating predictions of emergent abilities and agent performance across models. The findings suggest that three principal components explain significant variance in performance, allowing for accurate extrapolation of capabilities, including those of models like GPT-4.

### Strengths and Weaknesses
Strengths:
1. The paper offers an elegant solution to the misalignment of scaling laws due to varying training recipes.
2. It provides a comprehensive analysis across 21 model families and various benchmarks, demonstrating strong extrapolation performance.
3. The framework is well-structured and contributes valuable insights into the scaling of LLM capabilities.

Weaknesses:
1. The predictability of emergent abilities appears overclaimed, as the feasibility of reliable predictions is questionable, particularly with the provided fitting data and extrapolation results.
2. The assumption that different model families primarily differ in the speed of ability acquisition may not hold true for all training configurations.
3. The lack of uncertainty measures or confidence intervals for predictions raises concerns about the robustness of the findings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the relationship between model performance and surrogates of compute, particularly addressing the misalignment between equations. Additionally, we suggest providing a more thorough analysis of how training data distribution influences the applicability of observational scaling laws. It would be beneficial to include uncertainty measures for predictions to enhance the reliability of the results. Finally, we encourage the authors to explore the implications of their findings on evaluating pretraining data quality and to consider the effects of fine-tuning in future work.