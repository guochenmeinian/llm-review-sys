ID: EQhLbuitns
Title: HourVideo: 1-Hour Video-Language Understanding
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 8, 6, 9, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents HourVideo, a benchmark dataset for long-duration video-language understanding tasks, comprising 500 egocentric videos from the Ego4D dataset, spanning 20 to 120 minutes, and featuring 13,000 high-quality multiple-choice questions. The authors propose a task suite that includes summarization, perception, visual reasoning, and navigation, evaluating models on their performance in these areas. Initial results indicate that state-of-the-art multimodal models significantly underperform compared to human baselines, revealing a critical research gap in long-term video understanding.

### Strengths and Weaknesses
Strengths:
- Comprehensive Benchmark: The dataset covers a wide range of tasks requiring long-term comprehension, making it a valuable resource for advancing multimodal model capabilities.
- High-Quality Annotations: The questions are meticulously crafted to ensure they require information synthesis across multiple temporal segments, enhancing the benchmark's robustness.
- Diverse Task Suite: The inclusion of tasks like visual reasoning and navigation tests a modelâ€™s cognitive abilities in real-world scenarios, providing a holistic evaluation.

Weaknesses:
- Performance Metrics: The reliance on multiple-choice questions may oversimplify evaluation, not fully capturing the nuanced understanding required for long-term video comprehension.
- Limited Evaluations: The benchmark lacks evaluations of open-sourced Video LLMs, particularly those capable of processing long videos with memory, such as MovieChat.
- Missed Ablation Studies: The authors do not evaluate QAs independently or assess the performance of models using only a subset of frames from the videos, which could provide additional insights.

### Suggestions for Improvement
We recommend that the authors improve the evaluation metrics by incorporating more complex question types beyond multiple-choice to better capture nuanced understanding. Additionally, the authors should evaluate more open-sourced Video LLMs, particularly those that can handle long video inputs. We also suggest conducting ablation studies to assess the impact of independent QA evaluations and to explore the performance of models using sparsely sampled frames from the videos. Finally, we encourage the authors to clarify the comparison of human and model performance in the abstract and ensure that related works are adequately discussed in the paper.