ID: ww62xltEfB
Title: A provable control of sensitivity of neural networks through a direct parameterization of the overall bi-Lipschitzness
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 6, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel bi-Lipschitz neural network architecture (BLNN) that allows direct control and parameterization of Lipschitz and inverse Lipschitz constants using convex neural networks and the Legendre-Fenchel transformation. The authors provide a theoretical framework, empirical evaluations, and experiments demonstrating the model's utility in tasks such as uncertainty estimation and monotone regression, showing competitive performance compared to existing methods.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and clearly written, with a comprehensive background and related work presented in Appendix A.
- The originality of the approach is notable, as it utilizes convex neural networks and Legendre-Fenchel transformation to parameterize bi-Lipschitzness, which is distinct from existing methods.
- The authors provide detailed proofs and analyses, showcasing the expressive power of their networks and competitive experimental results across various tasks.

Weaknesses:
- The computational cost of the proposed approach may limit its applicability, and the authors should provide experiments comparing computational costs and training times for both architectures.
- There is insufficient exploration of scalability to larger networks or complex datasets, such as TinyImageNet.
- The paper lacks a detailed analysis of time and space complexity compared to traditional networks and does not adequately address hyperparameter sensitivity or scenarios where theoretical guarantees might not hold.

### Suggestions for Improvement
We recommend that the authors improve the paper by including experiments that assess computational costs and training times for both architectures. Additionally, a detailed analysis of time and space complexity compared to traditional networks would be beneficial. The authors should also explore scalability to larger datasets and provide a more comprehensive discussion on hyperparameter sensitivity. Finally, addressing potential extensions to other network architectures beyond feedforward networks would enhance the paper's contributions.