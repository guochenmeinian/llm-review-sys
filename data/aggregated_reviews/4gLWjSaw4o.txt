ID: 4gLWjSaw4o
Title: Recovering from Out-of-sample States via Inverse Dynamics in Offline Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 7, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a solution to the state distributional shift problem in offline reinforcement learning by introducing Out-of-sample State Recovery (OSR) and its enhancement, OSR-10-large-noise (OSR-10-l). The authors propose two methods, OSR and OSR-v, which utilize a learned inverse dynamics model to regularize the policy and address the challenge of recovering state distributions during testing. The theoretical framework supports OSR's effectiveness in aligning the transited state distribution with the offline dataset at out-of-sample states. Extensive experiments indicate that these methods achieve state-of-the-art performance across various offline RL benchmarks, including an out-of-sample MuJoCo benchmark. Additionally, the authors demonstrate that both OSR-10 and RORL improve their performance by 12.1% and 13.1%, respectively, when trained with larger perturbations in adversarial environments, enhancing the generalization capability and robustness of the learned agent against adversarial attacks. OSR-10-l significantly outperforms RORL in both observation perturbation benchmarks and OOS MuJoCo benchmarks.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and logically organized, making it easy to follow.
- It addresses a significant issue in offline reinforcement learning, linking state distribution correction with robust offline RL.
- The proposed methods demonstrate state-of-the-art performance in both general offline RL benchmarks and the out-of-sample MuJoCo benchmark.
- The effectiveness of OSR in recovering from specific perturbations is validated through visualizations.
- Comprehensive experiments conducted within a limited timeframe show significant improvements in robustness through larger perturbation scales.
- Clear results indicate OSR-10-l's superiority over RORL.

Weaknesses:
- A more comprehensive ablation study is needed, particularly exploring the performance of OSR with $\beta=0$ and comparisons with CQL and CQL+BC.
- Concerns exist regarding Eq (5), which assigns the same scale of Gaussian noise across dimensions; normalizing states by their mean and standard deviation may be beneficial.
- The paper lacks sufficient discussion of prior works in robust RL and robust offline RL, particularly in relation to the out-of-sample MuJoCo benchmark.
- The literature review does not include prior offline RL works like ROMI, which should be discussed for a more comprehensive review.
- Some technical details, such as derivations, need clarification and should be included in the appendix.
- Certain equations in the text are not equivalent, which requires revision.

### Suggestions for Improvement
We recommend that the authors improve the paper by conducting a more comprehensive ablation study, including scenarios such as OSR with $\beta=0$, and comparisons with CQL+BC, CQL+BC with state noise, and CQL+BC with both state and action noise. Additionally, normalizing the states by their mean and standard deviation could enhance the robustness of the method. It is also essential to include a subsection in the related work that focuses on prior works in robust RL and robust offline RL, and to compare the proposed methods against existing robust offline RL approaches, such as RORL, in the out-of-sample MuJoCo benchmark. Furthermore, we recommend including the derivations in the appendix to clarify technical details and revising Line 212 to address the issue of equivalent solutions, ensuring that the equations presented are accurate and clear. Finally, further discussion and comparisons with prior offline RL works that utilize a reverse model would strengthen the paper's contributions.