ID: yzZbwQPkmP
Title: SparseProp: Efficient Event-Based Simulation and Training of Sparse Recurrent Spiking Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 9, 7, 4, 4, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an efficient event-based algorithm named *SparseProp* for simulating and training sparse Spiking Neural Networks (SNNs). The method leverages the sparsity of the network to reduce computational costs from O(N) to O(log(N)) per spike, avoiding the need to iterate through all neurons. The authors propose two main contributions: a change in the temporal reference frame using phase representation and an efficient data representation aligned with the sparse connectivity of neurons. The algorithm demonstrates promising results, particularly in large-scale simulations. Additionally, SparseProp utilizes a priority queue to manage neuron states, enhancing simulation efficiency under specific conditions, particularly with spontaneous firing. However, the necessity of the priority queue in more common SNN scenarios, where neurons fire primarily due to excitatory inputs, is questioned.

### Strengths and Weaknesses
Strengths:
- The authors introduce a novel algorithm that effectively utilizes sparsity, achieving a high efficiency-to-complexity ratio.
- The exposition is generally clear, with well-defined algorithms for both sparse and non-sparse simulations, facilitating reader comprehension.
- Experimental results indicate significant improvements in simulation efficiency, with the ability to simulate large networks effectively.
- The authors provide a clear explanation of SparseProp's mechanics and its intended efficiency improvements in specific conditions.

Weaknesses:
- The paper lacks comparisons of the proposed method with conventional approaches in terms of result quality.
- The focus on homogeneous networks leaves a gap in understanding the algorithm's applicability to heterogeneous networks, which could be addressed with additional numerical results.
- The discussion on the assumptions regarding neuron firing dynamics and the implications of using phase representation is insufficient.
- The section on training lacks experimental validation, making claims about its effectiveness vague.
- The necessity of the priority queue in typical SNN scenarios is questioned, suggesting it may not be needed when spikes are the primary trigger for neuron firing.
- SparseProp may be less efficient than a simpler O(K) simulation method under common conditions, indicating a potential limitation in its applicability.

### Suggestions for Improvement
We recommend that the authors improve the comparison between the proposed method and conventional methods by including quality assessments of the results. Additionally, isolating the contributions of the two main innovations would enhance clarity. We suggest providing more references to foundational works, particularly regarding the phase representation and its implications. It would be beneficial to include numerical results for heterogeneous networks to strengthen the paper's claims. Furthermore, we advise revising the title to remove "training of RSNN" if the section lacks substantial experimental content. Lastly, addressing the assumptions related to external inputs and their impact on neuron dynamics in greater detail would enhance the robustness of the study. We also recommend that the authors clarify SparseProp's applicability by explicitly identifying scenarios where the O(K) simulation method cannot be utilized but SparseProp remains advantageous, and address concerns regarding the necessity of the priority queue in typical SNN settings.