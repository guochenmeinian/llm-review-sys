ID: pvEkYbUPVW
Title: Measuring Faithful and Plausible Visual Grounding in VQA
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new metric for evaluating the visual grounding abilities of VQA models, termed Faithful and Plausible Visual Grounding (FPVG). The authors propose that a VQA system's answer should remain unchanged when cropped to relevant image regions, while it should vary when cropped to irrelevant regions. The paper includes experiments demonstrating the effectiveness of FPVG across multiple VQA architectures and provides code for its implementation.

### Strengths and Weaknesses
Strengths:  
- The proposed metric FPVG is simple, intuitive, and well-articulated, potentially adding significant value to the VQA research community.  
- The paper is well written, with solid experimental backing for its claims.  
- The metric offers insights into model performance that are not captured by existing metrics, particularly regarding out-of-domain (OOD) performance.

Weaknesses:  
- There is a lack of novelty, as existing metrics for assessing visual grounding capabilities already exist, and the research questions may not significantly advance the field.  
- The metric's requirement for answers to change when cropped to irrelevant regions may be overly harsh, potentially penalizing models unnecessarily.  
- The writing and flow could be improved, particularly in the presentation of results and clarity in tables.  
- A detailed comparison with existing metrics focusing on faithfulness and plausibility is lacking, which would strengthen the paper's contribution.

### Suggestions for Improvement
We recommend that the authors improve the clarity and flow of the writing, particularly in the presentation of results. Simplifying the presentation of results and explicitly pointing out key takeaways in the results section would enhance understanding. Additionally, we suggest justifying the necessity of the requirement for answers to change when cropped to irrelevant regions, or exploring its removal to see if it affects the correlation with OOD performance. Finally, we encourage the authors to include a discussion comparing their metric with existing ones to help researchers decide which metrics to use or optimize for their models.