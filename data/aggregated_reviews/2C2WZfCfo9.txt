ID: 2C2WZfCfo9
Title: DOSE: Diffusion Dropout with Adaptive Prior for Speech Enhancement
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 7, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel model-agnostic approach called DOSE for speech enhancement using denoising diffusion probabilistic models (DDPMs). The authors propose two efficient condition-augmentation techniques to address the challenge of incorporating condition information into DDPMs, aiming to improve both the quality and efficiency of speech generation. Additionally, the paper discusses the challenges of low signal-to-noise ratio (SNR) data in diffusion models and proposes an adaptive strategy for improving performance, specifically through the use of an adaptive alpha predictor to balance the weights of the original condition factor and generated samples based on their quality. The authors emphasize the choice of DiffWave as the foundational architecture for fair comparisons with other models, ensuring minimal alterations to baseline settings. The paper demonstrates significant improvements in high-quality and stable speech generation through comprehensive experiments on benchmark datasets.

### Strengths and Weaknesses
Strengths:
1. The proposed method shows good generalization ability with strong performance in both matched and mismatched scenarios.
2. The paper provides detailed experimental results and thorough comparisons with existing diffusion enhancement and deterministic methods.
3. The introduction effectively outlines the problem of condition collapse in generative speech enhancement, and the writing is clear and well-structured.
4. The authors provide a thoughtful analysis of low SNR challenges and propose sensible solutions.
5. The choice of DiffWave as a widely recognized model ensures a fair comparison across algorithms.

Weaknesses:
1. Clarity issues arise regarding the dropout technique and its implications on the training and testing processes, particularly the lack of explicit division between them.
2. The dataset section lacks details about the noise characteristics in the Chime and Voicebank corpora.
3. The paper does not adequately address limitations and ethical considerations, and the conclusion mentions sensitivity to dropout probability and sampling indices without sufficient analysis.
4. The adaptive alpha predictor lacks generalizability, showing limited performance improvement over existing models.
5. The paper does not currently include comparisons with other model architectures, which could enhance understanding of generalization capabilities.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the dropout technique by explicitly delineating the training and testing processes. Additionally, providing more details about the noise characteristics in the datasets would enhance the reproducibility of the experiments. We suggest including qualitative analyses in the experimental section and conducting an ablation study to analyze the effectiveness of the proposed techniques. Furthermore, we recommend that the authors improve the generalizability of the adaptive alpha predictor and explore additional techniques such as data augmentation and adversarial training. We encourage the authors to delve deeper into the challenges of low SNR data in the supplementary materials and provide a roadmap for future research. Lastly, we suggest including experimental results involving other architectures in the revised version to better address concerns regarding model comparisons.