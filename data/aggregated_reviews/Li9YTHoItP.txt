ID: Li9YTHoItP
Title: Perception of Knowledge Boundary for Large Language Models through Semi-open-ended Question Answering
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 7, 8, 3, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the knowledge boundaries of Large Language Models (LLMs) through semi-open-ended questions, revealing limitations in their understanding. The authors propose a novel method utilizing an auxiliary model to identify low-probability ambiguous answers, constructing a dataset of 953 questions across 32 domains. Findings indicate that GPT-4 often produces unqualified answers and struggles with self-evaluation, underscoring the need for better detection of LLM knowledge boundaries.

### Strengths and Weaknesses
Strengths:  
1. The work introduces an innovative approach to evaluate LLMs using semi-open-ended questions, revealing limitations in traditional close-ended question methods.  
2. Experiments demonstrate significant improvements in understanding LLM knowledge boundaries, particularly in areas where models like GPT-4 may hallucinate, enhancing evaluation reliability.  
3. The paper is well-written and easy to follow.  

Weaknesses:  
1. The evaluation is limited to GPT-4 Turbo, which may not adequately represent overall LLM performance. The authors should analyze other mainstream models like Claude or LLaMA.  
2. Practical implications of the findings are not fully demonstrated; specific examples of applications should be provided to illustrate how these findings can enhance LLM reliability. Further explanation on improving training data or algorithms is needed.  
3. The paper notes GPT-4's poor performance on semi-open-ended questions but lacks a thorough analysis of the types of errors made and their causes.  

### Suggestions for Improvement
We recommend that the authors improve the evaluation scope by including other large language models such as Claude or LLaMA to provide a more comprehensive performance analysis. Additionally, we suggest providing specific examples of practical applications to illustrate how the findings can enhance LLM reliability. A detailed analysis of the types of errors made by GPT-4 on semi-open-ended questions should also be included. Finally, clarifying the meaning of "semi-open-ended questions" and providing a statistical overview of their prevalence in real-world applications would strengthen the paper.