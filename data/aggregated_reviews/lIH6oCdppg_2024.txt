ID: lIH6oCdppg
Title: On the Role of Attention Masks and LayerNorm in Transformers
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 4, 6, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the role of attention masks and layer normalization (LayerNorm) in addressing the rank collapse issue in transformer models. The authors conclude that as long as there is a token that all other tokens can attend to over a fixed number of layers, exponential rank collapse is inevitable. They demonstrate that local attention masks can mitigate this collapse and emphasize the significant role of LayerNorm in self-attention dynamics. The paper includes a thorough analysis and numerical experiments related to self-attention dynamics, expressivity, and versatility.

### Strengths and Weaknesses
Strengths:
- The paper provides a rigorous mathematical analysis of attention masks and LayerNorm, enhancing understanding of transformer models.
- A graph-theoretic approach is employed, allowing for generalization to sparse and causal attention.
- The authors present a compelling counterexample of collapse, showing that self-attention dynamics can stably process a rich set of sequences.
- The theoretical findings are supported by numerical experiments.

Weaknesses:
- The practical implications of the findings for future transformer designs remain unclear.
- The connection between the findings and the performance of existing large language models (LLMs) is not well-explained, particularly regarding the use of sparse attention.
- The numerical experiments are limited, relying on only 32 samples, which raises concerns about their robustness.

### Suggestions for Improvement
We recommend that the authors improve the discussion on how their findings can inform design choices in future transformer models and clarify the relationship between their results and the performance of existing LLMs. Additionally, the authors should consider expanding their numerical experiments to include more samples and deeper architectures, as modern transformers can exceed 100 layers. It would also be beneficial to provide clearer analysis on the differences in self-attention dynamics between causal and bidirectional attention. Finally, we suggest including a more detailed discussion of omitted architectural components, such as skip connections and positional encoders, to strengthen the paper's contributions.