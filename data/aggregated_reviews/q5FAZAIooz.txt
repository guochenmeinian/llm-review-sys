ID: q5FAZAIooz
Title: Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DIFF-FOLEY, a method for synchronized video-to-audio synthesis using a latent diffusion model (LDM). The authors propose a contrastive audio-visual pre-training (CAVP) approach to enhance the alignment of audio and video features. Key contributions include audio-visual contrastive pre-training, latent diffusion modeling, a temporal split & merge augmentation technique, and a combination of classifier guidance and classifier-free guidance. The authors demonstrate improved performance over previous methods on several metrics, although some limitations in comparison and evaluation are noted.

### Strengths and Weaknesses
Strengths:
- The paper makes significant contributions, particularly through the novel CAVP approach, which enhances audio-visual alignment.
- The writing is clear, with thorough discussions and well-presented quantitative results.
- The proposed method shows superior audio-visual synchronicity and improved inference time compared to existing methods.

Weaknesses:
- The comparison is limited to only two previous methods, raising questions about the robustness of the results; additional comparisons, such as with FoleyGAN, are necessary.
- The temporal split & merge augmentation lacks sufficient explanation and context, and previous similar techniques should be referenced.
- The evaluation metrics used, particularly FID and KL divergence, show the method's inferiority in some areas, and subjective evaluation metrics are absent.
- The analysis of the synchronicity classifier is insufficient, with unclear details on its construction and performance metrics.

### Suggestions for Improvement
We recommend that the authors improve the comparison by including additional relevant methods, such as FoleyGAN, to strengthen the validation of their results. Clarifying the temporal split & merge augmentation with references to similar techniques would enhance the discussion. We suggest including subjective evaluation metrics to provide a clearer picture of the generated audio's realism. Additionally, a more detailed analysis of the synchronicity classifier, including its construction and performance metrics, is necessary for a comprehensive understanding. Lastly, expanding the discussion on the broader societal impacts of the technology would enrich the paper's context.