ID: MRO2QhydPF
Title: Reinforcement Learning with Adaptive Regularization for Safe Control of Critical Systems
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 3, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the RL-ACR algorithm, which addresses safe reinforcement learning (RL) exploration by combining a safety regularizer with a conventional RL module through a learned "focus network." The authors demonstrate that their method can learn a safe policy with zero constraint violations and outperform model predictive control (MPC) in performance across various applications. The algorithm is particularly aimed at single-life settings where safety is critical during training. Additionally, the authors propose a novel approach that combines two independent policies using a safety regularizer in reinforcement learning (RL), employing the Soft Actor-Critic (SAC) algorithm. They claim that their method guarantees performance at least as good as the regularizer while improving safety, although challenges in establishing theoretical closed-loop stability guarantees are acknowledged.

### Strengths and Weaknesses
Strengths:
1. The approach is sound, with proofs indicating that the combined policy is unbiased if the RL module converges to the optimal policy.
2. The writing is clear, and the paper effectively illustrates the interaction of different components with well-designed figures and algorithms.
3. The empirical results show that the proposed method outperforms baselines in terms of safety and return.
4. The idea of weighting two independent policies introduces novel elements to the work.
5. The proposed method demonstrates improved safety compared to standard SAC, particularly in terms of the number of failures.

Weaknesses:
1. The assumption that the MPC policy is safe is overly strict and not explicitly stated, raising concerns about the model's accuracy and potential constraint violations.
2. The reliance on the assumption of SAC's convergence to the optimal policy is not sufficiently established in the manuscript.
3. The performance guarantees are limited, achieving results no better than SAC while not worse than the regularizer, which raises questions about the added complexity of the proposed method.
4. The computational cost is high, limiting the algorithm's applicability to nonlinear or high-dimensional inputs and unknown models.
5. There is no theoretical guarantee of closed-loop stability, which is crucial for control applications.
6. The framework's reliance on an accurate model for safety raises questions about its robustness in more complex tasks.

### Suggestions for Improvement
We recommend that the authors improve the theoretical foundation by providing a convergence result for the RL-ACR policy to the optimal policy, addressing the concerns regarding the assumptions made in Theorem 1. Additionally, we suggest including a theoretical guarantee of closed-loop stability, as this is essential for applications in dynamical systems. It would also be beneficial to clarify the computational resources and optimization packages used for solving the MPC problem, as well as to explore comparisons with more recent safe RL baselines. Furthermore, we recommend that the authors improve the clarity of Theorem 1 by explicitly referencing the proof of SAC's convergence to the optimal policy. We also suggest providing a more quantitative assessment of the performance improvements, moving beyond the qualitative terms "better" and "higher." Finally, we advise the authors to address the limitations of the method more thoroughly in the conclusion, particularly regarding the balance between safety and performance compared to existing methods, and to quantify the deviation of the current policy from the safety regularizer as a function of the mixing parameter $\beta$ to better assess safety levels.