ID: ginTcBUnL8
Title: SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 8, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SimMTM, a pre-training framework for masked time-series modeling that reconstructs time series by aggregating information from multiple masked series. The approach leverages complementary information from multiple series to enhance reconstruction, with a loss function combining reconstruction and contrastive losses. The authors provide extensive empirical evaluations on time series classification and forecasting tasks, demonstrating the effectiveness of the learned representations.

### Strengths and Weaknesses
Strengths:
- The proposed method is sound, novel, and well-motivated, effectively addressing the challenges of masked modeling in time-series data.
- The empirical evaluation is comprehensive, showcasing significant performance improvements across various tasks.
- The write-up is generally clear, with illustrative figures aiding understanding.

Weaknesses:
- Some equations, particularly Eq 4 and Eq 7, lack clarity and precision, leading to confusion regarding notation and intended meaning.
- The presentation quality is inconsistent, with issues in formula representation and notation that hinder comprehension.
- The comparison with prior literature is insufficient, lacking clear advantages of the proposed model over existing methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Eq 4 by summing over $z'$ instead of $s'$, and revise Eq 7 to more accurately reflect the intended intra-similarity maximization. Additionally, we suggest enhancing the consistency of notation throughout the paper, particularly in Equation 3, to avoid confusion. The authors should also provide a more thorough comparison with existing literature, specifically addressing the advantages of their approach over methods such as those referenced in [1, 2]. Finally, we encourage a more detailed discussion on the implications of using large-scale pre-training in forecasting, particularly regarding fairness and computational overhead during training and inference.