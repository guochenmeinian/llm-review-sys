ID: 3bdJJaxiwt
Title: Can LLMs Verify Arabic Claims? Evaluating the Arabic Fact-Checking Abilities of Multilingual LLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 6, 5
Original Confidences: 4, 3

Aggregated Review:
### Key Points
This paper presents an evaluation of whether LLMs can accurately verify Arabic claims framed as Yes/No responses. The authors motivate their study by highlighting the lack of definitive research on Arabic fact-checking, despite existing evaluations in English and Chinese. They utilize several state-of-the-art LLMs and prompting techniques, including zero-shot, English Chain-of-Thought (CoT), Self-consistency, and Cross-Lingual Prompting (CLP), using the X-fact dataset with 730 false claims and 41 true claims. The evaluation metric is accuracy across all class labels, with findings indicating that CLP is the most effective technique and that llama-3-8b achieves the highest performance.

### Strengths and Weaknesses
Strengths:
- The paper provides a detailed explanation of dataset construction and the implementation of various prompting approaches.
- The first five sections are well-written, clear, and concise.

Weaknesses:
- The empirical section contains redundant information, with excessive detail on prompt method accuracy that could be condensed.
- The dataset is imbalanced, and the authors only report accuracy, neglecting Precision, Recall, and F1 scores, which diminishes the impact of the results.
- The evaluation sections restate information from Table 1 without offering additional insights, and Figures 3, 4, and 5 do not contribute new information.
- Few-shot prompts are absent from the evaluation.

### Suggestions for Improvement
We recommend that the authors improve the conciseness of the empirical section by reducing redundancy and focusing on key insights, potentially showcasing LLM responses instead. We suggest replacing Figure 3 with a more suitable bar chart. Additionally, we urge the authors to report Precision, Recall, and F1 scores alongside accuracy to provide a more comprehensive evaluation of their results. Including a confusion matrix would also enhance the clarity of the findings. Furthermore, we recommend incorporating few-shot prompts into the evaluation and utilizing space saved from the empirical section to discuss the cost-vs-accuracy tradeoffs of different prompting styles, such as accuracy per-token metrics.