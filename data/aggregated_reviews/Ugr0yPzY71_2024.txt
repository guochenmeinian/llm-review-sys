ID: Ugr0yPzY71
Title: Faster Repeated Evasion Attacks in Tree Ensembles
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 6, 5, 7, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to accelerate the robustness verification of tree-based classifiers by focusing on a subset of relevant features, which often suffices for generating adversarial examples. The authors propose an algorithm that first identifies these relevant features and then attempts to generate adversarial examples using them. If unsuccessful, the algorithm expands to consider all features. The experimental evaluation demonstrates significant speed-ups in robustness verification, achieving up to 12x for gradient-boosted models and 6x for Random Forests across multiple datasets. Additionally, the paper addresses the performance of adversarial attacks against tree ensemble models, particularly in tabular data, although concerns arise regarding the understanding of adversarial examples and the choice of baselines for comparison.

### Strengths and Weaknesses
Strengths:
- **Original observation and algorithm**: The identification of relevant features for adversarial example generation is novel and optimizes existing verification algorithms.
- **Theoretical guarantees**: The paper provides two theorems ensuring that if an adversarial example is found using relevant features, it will also work with all features, and that the algorithm will find an adversarial example if it exists.
- **Comprehensive experimental evaluation**: The results convincingly show the speed-up achieved by focusing on relevant features, addressing a significant challenge in verifying tree-based models.
- The paper tackles an interesting and relevant problem in adversarial machine learning and demonstrates a willingness to address reviewer concerns and improve based on feedback.

Weaknesses:
- **Limited threat model**: The focus on the $\ell_\infty$ threat model restricts the applicability of the findings; exploring $\ell_1$ and $\ell_2$ attackers could enhance the paper's impact.
- **Lack of comparison with other efficiency methods**: The paper does not compare its approach with existing methods that improve robustness verification efficiency, particularly those involving tree-based classifiers that allow for polynomial-time verification.
- There is a lack of clarity regarding adversarial examples in tabular data, particularly in Table 1.
- The comparison of only two attack methods is insufficient, raising questions about the paper's alignment with community standards.
- Mischaracterization of the computational complexity of generating adversarial examples, which may mislead readers.

### Suggestions for Improvement
We recommend that the authors improve the paper by:
- Acknowledging the limitation of focusing solely on the $\ell_\infty$ threat model and discussing the potential for extending the approach to $\ell_1$ and $\ell_2$ norms.
- Including a comparison with existing methods that enhance robustness verification efficiency, particularly those related to tree-based classifiers, to highlight the pros and cons of their approach.
- Providing a more comprehensive discussion of related works, particularly regarding neural networks and adversarial examples in various contexts, to strengthen the paper's contribution.
- Enhancing the clarity of the presentation, especially in sections discussing the algorithm and its theoretical underpinnings, to improve reader comprehension.
- Improving the explanation of adversarial examples in Table 1 by providing specific data examples and corresponding adversarial instances.
- Considering more baseline comparisons to align with community expectations, specifying which attack methods could be relevant for tree ensembles.
- Clarifying the discussion on problem hardness, ensuring accurate terminology and context regarding the computational complexity of generating adversarial examples.