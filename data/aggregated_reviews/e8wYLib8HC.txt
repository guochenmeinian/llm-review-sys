ID: e8wYLib8HC
Title: Transformer Working Memory Enables Regular Language Reasoning And Natural Language Length Extrapolation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents RegularGPT, a modified transformer architecture aimed at improving generalization on regular languages like PARITY. The authors utilize weight sharing, adaptive scaling of layer depth by $O(\log n)$, and masked attention to process non-overlapping fixed-size input chunks. The model demonstrates effective recognition of regular languages through a divide-and-conquer approach operating in $O(\log n)$ steps. Experimental results indicate that RegularGPT outperforms baseline transformers on synthetic regular language tasks but struggles with Modular Arithmetic and shows mixed results in natural language modeling, primarily evaluating perplexity on the last token.

### Strengths and Weaknesses
Strengths:
- The theoretical explanation of RegularGPT's functionality is thorough and engaging.
- The method is conceptually simple and well-motivated for addressing regular languages.
- Experimental results show that RegularGPT outperforms baseline transformers on various regular language tasks.

Weaknesses:
- The experimental methodology is limited, particularly in natural language tasks, where results appear weaker than prior work.
- The evaluation focuses narrowly on perplexity of the last token, lacking broader insights into model performance.
- The choice of tasks and the limited range of evaluations may not adequately showcase the model's strengths and weaknesses.

### Suggestions for Improvement
We recommend that the authors expand the experimental methodology to include a wider variety of tasks, particularly in natural language processing, to better assess RegularGPT's capabilities. Additionally, we suggest that the authors explore different chunking methods (e.g., static, dynamic, overlapping) to understand their impact on performance. It would also be beneficial to provide a more detailed analysis of the training process and clarify the relationship between the tasks and the Chomsky hierarchy. Finally, we encourage the authors to consider evaluating the model's performance using full sequence perplexity rather than just the last token to gain a more comprehensive understanding of its learning efficacy.