ID: Ku4jYr8W0N
Title: Computationally Efficient Laplace Approximations for Neural Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 6, 6
Original Confidences: 3, 3

Aggregated Review:
### Key Points
This paper presents two novel methods for approximating the Hessian matrix in Laplace approximations for neural networks: Greedy-Laplace and Gradient-Laplace. These methods aim to alleviate computational bottlenecks in inverting large Hessian matrices by selecting parameter subsets that significantly influence posterior variance. The authors provide a clear motivation for sparse Hessian approximations and empirical evidence demonstrating that their methods outperform existing subnetwork-based Laplace approximations on synthetic datasets. The topic is timely and relevant, grounded in Bayesian statistics, and addresses the need for computationally tractable methods.

### Strengths and Weaknesses
Strengths:
- Introduces Greedy-Laplace and Gradient-Laplace, enhancing computational efficiency for uncertainty quantification.
- Reduces computational complexity, making it applicable to large neural networks.
- Includes rigorous benchmarking against state-of-the-art methods across multiple datasets.
- Addresses significant applications in healthcare and autonomous systems.
- Outperforms existing methods in accuracy and credible interval coverage.

Weaknesses:
- Complex mathematical details may challenge non-experts.
- Limited demonstrations of real-world applications.
- Minimal exploration of potential failure points for the methods.
- The gradient-based method may introduce additional computational overhead.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the trade-offs between computational complexity and approximation quality regarding posterior variance. It would be beneficial to include formal error bounds for the approximation using the Moore-Penrose inverse compared to the true inverse of the precision matrix. Additionally, a more detailed exploration of the theoretical properties of the Greedy-Laplace subset selection approach is warranted, particularly regarding potential suboptimal selections. To enhance practical relevance, we suggest testing the methods on real-world datasets and larger neural network architectures, such as convolutional neural networks or transformers. The authors should also address the scalability of the proposed methods to models with millions of parameters and compare runtime with other efficient uncertainty quantification methods. Furthermore, clarifying how Greedy-Laplace and Gradient-Laplace compare to popular Bayesian neural network approaches, such as variational inference or MCMC-based methods, would be valuable. Lastly, providing more details on when Gradient-Laplace is computationally preferable to Greedy-Laplace would aid practitioners in method selection.