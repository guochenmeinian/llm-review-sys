ID: cvaSru8LeO
Title: Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 6, 5, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents three new synthetically generated VQA evaluation benchmarks aimed at assessing the spatial reasoning capabilities of multimodal language models (MLMs) and visual language models (VLMs). The benchmarks include tasks related to spatial relationships, navigation, position understanding, and counting. The authors demonstrate that VLMs exhibit limited performance in tasks requiring detailed visual understanding and reasoning. Key findings indicate that VLMs rely heavily on textual information rather than visual cues, and they perform better than LLMs when utilizing spatial text information.

### Strengths and Weaknesses
Strengths:  
- Evaluating the spatial reasoning capabilities of state-of-the-art VLMs is impactful and relevant, highlighting significant limitations in these models.  
- The experimental analysis is comprehensive, involving both open-source and proprietary models, and the paper is well-written and structured, enhancing readability.

Weaknesses:  
- The use of synthetic data may introduce confounding factors unrelated to spatial understanding, making it difficult to disentangle OCR performance from spatial reasoning.  
- Claims regarding the performance gap between models and human levels lack concrete metrics for substantiation.  
- The rationale for using synthetic data over real image datasets is not adequately justified, and the benchmarks may not reflect real-world applications.  
- Some experimental settings and results are unclear or missing, raising questions about the validity of comparisons made between different models.

### Suggestions for Improvement
We recommend that the authors improve the justification for using synthetic data by discussing its necessity compared to existing real image datasets like GQA or Visual Genome. Additionally, providing specific human performance metrics to substantiate claims about model performance would strengthen the paper. We suggest exploring different sampling strategies or prompting techniques to yield insights into model performance, and clarifying the configurations used in experiments, such as the specific top-p and temperature settings. Finally, clearly specifying the size of each VQA evaluation benchmark in terms of data points would enhance the understanding of the evaluations conducted.