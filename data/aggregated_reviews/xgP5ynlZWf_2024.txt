ID: xgP5ynlZWf
Title: RestoreAgent: Autonomous Image Restoration Agent via Multimodal Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 5, 6, 8, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents RestoreAgent, an innovative image restoration pipeline that autonomously identifies degradation types and determines the optimal sequence of restoration tasks using multimodal large language models (MLLM). The authors analyze the limitations of all-in-one restoration models and task-specific models, demonstrating that RestoreAgent outperforms existing methods and human experts in handling complex degradations. The paper includes a method for constructing training data and showcases extensive experimental results validating the effectiveness of the proposed approach.

### Strengths and Weaknesses
Strengths:
1. The paper provides a comprehensive analysis of the challenges in image restoration, particularly regarding the execution order and model selection for multiple degradations.
2. RestoreAgent achieves state-of-the-art performance across various benchmarks and degradation tasks.
3. The system's ability to dynamically adapt to different degradation types and levels is a significant advancement in the field.
4. The presentation is clear, and the writing is well-organized.

Weaknesses:
1. There are incomplete descriptions regarding data construction, particularly concerning the degradation order and the composition of the paired training data.
2. The inference time for images of varying resolutions is not reported, raising concerns about the computational efficiency of the proposed method.
3. The potential for performance degradation on prior tasks when adapting to new tasks is not addressed, which could indicate a catastrophic forgetting issue.
4. The definitions of "ranking" and "balanced" in Table 1 are unclear, requiring further clarification.

### Suggestions for Improvement
We recommend that the authors improve the descriptions of data construction, particularly clarifying the degradation order and the specifics of the paired training data. Additionally, we suggest reporting the running time for input images of various resolutions to provide insights into computational efficiency. To address concerns regarding catastrophic forgetting, the authors should evaluate and report the performance of the fine-tuned model on previous tasks. Finally, clearer definitions for "ranking" and "balanced" in Table 1 would enhance reader comprehension.