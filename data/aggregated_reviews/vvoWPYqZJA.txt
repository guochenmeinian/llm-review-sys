ID: vvoWPYqZJA
Title: InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 8, 6, 6, 5, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents InstructBLIP, a vision-language instruction tuning framework that enhances the BLIP-2 model's instruction-following capabilities. It utilizes a diverse set of instruction data to train a multimodal LLM, transforming 26 datasets into an instruction-tuning format across 11 task categories. The authors propose an instruction-aware Q-Former, demonstrating state-of-the-art zero-shot performance on various held-out vision-language tasks. The study provides a comprehensive analysis of instruction tuning's effectiveness in the vision-language domain and evaluates InstructBLIP's performance across various benchmarks, including the LVLM-eHub Arena. The authors assert that their model demonstrates robust capabilities, although concerns about overfitting to academic datasets and the stability of leaderboard rankings are raised. The paper also discusses the architecture used for multi-turn conversations and the implications of training data overlap between TextCaps and TextVQA.

### Strengths and Weaknesses
Strengths:
- The paper offers a thorough examination of vision-language instruction tuning, covering a wide array of tasks.
- InstructBLIP extends BLIP with an instruction-aware Q-Former, showing improved performance over previous models.
- The work is well-organized, with clear motivations and effective ablation studies.
- The authors provide a comprehensive response to reviewer concerns, clarifying the number of samples used in the Arena and the model's performance across various benchmarks.
- InstructBLIP shows strong performance on public benchmarks, suggesting broad applicability in real-world scenarios.

Weaknesses:
- There is insufficient analysis of open-ended out-of-domain multimodal question answering capabilities, which is critical given the performance of models like GPT-4 and LLaVA.
- The instruction-following data largely derives from existing datasets through template conversion, potentially limiting instruction diversity.
- The framework is restricted to image-level tasks and does not address object-level tasks or image generation.
- There are concerns about InstructBLIP's tendency to overfit to academic benchmarks, particularly in generating detailed responses.
- The stability of the Arena rankings is questioned, with fluctuations noted across different versions.
- Potential overlap in training data between TextCaps and TextVQA raises issues regarding the validity of zero-shot claims.

### Suggestions for Improvement
We recommend that the authors improve the analysis of InstructBLIP's performance on open-ended out-of-domain multimodal question answering to provide a more robust justification of its capabilities. Additionally, consider incorporating a human evaluation to assess the model's performance more comprehensively. To enhance the diversity of instruction-following data, we suggest exploring additional sources beyond template-based conversions. We also recommend that the authors improve the discussion on overfitting, particularly regarding InstructBLIP's ability to generate detailed responses rather than defaulting to short answers. Furthermore, we suggest providing a clearer justification for the zero-shot evaluation claims, especially in light of the training data overlap. It would be beneficial for the authors to release a more recent snapshot of the leaderboard to enhance the stability of their evaluations. Finally, we encourage the authors to include a broader range of models in future evaluations and to gather diverse user feedback to strengthen their findings.