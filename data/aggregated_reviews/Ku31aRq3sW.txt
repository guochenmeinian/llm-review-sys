ID: Ku31aRq3sW
Title: How to Solve Contextual Goal-Oriented Problems with Offline Datasets?
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 5, 4, 7, -1, -1, -1, -1
Original Confidences: 2, 3, 2, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method to address Contextual Goal-Oriented Offline Reinforcement Learning (RL) tasks by proposing Contextual Goal-Oriented Data Augmentation (CODA). The authors demonstrate that CODA can learn near-optimal policies without negative labels and provide sufficient theoretical analysis and empirical evidence to support their claims. The methodology combines unlabeled dynamics datasets with labeled context-goal datasets to create an augmented Markov Decision Process (MDP), achieving a reward of 1 for goal states given context.

### Strengths and Weaknesses
Strengths:
- The contribution of the proposed method is interesting and addresses a challenging problem in context-defined goal-oriented RL policy learning without requiring labeled samples.
- Empirical evaluations show that CODA performs well compared to other baselines, indicating its effectiveness.
- Theoretical analysis is robust, providing a solid foundation for the proposed methodology.

Weaknesses:
- The novelty of the approach is limited, as the methodology for combining datasets is considered straightforward.
- The empirical evaluation is conducted in a single environment, raising questions about the generalizability of the results to more diverse settings.
- The paper lacks adequate experimentation and could benefit from more datasets and real-world scenarios to further validate the method.
- Presentation issues exist, including the need for clearer initialization of acronyms and corrections of typographical errors.

### Suggestions for Improvement
We recommend that the authors improve the Related Work section by clarifying the relationship between their method and existing Goal-conditioned RL approaches, including how these methods perform in empirical studies. Additionally, the authors should explore the incorporation of their data augmentation method with other Goal-conditioned RL techniques, such as relabeling, to enhance performance. More extensive discussions on the impressive results achieved through their straightforward data augmentation method are warranted. Finally, we suggest expanding the empirical evaluation to include a wider range of datasets and real-world scenarios to demonstrate the method's effectiveness more comprehensively.