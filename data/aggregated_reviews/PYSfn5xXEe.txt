ID: PYSfn5xXEe
Title: ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 5, 7, 5, 7, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for addressing the noise partial label learning problem, where the ground-truth label may not be included in the candidate label set. The authors propose the "Adjusting Label Importance Mechanism" (ALIM), which integrates the importance of labels based on model outputs. The method is evaluated against multiple benchmark datasets, demonstrating superior performance compared to existing approaches. The paper includes theoretical analysis and empirical results, highlighting the effectiveness of ALIM in the context of noisy partial label learning.

### Strengths and Weaknesses
Strengths:
1. The noise partial label learning issue is significant, and the authors provide a comprehensive summary of unresolved challenges in existing literature.
2. The paper includes rich experimental results and theoretical analysis, supporting the proposed framework.
3. ALIM serves as a plug-and-play solution for the noisy PLL task, showing competitive performance against advanced strategies.

Weaknesses:
1. The innovation of the proposed method is limited, primarily relying on post-processing of candidate and non-candidate label information.
2. The framework assumes that P(x) can yield better results in the noise partial label learning problem, which may not always hold true.
3. The method assigns confidence levels to non-candidate labels, potentially introducing noise.
4. The adaptive hyper-parameter lambda is not sufficiently analyzed in the experimental results, leaving its optimal value unclear.
5. The rationale for using Mixup Training to counteract noise effects in PLL is not adequately explained.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the core contribution and motivation in Section 2.2, explicitly stating the importance of the problem addressed. Additionally, please clarify the connection between the scaling factor $K$ and performance, as well as its selection process. The authors should also analyze the adaptive hyper-parameter lambda in the experimental results to determine its optimal setting. Furthermore, we suggest reformulating the optimization objective in Equation (5) to a $\min$ form for better compatibility with the loss function. Lastly, please address the potential typos and ensure all references to appendices are correctly formatted.