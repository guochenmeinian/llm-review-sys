ID: VHva3d836i
Title: WizardArena: Post-training Large Language Models via Simulated Offline Chatbot Arena
Conference: NeurIPS
Year: 2024
Number of Reviews: 24
Original Ratings: 5, 6, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents WizardArena, an offline simulated chatbot arena designed to evaluate and train large language models (LLMs) without human intervention. The authors propose a two-step process that involves selecting diverse and challenging prompts from the lmsys-chat-1m dataset for testing and constructing a judge model based on LLAMA3-70B-Chat. Additionally, the paper provides a detailed analysis of the DPO (Dynamic Pairwise Optimization) data construction process, highlighting the selection of Choose and Reject responses from various models, including Command R+, Qwen1.5-72B-Chat, and WizardLM-Î²-SFT. The experimental results indicate that the WizardLM-beta model's performance improves through iterative learning stages, demonstrating consistency with evaluations from GPT-4. The authors also propose innovative methods such as offline WizardArena and Pair-judge Battle for model post-training, emphasizing their effectiveness in enhancing model performance.

### Strengths and Weaknesses
Strengths:  
- The authors contribute significantly by constructing an offline chatbot arena, which is a novel approach.  
- The iterative learning process shows consistent performance improvement for the WizardLM-beta model.  
- The paper is well-organized and presents experimental results that align closely with existing evaluation frameworks.  
- The authors provide clear data on model performance and response selection, enhancing transparency.  
- Innovative methods for model training and evaluation are proposed, demonstrating potential for significant performance improvements.  

Weaknesses:  
- There are overlapping method descriptions in Sections 3 and 4, leading to unclear implementation details that affect the paper's soundness.  
- The data construction process lacks clarity, particularly regarding the handling of multi-turn dialogues and the definition of illegal conversations.  
- The experimental results and comparisons, particularly in Tables 2 and 3, require further clarification regarding model sources and fairness in comparisons.  
- The original version of the paper contains numerous areas needing improvement, as noted by multiple reviewers.  
- The authors acknowledge limitations in the quality of the sample data provided, which may affect reproducibility.  

### Suggestions for Improvement
We recommend that the authors improve clarity in the data processing steps, specifically defining illegal conversations and detailing the handling of multi-turn dialogues. Additionally, the authors should address potential redundancy in the Diverse and Hard Subsets and clarify how the different stages of data were obtained. We suggest enhancing the transparency of the experimental results by providing detailed sources for the data in Table 2 and ensuring fair comparisons in Table 3. Furthermore, we recommend improving the clarity and depth of discussions regarding the weak judge model's guidance for enhancing the strong student model and the self-evaluation capabilities of LLAMA3-70B within the multi-round training framework. We encourage the authors to expedite the review process for releasing their data and code to enhance transparency and reproducibility. Finally, including a comparative analysis with GPT-4 would strengthen the evaluation of the proposed methods.