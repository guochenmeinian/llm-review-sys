ID: 42mqpIrA39
Title: StackEval: Benchmarking LLMs in Coding Assistance
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 7, 6, 8, -1, -1
Original Confidences: 4, 4, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents StackEval, StackEval-Recent, and LLM-as-a-Judge, three benchmarks designed to evaluate Large Language Models (LLMs) in coding assistance tasks. The authors provide a comprehensive coding assistance benchmark (StackEval) with 925 questions across 25 programming languages, a recent benchmark (StackEval-Recent) with 300 questions from the latest Stack Overflow content, and an LLM-as-a-Judge benchmark featuring 136 LLM-generated answers validated by domain experts. The authors aim to enhance the evaluation framework for LLMs, addressing limitations in existing benchmarks and offering insights into LLM capabilities.

### Strengths and Weaknesses
Strengths:
- Comprehensive coverage of 25 programming languages, providing a diverse benchmark for coding assistance tasks.
- Introduction of StackEval-Recent, addressing the evaluation of LLM performance on new coding challenges.
- Innovative LLM-as-a-Judge benchmark, exploring the use of LLMs for evaluation tasks.
- Enhanced auto-evaluation methodology with a high success rate, contributing to reliable assessments.
- Clear commitment to ongoing updates and maintenance of the benchmarks.

Weaknesses:
- The validity of using existing LLMs for dataset creation lacks justification, particularly regarding the assessment of question type and complexity.
- Potential biases in using Stack Overflow as the primary source of questions are not sufficiently analyzed.
- The paper lacks a dedicated Limitations section and could benefit from a more in-depth ethical discussion.

### Suggestions for Improvement
We recommend that the authors improve the justification for using LLMs in dataset creation, particularly by comparing LLM assessments of question type and complexity to human evaluations. Additionally, a more detailed discussion of the biases inherent in using Stack Overflow data should be included. We suggest incorporating a Limitations section to address potential biases and the implications of using LLMs as judges. Furthermore, expanding the ethical considerations, including data privacy and the potential for misuse of the benchmarks, would enhance the paper's contribution to responsible AI development. Lastly, including visual aids or examples in the evaluation methodology could improve clarity and understanding.