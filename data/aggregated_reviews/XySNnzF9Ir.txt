ID: XySNnzF9Ir
Title: Automatic Evaluate Dialogue Appropriateness by Using Dialogue Act
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel metric for evaluating dialogue appropriateness, termed Dialogue Act Appropriateness (DAA), which is based on modeling dialogue act transitions. The authors validate this method using a dataset of 2716 turn-level appropriateness ratings across six dialogue agents, demonstrating a strong correlation with human evaluations. The study employs publicly available dialogue corpora and includes extensive experimentation with eight pre-trained language models.

### Strengths and Weaknesses
Strengths:
- The proposed DAA metric provides a stable evaluation of dialogue appropriateness, validated through rigorous statistical analysis and extensive experiments.
- A comprehensive dataset of appropriateness ratings from seven annotators enhances the reliability of the findings.
- The paper is well-structured, with clear descriptions of the DAA workflow and a strong correlation between the proposed metric and human judgments.

Weaknesses:
- The evaluation lacks empirical comparisons with existing dialogue appropriateness metrics, despite acknowledging them in the related work section.
- The focus on pragmatic appropriateness neglects the importance of semantic appropriateness, which is crucial for a holistic evaluation of dialogue.
- The reported correlations are system-wide, with no analysis at the dialogue or turn level, limiting the depth of the evaluation.

### Suggestions for Improvement
We recommend that the authors improve the empirical comparison of their proposed metric with existing dialogue appropriateness metrics to strengthen their claims. Additionally, addressing the integration of semantic appropriateness into their evaluation framework would enhance the robustness of their findings. It would also be beneficial to report DAA correlations at both the turn-level and dialogue-level to provide a more nuanced understanding of the metric's effectiveness. Finally, including an analysis of DAA with and without directive contexts could offer valuable insights into its applicability across different dialogue scenarios.