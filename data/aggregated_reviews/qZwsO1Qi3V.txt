ID: qZwsO1Qi3V
Title: Syntactic Substitutability as Unsupervised Dependency Syntax
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel technique for unsupervised dependency parsing that leverages syntactic substitutability to enhance the extraction of syntactic dependencies from the attention heads of Transformer-based language models. The authors propose a method that averages attention distributions from both the target sentence and syntactically isomorphic sentences generated via a masked language model like BERT. Empirical results indicate that this approach yields improved dependency structures, particularly for complex syntactic relationships such as long-distance subject-verb agreement.

### Strengths and Weaknesses
Strengths:
- The methodology is grounded in solid theoretical foundations and is straightforward.
- The paper is well-structured, clearly written, and presents a rigorous experimental analysis.
- The approach shows promise in enhancing the understanding of how Transformer models capture syntactic information.

Weaknesses:
- There is a lack of clarity regarding the sentence generation procedure and head selection, which raises concerns about the generalizability of the results across different models.
- The paper may benefit from a more concise presentation, as some sections contain redundant information.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the sentence generation procedure, particularly regarding the use of BERT and the POS tagger, to ensure that the generated sentences maintain syntactic rather than semantic similarity. Additionally, we suggest addressing the head selection process in more detail to enhance the reproducibility and generalizability of the findings. Finally, we encourage the authors to streamline the paper by reducing redundancy and focusing on core contributions, potentially condensing the work into a shorter format.