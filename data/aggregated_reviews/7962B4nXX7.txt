ID: 7962B4nXX7
Title: Learning Energy-based Model via Dual-MCMC Teaching
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 2, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for training and inference of energy-based models (EBMs) by introducing a joint learning framework that utilizes a generator model initialized via maximum likelihood estimation (MLE) through posterior sampling. The authors aim to address the challenges of MCMC sampling from EBMs, proposing a dual-MCMC approach that enhances sampling efficiency. They also introduce a cooperative learning analogy where the generator model acts as a "student" learning from a "teacher" (the EBM). The authors demonstrate that their generator model, which incorporates observed data, significantly outperforms a model trained without such observations in image synthesis (FID score: 39.25 vs. 58.74). The paper includes extensive experimental evaluations demonstrating the effectiveness of the proposed framework, particularly in improving accuracy and sample quality. The authors focus on short-run, non-convergent Langevin sampling due to its simplicity and efficiency, while acknowledging its limitations in density estimation.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written and well-presented, with solid experimental results that include fair comparisons with concurrent works.
- The proposed dual-MCMC teaching method has considerable potential impact and addresses an actual problem that has been largely overlooked in prior research.
- The numerical results for the proposed method are impressive, showcasing significant improvements over existing methods.
- The paper effectively clarifies the learning dynamics between the generator and EBM using an intuitive analogy.
- Empirical results show significant improvements in image synthesis quality with the proposed generator model.
- The discussion on the limitations of Langevin sampling is well-articulated and grounded in existing literature.

Weaknesses:
- Section 2, while clear and self-contained, is overly large and lacks direct connection to the proposed methodology; better explanations of the benefits of the components would enhance clarity.
- The justification for the Kullback-Leibler divergence term in the EBM learning objective is insufficiently grounded, lacking formal reasoning or verification from referenced literature.
- The energy profile in Figure 2 does not converge after 30 MCMC steps, and results from long-run MCMC revisions should be provided for verification.
- Training details, including optimization hyperparameters and Langevin dynamics step sizes, are missing.
- The reliance on short-run, non-convergent Langevin samplers may limit the density estimation capabilities.
- The generator's lower-dimensional latent space poses challenges for direct application of certain MCMC frameworks, potentially complicating the computation of importance weights.
- The motivation for learning the generator with observed samples is unclear, and the computational and memory costs of the method are not adequately balanced against its improved accuracy.

### Suggestions for Improvement
We recommend that the authors improve the connection between Section 2 and the proposed methodology by providing clearer explanations of the benefits of each component. Additionally, the authors should strengthen the justification for the Kullback-Leibler divergence term in the EBM learning objective with formal reasoning or literature support. It would be beneficial to include results for long-run MCMC revisions to demonstrate energy profile convergence and to provide comprehensive training details, including optimization hyperparameters and Langevin dynamics step sizes. Furthermore, we suggest that the authors enhance the exploration of the global-local MCMC algorithm by interleaving local Langevin samplers with global generator proposals to improve multi-modal traversal and negative sampling. Clarifying the motivation for learning the generator with observed samples and addressing the computational and memory costs in relation to accuracy would enhance the paper's overall impact. Lastly, a detailed comparison with the Divergence Triangle would help elucidate the differences and motivations behind the proposed method.