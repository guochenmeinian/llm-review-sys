ID: N2wYPMpifA
Title: Understanding Scaling Laws with Statistical and Approximation Theory for Transformer Neural Networks on Intrinsically Low-dimensional Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 20
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical framework for understanding the generalization error and scaling laws of transformer architectures when applied to beta-Holder continuous functions and low-dimensional data. The authors derive a generalization error bound that decomposes into approximation and variance errors, demonstrating that transformers can approximate such functions with a finite number of blocks, although this claim requires clarification regarding the dependence on the number of feedforward network (FFN) layers. Additionally, the paper introduces a theoretical framework for estimating the intrinsic dimension (ID) of textual datasets using Maximum Likelihood Estimation (MLE), arguing that MLE provides a non-linear measure of ID validated in prior works. The authors assert that their theoretical predictions align closely with empirical observations, particularly regarding transformer scaling laws. Furthermore, the paper aims to predict and explain scaling laws in deep learning models, incorporating data with an explicit sequential structure, although some reviewers question the realism of this approach.

### Strengths and Weaknesses
Strengths:
- The paper introduces novel approximation theory for transformers, particularly regarding intrinsic dimension.
- The use of MLE for estimating intrinsic dimension is well-supported by prior literature, enhancing the credibility of the approach.
- Empirical observations align well with theoretical predictions, enhancing the paper's credibility.
- The rigorous mathematical framework addresses significant open questions in the field of transformer scaling laws.
- The authors provide a general interpretation of data structures, suggesting that components can represent various types of sequential data.

Weaknesses:
- The claim that transformers can approximate functions to any precision with a finite number of blocks is misleading; it relies on assumptions about the number of FFN layers.
- The theory's reliance on low-dimensional manifold assumptions may not hold for complex, high-dimensional data.
- The practical applicability of the intrinsic dimension estimation process remains unclear.
- Some empirical results, particularly in Figure 3, are perceived as less accurate, raising concerns about the validity of predictions.
- The empirical results do not convincingly support the claims of "predicting" and "explaining" scaling laws, with predictions diverging significantly for certain datasets.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims regarding the approximation capabilities of transformers, particularly the implications of Theorem 2 concerning the number of FFN layers. Additionally, we suggest providing a more comprehensive discussion on the computational complexity of their proposed methods compared to feedforward networks. It would also be beneficial to include empirical evidence that validates the theoretical construction and its applicability to real-world scenarios. Furthermore, we recommend improving the clarity of the intrinsic dimension estimation process, particularly in relation to its practical implications, and providing a more detailed explanation of the connection between the modeling assumptions and real-world data. Including more empirical results across a wider range of model sizes would strengthen the experimental validation. Lastly, we encourage the authors to consider using "estimating" instead of "predicting" to describe their theoretical framework and to add a "Future Directions" section to guide readers on potential research avenues stemming from their findings.