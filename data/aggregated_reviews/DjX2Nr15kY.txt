ID: DjX2Nr15kY
Title: NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 5, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents NAR-Former V2, a modified Transformer-based model designed for neural network representation learning, incorporating graph-specific properties to enhance performance in latency and accuracy predictions. The authors demonstrate that NAR-Former V2 significantly improves latency prediction compared to its predecessor, NAR-Former, and establishes a new baseline in the field through extensive experiments on recent benchmarks.

### Strengths and Weaknesses
Strengths:  
- The paper is well-motivated, effectively integrating graph-specific properties into the Transformer architecture, which is a novel approach.
- NAR-Former V2 shows significant improvements in latency prediction performance and is well-benchmarked against recent datasets.
- The writing is clear and the organization of the paper is generally good.

Weaknesses:  
- The latency of NAR-Former V2 itself is not analyzed, and comparisons regarding model latency are missing.
- Specific numerical comparisons between NAR-Former V2 and NAR-Former are not adequately reported, particularly in lines 331-338 and Table 2.
- Performance on AlexNet and VGG is poor, raising questions about the model's effectiveness on networks without residual connections.
- The paper lacks context for "NAR" and assumes familiarity with NAR-Former, which may alienate less experienced readers.
- The organization could be improved by presenting the overall architecture diagram earlier to provide a clearer context.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing context for "NAR" and ensuring that all readers can follow the discussion. Additionally, we suggest moving the overall architecture diagram to the front to give readers a clearer understanding of the proposed model. It would be beneficial to include comparisons of model latency and to report specific numerical results when comparing NAR-Former V2 with NAR-Former. Furthermore, addressing the performance issues on AlexNet and VGG would strengthen the paper. Lastly, including standard deviations in performance metrics would enhance the robustness of the reported results.