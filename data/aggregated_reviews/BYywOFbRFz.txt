ID: BYywOFbRFz
Title: Waypoint Transformer: Reinforcement Learning via Supervised Learning with Intermediate Targets
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 4, 5, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Waypoint Transformer (WT), which addresses the limitations of existing decision transformer architectures, particularly the "stitching" problem related to global goals and the bias and variance of reward-to-go targets. The authors propose a method that predicts constant average and cumulative reward-to-go as short-term and intermediate goals, guiding the transformer to generate trajectories toward these goals. The method demonstrates strong performance across various benchmarks and reduces training time. Additionally, the authors provide a comparative analysis of WT and QDT, showing that WT outperforms QDT in most cases. They address concerns regarding the utility of reward information in sparse reward environments and the relevance of target states in undirected locomotion tasks, arguing that defining a global goal state is challenging in tasks like walking, where the objective is to maximize distance rather than achieve a specific position. However, they also discuss the limitations of their waypoint network, particularly in relation to terminal states and the necessity of intermediate goals.

### Strengths and Weaknesses
Strengths:
- The paper effectively analyzes the limitations of existing decision transformer architectures and proposes a novel solution by providing intermediate targets.
- It achieves state-of-the-art performance on multiple benchmarks compared to offline reinforcement learning methods and shows that removing action conditions can enhance performance and reduce training time.
- The method is straightforward to implement, and the paper is well-structured with helpful illustrations.
- The thorough comparison of QDT and WT highlights significant performance differences across tasks, and empirical results demonstrate the efficacy of the waypoint network in handling multimodal data.

Weaknesses:
- The novelty of the method is limited, as the concepts of constant and cumulative reward-to-go are already prevalent in recent offline reinforcement learning literature.
- The presentation could be improved; for instance, the distinction between constant average reward-to-go and updated cumulative reward-to-go is unclear.
- There is insufficient analysis of failure cases or limitations of the proposed method, and the claims regarding performance compared to state-of-the-art methods may not hold due to outdated baselines.
- The lack of empirical results for some claims, particularly regarding the waypoint network's performance and its comparison to DT, raises concerns about the robustness of the findings.
- The authors' assertion that the differences between D4RL v0 and v2 datasets are not substantial is contradicted by evidence showing significant performance disparities.
- The paper could benefit from clearer presentation of numerical results and additional analyses on the waypoint network's decision-making process.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation, particularly in distinguishing between the average and cumulative reward-to-go in their equations. Additionally, a more detailed analysis of the method's limitations and failure cases would strengthen the paper. We suggest including comparisons against more recent state-of-the-art algorithms, such as Q-learning transformers, to validate the claims of superior performance. Furthermore, we encourage the authors to explore the implications of conditioning on both rewards and goals and provide empirical results to support their approach. We also recommend improving the clarity of their numerical results by presenting them in a clear tabular format. Additionally, providing further empirical analysis of the waypoint network's performance, particularly in relation to its ability to determine optimal sub-goals, would be beneficial. Including DT results in the paper could enhance the discussion on baseline performance. Lastly, addressing the concerns regarding the training of the last few states in Section 4.1 would enhance the robustness of their analysis.