ID: LEed5Is4oi
Title: Robot Policy Learning with Temporal Optimal Transport Reward
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TemporalOT, an extension of the optimal transport-based proxy reward method in imitation learning, which incorporates temporal information through a masking mechanism and context embedding-based cost matrix. The authors propose that these modifications allow the agent to focus on more relevant information during learning. The experiments demonstrate that TemporalOT achieves higher success rates than traditional imitation learning and inverse reinforcement learning methods that do not account for temporal information. Additionally, the paper explores visual-similarity-based reward calculation and imitation learning, providing clarifications that effectively address follow-up questions.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and organized, clearly motivated by the need to address the lack of temporal consideration in OT-based reward calculations.
- Strong experimental results are reported, with thorough benchmarking across nine environments and helpful ablation studies that clarify the contributions of different components.
- The authors' responses enhance clarity and understanding of the key concepts discussed in the paper.

Weaknesses:
- The novelty of the approach is seen as incremental, primarily adding two modifications to vanilla OT.
- The presentation of the central claim is inconsistent with the rationale behind the proposed additions, leading to confusion regarding the treatment of temporal information.
- Some analyses lack depth, particularly regarding the performance of learning from demonstrations in specific tasks and the variability of important components across tasks.
- The temporal mask design may be ineffective in early learning stages, and the authors are encouraged to explore alternative designs or integrate it into the learning process.
- Minor issues include the need for clearer descriptions of task complexity and the absence of detailed examples for certain calculations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the central claim by explicitly addressing how the proposed components relate to the issue of overlooking temporal information. Additionally, the authors should provide a high-level guide for selecting parameters $k_c$ and $k_m$ under varying task conditions to enhance the generalization capability of their method. A more detailed discussion on the implications of different pretrained visual encoders on OT's efficacy would also be beneficial. Furthermore, we suggest that the authors analyze the reasons behind the performance discrepancies between ASD and vanilla OT in specific tasks, and consider applying the algorithm to real robots to validate its practical applicability. Lastly, including a brief overview of the nine meta-world tasks and improving the clarity of figures and tables would enhance the overall presentation. We also recommend that the authors continue to provide detailed explanations for complex concepts to further enhance clarity for readers.