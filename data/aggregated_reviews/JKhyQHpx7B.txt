ID: JKhyQHpx7B
Title: Vocabulary-free Image Classification
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 3, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the task of vocabulary-free image classification (VIC), which aims to classify images without a predefined set of categories. The authors propose a method called Category Search from External Databases (CaSED), which retrieves captions from an external text corpus to extract category names for classification. The classification process involves a score fusion of image-to-text matching and text-to-text matching scores. Additionally, the authors introduce instance-wise vocabulary prediction, allowing CaSED to operate online, unlike previous approaches that require simultaneous access to all images in the dataset. They clarify that their retrieval and scoring mechanisms are specifically tailored to this task, distinguishing it from existing methods. The authors emphasize that the use of a large textual database can yield effective results across diverse benchmarks without manual tuning.

### Strengths and Weaknesses
Strengths:
1. The paper is clearly written and easy to follow.
2. The exploration of vocabulary-free image classification is an interesting task that requires no prior knowledge of semantic class space.
3. The paper demonstrates consistent outperformance of CaSED over the more complex VLM, BLIP-2, across multiple datasets while utilizing fewer parameters.
4. The proposed benchmarks and metrics are valuable for future research in the domain.
5. The task definition and protocol are recognized as strengths by multiple reviewers.

Weaknesses:
1. Missing details on baselines and evaluation metrics.
2. The choice of the caption database significantly impacts performance, yet guidance for selecting an appropriate text database is absent.
3. A baseline comparison with WordNet and English words is missing, as it would be interesting to extract words from the PMD captions and apply CLIP for image-text matching.
4. Results in Table 4(b) indicate that using YFCC100M leads to worse outcomes compared to CC12M and Redcaps, necessitating further explanation.
5. The term "vocabulary-free" is misleading, as the method relies on a large-scale text database, which may not cover all semantic categories.
6. Some reviewers raised concerns regarding the novelty of the method and task, suggesting a need for clearer differentiation from existing works.
7. There is a perceived misalignment between the review comments and the final score, with some critiques focusing on novelty rather than technical flaws.

### Suggestions for Improvement
We recommend that the authors improve the clarity and completeness of the paper by providing additional information on the calculation of classical Cluster Accuracy, particularly in the context of open vocabulary. We suggest addressing the inefficiencies of retrieving diverse linguistic elements from image captions, as this may waste computational resources. Furthermore, we encourage the authors to discuss alternative strategies or optimizations to mitigate the computational burden. We also recommend that the authors improve the clarity of their novelty claims by explicitly differentiating their approach from existing methods in the literature. Additionally, addressing the reviewers' concerns regarding the task definition and terminology could strengthen the paper's position. Expanding on the limitations of CaSED in Sections 5 and 6 would enhance the paper's comprehensiveness. Lastly, we suggest that the authors provide more detailed comparisons with BLIP-2 to clarify the advantages of their approach and to clarify the implications of the choice of text database on performance, exploring the potential for automatic selection of relevant text databases based on the query image dataset.