ID: UEzKGW4U39
Title: Isotropy-Enhanced Conditional Masked Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper addresses the anisotropic problem in iterative non-autoregressive (NAR) neural machine translation models, particularly CMLM. The authors analyze this issue, confirming its existence and proposing two methods to alleviate it: (i) employing contrastive learning to enhance token representation discrimination, and (ii) introducing a "Look Neighbors" strategy to improve adjacent token representation learning. The authors quantitatively and qualitatively demonstrate that these methods reduce token similarity and enhance performance across multiple WMT datasets, while also showing compatibility with recent advancements like knowledge distillation and self-correction.

### Strengths and Weaknesses
Strengths:
- The paper tackles a significant issue in iterative NAR models, contributing to an active research area.
- It provides solid empirical analysis supported by visualizations and metrics.
- The proposed solutions are simple yet effective, consistently improving performance across datasets.
- Compatibility with other recent methods is well demonstrated.

Weaknesses:
- Performance gains, while present, are modest compared to prior methods.
- The analysis is primarily focused on one model (CMLM), lacking exploration of other NAR models.
- Important related works, such as NAT-REG and LAVA-NAT, are not adequately discussed, which could better motivate the proposed methods.
- The motivation for the Look Neighbors mechanism is vague, and its connection to contrastive loss is unclear.
- There is insufficient exploration of the causes of anisotropy and its impact on generation.
- Missing strong baselines, such as Diff-GLAT, limit the comparative analysis.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind the Look Neighbors mechanism and provide an ablation study to establish its effectiveness. Additionally, we suggest including a discussion of related works like NAT-REG and LAVA-NAT to better contextualize the proposed methods. Expanding the analysis to include more NAR models would strengthen the findings, and providing strong baselines for comparison is essential. Finally, we encourage the authors to clarify the specific failures in learning representations and the implications of anisotropic loss on model performance.