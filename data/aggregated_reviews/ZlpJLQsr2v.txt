ID: ZlpJLQsr2v
Title: Generalizable Implicit Motion Modeling for Video Frame Interpolation
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 3, 7, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Generalizable Implicit Motion Modeling (GIMM) module aimed at enhancing video frame interpolation (VFI) by refining optical flow. The authors propose a framework that includes normalization of bidirectional flows, a motion encoder for extracting spatiotemporal motion features, and an adaptive coordinate-based implicit neural representation (INR) to predict normalized flow maps. The framework is pre-trained with reconstruction loss and subsequently fine-tuned with a frame synthesis module. The authors argue that spatial coordinates are essential for achieving continuous modeling of spatiotemporal changes and claim that their method can be smoothly integrated with existing flow-based VFI works, enhancing arbitrary-timestep interpolation capabilities. The results indicate that GIMM achieves superior optical flow accuracy, which is critical for effective VFI.

### Strengths and Weaknesses
Strengths:
1. The visual quality of the synthesized videos and motion fields is impressive.
2. The approach of using INRs for motion modeling in VFI without requiring test-time training is a novel contribution.
3. The authors provide a novel approach to continuous motion modeling using INRs, which is deemed appropriate for video applications.
4. The integration of GIMM with existing VFI methods reportedly improves performance, as evidenced by experimental results.

Weaknesses:
1. The paper lacks comprehensive experimental validation, particularly on widely used public datasets like Vimeo90k, which raises concerns about the generalizability of the results.
2. The ablation studies are limited and do not adequately verify the effectiveness of key components such as normalization and motion encoding.
3. The explanations and analyses of the results are insufficient, particularly regarding the necessity of spatial coordinates in the INR, with concerns that they may be redundant if latent codes already encapsulate the necessary information.
4. The claim of "smooth integration" with existing methods like IFRNet is challenged, as it requires significant modifications to the original architecture, undermining the novelty of the integration.

### Suggestions for Improvement
We recommend that the authors improve their experimental validation by including results on public benchmarks like Vimeo90k and Adobe240fps to substantiate their claims. Additionally, the authors should enhance the ablation studies to experimentally verify the contributions of normalization and motion encoding, providing clearer insights into their importance. We also suggest that the authors offer more detailed explanations regarding the necessity of spatial coordinates in the INR, particularly addressing the redundancy concerns raised about latent codes. Furthermore, we recommend that the authors clarify their claims about the integration process with IFRNet, ensuring that the modifications required do not contradict the assertion of smooth integration. It may also be beneficial to provide a more detailed comparison with existing methods that demonstrate their claimed plug-in ability, addressing the concerns about the generalizability of their approach. Finally, including parameter and runtime comparisons in Table 3 would strengthen the evaluation of GIMM's efficiency and effectiveness when integrated with existing VFI methods.