ID: hTLIAYTi5w
Title: ClozEx: A Task toward Generation of English Cloze Explanation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new task called ClozEx, which focuses on generating explanations for cloze questions in language assessment, particularly for English as a Second Language (ESL) learners. The authors introduce a dataset created from templates applied to a corpus of news text and evaluate various large language models (LLMs) as baselines for this task. Human annotations assess the generated explanations based on Fluency and Validity, while automatic evaluation methods are also employed. The fine-tuned encoder-decoder LLM methods outperform decoder-only and prompting methods, with BART-large achieving the highest Validity score. The correlation between human and automatic evaluation scores is explored, revealing high correlation for GPT-Fluency but not for annotator Validity.

### Strengths and Weaknesses
Strengths:
- The task is relevant for developing language learning systems and provides valuable insights into the relationship between automated and human evaluation methods.
- The methods for dataset creation are innovative, combining crafted rules with large language models.
- Extensive evaluations of benchmark systems, including human assessments, are conducted.

Weaknesses:
- The recruitment of annotators from the authors' university may introduce bias, affecting reproducibility and the validity of annotations.
- The use of BLEU as an evaluation metric is deemed unsuitable for the explanation generation task, leading to concerns about the meaningfulness of results in Table 4.
- The dataset relies heavily on template generation, which may not reflect real ESL educational contexts and could lack diversity and difficulty.

### Suggestions for Improvement
We recommend that the authors improve the bias management of the annotations by providing more information about the annotators and considering a more diverse recruitment strategy. Additionally, we suggest that the authors clarify the rationale for using BLEU as a metric and explore alternative evaluation methods that better capture the quality of explanations. It would also be beneficial to conduct experiments addressing the potential lack of difficulty and diversity in the generated dataset. Finally, we encourage the authors to include a dataset with human-written questions and explanations to enhance the research's relevance and applicability.