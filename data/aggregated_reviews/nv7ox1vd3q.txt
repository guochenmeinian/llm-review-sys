ID: nv7ox1vd3q
Title: Precise asymptotics of reweighted least-squares algorithms for linear diagonal networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 6, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of high-dimensional dynamics in reweighted least-squares methods, specifically within "linear diagonal networks." The authors propose a general algorithm that encompasses various methods, including alternating minimization (AM) and reparameterized IRLS, as detailed in Equation (4) and Table 1. The main result, Theorem 1, establishes the asymptotic convergence in probability of any function \( g \in PL(2) \) to its corresponding expectation, with simulations supporting the theoretical findings.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written, easy to follow, and effectively motivates the study.  
- It unifies different lines of research through a general algorithm, allowing for specific method implementations via reweighting functions.  
- Theorem 1 is a significant contribution, applicable to any function in the class \( PL(2) \) or any continuous and bounded function.  
- The proofs are presented clearly, with technical points illustrated effectively.

Weaknesses:  
- The assumption of independent data batches is overly strict and may not reflect practical scenarios, as most optimization algorithms reuse data.  
- The experimental evaluations lack depth; for instance, the accuracy of error bounds for \( \ell_2 \) squared test loss is not assessed, and the non-monotonic errors in alternating minimization raise questions about the results.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the independence assumption by providing experiments that allow data reuse, similar to Figure 1, and commenting on the technical challenges of relaxing this assumption. Additionally, we suggest including experiments to verify the accuracy of error bounds for \( \ell_2 \) squared loss and clarifying the behavior of alternating minimization under different loss functions. Furthermore, we encourage the authors to enhance the motivation for their study by providing clearer take-home messages and discussing the implications of their findings in relation to feature learning.