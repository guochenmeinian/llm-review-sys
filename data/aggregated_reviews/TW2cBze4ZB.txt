ID: TW2cBze4ZB
Title: Contrastive Deterministic Autoencoders For Language Modeling
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a deterministic autoencoder for language modeling, proposing the Contrastive Entropic Autoencoder (CEAE) as an alternative to variational autoencoders (VAEs) to address the posterior collapse problem. The authors build on the work of Ghose et al. (2020) by incorporating a contrastive learning term to regularize the model's objective function, which approximates an entropic regularizer for Gaussian distributions in high dimensions. The authors demonstrate that their model outperforms various VAE models across three public datasets (PTB, Yelp, and Yahoo) and maintains consistency across different sequence-processing architectures, including LSTMs and Transformers.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel approach to mitigate posterior collapse in VAEs, showcasing strong empirical performance across multiple architectures.
- It provides a comprehensive review of existing studies on the posterior collapse issue and presents a well-structured model that integrates contrastive learning.
- The exploration of the latent space through interpolation offers valuable insights into the model's capabilities.

Weaknesses:
- The paper lacks a clear connection to the extensive literature on posterior collapse specific to language modeling, missing key references such as Bowman et al. (2016).
- The notation and terminology used are inconsistent and may confuse readers, particularly regarding the reconstruction loss and the use of capital and lowercase letters.
- The advantages of the proposed model over existing solutions are not clearly delineated, and comparisons with state-of-the-art models like T5 are absent.
- The paper does not adequately address the scaling capabilities of the method in the context of large-scale pretraining.

### Suggestions for Improvement
We recommend that the authors improve the paper's clarity by rewriting sections to better target an NLP audience, ensuring that relevant literature on posterior collapse and contrastive learning in NLP is cited. Additionally, the authors should clarify the reconstruction loss used in their model and ensure consistent notation throughout. We suggest including comparisons with T5 and other encoder-decoder models to better illustrate the advantages of the CEAE. Finally, the authors should address the scaling capabilities of their method in light of recent advancements in large language models.