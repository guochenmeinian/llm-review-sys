ID: oDeqjIM9Sk
Title: Weight decay induces low-rank attention layers
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 7, 3, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the impact of weight decay on matrix products, particularly in the context of transformers, and investigates the effects of low-rank approximation during training on attention layers. The authors demonstrate that L2 regularization of the matrices is equivalent to regularizing the nuclear norm of their product, which can lead to low-rank solutions at local minima. Theoretical findings indicate that under certain conditions, local minima of the L2 regularized loss align with those of the nuclear norm regularized loss, and the distance between these minima converges exponentially fast to zero. The authors clarify their boundedness assumption, asserting its relevance in practical scenarios where weight decay is applied, and provide sufficient conditions under which this assumption holds. Empirical validation is provided through experiments on various transformer models, addressing potential concerns regarding the convergence of weight norms in the context of various loss functions.

### Strengths and Weaknesses
Strengths:
- The topic is highly relevant, as weight decay is widely used in transformer training, and improved understanding could enhance practical performance.
- The theoretical framework is sound, supported by comprehensive empirical evaluations.
- The paper is well-structured and presents results clearly, although some figures could be improved in clarity and format.
- The authors provide a clear distinction between their work and prior studies, emphasizing the novel aspect of examining rank reduction during training.
- They offer a detailed explanation of the boundedness assumption and its practical implications, supported by sufficient conditions for various training dynamics.

Weaknesses:
- The novelty of the theoretical contributions is questioned, as some results overlap with previous works, particularly Theorem 3.1 from Wang and Jacot [2023].
- The reliance on gradient flow analysis may not accurately reflect real training dynamics.
- The discussion on linear models is limited to an underspecified regime, which may not align with standard practices in linear regression.
- The contribution regarding low-rank approximation's impact on generalization is perceived as limited due to architectural similarities.
- The authors' claims about the boundedness assumption may lack clarity, particularly in relation to certain loss functions that could lead to divergence.

### Suggestions for Improvement
We recommend that the authors improve the framing of their work to clarify its relevance to transformers, as the theoretical results seem more applicable to compressed sensing or matrix completion. Additionally, addressing kernel regimes, such as NTK, would enhance the practical relevance of the linear model discussion.

To strengthen the empirical analysis, we suggest quantifying the performance impact of rank loss in the ViT experiments and exploring the effects of effective learning rates when adjusting weight decay. Disentangling softmax temperature effects from rank effects in experiments would also provide clearer insights.

Furthermore, we recommend that the authors improve the clarity of their contribution by explicitly stating the significance of their findings in the context of existing literature. We suggest elaborating on the boundedness assumption, particularly addressing how it applies to loss functions that may diverge, to strengthen the theoretical foundation of their claims. Finally, we encourage the authors to enhance the exposition of mathematical statements, particularly in Theorem 3.4, to ensure clarity regarding the implications of boundedness assumptions during training and to include the explicit equation with the convergence rate as suggested for better clarity.