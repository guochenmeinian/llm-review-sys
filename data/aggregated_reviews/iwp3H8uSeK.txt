ID: iwp3H8uSeK
Title: Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 5, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for enhancing the robustness of image classification models through knowledge distillation, specifically by distilling CLIP models and augmenting adversarial learning with pre-trained generative models. The authors introduce a novel distillation objective called "DAD," which combines standard cross-entropy loss with KL divergence losses to improve model performance on out-of-distribution tasks. Extensive experiments validate the effectiveness of the proposed method across various settings. The authors also respond to reviewer feedback, indicating a willingness to revise based on the comments received and emphasizing the need to clarify the paper's main focus in the introduction.

### Strengths and Weaknesses
Strengths:
1. The extensive experiments cover diverse architectures for teacher and student models, as well as variations of ImageNet.
2. The authors provide theoretical insights into the benefits of a more diverse distribution for distillation.
3. The paper introduces a novel knowledge distillation loss that could have broader applicability in the field.
4. The authors demonstrate responsiveness to reviewer feedback and are open to making necessary revisions.
5. The acknowledgment of specific reviewer comments shows a commitment to improving the paper.

Weaknesses:
1. The research problem is not clearly defined, particularly regarding the focus on out-of-distribution robustness versus the initial claim of training small robust models without access to the teacher's training data.
2. The justification for using generative models to augment adversarial examples is insufficiently explained, lacking motivation and experimental support.
3. Many notations remain undefined, leading to confusion about specific terms and their roles in the proposed method.
4. The novelty appears limited, primarily leveraging VQ-GAN for distillation without clear rationale for its selection.
5. The scope of research is narrow, as experiments are confined to CLIP models, despite claims of broader applicability.
6. The introduction lacks clarity regarding the paper's main focus, which needs to be addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the research problem by explicitly stating the goals and contributions in the introduction. Additionally, the authors should provide a more robust justification for the use of generative models in augmenting adversarial examples, including theoretical motivation and experimental validation. It is crucial to define all notations clearly and to elaborate on the choice of VQ-GAN, potentially comparing it with other data augmentation techniques. We also suggest expanding the experimental scope beyond CLIP models to demonstrate the generalizability of the proposed method. Finally, we recommend that the authors improve the introduction by clearly outlining the paper's main focus, ensuring that it aligns with the feedback provided.