ID: XYw051ZmUn
Title: How does Gradient Descent Learn Features --- A Local Analysis for Regularized Two-Layer Neural Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 6, 6, 6, -1, -1
Original Confidences: 3, 2, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on how gradient descent achieves feature learning in two-layer neural networks, particularly focusing on the late stages of training dynamics. The authors propose that minimizing the population loss leads to a perfect recovery of the teacher network, extending previous findings that primarily addressed early-stage dynamics. The main theoretical contribution is a local landscape analysis demonstrating a strong notion of feature learning, where the second-layer weights can take negative values.

### Strengths and Weaknesses
Strengths:  
- The paper offers a significant theoretical contribution through a challenging local landscape analysis, extending prior work on feature learning.
- The results indicate that learning both layers of two-layer neural networks can lead to perfect recovery of the teacher network, surpassing previous literature.

Weaknesses:  
- The focus on population loss lacks discussion on empirical loss and sample complexity, which are crucial for the presented results.
- The presentation could be improved for clarity, particularly regarding the local loss landscape analysis and the connection between assumptions and theoretical results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the abstract by specifying the threshold for loss mentioned. Additionally, the relationship between assumption 3 and the information exponent should be made explicit. It would be beneficial to discuss the implications of a larger information exponent and clarify the rationale behind using the same regularization parameter for different layers. A finite sample analysis should be performed to address the realism of the expected loss scenario. We suggest that the authors provide a more formal discussion regarding the vague statements about neural networks learning target subspaces and include missing citations related to feature learning. Lastly, a schematic drawing of the descent direction could enhance reader comprehension.