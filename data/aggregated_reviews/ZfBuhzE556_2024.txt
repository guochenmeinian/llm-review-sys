ID: ZfBuhzE556
Title: $\beta$-DPO: Direct Preference Optimization with Dynamic $\beta$
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 3, 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a dynamic method for tuning the hyperparameter $\beta$ in Direct Preference Optimization (DPO) based on batch-level data quality, referred to as $\beta$-DPO. The authors propose that the optimal $\beta$ is influenced by the reward discrepancy in the data, leading to improved performance on the Anthropic HH and Reddit TL;DR summarization datasets compared to vanilla DPO. The method utilizes an implicit reward model for training, aiming to enhance alignment while addressing computational complexity and sensitivity issues associated with external reward models. The authors argue that data ordering has minimal impact on performance and employ a moving average updating scheme for the reward margin to stabilize the training process. The study empirically validates the effectiveness of the proposed method across various model sizes, including Pythia-410M, 1.4B, and 2.8B.

### Strengths and Weaknesses
Strengths:
1. The topic is meaningful, with potential implications for reinforcement learning from human feedback (RLHF) and alignment research.
2. The presentation is clear and well-organized, making the paper easy to read.
3. The empirical findings suggest that the proposed method improves performance across multiple DPO variants.
4. The authors demonstrate promising results and a constructive approach to addressing reviewer feedback.
5. The dynamic $\beta$ strategy shows strong generalizability and stable performance improvements across implicit and explicit reward models.
6. The method is straightforward to implement, requiring minimal additional parameter tuning.

Weaknesses:
1. The proposed method lacks soundness in both theoretical and experimental aspects, particularly due to reliance on small models, making it difficult to assess performance on larger state-of-the-art models.
2. The derivation of the dynamic terms is not adequately explained, introducing additional hyperparameters ($\beta_{0}$ and $\alpha$) that complicate the tuning process.
3. The evaluation is limited to the Anthropic HH dataset, and broader testing across diverse datasets and model sizes is necessary.
4. The implicit reward model is perceived to be generally poorer than explicit reward models, raising concerns about its effectiveness.
5. There is uncertainty regarding the sensitivity of the method to reward points, which may also affect DPO.
6. The potential instability introduced by the implicit reward model could impact the training process.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for the dynamic $\beta$ derivation and clarify how it captures global information from batch-level data. Additionally, the authors should address the complexity introduced by the additional hyperparameters and provide more details on the reward model used, including its implementation and implications for the proposed method. Expanding the evaluation to include larger models and different datasets would strengthen the claims made in the paper. We also suggest conducting ablation studies to assess the necessity of both dynamic scheduling and filtering techniques in various contexts. Furthermore, improving the evaluation of their method by testing it with external reward models would provide further insights. Conducting an ablation study on batch size and exploring the calibration of learning rate alongside dynamic $\beta$ would enhance the robustness of their findings. Finally, clarifying the implications of sensitivity to reward points and addressing concerns regarding non-stationarity in more detail would also strengthen the manuscript.