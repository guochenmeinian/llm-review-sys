ID: 0KvYLaTBTE
Title: Latent Plan Transformer for Trajectory Abstraction: Planning as Latent Space Inference
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Latent Plan Transformer (LPT), a novel framework for trajectory generative modeling that utilizes a top-down latent variable model to represent a plan for decision-making in the absence of step-wise rewards. The framework consists of three interconnected components: a neural transformation of Gaussian noise, a transformer-based trajectory generator, and a return estimator. LPT is optimized through maximum likelihood estimation on offline datasets of trajectory-return pairs and is evaluated across various environments, demonstrating competitive performance and addressing temporal consistency challenges.

### Strengths and Weaknesses
Strengths:
- LPT innovatively learns from trajectory-return pairs without step-wise rewards, effectively addressing temporal consistency issues.
- The paper offers a thorough analysis from a sequential decision-making perspective, emphasizing the importance of plan prediction.
- The method is well-evaluated, with LPT-EI showing superior performance compared to final-return baselines.
- The writing is clear and well-structured.

Weaknesses:
- Training and inference may be inefficient due to MCMC sampling, particularly in high-dimensional scenarios.
- The LPT may struggle with datasets that have skewed trajectory lengths, such as antmaze-large.
- There is a lack of theoretical analysis connecting motivations to MDP representation learning.
- The ablation study lacks sufficient discussion, particularly regarding comparisons with different priors for the latent vector.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis to clarify the benefits of the latent variable in decision-making and its implications for state compression/aggregation. Additionally, we suggest enhancing the discussion of the ablation study to include more comparisons with different latent priors and to ensure that model sizes are adjusted for fairness in baseline comparisons. Furthermore, we encourage the authors to provide results for DT/QDT using the same codebase and to clarify the performance of LPT on Antmaze medium/large tasks. Addressing these points will strengthen the paper's contributions and clarity.