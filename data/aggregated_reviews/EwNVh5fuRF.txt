ID: EwNVh5fuRF
Title: Select, Prompt, Filter: Distilling Large Language Models for Summarizing Conversations
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach for generating weakly supervised annotated datasets for conversation summarization using knowledge distillation of large language models (LLMs), specifically ChatGPT. The authors propose a three-step method—select, prompt, and filter—to create relevant training examples from an unannotated corpus. The approach is evaluated on three datasets (Ubuntu, NYC, BC3) and demonstrates improved performance over existing models, including ChatGPT and Pegasus, while requiring fewer training examples. The paper also includes an ablation study showing the contribution of each step to performance gains. 

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and clearly written.
- It introduces a practical method for generating weakly supervised datasets, addressing the scarcity of annotated training data.
- The approach shows significant performance improvements over direct LLM usage and competitive models.
- It effectively reduces the number of training examples needed while maintaining performance, promoting resource efficiency.

Weaknesses:
- The choice of XSum BART for experiments is not justified, and the selection of training parameters lacks clarity.
- The novelty of the approach is questioned, as producing training examples from larger models is a known effective strategy in NLP.
- The applicability of the method is limited to forum conversations, raising concerns about its generalizability to other text types.

### Suggestions for Improvement
We recommend that the authors improve the justification for selecting XSum BART and provide more details on how training parameters were determined. Additionally, it would be beneficial to address the applicability of the method beyond forum conversations. Clarifying the diversity of prompts and its impact on model quality would also enhance the paper. Finally, we suggest including more technical details and potentially publishing the code and generated dataset to facilitate reproducibility.