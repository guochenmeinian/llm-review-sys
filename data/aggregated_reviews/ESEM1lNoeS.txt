ID: ESEM1lNoeS
Title: AttrSeg: Open-Vocabulary Semantic Segmentation via Attribute Decomposition-Aggregation
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a decomposition-aggregation framework aimed at enhancing open-vocabulary semantic segmentation. The framework comprises a decomposition stage utilizing pre-trained language models (LLMs) like ChatGPT to extract textual attributes from class names, followed by an aggregation stage that employs a hierarchical structure to combine these attributes into a cohesive representation. The authors assert that their approach addresses limitations in existing vision-language pre-training methods, particularly regarding ambiguity and the handling of novel object categories during inference. Additionally, the authors explore the impact of dataset size and model scale on performance by conducting experiments on a larger dataset (COCO-Stuff) and a larger model (ViT-L). Experimental results indicate the proposed method's effectiveness across multiple datasets, demonstrating that using a larger dataset and model can significantly enhance performance.

### Strengths and Weaknesses
Strengths:
1. The framework's concept is straightforward and inspired by cognitive psychology, enhancing category comprehension through diverse attribute descriptions.
2. The experimental evaluation demonstrates superior performance compared to existing methods, supported by thorough ablation studies.
3. The authors effectively address previous concerns by expanding their experiments to include a larger dataset and model, yielding insightful results.
4. The comparative analysis between PASCAL VOC and COCO-Stuff highlights the benefits of increased data and model size.

Weaknesses:
1. The framework's performance is heavily dependent on the accuracy of the selected attributes, raising concerns about potential degradation in segmentation quality due to incorrect attributes. The authors should explore methods to mitigate risks during the decomposition stage.
2. The comparison primarily with LSeg, a relatively weak baseline, limits the demonstration of the framework's effectiveness. The authors should include comparisons with more recent state-of-the-art methods.
3. The experimental results on standard datasets like COCO-20 and PASCAL-5 do not adequately illustrate the framework's ability to handle neologisms and unnameability, as these datasets contain well-known classes that do not present significant ambiguity.
4. The improvement in performance when using a larger model is less pronounced compared to the gains from a larger dataset, which may warrant further investigation.

### Suggestions for Improvement
We recommend that the authors improve the robustness of the decomposition stage by exploring strategies to regulate the risk of inaccurate attribute selection during both training and inference. Additionally, the authors should conduct comparisons with more recent state-of-the-art works in open-vocabulary semantic segmentation to better validate their contributions. It would also be beneficial to clarify how the proposed method can handle ambiguous class names and to provide more comprehensive evaluations on datasets that present challenges related to neologisms and unnameability. Lastly, including ablation studies focused on the effectiveness of the decomposition process would strengthen the paper's argument. Furthermore, we recommend that the authors improve the analysis of the performance differences between model sizes, particularly focusing on the reasons behind the smaller gains observed with larger models, and further explore the implications of dataset size on model training to provide deeper insights.