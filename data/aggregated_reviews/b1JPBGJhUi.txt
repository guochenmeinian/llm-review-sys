ID: b1JPBGJhUi
Title: Stable Nonconvex-Nonconcave Training via Linear Interpolation
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 6, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 1, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of linear interpolation aimed at stabilizing neural network training, addressing instabilities caused by nonmonotonicity in the loss landscape. The authors propose a new optimization scheme, RAPP, which is the first explicit method to achieve last-iterate convergence rates for cohypomonotone problems. Additionally, they introduce the Lookahead algorithm by modifying RAPP's inner optimizer. The paper includes experimental validation on synthetic problems and GANs.

### Strengths and Weaknesses
Strengths:
- The paper provides a clear summary of the problem setting and effectively addresses questions implied by existing literature.
- It offers proofs that generalize several methods as instances of the (inexact) KM iteration.
- RAPP is shown to converge for a broader class of problems, including those with constraints, and the analysis of Lookahead algorithms is simplified.

Weaknesses:
- Formulation (4) is incorrect as $z$ and $z^â€™$ have different dimensions.
- The upper bound in Theorem 5.1 is too loose, as it approaches infinity when $\gamma\to 1/L$.
- Linear interpolation is not mentioned in the theorems, despite its inclusion in experiments, indicating a lack of theoretical support.
- Comparisons with other optimization schemes, such as Momentum, RMSprop, and Adadelta, are insufficient.

### Suggestions for Improvement
We recommend that the authors improve the clarity of formulation (4) to ensure dimensional consistency. Additionally, we suggest tightening the upper bound in Theorem 5.1 to provide a more accurate convergence characterization. The authors should incorporate linear interpolation into their theoretical framework and enhance the comparison of RAPP with other optimization algorithms to provide a more comprehensive evaluation. Lastly, including an analysis of wallclock time for the algorithms and experimenting with various hyperparameters for the Adam optimizer would strengthen the paper.