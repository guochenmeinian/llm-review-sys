ID: JrIPBXWiS8
Title: Resfusion: Denoising Diffusion Probabilistic Models for Image Restoration Based on Prior Residual Noise
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 6, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Resfusion, a general diffusion framework for image restoration that incorporates a residual term into the diffusion process to directly generate clean images from degraded ones. The authors introduce a smooth equivalence transformation for learning residual noise and demonstrate the effectiveness of Resfusion through extensive experiments and ablation studies on various datasets. The method shows competitive performance in tasks like shadow removal, low-light enhancement, and deraining, while also reducing the number of required sampling steps.

### Strengths and Weaknesses
Strengths:
- The idea of conditioning on the residual term is original and significant, allowing for a reverse process that starts from corrupted images mixed with noise.
- The proposed smooth equivalence transformation is promising and cleverly simplifies the learning of residual noise.
- The experimental results demonstrate non-trivial improvements and good performance across multiple tasks.

Weaknesses:
- The derivation in Section 2.2 is unclear, with several notations (e.g., $\alpha_t$, $\beta_t$) not defined, making it difficult for readers to follow.
- There is a lack of comparisons on inference speed, particularly regarding the sampling process.
- The overall presentation, especially the mathematical derivations, requires improvement for clarity and coherence.
- The paper does not provide sufficient analysis of the error introduced by ignoring terms in Eq. (6) or the scheduling of $\alpha_t$.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the derivations in Section 2.2 by providing detailed explanations and definitions for all notations used. Additionally, the authors should include comparisons on inference speed and unify the evaluation metrics across all tasks and datasets. It would also be beneficial to add more experimental analysis regarding the error introduced by ignoring terms in Eq. (6) and clarify the scheduling of $\alpha_t$. Finally, visualizations of the inference steps should be included to enhance understanding.