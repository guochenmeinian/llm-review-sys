ID: z6KNvOe9zQ
Title: Vision Model Pre-training on Interleaved Image-Text Data via Latent Compression Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a vision backbone pre-training method called Latent Compression Learning (LCL), which utilizes interleaved image-text data to enhance visual representation learning. The LCL approach maximizes mutual information in an autoregressive manner, integrating both discriminative and generative objectives. The extensive experiments indicate that LCL matches the performance of existing models like CLIP on paired datasets and effectively leverages interleaved pre-training data to learn robust visual representations from scratch.

### Strengths and Weaknesses
Strengths:  
1. The paper is well written and accessible, effectively motivating the need for pre-training on interleaved data.  
2. It introduces LCL, a novel pre-training method that utilizes interleaved image-text data, which is significant for leveraging large-scale web-crawled data.  
3. Extensive experiments validate the effectiveness of the proposed method across various datasets.  

Weaknesses:  
1. The performance of LCL using solely image-text pairs does not surpass the CLIP baseline, raising questions about the impact of sample size on performance gains observed with the MMC4 dataset.  
2. The lack of performance improvement with original interleaved datasets like Obelics suggests inefficiencies in utilizing directly crawled data.  
3. The paper does not adequately demonstrate scaling behavior across different dataset sizes, nor does it provide sufficient qualitative analyses or comparisons with other state-of-the-art methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of experimental results, particularly regarding the discrepancies in reported numbers for models like BEiT3. Additionally, including more multi-modal LLMs in Table 6 would enhance the completeness of comparisons. It is crucial to explicitly address the similarities with CoCa and clarify the novelty of the proposed objective. Furthermore, we suggest providing insights into the utilization of original interleaved datasets and demonstrating the model's performance across varying dataset scales. Lastly, qualitative experiments and visual analyses should be incorporated to further elucidate the advantages of the proposed method.