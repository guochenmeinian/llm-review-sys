ID: 6SRE9GZ9s6
Title: Preference-grounded Token-level Guidance for Language Model Fine-tuning
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 7, 7, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to address the misalignment between sequence-level preferences and token-level language model training in natural language generation (NLG). The authors propose an iterative training framework that integrates sequence-level preferences into token-level guidance, thereby mitigating granularity mismatches. They explore the relationship between token probabilities and sequence probabilities, asserting that viewing language model (LM) cross-entropy loss as token-level loss is a valid perspective. The authors emphasize their goal to ground sequence-level feedback into dense token-level guidance for LM training, demonstrating through experiments that their token-level reward-weighted maximum likelihood estimation (MLE) outperforms classical supervised MLE and enhances performance in unsupervised settings. The effectiveness of this framework is evaluated through experiments on discrete-prompt generation and text summarization tasks, showing competitive performance against baseline models.

### Strengths and Weaknesses
Strengths:
- The methodology of decomposing sentence-level preferences into token-level guidance is intuitive and rational, effectively addressing granularity mismatches in language model training.
- The authors provide a clear theoretical framework linking token and sequence probabilities, enhancing the understanding of LM training.
- The paper includes comprehensive ablation studies and comparative performance analyses, showcasing the potential benefits of the proposed model.
- Experimental results indicate that the proposed token-level guidance improves LM performance in both supervised and unsupervised contexts.

Weaknesses:
- The definition of 'preference' is ambiguous, suggesting human bias towards text quality, yet the use of the METEOR score as a sequence-level reward contradicts this understanding, raising questions about the evaluation methodology.
- Performance on the CNNDM and XSum tasks, measured in ROUGE scores, falls below benchmark levels, with a basic BART model outperforming the proposed method, questioning its effectiveness.
- The absence of accompanying code limits reproducibility, particularly given the complexity of applying reinforcement learning, which may hinder wider verification and applicability of the method.
- The evaluation is limited to two tasks without justification for their selection, and there is no human evaluation on summarization, which is critical for validating improvements in ROUGE scores.
- The authors may need to further clarify their stance on the dichotomy between token-level and sequence-level losses, as this could lead to confusion.
- The manuscript could benefit from a more explicit discussion on the implications of their findings in relation to existing literature.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definition of 'preference' and provide a more robust justification for the use of METEOR as a reward function. Additionally, conducting human evaluations on the summarization task would enhance the credibility of the results. The authors should also consider expanding the evaluation to include more diverse tasks where preference-based learning is relevant, such as toxicity avoidance and controllable generation. Furthermore, providing the code for the proposed method would facilitate reproducibility and verification of results. Lastly, we suggest analyzing the computational costs associated with the reward function retraining and ensuring that ROUGE score computations are fair and comparable across models. We also recommend that the authors clarify their position regarding the relationship between token-level and sequence-level losses and expand on the implications of their findings in relation to existing literature to provide a more comprehensive context for their contributions.