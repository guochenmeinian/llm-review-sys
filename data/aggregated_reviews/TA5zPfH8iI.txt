ID: TA5zPfH8iI
Title: B-cosification: Transforming Deep Neural Networks to be Inherently Interpretable
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 7, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents B-cosification, a method to convert pre-trained networks into B-cos networks, enabling fine-tuning for improved interpretability while maintaining predictive performance. The authors evaluate their approach on various CNN and Transformer architectures, including a case study on CLIP, demonstrating that B-cosified models achieve comparable accuracy and localization results to both the original models and standard B-cos networks. Additionally, the paper explores dynamic linearity, distinguishing it from piece-wise linearity by emphasizing that dynamic linearity involves transforming inputs with an input-dependent matrix \( \mathbf W(x) \). The authors clarify that B-cos transformations utilize a cosine factor to scale rows of a static matrix, contrasting with piece-wise linear ReLU networks. The implications of weight normalization in B-cos neurons are also discussed, highlighting weight-input alignment and its effects on model performance.

### Strengths and Weaknesses
Strengths:
- The work addresses the significant challenge of transforming pre-trained black-box models into inherently interpretable models without requiring complete retraining.
- The paper is well-structured, with detailed descriptions of the modifications needed for B-cosification.
- The experimental evaluation encompasses a broad range of architectures, showcasing the potential of the proposed method.
- The authors provide thorough clarifications on complex concepts such as dynamic linearity and weight normalization, enhancing the paper's technical depth.
- The rebuttal to reviewers' questions is comprehensive, indicating a strong engagement with feedback.

Weaknesses:
- The claim of inherent interpretability is questionable; B-cos networks may function more as post-hoc explanation methods rather than truly interpretable models.
- The lack of statistical significance measures, such as standard deviations, complicates the assessment of performance improvements.
- The novelty of the methodology is limited, as many B-cosification techniques have been previously introduced.
- The experimental settings and presentation could be clearer, with some sections being verbose or lacking clarity.
- The lack of available code on the GitHub repository raises concerns about reproducibility and usability, which could hinder the work's impact and adoption by the community.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims regarding inherent interpretability and provide evaluations of faithfulness to the underlying model, such as the Model Parameter Randomization Test. Additionally, including plots that show how accuracy and localization scores vary with fine-tuning would enhance the understanding of the method's performance. We also suggest providing standard deviations for performance metrics to assess statistical significance and addressing the clarity of the presentation by proofreading for typos and structural coherence. Furthermore, we urge the authors to publish their code promptly to ensure full reproducibility of the reported results, as this is crucial for the usability and distribution of their work. Finally, consider discussing the implications of the new aggregation method for CLIP's performance in more detail.