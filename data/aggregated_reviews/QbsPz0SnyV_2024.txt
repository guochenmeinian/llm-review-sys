ID: QbsPz0SnyV
Title: Facilitating Multimodal Classification via Dynamically Learning Modality Gap
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 3, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel multimodal learning method that addresses modality imbalance by dynamically integrating unsupervised contrastive learning and supervised multimodal learning. The authors propose two dynamic integration strategies: heuristic and learning-based. Experimental results demonstrate that the proposed method significantly outperforms existing multimodal learning methods across several datasets. The analysis of modality imbalance from the label-fitting perspective is particularly novel, and comprehensive experiments validate the effectiveness of the approach.

### Strengths and Weaknesses
Strengths:
- The analysis of modality imbalance from the label-fitting perspective is innovative, providing new insights into multimodal learning.
- The paper is well-organized, with clear writing and logical flow, making it easy to follow.
- Detailed methodological and experimental explanations support the effectiveness of the proposed method, showing its ability to eliminate modality imbalance.
- The authors design both heuristic and learning-based strategies for loss integration, analyzing their effects experimentally.

Weaknesses:
1. The paper presents insufficient analysis of modality imbalance, particularly in relation to fitting category labels, with only a simple experiment on the Kinetics Sounds dataset.
2. The distinction between mixed loss and label smoothing is unclear and requires clarification.
3. There is a lack of connection between the motivation to alleviate modality imbalance and the introduction of contrastive loss, which needs deeper analysis.
4. The practicality of dynamic weight methods is questionable due to insufficient discussion on computational costs and approximation techniques.
5. The motivation for using contrastive learning lacks depth, as it does not adequately explain how it mitigates modality imbalance.

### Suggestions for Improvement
We recommend that the authors improve the analysis of modality imbalance by providing more comprehensive experiments and explanations, particularly regarding the relationship between fitting category labels and modality imbalance. Clarifying the distinction between mixed loss and label smoothing is essential. We suggest that the authors strengthen the connection between their motivation and the use of contrastive loss by providing a more thorough theoretical analysis. Additionally, discussing the computational costs and practicality of the dynamic weight methods would enhance the paper's robustness. Finally, addressing the motivation for using contrastive learning in greater depth will improve the overall clarity and impact of the work.