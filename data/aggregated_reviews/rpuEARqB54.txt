ID: rpuEARqB54
Title: On the Role of Noise in the Sample Complexity of Learning Recurrent Neural Networks: Exponential Gaps for Long Sequences
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the sample complexity of learning noisy multi-layered recurrent neural networks with sigmoid activations, where independent mean-zero Gaussian noise is added to the activations. The authors prove that these networks can be learned with sample complexity $\tilde{O}(\frac{w\log(T/\sigma) + \log(1/\delta)}{\epsilon^2})$, which shows a significant improvement over the non-noisy case, where the lower bound is $\Omega(\frac{wT+\log(1/\delta)}{\epsilon^2})$. The techniques employed, particularly those derived from Fathollah Pour and Ashtiani (2022), focus on covering numbers under total variation distance, which may be of independent interest to the PAC learning community.

### Strengths and Weaknesses
Strengths:
- The paper effectively contrasts the learning complexities of noisy versus clean recurrent neural networks, revealing an exponential gap in sample complexity.
- The proof techniques involving random hypotheses and covering numbers are intriguing and contribute to the understanding of sample complexity.

Weaknesses:
- The motivation for studying noisy recurrent neural networks is unclear, as the noise assumption may not seem natural for neural networks.
- The use of ramp loss is questioned due to its non-convexity and limited practical application, which may weaken the overall motivation and significance of the results.
- There is a lack of experimental results to support the theoretical findings, and the implications of noise on expressive power and learning dynamics are not sufficiently discussed.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the practical implications of adding noise, particularly in the context of next-generation AI and sampling complexity challenges in deep learning. Additionally, we suggest that the authors clarify the motivation for using ramp loss and consider discussing its limitations more thoroughly. Including experimental results would strengthen the paper, providing empirical support for the theoretical claims. Finally, addressing the questions regarding the nature of noise in the network and the implications of larger noise levels on performance would enhance the clarity and depth of the analysis.