ID: 83LJRUzXWj
Title: Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 6, 5, 3, 6, 7, -1, -1
Original Confidences: 4, 4, 5, 4, 5, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to open-vocabulary panoptic segmentation by unifying the mask generator and CLIP classifier into a single-stage framework using a shared Frozen Convolutional CLIP backbone (FC-CLIP). This method accelerates training and inference while achieving state-of-the-art performance on various benchmarks. The authors argue that their approach is more efficient than previous two-stage methods, which typically involve separate backbones for mask generation and classification.

### Strengths and Weaknesses
Strengths:
1. The single-stage framework is concise and efficient, simplifying the process of training and inference.
2. The paper is well-written, clearly presenting the problem, methodology, and results, with effective visualizations.
3. FC-CLIP demonstrates strong performance across multiple benchmarks, surpassing previous models.

Weaknesses:
1. The novelty of the approach is limited, as it closely resembles F-vlm, with insufficient targeted improvements or innovations for panoptic segmentation.
2. The claim that CNN-based CLIP outperforms ViT-based CLIP for dense prediction lacks rigorous verification; further experiments are needed to substantiate this.
3. The paper lacks an ablation study to analyze the impact of various components on performance, and the technical details are not sufficiently self-contained.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their work by clearly articulating the specific contributions and differences from F-vlm. Additionally, conducting a thorough ablation study to evaluate the impact of different components of the FC-CLIP model on performance is essential. We suggest that the authors rigorously validate the claim regarding the superiority of CNN-based CLIP over ViT-based CLIP by including experiments that utilize ground truth masks for evaluation. Furthermore, clarifying the technical details of the mask generator and classifier, as well as addressing any potential typos in equations, would enhance the paper's clarity and rigor. Finally, we encourage the authors to discuss potential biases in the pre-trained CLIP model and their implications for the FC-CLIP model's performance.