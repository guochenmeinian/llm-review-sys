ID: fhCSDMkrFr
Title: Contrasting Sequence with Structure: Pre-training Graph Representations with PLMs
Conference: NeurIPS
Year: 2023
Number of Reviews: 1
Original Ratings: 7
Original Confidences: 3

Aggregated Review:
### Key Points
This paper presents a method that leverages the CLIP strategy to pretrain a network for both protein sequence and structure. The generated representations demonstrate performance comparable to state-of-the-art (SOTA) methods in selected protein function prediction tasks and protein- and residue-level interaction prediction tasks. The authors propose that this foundational work has significant potential for scalability with larger datasets and fine-tuning for specific tasks, particularly regarding high-quality predicted structures.

### Strengths and Weaknesses
Strengths:  
- The paper is clear and easy to read.  
- The approach shows competitive performance with SOTA methods in relevant tasks.  
- The potential for optimization and extension is well articulated.  

Weaknesses:  
- There is no major improvement in performance noted.  
- The choice of a small ESM model raises questions, and there is a lack of weighting across different loss terms.

### Suggestions for Improvement
We recommend that the authors improve the model by testing at least the 650M ESM variant. Additionally, we suggest investigating the decision regarding the absence of weighting across different loss terms, as this could impact the overall performance. Lastly, we advise rephrasing line 214 to clarify that "the best BioCLIP performance is within 7% of the performance achieved by DeepFRI."