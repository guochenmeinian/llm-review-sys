ID: wqo6xEMyk9
Title: ProG: A Graph Prompt Learning Benchmark
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 5, 8, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ProG, a comprehensive benchmark for evaluating graph prompt learning methods, integrating six pre-training methods and five state-of-the-art graph prompt techniques across fifteen diverse datasets. The benchmark addresses critical challenges in graph prompt learning, such as unifying diverse models and evaluating the quality of graph prompts. Additionally, the authors propose a few-shot learning approach for graph data, aiming to enhance performance compared to semi-supervised learning settings, with a universal graph prompt designed to manage complex graph data across various domains. ProG is publicly available as an open-source library, enhancing transparency and usability for researchers.

### Strengths and Weaknesses
Strengths:
1. ProG includes a wide range of pre-training methods and graph prompt techniques, evaluated on fifteen diverse datasets, providing a thorough assessment across various domains.
2. The benchmark introduces a unified framework categorizing graph prompts into two main approaches, facilitating understanding and comparison of different techniques.
3. The authors demonstrate an innovative approach to few-shot learning in graph data, with the concept of a universal graph prompt addressing the challenge of handling diverse graph data.
4. ProG is an open-source library that promotes transparency and reproducibility, streamlining the execution of various graph prompt models.

Weaknesses:
1. The usage of graph prompts remains unclear, particularly regarding their application in business contexts versus customer interactions in NLP. The paper lacks datasets with multiple downstream tasks, raising concerns about the necessity of pre-training on citation networks for molecular graphs.
2. The performance of all methods is lower than that of semi-supervised learning, raising questions about the necessity of the proposed few-shot learning capability.
3. The evaluation of zero/few shot learning is ambiguous, as current graph learning primarily relies on end-to-end training, with node classification adapting to semi-supervised training requiring few labeled data.
4. There is a lack of conducted graph reasoning tasks to substantiate claims regarding reasoning ability, and the rationale for developing a model to manage different domains without performance improvement over semi-supervised learning is unclear.
5. The paper does not adequately address the increase in standard deviation associated with graph prompt methods, which could cloud the significance of results.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the usage of graph prompts and their relevance to business applications. Additionally, the authors should provide datasets with multiple downstream tasks to strengthen the benchmark's applicability. We suggest enhancing the performance of their methods to exceed that of semi-supervised learning settings and conducting graph reasoning tasks to validate claims about reasoning ability. Furthermore, we encourage the authors to clarify the purpose and benefits of creating a universal graph prompt for handling diverse domains, especially in light of the current performance metrics. Finally, addressing minor grammatical errors and clarifying the rationale behind the classification of graph prompts would further enhance the paper's quality.