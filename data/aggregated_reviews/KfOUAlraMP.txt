ID: KfOUAlraMP
Title: Wasserstein distributional robustness of neural networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 8, 4, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel adversarial attack methodology based on Wasserstein distributionally robust optimization (WDRO), introducing a first-order approximation of the attack loss for computational efficiency. The authors propose the ReDLR loss, which generalizes established methods like FGSM and TRADES, and provides theoretical bounds on adversarial accuracy and out-of-sample performance. Experiments on the CIFAR-10 dataset demonstrate the method's effectiveness compared to robust baseline methods.

### Strengths and Weaknesses
Strengths:
- The ReDLR loss is simple yet effective, grounded in the intuition that images classified far from the decision boundary should be perturbed more aggressively.
- The theoretical analysis is robust, offering valuable insights for future research, particularly in the section on "Bounds on Out-of-Sample Performance."
- Empirical results show significant improvements over existing methods, validating the theoretical framework.
- The paper is well-structured and accessible, making it suitable for readers new to DLR-based methods.

Weaknesses:
- The number of experiments is limited to CIFAR-10 with few baselines, which may not fully demonstrate the method's generalizability.
- The guarantees provided are asymptotic and depend on the first-order approximation, raising questions about practical applicability.
- Certain key concepts, such as S^c and Assumption 4.1, lack clear definitions and experimental validation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the intuition presented in lines 171-173 by emphasizing it in the introduction and earlier sections of the method. Additionally, please provide a clear definition of S^c upon its first appearance and consider offering experimental analysis to validate Assumption 4.1 across various neural network architectures. To strengthen the paper, we suggest including comparisons with more adversarial attack methods and conducting experiments on additional datasets beyond CIFAR-10 to enhance the robustness of the findings.