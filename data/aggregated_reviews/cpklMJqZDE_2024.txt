ID: cpklMJqZDE
Title: Unrolled denoising networks provably learn to perform optimal Bayesian inference
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 6, 8, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the performance of neural network-based methods for inverse problems, particularly focusing on unrolling Approximate Message Passing (AMP) for compressive sensing and rank-one PCA. The authors propose that when trained with gradient-based methods, these networks can achieve performance comparable to optimal prior-aware algorithms. They demonstrate that under specific conditions, such as using a product prior, the parameters of the network converge to the optimal denoisers used in Bayes AMP. The theoretical framework combines state evolution and Neural Tangent Kernel (NTK) analysis, supported by empirical results.

### Strengths and Weaknesses
Strengths:
- The work addresses a significant and timely question regarding the superiority of neural network methods over hand-crafted priors.
- The paper is well-written, self-contained, and includes a clear introduction to AMP, making it accessible to a broader audience.
- The theoretical contributions are robust, with sufficient numerical evidence validating the claims.

Weaknesses:
- The assumptions regarding the prior distribution being a smooth product distribution are limiting and may not reflect real-world applications.
- Some aspects of readability and presentation, particularly concerning the main theorems, could be improved.
- The practical applicability of the results is constrained due to the niche focus and the specific conditions required for AMP.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the main theorems and address the readability issues noted in the reviews. Additionally, consider relaxing the assumptions on the prior distribution to enhance the generalizability of the results. It would be beneficial to provide more context regarding the function $\phi$ in Theorem 2 and clarify the implications of the learning rate and number of steps. Further, we suggest including experiments with non-product priors to explore the robustness of the findings and discussing the practical implications of the assumptions made in the theoretical framework.