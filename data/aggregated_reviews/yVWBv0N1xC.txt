ID: yVWBv0N1xC
Title: LayerNAS: Neural Architecture Search in Polynomial Complexity
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LayerNAS, a method for neural architecture search (NAS) that reduces computational costs by transforming the multi-objective NAS problem into a combinatorial optimization problem. The authors propose a layerwise search approach, allowing for polynomial computational complexity. LayerNAS is empirically validated on datasets such as ImageNet, CIFAR-10, CIFAR-100, and Imagenet16, demonstrating its effectiveness.

### Strengths and Weaknesses
Strengths:
- The reduction of computational cost in NAS is significant and relevant to the NeurIPS community.
- The paper provides a clear framework and empirical validation for the proposed method.
- Limitations and assumptions are explicitly discussed, enabling further research.

Weaknesses:
- The novelty of the proposed method is not clearly articulated.
- The analysis of search costs is limited to MAdds, neglecting other important metrics like energy consumption and latency.
- Some formatting issues exist, such as the abstract not being a single paragraph.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the novelty of LayerNAS in the introduction. Additionally, we urge the authors to include a detailed analysis of the cost distribution over layers and the implications of different cost thresholds on the obtained architectures. Including numerical results on the cost per layer and additional experiments on transfer learning would enhance the paper's contributions. Furthermore, we suggest revising Table 2 to include more informative details such as training epochs and augmentations, and aligning parameters and MAdds for clearer comparisons.