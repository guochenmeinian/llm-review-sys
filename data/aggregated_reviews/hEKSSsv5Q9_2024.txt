ID: hEKSSsv5Q9
Title: DALD: Improving Logits-based Detector without Logits from Black-box LLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 8, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Distribution-Aligned LLMs Detection (DALD) framework, which enhances the detection of LLM-generated text by aligning the distribution of surrogate models with that of target models. The authors demonstrate the effectiveness of DALD through extensive experiments on various closed-source and open-source models, achieving state-of-the-art performance in zero-shot detection scenarios. The method is adaptable, requiring minimal fine-tuning, and shows robustness against adversarial attacks and non-English text.

### Strengths and Weaknesses
Strengths:
- The paper effectively reflects on existing logits-based methods and proposes the DALD framework, achieving superior detection performance through distribution alignment.
- The use of small-scale datasets for fine-tuning is cost-effective and allows for rapid adaptation to model updates.
- Comprehensive ablation experiments validate the framework's performance in diverse scenarios, including Non-English Detection and Adversarial Attack.

Weaknesses:
- The method's interpretability is insufficient, particularly regarding why training on multiple source model datasets yields better results than a single source model, which contradicts the foundational premise of aligning distributions.
- There is a lack of detail about the fine-tuning datasets, including their sources and attributes, which hinders reproducibility and understanding of the method's effectiveness.
- The testing dataset is limited, containing only 150 human-written samples, raising concerns about potential bias in the experimental results.

### Suggestions for Improvement
We recommend that the authors improve the interpretability of the DALD framework by providing a clearer explanation of the performance discrepancies observed when training on multiple versus single source model datasets. Additionally, we suggest including more detailed descriptions of the fine-tuning datasets to enhance reproducibility. Expanding the testing dataset beyond 150 samples would also strengthen the validity of the experimental results. Finally, addressing the limitations and generalizability of the method in greater detail would provide a more comprehensive understanding of its applicability across different contexts.