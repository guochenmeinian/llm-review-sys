ID: YZoGmJrOS9
Title: Can Custom Models Learn In-Context? An Exploration of Hybrid Architecture Performance on In-Context Learning Tasks
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of various attention-based architectures combined with Mamba across multiple in-context regression tasks, focusing on the relationship between architectural choices and performance. The authors investigate different configurations, including normalization, positional encodings, and activations, revealing varying capacities for in-context learning (ICL) tasks. A new metric, the "ICL regression score," is introduced to evaluate model performance relative to a baseline and allows for quick comparisons of architectural performance on synthetic tasks. The authors emphasize the development of a modular codebase that enhances open-source contributions and facilitates independent extensions to model architecture and evaluation techniques.

### Strengths and Weaknesses
Strengths:
- The codebase for benchmarking ICL abilities is a valuable resource for understanding hybrid model capabilities.
- The exploration of nine hybrid model configurations across eight tasks provides a comprehensive analysis of ICL performance.
- The introduction of a novel ICL regression score allows for quick comparisons of architectural performance on synthetic tasks.
- The authors have improved the codebase's modularity and implementation, adhering to best practices in open-source repository management.

Weaknesses:
- The originality and significance of the work appear limited, as it closely relates to prior studies without substantial new insights.
- Clarity is lacking; important details are missing, and the paper could benefit from improved readability and explanations of key concepts.
- The empirical results are difficult to interpret, with no follow-up ablation studies to explain observed failures in certain architectures.
- The paper lacks clarity in its empirical analysis, with limited motivation and digestible results for practitioners.
- The overall scientific findings are deemed insufficient, and the manuscript appears unfinished, requiring further exploration of experiments and clearer takeaways.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing detailed explanations of key concepts, such as the "zero estimator" and the ICL regression score. Additionally, conducting follow-up ablation studies to explore the reasons behind the performance of specific hybrid architectures would enhance the contribution of the work. To strengthen the empirical findings, consider increasing the number of training runs per model-task pair to provide more robust results and constructing confidence intervals to support the findings. Furthermore, we suggest revising the presentation of figures and tables for better readability, including clearer labels and larger text. Lastly, addressing the motivation behind the choice of hybrid architectures and their practical applications would provide valuable context for readers and help articulate the major takeaways more explicitly for practitioners.