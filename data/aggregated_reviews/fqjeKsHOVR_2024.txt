ID: fqjeKsHOVR
Title: Harmonizing Visual Text Comprehension and Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 8, 8, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TextHarmony, a multimodal generative model designed for the unified comprehension and generation of visual text. The authors address performance degradation caused by modality inconsistency through the introduction of Slide-LoRA, which combines modality-specific and modality-agnostic LoRA experts. Additionally, a high-quality dataset, DetailedTextCaps-100K, is developed to enhance visual text generation capabilities. Experimental results indicate that TextHarmony achieves performance comparable to modality-specific models with only a slight increase in parameters.

### Strengths and Weaknesses
Strengths:
1. The unification of visual text comprehension and generation within a single model is a novel contribution that broadens the scope of multimodal applications.
2. The proposed Slide-LoRA method effectively addresses modality inconsistency and demonstrates solid experimental results.
3. The paper is well-organized, presenting reasonable motivation and insights.

Weaknesses:
1. The connection between this work and visual text is not clearly articulated, and elaboration on how multimodal generation is achieved would be beneficial.
2. Comparisons with existing multimodal generative models, such as DreamLLM and Emu, are lacking.
3. The performance of TextHarmony is not consistently superior to existing baselines, raising questions about the advantages of combining comprehension and generation.
4. Certain experimental settings and arguments require further clarification and evidence to support claims made in the paper.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the connection between their work and visual text by elaborating on the specifics of multimodal generation. Additionally, including comparisons with other multimodal generative models like DreamLLM and Emu in the main text would strengthen the paper. Reporting the model size and inference speed of TextHarmony is also advisable. Finally, providing more explanations or evidence for certain claims and clarifying experimental settings will enhance the overall rigor of the study.