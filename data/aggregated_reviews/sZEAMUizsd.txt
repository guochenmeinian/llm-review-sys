ID: sZEAMUizsd
Title: Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 6
Original Ratings: -1, -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a quantization framework, Outlier Suppression+, aimed at improving the efficiency of large language models (LLMs) by suppressing outliers through channel-wise shifting and scaling operations. The authors evaluate their method across various models (BERT, OPTs, BLOOM) and tasks (GLUE, PIQA, HellaSwag), demonstrating significant performance improvements, particularly in INT6 quantization, with minimal degradation compared to FP16. The method's efficiency is underscored by real latency measurements, showing a 1.5x speed-up over FP16.

### Strengths and Weaknesses
Strengths:
- The motivation for addressing outlier characteristics in LLMs is clear and reasonable.
- Extensive experiments across different network sizes and quantization settings validate the proposed method's efficacy.
- The paper is well-structured, facilitating comprehension, and presents credible real-world latency results.

Weaknesses:
- The novelty of the proposed channel-wise shifting and scaling is questionable, as it resembles existing techniques like those in SmoothQuant and AWQ, which are not adequately discussed.
- Generalization to other models, particularly LLaMA, is not sufficiently explored, raising concerns about the method's broader applicability.
- The analysis of how shifting and scaling influence quantization error is insufficient, lacking a detailed examination of their effects on subsequent layers.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the novelty of their method by clearly differentiating it from SmoothQuant and AWQ. Additionally, expanding the generalization of the method to include widely-used models like LLaMA would strengthen the paper. We also suggest providing a more comprehensive analysis of how the proposed shifting and scaling methods affect quantization error, particularly regarding their correlation with module-specific distributions. Finally, clarifying the implementation details for 8-bit model inference and addressing the potential for combining this method with per-token quantization would enhance the paper's robustness.