ID: vecFROHnL4
Title: MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 6, 7, 7, -1, -1
Original Confidences: 3, 4, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents MARVEL, a benchmark designed to evaluate the abstract visual reasoning (AVR) abilities of multi-modal large language models (MLLMs). The benchmark includes 770 puzzles based on six core cognitive patterns, geometric and abstract shapes, and five task configurations. It features a hierarchical evaluation framework that integrates both AVR and perception questions, allowing for a nuanced assessment of models' reasoning and perception capabilities. Experiments demonstrate that all tested MLLMs perform significantly worse than humans on MARVEL, revealing substantial gaps in their performance and highlighting the importance of visual perception in abstract reasoning.

### Strengths and Weaknesses
Strengths:
- The paper introduces a well-structured benchmark, MARVEL, that effectively addresses limitations of existing AVR benchmarks by incorporating diverse reasoning patterns and a hierarchical evaluation framework.
- The study tackles significant questions regarding MLLMs' abstract visual reasoning abilities, emphasizing the need for further research due to notable performance gaps compared to human capabilities.
- The writing is clear and accessible, enhancing the communication of objectives, methodology, and findings.
- Comprehensive experiments on ten representative MLLMs provide valuable insights into their performance and limitations, particularly in visual comprehension.

Weaknesses:
- The dataset's challenging nature may stem from MLLMs not being trained on similar tasks, suggesting a need for additional fine-tuning experiments to enhance comprehensiveness.
- The benchmark's reliability is questioned, as AVR questions are also difficult for humans; expert evaluations may be necessary to confirm its validity.
- The paper lacks discussions on potential enhancements for MLLMs' abstract visual reasoning capabilities, which would add value to the contributions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of certain concepts by including more examples, particularly for terms like "reassembling format," "geometric shapes," and "inductive bias." Additionally, we suggest conducting further fine-tuning experiments, possibly dividing the dataset into 4:1 training and evaluation sets, to explore performance improvements. To enhance the benchmark's validity, consider implementing a visual perception difficulty curriculum and presenting adaptation results alongside corresponding evaluations for direct comparison. Finally, we encourage a more thorough discussion on the practical significance of the AVR task and enhanced quantitative comparisons with existing benchmarks to clarify MARVEL's positioning within the field.