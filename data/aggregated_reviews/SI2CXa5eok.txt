ID: SI2CXa5eok
Title: AMR Parsing with Causal Hierarchical Attention and Pointers
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for AMR parsing called CHAP, which enhances a translation-based approach by incorporating a decoder that models structure while remaining compatible with pretrained transformer decoders like BART. The architecture features three key modifications: (1) a hierarchical attention mechanism based on Transformer Grammars, (2) a linearized tree representing a depth-first traversal of the AMR, and (3) a coreference layer utilizing an attention-based pointer mechanism to connect nodes, thereby eliminating arbitrarily-named variables. The authors demonstrate that CHAP outperforms several baselines on AMR 2.0 and AMR 3.0 datasets, with an ablation study indicating that the combination of all aspects of CHAP yields the best performance.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, organized, and motivated, making it enjoyable to read.
- It presents state-of-the-art results and innovative improvements over prior work.
- The authors justify their architectural choices by exploring multiple alternatives for hierarchical attention.

Weaknesses:
- The experimental section lacks standard deviations or significance testing for close SMATCH scores, raising concerns about the reliability of the results.
- The applicability of the method appears somewhat narrow, with no discussion of applications beyond AMR parsing.
- Some presentation issues and unclarities need to be addressed for a camera-ready version.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental results by including standard deviations or significance testing for SMATCH scores. Additionally, addressing the narrow applicability by discussing potential applications beyond AMR parsing would strengthen the paper. We suggest refining the presentation issues noted by reviewers, such as ensuring figures are correctly placed and enhancing the clarity of the algorithm descriptions. Finally, we encourage the authors to clarify any ambiguous terms and ensure consistent terminology throughout the paper.