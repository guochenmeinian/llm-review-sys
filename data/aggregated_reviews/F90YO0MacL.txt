ID: F90YO0MacL
Title: Towards Learning to Reason: Comparing LLMs with Neuro-Symbolic on Arithmetic Relations in Abstract Reasoning
Conference: AAAI
Year: 2024
Number of Reviews: 2
Original Ratings: 6, 6
Original Confidences: 5, 2

Aggregated Review:
### Key Points
This paper presents a comparison between leading large language models (LLMs) such as GPT-4 and Llama-3 and the Abductive Rule Learner with Context-awareness (ARLC), a neuro-symbolic approach, in solving Ravenâ€™s progressive matrices (RPM). The authors extend the RPM tests to larger matrices to facilitate out-of-distribution evaluation, thereby better assessing the models' reasoning abilities. The paper explores the application of vector symbolic architecture and abductive rule learning in abstract reasoning tasks.

### Strengths and Weaknesses
Strengths:  
- Comprehensive empirical study of various models on an interesting reasoning task.  
- Mostly well-written and easy to follow.  

Weaknesses:  
- Limited insights, as LLMs are already known to struggle with arithmetic reasoning.  
- Insufficient discussion on the significance of the I-RAVEN dataset and the potential applicability of ARLC beyond the RPM task.  

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the variable names in Equations 2 and 3, specifically defining c1, c2,...c12, and clarifying whether i and j represent row and column numbers. Additionally, we suggest providing an explanation for the lower accuracy of ARLC_p->1 compared to ARLC_learn, as well as including results for ARLC_p->1 in Tables 5 and 6. It would be beneficial to elaborate on the manual programming of weights and rule initialization for a better understanding of the different variants. Furthermore, we encourage the authors to consider providing knowledge about the rules to LLMs for a fairer comparison and to explore the application of other interpretable rule learning frameworks like ProbFOIL. Lastly, including examples of the rules learned by the approach would enhance comprehension of the significance of vector symbolic architecture.