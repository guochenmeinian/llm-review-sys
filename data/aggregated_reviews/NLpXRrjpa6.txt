ID: NLpXRrjpa6
Title: Policy Gradient for Rectangular Robust Markov Decision Processes
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 7, 6, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates policy gradient methods within the framework of robust Markov Decision Processes (MDPs) characterized by unknown transition kernels. The authors propose a robust policy gradient (RPG) algorithm that computes policy gradients under a rectangular uncertainty set, achieving the same time complexity as non-robust methods. The paper provides closed-form expressions for worst-case rewards and transition models, and demonstrates the efficiency of the proposed algorithm through numerical simulations.

### Strengths and Weaknesses
Strengths:
- The integration of policy gradients with robust MDPs presents a practical approach with promising results.
- The paper is well-structured and accessible, providing clear theoretical insights and a thorough literature review.
- The proposed method is computationally efficient, requiring only nominal data samples, and is supported by theoretical guarantees.

Weaknesses:
- The modeling of the uncertainty set is restrictive, particularly in high-dimensional settings, as it requires the nominal transition kernel to be strictly positive everywhere.
- The motivation for using rectangular ambiguity sets is unclear, and the paper lacks a discussion on the statistical properties of the chosen uncertainty set.
- Numerical experiments are limited, lacking detailed settings for replication and failing to evaluate the performance of learned policies against state-of-the-art baselines.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the uncertainty set modeling by providing a more robust justification for the choice of the rectangular ambiguity set and addressing its limitations. Additionally, we suggest enhancing the experimental section by including more comprehensive results that evaluate the performance of the proposed RPG algorithm against baseline methods, ideally presented in visual formats. Furthermore, we encourage the authors to centralize the notation definitions to aid reader comprehension and to discuss the implications of their method in the context of stochastic approximation more thoroughly.