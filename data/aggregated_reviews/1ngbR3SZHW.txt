ID: 1ngbR3SZHW
Title: What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks
Conference: NeurIPS
Year: 2023
Number of Reviews: 32
Original Ratings: 6, 7, 7, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive benchmark for evaluating large language models (LLMs) such as GPT-3.5, GPT-4, Davinci-003, Llama2-13b-chat, and Galactic-30b in the chemistry domain, focusing on practical tasks including molecular property prediction, synthesis prediction, text-based molecule generation, and captioning. The authors apply in-context learning (ICL) prompting to enhance model performance and acknowledge the limitations of their benchmark, particularly its lack of coverage in areas like spectroscopy. They highlight the challenges of designing advanced prompting techniques tailored to specific tasks and LLMs, while also addressing the potential negative impacts of generating harmful chemicals and the need for ethical oversight in AI applications. Experimental results indicate that while LLMs achieve competitive results in several tasks, they struggle notably with reaction prediction, retrosynthesis, and name prediction.

### Strengths and Weaknesses
Strengths:
- The benchmark is recognized as one of the most comprehensive in the field, covering a wide range of chemistry-related tasks.
- The work is well-motivated and provides valuable insights for leveraging LLMs in chemical science.
- The implementation of ICL prompts effectively enhances LLM performance by providing relevant chemical context.
- The authors have incorporated additional LLMs and baselines, enhancing the comprehensiveness of their benchmarking.
- The introduction of the iupac2formula task demonstrates responsiveness to reviewer feedback and addresses limitations in the original approach.
- The experimental design is robust, with thorough discussions on benchmarking results and potential lessons learned.

Weaknesses:
- The benchmark primarily focuses on property prediction and molecular understanding, neglecting the use of LLMs as planners for chemical experiments.
- The evaluation is limited to a few models, with the potential for a more comprehensive benchmark by including additional LLMs like LLaMA and Chinchilla.
- The choice of using top-k similar molecules in ICL may limit diversity; exploring dissimilar molecules could provide richer chemical information.
- The work does not consider tuning the models specifically for chemical tasks, which may restrict their full potential.
- The handling of SMILES representations may lead to invalid molecules; alternative representations like SELFIES could be explored, although SELFIES performance was found to be inferior to SMILES.
- In name prediction tasks, the formula-to-SMILES mapping is ill-posed, leading to low accuracy.
- The inclusion of only MoIR as a baseline in property prediction limits the comparative evaluation; additional domain-specific models should be considered.
- Some areas, such as the exploration of advanced prompting techniques and the inclusion of more LLMs fine-tuned on chemistry data, could be expanded.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by including experiments that utilize LLMs as planners for chemical experiments, as mentioned in the Related Works. Additionally, expanding the evaluation to include more LLMs, such as LLaMA and Chinchilla, would provide a more comprehensive analysis. We suggest exploring the use of dissimilar molecules in ICL to enhance the diversity of chemical information. Furthermore, the authors should consider tuning the models specifically for chemical tasks to fully leverage their capabilities. We encourage the authors to investigate alternative string representations beyond SMILES and SELFIES that may yield better performance. In name prediction tasks, addressing the ill-posed nature of formula-to-SMILES mapping is crucial, and including more baselines in property prediction would strengthen the evaluation. Lastly, we recommend that the authors provide clearer discussions on the limitations of their benchmark, specifically regarding areas not covered, such as spectroscopy, and conduct a more detailed analysis of the discrepancies observed in the performance of models across different datasets, particularly in the context of the Photoswitch dataset.