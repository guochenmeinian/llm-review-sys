ID: LawAC9vh8q
Title: Enhancing the Ranking Context of Dense Retrieval through Reciprocal Nearest Neighbors
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an improved similarity metric based on reciprocal nearest neighbors and Jaccard similarity, which is effective for inference-time reranking and offline label smoothing of training data. The authors propose an evidence-based label smoothing method that mitigates false negatives by leveraging the similarity of candidate documents to annotated ground truth, enhancing training efficiency and effectiveness.

### Strengths and Weaknesses
Strengths:  
- The proposed methods are novel and practical, demonstrating soundness through detailed analysis and extensive experimentation on large-scale datasets.  
- The innovative use of the rNN method effectively mines additional relationship information, improving the efficiency of training.  

Weaknesses:  
- The paper lacks comparisons with contrastive learning loss functions and other models, which could provide a broader context for the results.  
- Some claims, such as the obscurity in lines 152-157 and the undefined concept of ranking context, detract from clarity.  
- The reported improvements may be seen as expected due to the common practice of using reciprocal nearest neighbors in research.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims, particularly in lines 152-157, and formally define the concept of ranking context used throughout the paper. Additionally, we suggest that the authors enhance Figure 1 by including multiple subfigures to illustrate differences in rankings with varying similarity definitions and the addition of new data points. Furthermore, we encourage the authors to discuss related works that address the challenges of relevance annotation sparsity to provide a more comprehensive context for their contributions.