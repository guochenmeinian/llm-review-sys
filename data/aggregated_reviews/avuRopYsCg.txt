ID: avuRopYsCg
Title: Discovering Intrinsic Spatial-Temporal Logic Rules to Explain Human Actions
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 7, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 2, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a model for inferring rules based on observation trajectories of (entity, time, location) to predict future events by maximizing probabilities of certain "events." The authors propose using an "encoder" network, similar to a variational autoencoder, with a transformer to define the probability distribution over possible rules. A decoder samples these rules to generate the next event. The model demonstrates improved trajectory prediction results on two datasets compared to existing models, with examples of learned rules provided.

### Strengths and Weaknesses
Strengths:
- A robust framework for representing spatio-temporal action rules based on logic is proposed.
- The "rules" are modeled as latent variables, allowing inference via the E-M algorithm in an encoder-decoder setup.
- Strong empirical results on trajectory prediction tasks are presented, along with illustrative examples.

Weaknesses:
- Clarity is lacking in estimating goal states and generating future trajectories, particularly in figures and experiments.
- The predicates must be manually defined, which may require redefinition for new scenarios.
- The evaluation is limited to two datasets, missing widely used benchmarks like ETH/UCY.

### Suggestions for Improvement
We recommend that the authors improve clarity on how goal states are estimated and how future trajectories are generated, particularly in figures. Additionally, we suggest including an algorithm that outlines the trajectory prediction process step-by-step. Conducting experiments in a synthetic setting with ground truth data could demonstrate the model's ability to recover rules. We also encourage the authors to explore other applications of this framework beyond trajectory prediction and consider whether the "rules" can relate to "policies" in Reinforcement Learning. Lastly, we recommend investigating whether a simpler model than a transformer could effectively model the rule probability distribution.