ID: VIRKdeFJIg
Title: Improving multimodal datasets with image captioning
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 8, 6, 6, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the use of synthetic captions for pre-training CLIP-based models instead of alt-text on large-scale datasets. The authors demonstrate that training CLIP-based models on synthetic captions, filtered for caption-image similarity, enhances retrieval performance on MS-COCO and Flickr datasets. The analysis reveals that improved caption alignment and image quality significantly impact performance, with synthetic captions offering advantages over raw alt-text. Furthermore, the authors explore the effectiveness of synthetic captions in enhancing multimodal datasets through the DataComp benchmark, which includes 38 diverse datasets. They find that the performance of models using synthetic captions is consistent across various architectures and tasks, as evidenced by new experiments in visual question answering (VQA) that show improved scores when synthetic captions are included. The authors clarify that while synthetic captions can enhance performance, they do not serve as a universal solution for all tasks. They also analyze the impact of synthetic captions on model performance regarding race and gender biases, demonstrating that synthetic captions improve outcomes for disadvantaged groups and reduce performance gaps. The authors have released code and data, including BLIP2 captions for both medium and large pools of DataComp.

### Strengths and Weaknesses
Strengths:
- The paper provides valuable insights into improving CLIP-like model training through data filtering, yielding strong and convincing results.
- It includes extensive empirical evidence supporting the use of synthetic captions across multiple tasks and models, with new experiments in VQA showing significant improvements.
- The manuscript is well-written, clear, and presents large-scale experiments that support its claims.
- The analysis of biases related to synthetic captions provides valuable insights into fairness in model performance.
- The release of code and data enhances reproducibility and accessibility of the research.

Weaknesses:
- The fit of the paper within the datasets and benchmarks track is unclear, as it primarily focuses on synthetic data's impact rather than dataset creation.
- The effectiveness of filtering methods is noted to be inconsistent across different datasets, which requires further exploration.
- The potential over-specialization due to the choice of captioning model architectures raises concerns about generalization to other tasks.
- Some figures lack sufficient explanation, making it difficult for readers to fully grasp the presented data, and certain figures and concepts, such as filtering impacts and definitions of "noise level," require further clarification.
- The discussion on the potential biases introduced by synthetic captions is acknowledged but requires more depth.
- Concerns about the ability to incorporate all updated details and experiments may affect the overall evaluation of the work.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper's fit within the datasets and benchmarks track by considering the release of the captioning data. Additionally, we suggest discussing the potential "self-distillation" effect of using specific captioning models and addressing how this may affect performance across various downstream tasks. The authors should also soften claims regarding clip similarity and image-text alignment, ensuring that the language accurately reflects the nuances of these concepts. Further analysis of filtering impacts, particularly in Figure 2, and a clearer definition of terms like "noise level" and "image quality" would enhance the manuscript. We also recommend that the authors improve the clarity of figures by providing more comprehensive legends and explanations to ensure that readers can easily interpret the data presented. It would be beneficial to expand the discussion on the implications of using synthetic captions, particularly regarding potential biases and their effects on model performance across various tasks. Finally, we encourage the authors to clarify the computational overhead associated with using synthetic data, as this is a critical consideration for practical applications, and to provide further analysis on track fit to strengthen the paper's alignment with the conference themes.