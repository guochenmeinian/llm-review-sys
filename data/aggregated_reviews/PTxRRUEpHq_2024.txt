ID: PTxRRUEpHq
Title: Gradient Methods for Online DR-Submodular Maximization with Stochastic Long-Term Constraints
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 6, 5, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into online DR-submodular maximization under long-term stochastic constraints, focusing on a stochastic setting where objective functions are i.i.d. sampled. The authors propose a stochastic gradient ascent-based algorithm that achieves $O(\sqrt{T})$ regret and constraint violation in both semi-bandit and first-order feedback settings. Notably, their approach requires only one gradient query per round, significantly improving query efficiency compared to previous works that necessitate $\sqrt{T}$ queries.

### Strengths and Weaknesses
Strengths:
- The paper introduces novel settings of stochastic utility and semi-bandit feedback, which extend beyond prior literature.
- The proposed algorithms are the first of their kind in these settings and demonstrate improved query efficiency.
- The writing is clear and well-structured, making the results accessible.

Weaknesses:
- The motivation for the stochastic DR-submodular setting is insufficiently articulated, particularly regarding the transition from adversarial to stochastic settings.
- The theoretical contributions, while sound, lack groundbreaking novelty, with some results being straightforward applications of existing techniques.
- The empirical evaluation is notably absent, which limits the demonstration of the proposed approach's effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the motivation for their stochastic DR-submodular setting by clearly explaining the limitations of previous adversarial approaches and the significance of relaxing this assumption. Additionally, we suggest incorporating empirical evaluations to illustrate the practical applicability of the proposed algorithms, potentially comparing against random baselines and existing methods where applicable. Finally, we encourage the authors to highlight the distinctions in their analysis and contributions compared to previous works to strengthen the perceived novelty of their results.