ID: NJPSvv0u3R
Title: Robust low-rank training via approximate orthonormal constraints
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 5, 6, 7, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a training approach that enhances the robustness of low-rank deep neural network (DNN) models by constraining low-rank weight matrices to be approximately orthogonal. The authors demonstrate that conventional low-rank training methods suffer from poor adversarial robustness due to ill-conditioned weight matrices, leading to large condition numbers. The proposed method shows substantial empirical gains, even with limited adversarial attacks, and includes a theoretical approximation guarantee to support its numerical advantages.

### Strengths and Weaknesses
Strengths:
- The paper addresses an important issue regarding the brittleness of low-rank models to adversarial perturbations and proposes a clear solution.
- It provides substantial empirical results and a well-organized presentation with clear explanations and visualizations.
- The theoretical inclusion of an approximation guarantee enhances the understanding of the method's effectiveness.

Weaknesses:
- The observation regarding why low-rank constrained matrices lead to large condition numbers is left unexplained.
- Empirical evaluations are limited to the fast gradient sign method (FGSM), which is considered a weak attack; a more robust evaluation using projected gradient descent (PGD) is recommended.
- The contribution is viewed as incremental, with prior work addressing similar issues without adequate comparison or acknowledgment.
- Some terminologies and notations are not clearly defined, which may confuse readers unfamiliar with the concepts.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the unexplained observation regarding condition numbers in low-rank matrices. Additionally, incorporating evaluations using PGD would strengthen the empirical findings. It is crucial to provide clearer definitions for key terminologies and notations, such as the Stiefel manifold and feature space, to enhance accessibility. The authors should also address the incremental nature of their contribution by comparing their method with existing works and discussing the advantages of low-rank training over other compression techniques. Finally, a well-defined conclusion summarizing the impact of the work and potential future directions should be included to provide closure to the paper.