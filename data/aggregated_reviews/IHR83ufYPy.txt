ID: IHR83ufYPy
Title: Leveraging sparse and shared feature activations for disentangled representation learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 7, 7, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel regularization technique for disentangled representation learning, based on two principles: 1) sparsity and 2) minimality. The authors propose that access to multiple tasks during training, where each task utilizes a subset of features, allows for the identification of disentangled factors. Theoretical results demonstrate that these principles are sufficient for achieving disentanglement. The approach is validated through empirical results, showing advantages over standard empirical risk minimization (ERM) training. Additionally, the authors respond to reviewer feedback, indicating their efforts to address questions and concerns raised during the review process, and express gratitude for the reviewers' suggestions.

### Strengths and Weaknesses
Strengths:
- The formalization of sparse sufficiency and minimality as principles for disentangled representation learning is a significant contribution.
- The empirical evaluation extends beyond disentanglement, demonstrating practical benefits of the proposed method.
- The overall motivation and idea are sound, with promising results in both synthetic and real-world scenarios.
- The authors have effectively addressed reviewer concerns, leading to an increase in the review score, and the clarification regarding disentanglement and the distinction between ERM has been positively received.

Weaknesses:
- Clarity issues persist, with missing details and symbols that hinder understanding, such as the unclear definitions in Proposition 2.1 and the introduction of the symbol $f_t^*$.
- The empirical evaluation lacks comparisons with other disentangled representation learning methods and few-shot/meta-learning techniques, particularly in Tables 1, 2, and 4.
- The connection between the sharing regularizer and the minimality principle is not well articulated, requiring further clarification.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by explicitly defining all symbols and terms, particularly in Proposition 2.1. Additionally, the authors should enhance the empirical evaluation by including comparisons with other disentangled representation learning methods and few-shot/meta-learning approaches in the main text, rather than relegating them to the appendix. Clarifying the relationship between the sharing regularizer and the minimality principle is also crucial for understanding the proposed method's effectiveness. Furthermore, we recommend that the authors improve clarity in their responses to ensure all reviewer concerns are thoroughly addressed and remain open to further feedback to enhance the paper's quality.