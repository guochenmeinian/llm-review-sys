ID: mljDUaQpln
Title: Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 4, 7, 8, -1
Original Confidences: 3, 4, 3, -1

Aggregated Review:
### Key Points
This paper presents Additional Logic Pre-Training (ALPT) to enhance the logical reasoning abilities of large language models (LLMs) using a synthetic corpus named PureLogicDiverse (PLD). The authors propose four design principles for the synthetic dataset, including reasoning with unknown facts, illogical reasoning, diverse reasoning rules, and linguistic expressions. By training llama-7b and 70b models on PLD, the authors demonstrate improvements in various benchmarks, confirming the effectiveness of their methods.

### Strengths and Weaknesses
Strengths:  
1. The experiments are solid, testing ALPT on different scales of LLMs across various NLP tasks, including logical reasoning and reading comprehension.  
2. The performance gain on the 70b model is notably larger than that on the 7b model, indicating the potential of ALPT.  
3. The paper is well-organized and presents a fluent narrative from design principles to dataset creation.  
4. An anonymous link for all code, model, and data is provided to ensure reproducibility.  

Weaknesses:  
1. The contribution appears incremental, with design principles resembling previous work, and results seem marginal compared to FLD baselines.  
2. The writing is complex, making it challenging for non-experts to understand; key descriptions are relegated to the appendix.  
3. There is a risk of overfitting to synthetic logic patterns, which may limit generalizability.  
4. The necessity of a wide vocabulary size is questioned, as it may not significantly impact performance and could complicate dataset efficiency.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in sections that explain the generator's functionality, to make it more accessible to non-experts. Additionally, providing results for FLD in Table 3 would strengthen the comparative analysis. It would also be beneficial to include statistics on dataset settings, such as the number of steps in training splits and the average number of rules per sample, to clarify comparisons with previous work. Finally, addressing the potential overfitting issue and the necessity of the vocabulary size could enhance the robustness of the findings.