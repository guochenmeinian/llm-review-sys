ID: 2anfut5geh
Title: Challenges in Context-Aware Neural Machine Translation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an in-depth analysis of context-aware neural machine translation (NMT), highlighting two main findings: 1) existing document-level corpora exhibit a sparse number of discourse phenomena that require inter-sentential context for accurate translation, and 2) context is less effective for tense and discourse markers. The authors propose a paragraph-to-paragraph translation approach and introduce a new paragraph-aligned Chinese-English dataset. The paper also evaluates the efficacy of context-aware NMT, comparing it to a strong sentence-level baseline, and discusses the challenges faced in this domain.

### Strengths and Weaknesses
Strengths:  
- The analysis is comprehensive, particularly regarding discourse phenomena sparsity and model performance.  
- The paper is well-written, with smooth text and extensive analyses that provide valuable insights into context-aware translation challenges.  
- The introduction of a challenging paragraph-aligned dataset is a significant contribution that may stimulate further research.  

Weaknesses:  
- Some findings are not novel and have been previously proposed in other studies.  
- The paragraph-to-paragraph translation framework may be perceived as simplistic and lacking in innovation.  
- The small size of the collected dataset raises questions about its overall usefulness.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contributions by addressing previously established findings more critically. Additionally, clarifying the source of target context during inference and whether the target-side input contexts are noised during training would enhance the paper's rigor. Including performance scores for pre-trained but not fine-tuned models could provide a more comprehensive understanding of the model's capabilities. Finally, addressing the potential misleading implications of the sentence-level NMT baseline's performance in section 4.3 would strengthen the paper's argument.