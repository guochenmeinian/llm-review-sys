ID: 83e3DPVrFC
Title: Rethinking The Training And Evaluation of Rich-Context Layout-to-Image Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a layout-to-image (L2I) generation method that utilizes a regional cross-attention module to enhance the representation of complex layout descriptions. The authors propose two new metrics for evaluating model performance in rich-context scenarios, validated through a comprehensive user study. The evaluation considers both training and performance aspects, aiming to address limitations in previous works.

### Strengths and Weaknesses
Strengths:
- The introduction of a regional cross-attention module is novel and improves the handling of complex layout descriptions compared to traditional self-attention approaches.
- The paper provides rigorous experimental validation, demonstrating notable performance improvements in rich-context layout-to-image generation.

Weaknesses:
- The proposed method's GFLOPs are presented, but the impact of region reorganization and regional cross-attention on real-time throughput is not analyzed.
- The performance gains may stem from the constructed rich-context dataset rather than the proposed module itself.
- Comparisons with other baselines may be unfair due to differences in training datasets.
- The new evaluation metrics, while reasonable, are intuitive extensions of existing metrics and may not constitute a strong contribution.
- The analysis of overlapping issues and desired properties for the layout-conditioning module lacks depth, and visual results for ablations would enhance understanding.

### Suggestions for Improvement
We recommend that the authors improve the analysis of runtime costs associated with the regional cross-attention module. Additionally, the authors should clarify whether performance gains are due to the dataset or the proposed method. To ensure fairness, we suggest that the authors retrain the compared baselines on the same dataset. We also encourage the authors to provide more detailed descriptions of the proposed evaluation methods and regional cross-attention in the main text or appendix. Including pseudocode for the algorithm and expanding the related work section would enhance clarity and comprehensiveness. Finally, addressing the diversity analysis of generated images would strengthen the paper's contributions.