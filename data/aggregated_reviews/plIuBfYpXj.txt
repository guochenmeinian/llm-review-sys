ID: plIuBfYpXj
Title: Towards Heterogeneous Long-tailed Learning: Benchmarking, Metrics, and Toolbox
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 8, 7, 6, 5, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 5, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new benchmark for learning on long-tail data, proposing a methodology for combining metrics to assess and characterize long-tailedness across various data types (tabular, sequential, grid/image, relational/graph) and problem domains. The authors evaluate a set of state-of-the-art (SOTA) algorithms using this benchmark, which is comprehensive and well-structured, addressing a significant issue that has received inadequate attention in the research community.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and presents a persuasive argument for the benchmark.  
- It addresses a critical problem in long-tailed learning that has been overlooked.  
- The benchmark is comprehensive, covering multiple datasets and domains, and includes a fair evaluation of 16 SOTA algorithms.

Weaknesses:  
- The novelty of the work appears limited, resembling more of an engineering project rather than groundbreaking research.  
- Some important aspects, such as imbalanced regression and certain data types like remote sensing and point cloud data, are not adequately addressed.  
- The evaluation metrics used are existing ones, and there is a lack of new datasets proposed.

### Suggestions for Improvement
We recommend that the authors improve the discussion of limitations and potential future enhancements to the benchmark. Additionally, consider including a study on data distribution for a more comprehensive understanding. It would be beneficial to discuss imbalanced regression in the context of the title or adjust the title to reflect a focus on long-tailed classification. We suggest updating the algorithms in Table 4 to include more recent SOTA work, such as SHIKE (CVPR 2023). Furthermore, we encourage the authors to propose a new, more comprehensive evaluation metric and to maintain the GitHub repository actively, allowing for community contributions. Lastly, we recommend introducing and discussing the three angles in Section 2.2 to clarify their support for the structure of HeroLT.