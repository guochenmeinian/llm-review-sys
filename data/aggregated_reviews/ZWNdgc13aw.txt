ID: ZWNdgc13aw
Title: NeoRL: Efficient Exploration for Nonepisodic RL
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 6, 8, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 2, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents NeoRL, a model-based reinforcement learning (RL) algorithm designed for continuous state-action spaces in nonepisodic settings, allowing the agent to learn from a single trajectory without resets. The authors provide the first regret bound for nonepisodic RL in general nonlinear systems, leveraging well-calibrated probabilistic models for exploration. The empirical results indicate that NeoRL achieves sublinear regret and converges to optimal average cost across various environments.

### Strengths and Weaknesses
Strengths:
1. The paper addresses an important gap in the literature by providing a regret bound for nonepisodic RL in nonlinear systems.
2. NeoRL is based on the optimism principle, offering a theoretically justified exploration strategy.
3. The experiments demonstrate the practical effectiveness of NeoRL, showing lower average cost and cumulative regret compared to baselines.

Weaknesses:
1. The optimization problem in Equation (6) for policy selection may be computationally expensive for high-dimensional systems.
2. The paper lacks a comprehensive evaluation across diverse environments and comparisons with recent state-of-the-art methods.
3. The presentation may be challenging for readers without a strong control theory background, making it difficult to discern the algorithmic contributions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing a more intuitive explanation of how NeoRL differs from existing frameworks, particularly [1,2], and which specific parts enhance performance in nonepisodic settings. Additionally, we suggest including comparisons with more recent model-based RL algorithms, such as [3-7], to strengthen the empirical results. It would also be beneficial to analyze how critical design choices, like the planning horizon H_0, influence the performance of NeoRL.