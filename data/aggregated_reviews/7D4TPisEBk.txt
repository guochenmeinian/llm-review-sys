ID: 7D4TPisEBk
Title: Selective Demonstrations for Cross-domain Text-to-SQL
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for selecting demonstration instances for in-context learning in generating SQL queries from text descriptions, focusing on cross-domain text-to-SQL tasks. The authors propose ODIS, which combines synthetic in-domain demonstrations with out-of-domain examples, showing that this approach yields optimal performance. The analysis highlights the contributions of SQL constructs in in-domain demonstrations to the learning process. Experiments demonstrate that ODIS outperforms existing methods on two benchmarks, Spider and KaggleDBQA.

### Strengths and Weaknesses
Strengths:
- The analysis of the importance of SQL constructs in in-context learning is valuable and potentially extendable to other tasks.
- The combination of synthetic in-domain and out-of-domain demonstrations is a novel and effective approach, achieving state-of-the-art results.

Weaknesses:
- There are discrepancies between the results presented in Figure 3 and Tables 1 and 2, raising questions about the consistency of the findings.
- The novelty of using similarity for demonstration retrieval is limited, and the justification for the experimental setting is weak, particularly regarding the effort required for in-domain example annotation.
- The paper lacks experiments on the Spider test set, limiting the evaluation of generalizability.

### Suggestions for Improvement
We recommend that the authors improve the clarity and consistency of their results, particularly addressing the discrepancies noted between Figure 3 and Tables 1 and 2. Additionally, we suggest including experiments with GPT-3.5 to provide a more comprehensive evaluation of the proposed approach. The authors should also consider conducting empirical comparisons against gold in-domain few-shot in-context learning and few-shot fine-tuning methods to strengthen their claims. Finally, providing more detailed examples in the appendix would enhance reader understanding of the proposed methods.