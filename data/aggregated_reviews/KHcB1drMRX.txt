ID: KHcB1drMRX
Title: Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 7, 6, 5, -1, -1
Original Confidences: 5, 3, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents Chain-of-Sight (CoS), a novel vision-language bridge module aimed at accelerating the pre-training of Multimodal Large Language Models (MLLMs). The authors propose a sequence of visual resamplers that effectively capture visual details at various spatial scales, leading to a significant reduction in visual tokens during pre-training while maintaining or enhancing performance. The authors conduct extensive experiments, including evaluations on diverse benchmarks and scaling experiments, to support their model design and assumptions.

### Strengths and Weaknesses
Strengths:
1. The Chain-of-Sight approach innovatively combines multi-scale visual processing and token scaling, addressing a critical efficiency bottleneck in MLLM pre-training.
2. The method demonstrates significant pre-training acceleration, achieving up to a 73% reduction in wall-clock time without sacrificing performance.
3. The paper is well-organized and clearly written, with a comprehensive ablation study across various tasks.
4. The proposed method shows performance comparable to or better than existing approaches despite shorter training times.

Weaknesses:
1. The authors do not discuss the potential impact of training data scale, which could clarify how Chain-of-Sight improves performance across tasks.
2. Experimental results are limited to CLIP-ViT-L/14 as the visual encoder and Vicuna, necessitating results from other models for broader validation.
3. The explanation regarding the determination of token numbers during pre-training and fine-tuning is insufficient.
4. The hierarchical (multi-scale) visual input method has been explored in prior works, contradicting claims of under-exploration in the context of MLLMs.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the impact of training data scale to clarify its effects on performance. Additionally, including experimental results from a wider range of visual encoders would strengthen the findings. The authors should provide a clearer explanation of how the number of tokens during pre-training and fine-tuning was determined. Furthermore, we suggest addressing the existing literature on multi-scale strategies to accurately reflect the novelty of their approach. Lastly, clarifying the settings of ablation experiments and ensuring the accuracy of experimental results in tables would enhance the paper's rigor.