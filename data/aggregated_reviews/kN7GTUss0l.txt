ID: kN7GTUss0l
Title: This Too Shall Pass: Removing Stale Observations in Dynamic Bayesian Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 3, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel algorithm for dynamic Bayesian optimization (DBO), referred to as W-DBO, which utilizes a Wasserstein distance-based criterion to identify and remove irrelevant observations from the dataset. The authors argue that as functions change over time, certain data points become less relevant, and their removal can enhance computational efficiency while maintaining high sampling frequency. The authors clarify that the runtime metric used in their evaluation is influenced by implementation quality and hyperparameters, emphasizing the importance of defining observation relevancy through the Wasserstein distance between Gaussian process (GP) posteriors. The proposed W-DBO algorithm is shown to outperform existing methods in various benchmarks, demonstrating its effectiveness in optimizing time-varying functions.

### Strengths and Weaknesses
Strengths:
- Originality: The introduction of a Wasserstein distance-based criterion is a novel approach to addressing the challenge of managing irrelevant data in DBO.
- Quality: The paper is well-written, with clear figures and tables that effectively convey results.
- Clarity: The methodology is clearly articulated, making it easy to understand the authors' approach and its significance.
- Significance: The problem addressed is relevant and important, with potential applications across various domains.
- Empirical Evidence: The authors provide a clear definition of observation relevancy and support it with empirical evidence.
- Consistency: The implementation of algorithms is consistent, utilizing the same BO library, which enhances the reliability of the results.

Weaknesses:
- Lack of comparison: The paper does not sufficiently compare the proposed method with existing sparse Gaussian process techniques and their relevance to online learning.
- Evaluation concerns: The experimental setup lacks a conventional framework, particularly regarding the use of non-ARD kernels, which may skew benchmarking fairness.
- Limited dynamic testing: Most synthetic benchmarks do not exhibit time-dependence, which is critical for validating the algorithm's applicability in dynamic settings.
- Sample Efficiency: The evaluation primarily focuses on runtime, which may not fully capture the sample efficiency of the algorithms.
- Evidence of Irrelevancy: There is a lack of direct evidence demonstrating that removed points are indeed irrelevant, as requested by reviewers.
- Missing Plots: The authors do not include plots with the number of iterations on the x-axis, which could provide additional insights.

### Suggestions for Improvement
We recommend that the authors improve the discussion of sparse Gaussian processes and their relationship to online learning, including comparisons with relevant methods. Additionally, we suggest running experiments with an ARD kernel on symmetric test functions to ensure fairness in benchmarking. It would also be beneficial to include real-world examples or dynamic optimization case studies to strengthen the motivation for the proposed method. We encourage the authors to provide clearer evidence for the staleness of removed observations, possibly by assessing predictive performance before and after removal compared to previous methods. Furthermore, we recommend including plots with the number of iterations on the x-axis in a dedicated appendix to enhance clarity in the evaluation. Lastly, we urge the authors to discuss the implications of their relevancy metric and removal policy more thoroughly to address concerns about potential confounding factors.