ID: 3LdaPmAnji
Title: EDeR: Towards Understanding Dependency Relations Between Events
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Event Dependency Relation (EDeR) dataset, annotating binary argument relations on nearly 12,000 predicate pairs from OntoNotes. The dataset categorizes relations into four labels: **required**, **optional**, **condition**, and **independent**. The authors propose baseline models for relation classification using fine-tuned pretrained language models and few-shot settings with ChatGPT, demonstrating performance variation across input representations. The results suggest that integrating event dependency information may enhance performance in related tasks such as semantic role labeling and coreference resolution.

### Strengths and Weaknesses
Strengths:
- The dataset is well-constructed and relevant to the NLP community, particularly in event semantics and information extraction.
- The experiments establish accessible baselines and demonstrate the dataset's utility for existing tasks.
- High-quality annotation results are achieved through a rigorous multi-level qualification-based procedure.

Weaknesses:
- The novelty of the paper is perceived as insufficient.
- The paper lacks standard inter-annotator agreement metrics, making it challenging to fully assess dataset quality.
- Experimental results do not effectively showcase the benefits of fine-grained classes.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the data collection process, specifically addressing how the total number of documents for annotation was determined. Additionally, we suggest including inter-annotator agreement metrics and other quality statistics to enhance the dataset's credibility. The authors should clarify the rationale behind treating non-argument labels as one category, as the distinction is semantically significant. Furthermore, we advise a careful proofreading to rectify typos and improve presentation, including explicitly presenting majority class baseline accuracy and F1 scores in Table 3. Lastly, providing explicit examples of input variations and a breakdown of model performance by label would enrich the analysis.