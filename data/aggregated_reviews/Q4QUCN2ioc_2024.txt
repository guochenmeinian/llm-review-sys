ID: Q4QUCN2ioc
Title: An End-To-End Graph Attention Network Hashing for Cross-Modal Retrieval
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 6, 3, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 5, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a cross-modal hash retrieval method utilizing graph attention networks, termed EGATH. The authors propose an end-to-end architecture that integrates CLIP and Transformer techniques, featuring three main components: feature extraction, a graph attention classifier, and a hash code module. The model effectively captures semantic information and optimizes hash codes through a dedicated strategy and loss function. Extensive experiments demonstrate that the proposed method outperforms existing approaches in cross-modal hash retrieval.

### Strengths and Weaknesses
Strengths:  
1. The paper offers a novel perspective on cross-modal hash retrieval by integrating advanced techniques within its framework.  
2. Utilizing GAT as a tag classifier enhances the semantic capture of tags, leading to improved experimental outcomes.  
3. The framework's components are validated through comprehensive experiments, confirming their necessity.  

Weaknesses:  
1. Insufficient detail is provided on the integration of CLIP and Transformer models.  
2. The advantages of the Graph Attention Network over other methods require further clarification.  
3. The paper contains typographical errors in mathematical formulas, such as the notation of vectors and matrices, and punctuation issues in equations.  
4. The title "Network Hash" appears uncommon and may need reconsideration.  
5. The results lack comparison with a broader range of recent methods, limiting the significance of the contributions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the integration process for CLIP and Transformer models. Additionally, further explanation of the Graph Attention Network's advantages over alternative methods is necessary. We suggest correcting typographical errors in mathematical formulas, including ensuring proper notation and punctuation. The authors should also consider expanding the experimental comparisons to include more recent works in the field. Lastly, adding an Algorithm table to summarize the training and optimization process would enhance the paper's clarity.