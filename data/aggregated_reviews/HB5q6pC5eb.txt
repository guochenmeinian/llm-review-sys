ID: HB5q6pC5eb
Title: PertEval: Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant Perturbations
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 8, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents PertEval, an evaluation toolkit designed to assess the reliability and robustness of large language models (LLMs) through knowledge-invariant perturbations. The authors propose innovative perturbation strategies and validation methods, including LLM-based knowledge invariance scoring and testing on master questions. While the methodology is well-structured and the findings reveal significant performance gaps in contemporary models, the validation of knowledge invariance is criticized for being superficial and lacking comprehensive human evaluation.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel evaluation toolkit, PertEval, which systematically addresses the limitations of traditional benchmarks in evaluating LLM knowledge capacity.
2. It employs comprehensive validation techniques, ensuring reproducibility and transparency in the evaluation process.
3. The clarity and logical structure of the paper facilitate understanding, with detailed explanations of tables and figures.

Weaknesses:
1. The validation of knowledge invariance relies on limited automatic tests without manual evaluation, raising concerns about the robustness of the claims.
2. The evaluation primarily uses GPT-4, which may introduce bias, and the implications of this choice are not adequately discussed.
3. The paper lacks a thorough comparison with prior work, which diminishes its contextual relevance.

### Suggestions for Improvement
We recommend that the authors improve the validation of knowledge invariance by incorporating human evaluation to complement the automatic scoring methods. Additionally, it would be beneficial to provide more concrete examples and detailed explanations of the perturbation strategies in the main text rather than relegating them to the appendix. The authors should also clarify the limitations of their approach and discuss the potential for adaptive optimization of LLMs against PertEval. Furthermore, we suggest enhancing the literature review to better situate their work within the existing body of research and to address the gaps in prior studies. Lastly, including the exact costs in USD per experiment in the main body would provide clearer context for the methodology.