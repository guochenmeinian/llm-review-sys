ID: 5kthqxbK7r
Title: On the Efficiency of ERM in Feature Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 4, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of empirical risk minimization (ERM) within a feature learning context, demonstrating that the statistical error converges to a quantity dependent on an empirical process defined by optimal predictors. The authors derive both asymptotic and non-asymptotic results, revealing that ERM behaves similarly to an oracle procedure when a unique optimal feature map exists. Additionally, the paper provides a theoretical framework related to statistical learning theory (STL) and aims to derive upper and lower bounds on the performance of machine learning procedures. The authors acknowledge a potential misalignment in the use of the term "statistical learning theory" and plan to clarify this in the paper. They emphasize the importance of matching lower bounds for a purely theoretical paper, noting that while their upper bound is asymptotically tight, a matching lower bound is not currently established.

### Strengths and Weaknesses
Strengths:
- The formalism is exceptionally clean and lends itself to further study in various settings.
- The authors provide sharp asymptotic results that offer insights into limiting distributions using universal, Gaussian empirical processes.
- The proofs are effective and demonstrate a strong command of the relevant technical machinery.
- The writing is clear, and the presentation builds considerable intuition before introducing more complex non-asymptotic bounds.
- The paper connects to statistical learning theory and discusses the implications of derived bounds, showing a willingness to clarify terminological discrepancies and engage with reviewer feedback.

Weaknesses:
- The qualitative findings are not particularly surprising, as the localization phenomenon is already well-known in the context of ERM.
- The non-asymptotic dependence on the error probability $\delta$ is poor, with an $O(1/\delta)$ dependence that is not integrable, limiting the practical applicability of the results.
- The analysis overlooks the role of the learning algorithm and its implicit biases, which could affect the effective size of the features.
- The title may mislead readers into thinking the paper focuses on how ERM aids feature learning, rather than its statistical performance in this context.
- The discussion on the potential to explain the double descent phenomenon in deep learning is deemed unnecessary and lacks relevance.
- The paper does not provide a minimax lower bound, which is considered a critical aspect of STL.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contributions by addressing the misleading title and providing a more thorough literature review on feature learning, particularly regarding recent studies on neural collapse. Additionally, we suggest including the counterexample from the rebuttal in the main text as a motivational and didactic example. The authors should elaborate on the intuition behind the terms defined in the paper, such as $G_n(t)$ and $\Lambda_n(t)$, and consider adding a section in the appendix that clarifies the concepts of Glivenko-Cantelli and Donsker classes. Furthermore, we recommend improving the clarity of their terminology regarding statistical learning theory and explicitly addressing the sharpness of the derived bounds, including a discussion on lower bounds. We suggest removing the discussion about the potential to explain the double descent phenomenon as it does not contribute meaningfully to the paper's objectives. Finally, we encourage the authors to include a minimax lower bound to enhance the paper's contribution to the field and to discuss the implications of their findings for generalization in deep learning more comprehensively, including potential limitations of their assumptions.