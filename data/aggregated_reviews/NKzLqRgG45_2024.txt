ID: NKzLqRgG45
Title: Parameter-Inverted Image Pyramid Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 9, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method called Parameter-Inverted Image Pyramid Networks (PIIP) that addresses the computational overhead of image pyramids by utilizing models of varying sizes for different image resolutions. The authors propose a feature interaction mechanism to integrate information across spatial scales, achieving significant results in object detection, segmentation, and classification tasks. Extensive experiments validate the effectiveness of the proposed framework.

### Strengths and Weaknesses
Strengths:
1. The paper is well-organized and presents its content clearly.
2. The parameter-inverted approach, where larger models are used for smaller images, is novel and intriguing.
3. The method is straightforward, leveraging existing ViT models, and can be easily extended to other tasks.
4. The experiments are thorough and convincing, showcasing impressive performance, particularly with 6B models.

Weaknesses:
1. The authors do not clarify if the FLOPs and parameters in Tables 1 and 3 include a branch merging module.
2. The discussion of other multi-resolution networks, such as HRNet, is insufficient.
3. The framework could be further validated using stronger pre-training methods, like ViTDet-L versus PIIP-SBL.
4. The authors mention layer-wise learning rate decay but do not explain how to handle ViT combinations with differing layer counts.
5. The text in Figure 1 could be larger for better readability.

### Suggestions for Improvement
We recommend that the authors improve clarity by specifying whether the FLOPs and parameters in Tables 1 and 3 include a branch merging module. Additionally, we suggest expanding the discussion on other multi-resolution networks, such as HRNet, to provide a more comprehensive context. The authors should also consider validating the framework with stronger pre-training methods, like ViTDet-L versus PIIP-SBL, to enhance the method's effectiveness. Furthermore, we advise the authors to clarify their approach to layer-wise learning rate decay in the context of different ViT layer counts. Lastly, we recommend increasing the text size in Figure 1 for improved readability.