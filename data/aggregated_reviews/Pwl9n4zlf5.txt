ID: Pwl9n4zlf5
Title: AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive Environmental Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 8, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AutoManual, a framework that enables LLM agents to autonomously develop an understanding of new environments and generate instruction manuals through interactive learning. The system consists of three components: a Planner agent that generates code-based plans, a Builder agent that updates a structured rule system based on observed trajectories, and a Formulator agent that compiles these rules into a manual. To reduce hallucinations in rule management, the authors implement a case-conditioned prompting strategy for the Builder. Evaluated on the ALFWorld and MiniWoB++ benchmarks, AutoManual shows high success rates and aims to overcome limitations of previous methods, such as path dependency and reliance on extensive human instructions.

### Strengths and Weaknesses
Strengths:
- The framework is original and creatively addresses online rule learning and manual generation by LLM agents.
- The case-conditioned prompting strategy for rule management is innovative and effectively mitigates hallucinations.
- The paper includes detailed examples, case studies, and careful comparisons to relevant baselines, enhancing its quality.
- The overall framework and key components are clearly explained, making the paper well-structured and easy to follow.

Weaknesses:
- The heavy reliance on advanced LLMs like GPT-4 and oracle feedback during rule construction limits broader applicability; results from fine-tuning smaller models on generated data are needed.
- Scalability to larger, more complex environments is uncertain, as the current approach places all rules in the LLM context.
- Evaluation of the generated manuals' quality and usefulness for humans is limited, and there is a lack of theoretical analysis on the rule learning process.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including results from fine-tuning smaller models on the generated data to assess broader applicability. Additionally, clarifying how the method can scale to more complex environments, such as SWE-Bench, would strengthen the paper. We suggest providing a more thorough evaluation of the quality and usefulness of the generated manuals for human users, potentially including user studies. Furthermore, addressing the incomplete ablation table and justifying the selection of specific ablations would enhance the paper's rigor. Lastly, including a GPT-3 baseline for AdaPlanner or explaining the rationale for its exclusion would address significant concerns raised in the reviews.