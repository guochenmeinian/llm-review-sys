ID: JtIqG47DAQ
Title: Penalising the biases in norm regularisation enforces sparsity
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 7, 5, 5, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical investigation of the representational cost of a single hidden layer ReLU network with bias parameters included in the regularization term. The authors propose that regularizing both weights and biases leads to minimum-norm interpolators that exhibit sparsity, defined as having a minimal number of kinks. The results indicate that the representational cost is characterized by the total variation of the second derivative of the function, weighted by a factor of \(\sqrt{1+x^2}\). The uniqueness of the minimum-norm interpolator is established, and the implications of this regularization approach are discussed.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and effectively positioned within the context of prior work.
- It provides rigorous and novel results regarding the impact of bias regularization, demonstrating that it enforces sparsity and uniqueness in the interpolated functions.
- The use of a dynamic programming algorithm to prove the uniqueness and sparsity properties is particularly commendable.

Weaknesses:
- The analysis is limited to univariate functions and a simplistic model, which may restrict its practical relevance.
- The relationship between the number of kinks and generalization properties is not clearly articulated, raising questions about the significance of sparsity in this context.
- The conditions under which the minimum-norm interpolator is the sparsest are overly restrictive, and the implications of the weight factor \(\sqrt{1+x^2}\) on the norm's properties are not well understood.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the practical implications of bias regularization, particularly in relation to common practices in neural network training. Clarifying the significance of sparsity in terms of kinks and its connection to generalization would strengthen the paper's claims. Additionally, addressing the restrictive conditions for the minimum-norm interpolator and providing insights into the implications of the weight factor on the norm's properties would enhance the theoretical framework. Lastly, expanding the analysis beyond univariate functions could broaden the applicability of the findings.