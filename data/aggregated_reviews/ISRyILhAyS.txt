ID: ISRyILhAyS
Title: Coordinating Distributed Example Orders for Provably Accelerated Training
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 5, 7, 5, 5, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Coordinated Distributed Gradient Balancing (CD-GraB) algorithm, which extends the centralized GraB method to a distributed setting, enabling provably accelerated training. The authors leverage kernel thinning to enhance the herding framework, demonstrating a linear speedup in convergence rate compared to GraB. The theoretical foundations are supported by empirical results across various benchmark tasks, showcasing the algorithm's efficiency over random reshuffling. While the method has been theoretically validated and prior concerns regarding empirical evaluation have been addressed, there remains uncertainty about its practical applicability within the community. The problem setting appears to be self-imposed, as distributed workers in cluster-based workloads typically access a shared file system, which mitigates the limitations of local samples. Furthermore, the issue of gradient disparities, a core challenge in the proposed setting, could be alleviated through established strategies such as gossip or communication primitives.

### Strengths and Weaknesses
Strengths:
- The proposed CD-GraB algorithm is innovative and effectively utilizes stale gradients to improve convergence rates.
- The theoretical foundation of the work is solid, with concerns about empirical evaluation adequately addressed by the authors.
- The paper provides a thorough theoretical analysis and well-executed empirical evaluations, clearly demonstrating the benefits of CD-GraB.
- The presentation is generally clear, with polished diagrams and reasonable notation.

Weaknesses:
- The practical applicability of the proposed method is questionable, potentially limiting its relevance to the community.
- The presentation could be improved, particularly in defining notation in Algorithm 1, which is not self-explanatory.
- The method relies on a specific parameter server architecture and may not efficiently handle large-scale training scenarios.
- The convergence analysis may not be applicable to adaptive optimization methods like AdamW, and the paper lacks a discussion on the implications of data heterogeneity.
- The problem setting seems artificially constrained, and existing strategies to manage gradient disparities are not fully explored.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Algorithm 1 by defining all notation used. Additionally, addressing the practical limitations of the proposed method in large-scale settings is essential; a discussion on how CD-GraB can be adapted for more general distributed training scenarios would be beneficial. We also suggest including experiments that explore the method's performance with different architectures, such as Transformers, and in heterogeneous data settings. Furthermore, demonstrating the method's effectiveness in real-world scenarios, particularly in cluster-based workloads where distributed workers utilize a shared file system, would enhance its practical applicability. Lastly, incorporating discussions on existing strategies for reducing gradient disparities, such as gossip or communication primitives, and adding a clear limitations section to discuss the gap in performance compared to state-of-the-art methods would strengthen the paper.