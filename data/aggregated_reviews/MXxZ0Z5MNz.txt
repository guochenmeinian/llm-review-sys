ID: MXxZ0Z5MNz
Title: Efficient Training of Energy-Based Models Using Jarzynski Equality
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 4, 7, 7, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a modification of the Energy-Based Model (EBM) training procedure inspired by Jarzynski equality and Sequential Monte Carlo, aiming to improve sampling efficiency and gradient estimation. The authors propose a new loss function that is valid even for non-convergent ULA samples, along with a sampling procedure that evolves both particles and model parameters, allowing for consistent convergence to the desired distribution. The method is validated on moderate-dimensional Gaussian mixtures and a subset of the MNIST dataset, with additional experiments on CIFAR-10 introduced during discussions. Experimental results indicate that the proposed method learns correct relative weightings of modes more effectively than standard EBM models, demonstrating its effectiveness in recovering modes and proportions accurately in toy examples.

### Strengths and Weaknesses
Strengths:
- The paper introduces a theoretically sound and novel approach to EBM training, addressing the slow mixing problem of conventional methods.
- The theoretical development of the proposed loss function is a major contribution, being valid for non-convergent ULA samples.
- The proposed method demonstrates the ability to recover modes and proportions accurately in toy examples, showing its effectiveness compared to standard methods.
- The writing is clear, and the results are well-presented, making the paper easy to understand.
- The proposed method does not significantly increase computational costs compared to standard EBM methods.
- A thorough appendix includes full proofs and additional experimental results.

Weaknesses:
- The empirical evaluation is limited, focusing primarily on toy datasets like MNIST, which raises concerns about the method's scalability and practical applicability to more complex datasets like CIFAR-10.
- The scope of experimental results is limited, and there is a lack of quantitative comparisons with existing methods, making it difficult to assess the proposed method's performance.
- The theoretical validity of the weight resampling technique in Section 3.3 is not discussed, raising concerns about its consistency with the loss derivation.
- The paper does not adequately address the computational and memory burdens associated with the proposed algorithm, particularly regarding the tracking of additional information.

### Suggestions for Improvement
We recommend that the authors improve the empirical evaluation by including more complex datasets, such as CIFAR-10 or CIFAR-100, and provide standard performance metrics like FID or Inception Score to substantiate their claims. Additionally, the authors should discuss the theoretical validity of the weight sampling step in Section 3.3, providing proof if it is consistent with the loss function or acknowledging it as a limitation if not. It would also be beneficial to clarify the scalability of their method, particularly concerning sample efficiency and computational demands. Lastly, addressing the biasedness of the estimator and providing a clearer discussion on the choice of batch sizes used in experiments would enhance the paper's rigor. Explicit sections discussing limitations and broader impacts should also be included in the paper.