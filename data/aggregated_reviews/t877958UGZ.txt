ID: t877958UGZ
Title: Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 4, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for efficient vision-language instruction tuning using Mixture-of-Modality Adaptation (MMA) with lightweight adapters. The authors propose a routing mechanism that allows the model to adaptively switch between single-modal and multi-modal instructions. The method is applied to the LLaMA model, achieving competitive performance on the ScienceQA dataset while maintaining a low number of trainable parameters.

### Strengths and Weaknesses
Strengths:
- The proposed method significantly reduces the fine-tuning cost of LLMs with multimodal inputs.
- The approach is conceptually concise and can be integrated into various models, enhancing both LLMs and vision encoders.
- Empirical results on ScienceQA are promising, outperforming some strong competitors like GPT-4.

Weaknesses:
- The evaluation is limited, primarily focusing on ScienceQA, which does not sufficiently demonstrate the model's multimodal capabilities across other datasets.
- The necessity of the routing mechanism is unclear, as previous works like BLIP-2 and MiniGPT4 have successfully used frozen LLMs without parameter updates.
- The paper does not adequately discuss the role of the routing function or how it preserves NLP capabilities while learning visual information.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including results from additional datasets, such as NoCaps and COCO, to provide a broader understanding of the model's capabilities. Additionally, we suggest clarifying the necessity of the routing mechanism and discussing its impact on performance, particularly regarding the scale factor's influence on the routing function. Furthermore, the authors should address the similarities with LLaMA-Adapter earlier in the paper and ensure a fair comparison using the same instruction tuning datasets.