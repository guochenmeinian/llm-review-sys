ID: 6uRrwWhZlM
Title: Prompt Optimization with EASE? Efficient Ordering-aware Automated Selection of Exemplars
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 6, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents EASE, an exemplar selection algorithm for in-context learning (ICL) of large language models (LLMs). EASE optimizes the selection and ordering of exemplars without requiring model fine-tuning or test-time retrieval, using a neural network to predict prompt performance based on hidden embeddings. The algorithm employs a bandit approach and optimal transport techniques to efficiently identify high-quality exemplars. Empirical results indicate that EASE outperforms various baselines, particularly in out-of-distribution (OOD) tasks, although its performance on Instruction Induction (II) tasks is close to random search baselines.

### Strengths and Weaknesses
Strengths:
- The paper covers a wide range of baseline methods and tasks in its experiments.
- It provides valuable insights into the impact of exemplar selection on specific tasks.
- The introduction of a novel approach combining neural bandit optimization and optimal transport is significant.

Weaknesses:
- EASE's performance on II tasks is nearly equivalent to random search for most tasks.
- The reliance on gpt-3.5-turbo-1106 for experiments raises reproducibility concerns due to potential unknown pre- and post-processing.
- The literature review lacks depth, omitting relevant prompt optimization approaches and failing to justify the chosen baseline set.
- The presentation is somewhat unclear, lacking a dedicated related work section and not adequately addressing the computational bottleneck of on-the-fly embedding computations.

### Suggestions for Improvement
We recommend that the authors improve the literature review by including a comprehensive related work section that discusses other prompt optimization methods. Additionally, the authors should validate EASE on open-weight models to enhance reproducibility. To address the computational bottleneck, we suggest providing insights or potential solutions for mitigating the on-the-fly computation of embeddings. Furthermore, we encourage the authors to conduct more experiments with real datasets and clarify the rationale behind exemplar selection to strengthen the paper's contributions. Lastly, enhancing the clarity of the presentation and addressing the complexity of joint optimization would benefit the overall readability and understanding of the method.