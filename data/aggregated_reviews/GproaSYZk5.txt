ID: GproaSYZk5
Title: Universal In-Context Approximation By Prompting Fully Recurrent Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 8, 7, 4, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the in-context learning capabilities of recurrent neural network (RNN) architectures, including RNNs, LSTMs, and gated models, proposing a new programming language, LSRL, to facilitate this exploration. The authors demonstrate that these architectures can serve as universal in-context approximators, capable of computing any function with the appropriate prompt. They also highlight the importance of multiplicative gating for enhancing numerical stability in these models.

### Strengths and Weaknesses
Strengths:
- The paper provides an intriguing perspective on universal approximation in the context of in-context learning.
- The introduction of LSRL as a programming language is innovative and may have independent significance.
- The exploration of numerical stability issues related to gating offers valuable insights into the role of gating in recurrent architectures.

Weaknesses:
- The review of recurrent architectures is inaccurate, particularly regarding the properties of Mamba and Hawk, which need clarification.
- The paper's density and complexity hinder readability, and the construction of proofs could benefit from clearer exposition.
- The practical implications of proving linear RNNs as universal approximators are questionable, given their known limitations in in-context learning.

### Suggestions for Improvement
We recommend that the authors improve the accuracy of their review on recurrent architectures, particularly regarding the properties of Mamba and Hawk, to enhance clarity from Section 2 onward. Additionally, we suggest including synthetic numerical experiments to complement the theoretical findings, such as comparing gradient descent solutions with constructed solutions and confirming the importance of gating. The authors should also clarify the practical significance of their findings on linear RNNs and address the unusual definition of gated RNNs used in the paper. Finally, we encourage the authors to provide a clearer distinction between their theoretical framework and existing expressivity studies on RNNs and feedforward networks.