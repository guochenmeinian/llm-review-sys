ID: pGOBEYcXzs
Title: Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 3, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for post-training binarization of large language models (LLMs) called BinaryMoS, which utilizes a mixture of scaling weights for the linear layer of binarized LLMs. Unlike the static scales in OneBit, BinaryMoS employs adaptive scaling weight experts based on the input representation of current tokens, resulting in enhanced representation capabilities with minimal computational and memory overhead. Experimental results indicate that BinaryMoS outperforms existing binarization methods across various LLM benchmarks.

### Strengths and Weaknesses
Strengths:
- BinaryMoS maintains minimal computational and memory overhead while keeping the architecture simple.
- The mixture-of-expert technique has potential applicability to other quantization methods.
- The method demonstrates effectiveness across a wide range of LLMs and outperforms other binarization techniques.

Weaknesses:
- The paper lacks a discussion on the cost of post-training adaptations, with training requiring three epochs over the selected dataset.
- Improvements over OneBit are marginal, and performance remains significantly lower than less aggressively quantized methods.
- The scientific and technological significance of the proposed method is perceived as low, with insufficient analysis on the number of scaling experts and their impact on performance.
- The motivation for extending ideas from Mixture-of-Experts (MoE) to One-Bit is unclear, and the benefits of using multiple experts compared to fewer are not convincingly demonstrated.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the cost of post-training adaptations and clarify the motivation for applying MoE concepts to One-Bit models. Additionally, a more thorough analysis of the scaling experts' impact on performance should be included, particularly regarding the significance of using multiple experts. We suggest addressing the omission of latency measurements compared to other binarization methods, as this is critical for real-world applications. Lastly, we encourage the authors to provide a clearer explanation of the equations presented, particularly regarding notation and dimension mismatches, to enhance clarity and correctness.