ID: WxnrX42rnS
Title: STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Stochastic Transformer-based wORld Model (STORM), an innovative world model architecture designed for reinforcement learning (RL). STORM employs a stochastic variational autoencoder to encode image inputs and utilizes a GPT-like sequential model to predict latent states. The architecture is trained on dynamics and policy based on outputs from these models, demonstrating improved performance and speed in the Atari100k environment compared to classic baseline methods.

### Strengths and Weaknesses
Strengths:
1. The authors have effectively combined previously introduced components into a cohesive and efficient method.
2. Experimental results validate STORM's effectiveness, particularly in enhancing the time efficiency of Model-Based Reinforcement Learning methods.

Weaknesses:
1. The paper lacks a comparison with Speedy Zero, a recent model-based RL method that also demonstrates good time efficiency and performance on Atari.
2. The generalizability of STORM to other tasks beyond Atari, such as MuJoCo or DMC, remains uncertain, which could diminish the practical significance of the work.
3. Further analysis on the advantages of using a stochastic variational autoencoder is needed.
4. The method's novelty is questioned, as STORM appears to be a minor variant of TWM with limited exploration of its incremental improvements.
5. The results show marginal improvements over DreamerV3, and recent model-free methods achieve similar or better performance.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including a comparison with Speedy Zero to provide a more comprehensive evaluation. Additionally, the authors should explore STORM's applicability to other environments to enhance its generalizability. A deeper analysis of the benefits of the stochastic variational autoencoder is necessary to clarify its contribution to STORM's performance. Furthermore, we suggest that the authors address the lack of novelty by clearly articulating the specific components that contribute to STORM's superior performance compared to TWM and other models. Lastly, expanding the ablation studies to include a wider range of tasks would strengthen the evaluation of the proposed method.