ID: fFJThJ94rY
Title: Switching Autoregressive Low-rank Tensor Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 8, 7, 6, 4, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Switching Autoregressive Low-rank Tensor (SALT) model, which integrates a low-rank tensor parameterization of autoregressive models with switching dynamics. The authors propose two main contributions: (1) the SALT model itself, which enhances existing tensor parametrizations with an EM-based learning and inference algorithm featuring closed-form updates, and (2) a theoretical connection between Linear Dynamical Systems (LDS) and Low-Rank Tensor Autoregression. The model is evaluated on simulated, behavioral, and neural datasets.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, providing clear explanations of the model and its components, despite the complexity of the notations.
- The theoretical link between LDS and Low-Rank Tensor AR models is significant and novel, aiding in understanding the relationship between these models and guiding tensor rank specification.
- SALT's closed-form updates for inference and learning are practically convincing, and the model effectively analyzes low-dimensional representations of observed time series.
- Empirical evaluations on both simulated and real-world datasets demonstrate the model's effectiveness, and the authors provide code for reproducibility.

Weaknesses:
- The title of Section 3.3 is misleading, as the theoretical connection pertains to LDS and Low Rank Tensor AR models, not SALT and sLDS.
- The graphical representation in Figure 1 is incorrect, lacking an arrow between \(z_t\) and \(z_{t+1}\).
- The paper does not adequately compare SALT to state-of-the-art models or discuss limitations, particularly regarding the Gaussian noise assumption and the lack of a thorough theoretical analysis.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Section 3.3 to accurately reflect the theoretical contributions related to LDS and Low Rank Tensor AR models. Additionally, we suggest correcting the graphical representation in Figure 1 to accurately depict the model dynamics. It would be beneficial to include a comparison of SALT with more recent state-of-the-art models, such as deep switching autoregressive factorization, and to provide a detailed discussion of the model's limitations, including its reliance on Gaussian noise models. Furthermore, we encourage the authors to clarify the hyperparameter selection process and consider discussing the implications of using L2 regularization in AR-HMMs more explicitly.