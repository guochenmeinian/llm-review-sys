ID: aTXhTD44nF
Title: USDC: A Dataset of $\underline{U}$ser $\underline{S}$tance and $\underline{D}$ogmatism in Long $\underline{C}$onversations
Conference: NeurIPS
Year: 2024
Number of Reviews: 24
Original Ratings: 6, 4, 4, 4, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the USDC dataset, a large-scale collection of 764 multi-user Reddit conversations annotated for user stance and dogmatism using LLMs like GPT-4 and Mistral. The authors propose that this dataset enhances the understanding of opinion shifts in long conversations and demonstrate its utility by fine-tuning smaller LLMs for stance and dogmatism classification tasks. The manuscript also addresses reviewer concerns, including analyses of the "lost in the middle" issue, recency bias, and conflict resolution between model annotations. The authors have expanded the discussion on the validation of LLM-generated annotations and provided inter-annotator agreement with human labels, clarifying how their work differs from prior studies.

### Strengths and Weaknesses
Strengths:
- The writing is generally clear and well-documented.
- The USDC dataset is a valuable resource for studying user opinions in long conversations.
- The use of advanced LLMs for annotation is innovative and enhances classification tasks.
- Comprehensive addressing of reviewer concerns through additional experiments and clarifications.
- Significant expansion of the discussion on LLM-generated annotations and inter-annotator agreement.
- Inclusion of detailed analyses that enhance the manuscript's quality.

Weaknesses:
- The heavy reliance on LLMs for annotation raises concerns about the reliability of the labels, as there is insufficient analysis of LLM annotation quality compared to human annotations.
- Some reviewers still express reservations regarding the comparison of inter-annotator agreement (IAA) between human annotators and LLM annotators, indicating a need for further clarity.
- The dataset lacks a comprehensive discussion of its advantages over existing datasets, making it difficult to assess its originality and contribution.
- The evaluation methods and the dataset's utility are not convincingly demonstrated, particularly regarding the effectiveness of LLM-generated labels.

### Suggestions for Improvement
We recommend that the authors improve the reliability of the dataset by incorporating a more detailed analysis of LLM annotations, including situations where LLMs may introduce errors and how these can be mitigated. Additionally, a comparative analysis with existing conversational stance-related datasets is necessary to clarify the dataset's unique contributions. We suggest that the authors conduct a rigorous evaluation of the dataset quality, potentially including human-in-the-loop assessments, and explicitly address ethical considerations related to data privacy and potential misuse. Furthermore, we advise revising the terminology in Section 4.3 to clarify the distinction between fine-tuning and instruction tuning. Lastly, we recommend improving the comparison of inter-annotator agreement by including IAA scores between human annotators themselves and comparing these with IAA scores between LLM annotators, providing a more meaningful context for the results presented. Correcting minor typos throughout the paper will also enhance clarity.