ID: zD6lXmTPPh
Title: A Novel Framework for Policy Mirror Descent with General Parameterization and Linear Convergence
Conference: NeurIPS
Year: 2023
Number of Reviews: 24
Original Ratings: 6, 7, 7, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a policy update scheme based on mirror descent for general function approximation, offering theoretical advancements in reinforcement learning (RL) by establishing upper bounds for algorithms utilizing mirror maps and parameterization functions. The authors claim to achieve linear convergence guarantees for policy mirror descent methods with general policy parameterization, extending previous results that were limited to specific settings. They provide a detailed analysis of convergence rates, specifically sub-linear and linear convergence as outlined in Theorem 4.3, and validate their theoretical results through numerical experiments aimed at demonstrating the decay rates of their bounds. However, the novelty and clarity of the theoretical contributions are questioned, particularly in relation to existing literature, and concerns are raised regarding the implications of approximation error on convergence.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with rigorous and well-defined main statements.
- It contributes to understanding function approximation in policy mirror descent methods, unifying many past approaches under a new framework.
- The theoretical analysis is comprehensive, and the proposed algorithm is straightforward to implement.
- The authors provide a comprehensive theoretical framework that enhances the understanding of convergence in RL algorithms.
- Empirical evaluations included in the rebuttal support the theoretical claims, particularly regarding convergence rates.
- The authors have made efforts to clarify their methodology and results in response to reviewer feedback.

Weaknesses:
- The main results, particularly Theorem 4.3, do not provide a meaningful upper bound on the sub-optimality gap due to unclear scaling of certain terms.
- Assumptions made on the policy iterate $\pi^t$ are considered inappropriate for the analysis of convergence.
- The theoretical results in some sections are adaptations of existing work, lacking new insights.
- Some reviewers express skepticism regarding the clarity and accuracy of the convergence rate charts, questioning the fitting of lines to the data.
- The absence of additional plots and experiments limits the ability to fully validate the theoretical claims.
- Concerns are raised about the potential vacuity of the bounds presented, particularly regarding the constants involved.

### Suggestions for Improvement
We recommend that the authors improve the characterization of the scales of terms involving $\epsilon_{approx}$, $v_\mu$, and $C_v$ to enhance the meaningfulness of their analysis. Additionally, we suggest revising the assumptions on $\pi^t$ to apply to all policies in the class, ensuring a more natural framework. To strengthen the paper, we encourage the inclusion of experimental results that demonstrate the practical implementation of Algorithm 1 and validate the theoretical claims. We also recommend improving the clarity of the convergence rate charts by providing log-log plots that clearly illustrate the slopes corresponding to the predicted rates, along with a more detailed explanation of how the slopes were computed. Furthermore, we suggest conducting additional experiments to validate their theoretical predictions, particularly by varying the $\gamma$ constant and observing the effects on convergence rates. Lastly, addressing concerns about the bounds being vacuous by providing comparisons with existing literature could strengthen the paper's arguments.