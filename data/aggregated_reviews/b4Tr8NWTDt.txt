ID: b4Tr8NWTDt
Title: Co-Learning Empirical Games and World Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 3, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to Policy Space Response Oracles (PSRO) by integrating a learned world model that operates concurrently with the iterative PSRO strategy expansion. The authors demonstrate that world models benefit from training on diverse strategy profiles generated through PSRO, leading to improved sample efficiency in PSRO best responses. The proposed Dyna-PSRO method shows lower-regret solutions compared to traditional PSRO, as evidenced by experiments across three games. Additionally, the paper provides a comprehensive analysis of the SumRegret metric and its implications for evaluating regret results without the final datapoint. The authors propose that combined-game regret serves as an effective approximation of regret while acknowledging the need for stronger methods to approximate Nash equilibrium. Experiments on strategic diversity indicate that a more diverse world model correlates with lower observation cross-entropy, supporting the thesis that diversity enhances model performance. The discussion on decision-time planning results emphasizes the multifaceted nature of planning, suggesting that agents employing both decision-time and background planning (DT+BG) are unlikely to perform worse than those using neither. The Dyna-PSRO results highlight the importance of including detailed walltime measurements and hyperparameter settings in the appendix for clarity.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant issue in PSRO literature regarding the need to re-learn policies from scratch, proposing a co-learning approach that leverages diverse experiences.
- The presentation is clear, and the experiments are well-designed, contributing valuable insights into the integration of world models with PSRO.
- The proposed Dyna-PSRO method is sound and demonstrates impressive performance improvements.
- The paper effectively addresses the SumRegret metric and its relevance to regret evaluation.
- The experiments demonstrate a clear relationship between strategic diversity and model performance.
- The authors provide detailed responses and clarifications that enhance the understanding of their methodology and findings.

Weaknesses:
- The related work section lacks references to several important PSRO variants and diversity-based frameworks, which diminishes the paper's contextual grounding.
- The paper could benefit from additional comparisons with other algorithms, such as NeuPL, and a more thorough exploration of the implications of strategic diversity.
- Some experimental results and figures are difficult to interpret, and the clarity of certain concepts, such as the SumRegret metric, could be improved.
- There is a lack of principled grounding for the proposed post-run experiments to test convergence, which may leave some readers unsatisfied.
- The terminology used, particularly regarding "policy" and "strategy," could lead to confusion without clear definitions.

### Suggestions for Improvement
We recommend that the authors improve the related work section by incorporating references to key PSRO variants and diversity-based frameworks to better position their contribution within the existing literature. Additionally, we suggest including comparisons with other algorithms like NeuPL to enhance the robustness of the evaluation. Expanding the experimental results to include more complex games and additional baselines would provide a clearer understanding of the generalizability of Dyna-PSRO. Furthermore, we advise clarifying the presentation of figures and metrics, particularly the SumRegret metric, to enhance reader comprehension. We also recommend improving the clarity of terminology by defining "policy" and "strategy" more explicitly to avoid ambiguity. Including a chart comparing the performance of DT, BG, and DT+BG would reinforce the claims about their effectiveness in combination. It would be beneficial to address the computational demands of the algorithms in the limitations section, providing insights into scaling in terms of time and memory. Lastly, we encourage the authors to explore the stability of the combined game in future work, as this could yield valuable insights into the convergence of their methods.