ID: C4SInFLvuB
Title: Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 6, 6, 4, 4, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a thorough investigation into the effects of reshuffling resampling splits on the generalization performance of hyperparameter optimization (HPO) strategies. The authors argue that reshuffling can lead to statistically significant improvements in model performance, particularly when the loss surface is flat and its estimate is noisy. They provide a theoretical analysis explaining how reshuffling impacts the asymptotic behavior of the validation loss surface and demonstrate its practical effectiveness through simulations and experiments. The findings indicate that reshuffling can yield test performances competitive with fixed splits and significantly enhance results for single train-validation holdout protocols.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel approach to HPO by proposing reshuffling of train-validation splits, contrasting with fixed splits for consistency.
2. It combines theoretical insights with empirical validation, particularly through a simulation study that confirms the theoretical results.
3. The writing is generally clear and well-organized, making complex ideas accessible.

Weaknesses:
1. The generalizability of results is limited, as experiments focus on specific datasets and HPO strategies; broader testing is needed.
2. There is insufficient discussion on the computational costs associated with reshuffling, which could be significant for large-scale applications.
3. The theoretical analysis lacks rigor in certain areas, particularly regarding the assumptions made in Theorems 2.1 and 2.2.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by conducting experiments on a wider range of datasets, models (e.g., SVM, ResNet on CIFAR-10/100, or ImageNet), and HPO methods (e.g., successive halving). Additionally, we suggest that the authors provide a clearer discussion on the computational costs of reshuffling and consider including a notation table to enhance readability. To aid understanding, we encourage the authors to add pseudo-code highlighting the reshuffling procedure and clarify the meaning of key variables introduced throughout the paper.