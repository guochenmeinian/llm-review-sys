ID: HMhEFKDQ6J
Title: Unifying GANs and Score-Based Diffusion as Generative Particle Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 6, 6, 5, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 2, 2, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a unified framework for generative modeling that connects particle flow methods, including score-based diffusion models and GANs. The authors propose two novel models: Score GAN, which utilizes score-based gradients for generator training, and Discriminator Flow, which synthesizes samples solely through the discriminator. The authors emphasize the importance of the noise level $\sigma$ and the parameter $K$, which dictates the number of score updates per generator update. They clarify that the training process involves continuous updates to the score of the generated distribution, akin to a discriminator, and that a higher learning rate ratio $r = \frac{\lambda}{\eta}$ can reduce the necessary number of steps $K$. The paper aims to highlight the theoretical connections between these generative approaches, although the performance of the proposed models does not achieve state-of-the-art results. Additionally, the authors discuss the implementation of time embeddings and the challenges of discretization in diffusion models, suggesting that their framework may mitigate discretization errors.

### Strengths and Weaknesses
Strengths:
- The formulation is well-presented and clearly articulates the unification of score-based and GAN-based modeling.
- The authors provide a clear and detailed explanation of the training dynamics of Score GANs.
- The contributions are clearly defined, and the limitations regarding performance expectations are transparently addressed.
- The rebuttal effectively addresses reviewer concerns, enhancing the clarity of the paper.
- The theoretical foundation is robust, and the paper is generally well-organized and readable, with valuable implications for future research.

Weaknesses:
- The training algorithms for Score GANs and Discriminator Flow lack clarity, particularly in how training progresses.
- The transition between key equations is treated too casually, potentially limiting the scope of the proposed framework.
- The experimental validation is limited, with only a few datasets tested, and the results do not surpass existing baseline methods.
- The paper is primarily theoretical, lacking robust empirical validation, and some reviewers express a desire for stronger experimental results, particularly for a conference like NeurIPS.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the training algorithms for Score GANs and Discriminator Flow, providing more detailed explanations of how the score networks are trained and utilized. Additionally, we suggest including discussions on the impact of sampling/gridding schemes on the formulations, particularly regarding the links between Langevin dynamics and the proposed models. The authors should also consider elaborating on the implications of the stochastic components in their equations and how these relate to generator training. Furthermore, we recommend that the authors enhance the empirical component of the paper by conducting more extensive experiments, including ablation studies on hyper-parameters and comparisons with a wider range of datasets and architectures to better validate their claims. Exploring the impact of different discretization methods on performance could also enhance the practical applicability of their framework.