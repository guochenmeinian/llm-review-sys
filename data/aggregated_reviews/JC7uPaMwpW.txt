ID: JC7uPaMwpW
Title: KBioXLM: A Knowledge-anchored Biomedical Multilingual Pretrained Language Model
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents KBioXLM, a bilingual model for the biomedical domain, leveraging the multilingual XLM-R model and integrating three levels of knowledge: entity, fact, and passage. The authors propose a knowledge-centric pretraining strategy aimed at enhancing data efficiency in multilingual settings. The model demonstrates superior performance on various biomedical benchmarks compared to existing multilingual models, particularly in cross-lingual zero-shot and few-shot scenarios.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated, highlighting the importance of biomedical applications for minor languages.
- KBioXLM shows significant performance improvements, exceeding previous models by over 10 points in specific tasks.
- The study provides valuable insights into the role of knowledge in bridging multilingual capabilities.

Weaknesses:
- The model is described as bilingual, yet it primarily functions as a multilingual model, lacking clarity on the motivation for this distinction.
- The introduction is confusing, focusing more on methods and results rather than the rationale for a bilingual model versus a cross-lingual one.
- Limited evaluation scope, as the model is only tested on Chinese, with no exploration of additional languages or the impact of fewer tokens at each granularity level.

### Suggestions for Improvement
We recommend that the authors improve the introduction by clearly articulating the motivation for developing a bilingual model instead of relying on a cross-lingual approach, including the advantages and disadvantages of each. Additionally, we suggest including a schema or figure to illustrate the methodology and providing more details about the manual verification process for divergences. It would be beneficial to discuss the performance of the model with fewer tokens at each granularity level and to include English-only and Chinese-only biomedical PLMs in the results for comparative insights. Lastly, please ensure consistent terminology for Chinese throughout the paper and clarify whether "expertly translated" refers to manual translation.