ID: D19UyP4HYk
Title: Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the presence of metacognitive knowledge in large language models (LLMs), focusing on their reasoning and skill application in mathematical contexts. The authors propose a prompt-guided interaction procedure that allows an LLM to assign skill labels to math questions and perform semantic clustering, creating interpretable families of skill labels. Validation through experiments on the GSM8K and MATH datasets shows that providing exemplar solved questions associated with skill labels improves the accuracy of various LLMs, including code-assisted models. The authors introduce a Skill Exemplar Repository to enhance in-context learning capabilities, although they acknowledge the need for a more advanced system to account for multiple skills per question. The methodology is intended to be domain-agnostic, but further evidence is required to substantiate this claim.

### Strengths and Weaknesses
Strengths:
- The structured approach of using skill exemplars provides explicit skill labeling, enhancing targeted learning and mimicking human problem-solving techniques.
- The originality of the framework for extracting metacognitive knowledge from LLMs is notable, leveraging existing capabilities to create a Skill Exemplar Repository that improves in-context learning.
- The thorough experimental validation using well-known datasets demonstrates the methodology's effectiveness, with significant performance improvements across various LLMs.

Weaknesses:
- The methodology's limitation of assigning only one skill per math question oversimplifies the problem-solving process, potentially reducing generalizability and accuracy.
- The approach may not scale well for complex tasks requiring multiple overlapping skills and demands a highly curated dataset, which can be resource-intensive.
- The related work section is underdeveloped, and some references are incomplete, which detracts from the paper's clarity.

### Suggestions for Improvement
We recommend that the authors improve the methodology by implementing a more advanced system that allows for the assignment of multiple skills to each math question. Providing preliminary analysis or examples of this approach could enhance the model's robustness and applicability. Additionally, we suggest expanding the related work section to include discussions on other methods, such as TORA, and addressing how the proposed framework handles tasks requiring complex skill definitions. Clarifying the granularity of skill clusters and their tunability for different datasets would also strengthen the paper. Finally, ensuring all references are complete and consistently formatted will enhance overall clarity.