ID: NLFqlDeuzt
Title: Understanding the Limitations of Deep Models for Molecular property prediction: Insights and Solutions
Conference: NeurIPS
Year: 2023
Number of Reviews: 35
Original Ratings: 5, 4, 3, 7, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a systematic evaluation of deep versus non-deep machine learning models for molecular property prediction, utilizing 15 datasets from MoleculeNet. The authors investigate the reasons behind the superior performance of non-deep models, attributing it to the non-smooth nature of molecular data and the mixing of input features. They propose a novel feature mapping method called Independent Feature Mapping (IFM) to enhance deep model performance. Additionally, the paper analyzes the performance of tree-based models, specifically Random Forests (RFs) and XGBoost (XGB), arguing for their superiority over traditional non-deep models. The authors conduct experiments examining the effects of smoothing and activity cliffs on model performance, asserting that their findings validate their conclusions.

### Strengths and Weaknesses
Strengths:
- The exploration of why deep learning models underperform in molecular property prediction is a valuable contribution, addressing a gap in existing literature.
- The analysis of data smoothness and its implications for model performance is insightful.
- The use of the MoleculeNet benchmark is appropriate and relevant for the community.
- The authors provide a clear distinction between IFM and existing methodologies, supported by theoretical justifications.
- The feature selection process is well-defined, addressing common issues such as missing values and high correlation among features.
- The empirical results demonstrate that non-deep models can outperform deep models on larger datasets, contributing valuable insights to the field.
- The authors provide a clear argument for the superiority of tree models in molecular property prediction, supported by experimental evidence.

Weaknesses:
- The related work section lacks discussion of significant prior studies, including those that benchmark various methods on similar datasets.
- The assertion regarding the non-smooth nature of molecular data requires clarification and evidence.
- The proposed IFM method lacks sufficient novelty and clarity, and its effectiveness is not thoroughly validated.
- The experiments do not convincingly support the authors' conclusions, particularly regarding the definitions of "irregular data patterns" and "smoothness."
- The novelty of the conclusions regarding the superiority of RFs is questioned, as it is considered generally known in the field.
- Conclusions drawn regarding the sensitivity of deep models to structural changes lack sufficient empirical support, as they rely heavily on error rates without examining prediction changes.
- The absence of error bars in tables obscures the statistical significance of performance differences.

### Suggestions for Improvement
We recommend that the authors improve the related work section by including discussions of significant prior studies, such as Wu et al. (2018) and others that benchmark non-deep methods. Additionally, we suggest that the authors clarify their definitions of "smooth" and "irregular" data patterns, providing evidence for their claims. The explanation of the IFM method should be enhanced, and its experimental validation should be more comprehensive. We also recommend that the authors improve the novelty aspect of their methodology by further clarifying its unique contributions compared to existing techniques. Furthermore, we suggest conducting more comprehensive experiments on sinusoidal featurization, ensuring that all relevant factors are controlled. To enhance clarity, please include error bars in the tables to indicate statistical significance. Lastly, we encourage the authors to clarify their experimental methodology, particularly in relation to hyperparameter tuning and the handling of outliers, to strengthen the validity of their conclusions.