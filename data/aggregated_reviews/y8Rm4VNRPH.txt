ID: y8Rm4VNRPH
Title: Parallelizing Linear Transformers with the Delta Rule over Sequence Length
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 8, 7, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Delta Rule method for constructing state updates in Linear Attention and introduces a chunk-wise training approach that allows the computational cost to grow subquadratically with text length. The authors validate their model architecture through experiments on three synthetic benchmarks (MQAR, MAD, RegBench) and real-world tasks involving Common Sense Reasoning and Retrieval, demonstrating effectiveness across scales from 340M to 1.3B parameters. Additionally, the paper explores combining the Delta Rule with Sliding Window Attention and Global Attention, showing positive impacts on performance.

### Strengths and Weaknesses
Strengths:
- The paper provides a solid derivation and a general method for state updates in Linear Models.
- Comprehensive experiments effectively validate the model architecture and demonstrate significant performance improvements over strong baselines like Mamba and GLA.

Weaknesses:
- The authors have not conducted experiments on long context scenarios, such as extrapolation or tasks requiring extensive search capabilities.
- While the Delta Net's algorithmic speed increases linearly, it appears slower than GLA; an analysis of contributing factors is needed.
- Further explanation of the unique benefits of Delta Net updates compared to GLA operators, including theoretical analysis, is required.

### Suggestions for Improvement
We recommend that the authors improve their exploration of long context capabilities, particularly regarding generalization in such scenarios. Additionally, we suggest analyzing the factors that contribute to the slower speed of Delta Net compared to GLA. It would also be beneficial to provide a more detailed discussion of the insights from Delta Net updates, including any theoretical analysis. Finally, we encourage the authors to include results for DeltaNet without convolution for the 1.3B model to clarify its performance relative to GLA.