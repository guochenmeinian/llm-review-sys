ID: 8aunGrXdkl
Title: Convex and Non-convex Optimization Under Generalized Smoothness
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 3, 7, 8, 6, 8, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents a generalization of classical Lipschitz smoothness through the introduction of $\ell$-smoothness, which bounds the Hessian norm by a non-decreasing function of the gradient norm. The authors demonstrate convergence rates for gradient descent under various settings, including convex, strongly convex, non-convex, and stochastic scenarios, effectively recovering classical optimal rates.

### Strengths and Weaknesses
Strengths:
- The paper is well presented, with clear explanations and rigorous mathematical statements.
- The technical contribution is significant, extending existing smoothness concepts and providing insights into convergence behavior under relaxed conditions.

Weaknesses:
- The theoretical results, while extensive, may not be novel, as they often recover classical analyses.
- The paper lacks numerical experiments, which are crucial for validating theoretical findings in machine learning and optimization.
- Some minor theoretical inconsistencies and ambiguities exist, particularly regarding assumptions and definitions.

### Suggestions for Improvement
We recommend that the authors improve the related work section to provide a more comprehensive overview of existing literature, as gradient-based optimization is an active research area. Additionally, including numerical experiments would enhance the practical relevance of the theoretical contributions. We also suggest clarifying the assumptions regarding the function's differentiability and addressing the potential limitations of the proposed $\ell$-smoothness condition, particularly its implications for various classes of functions. Lastly, we encourage the authors to explore expected smoothness assumptions, which could broaden the applicability of their results.