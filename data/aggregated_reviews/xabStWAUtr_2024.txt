ID: xabStWAUtr
Title: Co-occurrence is not Factual Association in Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 7, 5, 7, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into how language models acquire factual knowledge during fine-tuning, distinguishing between co-occurrence statistics and factual associations. The authors demonstrate that narrative input tends to teach co-occurrence, while referencing input enhances factual association learning. They find that models learning factual associations generalize better, particularly in multi-hop reasoning tasks. The authors propose resetting the upper layers of the model to improve factual association learning when fine-tuning on narrative input. Their experiments validate these findings across synthetic and real-world datasets, revealing distinct storage of knowledge in different model layers.

### Strengths and Weaknesses
Strengths:
1. The identification of two forms of knowledge representation provides valuable insights into model generalization.
2. Comprehensive experiments using synthetic and real-world datasets validate the authors' claims.
3. Innovative training strategies, including training on implicit associations and active forgetting of co-occurrence statistics, enhance factual learning.
4. The paper is well-structured and free of grammatical errors, facilitating readability.

Weaknesses:
1. The analysis is limited to triplet representations, which may not encompass all knowledge forms in reasoning tasks.
2. Insufficient insights are provided regarding why narrative input favors co-occurrence learning over factual associations.
3. The scope of text variations is narrow, focusing only on narrative and implicit associations.
4. The paper does not explore the impact of model scaling or different reasoning types beyond the specific focus on multi-hop reasoning.

### Suggestions for Improvement
We recommend that the authors improve the analysis by extending their experiments to include more realistic settings where sentences contain multiple pieces of knowledge. Additionally, exploring the effects of model scaling on learning behavior would provide further insights. We suggest discussing the reasons behind narrative input's tendency to teach co-occurrence statistics in more depth in Section 3. To enhance the robustness of their findings, the authors should consider evaluating their methods across a broader range of reasoning tasks and textual representations. Finally, clarifying the evaluation metrics used for question answering tasks in the main body of the paper would strengthen the presentation.