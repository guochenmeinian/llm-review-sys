ID: qs4swxtIAQ
Title: TabMT: Generating tabular data with masked transformers
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 4, 7, 4, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TabMT, a novel Masked Transformer architecture aimed at generating synthetic tabular data. The authors demonstrate that TabMT effectively handles heterogeneous data fields and missing data, showcasing its applicability in privacy-sensitive contexts. The model employs a unique masking strategy, sampling the masking probability from a uniform distribution and predicting masked values in random order, which contributes to its high performance across various dataset sizes.

### Strengths and Weaknesses
Strengths:
- The motivation for the study is clear, addressing an important yet less explored area of tabular data generation.
- The experimental design is comprehensive, covering data quality, privacy, sample novelty, missing data, and scaling.
- The paper is well-written, with clear definitions of the problem and effective methods presented.

Weaknesses:
- The motivations behind certain design choices, such as temperature scaling and privacy considerations, are inadequately explained.
- The introduction lacks detail regarding privacy budgets and the definition of data "novelty."
- The model's structure modifications are limited to the input layer, lacking improvements in the multi-layer transformer structure, which may restrict its applicability.
- The resource requirements for training the model may limit accessibility for general users.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivations for design choices, particularly regarding temperature scaling and privacy aspects. It would be beneficial to provide a detailed explanation of privacy budgets and the definition of data "novelty." Additionally, consider reorganizing the structure to enhance readability, possibly by moving key paragraphs to the beginning of the methodology section. We also suggest including comparisons with recent state-of-the-art generative models, such as STaSy, to strengthen the evaluation. Finally, addressing the resource intensity of training the model by providing a pre-trained model could enhance accessibility for users.