ID: EavQwYUuJp
Title: Improving Model Merging with Natural Niches
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 4, 7, 9, 7
Original Confidences: 3, 4, 4, 3

Aggregated Review:
### Key Points
This paper presents a method for model merging using an evolutionary algorithm called Natural Niches (NN). The authors claim three key features: dynamic adjustment of merging boundaries, a diversity preservation mechanism, and a heuristic-based mate selection strategy. The algorithm iteratively selects parent models based on complementarity, performs parameter-space crossover using spherical linear interpolation (SLERP), and maintains diversity through competitive resource allocation. The effectiveness of the method is validated through experiments on Evolving MNIST classifiers and large language models (LLMs).

### Strengths and Weaknesses
Strengths:
- The writing is clear and easy to follow.
- The use of an evolutionary algorithm for model merging is both interesting and novel.
- The proposed method shows strong results on various language tasks and maintains a diverse model archive.

Weaknesses:
- The mutation operator is ineffective in LLM merging.
- Limited comparisons with other methods, only contrasting with CMA-ES.
- Computational costs appear high compared to other merging methods.
- Sections 3.1 and 3.2 are difficult to understand, and notations could be improved.
- There is a minor citation error regarding "Ties-merging."

### Suggestions for Improvement
We recommend that the authors improve the clarity of Sections 3.1 and 3.2 by enhancing notations and providing a more detailed background on genetic algorithms to aid reader comprehension. Additionally, clarifying Eqn 3 regarding the maximization variable would be beneficial. We suggest adding a descriptive figure or pseudo-code to illustrate the NN algorithm's pipeline, as the current presentation may leave readers confused about the transition from one split-point to multiple boundaries. An ablation study comparing SLERP with Euclidean linear interpolations could address concerns about interpolation methods. Furthermore, a more detailed discussion on why pre-trained models perform worse in Fig 2 is necessary, along with an explanation of the statement regarding mutations on pre-trained models not scaling well.