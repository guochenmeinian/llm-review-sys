ID: yPkbdJxQ0o
Title: Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 5, 6, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the MoDo algorithm, a variant of MGDA that employs double sampling to achieve an unbiased stochastic estimate of the gradient in multi-objective optimization. The authors provide a thorough theoretical analysis of the errors associated with multi-objective optimization, specifically focusing on optimization error, generalization error, and conflict-avoidance direction error. The work highlights a three-way tradeoff among these errors and offers insights into dynamic weighting in multi-objective learning. Additionally, the authors discuss the motivation behind stochastic multi-objective optimization (MOO) algorithms, emphasizing that full-batch gradients are impractical for large-scale problems due to high memory and computational demands. They note that most practical multi-task learning algorithms utilize stochastic (mini-batch) gradients, revealing a significant gap in algorithm design and analysis that traditionally focuses on deterministic settings. The paper references recent works demonstrating the non-convergence of vanilla mini-batch MGDA and similar methods to Pareto optimal solutions, supported by both theoretical proofs and numerical examples.

### Strengths and Weaknesses
Strengths:
- The paper delivers a comprehensive analysis of optimization and generalization errors in multi-objective optimization, contributing valuable theoretical insights.
- The exposition is clear, making complex concepts accessible, particularly through well-illustrated figures and tables.
- The MoDo algorithm is intuitively designed, effectively illustrating the tradeoffs involved in dynamic weighting.
- The authors provide a clear rationale for the use of stochastic MOO algorithms, addressing practical limitations of full-batch gradients.
- The discussion is well-supported by references to recent literature that validates their claims regarding convergence issues in existing methods.

Weaknesses:
- Empirical comparisons are limited, as MoDo is only evaluated against MGDA, neglecting other algorithms like CAGrad and PCGrad, which may demonstrate superior performance.
- In two out of three objectives, MGDA outperforms MoDo, raising questions about the proposed method's effectiveness.
- The algorithm's computational efficiency is compromised due to the need for three batches of gradients per iteration.
- The theoretical analysis relies on specific assumptions that may not hold in practical scenarios, limiting the generalizability of the findings.
- The response lacks specific empirical results or detailed proofs to substantiate claims about non-convergence, which could strengthen the argument.

### Suggestions for Improvement
We recommend that the authors improve the empirical evaluation by including comparisons with a broader range of algorithms, such as CAGrad and PCGrad, to better demonstrate the effectiveness of MoDo. Additionally, addressing the computational efficiency of the algorithm would enhance its practical applicability. We suggest providing more intuitive explanations of the proofs within the main text to aid reader comprehension. Furthermore, we recommend including specific empirical results or detailed proofs that illustrate the non-convergence of vanilla mini-batch MGDA and similar methods, which would enhance the robustness of their claims and provide clearer insights into the practical implications of their findings. Lastly, clarifying how the assumptions made in the analysis can be relaxed would strengthen the paper's relevance to real-world applications.