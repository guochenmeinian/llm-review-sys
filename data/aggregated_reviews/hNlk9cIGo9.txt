ID: hNlk9cIGo9
Title: Faster Algorithms for User-Level Private Stochastic Convex Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 22
Original Ratings: 7, 4, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper revisits the user-level private stochastic convex optimization (SCO) problem and presents a novel approach to mean estimation under user-level differential privacy (DP). The authors propose new algorithms that enhance the state-of-the-art in this area, including a linear-time algorithm that achieves the same excess risk as prior algorithms but with milder assumptions on the smooth parameter $\beta$ and without requiring the number of users $n$ to depend on the dimension $d$. Additionally, they introduce an optimal algorithm that improves gradient complexity in both smooth and non-smooth settings. The authors argue that existing solutions require strict user size conditions and do not effectively handle non-smooth functions. They incorporate outlier removal techniques to enhance their algorithm's performance and mitigate sensitivity issues, clarifying that the i.i.d. assumption cannot be used in privacy analysis, as user-level DP must hold for all adjacent databases. The paper also discusses the implications of these algorithms in practical applications.

### Strengths and Weaknesses
Strengths:  
1. The study of user-level DP SCO is crucial, especially as users often hold multiple data points.  
2. The proposed algorithms relax previous assumptions and improve runtime, contributing significantly to the field.  
3. The authors provide a significant improvement in runtime and user requirements, with logarithmic dependence on dimensionality compared to polynomial dependence in prior works.  
4. The writing is clear, and the theoretical results are well-presented.  
5. The proposed method is novel and addresses key limitations in existing literature, particularly regarding non-smooth functions.  

Weaknesses:  
1. The linear-time algorithm has a suboptimal excess risk, while the optimal algorithm has a higher gradient complexity than linear.  
2. The complexity of the proposed algorithms may be excessive, lacking a comparison with simpler approaches like DP-SGD.  
3. Some claims regarding practical applicability, particularly in modern ML contexts, are not sufficiently supported by evidence.  
4. The discussion surrounding outlier removal and its necessity is not fully convincing to some reviewers.  
5. There are inaccuracies in the stated user size requirements, leading to confusion about the conditions necessary for optimal performance.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the second algorithm's design and provide more intuitive explanations, particularly regarding the advantages of outlier-iterate removal over outlier-gradient removal. Additionally, including a comparison table with previous results would enhance the contribution's visibility. The authors should also address the feasibility of their algorithms in practical scenarios, especially concerning the memory requirements for large models. Furthermore, we suggest improving the clarity and persuasiveness of the discussion on outlier removal, addressing specific concerns raised by reviewers. Please ensure that the user size requirements are accurately stated and contextualized to avoid confusion. Lastly, further elaboration on the implications of using user-level mean estimation algorithms for gradient estimation would strengthen the paper.