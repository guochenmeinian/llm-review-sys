ID: Lr2swAfwff
Title: Bridging RL Theory and Practice with the Effective Horizon
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 8, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical advancement in reinforcement learning (RL) by introducing a new complexity metric called **effective horizon**, which quantifies the number of look-ahead search steps required to identify an optimal action in deterministic Markov Decision Processes (MDPs). The authors curated a dataset named BRIDGE, consisting of 155 deterministic MDPs, and demonstrated that deep RL algorithms like PPO and DQN perform better when the policy acting greedily with respect to the random policy's Q function is optimal. The effective horizon is shown to correlate well with empirical performance, particularly in the context of reward shaping and policy initialization. Additionally, the authors analyze the robustness of deep RL results by re-running experiments with the Stable Baselines 3 (SB3) implementations of PPO and DQN, confirming that the effective horizon-based bounds correlate best with empirical performance. The results indicate that the main conclusions remain unchanged when using SB3 instead of RLlib.

### Strengths and Weaknesses
Strengths:
- The paper provides a valuable benchmark for deterministic MDPs, facilitating theoretical analysis beyond black-box algorithm results.
- The finding regarding the optimality of policies acting greedily with respect to random policies is both surprising and quantitatively validated.
- The effective horizon metric is efficient and predictive of agent performance, enhancing future theoretical studies.
- The experiments demonstrate the robustness of the main results across different implementations, confirming that the effective horizon is a strong predictor of performance.
- The correlation between empirical sample complexities of the two implementations (0.94) suggests a high degree of similarity.
- The writing is clear and well-structured, making the paper accessible.

Weaknesses:
- Concerns arise regarding the PPO training results on Atari, where the performance is significantly lower than that reported by popular RL libraries, despite using more samples.
- The paper lacks clarity in referencing figures and tables from the appendix, which may confuse readers.
- The effective horizon's applicability to algorithms with intrinsic rewards is limited due to non-stationarity issues.
- The SB3 implementation performed worse than RLlib in 40% of environments, indicating variability in performance across implementations.
- Long-horizon Atari experiments have not yet been re-run with SB3, leaving a gap in the analysis.

### Suggestions for Improvement
We recommend that the authors improve the clarity of references to appendix figures and tables to avoid confusion. Additionally, we suggest conducting further experiments with SB3's Atari wrappers to validate the PPO results and address the performance discrepancies observed. It would also be beneficial to provide more detailed explanations of the effective horizon's implications, particularly in relation to its dependence on policy and reward shaping. Furthermore, we recommend re-running the long-horizon Atari experiments using SB3 to further validate the robustness of the findings and providing a more detailed discussion on the performance discrepancies observed in the environments where SB3 underperformed. Lastly, a more thorough literature review on GORP's novelty compared to similar algorithms would strengthen the paper's contribution.