ID: m9uHv1Pxq7
Title: Learning Motion Refinement for Unsupervised Face Animation
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 5, 6, 4, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 3, 2, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel non-prior-based motion refinement approach for unsupervised face animation, aimed at enhancing motion representation and capturing finer facial motions. The authors introduce a structure correlation volume built from keypoint features, which iteratively refines coarse motion fields estimated by prior models. Extensive experiments demonstrate the method's effectiveness, showing improvements in both qualitative and quantitative results over existing techniques. The authors argue that their contribution is not merely incremental and can generalize to enhance various prior-based animation techniques, although they acknowledge limitations in identity preservation compared to methods like Face Vid2Vid, which utilizes 3D head pose supervision.

### Strengths and Weaknesses
Strengths:
1. The proposed non-prior-based motion refinement module effectively captures fine facial motions, particularly around the eyes, with fewer uncanny artifacts compared to prior art.
2. The manuscript is well-written and clearly articulates its main ideas and contributions.
3. Extensive experiments validate the method's performance, with results surpassing many state-of-the-art approaches.
4. The approach demonstrates generalizability to improve existing prior-based methods while maintaining a reasonable tradeoff between inference speed and performance.

Weaknesses:
1. The video results lack impressive visual quality, with some reviewers noting that the generated video quality remains a concern.
2. The paper does not provide quantitative evaluations using standard metrics like CSIM and FID for cross-identity reenactment, raising concerns about identity preservation.
3. The introduction is limited in scope and should better outline the contributions of the work.
4. The method exhibits limitations in identity preservation, which is a common issue in unsupervised animation techniques.

### Suggestions for Improvement
We recommend that the authors improve the introduction to clearly articulate the contributions of their work. Additionally, providing quantitative results for cross-identity reenactment tasks using metrics such as CSIM and FID would enhance the evaluation of identity preservation. We suggest that the authors explore the incorporation of 3D supervision in future work to address identity preservation limitations. Clarifying the significance of flow in the figures and addressing the identity shift during animation with metrics like FaceIDLoss or L2-loss is crucial. We also recommend including a comparison with Face Vid2Vid to highlight differences in performance, particularly regarding identity preservation and motion modeling. Lastly, a computation cost evaluation at inference, including Flops, memory, and FPS, compared to other state-of-the-art methods, would be beneficial. Enhancing the quality of the generated videos is also essential, as this remains a critical point of feedback from reviewers. Including the table from Q3 of the general response in the main paper would strengthen the presentation of their results.