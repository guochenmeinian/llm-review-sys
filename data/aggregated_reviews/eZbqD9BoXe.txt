ID: eZbqD9BoXe
Title: Graph-Structured Gaussian Processes for Transferable Graph Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 4, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a graph-structured Gaussian process framework (GraphGP) aimed at addressing the transferable graph learning problem, particularly between homophilic and heterophilic graphs. The authors propose a structure-aware neural network to encode both local node representation and global graph representation. The effectiveness of GraphGP is validated through theoretical analysis and experimental results, demonstrating superior performance in various transferable node regression tasks.

### Strengths and Weaknesses
Strengths:
1. The motivation for using Gaussian processes in graph transfer learning is novel and interesting.
2. The paper provides a comprehensive theoretical analysis and clear problem definition.
3. The authors effectively tackle knowledge transferability using a simple neighborhood selection strategy.

Weaknesses:
1. The organization and presentation of the paper are poor, with unclear relationships between problem formulations and proposed methods.
2. The methodology section lacks clarity regarding the algorithm flow and requires a more detailed algorithm complexity and convergence analysis.
3. The experimental section is weak, missing comparisons with existing graph neural network models and lacking real-world datasets to validate the algorithm's effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the algorithm flow description in the methodology section by adding brief descriptions, an algorithm table, and an overall framework figure. Additionally, the authors should conduct more transfer learning tasks in the experiments, such as classification tasks, and include various real-world datasets to enhance the validation of GraphGP. Furthermore, we suggest providing a more detailed explanation of the kernel function used in the experiments and addressing the limitations of the proposed methods more explicitly.