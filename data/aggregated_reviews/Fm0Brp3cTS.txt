ID: Fm0Brp3cTS
Title: UPTON: Preventing Authorship Leakage from Public Text Release via Data Poisoning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents UPTON, a framework designed to protect authors' identities from authorship attribution tools, particularly benefiting activists and whistle-blowers. UPTON operates without requiring access to the attacker model and utilizes synonym substitution to modify text, thereby assigning different labels to thwart attribution attempts. The authors validate UPTON across three scenarios: cold start, warm start, and incremental-release, demonstrating a reduction in attribution model accuracy to about 35%.

### Strengths and Weaknesses
Strengths:
- The authors conducted comprehensive experiments across three different scenarios, showcasing superior results compared to alternative methods like sample-wide and non-target label assigning.
- The paper is well-written and addresses an important and contemporary problem, generating significant interest.
- The discussion section is robust, preemptively addressing potential reader questions.

Weaknesses:
- There is uncertainty regarding how word-level synonym replacement can influence a distinct model (F_A) when the attack model is independent and inaccessible, suggesting that F_A should be static without parameter tuning.
- The main contribution appears to be the label-assigning process for poisoned data, which may lack sufficient novelty for an EMNLP paper.
- The paper only measures bertscore for text similarity, neglecting other metrics that could assess the stealthiness of the generated content.
- The motivation for replacing original content with machine-generated text raises questions about user desires for anonymity versus recognition.
- Assumptions about the attacker model, particularly its nature as a transformers-based model, require further clarification.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how word-level synonym replacement affects the model (F_A) and address the implications of using a static model. Additionally, we suggest enhancing the novelty of the contribution by elaborating on the label-assigning process. The authors should consider incorporating additional metrics to evaluate the stealthiness of the generated content beyond bertscore. Furthermore, we encourage the authors to clarify the motivation behind replacing original content with machine-generated text and explicitly state assumptions regarding the attacker model.