ID: 7jYZd05yjJ
Title: ClusterLLM: Large Language Models as a Guide for Text Clustering
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CLUSTERLLM, a text clustering framework that utilizes feedback from an instruction-tuned LLM. The authors propose a two-step fine-tuning process that enhances clustering performance by refining clusters through triplet queries and establishing a cluster hierarchy via pairwise queries. The framework demonstrates gains over state-of-the-art methods across multiple datasets, employing a novel entropy-based sampling technique to optimize LLM queries.

### Strengths and Weaknesses
Strengths:
- The paper advances clustering literature by leveraging instruction-tuned LLMs.
- It introduces an effective entropy-based triplet selection method that improves clustering performance.
- Comprehensive experiments validate the framework's superiority over existing methods, with meaningful figures and a well-structured presentation.

Weaknesses:
- The paper suffers from significant grammatical errors and unclear sentences, impacting readability.
- The exploration of cluster granularity is underdeveloped and could benefit from further elaboration.
- Some results discussed in the main text reference the appendix, which may confuse readers.

### Suggestions for Improvement
We recommend that the authors improve the clarity and grammatical quality of the paper by thoroughly proofreading and correcting errors. Additionally, we suggest that the authors provide a more detailed exploration of the cluster granularity aspect, potentially relocating this discussion to an appendix or future work section. It would also be beneficial to evaluate more than one clustering metric and ensure that all crucial information is contained within the main body of the paper to enhance accessibility. Lastly, we encourage the authors to clarify the experimental comparisons and ensure that results presented in the appendix are not discussed in the main text.