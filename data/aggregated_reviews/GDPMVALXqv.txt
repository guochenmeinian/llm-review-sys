ID: GDPMVALXqv
Title: Using In-Context Learning to Improve Dialogue Safety
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a retrieval-based framework aimed at reducing toxicity in language model (LM) generated responses through prompting and in-context learning. The authors propose a method that retrieves safe demonstrations similar to the current dialogue context and uses these to prompt the LM for response generation. Evaluations on three safety-related dialogue datasets demonstrate that this approach can effectively reduce toxicity while maintaining engagement and coherence, performing competitively with established training methods like DIRECTOR.

### Strengths and Weaknesses
Strengths:
- The authors introduce a retrieval+prompting method that effectively reduces toxicity without significant loss of dialogue quality.
- A comprehensive set of experiments supports their claims, considering factors such as model size and the number of safe demonstrations.
- This study represents the first large-scale evaluation of in-context learning for dialogue safety, contributing valuable insights to the field.

Weaknesses:
- Human evaluations are limited to comparisons against baselines lacking safety mechanisms.
- The method's reliance on prompts designed by the authors necessitates comparisons with other prompting-based methods, which are not adequately addressed.
- Experiments are confined to short dialogue contexts (up to 2 turns), a limitation acknowledged by the authors.
- The methodology lacks novelty, primarily focusing on verifying existing approaches rather than introducing innovative techniques.
- The evaluation tools used, such as the safety classifier, may not adequately capture context-sensitive safety issues.

### Suggestions for Improvement
We recommend that the authors improve the robustness of their methodology by addressing potential risks associated with retrieving unsafe demonstrations, including how their approach would handle a compromised retrieval database. Additionally, we suggest incorporating comparisons with other prompting methods and utilizing more advanced safety evaluation tools, such as LLM-eval, to enhance the accuracy of their results. Finally, expanding the scope of experiments to include longer dialogue contexts and a more diverse set of toxicity classifiers would provide a more comprehensive analysis of their approach's effectiveness.