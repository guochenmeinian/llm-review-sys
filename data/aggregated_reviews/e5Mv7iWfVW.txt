ID: e5Mv7iWfVW
Title: What Rotary Position Embedding Can Tell Us: Identifying Query and Key Weights Corresponding to Basic Syntactic or High-level Semantic Information
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 2, 2, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates how Transformer models with RoPE embeddings adjust their weights during pre-training and fine-tuning. The authors theoretically demonstrate that RoPE separates key and query pairs into near-orthogonal and non-orthogonal weight vector pairs, which exhibit different sensitivities to input embeddings. Empirical studies reveal that non-orthogonal pairs focus on basic syntactic information, while nearly orthogonal pairs emphasize high-level semantic information. The authors propose a fine-tuning method that updates only the orthogonal pairs of weights, significantly reducing the number of trainable parameters while maintaining or enhancing performance and mitigating overfitting.

### Strengths and Weaknesses
Strengths:  
- The paper uncovers a significant property of attention weight vector pairs, enhancing understanding of language abstraction learning in model layers.  
- It is well-written, with substantial support for its claims, including detailed appendices.  
- The proposed approach for efficient fine-tuning advances parameter-efficient techniques and is applicable across various contexts.  
- The authors provide solid theoretical insights and empirical evidence, particularly on models like Alpaca fine-tuned Llama-2, demonstrating practical benefits.

Weaknesses:  
- The experimental results lack diversity and depth, particularly in Chapters 3 and 4, with limited tasks and models tested.  
- The analysis in Section 3.2 is the only part linking abstraction levels to weight pair angles, relying heavily on this hypothesis without extensive supporting examples.  
- The introduction does not clearly define "Basic Syntactic Information" and "High-level Semantic Information," which are crucial for understanding the paper's claims.  
- Some figures have small font sizes, making them difficult to read, and there are grammatical errors and typos throughout the paper.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the introduction by providing clear definitions and examples of "Basic Syntactic Information" and "High-level Semantic Information." Additionally, the authors should expand the experimental section to include a broader range of models and tasks to strengthen the generalizability of their findings. It would also be beneficial to clarify the motivation behind how the angle between weight vector pairs affects RoPE. Lastly, addressing the grammatical errors and ensuring figures are legible will enhance the overall presentation of the paper.