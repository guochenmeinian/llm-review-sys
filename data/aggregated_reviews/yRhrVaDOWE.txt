ID: yRhrVaDOWE
Title: Diffusion-based Curriculum Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 26
Original Ratings: 6, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DiCuRL (Diffusion Curriculum Reinforcement Learning), a novel approach that utilizes diffusion models for curriculum learning in multi-goal reinforcement learning (RL). The authors propose a framework that generates goals conditioned on the current state, aiming to enhance exploration efficiency and facilitate online learning without requiring domain-specific knowledge. The evaluation includes comparisons with nine state-of-the-art curriculum RL baselines across three maze environments and two robotic manipulation tasks, demonstrating that DiCuRL performs comparably to existing methods in maze navigation tasks and effectively generates reachable yet challenging goals. The authors also highlight that their method does not rely on prior expert data or pre-collected datasets, addressing a limitation found in existing works.

### Strengths and Weaknesses
Strengths:
- The motivation for applying curriculum learning to multi-goal RL is compelling, and the implementation of diffusion models for goal generation is a notable innovation.
- The writing is generally clear, with effective use of figures to illustrate concepts, and the revised introduction and related work sections enhance coherence.
- The empirical evaluation shows competitive performance, and the additional results and ablation studies provided in the rebuttal enhance the validity of the proposed approach.
- The related work section is comprehensive, covering significant advancements in curriculum RL and including relevant references.

Weaknesses:
- The introduction is overly dense, making it difficult to follow; it should be restructured for clarity.
- The method is limited to multi-goal RL, which may restrict its applicability in real-world scenarios where specifying goals is challenging.
- Evaluation is confined to goal-conditioned scenarios, lacking diversity in task domains; additional evaluations in varied environments would strengthen the findings.
- The significance of results is questionable, as overlapping curves in figures complicate interpretation.
- The discussion of related work is seen as inadequate without clear differentiation from existing methods, particularly regarding diffusion models in RL.

### Suggestions for Improvement
We recommend that the authors improve the introduction by breaking it into clearer, more focused paragraphs to enhance readability. Additionally, consider extending the applicability of DiCuRL beyond multi-goal RL to enhance its relevance in real-world applications. Expanding the evaluation to include navigation tasks with discrete state and action spaces, locomotion tasks, and image-based games would provide a more robust validation of the method's effectiveness. We suggest clarifying the significance of results by refining the plotting scheme to avoid overlapping curves and improve interpretability. Furthermore, please ensure that the related work section includes relevant literature on diffusion models in RL and provides a detailed discussion of how their work differentiates from the references provided by reviewers. Lastly, we encourage the authors to conduct an ablation study to elucidate the contributions of the Q-function and AIM reward components in the proposed framework.