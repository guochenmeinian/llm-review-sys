ID: Sf17j2pkCU
Title: Optimistic Exploration in Reinforcement Learning Using Symbolic Model Estimates
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 4, 7, 3, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for optimistic exploration in sparse reward settings within reinforcement learning (RL) by learning optimistic symbolic approximations of the underlying world model. The authors propose using these symbolic models with a diverse planner to generate exploration plans, facilitating learning. The method is evaluated across four benchmark domains, demonstrating superior performance compared to three baseline methods in terms of problems solved. Additionally, the method effectively incorporates human input through lifted representations in the symbolic model.

### Strengths and Weaknesses
Strengths:
- The approach of learning optimistic symbolic models for exploration is technically interesting and novel.
- The paper is well-written and easy to follow, making complex ideas accessible.
- The exploration problem in RL is significant, and leveraging symbolic methods represents a promising direction.
- The experimental design is sound, with evaluations against established methods in traditional benchmark domains.

Weaknesses:
- The lack of reported results for R-max and SMDP baselines due to timeouts limits the comparison of the proposed method with these approaches.
- The integration of the symbolic model with Q-learning is unclear, necessitating a more detailed explanation or pseudo-code.
- The deterministic assumption of the environment limits the method's applicability, and the paper does not adequately address potential challenges in stochastic settings.
- The presentation suffers from clarity issues, with some technical terms and concepts inadequately explained.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the integration between the proposed method and Q-learning by providing pseudo-code for a complete Q-learning algorithm that incorporates their approach. Additionally, it would be beneficial to report results for R-max and SMDP under a higher time limit to facilitate a more comprehensive comparison. We suggest that the authors acknowledge the limitations of assuming a deterministic environment more explicitly and discuss potential strategies for adapting their method to stochastic settings. Finally, enhancing the presentation by breaking down complex ideas into clearer explanations and providing visual examples would significantly improve reader comprehension.