ID: iEeiZlTbts
Title: No Regrets: Investigating and Improving Regret Approximations for Curriculum Discovery
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new Unsupervised Environment Design (UED) method called Sampling For Learnability (SFL), which prioritizes environment configurations with high learnability scores defined as \( p \cdot (1-p) \), where \( p \) is the success rate. The authors investigate the limitations of existing UED approaches, such as Domain Randomisation, Prioritised Level Replay (PLR), and ACCEL, demonstrating that their scoring mechanisms do not effectively correlate with learnability, leading to sub-optimal performance. The authors conduct experiments in environments like MiniGrid and JaxNav, showing that SFL outperforms these existing methods, particularly in terms of robustness as measured by the conditional value-at-risk (CVaR). Additionally, the authors emphasize that SFL is robust to hyperparameter choices, with key parameters being the number of levels sampled ($N$), buffer size ($K$), and buffer update period ($T$). Empirical results indicate that SFL improves learnability and solvability compared to existing methods, although differences in metrics such as shortest path and wall count are less pronounced. The authors also explore various definitions of learnability and their implications for performance, asserting that their approach is empirically justified.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant issue in UED, highlighting the sub-optimality of current methods and providing a plausible explanation for their poor performance.
- The proposed SFL algorithm is novel, straightforward, and yields promising results across multiple environments.
- SFL shows robust performance across different hyperparameter settings.
- The use of CVaR as a metric for evaluating robustness is a principled approach that adds depth to the analysis.
- Empirical results substantiate the claims of improved learnability and solvability compared to existing methods.
- The authors provide a comprehensive analysis of learnability metrics and their correlation with performance.

Weaknesses:
- The definition of learnability is questioned, particularly the rationale behind valuing environments solved 50% of the time equally to those solved 95% of the time.
- Many parameter choices lack justification, and the experiments are limited in scope, necessitating broader testing to substantiate claims of SFL's superiority.
- The evaluation metrics used may not fully capture the complexity of level difficulty.
- The writing is at times unclear and disorganized, with key concepts introduced before they are defined.
- There is a lack of exploration in more challenging environments, such as the 25 block setting, where UED methods are typically more effective.

### Suggestions for Improvement
We recommend that the authors improve the justification for their definition of learnability through additional experiments. Clarifying the rationale behind parameter choices, particularly hyperparameter \( \rho \), would strengthen the methodology. We also suggest improving the clarity of the relationship between hyperparameters and performance by providing additional visualizations or examples of environment features common in levels sampled with SFL. Expanding the experimental scope to include more environments and seeds, particularly in the more challenging 25 block setting, would provide a more comprehensive validation of SFL's performance. Additionally, incorporating the results on average vs. top-k learnability into the appendix of the final manuscript for better accessibility, and enhancing the clarity of the writing by ensuring that key terms are defined before use will improve the overall presentation of the paper.