ID: dqT9MC5NQl
Title: Approximately Equivariant Neural Processes
Conference: NeurIPS
Year: 2024
Number of Reviews: 22
Original Ratings: 5, 8, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an implementation of approximately equivariant neural processes (NP) by relaxing the equivariance of the NP decoder through the introduction of several learnable parameters as fixed inputs alongside data embeddings. The authors propose a new framework for soft/approximate equivariance based on functional analysis, applied to equivariant neural processes. They emphasize the significance of log-likelihood metrics for evaluating model predictions on unseen target data given context data, arguing that while context log-likelihood can serve as a diagnostic tool, it does not directly correlate with the model's ability to predict unseen targets, especially when the distributions of context and target data differ. The experiments demonstrate the method's effectiveness across various datasets, particularly in regression tasks, showing that additional inputs can enhance expressivity while breaking equivariance to a controlled extent.

### Strengths and Weaknesses
Strengths:
1. The proposed method is applicable across different architectures and is straightforward to implement by simply adding parameters as inputs.
2. The framework is conceptually well-grounded and general, with promising experimental results indicating improved accuracy.
3. The authors provide a clear framework for measuring practical performance through log-likelihoods of held-out target data.
4. Empirical results show that additional inputs improve model performance while maintaining a principled theoretical motivation.
5. The inclusion of quantitative metrics for equivariance error strengthens the argument for the models' approximate equivariance.

Weaknesses:
1. There is no supplemental material for source code, which limits reproducibility.
2. A concept figure would enhance understanding of the paper's intuition.
3. The analysis of equivariance error in trained models is absent, raising questions about the actual reflection of approximate equivariance in the dataset.
4. The paper only reports log-likelihood for target data, potentially leading to overfitting; separate log-likelihoods for context and target data are necessary for a comprehensive evaluation.
5. The number of fixed inputs needs analysis to validate its control over approximate equivariance.
6. All experiments focus solely on regression tasks, neglecting the generalization performance of NP across diverse task types.
7. There is insufficient exploration of the implications of high uncertainty in context reconstruction, which may misrepresent the stochastic nature of the processes involved.

### Suggestions for Improvement
We recommend that the authors improve the paper by including supplemental material for source code to enhance reproducibility. Additionally, incorporating a concept figure would aid in conveying the paper's intuition. It is essential to conduct an analysis of equivariance error in the trained models to ascertain the degree to which the model reflects the approximate equivariance of the dataset. We suggest reporting log-likelihoods for both context and target data separately to provide a clearer picture of the method's performance. Furthermore, an analysis of the number of fixed inputs should be included to substantiate its role in controlling approximate equivariance. Lastly, we encourage the authors to expand their experiments beyond regression tasks to demonstrate the method's versatility and generalization capabilities, and to include a more thorough discussion on the implications of high uncertainty in context reconstruction to enhance the understanding of the model's stochastic processes.