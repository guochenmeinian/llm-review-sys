ID: TXoZiUZywf
Title: Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 8, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the stochastic linear bandits problem and proposes an improved algorithm that utilizes a novel tail bound for adaptive martingale mixtures to construct tighter upper confidence bounds, leading to reduced regret compared to existing algorithms. The authors introduce two methods for computing confidence bounds: Convex Martingale Mixture UCB (CMM-UCB) and Analytic Martingale Mixture UCB (AMM-UCB), and demonstrate their performance through experiments on hyperparameter tuning tasks.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and presents an innovative technique that enhances the UCB algorithms, which have remained largely unchanged for over a decade.  
- The proposed methods leverage recent advancements in mean estimation and PAC-Bayesian generalization bounds, with clear and satisfying derivations.  
- Empirical results validate the performance gains of the proposed algorithms over existing linear bandit methods.  

Weaknesses:  
- The theoretical worst-case bound remains unchanged from previous work, limiting the paper's contribution.  
- The assumption of Gaussian mixture distributions raises questions about practical applicability and consequences if this assumption fails.  
- Many notations are inadequately defined, leading to potential confusion regarding their connections to existing regret analysis and parameter computations.  
- The choice of predictions \( \mu_t \) and \( T_t \) in the proposed methods lacks clarity, leaving uncertainty about their impact on the quality of guarantees.

### Suggestions for Improvement
We recommend that the authors improve the theoretical contributions by demonstrating how their methodology could yield improved bounds compared to previous results. Additionally, please clarify the practical implications of the Gaussian mixture assumption and address the inadequately defined notations throughout the paper. It would also be beneficial to elaborate on the selection of \( \mu_t \) and \( T_t \) to enhance understanding of their influence on the algorithm's performance. Finally, we suggest removing the claim regarding robustness to misspecification in the abstract, as it may mislead readers regarding the frequentist context of the work.