ID: IdQuUYMA1t
Title: DASH: Warm-Starting Neural Network Training in Stationary Settings without Loss of Plasticity
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 5, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper examines the impact of warm-starting on neural networks, particularly how it leads to poor generalization due to noise memorization. The authors propose a novel method called DASH, which selectively forgets previously memorized noise while preserving learned features. DASH operates on a per-neuron basis, utilizing cosine similarity between the negative gradient and neuron weights to inform weight adjustments. The authors present experimental results demonstrating DASH's effectiveness across various problems, often outperforming existing methods. Additionally, the paper explores DASH's performance in Class Incremental Learning (CIL) settings, revealing that in a traditional CIL setup, DASH performed poorly, achieving only 10% test accuracy, similar to warm-starting and random guessing. However, in a modified "data accumulation" setup, DASH demonstrated improved performance with a final accuracy of 83.74% and an average test accuracy of 87.86%. The authors attribute this success to the incorporation of previous data and the design of DASH, while acknowledging the need for further exploration in truly non-stationary environments.

### Strengths and Weaknesses
Strengths:
- The paper introduces a feature learning framework that effectively explores warm-start and cold-start training, supported by theoretical grounding and experimentation.
- DASH shows significant performance improvements in vanilla settings, outperforming other methods on small datasets and rivaling them in state-of-the-art (SoTA) settings.
- The authors effectively addressed major concerns regarding warm-starting issues and the loss of plasticity.
- The experimental results in the modified CIL environment demonstrate DASH's potential effectiveness in certain non-stationary settings.

Weaknesses:
- The compute overhead associated with DASH's gradient-based approach is unclear, and comparisons regarding compute and memory overhead are lacking.
- The assumption that initial gradient norm reliably predicts training steps is insufficiently supported, particularly in the context of warm-starting.
- The experiments do not establish a clear link between learned data and cosine similarity, and the evaluation is limited to toy datasets, raising questions about DASH's scalability and generality.
- The performance of DASH in standard CIL settings is inadequate, raising questions about its significance.
- The theoretical framework lacks rigor and clarity, leading to potential misunderstandings regarding the relationship between the framework and the algorithm.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the theoretical framework and its connection to the DASH algorithm, particularly regarding the rationale for using cosine similarity. Specifically, consider reorganizing the paper to present the main intuition behind the theoretical framework at the beginning of Section 2 and moving the motivation for the idealized method to the end of Section 3. Additionally, we suggest including a subsection at the beginning of Section 4 to clarify the relationship between DASH and the theoretical findings. Conducting ablation studies could elucidate the relationship between learned features and gradient cosine similarity. We also suggest including quantitative comparisons of compute and memory overhead associated with DASH and its baselines. Furthermore, we encourage the authors to conduct comprehensive hyperparameter tuning for DASH in the state-of-the-art setting and to release the code to ensure reproducibility and transparency. Lastly, addressing the limitations of the work more explicitly would enhance the paper's rigor and exploring DASH's performance in more realistic online learning settings would be beneficial.