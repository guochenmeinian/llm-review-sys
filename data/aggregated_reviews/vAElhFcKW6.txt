ID: vAElhFcKW6
Title: Reflexion: language agents with verbal reinforcement learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 6, 4, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Reflexion, a framework designed to enhance the learning capabilities of language agents through linguistic feedback. By allowing agents to reflect on task feedback and store these reflections in an episodic memory buffer, Reflexion aims to improve decision-making across various tasks, including text games, question answering, and code generation. The authors demonstrate that Reflexion outperforms baseline agents and includes thorough ablation studies to support their findings.

### Strengths and Weaknesses
Strengths:
- The framework introduces a novel approach to reinforcement learning by utilizing linguistic feedback, marking a significant departure from traditional methods.
- The paper is well-structured, with clear explanations and helpful visualizations that enhance reader comprehension.
- Extensive experiments across diverse tasks validate the effectiveness of Reflexion, with promising results and the introduction of a new benchmark, LeetcodeHardGym.

Weaknesses:
- Reflexion's generalization ability appears limited, as the framework's key components may require different designs for various tasks, potentially affecting performance.
- The evaluation methodology in the question-answering task may be biased due to the use of environment binary rewards, which could leak information from testing question-answer pairs.
- There is a lack of comparative analysis with related works mentioned in the literature review, which could strengthen the paper's contributions.

### Suggestions for Improvement
We recommend that the authors improve the integration of Reflexion with existing literature on learning from language feedback, particularly in relation to LLMs and RL agents. Additionally, it would be beneficial to acknowledge the potential data contamination issues associated with GPT-4 evaluations and consider evaluating against more transparent open language models. We also suggest providing statistics on the running time of the Reflexion framework and comparing it with other methods in terms of inference efficiency. Lastly, including error bars in the results would enhance the robustness of the findings and provide clearer insights into performance variability.