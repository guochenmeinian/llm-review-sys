ID: pirH9ycaNg
Title: Kernelized Reinforcement Learning with Order Optimal Regret Bounds
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a reinforcement learning (RL) algorithm called $\pi$-KRVI that achieves order-optimal regret through local kernelized optimistic least squares value iteration. The authors propose a method that partitions the state-action space, updating Q values within each cell using Gaussian Process-based Upper Confidence bounds. The work claims to establish the first sublinear regret bound in a general setting, scaling as $O(\sqrt{\Gamma(T) T})$, where $\Gamma(T)$ denotes maximum information gain.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a challenging problem in online RL for continuous state and action spaces, providing significant theoretical contributions.
2. It is well-written and presents a clear methodology, with a sound theoretical foundation for the proposed algorithm.

Weaknesses:
1. The stated results may be incorrect, as the claimed regret bounds appear to contradict known lower bounds in linear bandit settings.
2. The paper lacks numerical results, leaving questions about the practical applicability of the proposed methods.
3. There is insufficient discussion of relevant literature on RL beyond tabular and linear function approximation, which could strengthen the context of the work.
4. Several technical aspects, such as the definitions of parameters and the implications of assumptions, are unclear or inadequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the results by explicitly addressing the correctness of the stated bounds in relation to known lower bounds. Additionally, it would be beneficial to include numerical studies to demonstrate the algorithm's practical performance, particularly in challenging environments. We suggest that the authors provide a more comprehensive discussion of the existing literature on RL, particularly works that extend beyond linear approximations. Furthermore, clarifying the definitions of parameters like $\eta$ and providing proofs for key lemmas would enhance the paper's rigor. Lastly, we encourage the authors to consider incorporating stochastic rewards into their framework to better align with bandit settings and to discuss the implications of their assumptions more thoroughly.