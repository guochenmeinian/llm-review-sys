ID: HFQFAyNucq
Title: ResMem: Learn what you can and memorize the rest
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 5, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the residual-memorization (ResMem) algorithm, which aims to enhance neural network generalization by explicitly memorizing the residuals between the network's outputs and training labels using a k-Nearest Neighbors (kNN) regressor. The authors provide both theoretical and empirical validation, demonstrating that ResMem yields modest but consistent improvements in performance over standard neural networks across image classification and language modeling tasks.

### Strengths and Weaknesses
Strengths:  
- The proposed method is novel and offers a practical approach to leveraging memorization in neural networks.  
- The writing is clear, and the paper is well-structured, with both theoretical and empirical justifications supporting the claims.  
- The theoretical analysis includes a risk bound proof, adding depth to the contribution.  

Weaknesses:  
- The assumption of the proof (linear function, k=1) is impractical and misaligned with experimental conditions.  
- The empirical results raise concerns about the statistical reliability, particularly regarding whether results are averaged over multiple runs with different weight initializations.  
- The choice of a two-stage process over joint training of DeepNet and kNN is questioned, as is the role of the first training phase's duration.  
- The memory and computational burden of the ResMem method require further discussion, especially given the need to store training data representations.  
- The clarity of certain theoretical claims, such as the implications of Assumption 3.1 and the significance of the $T_2$ term in error analysis, needs improvement.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the theoretical results by including comparisons with cases without ResMem, particularly regarding the generalization error. Additionally, the authors should clarify the role of the first training phase's duration and its impact on the second stage's timing. It would be beneficial to provide a detailed discussion on the memory and computational costs associated with the ResMem algorithm, particularly in relation to large datasets. Furthermore, we suggest that the authors ensure that the reported results are averages over multiple runs to enhance statistical reliability. Lastly, the authors should consider revising the phrasing around memorization and generalization to avoid confusion, perhaps stating that "neural networks memorize and generalize."