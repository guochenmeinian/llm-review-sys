ID: P04rLpllH7
Title: A Black-Box Attack on Code Models via Representation Nearest Neighbor Search
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an adversarial attack method on programming-language models, specifically targeting models like codeT5 and codebert. The authors propose a representation nearest neighbor search (RNNS) approach that replaces variable names in input code to manipulate output labels. The method involves selecting variables based on their variance impact on output probabilities and finding substitutes through a nearest neighbors search in a learned embedding space. Empirical results indicate that RNNS achieves higher attack success rates compared to baseline methods while maintaining lower query times.

### Strengths and Weaknesses
Strengths:
- The search algorithm is novel and effectively utilizes embedding space for adversarial attacks.
- The empirical results demonstrate the superiority of RNNS over baseline methods.
- The paper identifies and addresses limitations in current research, providing innovative solutions.

Weaknesses:
- The relevance of the work to the broader NLP community is questionable, as much of the cited literature pertains to programming languages rather than human language.
- The abstract is overly lengthy and lacks clarity on key points and advantages.
- Some methodological details are insufficiently explained, leading to ambiguity regarding specific processes and contributions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the abstract to succinctly highlight the key contributions and advantages of the paper. Additionally, we suggest providing more detailed explanations of the methods, particularly regarding ExtractVar and RankVarsWithUncertainty in section 3.2.1. It would also be beneficial to discuss the implications of high query times observed in some datasets, as this may relate to the attack success rates. Furthermore, clarifying the threat model and the data used to train the encoder model would enhance the paper's rigor. Lastly, we encourage the authors to explore the integration of combinatorial optimization techniques and locality sensitive hashing to potentially accelerate the attack process.