ID: Mm5GXKvpXm
Title: CReTIHC: Designing Causal Reasoning Tasks about Temporal Interventions and Hallucinated Confoundings
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a causal reasoning dataset, constructed from existing causal inference datasets, utilizing verbal hallucinations and temporal interventions. The dataset aims to predict causal links between clauses and assess whether another clause strengthens or weakens this relationship. Additionally, the authors propose CReTIHC, a novel dataset designed to enhance the causal reasoning abilities of large language models (LLMs).

### Strengths and Weaknesses
Strengths:
- The paper provides an interesting approach to combining causal inference theory for improving commonsense reasoning models.
- It contributes to the literature on causal reasoning capabilities and offers a more challenging dataset for model evaluation.

Weaknesses:
- The main part of the data collection is relegated to the appendix, which should be included in the main body.
- The article lacks evaluation of state-of-the-art models like GPT-4 and does not provide a thorough analysis of the results or the newly obtained data.
- Concerns exist regarding the quality of the generated dataset and the circular evaluation of models generating and being evaluated on the same tasks.
- The explanation in some sections is repetitive, and certain phrases, such as "reversely weaken," lack clarity.

### Suggestions for Improvement
We recommend that the authors improve the paper by including the main data collection details in the body rather than the appendix. Additionally, the authors should evaluate state-of-the-art models like GPT-4 and provide a more in-depth analysis of the dataset, including data distribution and comparisons with existing datasets. We also suggest that the authors clarify ambiguous terms and phrases, such as "reversely weaken," and consider including a section on human evaluation of the generated data to address concerns about quality and circular evaluation.