ID: iifVZTrqDb
Title: ReLoRA: High-Rank Training Through Low-Rank Updates
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4

Aggregated Review:
### Key Points
This paper presents "ReLoRA: High-Rank Training Through Low-Rank Updates," a novel method for training large neural networks that utilizes iterative low-rank updates to enhance the efficiency of high-rank networks. The authors demonstrate the effectiveness of ReLoRA on transformer language models with up to 1.3B parameters, achieving performance comparable to traditional training methods. Extensive experiments validate the proposed method's efficiency and explore the impact of various components on performance.

### Strengths and Weaknesses
Strengths:
- The training methodology is innovative and combines multiple techniques effectively.
- The paper is well-written and easy to follow, providing valuable insights.
- Results indicate significant speedups and potential improvements in pre-training.

Weaknesses:
- The focus on fine-tuning dilutes the main narrative centered on pre-training, with weak fine-tuning results due to insufficient token usage compared to conventional T5 models.
- The training recipe's complexity and incompatibility with conventional training methods are concerning.
- Pre-training results lack clarity regarding model convergence and data repetition, and high perplexity values raise questions about the proposed method's effectiveness.
- Evaluation across different models is inconsistent and potentially confusing.

### Suggestions for Improvement
We recommend that the authors improve the focus on pre-training, as it is the primary emphasis of the paper, and minimize discussions on fine-tuning. The authors should provide clearer evaluations by including training curves to demonstrate convergence and addressing the perplexity baseline for better context. Additionally, we suggest limiting the evaluation to a specific set of models to enhance clarity. Finally, the authors should include standard deviations or confidence intervals for results to substantiate claims of significance.