ID: iuqCXg1Gng
Title: Saddle-to-Saddle Dynamics in Diagonal Linear Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 8, 6, 7, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 3, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the gradient flow dynamics in 2-layer diagonal linear networks for linear regression tasks, focusing on the behavior as the initialization scale approaches zero. The authors demonstrate that the gradient flow exhibits saddle-to-saddle dynamics, ultimately converging to the minimum $\ell_1$ norm solution. They provide an algorithm to compute the times and locations of these saddle points, revealing that each saddle corresponds to a constrained minimization problem with fixed coordinates. The analysis is conducted under a RIP assumption on the data, illustrating incremental learning through the sequential addition of coordinates at each saddle.

### Strengths and Weaknesses
Strengths:
- The paper is technically sound, well-written, and presents the material in an intuitive manner.
- It offers a novel characterization of gradient flow dynamics under minimal data assumptions, particularly the piecewise constant trajectory as $\alpha \rightarrow 0$.
- The explicit characterization of saddle points and the observation of deactivating coordinates are significant contributions.
- The paper addresses an important aspect of implicit regularization in a common toy problem, enhancing understanding of gradient descent dynamics.

Weaknesses:
- The analysis is limited to the quadratically parameterized regression setting, which may not generalize to more practical scenarios.
- The jump times and saddle locations are defined implicitly via Algorithm 1, making interpretation of intermediate iterates challenging.
- The entire analysis is conducted in the $\alpha \rightarrow 0$ limit, lacking exploration of small but finite initializations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definitions of jump times and saddle locations, possibly by providing more explicit interpretations of Algorithm 1. Additionally, exploring the dynamics for small but non-zero $\alpha$ could enhance the paper's impact. We suggest including a discussion on the implications of the identity activation function and how the findings might extend to nonlinear activation functions. Finally, addressing the significance of the connection to the Homotopy algorithm would provide valuable context for readers unfamiliar with it.