ID: fjXTcUUgaC
Title: Policy Finetuning in Reinforcement Learning via Design of Experiments using Offline Data
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 7, 5, 7, 7, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 3, 2, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel algorithm for policy fine-tuning in reinforcement learning, utilizing a dataset of pre-collected experiences to design a non-reactive exploratory policy that ultimately yields a locally near-optimal policy. The authors establish a nearly minimax-optimal upper bound for the sample complexity required to learn a local Îµ-optimal policy. The work addresses the practical need for non-reactive exploration in scenarios where switching policies incurs significant costs.

### Strengths and Weaknesses
Strengths:
- The paper introduces a unique setting where an agent can leverage an offline dataset while exploring the environment online, minimizing engineering costs associated with policy switching.
- The concept of sparsified MDP effectively combines optimism and pessimism principles, contributing interesting theoretical insights.
- The proofs are rigorous and the sample complexity bounds are tighter than previous methods, enhancing the theoretical framework of reinforcement learning.
- The paper is well-written and presents its ideas clearly, making it accessible despite its technical nature.

Weaknesses:
- The algorithm is limited to a tabular setting, restricting its broader applicability.
- The exploratory policy is confined to the sparsified MDP, which may overly limit the final policy by relying heavily on the offline dataset.
- Transitions that do not meet the threshold are discarded, potentially losing valuable information about the environment.
- The paper lacks empirical evaluations, which would demonstrate the algorithm's practical effectiveness and robustness.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their algorithm by exploring its application in larger state and action spaces, potentially incorporating function approximation. Additionally, we suggest providing empirical evaluations to compare the proposed algorithm's performance against fully offline or fully online methods. Clarifying the non-reactive nature of the policy and its implications in practical scenarios would also enhance the paper's impact. Finally, addressing the limitations of the work explicitly would provide a more comprehensive understanding of the study's scope and future directions.