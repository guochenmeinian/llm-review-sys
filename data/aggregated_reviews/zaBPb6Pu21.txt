ID: zaBPb6Pu21
Title: Chinese Lexical Substitution: Dataset and Method
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel large-scale Chinese lexical substitution (LS) dataset, CHNLS, created through human-machine collaboration, containing 33,695 instances across three text genres: News, Novels, and Wikipedia. The authors propose four LS methods—dictionary, embedding, BERT, and paraphraser-based approaches—and an ensemble method that combines these approaches, demonstrating superior performance on CHNLS evaluation. The paper includes both quantitative and qualitative analyses, highlighting the dataset's high coverage and quality, addressing the lack of Chinese LS datasets, and enabling future research in this area.

### Strengths and Weaknesses
Strengths:
- The creation of CHNLS fills a significant gap in Chinese NLP, being the first benchmark for Chinese LS.
- The collaborative annotation approach is innovative and enhances dataset coverage.
- Comprehensive experiments validate the dataset's utility and the effectiveness of the ensemble method.
- The paper is well-organized, clearly framed, and includes meaningful comparisons to existing datasets.

Weaknesses:
- CHNLS covers only three genres, suggesting a need for greater diversity.
- Some subjectivity and noise may arise from human evaluation of machine-generated substitutes, with unclear inter-annotator agreement.
- The ensemble method lacks detailed explanation regarding its construction and scoring scheme.
- There is limited analysis of generalization across genres, and the dataset quality requires improvement, as evidenced by errors in specific examples.

### Suggestions for Improvement
We recommend that the authors improve the diversity of the dataset to enhance its effectiveness. Additionally, the authors should clarify the quality assurance measures taken during the annotation process and ensure consistency among annotators. It would be beneficial to provide a more sophisticated explanation of the ensemble method and its evaluation across existing methods. Furthermore, we suggest including cross-genre evaluation and addressing the potential noise introduced by automatic translation. Lastly, the authors should integrate inter-annotator agreement statistics into their analysis.