ID: sbsaRj475E
Title: DiP-GO: A Diffusion Pruner via Few-step Gradient Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 4, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DiP-GO, a novel pruning method for diffusion models that addresses the high computational costs associated with their multi-step denoising process. The authors propose a SuperNet with backup connections to facilitate intelligent pruning without the need for retraining, transforming the pruning process into a SubNet search. The plugin pruner network identifies redundant computations through gradient optimization, enhancing efficiency. The authors utilize learnable queries to predict importance scores for computational blocks, further improving efficiency while maintaining performance. Extensive experiments validate DiP-GO's effectiveness across various diffusion models, achieving significant speedups of up to 4.4× on Stable Diffusion 1.5 with minimal loss in quality. The paper also includes detailed analyses of pruning patterns and hyperparameters, contributing to a deeper understanding of their approach.

### Strengths and Weaknesses
Strengths:
- DiP-GO efficiently predicts importance scores for computational blocks, enabling dynamic pruning without retraining.
- The pruner network utilizes consistency and sparsity optimization losses to balance generation quality and computational efficiency.
- Extensive experiments showcase superior performance and effective pruning strategies across different models, demonstrating versatility and robustness.
- Clear explanations of design choices, including the use of time embedding and learnable queries, enhance the paper's clarity.

Weaknesses:
- The vast search space may complicate the optimization process for identifying the optimal SubNet.
- Higher pruning ratios risk performance degradation, potentially compromising image quality and introducing artifacts, particularly in background details.
- Some definitions and values, such as the embedding dimension and covariance constant, require clarification.
- Missing data in initial submissions, particularly regarding few-step results and measurement methods, limits the robustness of the findings.
- The pruning strategy's effectiveness could be better contextualized against baseline methods.

### Suggestions for Improvement
We recommend that the authors clarify how the additional backup connections in the SuperNet are constructed—whether they are predefined or learned during training. Additionally, please improve the clarity of definitions, particularly for the embedding dimension $D$ and covariance constant $\beta$, in the revised version. Providing detailed computational resource requirements for training the pruner network compared to traditional methods would enhance understanding. The authors should also address the variability of the threshold $\tau$ across different datasets. Furthermore, including all relevant data, including few-step results and detailed measurement methods, would strengthen the robustness of the findings. A more intuitive design for the pruner network, possibly using $2N$ learnable queries and a timestep embedding vector, could improve efficiency. Lastly, including qualitative comparisons with baselines and analyzing pruning patterns concerning time-steps would further strengthen the paper.