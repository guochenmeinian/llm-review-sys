ID: q9dKv1AK6l
Title: Small steps no more: Global convergence of stochastic gradient bandits for arbitrary learning rates
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 6, 8, 5, 5, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of the stochastic gradient bandit algorithm, demonstrating that it converges to a globally optimal policy almost surely using any constant learning rate, even when traditional smoothness and noise control assumptions are not met. The authors provide proofs based on contradiction and reduction, revealing how the algorithm balances exploration and exploitation. Additionally, numerical simulations are included to support their theoretical findings.

### Strengths and Weaknesses
Strengths:
1. The paper offers a significant theoretical contribution by proving global convergence for any constant learning rate, advancing the understanding of stochastic gradient methods in bandit settings.
2. The analysis provides novel insights into the exploration-exploitation trade-offs inherent in the algorithm.
3. The proofs are presented with clarity and rigor, making the work accessible to readers with a solid background in stochastic optimization and reinforcement learning.
4. Empirical validation through simulations supports the theoretical results.

Weaknesses:
1. The practical implications of the findings, particularly regarding the choice of learning rate in real-world applications, are not thoroughly discussed.
2. While the paper establishes almost sure convergence, it does not provide specific rates of convergence, limiting practical utility.
3. The study is confined to multi-armed bandits, and extending the results to more general reinforcement learning settings would enhance its impact.
4. Assumption 1, which states that the true mean reward has no ties, is restrictive and may not be realistic in applications.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the practical implications of their findings, particularly how practitioners might choose an appropriate learning rate based on the results. Additionally, including a detailed analysis or conjectures about the rate of convergence would enhance the practical utility of the results. Addressing the limitations of Assumption 1 and discussing potential relaxations would strengthen the paper. Furthermore, we suggest extending the analysis to more complex reinforcement learning settings and comparing the stochastic gradient bandit algorithm with other bandit algorithms like UCB and Thompson Sampling in numerical experiments.