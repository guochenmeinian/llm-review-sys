ID: tP50lLiZIo
Title: Non-Stationary Bandits with Auto-Regressive Temporal Dependency
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 6, 5, 3, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of non-stationary bandit learning where rewards evolve according to an AR(1) model, proposing the AR2 algorithm to balance exploration and exploitation. The authors introduce a new performance metric, dynamic steady-state regret, and establish both upper and lower bounds on regret. Additionally, the paper investigates AR-based non-stationary bandits under a strong dynamic benchmark, aiming to characterize per-round regret concerning AR parameters. The authors propose an adaptable algorithm that outperforms existing non-stationary benchmarks. Theorem 5.1 is clarified to hold due to the clustering of expected rewards around zero in the small-$\alpha$ regime, rather than the long-run average reward approaching zero. The work contributes to the understanding of non-stationary multi-armed bandit problems, particularly in the context of unbounded non-stationarity, and includes a real-world case study on tourism demand prediction.

### Strengths and Weaknesses
Strengths:
- The paper contributes to the non-stationary case, contrasting with the majority of results focused on stationary settings.
- The use of the AR(1) model is well-motivated and provides a novel perspective on handling non-stationarity.
- The manuscript is well-organized, clearly presented, and includes a high-quality real-world case study.
- The authors provide a novel approach to AR-based non-stationary bandits, demonstrating superior performance against benchmarks.
- The adaptability of the algorithm to various time-varying environments is well-supported by theoretical and empirical evidence.
- The clarification of Theorem 5.1 enhances understanding of the conditions under which it holds.

Weaknesses:
- The definition of the AR(1) model with α∈(0,1) is problematic, as it qualifies as stationary rather than non-stationary.
- The assumption of bounded expected rewards within [-R, R] is unrealistic in a non-stationary environment, and the implications for regret are not adequately addressed.
- The justification for the alternation mechanism in the AR2 algorithm lacks clarity, particularly regarding the balance of exploration and exploitation.
- Theoretical results, particularly Theorem 5.1, do not adequately support the performance of AR2, especially in regimes where assumptions are violated.
- The assumption that $k \leq \mathcal{K}(\alpha)$ may limit the generalizability of the regret bounds.
- The ambiguity regarding the dominance of terms in the regret bound could lead to confusion about the conditions under which the bounds apply.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the theoretical claims, particularly regarding the implications of the AR(1) model's parameters and the bounded expected rewards. Additionally, the authors should provide a more thorough justification for the alternation mechanism in the AR2 algorithm, explaining why the exploration and exploitation periods are structured as they are. It would also be beneficial to clarify the relationship between the regret bounds and the parameter R, ensuring that the influence of R is accurately reflected in the analysis. Furthermore, we suggest that the authors explicitly address the implications of the assumption that $k \leq \mathcal{K}(\alpha)$ and consider discussing the performance of their algorithm in scenarios where this assumption does not hold. Finally, we encourage the authors to provide further justification for their choice of data simulation methods and to conduct experiments using real data, as the current reliance on synthetic data generated from an AR-4 model may undermine the validity of the case study.