ID: RwBObRsIzC
Title: Zero-Shot Tokenizer Transfer
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 7, 6, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to separating the tokenizer from the language model (LM) through zero-shot tokenizer transfer. The authors propose training a hypernetwork that maps embedding parameters between tokenizers, addressing limitations of fixed tokenizers in modern LMs. Extensive experiments demonstrate the effectiveness of this approach, even in zero-shot settings.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and easy to follow, with a clear description of the zero-shot tokenizer transfer problem and a state-of-the-art solution.  
- The authors provide detailed technical insights, such as the text sampling strategy and loss function design.  
- Experiments across various architectures, tokenizers, and tasks validate the proposed method's effectiveness.  

Weaknesses:  
- While technically sound, the paper could benefit from additional experiments on scaling with larger language models (LLMs) and testing with other open-sourced LLMs.  
- The training of the hypernetwork is time-consuming and specific to each LLM, which may limit practical efficiency compared to retraining the embedding layer for each model.  
- Concerns exist regarding the evaluation settings, particularly the overlap of token distributions between training and evaluation tokenizers.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by analyzing the overlap of token distributions between the original and target tokenizers, particularly for unseen languages. Additionally, we suggest moving Appendix G to the main text to illustrate the approach's applicability across different models. Further experiments should be conducted to assess how the hypernetwork generalizes to larger models and different modalities. Lastly, clarifying the hyperparameter settings, such as the max token length, would enhance the paper's rigor.