ID: ARAxPPIAhq
Title: xLSTM: Extended Long Short-Term Memory
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 10, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Extended Long Short-Term Memory (xLSTM), which enhances traditional LSTMs through exponential gating and new matrix memory, addressing LSTM limitations and improving memory storage and parallelization. The authors validate their method with exhaustive experiments, demonstrating that xLSTMs are competitive with State Space Models (SSMs) and Transformer-based architectures. The architecture comprises two main components: sLSTM, which integrates exponential gates and a stabilizing normalizer, and mLSTM, which employs matrix-form hidden states for improved memory capacity. The xLSTM architecture is constructed through residual stacking of these components.

### Strengths and Weaknesses
Strengths:
- The paper introduces an innovative approach to LSTM architecture that significantly enhances memory capacity and performance.
- It provides a comprehensive evaluation across various tasks, confirming the effectiveness of xLSTMs.
- The methodology allows for linear computation and constant memory complexity with respect to sequence length.

Weaknesses:
- Some notation is confusing, particularly regarding whether variables are scalars or vectors, leading to ambiguity in model definitions.
- The overall design of mLSTM lacks novelty, as similar concepts have been previously implemented in architectures like RetNet and GLA.
- The proposed exponential gating may hinder parallelization, impacting the efficiency of training and inference.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their notation by explicitly defining whether variables are scalars or vectors, as this would enhance understanding. Additionally, we suggest providing a more precise definition of the entire model in the main text rather than relying on figures. It would also be beneficial to discuss the implications of the exponential gating on parallelization more thoroughly, as well as to consider alternative normalization techniques. Lastly, we encourage the authors to include comparisons with existing architectures like GLA and to explore the potential for improved performance through architectural variations.