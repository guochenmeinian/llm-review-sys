ID: xbSu3ajAqh
Title: Attend First, Consolidate Later: On the Importance of Attention in Different LLM Layers
Conference: EMNLP/2024/Workshop/BlackBoxNLP
Year: 2024
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: 3, 4, 4

Aggregated Review:
### Key Points
This paper presents an investigation into the importance of different layers in large language models (LLMs) by manipulating the input to the attention mechanism. The authors replace previous hidden states with random vectors, freeze hidden representations, or use counterfactual tokens across various tasks and models. The findings indicate that models exhibit robustness to noise in higher layers but perform poorly when corruption occurs in earlier layers, suggesting a two-phase processing model. The study proposes that information is initially gathered from previous tokens before being refined internally in later layers.

### Strengths and Weaknesses
Strengths:
- The paper employs careful manipulations of token representations, demonstrating that model behavior can change significantly based on the layer affected by noise.
- It is well-written and easy to follow, with a comprehensive study involving multiple LLMs and datasets.
- Results are consistent across models and interventions, providing strong support for the two-stage processing hypothesis.

Weaknesses:
- The novelty of the findings is diminished by the omission of relevant research on pruning attention heads and previous interpretability work, which have explored similar principles.
- The analysis is relatively coarse-grained; broad conclusions about the importance of top layers may overlook specific sensitivities in certain models, suggesting a need for a more fine-grained analysis.

### Suggestions for Improvement
We recommend that the authors improve the literature review by incorporating discussions on pruning attention heads (e.g., Michel et al. 2019, Voita et al. 2019) and previous interpretability work (e.g., Meng et al. 2022) to enhance the novelty of their findings. Additionally, a more detailed analysis of the final layers' functions and their relevance in specific contexts would strengthen the paper. We also suggest clarifying the distinction between freezing hidden states and early exiting, as this could provide valuable insights into the necessity of internal processing. Finally, exploring the potential for models to recover from noise in later layers and conducting experiments on the effects of replacing hidden states across layers could yield interesting results.