ID: PXkS70nuNp
Title: CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of Offsite-Tuning (OFT), a technique for transferring transformer blocks between centralized large language models (LLMs) and downstream emulators, addressing privacy concerns associated with using private instruction data. The authors conduct an empirical analysis revealing a modular structure within LLM layers that emerges as model size increases, alongside subtle changes in representation and predictions. They propose CRaSh (Clustering, Removing, and Sharing), a training-free strategy that significantly enhances OFT performance with billions of parameters. The analysis of the loss landscape indicates linear connectivity among fine-tuning optima, supporting the effectiveness of CRaSh and OFT.

### Strengths and Weaknesses
Strengths:
- The paper provides an in-depth empirical analysis of LLMs, offering valuable insights into their representation and functional similarity.
- The proposed CRaSh strategy is novel and demonstrates significant improvements in OFT performance across various datasets.
- The exploration of privacy concerns in tuning centralized LLMs with private data is timely and relevant.

Weaknesses:
- The motivation for basing the proposed approach on OFT lacks clarity, particularly regarding its advantages as a privacy-preserving method.
- More background on OFT is needed to make the work more self-contained.
- The method reduces the number of parameters to train but does not address inference cost, raising questions about its long-term effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation for using OFT, explicitly discussing its privacy-preserving advantages and how it facilitates the adaptation of large pretrained models without full access. Additionally, providing more background details on OFT would enhance the paper's self-containment. We also suggest including a more comprehensive analysis and ablation studies of the CRaSh method, particularly in terms of computational budget comparisons and the performance of emulators before and after integration with the original LLM. Addressing these points will strengthen the overall impact of the paper.