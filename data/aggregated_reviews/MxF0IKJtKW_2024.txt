ID: MxF0IKJtKW
Title: SlimGPT: Layer-wise Structured Pruning for Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SlimGPT, a novel framework for structured pruning of large language models (LLMs) that modifies the Optimal Brain Surgeon (OBS) framework. It introduces Batched Greedy Pruning to enhance head-wise pruning performance through Cholesky decomposition and improves feedforward network (FFN) pruning efficiency via Dynamic Group Size. Additionally, SlimGPT employs an Incremental Pruning Ratio to address error accumulation in layer-wise pruning. Experimental results on LLaMA, LLaMA-2, and other popular LLMs demonstrate that SlimGPT achieves state-of-the-art results in LLM pruning.

### Strengths and Weaknesses
Strengths:
1. The paper extends the OBS framework to structured pruning with a strong theoretical foundation, supported by thorough technical details.
2. Extensive experimental results validate that SlimGPT achieves state-of-the-art performance, surpassing existing methods in the field.

Weaknesses:
1. The Incremental Pruning Ratio strategy, particularly the logarithmic increasing method, requires further exploration; alternative strategies should be tested to maximize SlimGPT's potential.
2. The paper lacks a careful comparison of throughput (e.g., tokens per second) between pruned models generated by SlimGPT and competing baselines, which is essential to demonstrate deployment benefits.
3. Evaluation is limited primarily to the LLaMA family, raising concerns about the generalizability of results across other popular LLMs.
4. The empirical design of layer-wise sparsity lacks theoretical analysis, and the evaluation methodology does not adequately control for calibration dataset selection.

### Suggestions for Improvement
We recommend that the authors improve the exploration of different increasing strategies for the Incremental Pruning Ratio to fully leverage SlimGPT's capabilities. Additionally, the authors should provide a detailed comparison of throughput metrics between SlimGPT and competing methods to substantiate claims of efficiency. Expanding the evaluation to include a broader range of LLMs beyond the LLaMA family, as well as incorporating common intelligence datasets like MMLU and GSM8k, would enhance the comprehensiveness of the study. Finally, a more thorough theoretical analysis of layer-wise sparsity design and a discussion of the pros and cons of pruning compared to other efficiency techniques, such as quantization, would strengthen the paper's contribution.