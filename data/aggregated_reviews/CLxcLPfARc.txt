ID: CLxcLPfARc
Title: Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 4, 4, 5, 7, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 5, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents embedding space attacks as a novel threat model targeting open-source large language models (LLMs). The authors demonstrate that these attacks can effectively bypass safety alignments and extract supposedly unlearned information from LLMs. The methodology is applied to two main goals: breaking safety guardrails in aligned models and showing that unlearned information is not actually forgotten. The authors conducted extensive experiments across multiple datasets and models, introducing new evaluation metrics such as the cumulative success rate for assessing unlearning quality.

### Strengths and Weaknesses
Strengths:
- The introduction of a new type of adversarial attack provides a fresh perspective on security in open-source LLMs.
- The research addresses an urgent need for comprehensive security strategies as open-source models become more prevalent.
- The paper is well-structured, with extensive evaluations demonstrating the effectiveness of the proposed attack.

Weaknesses:
- The paper lacks methodological novelty, as the proposed attack is heavily inspired by existing adversarial techniques.
- The evaluation methodology is questionable; the use of the toxic-bert model for toxicity detection may not be reliable, and comparisons with prefilling attacks are insufficient.
- There is a lack of clarity regarding the utility metrics and attack success metrics, with multiple metrics presented without clear justification for their relevance.

### Suggestions for Improvement
We recommend that the authors improve the clarity and justification of the utility metrics used in the paper, as the current terminology may not accurately reflect the attack's objectives. Additionally, we suggest that the authors streamline the attack success metrics by selecting one or two that best correlate with human judgment, rather than presenting multiple metrics without clear motivation. A more detailed discussion of the evaluation methodology is necessary, particularly regarding the reliability of the toxic-bert model and comparisons to prefilling attacks. Finally, we encourage the authors to explore and clarify the novelty of their approach, emphasizing any unique contributions to the field.