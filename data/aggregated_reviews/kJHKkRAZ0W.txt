ID: kJHKkRAZ0W
Title: NTKCPL: Active Learning on Top of Self-Supervised Model by Estimating True Coverage
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 7, 4, 6, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 2, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel active learning strategy called NTKCPL, which integrates self-supervised learning with neural tangent kernel (NTK) approximation to enhance empirical risk estimation. The method iteratively selects unlabeled data samples for annotation based on their potential to minimize classification error, demonstrating superior performance across various datasets compared to existing methods. Additionally, the authors propose a method for generating Clustered Prediction Labels (CPL) aimed at improving classification accuracy by refining clusters based on the purity of true labels. They emphasize that the ideal CPL should consist of clusters containing only one true label category to minimize impurity error and reduce the risk of over-clustering error. The authors provide both theoretical and empirical validation for their approaches, showing improvements over baseline techniques in low and high budget scenarios.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and informative, making complex concepts accessible.
- It includes a comprehensive analysis and thorough experimental results, validating the proposed methods' effectiveness.
- The originality of combining NTK with clustering-based pseudo-labels for active learning is noteworthy.
- The paper offers a clear mathematical framework for analyzing the errors associated with CPL.
- The iterative refinement method proposed for CPL generation effectively addresses issues of cluster purity.

Weaknesses:
- The clarity of the writing is inconsistent, with several notational ambiguities and unclear definitions throughout the paper.
- The novelty of the approach may not be strong enough for publication, as it builds on existing concepts without sufficiently differentiating itself.
- There is a lack of detailed analysis regarding the computational complexity and scalability of the proposed method across different datasets and budget regimes.
- The mathematical notations used in the paper are not sufficiently clear, which may hinder understanding.
- The proposed method's description lacks clarity regarding the budget allocation for clustering.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing by refining notational definitions and ensuring consistency throughout the paper. Specifically, Section 3.3 should be simplified to better explain the arguments leading to the propositions. Additionally, we suggest including a comprehensive analysis of how NTKCPL scales with data size and dimensionality, as well as a comparison of running times with other state-of-the-art methods. The authors should also consider adding more comparisons with additional self-supervised learning methods and clarify the significance of the "Effective Budget Ratio" in their results. Furthermore, we recommend that the authors improve the clarity of mathematical notations to enhance reader comprehension and provide a more detailed explanation of the iterative refinement method, particularly regarding the budget allocation for clustering. Finally, addressing the concerns raised about the overall clarity and coherence of the paper will strengthen its impact.