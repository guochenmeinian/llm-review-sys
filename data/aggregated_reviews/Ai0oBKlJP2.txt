ID: Ai0oBKlJP2
Title: ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the effectiveness of ChatGPT in non-English language tasks, addressing the gap in its performance evaluation across diverse languages. The authors conduct extensive testing across seven tasks and 37 languages, revealing that ChatGPT's performance in zero-shot learning is generally inferior to supervised models, particularly in languages with varying resource availability. The findings indicate a bias towards English, as ChatGPT performs better with English prompts and in tasks requiring complex reasoning.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant gap in evaluating large language models (LLMs) like ChatGPT for diverse languages.
- It provides a comprehensive evaluation across 37 languages and 7 tasks, contributing valuable insights into the capabilities and limitations of ChatGPT.

Weaknesses:
- The conclusions are somewhat unsurprising, as the underperformance of non-English tasks is expected due to the predominance of English in LLM training data.
- The presentation resembles a technical report rather than a structured paper, lacking a clear roadmap and depth in analysis.
- The version of ChatGPT used is unspecified, which may mislead future research.

### Suggestions for Improvement
We recommend that the authors improve the paper's structure to provide a clearer roadmap and deeper analysis of findings. Additionally, including results from different LLMs and varying model sizes would enhance the study's comprehensiveness. The authors should also consider incorporating few-shot learning results, as this could yield significant insights. Furthermore, we suggest reporting evaluation metrics primarily in F1-score rather than accuracy, and ensuring that prompt engineering is addressed when discussing zero-shot performance. Lastly, including references to recent studies on prompt effectiveness in other languages would strengthen the claims made in the paper.