ID: ulqYwmcUnL
Title: XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the UP benchmark, a user-centric scarce-data benchmark for under-represented languages, addressing the lack of resources and evaluation frameworks for these languages. It evaluates language models across 88 under-represented languages over nine tasks, including newly created datasets for OCR, auto-complete, question answering, semantic parsing, and transliteration. The benchmark emphasizes few-shot settings and aims to shift focus from high-resource to low-resource languages.

### Strengths and Weaknesses
Strengths:
1. The UP benchmark provides a large-scale evaluation of language models across diverse tasks for under-represented languages.
2. It introduces new datasets that can significantly benefit the NLP community by enhancing the development of models for these languages.
3. The paper clarifies the notion of user-centric tasks, justifying the inclusion of tasks like ASR and MT.

Weaknesses:
1. The choice and number of baselines are not sufficiently justified, raising concerns about the limited model evaluation.
2. The experimental setup for ASR is flawed, and tasks like NER and QA are not traditionally treated as sequence-to-sequence tasks.
3. The analyses section lacks depth, failing to explore reasons for performance discrepancies among languages, particularly African languages.
4. The estimation of annotation effort is underspecified, lacking quantification and comparative measurement.

### Suggestions for Improvement
We recommend that the authors improve the justification for the choice and number of baselines, potentially including a wider range of models. Additionally, the authors should revise the experimental setup for ASR and clarify the treatment of NER and QA tasks. We suggest enhancing the analyses section with deeper insights into performance variations among languages and providing references to existing literature. Furthermore, we encourage the authors to quantify the estimation of annotation effort and provide detailed descriptions of dataset generation processes to assess quality effectively.