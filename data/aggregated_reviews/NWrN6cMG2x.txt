ID: NWrN6cMG2x
Title: Moment Matching Denoising Gibbs Sampling
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 5, 5, 5, 6, -1, -1, -1
Original Confidences: 3, 1, 4, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new denoising score matching (DSM) technique for training energy-based models (EBMs) that addresses the inconsistency problem by training a "noisy" model and sampling "clean" samples from it. The authors theoretically demonstrate that a "clean" model is uniquely identified by an EBM learning a noisy data distribution. They introduce a sampling strategy inspired by Gibbs sampling to approximate the denoising distribution, allowing sampling from the clean model without retraining. The paper also discusses scaling the approach to high-dimensional data, overcoming the quadratic cost of evaluating the Hessian of the DSM.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow, with most claims justified by citations or mathematical proofs.
- Prior work is appropriately cited, and experiments on synthetic and real-world datasets (MNIST/CIFAR) effectively assess the proposed technique's quality.
- The theoretical derivation and presentation are clear and well-defined.

Weaknesses:
- The overall contribution to the field of generative models is unclear, particularly regarding the scalability issues introduced by the denoising techniques.
- Limited experiments conducted may not effectively validate the proposed method; further experiments across a wider range of scenarios and datasets are needed.
- The effectiveness of the proposed method in general is uncertain, as some image examples show significant performance gaps compared to existing methods.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the quality of the approximation by comparing the full Hessian with its approximation to address concerns about training stability and output sample quality. Additionally, providing a detailed description of the full sampling algorithm would clarify the training and sampling procedure for readers. Expanding the experimental results to include a broader range of datasets would enhance the robustness and generalizability of the findings. Finally, addressing the inconsistencies noted in the performance of the proposed method compared to existing techniques would strengthen the paper's contributions.