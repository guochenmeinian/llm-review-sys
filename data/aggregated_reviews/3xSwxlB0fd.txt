ID: 3xSwxlB0fd
Title: Uncoupled and Convergent Learning in Two-Player Zero-Sum Markov Games with Bandit Feedback
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 6, 7, 6, 6, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 2, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on designing uncoupled learning dynamics that converge to Nash equilibria in two-player zero-sum Markov games, introducing the first dynamics that achieve last-iterate convergence in self-play with bandit feedback. The authors propose an algorithm that guarantees convergence under specific conditions, achieving rates of O(t^{-1/8}) for standard matrix games, O(t^{-1/9}) for irreducible Markov games, and O(t^{-1/10}) for general Markov games, defined by a new notion of path convergence. The paper also incorporates entropy regularization into online mirror descent, enhancing the theoretical and practical aspects of the algorithms.

### Strengths and Weaknesses
Strengths:  
- The problem is well-motivated and relevant, with the last-iterate convergence property under bandit feedback being particularly suitable for many applications.  
- The technical contributions appear novel, and the paper is well-written, making complex ideas accessible.  
- The proposed algorithms do not require players to exchange information, extending to general Markov games without assumptions on dynamics.  

Weaknesses:  
- The results seem to be minor adaptations of existing techniques, lacking a clear discussion on the novelty of the approach.  
- A significant portion of the paper is dedicated to matrix and irreducible Markov games, while the section on general Markov games should be expanded.  
- There is insufficient detail regarding the computational cost of updating strategies and the implications of path convergence on average-iterate convergence.  
- The lack of experimental results limits the ability to frame the theoretical findings against current state-of-the-art methods.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the novelty of their approach to clarify how it differs from existing techniques. Additionally, we suggest expanding the section on general Markov games to provide a more comprehensive analysis. It would also be beneficial to include details on how optimism can be leveraged in the algorithm and to clarify the computational cost associated with strategy updates. Finally, we encourage the authors to conduct experiments comparing their algorithms to current state-of-the-art methods to better contextualize their results.