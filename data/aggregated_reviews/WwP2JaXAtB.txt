ID: WwP2JaXAtB
Title: Probabilistic Invariant Learning with Randomized Linear Classifiers
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 6, 7, 4, 5, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Randomized Linear Classifiers (RLCs) as a novel approach to binary classification that leverages randomness to achieve universality and invariance while minimizing computational requirements. The authors derive a trade-off between the probability of accurate predictions and the number of samples needed for majority voting in RLCs. They extend the application of RLCs to invariant learning problems involving sets, graphs, and spherical data, demonstrating their potential advantages over deterministic models like DeepSets and GNNs.

### Strengths and Weaknesses
Strengths:
- **Clarity:** The paper is well-written and understandable despite its theoretical complexity.
- **Originality:** It introduces the use of probabilistic classifiers for invariant learning, a first in this context.
- **Significance:** The work addresses computational barriers in deterministic settings, providing a new perspective on problem-solving.
- **Quality:** The mathematical analysis is rigorous, with Theorem 1 offering practical insights into the sample-accuracy trade-off.

Weaknesses:
- The analysis is limited to binary classification, and the claimed computational savings are not clearly substantiated by theorems.
- Experimental designs do not effectively highlight computational advantages, focusing instead on small-scale accuracy improvements.
- The assertion regarding GNN limitations is inaccurate, as modern architectures can address these issues.
- RLCs may not be computationally efficient at inference, as each prediction requires multiple evaluations.
- No code has been shared, raising reproducibility concerns.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the computational resource savings claimed in the paper, ensuring that theorems directly support these assertions. Additionally, we suggest designing experiments that better illustrate the computational advantages of RLCs, particularly by comparing them against strong baselines with sufficient capacity. It would also be beneficial to discuss the implications of the external randomness on generalization performance and to clarify the impact of the hyperparameter $m$ on model accuracy. Finally, sharing code would enhance reproducibility and facilitate further exploration of the proposed methods.