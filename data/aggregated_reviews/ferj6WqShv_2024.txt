ID: ferj6WqShv
Title: Exploiting Descriptive Completeness Prior for Cross Modal Hashing with Incomplete Labels
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 6, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach called Prompt Contrastive Recovery for Incomplete Labels (PCRIL) aimed at addressing the challenges of cross-modal hashing with incomplete labels. The authors propose a CLIP-based prompting scheme and a complementary semantic propagation mechanism to restore unknown labels and enhance pairwise similarities. The methodology includes a learnable prompt for encoding class combinations and employs augmentation techniques to manage extreme cases of unknown labels. Experimental results demonstrate significant improvements in mean Average Precision (mAP) compared to existing state-of-the-art methods.

### Strengths and Weaknesses
Strengths:
1. The paper effectively tackles the common issue of incomplete annotations in cross-modal hashing, presenting a technically sound solution.
2. The proposed PCR module successfully restores information semantics through label prompts, as evidenced by experimental validation.
3. The combination of prompt processing and contrastive learning is innovative and enhances similarity learning.

Weaknesses:
1. The novelty of the proposed method is limited, with insufficient comparison to recent state-of-the-art methods, particularly those published within the last two years.
2. Some techniques lack clear motivation, such as the asymmetric nature of mix-up augmentation, and the paper does not adequately analyze the effects of prompt construction.
3. The Method section requires more clarity, as the separation of figures and explanations makes it challenging to follow the innovative aspects of the model.

### Suggestions for Improvement
We recommend that the authors improve the comparison with recent state-of-the-art methods to enhance the credibility of their experimental results. Additionally, conducting more experiments on the IAPR TC-12 dataset would provide a clearer illustration of the model's effectiveness. The authors should elaborate on the motivation behind the proposed techniques, particularly regarding negative subsets and contrastive learning, and clarify the theoretical foundations of their methods. Finally, a more cohesive presentation of the Method section is necessary to facilitate understanding of the proposed framework.