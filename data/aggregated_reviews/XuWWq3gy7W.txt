ID: XuWWq3gy7W
Title: BitDelta: Your Fine-Tune May Only Be Worth One Bit
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 4, 5, 6, 7, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents BitDelta, a method that quantizes the aggregated weight updates (referred to as "delta") to 1-bit after full fine-tuning. The authors claim that this approach reveals the redundancy of delta information and is beneficial for multi-client-single-server applications, achieving up to 10x memory reduction and similar latency improvements. The method decomposes fine-tuned models into a pretrained version and a binary delta, utilizing self-distillation to learn optimal scaling factors.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a significant issue in fine-tuning large language models (LLMs) with a novel 1-bit quantization approach.
2. Experiments are conducted on prominent models such as LLaMa-2 and Mistral.
3. The authors provide a kernel for INT1xFP16 matrix multiplication, contributing to practical implementation.

Weaknesses:
1. The claim of redundancy in delta information is not novel, as it aligns with existing PEFT methods like LoRA.
2. The paper fails to account for full fine-tuning costs, only measuring memory and latency during the serving phase. A comprehensive comparison of "full-fine tuning+1-bit optimization+serving" against "fine-tuning+serving" in PEFT schemes is recommended.
3. Table 6 presents higher accuracy for FP+delta compared to GPTQ, but a memory vs. accuracy tradeoff analysis as a function of client numbers is needed.
4. The paper lacks a performance model for decoding latency based on client numbers, which is crucial to its claims.
5. The specific decomposition method for binarizing deltas is not adequately described, and comparisons with existing low-bit fine-tuning methods like Q-LoRA and QA-LoRA are necessary.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by explicitly defining and presenting the limitations of their method. Additionally, the authors should provide a detailed comparison of memory consumption across different compression techniques and clarify the initialization method for scaling factors used in self-distillation. It would also be beneficial to include a performance model for decoding latency as a function of client numbers and to discuss the potential integration of BitDelta with other compression technologies. Lastly, the authors should ensure that all figures and tables are referenced appropriately within the text to enhance understanding.