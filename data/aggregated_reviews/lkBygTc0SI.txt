ID: lkBygTc0SI
Title: Do SSL Models Have Déjà Vu? A Case of Unintended Memorization in Self-supervised Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 6, 8, 6, 6, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a thorough investigation into the undesired phenomenon of "deja-vu" memorization in self-supervised learning (SSL) models, where models memorize specific training data instead of learning meaningful representations. The authors define a "deja-vu" score that quantifies how much information about a training sample can be reconstructed from a pretrained model. They provide extensive empirical results demonstrating the conditions under which memorization occurs and propose methodologies to mitigate this effect.

### Strengths and Weaknesses
Strengths:
- The authors introduce a novel concept and testing methodology for assessing memorization in SSL models.
- Comprehensive empirical results are provided, exploring various aspects of SSL modeling and training.
- Clear presentation of results and methodologies enhances understanding.
- The paper lays out significant directions for future research on privacy risks associated with SSL.

Weaknesses:
- The impact of different augmentations on memorization is not discussed, despite their importance in SSL performance.
- Confidence in the memorization estimates is limited, as comparisons are made with only one reference SSL model.
- The necessity of the generative model remains unclear, as kNN images appear sufficient.
- Results are based solely on ImageNet data, raising questions about generalizability to other datasets.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the impact of different augmentations on memorization, as this could provide valuable insights into SSL performance. Additionally, including more reference SSL models would enhance confidence in the memorization results. Clarifying the role of the generative model in the context of kNN images is also essential. Finally, exploring the generalizability of findings across different datasets, such as CelebA or medical imaging datasets, would strengthen the paper's contributions.