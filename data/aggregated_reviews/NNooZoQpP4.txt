ID: NNooZoQpP4
Title: To Stay or Not to Stay in the Pre-train Basin: Insights on Ensembling in Transfer Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on ensembling in transfer learning, specifically addressing the dilemma of pre-training constraining models to a single "basin" in the loss landscape. The authors propose the StarSSE method, which involves fine-tuning a pre-trained model and then performing multiple fine-tunings that branch out from this initial model. The results indicate that staying within the pre-trained basin is crucial for ensemble performance, as diverging from it can degrade model quality.

### Strengths and Weaknesses
Strengths:
- The proposed StarSSE method is well-motivated and shows promising performance across the datasets examined, representing a practical update to existing techniques.
- The empirical analysis supports the argument that remaining in the pre-trained basin enhances ensemble quality, contributing valuable insights to the field.

Weaknesses:
- The experiments are limited in scale, which is a concern given the paper's focus on pre-training. The authors could benefit from exploring larger datasets and comparing StarSSE against diversified Local DEs.
- There are indications of overfitting in the results, particularly in Figures 2 and 3, which could be addressed through standard techniques such as label smoothing or data augmentation.
- Key terms like "basin" are frequently used but not formally defined, leading to ambiguity in strong claims about their impact on model performance.

### Suggestions for Improvement
We recommend that the authors improve the scale of their experiments by including a broader range of datasets to validate the efficacy of StarSSE. Additionally, we suggest that the authors investigate and report on the performance of StarSSE compared to diversified Local DEs, as well as explore techniques to mitigate overfitting. Clarifying the definitions of key terms and providing a limitations section would also enhance the paper's rigor. Finally, including a table of results for optimal hyperparameter settings would aid in quantitatively evaluating the performance of StarSSE against SSE.