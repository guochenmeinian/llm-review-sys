ID: zogaeVpbaE
Title: DevBench: A multimodal developmental benchmark for language learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 7, 5, 6, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DevBench, a multimodal benchmark designed to evaluate language models through seven tasks that assess lexical, syntactic, and semantic abilities. The tasks are inspired by human language development and aim to measure machine-human similarities, potentially guiding improvements in language model training. The evaluations indicate that machine-human similarities correlate with various features, suggesting insights into the learning processes of both machines and humans.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with a strong motivation and clear comparisons to prior works.
- DevBench introduces alternative evaluation metrics beyond traditional accuracy, focusing on human-model response similarities and performance trajectories.
- It encompasses a wide range of tasks, with each category represented by existing datasets and aligned with typical human developmental stages.

Weaknesses:
- The selection of baseline models is limited, excluding advanced vision-language models such as lava-1.5 and GPT-4o, which could enhance the evaluation. 
- The interpretation of results, particularly regarding KL divergence measurements, is unclear, raising questions about model performance consistency.
- The GitHub repository is currently inaccessible, hindering reproducibility and community engagement.

### Suggestions for Improvement
We recommend that the authors improve the selection of baseline models by including advanced vision-language models like lava-1.5 and GPT-4o. For GPT-4o, consider sampling at a higher temperature to obtain logits. Additionally, clarify the interpretation of KL divergence results, particularly for tasks where models score similarly, and provide insights on how a random baseline would perform. We suggest including specific details about the experimental setup in the appendix, such as user demographics and the main goals of source papers. Furthermore, clarify the methods used for calculating (dis)similarity between human and model responses, and strengthen the conclusion by discussing how the benchmark can inspire future research directions in both ML and cognitive science. Lastly, ensure the GitHub link is functional and provide a dataset card for better documentation.