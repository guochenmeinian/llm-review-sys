ID: Sf3t6Bth4P
Title: Enhancing Sharpness-Aware Optimization Through Variance Suppression
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 4, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper proposes a method that applies Exponential Moving Average (EMA) on batch gradients to enhance the sharpness-aware minimization (SAM) framework, aiming to alleviate gradient noise and improve test accuracy. Theoretical analysis indicates that the proposed method achieves a similar convergence rate as the original SAM, while empirical results demonstrate its effectiveness across various applications, including image classification and neural machine translation. The method also shows robustness against large label noise.

### Strengths and Weaknesses
Strengths:
- The proposed method is easy to implement and understand.
- The theoretical analysis is sound, providing sufficient justification for the variance suppression approach.
- Empirical results indicate improvements in performance, particularly under label noise conditions.

Weaknesses:
- Some claims lack adequate support, particularly regarding the relationship between gradient noise and generalization performance.
- The introduction is overly focused on related works, detracting from experimental results that could substantiate key claims.
- The experimental comparisons are limited, missing several baseline methods, including Fisher SAM, and do not fully explore the integration of the proposed method with other SAM variants.

### Suggestions for Improvement
We recommend that the authors improve the support for claims made in Lines 47-48 and 50 by providing additional evidence or experimental results. It would be beneficial to include a more comprehensive literature review addressing previous findings on SAM and its relationship with minibatch noise. Additionally, we suggest that the authors add more baseline methods to their experiments, particularly Fisher SAM and ASAM combined with VaSSO, to strengthen their comparative analysis. Finally, clarifying the computational implications of integrating VaSSO with efficient SAM variants would enhance the paper's clarity and depth.