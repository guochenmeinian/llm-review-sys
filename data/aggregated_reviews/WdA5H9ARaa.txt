ID: WdA5H9ARaa
Title: Benchmark Inflation: Revealing LLM Performance Gaps Using Retro-Holdouts
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 6, 6, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two major contributions: a procedure for creating statistically-indistinguishable retro-holdout evaluation test sets and the resulting dataset, Retro-TruthfulQA. The authors evaluate 20 LLMs on this dataset, demonstrating that many models exhibit lower scores compared to the original TruthfulQA dataset, indicating potential overfitting or evaluation gaming. The authors argue that showcasing the measurable gap between real-world LLM performance and public dataset scores, particularly on TruthfulQA, is sufficient to support their claims. They emphasize that their approach is not adversarial and that intent tuning should not influence the results. The statistical tests employed ensure the retro-holdout dataset is indistinguishable, reinforcing their findings regarding benchmark inflation in LLM evaluation.

### Strengths and Weaknesses
Strengths:
- The procedure for creating retro-holdouts is thorough and has the potential to inspire further research.
- The study effectively highlights a significant gap between actual LLM performance and public dataset scores.
- The focus on a single dataset, TruthfulQA, allows for a detailed and in-depth analysis.
- The paper convincingly argues the presence of benchmark inflation in TruthfulQA, highlighting the need for improved evaluation methodologies.
- The authors provide a solid methodological foundation for their claims regarding benchmark inflation.
- The experimental results validate the proposed method's effectiveness.

Weaknesses:
- The paper lacks detailed discussion on model performance and evidence of evaluation gaming, particularly regarding which models overfit.
- Certain sections, such as the dataset construction process and iterative methods, are described vaguely, hindering reproducibility.
- Some visualizations, like Figure 4, are unclear and require better presentation.
- The analysis may benefit from the inclusion of additional datasets to further validate the findings.
- There is a lack of clarity on the expectations for incorporating different datasets, which could enhance the robustness of the study.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their visualizations and provide more detailed descriptions of the dataset construction process, including the iterative methods used. Additionally, further exploration of model performance on the dataset and a discussion on the implications of their findings would enhance the paper. Addressing the vagueness in sections 2.1 and 2.3 regarding the dataset formulation and testing processes is crucial for reproducibility. We also suggest that the authors consider validating their methodology on additional benchmarks beyond TruthfulQA to demonstrate its generalizability and strengthen the overall validation of their findings.