ID: ddldNozhnM
Title: CLEME: Debiasing Multi-reference Evaluation for Grammatical Error Correction
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Chunk-Level Multi-Reference Metric (CLEME) aimed at addressing the multi-reference evaluation issue in Grammatical Error Correction (GEC). By converting references and source inputs into chunk sequences with consistent boundaries, CLEME seeks to eliminate bias in evaluation. The authors assert that their correction independent assumption allows for effective metric calculations by comparing hypothesis corrections with all golden corrections across references. Experimental results indicate that CLEME effectively addresses the targeted issue.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to understand.
- It highlights an important aspect of GEC, specifically bias reduction in multi-reference evaluation, supported by preliminary experiments.
- The final metric shows strong correlations with human judgments, outperforming many previous metrics.

Weaknesses:
- The chunk-based design may be overly strict, potentially underestimating GEC systems' performance, as illustrated by discrepancies in true and false positives.
- The correction independence assumption's applicability to GEC tasks in general is unclear, and the experiments are limited to datasets based on CoNLL-2014, lacking consideration of other datasets like JFLEG.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the human evaluation details and provide more information about the CLEME toolkit, including core pseudo code and visualization results. Additionally, conducting experiments on other languages, such as Chinese, would strengthen the validation of their metric. We also suggest addressing the readability of Figures 1 and 2 by increasing the font size, as well as clarifying the impact of local versus non-local errors on CLEME's performance. Finally, elaborating on the relationship between their approach and existing metrics like ERRANT would enhance the paper's contribution clarity.