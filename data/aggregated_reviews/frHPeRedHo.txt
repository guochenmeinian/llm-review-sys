ID: frHPeRedHo
Title: Agnostically Learning Single-Index Models using Omnipredictors
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 6, 8, 5, 4, -1, -1
Original Confidences: 4, 3, 4, 1, -1, -1

Aggregated Review:
### Key Points
This paper presents an algorithm for learning Single Index Models (SIMs) with arbitrary monotone and Lipschitz activations under mild distributional assumptions. The authors achieve an error guarantee of $O(B \sqrt{\lambda} \sqrt{\mathrm{opt}}) + \epsilon$, where $B$ is the norm of the weight vector and $\lambda$ bounds the variance. The main contributions include linking the $\ell_2$ loss of bi-Lipschitz activations to matching loss and proposing a method using omnipredictors to minimize matching loss across all 1-Lipschitz activations. The authors also provide stronger guarantees for logistic regression and discuss the inherent dependence of the norm $B$ in the final $\ell_2$ loss bound.

### Strengths and Weaknesses
Strengths:  
- The algorithm operates under very mild assumptions on the marginal distribution.  
- The use of Fenchel duality to analyze the distortion between $\ell_2$ loss and matching loss is innovative and original.  
- The paper addresses a challenging problem in the non-realizable case with relaxed conditions compared to prior works.  

Weaknesses:  
- The presentation lacks clarity, with insufficient definition of symbols and a confusing description of the proposed algorithm.  
- The result regarding the $\ell_2$ error for sigmoid activation appears weak compared to existing literature.  
- The claims of agnostic learnability are misleading, as the paper assumes a bounded second moment, which contradicts the definition of agnostic learning.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by clearly defining all symbols used, particularly those that appear in the proofs. Additionally, the authors should provide a more detailed description of the algorithm and explicitly state the sample complexity and runtime. A thorough comparison with related works would strengthen the significance of their results, particularly regarding the $\ell_2$ error of learning sigmoid activation. Furthermore, we suggest that the authors clarify the claims of agnostic learnability and consider revising Definitions 1.4 and 1.5 to Assumptions for better clarity.