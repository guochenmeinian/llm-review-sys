ID: 2n1Ysn1EDl
Title: MambaLRP: Explaining Selective State Space Sequence Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for applying Layer-wise Relevance Propagation (LRP) to Mamba models, demonstrating that direct application of LRP leads to poor performance. The authors propose modifications to recover propagation rules, resulting in a method that outperforms alternatives. The paper includes various applications, such as identifying gender biases and assessing long-range capabilities of Mamba models. Additionally, the authors provide a comprehensive evaluation of their explanation method for Mamba-based models, distinguishing it from evaluations of the Mamba architecture itself. They emphasize that their work represents the most thorough assessment of various attribution methods for Mamba models to date and plan to include additional NLP experiments on the 2.8B model in the final draft. The reproducibility of their implementation and the benchmarking suite are highlighted as significant contributions, especially given the challenges associated with Mamba and Mamba2 models.

### Strengths and Weaknesses
Strengths:
1. **Impact:** The work addresses an important need for attribution methods in Mamba models, which are gaining traction in the community.
2. **Simplicity:** The method is straightforward with minimal hyper-parameters, making it adaptable for various applications.
3. **Informative Ablation Studies:** The paper provides insightful comparisons and ablation studies that clarify the contributions of each modification.
4. **Applicability:** The use cases demonstrate the method's effectiveness in exploring gender bias and long-range abilities of Mamba models.
5. **Comprehensive Evaluation:** The breadth of evaluation and the comprehensive nature of the experiments are commendable.
6. **Reproducibility:** The authors' commitment to providing a user-friendly GitHub repository enhances reproducibility and accessibility.

Weaknesses:
1. **Comparison with Previous Work:** The paper lacks clarity on which results are reproduced and which are from prior work. Direct comparisons with existing methods, particularly [4] and [Uni], are insufficiently addressed.
2. **Insufficient Empirical Analysis:** The evaluation of long-range capabilities could benefit from comparisons with Transformers and additional empirical justifications for claims made.
3. **Limited Novelty:** The method's novelty is somewhat constrained, primarily involving established techniques applied to Mamba, which may not yield new insights into model behavior.
4. **Initial Submission Concerns:** Some reviewers noted limited empirical analysis in the initial submission, which was only partially addressed in the rebuttal.
5. **Unpublished Status:** The unpublished status of the MambaAttr2024 manuscript raises concerns about the scrutiny of prior evaluations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of comparisons with previous work by explicitly indicating which results are reproduced and ensuring metrics are aligned. Additionally, we suggest including direct comparisons with [Uni] and conducting evaluations on larger Mamba models to demonstrate efficacy. For the long-range experiments, comparing Mamba's performance against Transformers like Pythia could yield valuable insights. Furthermore, empirical justification for claims regarding biases in linear and convolution layers should be provided. We also recommend improving the empirical analysis by including the full analysis between Transformers and Mamba, similar to Figures 6 and 7 in the paper. Lastly, we encourage the authors to ensure that the GitHub repository includes detailed instructions for replicating results and demo Jupyter Notebooks to facilitate ease of use.