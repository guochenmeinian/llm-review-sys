ID: 3F1qEXWKFE
Title: PIVOINE: Instruction Tuning for Open-world Entity Profiling
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to Open World Information Extraction (IE) through instruction tuning, introducing the INSTRUCTOPENWIKI dataset and the PIVOINE model, which is based on BLOOM. The authors propose generating comprehensive entity profiles by integrating multiple tasks, a significant advancement over traditional methods that focus on isolated tasks. The experiments demonstrate PIVOINE's effectiveness in following instructions and extracting entity profiles, showcasing its generalization capabilities on unseen instructions and out-of-ontology cases.

### Strengths and Weaknesses
Strengths:
- The paper addresses a relatively unexplored area in instruction-tuning for IE, providing a substantial dataset and demonstrating the model's superior performance over non-LLM baselines and vanilla ChatGPT.
- The experimental design is comprehensive, and the writing is clear and accessible, making the concepts easy to understand.
- The method of constructing a strictly open-world test set minimizes potential data leakage and enhances the completeness of training annotations.

Weaknesses:
- The task's designation as "open-world" is misleading, as it primarily focuses on entity-related tasks and does not encompass broader open-domain challenges.
- Concerns about data leakage persist, as the dataset is derived from Wikipedia, raising questions about the model's ability to generalize to truly unseen data.
- The evaluation metrics are limited to single metrics without a comprehensive assessment of entity-level performance, and comparisons with other powerful instruction-tuning models are lacking.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the task definition by accurately representing it as "entity profiling" rather than "open-world IE." Additionally, we suggest addressing potential data leakage issues more thoroughly, possibly by providing evidence that the model has not been exposed to the test data. It would be beneficial to include comparisons with existing instruction-tuning models like Alpaca to strengthen the claims of effectiveness. Furthermore, we encourage the authors to expand the evaluation metrics to include comprehensive assessments at the entity level and clarify the criteria for unseen instructions, particularly regarding cross-instructions.