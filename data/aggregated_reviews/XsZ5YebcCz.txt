ID: XsZ5YebcCz
Title: Mildly Constrained Evaluation Policy for Offline Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 3, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Mildly Constrained Evaluation Policy (MCEP) for offline reinforcement learning (RL), addressing excessive policy constraints during test-time inference. The authors propose using a more constrained target policy for value estimation and a less restrictive evaluation policy, demonstrating that milder constraints can enhance performance. Empirical results on the D4RL dataset indicate improvements over vanilla TD3BC and AWAC, although not achieving state-of-the-art results.

### Strengths and Weaknesses
Strengths:
- The investigation of policy constraints in offline RL is significant, and the separation of policies for value estimation and inference is a novel perspective.
- MCEP is easy to implement and can be integrated into various offline RL algorithms.
- The empirical findings are thorough, with extensive ablation studies demonstrating the method's effectiveness and addressing the trade-off between stability and performance.

Weaknesses:
- The primary finding that milder constraints may improve inference is largely based on experimental evaluations, with inconsistent results across tasks (only 6 out of 9 tasks show this pattern).
- The contribution of MCEP is limited, primarily extracting a less restrictive policy post-RL learning, and lacks theoretical support.
- The paper introduces additional hyperparameters that complicate tuning, and the writing could be clearer, particularly regarding notations and definitions.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis to support the findings regarding milder constraints, as this would strengthen the paper's claims. Additionally, clarifying notations in Algorithm 1 and ensuring consistent definitions would enhance readability. We suggest removing line 7 of Algorithm 1 to streamline the process, as $\pi_e$ does not participate in policy evaluation. Furthermore, presenting $\alpha$ values in a table instead of in Figure 5 could improve clarity. Lastly, addressing the inconsistencies in results, particularly in the pen task, would provide a more comprehensive understanding of the method's performance.