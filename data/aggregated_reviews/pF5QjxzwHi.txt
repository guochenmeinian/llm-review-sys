ID: pF5QjxzwHi
Title: Continuous Perception Benchmark
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 6, 4, 4, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark for evaluating visual models' ability to process visual inputs continuously, specifically through a video question answering task that requires models to answer questions about objects in a video. The authors generate 200 synthetic videos featuring a moving camera and use counting instances of objects as the primary task. The benchmark is applied to seven state-of-the-art visual language models (VLMs), revealing their poor performance and highlighting the necessity for advancements in continuous perception.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant problem in the field of continuous perception, emphasizing its importance for next-generation models.
- It is well-written, with clear communication of the motivation and methodology.
- The evaluation includes a comprehensive comparison of various relevant VLMs, demonstrating their struggles with the proposed task.

Weaknesses:
- The benchmark's novelty compared to existing benchmarks like EM-EQA is unclear, as both involve similar video question answering tasks.
- The necessity of "continuous perception" is questioned; subsampling could suffice if done effectively.
- The counting task may not be the best measure of model performance, given known difficulties with counting tasks.
- The reliance on synthetic data raises concerns about the generalizability of results, as models are typically trained on real-world images.

### Suggestions for Improvement
We recommend that the authors clarify how their benchmark differs from the EM-EQA benchmark and explicitly address the significance of continuous perception in their task. Additionally, consider exploring alternative tasks beyond counting, such as those that assess event continuity and causal relationships, which may provide a more robust evaluation of model capabilities. We suggest including baseline classification performance of the models on real-world images to validate the benchmark's focus on continuous perception. Lastly, enhancing the documentation of the data generation process and providing a GitHub repository for the dataset would improve transparency and usability.