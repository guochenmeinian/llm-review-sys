ID: y6Ej5BZkrR
Title: Let's Think Frame by Frame with VIP: A Video Infilling and Prediction Dataset for Evaluating Video Chain-of-Thought
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents VIP, a novel inference-time dataset aimed at exploring video reasoning capabilities through a video chain-of-thought using keyframes. The authors propose two forms of keyframe descriptions: unstructured dense captions and FAMOuS (Focus, Action, Mood, Objects, and Setting) structured scene descriptions. The dataset includes two tasksâ€”Video Infilling, which evaluates models on generating intermediate keyframes, and Video Prediction, which tests their ability to predict future keyframes. The authors benchmark GPT-4 and VICUNA on VIP, demonstrating the challenges existing state-of-the-art models face in complex video reasoning tasks.

### Strengths and Weaknesses
Strengths:
- The construction of the VIP dataset is valid and meaningful, converting video generation into a language-centric task.
- The paper is well-organized, presenting a clear introduction to video reasoning and the motivation behind VIP.
- The proposed tasks provide valuable additions to the video reasoning research landscape, enhancing model evaluation in generating intermediate and future keyframes.

Weaknesses:
- The reliance on a single inference-time dataset for VideoCOT may be insufficient; a larger dataset for instruction-tuning is recommended.
- The evaluation of BERT-Score and Rough-L may focus too heavily on token/word-level similarity, suggesting that Sentence-BERT could be more applicable.
- The evaluation of LLMs could be criticized for using a single few-shot prompt, which may not capture the models' capabilities comprehensively.

### Suggestions for Improvement
We recommend that the authors improve the dataset by proposing a larger dataset for instruction-tuning to enhance video understanding and prediction. Additionally, consider exploring the prediction frame description to generate real frames through Stable Diffusion, as examples would be more engaging. We also suggest trying different methods for keyframe selection, such as those outlined in "An Efficient Keyframes Selection Based Framework for Video Captioning, ACL2021." Lastly, diversifying prompt formats for LLM evaluation, including zero-shot and few-shot techniques with varied wordings, could provide a more comprehensive assessment of model capabilities.