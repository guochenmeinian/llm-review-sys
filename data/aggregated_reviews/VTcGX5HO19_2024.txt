ID: VTcGX5HO19
Title: Bayesian Kernelized Tensor Factorization as Surrogate for Bayesian Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 30
Original Ratings: 4, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 4, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Bayesian Kernelized Tensor Factorization (BKTF) as a novel surrogate model for Bayesian optimization (BO). BKTF employs a low-rank tensor factorization with Gaussian process (GP) priors on latent factors, facilitating the modeling of complex, non-stationary, and non-separable functions. The model utilizes Gibbs sampling for full Bayesian inference and is trained using MCMC sampling, allowing for uncertainty quantification. Experimental results on benchmark functions and hyperparameter tuning tasks indicate that BKTF outperforms standard GP-based BO, particularly under limited sample sizes and evaluation budgets. The authors also explore Gaussian Process (GP) baselines for hyperparameter tuning, specifically focusing on optimizing log marginal likelihood.

### Strengths and Weaknesses
Strengths:
- BKTF introduces a creative approach to extend BO capabilities, effectively modeling complex objective functions.
- The methodology is grounded in a solid mathematical framework, with comprehensive details on model specification and MCMC inference.
- Extensive experiments validate BKTF's advantages over standard GP-BO, showcasing improved optimization performance and sample efficiency.
- The incorporation of tensor decomposition into BO represents a novel and efficient approach, enhancing performance in representing the D-dimensional Cartesian product space.
- The authors demonstrate engagement with reviewer comments and provide clarifications that enhance the paper's clarity.

Weaknesses:
- The comparison of BKTF is limited to basic GP-based BO models, lacking evaluations against advanced GP models like deep kernel learning or other scalable GP variants, which hinders the assessment of its performance gains.
- The relationship between BKTF and existing scalable GP methods is not adequately discussed, raising questions about its unique contributions.
- The experiments conducted are constrained to lower-dimensional synthetic functions, neglecting higher-dimensional benchmarks where BO is most beneficial.
- The MCMC inference may be slow for high-dimensional spaces, and the paper does not report wall-clock times, complicating practical feasibility assessments.
- The stopping criteria for optimization are not adequately defined, potentially leading to confusion about performance metrics.
- The complexity analysis lacks sufficient discussion on the implications of increasing rank \( R \) in the models.
- The GP baselines do not meet the highest standards, particularly in not addressing hyperparameter tuning through fully Bayesian methods.

### Suggestions for Improvement
We recommend that the authors improve the comparison of BKTF by including evaluations against more advanced GP models and other probabilistic regression approaches, such as neural processes or Bayesian neural networks. This would provide a clearer context for assessing the advantages of BKTF. 

We suggest that the authors clarify the relationship between BKTF and existing scalable GP methods, detailing any substantial advantages BKTF offers over these approaches. 

We encourage the authors to expand their experimental evaluations to include higher-dimensional benchmarks, which would better demonstrate the scalability and robustness of BKTF. 

We recommend that the authors report the wall-clock time for MCMC inference to assess the computational feasibility of BKTF, especially in high-dimensional settings. 

We urge the authors to provide a detailed explanation for the strong performance of BKTF in hyperparameter tuning tasks with limited data, ensuring consistency with results from synthetic test functions. 

Additionally, we recommend that the authors improve the clarity of the stopping criteria by ensuring it does not rely on knowledge of the true optimum, which is unavailable in practice. 

We suggest providing a more detailed discussion on the computational cost associated with increasing rank \( R \) to better inform readers of its implications. 

The authors should also ensure that the training procedures for GPgrid and GP models are consistent in their reported costs and include additional baselines in the synthetic problems to strengthen the paper's contributions. 

Finally, we encourage the authors to provide visual results, such as plots or tables, in the main text to further support their findings.