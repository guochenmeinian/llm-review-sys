ID: t9aThFL1lE
Title: UnlearnCanvas:  Stylized Image Dataset for Enhanced Machine Unlearning Evaluation in Diffusion Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 5, 9, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the **UnlearnCanvas** dataset, aimed at evaluating machine unlearning (MU) capabilities in diffusion models concerning artistic styles and associated objects. It identifies three key challenges in the field: the lack of consensus on unlearning targets, insufficient studies on model retainability post-unlearning, and low performance of diffusion model-generated images. The authors propose UnlearnCanvas to address these challenges, highlighting its advantages in dual supervision, retainability analysis, and accuracy enhancement. The dataset includes high-resolution images and detailed class labels, along with eleven benchmarks for evaluating nine MU methods. Furthermore, the paper explores diffusion models and proposes a quantitative comparison between UnlearnCanvas and WikiArt, demonstrating substantial efforts to respond to reviewers' questions during the rebuttal phase.

### Strengths and Weaknesses
Strengths:
1. The dataset and benchmarks facilitate precise evaluations of unlearning methods.
2. The application to style transfer demonstrates versatility.
3. Comprehensive evaluation through a detailed benchmark set enhances the assessment of MU methods.
4. The authors have effectively addressed reviewer concerns, demonstrating a commitment to improving the paper.
5. The responses provided during the rebuttal phase have been deemed satisfactory by at least one reviewer, leading to a raised score.

Weaknesses:
1. The reliance on Fotor for image generation raises concerns about style variability and representation.
2. The dataset's focus on predefined styles may limit broader applicability.
3. The clarity of benchmark results is compromised by unclear metrics and judgment rules.
4. The benchmarks are influenced by the performance of classifiers, raising concerns about their reliability.
5. The novelty of the work compared to existing literature is not clearly established.
6. There may still be lingering concerns from some reviewers that require further clarification.

### Suggestions for Improvement
We recommend that the authors improve the diversity of styles in the dataset to enhance its applicability. Additionally, we suggest clarifying the metrics used in benchmark results to ensure consistent application across cases. It would be beneficial to provide more guidance on the evaluation pipeline and the specific prompts used for metric evaluation. We also encourage the authors to discuss potential future research directions to address the challenges identified in their work. Finally, addressing the clarity of the definitions and implementation details of the benchmarked methods would strengthen the paper. Furthermore, we recommend that the authors improve clarity in their responses to ensure all reviewer concerns are fully addressed and actively seek feedback from reviewers to confirm that their revisions meet expectations.