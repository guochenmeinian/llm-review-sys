ID: Nc6U1Z0DDt
Title: Balaur: Language Model Pretraining with Lexical Semantic Relations
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method of pretraining a BERT-type language model (LM) on semantic relations, specifically focusing on hypernymy, hyponymy, antonymy, and synonymy. The authors propose that by bringing hyponym-hypernym vector pairs closer in the model's internal representations, the performance on hypernymy-related tasks can be improved. The dataset for pretraining is derived from WordNet, and the paper includes a newly created dataset of hypernymy pairs. However, the model's evaluation lacks breadth, as it does not assess performance against a wide range of existing datasets relevant to hypernymy.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and technically sound, providing clear implementation details.
- It introduces a new dataset and a well-motivated pretraining objective.
- The thorough evaluation demonstrates improvements in specific tasks related to lexical semantic relations.

Weaknesses:
- The evaluation appears ad-hoc and does not utilize a variety of relevant datasets.
- The model struggles with basic tasks, such as predicting in the sentence 'y is a type of [mask]', which is unexpected.
- The benefits of pretraining with lexical semantic relation (LSR) constraints are not clearly justified, and comparisons with fine-tuning approaches are missing.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including comparisons with a broader range of existing datasets and tasks relevant to hypernymy. Additionally, clarifying the advantages of pretraining with LSR constraints over fine-tuning would strengthen the paper's claims. The authors should also address the model's performance on simpler tasks and consider exploring autoregressive LMs for better handling of such artifacts. Lastly, providing a clearer distinction between the proposed model and existing LMs in the introduction would enhance the paper's clarity.