ID: ql6LVyi2Dg
Title: Stability and Generalization of the Decentralized Stochastic Gradient Descent Ascent Algorithm
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 5, 5, 6, 6, 5, -1, -1, -1, -1
Original Confidences: 4, 1, 4, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an algorithmic-stability analysis of the decentralized stochastic gradient descent-ascent (D-SGDA) algorithm, focusing on its application to min-max problems in distributed settings. The authors derive generalization bounds for D-SGDA under various convexity-concavity assumptions, specifically addressing weak and strong primal-dual generalization errors. The bounds depend on sample size, training horizon, and properties of the mixing matrix, such as the spectral gap. The results extend existing literature on decentralized gradient descent, providing simplified interpretations of complex bounds. The paper includes empirical experiments that support the theoretical claims regarding the influence of spectral gap and connectivity on generalization performance.

### Strengths and Weaknesses
Strengths:
1. The paper is the first to derive generalization bounds for D-SGDA, providing a unifying analysis of decentralized gradient descent and ascent algorithms.
2. The derived bounds are presented in relatively simple forms, offering insights into the impact of sample size and strong-convexity parameters on generalization.
3. The clarity of the writing facilitates understanding, and the empirical results substantiate the theoretical findings.

Weaknesses:
1. The generalization bounds are often vacuous for common learning rates and training time choices, particularly for strongly convex-strongly concave setups, where meaningful bounds only arise with time-decaying learning rates.
2. The results in Table 1 lack comparisons with Decentralized SGD (D-SGD) and SGDA methods, and the choice of step-size is not specified.
3. Certain assertions regarding the stability of decentralized SGD are misleading, as they do not accurately reflect the implications of the derived bounds.
4. The discussion on optimization guarantees with the proposed learning rate selections is insufficient, which could hinder understanding of final test error rates.

### Suggestions for Improvement
We recommend that the authors improve the comparison of their results with state-of-the-art generalization bounds for ordinary gradient descent-ascent with fixed learning rates to discuss the optimality of their rates. Additionally, the authors should clarify the results in Table 1 by including comparisons with D-SGD and SGDA methods and specifying the step-size for each row. A more precise discussion on the optimization guarantees of D-SGDA with the given learning rate selections would enhance the paper's clarity. Furthermore, we suggest rephrasing certain sections for clarity, particularly in the "Related Works" section and specific sentences that are currently ambiguous. Lastly, addressing the questions regarding the generalization of results to directed graphs and the implications of the mixing matrix would strengthen the analysis.