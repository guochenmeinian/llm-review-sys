ID: yS1dUkQFnu
Title: V-PETL Bench: A Unified Visual Parameter-Efficient Transfer Learning Benchmark
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 8, 5, 7, 7, -1
Original Confidences: 3, 5, 4, 4, -1

Aggregated Review:
### Key Points
This paper presents V-PETL Bench, a comprehensive benchmark for evaluating parameter-efficient transfer learning (PETL) techniques across 30 datasets and 25 algorithms, facilitating fair comparisons in vision tasks. The benchmark includes a novel Performance-Parameter Trade-off (PPT) metric for nuanced algorithm comparison and provides an organized open-source codebase, enhancing reproducibility and usability.

### Strengths and Weaknesses
Strengths:
1. The paper is well-structured and easy to follow.
2. It dedicates substantial GPU time to ensure fair comparisons among PEFT algorithms.
3. The benchmark is open-sourced and well-documented, supporting extensive evaluation across diverse datasets.

Weaknesses:
1. The documentation lacks critical details on how to run the provided code, and training logs and checkpoints are reportedly unavailable.
2. The evaluation requires significant computational resources (310 GPU days), which may limit accessibility for some researchers.
3. The proposed evaluation metric, Performance-Parameter Trade-off (PPT), needs further explanation.

### Suggestions for Improvement
We recommend that the authors improve the documentation by including detailed instructions on running the code and ensuring the availability of training logs and checkpoints. Additionally, we suggest providing a clearer explanation of the insights behind the Performance-Parameter Trade-off (PPT) metric. Lastly, consider using FLOPs as a comparison metric, as the number of parameters and memory may not accurately reflect the computational effort required for fine-tuning models.