ID: CW9SJyhpVt
Title: GV-Rep: A Large-Scale Dataset for Genetic Variant Representation Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 6, 6, 9, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GV-Rep, a benchmark dataset designed to enhance the understanding of genetic variants with over 7 million records and diverse annotations. The authors aim to improve the learning of genetic variant representations in genomic foundation models (GFMs) and provide a resource that bridges genomic data availability with actionable insights. The dataset's structure and experimental setup are well-documented, although the performance metrics indicate that models are not yet saturating their potential.

### Strengths and Weaknesses
Strengths:
- The dataset is extensive and well-annotated, addressing key challenges in genetic variant interpretation.
- It includes a broad range of genetic data and annotations, making it a valuable resource for various fields in genetics and personalized medicine.
- The paper is well-organized, with clear explanations of dataset construction and experimental results.

Weaknesses:
- The distinction between coding and noncoding variants is limited, which may hinder nuanced model evaluations.
- The AUROC scores for different tasks on the GV-Rep dataset are relatively low, indicating room for improvement in model performance.
- The paper lacks a detailed discussion on potential societal impacts and the key motivations behind the study.

### Suggestions for Improvement
We recommend that the authors improve the distinction between coding and noncoding variants to enhance the interpretability of model performance. Additionally, it would be beneficial to include a more comprehensive analysis of the societal implications of using this dataset, particularly regarding clinical decision-making. We suggest that the authors discuss the key motivations and novelty of their work more explicitly. Furthermore, we encourage the development or adaptation of deep learning models to better leverage the dataset's complexity, and to provide a user guide or tutorial for new users to facilitate dataset utilization.