ID: h3lddsY5nf
Title: SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 6, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SPIQA (Scientific Paper Image Question Answering), a large-scale dataset aimed at multimodal question answering on scientific papers, specifically addressing the interpretation of complex figures and tables. The dataset includes 270K questions derived from over 25,000 computer science papers and proposes three task formulations: direct QA with figures/tables, direct QA with full paper text, and chain-of-thought (CoT) QA. The authors evaluate 12 multimodal large language models (MLLMs) and introduce LLMLogScore (L3Score), a new evaluation metric for assessing answer quality.

### Strengths and Weaknesses
Strengths:
- SPIQA is the first large-scale dataset specifically designed for QA with scientific figures and tables, filling a significant gap in the literature.
- The dataset's size and the inclusion of complex visual data enhance the evaluation of AI systems' understanding of scientific literature.
- The introduction of L3Score demonstrates thoughtful consideration of existing evaluation metric limitations.
- The proposed tasks, particularly CoT QA, promote advanced reasoning capabilities in AI systems.

Weaknesses:
- The dataset's novelty may be questioned due to its integration of existing elements, potentially reducing originality.
- There is a reliance on LLMs for generating and evaluating QA pairs, which could introduce biases and affect the dataset's utility for human users.
- Concerns about the quality of the questions, including a lack of inter-annotator agreement metrics and uncertainty regarding test set quality, raise questions about the reliability of the validation process.

### Suggestions for Improvement
We recommend that the authors improve the validation of the generated questions by incorporating human evaluations to distinguish between LLM-generated and human-generated questions. Additionally, we suggest providing measures of inter-annotator agreement for the test set validation process to enhance reliability. It would be beneficial to include an ablation study to clarify whether performance gains stem from large-scale QA pair pretraining or unsupervised training on raw data. Furthermore, we encourage the authors to consider expanding the dataset to include QA in other computer science domains and to conduct additional analysis on the consistency between L3Score and other evaluation metrics.