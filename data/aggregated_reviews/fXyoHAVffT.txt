ID: fXyoHAVffT
Title: Unsupervised Candidate Answer Extraction through Differentiable Masker-Reconstructor Model
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach for unsupervised candidate answer extraction in question answering (QA) using the Differentiable Masker-Reconstructor (DMR) model. The authors propose a method that enhances self-consistency to extract salient information tokens, addressing limitations of existing Named Entity Recognition (NER) methods and deep learning approaches on partially annotated datasets. The DMR model classifies tokens into backbone and information tokens, with experiments conducted on the SQuAD and WH-C datasets. Additionally, the authors release two newly created datasets with exhaustively annotated candidate answers.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to read.
- It proposes an interesting unsupervised approach for answer candidate extraction.
- The DMR model achieves better performance in terms of F1 scores and introduces new datasets for candidate generation.

Weaknesses:
- The meaning of the term P_i in the equations is unclear, leading to potential confusion.
- The effectiveness of the length penalty on the first n tokens is not adequately demonstrated.
- There is insufficient evidence to confirm that the exhaustively annotated data improves QA performance, raising questions about the validity of extracting more answer candidate tokens.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the term P_i in section 3.1 to avoid confusion regarding its meaning. Additionally, the authors should provide more detailed evidence regarding the effectiveness of the length penalty on the first n tokens. It is crucial to verify the ground truth of the exhaustively annotated dataset and demonstrate its impact on QA performance, rather than solely focusing on answer candidate selection. Finally, we suggest including experiments that assess the effects of the proposed method on downstream QA tasks to strengthen the paper's contributions.