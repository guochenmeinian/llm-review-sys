ID: pNnvzQsS4P
Title: KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 6, 7, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to KV Cache compression in Large Language Models (LLMs) called Coupled Quantization (CQ). The authors analyze the correlation between different channels in the KV Cache from an information entropy perspective, revealing significant interdependencies. They propose a multi-channel joint non-uniform quantization method that achieves superior compression performance compared to previous per-channel quantization approaches. Experimental results demonstrate that by combining CQ with a sliding window approach, the KV Cache can potentially be compressed to 1 bit while maintaining model quality.

### Strengths and Weaknesses
Strengths:
- The paper provides a thorough analysis and clear motivation for Coupled Quantization, effectively justifying its design through information entropy.
- CQ shows good performance at low bit-widths, outperforming previous methods without additional overhead.
- The authors demonstrate substantial improvements in inference throughput and batch size compared to the FP16 baseline.

Weaknesses:
- The evaluation is limited to datasets primarily consisting of short-context QA or multiple-choice questions, which may not fully showcase the benefits of KV Cache quantization in long-text scenarios.
- The explanation of channel correlation is incomplete, particularly regarding its variability across different layers, raising questions about the method's general applicability.
- The improvement in inference throughput is mainly observed with large batch sizes, potentially limiting its effectiveness in smaller batch scenarios.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by testing KV Cache quantization on long-context benchmarks or multi-hop retrieval tasks, such as the RULER dataset, to better demonstrate the method's effectiveness. Additionally, we suggest providing a more comprehensive explanation of channel correlation across different layers to clarify the method's applicability. It would also be beneficial to include latency experiments alongside throughput measurements to give a clearer picture of the method's performance. Finally, addressing the overhead associated with storing centroids for each coupled key/value pair would enhance the clarity and robustness of the paper.