ID: vOX7Dfwo3v
Title: Symbol tuning improves in-context learning in language models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel fine-tuning method called symbolic tuning for large language models (LLMs), which replaces natural language labels and instructions with random symbols. The authors claim that this approach enhances performance on in-context learning and algorithmic reasoning tasks, allowing models to override prior knowledge when faced with contradictory information. Extensive experiments demonstrate the effectiveness of symbolic tuning across various NLP benchmarks.

### Strengths and Weaknesses
Strengths:  
- The symbolic tuning approach is simple, effective, and complements instruction tuning.  
- The paper provides a strong motivation for addressing prompt sensitivity in LLM performance.  
- Comprehensive experiments across a broad range of NLP tasks and model sizes support the findings.  
- The methodology is backed by clear analysis of the impact of symbolic tuning in different contexts.  

Weaknesses:  
- Reproducibility is a concern due to the reliance on closed-source models, limiting access for most researchers.  
- The effectiveness of symbolic tuning on open-source LLMs remains unclear.  
- There is a lack of clear evidence supporting the authors' hypotheses regarding the performance improvements from symbolic tuning.  

### Suggestions for Improvement
We recommend that the authors improve reproducibility by providing access to their code upon acceptance. Additionally, we suggest conducting experiments with open-source LLMs, such as Llama 1 and 2, to clarify the effectiveness of symbolic tuning in those contexts. Furthermore, we encourage the authors to strengthen their claims by providing more empirical evidence to support the hypothesized benefits of symbolic tuning, particularly in relation to algorithmic reasoning and flipped labels. Lastly, we advise ensuring adherence to the EMNLP 2023 template for formatting.