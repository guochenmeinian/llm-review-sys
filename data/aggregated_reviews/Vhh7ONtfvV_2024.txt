ID: Vhh7ONtfvV
Title: Decomposing and Interpreting Image Representations via Text in ViTs Beyond CLIP
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 4, 6, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for interpreting Vision Transformers (ViTs) by decomposing their representations and mapping these components to the CLIP space for text-based interpretation. The authors propose the RepDecompose algorithm to automate this decomposition and the CompAlign method to align component contributions with textual descriptions. The framework aims to enhance model interpretability and is demonstrated through applications such as image-text retrieval and spurious correlation mitigation. The authors provide strong experimental evidence for zero-shot spurious correlation mitigation and image-based image retrieval, utilizing RepDecompose to extract component contributions \(c_i\) for representation \(z = \sum c_i\) and employing trained linear maps \(f_i\) from CompAlign to map these components. The scoring function identifies salient properties encoded by \(c_i\), which are then selected or ablated for specific tasks.

### Strengths and Weaknesses
Strengths:  
- The authors tackle an important issue of model interpretability beyond CLIP, addressing a significant bottleneck in understanding various ViT architectures.  
- The paper exhibits clarity and coherence, with effective visual aids that enhance comprehension.  
- The proposed methods provide valuable tools for in-depth analysis of ViTs, with comprehensive ablation studies validating the framework's capabilities.  
- The authors acknowledge the limitations of their metrics and express willingness to conduct additional experiments to clarify concerns.

Weaknesses:  
- The interpretation primarily reflects CLIP features rather than the original models like DINO, raising concerns about the preservation of semantics in the mapping process.  
- The validity of the RepDecompose algorithm is not sufficiently demonstrated, with low match rates questioning its effectiveness.  
- There is a lack of quantitative evaluations for certain applications, such as token contribution visualization and image retrieval performance.  
- The algorithm's description is unclear, with unexplained steps that hinder reproducibility.  
- The interpretation of the mapping process is questioned, with concerns that the experiments may not adequately demonstrate the faithfulness of the component mappings.  
- There is a lack of clarity regarding the tuning of the number of components \(k\) for image-based retrieval, which could lead to misleading results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the algorithm by providing a detailed explanation and an illustrative example for the RepDecompose and CompAlign processes. Additionally, the authors should conduct experiments to validate the preservation of semantics when mapping components to CLIP space, addressing the concerns raised about the interpretation of non-CLIP models. We also suggest including quantitative evaluations for the applications discussed, particularly in token contribution visualization and retrieval tasks, to strengthen the empirical findings. Furthermore, we recommend that the authors explicitly detail the tuning process for \(k\) in the image-based retrieval experiments. Lastly, conducting experiments to directly assess the faithfulness of the mapper components, such as evaluating the accuracy of samples classified differently by DINO and CLIP, and applying interpretation techniques like Grad-CAM or Rollout to analyze feature relevance before and after mapping, would provide a more robust validation of their interpretation technique.