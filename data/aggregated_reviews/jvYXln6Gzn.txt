ID: jvYXln6Gzn
Title: Auxiliary Losses for Learning Generalizable Concept-based Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 27
Original Ratings: 6, 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Coop-CBM, a concept bottleneck model (CBM) that integrates a novel multi-task loss and concept orthogonality loss (COL) to enhance the quality of concept representations while maintaining interpretability. The authors argue that COL encourages disentanglement between concept predictions, thereby improving concept accuracy and downstream task performance. Extensive evaluations across four real-world datasets, including CUB, AwA2, and TIL, demonstrate that Coop-CBMs outperform previous models and standard black-box approaches by 1-3%, particularly under distribution shifts. However, reviewers express skepticism about the relationship between concept orthogonalization and information leakage, suggesting that mutual orthogonalization does not necessarily imply the absence of leakage.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with a clear organization and fluid presentation.
- The concept of Coop-CBMs combined with COL is both simple and effective, leading to increased prediction accuracy, particularly in noisy environments.
- Extensive experiments across multiple datasets and settings provide solid validation of the proposed method, including valuable insights into the robustness of Coop-CBM under extreme conditions.
- The authors effectively clarify the role of COL in enhancing concept representation and accuracy, addressing important issues like information leakage.

Weaknesses:
- Individual contributions, while valuable, may lack significant novelty or impact, particularly the improvements in accuracy and the evaluation under distribution shifts.
- The paper's structure, particularly in the introduction, is somewhat disorganized, making it difficult to follow the progression of ideas.
- The discussion surrounding COL is sometimes confusing and potentially misleading, particularly regarding its implications for concept similarity and orthogonality.
- Notations such as $\hat{\theta}$ and $\hat{\Phi}$ in Section 3 are not clearly defined, leading to confusion.
- The authors do not provide experimental results for CBM alone, which would clarify the performance of Coop-CBM.
- The assumption that COL directly mitigates information leakage is contested; reviewers argue that concept mutual orthogonalization and target information leakage are distinct phenomena.
- The large number of hyperparameters in Coop-CBM's training may limit its practical application.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the statements surrounding COL to avoid confusion, particularly regarding the assumptions about concept orthogonality and its implications for feature representations. Additionally, we suggest conducting a more detailed analysis of the impact of loss weights, especially $\lambda$, and including an ablation study to clarify the robustness of the results. We also recommend improving the structure and content arrangement in the introduction section for clarity and conducting a more thorough literature review in Section 2 regarding the relationship between Concept-Based Models and explainability. Please clarify the notations in the methods section and specify whether the final method is Coop-CBM or Coop-CBM + COL. Including classification results for using only Coop-CBM on the CUB, AwA2, and TIL datasets would be beneficial. Furthermore, discussing the significance of the proposed method beyond benchmark improvements and addressing the limitations of the proposed methods in the Discussion and Conclusion section would strengthen the overall argument and validity of the findings. Lastly, we encourage the authors to include a sigmoidal joint CBM baseline in the comparison to enhance reproducibility and to integrate clipping of concept predictions into the main method to mitigate leakage, as the results indicate it may be beneficial.