ID: sxZlp9ZoHD
Title: Retentive Network
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 5, 4, 6, -1, -1, -1
Original Confidences: 4, 5, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Retentive Network (RetNet) as a foundational architecture for large language models, featuring a multi-scale retention mechanism with three computation paradigms: parallel, recurrent, and chunkwise recurrent. The retention mechanism utilizes a recurrent modeling formulation to derive a parallel formulation, mapping input vectors to state vectors and implementing a linear transform for sequence encoding. The retention layer employs learnable matrices and a complex position embedding, integrating causal masking and exponential decay along relative distance. RetNet achieves low-cost inference, efficient long-sequence modeling, and competitive performance compared to Transformers. Experimental results indicate its superiority in language modeling, inference cost, and training throughput.

### Strengths and Weaknesses
Strengths:
1. RetNet demonstrates competitive performance in language modeling and knowledge-intensive tasks, showing potential to replace Transformers for large language models.
2. It achieves significantly better inference efficiency in terms of memory, speed, and latency.

Weaknesses:
1. The paper presents scaling curves for RetNet and Transformer, noting RetNet's advantage in size scaling beyond 2B, yet lacks a detailed explanation for this trend, which could enhance understanding of RetNet's effectiveness.
2. The use of $\gamma$ in RetNet appears heuristic; while it enhances non-linearity and performance, the rationale for its selection and its impact on model behavior require further clarification.

### Suggestions for Improvement
We recommend that the authors improve the explanation of why RetNet begins to outperform Transformers at model sizes larger than 2B, detailing specific characteristics or mechanisms contributing to this performance. Additionally, exploring different learning rates and their effects on RetNet's scalability would be beneficial. A more comprehensive discussion on the selection of $\gamma$, its effects on performance, and sensitivity analysis could strengthen the argument for its use. Furthermore, we suggest including comparisons with other linear attention models on downstream tasks using standard metrics, and addressing the lack of evaluation on open-source pretraining data to enhance reproducibility. Lastly, incorporating training loss curves for both Transformer and RetNet would provide valuable insights into their performance dynamics.