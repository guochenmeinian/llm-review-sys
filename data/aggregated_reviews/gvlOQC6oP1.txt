ID: gvlOQC6oP1
Title: Image Copy Detection for Diffusion Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel task of Image Copy Detection for Diffusion Models (ICDiff), focusing on detecting and evaluating the degree of image copy for images generated by text-to-image (T2I) generative diffusion models. The authors propose a new dataset, D-Rep, created by generating 40k images using prompts from DiffusionDB and scoring them with human annotators. They introduce a probability density-based embedding model (PDF-Embedding) to score the degree of image copy, demonstrating its effectiveness in experiments. However, the quantitative experiments are limited to the D-Rep dataset, and comparisons with other methods may be unfair due to differences in training.

### Strengths and Weaknesses
Strengths:
- The writing is clear, and the paper's structure is easy to follow.
- The task of Image Copy Detection for Diffusion Models is novel and meaningful, addressing important copyright issues.
- The proposed PDF-Embedding serves as an efficient baseline for the task.

Weaknesses:
- The paper lacks completeness, frequently referring to the appendix for critical details, such as model training and architecture.
- Quantitative experiments are limited to one dataset generated by a single T2I model, hindering validation of the proposed method's effectiveness across different generative models.
- Comparisons with other methods are unfair, as many were not trained on the D-Rep training split.
- Sections 5.2 and 5.3 contain significant overlap.
- Missing details regarding the dataset's label distribution and the labeling process.

### Suggestions for Improvement
We recommend that the authors improve the completeness of the paper by including essential details on model training, architecture, and comparison methods directly in the main text rather than relying on appendix references. Additionally, conducting more comprehensive experiments across various datasets and generative models would strengthen the validation of the proposed method. To ensure fair comparisons, either train the compared methods on the same training split or evaluate them in a zero-shot manner. We also suggest clarifying the rationale behind the use of multiple vector sets for similarity measurement and considering a continuous labeling approach for practical applications. Lastly, providing more information on the dataset's label distribution and the labeling process would enhance the paper's robustness.