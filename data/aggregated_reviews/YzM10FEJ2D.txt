ID: YzM10FEJ2D
Title: Touchstone Benchmark: Are We on the Right Way for Evaluating AI Algorithms for Medical Segmentation?
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Touchstone, a large-scale benchmark for evaluating AI algorithms in medical image segmentation, specifically for abdominal CT volumes. The authors claim to address limitations of existing benchmarks, including small test sets, in-distribution evaluation, and oversimplified metrics. The four main contributions are: 1) a large and diverse dataset comprising 5,195 CT volumes from 76 hospitals for training and 6,933 CT volumes from 8 additional hospitals for testing; 2) out-of-distribution (OOD) evaluation for a more realistic assessment of algorithm generalization; 3) comprehensive evaluation of 16 state-of-the-art segmentation algorithms; and 4) analysis of algorithm performance across different demographic groups and imaging characteristics. The authors also emphasize the importance of comprehensive metadata for reliable conclusions, acknowledging limitations due to inconsistent metadata availability, particularly regarding race. They highlight the multi-confounder effect, noting that variations in patient demographics can lead to inconsistent performance observations. The authors have released detailed performance metrics and data on GitHub to facilitate further analysis. While the paper is strong and impactful, some claims appear overstated and require qualification.

### Strengths and Weaknesses
Strengths:  
1) The scale and diversity of the dataset effectively address key limitations in existing medical AI benchmarks.  
2) The comprehensive evaluation of top segmentation algorithms provides valuable insights into the current state-of-the-art.  
3) The analysis across various demographic subgroups is crucial for understanding potential biases.  
4) The authors' commitment to maintaining and expanding the benchmark is commendable.  
5) Significant revisions have been made to address reviewer feedback, enhancing the manuscript's quality.  
6) The release of performance metrics and the live leaderboard for the Touchstone Benchmark increases transparency and accessibility.

Weaknesses:  
1) The focus on a single task (anatomical segmentation) limits the benchmark's scope.  
2) Claims regarding the benchmark's "fairness" and "widespread adoption" are unclear and may be overstated.  
3) The reliance on standard metrics like the Dice score, despite criticism of oversimplified metrics, raises questions about metric diversity.  
4) The analysis of model architectures is somewhat superficial, lacking deeper insights.  
5) The inconsistency of metadata across datasets remains a critical limitation that affects the analysis.  
6) The authors acknowledge that their initial claims lacked sufficient evidence, indicating a need for more rigorous analysis.

### Suggestions for Improvement
We recommend that the authors improve the paper by addressing the following questions in their rebuttal:  
1) Metric diversity: Elaborate on the reliance on Dice scores and consider including more advanced or task-specific metrics to enhance the benchmark's value.  
2) Architectural insights: Provide deeper analysis on why certain architectures perform better on specific tasks or anatomical structures.  
3) Clinical relevance: Discuss plans to incorporate more clinically relevant tasks, such as pathology detection or severity grading, in future iterations.  
4) Error analysis: Conduct a thorough error analysis to identify common failure modes across algorithms and specific challenges posed by certain images or anatomical variations.  
5) Longitudinal evaluation: Clarify plans for maintaining and updating the benchmark over time to ensure its continued relevance.  
6) Metadata consistency: Improve the consistency of metadata across datasets to strengthen the analysis and control for multiple combinations of confounders to enhance the robustness of findings.  
7) Clarify the annotation protocol for the JHH subset and address potential issues regarding metadata consistency across datasets, as this impacts the validity of claims made in the paper.  
8) Continue to engage with the community for additional collaborations to enrich the dataset with detailed metadata.