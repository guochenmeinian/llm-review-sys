ID: t8M1mNueVf
Title: Doubly Calibrated Estimator for Recommendation on Data Missing Not At Random
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel "Doubly Calibrated Estimator" methodology that addresses selection bias in recommender systems by recalibrating both imputation and propensity models. The authors argue that existing doubly robust (DR) estimators often produce miscalibrated errors and propensity scores, and their approach incorporates calibration experts for different user groups, enhancing estimator accuracy and reliability. Extensive experiments on real-world datasets demonstrate the proposed method's effectiveness in reducing bias and variance without requiring additional unbiased data.

### Strengths and Weaknesses
Strengths:
- The paper provides an interpretable theoretical analysis linking calibration errors to the bias and variance of DR estimators.
- Extensive experiments and an ablation study validate the proposed approach and its advantages over existing methods.
- The manuscript is well-written, making the arguments clear and easy to follow.

Weaknesses:
- The discussion of related work on calibration is insufficient, particularly regarding off-policy evaluation literature, which could enhance reader understanding.
- The novelty of adding calibration to DR estimators may be marginal, and the tightness of the provided bounds on bias and variance is unclear.
- The empirical evaluation is limited to small datasets, which may not reflect real-world scalability and effectiveness.
- The absence of online A/B testing raises concerns about the practical applicability of the results.

### Suggestions for Improvement
We recommend that the authors improve the discussion of related work on calibration, particularly referencing Raghu et al. (2018) and other recent studies. Additionally, we encourage the authors to clarify the role of maximum calibration error (MCE) in their analysis, as its redundancy with expected calibration error (ECE) is unclear. To strengthen the theoretical framework, we suggest testing the plausibility of the bounds using synthetic data. Furthermore, conducting experiments on larger datasets would provide a more rigorous evaluation of the proposed method's scalability. Lastly, we advise the authors to consider performing online A/B tests to validate the empirical results in a practical setting.