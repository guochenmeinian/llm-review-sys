ID: 08GbdALmEs
Title: Learning Versatile Skills with Curriculum Masking
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 5, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CurrMask, a novel masked prediction approach for unsupervised reinforcement learning (RL) pretraining that learns skills of varying complexity through block-wise masking and adaptively adjusts masking schemes for training efficiency. Unlike previous methods that utilize random token-level masking, CurrMask employs a pool of masking schemes with different block sizes to capture temporal dependencies at various scales. The method trains a bandit model to schedule these masking schemes, using target loss decrease as the reward. Extensive evaluations across three downstream tasks in the DeepMind control suite demonstrate CurrMask's strong empirical performance in zero-shot inference and finetuning, generally outperforming baseline methods.

### Strengths and Weaknesses
Strengths:
- The framework creatively integrates curriculum skill learning into masked prediction for RL pretraining, enhancing long-term reasoning and training efficiency.
- The method shows performance improvements across various downstream tasks.
- A comprehensive analysis elucidates the effectiveness of each component in the proposed method.
- The paper is well-written and structured.

Weaknesses:
- The interleaving of masked prediction pretraining with a bandit training process may introduce instability due to non-stationary reward distributions.
- The definition of "skill" could be more precise, as not all state-action sequences are beneficial for downstream tasks; random masking may lead to memorization of arbitrary segments rather than reusable behaviors.
- The necessity of block-wise masking is questioned, as varying mask ratios could achieve similar outcomes without it.
- The clarity of Figure 4(a) as evidence of long-term prediction capability is insufficient, and the impact of prediction horizon on attention maps is unclear.
- The stability of reward calculation with varying intervals ($I$) remains uncertain.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the "task selection" method, specifically explaining the roles of \(\omega_i\), \(\omega'_i\), and \(K\) in Section 4.3, as well as the intuition behind the equations presented. Additionally, we suggest including comparisons with existing skill learning methods that utilize offline data to strengthen the empirical foundation of the work. Visualizations of the proposed masking curriculums would enhance understanding of their impact on performance. Furthermore, conducting experiments to compare token-wise and block-wise masking would substantiate the choice of block-wise masking. Finally, a time complexity analysis of the method should be included to provide insights into computational efficiency.