ID: j4QVhftpYM
Title: Resolving the Tug-of-War: A Separation of Communication and Learning in Federated Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 7, 6, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a two-layer federated learning (FL) framework called FedSep, which separates communication and learning parameters. The authors propose a bilevel optimization formulation that guarantees convergence and addresses communication-efficient FL and model-heterogeneous FL. The framework is novel, with a solid theoretical convergence analysis and practical applications. Additionally, the authors investigate the performance of the FedSep algorithm under varying compression rates, proposing a correction to the error-feedback term in the Top-K baseline, which they found to weaken the benefits of error-feedback. The results indicate significant accuracy improvements for both I.I.D and Non I.I.D MNIST datasets, although Top-K diverges at high compression rates, while FedSep maintains good performance. The authors also discuss the challenges of analyzing non-smooth, non-strongly-convex optimization problems and plan to conduct a grid search for optimal learning rates.

### Strengths and Weaknesses
Strengths:
- FedSep offers an innovative approach by separating communication and learning in FL.
- The theoretical convergence analysis demonstrates a sublinear rate.
- The framework effectively addresses communication-efficient and model-heterogeneous FL scenarios.
- The authors provide a detailed analysis of the FedSep algorithm and its performance metrics, demonstrating significant improvements in accuracy.
- Empirical results validate the proposed corrections and adjustments to the algorithm.
- The authors acknowledge the complexity of the optimization problem and outline future research directions.

Weaknesses:
- The convergence rate is sublinear without showing speedup concerning the number of clients or local steps, raising questions about its competitiveness with standard FL algorithms.
- In model-heterogeneous FL, validation data is used in training, which may bias comparisons with other algorithms.
- The reliance on analyticity and strong convexity in the second-level problem may not hold in many cases, particularly for the formulation in Section 4.1.
- The convergence analysis is limited due to the non-smooth and non-strongly-convex nature of the optimization problem, which the authors plan to address in future work.
- The Top-K method's divergence under high compression rates raises concerns about its reliability in practical applications.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind the bilevel formulation and its advantages over existing communication-efficient algorithms. It would be beneficial to formalize the tension between communication and learning more explicitly. Additionally, we suggest addressing the concerns regarding the use of validation data in training for model-heterogeneous FL to ensure fair comparisons. The authors should also clarify the computational overhead introduced by the encoder/decoder structure and provide a detailed discussion of the technical challenges and novelties in extending common bilevel optimization algorithms. We recommend improving the convergence analysis by exploring the implications of the non-smooth, non-strongly-convex optimization problem in greater depth. Furthermore, we suggest conducting a comprehensive grid search for learning rates for each algorithm to identify optimal settings, as this could enhance the performance of the Top-K method and provide more robust results. Finally, please ensure that the updated results from the grid search are included in the final version of the paper.