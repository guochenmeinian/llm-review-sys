ID: fCvJrponuK
Title: Sparse Black-Box Multimodal Attack for Vision-Language Adversary Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Sparse Multimodal Attack (SparseMA), a black-box adversarial attack method targeting vision-language multimodal classifiers. The authors propose a novel strategy that discretizes input images and performs attacks in the joint space of image patches and text tokens, demonstrating superior performance compared to baselines that attack image and text spaces separately. The effectiveness of SparseMA is evaluated against three baseline methods across distinct datasets.

### Strengths and Weaknesses
Strengths:
- The research motivation is effectively presented, particularly through Figure 1.
- The paper provides a clear algorithmic description and overview of SparseMA, enhancing comprehensibility.
- SparseMA achieves the highest attack success rate in 7 out of 9 cases, surpassing all unimodal and multimodal baseline methods.

Weaknesses:
- The claim that the proposed attack preserves semantic integrity is contradicted by observations in Figure 1, necessitating clarification.
- The distinctiveness of the authors' contributions is undermined by existing methods that can convert white-box attacks to black-box attacks.
- The organization of the manuscript could confuse readers, as the focus on vision-language multimodal classification is only clarified in the experiments section.
- The paper lacks sufficient implementation details for the Sp-RS&PWWS baseline and does not adequately define the 'multimodal similarity' metric, raising concerns about the validity of comparisons made.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the semantic integrity of adversarial images, particularly in relation to Figure 1. Additionally, the authors should provide a more detailed explanation of the uniqueness of their contributions compared to existing methods. We suggest that the authors clarify their focus on vision-language multimodal classification earlier in the manuscript to avoid reader confusion. Furthermore, the authors should include more implementation details for the Sp-RS&PWWS baseline and provide a clear definition of the 'multimodal similarity' metric to enhance the validity of their comparisons. Lastly, addressing the computational overhead associated with Replacement Order Determination and Candidate Substitution would strengthen the paper.