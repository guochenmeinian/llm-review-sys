ID: zb8jLAh2VN
Title: Inference of Neural Dynamics Using Switching Recurrent Neural Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 5, 6, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a switching RNN (SRNN) framework designed to model neural activity, extending switching linear dynamical system models to accommodate non-linear dynamics. The authors propose that these non-linear dynamics enhance the model's expressiveness, allowing for improved segmentation of neural activity into distinct behavioral states. The SRNN is fitted using variational inference and is applied to synthetic data and three distinct neural datasets, demonstrating superior performance over SLDS and rSLDS in segmenting activity. Additionally, the paper provides a comprehensive analysis of determining relevant hyperparameters, specifically the number of discrete and continuous latent states, through a hyperparameter sweep on reaching and decision-making datasets. The authors report that 61% of SRNNs with a higher number of discrete hidden states converge to the optimal number of states (K=5), while 94% of those with fewer states (K=4) reuse hidden states. The reconstruction performance plateaus at the optimal number of discrete states, and variability metrics indicate that K=5 yields lower variability compared to K=4 and K=6. The authors also implement a ‘co-smoothing’ method, showing that K=5 performs well in reconstructing data, and discuss the implications of their findings on model interpretability and performance.

### Strengths and Weaknesses
Strengths:  
1. The paper addresses the need for increased model complexity to leverage large-scale neural datasets by incorporating non-linear dynamics and a fitting approach suitable for such data.  
2. The extensive experimental section, particularly the results on the decision-making dataset, showcases the model's effectiveness across multiple neural datasets.  
3. The literature review is comprehensive, effectively contextualizing the work within existing studies.  
4. The paper offers a detailed exploration of hyperparameter selection metrics, including convergence, reuse of states, reconstruction performance, and variability across conditions.  
5. The implementation of a ‘co-smoothing’ method enhances the robustness of the findings.  
6. The results demonstrate the utility of SRNNs in recovering behaviorally relevant states effectively.

Weaknesses:  
1. The authors do not provide an experimental comparison with existing models like SNLDS, which is crucial given the similarities in motivation and novelty.  
2. The subjective nature of behavioral segmentations raises concerns about the model's ability to infer the number of discrete states from data, as the authors set this number based on known behavioral states.  
3. The paper requires editing for proper reference formatting and clarity, particularly in the explanation of variational inference and the details of experiments like the Lorenz attractor setup.  
4. Some results appear inconsistent, as prediction and reconstruction performance across models seem similar despite significant discrepancies in inferred discrete states.  
5. There are concerns regarding the impact of the proposed method on the broader neuroscience community, particularly related to interpretability.  
6. The novelty of the approach compared to existing models like ARHMMs, SLDS, and mrSDS is not convincingly established, raising questions about its originality and innovation.

### Suggestions for Improvement
We recommend that the authors improve the comparison with existing models such as SNLDS to clarify the contributions of their work. Additionally, the authors should explore methods for automatically determining the number of discrete states, potentially employing "co-smoothing" as suggested. We also suggest that the authors clarify the mathematical details, particularly in Equation (2), and ensure that the cross-validation techniques are explicitly described for each dataset. Furthermore, we recommend that the authors improve the clarity of the contribution by providing a compelling description of the novelty of their approach relative to existing models, particularly ARHMMs and their extensions. Including comparisons with simpler models such as ARHMMs could strengthen the argument for the advantages of SRNNs. Lastly, addressing the interpretability challenges and demonstrating how their method can be applied in real-world neuroscience contexts would enhance the paper's impact.