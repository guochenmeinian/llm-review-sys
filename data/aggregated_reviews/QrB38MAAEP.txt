ID: QrB38MAAEP
Title: Towards a Unified Framework of Contrastive Learning for Disentangled Representations
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 6, 7, 7, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 3, 4, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new dissimilarity measure function for contrastive representation learning, addressing the identifiability of factors of variation across four contrastive objectives. The authors establish a connection between learned representations and true factors, extending their findings to non-convex latent spaces and non-uniform marginal distributions. Empirical validation is conducted on various datasets, and the theoretical framework is built upon previous work while relaxing certain assumptions.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written and effectively positions its approach within the literature of contrastive learning.
- The proposed similarity measure function is straightforward to implement.
- The discussion on strong and weak identifiability under varying conditions is insightful, and the mathematical derivations are sound.
- The theoretical contributions are significant, particularly the removal of the independence assumption in disentangled representation analysis.

Weaknesses:
- The problem definition is somewhat unclear, particularly regarding the practical identifiability of true factors when they are typically unknown in contrastive learning.
- There is a lack of discussion on the necessity of data augmentations when training with contrastive loss.
- The gap between theoretical claims and practical experimental settings raises concerns about the applicability of the findings.
- The contributions of existing related studies are not adequately discussed, particularly those that also address disentanglement without independence assumptions.
- A fair comparison between contrastive objectives is needed, especially regarding the modifications made to the original spectral contrastive learning objective.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the problem definition, specifically addressing how identifiability can be practically achieved when true factors are unknown. Additionally, we suggest including a discussion on the role of data augmentations in training with contrastive loss. It would be beneficial to explicitly state the contributions of related studies that have tackled similar issues, particularly those that do not rely on independence assumptions. Furthermore, we advise conducting a thorough comparison of the modified objective function with the original spectral contrastive learning objective to clarify any potential numerical instabilities. Lastly, enhancing the discussion on the practical applicability of the theoretical claims will strengthen the manuscript's significance.