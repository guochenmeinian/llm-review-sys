ID: kJkp2ECJT7
Title: Towards Flexible Visual Relationship Segmentation
Conference: NeurIPS
Year: 2024
Number of Reviews: 22
Original Ratings: 5, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a flexible framework for human-object interaction (HOI) detection, scene graph generation (SGG), and referring expression comprehension (REC) tasks. The authors propose leveraging pretrained vision-language models to ground textual features to visual relationships in images, achieving state-of-the-art (SOTA) performance on various visual relationship detection and segmentation tasks. The integration of standard, promptable, and open-vocabulary capabilities is a notable aspect of the proposed model.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow.
2. The problem addressed is novel, unifying various visual relationship detection and segmentation tasks, which could stimulate new research interests.
3. The method demonstrates competitive performance, achieving SOTA results on most visual relationship detection tasks.

Weaknesses:
1. The significant performance improvement is largely attributed to the mask head from SAM, raising questions about the unified framework's benefits when only the box head is used, as it underperforms compared to current SOTA methods.
2. The major technical contributions remain unclear, as the model architecture and losses closely resemble previous dual-decoder architectures.
3. The experimental comparisons may be unfair due to differing backbone models, and results on REC benchmarks are missing despite being mentioned in the abstract.
4. The necessity for segmentation over traditional detection tasks is not adequately justified, and more visualizations of fine-grained masks are needed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their technical contributions, particularly in distinguishing their model from previous works. Additionally, the authors should provide a more thorough justification for using segmentation instead of detection and include results on REC benchmarks. To enhance the experimental comparisons, we suggest using consistent backbone models across methods. Finally, incorporating more visualizations of the generated masks would help demonstrate the effectiveness of the proposed approach.